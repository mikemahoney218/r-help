From @@h|mk@poor @end|ng |rom gm@||@com  Wed Jun  1 07:54:21 2022
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Wed, 1 Jun 2022 11:24:21 +0530
Subject: [R] R_LIBS var needed to be set after upgrade to R 4.2.2
Message-ID: <CAC8=1eppHfgnZkhE2tS_ko3Db0DNSMcs0fbYR++nYNsWqMLpvQ@mail.gmail.com>

Dear All,

I upgraded to R 4.2.2 on Debian 10 today.

The R shell incantation worked fine and all libraries would load but,
I needed to point the R_LIBS variable to
/usr/local/lib/R/site-library/ in order for the R --vanilla < myfile.R
incantation to find the libraries.

May I ask, why was this ? I never needed to do this on any previous
upgrade to R.

Many thanks,
Ashim


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Wed Jun  1 10:35:48 2022
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Wed, 1 Jun 2022 10:35:48 +0200
Subject: [R] R_LIBS var needed to be set after upgrade to R 4.2.2
In-Reply-To: <CAC8=1eppHfgnZkhE2tS_ko3Db0DNSMcs0fbYR++nYNsWqMLpvQ@mail.gmail.com>
References: <CAC8=1eppHfgnZkhE2tS_ko3Db0DNSMcs0fbYR++nYNsWqMLpvQ@mail.gmail.com>
Message-ID: <25239.9444.688970.964168@stat.math.ethz.ch>

>>>>> Ashim Kapoor 
>>>>>     on Wed, 1 Jun 2022 11:24:21 +0530 writes:

    > Dear All,

> I upgraded to R 4.2.2  on Debian 10 today.

Well, I assume you mean R 4.2.0 .. at least that one exists.

    > The R shell incantation worked fine and all libraries would load but,
    > I needed to point the R_LIBS variable to
    > /usr/local/lib/R/site-library/ in order for the R --vanilla < myfile.R
    > incantation to find the libraries.

you mean other installed *packages*

    > May I ask, why was this ? I never needed to do this on any previous
    > upgrade to R.

Well,  for me, the     R --vanilla   form also only sees the
29 (14 "base" + 15 "Recommended") packages that come with R.

Debian (& Ubuntu etc)  have used a similar setup where the
default {R-level}  .libPaths() has contained three libraries,
via

R_LIBS_SITE=${R_LIBS_SITE-'/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library'}

see

   https://cloud.r-project.org/bin/linux/debian/#pathways-to-r-packages

also for much more.
Note (also from the above CRAN page
      https://cloud.r-project.org/bin/linux/debian/
   [ Remember "menu"  "Binaries" -> "Linux" -> "Debian" ]
   
The good thing about the Debian (and derivatives) setup is that
it also separates (as I do) the  "packages that come with R" in
one library (= /usr/lib/R/library) from packages that are
installed differently.


    > Many thanks,
    > Ashim


From @@h|mk@poor @end|ng |rom gm@||@com  Wed Jun  1 11:00:58 2022
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Wed, 1 Jun 2022 14:30:58 +0530
Subject: [R] R_LIBS var needed to be set after upgrade to R 4.2.2
In-Reply-To: <25239.9444.688970.964168@stat.math.ethz.ch>
References: <CAC8=1eppHfgnZkhE2tS_ko3Db0DNSMcs0fbYR++nYNsWqMLpvQ@mail.gmail.com>
 <25239.9444.688970.964168@stat.math.ethz.ch>
Message-ID: <CAC8=1er4BwEJ0zousw0rC-g75G8HviGxqZ+1jB0heD2379x-wA@mail.gmail.com>

Dear Sir,

> > I upgraded to R 4.2.2  on Debian 10 today.
>
> Well, I assume you mean R 4.2.0 .. at least that one exists.

My bad, yes I made a typo. I did mean R 4.2.0.

>     > The R shell incantation worked fine and all libraries would load but,
>     > I needed to point the R_LIBS variable to
>     > /usr/local/lib/R/site-library/ in order for the R --vanilla < myfile.R
>     > incantation to find the libraries.
>
> you mean other installed *packages*
>
>     > May I ask, why was this ? I never needed to do this on any previous
>     > upgrade to R.
>
> Well,  for me, the     R --vanilla   form also only sees the
> 29 (14 "base" + 15 "Recommended") packages that come with R.

Has this ALWAYS been the case for you ? Even with prior versions of R ?

> Debian (& Ubuntu etc)  have used a similar setup where the
> default {R-level}  .libPaths() has contained three libraries,
> via
>
> R_LIBS_SITE=${R_LIBS_SITE-'/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library'}
>
> see
>
>    https://cloud.r-project.org/bin/linux/debian/#pathways-to-r-packages
>
> also for much more.
> Note (also from the above CRAN page
>       https://cloud.r-project.org/bin/linux/debian/
>    [ Remember "menu"  "Binaries" -> "Linux" -> "Debian" ]
>
> The good thing about the Debian (and derivatives) setup is that
> it also separates (as I do) the  "packages that come with R" in
> one library (= /usr/lib/R/library) from packages that are
> installed differently.

My confusion is : Earlier R --vanilla incantation was working fine,
even without my intervening and
adding to R_LIBS. That is why I was confused.

My main query is : Is there anything special to R 4.2.0 which needs
R_LIBS to be setup seperately?

THAT is my query.

Best Regards,
Ashim


From j@de@shod@@ m@iii@g oii googiem@ii@com  Wed Jun  1 10:12:25 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Wed, 1 Jun 2022 09:12:25 +0100
Subject: [R] mgcv: concurvity/ auto-correlation in GAM predicting deaths
 from weather time series with distributed lag
Message-ID: <CANg3_k_YSkTKLgSkWkEuxuFe_3tOaC0vBxBAUSt7KucbGZ5h=w@mail.gmail.com>

Hello all,

First posting to this list, hope you can bear with me - not a
statistician here... (my field is public health).

Have tried posting on Cross Validated (stack exchange) but it seems
that there are not many people there working with GAMs. Have been
doing quite a bit of reading/ googling, but am being halted by my lack
of knowledge of matrix algebra (not a mathematician here either) and
don't know anyone who works with GAMs, so this list really is my last
hope in my struggles.

My question is: what should I be more worried about in a time series
GAM with distributed lag? Highly significant k-index for time terms or
high concurvity between time and explanatory variables? Or are both
two sides of the same coin?  And what should/ can I do about either?
For description of the problem, please see below.

I am running various GAM models (using mgcv) to estimate daily number
of deaths from daily time series of several weather variables such as
temperature, humidity and precipitation among others. The primary aim
is to get more insight into the (complex) relationship between these
variables (rather than pure prediction performance). The models have
distributed lag terms because deaths may occur over several days
following exposure. Modelling lag is based on the  example in Simon
Wood's 2017 book, p. 349 (and the gamair package documentation
(https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)).
Code is listed at the bottom of this post. Data (as well as R script,
and model output files) are available on Github:
https://github.com/JadeShodan/heat-mortality/tree/main/social_media_posts/r_help/concurvity

Model m1 below is relatively simple with just maximum temperature,
total daily precipitation and lag, and default k for (year) and
seasonality (doy = day of year). (Heap is a categorical variable
coding for the fact that all deaths for which no specific date was
known (data is from a low income country with problematic death
registration) were assigned to the 15th day of the month in which the
deaths were thought to have occurred. This categorical variable has
169 levels (0 for all non-heaping days, 1 for the first heaping day, 2
for the second ? 168 for the final heaping day over the 14 year
period).

When I run this model:

m1 <- gam(deaths~te(year, doy, bs = c("cr", "cc")) + heap +
                                te(temp_max, lag, k=c(10,4))+
te(precip_daily_total, lag, k=c(10,4)),
                                data = dat, family = nb, method =
'REML', select = TRUE, knots = knots)

I get a highly significant k-index for (year, doy) - not sure why I am
getting NAs either:

                                         k'               edf
k-index   p-value
te(year,doy)                     19.00000 13.17462    0.93     <2e-16 ***
te(temp_max,lag)            39.00000  4.23584      NA      NA
te(precip_daily_total,lag) 36.00000  0.00929      NA      NA

Concurvity between (year, doy) and (temp_max, lag) is very high too (0.83):

                                              para
te(year,doy)      te(temp_max,lag)      te(precip_daily_total,lag)
para                                 1.000000e+00     2.437000e-31
    0.3324461                 0.6666532
te(year,doy)                      2.149940e-31     1.000000e+00
0.8285219                  0.5601749
te(temp_max,lag)            3.329829e-01      8.268335e-01
1.0000000                  0.5861324
te(precip_daily_total,lag) 6.666532e-01      5.601749e-01
0.5975987                 1.0000000

To reduce significance of p for the k-index, increasing k helps
somewhat, but only up to a point:

m2 <- gam(deaths~te(year, doy, bs = c("cr", "cc"), k=c(7, 20)) + heap
+ te(temp_max, lag, k=c(10,4))+ te(precip_daily_total, lag,
k=c(10,4)), data = dat, family = nb, method = 'REML', select = TRUE,
knots = knots)

                                                k'      edf
k-index p-value
te(year,doy)                     132.0000  24.9619    0.94    0.01 **
te(temp_max,lag)            39.0000   3.8914      NA      NA
te(precip_daily_total,lag)  36.0000   0.0128      NA      NA

However, with increasing k for (year, doy) concurvity for (temp_max,
lag) and (precipitation, lag) with  (year, doy) increases even more
(which also happens in models with predictors other than temp_max):

                                           para
te(year,doy)           te(temp_max,lag)     te(precip_daily_total,lag)
para                                   1.000000e+00    3.260062e-32
    0.3235908                  0.6666532
te(year,doy)                       4.530791e-32     1.000000e+00
 0.9206842                  0.7261335
te(temp_max,lag)              3.298028e-01     9.157306e-01
1.0000000                  0.5831089
te(precip_daily_total,lag)   6.666532e-01     7.261335e-01
0.5805672                  1.0000000

Is this happening because of the lag term? Should I be worried about
this? Is there anything I can do about this? I saw lecture slides by
Simon Wood where he advises that the functions sp.vcov and gam.vcomp
can help shine light on the problem, but I don't know how to interpret
the output from these functions.

In models with more explanatory weather variables I also have high
concurvity - over 0.80 - between e.g. lagged humidity and lagged
temperature. Both variables are of interest (in fact the purpose of
this study) and should not be dropped, but I guess that's another
story. I posted about it on Cross Validated:
https://stats.stackexchange.com/questions/576688/assessing-impact-of-concurvity-in-gam-using-time-series-of-weather-and-mortality
and any thoughts about this are welcome too.

Posting output from summary() and gam.check() would make this post too
long, so instead I've put text files on Github:

output model 1:
https://github.com/JadeShodan/heat-mortality/blob/main/social_media_posts/r_help/concurvity/model_m1_output.txt

ouput model 2:

https://github.com/JadeShodan/heat-mortality/blob/main/social_media_posts/r_help/concurvity/model_m2_output.txt


I'd be very grateful for any help anyone might be able to offer!

Jade


###############################  Model code  ################################
library(readr)
library(mgcv)

df <- read_rds("data_crossvalidated_post2.rds")
# data available on github: https://github.com/JadeShodan/heat-mortality

# Create matrices for lagged weather variables (6 day lags)
lagard <- function(x,n.lag=7) {
n <- length(x); X <- matrix(NA,n,n.lag)
for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
X
}

dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
deaths=df$deaths_total, doy=df$doy, year = df$year, month = df$month,
weekday = df$weekday, week = df$week, monthday = df$monthday, time =
df$time, heap=df$heap, heap_bin = df$heap_bin)
dat$temp_max <- lagard(df$temp_max)
dat$precip_daily_total <- lagard( df$precip_daily_total)


knots <- list(doy=c(0.5, 366.5)) # set knots for cyclic spline for doy

m1 <- gam(deaths~te(year, doy, bs = c("cr", "cc")) + heap +
                                te(temp_max, lag, k=c(10,4))+
                                te(precip_daily_total, lag, k=c(10,4)),
                               data = dat, family = nb, method =
'REML', select = TRUE, knots =knots)

# now increase k for (year, doy)
m2 <- gam(deaths~te(year, doy, bs = c("cr", "cc"), k=c(7, 20)) + heap +
                                te(temp_max, lag, k=c(10,4))+
                                te(precip_daily_total, lag, k=c(10,4)),
                                data = dat, family = nb, method =
'REML', select = TRUE, knots = knots)

gam.check(m1, rep=1000)
gam.check(m2, rep=1000)
concurvity(m1, full=FALSE)$worst
concurvity(m2, full=FALSE)$worst


From m@rek@h|@v@c @end|ng |rom gm@||@com  Wed Jun  1 13:33:25 2022
From: m@rek@h|@v@c @end|ng |rom gm@||@com (Marek Hlavac)
Date: Wed, 1 Jun 2022 13:33:25 +0200
Subject: [R] Calling stargazer() with do.call() in R 4.2.0
In-Reply-To: <CAMTWbJgxm7DmWr-agv5HLyg15wj_qu0055XQ0fLAe=q9hMzjTw@mail.gmail.com>
References: <CAMTWbJiVBXDA5ABx2zGOueDk3OOPLCvWcjqJhY21tCSm5YWHMw@mail.gmail.com>
 <af234546-216c-52d0-060e-d6dbc47dbc7c@statistik.tu-dortmund.de>
 <CAMTWbJjb+XDS2zL18vScsYJa8+B5SRQdiEaSGUVbatWFYoburQ@mail.gmail.com>
 <CAPcHnpSe8Q0C5cMPfbVtJe8rZeHtYTpG16otS4=zYqjtraVEmA@mail.gmail.com>
 <CAMTWbJgxm7DmWr-agv5HLyg15wj_qu0055XQ0fLAe=q9hMzjTw@mail.gmail.com>
Message-ID: <CAP7TaCovurYpGXQ0M2KdMhansUfLq_xFPrgh-5hh8S3E1ixbdA@mail.gmail.com>

Thanks, everyone, will work on it this weekend.

On Sun, May 29, 2022 at 6:52 AM Arne Henningsen
<arne.henningsen at gmail.com> wrote:
>
> Dear Andrew
>
> Thanks a lot for investigating the problem and for suggesting a
> solution and a workaround! I have asked the maintainer of the
> 'stargazer' package to fix the problem. Until then, we can use the
> workaround that you suggested.
>
> Best regards,
> Arne
>
> On Sat, 28 May 2022 at 23:13, Andrew Simmons <akwsimmo at gmail.com> wrote:
> >
> > Hello,
> >
> >
> > I don't have the slightest clue what stargazer is supposed to be
> > doing, but it seems as though it's trying to create names for the ...
> > list using:
> > object.names.string <- deparse(substitute(list(...)))
> >
> > It makes an error in assuming that the return value of deparse will be
> > a character string. For stargazer(res), object.names.string becomes:
> > "list(res)"
> >
> > while for do.call(stargazer, list(res)), object.names.string becomes:
> > [1] "list(structure(list(coefficients = c(`(Intercept)` =
> > 6.41594246095523, "
> > [2] "UrbanPop = 0.0209346588197249), residuals = c(Alabama =
> > 5.56984732750071, "
> > [3] "Alaska = 2.57919391569797, Arizona = 0.009284833466778, Arkansas
> > = 1.33732459805853, "
> > [4] "California = 0.679003586449805, Colorado = -0.148845848893771, "
> > [5] "Connecticut = -4.72791119007405, Delaware = -2.02323789597542, "
> > [6] "Florida = 7.30928483346678, Georgia = 9.72797800986128, Hawaii =
> > -2.8535191429924, "
> > [7] "Idaho = -4.94641403722037, Illinois = 2.2464808570076, Indiana =
> > -0.576695284237348, "
> > [8] "Iowa = -5.40921801367955, Kansas = -1.79762994305707, Kentucky =
> > 2.19545528041907, "
> > [9] "Louisiana = 7.60237005694293, Maine = -5.3836100607612, Maryland
> > = 3.4814353981232, "
> >  [ reached getOption("max.print") -- omitted 110 entries ]
> >
> > perhaps the package maintainer could change the line from:
> > object.names.string <- deparse(substitute(list(...)))
> > to:
> > object.names.string <- deparse1(substitute(list(...)), collapse = "")
> >
> > or you could change your code to:
> > do.call(stargazer, alist(res))
> >
> > please note that using alist instead of list is only a workaround, you
> > should still let the package maintainer know of this bug. If the
> > maintainer asks, this is what I used to get the strings above:
> > fun <- \(...) deparse(substitute(list(...)))
> > data("USArrests")
> > res <- lm( Murder ~ UrbanPop, data = USArrests)
> > fun(res)
> > print(do.call("fun", list(res)), max = 9)
> >
> > On Sat, May 28, 2022 at 4:41 PM Arne Henningsen
> > <arne.henningsen at gmail.com> wrote:
> > >
> > > On Sat, 28 May 2022 at 01:21, Uwe Ligges
> > > <ligges at statistik.tu-dortmund.de> wrote:
> > > > On 27.05.2022 17:29, Arne Henningsen wrote:
> > > >> Dear all  (cc Marek = maintainer of the stargazer package)
> > > >>
> > > >> We use do.call() to automatically create many LaTeX tables with
> > > >> stargazer but after upgrading to R 4.2.0, this no longer works. I
> > > >> illustrate this with a simple reproducible example:
> > > >>
> > > >> R> data("USArrests")
> > > >> R> res <- lm( Murder ~ UrbanPop, data = USArrests )
> > > >> R> library(stargazer)
> > > >> R> stargazer(res)  # works as expected
> > > >> R> do.call( stargazer, list(res) )
> > > >> Error in if (is.na(s)) { : the condition has length > 1
> > > >
> > > > Without looking at the code in detail: The line aboce suggests the code
> > > > needs an any():    if(any(is.na(x))) raher than if(is.na(x)).
> > >
> > > Yes, this is likely a problem in the 'stargazer' package.
> > >
> > > ... but why does the problem occur when using do.call( stargazer, )
> > > but the problem does *not* occur when using stargazer() directly?
> > >
> > > Best regards,
> > > Arne
> > >
> > > >> Any ideas what we can do so that the last command works with R 4.2.0?
> > > >>
> > > >> /Arne
> > >
> > > --
> > > Arne Henningsen
> > > http://www.arne-henningsen.name
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Arne Henningsen
> http://www.arne-henningsen.name


From jeroen @end|ng |rom berke|ey@edu  Wed Jun  1 14:05:51 2022
From: jeroen @end|ng |rom berke|ey@edu (Jeroen Ooms)
Date: Wed, 1 Jun 2022 14:05:51 +0200
Subject: [R] R_LIBS var needed to be set after upgrade to R 4.2.2
In-Reply-To: <CAC8=1er4BwEJ0zousw0rC-g75G8HviGxqZ+1jB0heD2379x-wA@mail.gmail.com>
References: <CAC8=1eppHfgnZkhE2tS_ko3Db0DNSMcs0fbYR++nYNsWqMLpvQ@mail.gmail.com>
 <25239.9444.688970.964168@stat.math.ethz.ch>
 <CAC8=1er4BwEJ0zousw0rC-g75G8HviGxqZ+1jB0heD2379x-wA@mail.gmail.com>
Message-ID: <CABFfbXu4eLha3rUaLFk=8N2c2xEO2B+abHSnZOePfCrCwEMuxg@mail.gmail.com>

On Wed, Jun 1, 2022 at 11:02 AM Ashim Kapoor <ashimkapoor at gmail.com> wrote:
>
> My confusion is : Earlier R --vanilla incantation was working fine,
> even without my intervening and
> adding to R_LIBS. That is why I was confused.
>
> My main query is : Is there anything special to R 4.2.0 which needs
> R_LIBS to be setup seperately?

I was hit by this problem as well a few weeks ago. You may get a more
detailed answer in r-sig-debian or from Dirk directly, but from what I
understood, this is now expected behavior: indeed if you start R
--vanilla then /usr/local/lib is no longer included in the library
path.

The reason is that R-core made a change in 4.2.0 to pre-set values for
R_LIBS_USER and R_LIBS_SITE in Renviron [1] which would take
precedence over the proper distro defaults (that include
/usr/local/lib) as configured by the r-base deb/rpm packages. To
mitigate the problem the the r-base deb package moved the appropriate
R_LIBS_USER and R_LIBS_SITE definitions into Renviron.site, which
takes precedence over Renviron. However a side effect of this solution
is that Renviron.site is ignored in --vanilla mode. At least this is
my best understanding of the problem.

[1] https://cran.r-project.org/doc/manuals/r-release/NEWS.html


From bh@@k@r@ko|k@t@ @end|ng |rom gm@||@com  Wed Jun  1 18:22:14 2022
From: bh@@k@r@ko|k@t@ @end|ng |rom gm@||@com (Bhaskar Mitra)
Date: Wed, 1 Jun 2022 09:22:14 -0700
Subject: [R] Request for some help about uncertainty analysis using
 bootstrap approach
In-Reply-To: <dae65208-0756-29b9-fbe0-e4c080c95a83@sapo.pt>
References: <CAEGXkYUo49WaArytfRffwxg87ZxNph1w-RNaZyi0Dgna=YKTbg@mail.gmail.com>
 <dae65208-0756-29b9-fbe0-e4c080c95a83@sapo.pt>
Message-ID: <CAEGXkYUnzyNMFuM527X9HGj2d0hJwb1F7M5TLj1oqNToz2NF2w@mail.gmail.com>

Thanks Rui. This is really helpful.

Regards,
Bhaskar

On Tue, May 31, 2022 at 4:03 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> You can use package boot to bootstrap the statistic for you.
> Write a function to compute the new column and assign the column means
> to the new variable Z or, like in the code below Z2 (so that you can
> compare to the Z column of simple averages).
>
> library(dplyr)
> library(boot)
>
> boot_uncert <- function(data, indices) {
>    data[indices, ] %>%
>      group_by(Group) %>%
>      mutate(Y = mean(X, na.rm = TRUE),
>             Z = coalesce(X, Y)) %>%
>      pull(Z)
> }
>
> Df1 <- Df1 %>%
>    group_by(Group) %>%
>    mutate(Y = mean(X, na.rm = TRUE),
>           Z = coalesce(X, Y)) %>%
>    ungroup()
>
> set.seed(2022)
> R <- 1e3
>
> Df1 %>%
>    mutate(Z2 = colMeans(boot(., boot_uncert, R = R)$t, na.rm = TRUE),
>           Z2 = coalesce(X, Z2))
>
> Hope this helps,
>
> Rui Barradas
>
>
> ?s 22:23 de 29/05/2022, Bhaskar Mitra escreveu:
> > Hello Everyone,
> >
> > I have a query about uncertainty analysis and would really appreciate
> some
> > help  in this regard.
> >
> > I intend to gapfill the NAs in the ?X? column of the dataframe (Df1). I
> > have grouped the data using the column ?Group? ,
> > determined the mean and generated the ?Z? column.
> >
> > While I am using the mean and standard error approach to generate the
> > uncertainty analysis, can we use the bootstrap approach to
> > generate the uncertainty for the ?Z? column? Any help in this regard will
> > be really appreciated.
> >
> > Regards,
> > Bhaskar
> > ---------------------------------------------------------------
> >
> > Df1 <-
> >
> > Group     X            Y    Z
> > 1           2              3     2
> > 1          NA            3     3
> > 1            3             3     3
> > 1           4              3    4
> > 2            2             2    1
> > 2          NA            2    3
> > 2           NA           2    3
> > 2            4             2    4
> > 3             2            2    2
> > 3         NA             2     2
> > 3              2           2     2
> >
> >
> -------------------------------------------------------------------------------
> > Codes:
> >
> > Df1 <- Df1 %>% group_by(Group) %>% summarise(Y= mean(X), na.rm=T)
> >
> > Df1  <- Df1%>% mutate(Z= coalesce(X,Y))
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @@h|mk@poor @end|ng |rom gm@||@com  Thu Jun  2 06:09:02 2022
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Thu, 2 Jun 2022 09:39:02 +0530
Subject: [R] R_LIBS var needed to be set after upgrade to R 4.2.2
In-Reply-To: <CABFfbXu4eLha3rUaLFk=8N2c2xEO2B+abHSnZOePfCrCwEMuxg@mail.gmail.com>
References: <CAC8=1eppHfgnZkhE2tS_ko3Db0DNSMcs0fbYR++nYNsWqMLpvQ@mail.gmail.com>
 <25239.9444.688970.964168@stat.math.ethz.ch>
 <CAC8=1er4BwEJ0zousw0rC-g75G8HviGxqZ+1jB0heD2379x-wA@mail.gmail.com>
 <CABFfbXu4eLha3rUaLFk=8N2c2xEO2B+abHSnZOePfCrCwEMuxg@mail.gmail.com>
Message-ID: <CAC8=1eoTy7abPxRWeLk4EbvBVb3_gX0QTjx=1j9iXjGdP2113Q@mail.gmail.com>

Dear Sir,

Many thanks.

Best Regards,
Ashim

On Wed, Jun 1, 2022 at 5:36 PM Jeroen Ooms <jeroen at berkeley.edu> wrote:
>
> On Wed, Jun 1, 2022 at 11:02 AM Ashim Kapoor <ashimkapoor at gmail.com> wrote:
> >
> > My confusion is : Earlier R --vanilla incantation was working fine,
> > even without my intervening and
> > adding to R_LIBS. That is why I was confused.
> >
> > My main query is : Is there anything special to R 4.2.0 which needs
> > R_LIBS to be setup seperately?
>
> I was hit by this problem as well a few weeks ago. You may get a more
> detailed answer in r-sig-debian or from Dirk directly, but from what I
> understood, this is now expected behavior: indeed if you start R
> --vanilla then /usr/local/lib is no longer included in the library
> path.
>
> The reason is that R-core made a change in 4.2.0 to pre-set values for
> R_LIBS_USER and R_LIBS_SITE in Renviron [1] which would take
> precedence over the proper distro defaults (that include
> /usr/local/lib) as configured by the r-base deb/rpm packages. To
> mitigate the problem the the r-base deb package moved the appropriate
> R_LIBS_USER and R_LIBS_SITE definitions into Renviron.site, which
> takes precedence over Renviron. However a side effect of this solution
> is that Renviron.site is ignored in --vanilla mode. At least this is
> my best understanding of the problem.
>
> [1] https://cran.r-project.org/doc/manuals/r-release/NEWS.html


From @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t  Thu Jun  2 08:12:44 2022
From: @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t (Stefano Sofia)
Date: Thu, 2 Jun 2022 06:12:44 +0000
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
Message-ID: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>

Dear R-list users,

for each winter season from 2000 to 2022 I have a data frame collecting for different weather stations snowpack height (Hs), snowfall in the last 24h (Hn) and a validation flag.

Suppose I have these three following data frames


df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 40, 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0, 0, 0, NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10, 0, 5), Station2_flag = c(0, 0, 0, 1, 0))


df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 60, 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))


df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station2_Hs = c(50, 60, 70, NA, NA), Station2_Hn = c(20, 20, 20, NA, NA), Station2_flag = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))


As you can see, each data frame can have different stations loaded.

I would need to call rbind matching data frames by column name (i.e. by station name), keeping in mind that the number of stations loaded in each data frame may differ. The result should be

data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs Station2_Hn Station2_flag Station3_Hs Station3_Hn Station3_flag
2000-12-01 30 10 0 20 0 0 NA NA NA
2000-12-02 40 20 0 20 0 0 NA NA NA
2000-12-03 50 10 0 30 10 0 NA NA NA
2000-12-04 NA NA NA 30 0 0 NA NA NA
2000-12-05 55 5 0 0 5 0 NA NA NA
2001-12-01 50 20 0 NA NA NA 20 0 0
2001-12-02 60 20 0 NA NA NA 20 0 0
2001-12-03 70 20 0 NA NA NA 30 10 0
2001-12-04 NA NA NA NA NA NA 30 0 1
2001-12-05 NA NA NA NA NA NA 0 5 0
2002-12-01 NA NA NA 50 20 0 20 0 0
2002-12-02 NA NA NA 60 20 0 20 0 0
2002-12-03 NA NA NA 70 20 0 30 10 0
2002-12-04 NA NA NA NA NA NA 30 0 1
2002-12-05 NA NA NA NA NA NA 0 5 0

I tried this code

df_list <- list(df1, df2, df3)
allNms <- unique(unlist(lapply(df_list, names)))
do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x, sapply(setdiff(allNms, names(x)), function(y) NA)))), make.row.names=FALSE))

but I get this error:
Error in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE,  :
  arguments imply differing number of rows

Could someone please help me?


Thank you for your attention

Stefano


         (oo)
--oOO--( )--OOo--------------------------------------
Stefano Sofia PhD
Civil Protection - Marche Region - Italy
Meteo Section
Snow Section
Via del Colle Ameno 5
60126 Torrette di Ancona, Ancona (AN)
Uff: +39 071 806 7743
E-mail: stefano.sofia at regione.marche.it
---Oo---------oO----------------------------------------

________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

--
Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non infetto.
This message was scanned by Libraesva ESG and is believed to be clean.


	[[alternative HTML version deleted]]


From bh@@k@r@ko|k@t@ @end|ng |rom gm@||@com  Thu Jun  2 01:17:18 2022
From: bh@@k@r@ko|k@t@ @end|ng |rom gm@||@com (Bhaskar Mitra)
Date: Wed, 1 Jun 2022 16:17:18 -0700
Subject: [R] Request for some help: Error when joining multiple csv files
 together
Message-ID: <CAEGXkYV-F6sRgYaq=57ic-iXF52r-4GyLWW4485f+Rzz3wq_KA@mail.gmail.com>

Hello Everyone,

I have a bunch of csv files, all with common headers (d1, d2, d3, d4).
When I am trying to join the csv files together, with the following code
(shown below),
I am getting a warning  when the files are joined. The values under columns
d3 and d4 are not joined properly.

The code and the error are given below. I would really appreciate some help
in this regard.

regards,
bhaskar


#---code--------------------------------

df <- list.files(pattern = "*.csv") %>%
  lapply(read_csv) %>%
  bind_rows

#---code--------------------------------

Header of each file:

d1  d2  d3  d4
3   4    NA   NA
4   5   NA   7
5   6   8   8
6   7   8   NA

#--------------------------------------------------------

#Warning when the codes run --------------------------------------

Column specification
?????????????????????????????????????????????????????????????????????????????????????????????????????????????????
cols(
  .default = col_double(),
  name = col_character(),
  d1 = col_datetime(format = ""),
  d2 = col_character(),
  d3 = col_logical(),
  d4 = col_logical()
)

Warning: 48766 parsing failures.
* row*    *col*          * expected  *                         *actual*
     *file*
3529   d3           1/0/T/F/TRUE/FALSE       100             'file1.csv'
3529   d4            1/0/T/F/TRUE/FALSE      100             'file1.csv'

.... ..... .................. ...... ................................

	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Thu Jun  2 09:26:02 2022
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Thu, 2 Jun 2022 09:26:02 +0200
Subject: [R] R_LIBS var needed to be set after upgrade to R 4.2.2
In-Reply-To: <CAC8=1er4BwEJ0zousw0rC-g75G8HviGxqZ+1jB0heD2379x-wA@mail.gmail.com>
References: <CAC8=1eppHfgnZkhE2tS_ko3Db0DNSMcs0fbYR++nYNsWqMLpvQ@mail.gmail.com>
 <25239.9444.688970.964168@stat.math.ethz.ch>
 <CAC8=1er4BwEJ0zousw0rC-g75G8HviGxqZ+1jB0heD2379x-wA@mail.gmail.com>
Message-ID: <25240.26122.534907.343899@stat.math.ethz.ch>

>>>>> Ashim Kapoor 
>>>>>     on Wed, 1 Jun 2022 14:30:58 +0530 writes:

    > Dear Sir,
    >> > I upgraded to R 4.2.2  on Debian 10 today.
    >> 
    >> Well, I assume you mean R 4.2.0 .. at least that one exists.

    > My bad, yes I made a typo. I did mean R 4.2.0.

    >> > The R shell incantation worked fine and all libraries would load but,
    >> > I needed to point the R_LIBS variable to
    >> > /usr/local/lib/R/site-library/ in order for the R --vanilla < myfile.R
    >> > incantation to find the libraries.
    >> 
    >> you mean other installed *packages*
    >> 
    >> > May I ask, why was this ? I never needed to do this on any previous
    >> > upgrade to R.
    >> 
    >> Well,  for me, the     R --vanilla   form also only sees the
    >> 29 (14 "base" + 15 "Recommended") packages that come with R.

    > Has this ALWAYS been the case for you ? Even with prior versions of R ?

Yes; I'm sorry this was unclear.

    >> Debian (& Ubuntu etc)  have used a similar setup where the
    >> default {R-level}  .libPaths() has contained three libraries,
    >> via
    >> 
    >> R_LIBS_SITE=${R_LIBS_SITE-'/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library'}
    >> 
    >> see
    >> 
    >> https://cloud.r-project.org/bin/linux/debian/#pathways-to-r-packages
    >> 
    >> also for much more.
    >> Note (also from the above CRAN page
    >> https://cloud.r-project.org/bin/linux/debian/
    >> [ Remember "menu"  "Binaries" -> "Linux" -> "Debian" ]
    >> 
    >> The good thing about the Debian (and derivatives) setup is that
    >> it also separates (as I do) the  "packages that come with R" in
    >> one library (= /usr/lib/R/library) from packages that are
    >> installed differently.

    > My confusion is : Earlier R --vanilla incantation was working fine,
    > even without my intervening and
    > adding to R_LIBS. That is why I was confused.

    > My main query is : Is there anything special to R 4.2.0 which needs
    > R_LIBS to be setup seperately?

to which Jeroen Ooms answered nicely -- thank you, Jeroen!

    > I was hit by this problem as well a few weeks ago. You may get a more
    > detailed answer in r-sig-debian or from Dirk directly, but from what I
    > understood, this is now expected behavior: indeed if you start R
    > --vanilla then /usr/local/lib is no longer included in the library path.

    > The reason is that R-core made a change in 4.2.0 to pre-set values for
    > R_LIBS_USER and R_LIBS_SITE in Renviron [1] which would take
    > precedence over the proper distro defaults (that include
    > /usr/local/lib) as configured by the r-base deb/rpm packages. To
    > mitigate the problem the r-base deb package moved the appropriate
    > R_LIBS_USER and R_LIBS_SITE definitions into Renviron.site, which
    > takes precedence over Renviron. However a side effect of this solution
    > is that Renviron.site is ignored in --vanilla mode. At least this is
    > my best understanding of the problem.

    > [1] https://cran.r-project.org/doc/manuals/r-release/NEWS.html

Yes, as

     R --help | grep -A1 -e --vanilla | head -2

has given
 
  --vanilla		Combine --no-save, --no-restore, --no-site-file,
			--no-init-file and --no-environ

(unchanged for many years).


Being a Linux user of more than 25 years, I've not been much of a
Debian-or-derivatives user in recent years (apart from setting up
and maintaining Ubuntu LTS on my wife's computer) mainly because
of lazyness as our IT staff helps me solve all problems with
Fedora quickly, including lowelevel device-related ones,
I think that Debian(+derivatives) has always been the exception
among the Linux distros and for all the others, '--vanilla'
really meant "vanilla" in a loose sense, i.e., "just base R".

I agree that I had wanted a "vanilla+strawberry" version myself, really.
which would be just not have the  --no-site-file  switch,
so we could call it
   --vanilla+site

{Others may hate  "feature creep" and yet more output from
'R --help'  ..}

Martin


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Jun  2 09:30:01 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 2 Jun 2022 08:30:01 +0100
Subject: [R] 
 Request for some help: Error when joining multiple csv files
 together
In-Reply-To: <CAEGXkYV-F6sRgYaq=57ic-iXF52r-4GyLWW4485f+Rzz3wq_KA@mail.gmail.com>
References: <CAEGXkYV-F6sRgYaq=57ic-iXF52r-4GyLWW4485f+Rzz3wq_KA@mail.gmail.com>
Message-ID: <f040297a-71b6-dadd-8e34-b1a523d105e9@sapo.pt>

Hello,

I'm seeing two obvious errors, those are not csv files and the columns 
spec is wrong, you have a spec of 6 columns but the posted data only has 
4 and of different classes.

If the data is in comma separated values (CSV) files the following 
worked without errors.


library(readr)

col_spec <- cols(
   d1 = col_double(),
   d2 = col_double(),
   d3 = col_double(),
   d4 = col_double()
)

list.files(pattern = "*\\.csv$") |>
   lapply(read_csv, col_types = col_spec) |>
   dplyr::bind_rows()


If the data is like in the question, try instead


list.files(pattern = "*\\.csv$") |>
   lapply(read_delim, delim = " ", col_types = col_spec) |>
   dplyr::bind_rows()


Hope this helps,

Rui Barradas

?s 00:17 de 02/06/2022, Bhaskar Mitra escreveu:
> Hello Everyone,
> 
> I have a bunch of csv files, all with common headers (d1, d2, d3, d4).
> When I am trying to join the csv files together, with the following code
> (shown below),
> I am getting a warning  when the files are joined. The values under columns
> d3 and d4 are not joined properly.
> 
> The code and the error are given below. I would really appreciate some help
> in this regard.
> 
> regards,
> bhaskar
> 
> 
> #---code--------------------------------
> 
> df <- list.files(pattern = "*.csv") %>%
>    lapply(read_csv) %>%
>    bind_rows
> 
> #---code--------------------------------
> 
> Header of each file:
> 
> d1  d2  d3  d4
> 3   4    NA   NA
> 4   5   NA   7
> 5   6   8   8
> 6   7   8   NA
> 
> #--------------------------------------------------------
> 
> #Warning when the codes run --------------------------------------
> 
> Column specification
> ?????????????????????????????????????????????????????????????????????????????????????????????????????????????????
> cols(
>    .default = col_double(),
>    name = col_character(),
>    d1 = col_datetime(format = ""),
>    d2 = col_character(),
>    d3 = col_logical(),
>    d4 = col_logical()
> )
> 
> Warning: 48766 parsing failures.
> * row*    *col*          * expected  *                         *actual*
>       *file*
> 3529   d3           1/0/T/F/TRUE/FALSE       100             'file1.csv'
> 3529   d4            1/0/T/F/TRUE/FALSE      100             'file1.csv'
> 
> .... ..... .................. ...... ................................
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Jun  2 09:37:50 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 2 Jun 2022 08:37:50 +0100
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
References: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
Message-ID: <ca06d276-9e3a-dd6b-da51-30b07b7e7423@sapo.pt>

Hello,

Here are two ways, both with dplyr::bind_rows.


res1 <- Reduce(dplyr::bind_rows, df_list)
res2 <- do.call(dplyr::bind_rows, df_list)
identical(res1, res2)
# [1] TRUE


Hope this helps,

Rui Barradas

?s 07:12 de 02/06/2022, Stefano Sofia escreveu:
> Dear R-list users,
> 
> for each winter season from 2000 to 2022 I have a data frame collecting for different weather stations snowpack height (Hs), snowfall in the last 24h (Hn) and a validation flag.
> 
> Suppose I have these three following data frames
> 
> 
> df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 40, 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0, 0, 0, NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10, 0, 5), Station2_flag = c(0, 0, 0, 1, 0))
> 
> 
> df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 60, 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> 
> 
> df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station2_Hs = c(50, 60, 70, NA, NA), Station2_Hn = c(20, 20, 20, NA, NA), Station2_flag = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> 
> 
> As you can see, each data frame can have different stations loaded.
> 
> I would need to call rbind matching data frames by column name (i.e. by station name), keeping in mind that the number of stations loaded in each data frame may differ. The result should be
> 
> data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs Station2_Hn Station2_flag Station3_Hs Station3_Hn Station3_flag
> 2000-12-01 30 10 0 20 0 0 NA NA NA
> 2000-12-02 40 20 0 20 0 0 NA NA NA
> 2000-12-03 50 10 0 30 10 0 NA NA NA
> 2000-12-04 NA NA NA 30 0 0 NA NA NA
> 2000-12-05 55 5 0 0 5 0 NA NA NA
> 2001-12-01 50 20 0 NA NA NA 20 0 0
> 2001-12-02 60 20 0 NA NA NA 20 0 0
> 2001-12-03 70 20 0 NA NA NA 30 10 0
> 2001-12-04 NA NA NA NA NA NA 30 0 1
> 2001-12-05 NA NA NA NA NA NA 0 5 0
> 2002-12-01 NA NA NA 50 20 0 20 0 0
> 2002-12-02 NA NA NA 60 20 0 20 0 0
> 2002-12-03 NA NA NA 70 20 0 30 10 0
> 2002-12-04 NA NA NA NA NA NA 30 0 1
> 2002-12-05 NA NA NA NA NA NA 0 5 0
> 
> I tried this code
> 
> df_list <- list(df1, df2, df3)
> allNms <- unique(unlist(lapply(df_list, names)))
> do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x, sapply(setdiff(allNms, names(x)), function(y) NA)))), make.row.names=FALSE))
> 
> but I get this error:
> Error in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE,  :
>    arguments imply differing number of rows
> 
> Could someone please help me?
> 
> 
> Thank you for your attention
> 
> Stefano
> 
> 
>           (oo)
> --oOO--( )--OOo--------------------------------------
> Stefano Sofia PhD
> Civil Protection - Marche Region - Italy
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona (AN)
> Uff: +39 071 806 7743
> E-mail: stefano.sofia at regione.marche.it
> ---Oo---------oO----------------------------------------
> 
> ________________________________
> 
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> 
> --
> 
> Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non infetto.
> 
> This message was scanned by Libraesva ESG and is believed to be clean.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Jun  2 09:46:02 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 2 Jun 2022 08:46:02 +0100
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <ca06d276-9e3a-dd6b-da51-30b07b7e7423@sapo.pt>
References: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
 <ca06d276-9e3a-dd6b-da51-30b07b7e7423@sapo.pt>
Message-ID: <bc824824-6873-b9ea-a007-6785ad2bde84@sapo.pt>

Hello,

And a base R only version.
Row binding code taken from StackOverflow [1].


rowbind <- function(x, y, all_cols = FALSE) {
   if(all_cols) {
     x[setdiff(names(y), names(x))] <- NA
     y[setdiff(names(x), names(y))] <- NA
   }
   rbind(x, y)
}

res3 <- Reduce(\(x, y) rowbind(x, y, all_cols = TRUE), df_list)
identical(res1, res3)
# [1] TRUE


[1] https://stackoverflow.com/a/46635610/8245406

Hope this helps,

Rui Barradas

?s 08:37 de 02/06/2022, Rui Barradas escreveu:
> Hello,
> 
> Here are two ways, both with dplyr::bind_rows.
> 
> 
> res1 <- Reduce(dplyr::bind_rows, df_list)
> res2 <- do.call(dplyr::bind_rows, df_list)
> identical(res1, res2)
> # [1] TRUE
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 07:12 de 02/06/2022, Stefano Sofia escreveu:
>> Dear R-list users,
>>
>> for each winter season from 2000 to 2022 I have a data frame 
>> collecting for different weather stations snowpack height (Hs), 
>> snowfall in the last 24h (Hn) and a validation flag.
>>
>> Suppose I have these three following data frames
>>
>>
>> df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01", 
>> format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05", 
>> format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 
>> 40, 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = 
>> c(0, 0, 0, NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = 
>> c(0, 0, 10, 0, 5), Station2_flag = c(0, 0, 0, 1, 0))
>>
>>
>> df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01", 
>> format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05", 
>> format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 
>> 60, 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = 
>> c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = 
>> c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
>>
>>
>> df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01", 
>> format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05", 
>> format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station2_Hs = c(50, 
>> 60, 70, NA, NA), Station2_Hn = c(20, 20, 20, NA, NA), Station2_flag = 
>> c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = 
>> c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
>>
>>
>> As you can see, each data frame can have different stations loaded.
>>
>> I would need to call rbind matching data frames by column name (i.e. 
>> by station name), keeping in mind that the number of stations loaded 
>> in each data frame may differ. The result should be
>>
>> data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs 
>> Station2_Hn Station2_flag Station3_Hs Station3_Hn Station3_flag
>> 2000-12-01 30 10 0 20 0 0 NA NA NA
>> 2000-12-02 40 20 0 20 0 0 NA NA NA
>> 2000-12-03 50 10 0 30 10 0 NA NA NA
>> 2000-12-04 NA NA NA 30 0 0 NA NA NA
>> 2000-12-05 55 5 0 0 5 0 NA NA NA
>> 2001-12-01 50 20 0 NA NA NA 20 0 0
>> 2001-12-02 60 20 0 NA NA NA 20 0 0
>> 2001-12-03 70 20 0 NA NA NA 30 10 0
>> 2001-12-04 NA NA NA NA NA NA 30 0 1
>> 2001-12-05 NA NA NA NA NA NA 0 5 0
>> 2002-12-01 NA NA NA 50 20 0 20 0 0
>> 2002-12-02 NA NA NA 60 20 0 20 0 0
>> 2002-12-03 NA NA NA 70 20 0 30 10 0
>> 2002-12-04 NA NA NA NA NA NA 30 0 1
>> 2002-12-05 NA NA NA NA NA NA 0 5 0
>>
>> I tried this code
>>
>> df_list <- list(df1, df2, df3)
>> allNms <- unique(unlist(lapply(df_list, names)))
>> do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x, 
>> sapply(setdiff(allNms, names(x)), function(y) NA)))), 
>> make.row.names=FALSE))
>>
>> but I get this error:
>> Error in (function (..., row.names = NULL, check.rows = FALSE, 
>> check.names = TRUE,? :
>> ?? arguments imply differing number of rows
>>
>> Could someone please help me?
>>
>>
>> Thank you for your attention
>>
>> Stefano
>>
>>
>> ????????? (oo)
>> --oOO--( )--OOo--------------------------------------
>> Stefano Sofia PhD
>> Civil Protection - Marche Region - Italy
>> Meteo Section
>> Snow Section
>> Via del Colle Ameno 5
>> 60126 Torrette di Ancona, Ancona (AN)
>> Uff: +39 071 806 7743
>> E-mail: stefano.sofia at regione.marche.it
>> ---Oo---------oO----------------------------------------
>>
>> ________________________________
>>
>> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere 
>> informazioni confidenziali, pertanto ? destinato solo a persone 
>> autorizzate alla ricezione. I messaggi di posta elettronica per i 
>> client di Regione Marche possono contenere informazioni confidenziali 
>> e con privilegi legali. Se non si ? il destinatario specificato, non 
>> leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? 
>> ricevuto questo messaggio per errore, inoltrarlo al mittente ed 
>> eliminarlo completamente dal sistema del proprio computer. Ai sensi 
>> dell'art. 6 della DGR n. 1394/2008 si segnala che, in caso di 
>> necessit? ed urgenza, la risposta al presente messaggio di posta 
>> elettronica pu? essere visionata da persone estranee al destinatario.
>> IMPORTANT NOTICE: This e-mail message is intended to be received only 
>> by persons entitled to receive the confidential information it may 
>> contain. E-mail messages to clients of Regione Marche may contain 
>> information that is confidential and legally privileged. Please do not 
>> read, copy, forward, or store this message unless you are an intended 
>> recipient of it. If you have received this message in error, please 
>> forward it to the sender and delete it completely from your computer 
>> system.
>>
>> -- 
>>
>> Questo messaggio? stato analizzato da Libraesva ESG ed? risultato non 
>> infetto.
>>
>> This message was scanned by Libraesva ESG and is believed to be clean.
>>
>>
>> ????[[alternative HTML version deleted]]
>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @@h|mk@poor @end|ng |rom gm@||@com  Thu Jun  2 10:00:27 2022
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Thu, 2 Jun 2022 13:30:27 +0530
Subject: [R] R_LIBS var needed to be set after upgrade to R 4.2.2
In-Reply-To: <25240.26122.534907.343899@stat.math.ethz.ch>
References: <CAC8=1eppHfgnZkhE2tS_ko3Db0DNSMcs0fbYR++nYNsWqMLpvQ@mail.gmail.com>
 <25239.9444.688970.964168@stat.math.ethz.ch>
 <CAC8=1er4BwEJ0zousw0rC-g75G8HviGxqZ+1jB0heD2379x-wA@mail.gmail.com>
 <25240.26122.534907.343899@stat.math.ethz.ch>
Message-ID: <CAC8=1epPYOT1j9arYvOVBOv=ehm78WJ2gc4AL296EXPqak-bqA@mail.gmail.com>

Dear Sir,

Many thanks for the reply.

Best,
Ashim

On Thu, Jun 2, 2022 at 12:56 PM Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>
> >>>>> Ashim Kapoor
> >>>>>     on Wed, 1 Jun 2022 14:30:58 +0530 writes:
>
>     > Dear Sir,
>     >> > I upgraded to R 4.2.2  on Debian 10 today.
>     >>
>     >> Well, I assume you mean R 4.2.0 .. at least that one exists.
>
>     > My bad, yes I made a typo. I did mean R 4.2.0.
>
>     >> > The R shell incantation worked fine and all libraries would load but,
>     >> > I needed to point the R_LIBS variable to
>     >> > /usr/local/lib/R/site-library/ in order for the R --vanilla < myfile.R
>     >> > incantation to find the libraries.
>     >>
>     >> you mean other installed *packages*
>     >>
>     >> > May I ask, why was this ? I never needed to do this on any previous
>     >> > upgrade to R.
>     >>
>     >> Well,  for me, the     R --vanilla   form also only sees the
>     >> 29 (14 "base" + 15 "Recommended") packages that come with R.
>
>     > Has this ALWAYS been the case for you ? Even with prior versions of R ?
>
> Yes; I'm sorry this was unclear.
>
>     >> Debian (& Ubuntu etc)  have used a similar setup where the
>     >> default {R-level}  .libPaths() has contained three libraries,
>     >> via
>     >>
>     >> R_LIBS_SITE=${R_LIBS_SITE-'/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library'}
>     >>
>     >> see
>     >>
>     >> https://cloud.r-project.org/bin/linux/debian/#pathways-to-r-packages
>     >>
>     >> also for much more.
>     >> Note (also from the above CRAN page
>     >> https://cloud.r-project.org/bin/linux/debian/
>     >> [ Remember "menu"  "Binaries" -> "Linux" -> "Debian" ]
>     >>
>     >> The good thing about the Debian (and derivatives) setup is that
>     >> it also separates (as I do) the  "packages that come with R" in
>     >> one library (= /usr/lib/R/library) from packages that are
>     >> installed differently.
>
>     > My confusion is : Earlier R --vanilla incantation was working fine,
>     > even without my intervening and
>     > adding to R_LIBS. That is why I was confused.
>
>     > My main query is : Is there anything special to R 4.2.0 which needs
>     > R_LIBS to be setup seperately?
>
> to which Jeroen Ooms answered nicely -- thank you, Jeroen!
>
>     > I was hit by this problem as well a few weeks ago. You may get a more
>     > detailed answer in r-sig-debian or from Dirk directly, but from what I
>     > understood, this is now expected behavior: indeed if you start R
>     > --vanilla then /usr/local/lib is no longer included in the library path.
>
>     > The reason is that R-core made a change in 4.2.0 to pre-set values for
>     > R_LIBS_USER and R_LIBS_SITE in Renviron [1] which would take
>     > precedence over the proper distro defaults (that include
>     > /usr/local/lib) as configured by the r-base deb/rpm packages. To
>     > mitigate the problem the r-base deb package moved the appropriate
>     > R_LIBS_USER and R_LIBS_SITE definitions into Renviron.site, which
>     > takes precedence over Renviron. However a side effect of this solution
>     > is that Renviron.site is ignored in --vanilla mode. At least this is
>     > my best understanding of the problem.
>
>     > [1] https://cran.r-project.org/doc/manuals/r-release/NEWS.html
>
> Yes, as
>
>      R --help | grep -A1 -e --vanilla | head -2
>
> has given
>
>   --vanilla             Combine --no-save, --no-restore, --no-site-file,
>                         --no-init-file and --no-environ
>
> (unchanged for many years).
>
>
> Being a Linux user of more than 25 years, I've not been much of a
> Debian-or-derivatives user in recent years (apart from setting up
> and maintaining Ubuntu LTS on my wife's computer) mainly because
> of lazyness as our IT staff helps me solve all problems with
> Fedora quickly, including lowelevel device-related ones,
> I think that Debian(+derivatives) has always been the exception
> among the Linux distros and for all the others, '--vanilla'
> really meant "vanilla" in a loose sense, i.e., "just base R".
>
> I agree that I had wanted a "vanilla+strawberry" version myself, really.
> which would be just not have the  --no-site-file  switch,
> so we could call it
>    --vanilla+site
>
> {Others may hate  "feature creep" and yet more output from
> 'R --help'  ..}
>
> Martin


From Qu|r|n_St|er @end|ng |rom gmx@de  Thu Jun  2 16:49:27 2022
From: Qu|r|n_St|er @end|ng |rom gmx@de (Quirin Stier)
Date: Thu, 2 Jun 2022 16:49:27 +0200
Subject: [R] Install OpenCL
Message-ID: <1e3ff528-1d77-6a66-3184-0f94ce826f62@gmx.de>

Hi everyone,

the installation of OpenCL on windows with CUDA 11.7 failed. I can set
up tensorflow in python and communicate with the GPU, so I assume CUDA
is set up correctly. OpenCL headers are also available through the CUDA
toolkit.

However, the package installation of OpenCL in R fails. There is not
much documentation available. Can anyone help with that please?

 > install.packages("OpenCL")
Installiere Paket nach ?C:/Users/quiri/AppData/Local/R/win-library/4.2?
(da ?lib? nicht spezifiziert)
Paket, das nur als Quelltext vorliegt und eventuell ?bersetzung von
C/C++/Fortran ben?tigt.: ?OpenCL?
installiere Quellpaket ?OpenCL?

trying URL 'https://cran.rstudio.com/src/contrib/OpenCL_0.2-2.tar.gz'
Content type 'application/x-gzip' length 20881 bytes (20 KB)
downloaded 20 KB

* installing *source* package 'OpenCL' ...
** Paket 'OpenCL' erfolgreich entpackt und MD5 Summen ?berpr?ft
** using staged installation

ERROR: OCL not set!

You will need a working OpenCL SDK with headers
and libraries for both i386 and x64

Set OCL to the root of the SDK

You can also set individial variables OCLINC,
OCL32LIB and OCL64LIB. If they are not set,
the default layout will be assumed.

ERROR: configuration failed for package 'OpenCL'
* removing 'C:/Users/quiri/AppData/Local/R/win-library/4.2/OpenCL'
Warning in install.packages :
 ? Installation des Pakets ?OpenCL? hatte Exit-Status ungleich 0

Die heruntergeladenen Quellpakete sind in
 ??C:\Users\quiri\AppData\Local\Temp\RtmpeepQng\downloaded_packages?


Best regards,

Quirin


From jr@| @end|ng |rom po@teo@no  Thu Jun  2 17:43:32 2022
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Thu,  2 Jun 2022 15:43:32 +0000
Subject: [R] Install OpenCL
In-Reply-To: <1e3ff528-1d77-6a66-3184-0f94ce826f62@gmx.de>
References: <1e3ff528-1d77-6a66-3184-0f94ce826f62@gmx.de>
Message-ID: <YpjapBDJz46YX7Fy@posteo.no>

Dear Quirin,

To be able to install OpenCL on 
ArchLinux, I needed to install 
opencl-headers first, because my system 
complained about CL/opencl.h ...  Maybe 
there is something like that on Windows 
... 

Best,
Rasmus

[1] https://archlinux.org/packages/extra/any/opencl-headers/


From kry|ov@r00t @end|ng |rom gm@||@com  Thu Jun  2 17:46:55 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Thu, 2 Jun 2022 18:46:55 +0300
Subject: [R] Install OpenCL
In-Reply-To: <1e3ff528-1d77-6a66-3184-0f94ce826f62@gmx.de>
References: <1e3ff528-1d77-6a66-3184-0f94ce826f62@gmx.de>
Message-ID: <20220602184655.1c840903@arachnoid>

? Thu, 2 Jun 2022 16:49:27 +0200
Quirin Stier <Quirin_Stier at gmx.de> ?????:

> ERROR: OCL not set!
> 
> You will need a working OpenCL SDK with headers
> and libraries for both i386 and x64
> 
> Set OCL to the root of the SDK

Admittedly, it's not mentioned in the package's INSTALL file, but as
the configure script says, you need to set the "OCL" environment
variable to the path where you have OpenCL SDK installed. Use
Sys.setenv() to set the variable before running install.packages().

Also, make sure that the OpenCL DLLs are available via the %PATH%
variable.

-- 
Best regards,
Ivan


From bgunter@4567 @end|ng |rom gm@||@com  Thu Jun  2 22:04:15 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 2 Jun 2022 13:04:15 -0700
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
References: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
Message-ID: <CAGxFJbS9+zZMRSaRU7KhBEJUWb8kiKjy51xe5TtjyDboKDafxQ@mail.gmail.com>

Well, it seems better to me to put all the data frames in long format and
then rbind them instead of the other way round, which results in the piles
of NA's you see. I note also, FWIW, that this accords with the so-called
"tidy" format that many advocate these days. You can always subset (rows)
and choose by station, date, etc. as needed, of course from the long format.

Because of the regularity of your data frames, it is easy to do this. Here
is a little base R function that "reforms" each data frame (I suspect Rui
may well provide a more elegant version, though):


reform<- function(dat){
   nm <- names(dat)
   stanums <- unique(gsub("[^[:digit:]]","", nm[-1])) ## station numbers
present
    z <- do.call(rbind, lapply(stanums,
      \(i)
      structure(dat[,grep(i, nm, fixed = TRUE)],
                names = c("Hs", "Hn", "flag"))))
   data.frame(POSIX =rep(dat[,1], length(stanums)),
              Station = rep(stanums, e = nrow(dat)),
              z)
}

e.g.
> reform(df2)
        POSIX Station Hs Hn flag
1  2001-12-01       1 50 20    0
2  2001-12-02       1 60 20    0
3  2001-12-03       1 70 20    0
4  2001-12-04       1 NA NA   NA
5  2001-12-05       1 NA NA   NA
6  2001-12-01       3 20  0    0
7  2001-12-02       3 20  0    0
8  2001-12-03       3 30 10    0
9  2001-12-04       3 30  0    1
10 2001-12-05       3  0  5    0

A call to rbind() of the following form then gives you all your data in
long form(you may wish to use some shortcuts to form the list of frames):

> do.call(rbind, lapply(list(df1, df2, df3), reform))
        POSIX Station Hs Hn flag
1  2000-12-01       1 30 10    0
2  2000-12-02       1 40 20    0
3  2000-12-03       1 50 10    0
4  2000-12-04       1 NA NA   NA
5  2000-12-05       1 55  5    0
6  2000-12-01       2 20  0    0
7  2000-12-02       2 20  0    0
8  2000-12-03       2 30 10    0
9  2000-12-04       2 30  0    1
10 2000-12-05       2  0  5    0
11 2001-12-01       1 50 20    0
12 2001-12-02       1 60 20    0
13 2001-12-03       1 70 20    0
14 2001-12-04       1 NA NA   NA
15 2001-12-05       1 NA NA   NA
16 2001-12-01       3 20  0    0
17 2001-12-02       3 20  0    0
18 2001-12-03       3 30 10    0
19 2001-12-04       3 30  0    1
20 2001-12-05       3  0  5    0
21 2002-12-01       2 50 20    0
22 2002-12-02       2 60 20    0
23 2002-12-03       2 70 20    0
24 2002-12-04       2 NA NA   NA
25 2002-12-05       2 NA NA   NA
26 2002-12-01       3 20  0    0
27 2002-12-02       3 20  0    0
28 2002-12-03       3 30 10    0
29 2002-12-04       3 30  0    1
30 2002-12-05       3  0  5    0


Cheers,
Bert Gunter




On Wed, Jun 1, 2022 at 11:13 PM Stefano Sofia <
stefano.sofia at regione.marche.it> wrote:

> Dear R-list users,
>
> for each winter season from 2000 to 2022 I have a data frame collecting
> for different weather stations snowpack height (Hs), snowfall in the last
> 24h (Hn) and a validation flag.
>
> Suppose I have these three following data frames
>
>
> df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01",
> format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05",
> format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 40,
> 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0, 0, 0,
> NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10, 0,
> 5), Station2_flag = c(0, 0, 0, 1, 0))
>
>
> df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01",
> format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05",
> format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 60,
> 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0, 0,
> 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10,
> 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
>
>
> df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01",
> format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05",
> format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station2_Hs = c(50, 60,
> 70, NA, NA), Station2_Hn = c(20, 20, 20, NA, NA), Station2_flag = c(0, 0,
> 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10,
> 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
>
>
> As you can see, each data frame can have different stations loaded.
>
> I would need to call rbind matching data frames by column name (i.e. by
> station name), keeping in mind that the number of stations loaded in each
> data frame may differ. The result should be
>
> data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs Station2_Hn
> Station2_flag Station3_Hs Station3_Hn Station3_flag
> 2000-12-01 30 10 0 20 0 0 NA NA NA
> 2000-12-02 40 20 0 20 0 0 NA NA NA
> 2000-12-03 50 10 0 30 10 0 NA NA NA
> 2000-12-04 NA NA NA 30 0 0 NA NA NA
> 2000-12-05 55 5 0 0 5 0 NA NA NA
> 2001-12-01 50 20 0 NA NA NA 20 0 0
> 2001-12-02 60 20 0 NA NA NA 20 0 0
> 2001-12-03 70 20 0 NA NA NA 30 10 0
> 2001-12-04 NA NA NA NA NA NA 30 0 1
> 2001-12-05 NA NA NA NA NA NA 0 5 0
> 2002-12-01 NA NA NA 50 20 0 20 0 0
> 2002-12-02 NA NA NA 60 20 0 20 0 0
> 2002-12-03 NA NA NA 70 20 0 30 10 0
> 2002-12-04 NA NA NA NA NA NA 30 0 1
> 2002-12-05 NA NA NA NA NA NA 0 5 0
>
> I tried this code
>
> df_list <- list(df1, df2, df3)
> allNms <- unique(unlist(lapply(df_list, names)))
> do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x,
> sapply(setdiff(allNms, names(x)), function(y) NA)))), make.row.names=FALSE))
>
> but I get this error:
> Error in (function (..., row.names = NULL, check.rows = FALSE, check.names
> = TRUE,  :
>   arguments imply differing number of rows
>
> Could someone please help me?
>
>
> Thank you for your attention
>
> Stefano
>
>
>          (oo)
> --oOO--( )--OOo--------------------------------------
> Stefano Sofia PhD
> Civil Protection - Marche Region - Italy
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona (AN)
> Uff: +39 071 806 7743
> E-mail: stefano.sofia at regione.marche.it
> ---Oo---------oO----------------------------------------
>
> ________________________________
>
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
> informazioni confidenziali, pertanto ? destinato solo a persone autorizzate
> alla ricezione. I messaggi di posta elettronica per i client di Regione
> Marche possono contenere informazioni confidenziali e con privilegi legali.
> Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o
> archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore,
> inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio
> computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che, in
> caso di necessit? ed urgenza, la risposta al presente messaggio di posta
> elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by
> persons entitled to receive the confidential information it may contain.
> E-mail messages to clients of Regione Marche may contain information that
> is confidential and legally privileged. Please do not read, copy, forward,
> or store this message unless you are an intended recipient of it. If you
> have received this message in error, please forward it to the sender and
> delete it completely from your computer system.
>
> --
> Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non
> infetto.
> This message was scanned by Libraesva ESG and is believed to be clean.
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Fri Jun  3 02:26:53 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Thu, 2 Jun 2022 20:26:53 -0400
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <18788_1654150401_2526DLjR006367_53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
References: <18788_1654150401_2526DLjR006367_53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
Message-ID: <f453cd34-b3bd-43f5-755b-bf01f7c78fe7@mcmaster.ca>

Dear Stefano,

I don't believe that your question has been answered.

You can use merge(), twice:

------- snip --------

 > merge(merge(df1, df2, all=TRUE), df3, all=TRUE)
    data_POSIX Station2_Hs Station2_Hn Station2_flag Station3_Hs Station3_Hn
1  2000-12-01          20           0             0          NA          NA
2  2000-12-02          20           0             0          NA          NA
3  2000-12-03          30          10             0          NA          NA
4  2000-12-04          30           0             1          NA          NA
5  2000-12-05           0           5             0          NA          NA
6  2001-12-01          NA          NA            NA          20           0
7  2001-12-02          NA          NA            NA          20           0
8  2001-12-03          NA          NA            NA          30          10
9  2001-12-04          NA          NA            NA          30           0
10 2001-12-05          NA          NA            NA           0           5
11 2002-12-01          50          20             0          20           0
12 2002-12-02          60          20             0          20           0
13 2002-12-03          70          20             0          30          10
14 2002-12-04          NA          NA            NA          30           0
15 2002-12-05          NA          NA            NA           0           5
    Station3_flag Station1_Hs Station1_Hn Station1_flag
1             NA          30          10             0
2             NA          40          20             0
3             NA          50          10             0
4             NA          NA          NA            NA
5             NA          55           5             0
6              0          50          20             0
7              0          60          20             0
8              0          70          20             0
9              1          NA          NA            NA
10             0          NA          NA            NA
11             0          NA          NA            NA
12             0          NA          NA            NA
13             0          NA          NA            NA
14             1          NA          NA            NA
15             0          NA          NA            NA

------- snip --------

The columns aren't in the order that you specified but, if that's 
important, you can simply reorder them.

I hope this helps,
  John

-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/

On 2022-06-02 2:12 a.m., Stefano Sofia wrote:
> Dear R-list users,
> 
> for each winter season from 2000 to 2022 I have a data frame collecting for different weather stations snowpack height (Hs), snowfall in the last 24h (Hn) and a validation flag.
> 
> Suppose I have these three following data frames
> 
> 
> df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 40, 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0, 0, 0, NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10, 0, 5), Station2_flag = c(0, 0, 0, 1, 0))
> 
> 
> df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 60, 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> 
> 
> df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station2_Hs = c(50, 60, 70, NA, NA), Station2_Hn = c(20, 20, 20, NA, NA), Station2_flag = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> 
> 
> As you can see, each data frame can have different stations loaded.
> 
> I would need to call rbind matching data frames by column name (i.e. by station name), keeping in mind that the number of stations loaded in each data frame may differ. The result should be
> 
> data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs Station2_Hn Station2_flag Station3_Hs Station3_Hn Station3_flag
> 2000-12-01 30 10 0 20 0 0 NA NA NA
> 2000-12-02 40 20 0 20 0 0 NA NA NA
> 2000-12-03 50 10 0 30 10 0 NA NA NA
> 2000-12-04 NA NA NA 30 0 0 NA NA NA
> 2000-12-05 55 5 0 0 5 0 NA NA NA
> 2001-12-01 50 20 0 NA NA NA 20 0 0
> 2001-12-02 60 20 0 NA NA NA 20 0 0
> 2001-12-03 70 20 0 NA NA NA 30 10 0
> 2001-12-04 NA NA NA NA NA NA 30 0 1
> 2001-12-05 NA NA NA NA NA NA 0 5 0
> 2002-12-01 NA NA NA 50 20 0 20 0 0
> 2002-12-02 NA NA NA 60 20 0 20 0 0
> 2002-12-03 NA NA NA 70 20 0 30 10 0
> 2002-12-04 NA NA NA NA NA NA 30 0 1
> 2002-12-05 NA NA NA NA NA NA 0 5 0
> 
> I tried this code
> 
> df_list <- list(df1, df2, df3)
> allNms <- unique(unlist(lapply(df_list, names)))
> do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x, sapply(setdiff(allNms, names(x)), function(y) NA)))), make.row.names=FALSE))
> 
> but I get this error:
> Error in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE,  :
>    arguments imply differing number of rows
> 
> Could someone please help me?
> 
> 
> Thank you for your attention
> 
> Stefano
> 
> 
>           (oo)
> --oOO--( )--OOo--------------------------------------
> Stefano Sofia PhD
> Civil Protection - Marche Region - Italy
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona (AN)
> Uff: +39 071 806 7743
> E-mail: stefano.sofia at regione.marche.it
> ---Oo---------oO----------------------------------------
> 
> ________________________________
> 
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> 
> --
> Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non infetto.
> This message was scanned by Libraesva ESG and is believed to be clean.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t  Fri Jun  3 14:43:56 2022
From: @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t (Stefano Sofia)
Date: Fri, 3 Jun 2022 12:43:56 +0000
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <CAPcHnpT_qj4UONt=kgOn9g+x4Od6AgabMR8+ypRNWe3Uy=mD1Q@mail.gmail.com>
References: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
 <CAPcHnpQARh_MdZhkCi5w_bkx8-2NrT3Vukr6tu6L_8R1MczBfQ@mail.gmail.com>
 <602a61e019da43a69d37c4d59f630f6b@regione.marche.it>,
 <CAPcHnpT_qj4UONt=kgOn9g+x4Od6AgabMR8+ypRNWe3Uy=mD1Q@mail.gmail.com>
Message-ID: <a06135d4a46244d283a6f5eb30d8aae1@regione.marche.it>

Thank you to all who provided useful hints, great as always.

In my opinion the solution given by Andrew is perfect, exactly what I wanted to do without changing the format of my data frames:


df_list <- list(df1, df2, df3)

allNms <- unique(unlist(lapply(df_list, names)))

do.call(rbind, c(lapply(df_list, function(x) data.frame(x, sapply(setdiff(allNms, names(x)), function(y) NA, simplify = FALSE))), make.row.names=FALSE))


Now I encountered this final problem: when I load my data frames in R through


df1 <- read.table(file="/mypath/df1.csv", header = TRUE, sep=" ", dec = ".", stringsAsFactors = TRUE)

df1$data_POSIX <- as.POSIXct(df1$data_POSIX, format = "%Y-%m-%d", tz="Etc/GMT-1")


I get the following error:


Error in data.frame(x, sapply(setdiff(allNms, names(x)), function(y) NA,  :
  arguments imply differing number of rows:5, 0


Why? df1 is a data frame correctly filled in.


Thank you again

Stefano


         (oo)
--oOO--( )--OOo--------------------------------------
Stefano Sofia PhD
Civil Protection - Marche Region - Italy
Meteo Section
Snow Section
Via del Colle Ameno 5
60126 Torrette di Ancona, Ancona (AN)
Uff: +39 071 806 7743
E-mail: stefano.sofia at regione.marche.it
---Oo---------oO----------------------------------------


________________________________
Da: Andrew Simmons <akwsimmo at gmail.com>
Inviato: venerd? 3 giugno 2022 11:06
A: Stefano Sofia
Oggetto: Re: [R] rbind of multiple data frames by column name, when each data frames can contain different columns

I think I see the problem. I forgot that lapply doesn't assign names where sapply does. I think you might want to use that instead, but also supply argument simplify as FALSE

sapply(setdiff(allNms, names(x)), function(y) NA, simplify = FALSE)

It should produce something more like

$Station2_Hs
[1] NA

$Station2_Hn
[1] NA

$Station2_flag
[1] NA

which should combine nicely with data.frame(x,)


On Fri, Jun 3, 2022, 03:25 Stefano Sofia <stefano.sofia at regione.marche.it<mailto:stefano.sofia at regione.marche.it>> wrote:

Good morning Andrew.

Thank you for your help.

Unfortunately your suggestion does not work, and I got different errors depending on the use of the three small examples or real data frames.

If I apply your code to the three small examples I gave you (df1, df2 and df3) I get:


Error in match.names(clabs, names(xi)) :
  names do not match previous names


I tried to understand the origin of the problem.


lapply(setdiff(allNms, names(x)), function(y) NA)

gives


[[1]]
[1] NA

[[2]]
[1] NA

[[3]]
[1] NA

[[4]]
[1] NA

[[5]]
[1] NA

[[6]]
[1] NA

[[7]]
[1] NA

[[8]]
[1] NA

[[9]]
[1] NA

[[10]]
[1] NA


and

data.frame(x, lapply(setdiff(allNms, names(x)), function(y) NA))

gives

   x NA. NA..1 NA..2 NA..3 NA..4 NA..5 NA..6 NA..7 NA..8 NA..9
1  1  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
2  1  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
3  1  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
4  2  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
5  2  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
6  3  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
7  4  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
8  4  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
9  4  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
10 5  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
11 6  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
12 6  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
13 6  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA


Why?

Unfortunately this code is too difficult for me.

Sorry for bothering you, thank you for what you have already done.

Stefano


         (oo)
--oOO--( )--OOo--------------------------------------
Stefano Sofia PhD
Civil Protection - Marche Region - Italy
Meteo Section
Snow Section
Via del Colle Ameno 5
60126 Torrette di Ancona, Ancona (AN)
Uff: +39 071 806 7743
E-mail: stefano.sofia at regione.marche.it<mailto:stefano.sofia at regione.marche.it>
---Oo---------oO----------------------------------------


________________________________
Da: Andrew Simmons <akwsimmo at gmail.com<mailto:akwsimmo at gmail.com>>
Inviato: gioved? 2 giugno 2022 08:21
A: Stefano Sofia
Oggetto: Re: [R] rbind of multiple data frames by column name, when each data frames can contain different columns

I would change this:
do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x, sapply(setdiff(allNms, names(x)), function(y) NA)))), make.row.names=FALSE))
to:
do.call(rbind, c(lapply(df_list, function(x) data.frame(x, lapply(setdiff(allNms, names(x)), function(y) NA))), make.row.names=FALSE))


On Thu, Jun 2, 2022, 02:13 Stefano Sofia <stefano.sofia at regione.marche.it<mailto:stefano.sofia at regione.marche.it>> wrote:
Dear R-list users,

for each winter season from 2000 to 2022 I have a data frame collecting for different weather stations snowpack height (Hs), snowfall in the last 24h (Hn) and a validation flag.

Suppose I have these three following data frames


df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 40, 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0, 0, 0, NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10, 0, 5), Station2_flag = c(0, 0, 0, 1, 0))


df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 60, 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))


df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"),   = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))


As you can see, each data frame can have different stations loaded.

I would need to call rbind matching data frames by column name (i.e. by station name), keeping in mind that the number of stations loaded in each data frame may differ. The result should be

data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs Station2_Hn Station2_flag Station3_Hs Station3_Hn Station3_flag
2000-12-01 30 10 0 20 0 0 NA NA NA
2000-12-02 40 20 0 20 0 0 NA NA NA
2000-12-03 50 10 0 30 10 0 NA NA NA
2000-12-04 NA NA NA 30 0 0 NA NA NA
2000-12-05 55 5 0 0 5 0 NA NA NA
2001-12-01 50 20 0 NA NA NA 20 0 0
2001-12-02 60 20 0 NA NA NA 20 0 0
2001-12-03 70 20 0 NA NA NA 30 10 0
2001-12-04 NA NA NA NA NA NA 30 0 1
2001-12-05 NA NA NA NA NA NA 0 5 0
2002-12-01 NA NA NA 50 20 0 20 0 0
2002-12-02 NA NA NA 60 20 0 20 0 0
2002-12-03 NA NA NA 70 20 0 30 10 0
2002-12-04 NA NA NA NA NA NA 30 0 1
2002-12-05 NA NA NA NA NA NA 0 5 0

I tried this code

df_list <- list(df1, df2, df3)
allNms <- unique(unlist(lapply(df_list, names)))
do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x, sapply(setdiff(allNms, names(x)), function(y) NA)))), make.row.names=FALSE))

but I get this error:
Error in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE,  :
  arguments imply differing number of rows

Could someone please help me?


Thank you for your attention

Stefano


         (oo)
--oOO--( )--OOo--------------------------------------
Stefano Sofia PhD
Civil Protection - Marche Region - Italy
Meteo Section
Snow Section
Via del Colle Ameno 5
60126 Torrette di Ancona, Ancona (AN)
Uff: +39 071 806 7743
E-mail: stefano.sofia at regione.marche.it<mailto:stefano.sofia at regione.marche.it>
---Oo---------oO----------------------------------------

________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

--
Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non infetto.
This message was scanned by Libraesva ESG and is believed to be clean.


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help<https://urlsand.esvalabs.com/?u=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&e=5a635173&h=06ff70f3&f=y&p=y>
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html<https://urlsand.esvalabs.com/?u=http%3A%2F%2Fwww.R-project.org%2Fposting-guide.html&e=5a635173&h=e12f63e8&f=y&p=y>
and provide commented, minimal, self-contained, reproducible code.

--
Questo messaggio ? stato analizzato con Libraesva ESG ed ? risultato non infetto

________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

--
Questo messaggio ? stato analizzato con Libraesva ESG ed ? risultato non infetto.
This message has been checked by Libraesva ESG and is believed to be clean.

--
Questo messaggio ? stato analizzato con Libraesva ESG ed ? risultato non infetto

________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

--
Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non infetto.
This message was scanned by Libraesva ESG and is believed to be clean.


	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Fri Jun  3 15:36:45 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Fri, 3 Jun 2022 09:36:45 -0400
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <25376_1654260288_253CimUt010543_a06135d4a46244d283a6f5eb30d8aae1@regione.marche.it>
References: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
 <CAPcHnpQARh_MdZhkCi5w_bkx8-2NrT3Vukr6tu6L_8R1MczBfQ@mail.gmail.com>
 <602a61e019da43a69d37c4d59f630f6b@regione.marche.it>
 <CAPcHnpT_qj4UONt=kgOn9g+x4Od6AgabMR8+ypRNWe3Uy=mD1Q@mail.gmail.com>
 <25376_1654260288_253CimUt010543_a06135d4a46244d283a6f5eb30d8aae1@regione.marche.it>
Message-ID: <1adea36c-72cc-fafd-e887-93c016283927@mcmaster.ca>

Dear Stefano,

On 2022-06-03 8:43 a.m., Stefano Sofia wrote:
> Thank you to all who provided useful hints, great as always.
> 
> In my opinion the solution given by Andrew is perfect, exactly what I wanted to do without changing the format of my data frames:
> 
> 
> df_list <- list(df1, df2, df3)
> 
> allNms <- unique(unlist(lapply(df_list, names)))
> 
> do.call(rbind, c(lapply(df_list, function(x) data.frame(x, sapply(setdiff(allNms, names(x)), function(y) NA, simplify = FALSE))), make.row.names=FALSE))

I'm moderately surprised that you find this simpler than, say,

	merge(merge(df1, df2, all=TRUE), df3, all=TRUE)[allNms]

> 
> 
> Now I encountered this final problem: when I load my data frames in R through
> 
> 
> df1 <- read.table(file="/mypath/df1.csv", header = TRUE, sep=" ", dec = ".", stringsAsFactors = TRUE)
> 
> df1$data_POSIX <- as.POSIXct(df1$data_POSIX, format = "%Y-%m-%d", tz="Etc/GMT-1")
> 
> 
> I get the following error:
> 
> 
> Error in data.frame(x, sapply(setdiff(allNms, names(x)), function(y) NA,  :
>    arguments imply differing number of rows:5, 0
> 
> 
> Why? df1 is a data frame correctly filled in.

I can't duplicate this error (and you don't provide the data files). A 
couple of thoughts:

(1) Why set stringsAsFactors = TRUE when you plan to convert the first 
column to dates (though that doesn't account for the error)?

(2) If df1.csv is really a csv file (and why use the extension .csv if 
it isn't?), why specify sep=" " (though if the data file is really a csv 
file, the resulting df1 would be corrupted, and you say it isn't).

Best,
John

> 
> 
> Thank you again
> 
> Stefano
> 
> 
>           (oo)
> --oOO--( )--OOo--------------------------------------
> Stefano Sofia PhD
> Civil Protection - Marche Region - Italy
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona (AN)
> Uff: +39 071 806 7743
> E-mail: stefano.sofia at regione.marche.it
> ---Oo---------oO----------------------------------------
> 
> 
> ________________________________
> Da: Andrew Simmons <akwsimmo at gmail.com>
> Inviato: venerd? 3 giugno 2022 11:06
> A: Stefano Sofia
> Oggetto: Re: [R] rbind of multiple data frames by column name, when each data frames can contain different columns
> 
> I think I see the problem. I forgot that lapply doesn't assign names where sapply does. I think you might want to use that instead, but also supply argument simplify as FALSE
> 
> sapply(setdiff(allNms, names(x)), function(y) NA, simplify = FALSE)
> 
> It should produce something more like
> 
> $Station2_Hs
> [1] NA
> 
> $Station2_Hn
> [1] NA
> 
> $Station2_flag
> [1] NA
> 
> which should combine nicely with data.frame(x,)
> 
> 
> On Fri, Jun 3, 2022, 03:25 Stefano Sofia <stefano.sofia at regione.marche.it<mailto:stefano.sofia at regione.marche.it>> wrote:
> 
> Good morning Andrew.
> 
> Thank you for your help.
> 
> Unfortunately your suggestion does not work, and I got different errors depending on the use of the three small examples or real data frames.
> 
> If I apply your code to the three small examples I gave you (df1, df2 and df3) I get:
> 
> 
> Error in match.names(clabs, names(xi)) :
>    names do not match previous names
> 
> 
> I tried to understand the origin of the problem.
> 
> 
> lapply(setdiff(allNms, names(x)), function(y) NA)
> 
> gives
> 
> 
> [[1]]
> [1] NA
> 
> [[2]]
> [1] NA
> 
> [[3]]
> [1] NA
> 
> [[4]]
> [1] NA
> 
> [[5]]
> [1] NA
> 
> [[6]]
> [1] NA
> 
> [[7]]
> [1] NA
> 
> [[8]]
> [1] NA
> 
> [[9]]
> [1] NA
> 
> [[10]]
> [1] NA
> 
> 
> and
> 
> data.frame(x, lapply(setdiff(allNms, names(x)), function(y) NA))
> 
> gives
> 
>     x NA. NA..1 NA..2 NA..3 NA..4 NA..5 NA..6 NA..7 NA..8 NA..9
> 1  1  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 2  1  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 3  1  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 4  2  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 5  2  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 6  3  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 7  4  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 8  4  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 9  4  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 10 5  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 11 6  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 12 6  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 13 6  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 
> 
> Why?
> 
> Unfortunately this code is too difficult for me.
> 
> Sorry for bothering you, thank you for what you have already done.
> 
> Stefano
> 
> 
>           (oo)
> --oOO--( )--OOo--------------------------------------
> Stefano Sofia PhD
> Civil Protection - Marche Region - Italy
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona (AN)
> Uff: +39 071 806 7743
> E-mail: stefano.sofia at regione.marche.it<mailto:stefano.sofia at regione.marche.it>
> ---Oo---------oO----------------------------------------
> 
> 
> ________________________________
> Da: Andrew Simmons <akwsimmo at gmail.com<mailto:akwsimmo at gmail.com>>
> Inviato: gioved? 2 giugno 2022 08:21
> A: Stefano Sofia
> Oggetto: Re: [R] rbind of multiple data frames by column name, when each data frames can contain different columns
> 
> I would change this:
> do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x, sapply(setdiff(allNms, names(x)), function(y) NA)))), make.row.names=FALSE))
> to:
> do.call(rbind, c(lapply(df_list, function(x) data.frame(x, lapply(setdiff(allNms, names(x)), function(y) NA))), make.row.names=FALSE))
> 
> 
> On Thu, Jun 2, 2022, 02:13 Stefano Sofia <stefano.sofia at regione.marche.it<mailto:stefano.sofia at regione.marche.it>> wrote:
> Dear R-list users,
> 
> for each winter season from 2000 to 2022 I have a data frame collecting for different weather stations snowpack height (Hs), snowfall in the last 24h (Hn) and a validation flag.
> 
> Suppose I have these three following data frames
> 
> 
> df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 40, 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0, 0, 0, NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10, 0, 5), Station2_flag = c(0, 0, 0, 1, 0))
> 
> 
> df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 60, 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> 
> 
> df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"),   = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> 
> 
> As you can see, each data frame can have different stations loaded.
> 
> I would need to call rbind matching data frames by column name (i.e. by station name), keeping in mind that the number of stations loaded in each data frame may differ. The result should be
> 
> data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs Station2_Hn Station2_flag Station3_Hs Station3_Hn Station3_flag
> 2000-12-01 30 10 0 20 0 0 NA NA NA
> 2000-12-02 40 20 0 20 0 0 NA NA NA
> 2000-12-03 50 10 0 30 10 0 NA NA NA
> 2000-12-04 NA NA NA 30 0 0 NA NA NA
> 2000-12-05 55 5 0 0 5 0 NA NA NA
> 2001-12-01 50 20 0 NA NA NA 20 0 0
> 2001-12-02 60 20 0 NA NA NA 20 0 0
> 2001-12-03 70 20 0 NA NA NA 30 10 0
> 2001-12-04 NA NA NA NA NA NA 30 0 1
> 2001-12-05 NA NA NA NA NA NA 0 5 0
> 2002-12-01 NA NA NA 50 20 0 20 0 0
> 2002-12-02 NA NA NA 60 20 0 20 0 0
> 2002-12-03 NA NA NA 70 20 0 30 10 0
> 2002-12-04 NA NA NA NA NA NA 30 0 1
> 2002-12-05 NA NA NA NA NA NA 0 5 0
> 
> I tried this code
> 
> df_list <- list(df1, df2, df3)
> allNms <- unique(unlist(lapply(df_list, names)))
> do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x, sapply(setdiff(allNms, names(x)), function(y) NA)))), make.row.names=FALSE))
> 
> but I get this error:
> Error in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE,  :
>    arguments imply differing number of rows
> 
> Could someone please help me?
> 
> 
> Thank you for your attention
> 
> Stefano
> 
> 
>           (oo)
> --oOO--( )--OOo--------------------------------------
> Stefano Sofia PhD
> Civil Protection - Marche Region - Italy
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona (AN)
> Uff: +39 071 806 7743
> E-mail: stefano.sofia at regione.marche.it<mailto:stefano.sofia at regione.marche.it>
> ---Oo---------oO----------------------------------------
> 
> ________________________________
> 
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> 
> --
> Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non infetto.
> This message was scanned by Libraesva ESG and is believed to be clean.
> 
> 
>          [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help<https://urlsand.esvalabs.com/?u=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&e=5a635173&h=06ff70f3&f=y&p=y>
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html<https://urlsand.esvalabs.com/?u=http%3A%2F%2Fwww.R-project.org%2Fposting-guide.html&e=5a635173&h=e12f63e8&f=y&p=y>
> and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Questo messaggio ? stato analizzato con Libraesva ESG ed ? risultato non infetto
> 
> ________________________________
> 
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> 
> --
> Questo messaggio ? stato analizzato con Libraesva ESG ed ? risultato non infetto.
> This message has been checked by Libraesva ESG and is believed to be clean.
> 
> --
> Questo messaggio ? stato analizzato con Libraesva ESG ed ? risultato non infetto
> 
> ________________________________
> 
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> 
> --
> Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non infetto.
> This message was scanned by Libraesva ESG and is believed to be clean.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


From po|c1410 @end|ng |rom gm@||@com  Fri Jun  3 09:48:23 2022
From: po|c1410 @end|ng |rom gm@||@com (CALUM POLWART)
Date: Fri, 3 Jun 2022 08:48:23 +0100
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <CAGxFJbS9+zZMRSaRU7KhBEJUWb8kiKjy51xe5TtjyDboKDafxQ@mail.gmail.com>
References: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
 <CAGxFJbS9+zZMRSaRU7KhBEJUWb8kiKjy51xe5TtjyDboKDafxQ@mail.gmail.com>
Message-ID: <CA+etgPmdMNTxovwcU+Drrhi-6-Wua_dVO+fiJht7yZNVEt4QeQ@mail.gmail.com>

Bert! It sounds like you are warming to the the tidyverse! ;-)

I completed agree with your analysis this data would be best served long as
you have shown.

If the OP was going to use tidy to manipulate it, they can do the same with
tidyr::pivot_ functions (pivot_longer and pivot_wider). The result will be
the same - but understanding how it happened may be easier!

On Thu, 2 Jun 2022, 21:04 Bert Gunter, <bgunter.4567 at gmail.com> wrote:

> Well, it seems better to me to put all the data frames in long format and
> then rbind them instead of the other way round, which results in the piles
> of NA's you see. I note also, FWIW, that this accords with the so-called
> "tidy" format that many advocate these days. You can always subset (rows)
> and choose by station, date, etc. as needed, of course from the long
> format.
>
> Because of the regularity of your data frames, it is easy to do this. Here
> is a little base R function that "reforms" each data frame (I suspect Rui
> may well provide a more elegant version, though):
>
>
> reform<- function(dat){
>    nm <- names(dat)
>    stanums <- unique(gsub("[^[:digit:]]","", nm[-1])) ## station numbers
> present
>     z <- do.call(rbind, lapply(stanums,
>       \(i)
>       structure(dat[,grep(i, nm, fixed = TRUE)],
>                 names = c("Hs", "Hn", "flag"))))
>    data.frame(POSIX =rep(dat[,1], length(stanums)),
>               Station = rep(stanums, e = nrow(dat)),
>               z)
> }
>
> e.g.
> > reform(df2)
>         POSIX Station Hs Hn flag
> 1  2001-12-01       1 50 20    0
> 2  2001-12-02       1 60 20    0
> 3  2001-12-03       1 70 20    0
> 4  2001-12-04       1 NA NA   NA
> 5  2001-12-05       1 NA NA   NA
> 6  2001-12-01       3 20  0    0
> 7  2001-12-02       3 20  0    0
> 8  2001-12-03       3 30 10    0
> 9  2001-12-04       3 30  0    1
> 10 2001-12-05       3  0  5    0
>
> A call to rbind() of the following form then gives you all your data in
> long form(you may wish to use some shortcuts to form the list of frames):
>
> > do.call(rbind, lapply(list(df1, df2, df3), reform))
>         POSIX Station Hs Hn flag
> 1  2000-12-01       1 30 10    0
> 2  2000-12-02       1 40 20    0
> 3  2000-12-03       1 50 10    0
> 4  2000-12-04       1 NA NA   NA
> 5  2000-12-05       1 55  5    0
> 6  2000-12-01       2 20  0    0
> 7  2000-12-02       2 20  0    0
> 8  2000-12-03       2 30 10    0
> 9  2000-12-04       2 30  0    1
> 10 2000-12-05       2  0  5    0
> 11 2001-12-01       1 50 20    0
> 12 2001-12-02       1 60 20    0
> 13 2001-12-03       1 70 20    0
> 14 2001-12-04       1 NA NA   NA
> 15 2001-12-05       1 NA NA   NA
> 16 2001-12-01       3 20  0    0
> 17 2001-12-02       3 20  0    0
> 18 2001-12-03       3 30 10    0
> 19 2001-12-04       3 30  0    1
> 20 2001-12-05       3  0  5    0
> 21 2002-12-01       2 50 20    0
> 22 2002-12-02       2 60 20    0
> 23 2002-12-03       2 70 20    0
> 24 2002-12-04       2 NA NA   NA
> 25 2002-12-05       2 NA NA   NA
> 26 2002-12-01       3 20  0    0
> 27 2002-12-02       3 20  0    0
> 28 2002-12-03       3 30 10    0
> 29 2002-12-04       3 30  0    1
> 30 2002-12-05       3  0  5    0
>
>
> Cheers,
> Bert Gunter
>
>
>
>
> On Wed, Jun 1, 2022 at 11:13 PM Stefano Sofia <
> stefano.sofia at regione.marche.it> wrote:
>
> > Dear R-list users,
> >
> > for each winter season from 2000 to 2022 I have a data frame collecting
> > for different weather stations snowpack height (Hs), snowfall in the last
> > 24h (Hn) and a validation flag.
> >
> > Suppose I have these three following data frames
> >
> >
> > df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 40,
> > 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0, 0,
> 0,
> > NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10, 0,
> > 5), Station2_flag = c(0, 0, 0, 1, 0))
> >
> >
> > df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 60,
> > 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0, 0,
> > 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10,
> > 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> >
> >
> > df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station2_Hs = c(50, 60,
> > 70, NA, NA), Station2_Hn = c(20, 20, 20, NA, NA), Station2_flag = c(0, 0,
> > 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10,
> > 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> >
> >
> > As you can see, each data frame can have different stations loaded.
> >
> > I would need to call rbind matching data frames by column name (i.e. by
> > station name), keeping in mind that the number of stations loaded in each
> > data frame may differ. The result should be
> >
> > data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs Station2_Hn
> > Station2_flag Station3_Hs Station3_Hn Station3_flag
> > 2000-12-01 30 10 0 20 0 0 NA NA NA
> > 2000-12-02 40 20 0 20 0 0 NA NA NA
> > 2000-12-03 50 10 0 30 10 0 NA NA NA
> > 2000-12-04 NA NA NA 30 0 0 NA NA NA
> > 2000-12-05 55 5 0 0 5 0 NA NA NA
> > 2001-12-01 50 20 0 NA NA NA 20 0 0
> > 2001-12-02 60 20 0 NA NA NA 20 0 0
> > 2001-12-03 70 20 0 NA NA NA 30 10 0
> > 2001-12-04 NA NA NA NA NA NA 30 0 1
> > 2001-12-05 NA NA NA NA NA NA 0 5 0
> > 2002-12-01 NA NA NA 50 20 0 20 0 0
> > 2002-12-02 NA NA NA 60 20 0 20 0 0
> > 2002-12-03 NA NA NA 70 20 0 30 10 0
> > 2002-12-04 NA NA NA NA NA NA 30 0 1
> > 2002-12-05 NA NA NA NA NA NA 0 5 0
> >
> > I tried this code
> >
> > df_list <- list(df1, df2, df3)
> > allNms <- unique(unlist(lapply(df_list, names)))
> > do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x,
> > sapply(setdiff(allNms, names(x)), function(y) NA)))),
> make.row.names=FALSE))
> >
> > but I get this error:
> > Error in (function (..., row.names = NULL, check.rows = FALSE,
> check.names
> > = TRUE,  :
> >   arguments imply differing number of rows
> >
> > Could someone please help me?
> >
> >
> > Thank you for your attention
> >
> > Stefano
> >
> >
> >          (oo)
> > --oOO--( )--OOo--------------------------------------
> > Stefano Sofia PhD
> > Civil Protection - Marche Region - Italy
> > Meteo Section
> > Snow Section
> > Via del Colle Ameno 5
> > 60126 Torrette di Ancona, Ancona (AN)
> > Uff: +39 071 806 7743
> > E-mail: stefano.sofia at regione.marche.it
> > ---Oo---------oO----------------------------------------
> >
> > ________________________________
> >
> > AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
> > informazioni confidenziali, pertanto ? destinato solo a persone
> autorizzate
> > alla ricezione. I messaggi di posta elettronica per i client di Regione
> > Marche possono contenere informazioni confidenziali e con privilegi
> legali.
> > Se non si ? il destinatario specificato, non leggere, copiare, inoltrare
> o
> > archiviare questo messaggio. Se si ? ricevuto questo messaggio per
> errore,
> > inoltrarlo al mittente ed eliminarlo completamente dal sistema del
> proprio
> > computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che, in
> > caso di necessit? ed urgenza, la risposta al presente messaggio di posta
> > elettronica pu? essere visionata da persone estranee al destinatario.
> > IMPORTANT NOTICE: This e-mail message is intended to be received only by
> > persons entitled to receive the confidential information it may contain.
> > E-mail messages to clients of Regione Marche may contain information that
> > is confidential and legally privileged. Please do not read, copy,
> forward,
> > or store this message unless you are an intended recipient of it. If you
> > have received this message in error, please forward it to the sender and
> > delete it completely from your computer system.
> >
> > --
> > Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non
> > infetto.
> > This message was scanned by Libraesva ESG and is believed to be clean.
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From d@vidwk@tz m@iii@g oii gm@ii@com  Fri Jun  3 20:12:31 2022
From: d@vidwk@tz m@iii@g oii gm@ii@com (d@vidwk@tz m@iii@g oii gm@ii@com)
Date: Fri, 3 Jun 2022 11:12:31 -0700
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <CA+etgPmdMNTxovwcU+Drrhi-6-Wua_dVO+fiJht7yZNVEt4QeQ@mail.gmail.com>
References: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
 <CAGxFJbS9+zZMRSaRU7KhBEJUWb8kiKjy51xe5TtjyDboKDafxQ@mail.gmail.com>
 <CA+etgPmdMNTxovwcU+Drrhi-6-Wua_dVO+fiJht7yZNVEt4QeQ@mail.gmail.com>
Message-ID: <CAL+5cujqHe15QWA+fqw+K1AYvSCe90FRTMUYqp_83G=PMzdccw@mail.gmail.com>

library(data.table)
?rbindlist

On Fri, Jun 3, 2022 at 11:06 AM CALUM POLWART <polc1410 at gmail.com> wrote:

> Bert! It sounds like you are warming to the the tidyverse! ;-)
>
> I completed agree with your analysis this data would be best served long as
> you have shown.
>
> If the OP was going to use tidy to manipulate it, they can do the same with
> tidyr::pivot_ functions (pivot_longer and pivot_wider). The result will be
> the same - but understanding how it happened may be easier!
>
> On Thu, 2 Jun 2022, 21:04 Bert Gunter, <bgunter.4567 at gmail.com> wrote:
>
> > Well, it seems better to me to put all the data frames in long format and
> > then rbind them instead of the other way round, which results in the
> piles
> > of NA's you see. I note also, FWIW, that this accords with the so-called
> > "tidy" format that many advocate these days. You can always subset (rows)
> > and choose by station, date, etc. as needed, of course from the long
> > format.
> >
> > Because of the regularity of your data frames, it is easy to do this.
> Here
> > is a little base R function that "reforms" each data frame (I suspect Rui
> > may well provide a more elegant version, though):
> >
> >
> > reform<- function(dat){
> >    nm <- names(dat)
> >    stanums <- unique(gsub("[^[:digit:]]","", nm[-1])) ## station numbers
> > present
> >     z <- do.call(rbind, lapply(stanums,
> >       \(i)
> >       structure(dat[,grep(i, nm, fixed = TRUE)],
> >                 names = c("Hs", "Hn", "flag"))))
> >    data.frame(POSIX =rep(dat[,1], length(stanums)),
> >               Station = rep(stanums, e = nrow(dat)),
> >               z)
> > }
> >
> > e.g.
> > > reform(df2)
> >         POSIX Station Hs Hn flag
> > 1  2001-12-01       1 50 20    0
> > 2  2001-12-02       1 60 20    0
> > 3  2001-12-03       1 70 20    0
> > 4  2001-12-04       1 NA NA   NA
> > 5  2001-12-05       1 NA NA   NA
> > 6  2001-12-01       3 20  0    0
> > 7  2001-12-02       3 20  0    0
> > 8  2001-12-03       3 30 10    0
> > 9  2001-12-04       3 30  0    1
> > 10 2001-12-05       3  0  5    0
> >
> > A call to rbind() of the following form then gives you all your data in
> > long form(you may wish to use some shortcuts to form the list of frames):
> >
> > > do.call(rbind, lapply(list(df1, df2, df3), reform))
> >         POSIX Station Hs Hn flag
> > 1  2000-12-01       1 30 10    0
> > 2  2000-12-02       1 40 20    0
> > 3  2000-12-03       1 50 10    0
> > 4  2000-12-04       1 NA NA   NA
> > 5  2000-12-05       1 55  5    0
> > 6  2000-12-01       2 20  0    0
> > 7  2000-12-02       2 20  0    0
> > 8  2000-12-03       2 30 10    0
> > 9  2000-12-04       2 30  0    1
> > 10 2000-12-05       2  0  5    0
> > 11 2001-12-01       1 50 20    0
> > 12 2001-12-02       1 60 20    0
> > 13 2001-12-03       1 70 20    0
> > 14 2001-12-04       1 NA NA   NA
> > 15 2001-12-05       1 NA NA   NA
> > 16 2001-12-01       3 20  0    0
> > 17 2001-12-02       3 20  0    0
> > 18 2001-12-03       3 30 10    0
> > 19 2001-12-04       3 30  0    1
> > 20 2001-12-05       3  0  5    0
> > 21 2002-12-01       2 50 20    0
> > 22 2002-12-02       2 60 20    0
> > 23 2002-12-03       2 70 20    0
> > 24 2002-12-04       2 NA NA   NA
> > 25 2002-12-05       2 NA NA   NA
> > 26 2002-12-01       3 20  0    0
> > 27 2002-12-02       3 20  0    0
> > 28 2002-12-03       3 30 10    0
> > 29 2002-12-04       3 30  0    1
> > 30 2002-12-05       3  0  5    0
> >
> >
> > Cheers,
> > Bert Gunter
> >
> >
> >
> >
> > On Wed, Jun 1, 2022 at 11:13 PM Stefano Sofia <
> > stefano.sofia at regione.marche.it> wrote:
> >
> > > Dear R-list users,
> > >
> > > for each winter season from 2000 to 2022 I have a data frame collecting
> > > for different weather stations snowpack height (Hs), snowfall in the
> last
> > > 24h (Hn) and a validation flag.
> > >
> > > Suppose I have these three following data frames
> > >
> > >
> > > df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01",
> > > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05",
> > > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30,
> 40,
> > > 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0,
> 0,
> > 0,
> > > NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10,
> 0,
> > > 5), Station2_flag = c(0, 0, 0, 1, 0))
> > >
> > >
> > > df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01",
> > > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05",
> > > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50,
> 60,
> > > 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0,
> 0,
> > > 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0,
> 10,
> > > 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> > >
> > >
> > > df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01",
> > > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05",
> > > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station2_Hs = c(50,
> 60,
> > > 70, NA, NA), Station2_Hn = c(20, 20, 20, NA, NA), Station2_flag = c(0,
> 0,
> > > 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0,
> 10,
> > > 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> > >
> > >
> > > As you can see, each data frame can have different stations loaded.
> > >
> > > I would need to call rbind matching data frames by column name (i.e. by
> > > station name), keeping in mind that the number of stations loaded in
> each
> > > data frame may differ. The result should be
> > >
> > > data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs
> Station2_Hn
> > > Station2_flag Station3_Hs Station3_Hn Station3_flag
> > > 2000-12-01 30 10 0 20 0 0 NA NA NA
> > > 2000-12-02 40 20 0 20 0 0 NA NA NA
> > > 2000-12-03 50 10 0 30 10 0 NA NA NA
> > > 2000-12-04 NA NA NA 30 0 0 NA NA NA
> > > 2000-12-05 55 5 0 0 5 0 NA NA NA
> > > 2001-12-01 50 20 0 NA NA NA 20 0 0
> > > 2001-12-02 60 20 0 NA NA NA 20 0 0
> > > 2001-12-03 70 20 0 NA NA NA 30 10 0
> > > 2001-12-04 NA NA NA NA NA NA 30 0 1
> > > 2001-12-05 NA NA NA NA NA NA 0 5 0
> > > 2002-12-01 NA NA NA 50 20 0 20 0 0
> > > 2002-12-02 NA NA NA 60 20 0 20 0 0
> > > 2002-12-03 NA NA NA 70 20 0 30 10 0
> > > 2002-12-04 NA NA NA NA NA NA 30 0 1
> > > 2002-12-05 NA NA NA NA NA NA 0 5 0
> > >
> > > I tried this code
> > >
> > > df_list <- list(df1, df2, df3)
> > > allNms <- unique(unlist(lapply(df_list, names)))
> > > do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x,
> > > sapply(setdiff(allNms, names(x)), function(y) NA)))),
> > make.row.names=FALSE))
> > >
> > > but I get this error:
> > > Error in (function (..., row.names = NULL, check.rows = FALSE,
> > check.names
> > > = TRUE,  :
> > >   arguments imply differing number of rows
> > >
> > > Could someone please help me?
> > >
> > >
> > > Thank you for your attention
> > >
> > > Stefano
> > >
> > >
> > >          (oo)
> > > --oOO--( )--OOo--------------------------------------
> > > Stefano Sofia PhD
> > > Civil Protection - Marche Region - Italy
> > > Meteo Section
> > > Snow Section
> > > Via del Colle Ameno 5
> > > 60126 Torrette di Ancona, Ancona (AN)
> > > Uff: +39 071 806 7743
> > > E-mail: stefano.sofia at regione.marche.it
> > > ---Oo---------oO----------------------------------------
> > >
> > > ________________________________
> > >
> > > AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
> > > informazioni confidenziali, pertanto ? destinato solo a persone
> > autorizzate
> > > alla ricezione. I messaggi di posta elettronica per i client di Regione
> > > Marche possono contenere informazioni confidenziali e con privilegi
> > legali.
> > > Se non si ? il destinatario specificato, non leggere, copiare,
> inoltrare
> > o
> > > archiviare questo messaggio. Se si ? ricevuto questo messaggio per
> > errore,
> > > inoltrarlo al mittente ed eliminarlo completamente dal sistema del
> > proprio
> > > computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che,
> in
> > > caso di necessit? ed urgenza, la risposta al presente messaggio di
> posta
> > > elettronica pu? essere visionata da persone estranee al destinatario.
> > > IMPORTANT NOTICE: This e-mail message is intended to be received only
> by
> > > persons entitled to receive the confidential information it may
> contain.
> > > E-mail messages to clients of Regione Marche may contain information
> that
> > > is confidential and legally privileged. Please do not read, copy,
> > forward,
> > > or store this message unless you are an intended recipient of it. If
> you
> > > have received this message in error, please forward it to the sender
> and
> > > delete it completely from your computer system.
> > >
> > > --
> > > Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non
> > > infetto.
> > > This message was scanned by Libraesva ESG and is believed to be clean.
> > >
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @v|gro@@ @end|ng |rom ver|zon@net  Fri Jun  3 20:51:15 2022
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Fri, 3 Jun 2022 18:51:15 +0000 (UTC)
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <CA+etgPmdMNTxovwcU+Drrhi-6-Wua_dVO+fiJht7yZNVEt4QeQ@mail.gmail.com>
References: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
 <CAGxFJbS9+zZMRSaRU7KhBEJUWb8kiKjy51xe5TtjyDboKDafxQ@mail.gmail.com>
 <CA+etgPmdMNTxovwcU+Drrhi-6-Wua_dVO+fiJht7yZNVEt4QeQ@mail.gmail.com>
Message-ID: <1385052253.3823207.1654282275962@mail.yahoo.com>

Complexity has all kinds of costs and tradeoffs. In one sense?you are doing a conceptual merge between related but not?identical database tables albeit from a set of .CSV files.

You do have some??conceptually direct ways using base R or?packages like dplyr that??possibly do it in a more complex way?outside your view. Hypothetically, you could even work?sideways and open a connection to a database and store the?contents of each .CSV files into a relational table and then?issue some SQL commands asking to do the merge and read them?back but that would be weird and overkill as R has similar abilities.

But underneath the covers, as shown by the longer code, the R?functionality has to determine what field/column names are common?between pairs of data.frames and fill any missing ones with NA and?so on, perhaps faster than the code being shown and perhaps not.?Much depends on the code used and how general it is as it is?often weighted down by lots of additional functionality you are?not using, and whether it has big chunks being used that were?rewritten in C or C++ or other libraries from FORTRAN ...

But unless the amount of data gets huge and it takes too much?time or memory, I would opt for the conceptually simpler method of?using merge() or the full_join() in dplyr, or as pointed out,?other techniques by changing the formats. I can think of other ways?such as getting the combined column names from all the files,?then adding missing ones to each data.frame full of NA as needed, then?reordering the columns so they all are identical, and finally vertically?combining all the resulting rows. Again note the internal methods used?by any merge method (including in SQL) may simply automate tasks.

The more complex methods shown strike me as a tad painful to read, let?alone to modify when your needs change. Just because some people can?program all kinds of things in low-level languages like assembler, that does?not justify spending months on what can be done in days or even minutes?using pre-built and pre-tested methods layered one upon another.

If there is already an acceptable function that already accepts a list of such?overlapping data.frames, then other than HW problems, that might be even better.?It looks fairly easy to construct such a function based on the discussion so I wonder?if some package already has one.


-----Original Message-----
From: CALUM POLWART <polc1410 at gmail.com>
To: Bert Gunter <bgunter.4567 at gmail.com>
Cc: r-help at R-project.org <r-help at r-project.org>; Stefano Sofia <stefano.sofia at regione.marche.it>
Sent: Fri, Jun 3, 2022 3:48 am
Subject: Re: [R] rbind of multiple data frames by column name, when each data frames can contain different columns

Bert! It sounds like you are warming to the the tidyverse! ;-)

I completed agree with your analysis this data would be best served long as
you have shown.

If the OP was going to use tidy to manipulate it, they can do the same with
tidyr::pivot_ functions (pivot_longer and pivot_wider). The result will be
the same - but understanding how it happened may be easier!

On Thu, 2 Jun 2022, 21:04 Bert Gunter, <bgunter.4567 at gmail.com> wrote:

> Well, it seems better to me to put all the data frames in long format and
> then rbind them instead of the other way round, which results in the piles
> of NA's you see. I note also, FWIW, that this accords with the so-called
> "tidy" format that many advocate these days. You can always subset (rows)
> and choose by station, date, etc. as needed, of course from the long
> format.
>
> Because of the regularity of your data frames, it is easy to do this. Here
> is a little base R function that "reforms" each data frame (I suspect Rui
> may well provide a more elegant version, though):
>
>
> reform<- function(dat){
>? ? nm <- names(dat)
>? ? stanums <- unique(gsub("[^[:digit:]]","", nm[-1])) ## station numbers
> present
>? ? z <- do.call(rbind, lapply(stanums,
>? ? ? \(i)
>? ? ? structure(dat[,grep(i, nm, fixed = TRUE)],
>? ? ? ? ? ? ? ? names = c("Hs", "Hn", "flag"))))
>? ? data.frame(POSIX =rep(dat[,1], length(stanums)),
>? ? ? ? ? ? ? Station = rep(stanums, e = nrow(dat)),
>? ? ? ? ? ? ? z)
> }
>
> e.g.
> > reform(df2)
>? ? ? ? POSIX Station Hs Hn flag
> 1? 2001-12-01? ? ? 1 50 20? ? 0
> 2? 2001-12-02? ? ? 1 60 20? ? 0
> 3? 2001-12-03? ? ? 1 70 20? ? 0
> 4? 2001-12-04? ? ? 1 NA NA? NA
> 5? 2001-12-05? ? ? 1 NA NA? NA
> 6? 2001-12-01? ? ? 3 20? 0? ? 0
> 7? 2001-12-02? ? ? 3 20? 0? ? 0
> 8? 2001-12-03? ? ? 3 30 10? ? 0
> 9? 2001-12-04? ? ? 3 30? 0? ? 1
> 10 2001-12-05? ? ? 3? 0? 5? ? 0
>
> A call to rbind() of the following form then gives you all your data in
> long form(you may wish to use some shortcuts to form the list of frames):
>
> > do.call(rbind, lapply(list(df1, df2, df3), reform))
>? ? ? ? POSIX Station Hs Hn flag
> 1? 2000-12-01? ? ? 1 30 10? ? 0
> 2? 2000-12-02? ? ? 1 40 20? ? 0
> 3? 2000-12-03? ? ? 1 50 10? ? 0
> 4? 2000-12-04? ? ? 1 NA NA? NA
> 5? 2000-12-05? ? ? 1 55? 5? ? 0
> 6? 2000-12-01? ? ? 2 20? 0? ? 0
> 7? 2000-12-02? ? ? 2 20? 0? ? 0
> 8? 2000-12-03? ? ? 2 30 10? ? 0
> 9? 2000-12-04? ? ? 2 30? 0? ? 1
> 10 2000-12-05? ? ? 2? 0? 5? ? 0
> 11 2001-12-01? ? ? 1 50 20? ? 0
> 12 2001-12-02? ? ? 1 60 20? ? 0
> 13 2001-12-03? ? ? 1 70 20? ? 0
> 14 2001-12-04? ? ? 1 NA NA? NA
> 15 2001-12-05? ? ? 1 NA NA? NA
> 16 2001-12-01? ? ? 3 20? 0? ? 0
> 17 2001-12-02? ? ? 3 20? 0? ? 0
> 18 2001-12-03? ? ? 3 30 10? ? 0
> 19 2001-12-04? ? ? 3 30? 0? ? 1
> 20 2001-12-05? ? ? 3? 0? 5? ? 0
> 21 2002-12-01? ? ? 2 50 20? ? 0
> 22 2002-12-02? ? ? 2 60 20? ? 0
> 23 2002-12-03? ? ? 2 70 20? ? 0
> 24 2002-12-04? ? ? 2 NA NA? NA
> 25 2002-12-05? ? ? 2 NA NA? NA
> 26 2002-12-01? ? ? 3 20? 0? ? 0
> 27 2002-12-02? ? ? 3 20? 0? ? 0
> 28 2002-12-03? ? ? 3 30 10? ? 0
> 29 2002-12-04? ? ? 3 30? 0? ? 1
> 30 2002-12-05? ? ? 3? 0? 5? ? 0
>
>
> Cheers,
> Bert Gunter
>
>
>
>
> On Wed, Jun 1, 2022 at 11:13 PM Stefano Sofia <
> stefano.sofia at regione.marche.it> wrote:
>
> > Dear R-list users,
> >
> > for each winter season from 2000 to 2022 I have a data frame collecting
> > for different weather stations snowpack height (Hs), snowfall in the last
> > 24h (Hn) and a validation flag.
> >
> > Suppose I have these three following data frames
> >
> >
> > df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 40,
> > 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0, 0,
> 0,
> > NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10, 0,
> > 5), Station2_flag = c(0, 0, 0, 1, 0))
> >
> >
> > df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 60,
> > 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0, 0,
> > 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10,
> > 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> >
> >
> > df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station2_Hs = c(50, 60,
> > 70, NA, NA), Station2_Hn = c(20, 20, 20, NA, NA), Station2_flag = c(0, 0,
> > 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10,
> > 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> >
> >
> > As you can see, each data frame can have different stations loaded.
> >
> > I would need to call rbind matching data frames by column name (i.e. by
> > station name), keeping in mind that the number of stations loaded in each
> > data frame may differ. The result should be
> >
> > data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs Station2_Hn
> > Station2_flag Station3_Hs Station3_Hn Station3_flag
> > 2000-12-01 30 10 0 20 0 0 NA NA NA
> > 2000-12-02 40 20 0 20 0 0 NA NA NA
> > 2000-12-03 50 10 0 30 10 0 NA NA NA
> > 2000-12-04 NA NA NA 30 0 0 NA NA NA
> > 2000-12-05 55 5 0 0 5 0 NA NA NA
> > 2001-12-01 50 20 0 NA NA NA 20 0 0
> > 2001-12-02 60 20 0 NA NA NA 20 0 0
> > 2001-12-03 70 20 0 NA NA NA 30 10 0
> > 2001-12-04 NA NA NA NA NA NA 30 0 1
> > 2001-12-05 NA NA NA NA NA NA 0 5 0
> > 2002-12-01 NA NA NA 50 20 0 20 0 0
> > 2002-12-02 NA NA NA 60 20 0 20 0 0
> > 2002-12-03 NA NA NA 70 20 0 30 10 0
> > 2002-12-04 NA NA NA NA NA NA 30 0 1
> > 2002-12-05 NA NA NA NA NA NA 0 5 0
> >
> > I tried this code
> >
> > df_list <- list(df1, df2, df3)
> > allNms <- unique(unlist(lapply(df_list, names)))
> > do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x,
> > sapply(setdiff(allNms, names(x)), function(y) NA)))),
> make.row.names=FALSE))
> >
> > but I get this error:
> > Error in (function (..., row.names = NULL, check.rows = FALSE,
> check.names
> > = TRUE,? :
> >? arguments imply differing number of rows
> >
> > Could someone please help me?
> >
> >
> > Thank you for your attention
> >
> > Stefano
> >
> >
> >? ? ? ? ? (oo)
> > --oOO--( )--OOo--------------------------------------
> > Stefano Sofia PhD
> > Civil Protection - Marche Region - Italy
> > Meteo Section
> > Snow Section
> > Via del Colle Ameno 5
> > 60126 Torrette di Ancona, Ancona (AN)
> > Uff: +39 071 806 7743
> > E-mail: stefano.sofia at regione.marche.it
> > ---Oo---------oO----------------------------------------
> >
> > ________________________________
> >
> > AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
> > informazioni confidenziali, pertanto ? destinato solo a persone
> autorizzate
> > alla ricezione. I messaggi di posta elettronica per i client di Regione
> > Marche possono contenere informazioni confidenziali e con privilegi
> legali.
> > Se non si ? il destinatario specificato, non leggere, copiare, inoltrare
> o
> > archiviare questo messaggio. Se si ? ricevuto questo messaggio per
> errore,
> > inoltrarlo al mittente ed eliminarlo completamente dal sistema del
> proprio
> > computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che, in
> > caso di necessit? ed urgenza, la risposta al presente messaggio di posta
> > elettronica pu? essere visionata da persone estranee al destinatario.
> > IMPORTANT NOTICE: This e-mail message is intended to be received only by
> > persons entitled to receive the confidential information it may contain.
> > E-mail messages to clients of Regione Marche may contain information that
> > is confidential and legally privileged. Please do not read, copy,
> forward,
> > or store this message unless you are an intended recipient of it. If you
> > have received this message in error, please forward it to the sender and
> > delete it completely from your computer system.
> >
> > --
> > Questo messaggio? stato analizzato da Libraesva ESG ed? risultato non
> > infetto.
> > This message was scanned by Libraesva ESG and is believed to be clean.
> >
> >
> >? ? ? ? [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

??? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From djnord|und @end|ng |rom gm@||@com  Sat Jun  4 09:31:29 2022
From: djnord|und @end|ng |rom gm@||@com (Daniel Nordlund)
Date: Sat, 4 Jun 2022 00:31:29 -0700
Subject: [R] bootstrap CI of the difference between 2 Cramer's V
In-Reply-To: <2109899925.6516974.1653762082722@mail.yahoo.com>
References: <2109899925.6516974.1653762082722.ref@mail.yahoo.com>
 <2109899925.6516974.1653762082722@mail.yahoo.com>
Message-ID: <a5972539-71a0-864e-4843-5144698ac60b@gmail.com>

On 5/28/2022 11:21 AM, varin sacha via R-help wrote:
> Dear R-experts,
>
> While comparing groups, it is better to assess confidence intervals of those differences rather than comparing confidence intervals for each group.
> I am trying to calculate the CIs of the difference between the two Cramer's V and not the CI to the estimate of each group?s Cramer's V.
>
> Here below my toy R example. There are error messages. Any help would be highly appreciated.
>
> ##############################
> library(questionr)
> library(boot)
>
> gender1<-c("M","F","F","F","M","M","F","F","F","M","M","F","M","M","F","M","M","F","M","F","F","F","M","M","M","F","F","M","M","M","F","M","F","F","F","M","M","F","M","F")
> color1<-c("blue","green","black","black","green","green","blue","blue","green","black","blue","green","blue","black","black","blue","green","blue","green","black","blue","blue","black","black","green","green","blue","green","black","green","blue","black","black","blue","green","green","green","blue","blue","black")
>
> gender2<-c("F","F","F","M","M","F","M","M","M","F","F","M","F","M","F","F","M","M","M","F","M","M","M","F","F","F","M","M","M","F","M","M","M","F","F","F","M","F","F","F")
> color2<-c("green","blue","black","blue","blue","blue","green","blue","green","black","blue","black","blue","blue","black","blue","blue","green","blue","black","blue","blue","black","black","green","blue","black","green","blue","green","black","blue","black","blue","green","blue","green","green","blue","black")
>
> f1=data.frame(gender1,color1)
> tab1<-table(gender1,color1)
> e1<-cramer.v(tab1)
>
> f2=data.frame(gender2,color2)
> tab2<-table(gender2,color2)
> e2<-cramer.v(tab2)
>
> f3<-data.frame(e1-e2)
>
> cramerdiff=function(x,w){
> y<-tapply(x[w,1], x[w,2],cramer.v)
> y[1]-y[2]
> }
>
> results<-boot(data=f3,statistic=cramerdiff,R=2000)
> results
>
> boot.ci(results,type="all")
> ##############################
>
>   
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

I don't know if someone responded offline, but if not, there are a 
couple of problems with your code. ? First, the f3 dataframe is not what 
you think it is.? Second, your cramerdiff function isn't going to 
produce the results that you want.

I would put your data into a single dataframe with a variable 
designating which group data came from.? Then use that variable as the 
strata variable in the boot function to resample within groups.? So 
something like this:

f1 <- data.frame(gender=gender1,color=color1,group='grp1')
f2 <- data.frame(gender=gender2,color=color2,group='grp2')
f3 <- rbind(f1,f2)

cramerdiff <- function(x, ndx) {
 ?? # calculate cramer.v for group 1 bootstrap sample
 ?? g1 <-x[ndx,][x[,3]=='grp1',]
 ?? cramer_g1 <- cramer.v(table(g1[,1:2]))
 ?? # calculate cramer.v for group 2 bootstrap sample
 ?? g2 <-x[ndx,][x[,3]=='grp2',]
 ?? cramer_g2 <- cramer.v(table(g2[,1:2]))
 ?? # calculate difference
 ?? cramer_g1-cramer_g2
 ?? }
# use strata parameter in function boot to resample within each group
results <- boot(data=f3,statistic=cramerdiff, 
strata=as.factor(f3$group),R=2000)
results
boot.ci(results)


Hope this is helpful,

Dan

-- 
Daniel Nordlund
Port Townsend, WA  USA


-- 
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus


From tebert @end|ng |rom u||@edu  Sat Jun  4 13:11:42 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Sat, 4 Jun 2022 11:11:42 +0000
Subject: [R] bootstrap CI of the difference between 2 Cramer's V
In-Reply-To: <a5972539-71a0-864e-4843-5144698ac60b@gmail.com>
References: <2109899925.6516974.1653762082722.ref@mail.yahoo.com>
 <2109899925.6516974.1653762082722@mail.yahoo.com>
 <a5972539-71a0-864e-4843-5144698ac60b@gmail.com>
Message-ID: <BN6PR2201MB15530226780B90DD66CFD955CFA09@BN6PR2201MB1553.namprd22.prod.outlook.com>

I would calculate the difference and the CI about that difference. You would not get the same thing by comparing the bootstrap CI of the group means.
One use for this is to determine if the confidence interval for the difference in means includes zero. An alternative would be to use a more conventional test (rather than calculate a difference) and then find a mean p-value and a confidence interval about the p-value. This gives a better assessment of the p-value but is harder to decide if the test outcome is "significant." 

You might also consider whether you want a permutation test, a randomization test, or a bootstrap. A permutation test will look at all possible combinations of the data once. Use this approach when computationally reasonable. A randomization test will look at a random subset of all possible combinations, but may include repeats of some combinations. Both of these do not replace values. The bootstrap replaces values and will therefore tend to minimize the effects of outliers in the data. With small datasets a risk is that there are few permutations and performing a randomization test with 1,000,000 randomizations on data with 4000 permutations is not good.

Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Daniel Nordlund
Sent: Saturday, June 4, 2022 3:31 AM
To: varin sacha <varinsacha at yahoo.fr>; r-help at r-project.org
Subject: Re: [R] bootstrap CI of the difference between 2 Cramer's V

[External Email]

On 5/28/2022 11:21 AM, varin sacha via R-help wrote:
> Dear R-experts,
>
> While comparing groups, it is better to assess confidence intervals of those differences rather than comparing confidence intervals for each group.
> I am trying to calculate the CIs of the difference between the two Cramer's V and not the CI to the estimate of each group?s Cramer's V.
>
> Here below my toy R example. There are error messages. Any help would be highly appreciated.
>
> ##############################
> library(questionr)
> library(boot)
>
> gender1<-c("M","F","F","F","M","M","F","F","F","M","M","F","M","M","F"
> ,"M","M","F","M","F","F","F","M","M","M","F","F","M","M","M","F","M","
> F","F","F","M","M","F","M","F")
> color1<-c("blue","green","black","black","green","green","blue","blue"
> ,"green","black","blue","green","blue","black","black","blue","green",
> "blue","green","black","blue","blue","black","black","green","green","
> blue","green","black","green","blue","black","black","blue","green","g
> reen","green","blue","blue","black")
>
> gender2<-c("F","F","F","M","M","F","M","M","M","F","F","M","F","M","F"
> ,"F","M","M","M","F","M","M","M","F","F","F","M","M","M","F","M","M","
> M","F","F","F","M","F","F","F")
> color2<-c("green","blue","black","blue","blue","blue","green","blue","
> green","black","blue","black","blue","blue","black","blue","blue","gre
> en","blue","black","blue","blue","black","black","green","blue","black
> ","green","blue","green","black","blue","black","blue","green","blue",
> "green","green","blue","black")
>
> f1=data.frame(gender1,color1)
> tab1<-table(gender1,color1)
> e1<-cramer.v(tab1)
>
> f2=data.frame(gender2,color2)
> tab2<-table(gender2,color2)
> e2<-cramer.v(tab2)
>
> f3<-data.frame(e1-e2)
>
> cramerdiff=function(x,w){
> y<-tapply(x[w,1], x[w,2],cramer.v)
> y[1]-y[2]
> }
>
> results<-boot(data=f3,statistic=cramerdiff,R=2000)
> results
>
> boot.ci(results,type="all")
> ##############################
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=9NrsizTQzUnuqLRvbQaINvkX7iBIqmQgfbus-vohqP_KZnrkn_b1iH1ma
> wVqPzLz&s=a-dJNz_c6kgMANbI7VZ9N96pRhcKodeukMsVJ0Ol2qc&e=
> PLEASE do read the posting guide 
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=9NrsizTQzUnuqLRvbQaINvkX7iBIqmQgfbus-vohqP_KZnrkn_b1iH1m
> awVqPzLz&s=iLlqKhwHsqxsBYuq5S1ooeyH3sepv85k8fhSi27sOG8&e=
> and provide commented, minimal, self-contained, reproducible code.

I don't know if someone responded offline, but if not, there are a
couple of problems with your code.   First, the f3 dataframe is not what
you think it is.  Second, your cramerdiff function isn't going to produce the results that you want.

I would put your data into a single dataframe with a variable designating which group data came from.  Then use that variable as the strata variable in the boot function to resample within groups.  So something like this:

f1 <- data.frame(gender=gender1,color=color1,group='grp1')
f2 <- data.frame(gender=gender2,color=color2,group='grp2')
f3 <- rbind(f1,f2)

cramerdiff <- function(x, ndx) {
    # calculate cramer.v for group 1 bootstrap sample
    g1 <-x[ndx,][x[,3]=='grp1',]
    cramer_g1 <- cramer.v(table(g1[,1:2]))
    # calculate cramer.v for group 2 bootstrap sample
    g2 <-x[ndx,][x[,3]=='grp2',]
    cramer_g2 <- cramer.v(table(g2[,1:2]))
    # calculate difference
    cramer_g1-cramer_g2
    }
# use strata parameter in function boot to resample within each group results <- boot(data=f3,statistic=cramerdiff,
strata=as.factor(f3$group),R=2000)
results
boot.ci(results)


Hope this is helpful,

Dan

--
Daniel Nordlund
Port Townsend, WA  USA


--
This email has been checked for viruses by Avast antivirus software.
https://urldefense.proofpoint.com/v2/url?u=https-3A__www.avast.com_antivirus&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=9NrsizTQzUnuqLRvbQaINvkX7iBIqmQgfbus-vohqP_KZnrkn_b1iH1mawVqPzLz&s=hO-ovpt1HbZ1YM4mIaOCGdPXuVtxnWfAk8ro5PgkZvw&e=

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=9NrsizTQzUnuqLRvbQaINvkX7iBIqmQgfbus-vohqP_KZnrkn_b1iH1mawVqPzLz&s=a-dJNz_c6kgMANbI7VZ9N96pRhcKodeukMsVJ0Ol2qc&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=9NrsizTQzUnuqLRvbQaINvkX7iBIqmQgfbus-vohqP_KZnrkn_b1iH1mawVqPzLz&s=iLlqKhwHsqxsBYuq5S1ooeyH3sepv85k8fhSi27sOG8&e=
and provide commented, minimal, self-contained, reproducible code.

From bh@@k@r@ko|k@t@ @end|ng |rom gm@||@com  Sat Jun  4 23:19:28 2022
From: bh@@k@r@ko|k@t@ @end|ng |rom gm@||@com (Bhaskar Mitra)
Date: Sat, 4 Jun 2022 14:19:28 -0700
Subject: [R] 
 Request for some help: Error when joining multiple csv files
 together
In-Reply-To: <f040297a-71b6-dadd-8e34-b1a523d105e9@sapo.pt>
References: <CAEGXkYV-F6sRgYaq=57ic-iXF52r-4GyLWW4485f+Rzz3wq_KA@mail.gmail.com>
 <f040297a-71b6-dadd-8e34-b1a523d105e9@sapo.pt>
Message-ID: <CAEGXkYVWcGsFMOdNBuKne2oVDCO-6j17R6cCcdtyVUeq_f0rbw@mail.gmail.com>

Thanks Rui.
The first approach suggested by you worked perfectly.

best,
bhaskar

On Thu, Jun 2, 2022 at 12:30 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> I'm seeing two obvious errors, those are not csv files and the columns
> spec is wrong, you have a spec of 6 columns but the posted data only has
> 4 and of different classes.
>
> If the data is in comma separated values (CSV) files the following
> worked without errors.
>
>
> library(readr)
>
> col_spec <- cols(
>    d1 = col_double(),
>    d2 = col_double(),
>    d3 = col_double(),
>    d4 = col_double()
> )
>
> list.files(pattern = "*\\.csv$") |>
>    lapply(read_csv, col_types = col_spec) |>
>    dplyr::bind_rows()
>
>
> If the data is like in the question, try instead
>
>
> list.files(pattern = "*\\.csv$") |>
>    lapply(read_delim, delim = " ", col_types = col_spec) |>
>    dplyr::bind_rows()
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 00:17 de 02/06/2022, Bhaskar Mitra escreveu:
> > Hello Everyone,
> >
> > I have a bunch of csv files, all with common headers (d1, d2, d3, d4).
> > When I am trying to join the csv files together, with the following code
> > (shown below),
> > I am getting a warning  when the files are joined. The values under
> columns
> > d3 and d4 are not joined properly.
> >
> > The code and the error are given below. I would really appreciate some
> help
> > in this regard.
> >
> > regards,
> > bhaskar
> >
> >
> > #---code--------------------------------
> >
> > df <- list.files(pattern = "*.csv") %>%
> >    lapply(read_csv) %>%
> >    bind_rows
> >
> > #---code--------------------------------
> >
> > Header of each file:
> >
> > d1  d2  d3  d4
> > 3   4    NA   NA
> > 4   5   NA   7
> > 5   6   8   8
> > 6   7   8   NA
> >
> > #--------------------------------------------------------
> >
> > #Warning when the codes run --------------------------------------
> >
> > Column specification
> >
> ?????????????????????????????????????????????????????????????????????????????????????????????????????????????????
> > cols(
> >    .default = col_double(),
> >    name = col_character(),
> >    d1 = col_datetime(format = ""),
> >    d2 = col_character(),
> >    d3 = col_logical(),
> >    d4 = col_logical()
> > )
> >
> > Warning: 48766 parsing failures.
> > * row*    *col*          * expected  *                         *actual*
> >       *file*
> > 3529   d3           1/0/T/F/TRUE/FALSE       100             'file1.csv'
> > 3529   d4            1/0/T/F/TRUE/FALSE      100             'file1.csv'
> >
> > .... ..... .................. ...... ................................
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Sun Jun  5 18:21:08 2022
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Sun, 5 Jun 2022 16:21:08 +0000 (UTC)
Subject: [R] bootstrap CI of the difference between 2 Cramer's V
In-Reply-To: <a5972539-71a0-864e-4843-5144698ac60b@gmail.com>
References: <2109899925.6516974.1653762082722.ref@mail.yahoo.com>
 <2109899925.6516974.1653762082722@mail.yahoo.com>
 <a5972539-71a0-864e-4843-5144698ac60b@gmail.com>
Message-ID: <317595679.11661946.1654446068116@mail.yahoo.com>

Dear Daniel,
Dear R-experts,

I really thank you a lot Daniel. Nobody had answered to me offline. So, thanks.
I have tried in the same vein for the Goodman-Kruskal gamma for ordinal data. There is an error message at the end of the code. Thanks for your help.


##############################
library(ryouready)
library(boot)

shopping1<-c("tr?s important","important","pas important","pas important","important","tr?s important","important","pas important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","important")

statut1<-c("riche","pas riche","moyennement riche","moyennement riche","riche","pas riche","moyennement riche","moyennement riche","riche","pas riche","moyennement riche","riche","pas riche","pas riche","riche","moyennement riche","riche","pas riche","pas riche","pas riche","riche","riche","moyennement riche","riche","riche","moyennement riche","moyennement riche","moyennement riche","pas riche","pas riche","riche","pas riche","riche","pas riche","riche","moyennement riche","riche","pas riche","moyennement riche","riche")

shopping2<-c("important","pas important","tr?s important","tr?s important","important","tr?s important","pas important","important","pas important","tr?s important","important","important","important","important","pas important","tr?s important","tr?s important","important","pas important","tr?s important","pas important","tr?s important","pas important","tr?s important","important","tr?s important","important","pas important","pas important","important","pas important","tr?s important","pas important","pas important","important","important","tr?s important","tr?s important","pas important","pas important")

statut2<-c("moyennement riche","pas riche","riche","moyennement riche","moyennement riche","moyennement riche","pas riche","riche","riche","pas riche","moyennement riche","riche","riche","riche","riche","riche","pas riche","moyennement riche","moyennement riche","pas riche","moyennement riche","pas riche","pas riche","pas riche","moyennement riche","riche","moyennement riche","riche","pas riche","riche","moyennement riche","blue","moyennement riche","pas riche","pas riche","riche","riche","pas riche","pas riche","pas riche")

f1 <- data.frame(shopping=shopping1,statut=statut1,group='grp1')
f2 <- data.frame(shopping=shopping2,statut=statut2,group='grp2')
f3 <- rbind(f1,f2)

G <- function(x, index) {
?? 
# calculate goodman for group 1 bootstrap sample
?? g1 <-x[index,][x[,3]=='grp1',]
?? goodman_g1 <- cor(data[index,][1,2])
??
?# calculate goodman for group 2 bootstrap sample
?? g2 <-x[index,][x[,3]=='grp2',]
?? goodman_g2 <- cor(data[index,][3,4])
??
?# calculate difference
?? goodman_g1-goodman_g2
?? }
?

# use strata parameter in function boot to resample within each group
results <- boot(data=f3,statistic=G, strata=as.factor(f3$group),R=2000)

results
boot.ci(results)
##############################



Le samedi 4 juin 2022 ? 09:31:36 UTC+2, Daniel Nordlund <djnordlund at gmail.com> a ?crit : 





On 5/28/2022 11:21 AM, varin sacha via R-help wrote:
> Dear R-experts,
>
> While comparing groups, it is better to assess confidence intervals of those differences rather than comparing confidence intervals for each group.
> I am trying to calculate the CIs of the difference between the two Cramer's V and not the CI to the estimate of each group?s Cramer's V.
>
> Here below my toy R example. There are error messages. Any help would be highly appreciated.
>
> ##############################
> library(questionr)
> library(boot)
>
> gender1<-c("M","F","F","F","M","M","F","F","F","M","M","F","M","M","F","M","M","F","M","F","F","F","M","M","M","F","F","M","M","M","F","M","F","F","F","M","M","F","M","F")
> color1<-c("blue","green","black","black","green","green","blue","blue","green","black","blue","green","blue","black","black","blue","green","blue","green","black","blue","blue","black","black","green","green","blue","green","black","green","blue","black","black","blue","green","green","green","blue","blue","black")
>
> gender2<-c("F","F","F","M","M","F","M","M","M","F","F","M","F","M","F","F","M","M","M","F","M","M","M","F","F","F","M","M","M","F","M","M","M","F","F","F","M","F","F","F")
> color2<-c("green","blue","black","blue","blue","blue","green","blue","green","black","blue","black","blue","blue","black","blue","blue","green","blue","black","blue","blue","black","black","green","blue","black","green","blue","green","black","blue","black","blue","green","blue","green","green","blue","black")
>
> f1=data.frame(gender1,color1)
> tab1<-table(gender1,color1)
> e1<-cramer.v(tab1)
>
> f2=data.frame(gender2,color2)
> tab2<-table(gender2,color2)
> e2<-cramer.v(tab2)
>
> f3<-data.frame(e1-e2)
>
> cramerdiff=function(x,w){
> y<-tapply(x[w,1], x[w,2],cramer.v)
> y[1]-y[2]
> }
>
> results<-boot(data=f3,statistic=cramerdiff,R=2000)
> results
>
> boot.ci(results,type="all")
> ##############################
>
>? 
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

I don't know if someone responded offline, but if not, there are a 
couple of problems with your code. ? First, the f3 dataframe is not what 
you think it is.? Second, your cramerdiff function isn't going to 
produce the results that you want.

I would put your data into a single dataframe with a variable 
designating which group data came from.? Then use that variable as the
strata variable in the boot function to resample within groups.? So 
something like this:

f1 <- data.frame(gender=gender1,color=color1,group='grp1')
f2 <- data.frame(gender=gender2,color=color2,group='grp2')
f3 <- rbind(f1,f2)

cramerdiff <- function(x, ndx) {
?? # calculate cramer.v for group 1 bootstrap sample
?? g1 <-x[ndx,][x[,3]=='grp1',]
?? cramer_g1 <- cramer.v(table(g1[,1:2]))
?? # calculate cramer.v for group 2 bootstrap sample
?? g2 <-x[ndx,][x[,3]=='grp2',]
?? cramer_g2 <- cramer.v(table(g2[,1:2]))
?? # calculate difference
?? cramer_g1-cramer_g2
?? }
# use strata parameter in function boot to resample within each group
results <- boot(data=f3,statistic=cramerdiff, 
strata=as.factor(f3$group),R=2000)

results
boot.ci(results)



Hope this is helpful,

Dan

-- 
Daniel Nordlund
Port Townsend, WA? USA


-- 
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus


From j@de@shod@@ m@iii@g oii googiem@ii@com  Sun Jun  5 21:01:38 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Sun, 5 Jun 2022 20:01:38 +0100
Subject: [R] High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
Message-ID: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>

Hello everyone,

A few days ago I asked a question about concurvity in a GAM (the
anologue of collinearity in a GLM) implemented in mgcv. I think my
question was a bit unfocussed, so I am retrying again, but with
additional information included about the autocorrelation function. I
have also posted about this on Cross Validated. Given all the model
output, it might make for easier
reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea

As mentioned previously, I have problems with concurvity in my thesis
research, and don't have access to a statistician who works with time
series, GAMs or R. I'd be very grateful for any (partial) answer,
however short. I'll gladly return the favour where I can! For really
helpful input I'd be more than happy to offer co-authorship on
publication. Deadlines are very close, and I'm heading towards having
no results at all if I can't solve this concurvity issue :(

I'm using GAMs to try to understand the relationship between deaths
and heat-related variables (e.g. temperature and humidity), using
daily time series over a 14-year period from a tropical, low-income
country. My aim is to understand the relationship between these
variables and deaths, rather than pure prediction performance.

The GAMs include distributed lag models (set up as 7-column matrices,
see code at bottom of post), since deaths may occur over several days
following exposure.

Simple GAMs with just time, lagged temperature and lagged
precipitation (a potential confounder) show very high concurvity
between lagged temperature and time, regardless of the many different
ways I have tried to decompose time. The autocorrelation functions
(ACF) however, shows values close to zero, only just about breaching
the 'significance line' in a few instances. It does show patterning
though, although the regularity is difficult to define.

My questions are:
1) Should I be worried about the high concurvity, or can I ignore it
given the mostly non-significant ACF? I've read dozens of
heat-mortality modelling studies and none report on concurvity between
weather variables and time (though one 2012 paper discussed
autocorrelation).

2) If I cannot ignore it, what should I do to resolve it? Would
including an autoregressive term be appropriate, and if so, where can
I find a coded example of how to do this? I've also come across
sequential regression][1]. Would this be more or less appropriate? If
appropriate, a pointer to an example would be really appreciated!

Some example GAMs are specified as follows:
```r
conc38b <- gam(deaths~te(year, month, week, weekday,
bs=c("cr","cc","cc","cc")) + heap +
                      te(temp_max, lag, k=c(10, 3)) +
                      te(precip_daily_total, lag, k=c(10, 3)),
                      data = dat, family = nb, method = 'REML', select = TRUE,
                      knots = list(month = c(0.5, 12.5), week = c(0.5,
52.5), weekday = c(0, 6.5)))
```
Concurvity for the above model between (temp_max, lag) and (year,
month, week, weekday) is 0.91:

```r
$worst
                                    para te(year,month,week,weekday)
te(temp_max,lag) te(precip_daily_total,lag)
para                        1.000000e+00                1.125625e-29
     0.3150073                  0.6666348
te(year,month,week,weekday) 1.400648e-29                1.000000e+00
     0.9060552                  0.6652313
te(temp_max,lag)            3.152795e-01                8.998113e-01
     1.0000000                  0.5781015
te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
     0.5805159                  1.0000000
```

Output from ```gam.check()```:
```r
Method: REML   Optimizer: outer newton
full convergence after 16 iterations.
Gradient range [-0.01467332,0.003096643]
(score 8915.994 & scale 1).
Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
Model rank =  544 / 544

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

                                  k'      edf k-index p-value
te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
te(temp_max,lag)             29.0000   3.3681      NA      NA
te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
```

Some output from ```summary(conc38b)```:
```r
Approximate significance of smooth terms:
                                  edf Ref.df  Chi.sq p-value
te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
te(temp_max,lag)             3.368129     27  11.130 0.00145 **
te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

R-sq.(adj) =  0.839   Deviance explained = 53.3%
-REML =   8916  Scale est. = 1         n = 5107
```


Below are the ACF plots (note limit y-axis = 0.1 for clarity of
pattern). They show peaks at 5 and 15, and then there seems to be a
recurring pattern at multiples of approx. 30 (suggesting month is not
modelled adequately?). Not sure what would cause the spikes at 5 and
15. There is heaping of deaths on the 15th day of each month, to which
deaths with unknown date were allocated. This heaping was modelled
with categorical variable/ factor ```heap``` with 169 levels (0 for
all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
heaping day over the 14-year period):

  [2]: https://i.stack.imgur.com/FzKyM.png
  [3]: https://i.stack.imgur.com/fE3aL.png


I get an identical looking ACF when I decompose time into (year,
month, monthday) as in model conc39 below, although concurvity between
(temp_max, lag) and the time term has now dropped somewhat to 0.83:

```r
conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
                     te(temp_max, lag, k=c(10, 4)) +
                     te(precip_daily_total, lag, k=c(10, 4)),
                     data = dat, family = nb, method = 'REML', select = TRUE,
                     knots = list(month = c(0.5, 12.5)))
```
```r

Method: REML   Optimizer: outer newton
full convergence after 14 iterations.
Gradient range [-0.001578187,6.155096e-05]
(score 8915.763 & scale 1).
Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
Model rank =  323 / 323

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

                                k'     edf k-index p-value
te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
te(temp_max,lag)           39.0000  4.0875      NA      NA
te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
```
Some output from ```summary(conc39)```:
```r
Approximate significance of smooth terms:
                                edf Ref.df  Chi.sq  p-value
te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
te(precip_daily_total,lag)  0.01173     36   0.008    0.557
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

R-sq.(adj) =  0.839   Deviance explained = 53.8%
-REML =   8915  Scale est. = 1         n = 5107
```


```r
$worst
                                   para te(year,month,monthday)
te(temp_max,lag) te(precip_daily_total,lag)
para                       1.000000e+00            3.261007e-31
0.3313549                  0.6666532
te(year,month,monthday)    3.060763e-31            1.000000e+00
0.8266086                  0.5670777
te(temp_max,lag)           3.331014e-01            8.225942e-01
1.0000000                  0.5840875
te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
0.5939380                  1.0000000
```

Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
various choices for k or as ```s(time)``` with various k does not
reduce concurvity either.


The default approach in time series studies of heat-mortality is to
model time with fixed df, generally between 7-10 df per year of data.
I am, however, apprehensive about this approach because a) mortality
profiles vary with locality due to sociodemographic and environmental
characteristics and b) the choice of df is based on higher income
countries (where nearly all these studies have been done) with
different mortality profiles and so may not be appropriate for
tropical, low-income countries.

Although the approach of fixing (high) df does remove more temporal
patterns from the ACF (see model and output below), concurvity between
time and lagged temperature has now risen to 0.99! Moreover,
temperature (which has been a consistent, highly significant predictor
in every model of the tens (hundreds?) I have run, has now turned
non-significant. I am guessing this is because time is now a very
wiggly function that not only models/ removes seasonal variation, but
also some of the day-to-day variation that is needed for the
temperature smooth  :

```r
conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
                      te(temp_max, lag, k=c(10,3)) +
                      te(precip_daily_total, lag, k=c(10,3)),
                      data = dat, family = nb, method = 'REML', select = TRUE)
```
Output from ```gam.check(conc20a, rep = 1000)```:

```r
Method: REML   Optimizer: outer newton
full convergence after 9 iterations.
Gradient range [-0.0008983099,9.546022e-05]
(score 8750.13 & scale 1).
Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
Model rank =  336 / 336

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

                                 k'      edf k-index p-value
s(time)                    111.0000 111.0000    0.98    0.56
te(temp_max,lag)            29.0000   0.6548      NA      NA
te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
```
Output from ```concurvity(conc20a, full=FALSE)$worst```:

```r
                                   para      s(time) te(temp_max,lag)
te(precip_daily_total,lag)
para                       1.000000e+00 2.462064e-19        0.3165236
                0.6666348
s(time)                    2.462398e-19 1.000000e+00        0.9930674
                0.6879284
te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
                0.5788711
te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
                1.0000000

```

Some output from ```summary(conc20a)```:
```r
Approximate significance of smooth terms:
                                 edf Ref.df  Chi.sq p-value
s(time)                    1.110e+02    111 419.375  <2e-16 ***
te(temp_max,lag)           6.548e-01     27   0.895   0.249
te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

R-sq.(adj) =  0.843   Deviance explained = 56.1%
-REML = 8750.1  Scale est. = 1         n = 5107
```

ACF functions:

[4]: https://i.stack.imgur.com/7nbXS.png
[5]: https://i.stack.imgur.com/pNnZU.png

Data can be found on my [GitHub][6] site in the file
[data_cross_validated_post2.rds][7]. A csv version is also available.
This is my code:

```r
library(readr)
library(mgcv)

df <- read_rds("data_crossvalidated_post2.rds")

# Create matrices for lagged weather variables (6 day lags) based on
example by Simon Wood
# in his 2017 book ("Generalized additive models: an introduction with
R", p. 349) and
# gamair package documentation
(https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)

lagard <- function(x,n.lag=7) {
n <- length(x); X <- matrix(NA,n,n.lag)
for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
X
}

dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
weekday = df$weekday, week = df$week, monthday = df$monthday, time =
df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
= df$precip_hourly_dailysum)
dat$temp_max <- lagard(df$temp_max)
dat$temp_min <- lagard(df$temp_min)
dat$temp_mean <- lagard(df$temp_mean)
dat$wbgt_max <- lagard(df$wbgt_max)
dat$wbgt_mean <- lagard(df$wbgt_mean)
dat$wbgt_min <- lagard(df$wbgt_min)
dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
dat$sh_mean <- lagard(df$sh_mean)
dat$solar_mean <- lagard(df$solar_mean)
dat$wind2m_mean <- lagard(df$wind2m_mean)
dat$sh_max <- lagard(df$sh_max)
dat$solar_max <- lagard(df$solar_max)
dat$wind2m_max <- lagard(df$wind2m_max)
dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
dat$precip_hourly <- lagard(df$precip_hourly)
dat$precip_daily_total <- lagard( df$precip_daily_total)
dat$temp <- lagard(df$temp)
dat$sh <- lagard(df$sh)
dat$rh <- lagard(df$rh)
dat$solar <- lagard(df$solar)
dat$wind2m <- lagard(df$wind2m)


conc38b <- gam(deaths~te(year, month, week, weekday,
bs=c("cr","cc","cc","cc")) + heap +
                      te(temp_max, lag, k=c(10, 3)) +
                      te(precip_daily_total, lag, k=c(10, 3)),
                      data = dat, family = nb, method = 'REML', select = TRUE,
                      knots = list(month = c(0.5, 12.5), week = c(0.5,
52.5), weekday = c(0, 6.5)))

conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
                     te(temp_max, lag, k=c(10, 4)) +
                     te(precip_daily_total, lag, k=c(10, 4)),
                     data = dat, family = nb, method = 'REML', select = TRUE,
                     knots = list(month = c(0.5, 12.5)))

conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
                      te(temp_max, lag, k=c(10,3)) +
                      te(precip_daily_total, lag, k=c(10,3)),
                      data = dat, family = nb, method = 'REML', select = TRUE)

```
Thank you if you've read this far!! :-))

  [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
  [2]: https://i.stack.imgur.com/FzKyM.png
  [3]: https://i.stack.imgur.com/fE3aL.png
  [4]: https://i.stack.imgur.com/7nbXS.png
  [5]: https://i.stack.imgur.com/pNnZU.png
  [6]: https://github.com/JadeShodan/heat-mortality
  [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds


From djnord|und @end|ng |rom gm@||@com  Mon Jun  6 02:40:59 2022
From: djnord|und @end|ng |rom gm@||@com (Daniel Nordlund)
Date: Sun, 5 Jun 2022 17:40:59 -0700
Subject: [R] bootstrap CI of the difference between 2 Cramer's V
In-Reply-To: <317595679.11661946.1654446068116@mail.yahoo.com>
References: <2109899925.6516974.1653762082722.ref@mail.yahoo.com>
 <2109899925.6516974.1653762082722@mail.yahoo.com>
 <a5972539-71a0-864e-4843-5144698ac60b@gmail.com>
 <317595679.11661946.1654446068116@mail.yahoo.com>
Message-ID: <9bc0d7b2-04fc-8e7a-72bd-2eaaadd91ccf@gmail.com>

There are a few problems with the "rewrite" of the code, both 
syntactically and conceptually.
1. Goodman-Kruskal gamma is for ordinal data.? You should create your 
"shopping" and "statut" variables as factors, ordered from lowest to 
highest using the levels= parameter in? the function, factor
2. In your function, G, you use "data[index,][1,2]"? where you should 
have used either "g1[,c(1,2)]", or "g2[,c(1,2)]".? You should read up on 
Indexing using [] on data frames, to make sure you understand what the 
original code was doing.
3.? The base cor function does not calculate a Goodman-Kruskal gamma 
(unless somebody has written a new version).? So you need to find an 
appropriate function and you may need to structure your data differently 
for calculating gamma, depending on what parameters the function 
demands.? Google is your friend here, search for??? "R Goodman Kruskal 
gamma"

Since this is looking like homework to me, I suggest you ask your 
instructor about some of this.

Best of luck,

Dan


On 6/5/2022 9:21 AM, varin sacha wrote:
> Dear Daniel,
> Dear R-experts,
>
> I really thank you a lot Daniel. Nobody had answered to me offline. So, thanks.
> I have tried in the same vein for the Goodman-Kruskal gamma for ordinal data. There is an error message at the end of the code. Thanks for your help.
>
>
> ##############################
> library(ryouready)
> library(boot)
>
> shopping1<-c("tr?s important","important","pas important","pas important","important","tr?s important","important","pas important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","important")
>
> statut1<-c("riche","pas riche","moyennement riche","moyennement riche","riche","pas riche","moyennement riche","moyennement riche","riche","pas riche","moyennement riche","riche","pas riche","pas riche","riche","moyennement riche","riche","pas riche","pas riche","pas riche","riche","riche","moyennement riche","riche","riche","moyennement riche","moyennement riche","moyennement riche","pas riche","pas riche","riche","pas riche","riche","pas riche","riche","moyennement riche","riche","pas riche","moyennement riche","riche")
>
> shopping2<-c("important","pas important","tr?s important","tr?s important","important","tr?s important","pas important","important","pas important","tr?s important","important","important","important","important","pas important","tr?s important","tr?s important","important","pas important","tr?s important","pas important","tr?s important","pas important","tr?s important","important","tr?s important","important","pas important","pas important","important","pas important","tr?s important","pas important","pas important","important","important","tr?s important","tr?s important","pas important","pas important")
>
> statut2<-c("moyennement riche","pas riche","riche","moyennement riche","moyennement riche","moyennement riche","pas riche","riche","riche","pas riche","moyennement riche","riche","riche","riche","riche","riche","pas riche","moyennement riche","moyennement riche","pas riche","moyennement riche","pas riche","pas riche","pas riche","moyennement riche","riche","moyennement riche","riche","pas riche","riche","moyennement riche","blue","moyennement riche","pas riche","pas riche","riche","riche","pas riche","pas riche","pas riche")
>
> f1 <- data.frame(shopping=shopping1,statut=statut1,group='grp1')
> f2 <- data.frame(shopping=shopping2,statut=statut2,group='grp2')
> f3 <- rbind(f1,f2)
>
> G <- function(x, index) {
>     
> # calculate goodman for group 1 bootstrap sample
>  ?? g1 <-x[index,][x[,3]=='grp1',]
>  ?? goodman_g1 <- cor(data[index,][1,2])
>    
>  ?# calculate goodman for group 2 bootstrap sample
>  ?? g2 <-x[index,][x[,3]=='grp2',]
>  ?? goodman_g2 <- cor(data[index,][3,4])
>    
>  ?# calculate difference
>  ?? goodman_g1-goodman_g2
>  ?? }
>   
>
> # use strata parameter in function boot to resample within each group
> results <- boot(data=f3,statistic=G, strata=as.factor(f3$group),R=2000)
>
> results
> boot.ci(results)
> ##############################
>
>
>
> Le samedi 4 juin 2022 ? 09:31:36 UTC+2, Daniel Nordlund <djnordlund at gmail.com> a ?crit :
>
>
>
>
>
> On 5/28/2022 11:21 AM, varin sacha via R-help wrote:
>> Dear R-experts,
>>
>> While comparing groups, it is better to assess confidence intervals of those differences rather than comparing confidence intervals for each group.
>> I am trying to calculate the CIs of the difference between the two Cramer's V and not the CI to the estimate of each group?s Cramer's V.
>>
>> Here below my toy R example. There are error messages. Any help would be highly appreciated.
>>
>> ##############################
>> library(questionr)
>> library(boot)
>>
>> gender1<-c("M","F","F","F","M","M","F","F","F","M","M","F","M","M","F","M","M","F","M","F","F","F","M","M","M","F","F","M","M","M","F","M","F","F","F","M","M","F","M","F")
>> color1<-c("blue","green","black","black","green","green","blue","blue","green","black","blue","green","blue","black","black","blue","green","blue","green","black","blue","blue","black","black","green","green","blue","green","black","green","blue","black","black","blue","green","green","green","blue","blue","black")
>>
>> gender2<-c("F","F","F","M","M","F","M","M","M","F","F","M","F","M","F","F","M","M","M","F","M","M","M","F","F","F","M","M","M","F","M","M","M","F","F","F","M","F","F","F")
>> color2<-c("green","blue","black","blue","blue","blue","green","blue","green","black","blue","black","blue","blue","black","blue","blue","green","blue","black","blue","blue","black","black","green","blue","black","green","blue","green","black","blue","black","blue","green","blue","green","green","blue","black")
>>
>> f1=data.frame(gender1,color1)
>> tab1<-table(gender1,color1)
>> e1<-cramer.v(tab1)
>>
>> f2=data.frame(gender2,color2)
>> tab2<-table(gender2,color2)
>> e2<-cramer.v(tab2)
>>
>> f3<-data.frame(e1-e2)
>>
>> cramerdiff=function(x,w){
>> y<-tapply(x[w,1], x[w,2],cramer.v)
>> y[1]-y[2]
>> }
>>
>> results<-boot(data=f3,statistic=cramerdiff,R=2000)
>> results
>>
>> boot.ci(results,type="all")
>> ##############################
>>
>>    
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> I don't know if someone responded offline, but if not, there are a
> couple of problems with your code. ? First, the f3 dataframe is not what
> you think it is.? Second, your cramerdiff function isn't going to
> produce the results that you want.
>
> I would put your data into a single dataframe with a variable
> designating which group data came from.? Then use that variable as the
> strata variable in the boot function to resample within groups.? So
> something like this:
>
> f1 <- data.frame(gender=gender1,color=color1,group='grp1')
> f2 <- data.frame(gender=gender2,color=color2,group='grp2')
> f3 <- rbind(f1,f2)
>
> cramerdiff <- function(x, ndx) {
>  ?? # calculate cramer.v for group 1 bootstrap sample
>  ?? g1 <-x[ndx,][x[,3]=='grp1',]
>  ?? cramer_g1 <- cramer.v(table(g1[,1:2]))
>  ?? # calculate cramer.v for group 2 bootstrap sample
>  ?? g2 <-x[ndx,][x[,3]=='grp2',]
>  ?? cramer_g2 <- cramer.v(table(g2[,1:2]))
>  ?? # calculate difference
>  ?? cramer_g1-cramer_g2
>  ?? }
> # use strata parameter in function boot to resample within each group
> results <- boot(data=f3,statistic=cramerdiff,
> strata=as.factor(f3$group),R=2000)
>
> results
> boot.ci(results)
>
>
>
> Hope this is helpful,
>
> Dan
>

-- 
Daniel Nordlund
Port Townsend, WA  USA


-- 
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus


From tebert @end|ng |rom u||@edu  Mon Jun  6 02:54:35 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Mon, 6 Jun 2022 00:54:35 +0000
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
Message-ID: <BN6PR2201MB1553A755FBD07E5B894D28B4CFA29@BN6PR2201MB1553.namprd22.prod.outlook.com>

You are welcome to ask here. However, you should try contacting the authors of the gam package. Package authors are often extraordinarily helpful.
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of jade.shodan--- via R-help
Sent: Sunday, June 5, 2022 3:02 PM
To: r-help at r-project.org
Subject: [R] High concurvity/ collinearity between time and temperature in GAM predicting deaths but low ACF. Does this matter?

[External Email]

Hello everyone,

A few days ago I asked a question about concurvity in a GAM (the anologue of collinearity in a GLM) implemented in mgcv. I think my question was a bit unfocussed, so I am retrying again, but with additional information included about the autocorrelation function. I have also posted about this on Cross Validated. Given all the model output, it might make for easier reading:https://urldefense.proofpoint.com/v2/url?u=https-3A__stats.stackexchange.com_questions_577790_high-2Dconcurvity-2Dcollinearity-2Dbetween-2Dtime-2Dand-2Dtemperature-2Din-2Dgam-2Dpredicting-2Ddea&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=UnxUj1p0u7yNcAwVR0Na6FmRBibHxhuhscEcpFA2qRQ&e=

As mentioned previously, I have problems with concurvity in my thesis research, and don't have access to a statistician who works with time series, GAMs or R. I'd be very grateful for any (partial) answer, however short. I'll gladly return the favour where I can! For really helpful input I'd be more than happy to offer co-authorship on publication. Deadlines are very close, and I'm heading towards having no results at all if I can't solve this concurvity issue :(

I'm using GAMs to try to understand the relationship between deaths and heat-related variables (e.g. temperature and humidity), using daily time series over a 14-year period from a tropical, low-income country. My aim is to understand the relationship between these variables and deaths, rather than pure prediction performance.

The GAMs include distributed lag models (set up as 7-column matrices, see code at bottom of post), since deaths may occur over several days following exposure.

Simple GAMs with just time, lagged temperature and lagged precipitation (a potential confounder) show very high concurvity between lagged temperature and time, regardless of the many different ways I have tried to decompose time. The autocorrelation functions
(ACF) however, shows values close to zero, only just about breaching the 'significance line' in a few instances. It does show patterning though, although the regularity is difficult to define.

My questions are:
1) Should I be worried about the high concurvity, or can I ignore it given the mostly non-significant ACF? I've read dozens of heat-mortality modelling studies and none report on concurvity between weather variables and time (though one 2012 paper discussed autocorrelation).

2) If I cannot ignore it, what should I do to resolve it? Would including an autoregressive term be appropriate, and if so, where can I find a coded example of how to do this? I've also come across sequential regression][1]. Would this be more or less appropriate? If appropriate, a pointer to an example would be really appreciated!

Some example GAMs are specified as follows:
```r
conc38b <- gam(deaths~te(year, month, week, weekday,
bs=c("cr","cc","cc","cc")) + heap +
                      te(temp_max, lag, k=c(10, 3)) +
                      te(precip_daily_total, lag, k=c(10, 3)),
                      data = dat, family = nb, method = 'REML', select = TRUE,
                      knots = list(month = c(0.5, 12.5), week = c(0.5, 52.5), weekday = c(0, 6.5))) ``` Concurvity for the above model between (temp_max, lag) and (year, month, week, weekday) is 0.91:

```r
$worst
                                    para te(year,month,week,weekday)
te(temp_max,lag) te(precip_daily_total,lag)
para                        1.000000e+00                1.125625e-29
     0.3150073                  0.6666348
te(year,month,week,weekday) 1.400648e-29                1.000000e+00
     0.9060552                  0.6652313
te(temp_max,lag)            3.152795e-01                8.998113e-01
     1.0000000                  0.5781015
te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
     0.5805159                  1.0000000
```

Output from ```gam.check()```:
```r
Method: REML   Optimizer: outer newton
full convergence after 16 iterations.
Gradient range [-0.01467332,0.003096643] (score 8915.994 & scale 1).
Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
Model rank =  544 / 544

Basis dimension (k) checking results. Low p-value (k-index<1) may indicate that k is too low, especially if edf is close to k'.

                                  k'      edf k-index p-value
te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
te(temp_max,lag)             29.0000   3.3681      NA      NA
te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 ```

Some output from ```summary(conc38b)```:
```r
Approximate significance of smooth terms:
                                  edf Ref.df  Chi.sq p-value
te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
te(temp_max,lag)             3.368129     27  11.130 0.00145 **
te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

R-sq.(adj) =  0.839   Deviance explained = 53.3%
-REML =   8916  Scale est. = 1         n = 5107
```


Below are the ACF plots (note limit y-axis = 0.1 for clarity of pattern). They show peaks at 5 and 15, and then there seems to be a recurring pattern at multiples of approx. 30 (suggesting month is not modelled adequately?). Not sure what would cause the spikes at 5 and 15. There is heaping of deaths on the 15th day of each month, to which deaths with unknown date were allocated. This heaping was modelled with categorical variable/ factor ```heap``` with 169 levels (0 for all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent heaping day over the 14-year period):

  [2]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_FzKyM.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=rhd6ZkNNDyYd1zntgAjnNzZFkYPFica9xzx9ruBHG9g&e=
  [3]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_fE3aL.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=DUqm7oXz2zc3oaDR6ESbWGKZdinIsZf-ULGgDsyIOfM&e=


I get an identical looking ACF when I decompose time into (year, month, monthday) as in model conc39 below, although concurvity between (temp_max, lag) and the time term has now dropped somewhat to 0.83:

```r
conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
                     te(temp_max, lag, k=c(10, 4)) +
                     te(precip_daily_total, lag, k=c(10, 4)),
                     data = dat, family = nb, method = 'REML', select = TRUE,
                     knots = list(month = c(0.5, 12.5))) ``` ```r

Method: REML   Optimizer: outer newton
full convergence after 14 iterations.
Gradient range [-0.001578187,6.155096e-05] (score 8915.763 & scale 1).
Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
Model rank =  323 / 323

Basis dimension (k) checking results. Low p-value (k-index<1) may indicate that k is too low, especially if edf is close to k'.

                                k'     edf k-index p-value
te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
te(temp_max,lag)           39.0000  4.0875      NA      NA
te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 ``` Some output from ```summary(conc39)```:
```r
Approximate significance of smooth terms:
                                edf Ref.df  Chi.sq  p-value
te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
te(precip_daily_total,lag)  0.01173     36   0.008    0.557
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

R-sq.(adj) =  0.839   Deviance explained = 53.8%
-REML =   8915  Scale est. = 1         n = 5107
```


```r
$worst
                                   para te(year,month,monthday)
te(temp_max,lag) te(precip_daily_total,lag)
para                       1.000000e+00            3.261007e-31
0.3313549                  0.6666532
te(year,month,monthday)    3.060763e-31            1.000000e+00
0.8266086                  0.5670777
te(temp_max,lag)           3.331014e-01            8.225942e-01
1.0000000                  0.5840875
te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
0.5939380                  1.0000000
```

Modelling time as ```te(year, doy)``` with a cyclic spline for doy and various choices for k or as ```s(time)``` with various k does not reduce concurvity either.


The default approach in time series studies of heat-mortality is to model time with fixed df, generally between 7-10 df per year of data.
I am, however, apprehensive about this approach because a) mortality profiles vary with locality due to sociodemographic and environmental characteristics and b) the choice of df is based on higher income countries (where nearly all these studies have been done) with different mortality profiles and so may not be appropriate for tropical, low-income countries.

Although the approach of fixing (high) df does remove more temporal patterns from the ACF (see model and output below), concurvity between time and lagged temperature has now risen to 0.99! Moreover, temperature (which has been a consistent, highly significant predictor in every model of the tens (hundreds?) I have run, has now turned non-significant. I am guessing this is because time is now a very wiggly function that not only models/ removes seasonal variation, but also some of the day-to-day variation that is needed for the temperature smooth  :

```r
conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
                      te(temp_max, lag, k=c(10,3)) +
                      te(precip_daily_total, lag, k=c(10,3)),
                      data = dat, family = nb, method = 'REML', select = TRUE) ``` Output from ```gam.check(conc20a, rep = 1000)```:

```r
Method: REML   Optimizer: outer newton
full convergence after 9 iterations.
Gradient range [-0.0008983099,9.546022e-05] (score 8750.13 & scale 1).
Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
Model rank =  336 / 336

Basis dimension (k) checking results. Low p-value (k-index<1) may indicate that k is too low, especially if edf is close to k'.

                                 k'      edf k-index p-value
s(time)                    111.0000 111.0000    0.98    0.56
te(temp_max,lag)            29.0000   0.6548      NA      NA
te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
```
Output from ```concurvity(conc20a, full=FALSE)$worst```:

```r
                                   para      s(time) te(temp_max,lag)
te(precip_daily_total,lag)
para                       1.000000e+00 2.462064e-19        0.3165236
                0.6666348
s(time)                    2.462398e-19 1.000000e+00        0.9930674
                0.6879284
te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
                0.5788711
te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
                1.0000000

```

Some output from ```summary(conc20a)```:
```r
Approximate significance of smooth terms:
                                 edf Ref.df  Chi.sq p-value
s(time)                    1.110e+02    111 419.375  <2e-16 ***
te(temp_max,lag)           6.548e-01     27   0.895   0.249
te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

R-sq.(adj) =  0.843   Deviance explained = 56.1%
-REML = 8750.1  Scale est. = 1         n = 5107
```

ACF functions:

[4]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_7nbXS.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=hXhWH-VySi9i27hNDKK184WiLooYmhdni7_7JOLhRcI&e=
[5]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_pNnZU.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=HV6sMbzNkG-NdJZBjZRAjrvCDl2VtMPfl5rY2Ss8muM&e=

Data can be found on my [GitHub][6] site in the file [data_cross_validated_post2.rds][7]. A csv version is also available.
This is my code:

```r
library(readr)
library(mgcv)

df <- read_rds("data_crossvalidated_post2.rds")

# Create matrices for lagged weather variables (6 day lags) based on example by Simon Wood # in his 2017 book ("Generalized additive models: an introduction with R", p. 349) and # gamair package documentation (https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dproject.org_web_packages_gamair_gamair.pdf&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=RmLMXgEhg76PLgzNq7CbZkD29EGQili4Pkd7ESctrJ0&e= , p. 54)

lagard <- function(x,n.lag=7) {
n <- length(x); X <- matrix(NA,n,n.lag)
for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1] X }

dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month, weekday = df$weekday, week = df$week, monthday = df$monthday, time = df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum = df$precip_hourly_dailysum) dat$temp_max <- lagard(df$temp_max) dat$temp_min <- lagard(df$temp_min) dat$temp_mean <- lagard(df$temp_mean) dat$wbgt_max <- lagard(df$wbgt_max) dat$wbgt_mean <- lagard(df$wbgt_mean) dat$wbgt_min <- lagard(df$wbgt_min) dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max) dat$sh_mean <- lagard(df$sh_mean) dat$solar_mean <- lagard(df$solar_mean) dat$wind2m_mean <- lagard(df$wind2m_mean) dat$sh_max <- lagard(df$sh_max) dat$solar_max <- lagard(df$solar_max) dat$wind2m_max <- lagard(df$wind2m_max) dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean) dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum) dat$precip_hourly <- lagard(df$precip_hourly) dat$precip_daily_total <- lagard( df$precip_daily_total) dat$temp <- lagard(df$temp) dat$sh <- lagard(df$sh) dat$rh <- lagard(df$rh) dat$solar <- lagard(df$solar) dat$wind2m <- lagard(df$wind2m)


conc38b <- gam(deaths~te(year, month, week, weekday,
bs=c("cr","cc","cc","cc")) + heap +
                      te(temp_max, lag, k=c(10, 3)) +
                      te(precip_daily_total, lag, k=c(10, 3)),
                      data = dat, family = nb, method = 'REML', select = TRUE,
                      knots = list(month = c(0.5, 12.5), week = c(0.5, 52.5), weekday = c(0, 6.5)))

conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
                     te(temp_max, lag, k=c(10, 4)) +
                     te(precip_daily_total, lag, k=c(10, 4)),
                     data = dat, family = nb, method = 'REML', select = TRUE,
                     knots = list(month = c(0.5, 12.5)))

conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
                      te(temp_max, lag, k=c(10,3)) +
                      te(precip_daily_total, lag, k=c(10,3)),
                      data = dat, family = nb, method = 'REML', select = TRUE)

```
Thank you if you've read this far!! :-))

  [1]: https://urldefense.proofpoint.com/v2/url?u=https-3A__scholar.google.co.uk_scholar-3Foutput-3Dinstlink-26q-3Dinfo-3APKdjq7ZwozEJ-3Ascholar.google.com_-26hl-3Den-26as-5Fsdt-3D0-2C5-26scillfp-3D17865929886710916120-26oi-3Dlle&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=05YTFJr01J0QkoraKsVufRrfVvevvADTCCCjxskRbfY&e=
  [2]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_FzKyM.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=rhd6ZkNNDyYd1zntgAjnNzZFkYPFica9xzx9ruBHG9g&e=
  [3]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_fE3aL.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=DUqm7oXz2zc3oaDR6ESbWGKZdinIsZf-ULGgDsyIOfM&e=
  [4]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_7nbXS.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=hXhWH-VySi9i27hNDKK184WiLooYmhdni7_7JOLhRcI&e=
  [5]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_pNnZU.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=HV6sMbzNkG-NdJZBjZRAjrvCDl2VtMPfl5rY2Ss8muM&e=
  [6]: https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_JadeShodan_heat-2Dmortality&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=zxKWkmFT-DWpRAx6t0DRbV9ldSbgyIE6V3LdJBm4ULU&e=
  [7]: https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_JadeShodan_heat-2Dmortality_blob_main_data-5Fcross-5Fvalidated-5Fpost2.rds&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=VIm9gkrPHFVXJFZeg6sMnKYGcLD5HR3BKqh4z8iIQjc&e=

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=QP0xRvlj8tppy7hmTxhzrD62Hd1N4mXYmPKW48XiiRg&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=M7FO2Q09m7Or14GXa16BtzX1E8Xa5AWB-eN9ZTGlcjA&e=
and provide commented, minimal, self-contained, reproducible code.

From j@de@shod@@ m@iii@g oii googiem@ii@com  Mon Jun  6 11:40:39 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Mon, 6 Jun 2022 10:40:39 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <BN6PR2201MB1553A755FBD07E5B894D28B4CFA29@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <BN6PR2201MB1553A755FBD07E5B894D28B4CFA29@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <CANg3_k9POxE3n982OjiUy9gVh7MbvKXFWWOiJVRQaUxta-EKdA@mail.gmail.com>

Hi Tim,

Thanks for the suggestion. I think the author of the mgcv package
(Simon Wood) is/ was (?) on this list, so I thought I'd give it a try
here first. Plus I already emailed him recently about a different
issue, so I didn't want to come on too strongly by emailing him again
so shortly after. But I will try that if no luck here. Thanks!

On Mon, 6 Jun 2022 at 01:54, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>
> You are welcome to ask here. However, you should try contacting the authors of the gam package. Package authors are often extraordinarily helpful.
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of jade.shodan--- via R-help
> Sent: Sunday, June 5, 2022 3:02 PM
> To: r-help at r-project.org
> Subject: [R] High concurvity/ collinearity between time and temperature in GAM predicting deaths but low ACF. Does this matter?
>
> [External Email]
>
> Hello everyone,
>
> A few days ago I asked a question about concurvity in a GAM (the anologue of collinearity in a GLM) implemented in mgcv. I think my question was a bit unfocussed, so I am retrying again, but with additional information included about the autocorrelation function. I have also posted about this on Cross Validated. Given all the model output, it might make for easier reading:https://urldefense.proofpoint.com/v2/url?u=https-3A__stats.stackexchange.com_questions_577790_high-2Dconcurvity-2Dcollinearity-2Dbetween-2Dtime-2Dand-2Dtemperature-2Din-2Dgam-2Dpredicting-2Ddea&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=UnxUj1p0u7yNcAwVR0Na6FmRBibHxhuhscEcpFA2qRQ&e=
>
> As mentioned previously, I have problems with concurvity in my thesis research, and don't have access to a statistician who works with time series, GAMs or R. I'd be very grateful for any (partial) answer, however short. I'll gladly return the favour where I can! For really helpful input I'd be more than happy to offer co-authorship on publication. Deadlines are very close, and I'm heading towards having no results at all if I can't solve this concurvity issue :(
>
> I'm using GAMs to try to understand the relationship between deaths and heat-related variables (e.g. temperature and humidity), using daily time series over a 14-year period from a tropical, low-income country. My aim is to understand the relationship between these variables and deaths, rather than pure prediction performance.
>
> The GAMs include distributed lag models (set up as 7-column matrices, see code at bottom of post), since deaths may occur over several days following exposure.
>
> Simple GAMs with just time, lagged temperature and lagged precipitation (a potential confounder) show very high concurvity between lagged temperature and time, regardless of the many different ways I have tried to decompose time. The autocorrelation functions
> (ACF) however, shows values close to zero, only just about breaching the 'significance line' in a few instances. It does show patterning though, although the regularity is difficult to define.
>
> My questions are:
> 1) Should I be worried about the high concurvity, or can I ignore it given the mostly non-significant ACF? I've read dozens of heat-mortality modelling studies and none report on concurvity between weather variables and time (though one 2012 paper discussed autocorrelation).
>
> 2) If I cannot ignore it, what should I do to resolve it? Would including an autoregressive term be appropriate, and if so, where can I find a coded example of how to do this? I've also come across sequential regression][1]. Would this be more or less appropriate? If appropriate, a pointer to an example would be really appreciated!
>
> Some example GAMs are specified as follows:
> ```r
> conc38b <- gam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc")) + heap +
>                       te(temp_max, lag, k=c(10, 3)) +
>                       te(precip_daily_total, lag, k=c(10, 3)),
>                       data = dat, family = nb, method = 'REML', select = TRUE,
>                       knots = list(month = c(0.5, 12.5), week = c(0.5, 52.5), weekday = c(0, 6.5))) ``` Concurvity for the above model between (temp_max, lag) and (year, month, week, weekday) is 0.91:
>
> ```r
> $worst
>                                     para te(year,month,week,weekday)
> te(temp_max,lag) te(precip_daily_total,lag)
> para                        1.000000e+00                1.125625e-29
>      0.3150073                  0.6666348
> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
>      0.9060552                  0.6652313
> te(temp_max,lag)            3.152795e-01                8.998113e-01
>      1.0000000                  0.5781015
> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
>      0.5805159                  1.0000000
> ```
>
> Output from ```gam.check()```:
> ```r
> Method: REML   Optimizer: outer newton
> full convergence after 16 iterations.
> Gradient range [-0.01467332,0.003096643] (score 8915.994 & scale 1).
> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> Model rank =  544 / 544
>
> Basis dimension (k) checking results. Low p-value (k-index<1) may indicate that k is too low, especially if edf is close to k'.
>
>                                   k'      edf k-index p-value
> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> te(temp_max,lag)             29.0000   3.3681      NA      NA
> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 ```
>
> Some output from ```summary(conc38b)```:
> ```r
> Approximate significance of smooth terms:
>                                   edf Ref.df  Chi.sq p-value
> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> -REML =   8916  Scale est. = 1         n = 5107
> ```
>
>
> Below are the ACF plots (note limit y-axis = 0.1 for clarity of pattern). They show peaks at 5 and 15, and then there seems to be a recurring pattern at multiples of approx. 30 (suggesting month is not modelled adequately?). Not sure what would cause the spikes at 5 and 15. There is heaping of deaths on the 15th day of each month, to which deaths with unknown date were allocated. This heaping was modelled with categorical variable/ factor ```heap``` with 169 levels (0 for all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent heaping day over the 14-year period):
>
>   [2]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_FzKyM.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=rhd6ZkNNDyYd1zntgAjnNzZFkYPFica9xzx9ruBHG9g&e=
>   [3]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_fE3aL.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=DUqm7oXz2zc3oaDR6ESbWGKZdinIsZf-ULGgDsyIOfM&e=
>
>
> I get an identical looking ACF when I decompose time into (year, month, monthday) as in model conc39 below, although concurvity between (temp_max, lag) and the time term has now dropped somewhat to 0.83:
>
> ```r
> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>                      te(temp_max, lag, k=c(10, 4)) +
>                      te(precip_daily_total, lag, k=c(10, 4)),
>                      data = dat, family = nb, method = 'REML', select = TRUE,
>                      knots = list(month = c(0.5, 12.5))) ``` ```r
>
> Method: REML   Optimizer: outer newton
> full convergence after 14 iterations.
> Gradient range [-0.001578187,6.155096e-05] (score 8915.763 & scale 1).
> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> Model rank =  323 / 323
>
> Basis dimension (k) checking results. Low p-value (k-index<1) may indicate that k is too low, especially if edf is close to k'.
>
>                                 k'     edf k-index p-value
> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> te(temp_max,lag)           39.0000  4.0875      NA      NA
> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 ``` Some output from ```summary(conc39)```:
> ```r
> Approximate significance of smooth terms:
>                                 edf Ref.df  Chi.sq  p-value
> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> -REML =   8915  Scale est. = 1         n = 5107
> ```
>
>
> ```r
> $worst
>                                    para te(year,month,monthday)
> te(temp_max,lag) te(precip_daily_total,lag)
> para                       1.000000e+00            3.261007e-31
> 0.3313549                  0.6666532
> te(year,month,monthday)    3.060763e-31            1.000000e+00
> 0.8266086                  0.5670777
> te(temp_max,lag)           3.331014e-01            8.225942e-01
> 1.0000000                  0.5840875
> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> 0.5939380                  1.0000000
> ```
>
> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and various choices for k or as ```s(time)``` with various k does not reduce concurvity either.
>
>
> The default approach in time series studies of heat-mortality is to model time with fixed df, generally between 7-10 df per year of data.
> I am, however, apprehensive about this approach because a) mortality profiles vary with locality due to sociodemographic and environmental characteristics and b) the choice of df is based on higher income countries (where nearly all these studies have been done) with different mortality profiles and so may not be appropriate for tropical, low-income countries.
>
> Although the approach of fixing (high) df does remove more temporal patterns from the ACF (see model and output below), concurvity between time and lagged temperature has now risen to 0.99! Moreover, temperature (which has been a consistent, highly significant predictor in every model of the tens (hundreds?) I have run, has now turned non-significant. I am guessing this is because time is now a very wiggly function that not only models/ removes seasonal variation, but also some of the day-to-day variation that is needed for the temperature smooth  :
>
> ```r
> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>                       te(temp_max, lag, k=c(10,3)) +
>                       te(precip_daily_total, lag, k=c(10,3)),
>                       data = dat, family = nb, method = 'REML', select = TRUE) ``` Output from ```gam.check(conc20a, rep = 1000)```:
>
> ```r
> Method: REML   Optimizer: outer newton
> full convergence after 9 iterations.
> Gradient range [-0.0008983099,9.546022e-05] (score 8750.13 & scale 1).
> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> Model rank =  336 / 336
>
> Basis dimension (k) checking results. Low p-value (k-index<1) may indicate that k is too low, especially if edf is close to k'.
>
>                                  k'      edf k-index p-value
> s(time)                    111.0000 111.0000    0.98    0.56
> te(temp_max,lag)            29.0000   0.6548      NA      NA
> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> ```
> Output from ```concurvity(conc20a, full=FALSE)$worst```:
>
> ```r
>                                    para      s(time) te(temp_max,lag)
> te(precip_daily_total,lag)
> para                       1.000000e+00 2.462064e-19        0.3165236
>                 0.6666348
> s(time)                    2.462398e-19 1.000000e+00        0.9930674
>                 0.6879284
> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
>                 0.5788711
> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
>                 1.0000000
>
> ```
>
> Some output from ```summary(conc20a)```:
> ```r
> Approximate significance of smooth terms:
>                                  edf Ref.df  Chi.sq p-value
> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> -REML = 8750.1  Scale est. = 1         n = 5107
> ```
>
> ACF functions:
>
> [4]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_7nbXS.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=hXhWH-VySi9i27hNDKK184WiLooYmhdni7_7JOLhRcI&e=
> [5]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_pNnZU.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=HV6sMbzNkG-NdJZBjZRAjrvCDl2VtMPfl5rY2Ss8muM&e=
>
> Data can be found on my [GitHub][6] site in the file [data_cross_validated_post2.rds][7]. A csv version is also available.
> This is my code:
>
> ```r
> library(readr)
> library(mgcv)
>
> df <- read_rds("data_crossvalidated_post2.rds")
>
> # Create matrices for lagged weather variables (6 day lags) based on example by Simon Wood # in his 2017 book ("Generalized additive models: an introduction with R", p. 349) and # gamair package documentation (https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dproject.org_web_packages_gamair_gamair.pdf&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=RmLMXgEhg76PLgzNq7CbZkD29EGQili4Pkd7ESctrJ0&e= , p. 54)
>
> lagard <- function(x,n.lag=7) {
> n <- length(x); X <- matrix(NA,n,n.lag)
> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1] X }
>
> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month, weekday = df$weekday, week = df$week, monthday = df$monthday, time = df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum = df$precip_hourly_dailysum) dat$temp_max <- lagard(df$temp_max) dat$temp_min <- lagard(df$temp_min) dat$temp_mean <- lagard(df$temp_mean) dat$wbgt_max <- lagard(df$wbgt_max) dat$wbgt_mean <- lagard(df$wbgt_mean) dat$wbgt_min <- lagard(df$wbgt_min) dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max) dat$sh_mean <- lagard(df$sh_mean) dat$solar_mean <- lagard(df$solar_mean) dat$wind2m_mean <- lagard(df$wind2m_mean) dat$sh_max <- lagard(df$sh_max) dat$solar_max <- lagard(df$solar_max) dat$wind2m_max <- lagard(df$wind2m_max) dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean) dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum) dat$precip_hourly <- lagard(df$precip_hourly) dat$precip_daily_total <- lagard( df$precip_daily_total) dat$temp <- lagard(df$temp) dat$sh <- lagard(df$sh) dat$rh <- lagard(df$rh) dat$solar <- lagard(df$solar) dat$wind2m <- lagard(df$wind2m)
>
>
> conc38b <- gam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc")) + heap +
>                       te(temp_max, lag, k=c(10, 3)) +
>                       te(precip_daily_total, lag, k=c(10, 3)),
>                       data = dat, family = nb, method = 'REML', select = TRUE,
>                       knots = list(month = c(0.5, 12.5), week = c(0.5, 52.5), weekday = c(0, 6.5)))
>
> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>                      te(temp_max, lag, k=c(10, 4)) +
>                      te(precip_daily_total, lag, k=c(10, 4)),
>                      data = dat, family = nb, method = 'REML', select = TRUE,
>                      knots = list(month = c(0.5, 12.5)))
>
> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>                       te(temp_max, lag, k=c(10,3)) +
>                       te(precip_daily_total, lag, k=c(10,3)),
>                       data = dat, family = nb, method = 'REML', select = TRUE)
>
> ```
> Thank you if you've read this far!! :-))
>
>   [1]: https://urldefense.proofpoint.com/v2/url?u=https-3A__scholar.google.co.uk_scholar-3Foutput-3Dinstlink-26q-3Dinfo-3APKdjq7ZwozEJ-3Ascholar.google.com_-26hl-3Den-26as-5Fsdt-3D0-2C5-26scillfp-3D17865929886710916120-26oi-3Dlle&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=05YTFJr01J0QkoraKsVufRrfVvevvADTCCCjxskRbfY&e=
>   [2]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_FzKyM.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=rhd6ZkNNDyYd1zntgAjnNzZFkYPFica9xzx9ruBHG9g&e=
>   [3]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_fE3aL.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=DUqm7oXz2zc3oaDR6ESbWGKZdinIsZf-ULGgDsyIOfM&e=
>   [4]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_7nbXS.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=hXhWH-VySi9i27hNDKK184WiLooYmhdni7_7JOLhRcI&e=
>   [5]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_pNnZU.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=HV6sMbzNkG-NdJZBjZRAjrvCDl2VtMPfl5rY2Ss8muM&e=
>   [6]: https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_JadeShodan_heat-2Dmortality&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=zxKWkmFT-DWpRAx6t0DRbV9ldSbgyIE6V3LdJBm4ULU&e=
>   [7]: https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_JadeShodan_heat-2Dmortality_blob_main_data-5Fcross-5Fvalidated-5Fpost2.rds&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=VIm9gkrPHFVXJFZeg6sMnKYGcLD5HR3BKqh4z8iIQjc&e=
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=QP0xRvlj8tppy7hmTxhzrD62Hd1N4mXYmPKW48XiiRg&e=
> PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=M7FO2Q09m7Or14GXa16BtzX1E8Xa5AWB-eN9ZTGlcjA&e=
> and provide commented, minimal, self-contained, reproducible code.


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Sun Jun  5 11:52:20 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Sun, 5 Jun 2022 14:52:20 +0500
Subject: [R] A humble request
Message-ID: <CAMfKi3KBxpQTdP=EX6vF5hcfM76MbhbdR_D6G-8Ly3myve1vOw@mail.gmail.com>

*Hi, Dear Respected Professors! I hope that you are doing well. *
Kindly help me regarding the following:
When I run the following code in R
d1=as.data.frame(mra(valq, wf = "la8", J = 8, method = "modwt", boundary =
"periodic"))
I face the following error
Error in modwt(x, wf, J, "periodic") : wavelet transform exceeds sample
size in modwt

Kindly help me to fix it.

Regards
Muhammad Zubair Chishti

	[[alternative HTML version deleted]]


From john@@t@p|e@ @end|ng |rom me@com  Mon Jun  6 15:05:29 2022
From: john@@t@p|e@ @end|ng |rom me@com (John RF Staples)
Date: Mon, 6 Jun 2022 14:05:29 +0100
Subject: [R] Build a data.table via two string lists
Message-ID: <6847A6BF-AEBA-47D0-9F78-1680A3F2BFA0@me.com>

Hello everyone,

I have the following two structures:

# str(cl)
# chr [1:5] "timestamp" "assess" "catheter" "service" "ref"

# str(rw)
# chr [1:5] "1654508112" "3" "gel" "2785" ?16545081121a8d8f956cb871053a3f56b782b70a76"

From which I would like to generate a 5 col x 1 row data.table; the question is ?how??

?cl? represents the column names; ?rw? the row values.

With anticipatory thanks ?
John

From w||||@mwdun|@p @end|ng |rom gm@||@com  Mon Jun  6 19:08:24 2022
From: w||||@mwdun|@p @end|ng |rom gm@||@com (Bill Dunlap)
Date: Mon, 6 Jun 2022 10:08:24 -0700
Subject: [R] Build a data.table via two string lists
In-Reply-To: <6847A6BF-AEBA-47D0-9F78-1680A3F2BFA0@me.com>
References: <6847A6BF-AEBA-47D0-9F78-1680A3F2BFA0@me.com>
Message-ID: <CAHqSRuQOgKDBjstV3RNhNycbYOsA9V7iJREKFFSYyPZOZ8Z3Qg@mail.gmail.com>

You could turn your row values in a list, attach names to that list, and
pass the list to data.frame():
> cl <- c("timestamp", "assess", "catheter", "service", "ref")
> rw <- c("1654508112", "3", "gel", "2785",
"16545081121a8d8f956cb871053a3f56b782b70a76")
> df <- data.frame(setNames(as.list(rw), nm=cl))
> df
   timestamp assess catheter service
 ref
1 1654508112      3      gel    2785
16545081121a8d8f956cb871053a3f56b782b70a76

At this point all the columns have type "character".  You may want to
convert some columns to more meaningful types, e.g., "timestamp" to a
date/time class and "assess" to a number.  E.g.,
> df[["timestamp"]] <- as.POSIXct(as.numeric(df[["timestamp"]]),
origin=as.POSIXct("1970/01/01"))
> df[["assess"]] <- as.numeric(df[["assess"]])
> df
            timestamp assess catheter service
         ref
1 2022-06-06 10:35:12      3      gel    2785
16545081121a8d8f956cb871053a3f56b782b70a76
> str(df)
'data.frame':   1 obs. of  5 variables:
 $ timestamp: POSIXct, format: "2022-06-06 10:35:12"
 $ assess   : num 3
 $ catheter : chr "gel"
 $ service  : chr "2785"
 $ ref      : chr "16545081121a8d8f956cb871053a3f56b782b70a76"

-Bill


On Mon, Jun 6, 2022 at 9:51 AM John RF Staples via R-help <
r-help at r-project.org> wrote:

> Hello everyone,
>
> I have the following two structures:
>
> # str(cl)
> # chr [1:5] "timestamp" "assess" "catheter" "service" "ref"
>
> # str(rw)
> # chr [1:5] "1654508112" "3" "gel" "2785"
> ?16545081121a8d8f956cb871053a3f56b782b70a76"
>
> From which I would like to generate a 5 col x 1 row data.table; the
> question is ?how??
>
> ?cl? represents the column names; ?rw? the row values.
>
> With anticipatory thanks ?
> John
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon Jun  6 21:17:21 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 6 Jun 2022 20:17:21 +0100
Subject: [R] bootstrap CI of the difference between 2 Cramer's V
In-Reply-To: <317595679.11661946.1654446068116@mail.yahoo.com>
References: <2109899925.6516974.1653762082722.ref@mail.yahoo.com>
 <2109899925.6516974.1653762082722@mail.yahoo.com>
 <a5972539-71a0-864e-4843-5144698ac60b@gmail.com>
 <317595679.11661946.1654446068116@mail.yahoo.com>
Message-ID: <0e2dd5f6-c0e8-5b72-37f4-4115794319f8@sapo.pt>

Hello,

Here is your code, corrected. It uses a Goodman-Kruskal gamma function 
in package DescTools.
Function G probably doesn't need tryCatch but I had errors in some test 
runs.


library(boot)

G <- function(x, index, cols = 1:2) {
   y <- x[index, cols]    # bootstrapped data
   g <- x$group[index]    # and groups
   # calculate gamma for each group bootstrap sample
   # (trap errors in case one of the groups is empty)
   g <- tryCatch(
     lapply(split(y[1:2], g), \(x) {
       tbl <- table(x)
       DescTools::GoodmanKruskalGamma(tbl)
     }),
     error = function(e) list(NA_real_, NA_real_)
   )
   # calculate difference
   g[[1]] - g[[2]]
}

set.seed(2022)
# use strata parameter in function boot to resample within each group
results <- boot(
   data = f3,
   statistic = G,
   R = 2000,
   strata = as.factor(f3$group),
   cols = 1:2
)

results
boot.ci(results)


Hope this helps,

Rui Barradas


?s 17:21 de 05/06/2022, varin sacha via R-help escreveu:
> Dear Daniel,
> Dear R-experts,
> 
> I really thank you a lot Daniel. Nobody had answered to me offline. So, thanks.
> I have tried in the same vein for the Goodman-Kruskal gamma for ordinal data. There is an error message at the end of the code. Thanks for your help.
> 
> 
> ##############################
> library(ryouready)
> library(boot)
> 
> shopping1<-c("tr?s important","important","pas important","pas important","important","tr?s important","important","pas important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","important")
> 
> statut1<-c("riche","pas riche","moyennement riche","moyennement riche","riche","pas riche","moyennement riche","moyennement riche","riche","pas riche","moyennement riche","riche","pas riche","pas riche","riche","moyennement riche","riche","pas riche","pas riche","pas riche","riche","riche","moyennement riche","riche","riche","moyennement riche","moyennement riche","moyennement riche","pas riche","pas riche","riche","pas riche","riche","pas riche","riche","moyennement riche","riche","pas riche","moyennement riche","riche")
> 
> shopping2<-c("important","pas important","tr?s important","tr?s important","important","tr?s important","pas important","important","pas important","tr?s important","important","important","important","important","pas important","tr?s important","tr?s important","important","pas important","tr?s important","pas important","tr?s important","pas important","tr?s important","important","tr?s important","important","pas important","pas important","important","pas important","tr?s important","pas important","pas important","important","important","tr?s important","tr?s important","pas important","pas important")
> 
> statut2<-c("moyennement riche","pas riche","riche","moyennement riche","moyennement riche","moyennement riche","pas riche","riche","riche","pas riche","moyennement riche","riche","riche","riche","riche","riche","pas riche","moyennement riche","moyennement riche","pas riche","moyennement riche","pas riche","pas riche","pas riche","moyennement riche","riche","moyennement riche","riche","pas riche","riche","moyennement riche","blue","moyennement riche","pas riche","pas riche","riche","riche","pas riche","pas riche","pas riche")
> 
> f1 <- data.frame(shopping=shopping1,statut=statut1,group='grp1')
> f2 <- data.frame(shopping=shopping2,statut=statut2,group='grp2')
> f3 <- rbind(f1,f2)
> 
> G <- function(x, index) {
>     
> # calculate goodman for group 1 bootstrap sample
>  ?? g1 <-x[index,][x[,3]=='grp1',]
>  ?? goodman_g1 <- cor(data[index,][1,2])
>    
>  ?# calculate goodman for group 2 bootstrap sample
>  ?? g2 <-x[index,][x[,3]=='grp2',]
>  ?? goodman_g2 <- cor(data[index,][3,4])
>    
>  ?# calculate difference
>  ?? goodman_g1-goodman_g2
>  ?? }
>   
> 
> # use strata parameter in function boot to resample within each group
> results <- boot(data=f3,statistic=G, strata=as.factor(f3$group),R=2000)
> 
> results
> boot.ci(results)
> ##############################
> 
> 
> 
> Le samedi 4 juin 2022 ? 09:31:36 UTC+2, Daniel Nordlund <djnordlund at gmail.com> a ?crit :
> 
> 
> 
> 
> 
> On 5/28/2022 11:21 AM, varin sacha via R-help wrote:
>> Dear R-experts,
>>
>> While comparing groups, it is better to assess confidence intervals of those differences rather than comparing confidence intervals for each group.
>> I am trying to calculate the CIs of the difference between the two Cramer's V and not the CI to the estimate of each group?s Cramer's V.
>>
>> Here below my toy R example. There are error messages. Any help would be highly appreciated.
>>
>> ##############################
>> library(questionr)
>> library(boot)
>>
>> gender1<-c("M","F","F","F","M","M","F","F","F","M","M","F","M","M","F","M","M","F","M","F","F","F","M","M","M","F","F","M","M","M","F","M","F","F","F","M","M","F","M","F")
>> color1<-c("blue","green","black","black","green","green","blue","blue","green","black","blue","green","blue","black","black","blue","green","blue","green","black","blue","blue","black","black","green","green","blue","green","black","green","blue","black","black","blue","green","green","green","blue","blue","black")
>>
>> gender2<-c("F","F","F","M","M","F","M","M","M","F","F","M","F","M","F","F","M","M","M","F","M","M","M","F","F","F","M","M","M","F","M","M","M","F","F","F","M","F","F","F")
>> color2<-c("green","blue","black","blue","blue","blue","green","blue","green","black","blue","black","blue","blue","black","blue","blue","green","blue","black","blue","blue","black","black","green","blue","black","green","blue","green","black","blue","black","blue","green","blue","green","green","blue","black")
>>
>> f1=data.frame(gender1,color1)
>> tab1<-table(gender1,color1)
>> e1<-cramer.v(tab1)
>>
>> f2=data.frame(gender2,color2)
>> tab2<-table(gender2,color2)
>> e2<-cramer.v(tab2)
>>
>> f3<-data.frame(e1-e2)
>>
>> cramerdiff=function(x,w){
>> y<-tapply(x[w,1], x[w,2],cramer.v)
>> y[1]-y[2]
>> }
>>
>> results<-boot(data=f3,statistic=cramerdiff,R=2000)
>> results
>>
>> boot.ci(results,type="all")
>> ##############################
>>
>>    
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> I don't know if someone responded offline, but if not, there are a
> couple of problems with your code. ? First, the f3 dataframe is not what
> you think it is.? Second, your cramerdiff function isn't going to
> produce the results that you want.
> 
> I would put your data into a single dataframe with a variable
> designating which group data came from.? Then use that variable as the
> strata variable in the boot function to resample within groups.? So
> something like this:
> 
> f1 <- data.frame(gender=gender1,color=color1,group='grp1')
> f2 <- data.frame(gender=gender2,color=color2,group='grp2')
> f3 <- rbind(f1,f2)
> 
> cramerdiff <- function(x, ndx) {
>  ?? # calculate cramer.v for group 1 bootstrap sample
>  ?? g1 <-x[ndx,][x[,3]=='grp1',]
>  ?? cramer_g1 <- cramer.v(table(g1[,1:2]))
>  ?? # calculate cramer.v for group 2 bootstrap sample
>  ?? g2 <-x[ndx,][x[,3]=='grp2',]
>  ?? cramer_g2 <- cramer.v(table(g2[,1:2]))
>  ?? # calculate difference
>  ?? cramer_g1-cramer_g2
>  ?? }
> # use strata parameter in function boot to resample within each group
> results <- boot(data=f3,statistic=cramerdiff,
> strata=as.factor(f3$group),R=2000)
> 
> results
> boot.ci(results)
> 
> 
> 
> Hope this is helpful,
> 
> Dan
>


From drj|m|emon @end|ng |rom gm@||@com  Mon Jun  6 23:27:00 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Tue, 7 Jun 2022 07:27:00 +1000
Subject: [R] A humble request
In-Reply-To: <CAMfKi3KBxpQTdP=EX6vF5hcfM76MbhbdR_D6G-8Ly3myve1vOw@mail.gmail.com>
References: <CAMfKi3KBxpQTdP=EX6vF5hcfM76MbhbdR_D6G-8Ly3myve1vOw@mail.gmail.com>
Message-ID: <CA+8X3fW1x77nPM9XssTYjGK8=j59tyGCmvceptpm7Xvu1W7myQ@mail.gmail.com>

Hi Muhammad,
You may be using the "mra" function in the "waveslim" package. At a
wild guess, if "valq" is a vector or time series, it is too short to
use J=8. Try smaller values for J to see if this is the problem.
Otherwise ask your teacher.

Jim

On Tue, Jun 7, 2022 at 2:50 AM Muhammad Zubair Chishti
<mzchishti at eco.qau.edu.pk> wrote:
>
> *Hi, Dear Respected Professors! I hope that you are doing well. *
> Kindly help me regarding the following:
> When I run the following code in R
> d1=as.data.frame(mra(valq, wf = "la8", J = 8, method = "modwt", boundary =
> "periodic"))
> I face the following error
> Error in modwt(x, wf, J, "periodic") : wavelet transform exceeds sample
> size in modwt
>
> Kindly help me to fix it.
>
> Regards
> Muhammad Zubair Chishti
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tebert @end|ng |rom u||@edu  Tue Jun  7 02:22:31 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 7 Jun 2022 00:22:31 +0000
Subject: [R] Build a data.table via two string lists
In-Reply-To: <CAHqSRuQOgKDBjstV3RNhNycbYOsA9V7iJREKFFSYyPZOZ8Z3Qg@mail.gmail.com>
References: <6847A6BF-AEBA-47D0-9F78-1680A3F2BFA0@me.com>
 <CAHqSRuQOgKDBjstV3RNhNycbYOsA9V7iJREKFFSYyPZOZ8Z3Qg@mail.gmail.com>
Message-ID: <BN6PR2201MB1553D19D2BDD95F3E9D8D54BCFA59@BN6PR2201MB1553.namprd22.prod.outlook.com>

Maybe I don't quite understand, but to get a 1 row data table the cl has to be column names.

colnames(df) <- cl


Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Bill Dunlap
Sent: Monday, June 6, 2022 1:08 PM
To: John RF Staples <john.staples at me.com>
Cc: r-help at R-project.org
Subject: Re: [R] Build a data.table via two string lists

[External Email]

You could turn your row values in a list, attach names to that list, and pass the list to data.frame():
> cl <- c("timestamp", "assess", "catheter", "service", "ref") rw <- 
> c("1654508112", "3", "gel", "2785",
"16545081121a8d8f956cb871053a3f56b782b70a76")
> df <- data.frame(setNames(as.list(rw), nm=cl)) df
   timestamp assess catheter service
 ref
1 1654508112      3      gel    2785
16545081121a8d8f956cb871053a3f56b782b70a76

At this point all the columns have type "character".  You may want to convert some columns to more meaningful types, e.g., "timestamp" to a date/time class and "assess" to a number.  E.g.,
> df[["timestamp"]] <- as.POSIXct(as.numeric(df[["timestamp"]]),
origin=as.POSIXct("1970/01/01"))
> df[["assess"]] <- as.numeric(df[["assess"]]) df
            timestamp assess catheter service
         ref
1 2022-06-06 10:35:12      3      gel    2785
16545081121a8d8f956cb871053a3f56b782b70a76
> str(df)
'data.frame':   1 obs. of  5 variables:
 $ timestamp: POSIXct, format: "2022-06-06 10:35:12"
 $ assess   : num 3
 $ catheter : chr "gel"
 $ service  : chr "2785"
 $ ref      : chr "16545081121a8d8f956cb871053a3f56b782b70a76"

-Bill


On Mon, Jun 6, 2022 at 9:51 AM John RF Staples via R-help < r-help at r-project.org> wrote:

> Hello everyone,
>
> I have the following two structures:
>
> # str(cl)
> # chr [1:5] "timestamp" "assess" "catheter" "service" "ref"
>
> # str(rw)
> # chr [1:5] "1654508112" "3" "gel" "2785"
> ?16545081121a8d8f956cb871053a3f56b782b70a76"
>
> From which I would like to generate a 5 col x 1 row data.table; the 
> question is ?how??
>
> ?cl? represents the column names; ?rw? the row values.
>
> With anticipatory thanks ?
> John
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=K1a_rA-Z0ef6qHvn7vTcCIiwqM3-TiLHpEkHxPn9eW2bhsutM9gsG6l7Z
> IygZrrc&s=Kv3c5xefTDI4vqOIrrz8s6kS-9Yt30ttEFpzh6uMVq8&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=K1a_rA-Z0ef6qHvn7vTcCIiwqM3-TiLHpEkHxPn9eW2bhsutM9gsG6l7
> ZIygZrrc&s=iXx9A0KYXW8fUfuPHkg2NpKITg-FGyQuYTU5vSA4zOo&e=
> and provide commented, minimal, self-contained, reproducible code.
>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=K1a_rA-Z0ef6qHvn7vTcCIiwqM3-TiLHpEkHxPn9eW2bhsutM9gsG6l7ZIygZrrc&s=Kv3c5xefTDI4vqOIrrz8s6kS-9Yt30ttEFpzh6uMVq8&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=K1a_rA-Z0ef6qHvn7vTcCIiwqM3-TiLHpEkHxPn9eW2bhsutM9gsG6l7ZIygZrrc&s=iXx9A0KYXW8fUfuPHkg2NpKITg-FGyQuYTU5vSA4zOo&e=
and provide commented, minimal, self-contained, reproducible code.

From john@@t@p|e@ @end|ng |rom |c|oud@com  Mon Jun  6 20:53:15 2022
From: john@@t@p|e@ @end|ng |rom |c|oud@com (John RF Staples)
Date: Mon, 6 Jun 2022 19:53:15 +0100
Subject: [R] Build a data.table via two string lists
In-Reply-To: <6847A6BF-AEBA-47D0-9F78-1680A3F2BFA0@me.com>
References: <6847A6BF-AEBA-47D0-9F78-1680A3F2BFA0@me.com>
Message-ID: <2AC30BF4-A9A2-4A9E-B50D-5909CAD85DE5@icloud.com>

Hi, thanks for the comprehensive reply; it is appreciated.
John Staples

Sent from my iPad

> On 6 Jun 2022, at 14:05, John RF Staples <john.staples at me.com> wrote:
> 
> ?Hello everyone,
> 
> I have the following two structures:
> 
> # str(cl)
> # chr [1:5] "timestamp" "assess" "catheter" "service" "ref"
> 
> # str(rw)
> # chr [1:5] "1654508112" "3" "gel" "2785" ?16545081121a8d8f956cb871053a3f56b782b70a76"
> 
> From which I would like to generate a 5 col x 1 row data.table; the question is ?how??
> 
> ?cl? represents the column names; ?rw? the row values.
> 
> With anticipatory thanks ?
> John


From er|nm@hodge@@ @end|ng |rom gm@||@com  Tue Jun  7 21:38:43 2022
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Tue, 7 Jun 2022 15:38:43 -0400
Subject: [R] Mixing plotting symbols with text
Message-ID: <CACxE24=pgrsjNS2hq8U5pxdz6wSfh0+Ppn2y=dH32nG0znQgNg@mail.gmail.com>

Hello!

Hope you?re having a great day!

I would like to combine plotting symbols with text.

I have tried
xp2a <- expression(paste(pch =3, ?my stuff?))
But when I use that as a plot title, it just shows as ?3 my stuff?.

I have a feeling that it?s going to be something very straightforward that
I am missing.

Thanks for your help,
Sincerely,
Erin
-- 
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Tue Jun  7 22:08:38 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 7 Jun 2022 20:08:38 +0000
Subject: [R] Mixing plotting symbols with text
In-Reply-To: <CACxE24=pgrsjNS2hq8U5pxdz6wSfh0+Ppn2y=dH32nG0znQgNg@mail.gmail.com>
References: <CACxE24=pgrsjNS2hq8U5pxdz6wSfh0+Ppn2y=dH32nG0znQgNg@mail.gmail.com>
Message-ID: <BN6PR2201MB15536FDF31C74EDF7BBF7D77CFA59@BN6PR2201MB1553.namprd22.prod.outlook.com>

What is the result that you want?
If you cannot print the symbol that you want in a text format for this email, can you give us the Unicode?
You can find that here: https://en.wikipedia.org/wiki/List_of_Unicode_characters
But you have to scroll down a bit to get the list.


Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Erin Hodgess
Sent: Tuesday, June 7, 2022 3:39 PM
To: r-help at r-project.org
Subject: [R] Mixing plotting symbols with text

[External Email]

Hello!

Hope you?re having a great day!

I would like to combine plotting symbols with text.

I have tried
xp2a <- expression(paste(pch =3, ?my stuff?)) But when I use that as a plot title, it just shows as ?3 my stuff?.

I have a feeling that it?s going to be something very straightforward that I am missing.

Thanks for your help,
Sincerely,
Erin
--
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=Ca24Bry9icNvM5nMbklTt79BL8XEZwCr23gdqmPhbctGT3LAmDHnbSQVZbTQzIsJ&s=6B6Z3cxtD3WY5Aj9Pn0JncHo45ujheBej5uQytJmwig&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=Ca24Bry9icNvM5nMbklTt79BL8XEZwCr23gdqmPhbctGT3LAmDHnbSQVZbTQzIsJ&s=nxJBNhFueZylG4iHMRAjipCAASj2UFNzde6qi7tM7Bk&e=
and provide commented, minimal, self-contained, reproducible code.

From er|nm@hodge@@ @end|ng |rom gm@||@com  Tue Jun  7 22:14:22 2022
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Tue, 7 Jun 2022 16:14:22 -0400
Subject: [R] Mixing plotting symbols with text
In-Reply-To: <CAGxFJbQwaLUKe58D3LRBMRUDUBaoGnU45P75sdVXfyawZ=mYUA@mail.gmail.com>
References: <CACxE24=pgrsjNS2hq8U5pxdz6wSfh0+Ppn2y=dH32nG0znQgNg@mail.gmail.com>
 <CAGxFJbQwaLUKe58D3LRBMRUDUBaoGnU45P75sdVXfyawZ=mYUA@mail.gmail.com>
Message-ID: <CACxE24nyfLcYETABp0L386eCg7bf18JyPytDvo=UTi2Big-MtA@mail.gmail.com>

Thank you everyone, for your help!!

Sincerely,
Erin


On Tue, Jun 7, 2022 at 4:13 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> I assume this is using R's basic plotting engine.
>
> If so, you **might** be able to do this nicely with legend(). Jim Lemon's
> plotrix package might also have something for you.
>
> Otherwise you can set plot limits explicitly, and then plot the point
> outside those limits with your desired pch by setting xpd = TRUE. Something
> like:
>
> plot(c(1,1.5,2), c(3,5.5,4), pch = c(1,3,1), ylim = c(2.5,4.5),
>      xpd = TRUE)
>
> Then use mtext() to add your text nearby.
> Inelegant, but it should work.
>
> Cheers,
> Bert Gunter
>
>
> On Tue, Jun 7, 2022 at 12:39 PM Erin Hodgess <erinm.hodgess at gmail.com>
> wrote:
>
>> Hello!
>>
>> Hope you?re having a great day!
>>
>> I would like to combine plotting symbols with text.
>>
>> I have tried
>> xp2a <- expression(paste(pch =3, ?my stuff?))
>> But when I use that as a plot title, it just shows as ?3 my stuff?.
>>
>> I have a feeling that it?s going to be something very straightforward that
>> I am missing.
>>
>> Thanks for your help,
>> Sincerely,
>> Erin
>> --
>> Erin Hodgess, PhD
>> mailto: erinm.hodgess at gmail.com
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>
>
>> and provide commented, minimal, self-contained, reproducible code.
>>
> --
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Tue Jun  7 22:12:56 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 7 Jun 2022 13:12:56 -0700
Subject: [R] Mixing plotting symbols with text
In-Reply-To: <CACxE24=pgrsjNS2hq8U5pxdz6wSfh0+Ppn2y=dH32nG0znQgNg@mail.gmail.com>
References: <CACxE24=pgrsjNS2hq8U5pxdz6wSfh0+Ppn2y=dH32nG0znQgNg@mail.gmail.com>
Message-ID: <CAGxFJbQwaLUKe58D3LRBMRUDUBaoGnU45P75sdVXfyawZ=mYUA@mail.gmail.com>

I assume this is using R's basic plotting engine.

If so, you **might** be able to do this nicely with legend(). Jim Lemon's
plotrix package might also have something for you.

Otherwise you can set plot limits explicitly, and then plot the point
outside those limits with your desired pch by setting xpd = TRUE. Something
like:

plot(c(1,1.5,2), c(3,5.5,4), pch = c(1,3,1), ylim = c(2.5,4.5),
     xpd = TRUE)

Then use mtext() to add your text nearby.
Inelegant, but it should work.

Cheers,
Bert Gunter


On Tue, Jun 7, 2022 at 12:39 PM Erin Hodgess <erinm.hodgess at gmail.com>
wrote:

> Hello!
>
> Hope you?re having a great day!
>
> I would like to combine plotting symbols with text.
>
> I have tried
> xp2a <- expression(paste(pch =3, ?my stuff?))
> But when I use that as a plot title, it just shows as ?3 my stuff?.
>
> I have a feeling that it?s going to be something very straightforward that
> I am missing.
>
> Thanks for your help,
> Sincerely,
> Erin
> --
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Wed Jun  8 00:41:23 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 8 Jun 2022 08:41:23 +1000
Subject: [R] Mixing plotting symbols with text
In-Reply-To: <CACxE24=pgrsjNS2hq8U5pxdz6wSfh0+Ppn2y=dH32nG0znQgNg@mail.gmail.com>
References: <CACxE24=pgrsjNS2hq8U5pxdz6wSfh0+Ppn2y=dH32nG0znQgNg@mail.gmail.com>
Message-ID: <CA+8X3fV+a1wFq0mU6eib7KU1HDerEO=c7DkUf9yqbXVDSNBqBg@mail.gmail.com>

Hi Erin,
Here's a kinda clunky way to get your plot title. It can be modified
to do fancier things like intersperse symbols with letters but you may
need no more than this. Have fun.

par(mar=c(5,4,5,2))
plot(0,xlim=c(-1,1),ylim=c(-1,1),type="n",xlab="Nonsense",ylab="Intensity")
points(runif(4,-1,1),runif(4,-1,1),pch=3)
points(runif(4,-1,1),runif(4,-1,1),pch=4)
symbol_n_text<-function(x,y,pch,txt,adj=0,cex=1) {
 oldcex<-par(cex=cex)
 x<-x-(strwidth("M ")+strwidth(txt))/2
 points(x,y,pch=pch)
 x<-x+strwidth("M ")
 text(x,y,txt,adj=adj)
 par(oldcex)
}
par(xpd=TRUE)
symbol_n_text(0,1.35,3,"your stuff",cex=1.5)
symbol_n_text(0,1.2,4,"my stuff",cex=1.5)

Thanks for the nod, Bert.

Jim

On Wed, Jun 8, 2022 at 5:39 AM Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>
> Hello!
>
> Hope you?re having a great day!
>
> I would like to combine plotting symbols with text.
>
> I have tried
> xp2a <- expression(paste(pch =3, ?my stuff?))
> But when I use that as a plot title, it just shows as ?3 my stuff?.
>
> I have a feeling that it?s going to be something very straightforward that
> I am missing.
>
> Thanks for your help,
> Sincerely,
> Erin
> --
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @|mon@wood @end|ng |rom b@th@edu  Wed Jun  8 13:04:59 2022
From: @|mon@wood @end|ng |rom b@th@edu (Simon Wood)
Date: Wed, 8 Jun 2022 12:04:59 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
Message-ID: <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>

I would not worry too much about high concurvity between variables like 
temperature and time. This just reflects the fact that temperature has a 
strong temporal pattern.

I would also not be too worried about the low p-values on the k check. 
The check only looks for pattern in the residuals when they are ordered 
with respect to the variables of a smooth. When you have time series 
data and some smooths involve time then it's hard not to pick up some 
degree of residual auto-correlation, which you often would not want to 
model with a higher rank smoother.

The NAs? for the distributed lag terms just reflect the fact that there 
is no obvious way to order the residuals w.r.t. the covariates for such 
terms, so the simple check for residual pattern is not really possible.

One simple approach is to fit using bam(...,discrete=TRUE) which will 
let you specify an AR1 parameter to mop up some of the residual 
auto-correlation without resorting to a high rank smooth that then does 
all the work of the covariates as well. The AR1 parameter can be set by 
looking at the ACF of the residuals of the model without this. You need 
to look at the ACF of suitably standardized residuals to check how well 
this has worked.

best,

Simon

On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> Hello everyone,
>
> A few days ago I asked a question about concurvity in a GAM (the
> anologue of collinearity in a GLM) implemented in mgcv. I think my
> question was a bit unfocussed, so I am retrying again, but with
> additional information included about the autocorrelation function. I
> have also posted about this on Cross Validated. Given all the model
> output, it might make for easier
> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
>
> As mentioned previously, I have problems with concurvity in my thesis
> research, and don't have access to a statistician who works with time
> series, GAMs or R. I'd be very grateful for any (partial) answer,
> however short. I'll gladly return the favour where I can! For really
> helpful input I'd be more than happy to offer co-authorship on
> publication. Deadlines are very close, and I'm heading towards having
> no results at all if I can't solve this concurvity issue :(
>
> I'm using GAMs to try to understand the relationship between deaths
> and heat-related variables (e.g. temperature and humidity), using
> daily time series over a 14-year period from a tropical, low-income
> country. My aim is to understand the relationship between these
> variables and deaths, rather than pure prediction performance.
>
> The GAMs include distributed lag models (set up as 7-column matrices,
> see code at bottom of post), since deaths may occur over several days
> following exposure.
>
> Simple GAMs with just time, lagged temperature and lagged
> precipitation (a potential confounder) show very high concurvity
> between lagged temperature and time, regardless of the many different
> ways I have tried to decompose time. The autocorrelation functions
> (ACF) however, shows values close to zero, only just about breaching
> the 'significance line' in a few instances. It does show patterning
> though, although the regularity is difficult to define.
>
> My questions are:
> 1) Should I be worried about the high concurvity, or can I ignore it
> given the mostly non-significant ACF? I've read dozens of
> heat-mortality modelling studies and none report on concurvity between
> weather variables and time (though one 2012 paper discussed
> autocorrelation).
>
> 2) If I cannot ignore it, what should I do to resolve it? Would
> including an autoregressive term be appropriate, and if so, where can
> I find a coded example of how to do this? I've also come across
> sequential regression][1]. Would this be more or less appropriate? If
> appropriate, a pointer to an example would be really appreciated!
>
> Some example GAMs are specified as follows:
> ```r
> conc38b <- gam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc")) + heap +
>                        te(temp_max, lag, k=c(10, 3)) +
>                        te(precip_daily_total, lag, k=c(10, 3)),
>                        data = dat, family = nb, method = 'REML', select = TRUE,
>                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> 52.5), weekday = c(0, 6.5)))
> ```
> Concurvity for the above model between (temp_max, lag) and (year,
> month, week, weekday) is 0.91:
>
> ```r
> $worst
>                                      para te(year,month,week,weekday)
> te(temp_max,lag) te(precip_daily_total,lag)
> para                        1.000000e+00                1.125625e-29
>       0.3150073                  0.6666348
> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
>       0.9060552                  0.6652313
> te(temp_max,lag)            3.152795e-01                8.998113e-01
>       1.0000000                  0.5781015
> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
>       0.5805159                  1.0000000
> ```
>
> Output from ```gam.check()```:
> ```r
> Method: REML   Optimizer: outer newton
> full convergence after 16 iterations.
> Gradient range [-0.01467332,0.003096643]
> (score 8915.994 & scale 1).
> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> Model rank =  544 / 544
>
> Basis dimension (k) checking results. Low p-value (k-index<1) may
> indicate that k is too low, especially if edf is close to k'.
>
>                                    k'      edf k-index p-value
> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> te(temp_max,lag)             29.0000   3.3681      NA      NA
> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> ```
>
> Some output from ```summary(conc38b)```:
> ```r
> Approximate significance of smooth terms:
>                                    edf Ref.df  Chi.sq p-value
> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> -REML =   8916  Scale est. = 1         n = 5107
> ```
>
>
> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> pattern). They show peaks at 5 and 15, and then there seems to be a
> recurring pattern at multiples of approx. 30 (suggesting month is not
> modelled adequately?). Not sure what would cause the spikes at 5 and
> 15. There is heaping of deaths on the 15th day of each month, to which
> deaths with unknown date were allocated. This heaping was modelled
> with categorical variable/ factor ```heap``` with 169 levels (0 for
> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> heaping day over the 14-year period):
>
>    [2]: https://i.stack.imgur.com/FzKyM.png
>    [3]: https://i.stack.imgur.com/fE3aL.png
>
>
> I get an identical looking ACF when I decompose time into (year,
> month, monthday) as in model conc39 below, although concurvity between
> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
>
> ```r
> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>                       te(temp_max, lag, k=c(10, 4)) +
>                       te(precip_daily_total, lag, k=c(10, 4)),
>                       data = dat, family = nb, method = 'REML', select = TRUE,
>                       knots = list(month = c(0.5, 12.5)))
> ```
> ```r
>
> Method: REML   Optimizer: outer newton
> full convergence after 14 iterations.
> Gradient range [-0.001578187,6.155096e-05]
> (score 8915.763 & scale 1).
> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> Model rank =  323 / 323
>
> Basis dimension (k) checking results. Low p-value (k-index<1) may
> indicate that k is too low, especially if edf is close to k'.
>
>                                  k'     edf k-index p-value
> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> te(temp_max,lag)           39.0000  4.0875      NA      NA
> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> ```
> Some output from ```summary(conc39)```:
> ```r
> Approximate significance of smooth terms:
>                                  edf Ref.df  Chi.sq  p-value
> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> -REML =   8915  Scale est. = 1         n = 5107
> ```
>
>
> ```r
> $worst
>                                     para te(year,month,monthday)
> te(temp_max,lag) te(precip_daily_total,lag)
> para                       1.000000e+00            3.261007e-31
> 0.3313549                  0.6666532
> te(year,month,monthday)    3.060763e-31            1.000000e+00
> 0.8266086                  0.5670777
> te(temp_max,lag)           3.331014e-01            8.225942e-01
> 1.0000000                  0.5840875
> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> 0.5939380                  1.0000000
> ```
>
> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> various choices for k or as ```s(time)``` with various k does not
> reduce concurvity either.
>
>
> The default approach in time series studies of heat-mortality is to
> model time with fixed df, generally between 7-10 df per year of data.
> I am, however, apprehensive about this approach because a) mortality
> profiles vary with locality due to sociodemographic and environmental
> characteristics and b) the choice of df is based on higher income
> countries (where nearly all these studies have been done) with
> different mortality profiles and so may not be appropriate for
> tropical, low-income countries.
>
> Although the approach of fixing (high) df does remove more temporal
> patterns from the ACF (see model and output below), concurvity between
> time and lagged temperature has now risen to 0.99! Moreover,
> temperature (which has been a consistent, highly significant predictor
> in every model of the tens (hundreds?) I have run, has now turned
> non-significant. I am guessing this is because time is now a very
> wiggly function that not only models/ removes seasonal variation, but
> also some of the day-to-day variation that is needed for the
> temperature smooth  :
>
> ```r
> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>                        te(temp_max, lag, k=c(10,3)) +
>                        te(precip_daily_total, lag, k=c(10,3)),
>                        data = dat, family = nb, method = 'REML', select = TRUE)
> ```
> Output from ```gam.check(conc20a, rep = 1000)```:
>
> ```r
> Method: REML   Optimizer: outer newton
> full convergence after 9 iterations.
> Gradient range [-0.0008983099,9.546022e-05]
> (score 8750.13 & scale 1).
> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> Model rank =  336 / 336
>
> Basis dimension (k) checking results. Low p-value (k-index<1) may
> indicate that k is too low, especially if edf is close to k'.
>
>                                   k'      edf k-index p-value
> s(time)                    111.0000 111.0000    0.98    0.56
> te(temp_max,lag)            29.0000   0.6548      NA      NA
> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> ```
> Output from ```concurvity(conc20a, full=FALSE)$worst```:
>
> ```r
>                                     para      s(time) te(temp_max,lag)
> te(precip_daily_total,lag)
> para                       1.000000e+00 2.462064e-19        0.3165236
>                  0.6666348
> s(time)                    2.462398e-19 1.000000e+00        0.9930674
>                  0.6879284
> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
>                  0.5788711
> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
>                  1.0000000
>
> ```
>
> Some output from ```summary(conc20a)```:
> ```r
> Approximate significance of smooth terms:
>                                   edf Ref.df  Chi.sq p-value
> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> -REML = 8750.1  Scale est. = 1         n = 5107
> ```
>
> ACF functions:
>
> [4]: https://i.stack.imgur.com/7nbXS.png
> [5]: https://i.stack.imgur.com/pNnZU.png
>
> Data can be found on my [GitHub][6] site in the file
> [data_cross_validated_post2.rds][7]. A csv version is also available.
> This is my code:
>
> ```r
> library(readr)
> library(mgcv)
>
> df <- read_rds("data_crossvalidated_post2.rds")
>
> # Create matrices for lagged weather variables (6 day lags) based on
> example by Simon Wood
> # in his 2017 book ("Generalized additive models: an introduction with
> R", p. 349) and
> # gamair package documentation
> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
>
> lagard <- function(x,n.lag=7) {
> n <- length(x); X <- matrix(NA,n,n.lag)
> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> X
> }
>
> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> = df$precip_hourly_dailysum)
> dat$temp_max <- lagard(df$temp_max)
> dat$temp_min <- lagard(df$temp_min)
> dat$temp_mean <- lagard(df$temp_mean)
> dat$wbgt_max <- lagard(df$wbgt_max)
> dat$wbgt_mean <- lagard(df$wbgt_mean)
> dat$wbgt_min <- lagard(df$wbgt_min)
> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> dat$sh_mean <- lagard(df$sh_mean)
> dat$solar_mean <- lagard(df$solar_mean)
> dat$wind2m_mean <- lagard(df$wind2m_mean)
> dat$sh_max <- lagard(df$sh_max)
> dat$solar_max <- lagard(df$solar_max)
> dat$wind2m_max <- lagard(df$wind2m_max)
> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> dat$precip_hourly <- lagard(df$precip_hourly)
> dat$precip_daily_total <- lagard( df$precip_daily_total)
> dat$temp <- lagard(df$temp)
> dat$sh <- lagard(df$sh)
> dat$rh <- lagard(df$rh)
> dat$solar <- lagard(df$solar)
> dat$wind2m <- lagard(df$wind2m)
>
>
> conc38b <- gam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc")) + heap +
>                        te(temp_max, lag, k=c(10, 3)) +
>                        te(precip_daily_total, lag, k=c(10, 3)),
>                        data = dat, family = nb, method = 'REML', select = TRUE,
>                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> 52.5), weekday = c(0, 6.5)))
>
> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>                       te(temp_max, lag, k=c(10, 4)) +
>                       te(precip_daily_total, lag, k=c(10, 4)),
>                       data = dat, family = nb, method = 'REML', select = TRUE,
>                       knots = list(month = c(0.5, 12.5)))
>
> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>                        te(temp_max, lag, k=c(10,3)) +
>                        te(precip_daily_total, lag, k=c(10,3)),
>                        data = dat, family = nb, method = 'REML', select = TRUE)
>
> ```
> Thank you if you've read this far!! :-))
>
>    [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
>    [2]: https://i.stack.imgur.com/FzKyM.png
>    [3]: https://i.stack.imgur.com/fE3aL.png
>    [4]: https://i.stack.imgur.com/7nbXS.png
>    [5]: https://i.stack.imgur.com/pNnZU.png
>    [6]: https://github.com/JadeShodan/heat-mortality
>    [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Simon Wood, School of Mathematics, University of Edinburgh,
https://www.maths.ed.ac.uk/~swood34/


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Wed Jun  8 20:35:46 2022
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Wed, 8 Jun 2022 20:35:46 +0200
Subject: [R] bootstrap CI of the difference between 2 Cramer's V
In-Reply-To: <0e2dd5f6-c0e8-5b72-37f4-4115794319f8@sapo.pt>
References: <0e2dd5f6-c0e8-5b72-37f4-4115794319f8@sapo.pt>
Message-ID: <BAF5F99C-16A6-4705-B651-F7CD16140302@yahoo.fr>

Dear Rui, 
Dear Daniel,

Many thanks for your code. It perfectly works.
Thanks for your answer.

Best,
S.

Envoy? de mon iPhone

> Le 6 juin 2022 ? 21:17, Rui Barradas <ruipbarradas at sapo.pt> a ?crit :
> 
> ?Hello,
> 
> Here is your code, corrected. It uses a Goodman-Kruskal gamma function in package DescTools.
> Function G probably doesn't need tryCatch but I had errors in some test runs.
> 
> 
> library(boot)
> 
> G <- function(x, index, cols = 1:2) {
>  y <- x[index, cols]    # bootstrapped data
>  g <- x$group[index]    # and groups
>  # calculate gamma for each group bootstrap sample
>  # (trap errors in case one of the groups is empty)
>  g <- tryCatch(
>    lapply(split(y[1:2], g), \(x) {
>      tbl <- table(x)
>      DescTools::GoodmanKruskalGamma(tbl)
>    }),
>    error = function(e) list(NA_real_, NA_real_)
>  )
>  # calculate difference
>  g[[1]] - g[[2]]
> }
> 
> set.seed(2022)
> # use strata parameter in function boot to resample within each group
> results <- boot(
>  data = f3,
>  statistic = G,
>  R = 2000,
>  strata = as.factor(f3$group),
>  cols = 1:2
> )
> 
> results
> boot.ci(results)
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> 
> ?s 17:21 de 05/06/2022, varin sacha via R-help escreveu:
>> Dear Daniel,
>> Dear R-experts,
>> I really thank you a lot Daniel. Nobody had answered to me offline. So, thanks.
>> I have tried in the same vein for the Goodman-Kruskal gamma for ordinal data. There is an error message at the end of the code. Thanks for your help.
>> ##############################
>> library(ryouready)
>> library(boot)
>> shopping1<-c("tr?s important","important","pas important","pas important","important","tr?s important","important","pas important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","important")
>> statut1<-c("riche","pas riche","moyennement riche","moyennement riche","riche","pas riche","moyennement riche","moyennement riche","riche","pas riche","moyennement riche","riche","pas riche","pas riche","riche","moyennement riche","riche","pas riche","pas riche","pas riche","riche","riche","moyennement riche","riche","riche","moyennement riche","moyennement riche","moyennement riche","pas riche","pas riche","riche","pas riche","riche","pas riche","riche","moyennement riche","riche","pas riche","moyennement riche","riche")
>> shopping2<-c("important","pas important","tr?s important","tr?s important","important","tr?s important","pas important","important","pas important","tr?s important","important","important","important","important","pas important","tr?s important","tr?s important","important","pas important","tr?s important","pas important","tr?s important","pas important","tr?s important","important","tr?s important","important","pas important","pas important","important","pas important","tr?s important","pas important","pas important","important","important","tr?s important","tr?s important","pas important","pas important")
>> statut2<-c("moyennement riche","pas riche","riche","moyennement riche","moyennement riche","moyennement riche","pas riche","riche","riche","pas riche","moyennement riche","riche","riche","riche","riche","riche","pas riche","moyennement riche","moyennement riche","pas riche","moyennement riche","pas riche","pas riche","pas riche","moyennement riche","riche","moyennement riche","riche","pas riche","riche","moyennement riche","blue","moyennement riche","pas riche","pas riche","riche","riche","pas riche","pas riche","pas riche")
>> f1 <- data.frame(shopping=shopping1,statut=statut1,group='grp1')
>> f2 <- data.frame(shopping=shopping2,statut=statut2,group='grp2')
>> f3 <- rbind(f1,f2)
>> G <- function(x, index) {
>>    # calculate goodman for group 1 bootstrap sample
>>    g1 <-x[index,][x[,3]=='grp1',]
>>    goodman_g1 <- cor(data[index,][1,2])
>>     # calculate goodman for group 2 bootstrap sample
>>    g2 <-x[index,][x[,3]=='grp2',]
>>    goodman_g2 <- cor(data[index,][3,4])
>>     # calculate difference
>>    goodman_g1-goodman_g2
>>    }
>>  # use strata parameter in function boot to resample within each group
>> results <- boot(data=f3,statistic=G, strata=as.factor(f3$group),R=2000)
>> results
>> boot.ci(results)
>> ##############################
>> Le samedi 4 juin 2022 ? 09:31:36 UTC+2, Daniel Nordlund <djnordlund at gmail.com> a ?crit :
>>> On 5/28/2022 11:21 AM, varin sacha via R-help wrote:
>>> Dear R-experts,
>>> 
>>> While comparing groups, it is better to assess confidence intervals of those differences rather than comparing confidence intervals for each group.
>>> I am trying to calculate the CIs of the difference between the two Cramer's V and not the CI to the estimate of each group?s Cramer's V.
>>> 
>>> Here below my toy R example. There are error messages. Any help would be highly appreciated.
>>> 
>>> ##############################
>>> library(questionr)
>>> library(boot)
>>> 
>>> gender1<-c("M","F","F","F","M","M","F","F","F","M","M","F","M","M","F","M","M","F","M","F","F","F","M","M","M","F","F","M","M","M","F","M","F","F","F","M","M","F","M","F")
>>> color1<-c("blue","green","black","black","green","green","blue","blue","green","black","blue","green","blue","black","black","blue","green","blue","green","black","blue","blue","black","black","green","green","blue","green","black","green","blue","black","black","blue","green","green","green","blue","blue","black")
>>> 
>>> gender2<-c("F","F","F","M","M","F","M","M","M","F","F","M","F","M","F","F","M","M","M","F","M","M","M","F","F","F","M","M","M","F","M","M","M","F","F","F","M","F","F","F")
>>> color2<-c("green","blue","black","blue","blue","blue","green","blue","green","black","blue","black","blue","blue","black","blue","blue","green","blue","black","blue","blue","black","black","green","blue","black","green","blue","green","black","blue","black","blue","green","blue","green","green","blue","black")
>>> 
>>> f1=data.frame(gender1,color1)
>>> tab1<-table(gender1,color1)
>>> e1<-cramer.v(tab1)
>>> 
>>> f2=data.frame(gender2,color2)
>>> tab2<-table(gender2,color2)
>>> e2<-cramer.v(tab2)
>>> 
>>> f3<-data.frame(e1-e2)
>>> 
>>> cramerdiff=function(x,w){
>>> y<-tapply(x[w,1], x[w,2],cramer.v)
>>> y[1]-y[2]
>>> }
>>> 
>>> results<-boot(data=f3,statistic=cramerdiff,R=2000)
>>> results
>>> 
>>> boot.ci(results,type="all")
>>> ##############################
>>> 
>>>   
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> I don't know if someone responded offline, but if not, there are a
>> couple of problems with your code.   First, the f3 dataframe is not what
>> you think it is.  Second, your cramerdiff function isn't going to
>> produce the results that you want.
>> I would put your data into a single dataframe with a variable
>> designating which group data came from.  Then use that variable as the
>> strata variable in the boot function to resample within groups.  So
>> something like this:
>> f1 <- data.frame(gender=gender1,color=color1,group='grp1')
>> f2 <- data.frame(gender=gender2,color=color2,group='grp2')
>> f3 <- rbind(f1,f2)
>> cramerdiff <- function(x, ndx) {
>>    # calculate cramer.v for group 1 bootstrap sample
>>    g1 <-x[ndx,][x[,3]=='grp1',]
>>    cramer_g1 <- cramer.v(table(g1[,1:2]))
>>    # calculate cramer.v for group 2 bootstrap sample
>>    g2 <-x[ndx,][x[,3]=='grp2',]
>>    cramer_g2 <- cramer.v(table(g2[,1:2]))
>>    # calculate difference
>>    cramer_g1-cramer_g2
>>    }
>> # use strata parameter in function boot to resample within each group
>> results <- boot(data=f3,statistic=cramerdiff,
>> strata=as.factor(f3$group),R=2000)
>> results
>> boot.ci(results)
>> Hope this is helpful,
>> Dan


From j@de@shod@@ m@iii@g oii googiem@ii@com  Wed Jun  8 19:15:11 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Wed, 8 Jun 2022 18:15:11 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
Message-ID: <CANg3_k-EXdbmLVNbEH9nN=FE3FYs4uqXgVtH3V35o6BU0z=rTw@mail.gmail.com>

Hi Simon,

Thanks so much for this!! I have two follow up questions, if you don't mind.

1. Does including an autoregressive term not adjust away part of the
effect of the response in a distributed lag model (where the outcome
accumulates over time)?
2. I've tried to fit a model using bam (just a first attempt without
AR term), but including the factor variable heap creates errors:

bam0 <- bam(deaths~te(year, month, week, weekday,
bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
                      te(temp_max, lag, k=c(8, 3)) +
                      te(precip_daily_total, lag, k=c(8, 3)),
                      data = dat, family = nb, method = 'fREML',
select = TRUE, discrete = TRUE,
                      knots = list(month = c(0.5, 12.5), week = c(0.5,
52.5), weekday = c(0, 6.5)))

This model results in errors:

Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
  step failure in theta estimation
Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
object$prior.weights)) :
  NaNs produced


Including heap as as.numeric(heap) runs the model without error
messages or warnings, but model diagnostics look terrible, and it also
doesn't make sense (to me) to make heap a numeric. The factor variable
heap (with 169 levels) codes the fact that all deaths for which no
date was known, were registered on the 15th day of each month. I've
coded all non-heaping days as 0. All heaping days were coded as a
value between 1-168. The time series spans 14 years, so a heaping day
in each month results in 14*12 levels = 168, plus one level for
non-heaping days.

So my second question is: Does bam allow factor variables? And if not,
how should I model this heaping on the 15th day of the month instead?

With thanks,

Jade

On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
>
> I would not worry too much about high concurvity between variables like
> temperature and time. This just reflects the fact that temperature has a
> strong temporal pattern.
>
> I would also not be too worried about the low p-values on the k check.
> The check only looks for pattern in the residuals when they are ordered
> with respect to the variables of a smooth. When you have time series
> data and some smooths involve time then it's hard not to pick up some
> degree of residual auto-correlation, which you often would not want to
> model with a higher rank smoother.
>
> The NAs  for the distributed lag terms just reflect the fact that there
> is no obvious way to order the residuals w.r.t. the covariates for such
> terms, so the simple check for residual pattern is not really possible.
>
> One simple approach is to fit using bam(...,discrete=TRUE) which will
> let you specify an AR1 parameter to mop up some of the residual
> auto-correlation without resorting to a high rank smooth that then does
> all the work of the covariates as well. The AR1 parameter can be set by
> looking at the ACF of the residuals of the model without this. You need
> to look at the ACF of suitably standardized residuals to check how well
> this has worked.
>
> best,
>
> Simon
>
> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> > Hello everyone,
> >
> > A few days ago I asked a question about concurvity in a GAM (the
> > anologue of collinearity in a GLM) implemented in mgcv. I think my
> > question was a bit unfocussed, so I am retrying again, but with
> > additional information included about the autocorrelation function. I
> > have also posted about this on Cross Validated. Given all the model
> > output, it might make for easier
> > reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> >
> > As mentioned previously, I have problems with concurvity in my thesis
> > research, and don't have access to a statistician who works with time
> > series, GAMs or R. I'd be very grateful for any (partial) answer,
> > however short. I'll gladly return the favour where I can! For really
> > helpful input I'd be more than happy to offer co-authorship on
> > publication. Deadlines are very close, and I'm heading towards having
> > no results at all if I can't solve this concurvity issue :(
> >
> > I'm using GAMs to try to understand the relationship between deaths
> > and heat-related variables (e.g. temperature and humidity), using
> > daily time series over a 14-year period from a tropical, low-income
> > country. My aim is to understand the relationship between these
> > variables and deaths, rather than pure prediction performance.
> >
> > The GAMs include distributed lag models (set up as 7-column matrices,
> > see code at bottom of post), since deaths may occur over several days
> > following exposure.
> >
> > Simple GAMs with just time, lagged temperature and lagged
> > precipitation (a potential confounder) show very high concurvity
> > between lagged temperature and time, regardless of the many different
> > ways I have tried to decompose time. The autocorrelation functions
> > (ACF) however, shows values close to zero, only just about breaching
> > the 'significance line' in a few instances. It does show patterning
> > though, although the regularity is difficult to define.
> >
> > My questions are:
> > 1) Should I be worried about the high concurvity, or can I ignore it
> > given the mostly non-significant ACF? I've read dozens of
> > heat-mortality modelling studies and none report on concurvity between
> > weather variables and time (though one 2012 paper discussed
> > autocorrelation).
> >
> > 2) If I cannot ignore it, what should I do to resolve it? Would
> > including an autoregressive term be appropriate, and if so, where can
> > I find a coded example of how to do this? I've also come across
> > sequential regression][1]. Would this be more or less appropriate? If
> > appropriate, a pointer to an example would be really appreciated!
> >
> > Some example GAMs are specified as follows:
> > ```r
> > conc38b <- gam(deaths~te(year, month, week, weekday,
> > bs=c("cr","cc","cc","cc")) + heap +
> >                        te(temp_max, lag, k=c(10, 3)) +
> >                        te(precip_daily_total, lag, k=c(10, 3)),
> >                        data = dat, family = nb, method = 'REML', select = TRUE,
> >                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> > 52.5), weekday = c(0, 6.5)))
> > ```
> > Concurvity for the above model between (temp_max, lag) and (year,
> > month, week, weekday) is 0.91:
> >
> > ```r
> > $worst
> >                                      para te(year,month,week,weekday)
> > te(temp_max,lag) te(precip_daily_total,lag)
> > para                        1.000000e+00                1.125625e-29
> >       0.3150073                  0.6666348
> > te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> >       0.9060552                  0.6652313
> > te(temp_max,lag)            3.152795e-01                8.998113e-01
> >       1.0000000                  0.5781015
> > te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> >       0.5805159                  1.0000000
> > ```
> >
> > Output from ```gam.check()```:
> > ```r
> > Method: REML   Optimizer: outer newton
> > full convergence after 16 iterations.
> > Gradient range [-0.01467332,0.003096643]
> > (score 8915.994 & scale 1).
> > Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> > Model rank =  544 / 544
> >
> > Basis dimension (k) checking results. Low p-value (k-index<1) may
> > indicate that k is too low, especially if edf is close to k'.
> >
> >                                    k'      edf k-index p-value
> > te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> > te(temp_max,lag)             29.0000   3.3681      NA      NA
> > te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > ```
> >
> > Some output from ```summary(conc38b)```:
> > ```r
> > Approximate significance of smooth terms:
> >                                    edf Ref.df  Chi.sq p-value
> > te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> > te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> > te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >
> > R-sq.(adj) =  0.839   Deviance explained = 53.3%
> > -REML =   8916  Scale est. = 1         n = 5107
> > ```
> >
> >
> > Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> > pattern). They show peaks at 5 and 15, and then there seems to be a
> > recurring pattern at multiples of approx. 30 (suggesting month is not
> > modelled adequately?). Not sure what would cause the spikes at 5 and
> > 15. There is heaping of deaths on the 15th day of each month, to which
> > deaths with unknown date were allocated. This heaping was modelled
> > with categorical variable/ factor ```heap``` with 169 levels (0 for
> > all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> > heaping day over the 14-year period):
> >
> >    [2]: https://i.stack.imgur.com/FzKyM.png
> >    [3]: https://i.stack.imgur.com/fE3aL.png
> >
> >
> > I get an identical looking ACF when I decompose time into (year,
> > month, monthday) as in model conc39 below, although concurvity between
> > (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> >
> > ```r
> > conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >                       te(temp_max, lag, k=c(10, 4)) +
> >                       te(precip_daily_total, lag, k=c(10, 4)),
> >                       data = dat, family = nb, method = 'REML', select = TRUE,
> >                       knots = list(month = c(0.5, 12.5)))
> > ```
> > ```r
> >
> > Method: REML   Optimizer: outer newton
> > full convergence after 14 iterations.
> > Gradient range [-0.001578187,6.155096e-05]
> > (score 8915.763 & scale 1).
> > Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> > Model rank =  323 / 323
> >
> > Basis dimension (k) checking results. Low p-value (k-index<1) may
> > indicate that k is too low, especially if edf is close to k'.
> >
> >                                  k'     edf k-index p-value
> > te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> > te(temp_max,lag)           39.0000  4.0875      NA      NA
> > te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > ```
> > Some output from ```summary(conc39)```:
> > ```r
> > Approximate significance of smooth terms:
> >                                  edf Ref.df  Chi.sq  p-value
> > te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> > te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> > te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >
> > R-sq.(adj) =  0.839   Deviance explained = 53.8%
> > -REML =   8915  Scale est. = 1         n = 5107
> > ```
> >
> >
> > ```r
> > $worst
> >                                     para te(year,month,monthday)
> > te(temp_max,lag) te(precip_daily_total,lag)
> > para                       1.000000e+00            3.261007e-31
> > 0.3313549                  0.6666532
> > te(year,month,monthday)    3.060763e-31            1.000000e+00
> > 0.8266086                  0.5670777
> > te(temp_max,lag)           3.331014e-01            8.225942e-01
> > 1.0000000                  0.5840875
> > te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> > 0.5939380                  1.0000000
> > ```
> >
> > Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> > various choices for k or as ```s(time)``` with various k does not
> > reduce concurvity either.
> >
> >
> > The default approach in time series studies of heat-mortality is to
> > model time with fixed df, generally between 7-10 df per year of data.
> > I am, however, apprehensive about this approach because a) mortality
> > profiles vary with locality due to sociodemographic and environmental
> > characteristics and b) the choice of df is based on higher income
> > countries (where nearly all these studies have been done) with
> > different mortality profiles and so may not be appropriate for
> > tropical, low-income countries.
> >
> > Although the approach of fixing (high) df does remove more temporal
> > patterns from the ACF (see model and output below), concurvity between
> > time and lagged temperature has now risen to 0.99! Moreover,
> > temperature (which has been a consistent, highly significant predictor
> > in every model of the tens (hundreds?) I have run, has now turned
> > non-significant. I am guessing this is because time is now a very
> > wiggly function that not only models/ removes seasonal variation, but
> > also some of the day-to-day variation that is needed for the
> > temperature smooth  :
> >
> > ```r
> > conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >                        te(temp_max, lag, k=c(10,3)) +
> >                        te(precip_daily_total, lag, k=c(10,3)),
> >                        data = dat, family = nb, method = 'REML', select = TRUE)
> > ```
> > Output from ```gam.check(conc20a, rep = 1000)```:
> >
> > ```r
> > Method: REML   Optimizer: outer newton
> > full convergence after 9 iterations.
> > Gradient range [-0.0008983099,9.546022e-05]
> > (score 8750.13 & scale 1).
> > Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> > Model rank =  336 / 336
> >
> > Basis dimension (k) checking results. Low p-value (k-index<1) may
> > indicate that k is too low, especially if edf is close to k'.
> >
> >                                   k'      edf k-index p-value
> > s(time)                    111.0000 111.0000    0.98    0.56
> > te(temp_max,lag)            29.0000   0.6548      NA      NA
> > te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> > ```
> > Output from ```concurvity(conc20a, full=FALSE)$worst```:
> >
> > ```r
> >                                     para      s(time) te(temp_max,lag)
> > te(precip_daily_total,lag)
> > para                       1.000000e+00 2.462064e-19        0.3165236
> >                  0.6666348
> > s(time)                    2.462398e-19 1.000000e+00        0.9930674
> >                  0.6879284
> > te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> >                  0.5788711
> > te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> >                  1.0000000
> >
> > ```
> >
> > Some output from ```summary(conc20a)```:
> > ```r
> > Approximate significance of smooth terms:
> >                                   edf Ref.df  Chi.sq p-value
> > s(time)                    1.110e+02    111 419.375  <2e-16 ***
> > te(temp_max,lag)           6.548e-01     27   0.895   0.249
> > te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >
> > R-sq.(adj) =  0.843   Deviance explained = 56.1%
> > -REML = 8750.1  Scale est. = 1         n = 5107
> > ```
> >
> > ACF functions:
> >
> > [4]: https://i.stack.imgur.com/7nbXS.png
> > [5]: https://i.stack.imgur.com/pNnZU.png
> >
> > Data can be found on my [GitHub][6] site in the file
> > [data_cross_validated_post2.rds][7]. A csv version is also available.
> > This is my code:
> >
> > ```r
> > library(readr)
> > library(mgcv)
> >
> > df <- read_rds("data_crossvalidated_post2.rds")
> >
> > # Create matrices for lagged weather variables (6 day lags) based on
> > example by Simon Wood
> > # in his 2017 book ("Generalized additive models: an introduction with
> > R", p. 349) and
> > # gamair package documentation
> > (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> >
> > lagard <- function(x,n.lag=7) {
> > n <- length(x); X <- matrix(NA,n,n.lag)
> > for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > X
> > }
> >
> > dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> > deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> > weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> > df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> > = df$precip_hourly_dailysum)
> > dat$temp_max <- lagard(df$temp_max)
> > dat$temp_min <- lagard(df$temp_min)
> > dat$temp_mean <- lagard(df$temp_mean)
> > dat$wbgt_max <- lagard(df$wbgt_max)
> > dat$wbgt_mean <- lagard(df$wbgt_mean)
> > dat$wbgt_min <- lagard(df$wbgt_min)
> > dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> > dat$sh_mean <- lagard(df$sh_mean)
> > dat$solar_mean <- lagard(df$solar_mean)
> > dat$wind2m_mean <- lagard(df$wind2m_mean)
> > dat$sh_max <- lagard(df$sh_max)
> > dat$solar_max <- lagard(df$solar_max)
> > dat$wind2m_max <- lagard(df$wind2m_max)
> > dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> > dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> > dat$precip_hourly <- lagard(df$precip_hourly)
> > dat$precip_daily_total <- lagard( df$precip_daily_total)
> > dat$temp <- lagard(df$temp)
> > dat$sh <- lagard(df$sh)
> > dat$rh <- lagard(df$rh)
> > dat$solar <- lagard(df$solar)
> > dat$wind2m <- lagard(df$wind2m)
> >
> >
> > conc38b <- gam(deaths~te(year, month, week, weekday,
> > bs=c("cr","cc","cc","cc")) + heap +
> >                        te(temp_max, lag, k=c(10, 3)) +
> >                        te(precip_daily_total, lag, k=c(10, 3)),
> >                        data = dat, family = nb, method = 'REML', select = TRUE,
> >                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> > 52.5), weekday = c(0, 6.5)))
> >
> > conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >                       te(temp_max, lag, k=c(10, 4)) +
> >                       te(precip_daily_total, lag, k=c(10, 4)),
> >                       data = dat, family = nb, method = 'REML', select = TRUE,
> >                       knots = list(month = c(0.5, 12.5)))
> >
> > conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >                        te(temp_max, lag, k=c(10,3)) +
> >                        te(precip_daily_total, lag, k=c(10,3)),
> >                        data = dat, family = nb, method = 'REML', select = TRUE)
> >
> > ```
> > Thank you if you've read this far!! :-))
> >
> >    [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> >    [2]: https://i.stack.imgur.com/FzKyM.png
> >    [3]: https://i.stack.imgur.com/fE3aL.png
> >    [4]: https://i.stack.imgur.com/7nbXS.png
> >    [5]: https://i.stack.imgur.com/pNnZU.png
> >    [6]: https://github.com/JadeShodan/heat-mortality
> >    [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Simon Wood, School of Mathematics, University of Edinburgh,
> https://www.maths.ed.ac.uk/~swood34/
>


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Thu Jun  9 15:52:35 2022
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Thu, 9 Jun 2022 14:52:35 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k-EXdbmLVNbEH9nN=FE3FYs4uqXgVtH3V35o6BU0z=rTw@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-EXdbmLVNbEH9nN=FE3FYs4uqXgVtH3V35o6BU0z=rTw@mail.gmail.com>
Message-ID: <91a30ffd-1aec-0f44-17a0-b632fae91014@dewey.myzen.co.uk>

Dear Jade

Do you really need to fit a separate parameter for each heaping day? Can 
you not just make it a binary predictor or a categorical one with fewer 
levels, perhaps 14 (for heaping in each year) or 12 (for each calendar 
month). I have no idea whether that would help but it seems worth a try.

Michael

On 08/06/2022 18:15, jade.shodan--- via R-help wrote:
> Hi Simon,
> 
> Thanks so much for this!! I have two follow up questions, if you don't mind.
> 
> 1. Does including an autoregressive term not adjust away part of the
> effect of the response in a distributed lag model (where the outcome
> accumulates over time)?
> 2. I've tried to fit a model using bam (just a first attempt without
> AR term), but including the factor variable heap creates errors:
> 
> bam0 <- bam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
>                        te(temp_max, lag, k=c(8, 3)) +
>                        te(precip_daily_total, lag, k=c(8, 3)),
>                        data = dat, family = nb, method = 'fREML',
> select = TRUE, discrete = TRUE,
>                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> 52.5), weekday = c(0, 6.5)))
> 
> This model results in errors:
> 
> Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
>    step failure in theta estimation
> Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> object$prior.weights)) :
>    NaNs produced
> 
> 
> Including heap as as.numeric(heap) runs the model without error
> messages or warnings, but model diagnostics look terrible, and it also
> doesn't make sense (to me) to make heap a numeric. The factor variable
> heap (with 169 levels) codes the fact that all deaths for which no
> date was known, were registered on the 15th day of each month. I've
> coded all non-heaping days as 0. All heaping days were coded as a
> value between 1-168. The time series spans 14 years, so a heaping day
> in each month results in 14*12 levels = 168, plus one level for
> non-heaping days.
> 
> So my second question is: Does bam allow factor variables? And if not,
> how should I model this heaping on the 15th day of the month instead?
> 
> With thanks,
> 
> Jade
> 
> On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
>>
>> I would not worry too much about high concurvity between variables like
>> temperature and time. This just reflects the fact that temperature has a
>> strong temporal pattern.
>>
>> I would also not be too worried about the low p-values on the k check.
>> The check only looks for pattern in the residuals when they are ordered
>> with respect to the variables of a smooth. When you have time series
>> data and some smooths involve time then it's hard not to pick up some
>> degree of residual auto-correlation, which you often would not want to
>> model with a higher rank smoother.
>>
>> The NAs  for the distributed lag terms just reflect the fact that there
>> is no obvious way to order the residuals w.r.t. the covariates for such
>> terms, so the simple check for residual pattern is not really possible.
>>
>> One simple approach is to fit using bam(...,discrete=TRUE) which will
>> let you specify an AR1 parameter to mop up some of the residual
>> auto-correlation without resorting to a high rank smooth that then does
>> all the work of the covariates as well. The AR1 parameter can be set by
>> looking at the ACF of the residuals of the model without this. You need
>> to look at the ACF of suitably standardized residuals to check how well
>> this has worked.
>>
>> best,
>>
>> Simon
>>
>> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
>>> Hello everyone,
>>>
>>> A few days ago I asked a question about concurvity in a GAM (the
>>> anologue of collinearity in a GLM) implemented in mgcv. I think my
>>> question was a bit unfocussed, so I am retrying again, but with
>>> additional information included about the autocorrelation function. I
>>> have also posted about this on Cross Validated. Given all the model
>>> output, it might make for easier
>>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
>>>
>>> As mentioned previously, I have problems with concurvity in my thesis
>>> research, and don't have access to a statistician who works with time
>>> series, GAMs or R. I'd be very grateful for any (partial) answer,
>>> however short. I'll gladly return the favour where I can! For really
>>> helpful input I'd be more than happy to offer co-authorship on
>>> publication. Deadlines are very close, and I'm heading towards having
>>> no results at all if I can't solve this concurvity issue :(
>>>
>>> I'm using GAMs to try to understand the relationship between deaths
>>> and heat-related variables (e.g. temperature and humidity), using
>>> daily time series over a 14-year period from a tropical, low-income
>>> country. My aim is to understand the relationship between these
>>> variables and deaths, rather than pure prediction performance.
>>>
>>> The GAMs include distributed lag models (set up as 7-column matrices,
>>> see code at bottom of post), since deaths may occur over several days
>>> following exposure.
>>>
>>> Simple GAMs with just time, lagged temperature and lagged
>>> precipitation (a potential confounder) show very high concurvity
>>> between lagged temperature and time, regardless of the many different
>>> ways I have tried to decompose time. The autocorrelation functions
>>> (ACF) however, shows values close to zero, only just about breaching
>>> the 'significance line' in a few instances. It does show patterning
>>> though, although the regularity is difficult to define.
>>>
>>> My questions are:
>>> 1) Should I be worried about the high concurvity, or can I ignore it
>>> given the mostly non-significant ACF? I've read dozens of
>>> heat-mortality modelling studies and none report on concurvity between
>>> weather variables and time (though one 2012 paper discussed
>>> autocorrelation).
>>>
>>> 2) If I cannot ignore it, what should I do to resolve it? Would
>>> including an autoregressive term be appropriate, and if so, where can
>>> I find a coded example of how to do this? I've also come across
>>> sequential regression][1]. Would this be more or less appropriate? If
>>> appropriate, a pointer to an example would be really appreciated!
>>>
>>> Some example GAMs are specified as follows:
>>> ```r
>>> conc38b <- gam(deaths~te(year, month, week, weekday,
>>> bs=c("cr","cc","cc","cc")) + heap +
>>>                         te(temp_max, lag, k=c(10, 3)) +
>>>                         te(precip_daily_total, lag, k=c(10, 3)),
>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
>>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
>>> 52.5), weekday = c(0, 6.5)))
>>> ```
>>> Concurvity for the above model between (temp_max, lag) and (year,
>>> month, week, weekday) is 0.91:
>>>
>>> ```r
>>> $worst
>>>                                       para te(year,month,week,weekday)
>>> te(temp_max,lag) te(precip_daily_total,lag)
>>> para                        1.000000e+00                1.125625e-29
>>>        0.3150073                  0.6666348
>>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
>>>        0.9060552                  0.6652313
>>> te(temp_max,lag)            3.152795e-01                8.998113e-01
>>>        1.0000000                  0.5781015
>>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
>>>        0.5805159                  1.0000000
>>> ```
>>>
>>> Output from ```gam.check()```:
>>> ```r
>>> Method: REML   Optimizer: outer newton
>>> full convergence after 16 iterations.
>>> Gradient range [-0.01467332,0.003096643]
>>> (score 8915.994 & scale 1).
>>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
>>> Model rank =  544 / 544
>>>
>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>> indicate that k is too low, especially if edf is close to k'.
>>>
>>>                                     k'      edf k-index p-value
>>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
>>> te(temp_max,lag)             29.0000   3.3681      NA      NA
>>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>> ```
>>>
>>> Some output from ```summary(conc38b)```:
>>> ```r
>>> Approximate significance of smooth terms:
>>>                                     edf Ref.df  Chi.sq p-value
>>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
>>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
>>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>
>>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
>>> -REML =   8916  Scale est. = 1         n = 5107
>>> ```
>>>
>>>
>>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
>>> pattern). They show peaks at 5 and 15, and then there seems to be a
>>> recurring pattern at multiples of approx. 30 (suggesting month is not
>>> modelled adequately?). Not sure what would cause the spikes at 5 and
>>> 15. There is heaping of deaths on the 15th day of each month, to which
>>> deaths with unknown date were allocated. This heaping was modelled
>>> with categorical variable/ factor ```heap``` with 169 levels (0 for
>>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
>>> heaping day over the 14-year period):
>>>
>>>     [2]: https://i.stack.imgur.com/FzKyM.png
>>>     [3]: https://i.stack.imgur.com/fE3aL.png
>>>
>>>
>>> I get an identical looking ACF when I decompose time into (year,
>>> month, monthday) as in model conc39 below, although concurvity between
>>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
>>>
>>> ```r
>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>>>                        te(temp_max, lag, k=c(10, 4)) +
>>>                        te(precip_daily_total, lag, k=c(10, 4)),
>>>                        data = dat, family = nb, method = 'REML', select = TRUE,
>>>                        knots = list(month = c(0.5, 12.5)))
>>> ```
>>> ```r
>>>
>>> Method: REML   Optimizer: outer newton
>>> full convergence after 14 iterations.
>>> Gradient range [-0.001578187,6.155096e-05]
>>> (score 8915.763 & scale 1).
>>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
>>> Model rank =  323 / 323
>>>
>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>> indicate that k is too low, especially if edf is close to k'.
>>>
>>>                                   k'     edf k-index p-value
>>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
>>> te(temp_max,lag)           39.0000  4.0875      NA      NA
>>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>> ```
>>> Some output from ```summary(conc39)```:
>>> ```r
>>> Approximate significance of smooth terms:
>>>                                   edf Ref.df  Chi.sq  p-value
>>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
>>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
>>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>
>>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
>>> -REML =   8915  Scale est. = 1         n = 5107
>>> ```
>>>
>>>
>>> ```r
>>> $worst
>>>                                      para te(year,month,monthday)
>>> te(temp_max,lag) te(precip_daily_total,lag)
>>> para                       1.000000e+00            3.261007e-31
>>> 0.3313549                  0.6666532
>>> te(year,month,monthday)    3.060763e-31            1.000000e+00
>>> 0.8266086                  0.5670777
>>> te(temp_max,lag)           3.331014e-01            8.225942e-01
>>> 1.0000000                  0.5840875
>>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
>>> 0.5939380                  1.0000000
>>> ```
>>>
>>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
>>> various choices for k or as ```s(time)``` with various k does not
>>> reduce concurvity either.
>>>
>>>
>>> The default approach in time series studies of heat-mortality is to
>>> model time with fixed df, generally between 7-10 df per year of data.
>>> I am, however, apprehensive about this approach because a) mortality
>>> profiles vary with locality due to sociodemographic and environmental
>>> characteristics and b) the choice of df is based on higher income
>>> countries (where nearly all these studies have been done) with
>>> different mortality profiles and so may not be appropriate for
>>> tropical, low-income countries.
>>>
>>> Although the approach of fixing (high) df does remove more temporal
>>> patterns from the ACF (see model and output below), concurvity between
>>> time and lagged temperature has now risen to 0.99! Moreover,
>>> temperature (which has been a consistent, highly significant predictor
>>> in every model of the tens (hundreds?) I have run, has now turned
>>> non-significant. I am guessing this is because time is now a very
>>> wiggly function that not only models/ removes seasonal variation, but
>>> also some of the day-to-day variation that is needed for the
>>> temperature smooth  :
>>>
>>> ```r
>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>>>                         te(temp_max, lag, k=c(10,3)) +
>>>                         te(precip_daily_total, lag, k=c(10,3)),
>>>                         data = dat, family = nb, method = 'REML', select = TRUE)
>>> ```
>>> Output from ```gam.check(conc20a, rep = 1000)```:
>>>
>>> ```r
>>> Method: REML   Optimizer: outer newton
>>> full convergence after 9 iterations.
>>> Gradient range [-0.0008983099,9.546022e-05]
>>> (score 8750.13 & scale 1).
>>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
>>> Model rank =  336 / 336
>>>
>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>> indicate that k is too low, especially if edf is close to k'.
>>>
>>>                                    k'      edf k-index p-value
>>> s(time)                    111.0000 111.0000    0.98    0.56
>>> te(temp_max,lag)            29.0000   0.6548      NA      NA
>>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
>>> ```
>>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
>>>
>>> ```r
>>>                                      para      s(time) te(temp_max,lag)
>>> te(precip_daily_total,lag)
>>> para                       1.000000e+00 2.462064e-19        0.3165236
>>>                   0.6666348
>>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
>>>                   0.6879284
>>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
>>>                   0.5788711
>>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
>>>                   1.0000000
>>>
>>> ```
>>>
>>> Some output from ```summary(conc20a)```:
>>> ```r
>>> Approximate significance of smooth terms:
>>>                                    edf Ref.df  Chi.sq p-value
>>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
>>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
>>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>
>>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
>>> -REML = 8750.1  Scale est. = 1         n = 5107
>>> ```
>>>
>>> ACF functions:
>>>
>>> [4]: https://i.stack.imgur.com/7nbXS.png
>>> [5]: https://i.stack.imgur.com/pNnZU.png
>>>
>>> Data can be found on my [GitHub][6] site in the file
>>> [data_cross_validated_post2.rds][7]. A csv version is also available.
>>> This is my code:
>>>
>>> ```r
>>> library(readr)
>>> library(mgcv)
>>>
>>> df <- read_rds("data_crossvalidated_post2.rds")
>>>
>>> # Create matrices for lagged weather variables (6 day lags) based on
>>> example by Simon Wood
>>> # in his 2017 book ("Generalized additive models: an introduction with
>>> R", p. 349) and
>>> # gamair package documentation
>>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
>>>
>>> lagard <- function(x,n.lag=7) {
>>> n <- length(x); X <- matrix(NA,n,n.lag)
>>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
>>> X
>>> }
>>>
>>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
>>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
>>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
>>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
>>> = df$precip_hourly_dailysum)
>>> dat$temp_max <- lagard(df$temp_max)
>>> dat$temp_min <- lagard(df$temp_min)
>>> dat$temp_mean <- lagard(df$temp_mean)
>>> dat$wbgt_max <- lagard(df$wbgt_max)
>>> dat$wbgt_mean <- lagard(df$wbgt_mean)
>>> dat$wbgt_min <- lagard(df$wbgt_min)
>>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
>>> dat$sh_mean <- lagard(df$sh_mean)
>>> dat$solar_mean <- lagard(df$solar_mean)
>>> dat$wind2m_mean <- lagard(df$wind2m_mean)
>>> dat$sh_max <- lagard(df$sh_max)
>>> dat$solar_max <- lagard(df$solar_max)
>>> dat$wind2m_max <- lagard(df$wind2m_max)
>>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
>>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
>>> dat$precip_hourly <- lagard(df$precip_hourly)
>>> dat$precip_daily_total <- lagard( df$precip_daily_total)
>>> dat$temp <- lagard(df$temp)
>>> dat$sh <- lagard(df$sh)
>>> dat$rh <- lagard(df$rh)
>>> dat$solar <- lagard(df$solar)
>>> dat$wind2m <- lagard(df$wind2m)
>>>
>>>
>>> conc38b <- gam(deaths~te(year, month, week, weekday,
>>> bs=c("cr","cc","cc","cc")) + heap +
>>>                         te(temp_max, lag, k=c(10, 3)) +
>>>                         te(precip_daily_total, lag, k=c(10, 3)),
>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
>>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
>>> 52.5), weekday = c(0, 6.5)))
>>>
>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>>>                        te(temp_max, lag, k=c(10, 4)) +
>>>                        te(precip_daily_total, lag, k=c(10, 4)),
>>>                        data = dat, family = nb, method = 'REML', select = TRUE,
>>>                        knots = list(month = c(0.5, 12.5)))
>>>
>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>>>                         te(temp_max, lag, k=c(10,3)) +
>>>                         te(precip_daily_total, lag, k=c(10,3)),
>>>                         data = dat, family = nb, method = 'REML', select = TRUE)
>>>
>>> ```
>>> Thank you if you've read this far!! :-))
>>>
>>>     [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
>>>     [2]: https://i.stack.imgur.com/FzKyM.png
>>>     [3]: https://i.stack.imgur.com/fE3aL.png
>>>     [4]: https://i.stack.imgur.com/7nbXS.png
>>>     [5]: https://i.stack.imgur.com/pNnZU.png
>>>     [6]: https://github.com/JadeShodan/heat-mortality
>>>     [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Simon Wood, School of Mathematics, University of Edinburgh,
>> https://www.maths.ed.ac.uk/~swood34/
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From @|mon@wood @end|ng |rom b@th@edu  Thu Jun  9 16:24:51 2022
From: @|mon@wood @end|ng |rom b@th@edu (Simon Wood)
Date: Thu, 9 Jun 2022 15:24:51 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k-g=M41cxom6DVdTunFjgZ6VfKobqW__Mq351pDpY22PQ@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-g=M41cxom6DVdTunFjgZ6VfKobqW__Mq351pDpY22PQ@mail.gmail.com>
Message-ID: <f371005e-6f57-2007-f859-d66595a19c0f@bath.edu>


On 09/06/2022 12:30, jade.shodan at googlemail.com wrote:
> Hi Simon,
>
> Thanks so much for this!! (Apologies if this is a double posting. I
> seem to have a problem getting messages through to the list).
>
> I have two follow up questions, if you don't mind.
>
> 1. Does including an autoregressive term not adjust away part of the
> effect of the response in a distributed lag model (where the outcome
> accumulates over time)?

- the hope is that it approximately deals with short timescale stuff 
without interfering with the longer timescales to the same extent as a 
high rank smooth would.

> 2. I've tried to fit a model using bam (just a first attempt without
> AR term), but including the factor variable heap creates errors:
>
> bam0 <- bam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
>                                       te(temp_max, lag, k=c(8, 3)) +
>                                       te(precip_daily_total, lag, k=c(8, 3)),
>                                      data = dat, family = nb, method =
> 'fREML', select = TRUE, discrete = TRUE,
>                                      knots = list(month = c(0.5, 12.5),
> week = c(0.5, 52.5), weekday = c(0, 6.5)))
>
> This model results in errors:
>
> Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
>    step failure in theta estimation
> Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> object$prior.weights)) :
>    NaNs produced
>
Did it actually fail, or simply generate warnings?

'bam' handles factors, but if I understand your model right there is one 
free parameter for each observation falling on the 15th, so that you 
will fit data for those days exactly (and might just as well have 
dropped them, for all the information they contribute to the rest of the 
model). If you want a structure like this, I'd be inclined to make the 
heap variable random, something like...

aheap <- heap!="0"

heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap day 
+ a r.e. for each heap day - no effect on non-heap days

best,

Simon

> Including heap as as.numeric(heap) runs the model without error
> messages or warnings, but model diagnostics look terrible, and it also
> doesn't make sense (to me) to make heap a numeric. The factor variable
> heap (with 169 levels) codes the fact that all deaths for which no
> date was known, were registered on the 15th day of each month. I've
> coded all non-heaping days as 0. All heaping days were coded as a
> value between 1-168. The time series spans 14 years, so a heaping day
> in each month results in 14*12 levels = 168, plus one level for
> non-heaping days.
>
> So my second question is: Does bam allow factor variables? And if not,
> how should I model this heaping on the 15th day of the month instead?
>
> With thanks,
>
> Jade
>
> On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
>> I would not worry too much about high concurvity between variables like
>> temperature and time. This just reflects the fact that temperature has a
>> strong temporal pattern.
>>
>> I would also not be too worried about the low p-values on the k check.
>> The check only looks for pattern in the residuals when they are ordered
>> with respect to the variables of a smooth. When you have time series
>> data and some smooths involve time then it's hard not to pick up some
>> degree of residual auto-correlation, which you often would not want to
>> model with a higher rank smoother.
>>
>> The NAs  for the distributed lag terms just reflect the fact that there
>> is no obvious way to order the residuals w.r.t. the covariates for such
>> terms, so the simple check for residual pattern is not really possible.
>>
>> One simple approach is to fit using bam(...,discrete=TRUE) which will
>> let you specify an AR1 parameter to mop up some of the residual
>> auto-correlation without resorting to a high rank smooth that then does
>> all the work of the covariates as well. The AR1 parameter can be set by
>> looking at the ACF of the residuals of the model without this. You need
>> to look at the ACF of suitably standardized residuals to check how well
>> this has worked.
>>
>> best,
>>
>> Simon
>>
>> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
>>> Hello everyone,
>>>
>>> A few days ago I asked a question about concurvity in a GAM (the
>>> anologue of collinearity in a GLM) implemented in mgcv. I think my
>>> question was a bit unfocussed, so I am retrying again, but with
>>> additional information included about the autocorrelation function. I
>>> have also posted about this on Cross Validated. Given all the model
>>> output, it might make for easier
>>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
>>>
>>> As mentioned previously, I have problems with concurvity in my thesis
>>> research, and don't have access to a statistician who works with time
>>> series, GAMs or R. I'd be very grateful for any (partial) answer,
>>> however short. I'll gladly return the favour where I can! For really
>>> helpful input I'd be more than happy to offer co-authorship on
>>> publication. Deadlines are very close, and I'm heading towards having
>>> no results at all if I can't solve this concurvity issue :(
>>>
>>> I'm using GAMs to try to understand the relationship between deaths
>>> and heat-related variables (e.g. temperature and humidity), using
>>> daily time series over a 14-year period from a tropical, low-income
>>> country. My aim is to understand the relationship between these
>>> variables and deaths, rather than pure prediction performance.
>>>
>>> The GAMs include distributed lag models (set up as 7-column matrices,
>>> see code at bottom of post), since deaths may occur over several days
>>> following exposure.
>>>
>>> Simple GAMs with just time, lagged temperature and lagged
>>> precipitation (a potential confounder) show very high concurvity
>>> between lagged temperature and time, regardless of the many different
>>> ways I have tried to decompose time. The autocorrelation functions
>>> (ACF) however, shows values close to zero, only just about breaching
>>> the 'significance line' in a few instances. It does show patterning
>>> though, although the regularity is difficult to define.
>>>
>>> My questions are:
>>> 1) Should I be worried about the high concurvity, or can I ignore it
>>> given the mostly non-significant ACF? I've read dozens of
>>> heat-mortality modelling studies and none report on concurvity between
>>> weather variables and time (though one 2012 paper discussed
>>> autocorrelation).
>>>
>>> 2) If I cannot ignore it, what should I do to resolve it? Would
>>> including an autoregressive term be appropriate, and if so, where can
>>> I find a coded example of how to do this? I've also come across
>>> sequential regression][1]. Would this be more or less appropriate? If
>>> appropriate, a pointer to an example would be really appreciated!
>>>
>>> Some example GAMs are specified as follows:
>>> ```r
>>> conc38b <- gam(deaths~te(year, month, week, weekday,
>>> bs=c("cr","cc","cc","cc")) + heap +
>>>                         te(temp_max, lag, k=c(10, 3)) +
>>>                         te(precip_daily_total, lag, k=c(10, 3)),
>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
>>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
>>> 52.5), weekday = c(0, 6.5)))
>>> ```
>>> Concurvity for the above model between (temp_max, lag) and (year,
>>> month, week, weekday) is 0.91:
>>>
>>> ```r
>>> $worst
>>>                                       para te(year,month,week,weekday)
>>> te(temp_max,lag) te(precip_daily_total,lag)
>>> para                        1.000000e+00                1.125625e-29
>>>        0.3150073                  0.6666348
>>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
>>>        0.9060552                  0.6652313
>>> te(temp_max,lag)            3.152795e-01                8.998113e-01
>>>        1.0000000                  0.5781015
>>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
>>>        0.5805159                  1.0000000
>>> ```
>>>
>>> Output from ```gam.check()```:
>>> ```r
>>> Method: REML   Optimizer: outer newton
>>> full convergence after 16 iterations.
>>> Gradient range [-0.01467332,0.003096643]
>>> (score 8915.994 & scale 1).
>>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
>>> Model rank =  544 / 544
>>>
>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>> indicate that k is too low, especially if edf is close to k'.
>>>
>>>                                     k'      edf k-index p-value
>>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
>>> te(temp_max,lag)             29.0000   3.3681      NA      NA
>>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>> ```
>>>
>>> Some output from ```summary(conc38b)```:
>>> ```r
>>> Approximate significance of smooth terms:
>>>                                     edf Ref.df  Chi.sq p-value
>>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
>>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
>>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>
>>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
>>> -REML =   8916  Scale est. = 1         n = 5107
>>> ```
>>>
>>>
>>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
>>> pattern). They show peaks at 5 and 15, and then there seems to be a
>>> recurring pattern at multiples of approx. 30 (suggesting month is not
>>> modelled adequately?). Not sure what would cause the spikes at 5 and
>>> 15. There is heaping of deaths on the 15th day of each month, to which
>>> deaths with unknown date were allocated. This heaping was modelled
>>> with categorical variable/ factor ```heap``` with 169 levels (0 for
>>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
>>> heaping day over the 14-year period):
>>>
>>>     [2]: https://i.stack.imgur.com/FzKyM.png
>>>     [3]: https://i.stack.imgur.com/fE3aL.png
>>>
>>>
>>> I get an identical looking ACF when I decompose time into (year,
>>> month, monthday) as in model conc39 below, although concurvity between
>>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
>>>
>>> ```r
>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>>>                        te(temp_max, lag, k=c(10, 4)) +
>>>                        te(precip_daily_total, lag, k=c(10, 4)),
>>>                        data = dat, family = nb, method = 'REML', select = TRUE,
>>>                        knots = list(month = c(0.5, 12.5)))
>>> ```
>>> ```r
>>>
>>> Method: REML   Optimizer: outer newton
>>> full convergence after 14 iterations.
>>> Gradient range [-0.001578187,6.155096e-05]
>>> (score 8915.763 & scale 1).
>>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
>>> Model rank =  323 / 323
>>>
>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>> indicate that k is too low, especially if edf is close to k'.
>>>
>>>                                   k'     edf k-index p-value
>>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
>>> te(temp_max,lag)           39.0000  4.0875      NA      NA
>>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>> ```
>>> Some output from ```summary(conc39)```:
>>> ```r
>>> Approximate significance of smooth terms:
>>>                                   edf Ref.df  Chi.sq  p-value
>>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
>>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
>>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>
>>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
>>> -REML =   8915  Scale est. = 1         n = 5107
>>> ```
>>>
>>>
>>> ```r
>>> $worst
>>>                                      para te(year,month,monthday)
>>> te(temp_max,lag) te(precip_daily_total,lag)
>>> para                       1.000000e+00            3.261007e-31
>>> 0.3313549                  0.6666532
>>> te(year,month,monthday)    3.060763e-31            1.000000e+00
>>> 0.8266086                  0.5670777
>>> te(temp_max,lag)           3.331014e-01            8.225942e-01
>>> 1.0000000                  0.5840875
>>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
>>> 0.5939380                  1.0000000
>>> ```
>>>
>>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
>>> various choices for k or as ```s(time)``` with various k does not
>>> reduce concurvity either.
>>>
>>>
>>> The default approach in time series studies of heat-mortality is to
>>> model time with fixed df, generally between 7-10 df per year of data.
>>> I am, however, apprehensive about this approach because a) mortality
>>> profiles vary with locality due to sociodemographic and environmental
>>> characteristics and b) the choice of df is based on higher income
>>> countries (where nearly all these studies have been done) with
>>> different mortality profiles and so may not be appropriate for
>>> tropical, low-income countries.
>>>
>>> Although the approach of fixing (high) df does remove more temporal
>>> patterns from the ACF (see model and output below), concurvity between
>>> time and lagged temperature has now risen to 0.99! Moreover,
>>> temperature (which has been a consistent, highly significant predictor
>>> in every model of the tens (hundreds?) I have run, has now turned
>>> non-significant. I am guessing this is because time is now a very
>>> wiggly function that not only models/ removes seasonal variation, but
>>> also some of the day-to-day variation that is needed for the
>>> temperature smooth  :
>>>
>>> ```r
>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>>>                         te(temp_max, lag, k=c(10,3)) +
>>>                         te(precip_daily_total, lag, k=c(10,3)),
>>>                         data = dat, family = nb, method = 'REML', select = TRUE)
>>> ```
>>> Output from ```gam.check(conc20a, rep = 1000)```:
>>>
>>> ```r
>>> Method: REML   Optimizer: outer newton
>>> full convergence after 9 iterations.
>>> Gradient range [-0.0008983099,9.546022e-05]
>>> (score 8750.13 & scale 1).
>>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
>>> Model rank =  336 / 336
>>>
>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>> indicate that k is too low, especially if edf is close to k'.
>>>
>>>                                    k'      edf k-index p-value
>>> s(time)                    111.0000 111.0000    0.98    0.56
>>> te(temp_max,lag)            29.0000   0.6548      NA      NA
>>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
>>> ```
>>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
>>>
>>> ```r
>>>                                      para      s(time) te(temp_max,lag)
>>> te(precip_daily_total,lag)
>>> para                       1.000000e+00 2.462064e-19        0.3165236
>>>                   0.6666348
>>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
>>>                   0.6879284
>>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
>>>                   0.5788711
>>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
>>>                   1.0000000
>>>
>>> ```
>>>
>>> Some output from ```summary(conc20a)```:
>>> ```r
>>> Approximate significance of smooth terms:
>>>                                    edf Ref.df  Chi.sq p-value
>>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
>>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
>>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>
>>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
>>> -REML = 8750.1  Scale est. = 1         n = 5107
>>> ```
>>>
>>> ACF functions:
>>>
>>> [4]: https://i.stack.imgur.com/7nbXS.png
>>> [5]: https://i.stack.imgur.com/pNnZU.png
>>>
>>> Data can be found on my [GitHub][6] site in the file
>>> [data_cross_validated_post2.rds][7]. A csv version is also available.
>>> This is my code:
>>>
>>> ```r
>>> library(readr)
>>> library(mgcv)
>>>
>>> df <- read_rds("data_crossvalidated_post2.rds")
>>>
>>> # Create matrices for lagged weather variables (6 day lags) based on
>>> example by Simon Wood
>>> # in his 2017 book ("Generalized additive models: an introduction with
>>> R", p. 349) and
>>> # gamair package documentation
>>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
>>>
>>> lagard <- function(x,n.lag=7) {
>>> n <- length(x); X <- matrix(NA,n,n.lag)
>>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
>>> X
>>> }
>>>
>>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
>>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
>>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
>>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
>>> = df$precip_hourly_dailysum)
>>> dat$temp_max <- lagard(df$temp_max)
>>> dat$temp_min <- lagard(df$temp_min)
>>> dat$temp_mean <- lagard(df$temp_mean)
>>> dat$wbgt_max <- lagard(df$wbgt_max)
>>> dat$wbgt_mean <- lagard(df$wbgt_mean)
>>> dat$wbgt_min <- lagard(df$wbgt_min)
>>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
>>> dat$sh_mean <- lagard(df$sh_mean)
>>> dat$solar_mean <- lagard(df$solar_mean)
>>> dat$wind2m_mean <- lagard(df$wind2m_mean)
>>> dat$sh_max <- lagard(df$sh_max)
>>> dat$solar_max <- lagard(df$solar_max)
>>> dat$wind2m_max <- lagard(df$wind2m_max)
>>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
>>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
>>> dat$precip_hourly <- lagard(df$precip_hourly)
>>> dat$precip_daily_total <- lagard( df$precip_daily_total)
>>> dat$temp <- lagard(df$temp)
>>> dat$sh <- lagard(df$sh)
>>> dat$rh <- lagard(df$rh)
>>> dat$solar <- lagard(df$solar)
>>> dat$wind2m <- lagard(df$wind2m)
>>>
>>>
>>> conc38b <- gam(deaths~te(year, month, week, weekday,
>>> bs=c("cr","cc","cc","cc")) + heap +
>>>                         te(temp_max, lag, k=c(10, 3)) +
>>>                         te(precip_daily_total, lag, k=c(10, 3)),
>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
>>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
>>> 52.5), weekday = c(0, 6.5)))
>>>
>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>>>                        te(temp_max, lag, k=c(10, 4)) +
>>>                        te(precip_daily_total, lag, k=c(10, 4)),
>>>                        data = dat, family = nb, method = 'REML', select = TRUE,
>>>                        knots = list(month = c(0.5, 12.5)))
>>>
>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>>>                         te(temp_max, lag, k=c(10,3)) +
>>>                         te(precip_daily_total, lag, k=c(10,3)),
>>>                         data = dat, family = nb, method = 'REML', select = TRUE)
>>>
>>> ```
>>> Thank you if you've read this far!! :-))
>>>
>>>     [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
>>>     [2]: https://i.stack.imgur.com/FzKyM.png
>>>     [3]: https://i.stack.imgur.com/fE3aL.png
>>>     [4]: https://i.stack.imgur.com/7nbXS.png
>>>     [5]: https://i.stack.imgur.com/pNnZU.png
>>>     [6]: https://github.com/JadeShodan/heat-mortality
>>>     [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> --
>> Simon Wood, School of Mathematics, University of Edinburgh,
>> https://www.maths.ed.ac.uk/~swood34/
>>
-- 
Simon Wood, School of Mathematics, University of Edinburgh,
https://www.maths.ed.ac.uk/~swood34/


From j@de@shod@@ m@iii@g oii googiem@ii@com  Thu Jun  9 13:30:59 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Thu, 9 Jun 2022 12:30:59 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
Message-ID: <CANg3_k-g=M41cxom6DVdTunFjgZ6VfKobqW__Mq351pDpY22PQ@mail.gmail.com>

Hi Simon,

Thanks so much for this!! (Apologies if this is a double posting. I
seem to have a problem getting messages through to the list).

I have two follow up questions, if you don't mind.

1. Does including an autoregressive term not adjust away part of the
effect of the response in a distributed lag model (where the outcome
accumulates over time)?
2. I've tried to fit a model using bam (just a first attempt without
AR term), but including the factor variable heap creates errors:

bam0 <- bam(deaths~te(year, month, week, weekday,
bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
                                     te(temp_max, lag, k=c(8, 3)) +
                                     te(precip_daily_total, lag, k=c(8, 3)),
                                    data = dat, family = nb, method =
'fREML', select = TRUE, discrete = TRUE,
                                    knots = list(month = c(0.5, 12.5),
week = c(0.5, 52.5), weekday = c(0, 6.5)))

This model results in errors:

Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
  step failure in theta estimation
Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
object$prior.weights)) :
  NaNs produced


Including heap as as.numeric(heap) runs the model without error
messages or warnings, but model diagnostics look terrible, and it also
doesn't make sense (to me) to make heap a numeric. The factor variable
heap (with 169 levels) codes the fact that all deaths for which no
date was known, were registered on the 15th day of each month. I've
coded all non-heaping days as 0. All heaping days were coded as a
value between 1-168. The time series spans 14 years, so a heaping day
in each month results in 14*12 levels = 168, plus one level for
non-heaping days.

So my second question is: Does bam allow factor variables? And if not,
how should I model this heaping on the 15th day of the month instead?

With thanks,

Jade

On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
>
> I would not worry too much about high concurvity between variables like
> temperature and time. This just reflects the fact that temperature has a
> strong temporal pattern.
>
> I would also not be too worried about the low p-values on the k check.
> The check only looks for pattern in the residuals when they are ordered
> with respect to the variables of a smooth. When you have time series
> data and some smooths involve time then it's hard not to pick up some
> degree of residual auto-correlation, which you often would not want to
> model with a higher rank smoother.
>
> The NAs  for the distributed lag terms just reflect the fact that there
> is no obvious way to order the residuals w.r.t. the covariates for such
> terms, so the simple check for residual pattern is not really possible.
>
> One simple approach is to fit using bam(...,discrete=TRUE) which will
> let you specify an AR1 parameter to mop up some of the residual
> auto-correlation without resorting to a high rank smooth that then does
> all the work of the covariates as well. The AR1 parameter can be set by
> looking at the ACF of the residuals of the model without this. You need
> to look at the ACF of suitably standardized residuals to check how well
> this has worked.
>
> best,
>
> Simon
>
> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> > Hello everyone,
> >
> > A few days ago I asked a question about concurvity in a GAM (the
> > anologue of collinearity in a GLM) implemented in mgcv. I think my
> > question was a bit unfocussed, so I am retrying again, but with
> > additional information included about the autocorrelation function. I
> > have also posted about this on Cross Validated. Given all the model
> > output, it might make for easier
> > reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> >
> > As mentioned previously, I have problems with concurvity in my thesis
> > research, and don't have access to a statistician who works with time
> > series, GAMs or R. I'd be very grateful for any (partial) answer,
> > however short. I'll gladly return the favour where I can! For really
> > helpful input I'd be more than happy to offer co-authorship on
> > publication. Deadlines are very close, and I'm heading towards having
> > no results at all if I can't solve this concurvity issue :(
> >
> > I'm using GAMs to try to understand the relationship between deaths
> > and heat-related variables (e.g. temperature and humidity), using
> > daily time series over a 14-year period from a tropical, low-income
> > country. My aim is to understand the relationship between these
> > variables and deaths, rather than pure prediction performance.
> >
> > The GAMs include distributed lag models (set up as 7-column matrices,
> > see code at bottom of post), since deaths may occur over several days
> > following exposure.
> >
> > Simple GAMs with just time, lagged temperature and lagged
> > precipitation (a potential confounder) show very high concurvity
> > between lagged temperature and time, regardless of the many different
> > ways I have tried to decompose time. The autocorrelation functions
> > (ACF) however, shows values close to zero, only just about breaching
> > the 'significance line' in a few instances. It does show patterning
> > though, although the regularity is difficult to define.
> >
> > My questions are:
> > 1) Should I be worried about the high concurvity, or can I ignore it
> > given the mostly non-significant ACF? I've read dozens of
> > heat-mortality modelling studies and none report on concurvity between
> > weather variables and time (though one 2012 paper discussed
> > autocorrelation).
> >
> > 2) If I cannot ignore it, what should I do to resolve it? Would
> > including an autoregressive term be appropriate, and if so, where can
> > I find a coded example of how to do this? I've also come across
> > sequential regression][1]. Would this be more or less appropriate? If
> > appropriate, a pointer to an example would be really appreciated!
> >
> > Some example GAMs are specified as follows:
> > ```r
> > conc38b <- gam(deaths~te(year, month, week, weekday,
> > bs=c("cr","cc","cc","cc")) + heap +
> >                        te(temp_max, lag, k=c(10, 3)) +
> >                        te(precip_daily_total, lag, k=c(10, 3)),
> >                        data = dat, family = nb, method = 'REML', select = TRUE,
> >                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> > 52.5), weekday = c(0, 6.5)))
> > ```
> > Concurvity for the above model between (temp_max, lag) and (year,
> > month, week, weekday) is 0.91:
> >
> > ```r
> > $worst
> >                                      para te(year,month,week,weekday)
> > te(temp_max,lag) te(precip_daily_total,lag)
> > para                        1.000000e+00                1.125625e-29
> >       0.3150073                  0.6666348
> > te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> >       0.9060552                  0.6652313
> > te(temp_max,lag)            3.152795e-01                8.998113e-01
> >       1.0000000                  0.5781015
> > te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> >       0.5805159                  1.0000000
> > ```
> >
> > Output from ```gam.check()```:
> > ```r
> > Method: REML   Optimizer: outer newton
> > full convergence after 16 iterations.
> > Gradient range [-0.01467332,0.003096643]
> > (score 8915.994 & scale 1).
> > Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> > Model rank =  544 / 544
> >
> > Basis dimension (k) checking results. Low p-value (k-index<1) may
> > indicate that k is too low, especially if edf is close to k'.
> >
> >                                    k'      edf k-index p-value
> > te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> > te(temp_max,lag)             29.0000   3.3681      NA      NA
> > te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > ```
> >
> > Some output from ```summary(conc38b)```:
> > ```r
> > Approximate significance of smooth terms:
> >                                    edf Ref.df  Chi.sq p-value
> > te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> > te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> > te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >
> > R-sq.(adj) =  0.839   Deviance explained = 53.3%
> > -REML =   8916  Scale est. = 1         n = 5107
> > ```
> >
> >
> > Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> > pattern). They show peaks at 5 and 15, and then there seems to be a
> > recurring pattern at multiples of approx. 30 (suggesting month is not
> > modelled adequately?). Not sure what would cause the spikes at 5 and
> > 15. There is heaping of deaths on the 15th day of each month, to which
> > deaths with unknown date were allocated. This heaping was modelled
> > with categorical variable/ factor ```heap``` with 169 levels (0 for
> > all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> > heaping day over the 14-year period):
> >
> >    [2]: https://i.stack.imgur.com/FzKyM.png
> >    [3]: https://i.stack.imgur.com/fE3aL.png
> >
> >
> > I get an identical looking ACF when I decompose time into (year,
> > month, monthday) as in model conc39 below, although concurvity between
> > (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> >
> > ```r
> > conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >                       te(temp_max, lag, k=c(10, 4)) +
> >                       te(precip_daily_total, lag, k=c(10, 4)),
> >                       data = dat, family = nb, method = 'REML', select = TRUE,
> >                       knots = list(month = c(0.5, 12.5)))
> > ```
> > ```r
> >
> > Method: REML   Optimizer: outer newton
> > full convergence after 14 iterations.
> > Gradient range [-0.001578187,6.155096e-05]
> > (score 8915.763 & scale 1).
> > Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> > Model rank =  323 / 323
> >
> > Basis dimension (k) checking results. Low p-value (k-index<1) may
> > indicate that k is too low, especially if edf is close to k'.
> >
> >                                  k'     edf k-index p-value
> > te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> > te(temp_max,lag)           39.0000  4.0875      NA      NA
> > te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > ```
> > Some output from ```summary(conc39)```:
> > ```r
> > Approximate significance of smooth terms:
> >                                  edf Ref.df  Chi.sq  p-value
> > te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> > te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> > te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >
> > R-sq.(adj) =  0.839   Deviance explained = 53.8%
> > -REML =   8915  Scale est. = 1         n = 5107
> > ```
> >
> >
> > ```r
> > $worst
> >                                     para te(year,month,monthday)
> > te(temp_max,lag) te(precip_daily_total,lag)
> > para                       1.000000e+00            3.261007e-31
> > 0.3313549                  0.6666532
> > te(year,month,monthday)    3.060763e-31            1.000000e+00
> > 0.8266086                  0.5670777
> > te(temp_max,lag)           3.331014e-01            8.225942e-01
> > 1.0000000                  0.5840875
> > te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> > 0.5939380                  1.0000000
> > ```
> >
> > Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> > various choices for k or as ```s(time)``` with various k does not
> > reduce concurvity either.
> >
> >
> > The default approach in time series studies of heat-mortality is to
> > model time with fixed df, generally between 7-10 df per year of data.
> > I am, however, apprehensive about this approach because a) mortality
> > profiles vary with locality due to sociodemographic and environmental
> > characteristics and b) the choice of df is based on higher income
> > countries (where nearly all these studies have been done) with
> > different mortality profiles and so may not be appropriate for
> > tropical, low-income countries.
> >
> > Although the approach of fixing (high) df does remove more temporal
> > patterns from the ACF (see model and output below), concurvity between
> > time and lagged temperature has now risen to 0.99! Moreover,
> > temperature (which has been a consistent, highly significant predictor
> > in every model of the tens (hundreds?) I have run, has now turned
> > non-significant. I am guessing this is because time is now a very
> > wiggly function that not only models/ removes seasonal variation, but
> > also some of the day-to-day variation that is needed for the
> > temperature smooth  :
> >
> > ```r
> > conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >                        te(temp_max, lag, k=c(10,3)) +
> >                        te(precip_daily_total, lag, k=c(10,3)),
> >                        data = dat, family = nb, method = 'REML', select = TRUE)
> > ```
> > Output from ```gam.check(conc20a, rep = 1000)```:
> >
> > ```r
> > Method: REML   Optimizer: outer newton
> > full convergence after 9 iterations.
> > Gradient range [-0.0008983099,9.546022e-05]
> > (score 8750.13 & scale 1).
> > Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> > Model rank =  336 / 336
> >
> > Basis dimension (k) checking results. Low p-value (k-index<1) may
> > indicate that k is too low, especially if edf is close to k'.
> >
> >                                   k'      edf k-index p-value
> > s(time)                    111.0000 111.0000    0.98    0.56
> > te(temp_max,lag)            29.0000   0.6548      NA      NA
> > te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> > ```
> > Output from ```concurvity(conc20a, full=FALSE)$worst```:
> >
> > ```r
> >                                     para      s(time) te(temp_max,lag)
> > te(precip_daily_total,lag)
> > para                       1.000000e+00 2.462064e-19        0.3165236
> >                  0.6666348
> > s(time)                    2.462398e-19 1.000000e+00        0.9930674
> >                  0.6879284
> > te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> >                  0.5788711
> > te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> >                  1.0000000
> >
> > ```
> >
> > Some output from ```summary(conc20a)```:
> > ```r
> > Approximate significance of smooth terms:
> >                                   edf Ref.df  Chi.sq p-value
> > s(time)                    1.110e+02    111 419.375  <2e-16 ***
> > te(temp_max,lag)           6.548e-01     27   0.895   0.249
> > te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >
> > R-sq.(adj) =  0.843   Deviance explained = 56.1%
> > -REML = 8750.1  Scale est. = 1         n = 5107
> > ```
> >
> > ACF functions:
> >
> > [4]: https://i.stack.imgur.com/7nbXS.png
> > [5]: https://i.stack.imgur.com/pNnZU.png
> >
> > Data can be found on my [GitHub][6] site in the file
> > [data_cross_validated_post2.rds][7]. A csv version is also available.
> > This is my code:
> >
> > ```r
> > library(readr)
> > library(mgcv)
> >
> > df <- read_rds("data_crossvalidated_post2.rds")
> >
> > # Create matrices for lagged weather variables (6 day lags) based on
> > example by Simon Wood
> > # in his 2017 book ("Generalized additive models: an introduction with
> > R", p. 349) and
> > # gamair package documentation
> > (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> >
> > lagard <- function(x,n.lag=7) {
> > n <- length(x); X <- matrix(NA,n,n.lag)
> > for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > X
> > }
> >
> > dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> > deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> > weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> > df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> > = df$precip_hourly_dailysum)
> > dat$temp_max <- lagard(df$temp_max)
> > dat$temp_min <- lagard(df$temp_min)
> > dat$temp_mean <- lagard(df$temp_mean)
> > dat$wbgt_max <- lagard(df$wbgt_max)
> > dat$wbgt_mean <- lagard(df$wbgt_mean)
> > dat$wbgt_min <- lagard(df$wbgt_min)
> > dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> > dat$sh_mean <- lagard(df$sh_mean)
> > dat$solar_mean <- lagard(df$solar_mean)
> > dat$wind2m_mean <- lagard(df$wind2m_mean)
> > dat$sh_max <- lagard(df$sh_max)
> > dat$solar_max <- lagard(df$solar_max)
> > dat$wind2m_max <- lagard(df$wind2m_max)
> > dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> > dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> > dat$precip_hourly <- lagard(df$precip_hourly)
> > dat$precip_daily_total <- lagard( df$precip_daily_total)
> > dat$temp <- lagard(df$temp)
> > dat$sh <- lagard(df$sh)
> > dat$rh <- lagard(df$rh)
> > dat$solar <- lagard(df$solar)
> > dat$wind2m <- lagard(df$wind2m)
> >
> >
> > conc38b <- gam(deaths~te(year, month, week, weekday,
> > bs=c("cr","cc","cc","cc")) + heap +
> >                        te(temp_max, lag, k=c(10, 3)) +
> >                        te(precip_daily_total, lag, k=c(10, 3)),
> >                        data = dat, family = nb, method = 'REML', select = TRUE,
> >                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> > 52.5), weekday = c(0, 6.5)))
> >
> > conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >                       te(temp_max, lag, k=c(10, 4)) +
> >                       te(precip_daily_total, lag, k=c(10, 4)),
> >                       data = dat, family = nb, method = 'REML', select = TRUE,
> >                       knots = list(month = c(0.5, 12.5)))
> >
> > conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >                        te(temp_max, lag, k=c(10,3)) +
> >                        te(precip_daily_total, lag, k=c(10,3)),
> >                        data = dat, family = nb, method = 'REML', select = TRUE)
> >
> > ```
> > Thank you if you've read this far!! :-))
> >
> >    [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> >    [2]: https://i.stack.imgur.com/FzKyM.png
> >    [3]: https://i.stack.imgur.com/fE3aL.png
> >    [4]: https://i.stack.imgur.com/7nbXS.png
> >    [5]: https://i.stack.imgur.com/pNnZU.png
> >    [6]: https://github.com/JadeShodan/heat-mortality
> >    [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Simon Wood, School of Mathematics, University of Edinburgh,
> https://www.maths.ed.ac.uk/~swood34/
>


From j@de@shod@@ m@iii@g oii googiem@ii@com  Thu Jun  9 16:27:04 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Thu, 9 Jun 2022 15:27:04 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <91a30ffd-1aec-0f44-17a0-b632fae91014@dewey.myzen.co.uk>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-EXdbmLVNbEH9nN=FE3FYs4uqXgVtH3V35o6BU0z=rTw@mail.gmail.com>
 <91a30ffd-1aec-0f44-17a0-b632fae91014@dewey.myzen.co.uk>
Message-ID: <CANg3_k-0=Z4KUE52mxHrSj4c+9bhJ+C4HDqxM2HqqVJ9um0WBQ@mail.gmail.com>

Hi Michael,

Thanks for the reply! When I ran the gam  with the gam() function, the
model worked fine with heap having 169 levels. The same model with
bam() however, fails.  I don't understand the difference between bam()
and gam() at all (other than computational efficiency), but could the
fact that each level only has 1 data point be the reason for it?

These heaping days are very large measurement errors I need to get rid
off, so I just want to take them out of the model altogether. (My data
is quite noisy already, because date of death is based on memory
recall, rather than formal death registration, due to the data being
from a low income country). Median of deaths is approx. 2 per day, but
on heaping days it can be as high as 50 or so.

My understanding was that coding with 169 levels would effectively
take these measurements out of the model (but do correct me if I'm
wrong!)  I originally coded heap as a binary variable with 0 for
non-heaping days and 1 for heaping days, but was told that meant that
I was assuming the effect was the same for all heaping days. If I
coded with 12 or 14 levels, wouldn't that leave a lot of noise in the
data?

Jade

On Thu, 9 Jun 2022 at 14:52, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>
> Dear Jade
>
> Do you really need to fit a separate parameter for each heaping day? Can
> you not just make it a binary predictor or a categorical one with fewer
> levels, perhaps 14 (for heaping in each year) or 12 (for each calendar
> month). I have no idea whether that would help but it seems worth a try.
>
> Michael
>
> On 08/06/2022 18:15, jade.shodan--- via R-help wrote:
> > Hi Simon,
> >
> > Thanks so much for this!! I have two follow up questions, if you don't mind.
> >
> > 1. Does including an autoregressive term not adjust away part of the
> > effect of the response in a distributed lag model (where the outcome
> > accumulates over time)?
> > 2. I've tried to fit a model using bam (just a first attempt without
> > AR term), but including the factor variable heap creates errors:
> >
> > bam0 <- bam(deaths~te(year, month, week, weekday,
> > bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> >                        te(temp_max, lag, k=c(8, 3)) +
> >                        te(precip_daily_total, lag, k=c(8, 3)),
> >                        data = dat, family = nb, method = 'fREML',
> > select = TRUE, discrete = TRUE,
> >                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> > 52.5), weekday = c(0, 6.5)))
> >
> > This model results in errors:
> >
> > Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
> >    step failure in theta estimation
> > Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> > object$prior.weights)) :
> >    NaNs produced
> >
> >
> > Including heap as as.numeric(heap) runs the model without error
> > messages or warnings, but model diagnostics look terrible, and it also
> > doesn't make sense (to me) to make heap a numeric. The factor variable
> > heap (with 169 levels) codes the fact that all deaths for which no
> > date was known, were registered on the 15th day of each month. I've
> > coded all non-heaping days as 0. All heaping days were coded as a
> > value between 1-168. The time series spans 14 years, so a heaping day
> > in each month results in 14*12 levels = 168, plus one level for
> > non-heaping days.
> >
> > So my second question is: Does bam allow factor variables? And if not,
> > how should I model this heaping on the 15th day of the month instead?
> >
> > With thanks,
> >
> > Jade
> >
> > On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
> >>
> >> I would not worry too much about high concurvity between variables like
> >> temperature and time. This just reflects the fact that temperature has a
> >> strong temporal pattern.
> >>
> >> I would also not be too worried about the low p-values on the k check.
> >> The check only looks for pattern in the residuals when they are ordered
> >> with respect to the variables of a smooth. When you have time series
> >> data and some smooths involve time then it's hard not to pick up some
> >> degree of residual auto-correlation, which you often would not want to
> >> model with a higher rank smoother.
> >>
> >> The NAs  for the distributed lag terms just reflect the fact that there
> >> is no obvious way to order the residuals w.r.t. the covariates for such
> >> terms, so the simple check for residual pattern is not really possible.
> >>
> >> One simple approach is to fit using bam(...,discrete=TRUE) which will
> >> let you specify an AR1 parameter to mop up some of the residual
> >> auto-correlation without resorting to a high rank smooth that then does
> >> all the work of the covariates as well. The AR1 parameter can be set by
> >> looking at the ACF of the residuals of the model without this. You need
> >> to look at the ACF of suitably standardized residuals to check how well
> >> this has worked.
> >>
> >> best,
> >>
> >> Simon
> >>
> >> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> >>> Hello everyone,
> >>>
> >>> A few days ago I asked a question about concurvity in a GAM (the
> >>> anologue of collinearity in a GLM) implemented in mgcv. I think my
> >>> question was a bit unfocussed, so I am retrying again, but with
> >>> additional information included about the autocorrelation function. I
> >>> have also posted about this on Cross Validated. Given all the model
> >>> output, it might make for easier
> >>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> >>>
> >>> As mentioned previously, I have problems with concurvity in my thesis
> >>> research, and don't have access to a statistician who works with time
> >>> series, GAMs or R. I'd be very grateful for any (partial) answer,
> >>> however short. I'll gladly return the favour where I can! For really
> >>> helpful input I'd be more than happy to offer co-authorship on
> >>> publication. Deadlines are very close, and I'm heading towards having
> >>> no results at all if I can't solve this concurvity issue :(
> >>>
> >>> I'm using GAMs to try to understand the relationship between deaths
> >>> and heat-related variables (e.g. temperature and humidity), using
> >>> daily time series over a 14-year period from a tropical, low-income
> >>> country. My aim is to understand the relationship between these
> >>> variables and deaths, rather than pure prediction performance.
> >>>
> >>> The GAMs include distributed lag models (set up as 7-column matrices,
> >>> see code at bottom of post), since deaths may occur over several days
> >>> following exposure.
> >>>
> >>> Simple GAMs with just time, lagged temperature and lagged
> >>> precipitation (a potential confounder) show very high concurvity
> >>> between lagged temperature and time, regardless of the many different
> >>> ways I have tried to decompose time. The autocorrelation functions
> >>> (ACF) however, shows values close to zero, only just about breaching
> >>> the 'significance line' in a few instances. It does show patterning
> >>> though, although the regularity is difficult to define.
> >>>
> >>> My questions are:
> >>> 1) Should I be worried about the high concurvity, or can I ignore it
> >>> given the mostly non-significant ACF? I've read dozens of
> >>> heat-mortality modelling studies and none report on concurvity between
> >>> weather variables and time (though one 2012 paper discussed
> >>> autocorrelation).
> >>>
> >>> 2) If I cannot ignore it, what should I do to resolve it? Would
> >>> including an autoregressive term be appropriate, and if so, where can
> >>> I find a coded example of how to do this? I've also come across
> >>> sequential regression][1]. Would this be more or less appropriate? If
> >>> appropriate, a pointer to an example would be really appreciated!
> >>>
> >>> Some example GAMs are specified as follows:
> >>> ```r
> >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> >>> bs=c("cr","cc","cc","cc")) + heap +
> >>>                         te(temp_max, lag, k=c(10, 3)) +
> >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>> 52.5), weekday = c(0, 6.5)))
> >>> ```
> >>> Concurvity for the above model between (temp_max, lag) and (year,
> >>> month, week, weekday) is 0.91:
> >>>
> >>> ```r
> >>> $worst
> >>>                                       para te(year,month,week,weekday)
> >>> te(temp_max,lag) te(precip_daily_total,lag)
> >>> para                        1.000000e+00                1.125625e-29
> >>>        0.3150073                  0.6666348
> >>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> >>>        0.9060552                  0.6652313
> >>> te(temp_max,lag)            3.152795e-01                8.998113e-01
> >>>        1.0000000                  0.5781015
> >>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> >>>        0.5805159                  1.0000000
> >>> ```
> >>>
> >>> Output from ```gam.check()```:
> >>> ```r
> >>> Method: REML   Optimizer: outer newton
> >>> full convergence after 16 iterations.
> >>> Gradient range [-0.01467332,0.003096643]
> >>> (score 8915.994 & scale 1).
> >>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> >>> Model rank =  544 / 544
> >>>
> >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>> indicate that k is too low, especially if edf is close to k'.
> >>>
> >>>                                     k'      edf k-index p-value
> >>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> >>> te(temp_max,lag)             29.0000   3.3681      NA      NA
> >>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>> ```
> >>>
> >>> Some output from ```summary(conc38b)```:
> >>> ```r
> >>> Approximate significance of smooth terms:
> >>>                                     edf Ref.df  Chi.sq p-value
> >>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> >>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> >>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>
> >>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> >>> -REML =   8916  Scale est. = 1         n = 5107
> >>> ```
> >>>
> >>>
> >>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> >>> pattern). They show peaks at 5 and 15, and then there seems to be a
> >>> recurring pattern at multiples of approx. 30 (suggesting month is not
> >>> modelled adequately?). Not sure what would cause the spikes at 5 and
> >>> 15. There is heaping of deaths on the 15th day of each month, to which
> >>> deaths with unknown date were allocated. This heaping was modelled
> >>> with categorical variable/ factor ```heap``` with 169 levels (0 for
> >>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> >>> heaping day over the 14-year period):
> >>>
> >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> >>>
> >>>
> >>> I get an identical looking ACF when I decompose time into (year,
> >>> month, monthday) as in model conc39 below, although concurvity between
> >>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> >>>
> >>> ```r
> >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >>>                        te(temp_max, lag, k=c(10, 4)) +
> >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> >>>                        knots = list(month = c(0.5, 12.5)))
> >>> ```
> >>> ```r
> >>>
> >>> Method: REML   Optimizer: outer newton
> >>> full convergence after 14 iterations.
> >>> Gradient range [-0.001578187,6.155096e-05]
> >>> (score 8915.763 & scale 1).
> >>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> >>> Model rank =  323 / 323
> >>>
> >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>> indicate that k is too low, especially if edf is close to k'.
> >>>
> >>>                                   k'     edf k-index p-value
> >>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> >>> te(temp_max,lag)           39.0000  4.0875      NA      NA
> >>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>> ```
> >>> Some output from ```summary(conc39)```:
> >>> ```r
> >>> Approximate significance of smooth terms:
> >>>                                   edf Ref.df  Chi.sq  p-value
> >>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> >>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> >>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>
> >>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> >>> -REML =   8915  Scale est. = 1         n = 5107
> >>> ```
> >>>
> >>>
> >>> ```r
> >>> $worst
> >>>                                      para te(year,month,monthday)
> >>> te(temp_max,lag) te(precip_daily_total,lag)
> >>> para                       1.000000e+00            3.261007e-31
> >>> 0.3313549                  0.6666532
> >>> te(year,month,monthday)    3.060763e-31            1.000000e+00
> >>> 0.8266086                  0.5670777
> >>> te(temp_max,lag)           3.331014e-01            8.225942e-01
> >>> 1.0000000                  0.5840875
> >>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> >>> 0.5939380                  1.0000000
> >>> ```
> >>>
> >>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> >>> various choices for k or as ```s(time)``` with various k does not
> >>> reduce concurvity either.
> >>>
> >>>
> >>> The default approach in time series studies of heat-mortality is to
> >>> model time with fixed df, generally between 7-10 df per year of data.
> >>> I am, however, apprehensive about this approach because a) mortality
> >>> profiles vary with locality due to sociodemographic and environmental
> >>> characteristics and b) the choice of df is based on higher income
> >>> countries (where nearly all these studies have been done) with
> >>> different mortality profiles and so may not be appropriate for
> >>> tropical, low-income countries.
> >>>
> >>> Although the approach of fixing (high) df does remove more temporal
> >>> patterns from the ACF (see model and output below), concurvity between
> >>> time and lagged temperature has now risen to 0.99! Moreover,
> >>> temperature (which has been a consistent, highly significant predictor
> >>> in every model of the tens (hundreds?) I have run, has now turned
> >>> non-significant. I am guessing this is because time is now a very
> >>> wiggly function that not only models/ removes seasonal variation, but
> >>> also some of the day-to-day variation that is needed for the
> >>> temperature smooth  :
> >>>
> >>> ```r
> >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >>>                         te(temp_max, lag, k=c(10,3)) +
> >>>                         te(precip_daily_total, lag, k=c(10,3)),
> >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> >>> ```
> >>> Output from ```gam.check(conc20a, rep = 1000)```:
> >>>
> >>> ```r
> >>> Method: REML   Optimizer: outer newton
> >>> full convergence after 9 iterations.
> >>> Gradient range [-0.0008983099,9.546022e-05]
> >>> (score 8750.13 & scale 1).
> >>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> >>> Model rank =  336 / 336
> >>>
> >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>> indicate that k is too low, especially if edf is close to k'.
> >>>
> >>>                                    k'      edf k-index p-value
> >>> s(time)                    111.0000 111.0000    0.98    0.56
> >>> te(temp_max,lag)            29.0000   0.6548      NA      NA
> >>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> >>> ```
> >>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
> >>>
> >>> ```r
> >>>                                      para      s(time) te(temp_max,lag)
> >>> te(precip_daily_total,lag)
> >>> para                       1.000000e+00 2.462064e-19        0.3165236
> >>>                   0.6666348
> >>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
> >>>                   0.6879284
> >>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> >>>                   0.5788711
> >>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> >>>                   1.0000000
> >>>
> >>> ```
> >>>
> >>> Some output from ```summary(conc20a)```:
> >>> ```r
> >>> Approximate significance of smooth terms:
> >>>                                    edf Ref.df  Chi.sq p-value
> >>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> >>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> >>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>
> >>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> >>> -REML = 8750.1  Scale est. = 1         n = 5107
> >>> ```
> >>>
> >>> ACF functions:
> >>>
> >>> [4]: https://i.stack.imgur.com/7nbXS.png
> >>> [5]: https://i.stack.imgur.com/pNnZU.png
> >>>
> >>> Data can be found on my [GitHub][6] site in the file
> >>> [data_cross_validated_post2.rds][7]. A csv version is also available.
> >>> This is my code:
> >>>
> >>> ```r
> >>> library(readr)
> >>> library(mgcv)
> >>>
> >>> df <- read_rds("data_crossvalidated_post2.rds")
> >>>
> >>> # Create matrices for lagged weather variables (6 day lags) based on
> >>> example by Simon Wood
> >>> # in his 2017 book ("Generalized additive models: an introduction with
> >>> R", p. 349) and
> >>> # gamair package documentation
> >>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> >>>
> >>> lagard <- function(x,n.lag=7) {
> >>> n <- length(x); X <- matrix(NA,n,n.lag)
> >>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> >>> X
> >>> }
> >>>
> >>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> >>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> >>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> >>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> >>> = df$precip_hourly_dailysum)
> >>> dat$temp_max <- lagard(df$temp_max)
> >>> dat$temp_min <- lagard(df$temp_min)
> >>> dat$temp_mean <- lagard(df$temp_mean)
> >>> dat$wbgt_max <- lagard(df$wbgt_max)
> >>> dat$wbgt_mean <- lagard(df$wbgt_mean)
> >>> dat$wbgt_min <- lagard(df$wbgt_min)
> >>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> >>> dat$sh_mean <- lagard(df$sh_mean)
> >>> dat$solar_mean <- lagard(df$solar_mean)
> >>> dat$wind2m_mean <- lagard(df$wind2m_mean)
> >>> dat$sh_max <- lagard(df$sh_max)
> >>> dat$solar_max <- lagard(df$solar_max)
> >>> dat$wind2m_max <- lagard(df$wind2m_max)
> >>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> >>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> >>> dat$precip_hourly <- lagard(df$precip_hourly)
> >>> dat$precip_daily_total <- lagard( df$precip_daily_total)
> >>> dat$temp <- lagard(df$temp)
> >>> dat$sh <- lagard(df$sh)
> >>> dat$rh <- lagard(df$rh)
> >>> dat$solar <- lagard(df$solar)
> >>> dat$wind2m <- lagard(df$wind2m)
> >>>
> >>>
> >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> >>> bs=c("cr","cc","cc","cc")) + heap +
> >>>                         te(temp_max, lag, k=c(10, 3)) +
> >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>> 52.5), weekday = c(0, 6.5)))
> >>>
> >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >>>                        te(temp_max, lag, k=c(10, 4)) +
> >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> >>>                        knots = list(month = c(0.5, 12.5)))
> >>>
> >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >>>                         te(temp_max, lag, k=c(10,3)) +
> >>>                         te(precip_daily_total, lag, k=c(10,3)),
> >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> >>>
> >>> ```
> >>> Thank you if you've read this far!! :-))
> >>>
> >>>     [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> >>>     [4]: https://i.stack.imgur.com/7nbXS.png
> >>>     [5]: https://i.stack.imgur.com/pNnZU.png
> >>>     [6]: https://github.com/JadeShodan/heat-mortality
> >>>     [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Simon Wood, School of Mathematics, University of Edinburgh,
> >> https://www.maths.ed.ac.uk/~swood34/
> >>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html


From j@de@shod@@ m@iii@g oii googiem@ii@com  Thu Jun  9 17:57:45 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Thu, 9 Jun 2022 16:57:45 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <f371005e-6f57-2007-f859-d66595a19c0f@bath.edu>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-g=M41cxom6DVdTunFjgZ6VfKobqW__Mq351pDpY22PQ@mail.gmail.com>
 <f371005e-6f57-2007-f859-d66595a19c0f@bath.edu>
Message-ID: <CANg3_k_qMX24_a-Hppk6knrH-nt1QMLj2FFSJbkMJc0TBxqzQA@mail.gmail.com>

Hi Simon,

(Sorry, replies and answers are out of sync due to my problems posting
to the list/ messages being held for moderation)

> Did it actually fail, or simply generate warnings?
The model was computed (you're right, not a model failure), but
resulted in warnings (as per previous post) which, frankly, I didn't
understand.

> if I understand your model right there is one free parameter for each observation falling on the 15th
That's right, one observation on each 15th day of the month.

Thank you for the suggestion about the random effects! I had been
wondering about how I could model this heaping with a smooth!

Quick question:

You proposed:

aheap <- heap!="0"
heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap
day + a r.e. for each heap day - no effect on non-heap days

Is there a typo in the code above? I don't see the newly created
variable aheap in the model?
Should it read "by = aheap" as follows:

heap + s(heap,bs="re",by=aheap)   ?

So a full model might then look like the one below?

 bam0 <- bam(deaths~te(year, month, week, weekday,
bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
s(heap,bs="re",by=aheap)
                                     te(temp_max, lag, k=c(8, 3)) +
                                     te(precip_daily_total, lag, k=c(8, 3)),
                                     data = dat, family = nb, method =
'fREML', select = TRUE, discrete = TRUE,
                                   knots = list(month = c(0.5, 12.5),
week = c(0.5, 52.5), weekday = c(0, 6.5)))

One, hopefully final (!) question:

Is it actually useful at all to keep these observations on the 15th
day of each month (which are huge errors), or am I better off removing
them from the data set (or replacing them with e.g. median values)?
For temperature-mortality modelling it is the day-to-day variation in
deaths and temperature that is of interest. So is modelling heaping
actually useful at all, given that this variable changes on a monthly
basis? (I think you are alluding to this, but I just want to make
sure).

If I take them out altogether, would I be best off removing all data
for these dates, so that the time series jumps from day 14 to day 16?
Or would this create problems with e.g. the distributed lag model?

Sorry for all these questions! Have been struggling with this for
months (posted on Cross Validated about the heaping issue too), and
feel I am finally getting somewhere with your help!

Jade

On Thu, 9 Jun 2022 at 15:24, Simon Wood <simon.wood at bath.edu> wrote:
>
>
> On 09/06/2022 12:30, jade.shodan at googlemail.com wrote:
> > Hi Simon,
> >
> > Thanks so much for this!! (Apologies if this is a double posting. I
> > seem to have a problem getting messages through to the list).
> >
> > I have two follow up questions, if you don't mind.
> >
> > 1. Does including an autoregressive term not adjust away part of the
> > effect of the response in a distributed lag model (where the outcome
> > accumulates over time)?
>
> - the hope is that it approximately deals with short timescale stuff
> without interfering with the longer timescales to the same extent as a
> high rank smooth would.
>
> > 2. I've tried to fit a model using bam (just a first attempt without
> > AR term), but including the factor variable heap creates errors:
> >
> > bam0 <- bam(deaths~te(year, month, week, weekday,
> > bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> >                                       te(temp_max, lag, k=c(8, 3)) +
> >                                       te(precip_daily_total, lag, k=c(8, 3)),
> >                                      data = dat, family = nb, method =
> > 'fREML', select = TRUE, discrete = TRUE,
> >                                      knots = list(month = c(0.5, 12.5),
> > week = c(0.5, 52.5), weekday = c(0, 6.5)))
> >
> > This model results in errors:
> >
> > Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
> >    step failure in theta estimation
> > Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> > object$prior.weights)) :
> >    NaNs produced
> >
> Did it actually fail, or simply generate warnings?
>
> 'bam' handles factors, but if I understand your model right there is one
> free parameter for each observation falling on the 15th, so that you
> will fit data for those days exactly (and might just as well have
> dropped them, for all the information they contribute to the rest of the
> model). If you want a structure like this, I'd be inclined to make the
> heap variable random, something like...
>
> aheap <- heap!="0"
>
> heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap day
> + a r.e. for each heap day - no effect on non-heap days
>
> best,
>
> Simon
>
> > Including heap as as.numeric(heap) runs the model without error
> > messages or warnings, but model diagnostics look terrible, and it also
> > doesn't make sense (to me) to make heap a numeric. The factor variable
> > heap (with 169 levels) codes the fact that all deaths for which no
> > date was known, were registered on the 15th day of each month. I've
> > coded all non-heaping days as 0. All heaping days were coded as a
> > value between 1-168. The time series spans 14 years, so a heaping day
> > in each month results in 14*12 levels = 168, plus one level for
> > non-heaping days.
> >
> > So my second question is: Does bam allow factor variables? And if not,
> > how should I model this heaping on the 15th day of the month instead?
> >
> > With thanks,
> >
> > Jade
> >
> > On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
> >> I would not worry too much about high concurvity between variables like
> >> temperature and time. This just reflects the fact that temperature has a
> >> strong temporal pattern.
> >>
> >> I would also not be too worried about the low p-values on the k check.
> >> The check only looks for pattern in the residuals when they are ordered
> >> with respect to the variables of a smooth. When you have time series
> >> data and some smooths involve time then it's hard not to pick up some
> >> degree of residual auto-correlation, which you often would not want to
> >> model with a higher rank smoother.
> >>
> >> The NAs  for the distributed lag terms just reflect the fact that there
> >> is no obvious way to order the residuals w.r.t. the covariates for such
> >> terms, so the simple check for residual pattern is not really possible.
> >>
> >> One simple approach is to fit using bam(...,discrete=TRUE) which will
> >> let you specify an AR1 parameter to mop up some of the residual
> >> auto-correlation without resorting to a high rank smooth that then does
> >> all the work of the covariates as well. The AR1 parameter can be set by
> >> looking at the ACF of the residuals of the model without this. You need
> >> to look at the ACF of suitably standardized residuals to check how well
> >> this has worked.
> >>
> >> best,
> >>
> >> Simon
> >>
> >> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> >>> Hello everyone,
> >>>
> >>> A few days ago I asked a question about concurvity in a GAM (the
> >>> anologue of collinearity in a GLM) implemented in mgcv. I think my
> >>> question was a bit unfocussed, so I am retrying again, but with
> >>> additional information included about the autocorrelation function. I
> >>> have also posted about this on Cross Validated. Given all the model
> >>> output, it might make for easier
> >>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> >>>
> >>> As mentioned previously, I have problems with concurvity in my thesis
> >>> research, and don't have access to a statistician who works with time
> >>> series, GAMs or R. I'd be very grateful for any (partial) answer,
> >>> however short. I'll gladly return the favour where I can! For really
> >>> helpful input I'd be more than happy to offer co-authorship on
> >>> publication. Deadlines are very close, and I'm heading towards having
> >>> no results at all if I can't solve this concurvity issue :(
> >>>
> >>> I'm using GAMs to try to understand the relationship between deaths
> >>> and heat-related variables (e.g. temperature and humidity), using
> >>> daily time series over a 14-year period from a tropical, low-income
> >>> country. My aim is to understand the relationship between these
> >>> variables and deaths, rather than pure prediction performance.
> >>>
> >>> The GAMs include distributed lag models (set up as 7-column matrices,
> >>> see code at bottom of post), since deaths may occur over several days
> >>> following exposure.
> >>>
> >>> Simple GAMs with just time, lagged temperature and lagged
> >>> precipitation (a potential confounder) show very high concurvity
> >>> between lagged temperature and time, regardless of the many different
> >>> ways I have tried to decompose time. The autocorrelation functions
> >>> (ACF) however, shows values close to zero, only just about breaching
> >>> the 'significance line' in a few instances. It does show patterning
> >>> though, although the regularity is difficult to define.
> >>>
> >>> My questions are:
> >>> 1) Should I be worried about the high concurvity, or can I ignore it
> >>> given the mostly non-significant ACF? I've read dozens of
> >>> heat-mortality modelling studies and none report on concurvity between
> >>> weather variables and time (though one 2012 paper discussed
> >>> autocorrelation).
> >>>
> >>> 2) If I cannot ignore it, what should I do to resolve it? Would
> >>> including an autoregressive term be appropriate, and if so, where can
> >>> I find a coded example of how to do this? I've also come across
> >>> sequential regression][1]. Would this be more or less appropriate? If
> >>> appropriate, a pointer to an example would be really appreciated!
> >>>
> >>> Some example GAMs are specified as follows:
> >>> ```r
> >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> >>> bs=c("cr","cc","cc","cc")) + heap +
> >>>                         te(temp_max, lag, k=c(10, 3)) +
> >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>> 52.5), weekday = c(0, 6.5)))
> >>> ```
> >>> Concurvity for the above model between (temp_max, lag) and (year,
> >>> month, week, weekday) is 0.91:
> >>>
> >>> ```r
> >>> $worst
> >>>                                       para te(year,month,week,weekday)
> >>> te(temp_max,lag) te(precip_daily_total,lag)
> >>> para                        1.000000e+00                1.125625e-29
> >>>        0.3150073                  0.6666348
> >>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> >>>        0.9060552                  0.6652313
> >>> te(temp_max,lag)            3.152795e-01                8.998113e-01
> >>>        1.0000000                  0.5781015
> >>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> >>>        0.5805159                  1.0000000
> >>> ```
> >>>
> >>> Output from ```gam.check()```:
> >>> ```r
> >>> Method: REML   Optimizer: outer newton
> >>> full convergence after 16 iterations.
> >>> Gradient range [-0.01467332,0.003096643]
> >>> (score 8915.994 & scale 1).
> >>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> >>> Model rank =  544 / 544
> >>>
> >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>> indicate that k is too low, especially if edf is close to k'.
> >>>
> >>>                                     k'      edf k-index p-value
> >>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> >>> te(temp_max,lag)             29.0000   3.3681      NA      NA
> >>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>> ```
> >>>
> >>> Some output from ```summary(conc38b)```:
> >>> ```r
> >>> Approximate significance of smooth terms:
> >>>                                     edf Ref.df  Chi.sq p-value
> >>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> >>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> >>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>
> >>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> >>> -REML =   8916  Scale est. = 1         n = 5107
> >>> ```
> >>>
> >>>
> >>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> >>> pattern). They show peaks at 5 and 15, and then there seems to be a
> >>> recurring pattern at multiples of approx. 30 (suggesting month is not
> >>> modelled adequately?). Not sure what would cause the spikes at 5 and
> >>> 15. There is heaping of deaths on the 15th day of each month, to which
> >>> deaths with unknown date were allocated. This heaping was modelled
> >>> with categorical variable/ factor ```heap``` with 169 levels (0 for
> >>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> >>> heaping day over the 14-year period):
> >>>
> >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> >>>
> >>>
> >>> I get an identical looking ACF when I decompose time into (year,
> >>> month, monthday) as in model conc39 below, although concurvity between
> >>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> >>>
> >>> ```r
> >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >>>                        te(temp_max, lag, k=c(10, 4)) +
> >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> >>>                        knots = list(month = c(0.5, 12.5)))
> >>> ```
> >>> ```r
> >>>
> >>> Method: REML   Optimizer: outer newton
> >>> full convergence after 14 iterations.
> >>> Gradient range [-0.001578187,6.155096e-05]
> >>> (score 8915.763 & scale 1).
> >>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> >>> Model rank =  323 / 323
> >>>
> >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>> indicate that k is too low, especially if edf is close to k'.
> >>>
> >>>                                   k'     edf k-index p-value
> >>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> >>> te(temp_max,lag)           39.0000  4.0875      NA      NA
> >>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>> ```
> >>> Some output from ```summary(conc39)```:
> >>> ```r
> >>> Approximate significance of smooth terms:
> >>>                                   edf Ref.df  Chi.sq  p-value
> >>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> >>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> >>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>
> >>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> >>> -REML =   8915  Scale est. = 1         n = 5107
> >>> ```
> >>>
> >>>
> >>> ```r
> >>> $worst
> >>>                                      para te(year,month,monthday)
> >>> te(temp_max,lag) te(precip_daily_total,lag)
> >>> para                       1.000000e+00            3.261007e-31
> >>> 0.3313549                  0.6666532
> >>> te(year,month,monthday)    3.060763e-31            1.000000e+00
> >>> 0.8266086                  0.5670777
> >>> te(temp_max,lag)           3.331014e-01            8.225942e-01
> >>> 1.0000000                  0.5840875
> >>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> >>> 0.5939380                  1.0000000
> >>> ```
> >>>
> >>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> >>> various choices for k or as ```s(time)``` with various k does not
> >>> reduce concurvity either.
> >>>
> >>>
> >>> The default approach in time series studies of heat-mortality is to
> >>> model time with fixed df, generally between 7-10 df per year of data.
> >>> I am, however, apprehensive about this approach because a) mortality
> >>> profiles vary with locality due to sociodemographic and environmental
> >>> characteristics and b) the choice of df is based on higher income
> >>> countries (where nearly all these studies have been done) with
> >>> different mortality profiles and so may not be appropriate for
> >>> tropical, low-income countries.
> >>>
> >>> Although the approach of fixing (high) df does remove more temporal
> >>> patterns from the ACF (see model and output below), concurvity between
> >>> time and lagged temperature has now risen to 0.99! Moreover,
> >>> temperature (which has been a consistent, highly significant predictor
> >>> in every model of the tens (hundreds?) I have run, has now turned
> >>> non-significant. I am guessing this is because time is now a very
> >>> wiggly function that not only models/ removes seasonal variation, but
> >>> also some of the day-to-day variation that is needed for the
> >>> temperature smooth  :
> >>>
> >>> ```r
> >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >>>                         te(temp_max, lag, k=c(10,3)) +
> >>>                         te(precip_daily_total, lag, k=c(10,3)),
> >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> >>> ```
> >>> Output from ```gam.check(conc20a, rep = 1000)```:
> >>>
> >>> ```r
> >>> Method: REML   Optimizer: outer newton
> >>> full convergence after 9 iterations.
> >>> Gradient range [-0.0008983099,9.546022e-05]
> >>> (score 8750.13 & scale 1).
> >>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> >>> Model rank =  336 / 336
> >>>
> >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>> indicate that k is too low, especially if edf is close to k'.
> >>>
> >>>                                    k'      edf k-index p-value
> >>> s(time)                    111.0000 111.0000    0.98    0.56
> >>> te(temp_max,lag)            29.0000   0.6548      NA      NA
> >>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> >>> ```
> >>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
> >>>
> >>> ```r
> >>>                                      para      s(time) te(temp_max,lag)
> >>> te(precip_daily_total,lag)
> >>> para                       1.000000e+00 2.462064e-19        0.3165236
> >>>                   0.6666348
> >>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
> >>>                   0.6879284
> >>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> >>>                   0.5788711
> >>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> >>>                   1.0000000
> >>>
> >>> ```
> >>>
> >>> Some output from ```summary(conc20a)```:
> >>> ```r
> >>> Approximate significance of smooth terms:
> >>>                                    edf Ref.df  Chi.sq p-value
> >>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> >>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> >>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>
> >>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> >>> -REML = 8750.1  Scale est. = 1         n = 5107
> >>> ```
> >>>
> >>> ACF functions:
> >>>
> >>> [4]: https://i.stack.imgur.com/7nbXS.png
> >>> [5]: https://i.stack.imgur.com/pNnZU.png
> >>>
> >>> Data can be found on my [GitHub][6] site in the file
> >>> [data_cross_validated_post2.rds][7]. A csv version is also available.
> >>> This is my code:
> >>>
> >>> ```r
> >>> library(readr)
> >>> library(mgcv)
> >>>
> >>> df <- read_rds("data_crossvalidated_post2.rds")
> >>>
> >>> # Create matrices for lagged weather variables (6 day lags) based on
> >>> example by Simon Wood
> >>> # in his 2017 book ("Generalized additive models: an introduction with
> >>> R", p. 349) and
> >>> # gamair package documentation
> >>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> >>>
> >>> lagard <- function(x,n.lag=7) {
> >>> n <- length(x); X <- matrix(NA,n,n.lag)
> >>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> >>> X
> >>> }
> >>>
> >>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> >>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> >>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> >>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> >>> = df$precip_hourly_dailysum)
> >>> dat$temp_max <- lagard(df$temp_max)
> >>> dat$temp_min <- lagard(df$temp_min)
> >>> dat$temp_mean <- lagard(df$temp_mean)
> >>> dat$wbgt_max <- lagard(df$wbgt_max)
> >>> dat$wbgt_mean <- lagard(df$wbgt_mean)
> >>> dat$wbgt_min <- lagard(df$wbgt_min)
> >>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> >>> dat$sh_mean <- lagard(df$sh_mean)
> >>> dat$solar_mean <- lagard(df$solar_mean)
> >>> dat$wind2m_mean <- lagard(df$wind2m_mean)
> >>> dat$sh_max <- lagard(df$sh_max)
> >>> dat$solar_max <- lagard(df$solar_max)
> >>> dat$wind2m_max <- lagard(df$wind2m_max)
> >>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> >>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> >>> dat$precip_hourly <- lagard(df$precip_hourly)
> >>> dat$precip_daily_total <- lagard( df$precip_daily_total)
> >>> dat$temp <- lagard(df$temp)
> >>> dat$sh <- lagard(df$sh)
> >>> dat$rh <- lagard(df$rh)
> >>> dat$solar <- lagard(df$solar)
> >>> dat$wind2m <- lagard(df$wind2m)
> >>>
> >>>
> >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> >>> bs=c("cr","cc","cc","cc")) + heap +
> >>>                         te(temp_max, lag, k=c(10, 3)) +
> >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>> 52.5), weekday = c(0, 6.5)))
> >>>
> >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >>>                        te(temp_max, lag, k=c(10, 4)) +
> >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> >>>                        knots = list(month = c(0.5, 12.5)))
> >>>
> >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >>>                         te(temp_max, lag, k=c(10,3)) +
> >>>                         te(precip_daily_total, lag, k=c(10,3)),
> >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> >>>
> >>> ```
> >>> Thank you if you've read this far!! :-))
> >>>
> >>>     [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> >>>     [4]: https://i.stack.imgur.com/7nbXS.png
> >>>     [5]: https://i.stack.imgur.com/pNnZU.png
> >>>     [6]: https://github.com/JadeShodan/heat-mortality
> >>>     [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >> --
> >> Simon Wood, School of Mathematics, University of Edinburgh,
> >> https://www.maths.ed.ac.uk/~swood34/
> >>
> --
> Simon Wood, School of Mathematics, University of Edinburgh,
> https://www.maths.ed.ac.uk/~swood34/
>


From j@de@shod@@ m@iii@g oii googiem@ii@com  Fri Jun 10 11:30:02 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Fri, 10 Jun 2022 10:30:02 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k-0=Z4KUE52mxHrSj4c+9bhJ+C4HDqxM2HqqVJ9um0WBQ@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-EXdbmLVNbEH9nN=FE3FYs4uqXgVtH3V35o6BU0z=rTw@mail.gmail.com>
 <91a30ffd-1aec-0f44-17a0-b632fae91014@dewey.myzen.co.uk>
 <CANg3_k-0=Z4KUE52mxHrSj4c+9bhJ+C4HDqxM2HqqVJ9um0WBQ@mail.gmail.com>
Message-ID: <CANg3_k-1=WsD71xcSa7WV0Ef=tiY3_Gz2KxvmskY8QuJso+EYA@mail.gmail.com>

Hi Michael,

I don't think my reply to your email came through to the list, so am
resending (see below). Problems with subscription have now hopefully
been resolved. Apologies if this is a double posting!

On Thu, 9 Jun 2022 at 15:27, jade.shodan at googlemail.com
<jade.shodan at googlemail.com> wrote:
>
> Hi Michael,
>
> Thanks for the reply! When I ran the gam  with the gam() function, the
> model worked fine with heap having 169 levels. The same model with
> bam() however, fails.  I don't understand the difference between bam()
> and gam() at all (other than computational efficiency), but could the
> fact that each level only has 1 data point be the reason for it?
>
> These heaping days are very large measurement errors I need to get rid
> off, so I just want to take them out of the model altogether. (My data
> is quite noisy already, because date of death is based on memory
> recall, rather than formal death registration, due to the data being
> from a low income country). Median of deaths is approx. 2 per day, but
> on heaping days it can be as high as 50 or so.
>
> My understanding was that coding with 169 levels would effectively
> take these measurements out of the model (but do correct me if I'm
> wrong!)  I originally coded heap as a binary variable with 0 for
> non-heaping days and 1 for heaping days, but was told that meant that
> I was assuming the effect was the same for all heaping days. If I
> coded with 12 or 14 levels, wouldn't that leave a lot of noise in the
> data?
>
> Jade
>
> On Thu, 9 Jun 2022 at 14:52, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
> >
> > Dear Jade
> >
> > Do you really need to fit a separate parameter for each heaping day? Can
> > you not just make it a binary predictor or a categorical one with fewer
> > levels, perhaps 14 (for heaping in each year) or 12 (for each calendar
> > month). I have no idea whether that would help but it seems worth a try.
> >
> > Michael
> >
> > On 08/06/2022 18:15, jade.shodan--- via R-help wrote:
> > > Hi Simon,
> > >
> > > Thanks so much for this!! I have two follow up questions, if you don't mind.
> > >
> > > 1. Does including an autoregressive term not adjust away part of the
> > > effect of the response in a distributed lag model (where the outcome
> > > accumulates over time)?
> > > 2. I've tried to fit a model using bam (just a first attempt without
> > > AR term), but including the factor variable heap creates errors:
> > >
> > > bam0 <- bam(deaths~te(year, month, week, weekday,
> > > bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> > >                        te(temp_max, lag, k=c(8, 3)) +
> > >                        te(precip_daily_total, lag, k=c(8, 3)),
> > >                        data = dat, family = nb, method = 'fREML',
> > > select = TRUE, discrete = TRUE,
> > >                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> > > 52.5), weekday = c(0, 6.5)))
> > >
> > > This model results in errors:
> > >
> > > Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
> > >    step failure in theta estimation
> > > Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> > > object$prior.weights)) :
> > >    NaNs produced
> > >
> > >
> > > Including heap as as.numeric(heap) runs the model without error
> > > messages or warnings, but model diagnostics look terrible, and it also
> > > doesn't make sense (to me) to make heap a numeric. The factor variable
> > > heap (with 169 levels) codes the fact that all deaths for which no
> > > date was known, were registered on the 15th day of each month. I've
> > > coded all non-heaping days as 0. All heaping days were coded as a
> > > value between 1-168. The time series spans 14 years, so a heaping day
> > > in each month results in 14*12 levels = 168, plus one level for
> > > non-heaping days.
> > >
> > > So my second question is: Does bam allow factor variables? And if not,
> > > how should I model this heaping on the 15th day of the month instead?
> > >
> > > With thanks,
> > >
> > > Jade
> > >
> > > On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
> > >>
> > >> I would not worry too much about high concurvity between variables like
> > >> temperature and time. This just reflects the fact that temperature has a
> > >> strong temporal pattern.
> > >>
> > >> I would also not be too worried about the low p-values on the k check.
> > >> The check only looks for pattern in the residuals when they are ordered
> > >> with respect to the variables of a smooth. When you have time series
> > >> data and some smooths involve time then it's hard not to pick up some
> > >> degree of residual auto-correlation, which you often would not want to
> > >> model with a higher rank smoother.
> > >>
> > >> The NAs  for the distributed lag terms just reflect the fact that there
> > >> is no obvious way to order the residuals w.r.t. the covariates for such
> > >> terms, so the simple check for residual pattern is not really possible.
> > >>
> > >> One simple approach is to fit using bam(...,discrete=TRUE) which will
> > >> let you specify an AR1 parameter to mop up some of the residual
> > >> auto-correlation without resorting to a high rank smooth that then does
> > >> all the work of the covariates as well. The AR1 parameter can be set by
> > >> looking at the ACF of the residuals of the model without this. You need
> > >> to look at the ACF of suitably standardized residuals to check how well
> > >> this has worked.
> > >>
> > >> best,
> > >>
> > >> Simon
> > >>
> > >> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> > >>> Hello everyone,
> > >>>
> > >>> A few days ago I asked a question about concurvity in a GAM (the
> > >>> anologue of collinearity in a GLM) implemented in mgcv. I think my
> > >>> question was a bit unfocussed, so I am retrying again, but with
> > >>> additional information included about the autocorrelation function. I
> > >>> have also posted about this on Cross Validated. Given all the model
> > >>> output, it might make for easier
> > >>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> > >>>
> > >>> As mentioned previously, I have problems with concurvity in my thesis
> > >>> research, and don't have access to a statistician who works with time
> > >>> series, GAMs or R. I'd be very grateful for any (partial) answer,
> > >>> however short. I'll gladly return the favour where I can! For really
> > >>> helpful input I'd be more than happy to offer co-authorship on
> > >>> publication. Deadlines are very close, and I'm heading towards having
> > >>> no results at all if I can't solve this concurvity issue :(
> > >>>
> > >>> I'm using GAMs to try to understand the relationship between deaths
> > >>> and heat-related variables (e.g. temperature and humidity), using
> > >>> daily time series over a 14-year period from a tropical, low-income
> > >>> country. My aim is to understand the relationship between these
> > >>> variables and deaths, rather than pure prediction performance.
> > >>>
> > >>> The GAMs include distributed lag models (set up as 7-column matrices,
> > >>> see code at bottom of post), since deaths may occur over several days
> > >>> following exposure.
> > >>>
> > >>> Simple GAMs with just time, lagged temperature and lagged
> > >>> precipitation (a potential confounder) show very high concurvity
> > >>> between lagged temperature and time, regardless of the many different
> > >>> ways I have tried to decompose time. The autocorrelation functions
> > >>> (ACF) however, shows values close to zero, only just about breaching
> > >>> the 'significance line' in a few instances. It does show patterning
> > >>> though, although the regularity is difficult to define.
> > >>>
> > >>> My questions are:
> > >>> 1) Should I be worried about the high concurvity, or can I ignore it
> > >>> given the mostly non-significant ACF? I've read dozens of
> > >>> heat-mortality modelling studies and none report on concurvity between
> > >>> weather variables and time (though one 2012 paper discussed
> > >>> autocorrelation).
> > >>>
> > >>> 2) If I cannot ignore it, what should I do to resolve it? Would
> > >>> including an autoregressive term be appropriate, and if so, where can
> > >>> I find a coded example of how to do this? I've also come across
> > >>> sequential regression][1]. Would this be more or less appropriate? If
> > >>> appropriate, a pointer to an example would be really appreciated!
> > >>>
> > >>> Some example GAMs are specified as follows:
> > >>> ```r
> > >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> > >>> bs=c("cr","cc","cc","cc")) + heap +
> > >>>                         te(temp_max, lag, k=c(10, 3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> > >>> 52.5), weekday = c(0, 6.5)))
> > >>> ```
> > >>> Concurvity for the above model between (temp_max, lag) and (year,
> > >>> month, week, weekday) is 0.91:
> > >>>
> > >>> ```r
> > >>> $worst
> > >>>                                       para te(year,month,week,weekday)
> > >>> te(temp_max,lag) te(precip_daily_total,lag)
> > >>> para                        1.000000e+00                1.125625e-29
> > >>>        0.3150073                  0.6666348
> > >>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> > >>>        0.9060552                  0.6652313
> > >>> te(temp_max,lag)            3.152795e-01                8.998113e-01
> > >>>        1.0000000                  0.5781015
> > >>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> > >>>        0.5805159                  1.0000000
> > >>> ```
> > >>>
> > >>> Output from ```gam.check()```:
> > >>> ```r
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 16 iterations.
> > >>> Gradient range [-0.01467332,0.003096643]
> > >>> (score 8915.994 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> > >>> Model rank =  544 / 544
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                     k'      edf k-index p-value
> > >>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> > >>> te(temp_max,lag)             29.0000   3.3681      NA      NA
> > >>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>> ```
> > >>>
> > >>> Some output from ```summary(conc38b)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                     edf Ref.df  Chi.sq p-value
> > >>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> > >>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> > >>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> > >>> -REML =   8916  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>>
> > >>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> > >>> pattern). They show peaks at 5 and 15, and then there seems to be a
> > >>> recurring pattern at multiples of approx. 30 (suggesting month is not
> > >>> modelled adequately?). Not sure what would cause the spikes at 5 and
> > >>> 15. There is heaping of deaths on the 15th day of each month, to which
> > >>> deaths with unknown date were allocated. This heaping was modelled
> > >>> with categorical variable/ factor ```heap``` with 169 levels (0 for
> > >>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> > >>> heaping day over the 14-year period):
> > >>>
> > >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> > >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> > >>>
> > >>>
> > >>> I get an identical looking ACF when I decompose time into (year,
> > >>> month, monthday) as in model conc39 below, although concurvity between
> > >>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> > >>>
> > >>> ```r
> > >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> > >>>                        te(temp_max, lag, k=c(10, 4)) +
> > >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> > >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                        knots = list(month = c(0.5, 12.5)))
> > >>> ```
> > >>> ```r
> > >>>
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 14 iterations.
> > >>> Gradient range [-0.001578187,6.155096e-05]
> > >>> (score 8915.763 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> > >>> Model rank =  323 / 323
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                   k'     edf k-index p-value
> > >>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> > >>> te(temp_max,lag)           39.0000  4.0875      NA      NA
> > >>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>> ```
> > >>> Some output from ```summary(conc39)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                   edf Ref.df  Chi.sq  p-value
> > >>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> > >>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> > >>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> > >>> -REML =   8915  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>>
> > >>> ```r
> > >>> $worst
> > >>>                                      para te(year,month,monthday)
> > >>> te(temp_max,lag) te(precip_daily_total,lag)
> > >>> para                       1.000000e+00            3.261007e-31
> > >>> 0.3313549                  0.6666532
> > >>> te(year,month,monthday)    3.060763e-31            1.000000e+00
> > >>> 0.8266086                  0.5670777
> > >>> te(temp_max,lag)           3.331014e-01            8.225942e-01
> > >>> 1.0000000                  0.5840875
> > >>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> > >>> 0.5939380                  1.0000000
> > >>> ```
> > >>>
> > >>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> > >>> various choices for k or as ```s(time)``` with various k does not
> > >>> reduce concurvity either.
> > >>>
> > >>>
> > >>> The default approach in time series studies of heat-mortality is to
> > >>> model time with fixed df, generally between 7-10 df per year of data.
> > >>> I am, however, apprehensive about this approach because a) mortality
> > >>> profiles vary with locality due to sociodemographic and environmental
> > >>> characteristics and b) the choice of df is based on higher income
> > >>> countries (where nearly all these studies have been done) with
> > >>> different mortality profiles and so may not be appropriate for
> > >>> tropical, low-income countries.
> > >>>
> > >>> Although the approach of fixing (high) df does remove more temporal
> > >>> patterns from the ACF (see model and output below), concurvity between
> > >>> time and lagged temperature has now risen to 0.99! Moreover,
> > >>> temperature (which has been a consistent, highly significant predictor
> > >>> in every model of the tens (hundreds?) I have run, has now turned
> > >>> non-significant. I am guessing this is because time is now a very
> > >>> wiggly function that not only models/ removes seasonal variation, but
> > >>> also some of the day-to-day variation that is needed for the
> > >>> temperature smooth  :
> > >>>
> > >>> ```r
> > >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> > >>>                         te(temp_max, lag, k=c(10,3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10,3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> > >>> ```
> > >>> Output from ```gam.check(conc20a, rep = 1000)```:
> > >>>
> > >>> ```r
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 9 iterations.
> > >>> Gradient range [-0.0008983099,9.546022e-05]
> > >>> (score 8750.13 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> > >>> Model rank =  336 / 336
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                    k'      edf k-index p-value
> > >>> s(time)                    111.0000 111.0000    0.98    0.56
> > >>> te(temp_max,lag)            29.0000   0.6548      NA      NA
> > >>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> > >>> ```
> > >>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
> > >>>
> > >>> ```r
> > >>>                                      para      s(time) te(temp_max,lag)
> > >>> te(precip_daily_total,lag)
> > >>> para                       1.000000e+00 2.462064e-19        0.3165236
> > >>>                   0.6666348
> > >>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
> > >>>                   0.6879284
> > >>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> > >>>                   0.5788711
> > >>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> > >>>                   1.0000000
> > >>>
> > >>> ```
> > >>>
> > >>> Some output from ```summary(conc20a)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                    edf Ref.df  Chi.sq p-value
> > >>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> > >>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> > >>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> > >>> -REML = 8750.1  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>> ACF functions:
> > >>>
> > >>> [4]: https://i.stack.imgur.com/7nbXS.png
> > >>> [5]: https://i.stack.imgur.com/pNnZU.png
> > >>>
> > >>> Data can be found on my [GitHub][6] site in the file
> > >>> [data_cross_validated_post2.rds][7]. A csv version is also available.
> > >>> This is my code:
> > >>>
> > >>> ```r
> > >>> library(readr)
> > >>> library(mgcv)
> > >>>
> > >>> df <- read_rds("data_crossvalidated_post2.rds")
> > >>>
> > >>> # Create matrices for lagged weather variables (6 day lags) based on
> > >>> example by Simon Wood
> > >>> # in his 2017 book ("Generalized additive models: an introduction with
> > >>> R", p. 349) and
> > >>> # gamair package documentation
> > >>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> > >>>
> > >>> lagard <- function(x,n.lag=7) {
> > >>> n <- length(x); X <- matrix(NA,n,n.lag)
> > >>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > >>> X
> > >>> }
> > >>>
> > >>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> > >>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> > >>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> > >>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> > >>> = df$precip_hourly_dailysum)
> > >>> dat$temp_max <- lagard(df$temp_max)
> > >>> dat$temp_min <- lagard(df$temp_min)
> > >>> dat$temp_mean <- lagard(df$temp_mean)
> > >>> dat$wbgt_max <- lagard(df$wbgt_max)
> > >>> dat$wbgt_mean <- lagard(df$wbgt_mean)
> > >>> dat$wbgt_min <- lagard(df$wbgt_min)
> > >>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> > >>> dat$sh_mean <- lagard(df$sh_mean)
> > >>> dat$solar_mean <- lagard(df$solar_mean)
> > >>> dat$wind2m_mean <- lagard(df$wind2m_mean)
> > >>> dat$sh_max <- lagard(df$sh_max)
> > >>> dat$solar_max <- lagard(df$solar_max)
> > >>> dat$wind2m_max <- lagard(df$wind2m_max)
> > >>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> > >>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> > >>> dat$precip_hourly <- lagard(df$precip_hourly)
> > >>> dat$precip_daily_total <- lagard( df$precip_daily_total)
> > >>> dat$temp <- lagard(df$temp)
> > >>> dat$sh <- lagard(df$sh)
> > >>> dat$rh <- lagard(df$rh)
> > >>> dat$solar <- lagard(df$solar)
> > >>> dat$wind2m <- lagard(df$wind2m)
> > >>>
> > >>>
> > >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> > >>> bs=c("cr","cc","cc","cc")) + heap +
> > >>>                         te(temp_max, lag, k=c(10, 3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> > >>> 52.5), weekday = c(0, 6.5)))
> > >>>
> > >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> > >>>                        te(temp_max, lag, k=c(10, 4)) +
> > >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> > >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                        knots = list(month = c(0.5, 12.5)))
> > >>>
> > >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> > >>>                         te(temp_max, lag, k=c(10,3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10,3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> > >>>
> > >>> ```
> > >>> Thank you if you've read this far!! :-))
> > >>>
> > >>>     [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> > >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> > >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> > >>>     [4]: https://i.stack.imgur.com/7nbXS.png
> > >>>     [5]: https://i.stack.imgur.com/pNnZU.png
> > >>>     [6]: https://github.com/JadeShodan/heat-mortality
> > >>>     [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> > >>>
> > >>> ______________________________________________
> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >>> and provide commented, minimal, self-contained, reproducible code.
> > >>
> > >> --
> > >> Simon Wood, School of Mathematics, University of Edinburgh,
> > >> https://www.maths.ed.ac.uk/~swood34/
> > >>
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > --
> > Michael
> > http://www.dewey.myzen.co.uk/home.html


From j@de@shod@@ m@iii@g oii googiem@ii@com  Fri Jun 10 11:30:39 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Fri, 10 Jun 2022 10:30:39 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k_qMX24_a-Hppk6knrH-nt1QMLj2FFSJbkMJc0TBxqzQA@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-g=M41cxom6DVdTunFjgZ6VfKobqW__Mq351pDpY22PQ@mail.gmail.com>
 <f371005e-6f57-2007-f859-d66595a19c0f@bath.edu>
 <CANg3_k_qMX24_a-Hppk6knrH-nt1QMLj2FFSJbkMJc0TBxqzQA@mail.gmail.com>
Message-ID: <CANg3_k9pNAfJ8eC+XT3v8ugA6aXUcenmAQv7awHiGDOL_ytyHg@mail.gmail.com>

Hi Simon,

I don't think my reply to your email came through to the list, so am
resending (see below). Problems with subscription have now hopefully
been resolved. Apologies if this is a double posting!

On Thu, 9 Jun 2022 at 16:57, jade.shodan at googlemail.com
<jade.shodan at googlemail.com> wrote:
>
> Hi Simon,
>
> (Sorry, replies and answers are out of sync due to my problems posting
> to the list/ messages being held for moderation)
>
> > Did it actually fail, or simply generate warnings?
> The model was computed (you're right, not a model failure), but
> resulted in warnings (as per previous post) which, frankly, I didn't
> understand.
>
> > if I understand your model right there is one free parameter for each observation falling on the 15th
> That's right, one observation on each 15th day of the month.
>
> Thank you for the suggestion about the random effects! I had been
> wondering about how I could model this heaping with a smooth!
>
> Quick question:
>
> You proposed:
>
> aheap <- heap!="0"
> heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap
> day + a r.e. for each heap day - no effect on non-heap days
>
> Is there a typo in the code above? I don't see the newly created
> variable aheap in the model?
> Should it read "by = aheap" as follows:
>
> heap + s(heap,bs="re",by=aheap)   ?
>
> So a full model might then look like the one below?
>
>  bam0 <- bam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> s(heap,bs="re",by=aheap)
>                                      te(temp_max, lag, k=c(8, 3)) +
>                                      te(precip_daily_total, lag, k=c(8, 3)),
>                                      data = dat, family = nb, method =
> 'fREML', select = TRUE, discrete = TRUE,
>                                    knots = list(month = c(0.5, 12.5),
> week = c(0.5, 52.5), weekday = c(0, 6.5)))
>
> One, hopefully final (!) question:
>
> Is it actually useful at all to keep these observations on the 15th
> day of each month (which are huge errors), or am I better off removing
> them from the data set (or replacing them with e.g. median values)?
> For temperature-mortality modelling it is the day-to-day variation in
> deaths and temperature that is of interest. So is modelling heaping
> actually useful at all, given that this variable changes on a monthly
> basis? (I think you are alluding to this, but I just want to make
> sure).
>
> If I take them out altogether, would I be best off removing all data
> for these dates, so that the time series jumps from day 14 to day 16?
> Or would this create problems with e.g. the distributed lag model?
>
> Sorry for all these questions! Have been struggling with this for
> months (posted on Cross Validated about the heaping issue too), and
> feel I am finally getting somewhere with your help!
>
> Jade
>
> On Thu, 9 Jun 2022 at 15:24, Simon Wood <simon.wood at bath.edu> wrote:
> >
> >
> > On 09/06/2022 12:30, jade.shodan at googlemail.com wrote:
> > > Hi Simon,
> > >
> > > Thanks so much for this!! (Apologies if this is a double posting. I
> > > seem to have a problem getting messages through to the list).
> > >
> > > I have two follow up questions, if you don't mind.
> > >
> > > 1. Does including an autoregressive term not adjust away part of the
> > > effect of the response in a distributed lag model (where the outcome
> > > accumulates over time)?
> >
> > - the hope is that it approximately deals with short timescale stuff
> > without interfering with the longer timescales to the same extent as a
> > high rank smooth would.
> >
> > > 2. I've tried to fit a model using bam (just a first attempt without
> > > AR term), but including the factor variable heap creates errors:
> > >
> > > bam0 <- bam(deaths~te(year, month, week, weekday,
> > > bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> > >                                       te(temp_max, lag, k=c(8, 3)) +
> > >                                       te(precip_daily_total, lag, k=c(8, 3)),
> > >                                      data = dat, family = nb, method =
> > > 'fREML', select = TRUE, discrete = TRUE,
> > >                                      knots = list(month = c(0.5, 12.5),
> > > week = c(0.5, 52.5), weekday = c(0, 6.5)))
> > >
> > > This model results in errors:
> > >
> > > Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
> > >    step failure in theta estimation
> > > Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> > > object$prior.weights)) :
> > >    NaNs produced
> > >
> > Did it actually fail, or simply generate warnings?
> >
> > 'bam' handles factors, but if I understand your model right there is one
> > free parameter for each observation falling on the 15th, so that you
> > will fit data for those days exactly (and might just as well have
> > dropped them, for all the information they contribute to the rest of the
> > model). If you want a structure like this, I'd be inclined to make the
> > heap variable random, something like...
> >
> > aheap <- heap!="0"
> >
> > heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap day
> > + a r.e. for each heap day - no effect on non-heap days
> >
> > best,
> >
> > Simon
> >
> > > Including heap as as.numeric(heap) runs the model without error
> > > messages or warnings, but model diagnostics look terrible, and it also
> > > doesn't make sense (to me) to make heap a numeric. The factor variable
> > > heap (with 169 levels) codes the fact that all deaths for which no
> > > date was known, were registered on the 15th day of each month. I've
> > > coded all non-heaping days as 0. All heaping days were coded as a
> > > value between 1-168. The time series spans 14 years, so a heaping day
> > > in each month results in 14*12 levels = 168, plus one level for
> > > non-heaping days.
> > >
> > > So my second question is: Does bam allow factor variables? And if not,
> > > how should I model this heaping on the 15th day of the month instead?
> > >
> > > With thanks,
> > >
> > > Jade
> > >
> > > On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
> > >> I would not worry too much about high concurvity between variables like
> > >> temperature and time. This just reflects the fact that temperature has a
> > >> strong temporal pattern.
> > >>
> > >> I would also not be too worried about the low p-values on the k check.
> > >> The check only looks for pattern in the residuals when they are ordered
> > >> with respect to the variables of a smooth. When you have time series
> > >> data and some smooths involve time then it's hard not to pick up some
> > >> degree of residual auto-correlation, which you often would not want to
> > >> model with a higher rank smoother.
> > >>
> > >> The NAs  for the distributed lag terms just reflect the fact that there
> > >> is no obvious way to order the residuals w.r.t. the covariates for such
> > >> terms, so the simple check for residual pattern is not really possible.
> > >>
> > >> One simple approach is to fit using bam(...,discrete=TRUE) which will
> > >> let you specify an AR1 parameter to mop up some of the residual
> > >> auto-correlation without resorting to a high rank smooth that then does
> > >> all the work of the covariates as well. The AR1 parameter can be set by
> > >> looking at the ACF of the residuals of the model without this. You need
> > >> to look at the ACF of suitably standardized residuals to check how well
> > >> this has worked.
> > >>
> > >> best,
> > >>
> > >> Simon
> > >>
> > >> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> > >>> Hello everyone,
> > >>>
> > >>> A few days ago I asked a question about concurvity in a GAM (the
> > >>> anologue of collinearity in a GLM) implemented in mgcv. I think my
> > >>> question was a bit unfocussed, so I am retrying again, but with
> > >>> additional information included about the autocorrelation function. I
> > >>> have also posted about this on Cross Validated. Given all the model
> > >>> output, it might make for easier
> > >>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> > >>>
> > >>> As mentioned previously, I have problems with concurvity in my thesis
> > >>> research, and don't have access to a statistician who works with time
> > >>> series, GAMs or R. I'd be very grateful for any (partial) answer,
> > >>> however short. I'll gladly return the favour where I can! For really
> > >>> helpful input I'd be more than happy to offer co-authorship on
> > >>> publication. Deadlines are very close, and I'm heading towards having
> > >>> no results at all if I can't solve this concurvity issue :(
> > >>>
> > >>> I'm using GAMs to try to understand the relationship between deaths
> > >>> and heat-related variables (e.g. temperature and humidity), using
> > >>> daily time series over a 14-year period from a tropical, low-income
> > >>> country. My aim is to understand the relationship between these
> > >>> variables and deaths, rather than pure prediction performance.
> > >>>
> > >>> The GAMs include distributed lag models (set up as 7-column matrices,
> > >>> see code at bottom of post), since deaths may occur over several days
> > >>> following exposure.
> > >>>
> > >>> Simple GAMs with just time, lagged temperature and lagged
> > >>> precipitation (a potential confounder) show very high concurvity
> > >>> between lagged temperature and time, regardless of the many different
> > >>> ways I have tried to decompose time. The autocorrelation functions
> > >>> (ACF) however, shows values close to zero, only just about breaching
> > >>> the 'significance line' in a few instances. It does show patterning
> > >>> though, although the regularity is difficult to define.
> > >>>
> > >>> My questions are:
> > >>> 1) Should I be worried about the high concurvity, or can I ignore it
> > >>> given the mostly non-significant ACF? I've read dozens of
> > >>> heat-mortality modelling studies and none report on concurvity between
> > >>> weather variables and time (though one 2012 paper discussed
> > >>> autocorrelation).
> > >>>
> > >>> 2) If I cannot ignore it, what should I do to resolve it? Would
> > >>> including an autoregressive term be appropriate, and if so, where can
> > >>> I find a coded example of how to do this? I've also come across
> > >>> sequential regression][1]. Would this be more or less appropriate? If
> > >>> appropriate, a pointer to an example would be really appreciated!
> > >>>
> > >>> Some example GAMs are specified as follows:
> > >>> ```r
> > >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> > >>> bs=c("cr","cc","cc","cc")) + heap +
> > >>>                         te(temp_max, lag, k=c(10, 3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> > >>> 52.5), weekday = c(0, 6.5)))
> > >>> ```
> > >>> Concurvity for the above model between (temp_max, lag) and (year,
> > >>> month, week, weekday) is 0.91:
> > >>>
> > >>> ```r
> > >>> $worst
> > >>>                                       para te(year,month,week,weekday)
> > >>> te(temp_max,lag) te(precip_daily_total,lag)
> > >>> para                        1.000000e+00                1.125625e-29
> > >>>        0.3150073                  0.6666348
> > >>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> > >>>        0.9060552                  0.6652313
> > >>> te(temp_max,lag)            3.152795e-01                8.998113e-01
> > >>>        1.0000000                  0.5781015
> > >>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> > >>>        0.5805159                  1.0000000
> > >>> ```
> > >>>
> > >>> Output from ```gam.check()```:
> > >>> ```r
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 16 iterations.
> > >>> Gradient range [-0.01467332,0.003096643]
> > >>> (score 8915.994 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> > >>> Model rank =  544 / 544
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                     k'      edf k-index p-value
> > >>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> > >>> te(temp_max,lag)             29.0000   3.3681      NA      NA
> > >>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>> ```
> > >>>
> > >>> Some output from ```summary(conc38b)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                     edf Ref.df  Chi.sq p-value
> > >>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> > >>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> > >>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> > >>> -REML =   8916  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>>
> > >>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> > >>> pattern). They show peaks at 5 and 15, and then there seems to be a
> > >>> recurring pattern at multiples of approx. 30 (suggesting month is not
> > >>> modelled adequately?). Not sure what would cause the spikes at 5 and
> > >>> 15. There is heaping of deaths on the 15th day of each month, to which
> > >>> deaths with unknown date were allocated. This heaping was modelled
> > >>> with categorical variable/ factor ```heap``` with 169 levels (0 for
> > >>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> > >>> heaping day over the 14-year period):
> > >>>
> > >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> > >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> > >>>
> > >>>
> > >>> I get an identical looking ACF when I decompose time into (year,
> > >>> month, monthday) as in model conc39 below, although concurvity between
> > >>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> > >>>
> > >>> ```r
> > >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> > >>>                        te(temp_max, lag, k=c(10, 4)) +
> > >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> > >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                        knots = list(month = c(0.5, 12.5)))
> > >>> ```
> > >>> ```r
> > >>>
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 14 iterations.
> > >>> Gradient range [-0.001578187,6.155096e-05]
> > >>> (score 8915.763 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> > >>> Model rank =  323 / 323
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                   k'     edf k-index p-value
> > >>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> > >>> te(temp_max,lag)           39.0000  4.0875      NA      NA
> > >>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>> ```
> > >>> Some output from ```summary(conc39)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                   edf Ref.df  Chi.sq  p-value
> > >>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> > >>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> > >>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> > >>> -REML =   8915  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>>
> > >>> ```r
> > >>> $worst
> > >>>                                      para te(year,month,monthday)
> > >>> te(temp_max,lag) te(precip_daily_total,lag)
> > >>> para                       1.000000e+00            3.261007e-31
> > >>> 0.3313549                  0.6666532
> > >>> te(year,month,monthday)    3.060763e-31            1.000000e+00
> > >>> 0.8266086                  0.5670777
> > >>> te(temp_max,lag)           3.331014e-01            8.225942e-01
> > >>> 1.0000000                  0.5840875
> > >>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> > >>> 0.5939380                  1.0000000
> > >>> ```
> > >>>
> > >>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> > >>> various choices for k or as ```s(time)``` with various k does not
> > >>> reduce concurvity either.
> > >>>
> > >>>
> > >>> The default approach in time series studies of heat-mortality is to
> > >>> model time with fixed df, generally between 7-10 df per year of data.
> > >>> I am, however, apprehensive about this approach because a) mortality
> > >>> profiles vary with locality due to sociodemographic and environmental
> > >>> characteristics and b) the choice of df is based on higher income
> > >>> countries (where nearly all these studies have been done) with
> > >>> different mortality profiles and so may not be appropriate for
> > >>> tropical, low-income countries.
> > >>>
> > >>> Although the approach of fixing (high) df does remove more temporal
> > >>> patterns from the ACF (see model and output below), concurvity between
> > >>> time and lagged temperature has now risen to 0.99! Moreover,
> > >>> temperature (which has been a consistent, highly significant predictor
> > >>> in every model of the tens (hundreds?) I have run, has now turned
> > >>> non-significant. I am guessing this is because time is now a very
> > >>> wiggly function that not only models/ removes seasonal variation, but
> > >>> also some of the day-to-day variation that is needed for the
> > >>> temperature smooth  :
> > >>>
> > >>> ```r
> > >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> > >>>                         te(temp_max, lag, k=c(10,3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10,3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> > >>> ```
> > >>> Output from ```gam.check(conc20a, rep = 1000)```:
> > >>>
> > >>> ```r
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 9 iterations.
> > >>> Gradient range [-0.0008983099,9.546022e-05]
> > >>> (score 8750.13 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> > >>> Model rank =  336 / 336
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                    k'      edf k-index p-value
> > >>> s(time)                    111.0000 111.0000    0.98    0.56
> > >>> te(temp_max,lag)            29.0000   0.6548      NA      NA
> > >>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> > >>> ```
> > >>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
> > >>>
> > >>> ```r
> > >>>                                      para      s(time) te(temp_max,lag)
> > >>> te(precip_daily_total,lag)
> > >>> para                       1.000000e+00 2.462064e-19        0.3165236
> > >>>                   0.6666348
> > >>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
> > >>>                   0.6879284
> > >>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> > >>>                   0.5788711
> > >>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> > >>>                   1.0000000
> > >>>
> > >>> ```
> > >>>
> > >>> Some output from ```summary(conc20a)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                    edf Ref.df  Chi.sq p-value
> > >>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> > >>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> > >>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> > >>> -REML = 8750.1  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>> ACF functions:
> > >>>
> > >>> [4]: https://i.stack.imgur.com/7nbXS.png
> > >>> [5]: https://i.stack.imgur.com/pNnZU.png
> > >>>
> > >>> Data can be found on my [GitHub][6] site in the file
> > >>> [data_cross_validated_post2.rds][7]. A csv version is also available.
> > >>> This is my code:
> > >>>
> > >>> ```r
> > >>> library(readr)
> > >>> library(mgcv)
> > >>>
> > >>> df <- read_rds("data_crossvalidated_post2.rds")
> > >>>
> > >>> # Create matrices for lagged weather variables (6 day lags) based on
> > >>> example by Simon Wood
> > >>> # in his 2017 book ("Generalized additive models: an introduction with
> > >>> R", p. 349) and
> > >>> # gamair package documentation
> > >>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> > >>>
> > >>> lagard <- function(x,n.lag=7) {
> > >>> n <- length(x); X <- matrix(NA,n,n.lag)
> > >>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > >>> X
> > >>> }
> > >>>
> > >>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> > >>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> > >>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> > >>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> > >>> = df$precip_hourly_dailysum)
> > >>> dat$temp_max <- lagard(df$temp_max)
> > >>> dat$temp_min <- lagard(df$temp_min)
> > >>> dat$temp_mean <- lagard(df$temp_mean)
> > >>> dat$wbgt_max <- lagard(df$wbgt_max)
> > >>> dat$wbgt_mean <- lagard(df$wbgt_mean)
> > >>> dat$wbgt_min <- lagard(df$wbgt_min)
> > >>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> > >>> dat$sh_mean <- lagard(df$sh_mean)
> > >>> dat$solar_mean <- lagard(df$solar_mean)
> > >>> dat$wind2m_mean <- lagard(df$wind2m_mean)
> > >>> dat$sh_max <- lagard(df$sh_max)
> > >>> dat$solar_max <- lagard(df$solar_max)
> > >>> dat$wind2m_max <- lagard(df$wind2m_max)
> > >>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> > >>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> > >>> dat$precip_hourly <- lagard(df$precip_hourly)
> > >>> dat$precip_daily_total <- lagard( df$precip_daily_total)
> > >>> dat$temp <- lagard(df$temp)
> > >>> dat$sh <- lagard(df$sh)
> > >>> dat$rh <- lagard(df$rh)
> > >>> dat$solar <- lagard(df$solar)
> > >>> dat$wind2m <- lagard(df$wind2m)
> > >>>
> > >>>
> > >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> > >>> bs=c("cr","cc","cc","cc")) + heap +
> > >>>                         te(temp_max, lag, k=c(10, 3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> > >>> 52.5), weekday = c(0, 6.5)))
> > >>>
> > >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> > >>>                        te(temp_max, lag, k=c(10, 4)) +
> > >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> > >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                        knots = list(month = c(0.5, 12.5)))
> > >>>
> > >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> > >>>                         te(temp_max, lag, k=c(10,3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10,3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> > >>>
> > >>> ```
> > >>> Thank you if you've read this far!! :-))
> > >>>
> > >>>     [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> > >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> > >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> > >>>     [4]: https://i.stack.imgur.com/7nbXS.png
> > >>>     [5]: https://i.stack.imgur.com/pNnZU.png
> > >>>     [6]: https://github.com/JadeShodan/heat-mortality
> > >>>     [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> > >>>
> > >>> ______________________________________________
> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >>> and provide commented, minimal, self-contained, reproducible code.
> > >> --
> > >> Simon Wood, School of Mathematics, University of Edinburgh,
> > >> https://www.maths.ed.ac.uk/~swood34/
> > >>
> > --
> > Simon Wood, School of Mathematics, University of Edinburgh,
> > https://www.maths.ed.ac.uk/~swood34/
> >


From j@de@shod@@ m@iii@g oii googiem@ii@com  Fri Jun 10 12:01:24 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Fri, 10 Jun 2022 11:01:24 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k_qMX24_a-Hppk6knrH-nt1QMLj2FFSJbkMJc0TBxqzQA@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-g=M41cxom6DVdTunFjgZ6VfKobqW__Mq351pDpY22PQ@mail.gmail.com>
 <f371005e-6f57-2007-f859-d66595a19c0f@bath.edu>
 <CANg3_k_qMX24_a-Hppk6knrH-nt1QMLj2FFSJbkMJc0TBxqzQA@mail.gmail.com>
Message-ID: <CANg3_k8Z_D63TTiWPPS24PhRHP_86=TUoxETn-2GGTg0VDqopg@mail.gmail.com>

Hi Simon,

I don't think my reply to your email came through to the list, so am
resending (see below). I hope that problems with subscription and
posting have now
been resolved. Apologies to anyone who is receiving multiple copies!

On Thu, 9 Jun 2022 at 16:57, jade.shodan at googlemail.com
<jade.shodan at googlemail.com> wrote:
>
> Hi Simon,
>
> (Sorry, replies and answers are out of sync due to my problems posting
> to the list/ messages being held for moderation)
>
> > Did it actually fail, or simply generate warnings?
> The model was computed (you're right, not a model failure), but
> resulted in warnings (as per previous post) which, frankly, I didn't
> understand.
>
> > if I understand your model right there is one free parameter for each observation falling on the 15th
> That's right, one observation on each 15th day of the month.
>
> Thank you for the suggestion about the random effects! I had been
> wondering about how I could model this heaping with a smooth!
>
> Quick question:
>
> You proposed:
>
> aheap <- heap!="0"
> heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap
> day + a r.e. for each heap day - no effect on non-heap days
>
> Is there a typo in the code above? I don't see the newly created
> variable aheap in the model?
> Should it read "by = aheap" as follows:
>
> heap + s(heap,bs="re",by=aheap)   ?
>
> So a full model might then look like the one below?
>
>  bam0 <- bam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> s(heap,bs="re",by=aheap)
>                                      te(temp_max, lag, k=c(8, 3)) +
>                                      te(precip_daily_total, lag, k=c(8, 3)),
>                                      data = dat, family = nb, method =
> 'fREML', select = TRUE, discrete = TRUE,
>                                    knots = list(month = c(0.5, 12.5),
> week = c(0.5, 52.5), weekday = c(0, 6.5)))
>
> One, hopefully final (!) question:
>
> Is it actually useful at all to keep these observations on the 15th
> day of each month (which are huge errors), or am I better off removing
> them from the data set (or replacing them with e.g. median values)?
> For temperature-mortality modelling it is the day-to-day variation in
> deaths and temperature that is of interest. So is modelling heaping
> actually useful at all, given that this variable changes on a monthly
> basis? (I think you are alluding to this, but I just want to make
> sure).
>
> If I take them out altogether, would I be best off removing all data
> for these dates, so that the time series jumps from day 14 to day 16?
> Or would this create problems with e.g. the distributed lag model?
>
> Sorry for all these questions! Have been struggling with this for
> months (posted on Cross Validated about the heaping issue too), and
> feel I am finally getting somewhere with your help!
>
> Jade
>
> On Thu, 9 Jun 2022 at 15:24, Simon Wood <simon.wood at bath.edu> wrote:
> >
> >
> > On 09/06/2022 12:30, jade.shodan at googlemail.com wrote:
> > > Hi Simon,
> > >
> > > Thanks so much for this!! (Apologies if this is a double posting. I
> > > seem to have a problem getting messages through to the list).
> > >
> > > I have two follow up questions, if you don't mind.
> > >
> > > 1. Does including an autoregressive term not adjust away part of the
> > > effect of the response in a distributed lag model (where the outcome
> > > accumulates over time)?
> >
> > - the hope is that it approximately deals with short timescale stuff
> > without interfering with the longer timescales to the same extent as a
> > high rank smooth would.
> >
> > > 2. I've tried to fit a model using bam (just a first attempt without
> > > AR term), but including the factor variable heap creates errors:
> > >
> > > bam0 <- bam(deaths~te(year, month, week, weekday,
> > > bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> > >                                       te(temp_max, lag, k=c(8, 3)) +
> > >                                       te(precip_daily_total, lag, k=c(8, 3)),
> > >                                      data = dat, family = nb, method =
> > > 'fREML', select = TRUE, discrete = TRUE,
> > >                                      knots = list(month = c(0.5, 12.5),
> > > week = c(0.5, 52.5), weekday = c(0, 6.5)))
> > >
> > > This model results in errors:
> > >
> > > Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
> > >    step failure in theta estimation
> > > Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> > > object$prior.weights)) :
> > >    NaNs produced
> > >
> > Did it actually fail, or simply generate warnings?
> >
> > 'bam' handles factors, but if I understand your model right there is one
> > free parameter for each observation falling on the 15th, so that you
> > will fit data for those days exactly (and might just as well have
> > dropped them, for all the information they contribute to the rest of the
> > model). If you want a structure like this, I'd be inclined to make the
> > heap variable random, something like...
> >
> > aheap <- heap!="0"
> >
> > heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap day
> > + a r.e. for each heap day - no effect on non-heap days
> >
> > best,
> >
> > Simon
> >
> > > Including heap as as.numeric(heap) runs the model without error
> > > messages or warnings, but model diagnostics look terrible, and it also
> > > doesn't make sense (to me) to make heap a numeric. The factor variable
> > > heap (with 169 levels) codes the fact that all deaths for which no
> > > date was known, were registered on the 15th day of each month. I've
> > > coded all non-heaping days as 0. All heaping days were coded as a
> > > value between 1-168. The time series spans 14 years, so a heaping day
> > > in each month results in 14*12 levels = 168, plus one level for
> > > non-heaping days.
> > >
> > > So my second question is: Does bam allow factor variables? And if not,
> > > how should I model this heaping on the 15th day of the month instead?
> > >
> > > With thanks,
> > >
> > > Jade
> > >
> > > On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
> > >> I would not worry too much about high concurvity between variables like
> > >> temperature and time. This just reflects the fact that temperature has a
> > >> strong temporal pattern.
> > >>
> > >> I would also not be too worried about the low p-values on the k check.
> > >> The check only looks for pattern in the residuals when they are ordered
> > >> with respect to the variables of a smooth. When you have time series
> > >> data and some smooths involve time then it's hard not to pick up some
> > >> degree of residual auto-correlation, which you often would not want to
> > >> model with a higher rank smoother.
> > >>
> > >> The NAs  for the distributed lag terms just reflect the fact that there
> > >> is no obvious way to order the residuals w.r.t. the covariates for such
> > >> terms, so the simple check for residual pattern is not really possible.
> > >>
> > >> One simple approach is to fit using bam(...,discrete=TRUE) which will
> > >> let you specify an AR1 parameter to mop up some of the residual
> > >> auto-correlation without resorting to a high rank smooth that then does
> > >> all the work of the covariates as well. The AR1 parameter can be set by
> > >> looking at the ACF of the residuals of the model without this. You need
> > >> to look at the ACF of suitably standardized residuals to check how well
> > >> this has worked.
> > >>
> > >> best,
> > >>
> > >> Simon
> > >>
> > >> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> > >>> Hello everyone,
> > >>>
> > >>> A few days ago I asked a question about concurvity in a GAM (the
> > >>> anologue of collinearity in a GLM) implemented in mgcv. I think my
> > >>> question was a bit unfocussed, so I am retrying again, but with
> > >>> additional information included about the autocorrelation function. I
> > >>> have also posted about this on Cross Validated. Given all the model
> > >>> output, it might make for easier
> > >>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> > >>>
> > >>> As mentioned previously, I have problems with concurvity in my thesis
> > >>> research, and don't have access to a statistician who works with time
> > >>> series, GAMs or R. I'd be very grateful for any (partial) answer,
> > >>> however short. I'll gladly return the favour where I can! For really
> > >>> helpful input I'd be more than happy to offer co-authorship on
> > >>> publication. Deadlines are very close, and I'm heading towards having
> > >>> no results at all if I can't solve this concurvity issue :(
> > >>>
> > >>> I'm using GAMs to try to understand the relationship between deaths
> > >>> and heat-related variables (e.g. temperature and humidity), using
> > >>> daily time series over a 14-year period from a tropical, low-income
> > >>> country. My aim is to understand the relationship between these
> > >>> variables and deaths, rather than pure prediction performance.
> > >>>
> > >>> The GAMs include distributed lag models (set up as 7-column matrices,
> > >>> see code at bottom of post), since deaths may occur over several days
> > >>> following exposure.
> > >>>
> > >>> Simple GAMs with just time, lagged temperature and lagged
> > >>> precipitation (a potential confounder) show very high concurvity
> > >>> between lagged temperature and time, regardless of the many different
> > >>> ways I have tried to decompose time. The autocorrelation functions
> > >>> (ACF) however, shows values close to zero, only just about breaching
> > >>> the 'significance line' in a few instances. It does show patterning
> > >>> though, although the regularity is difficult to define.
> > >>>
> > >>> My questions are:
> > >>> 1) Should I be worried about the high concurvity, or can I ignore it
> > >>> given the mostly non-significant ACF? I've read dozens of
> > >>> heat-mortality modelling studies and none report on concurvity between
> > >>> weather variables and time (though one 2012 paper discussed
> > >>> autocorrelation).
> > >>>
> > >>> 2) If I cannot ignore it, what should I do to resolve it? Would
> > >>> including an autoregressive term be appropriate, and if so, where can
> > >>> I find a coded example of how to do this? I've also come across
> > >>> sequential regression][1]. Would this be more or less appropriate? If
> > >>> appropriate, a pointer to an example would be really appreciated!
> > >>>
> > >>> Some example GAMs are specified as follows:
> > >>> ```r
> > >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> > >>> bs=c("cr","cc","cc","cc")) + heap +
> > >>>                         te(temp_max, lag, k=c(10, 3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> > >>> 52.5), weekday = c(0, 6.5)))
> > >>> ```
> > >>> Concurvity for the above model between (temp_max, lag) and (year,
> > >>> month, week, weekday) is 0.91:
> > >>>
> > >>> ```r
> > >>> $worst
> > >>>                                       para te(year,month,week,weekday)
> > >>> te(temp_max,lag) te(precip_daily_total,lag)
> > >>> para                        1.000000e+00                1.125625e-29
> > >>>        0.3150073                  0.6666348
> > >>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> > >>>        0.9060552                  0.6652313
> > >>> te(temp_max,lag)            3.152795e-01                8.998113e-01
> > >>>        1.0000000                  0.5781015
> > >>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> > >>>        0.5805159                  1.0000000
> > >>> ```
> > >>>
> > >>> Output from ```gam.check()```:
> > >>> ```r
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 16 iterations.
> > >>> Gradient range [-0.01467332,0.003096643]
> > >>> (score 8915.994 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> > >>> Model rank =  544 / 544
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                     k'      edf k-index p-value
> > >>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> > >>> te(temp_max,lag)             29.0000   3.3681      NA      NA
> > >>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>> ```
> > >>>
> > >>> Some output from ```summary(conc38b)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                     edf Ref.df  Chi.sq p-value
> > >>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> > >>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> > >>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> > >>> -REML =   8916  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>>
> > >>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> > >>> pattern). They show peaks at 5 and 15, and then there seems to be a
> > >>> recurring pattern at multiples of approx. 30 (suggesting month is not
> > >>> modelled adequately?). Not sure what would cause the spikes at 5 and
> > >>> 15. There is heaping of deaths on the 15th day of each month, to which
> > >>> deaths with unknown date were allocated. This heaping was modelled
> > >>> with categorical variable/ factor ```heap``` with 169 levels (0 for
> > >>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> > >>> heaping day over the 14-year period):
> > >>>
> > >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> > >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> > >>>
> > >>>
> > >>> I get an identical looking ACF when I decompose time into (year,
> > >>> month, monthday) as in model conc39 below, although concurvity between
> > >>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> > >>>
> > >>> ```r
> > >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> > >>>                        te(temp_max, lag, k=c(10, 4)) +
> > >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> > >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                        knots = list(month = c(0.5, 12.5)))
> > >>> ```
> > >>> ```r
> > >>>
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 14 iterations.
> > >>> Gradient range [-0.001578187,6.155096e-05]
> > >>> (score 8915.763 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> > >>> Model rank =  323 / 323
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                   k'     edf k-index p-value
> > >>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> > >>> te(temp_max,lag)           39.0000  4.0875      NA      NA
> > >>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>> ```
> > >>> Some output from ```summary(conc39)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                   edf Ref.df  Chi.sq  p-value
> > >>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> > >>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> > >>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> > >>> -REML =   8915  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>>
> > >>> ```r
> > >>> $worst
> > >>>                                      para te(year,month,monthday)
> > >>> te(temp_max,lag) te(precip_daily_total,lag)
> > >>> para                       1.000000e+00            3.261007e-31
> > >>> 0.3313549                  0.6666532
> > >>> te(year,month,monthday)    3.060763e-31            1.000000e+00
> > >>> 0.8266086                  0.5670777
> > >>> te(temp_max,lag)           3.331014e-01            8.225942e-01
> > >>> 1.0000000                  0.5840875
> > >>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> > >>> 0.5939380                  1.0000000
> > >>> ```
> > >>>
> > >>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> > >>> various choices for k or as ```s(time)``` with various k does not
> > >>> reduce concurvity either.
> > >>>
> > >>>
> > >>> The default approach in time series studies of heat-mortality is to
> > >>> model time with fixed df, generally between 7-10 df per year of data.
> > >>> I am, however, apprehensive about this approach because a) mortality
> > >>> profiles vary with locality due to sociodemographic and environmental
> > >>> characteristics and b) the choice of df is based on higher income
> > >>> countries (where nearly all these studies have been done) with
> > >>> different mortality profiles and so may not be appropriate for
> > >>> tropical, low-income countries.
> > >>>
> > >>> Although the approach of fixing (high) df does remove more temporal
> > >>> patterns from the ACF (see model and output below), concurvity between
> > >>> time and lagged temperature has now risen to 0.99! Moreover,
> > >>> temperature (which has been a consistent, highly significant predictor
> > >>> in every model of the tens (hundreds?) I have run, has now turned
> > >>> non-significant. I am guessing this is because time is now a very
> > >>> wiggly function that not only models/ removes seasonal variation, but
> > >>> also some of the day-to-day variation that is needed for the
> > >>> temperature smooth  :
> > >>>
> > >>> ```r
> > >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> > >>>                         te(temp_max, lag, k=c(10,3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10,3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> > >>> ```
> > >>> Output from ```gam.check(conc20a, rep = 1000)```:
> > >>>
> > >>> ```r
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 9 iterations.
> > >>> Gradient range [-0.0008983099,9.546022e-05]
> > >>> (score 8750.13 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> > >>> Model rank =  336 / 336
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                    k'      edf k-index p-value
> > >>> s(time)                    111.0000 111.0000    0.98    0.56
> > >>> te(temp_max,lag)            29.0000   0.6548      NA      NA
> > >>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> > >>> ```
> > >>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
> > >>>
> > >>> ```r
> > >>>                                      para      s(time) te(temp_max,lag)
> > >>> te(precip_daily_total,lag)
> > >>> para                       1.000000e+00 2.462064e-19        0.3165236
> > >>>                   0.6666348
> > >>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
> > >>>                   0.6879284
> > >>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> > >>>                   0.5788711
> > >>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> > >>>                   1.0000000
> > >>>
> > >>> ```
> > >>>
> > >>> Some output from ```summary(conc20a)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                    edf Ref.df  Chi.sq p-value
> > >>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> > >>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> > >>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> > >>> -REML = 8750.1  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>> ACF functions:
> > >>>
> > >>> [4]: https://i.stack.imgur.com/7nbXS.png
> > >>> [5]: https://i.stack.imgur.com/pNnZU.png
> > >>>
> > >>> Data can be found on my [GitHub][6] site in the file
> > >>> [data_cross_validated_post2.rds][7]. A csv version is also available.
> > >>> This is my code:
> > >>>
> > >>> ```r
> > >>> library(readr)
> > >>> library(mgcv)
> > >>>
> > >>> df <- read_rds("data_crossvalidated_post2.rds")
> > >>>
> > >>> # Create matrices for lagged weather variables (6 day lags) based on
> > >>> example by Simon Wood
> > >>> # in his 2017 book ("Generalized additive models: an introduction with
> > >>> R", p. 349) and
> > >>> # gamair package documentation
> > >>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> > >>>
> > >>> lagard <- function(x,n.lag=7) {
> > >>> n <- length(x); X <- matrix(NA,n,n.lag)
> > >>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > >>> X
> > >>> }
> > >>>
> > >>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> > >>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> > >>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> > >>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> > >>> = df$precip_hourly_dailysum)
> > >>> dat$temp_max <- lagard(df$temp_max)
> > >>> dat$temp_min <- lagard(df$temp_min)
> > >>> dat$temp_mean <- lagard(df$temp_mean)
> > >>> dat$wbgt_max <- lagard(df$wbgt_max)
> > >>> dat$wbgt_mean <- lagard(df$wbgt_mean)
> > >>> dat$wbgt_min <- lagard(df$wbgt_min)
> > >>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> > >>> dat$sh_mean <- lagard(df$sh_mean)
> > >>> dat$solar_mean <- lagard(df$solar_mean)
> > >>> dat$wind2m_mean <- lagard(df$wind2m_mean)
> > >>> dat$sh_max <- lagard(df$sh_max)
> > >>> dat$solar_max <- lagard(df$solar_max)
> > >>> dat$wind2m_max <- lagard(df$wind2m_max)
> > >>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> > >>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> > >>> dat$precip_hourly <- lagard(df$precip_hourly)
> > >>> dat$precip_daily_total <- lagard( df$precip_daily_total)
> > >>> dat$temp <- lagard(df$temp)
> > >>> dat$sh <- lagard(df$sh)
> > >>> dat$rh <- lagard(df$rh)
> > >>> dat$solar <- lagard(df$solar)
> > >>> dat$wind2m <- lagard(df$wind2m)
> > >>>
> > >>>
> > >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> > >>> bs=c("cr","cc","cc","cc")) + heap +
> > >>>                         te(temp_max, lag, k=c(10, 3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> > >>> 52.5), weekday = c(0, 6.5)))
> > >>>
> > >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> > >>>                        te(temp_max, lag, k=c(10, 4)) +
> > >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> > >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                        knots = list(month = c(0.5, 12.5)))
> > >>>
> > >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> > >>>                         te(temp_max, lag, k=c(10,3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10,3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> > >>>
> > >>> ```
> > >>> Thank you if you've read this far!! :-))
> > >>>
> > >>>     [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> > >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> > >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> > >>>     [4]: https://i.stack.imgur.com/7nbXS.png
> > >>>     [5]: https://i.stack.imgur.com/pNnZU.png
> > >>>     [6]: https://github.com/JadeShodan/heat-mortality
> > >>>     [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> > >>>
> > >>> ______________________________________________
> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >>> and provide commented, minimal, self-contained, reproducible code.
> > >> --
> > >> Simon Wood, School of Mathematics, University of Edinburgh,
> > >> https://www.maths.ed.ac.uk/~swood34/
> > >>
> > --
> > Simon Wood, School of Mathematics, University of Edinburgh,
> > https://www.maths.ed.ac.uk/~swood34/
> >


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Fri Jun 10 12:19:57 2022
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Fri, 10 Jun 2022 12:19:57 +0200
Subject: [R] how to add comma to string vector?
Message-ID: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>

Hello,
I need to convert an R vector to a python array. Essentially, I got a
vector of strings where each element must be enclosed in single
quotes. The problem is that each closing single quote should contain a
comma. What is the regex trick to do that?
I tried with:
```
> (raws = c("field_1", "field_2",  "field_3"))
[1] "field_1" "field_2" "field_3"
> (items = sQuote(raws))
[1] "'field_1'" "'field_2'" "'field_3'"
> gsub("\'[:blank:]", "\',[:blank:]", items,  ignore.case = FALSE, perl = FALSE)
[1] "'field_1'" "'field_2'" "'field_3'"
> cat("python_Array = [", domain_list, "]\n")
python_Array = [ 'Index' 'Endpoints' 'Confounders' 'Arboexposure' ]
```
To note that `python_Array` does not have commas between elements and
`gsub` did not do anything...

Thank you


From jr@| @end|ng |rom po@teo@no  Fri Jun 10 12:31:56 2022
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Fri, 10 Jun 2022 10:31:56 +0000
Subject: [R] how to add comma to string vector?
In-Reply-To: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
References: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
Message-ID: <YqMdnKIYlOXIrw1X@posteo.no>

Hello ...  

raws <- c("field_1", "field_2",  "field_3")
paste0("['", paste0(raws, collapse="', '"), "']")

rasmus at eightforty ~ % python
Python 3.10.5 (main, Jun  6 2022, 18:49:26) [GCC 12.1.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> ['field_1', 'field_2', 'field_3']
['field_1', 'field_2', 'field_3']

?

R


From kry|ov@r00t @end|ng |rom gm@||@com  Fri Jun 10 12:34:00 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Fri, 10 Jun 2022 13:34:00 +0300
Subject: [R] how to add comma to string vector?
In-Reply-To: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
References: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
Message-ID: <20220610133400.14bdf8cf@arachnoid>

On Fri, 10 Jun 2022 12:19:57 +0200
Luigi Marongiu <marongiu.luigi at gmail.com> wrote:

> I need to convert an R vector to a python array.

Have you considered using the reticulate package on the R side or the
rpy2 package on the Python side? It's hard to cover all edge cases when
producing source code to be evaluated by another language. You may
encounter a corner case where you output a character special to Python
without properly escaping it and end up with an injection error (see
also: SQL injection vulnerability, the most widely known type of this
mistake). For producing R code from R values, there's deparse(), and
even that's not perfect:
https://bugs.r-project.org/show_bug.cgi?id=18232

Having said that, paste(sQuote(values, FALSE), collapse = ',') will do
the trick, but only if values are guaranteed not to contain single
quotes or other characters that have special meaning in Python. Note
the FALSE argument to sQuote: otherwise it could return ?Unicode
quotes?, `TeX quotes', or even ?guillemets?, depending on the options
set by the user.

-- 
Best regards,
Ivan


From tebert @end|ng |rom u||@edu  Fri Jun 10 12:48:26 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Fri, 10 Jun 2022 10:48:26 +0000
Subject: [R] how to add comma to string vector?
In-Reply-To: <20220610133400.14bdf8cf@arachnoid>
References: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
 <20220610133400.14bdf8cf@arachnoid>
Message-ID: <BN6PR2201MB15537DC973F89F73BDDBE727CFA69@BN6PR2201MB1553.namprd22.prod.outlook.com>

Have you considered saving it to a csv file in R and then reading the file in python?
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ivan Krylov
Sent: Friday, June 10, 2022 6:34 AM
To: Luigi Marongiu <marongiu.luigi at gmail.com>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] how to add comma to string vector?

[External Email]

On Fri, 10 Jun 2022 12:19:57 +0200
Luigi Marongiu <marongiu.luigi at gmail.com> wrote:

> I need to convert an R vector to a python array.

Have you considered using the reticulate package on the R side or the
rpy2 package on the Python side? It's hard to cover all edge cases when producing source code to be evaluated by another language. You may encounter a corner case where you output a character special to Python without properly escaping it and end up with an injection error (see
also: SQL injection vulnerability, the most widely known type of this mistake). For producing R code from R values, there's deparse(), and even that's not perfect:
https://urldefense.proofpoint.com/v2/url?u=https-3A__bugs.r-2Dproject.org_show-5Fbug.cgi-3Fid-3D18232&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=_jG6hqNCBjiWjIVFOx5Zm4cTTHAd-Rv2IBVjFZkWtwdR9QDc9qZI59xb7fZs6qtT&s=unYB3ajJ6LhkNKab1X8TGSNYaiYwhgEC7GZfknFfA9Q&e=

Having said that, paste(sQuote(values, FALSE), collapse = ',') will do the trick, but only if values are guaranteed not to contain single quotes or other characters that have special meaning in Python. Note the FALSE argument to sQuote: otherwise it could return ?Unicode quotes?, `TeX quotes', or even ?guillemets?, depending on the options set by the user.

--
Best regards,
Ivan

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=_jG6hqNCBjiWjIVFOx5Zm4cTTHAd-Rv2IBVjFZkWtwdR9QDc9qZI59xb7fZs6qtT&s=10-fNd6yyMRCfduzKbeG6MHUz5Tkf4rf6lbKc7OFuSM&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=_jG6hqNCBjiWjIVFOx5Zm4cTTHAd-Rv2IBVjFZkWtwdR9QDc9qZI59xb7fZs6qtT&s=j_RwC0Njzj5WVBbGGmslVCm2EoDQxs7iNbMXItIRobM&e=
and provide commented, minimal, self-contained, reproducible code.

From jr@| @end|ng |rom po@teo@no  Fri Jun 10 13:15:37 2022
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Fri, 10 Jun 2022 11:15:37 +0000
Subject: [R] how to add comma to string vector?
In-Reply-To: <BN6PR2201MB15537DC973F89F73BDDBE727CFA69@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
 <20220610133400.14bdf8cf@arachnoid>
 <BN6PR2201MB15537DC973F89F73BDDBE727CFA69@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <YqMn2StXjd7MvdxV@posteo.no>

> raws <- c("field_1", "field_2",  "field_3")
> reticulate::r_to_py(x=raws)
No non-system installation of Python could be found.
Would you like to download and install Miniconda?
Miniconda is an open source environment management system for Python.
See https://docs.conda.io/en/latest/miniconda.html for more details.

Would you like to install Miniconda? [Y/n]: n
Installation aborted.
['field_1', 'field_2', 'field_3']
> reticulate::py_config()
python:         /usr/bin/python3
libpython:      /usr/lib/libpython3.10.so
pythonhome:     //usr://usr
version:        3.10.5 (main, Jun  6 2022, 18:49:26) [GCC 12.1.0]
numpy:          /usr/lib/python3.10/site-packages/numpy
numpy_version:  1.22.4

python versions found:
 /usr/bin/python3
 /usr/bin/python
> reticulate::py_run_string('print(1+1)')
2
> reticulate::r_to_py(x=raws)
['field_1', 'field_2', 'field_3']


From pd@|gd @end|ng |rom gm@||@com  Fri Jun 10 14:01:41 2022
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Fri, 10 Jun 2022 14:01:41 +0200
Subject: [R] [Rd] R 4.2.1 scheduled for June 23
Message-ID: <492506A5-D562-4E87-BA6C-A25CB185AD57@gmail.com>

Full schedule available on developer.r-project.org in a short while.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

_______________________________________________
R-announce at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Fri Jun 10 14:09:12 2022
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Fri, 10 Jun 2022 14:09:12 +0200
Subject: [R] how to add comma to string vector?
In-Reply-To: <YqMdnKIYlOXIrw1X@posteo.no>
References: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
 <YqMdnKIYlOXIrw1X@posteo.no>
Message-ID: <CAMk+s2TkkPS-GOgbWJZ_tV2Y3pEUU1jZ43X2T6HO-h1AzimTDw@mail.gmail.com>

Thank you.
```
raws <- c("field_1", "field_2",  "field_3")
paste0("['", paste0(raws, collapse="', '"), "']")
```
gave the most fitting option...

On Fri, Jun 10, 2022 at 12:31 PM Rasmus Liland <jral at posteo.no> wrote:
>
> Hello ...
>
> raws <- c("field_1", "field_2",  "field_3")
> paste0("['", paste0(raws, collapse="', '"), "']")
>
> rasmus at eightforty ~ % python
> Python 3.10.5 (main, Jun  6 2022, 18:49:26) [GCC 12.1.0] on linux
> Type "help", "copyright", "credits" or "license" for more information.
> >>> ['field_1', 'field_2', 'field_3']
> ['field_1', 'field_2', 'field_3']
>
> ?
>
> R



-- 
Best regards,
Luigi


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri Jun 10 14:30:58 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 10 Jun 2022 13:30:58 +0100
Subject: [R] how to add comma to string vector?
In-Reply-To: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
References: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
Message-ID: <67183fd5-a04d-9ce5-a225-14fe985b07f3@sapo.pt>

Hello,

cat has a sep argument:

capture.output(cat(raws, sep = ","))
#[1] "field_1,field_2,field_3"


Instead of capture.output there is also cat argument file.

Hope this helps,

Rui Barradas


?s 11:19 de 10/06/2022, Luigi Marongiu escreveu:
> Hello,
> I need to convert an R vector to a python array. Essentially, I got a
> vector of strings where each element must be enclosed in single
> quotes. The problem is that each closing single quote should contain a
> comma. What is the regex trick to do that?
> I tried with:
> ```
>> (raws = c("field_1", "field_2",  "field_3"))
> [1] "field_1" "field_2" "field_3"
>> (items = sQuote(raws))
> [1] "'field_1'" "'field_2'" "'field_3'"
>> gsub("\'[:blank:]", "\',[:blank:]", items,  ignore.case = FALSE, perl = FALSE)
> [1] "'field_1'" "'field_2'" "'field_3'"
>> cat("python_Array = [", domain_list, "]\n")
> python_Array = [ 'Index' 'Endpoints' 'Confounders' 'Arboexposure' ]
> ```
> To note that `python_Array` does not have commas between elements and
> `gsub` did not do anything...
> 
> Thank you
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Wed Jun  8 06:44:39 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Wed, 8 Jun 2022 09:44:39 +0500
Subject: [R] A humble request regarding QAMG method in R
Message-ID: <CAMfKi3KtMG2cE6dNc6xYn--34XVNS-9tR7wr_h3KCuCeoP8VmA@mail.gmail.com>

Hi, Dear Respected Professors! I hope that you are doing well.
Kindly help me with the following:
I have the R-codes for the "Quantile Augmented Mean Group" method. The
relevant codes and papers are attached herewith.
My Issue:
When I run the given codes to estimate the results for my own data. I have
to face several errors. I humbly request the experts to help me to estimate
the results for my data.
Thank you so much for your precious time.

Regards

Muhammad Zubair Chishti
Ph.D. Student
School of Business,
Zhengzhou University, Henan, China.
My Google scholar link:
https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
My ResearchGate link: https://www.researchgate.net/profile/Muhammad-Chishti

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Journal of Applied Econometrics Volume issue 2020 [doi 10.1002_jae.2753] Harding, Matthew; Lamarche, Carlos; Pesaran, M. Hashem -- Common Correlated Effects Estimation of Heterogeneous Dynamic Panel_2.pdf
Type: application/pdf
Size: 1615687 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220608/1d9d2d17/attachment-0001.pdf>

From ||@t@ @end|ng |rom dewey@myzen@co@uk  Fri Jun 10 17:45:04 2022
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Fri, 10 Jun 2022 16:45:04 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k-1=WsD71xcSa7WV0Ef=tiY3_Gz2KxvmskY8QuJso+EYA@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-EXdbmLVNbEH9nN=FE3FYs4uqXgVtH3V35o6BU0z=rTw@mail.gmail.com>
 <91a30ffd-1aec-0f44-17a0-b632fae91014@dewey.myzen.co.uk>
 <CANg3_k-0=Z4KUE52mxHrSj4c+9bhJ+C4HDqxM2HqqVJ9um0WBQ@mail.gmail.com>
 <CANg3_k-1=WsD71xcSa7WV0Ef=tiY3_Gz2KxvmskY8QuJso+EYA@mail.gmail.com>
Message-ID: <bd555484-ba2c-024f-e5eb-9eb19b86761e@dewey.myzen.co.uk>

Dear Jade

It seems to me that there are several issues here.

1 - if you fit a separate parameter for each heaping day you efectively 
remove them from the model altogether. If they do carry meaningful 
information then that is undesirable.

2 - if the reason that there ae so many 15s is because people state, 
"Oh, it was in July" and either they or the interviewere impute 15 
because it si the month mid-point then it begins to look as though 
attempting to use daily data is fraught with problems and monthly might 
be better.

3 - if you fit a binary heap variable which is zero except for the 15 of 
the month and 1 for the 15 then you are taking out any general overall 
tendency to report 15 but you are allowing for the possibility that the 
relative size of heaping days can vary during the study period.

I am not an expert on this sort of model, that is Simon clearly, but it 
seems to me that, as usual, we have a mix of statistical and scientific 
questions here and you may need to rethink your sceientific model in the 
light of the issue with your data.

Michael

On 10/06/2022 10:30, jade.shodan at googlemail.com wrote:
> Hi Michael,
> 
> I don't think my reply to your email came through to the list, so am
> resending (see below). Problems with subscription have now hopefully
> been resolved. Apologies if this is a double posting!
> 
> On Thu, 9 Jun 2022 at 15:27, jade.shodan at googlemail.com
> <jade.shodan at googlemail.com> wrote:
>>
>> Hi Michael,
>>
>> Thanks for the reply! When I ran the gam  with the gam() function, the
>> model worked fine with heap having 169 levels. The same model with
>> bam() however, fails.  I don't understand the difference between bam()
>> and gam() at all (other than computational efficiency), but could the
>> fact that each level only has 1 data point be the reason for it?
>>
>> These heaping days are very large measurement errors I need to get rid
>> off, so I just want to take them out of the model altogether. (My data
>> is quite noisy already, because date of death is based on memory
>> recall, rather than formal death registration, due to the data being
>> from a low income country). Median of deaths is approx. 2 per day, but
>> on heaping days it can be as high as 50 or so.
>>
>> My understanding was that coding with 169 levels would effectively
>> take these measurements out of the model (but do correct me if I'm
>> wrong!)  I originally coded heap as a binary variable with 0 for
>> non-heaping days and 1 for heaping days, but was told that meant that
>> I was assuming the effect was the same for all heaping days. If I
>> coded with 12 or 14 levels, wouldn't that leave a lot of noise in the
>> data?
>>
>> Jade
>>
>> On Thu, 9 Jun 2022 at 14:52, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>>>
>>> Dear Jade
>>>
>>> Do you really need to fit a separate parameter for each heaping day? Can
>>> you not just make it a binary predictor or a categorical one with fewer
>>> levels, perhaps 14 (for heaping in each year) or 12 (for each calendar
>>> month). I have no idea whether that would help but it seems worth a try.
>>>
>>> Michael
>>>
>>> On 08/06/2022 18:15, jade.shodan--- via R-help wrote:
>>>> Hi Simon,
>>>>
>>>> Thanks so much for this!! I have two follow up questions, if you don't mind.
>>>>
>>>> 1. Does including an autoregressive term not adjust away part of the
>>>> effect of the response in a distributed lag model (where the outcome
>>>> accumulates over time)?
>>>> 2. I've tried to fit a model using bam (just a first attempt without
>>>> AR term), but including the factor variable heap creates errors:
>>>>
>>>> bam0 <- bam(deaths~te(year, month, week, weekday,
>>>> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
>>>>                         te(temp_max, lag, k=c(8, 3)) +
>>>>                         te(precip_daily_total, lag, k=c(8, 3)),
>>>>                         data = dat, family = nb, method = 'fREML',
>>>> select = TRUE, discrete = TRUE,
>>>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
>>>> 52.5), weekday = c(0, 6.5)))
>>>>
>>>> This model results in errors:
>>>>
>>>> Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
>>>>     step failure in theta estimation
>>>> Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
>>>> object$prior.weights)) :
>>>>     NaNs produced
>>>>
>>>>
>>>> Including heap as as.numeric(heap) runs the model without error
>>>> messages or warnings, but model diagnostics look terrible, and it also
>>>> doesn't make sense (to me) to make heap a numeric. The factor variable
>>>> heap (with 169 levels) codes the fact that all deaths for which no
>>>> date was known, were registered on the 15th day of each month. I've
>>>> coded all non-heaping days as 0. All heaping days were coded as a
>>>> value between 1-168. The time series spans 14 years, so a heaping day
>>>> in each month results in 14*12 levels = 168, plus one level for
>>>> non-heaping days.
>>>>
>>>> So my second question is: Does bam allow factor variables? And if not,
>>>> how should I model this heaping on the 15th day of the month instead?
>>>>
>>>> With thanks,
>>>>
>>>> Jade
>>>>
>>>> On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
>>>>>
>>>>> I would not worry too much about high concurvity between variables like
>>>>> temperature and time. This just reflects the fact that temperature has a
>>>>> strong temporal pattern.
>>>>>
>>>>> I would also not be too worried about the low p-values on the k check.
>>>>> The check only looks for pattern in the residuals when they are ordered
>>>>> with respect to the variables of a smooth. When you have time series
>>>>> data and some smooths involve time then it's hard not to pick up some
>>>>> degree of residual auto-correlation, which you often would not want to
>>>>> model with a higher rank smoother.
>>>>>
>>>>> The NAs  for the distributed lag terms just reflect the fact that there
>>>>> is no obvious way to order the residuals w.r.t. the covariates for such
>>>>> terms, so the simple check for residual pattern is not really possible.
>>>>>
>>>>> One simple approach is to fit using bam(...,discrete=TRUE) which will
>>>>> let you specify an AR1 parameter to mop up some of the residual
>>>>> auto-correlation without resorting to a high rank smooth that then does
>>>>> all the work of the covariates as well. The AR1 parameter can be set by
>>>>> looking at the ACF of the residuals of the model without this. You need
>>>>> to look at the ACF of suitably standardized residuals to check how well
>>>>> this has worked.
>>>>>
>>>>> best,
>>>>>
>>>>> Simon
>>>>>
>>>>> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
>>>>>> Hello everyone,
>>>>>>
>>>>>> A few days ago I asked a question about concurvity in a GAM (the
>>>>>> anologue of collinearity in a GLM) implemented in mgcv. I think my
>>>>>> question was a bit unfocussed, so I am retrying again, but with
>>>>>> additional information included about the autocorrelation function. I
>>>>>> have also posted about this on Cross Validated. Given all the model
>>>>>> output, it might make for easier
>>>>>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
>>>>>>
>>>>>> As mentioned previously, I have problems with concurvity in my thesis
>>>>>> research, and don't have access to a statistician who works with time
>>>>>> series, GAMs or R. I'd be very grateful for any (partial) answer,
>>>>>> however short. I'll gladly return the favour where I can! For really
>>>>>> helpful input I'd be more than happy to offer co-authorship on
>>>>>> publication. Deadlines are very close, and I'm heading towards having
>>>>>> no results at all if I can't solve this concurvity issue :(
>>>>>>
>>>>>> I'm using GAMs to try to understand the relationship between deaths
>>>>>> and heat-related variables (e.g. temperature and humidity), using
>>>>>> daily time series over a 14-year period from a tropical, low-income
>>>>>> country. My aim is to understand the relationship between these
>>>>>> variables and deaths, rather than pure prediction performance.
>>>>>>
>>>>>> The GAMs include distributed lag models (set up as 7-column matrices,
>>>>>> see code at bottom of post), since deaths may occur over several days
>>>>>> following exposure.
>>>>>>
>>>>>> Simple GAMs with just time, lagged temperature and lagged
>>>>>> precipitation (a potential confounder) show very high concurvity
>>>>>> between lagged temperature and time, regardless of the many different
>>>>>> ways I have tried to decompose time. The autocorrelation functions
>>>>>> (ACF) however, shows values close to zero, only just about breaching
>>>>>> the 'significance line' in a few instances. It does show patterning
>>>>>> though, although the regularity is difficult to define.
>>>>>>
>>>>>> My questions are:
>>>>>> 1) Should I be worried about the high concurvity, or can I ignore it
>>>>>> given the mostly non-significant ACF? I've read dozens of
>>>>>> heat-mortality modelling studies and none report on concurvity between
>>>>>> weather variables and time (though one 2012 paper discussed
>>>>>> autocorrelation).
>>>>>>
>>>>>> 2) If I cannot ignore it, what should I do to resolve it? Would
>>>>>> including an autoregressive term be appropriate, and if so, where can
>>>>>> I find a coded example of how to do this? I've also come across
>>>>>> sequential regression][1]. Would this be more or less appropriate? If
>>>>>> appropriate, a pointer to an example would be really appreciated!
>>>>>>
>>>>>> Some example GAMs are specified as follows:
>>>>>> ```r
>>>>>> conc38b <- gam(deaths~te(year, month, week, weekday,
>>>>>> bs=c("cr","cc","cc","cc")) + heap +
>>>>>>                          te(temp_max, lag, k=c(10, 3)) +
>>>>>>                          te(precip_daily_total, lag, k=c(10, 3)),
>>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE,
>>>>>>                          knots = list(month = c(0.5, 12.5), week = c(0.5,
>>>>>> 52.5), weekday = c(0, 6.5)))
>>>>>> ```
>>>>>> Concurvity for the above model between (temp_max, lag) and (year,
>>>>>> month, week, weekday) is 0.91:
>>>>>>
>>>>>> ```r
>>>>>> $worst
>>>>>>                                        para te(year,month,week,weekday)
>>>>>> te(temp_max,lag) te(precip_daily_total,lag)
>>>>>> para                        1.000000e+00                1.125625e-29
>>>>>>         0.3150073                  0.6666348
>>>>>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
>>>>>>         0.9060552                  0.6652313
>>>>>> te(temp_max,lag)            3.152795e-01                8.998113e-01
>>>>>>         1.0000000                  0.5781015
>>>>>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
>>>>>>         0.5805159                  1.0000000
>>>>>> ```
>>>>>>
>>>>>> Output from ```gam.check()```:
>>>>>> ```r
>>>>>> Method: REML   Optimizer: outer newton
>>>>>> full convergence after 16 iterations.
>>>>>> Gradient range [-0.01467332,0.003096643]
>>>>>> (score 8915.994 & scale 1).
>>>>>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
>>>>>> Model rank =  544 / 544
>>>>>>
>>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>>>>> indicate that k is too low, especially if edf is close to k'.
>>>>>>
>>>>>>                                      k'      edf k-index p-value
>>>>>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
>>>>>> te(temp_max,lag)             29.0000   3.3681      NA      NA
>>>>>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
>>>>>> ---
>>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>>> ```
>>>>>>
>>>>>> Some output from ```summary(conc38b)```:
>>>>>> ```r
>>>>>> Approximate significance of smooth terms:
>>>>>>                                      edf Ref.df  Chi.sq p-value
>>>>>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
>>>>>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
>>>>>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
>>>>>> ---
>>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>>>
>>>>>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
>>>>>> -REML =   8916  Scale est. = 1         n = 5107
>>>>>> ```
>>>>>>
>>>>>>
>>>>>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
>>>>>> pattern). They show peaks at 5 and 15, and then there seems to be a
>>>>>> recurring pattern at multiples of approx. 30 (suggesting month is not
>>>>>> modelled adequately?). Not sure what would cause the spikes at 5 and
>>>>>> 15. There is heaping of deaths on the 15th day of each month, to which
>>>>>> deaths with unknown date were allocated. This heaping was modelled
>>>>>> with categorical variable/ factor ```heap``` with 169 levels (0 for
>>>>>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
>>>>>> heaping day over the 14-year period):
>>>>>>
>>>>>>      [2]: https://i.stack.imgur.com/FzKyM.png
>>>>>>      [3]: https://i.stack.imgur.com/fE3aL.png
>>>>>>
>>>>>>
>>>>>> I get an identical looking ACF when I decompose time into (year,
>>>>>> month, monthday) as in model conc39 below, although concurvity between
>>>>>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
>>>>>>
>>>>>> ```r
>>>>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>>>>>>                         te(temp_max, lag, k=c(10, 4)) +
>>>>>>                         te(precip_daily_total, lag, k=c(10, 4)),
>>>>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
>>>>>>                         knots = list(month = c(0.5, 12.5)))
>>>>>> ```
>>>>>> ```r
>>>>>>
>>>>>> Method: REML   Optimizer: outer newton
>>>>>> full convergence after 14 iterations.
>>>>>> Gradient range [-0.001578187,6.155096e-05]
>>>>>> (score 8915.763 & scale 1).
>>>>>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
>>>>>> Model rank =  323 / 323
>>>>>>
>>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>>>>> indicate that k is too low, especially if edf is close to k'.
>>>>>>
>>>>>>                                    k'     edf k-index p-value
>>>>>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
>>>>>> te(temp_max,lag)           39.0000  4.0875      NA      NA
>>>>>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
>>>>>> ---
>>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>>> ```
>>>>>> Some output from ```summary(conc39)```:
>>>>>> ```r
>>>>>> Approximate significance of smooth terms:
>>>>>>                                    edf Ref.df  Chi.sq  p-value
>>>>>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
>>>>>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
>>>>>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
>>>>>> ---
>>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>>>
>>>>>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
>>>>>> -REML =   8915  Scale est. = 1         n = 5107
>>>>>> ```
>>>>>>
>>>>>>
>>>>>> ```r
>>>>>> $worst
>>>>>>                                       para te(year,month,monthday)
>>>>>> te(temp_max,lag) te(precip_daily_total,lag)
>>>>>> para                       1.000000e+00            3.261007e-31
>>>>>> 0.3313549                  0.6666532
>>>>>> te(year,month,monthday)    3.060763e-31            1.000000e+00
>>>>>> 0.8266086                  0.5670777
>>>>>> te(temp_max,lag)           3.331014e-01            8.225942e-01
>>>>>> 1.0000000                  0.5840875
>>>>>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
>>>>>> 0.5939380                  1.0000000
>>>>>> ```
>>>>>>
>>>>>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
>>>>>> various choices for k or as ```s(time)``` with various k does not
>>>>>> reduce concurvity either.
>>>>>>
>>>>>>
>>>>>> The default approach in time series studies of heat-mortality is to
>>>>>> model time with fixed df, generally between 7-10 df per year of data.
>>>>>> I am, however, apprehensive about this approach because a) mortality
>>>>>> profiles vary with locality due to sociodemographic and environmental
>>>>>> characteristics and b) the choice of df is based on higher income
>>>>>> countries (where nearly all these studies have been done) with
>>>>>> different mortality profiles and so may not be appropriate for
>>>>>> tropical, low-income countries.
>>>>>>
>>>>>> Although the approach of fixing (high) df does remove more temporal
>>>>>> patterns from the ACF (see model and output below), concurvity between
>>>>>> time and lagged temperature has now risen to 0.99! Moreover,
>>>>>> temperature (which has been a consistent, highly significant predictor
>>>>>> in every model of the tens (hundreds?) I have run, has now turned
>>>>>> non-significant. I am guessing this is because time is now a very
>>>>>> wiggly function that not only models/ removes seasonal variation, but
>>>>>> also some of the day-to-day variation that is needed for the
>>>>>> temperature smooth  :
>>>>>>
>>>>>> ```r
>>>>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>>>>>>                          te(temp_max, lag, k=c(10,3)) +
>>>>>>                          te(precip_daily_total, lag, k=c(10,3)),
>>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE)
>>>>>> ```
>>>>>> Output from ```gam.check(conc20a, rep = 1000)```:
>>>>>>
>>>>>> ```r
>>>>>> Method: REML   Optimizer: outer newton
>>>>>> full convergence after 9 iterations.
>>>>>> Gradient range [-0.0008983099,9.546022e-05]
>>>>>> (score 8750.13 & scale 1).
>>>>>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
>>>>>> Model rank =  336 / 336
>>>>>>
>>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>>>>> indicate that k is too low, especially if edf is close to k'.
>>>>>>
>>>>>>                                     k'      edf k-index p-value
>>>>>> s(time)                    111.0000 111.0000    0.98    0.56
>>>>>> te(temp_max,lag)            29.0000   0.6548      NA      NA
>>>>>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
>>>>>> ```
>>>>>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
>>>>>>
>>>>>> ```r
>>>>>>                                       para      s(time) te(temp_max,lag)
>>>>>> te(precip_daily_total,lag)
>>>>>> para                       1.000000e+00 2.462064e-19        0.3165236
>>>>>>                    0.6666348
>>>>>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
>>>>>>                    0.6879284
>>>>>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
>>>>>>                    0.5788711
>>>>>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
>>>>>>                    1.0000000
>>>>>>
>>>>>> ```
>>>>>>
>>>>>> Some output from ```summary(conc20a)```:
>>>>>> ```r
>>>>>> Approximate significance of smooth terms:
>>>>>>                                     edf Ref.df  Chi.sq p-value
>>>>>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
>>>>>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
>>>>>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
>>>>>> ---
>>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>>>
>>>>>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
>>>>>> -REML = 8750.1  Scale est. = 1         n = 5107
>>>>>> ```
>>>>>>
>>>>>> ACF functions:
>>>>>>
>>>>>> [4]: https://i.stack.imgur.com/7nbXS.png
>>>>>> [5]: https://i.stack.imgur.com/pNnZU.png
>>>>>>
>>>>>> Data can be found on my [GitHub][6] site in the file
>>>>>> [data_cross_validated_post2.rds][7]. A csv version is also available.
>>>>>> This is my code:
>>>>>>
>>>>>> ```r
>>>>>> library(readr)
>>>>>> library(mgcv)
>>>>>>
>>>>>> df <- read_rds("data_crossvalidated_post2.rds")
>>>>>>
>>>>>> # Create matrices for lagged weather variables (6 day lags) based on
>>>>>> example by Simon Wood
>>>>>> # in his 2017 book ("Generalized additive models: an introduction with
>>>>>> R", p. 349) and
>>>>>> # gamair package documentation
>>>>>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
>>>>>>
>>>>>> lagard <- function(x,n.lag=7) {
>>>>>> n <- length(x); X <- matrix(NA,n,n.lag)
>>>>>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
>>>>>> X
>>>>>> }
>>>>>>
>>>>>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
>>>>>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
>>>>>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
>>>>>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
>>>>>> = df$precip_hourly_dailysum)
>>>>>> dat$temp_max <- lagard(df$temp_max)
>>>>>> dat$temp_min <- lagard(df$temp_min)
>>>>>> dat$temp_mean <- lagard(df$temp_mean)
>>>>>> dat$wbgt_max <- lagard(df$wbgt_max)
>>>>>> dat$wbgt_mean <- lagard(df$wbgt_mean)
>>>>>> dat$wbgt_min <- lagard(df$wbgt_min)
>>>>>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
>>>>>> dat$sh_mean <- lagard(df$sh_mean)
>>>>>> dat$solar_mean <- lagard(df$solar_mean)
>>>>>> dat$wind2m_mean <- lagard(df$wind2m_mean)
>>>>>> dat$sh_max <- lagard(df$sh_max)
>>>>>> dat$solar_max <- lagard(df$solar_max)
>>>>>> dat$wind2m_max <- lagard(df$wind2m_max)
>>>>>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
>>>>>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
>>>>>> dat$precip_hourly <- lagard(df$precip_hourly)
>>>>>> dat$precip_daily_total <- lagard( df$precip_daily_total)
>>>>>> dat$temp <- lagard(df$temp)
>>>>>> dat$sh <- lagard(df$sh)
>>>>>> dat$rh <- lagard(df$rh)
>>>>>> dat$solar <- lagard(df$solar)
>>>>>> dat$wind2m <- lagard(df$wind2m)
>>>>>>
>>>>>>
>>>>>> conc38b <- gam(deaths~te(year, month, week, weekday,
>>>>>> bs=c("cr","cc","cc","cc")) + heap +
>>>>>>                          te(temp_max, lag, k=c(10, 3)) +
>>>>>>                          te(precip_daily_total, lag, k=c(10, 3)),
>>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE,
>>>>>>                          knots = list(month = c(0.5, 12.5), week = c(0.5,
>>>>>> 52.5), weekday = c(0, 6.5)))
>>>>>>
>>>>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>>>>>>                         te(temp_max, lag, k=c(10, 4)) +
>>>>>>                         te(precip_daily_total, lag, k=c(10, 4)),
>>>>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
>>>>>>                         knots = list(month = c(0.5, 12.5)))
>>>>>>
>>>>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>>>>>>                          te(temp_max, lag, k=c(10,3)) +
>>>>>>                          te(precip_daily_total, lag, k=c(10,3)),
>>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE)
>>>>>>
>>>>>> ```
>>>>>> Thank you if you've read this far!! :-))
>>>>>>
>>>>>>      [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
>>>>>>      [2]: https://i.stack.imgur.com/FzKyM.png
>>>>>>      [3]: https://i.stack.imgur.com/fE3aL.png
>>>>>>      [4]: https://i.stack.imgur.com/7nbXS.png
>>>>>>      [5]: https://i.stack.imgur.com/pNnZU.png
>>>>>>      [6]: https://github.com/JadeShodan/heat-mortality
>>>>>>      [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>> --
>>>>> Simon Wood, School of Mathematics, University of Edinburgh,
>>>>> https://www.maths.ed.ac.uk/~swood34/
>>>>>
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>> --
>>> Michael
>>> http://www.dewey.myzen.co.uk/home.html
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From @|mon@wood @end|ng |rom b@th@edu  Fri Jun 10 20:50:20 2022
From: @|mon@wood @end|ng |rom b@th@edu (Simon Wood)
Date: Fri, 10 Jun 2022 19:50:20 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k_qMX24_a-Hppk6knrH-nt1QMLj2FFSJbkMJc0TBxqzQA@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-g=M41cxom6DVdTunFjgZ6VfKobqW__Mq351pDpY22PQ@mail.gmail.com>
 <f371005e-6f57-2007-f859-d66595a19c0f@bath.edu>
 <CANg3_k_qMX24_a-Hppk6knrH-nt1QMLj2FFSJbkMJc0TBxqzQA@mail.gmail.com>
Message-ID: <80abf9e4-d864-43d3-e8ef-488239d22931@bath.edu>

Apologies, I meant...

aheap <- heap!="0"
aheap + s(heap,bs="re",by=aheap)

- fixed effect for heap day or not, and then an extra random effect if it is a heap day

... bit difficult to give a generic answer to the usefulness question - 
I guess you probably have to decide on that yourself.

Simon

On 09/06/2022 16:57, jade.shodan at googlemail.com wrote:
> Hi Simon,
>
> (Sorry, replies and answers are out of sync due to my problems posting
> to the list/ messages being held for moderation)
>
>> Did it actually fail, or simply generate warnings?
> The model was computed (you're right, not a model failure), but
> resulted in warnings (as per previous post) which, frankly, I didn't
> understand.
>
>> if I understand your model right there is one free parameter for each observation falling on the 15th
> That's right, one observation on each 15th day of the month.
>
> Thank you for the suggestion about the random effects! I had been
> wondering about how I could model this heaping with a smooth!
>
> Quick question:
>
> You proposed:
>
> aheap <- heap!="0"
> heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap
> day + a r.e. for each heap day - no effect on non-heap days
>
> Is there a typo in the code above? I don't see the newly created
> variable aheap in the model?
> Should it read "by = aheap" as follows:
>
> heap + s(heap,bs="re",by=aheap)   ?
>
> So a full model might then look like the one below?
>
>   bam0 <- bam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> s(heap,bs="re",by=aheap)
>                                       te(temp_max, lag, k=c(8, 3)) +
>                                       te(precip_daily_total, lag, k=c(8, 3)),
>                                       data = dat, family = nb, method =
> 'fREML', select = TRUE, discrete = TRUE,
>                                     knots = list(month = c(0.5, 12.5),
> week = c(0.5, 52.5), weekday = c(0, 6.5)))
>
> One, hopefully final (!) question:
>
> Is it actually useful at all to keep these observations on the 15th
> day of each month (which are huge errors), or am I better off removing
> them from the data set (or replacing them with e.g. median values)?
> For temperature-mortality modelling it is the day-to-day variation in
> deaths and temperature that is of interest. So is modelling heaping
> actually useful at all, given that this variable changes on a monthly
> basis? (I think you are alluding to this, but I just want to make
> sure).
>
> If I take them out altogether, would I be best off removing all data
> for these dates, so that the time series jumps from day 14 to day 16?
> Or would this create problems with e.g. the distributed lag model?
>
> Sorry for all these questions! Have been struggling with this for
> months (posted on Cross Validated about the heaping issue too), and
> feel I am finally getting somewhere with your help!
>
> Jade
>
> On Thu, 9 Jun 2022 at 15:24, Simon Wood <simon.wood at bath.edu> wrote:
>>
>> On 09/06/2022 12:30, jade.shodan at googlemail.com wrote:
>>> Hi Simon,
>>>
>>> Thanks so much for this!! (Apologies if this is a double posting. I
>>> seem to have a problem getting messages through to the list).
>>>
>>> I have two follow up questions, if you don't mind.
>>>
>>> 1. Does including an autoregressive term not adjust away part of the
>>> effect of the response in a distributed lag model (where the outcome
>>> accumulates over time)?
>> - the hope is that it approximately deals with short timescale stuff
>> without interfering with the longer timescales to the same extent as a
>> high rank smooth would.
>>
>>> 2. I've tried to fit a model using bam (just a first attempt without
>>> AR term), but including the factor variable heap creates errors:
>>>
>>> bam0 <- bam(deaths~te(year, month, week, weekday,
>>> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
>>>                                        te(temp_max, lag, k=c(8, 3)) +
>>>                                        te(precip_daily_total, lag, k=c(8, 3)),
>>>                                       data = dat, family = nb, method =
>>> 'fREML', select = TRUE, discrete = TRUE,
>>>                                       knots = list(month = c(0.5, 12.5),
>>> week = c(0.5, 52.5), weekday = c(0, 6.5)))
>>>
>>> This model results in errors:
>>>
>>> Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
>>>     step failure in theta estimation
>>> Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
>>> object$prior.weights)) :
>>>     NaNs produced
>>>
>> Did it actually fail, or simply generate warnings?
>>
>> 'bam' handles factors, but if I understand your model right there is one
>> free parameter for each observation falling on the 15th, so that you
>> will fit data for those days exactly (and might just as well have
>> dropped them, for all the information they contribute to the rest of the
>> model). If you want a structure like this, I'd be inclined to make the
>> heap variable random, something like...
>>
>> aheap <- heap!="0"
>>
>> heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap day
>> + a r.e. for each heap day - no effect on non-heap days
>>
>> best,
>>
>> Simon
>>
>>> Including heap as as.numeric(heap) runs the model without error
>>> messages or warnings, but model diagnostics look terrible, and it also
>>> doesn't make sense (to me) to make heap a numeric. The factor variable
>>> heap (with 169 levels) codes the fact that all deaths for which no
>>> date was known, were registered on the 15th day of each month. I've
>>> coded all non-heaping days as 0. All heaping days were coded as a
>>> value between 1-168. The time series spans 14 years, so a heaping day
>>> in each month results in 14*12 levels = 168, plus one level for
>>> non-heaping days.
>>>
>>> So my second question is: Does bam allow factor variables? And if not,
>>> how should I model this heaping on the 15th day of the month instead?
>>>
>>> With thanks,
>>>
>>> Jade
>>>
>>> On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
>>>> I would not worry too much about high concurvity between variables like
>>>> temperature and time. This just reflects the fact that temperature has a
>>>> strong temporal pattern.
>>>>
>>>> I would also not be too worried about the low p-values on the k check.
>>>> The check only looks for pattern in the residuals when they are ordered
>>>> with respect to the variables of a smooth. When you have time series
>>>> data and some smooths involve time then it's hard not to pick up some
>>>> degree of residual auto-correlation, which you often would not want to
>>>> model with a higher rank smoother.
>>>>
>>>> The NAs  for the distributed lag terms just reflect the fact that there
>>>> is no obvious way to order the residuals w.r.t. the covariates for such
>>>> terms, so the simple check for residual pattern is not really possible.
>>>>
>>>> One simple approach is to fit using bam(...,discrete=TRUE) which will
>>>> let you specify an AR1 parameter to mop up some of the residual
>>>> auto-correlation without resorting to a high rank smooth that then does
>>>> all the work of the covariates as well. The AR1 parameter can be set by
>>>> looking at the ACF of the residuals of the model without this. You need
>>>> to look at the ACF of suitably standardized residuals to check how well
>>>> this has worked.
>>>>
>>>> best,
>>>>
>>>> Simon
>>>>
>>>> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
>>>>> Hello everyone,
>>>>>
>>>>> A few days ago I asked a question about concurvity in a GAM (the
>>>>> anologue of collinearity in a GLM) implemented in mgcv. I think my
>>>>> question was a bit unfocussed, so I am retrying again, but with
>>>>> additional information included about the autocorrelation function. I
>>>>> have also posted about this on Cross Validated. Given all the model
>>>>> output, it might make for easier
>>>>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
>>>>>
>>>>> As mentioned previously, I have problems with concurvity in my thesis
>>>>> research, and don't have access to a statistician who works with time
>>>>> series, GAMs or R. I'd be very grateful for any (partial) answer,
>>>>> however short. I'll gladly return the favour where I can! For really
>>>>> helpful input I'd be more than happy to offer co-authorship on
>>>>> publication. Deadlines are very close, and I'm heading towards having
>>>>> no results at all if I can't solve this concurvity issue :(
>>>>>
>>>>> I'm using GAMs to try to understand the relationship between deaths
>>>>> and heat-related variables (e.g. temperature and humidity), using
>>>>> daily time series over a 14-year period from a tropical, low-income
>>>>> country. My aim is to understand the relationship between these
>>>>> variables and deaths, rather than pure prediction performance.
>>>>>
>>>>> The GAMs include distributed lag models (set up as 7-column matrices,
>>>>> see code at bottom of post), since deaths may occur over several days
>>>>> following exposure.
>>>>>
>>>>> Simple GAMs with just time, lagged temperature and lagged
>>>>> precipitation (a potential confounder) show very high concurvity
>>>>> between lagged temperature and time, regardless of the many different
>>>>> ways I have tried to decompose time. The autocorrelation functions
>>>>> (ACF) however, shows values close to zero, only just about breaching
>>>>> the 'significance line' in a few instances. It does show patterning
>>>>> though, although the regularity is difficult to define.
>>>>>
>>>>> My questions are:
>>>>> 1) Should I be worried about the high concurvity, or can I ignore it
>>>>> given the mostly non-significant ACF? I've read dozens of
>>>>> heat-mortality modelling studies and none report on concurvity between
>>>>> weather variables and time (though one 2012 paper discussed
>>>>> autocorrelation).
>>>>>
>>>>> 2) If I cannot ignore it, what should I do to resolve it? Would
>>>>> including an autoregressive term be appropriate, and if so, where can
>>>>> I find a coded example of how to do this? I've also come across
>>>>> sequential regression][1]. Would this be more or less appropriate? If
>>>>> appropriate, a pointer to an example would be really appreciated!
>>>>>
>>>>> Some example GAMs are specified as follows:
>>>>> ```r
>>>>> conc38b <- gam(deaths~te(year, month, week, weekday,
>>>>> bs=c("cr","cc","cc","cc")) + heap +
>>>>>                          te(temp_max, lag, k=c(10, 3)) +
>>>>>                          te(precip_daily_total, lag, k=c(10, 3)),
>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE,
>>>>>                          knots = list(month = c(0.5, 12.5), week = c(0.5,
>>>>> 52.5), weekday = c(0, 6.5)))
>>>>> ```
>>>>> Concurvity for the above model between (temp_max, lag) and (year,
>>>>> month, week, weekday) is 0.91:
>>>>>
>>>>> ```r
>>>>> $worst
>>>>>                                        para te(year,month,week,weekday)
>>>>> te(temp_max,lag) te(precip_daily_total,lag)
>>>>> para                        1.000000e+00                1.125625e-29
>>>>>         0.3150073                  0.6666348
>>>>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
>>>>>         0.9060552                  0.6652313
>>>>> te(temp_max,lag)            3.152795e-01                8.998113e-01
>>>>>         1.0000000                  0.5781015
>>>>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
>>>>>         0.5805159                  1.0000000
>>>>> ```
>>>>>
>>>>> Output from ```gam.check()```:
>>>>> ```r
>>>>> Method: REML   Optimizer: outer newton
>>>>> full convergence after 16 iterations.
>>>>> Gradient range [-0.01467332,0.003096643]
>>>>> (score 8915.994 & scale 1).
>>>>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
>>>>> Model rank =  544 / 544
>>>>>
>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>>>> indicate that k is too low, especially if edf is close to k'.
>>>>>
>>>>>                                      k'      edf k-index p-value
>>>>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
>>>>> te(temp_max,lag)             29.0000   3.3681      NA      NA
>>>>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
>>>>> ---
>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>> ```
>>>>>
>>>>> Some output from ```summary(conc38b)```:
>>>>> ```r
>>>>> Approximate significance of smooth terms:
>>>>>                                      edf Ref.df  Chi.sq p-value
>>>>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
>>>>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
>>>>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
>>>>> ---
>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>>
>>>>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
>>>>> -REML =   8916  Scale est. = 1         n = 5107
>>>>> ```
>>>>>
>>>>>
>>>>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
>>>>> pattern). They show peaks at 5 and 15, and then there seems to be a
>>>>> recurring pattern at multiples of approx. 30 (suggesting month is not
>>>>> modelled adequately?). Not sure what would cause the spikes at 5 and
>>>>> 15. There is heaping of deaths on the 15th day of each month, to which
>>>>> deaths with unknown date were allocated. This heaping was modelled
>>>>> with categorical variable/ factor ```heap``` with 169 levels (0 for
>>>>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
>>>>> heaping day over the 14-year period):
>>>>>
>>>>>      [2]: https://i.stack.imgur.com/FzKyM.png
>>>>>      [3]: https://i.stack.imgur.com/fE3aL.png
>>>>>
>>>>>
>>>>> I get an identical looking ACF when I decompose time into (year,
>>>>> month, monthday) as in model conc39 below, although concurvity between
>>>>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
>>>>>
>>>>> ```r
>>>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>>>>>                         te(temp_max, lag, k=c(10, 4)) +
>>>>>                         te(precip_daily_total, lag, k=c(10, 4)),
>>>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
>>>>>                         knots = list(month = c(0.5, 12.5)))
>>>>> ```
>>>>> ```r
>>>>>
>>>>> Method: REML   Optimizer: outer newton
>>>>> full convergence after 14 iterations.
>>>>> Gradient range [-0.001578187,6.155096e-05]
>>>>> (score 8915.763 & scale 1).
>>>>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
>>>>> Model rank =  323 / 323
>>>>>
>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>>>> indicate that k is too low, especially if edf is close to k'.
>>>>>
>>>>>                                    k'     edf k-index p-value
>>>>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
>>>>> te(temp_max,lag)           39.0000  4.0875      NA      NA
>>>>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
>>>>> ---
>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>> ```
>>>>> Some output from ```summary(conc39)```:
>>>>> ```r
>>>>> Approximate significance of smooth terms:
>>>>>                                    edf Ref.df  Chi.sq  p-value
>>>>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
>>>>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
>>>>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
>>>>> ---
>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>>
>>>>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
>>>>> -REML =   8915  Scale est. = 1         n = 5107
>>>>> ```
>>>>>
>>>>>
>>>>> ```r
>>>>> $worst
>>>>>                                       para te(year,month,monthday)
>>>>> te(temp_max,lag) te(precip_daily_total,lag)
>>>>> para                       1.000000e+00            3.261007e-31
>>>>> 0.3313549                  0.6666532
>>>>> te(year,month,monthday)    3.060763e-31            1.000000e+00
>>>>> 0.8266086                  0.5670777
>>>>> te(temp_max,lag)           3.331014e-01            8.225942e-01
>>>>> 1.0000000                  0.5840875
>>>>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
>>>>> 0.5939380                  1.0000000
>>>>> ```
>>>>>
>>>>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
>>>>> various choices for k or as ```s(time)``` with various k does not
>>>>> reduce concurvity either.
>>>>>
>>>>>
>>>>> The default approach in time series studies of heat-mortality is to
>>>>> model time with fixed df, generally between 7-10 df per year of data.
>>>>> I am, however, apprehensive about this approach because a) mortality
>>>>> profiles vary with locality due to sociodemographic and environmental
>>>>> characteristics and b) the choice of df is based on higher income
>>>>> countries (where nearly all these studies have been done) with
>>>>> different mortality profiles and so may not be appropriate for
>>>>> tropical, low-income countries.
>>>>>
>>>>> Although the approach of fixing (high) df does remove more temporal
>>>>> patterns from the ACF (see model and output below), concurvity between
>>>>> time and lagged temperature has now risen to 0.99! Moreover,
>>>>> temperature (which has been a consistent, highly significant predictor
>>>>> in every model of the tens (hundreds?) I have run, has now turned
>>>>> non-significant. I am guessing this is because time is now a very
>>>>> wiggly function that not only models/ removes seasonal variation, but
>>>>> also some of the day-to-day variation that is needed for the
>>>>> temperature smooth  :
>>>>>
>>>>> ```r
>>>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>>>>>                          te(temp_max, lag, k=c(10,3)) +
>>>>>                          te(precip_daily_total, lag, k=c(10,3)),
>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE)
>>>>> ```
>>>>> Output from ```gam.check(conc20a, rep = 1000)```:
>>>>>
>>>>> ```r
>>>>> Method: REML   Optimizer: outer newton
>>>>> full convergence after 9 iterations.
>>>>> Gradient range [-0.0008983099,9.546022e-05]
>>>>> (score 8750.13 & scale 1).
>>>>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
>>>>> Model rank =  336 / 336
>>>>>
>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>>>> indicate that k is too low, especially if edf is close to k'.
>>>>>
>>>>>                                     k'      edf k-index p-value
>>>>> s(time)                    111.0000 111.0000    0.98    0.56
>>>>> te(temp_max,lag)            29.0000   0.6548      NA      NA
>>>>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
>>>>> ```
>>>>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
>>>>>
>>>>> ```r
>>>>>                                       para      s(time) te(temp_max,lag)
>>>>> te(precip_daily_total,lag)
>>>>> para                       1.000000e+00 2.462064e-19        0.3165236
>>>>>                    0.6666348
>>>>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
>>>>>                    0.6879284
>>>>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
>>>>>                    0.5788711
>>>>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
>>>>>                    1.0000000
>>>>>
>>>>> ```
>>>>>
>>>>> Some output from ```summary(conc20a)```:
>>>>> ```r
>>>>> Approximate significance of smooth terms:
>>>>>                                     edf Ref.df  Chi.sq p-value
>>>>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
>>>>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
>>>>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
>>>>> ---
>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>>
>>>>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
>>>>> -REML = 8750.1  Scale est. = 1         n = 5107
>>>>> ```
>>>>>
>>>>> ACF functions:
>>>>>
>>>>> [4]: https://i.stack.imgur.com/7nbXS.png
>>>>> [5]: https://i.stack.imgur.com/pNnZU.png
>>>>>
>>>>> Data can be found on my [GitHub][6] site in the file
>>>>> [data_cross_validated_post2.rds][7]. A csv version is also available.
>>>>> This is my code:
>>>>>
>>>>> ```r
>>>>> library(readr)
>>>>> library(mgcv)
>>>>>
>>>>> df <- read_rds("data_crossvalidated_post2.rds")
>>>>>
>>>>> # Create matrices for lagged weather variables (6 day lags) based on
>>>>> example by Simon Wood
>>>>> # in his 2017 book ("Generalized additive models: an introduction with
>>>>> R", p. 349) and
>>>>> # gamair package documentation
>>>>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
>>>>>
>>>>> lagard <- function(x,n.lag=7) {
>>>>> n <- length(x); X <- matrix(NA,n,n.lag)
>>>>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
>>>>> X
>>>>> }
>>>>>
>>>>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
>>>>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
>>>>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
>>>>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
>>>>> = df$precip_hourly_dailysum)
>>>>> dat$temp_max <- lagard(df$temp_max)
>>>>> dat$temp_min <- lagard(df$temp_min)
>>>>> dat$temp_mean <- lagard(df$temp_mean)
>>>>> dat$wbgt_max <- lagard(df$wbgt_max)
>>>>> dat$wbgt_mean <- lagard(df$wbgt_mean)
>>>>> dat$wbgt_min <- lagard(df$wbgt_min)
>>>>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
>>>>> dat$sh_mean <- lagard(df$sh_mean)
>>>>> dat$solar_mean <- lagard(df$solar_mean)
>>>>> dat$wind2m_mean <- lagard(df$wind2m_mean)
>>>>> dat$sh_max <- lagard(df$sh_max)
>>>>> dat$solar_max <- lagard(df$solar_max)
>>>>> dat$wind2m_max <- lagard(df$wind2m_max)
>>>>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
>>>>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
>>>>> dat$precip_hourly <- lagard(df$precip_hourly)
>>>>> dat$precip_daily_total <- lagard( df$precip_daily_total)
>>>>> dat$temp <- lagard(df$temp)
>>>>> dat$sh <- lagard(df$sh)
>>>>> dat$rh <- lagard(df$rh)
>>>>> dat$solar <- lagard(df$solar)
>>>>> dat$wind2m <- lagard(df$wind2m)
>>>>>
>>>>>
>>>>> conc38b <- gam(deaths~te(year, month, week, weekday,
>>>>> bs=c("cr","cc","cc","cc")) + heap +
>>>>>                          te(temp_max, lag, k=c(10, 3)) +
>>>>>                          te(precip_daily_total, lag, k=c(10, 3)),
>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE,
>>>>>                          knots = list(month = c(0.5, 12.5), week = c(0.5,
>>>>> 52.5), weekday = c(0, 6.5)))
>>>>>
>>>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>>>>>                         te(temp_max, lag, k=c(10, 4)) +
>>>>>                         te(precip_daily_total, lag, k=c(10, 4)),
>>>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
>>>>>                         knots = list(month = c(0.5, 12.5)))
>>>>>
>>>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>>>>>                          te(temp_max, lag, k=c(10,3)) +
>>>>>                          te(precip_daily_total, lag, k=c(10,3)),
>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE)
>>>>>
>>>>> ```
>>>>> Thank you if you've read this far!! :-))
>>>>>
>>>>>      [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
>>>>>      [2]: https://i.stack.imgur.com/FzKyM.png
>>>>>      [3]: https://i.stack.imgur.com/fE3aL.png
>>>>>      [4]: https://i.stack.imgur.com/7nbXS.png
>>>>>      [5]: https://i.stack.imgur.com/pNnZU.png
>>>>>      [6]: https://github.com/JadeShodan/heat-mortality
>>>>>      [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> --
>>>> Simon Wood, School of Mathematics, University of Edinburgh,
>>>> https://www.maths.ed.ac.uk/~swood34/
>>>>
>> --
>> Simon Wood, School of Mathematics, University of Edinburgh,
>> https://www.maths.ed.ac.uk/~swood34/
>>
-- 
Simon Wood, School of Mathematics, University of Edinburgh,
https://www.maths.ed.ac.uk/~swood34/


From j@de@shod@@ m@iii@g oii googiem@ii@com  Fri Jun 10 21:19:10 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Fri, 10 Jun 2022 20:19:10 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <bd555484-ba2c-024f-e5eb-9eb19b86761e@dewey.myzen.co.uk>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-EXdbmLVNbEH9nN=FE3FYs4uqXgVtH3V35o6BU0z=rTw@mail.gmail.com>
 <91a30ffd-1aec-0f44-17a0-b632fae91014@dewey.myzen.co.uk>
 <CANg3_k-0=Z4KUE52mxHrSj4c+9bhJ+C4HDqxM2HqqVJ9um0WBQ@mail.gmail.com>
 <CANg3_k-1=WsD71xcSa7WV0Ef=tiY3_Gz2KxvmskY8QuJso+EYA@mail.gmail.com>
 <bd555484-ba2c-024f-e5eb-9eb19b86761e@dewey.myzen.co.uk>
Message-ID: <CANg3_k8VqTpqCw-W7YCZ4kZ_+gnsq5XhEKBCz64HGVoyOUC_Dw@mail.gmail.com>

Hi Michael,

Thanks for your input on this. It's appreciated!

 I think I did not make this very clear in my post here, but I am
trying to model day-to-day variation in deaths and temperature. In
other words, if people get exposed to high temperature today, are
there increased numbers of deaths over the following 6 days? Because
I'm trying to model day-to-day variation, I'm not interested in
seasonal effects (it's a nuisance variable), so I am trying to remove
seasonality.There's a somewhat better post of my problem on Coss
Validated: https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea

The deaths on the 15th of each month are huge errors. The data is from
a low-income country with less than adequate registration of deaths.
This data was collected by interviewers visiting housholds at half
yearly, or even yearly (or larger) intervals and asking if any deaths
had occurred over this period. If people could not remember the
specific date on which a death had occurred, it was - as a rule -
registered on the 15th of the month in which they though the death had
occurred. So the number of deaths on the 15th of each month does not
contribute anything useful to the model. It bears no relation to the
temperature on that day. It carries seasonal information, but I'm not
interested in that, and in fact need to remove that.  Hence why I
modelled those days as a factor variable with 169 levels (14 years of
data x 12 months +1 for all non-heaping days). Because there is only
one data point for each day, the idea was that this would remove any
effect of heaping. Following on from Simon Wood's comments, I'm now
thinking about using interpolation of deaths on those days, rather
than trying to control for heaping in the model. Removing rows of data
for each 15th day of the month doesn't seem right to me, because the
model includes distributed lag. Because of this distributed lag I am
still interested in the temperature on the 15th of each month, because
that they may have lagged effect over the following few days. I just
want to get rid of the error in deaths on the 15th, which I think I
can do by interpolation.

The ACF showed a peak at 15 (albeit barely significant). I don't
understand why that would happen given that heap as a factor with 169
levels should have effectively removed the effect of heaping, but it's
too much of a coincidence, so hopefully interpolation will help with
this too.

As for the concurvity between time and temperature, am going to see if
Simon's suggestion of adding an autoregressive term AR1 term does
anything to reduce it.

Thanks to both you and Simon for your input!

Jade

On Fri, 10 Jun 2022 at 16:45, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>
> Dear Jade
>
> It seems to me that there are several issues here.
>
> 1 - if you fit a separate parameter for each heaping day you efectively
> remove them from the model altogether. If they do carry meaningful
> information then that is undesirable.
>
> 2 - if the reason that there ae so many 15s is because people state,
> "Oh, it was in July" and either they or the interviewere impute 15
> because it si the month mid-point then it begins to look as though
> attempting to use daily data is fraught with problems and monthly might
> be better.
>
> 3 - if you fit a binary heap variable which is zero except for the 15 of
> the month and 1 for the 15 then you are taking out any general overall
> tendency to report 15 but you are allowing for the possibility that the
> relative size of heaping days can vary during the study period.
>
> I am not an expert on this sort of model, that is Simon clearly, but it
> seems to me that, as usual, we have a mix of statistical and scientific
> questions here and you may need to rethink your sceientific model in the
> light of the issue with your data.
>
> Michael
>
> On 10/06/2022 10:30, jade.shodan at googlemail.com wrote:
> > Hi Michael,
> >
> > I don't think my reply to your email came through to the list, so am
> > resending (see below). Problems with subscription have now hopefully
> > been resolved. Apologies if this is a double posting!
> >
> > On Thu, 9 Jun 2022 at 15:27, jade.shodan at googlemail.com
> > <jade.shodan at googlemail.com> wrote:
> >>
> >> Hi Michael,
> >>
> >> Thanks for the reply! When I ran the gam  with the gam() function, the
> >> model worked fine with heap having 169 levels. The same model with
> >> bam() however, fails.  I don't understand the difference between bam()
> >> and gam() at all (other than computational efficiency), but could the
> >> fact that each level only has 1 data point be the reason for it?
> >>
> >> These heaping days are very large measurement errors I need to get rid
> >> off, so I just want to take them out of the model altogether. (My data
> >> is quite noisy already, because date of death is based on memory
> >> recall, rather than formal death registration, due to the data being
> >> from a low income country). Median of deaths is approx. 2 per day, but
> >> on heaping days it can be as high as 50 or so.
> >>
> >> My understanding was that coding with 169 levels would effectively
> >> take these measurements out of the model (but do correct me if I'm
> >> wrong!)  I originally coded heap as a binary variable with 0 for
> >> non-heaping days and 1 for heaping days, but was told that meant that
> >> I was assuming the effect was the same for all heaping days. If I
> >> coded with 12 or 14 levels, wouldn't that leave a lot of noise in the
> >> data?
> >>
> >> Jade
> >>
> >> On Thu, 9 Jun 2022 at 14:52, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
> >>>
> >>> Dear Jade
> >>>
> >>> Do you really need to fit a separate parameter for each heaping day? Can
> >>> you not just make it a binary predictor or a categorical one with fewer
> >>> levels, perhaps 14 (for heaping in each year) or 12 (for each calendar
> >>> month). I have no idea whether that would help but it seems worth a try.
> >>>
> >>> Michael
> >>>
> >>> On 08/06/2022 18:15, jade.shodan--- via R-help wrote:
> >>>> Hi Simon,
> >>>>
> >>>> Thanks so much for this!! I have two follow up questions, if you don't mind.
> >>>>
> >>>> 1. Does including an autoregressive term not adjust away part of the
> >>>> effect of the response in a distributed lag model (where the outcome
> >>>> accumulates over time)?
> >>>> 2. I've tried to fit a model using bam (just a first attempt without
> >>>> AR term), but including the factor variable heap creates errors:
> >>>>
> >>>> bam0 <- bam(deaths~te(year, month, week, weekday,
> >>>> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> >>>>                         te(temp_max, lag, k=c(8, 3)) +
> >>>>                         te(precip_daily_total, lag, k=c(8, 3)),
> >>>>                         data = dat, family = nb, method = 'fREML',
> >>>> select = TRUE, discrete = TRUE,
> >>>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>>> 52.5), weekday = c(0, 6.5)))
> >>>>
> >>>> This model results in errors:
> >>>>
> >>>> Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
> >>>>     step failure in theta estimation
> >>>> Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> >>>> object$prior.weights)) :
> >>>>     NaNs produced
> >>>>
> >>>>
> >>>> Including heap as as.numeric(heap) runs the model without error
> >>>> messages or warnings, but model diagnostics look terrible, and it also
> >>>> doesn't make sense (to me) to make heap a numeric. The factor variable
> >>>> heap (with 169 levels) codes the fact that all deaths for which no
> >>>> date was known, were registered on the 15th day of each month. I've
> >>>> coded all non-heaping days as 0. All heaping days were coded as a
> >>>> value between 1-168. The time series spans 14 years, so a heaping day
> >>>> in each month results in 14*12 levels = 168, plus one level for
> >>>> non-heaping days.
> >>>>
> >>>> So my second question is: Does bam allow factor variables? And if not,
> >>>> how should I model this heaping on the 15th day of the month instead?
> >>>>
> >>>> With thanks,
> >>>>
> >>>> Jade
> >>>>
> >>>> On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
> >>>>>
> >>>>> I would not worry too much about high concurvity between variables like
> >>>>> temperature and time. This just reflects the fact that temperature has a
> >>>>> strong temporal pattern.
> >>>>>
> >>>>> I would also not be too worried about the low p-values on the k check.
> >>>>> The check only looks for pattern in the residuals when they are ordered
> >>>>> with respect to the variables of a smooth. When you have time series
> >>>>> data and some smooths involve time then it's hard not to pick up some
> >>>>> degree of residual auto-correlation, which you often would not want to
> >>>>> model with a higher rank smoother.
> >>>>>
> >>>>> The NAs  for the distributed lag terms just reflect the fact that there
> >>>>> is no obvious way to order the residuals w.r.t. the covariates for such
> >>>>> terms, so the simple check for residual pattern is not really possible.
> >>>>>
> >>>>> One simple approach is to fit using bam(...,discrete=TRUE) which will
> >>>>> let you specify an AR1 parameter to mop up some of the residual
> >>>>> auto-correlation without resorting to a high rank smooth that then does
> >>>>> all the work of the covariates as well. The AR1 parameter can be set by
> >>>>> looking at the ACF of the residuals of the model without this. You need
> >>>>> to look at the ACF of suitably standardized residuals to check how well
> >>>>> this has worked.
> >>>>>
> >>>>> best,
> >>>>>
> >>>>> Simon
> >>>>>
> >>>>> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> >>>>>> Hello everyone,
> >>>>>>
> >>>>>> A few days ago I asked a question about concurvity in a GAM (the
> >>>>>> anologue of collinearity in a GLM) implemented in mgcv. I think my
> >>>>>> question was a bit unfocussed, so I am retrying again, but with
> >>>>>> additional information included about the autocorrelation function. I
> >>>>>> have also posted about this on Cross Validated. Given all the model
> >>>>>> output, it might make for easier
> >>>>>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> >>>>>>
> >>>>>> As mentioned previously, I have problems with concurvity in my thesis
> >>>>>> research, and don't have access to a statistician who works with time
> >>>>>> series, GAMs or R. I'd be very grateful for any (partial) answer,
> >>>>>> however short. I'll gladly return the favour where I can! For really
> >>>>>> helpful input I'd be more than happy to offer co-authorship on
> >>>>>> publication. Deadlines are very close, and I'm heading towards having
> >>>>>> no results at all if I can't solve this concurvity issue :(
> >>>>>>
> >>>>>> I'm using GAMs to try to understand the relationship between deaths
> >>>>>> and heat-related variables (e.g. temperature and humidity), using
> >>>>>> daily time series over a 14-year period from a tropical, low-income
> >>>>>> country. My aim is to understand the relationship between these
> >>>>>> variables and deaths, rather than pure prediction performance.
> >>>>>>
> >>>>>> The GAMs include distributed lag models (set up as 7-column matrices,
> >>>>>> see code at bottom of post), since deaths may occur over several days
> >>>>>> following exposure.
> >>>>>>
> >>>>>> Simple GAMs with just time, lagged temperature and lagged
> >>>>>> precipitation (a potential confounder) show very high concurvity
> >>>>>> between lagged temperature and time, regardless of the many different
> >>>>>> ways I have tried to decompose time. The autocorrelation functions
> >>>>>> (ACF) however, shows values close to zero, only just about breaching
> >>>>>> the 'significance line' in a few instances. It does show patterning
> >>>>>> though, although the regularity is difficult to define.
> >>>>>>
> >>>>>> My questions are:
> >>>>>> 1) Should I be worried about the high concurvity, or can I ignore it
> >>>>>> given the mostly non-significant ACF? I've read dozens of
> >>>>>> heat-mortality modelling studies and none report on concurvity between
> >>>>>> weather variables and time (though one 2012 paper discussed
> >>>>>> autocorrelation).
> >>>>>>
> >>>>>> 2) If I cannot ignore it, what should I do to resolve it? Would
> >>>>>> including an autoregressive term be appropriate, and if so, where can
> >>>>>> I find a coded example of how to do this? I've also come across
> >>>>>> sequential regression][1]. Would this be more or less appropriate? If
> >>>>>> appropriate, a pointer to an example would be really appreciated!
> >>>>>>
> >>>>>> Some example GAMs are specified as follows:
> >>>>>> ```r
> >>>>>> conc38b <- gam(deaths~te(year, month, week, weekday,
> >>>>>> bs=c("cr","cc","cc","cc")) + heap +
> >>>>>>                          te(temp_max, lag, k=c(10, 3)) +
> >>>>>>                          te(precip_daily_total, lag, k=c(10, 3)),
> >>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE,
> >>>>>>                          knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>>>>> 52.5), weekday = c(0, 6.5)))
> >>>>>> ```
> >>>>>> Concurvity for the above model between (temp_max, lag) and (year,
> >>>>>> month, week, weekday) is 0.91:
> >>>>>>
> >>>>>> ```r
> >>>>>> $worst
> >>>>>>                                        para te(year,month,week,weekday)
> >>>>>> te(temp_max,lag) te(precip_daily_total,lag)
> >>>>>> para                        1.000000e+00                1.125625e-29
> >>>>>>         0.3150073                  0.6666348
> >>>>>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> >>>>>>         0.9060552                  0.6652313
> >>>>>> te(temp_max,lag)            3.152795e-01                8.998113e-01
> >>>>>>         1.0000000                  0.5781015
> >>>>>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> >>>>>>         0.5805159                  1.0000000
> >>>>>> ```
> >>>>>>
> >>>>>> Output from ```gam.check()```:
> >>>>>> ```r
> >>>>>> Method: REML   Optimizer: outer newton
> >>>>>> full convergence after 16 iterations.
> >>>>>> Gradient range [-0.01467332,0.003096643]
> >>>>>> (score 8915.994 & scale 1).
> >>>>>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> >>>>>> Model rank =  544 / 544
> >>>>>>
> >>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>>>>> indicate that k is too low, especially if edf is close to k'.
> >>>>>>
> >>>>>>                                      k'      edf k-index p-value
> >>>>>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> >>>>>> te(temp_max,lag)             29.0000   3.3681      NA      NA
> >>>>>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> >>>>>> ---
> >>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>>> ```
> >>>>>>
> >>>>>> Some output from ```summary(conc38b)```:
> >>>>>> ```r
> >>>>>> Approximate significance of smooth terms:
> >>>>>>                                      edf Ref.df  Chi.sq p-value
> >>>>>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> >>>>>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> >>>>>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> >>>>>> ---
> >>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>>>
> >>>>>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> >>>>>> -REML =   8916  Scale est. = 1         n = 5107
> >>>>>> ```
> >>>>>>
> >>>>>>
> >>>>>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> >>>>>> pattern). They show peaks at 5 and 15, and then there seems to be a
> >>>>>> recurring pattern at multiples of approx. 30 (suggesting month is not
> >>>>>> modelled adequately?). Not sure what would cause the spikes at 5 and
> >>>>>> 15. There is heaping of deaths on the 15th day of each month, to which
> >>>>>> deaths with unknown date were allocated. This heaping was modelled
> >>>>>> with categorical variable/ factor ```heap``` with 169 levels (0 for
> >>>>>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> >>>>>> heaping day over the 14-year period):
> >>>>>>
> >>>>>>      [2]: https://i.stack.imgur.com/FzKyM.png
> >>>>>>      [3]: https://i.stack.imgur.com/fE3aL.png
> >>>>>>
> >>>>>>
> >>>>>> I get an identical looking ACF when I decompose time into (year,
> >>>>>> month, monthday) as in model conc39 below, although concurvity between
> >>>>>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> >>>>>>
> >>>>>> ```r
> >>>>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >>>>>>                         te(temp_max, lag, k=c(10, 4)) +
> >>>>>>                         te(precip_daily_total, lag, k=c(10, 4)),
> >>>>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> >>>>>>                         knots = list(month = c(0.5, 12.5)))
> >>>>>> ```
> >>>>>> ```r
> >>>>>>
> >>>>>> Method: REML   Optimizer: outer newton
> >>>>>> full convergence after 14 iterations.
> >>>>>> Gradient range [-0.001578187,6.155096e-05]
> >>>>>> (score 8915.763 & scale 1).
> >>>>>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> >>>>>> Model rank =  323 / 323
> >>>>>>
> >>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>>>>> indicate that k is too low, especially if edf is close to k'.
> >>>>>>
> >>>>>>                                    k'     edf k-index p-value
> >>>>>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> >>>>>> te(temp_max,lag)           39.0000  4.0875      NA      NA
> >>>>>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> >>>>>> ---
> >>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>>> ```
> >>>>>> Some output from ```summary(conc39)```:
> >>>>>> ```r
> >>>>>> Approximate significance of smooth terms:
> >>>>>>                                    edf Ref.df  Chi.sq  p-value
> >>>>>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> >>>>>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> >>>>>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> >>>>>> ---
> >>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>>>
> >>>>>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> >>>>>> -REML =   8915  Scale est. = 1         n = 5107
> >>>>>> ```
> >>>>>>
> >>>>>>
> >>>>>> ```r
> >>>>>> $worst
> >>>>>>                                       para te(year,month,monthday)
> >>>>>> te(temp_max,lag) te(precip_daily_total,lag)
> >>>>>> para                       1.000000e+00            3.261007e-31
> >>>>>> 0.3313549                  0.6666532
> >>>>>> te(year,month,monthday)    3.060763e-31            1.000000e+00
> >>>>>> 0.8266086                  0.5670777
> >>>>>> te(temp_max,lag)           3.331014e-01            8.225942e-01
> >>>>>> 1.0000000                  0.5840875
> >>>>>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> >>>>>> 0.5939380                  1.0000000
> >>>>>> ```
> >>>>>>
> >>>>>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> >>>>>> various choices for k or as ```s(time)``` with various k does not
> >>>>>> reduce concurvity either.
> >>>>>>
> >>>>>>
> >>>>>> The default approach in time series studies of heat-mortality is to
> >>>>>> model time with fixed df, generally between 7-10 df per year of data.
> >>>>>> I am, however, apprehensive about this approach because a) mortality
> >>>>>> profiles vary with locality due to sociodemographic and environmental
> >>>>>> characteristics and b) the choice of df is based on higher income
> >>>>>> countries (where nearly all these studies have been done) with
> >>>>>> different mortality profiles and so may not be appropriate for
> >>>>>> tropical, low-income countries.
> >>>>>>
> >>>>>> Although the approach of fixing (high) df does remove more temporal
> >>>>>> patterns from the ACF (see model and output below), concurvity between
> >>>>>> time and lagged temperature has now risen to 0.99! Moreover,
> >>>>>> temperature (which has been a consistent, highly significant predictor
> >>>>>> in every model of the tens (hundreds?) I have run, has now turned
> >>>>>> non-significant. I am guessing this is because time is now a very
> >>>>>> wiggly function that not only models/ removes seasonal variation, but
> >>>>>> also some of the day-to-day variation that is needed for the
> >>>>>> temperature smooth  :
> >>>>>>
> >>>>>> ```r
> >>>>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >>>>>>                          te(temp_max, lag, k=c(10,3)) +
> >>>>>>                          te(precip_daily_total, lag, k=c(10,3)),
> >>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE)
> >>>>>> ```
> >>>>>> Output from ```gam.check(conc20a, rep = 1000)```:
> >>>>>>
> >>>>>> ```r
> >>>>>> Method: REML   Optimizer: outer newton
> >>>>>> full convergence after 9 iterations.
> >>>>>> Gradient range [-0.0008983099,9.546022e-05]
> >>>>>> (score 8750.13 & scale 1).
> >>>>>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> >>>>>> Model rank =  336 / 336
> >>>>>>
> >>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>>>>> indicate that k is too low, especially if edf is close to k'.
> >>>>>>
> >>>>>>                                     k'      edf k-index p-value
> >>>>>> s(time)                    111.0000 111.0000    0.98    0.56
> >>>>>> te(temp_max,lag)            29.0000   0.6548      NA      NA
> >>>>>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> >>>>>> ```
> >>>>>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
> >>>>>>
> >>>>>> ```r
> >>>>>>                                       para      s(time) te(temp_max,lag)
> >>>>>> te(precip_daily_total,lag)
> >>>>>> para                       1.000000e+00 2.462064e-19        0.3165236
> >>>>>>                    0.6666348
> >>>>>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
> >>>>>>                    0.6879284
> >>>>>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> >>>>>>                    0.5788711
> >>>>>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> >>>>>>                    1.0000000
> >>>>>>
> >>>>>> ```
> >>>>>>
> >>>>>> Some output from ```summary(conc20a)```:
> >>>>>> ```r
> >>>>>> Approximate significance of smooth terms:
> >>>>>>                                     edf Ref.df  Chi.sq p-value
> >>>>>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> >>>>>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> >>>>>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> >>>>>> ---
> >>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>>>
> >>>>>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> >>>>>> -REML = 8750.1  Scale est. = 1         n = 5107
> >>>>>> ```
> >>>>>>
> >>>>>> ACF functions:
> >>>>>>
> >>>>>> [4]: https://i.stack.imgur.com/7nbXS.png
> >>>>>> [5]: https://i.stack.imgur.com/pNnZU.png
> >>>>>>
> >>>>>> Data can be found on my [GitHub][6] site in the file
> >>>>>> [data_cross_validated_post2.rds][7]. A csv version is also available.
> >>>>>> This is my code:
> >>>>>>
> >>>>>> ```r
> >>>>>> library(readr)
> >>>>>> library(mgcv)
> >>>>>>
> >>>>>> df <- read_rds("data_crossvalidated_post2.rds")
> >>>>>>
> >>>>>> # Create matrices for lagged weather variables (6 day lags) based on
> >>>>>> example by Simon Wood
> >>>>>> # in his 2017 book ("Generalized additive models: an introduction with
> >>>>>> R", p. 349) and
> >>>>>> # gamair package documentation
> >>>>>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> >>>>>>
> >>>>>> lagard <- function(x,n.lag=7) {
> >>>>>> n <- length(x); X <- matrix(NA,n,n.lag)
> >>>>>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> >>>>>> X
> >>>>>> }
> >>>>>>
> >>>>>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> >>>>>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> >>>>>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> >>>>>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> >>>>>> = df$precip_hourly_dailysum)
> >>>>>> dat$temp_max <- lagard(df$temp_max)
> >>>>>> dat$temp_min <- lagard(df$temp_min)
> >>>>>> dat$temp_mean <- lagard(df$temp_mean)
> >>>>>> dat$wbgt_max <- lagard(df$wbgt_max)
> >>>>>> dat$wbgt_mean <- lagard(df$wbgt_mean)
> >>>>>> dat$wbgt_min <- lagard(df$wbgt_min)
> >>>>>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> >>>>>> dat$sh_mean <- lagard(df$sh_mean)
> >>>>>> dat$solar_mean <- lagard(df$solar_mean)
> >>>>>> dat$wind2m_mean <- lagard(df$wind2m_mean)
> >>>>>> dat$sh_max <- lagard(df$sh_max)
> >>>>>> dat$solar_max <- lagard(df$solar_max)
> >>>>>> dat$wind2m_max <- lagard(df$wind2m_max)
> >>>>>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> >>>>>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> >>>>>> dat$precip_hourly <- lagard(df$precip_hourly)
> >>>>>> dat$precip_daily_total <- lagard( df$precip_daily_total)
> >>>>>> dat$temp <- lagard(df$temp)
> >>>>>> dat$sh <- lagard(df$sh)
> >>>>>> dat$rh <- lagard(df$rh)
> >>>>>> dat$solar <- lagard(df$solar)
> >>>>>> dat$wind2m <- lagard(df$wind2m)
> >>>>>>
> >>>>>>
> >>>>>> conc38b <- gam(deaths~te(year, month, week, weekday,
> >>>>>> bs=c("cr","cc","cc","cc")) + heap +
> >>>>>>                          te(temp_max, lag, k=c(10, 3)) +
> >>>>>>                          te(precip_daily_total, lag, k=c(10, 3)),
> >>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE,
> >>>>>>                          knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>>>>> 52.5), weekday = c(0, 6.5)))
> >>>>>>
> >>>>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >>>>>>                         te(temp_max, lag, k=c(10, 4)) +
> >>>>>>                         te(precip_daily_total, lag, k=c(10, 4)),
> >>>>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> >>>>>>                         knots = list(month = c(0.5, 12.5)))
> >>>>>>
> >>>>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >>>>>>                          te(temp_max, lag, k=c(10,3)) +
> >>>>>>                          te(precip_daily_total, lag, k=c(10,3)),
> >>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE)
> >>>>>>
> >>>>>> ```
> >>>>>> Thank you if you've read this far!! :-))
> >>>>>>
> >>>>>>      [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> >>>>>>      [2]: https://i.stack.imgur.com/FzKyM.png
> >>>>>>      [3]: https://i.stack.imgur.com/fE3aL.png
> >>>>>>      [4]: https://i.stack.imgur.com/7nbXS.png
> >>>>>>      [5]: https://i.stack.imgur.com/pNnZU.png
> >>>>>>      [6]: https://github.com/JadeShodan/heat-mortality
> >>>>>>      [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>
> >>>>> --
> >>>>> Simon Wood, School of Mathematics, University of Edinburgh,
> >>>>> https://www.maths.ed.ac.uk/~swood34/
> >>>>>
> >>>>
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>
> >>> --
> >>> Michael
> >>> http://www.dewey.myzen.co.uk/home.html
> >
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html


From j@de@shod@@ m@iii@g oii googiem@ii@com  Fri Jun 10 21:36:18 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Fri, 10 Jun 2022 20:36:18 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <80abf9e4-d864-43d3-e8ef-488239d22931@bath.edu>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-g=M41cxom6DVdTunFjgZ6VfKobqW__Mq351pDpY22PQ@mail.gmail.com>
 <f371005e-6f57-2007-f859-d66595a19c0f@bath.edu>
 <CANg3_k_qMX24_a-Hppk6knrH-nt1QMLj2FFSJbkMJc0TBxqzQA@mail.gmail.com>
 <80abf9e4-d864-43d3-e8ef-488239d22931@bath.edu>
Message-ID: <CANg3_k_OZqM1H2iHn6exCpkcoG95scfQiejTPZwQjpqfJHQYAg@mail.gmail.com>

Thank you Simon!!

As you may have seen in the reply to Michael, I'm going to run with
interpolation of deaths on the 15th of each month, and then run bam()
with AR1 term.

Thanks again!

On Fri, 10 Jun 2022 at 19:50, Simon Wood <simon.wood at bath.edu> wrote:
>
> Apologies, I meant...
>
> aheap <- heap!="0"
> aheap + s(heap,bs="re",by=aheap)
>
> - fixed effect for heap day or not, and then an extra random effect if it is a heap day
>
> ... bit difficult to give a generic answer to the usefulness question -
> I guess you probably have to decide on that yourself.
>
> Simon
>
> On 09/06/2022 16:57, jade.shodan at googlemail.com wrote:
> > Hi Simon,
> >
> > (Sorry, replies and answers are out of sync due to my problems posting
> > to the list/ messages being held for moderation)
> >
> >> Did it actually fail, or simply generate warnings?
> > The model was computed (you're right, not a model failure), but
> > resulted in warnings (as per previous post) which, frankly, I didn't
> > understand.
> >
> >> if I understand your model right there is one free parameter for each observation falling on the 15th
> > That's right, one observation on each 15th day of the month.
> >
> > Thank you for the suggestion about the random effects! I had been
> > wondering about how I could model this heaping with a smooth!
> >
> > Quick question:
> >
> > You proposed:
> >
> > aheap <- heap!="0"
> > heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap
> > day + a r.e. for each heap day - no effect on non-heap days
> >
> > Is there a typo in the code above? I don't see the newly created
> > variable aheap in the model?
> > Should it read "by = aheap" as follows:
> >
> > heap + s(heap,bs="re",by=aheap)   ?
> >
> > So a full model might then look like the one below?
> >
> >   bam0 <- bam(deaths~te(year, month, week, weekday,
> > bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> > s(heap,bs="re",by=aheap)
> >                                       te(temp_max, lag, k=c(8, 3)) +
> >                                       te(precip_daily_total, lag, k=c(8, 3)),
> >                                       data = dat, family = nb, method =
> > 'fREML', select = TRUE, discrete = TRUE,
> >                                     knots = list(month = c(0.5, 12.5),
> > week = c(0.5, 52.5), weekday = c(0, 6.5)))
> >
> > One, hopefully final (!) question:
> >
> > Is it actually useful at all to keep these observations on the 15th
> > day of each month (which are huge errors), or am I better off removing
> > them from the data set (or replacing them with e.g. median values)?
> > For temperature-mortality modelling it is the day-to-day variation in
> > deaths and temperature that is of interest. So is modelling heaping
> > actually useful at all, given that this variable changes on a monthly
> > basis? (I think you are alluding to this, but I just want to make
> > sure).
> >
> > If I take them out altogether, would I be best off removing all data
> > for these dates, so that the time series jumps from day 14 to day 16?
> > Or would this create problems with e.g. the distributed lag model?
> >
> > Sorry for all these questions! Have been struggling with this for
> > months (posted on Cross Validated about the heaping issue too), and
> > feel I am finally getting somewhere with your help!
> >
> > Jade
> >
> > On Thu, 9 Jun 2022 at 15:24, Simon Wood <simon.wood at bath.edu> wrote:
> >>
> >> On 09/06/2022 12:30, jade.shodan at googlemail.com wrote:
> >>> Hi Simon,
> >>>
> >>> Thanks so much for this!! (Apologies if this is a double posting. I
> >>> seem to have a problem getting messages through to the list).
> >>>
> >>> I have two follow up questions, if you don't mind.
> >>>
> >>> 1. Does including an autoregressive term not adjust away part of the
> >>> effect of the response in a distributed lag model (where the outcome
> >>> accumulates over time)?
> >> - the hope is that it approximately deals with short timescale stuff
> >> without interfering with the longer timescales to the same extent as a
> >> high rank smooth would.
> >>
> >>> 2. I've tried to fit a model using bam (just a first attempt without
> >>> AR term), but including the factor variable heap creates errors:
> >>>
> >>> bam0 <- bam(deaths~te(year, month, week, weekday,
> >>> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> >>>                                        te(temp_max, lag, k=c(8, 3)) +
> >>>                                        te(precip_daily_total, lag, k=c(8, 3)),
> >>>                                       data = dat, family = nb, method =
> >>> 'fREML', select = TRUE, discrete = TRUE,
> >>>                                       knots = list(month = c(0.5, 12.5),
> >>> week = c(0.5, 52.5), weekday = c(0, 6.5)))
> >>>
> >>> This model results in errors:
> >>>
> >>> Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
> >>>     step failure in theta estimation
> >>> Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> >>> object$prior.weights)) :
> >>>     NaNs produced
> >>>
> >> Did it actually fail, or simply generate warnings?
> >>
> >> 'bam' handles factors, but if I understand your model right there is one
> >> free parameter for each observation falling on the 15th, so that you
> >> will fit data for those days exactly (and might just as well have
> >> dropped them, for all the information they contribute to the rest of the
> >> model). If you want a structure like this, I'd be inclined to make the
> >> heap variable random, something like...
> >>
> >> aheap <- heap!="0"
> >>
> >> heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap day
> >> + a r.e. for each heap day - no effect on non-heap days
> >>
> >> best,
> >>
> >> Simon
> >>
> >>> Including heap as as.numeric(heap) runs the model without error
> >>> messages or warnings, but model diagnostics look terrible, and it also
> >>> doesn't make sense (to me) to make heap a numeric. The factor variable
> >>> heap (with 169 levels) codes the fact that all deaths for which no
> >>> date was known, were registered on the 15th day of each month. I've
> >>> coded all non-heaping days as 0. All heaping days were coded as a
> >>> value between 1-168. The time series spans 14 years, so a heaping day
> >>> in each month results in 14*12 levels = 168, plus one level for
> >>> non-heaping days.
> >>>
> >>> So my second question is: Does bam allow factor variables? And if not,
> >>> how should I model this heaping on the 15th day of the month instead?
> >>>
> >>> With thanks,
> >>>
> >>> Jade
> >>>
> >>> On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
> >>>> I would not worry too much about high concurvity between variables like
> >>>> temperature and time. This just reflects the fact that temperature has a
> >>>> strong temporal pattern.
> >>>>
> >>>> I would also not be too worried about the low p-values on the k check.
> >>>> The check only looks for pattern in the residuals when they are ordered
> >>>> with respect to the variables of a smooth. When you have time series
> >>>> data and some smooths involve time then it's hard not to pick up some
> >>>> degree of residual auto-correlation, which you often would not want to
> >>>> model with a higher rank smoother.
> >>>>
> >>>> The NAs  for the distributed lag terms just reflect the fact that there
> >>>> is no obvious way to order the residuals w.r.t. the covariates for such
> >>>> terms, so the simple check for residual pattern is not really possible.
> >>>>
> >>>> One simple approach is to fit using bam(...,discrete=TRUE) which will
> >>>> let you specify an AR1 parameter to mop up some of the residual
> >>>> auto-correlation without resorting to a high rank smooth that then does
> >>>> all the work of the covariates as well. The AR1 parameter can be set by
> >>>> looking at the ACF of the residuals of the model without this. You need
> >>>> to look at the ACF of suitably standardized residuals to check how well
> >>>> this has worked.
> >>>>
> >>>> best,
> >>>>
> >>>> Simon
> >>>>
> >>>> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> >>>>> Hello everyone,
> >>>>>
> >>>>> A few days ago I asked a question about concurvity in a GAM (the
> >>>>> anologue of collinearity in a GLM) implemented in mgcv. I think my
> >>>>> question was a bit unfocussed, so I am retrying again, but with
> >>>>> additional information included about the autocorrelation function. I
> >>>>> have also posted about this on Cross Validated. Given all the model
> >>>>> output, it might make for easier
> >>>>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> >>>>>
> >>>>> As mentioned previously, I have problems with concurvity in my thesis
> >>>>> research, and don't have access to a statistician who works with time
> >>>>> series, GAMs or R. I'd be very grateful for any (partial) answer,
> >>>>> however short. I'll gladly return the favour where I can! For really
> >>>>> helpful input I'd be more than happy to offer co-authorship on
> >>>>> publication. Deadlines are very close, and I'm heading towards having
> >>>>> no results at all if I can't solve this concurvity issue :(
> >>>>>
> >>>>> I'm using GAMs to try to understand the relationship between deaths
> >>>>> and heat-related variables (e.g. temperature and humidity), using
> >>>>> daily time series over a 14-year period from a tropical, low-income
> >>>>> country. My aim is to understand the relationship between these
> >>>>> variables and deaths, rather than pure prediction performance.
> >>>>>
> >>>>> The GAMs include distributed lag models (set up as 7-column matrices,
> >>>>> see code at bottom of post), since deaths may occur over several days
> >>>>> following exposure.
> >>>>>
> >>>>> Simple GAMs with just time, lagged temperature and lagged
> >>>>> precipitation (a potential confounder) show very high concurvity
> >>>>> between lagged temperature and time, regardless of the many different
> >>>>> ways I have tried to decompose time. The autocorrelation functions
> >>>>> (ACF) however, shows values close to zero, only just about breaching
> >>>>> the 'significance line' in a few instances. It does show patterning
> >>>>> though, although the regularity is difficult to define.
> >>>>>
> >>>>> My questions are:
> >>>>> 1) Should I be worried about the high concurvity, or can I ignore it
> >>>>> given the mostly non-significant ACF? I've read dozens of
> >>>>> heat-mortality modelling studies and none report on concurvity between
> >>>>> weather variables and time (though one 2012 paper discussed
> >>>>> autocorrelation).
> >>>>>
> >>>>> 2) If I cannot ignore it, what should I do to resolve it? Would
> >>>>> including an autoregressive term be appropriate, and if so, where can
> >>>>> I find a coded example of how to do this? I've also come across
> >>>>> sequential regression][1]. Would this be more or less appropriate? If
> >>>>> appropriate, a pointer to an example would be really appreciated!
> >>>>>
> >>>>> Some example GAMs are specified as follows:
> >>>>> ```r
> >>>>> conc38b <- gam(deaths~te(year, month, week, weekday,
> >>>>> bs=c("cr","cc","cc","cc")) + heap +
> >>>>>                          te(temp_max, lag, k=c(10, 3)) +
> >>>>>                          te(precip_daily_total, lag, k=c(10, 3)),
> >>>>>                          data = dat, family = nb, method = 'REML', select = TRUE,
> >>>>>                          knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>>>> 52.5), weekday = c(0, 6.5)))
> >>>>> ```
> >>>>> Concurvity for the above model between (temp_max, lag) and (year,
> >>>>> month, week, weekday) is 0.91:
> >>>>>
> >>>>> ```r
> >>>>> $worst
> >>>>>                                        para te(year,month,week,weekday)
> >>>>> te(temp_max,lag) te(precip_daily_total,lag)
> >>>>> para                        1.000000e+00                1.125625e-29
> >>>>>         0.3150073                  0.6666348
> >>>>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> >>>>>         0.9060552                  0.6652313
> >>>>> te(temp_max,lag)            3.152795e-01                8.998113e-01
> >>>>>         1.0000000                  0.5781015
> >>>>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> >>>>>         0.5805159                  1.0000000
> >>>>> ```
> >>>>>
> >>>>> Output from ```gam.check()```:
> >>>>> ```r
> >>>>> Method: REML   Optimizer: outer newton
> >>>>> full convergence after 16 iterations.
> >>>>> Gradient range [-0.01467332,0.003096643]
> >>>>> (score 8915.994 & scale 1).
> >>>>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> >>>>> Model rank =  544 / 544
> >>>>>
> >>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>>>> indicate that k is too low, especially if edf is close to k'.
> >>>>>
> >>>>>                                      k'      edf k-index p-value
> >>>>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> >>>>> te(temp_max,lag)             29.0000   3.3681      NA      NA
> >>>>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> >>>>> ---
> >>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>> ```
> >>>>>
> >>>>> Some output from ```summary(conc38b)```:
> >>>>> ```r
> >>>>> Approximate significance of smooth terms:
> >>>>>                                      edf Ref.df  Chi.sq p-value
> >>>>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> >>>>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> >>>>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> >>>>> ---
> >>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>>
> >>>>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> >>>>> -REML =   8916  Scale est. = 1         n = 5107
> >>>>> ```
> >>>>>
> >>>>>
> >>>>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> >>>>> pattern). They show peaks at 5 and 15, and then there seems to be a
> >>>>> recurring pattern at multiples of approx. 30 (suggesting month is not
> >>>>> modelled adequately?). Not sure what would cause the spikes at 5 and
> >>>>> 15. There is heaping of deaths on the 15th day of each month, to which
> >>>>> deaths with unknown date were allocated. This heaping was modelled
> >>>>> with categorical variable/ factor ```heap``` with 169 levels (0 for
> >>>>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> >>>>> heaping day over the 14-year period):
> >>>>>
> >>>>>      [2]: https://i.stack.imgur.com/FzKyM.png
> >>>>>      [3]: https://i.stack.imgur.com/fE3aL.png
> >>>>>
> >>>>>
> >>>>> I get an identical looking ACF when I decompose time into (year,
> >>>>> month, monthday) as in model conc39 below, although concurvity between
> >>>>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> >>>>>
> >>>>> ```r
> >>>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >>>>>                         te(temp_max, lag, k=c(10, 4)) +
> >>>>>                         te(precip_daily_total, lag, k=c(10, 4)),
> >>>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> >>>>>                         knots = list(month = c(0.5, 12.5)))
> >>>>> ```
> >>>>> ```r
> >>>>>
> >>>>> Method: REML   Optimizer: outer newton
> >>>>> full convergence after 14 iterations.
> >>>>> Gradient range [-0.001578187,6.155096e-05]
> >>>>> (score 8915.763 & scale 1).
> >>>>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> >>>>> Model rank =  323 / 323
> >>>>>
> >>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>>>> indicate that k is too low, especially if edf is close to k'.
> >>>>>
> >>>>>                                    k'     edf k-index p-value
> >>>>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> >>>>> te(temp_max,lag)           39.0000  4.0875      NA      NA
> >>>>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> >>>>> ---
> >>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>> ```
> >>>>> Some output from ```summary(conc39)```:
> >>>>> ```r
> >>>>> Approximate significance of smooth terms:
> >>>>>                                    edf Ref.df  Chi.sq  p-value
> >>>>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> >>>>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> >>>>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> >>>>> ---
> >>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>>
> >>>>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> >>>>> -REML =   8915  Scale est. = 1         n = 5107
> >>>>> ```
> >>>>>
> >>>>>
> >>>>> ```r
> >>>>> $worst
> >>>>>                                       para te(year,month,monthday)
> >>>>> te(temp_max,lag) te(precip_daily_total,lag)
> >>>>> para                       1.000000e+00            3.261007e-31
> >>>>> 0.3313549                  0.6666532
> >>>>> te(year,month,monthday)    3.060763e-31            1.000000e+00
> >>>>> 0.8266086                  0.5670777
> >>>>> te(temp_max,lag)           3.331014e-01            8.225942e-01
> >>>>> 1.0000000                  0.5840875
> >>>>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> >>>>> 0.5939380                  1.0000000
> >>>>> ```
> >>>>>
> >>>>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> >>>>> various choices for k or as ```s(time)``` with various k does not
> >>>>> reduce concurvity either.
> >>>>>
> >>>>>
> >>>>> The default approach in time series studies of heat-mortality is to
> >>>>> model time with fixed df, generally between 7-10 df per year of data.
> >>>>> I am, however, apprehensive about this approach because a) mortality
> >>>>> profiles vary with locality due to sociodemographic and environmental
> >>>>> characteristics and b) the choice of df is based on higher income
> >>>>> countries (where nearly all these studies have been done) with
> >>>>> different mortality profiles and so may not be appropriate for
> >>>>> tropical, low-income countries.
> >>>>>
> >>>>> Although the approach of fixing (high) df does remove more temporal
> >>>>> patterns from the ACF (see model and output below), concurvity between
> >>>>> time and lagged temperature has now risen to 0.99! Moreover,
> >>>>> temperature (which has been a consistent, highly significant predictor
> >>>>> in every model of the tens (hundreds?) I have run, has now turned
> >>>>> non-significant. I am guessing this is because time is now a very
> >>>>> wiggly function that not only models/ removes seasonal variation, but
> >>>>> also some of the day-to-day variation that is needed for the
> >>>>> temperature smooth  :
> >>>>>
> >>>>> ```r
> >>>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >>>>>                          te(temp_max, lag, k=c(10,3)) +
> >>>>>                          te(precip_daily_total, lag, k=c(10,3)),
> >>>>>                          data = dat, family = nb, method = 'REML', select = TRUE)
> >>>>> ```
> >>>>> Output from ```gam.check(conc20a, rep = 1000)```:
> >>>>>
> >>>>> ```r
> >>>>> Method: REML   Optimizer: outer newton
> >>>>> full convergence after 9 iterations.
> >>>>> Gradient range [-0.0008983099,9.546022e-05]
> >>>>> (score 8750.13 & scale 1).
> >>>>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> >>>>> Model rank =  336 / 336
> >>>>>
> >>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>>>> indicate that k is too low, especially if edf is close to k'.
> >>>>>
> >>>>>                                     k'      edf k-index p-value
> >>>>> s(time)                    111.0000 111.0000    0.98    0.56
> >>>>> te(temp_max,lag)            29.0000   0.6548      NA      NA
> >>>>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> >>>>> ```
> >>>>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
> >>>>>
> >>>>> ```r
> >>>>>                                       para      s(time) te(temp_max,lag)
> >>>>> te(precip_daily_total,lag)
> >>>>> para                       1.000000e+00 2.462064e-19        0.3165236
> >>>>>                    0.6666348
> >>>>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
> >>>>>                    0.6879284
> >>>>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> >>>>>                    0.5788711
> >>>>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> >>>>>                    1.0000000
> >>>>>
> >>>>> ```
> >>>>>
> >>>>> Some output from ```summary(conc20a)```:
> >>>>> ```r
> >>>>> Approximate significance of smooth terms:
> >>>>>                                     edf Ref.df  Chi.sq p-value
> >>>>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> >>>>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> >>>>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> >>>>> ---
> >>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>>
> >>>>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> >>>>> -REML = 8750.1  Scale est. = 1         n = 5107
> >>>>> ```
> >>>>>
> >>>>> ACF functions:
> >>>>>
> >>>>> [4]: https://i.stack.imgur.com/7nbXS.png
> >>>>> [5]: https://i.stack.imgur.com/pNnZU.png
> >>>>>
> >>>>> Data can be found on my [GitHub][6] site in the file
> >>>>> [data_cross_validated_post2.rds][7]. A csv version is also available.
> >>>>> This is my code:
> >>>>>
> >>>>> ```r
> >>>>> library(readr)
> >>>>> library(mgcv)
> >>>>>
> >>>>> df <- read_rds("data_crossvalidated_post2.rds")
> >>>>>
> >>>>> # Create matrices for lagged weather variables (6 day lags) based on
> >>>>> example by Simon Wood
> >>>>> # in his 2017 book ("Generalized additive models: an introduction with
> >>>>> R", p. 349) and
> >>>>> # gamair package documentation
> >>>>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> >>>>>
> >>>>> lagard <- function(x,n.lag=7) {
> >>>>> n <- length(x); X <- matrix(NA,n,n.lag)
> >>>>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> >>>>> X
> >>>>> }
> >>>>>
> >>>>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> >>>>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> >>>>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> >>>>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> >>>>> = df$precip_hourly_dailysum)
> >>>>> dat$temp_max <- lagard(df$temp_max)
> >>>>> dat$temp_min <- lagard(df$temp_min)
> >>>>> dat$temp_mean <- lagard(df$temp_mean)
> >>>>> dat$wbgt_max <- lagard(df$wbgt_max)
> >>>>> dat$wbgt_mean <- lagard(df$wbgt_mean)
> >>>>> dat$wbgt_min <- lagard(df$wbgt_min)
> >>>>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> >>>>> dat$sh_mean <- lagard(df$sh_mean)
> >>>>> dat$solar_mean <- lagard(df$solar_mean)
> >>>>> dat$wind2m_mean <- lagard(df$wind2m_mean)
> >>>>> dat$sh_max <- lagard(df$sh_max)
> >>>>> dat$solar_max <- lagard(df$solar_max)
> >>>>> dat$wind2m_max <- lagard(df$wind2m_max)
> >>>>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> >>>>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> >>>>> dat$precip_hourly <- lagard(df$precip_hourly)
> >>>>> dat$precip_daily_total <- lagard( df$precip_daily_total)
> >>>>> dat$temp <- lagard(df$temp)
> >>>>> dat$sh <- lagard(df$sh)
> >>>>> dat$rh <- lagard(df$rh)
> >>>>> dat$solar <- lagard(df$solar)
> >>>>> dat$wind2m <- lagard(df$wind2m)
> >>>>>
> >>>>>
> >>>>> conc38b <- gam(deaths~te(year, month, week, weekday,
> >>>>> bs=c("cr","cc","cc","cc")) + heap +
> >>>>>                          te(temp_max, lag, k=c(10, 3)) +
> >>>>>                          te(precip_daily_total, lag, k=c(10, 3)),
> >>>>>                          data = dat, family = nb, method = 'REML', select = TRUE,
> >>>>>                          knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>>>> 52.5), weekday = c(0, 6.5)))
> >>>>>
> >>>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >>>>>                         te(temp_max, lag, k=c(10, 4)) +
> >>>>>                         te(precip_daily_total, lag, k=c(10, 4)),
> >>>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> >>>>>                         knots = list(month = c(0.5, 12.5)))
> >>>>>
> >>>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >>>>>                          te(temp_max, lag, k=c(10,3)) +
> >>>>>                          te(precip_daily_total, lag, k=c(10,3)),
> >>>>>                          data = dat, family = nb, method = 'REML', select = TRUE)
> >>>>>
> >>>>> ```
> >>>>> Thank you if you've read this far!! :-))
> >>>>>
> >>>>>      [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> >>>>>      [2]: https://i.stack.imgur.com/FzKyM.png
> >>>>>      [3]: https://i.stack.imgur.com/fE3aL.png
> >>>>>      [4]: https://i.stack.imgur.com/7nbXS.png
> >>>>>      [5]: https://i.stack.imgur.com/pNnZU.png
> >>>>>      [6]: https://github.com/JadeShodan/heat-mortality
> >>>>>      [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>> --
> >>>> Simon Wood, School of Mathematics, University of Edinburgh,
> >>>> https://www.maths.ed.ac.uk/~swood34/
> >>>>
> >> --
> >> Simon Wood, School of Mathematics, University of Edinburgh,
> >> https://www.maths.ed.ac.uk/~swood34/
> >>
> --
> Simon Wood, School of Mathematics, University of Edinburgh,
> https://www.maths.ed.ac.uk/~swood34/
>


From b@m|th030465 @end|ng |rom gm@||@com  Fri Jun 10 20:20:42 2022
From: b@m|th030465 @end|ng |rom gm@||@com (Brian Smith)
Date: Fri, 10 Jun 2022 14:20:42 -0400
Subject: [R] igraph: layout help
Message-ID: <CAEQKoCEsjz565HPLaPG6CS17B-TwxaiX9mFP5wTP5XnHb0QxFw@mail.gmail.com>

Hi,

I was trying to make a network plot of this data:

============
library(igraph)
library(network)

df1 <- data.frame(from="A",to=c("B","C","D","E","F","G"),value=1)
df2 <- data.frame(from="K",to=c("L","M","N"),value=1)
df3 <- data.frame(from="A",to="K",value=3)
my.df <- rbind(df1,df2,df3)

my.graph <- graph_from_data_frame(my.df,directed = F)
plot(my.graph)
============

What I wanted was for nodes A to G to be very close together (touching each
other). Similarly, nodes K to N should be very close together. The
connecting edge (A to K) between these sets of points/vertices should be
the only edge visible. How should I go about doing this?

Also, how can I change the parameters (node label, node color, etc.) of the
graph?

Any help or link to documentation would be helpful. Or would

thanks!

	[[alternative HTML version deleted]]


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Sat Jun 11 12:45:24 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Sat, 11 Jun 2022 15:45:24 +0500
Subject: [R] A humble request regarding QAMG method in R
In-Reply-To: <CAMfKi3Kc9Effi+SCwGJ-vU1QnLqkVYLkD1HXHNVYNO4YKdBq6g@mail.gmail.com>
References: <CAMfKi3KtMG2cE6dNc6xYn--34XVNS-9tR7wr_h3KCuCeoP8VmA@mail.gmail.com>
 <F8FF0C1D-4ACF-4843-AB19-68D58103CEE5@comcast.net>
 <CAMfKi3Kc9Effi+SCwGJ-vU1QnLqkVYLkD1HXHNVYNO4YKdBq6g@mail.gmail.com>
Message-ID: <CAMfKi3JHi-1bczRhrHqN+T9-o-fPYFFJ4YTnzJbnsyaH4S=Nqg@mail.gmail.com>

Hi, Dear Respected Professors! I hope that you are doing well. Now all
files are in .txt files. Now kindly help me with the following:

I have the R-codes for the "Quantile Augmented Mean Group" method. The
relevant codes and data are attached herewith.
Note: The link to the reference paper is:
https://www.econ.cam.ac.uk/people-files/emeritus/mhp1/fp20/qmg40-rev21.pdf
My Issue:
When I run the given codes to estimate the results for my own data. I have
to face several errors. I humbly request the experts to help me to estimate
the results for my data.
Thank you so much for your precious time.

Regards

Muhammad Zubair Chishti
Ph.D. Student
School of Business,
Zhengzhou University, Henan, China.
My Google scholar link:
https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
My ResearchGate link: https://www.researchgate.net/profile/Muhammad-Chishti




On Sat, Jun 11, 2022 at 9:28 AM Muhammad Zubair Chishti <
mzchishti at eco.qau.edu.pk> wrote:

> Hi, Dear Respected Professors! I hope that you are doing well. Now all
> files are in .txt files. Now kindly help me with the following:
>
> I have the R-codes for the "Quantile Augmented Mean Group" method. The
> relevant codes and papers are attached herewith.
> My Issue:
> When I run the given codes to estimate the results for my own data. I have
> to face several errors. I humbly request the experts to help me to estimate
> the results for my data.
> Thank you so much for your precious time.
>
> Regards
>
> Muhammad Zubair Chishti
> Ph.D. Student
> School of Business,
> Zhengzhou University, Henan, China.
> My Google scholar link:
> https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
> My ResearchGate link:
> https://www.researchgate.net/profile/Muhammad-Chishti
>
>
> On Sat, Jun 11, 2022 at 7:06 AM David Winsemius <dwinsemius at comcast.net>
> wrote:
>
>> Only the pdf attachment came through. If you want your mail-client to
>> label your code or data attachments they need to have a .txt file extension
>> or type.
>>
>> --
>> David
>>
>> > On Jun 7, 2022, at 9:44 PM, Muhammad Zubair Chishti <
>> mzchishti at eco.qau.edu.pk> wrote:
>> >
>> > Hi, Dear Respected Professors! I hope that you are doing well.
>> > Kindly help me with the following:
>> > I have the R-codes for the "Quantile Augmented Mean Group" method. The
>> > relevant codes and papers are attached herewith.
>> > My Issue:
>> > When I run the given codes to estimate the results for my own data. I
>> have
>> > to face several errors. I humbly request the experts to help me to
>> estimate
>> > the results for my data.
>> > Thank you so much for your precious time.
>> >
>> > Regards
>> >
>> > Muhammad Zubair Chishti
>> > Ph.D. Student
>> > School of Business,
>> > Zhengzhou University, Henan, China.
>> > My Google scholar link:
>> > https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
>> > My ResearchGate link:
>> https://www.researchgate.net/profile/Muhammad-Chishti
>> > <Journal of Applied Econometrics Volume issue 2020 [doi
>> 10.1002_jae.2753] Harding, Matthew; Lamarche, Carlos; Pesaran, M. Hashem --
>> Common Correlated Effects Estimation of Heterogeneous Dynamic
>> Panel_2.pdf>______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>>

From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Sat Jun 11 12:54:08 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Sat, 11 Jun 2022 15:54:08 +0500
Subject: [R] A humble request regarding QAMG method in R
Message-ID: <CAMfKi3Lqzk36et=32fKK36dfjZAC_y5oyTZqBKbUgxXnTuZDQA@mail.gmail.com>

Hi, Dear Respected Professors! I hope that you are doing well. Now all
files are in .txt files. Now kindly help me with the following:

I have the R-codes for the "Quantile Augmented Mean Group" method. The
relevant codes and data are attached herewith.
Note: The link to the reference paper is:
https://www.econ.cam.ac.uk/people-files/emeritus/mhp1/fp20/qmg40-rev21.pdf
My Issue:
When I run the given codes to estimate the results for my own data. I have
to face several errors. I humbly request the experts to help me to estimate
the results for my data.
Thank you so much for your precious time.

Regards

Muhammad Zubair Chishti
Ph.D. Student
School of Business,
Zhengzhou University, Henan, China.
My Google scholar link:
https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
My ResearchGate link: https://www.researchgate.net/profile/Muhammad-Chishti

From ||@t@ @end|ng |rom dewey@myzen@co@uk  Sat Jun 11 14:36:15 2022
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Sat, 11 Jun 2022 13:36:15 +0100
Subject: [R] A humble request regarding QAMG method in R
In-Reply-To: <CAMfKi3Lqzk36et=32fKK36dfjZAC_y5oyTZqBKbUgxXnTuZDQA@mail.gmail.com>
References: <CAMfKi3Lqzk36et=32fKK36dfjZAC_y5oyTZqBKbUgxXnTuZDQA@mail.gmail.com>
Message-ID: <7cd79bc1-a77e-c869-3c60-9e4429489fec@dewey.myzen.co.uk>

Did you forget to attach them? At any rate they did not arrive.

Michael

On 11/06/2022 11:54, Muhammad Zubair Chishti wrote:
> Hi, Dear Respected Professors! I hope that you are doing well. Now all
> files are in .txt files. Now kindly help me with the following:
> 
> I have the R-codes for the "Quantile Augmented Mean Group" method. The
> relevant codes and data are attached herewith.
> Note: The link to the reference paper is:
> https://www.econ.cam.ac.uk/people-files/emeritus/mhp1/fp20/qmg40-rev21.pdf
> My Issue:
> When I run the given codes to estimate the results for my own data. I have
> to face several errors. I humbly request the experts to help me to estimate
> the results for my data.
> Thank you so much for your precious time.
> 
> Regards
> 
> Muhammad Zubair Chishti
> Ph.D. Student
> School of Business,
> Zhengzhou University, Henan, China.
> My Google scholar link:
> https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
> My ResearchGate link: https://www.researchgate.net/profile/Muhammad-Chishti
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat Jun 11 15:56:54 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 11 Jun 2022 14:56:54 +0100
Subject: [R] igraph: layout help
In-Reply-To: <CAEQKoCEsjz565HPLaPG6CS17B-TwxaiX9mFP5wTP5XnHb0QxFw@mail.gmail.com>
References: <CAEQKoCEsjz565HPLaPG6CS17B-TwxaiX9mFP5wTP5XnHb0QxFw@mail.gmail.com>
Message-ID: <b0ed93e6-f6c0-fb0c-aa3a-cfd79c4ef314@sapo.pt>

Hello,

The code below is a hack.
First I create a layout ll, then see that vertex K coordinates are in 
the layout matrix 2nd row.

Now, to multiply K's coordinates by a number d>1 will move the point 
away from A. Add that value to the K group and plot.


ll <- layout_with_kk(my.graph)
d <- ll[2, ]*5
ll[c(2, 9:11), ] <- t(t(ll[c(2, 9:11), ]) + d)

plot(my.graph, layout = ll)


Hope this helps,

Rui Barradas

?s 19:20 de 10/06/2022, Brian Smith escreveu:
> Hi,
> 
> I was trying to make a network plot of this data:
> 
> ============
> library(igraph)
> library(network)
> 
> df1 <- data.frame(from="A",to=c("B","C","D","E","F","G"),value=1)
> df2 <- data.frame(from="K",to=c("L","M","N"),value=1)
> df3 <- data.frame(from="A",to="K",value=3)
> my.df <- rbind(df1,df2,df3)
> 
> my.graph <- graph_from_data_frame(my.df,directed = F)
> plot(my.graph)
> ============
> 
> What I wanted was for nodes A to G to be very close together (touching each
> other). Similarly, nodes K to N should be very close together. The
> connecting edge (A to K) between these sets of points/vertices should be
> the only edge visible. How should I go about doing this?
> 
> Also, how can I change the parameters (node label, node color, etc.) of the
> graph?
> 
> Any help or link to documentation would be helpful. Or would
> 
> thanks!
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From |_j_rod @end|ng |rom hotm@||@com  Sat Jun 11 17:14:11 2022
From: |_j_rod @end|ng |rom hotm@||@com (Frank S.)
Date: Sat, 11 Jun 2022 15:14:11 +0000
Subject: [R] Interpreting the result of a model with random effects
Message-ID: <AM7P191MB11223ACA72D5A4A90D4CD25FBAA99@AM7P191MB1122.EURP191.PROD.OUTLOOK.COM>

Dear R users,

 I'm analyzing a particular score "y" among several individuals, each of which belongs to a center, a factor with three
different levels (3 possible centers). I have treated the "center" as a fixed effect, and as a random term (package lme4):

1) model.fix <- glm(y ~ var.1 + var.2 + var.3 + var.4 + var.5 + center, family = "binomial", data = dat)
2) model.rand <- glmer(y ~ var.1 + var.2 + var.3 + var.4 + var.5 + (1 | center), family = "binomial", data = dat)

The issue is that both models provide exactly the same coefficients and p-values for the 5 baseline variables, so I assumed
that it was due to the small number of levels (in fact,  too few ). However, when computing anova(model.rand, model.fix),
the output indicates a p-value < 0.001 in favour of the "model.rand". What's happening? Should I take the random terms?

Thanks for any help!

Frank S.

	[[alternative HTML version deleted]]


From mm@|ten @end|ng |rom gm@||@com  Sat Jun 11 17:21:38 2022
From: mm@|ten @end|ng |rom gm@||@com (Mitchell Maltenfort)
Date: Sat, 11 Jun 2022 11:21:38 -0400
Subject: [R] Interpreting the result of a model with random effects
In-Reply-To: <AM7P191MB11223ACA72D5A4A90D4CD25FBAA99@AM7P191MB1122.EURP191.PROD.OUTLOOK.COM>
References: <AM7P191MB11223ACA72D5A4A90D4CD25FBAA99@AM7P191MB1122.EURP191.PROD.OUTLOOK.COM>
Message-ID: <CANOgrHY0Z4PQ7JTDFJGdpE8d6rcmqV=GYmw8sMiwQPxFGEiaeA@mail.gmail.com>

Looks like the center effect improves overall accuracy while being
independent of the other terms.

A few things to try

Compare coef(model.fix) to fixef(model.rand).

Add center as a fixed effect to model .fix

Try a conditional logit (clogit from survival)

See how consistent the coefficients are





On Sat, Jun 11, 2022 at 11:14 AM Frank S. <f_j_rod at hotmail.com> wrote:

> Dear R users,
>
>  I'm analyzing a particular score "y" among several individuals, each of
> which belongs to a center, a factor with three
> different levels (3 possible centers). I have treated the "center" as a
> fixed effect, and as a random term (package lme4):
>
> 1) model.fix <- glm(y ~ var.1 + var.2 + var.3 + var.4 + var.5 + center,
> family = "binomial", data = dat)
> 2) model.rand <- glmer(y ~ var.1 + var.2 + var.3 + var.4 + var.5 + (1 |
> center), family = "binomial", data = dat)
>
> The issue is that both models provide exactly the same coefficients and
> p-values for the 5 baseline variables, so I assumed
> that it was due to the small number of levels (in fact,  too few ).
> However, when computing anova(model.rand, model.fix),
> the output indicates a p-value < 0.001 in favour of the "model.rand".
> What's happening? Should I take the random terms?
>
> Thanks for any help!
>
> Frank S.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
-- 
Sent from Gmail Mobile

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sat Jun 11 18:57:45 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 11 Jun 2022 09:57:45 -0700
Subject: [R] Interpreting the result of a model with random effects
In-Reply-To: <AM7P191MB11223ACA72D5A4A90D4CD25FBAA99@AM7P191MB1122.EURP191.PROD.OUTLOOK.COM>
References: <AM7P191MB11223ACA72D5A4A90D4CD25FBAA99@AM7P191MB1122.EURP191.PROD.OUTLOOK.COM>
Message-ID: <CAGxFJbSvky6aHzrZ1FbVdwqiGWSAAfQNOijKQTKopvPd4Veuwg@mail.gmail.com>

Please note: I am not an expert.

1. If there are only 3 centers (you didn't say) , then they are not a
random selection from a larger collection of centers and a random
effects model is inappropriate anyway;

2. Otherwise, you want to estimate a centers variance component from a
sample of size 3 ??

3. You cannot -- or at least should not -- compare nested fixed
effects models (with and without 'center') with different random
effects structures. The number of df associated with random effects is
unknown, and standard (asymptotic) likelihood ratio tests are wrong.
There's a big literature on this.

So my answer is no -- the anova p-value comparison is nonsense.

Again, note my initial caveat -- perhaps it will serve as an
invitation for an expert to respond.


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Sat, Jun 11, 2022 at 8:14 AM Frank S. <f_j_rod at hotmail.com> wrote:
>
> Dear R users,
>
>  I'm analyzing a particular score "y" among several individuals, each of which belongs to a center, a factor with three
> different levels (3 possible centers). I have treated the "center" as a fixed effect, and as a random term (package lme4):
>
> 1) model.fix <- glm(y ~ var.1 + var.2 + var.3 + var.4 + var.5 + center, family = "binomial", data = dat)
> 2) model.rand <- glmer(y ~ var.1 + var.2 + var.3 + var.4 + var.5 + (1 | center), family = "binomial", data = dat)
>
> The issue is that both models provide exactly the same coefficients and p-values for the 5 baseline variables, so I assumed
> that it was due to the small number of levels (in fact,  too few ). However, when computing anova(model.rand, model.fix),
> the output indicates a p-value < 0.001 in favour of the "model.rand". What's happening? Should I take the random terms?
>
> Thanks for any help!
>
> Frank S.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @tt|tude@h@nt@nu1977 @end|ng |rom gm@||@com  Sat Jun 11 16:24:25 2022
From: @tt|tude@h@nt@nu1977 @end|ng |rom gm@||@com (Shantanu Shimpi)
Date: Sat, 11 Jun 2022 19:54:25 +0530
Subject: [R] How to do non-parametric calculations in R
Message-ID: <CAN5VMj4mGAdy7Bg513oDmM-9G+Jk_opm4LZtxPQmWmBEQ_9YfQ@mail.gmail.com>

Dear R community,

Please help me in knowing how to do following non-parametric tests:

   1.  kruskal-Wallis test
   2.  Wilcoxson rank sum test
   3.  Lee Cronbac Alpha test
   4.  Spearman's Rank correlation test
   5.  Henry Garrett method formula calculations
   6.  Factor Analysis
   7.  Chi square test

Kindly guide me on the above queries in the easiest way.

A R lover,
Col Shantanu.
India.
attitudeshantanu1977 at gmail.com

7722030088

	[[alternative HTML version deleted]]


From pro|jcn@@h @end|ng |rom gm@||@com  Sat Jun 11 22:53:37 2022
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Sat, 11 Jun 2022 16:53:37 -0400
Subject: [R] How to do non-parametric calculations in R
In-Reply-To: <CAN5VMj4mGAdy7Bg513oDmM-9G+Jk_opm4LZtxPQmWmBEQ_9YfQ@mail.gmail.com>
References: <CAN5VMj4mGAdy7Bg513oDmM-9G+Jk_opm4LZtxPQmWmBEQ_9YfQ@mail.gmail.com>
Message-ID: <a35b3702-f529-b001-6cc4-cea115a14cc6@gmail.com>

Homework!

On 2022-06-11 10:24, Shantanu Shimpi wrote:
> Dear R community,
> 
> Please help me in knowing how to do following non-parametric tests:
> 
>     1.  kruskal-Wallis test
>     2.  Wilcoxson rank sum test
>     3.  Lee Cronbac Alpha test
>     4.  Spearman's Rank correlation test
>     5.  Henry Garrett method formula calculations
>     6.  Factor Analysis
>     7.  Chi square test
> 
> Kindly guide me on the above queries in the easiest way.
> 
> A R lover,
> Col Shantanu.
> India.
> attitudeshantanu1977 at gmail.com
> 
> 7722030088
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat Jun 11 23:26:31 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 11 Jun 2022 14:26:31 -0700
Subject: [R] How to do non-parametric calculations in R
In-Reply-To: <a35b3702-f529-b001-6cc4-cea115a14cc6@gmail.com>
References: <CAN5VMj4mGAdy7Bg513oDmM-9G+Jk_opm4LZtxPQmWmBEQ_9YfQ@mail.gmail.com>
 <a35b3702-f529-b001-6cc4-cea115a14cc6@gmail.com>
Message-ID: <1D74B65E-079B-41E1-B0BC-3597D62FC98B@dcn.davis.ca.us>

Really? But it is such a random list that I thought it was a test of our ability to resist providing impromptu lectures on off-list-topics, since we all like to expound on "stuff" even when R isn't needed to understand them. Or perhaps "A R Lover" just didn't read the Posting Guide warnings about HTML email, homework, and statistics, and will soon have done do and be sharing some R code that is giving them an error.

On June 11, 2022 1:53:37 PM PDT, J C Nash <profjcnash at gmail.com> wrote:
>Homework!
>
>On 2022-06-11 10:24, Shantanu Shimpi wrote:
>> Dear R community,
>> 
>> Please help me in knowing how to do following non-parametric tests:
>> 
>>     1.  kruskal-Wallis test
>>     2.  Wilcoxson rank sum test
>>     3.  Lee Cronbac Alpha test
>>     4.  Spearman's Rank correlation test
>>     5.  Henry Garrett method formula calculations
>>     6.  Factor Analysis
>>     7.  Chi square test
>> 
>> Kindly guide me on the above queries in the easiest way.
>> 
>> A R lover,
>> Col Shantanu.
>> India.
>> attitudeshantanu1977 at gmail.com
>> 
>> 7722030088
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Sat Jun 11 14:42:33 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Sat, 11 Jun 2022 17:42:33 +0500
Subject: [R] A humble request regarding QAMG method in R
Message-ID: <CAMfKi3Kd9oP2nWe6boV4rNjVHAZ4eV=vwpnCAzh=6Gw-ZWrdbg@mail.gmail.com>

Hi, Dear Respected Professors! I hope that you are doing well. Now all
files are in .txt files. Now kindly help me with the following:

I have the R-codes for the "Quantile Augmented Mean Group" method. The
relevant codes and data are attached herewith.
Note: The link to the reference paper is:
https://www.econ.cam.ac.uk/people-files/emeritus/mhp1/fp20/qmg40-rev21.pdf
My Issue:
When I run the given codes to estimate the results for my own data. I have
to face several errors. I humbly request the experts to help me to estimate
the results for my data.
Thank you so much for your precious time.

Regards

Muhammad Zubair Chishti
Ph.D. Student
School of Business,
Zhengzhou University, Henan, China.
My Google scholar link:
https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
My ResearchGate link: https://www.researchgate.net/profile/Muhammad-Chishti

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: rq.fit.panel.all.revised.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220611/3e3486d0/attachment.txt>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: data.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220611/3e3486d0/attachment-0001.txt>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Table.3.1.and.Table.3.2.design1.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220611/3e3486d0/attachment-0002.txt>

From h@n@tezer@ @end|ng |rom gm@||@com  Sat Jun 11 23:36:21 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (hanatezera)
Date: Sun, 12 Jun 2022 00:36:21 +0300
Subject: [R] Changing sign of columns and values
Message-ID: <62a50adb.1c69fb81.f6535.329d@mx.google.com>

I have the following data set in data frameIDnumber? OA? EA? beta1? ? ? ? ? ? ? ? ? ? ? ? ?C? ? ? ?A? ? ? ?-0.052? ? ? ? ? ? ? ? ? ? ? ? ?G? ? ? ? A? ? ? ? 0.0983? ? ? ? ? ? ? ? ? ? ? ? ? G? ? ? ? T? ? ? ? -0.789....I want to change the sign of negative beta. If the negative value change to postive i want to switch its EA and OA.My desire result will beIDnumber? OA? EA? beta1? ? ? ? ? ? ? ? ? ? ? ? ?A? ? ? C? ? ?0.052? ? ? ? ? ? ? ? ? ? ? ? ?G? ? ? ? A? ? ? ? 0.0983? ? ? ? ? ? ? ? ? ? ? ? ? T? ? ? ? ?G? ? ? ? 0.789....Any one can help me with r codes??kind regards,Hana
	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun Jun 12 00:37:26 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 11 Jun 2022 15:37:26 -0700
Subject: [R] Changing sign of columns and values
In-Reply-To: <62a50adb.1c69fb81.f6535.329d@mx.google.com>
References: <62a50adb.1c69fb81.f6535.329d@mx.google.com>
Message-ID: <CAGxFJbTdmPcBx0tUntijFjQS-8D-srA73eMbG0GOr0EcPwVh7Q@mail.gmail.com>

This is a plain text list. Your html post got mangled (see below). You
are more likely to get a useful response if you follow the posting
guide (linked below) and post in plain text.

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Sat, Jun 11, 2022 at 3:27 PM hanatezera <hanatezera at gmail.com> wrote:
>
> I have the following data set in data frameIDnumber  OA  EA  beta1                         C       A       -0.052                         G        A        0.0983                          G        T        -0.789....I want to change the sign of negative beta. If the negative value change to postive i want to switch its EA and OA.My desire result will beIDnumber  OA  EA  beta1                         A      C     0.052                         G        A        0.0983                          T         G        0.789....Any one can help me with r codes? kind regards,Hana
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Sun Jun 12 00:59:27 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sun, 12 Jun 2022 08:59:27 +1000
Subject: [R] Changing sign of columns and values
In-Reply-To: <62a50adb.1c69fb81.f6535.329d@mx.google.com>
References: <62a50adb.1c69fb81.f6535.329d@mx.google.com>
Message-ID: <CA+8X3fX6_0ZD1-ictjoH2SSWg30B3MC3c8OKyNsPAA6+Ui+hyA@mail.gmail.com>

Hi Hana,
I think this is what you want:

# first read in your example
mydf<-read.table(text=
"IDnumber  OA  EA  beta
1   C       A       -0.05
2   G        A        0.098
3   G        T        -0.789",
header=TRUE,stringsAsFactors=FALSE)
# check it
mydf
 IDnumber OA EA   beta
1        1  C  A -0.050
2        2  G  A  0.098
3        3  G  T -0.789
# change values of mydf$beta to absolute values
mydf$beta<-abs(mydf$beta)
mydf
 IDnumber OA EA  beta
1        1  C  A 0.050
2        2  G  A 0.098
3        3  G  T 0.789

Jim

On Sun, Jun 12, 2022 at 8:27 AM hanatezera <hanatezera at gmail.com> wrote:
>
> I have the following data set in data frameIDnumber  OA  EA  beta1                         C       A       -0.052                         G        A        0.0983                          G        T        -0.789....I want to change the sign of negative beta. If the negative value change to postive i want to switch its EA and OA.My desire result will beIDnumber  OA  EA  beta1                         A      C     0.052                         G        A        0.0983                          T         G        0.789....Any one can help me with r codes? kind regards,Hana
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From h@n@tezer@ @end|ng |rom gm@||@com  Sun Jun 12 01:11:15 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (hanatezera)
Date: Sun, 12 Jun 2022 02:11:15 +0300
Subject: [R] Changing sign of columns and values
In-Reply-To: <CA+8X3fX6_0ZD1-ictjoH2SSWg30B3MC3c8OKyNsPAA6+Ui+hyA@mail.gmail.com>
Message-ID: <62a52119.1c69fb81.e10ce.36e6@mx.google.com>

Dear jim thanks for your help! I want to change also the value of OA and EF simultaneously.For instance i am looking the datamydf IDnumber OA EA? beta1??????? 1? A? C? 0.0502??????? 2? G? A 0.0983??????? 3? T? ?G 0.789Best,?Hana? ? ? ? ? ? ? ? ?
-------- Original message --------From: Jim Lemon <drjimlemon at gmail.com> Date: 6/12/22  1:59 AM  (GMT+03:00) To: hanatezera <hanatezera at gmail.com>, r-help mailing list <r-help at r-project.org> Subject: Re: [R] Changing sign of columns and values Hi Hana,I think this is what you want:# first read in your examplemydf<-read.table(text="IDnumber? OA? EA? beta1?? C?????? A?????? -0.052?? G??????? A??????? 0.0983?? G??????? T??????? -0.789",header=TRUE,stringsAsFactors=FALSE)# check itmydf IDnumber OA EA?? beta1??????? 1? C? A -0.0502??????? 2? G? A? 0.0983??????? 3? G? T -0.789# change values of mydf$beta to absolute valuesmydf$beta<-abs(mydf$beta)mydf IDnumber OA EA? beta1??????? 1? C? A 0.0502??????? 2? G? A 0.0983??????? 3? G? T 0.789JimOn Sun, Jun 12, 2022 at 8:27 AM hanatezera <hanatezera at gmail.com> wrote:>> I have the following data set in data frameIDnumber? OA? EA? beta1???????????????????????? C?????? A?????? -0.052???????????????????????? G??????? A??????? 0.0983????????????????????????? G??????? T??????? -0.789....I want to change the sign of negative beta. If the negative value change to postive i want to switch its EA and OA.My desire result will beIDnumber? OA? EA? beta1???????????????????????? A????? C???? 0.052???????????????????????? G??????? A??????? 0.0983????????????????????????? T???????? G??????? 0.789....Any one can help me with r codes? kind regards,Hana>???????? [[alternative HTML version deleted]]>> ______________________________________________> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see> https://stat.ethz.ch/mailman/listinfo/r-help> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html> and provide commented, minimal, self-contained, reproducible code.
	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Sun Jun 12 01:19:43 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Sat, 11 Jun 2022 23:19:43 +0000
Subject: [R] How to do non-parametric calculations in R
In-Reply-To: <1D74B65E-079B-41E1-B0BC-3597D62FC98B@dcn.davis.ca.us>
References: <CAN5VMj4mGAdy7Bg513oDmM-9G+Jk_opm4LZtxPQmWmBEQ_9YfQ@mail.gmail.com>
 <a35b3702-f529-b001-6cc4-cea115a14cc6@gmail.com>
 <1D74B65E-079B-41E1-B0BC-3597D62FC98B@dcn.davis.ca.us>
Message-ID: <BN6PR2201MB1553B210DFA6242A423899D8CFA99@BN6PR2201MB1553.namprd22.prod.outlook.com>

LOL. Thank goodness I successfully rolled my saving throw and resisted, at least for this round.
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Jeff Newmiller
Sent: Saturday, June 11, 2022 5:27 PM
To: r-help at r-project.org; J C Nash <profjcnash at gmail.com>
Subject: Re: [R] How to do non-parametric calculations in R

[External Email]

Really? But it is such a random list that I thought it was a test of our ability to resist providing impromptu lectures on off-list-topics, since we all like to expound on "stuff" even when R isn't needed to understand them. Or perhaps "A R Lover" just didn't read the Posting Guide warnings about HTML email, homework, and statistics, and will soon have done do and be sharing some R code that is giving them an error.

On June 11, 2022 1:53:37 PM PDT, J C Nash <profjcnash at gmail.com> wrote:
>Homework!
>
>On 2022-06-11 10:24, Shantanu Shimpi wrote:
>> Dear R community,
>>
>> Please help me in knowing how to do following non-parametric tests:
>>
>>     1.  kruskal-Wallis test
>>     2.  Wilcoxson rank sum test
>>     3.  Lee Cronbac Alpha test
>>     4.  Spearman's Rank correlation test
>>     5.  Henry Garrett method formula calculations
>>     6.  Factor Analysis
>>     7.  Chi square test
>>
>> Kindly guide me on the above queries in the easiest way.
>>
>> A R lover,
>> Col Shantanu.
>> India.
>> attitudeshantanu1977 at gmail.com
>>
>> 7722030088
>>
>>      [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
>> lman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVe
>> AsRzsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5Yb
>> F8c2Lixwy8&s=zhlVVKEML3MeOEdjlR2Z1gqYLVcrE0gpEiFPdo0MxNg&e=
>> PLEASE do read the posting guide 
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
>> rg_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kV
>> eAsRzsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5Y
>> bF8c2Lixwy8&s=IRGFPuDAXZu6xEr36GrRy5jeXkI0D62fDLt-FxbIqBs&e=
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailm
>an_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRz
>sn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5YbF8c2Li
>xwy8&s=zhlVVKEML3MeOEdjlR2Z1gqYLVcrE0gpEiFPdo0MxNg&e=
>PLEASE do read the posting guide 
>https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org
>_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsR
>zsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5YbF8c2L
>ixwy8&s=IRGFPuDAXZu6xEr36GrRy5jeXkI0D62fDLt-FxbIqBs&e=
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5YbF8c2Lixwy8&s=zhlVVKEML3MeOEdjlR2Z1gqYLVcrE0gpEiFPdo0MxNg&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5YbF8c2Lixwy8&s=IRGFPuDAXZu6xEr36GrRy5jeXkI0D62fDLt-FxbIqBs&e=
and provide commented, minimal, self-contained, reproducible code.


From twoo|m@n @end|ng |rom ont@rgettek@com  Sun Jun 12 01:48:57 2022
From: twoo|m@n @end|ng |rom ont@rgettek@com (Tom Woolman)
Date: Sat, 11 Jun 2022 19:48:57 -0400
Subject: [R] How to do non-parametric calculations in R
In-Reply-To: <BN6PR2201MB1553B210DFA6242A423899D8CFA99@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAN5VMj4mGAdy7Bg513oDmM-9G+Jk_opm4LZtxPQmWmBEQ_9YfQ@mail.gmail.com>
 <a35b3702-f529-b001-6cc4-cea115a14cc6@gmail.com>
 <1D74B65E-079B-41E1-B0BC-3597D62FC98B@dcn.davis.ca.us>
 <BN6PR2201MB1553B210DFA6242A423899D8CFA99@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <25f7067356d5b2f33675edd0ffa741a9@ontargettek.com>

Imagine that it's the year 2022 and you don't know how to look up 
information about performing a Kruskal-Wallis H test.

  It would take you longer to join the listserv and then write such a 
cokamemie email than to open the stats textbook you are supposed to have 
for the course, much less doing a simple search query.


On 2022-06-11 19:19, Ebert,Timothy Aaron wrote:
> LOL. Thank goodness I successfully rolled my saving throw and
> resisted, at least for this round.
> Tim
> 
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Jeff Newmiller
> Sent: Saturday, June 11, 2022 5:27 PM
> To: r-help at r-project.org; J C Nash <profjcnash at gmail.com>
> Subject: Re: [R] How to do non-parametric calculations in R
> 
> [External Email]
> 
> Really? But it is such a random list that I thought it was a test of
> our ability to resist providing impromptu lectures on off-list-topics,
> since we all like to expound on "stuff" even when R isn't needed to
> understand them. Or perhaps "A R Lover" just didn't read the Posting
> Guide warnings about HTML email, homework, and statistics, and will
> soon have done do and be sharing some R code that is giving them an
> error.
> 
> On June 11, 2022 1:53:37 PM PDT, J C Nash <profjcnash at gmail.com> wrote:
>> Homework!
>> 
>> On 2022-06-11 10:24, Shantanu Shimpi wrote:
>>> Dear R community,
>>> 
>>> Please help me in knowing how to do following non-parametric tests:
>>> 
>>>     1.  kruskal-Wallis test
>>>     2.  Wilcoxson rank sum test
>>>     3.  Lee Cronbac Alpha test
>>>     4.  Spearman's Rank correlation test
>>>     5.  Henry Garrett method formula calculations
>>>     6.  Factor Analysis
>>>     7.  Chi square test
>>> 
>>> Kindly guide me on the above queries in the easiest way.
>>> 
>>> A R lover,
>>> Col Shantanu.
>>> India.
>>> attitudeshantanu1977 at gmail.com
>>> 
>>> 7722030088
>>> 
>>>      [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
>>> lman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVe
>>> AsRzsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5Yb
>>> F8c2Lixwy8&s=zhlVVKEML3MeOEdjlR2Z1gqYLVcrE0gpEiFPdo0MxNg&e=
>>> PLEASE do read the posting guide
>>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
>>> rg_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kV
>>> eAsRzsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5Y
>>> bF8c2Lixwy8&s=IRGFPuDAXZu6xEr36GrRy5jeXkI0D62fDLt-FxbIqBs&e=
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailm
>> an_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRz
>> sn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5YbF8c2Li
>> xwy8&s=zhlVVKEML3MeOEdjlR2Z1gqYLVcrE0gpEiFPdo0MxNg&e=
>> PLEASE do read the posting guide
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org
>> _posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsR
>> zsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5YbF8c2L
>> ixwy8&s=IRGFPuDAXZu6xEr36GrRy5jeXkI0D62fDLt-FxbIqBs&e=
>> and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Sent from my phone. Please excuse my brevity.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5YbF8c2Lixwy8&s=zhlVVKEML3MeOEdjlR2Z1gqYLVcrE0gpEiFPdo0MxNg&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5YbF8c2Lixwy8&s=IRGFPuDAXZu6xEr36GrRy5jeXkI0D62fDLt-FxbIqBs&e=
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Sun Jun 12 05:42:52 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sun, 12 Jun 2022 13:42:52 +1000
Subject: [R] Changing sign of columns and values
In-Reply-To: <62a52119.1c69fb81.e10ce.36e6@mx.google.com>
References: <CA+8X3fX6_0ZD1-ictjoH2SSWg30B3MC3c8OKyNsPAA6+Ui+hyA@mail.gmail.com>
 <62a52119.1c69fb81.e10ce.36e6@mx.google.com>
Message-ID: <CA+8X3fUB-vzBi0Jfr66tDUXQT4tctENg4=4sBvWxsiKBaEBUvg@mail.gmail.com>

Hi Hana,
I didn't look closely. The simplest rule that I can see is that the
letters (nucleotides?) should be swapped if there has been a sign
change in the beta value. Therefore:

mydf<-read.table(text=
"IDnumber  OA  EA  beta
1   C       A       -0.05
2   G        A        0.098
3   G        T        -0.789",
header=TRUE,stringsAsFactors=FALSE)
# logical vector marking the rows to be swapped
swap_letters<-mydf$beta < 0
# save the OA values to be swapped
newEA<-mydf$OA[swap_letters]
# change the OAs to EAs
mydf$OA[swap_letters]<-mydf$EA[swap_letters]
# set the relevant EA values to the old OA values
mydf$EA[swap_letters]<-newEA
# change the beta values
mydf$beta<-abs(mydf$beta)
mydf

Jim



On Sun, Jun 12, 2022 at 9:11 AM hanatezera <hanatezera at gmail.com> wrote:
>
> Dear jim thanks for your help! I want to change also the value of OA and EF simultaneously.
> For instance i am looking the data
> mydf
> IDnumber OA EA  beta
> 1        1  A  C  0.050
> 2        2  G  A 0.098
> 3        3  T   G 0.789
>
> Best,
> Hana
>
>
>
> -------- Original message --------
> From: Jim Lemon <drjimlemon at gmail.com>
> Date: 6/12/22 1:59 AM (GMT+03:00)
> To: hanatezera <hanatezera at gmail.com>, r-help mailing list <r-help at r-project.org>
> Subject: Re: [R] Changing sign of columns and values
>
> Hi Hana,
> I think this is what you want:
>
> # first read in your example
> mydf<-read.table(text=
> "IDnumber  OA  EA  beta
> 1   C       A       -0.05
> 2   G        A        0.098
> 3   G        T        -0.789",
> header=TRUE,stringsAsFactors=FALSE)
> # check it
> mydf
> IDnumber OA EA   beta
> 1        1  C  A -0.050
> 2        2  G  A  0.098
> 3        3  G  T -0.789
> # change values of mydf$beta to absolute values
> mydf$beta<-abs(mydf$beta)
> mydf
> IDnumber OA EA  beta
> 1        1  C  A 0.050
> 2        2  G  A 0.098
> 3        3  G  T 0.789
>
> Jim
>
> On Sun, Jun 12, 2022 at 8:27 AM hanatezera <hanatezera at gmail.com> wrote:
> >
> > I have the following data set in data frameIDnumber  OA  EA  beta1                         C       A       -0.052                         G        A        0.0983                          G        T        -0.789....I want to change the sign of negative beta. If the negative value change to postive i want to switch its EA and OA.My desire result will beIDnumber  OA  EA  beta1                         A      C     0.052                         G        A        0.0983                          T         G        0.789....Any one can help me with r codes? kind regards,Hana
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From h@n@tezer@ @end|ng |rom gm@||@com  Sun Jun 12 13:14:34 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (hanatezera)
Date: Sun, 12 Jun 2022 14:14:34 +0300
Subject: [R] Changing sign of columns and values
In-Reply-To: <CA+8X3fUB-vzBi0Jfr66tDUXQT4tctENg4=4sBvWxsiKBaEBUvg@mail.gmail.com>
Message-ID: <62a5ca9c.1c69fb81.e10ce.4c1d@mx.google.com>

Dear Jim, Thanks a lot this is exactly i am looking for.Stay safe and blessed !Best,Hana
-------- Original message --------From: Jim Lemon <drjimlemon at gmail.com> Date: 6/12/22  6:43 AM  (GMT+03:00) To: hanatezera <hanatezera at gmail.com> Cc: r-help mailing list <r-help at r-project.org> Subject: Re: [R] Changing sign of columns and values Hi Hana,I didn't look closely. The simplest rule that I can see is that theletters (nucleotides?) should be swapped if there has been a signchange in the beta value. Therefore:mydf<-read.table(text="IDnumber? OA? EA? beta1?? C?????? A?????? -0.052?? G??????? A??????? 0.0983?? G??????? T??????? -0.789",header=TRUE,stringsAsFactors=FALSE)# logical vector marking the rows to be swappedswap_letters<-mydf$beta < 0# save the OA values to be swappednewEA<-mydf$OA[swap_letters]# change the OAs to EAsmydf$OA[swap_letters]<-mydf$EA[swap_letters]# set the relevant EA values to the old OA valuesmydf$EA[swap_letters]<-newEA# change the beta valuesmydf$beta<-abs(mydf$beta)mydfJimOn Sun, Jun 12, 2022 at 9:11 AM hanatezera <hanatezera at gmail.com> wrote:>> Dear jim thanks for your help! I want to change also the value of OA and EF simultaneously.> For instance i am looking the data> mydf> IDnumber OA EA? beta> 1??????? 1? A? C? 0.050> 2??????? 2? G? A 0.098> 3??????? 3? T?? G 0.789>> Best,> Hana>>>> -------- Original message --------> From: Jim Lemon <drjimlemon at gmail.com>> Date: 6/12/22 1:59 AM (GMT+03:00)> To: hanatezera <hanatezera at gmail.com>, r-help mailing list <r-help at r-project.org>> Subject: Re: [R] Changing sign of columns and values>> Hi Hana,> I think this is what you want:>> # first read in your example> mydf<-read.table(text=> "IDnumber? OA? EA? beta> 1?? C?????? A?????? -0.05> 2?? G??????? A??????? 0.098> 3?? G??????? T??????? -0.789",> header=TRUE,stringsAsFactors=FALSE)> # check it> mydf> IDnumber OA EA?? beta> 1??????? 1? C? A -0.050> 2??????? 2? G? A? 0.098> 3??????? 3? G? T -0.789> # change values of mydf$beta to absolute values> mydf$beta<-abs(mydf$beta)> mydf> IDnumber OA EA? beta> 1??????? 1? C? A 0.050> 2??????? 2? G? A 0.098> 3??????? 3? G? T 0.789>> Jim>> On Sun, Jun 12, 2022 at 8:27 AM hanatezera <hanatezera at gmail.com> wrote:> >> > I have the following data set in data frameIDnumber? OA? EA? beta1???????????????????????? C?????? A?????? -0.052???????????????????????? G??????? A??????? 0.0983????????????????????????? G??????? T??????? -0.789....I want to change the sign of negative beta. If the negative value change to postive i want to switch its EA and OA.My desire result will beIDnumber? OA? EA? beta1???????????????????????? A????? C???? 0.052???????????????????????? G??????? A??????? 0.0983????????????????????????? T???????? G??????? 0.789....Any one can help me with r codes? kind regards,Hana> >???????? [[alternative HTML version deleted]]> >> > ______________________________________________> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see> > https://stat.ethz.ch/mailman/listinfo/r-help> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html> > and provide commented, minimal, self-contained, reproducible code.
	[[alternative HTML version deleted]]


From h@n@tezer@ @end|ng |rom gm@||@com  Sun Jun 12 16:07:37 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Sun, 12 Jun 2022 17:07:37 +0300
Subject: [R] comparing and changing(swaping) the value of columns from two
 diffrent data farmes
Message-ID: <CAEuJ0VAtwkiAOd05hCGCdUP5_LMq2N-ZMbx1_Hyr5yjwCqJ6pg@mail.gmail.com>

I have the following two data frames
Data frame 1
"SNPID"	"OA"	"EA"	
"snp001"	"C"	"A"	
"snp002"	"G"	"A"	
"snp003"	"C"	"A"	
"snp004"	"G"	"A"	
"snp005"	"C"	"T"	

Data frame 2
SNPID	OA	EA	id00001	id00002	id00003	id00004	id00005
snp001	   A	C	 1.01	                   2	                  0.97	
          1.97	                  1.99
snp002	  A	G	 1.02	                  2	                   1	
         2	                      2
snp003	  C	A	  1	                 1.03	                    2	
          0	                       1
snp004	 A	G	   1.02	                   1.99	                     2	
             1.02	                    1.98
snp005	C	T	     1	                      0	                     1.01	
                    1	                        1

I want to  if  OA s and EAs in data frame 2 is the same as OAs and EAs
 data frame 1 in such case I want to keep all information in data
fram2 as it is . However if   OA s and EAs in data frame 2 is
different from OAs and EAs  data frame 1, I want to change OA s and
EAs in data frame 2 to  OAs and EAs  data frame 1 and   I want to
redefine the values of the dosages (ids) of the variant (the dosage d
for the i-th individual and the j-th variant would become d_ij
new=2-dij.
Dosage [j,]=2-dosage[j,]
My desire data frame looks like
Dataframe new
SNPID"	"OA"	   "EA"	       id00001	id00002	id00003	id00004	id00005
"snp001"	"C"	"A"	2-1.01	          2- 2	  2-    0.97	       2-1.97	
    2-1.99
"snp002"	"G"	"A"	2- 1.02	             2- 2	            2-1	
  2-2	                  2- 2
"snp003"	"C"	"A"	  1	               1.03	                    2	
            0	                       1
"snp004"	"G"	"A"	2-1.02	                 2-1.99	
2-2	               2-1.02	                 2-1.98
"snp005"	"C"	"T"	     1	                      0	
1.01	                       1	                        1
Dose any one can help me the r code for the above.
Kind regards,
Hana


From bgunter@4567 @end|ng |rom gm@||@com  Sun Jun 12 17:41:15 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 12 Jun 2022 08:41:15 -0700
Subject: [R] 
 comparing and changing(swaping) the value of columns from two
 diffrent data farmes
In-Reply-To: <CAEuJ0VAtwkiAOd05hCGCdUP5_LMq2N-ZMbx1_Hyr5yjwCqJ6pg@mail.gmail.com>
References: <CAEuJ0VAtwkiAOd05hCGCdUP5_LMq2N-ZMbx1_Hyr5yjwCqJ6pg@mail.gmail.com>
Message-ID: <CAGxFJbQqRZ261pXbWaQHxAou-bR7H9pnb5Ptc-uXN_Pb7hk4mg@mail.gmail.com>

Is this some sort of homework? This list has a no homework policy.

If not, please show us your attempt to solve the problem. You seem to
be asking us to do your work for you, which is not a good way to learn
R. Show us your attempt and the error messages and/or incorrect
results you got. You will improve your R skills faster this way. IMO
only, of course. The specifications you provided seem to require
fairly basic data manipulations that you should really learn how to do
yourself.

Also...
Thanks for the reproducible example -- that is a good way to get a
helpful response. But please give us the code that creates the example
rather than just providing the text, which requires us to read and
convert it ourselves, an extra step. In general, if we can just rerun
your code it makes it easier to help ... which makes it more likely
that someone will give you a useful response.

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Sun, Jun 12, 2022 at 7:07 AM anteneh asmare <hanatezera at gmail.com> wrote:
>
> I have the following two data frames
> Data frame 1
> "SNPID" "OA"    "EA"
> "snp001"        "C"     "A"
> "snp002"        "G"     "A"
> "snp003"        "C"     "A"
> "snp004"        "G"     "A"
> "snp005"        "C"     "T"
>
> Data frame 2
> SNPID   OA      EA      id00001 id00002 id00003 id00004 id00005
> snp001     A    C        1.01                      2                      0.97
>           1.97                    1.99
> snp002    A     G        1.02                     2                        1
>          2                            2
> snp003    C     A         1                      1.03                       2
>           0                            1
> snp004   A      G          1.02                    1.99                      2
>              1.02                           1.98
> snp005  C       T            1                        0                      1.01
>                     1                           1
>
> I want to  if  OA s and EAs in data frame 2 is the same as OAs and EAs
>  data frame 1 in such case I want to keep all information in data
> fram2 as it is . However if   OA s and EAs in data frame 2 is
> different from OAs and EAs  data frame 1, I want to change OA s and
> EAs in data frame 2 to  OAs and EAs  data frame 1 and   I want to
> redefine the values of the dosages (ids) of the variant (the dosage d
> for the i-th individual and the j-th variant would become d_ij
> new=2-dij.
> Dosage [j,]=2-dosage[j,]
> My desire data frame looks like
> Dataframe new
> SNPID"  "OA"       "EA"        id00001  id00002 id00003 id00004 id00005
> "snp001"        "C"     "A"     2-1.01            2- 2    2-    0.97           2-1.97
>     2-1.99
> "snp002"        "G"     "A"     2- 1.02              2- 2                   2-1
>   2-2                     2- 2
> "snp003"        "C"     "A"       1                    1.03                         2
>             0                          1
> "snp004"        "G"     "A"     2-1.02                   2-1.99
> 2-2                    2-1.02                    2-1.98
> "snp005"        "C"     "T"          1                        0
> 1.01                           1                                1
> Dose any one can help me the r code for the above.
> Kind regards,
> Hana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Sun Jun 12 19:11:46 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 12 Jun 2022 10:11:46 -0700
Subject: [R] 
 comparing and changing(swaping) the value of columns from two
 diffrent data farmes
In-Reply-To: <CAEuJ0VAnSXxicUAcukGUrGyy0ZF4yk5mmTx5GUXNnE7m12XJrQ@mail.gmail.com>
References: <CAEuJ0VAtwkiAOd05hCGCdUP5_LMq2N-ZMbx1_Hyr5yjwCqJ6pg@mail.gmail.com>
 <CAGxFJbQqRZ261pXbWaQHxAou-bR7H9pnb5Ptc-uXN_Pb7hk4mg@mail.gmail.com>
 <CAEuJ0VC-x6wOzE=Fgcr26Zq=A7mYbaonip0mPDb3Gz46w0KJ9A@mail.gmail.com>
 <CAGxFJbSuxDuGWw9b7C3ZFmtR3y1Qj7R7dchnH-J_drxo0vwhbg@mail.gmail.com>
 <CAEuJ0VAnSXxicUAcukGUrGyy0ZF4yk5mmTx5GUXNnE7m12XJrQ@mail.gmail.com>
Message-ID: <CAGxFJbRFAYuCCgJn-ZTjCf_zh143xAoDqm201u4SEG=BF+zoVw@mail.gmail.com>

Excellent!

I am sharing the below with the list in case it might be useful for others.

Here is a similar approach that uses logical indexing directly, rather
than through ifelse(), via a simple function to pick out the rows to
change. Figuring out the details of how it works may be helpful to you
in case it's not immediately obvious.

foo <- function(dat1, dat2, testcols){
      apply(dat1[, testcols] != dat2[,testcols],1,any) ## ?apply  ?any
for details
}

Yielding...

> wh <- foo(df1, df2, 2:3)
> wh ## logical index of rows that require changes
[1]  TRUE  TRUE FALSE  TRUE FALSE

> df2[wh, 4:8] <- 2 - df2[wh, 4:8]  ## per your specification
> ## and since df2's first 3 columns must be the same as df1's:
> df2[,2:3] <- df1[, 2:3]

> df2
   SNPID OA EA id00001 id00002 id00003 id00004 id00005
1 snp001  C  A    0.99    0.00    1.03    0.03    0.01
2 snp002  G  A    0.98    0.00    1.00    0.00    0.00
3 snp003  C  A    1.00    1.03    2.00    0.00    1.00
4 snp004  G  A    0.98    0.01    0.00    0.98    0.02
5 snp005  C  T    1.00    0.00    1.01    1.00    1.00



On Sun, Jun 12, 2022 at 9:32 AM anteneh asmare <hanatezera at gmail.com> wrote:
>
> Dear, Bert, Thanks i try to extract  dataframe1[ , c(2, 3)] and
> combine to dataframe2 , then i apply if else
> it works, Thanks
> best,
> Hana
>
> On 6/12/22, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> > No you don't. You need to learn R's data manipulation procedures so
> > that you can program such (in this case fairly simple) procedures
> > yourself.
> >
> > The ifelse() function might be a good place to start here.
> > ?ifelse
> >
> > Bert Gunter
> >
> >
> >
> > On Sun, Jun 12, 2022 at 8:58 AM anteneh asmare <hanatezera at gmail.com>
> > wrote:
> >>
> >> Dear Bert , Thanks for reply! I need the  the function or package that
> >> used to compare columns from two different data frames.
> >> Best,
> >> Hana
> >> On 6/12/22, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> >> > Is this some sort of homework? This list has a no homework policy.
> >> >
> >> > If not, please show us your attempt to solve the problem. You seem to
> >> > be asking us to do your work for you, which is not a good way to learn
> >> > R. Show us your attempt and the error messages and/or incorrect
> >> > results you got. You will improve your R skills faster this way. IMO
> >> > only, of course. The specifications you provided seem to require
> >> > fairly basic data manipulations that you should really learn how to do
> >> > yourself.
> >> >
> >> > Also...
> >> > Thanks for the reproducible example -- that is a good way to get a
> >> > helpful response. But please give us the code that creates the example
> >> > rather than just providing the text, which requires us to read and
> >> > convert it ourselves, an extra step. In general, if we can just rerun
> >> > your code it makes it easier to help ... which makes it more likely
> >> > that someone will give you a useful response.
> >> >
> >> > Bert Gunter
> >> >
> >> > "The trouble with having an open mind is that people keep coming along
> >> > and sticking things into it."
> >> > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >> >
> >> > On Sun, Jun 12, 2022 at 7:07 AM anteneh asmare <hanatezera at gmail.com>
> >> > wrote:
> >> >>
> >> >> I have the following two data frames
> >> >> Data frame 1
> >> >> "SNPID" "OA"    "EA"
> >> >> "snp001"        "C"     "A"
> >> >> "snp002"        "G"     "A"
> >> >> "snp003"        "C"     "A"
> >> >> "snp004"        "G"     "A"
> >> >> "snp005"        "C"     "T"
> >> >>
> >> >> Data frame 2
> >> >> SNPID   OA      EA      id00001 id00002 id00003 id00004 id00005
> >> >> snp001     A    C        1.01                      2
> >> >> 0.97
> >> >>           1.97                    1.99
> >> >> snp002    A     G        1.02                     2
> >> >> 1
> >> >>          2                            2
> >> >> snp003    C     A         1                      1.03
> >> >>  2
> >> >>           0                            1
> >> >> snp004   A      G          1.02                    1.99
> >> >>   2
> >> >>              1.02                           1.98
> >> >> snp005  C       T            1                        0
> >> >>   1.01
> >> >>                     1                           1
> >> >>
> >> >> I want to  if  OA s and EAs in data frame 2 is the same as OAs and EAs
> >> >>  data frame 1 in such case I want to keep all information in data
> >> >> fram2 as it is . However if   OA s and EAs in data frame 2 is
> >> >> different from OAs and EAs  data frame 1, I want to change OA s and
> >> >> EAs in data frame 2 to  OAs and EAs  data frame 1 and   I want to
> >> >> redefine the values of the dosages (ids) of the variant (the dosage d
> >> >> for the i-th individual and the j-th variant would become d_ij
> >> >> new=2-dij.
> >> >> Dosage [j,]=2-dosage[j,]
> >> >> My desire data frame looks like
> >> >> Dataframe new
> >> >> SNPID"  "OA"       "EA"        id00001  id00002 id00003 id00004
> >> >> id00005
> >> >> "snp001"        "C"     "A"     2-1.01            2- 2    2-    0.97
> >> >>     2-1.97
> >> >>     2-1.99
> >> >> "snp002"        "G"     "A"     2- 1.02              2- 2
> >> >>  2-1
> >> >>   2-2                     2- 2
> >> >> "snp003"        "C"     "A"       1                    1.03
> >> >>          2
> >> >>             0                          1
> >> >> "snp004"        "G"     "A"     2-1.02                   2-1.99
> >> >> 2-2                    2-1.02                    2-1.98
> >> >> "snp005"        "C"     "T"          1                        0
> >> >> 1.01                           1                                1
> >> >> Dose any one can help me the r code for the above.
> >> >> Kind regards,
> >> >> Hana
> >> >>
> >> >> ______________________________________________
> >> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> PLEASE do read the posting guide
> >> >> http://www.R-project.org/posting-guide.html
> >> >> and provide commented, minimal, self-contained, reproducible code.
> >> >
> >


From drj|m|emon @end|ng |rom gm@||@com  Mon Jun 13 01:41:48 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Mon, 13 Jun 2022 09:41:48 +1000
Subject: [R] Changing sign of columns and values
In-Reply-To: <CAEuJ0VAqKdBpQJ=Bih=9V3=bDQBL05d=okpfRxSa0fqTK4EvBg@mail.gmail.com>
References: <CA+8X3fUB-vzBi0Jfr66tDUXQT4tctENg4=4sBvWxsiKBaEBUvg@mail.gmail.com>
 <62a5ca9c.1c69fb81.e10ce.4c1d@mx.google.com>
 <CAEuJ0VAqKdBpQJ=Bih=9V3=bDQBL05d=okpfRxSa0fqTK4EvBg@mail.gmail.com>
Message-ID: <CA+8X3fWFE8zShP+depx-TeeFWZaYy=ySZpqDmcJkJwg95BTYcw@mail.gmail.com>

Hi Hana,
This is a bit more difficult, but the same basic steps apply. See the
comments for an explanation.
When you submit a problem like this, it is a lot easier if you send
the output from "dput" (e.g. dput(df1)) or set your data frames up
with "read.table" like I have done in the example below. It make it
lots easier for anyone who wants to reply to see what your dataset
looks like and input that dataset to devise a solution.
I have made some assumptions about marking the rows to be altered and
the arithmetic to do on those rows. What follows may not be as general
a solution as you want:

df1<-read.table(text=
 "SNPID OA    EA
 snp001        C     A
 snp002        G     A
 snp003        C     A
 snp004        G     A
 snp005        C     T",
 header=TRUE,stringsAsFactors=FALSE)

df2<-read.table(text=
 "SNPID   OA    EA   id00001 id00002 id00003 id00004 id00005
 snp001    A    C    1.01    2       0.97    1.97    1.99
 snp002    A    G    1.02    2       1       2       2
 snp003    C    A    1       1.03    2       0       1
 snp004    A    G    1.02    1.99    2       1.02    1.98
 snp005    C    T    1       0       1.01    1       1",
 header=TRUE,stringsAsFactors=FALSE)

# get a logical vector of the different XA values
reversals<-df1$EA != df2$EA | df1$OA != df2$OA
reversals

# set a variable to the maximum value of idxxxxx
# just to make the code easier to understand
maxid<-2

# create a copy of df2
dfnew<-df2

# now swap the XA columns and reflect the
# idxxxxxx values of the rows in which
# EA and OA are different between df1 and df2
# here I have looped through the rows
nrows<-dim(dfnew)[1]
idcols<-4:8
XAcols<-2:3
for(i in 1:nrows) {
 if(reversals[i]) {
  dfnew[i,XAcols]<-rev(dfnew[i,XAcols])
  dfnew[i,idcols]<-maxid-dfnew[i,idcols]
 }
}

dfnew

Jim

On Mon, Jun 13, 2022 at 12:09 AM anteneh asmare <hanatezera at gmail.com> wrote:
>
> Dear Jim, Morning I have the same issue regarding comparing and
> swapping the value of columns fro two different data frames.
> I have the following two data frames
> Data frame 1
> "SNPID" "OA"    "EA"
> "snp001"        "C"     "A"
> "snp002"        "G"     "A"
> "snp003"        "C"     "A"
> "snp004"        "G"     "A"
> "snp005"        "C"     "T"
>
> Data frame 2
> SNPID   OA      EA      id00001 id00002 id00003 id00004 id00005
> snp001     A    C        1.01                      2                      0.97
>           1.97                    1.99
> snp002    A     G        1.02                     2                        1
>          2                            2
> snp003    C     A         1                      1.03                       2
>           0                            1
> snp004   A      G          1.02                    1.99                      2
>              1.02                           1.98
> snp005  C       T            1                        0                      1.01
>                     1                           1
>
> I want to  if  OA s and EAs in data frame 2 is the same as OAs and EAs
>  data frame 1 in such case I want to keep all information in data
> fram2 as it is . However if   OA s and EAs in data frame 2 is
> different from OAs and EAs  data frame 1, I want to change OA s and
> EAs in data frame 2 to  OAs and EAs  data frame 1 and   I want to
> redefine the values of the dosages (ids) of the variant (the dosage d
> for the i-th individual and the j-th variant would become d_ij
> new=2-dij.
> Dosage [j,]=2-dosage[j,]
> My desire data frame looks like
> Dataframe new
> SNPID"  "OA"       "EA"        id00001  id00002 id00003 id00004 id00005
> "snp001"        "C"     "A"     2-1.01            2- 2    2-    0.97           2-1.97
>     2-1.99
> "snp002"        "G"     "A"     2- 1.02              2- 2                   2-1
>   2-2                     2- 2
> "snp003"        "C"     "A"       1                    1.03                         2
>             0                          1
> "snp004"        "G"     "A"     2-1.02                   2-1.99
> 2-2                    2-1.02                    2-1.98
> "snp005"        "C"     "T"          1                        0
> 1.01                           1                                1
>  can you help me the r code for the above.
> Kind regards,
> Hana
>
>
> On 6/12/22, hanatezera <hanatezera at gmail.com> wrote:
> > Dear Jim, Thanks a lot this is exactly i am looking for.Stay safe and
> > blessed !Best,Hana
> > -------- Original message --------From: Jim Lemon <drjimlemon at gmail.com>
> > Date: 6/12/22  6:43 AM  (GMT+03:00) To: hanatezera <hanatezera at gmail.com>
> > Cc: r-help mailing list <r-help at r-project.org> Subject: Re: [R] Changing
> > sign of columns and values Hi Hana,I didn't look closely. The simplest rule
> > that I can see is that theletters (nucleotides?) should be swapped if there
> > has been a signchange in the beta value.
> > Therefore:mydf<-read.table(text="IDnumber  OA  EA  beta1   C       A
> > -0.052   G        A        0.0983   G        T
> > -0.789",header=TRUE,stringsAsFactors=FALSE)# logical vector marking the rows
> > to be swappedswap_letters<-mydf$beta < 0# save the OA values to be
> > swappednewEA<-mydf$OA[swap_letters]# change the OAs to
> > EAsmydf$OA[swap_letters]<-mydf$EA[swap_letters]# set the relevant EA values
> > to the old OA valuesmydf$EA[swap_letters]<-newEA# change the beta
> > valuesmydf$beta<-abs(mydf$beta)mydfJimOn Sun, Jun 12, 2022 at 9:11 AM
> > hanatezera <hanatezera at gmail.com> wrote:>> Dear jim thanks for your help! I
> > want to change also the value of OA and EF simultaneously.> For instance i
> > am looking the data> mydf> IDnumber OA EA  beta> 1        1  A  C  0.050>
> > 2        2  G  A 0.098> 3        3  T   G 0.789>> Best,> Hana>>>> --------
> > Original message --------> From: Jim Lemon <drjimlemon at gmail.com>> Date:
> > 6/12/22 1:59 AM (GMT+03:00)> To: hanatezera <hanatezera at gmail.com>, r-help
> > mailing list <r-help at r-project.org>> Subject: Re: [R] Changing sign of
> > columns and values>> Hi Hana,> I think this is what you want:>> # first read
> > in your example> mydf<-read.table(text=> "IDnumber  OA  EA  beta> 1
> > C       A       -0.05> 2   G        A        0.098> 3   G        T
> > -0.789",> header=TRUE,stringsAsFactors=FALSE)> # check it> mydf> IDnumber OA
> > EA   beta> 1        1  C  A -0.050> 2        2  G  A  0.098> 3        3  G
> > T -0.789> # change values of mydf$beta to absolute values>
> > mydf$beta<-abs(mydf$beta)> mydf> IDnumber OA EA  beta> 1        1  C  A
> > 0.050> 2        2  G  A 0.098> 3        3  G  T 0.789>> Jim>> On Sun, Jun
> > 12, 2022 at 8:27 AM hanatezera <hanatezera at gmail.com> wrote:> >> > I have
> > the following data set in data frameIDnumber  OA  EA
> > beta1                         C       A       -0.052
> > G        A        0.0983                          G        T
> > -0.789....I want to change the sign of negative beta. If the negative value
> > change to postive i want to switch its EA and OA.My desire result will
> > beIDnumber  OA  EA  beta1                         A      C
> > 0.052                         G        A
> > 0.0983                          T         G        0.789....Any one can help
> > me with r codes? kind regards,Hana> >         [[alternative HTML version
> > deleted]]> >> > ______________________________________________> >
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see> >
> > https://stat.ethz.ch/mailman/listinfo/r-help> > PLEASE do read the posting
> > guide http://www.R-project.org/posting-guide.html> > and provide commented,
> > minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon Jun 13 05:13:12 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 13 Jun 2022 04:13:12 +0100
Subject: [R] Changing sign of columns and values
In-Reply-To: <CA+8X3fWFE8zShP+depx-TeeFWZaYy=ySZpqDmcJkJwg95BTYcw@mail.gmail.com>
References: <CA+8X3fUB-vzBi0Jfr66tDUXQT4tctENg4=4sBvWxsiKBaEBUvg@mail.gmail.com>
 <62a5ca9c.1c69fb81.e10ce.4c1d@mx.google.com>
 <CAEuJ0VAqKdBpQJ=Bih=9V3=bDQBL05d=okpfRxSa0fqTK4EvBg@mail.gmail.com>
 <CA+8X3fWFE8zShP+depx-TeeFWZaYy=ySZpqDmcJkJwg95BTYcw@mail.gmail.com>
Message-ID: <20eea35c-6ed9-8450-ab2f-40d151fb1249@sapo.pt>

Hello,

Using Jim's code, the following is vectorized and the result is 
identical to the for loop result.


# create a copy of df2
dfnew2 <- df2
dfnew2[reversals, XAcols] <- rev(dfnew2[reversals, XAcols])
dfnew2[reversals, idcols] <- maxid - dfnew2[reversals, idcols]

identical(dfnew, dfnew2)
#[1] TRUE


Hope this helps,

Rui Barradas

?s 00:41 de 13/06/2022, Jim Lemon escreveu:
> Hi Hana,
> This is a bit more difficult, but the same basic steps apply. See the
> comments for an explanation.
> When you submit a problem like this, it is a lot easier if you send
> the output from "dput" (e.g. dput(df1)) or set your data frames up
> with "read.table" like I have done in the example below. It make it
> lots easier for anyone who wants to reply to see what your dataset
> looks like and input that dataset to devise a solution.
> I have made some assumptions about marking the rows to be altered and
> the arithmetic to do on those rows. What follows may not be as general
> a solution as you want:
> 
> df1<-read.table(text=
>   "SNPID OA    EA
>   snp001        C     A
>   snp002        G     A
>   snp003        C     A
>   snp004        G     A
>   snp005        C     T",
>   header=TRUE,stringsAsFactors=FALSE)
> 
> df2<-read.table(text=
>   "SNPID   OA    EA   id00001 id00002 id00003 id00004 id00005
>   snp001    A    C    1.01    2       0.97    1.97    1.99
>   snp002    A    G    1.02    2       1       2       2
>   snp003    C    A    1       1.03    2       0       1
>   snp004    A    G    1.02    1.99    2       1.02    1.98
>   snp005    C    T    1       0       1.01    1       1",
>   header=TRUE,stringsAsFactors=FALSE)
> 
> # get a logical vector of the different XA values
> reversals<-df1$EA != df2$EA | df1$OA != df2$OA
> reversals
> 
> # set a variable to the maximum value of idxxxxx
> # just to make the code easier to understand
> maxid<-2
> 
> # create a copy of df2
> dfnew<-df2
> 
> # now swap the XA columns and reflect the
> # idxxxxxx values of the rows in which
> # EA and OA are different between df1 and df2
> # here I have looped through the rows
> nrows<-dim(dfnew)[1]
> idcols<-4:8
> XAcols<-2:3
> for(i in 1:nrows) {
>   if(reversals[i]) {
>    dfnew[i,XAcols]<-rev(dfnew[i,XAcols])
>    dfnew[i,idcols]<-maxid-dfnew[i,idcols]
>   }
> }
> 
> dfnew
> 
> Jim
> 
> On Mon, Jun 13, 2022 at 12:09 AM anteneh asmare <hanatezera at gmail.com> wrote:
>>
>> Dear Jim, Morning I have the same issue regarding comparing and
>> swapping the value of columns fro two different data frames.
>> I have the following two data frames
>> Data frame 1
>> "SNPID" "OA"    "EA"
>> "snp001"        "C"     "A"
>> "snp002"        "G"     "A"
>> "snp003"        "C"     "A"
>> "snp004"        "G"     "A"
>> "snp005"        "C"     "T"
>>
>> Data frame 2
>> SNPID   OA      EA      id00001 id00002 id00003 id00004 id00005
>> snp001     A    C        1.01                      2                      0.97
>>            1.97                    1.99
>> snp002    A     G        1.02                     2                        1
>>           2                            2
>> snp003    C     A         1                      1.03                       2
>>            0                            1
>> snp004   A      G          1.02                    1.99                      2
>>               1.02                           1.98
>> snp005  C       T            1                        0                      1.01
>>                      1                           1
>>
>> I want to  if  OA s and EAs in data frame 2 is the same as OAs and EAs
>>   data frame 1 in such case I want to keep all information in data
>> fram2 as it is . However if   OA s and EAs in data frame 2 is
>> different from OAs and EAs  data frame 1, I want to change OA s and
>> EAs in data frame 2 to  OAs and EAs  data frame 1 and   I want to
>> redefine the values of the dosages (ids) of the variant (the dosage d
>> for the i-th individual and the j-th variant would become d_ij
>> new=2-dij.
>> Dosage [j,]=2-dosage[j,]
>> My desire data frame looks like
>> Dataframe new
>> SNPID"  "OA"       "EA"        id00001  id00002 id00003 id00004 id00005
>> "snp001"        "C"     "A"     2-1.01            2- 2    2-    0.97           2-1.97
>>      2-1.99
>> "snp002"        "G"     "A"     2- 1.02              2- 2                   2-1
>>    2-2                     2- 2
>> "snp003"        "C"     "A"       1                    1.03                         2
>>              0                          1
>> "snp004"        "G"     "A"     2-1.02                   2-1.99
>> 2-2                    2-1.02                    2-1.98
>> "snp005"        "C"     "T"          1                        0
>> 1.01                           1                                1
>>   can you help me the r code for the above.
>> Kind regards,
>> Hana
>>
>>
>> On 6/12/22, hanatezera <hanatezera at gmail.com> wrote:
>>> Dear Jim, Thanks a lot this is exactly i am looking for.Stay safe and
>>> blessed !Best,Hana
>>> -------- Original message --------From: Jim Lemon <drjimlemon at gmail.com>
>>> Date: 6/12/22  6:43 AM  (GMT+03:00) To: hanatezera <hanatezera at gmail.com>
>>> Cc: r-help mailing list <r-help at r-project.org> Subject: Re: [R] Changing
>>> sign of columns and values Hi Hana,I didn't look closely. The simplest rule
>>> that I can see is that theletters (nucleotides?) should be swapped if there
>>> has been a signchange in the beta value.
>>> Therefore:mydf<-read.table(text="IDnumber  OA  EA  beta1   C       A
>>> -0.052   G        A        0.0983   G        T
>>> -0.789",header=TRUE,stringsAsFactors=FALSE)# logical vector marking the rows
>>> to be swappedswap_letters<-mydf$beta < 0# save the OA values to be
>>> swappednewEA<-mydf$OA[swap_letters]# change the OAs to
>>> EAsmydf$OA[swap_letters]<-mydf$EA[swap_letters]# set the relevant EA values
>>> to the old OA valuesmydf$EA[swap_letters]<-newEA# change the beta
>>> valuesmydf$beta<-abs(mydf$beta)mydfJimOn Sun, Jun 12, 2022 at 9:11 AM
>>> hanatezera <hanatezera at gmail.com> wrote:>> Dear jim thanks for your help! I
>>> want to change also the value of OA and EF simultaneously.> For instance i
>>> am looking the data> mydf> IDnumber OA EA  beta> 1        1  A  C  0.050>
>>> 2        2  G  A 0.098> 3        3  T   G 0.789>> Best,> Hana>>>> --------
>>> Original message --------> From: Jim Lemon <drjimlemon at gmail.com>> Date:
>>> 6/12/22 1:59 AM (GMT+03:00)> To: hanatezera <hanatezera at gmail.com>, r-help
>>> mailing list <r-help at r-project.org>> Subject: Re: [R] Changing sign of
>>> columns and values>> Hi Hana,> I think this is what you want:>> # first read
>>> in your example> mydf<-read.table(text=> "IDnumber  OA  EA  beta> 1
>>> C       A       -0.05> 2   G        A        0.098> 3   G        T
>>> -0.789",> header=TRUE,stringsAsFactors=FALSE)> # check it> mydf> IDnumber OA
>>> EA   beta> 1        1  C  A -0.050> 2        2  G  A  0.098> 3        3  G
>>> T -0.789> # change values of mydf$beta to absolute values>
>>> mydf$beta<-abs(mydf$beta)> mydf> IDnumber OA EA  beta> 1        1  C  A
>>> 0.050> 2        2  G  A 0.098> 3        3  G  T 0.789>> Jim>> On Sun, Jun
>>> 12, 2022 at 8:27 AM hanatezera <hanatezera at gmail.com> wrote:> >> > I have
>>> the following data set in data frameIDnumber  OA  EA
>>> beta1                         C       A       -0.052
>>> G        A        0.0983                          G        T
>>> -0.789....I want to change the sign of negative beta. If the negative value
>>> change to postive i want to switch its EA and OA.My desire result will
>>> beIDnumber  OA  EA  beta1                         A      C
>>> 0.052                         G        A
>>> 0.0983                          T         G        0.789....Any one can help
>>> me with r codes? kind regards,Hana> >         [[alternative HTML version
>>> deleted]]> >> > ______________________________________________> >
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see> >
>>> https://stat.ethz.ch/mailman/listinfo/r-help> > PLEASE do read the posting
>>> guide http://www.R-project.org/posting-guide.html> > and provide commented,
>>> minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From h@n@tezer@ @end|ng |rom gm@||@com  Mon Jun 13 09:22:19 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Mon, 13 Jun 2022 10:22:19 +0300
Subject: [R] Changing sign of columns and values
In-Reply-To: <CA+8X3fWFE8zShP+depx-TeeFWZaYy=ySZpqDmcJkJwg95BTYcw@mail.gmail.com>
References: <CA+8X3fUB-vzBi0Jfr66tDUXQT4tctENg4=4sBvWxsiKBaEBUvg@mail.gmail.com>
 <62a5ca9c.1c69fb81.e10ce.4c1d@mx.google.com>
 <CAEuJ0VAqKdBpQJ=Bih=9V3=bDQBL05d=okpfRxSa0fqTK4EvBg@mail.gmail.com>
 <CA+8X3fWFE8zShP+depx-TeeFWZaYy=ySZpqDmcJkJwg95BTYcw@mail.gmail.com>
Message-ID: <CAEuJ0VA6B+LJJRbZGp6D+HP33nmM9L+wdnPakVBg=-CxGkfEig@mail.gmail.com>

Dear Jim,  Good morning, hope you are doing very well, Here I want to
calculate the specific parameters based on the previous data. I have
attached below.
can you help me  with r functions or code to solve the above equations
Kind regards,
Hana


On 6/13/22, Jim Lemon <drjimlemon at gmail.com> wrote:
> Hi Hana,
> This is a bit more difficult, but the same basic steps apply. See the
> comments for an explanation.
> When you submit a problem like this, it is a lot easier if you send
> the output from "dput" (e.g. dput(df1)) or set your data frames up
> with "read.table" like I have done in the example below. It make it
> lots easier for anyone who wants to reply to see what your dataset
> looks like and input that dataset to devise a solution.
> I have made some assumptions about marking the rows to be altered and
> the arithmetic to do on those rows. What follows may not be as general
> a solution as you want:
>
> df1<-read.table(text=
>  "SNPID OA    EA
>  snp001        C     A
>  snp002        G     A
>  snp003        C     A
>  snp004        G     A
>  snp005        C     T",
>  header=TRUE,stringsAsFactors=FALSE)
>
> df2<-read.table(text=
>  "SNPID   OA    EA   id00001 id00002 id00003 id00004 id00005
>  snp001    A    C    1.01    2       0.97    1.97    1.99
>  snp002    A    G    1.02    2       1       2       2
>  snp003    C    A    1       1.03    2       0       1
>  snp004    A    G    1.02    1.99    2       1.02    1.98
>  snp005    C    T    1       0       1.01    1       1",
>  header=TRUE,stringsAsFactors=FALSE)
>
> # get a logical vector of the different XA values
> reversals<-df1$EA != df2$EA | df1$OA != df2$OA
> reversals
>
> # set a variable to the maximum value of idxxxxx
> # just to make the code easier to understand
> maxid<-2
>
> # create a copy of df2
> dfnew<-df2
>
> # now swap the XA columns and reflect the
> # idxxxxxx values of the rows in which
> # EA and OA are different between df1 and df2
> # here I have looped through the rows
> nrows<-dim(dfnew)[1]
> idcols<-4:8
> XAcols<-2:3
> for(i in 1:nrows) {
>  if(reversals[i]) {
>   dfnew[i,XAcols]<-rev(dfnew[i,XAcols])
>   dfnew[i,idcols]<-maxid-dfnew[i,idcols]
>  }
> }
>
> dfnew
>
> Jim
>
> On Mon, Jun 13, 2022 at 12:09 AM anteneh asmare <hanatezera at gmail.com>
> wrote:
>>
>> Dear Jim, Morning I have the same issue regarding comparing and
>> swapping the value of columns fro two different data frames.
>> I have the following two data frames
>> Data frame 1
>> "SNPID" "OA"    "EA"
>> "snp001"        "C"     "A"
>> "snp002"        "G"     "A"
>> "snp003"        "C"     "A"
>> "snp004"        "G"     "A"
>> "snp005"        "C"     "T"
>>
>> Data frame 2
>> SNPID   OA      EA      id00001 id00002 id00003 id00004 id00005
>> snp001     A    C        1.01                      2
>> 0.97
>>           1.97                    1.99
>> snp002    A     G        1.02                     2
>> 1
>>          2                            2
>> snp003    C     A         1                      1.03
>>  2
>>           0                            1
>> snp004   A      G          1.02                    1.99
>>   2
>>              1.02                           1.98
>> snp005  C       T            1                        0
>>   1.01
>>                     1                           1
>>
>> I want to  if  OA s and EAs in data frame 2 is the same as OAs and EAs
>>  data frame 1 in such case I want to keep all information in data
>> fram2 as it is . However if   OA s and EAs in data frame 2 is
>> different from OAs and EAs  data frame 1, I want to change OA s and
>> EAs in data frame 2 to  OAs and EAs  data frame 1 and   I want to
>> redefine the values of the dosages (ids) of the variant (the dosage d
>> for the i-th individual and the j-th variant would become d_ij
>> new=2-dij.
>> Dosage [j,]=2-dosage[j,]
>> My desire data frame looks like
>> Dataframe new
>> SNPID"  "OA"       "EA"        id00001  id00002 id00003 id00004 id00005
>> "snp001"        "C"     "A"     2-1.01            2- 2    2-    0.97
>>     2-1.97
>>     2-1.99
>> "snp002"        "G"     "A"     2- 1.02              2- 2
>>  2-1
>>   2-2                     2- 2
>> "snp003"        "C"     "A"       1                    1.03
>>          2
>>             0                          1
>> "snp004"        "G"     "A"     2-1.02                   2-1.99
>> 2-2                    2-1.02                    2-1.98
>> "snp005"        "C"     "T"          1                        0
>> 1.01                           1                                1
>>  can you help me the r code for the above.
>> Kind regards,
>> Hana
>>
>>
>> On 6/12/22, hanatezera <hanatezera at gmail.com> wrote:
>> > Dear Jim, Thanks a lot this is exactly i am looking for.Stay safe and
>> > blessed !Best,Hana
>> > -------- Original message --------From: Jim Lemon
>> > <drjimlemon at gmail.com>
>> > Date: 6/12/22  6:43 AM  (GMT+03:00) To: hanatezera
>> > <hanatezera at gmail.com>
>> > Cc: r-help mailing list <r-help at r-project.org> Subject: Re: [R]
>> > Changing
>> > sign of columns and values Hi Hana,I didn't look closely. The simplest
>> > rule
>> > that I can see is that theletters (nucleotides?) should be swapped if
>> > there
>> > has been a signchange in the beta value.
>> > Therefore:mydf<-read.table(text="IDnumber  OA  EA  beta1   C       A
>> > -0.052   G        A        0.0983   G        T
>> > -0.789",header=TRUE,stringsAsFactors=FALSE)# logical vector marking the
>> > rows
>> > to be swappedswap_letters<-mydf$beta < 0# save the OA values to be
>> > swappednewEA<-mydf$OA[swap_letters]# change the OAs to
>> > EAsmydf$OA[swap_letters]<-mydf$EA[swap_letters]# set the relevant EA
>> > values
>> > to the old OA valuesmydf$EA[swap_letters]<-newEA# change the beta
>> > valuesmydf$beta<-abs(mydf$beta)mydfJimOn Sun, Jun 12, 2022 at 9:11 AM
>> > hanatezera <hanatezera at gmail.com> wrote:>> Dear jim thanks for your
>> > help! I
>> > want to change also the value of OA and EF simultaneously.> For instance
>> > i
>> > am looking the data> mydf> IDnumber OA EA  beta> 1        1  A  C
>> > 0.050>
>> > 2        2  G  A 0.098> 3        3  T   G 0.789>> Best,> Hana>>>>
>> > --------
>> > Original message --------> From: Jim Lemon <drjimlemon at gmail.com>>
>> > Date:
>> > 6/12/22 1:59 AM (GMT+03:00)> To: hanatezera <hanatezera at gmail.com>,
>> > r-help
>> > mailing list <r-help at r-project.org>> Subject: Re: [R] Changing sign of
>> > columns and values>> Hi Hana,> I think this is what you want:>> # first
>> > read
>> > in your example> mydf<-read.table(text=> "IDnumber  OA  EA  beta> 1
>> > C       A       -0.05> 2   G        A        0.098> 3   G        T
>> > -0.789",> header=TRUE,stringsAsFactors=FALSE)> # check it> mydf>
>> > IDnumber OA
>> > EA   beta> 1        1  C  A -0.050> 2        2  G  A  0.098> 3        3
>> > G
>> > T -0.789> # change values of mydf$beta to absolute values>
>> > mydf$beta<-abs(mydf$beta)> mydf> IDnumber OA EA  beta> 1        1  C  A
>> > 0.050> 2        2  G  A 0.098> 3        3  G  T 0.789>> Jim>> On Sun,
>> > Jun
>> > 12, 2022 at 8:27 AM hanatezera <hanatezera at gmail.com> wrote:> >> > I
>> > have
>> > the following data set in data frameIDnumber  OA  EA
>> > beta1                         C       A       -0.052
>> > G        A        0.0983                          G        T
>> > -0.789....I want to change the sign of negative beta. If the negative
>> > value
>> > change to postive i want to switch its EA and OA.My desire result will
>> > beIDnumber  OA  EA  beta1                         A      C
>> > 0.052                         G        A
>> > 0.0983                          T         G        0.789....Any one can
>> > help
>> > me with r codes? kind regards,Hana> >         [[alternative HTML
>> > version
>> > deleted]]> >> > ______________________________________________> >
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see> >
>> > https://stat.ethz.ch/mailman/listinfo/r-help> > PLEASE do read the
>> > posting
>> > guide http://www.R-project.org/posting-guide.html> > and provide
>> > commented,
>> > minimal, self-contained, reproducible code.
>

From bgreen @end|ng |rom dy@on@br|@net@org@@u  Mon Jun 13 12:21:38 2022
From: bgreen @end|ng |rom dy@on@br|@net@org@@u (Bob Green)
Date: Mon, 13 Jun 2022 20:21:38 +1000
Subject: [R] Error creating a map with packages sf and tidyverse.
In-Reply-To: <mailman.366961.1.1655114402.34330.r-help@r-project.org>
References: <mailman.366961.1.1655114402.34330.r-help@r-project.org>
Message-ID: <mailman.366962.1.1655116326.1223.r-help@r-project.org>

I was hoping for advice about this code that I have run several 
times. When I recently ran the code, I received the error 
below:"Error in FUN(X[[i]], ...) : object 'fac' not found"

In R Studio the code runs, but instead of non-red countries being 
white, they are darker grey.

Is 'fill=fac', no longer valid, or is there some other issue?

Any advice is appreciated,

Regards

Bob

 > library(tidyverse)
-- Attaching packages 
----------------------------------------------------------------------------- 
tidyverse 1.3.1 --
v ggplot2 3.3.5     v purrr   0.3.4
v tibble  3.1.3     v dplyr   1.0.7
v tidyr   1.1.4     v stringr 1.4.0
v readr   2.1.1     v forcats 0.5.1
-- Conflicts 
-------------------------------------------------------------------------------- 
tidyverse_conflicts() --
x dplyr::filter() masks stats::filter()
x dplyr::lag()    masks stats::lag()
Warning messages:
1: In (function (kind = NULL, normal.kind = NULL, sample.kind = NULL)  :
   non-uniform 'Rounding' sampler used
2: In (function (kind = NULL, normal.kind = NULL, sample.kind = NULL)  :
   non-uniform 'Rounding' sampler used
 > library(sf)
Linking to GEOS 3.9.1, GDAL 3.2.1, PROJ 7.2.1; sf_use_s2() is TRUE
Warning message:
package 'sf' was built under R version 4.1.3
 >
 >
 >
 > world_sf = map_data("world") %>%
+    filter(region != "Antarctica") %>%
+    mutate(region=replace(region, subregion=="Alaska", "USA_Alaska")) %>%
+    st_as_sf(coords=c("long","lat"), crs=4326) %>%
+    group_by(region, group) %>% summarise(geometry=st_combine(geometry)) %>%
+    st_cast("POLYGON") %>% summarise(geometry=st_combine(geometry))
`summarise()` has grouped output by 'region'. You can override using 
the `.groups` argument.
 >
 > labs = tibble(region = c("Argentina", "Australia", 
"Barbados","Brazil", "England","Fiji", "France", "French Polynesia", 
"Ghana", "Italy","Japan", "New Zealand", "Papua New Guinea", "Peru", 
"Portugal", "Samoa", "Sao Tome and Principe", "Senegal", "Solomon 
Islands", "South Africa", "Spain", "Trinidad", "Tobago", "Turkey", 
"UK", "Uruguay", "USA"))
 > ggplot()+geom_sf(aes(fill=fac))+coord_sf(expand=FALSE)
Error in FUN(X[[i]], ...) : object 'fac' not found
 >
 > labels_sf =  st_point_on_surface(world_sf) %>% right_join(labs, by="region")
Warning messages:
1: In st_point_on_surface.sf(world_sf) :
   st_point_on_surface assumes attributes are constant over geometries of x
2: In st_point_on_surface.sfc(st_geometry(x)) :
   st_point_on_surface may not give correct results for longitude/latitude data
 >
 >
 > world_sf %>% mutate(fac = region %in% labs$region)  %>%
+    ggplot()+geom_sf(aes(fill=fac))+coord_sf( expand=FALSE)+
+    theme(axis.title = element_blank(), legend.position = "none")+
+    scale_fill_manual(values=c("FALSE"=NA, "TRUE"="red"))
 >


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Mon Jun 13 13:24:21 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Mon, 13 Jun 2022 16:24:21 +0500
Subject: [R] Kindly please help
Message-ID: <CAMfKi3JL2W8VVUTwvDJb8obWPXXMZi-OM6HZS8b2M1HJ_r1rTA@mail.gmail.com>

Hi, Dear Respected Professor! I hope that you are doing well. Kindly help
me with the following:

I have the R-codes for the "Quantile Augmented Mean Group" method. The
relevant codes and data are attached herewith.
Note: The link to the reference paper is:
https://www.econ.cam.ac.uk/people-files/emeritus/mhp1/fp20/qmg40-rev21.pdf
My Issue:
When I run the given codes to estimate the results for my own data. I have
to face several errors. I humbly request the experts to help me to estimate
the results for my data.
Thank you so much for your precious time.

Regards

Muhammad Zubair Chishti
Ph.D. Student
School of Business,
Zhengzhou University, Henan, China.
My Google scholar link:
https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
My ResearchGate link: https://www.researchgate.net/profile/Muhammad-Chishti

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Table.3.1.and.Table.3.2.design1.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220613/49e4b4c7/attachment.txt>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: rq.fit.panel.all.revised.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220613/49e4b4c7/attachment-0001.txt>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: data2.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220613/49e4b4c7/attachment-0002.txt>

From petr@p|k@| @end|ng |rom prechez@@cz  Mon Jun 13 14:12:26 2022
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Mon, 13 Jun 2022 12:12:26 +0000
Subject: [R] A humble request regarding QAMG method in R
In-Reply-To: <CAMfKi3Kd9oP2nWe6boV4rNjVHAZ4eV=vwpnCAzh=6Gw-ZWrdbg@mail.gmail.com>
References: <CAMfKi3Kd9oP2nWe6boV4rNjVHAZ4eV=vwpnCAzh=6Gw-ZWrdbg@mail.gmail.com>
Message-ID: <ff88294596d84a978a510888ec9401b6@SRVEXCHCM1302.precheza.cz>

Hallo

I tried to use your code but after about 20 minutes it still worked so I gave 
up. After I kill it, there were some warnings

9: In rq.fit.sfn(D, y, rhs = a) :
  tiny diagonals replaced with Inf when calling blkfct

but I cannot decipher what it is about.

Your code is quite complicated and you hardly get any answer if you do not 
specify better where are the errors and what they tell, especially if others 
observe the same (long) calculation.

Use traceback() and/or debug() functions to see what is going on.
You also maybe could try to test your functions on some smaller data example.

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Muhammad
> Zubair Chishti
> Sent: Saturday, June 11, 2022 2:43 PM
> To: r-help at r-project.org; David Winsemius <dwinsemius at comcast.net>;
> lists at dewey.myzen.co.uk
> Subject: [R] A humble request regarding QAMG method in R
>
> Hi, Dear Respected Professors! I hope that you are doing well. Now all files
> are in .txt files. Now kindly help me with the following:
>
> I have the R-codes for the "Quantile Augmented Mean Group" method. The
> relevant codes and data are attached herewith.
> Note: The link to the reference paper is:
> https://www.econ.cam.ac.uk/people-files/emeritus/mhp1/fp20/qmg40-
> rev21.pdf
> My Issue:
> When I run the given codes to estimate the results for my own data. I have 
> to
> face several errors. I humbly request the experts to help me to estimate the
> results for my data.
> Thank you so much for your precious time.
>
> Regards
>
> Muhammad Zubair Chishti
> Ph.D. Student
> School of Business,
> Zhengzhou University, Henan, China.
> My Google scholar link:
> https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
> My ResearchGate link: https://www.researchgate.net/profile/Muhammad-
> Chishti

From h@n@tezer@ @end|ng |rom gm@||@com  Mon Jun 13 16:38:58 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Mon, 13 Jun 2022 17:38:58 +0300
Subject: [R] Changing sign of columns and values
In-Reply-To: <CAEuJ0VA6B+LJJRbZGp6D+HP33nmM9L+wdnPakVBg=-CxGkfEig@mail.gmail.com>
References: <CA+8X3fUB-vzBi0Jfr66tDUXQT4tctENg4=4sBvWxsiKBaEBUvg@mail.gmail.com>
 <62a5ca9c.1c69fb81.e10ce.4c1d@mx.google.com>
 <CAEuJ0VAqKdBpQJ=Bih=9V3=bDQBL05d=okpfRxSa0fqTK4EvBg@mail.gmail.com>
 <CA+8X3fWFE8zShP+depx-TeeFWZaYy=ySZpqDmcJkJwg95BTYcw@mail.gmail.com>
 <CAEuJ0VA6B+LJJRbZGp6D+HP33nmM9L+wdnPakVBg=-CxGkfEig@mail.gmail.com>
Message-ID: <CAEuJ0VB3zNoXae8Y4Dyi9o5Ou+crGn0XYg3ExD8ako144LDtQA@mail.gmail.com>

Dear, Jim I try the following codes but I got an error
Dfm1<-data.table(Dfm1)
# the names of the columns to be multiplied by BETA
ID_names <- paste0("ID",4:8)
# the names of the new columns that have been multiplied by beta
new_ID_names <- paste0("new_",ID_names)
##### Now with a simple command we get the desired multiplication
datfm[, (new_ID_names) := .SD *BETA, .SDcols = ID_names]
datfm
str(datfm)
##### The PRS for each individual can be calculated using the colSums
function in base R:
PRS<-Dfm1[, colSums(.SD), .SDcols = new_ID_names]
PRS

On 6/13/22, anteneh asmare <hanatezera at gmail.com> wrote:
> Dear Jim,  Good morning, hope you are doing very well, Here I want to
> calculate the specific parameters based on the previous data. I have
> attached below.
> can you help me  with r functions or code to solve the above equations
> Kind regards,
> Hana
>
>
> On 6/13/22, Jim Lemon <drjimlemon at gmail.com> wrote:
>> Hi Hana,
>> This is a bit more difficult, but the same basic steps apply. See the
>> comments for an explanation.
>> When you submit a problem like this, it is a lot easier if you send
>> the output from "dput" (e.g. dput(df1)) or set your data frames up
>> with "read.table" like I have done in the example below. It make it
>> lots easier for anyone who wants to reply to see what your dataset
>> looks like and input that dataset to devise a solution.
>> I have made some assumptions about marking the rows to be altered and
>> the arithmetic to do on those rows. What follows may not be as general
>> a solution as you want:
>>
>> df1<-read.table(text=
>>  "SNPID OA    EA
>>  snp001        C     A
>>  snp002        G     A
>>  snp003        C     A
>>  snp004        G     A
>>  snp005        C     T",
>>  header=TRUE,stringsAsFactors=FALSE)
>>
>> df2<-read.table(text=
>>  "SNPID   OA    EA   id00001 id00002 id00003 id00004 id00005
>>  snp001    A    C    1.01    2       0.97    1.97    1.99
>>  snp002    A    G    1.02    2       1       2       2
>>  snp003    C    A    1       1.03    2       0       1
>>  snp004    A    G    1.02    1.99    2       1.02    1.98
>>  snp005    C    T    1       0       1.01    1       1",
>>  header=TRUE,stringsAsFactors=FALSE)
>>
>> # get a logical vector of the different XA values
>> reversals<-df1$EA != df2$EA | df1$OA != df2$OA
>> reversals
>>
>> # set a variable to the maximum value of idxxxxx
>> # just to make the code easier to understand
>> maxid<-2
>>
>> # create a copy of df2
>> dfnew<-df2
>>
>> # now swap the XA columns and reflect the
>> # idxxxxxx values of the rows in which
>> # EA and OA are different between df1 and df2
>> # here I have looped through the rows
>> nrows<-dim(dfnew)[1]
>> idcols<-4:8
>> XAcols<-2:3
>> for(i in 1:nrows) {
>>  if(reversals[i]) {
>>   dfnew[i,XAcols]<-rev(dfnew[i,XAcols])
>>   dfnew[i,idcols]<-maxid-dfnew[i,idcols]
>>  }
>> }
>>
>> dfnew
>>
>> Jim
>>
>> On Mon, Jun 13, 2022 at 12:09 AM anteneh asmare <hanatezera at gmail.com>
>> wrote:
>>>
>>> Dear Jim, Morning I have the same issue regarding comparing and
>>> swapping the value of columns fro two different data frames.
>>> I have the following two data frames
>>> Data frame 1
>>> "SNPID" "OA"    "EA"
>>> "snp001"        "C"     "A"
>>> "snp002"        "G"     "A"
>>> "snp003"        "C"     "A"
>>> "snp004"        "G"     "A"
>>> "snp005"        "C"     "T"
>>>
>>> Data frame 2
>>> SNPID   OA      EA      id00001 id00002 id00003 id00004 id00005
>>> snp001     A    C        1.01                      2
>>> 0.97
>>>           1.97                    1.99
>>> snp002    A     G        1.02                     2
>>> 1
>>>          2                            2
>>> snp003    C     A         1                      1.03
>>>  2
>>>           0                            1
>>> snp004   A      G          1.02                    1.99
>>>   2
>>>              1.02                           1.98
>>> snp005  C       T            1                        0
>>>   1.01
>>>                     1                           1
>>>
>>> I want to  if  OA s and EAs in data frame 2 is the same as OAs and EAs
>>>  data frame 1 in such case I want to keep all information in data
>>> fram2 as it is . However if   OA s and EAs in data frame 2 is
>>> different from OAs and EAs  data frame 1, I want to change OA s and
>>> EAs in data frame 2 to  OAs and EAs  data frame 1 and   I want to
>>> redefine the values of the dosages (ids) of the variant (the dosage d
>>> for the i-th individual and the j-th variant would become d_ij
>>> new=2-dij.
>>> Dosage [j,]=2-dosage[j,]
>>> My desire data frame looks like
>>> Dataframe new
>>> SNPID"  "OA"       "EA"        id00001  id00002 id00003 id00004 id00005
>>> "snp001"        "C"     "A"     2-1.01            2- 2    2-    0.97
>>>     2-1.97
>>>     2-1.99
>>> "snp002"        "G"     "A"     2- 1.02              2- 2
>>>  2-1
>>>   2-2                     2- 2
>>> "snp003"        "C"     "A"       1                    1.03
>>>          2
>>>             0                          1
>>> "snp004"        "G"     "A"     2-1.02                   2-1.99
>>> 2-2                    2-1.02                    2-1.98
>>> "snp005"        "C"     "T"          1                        0
>>> 1.01                           1                                1
>>>  can you help me the r code for the above.
>>> Kind regards,
>>> Hana
>>>
>>>
>>> On 6/12/22, hanatezera <hanatezera at gmail.com> wrote:
>>> > Dear Jim, Thanks a lot this is exactly i am looking for.Stay safe and
>>> > blessed !Best,Hana
>>> > -------- Original message --------From: Jim Lemon
>>> > <drjimlemon at gmail.com>
>>> > Date: 6/12/22  6:43 AM  (GMT+03:00) To: hanatezera
>>> > <hanatezera at gmail.com>
>>> > Cc: r-help mailing list <r-help at r-project.org> Subject: Re: [R]
>>> > Changing
>>> > sign of columns and values Hi Hana,I didn't look closely. The simplest
>>> > rule
>>> > that I can see is that theletters (nucleotides?) should be swapped if
>>> > there
>>> > has been a signchange in the beta value.
>>> > Therefore:mydf<-read.table(text="IDnumber  OA  EA  beta1   C       A
>>> > -0.052   G        A        0.0983   G        T
>>> > -0.789",header=TRUE,stringsAsFactors=FALSE)# logical vector marking
>>> > the
>>> > rows
>>> > to be swappedswap_letters<-mydf$beta < 0# save the OA values to be
>>> > swappednewEA<-mydf$OA[swap_letters]# change the OAs to
>>> > EAsmydf$OA[swap_letters]<-mydf$EA[swap_letters]# set the relevant EA
>>> > values
>>> > to the old OA valuesmydf$EA[swap_letters]<-newEA# change the beta
>>> > valuesmydf$beta<-abs(mydf$beta)mydfJimOn Sun, Jun 12, 2022 at 9:11 AM
>>> > hanatezera <hanatezera at gmail.com> wrote:>> Dear jim thanks for your
>>> > help! I
>>> > want to change also the value of OA and EF simultaneously.> For
>>> > instance
>>> > i
>>> > am looking the data> mydf> IDnumber OA EA  beta> 1        1  A  C
>>> > 0.050>
>>> > 2        2  G  A 0.098> 3        3  T   G 0.789>> Best,> Hana>>>>
>>> > --------
>>> > Original message --------> From: Jim Lemon <drjimlemon at gmail.com>>
>>> > Date:
>>> > 6/12/22 1:59 AM (GMT+03:00)> To: hanatezera <hanatezera at gmail.com>,
>>> > r-help
>>> > mailing list <r-help at r-project.org>> Subject: Re: [R] Changing sign of
>>> > columns and values>> Hi Hana,> I think this is what you want:>> #
>>> > first
>>> > read
>>> > in your example> mydf<-read.table(text=> "IDnumber  OA  EA  beta> 1
>>> > C       A       -0.05> 2   G        A        0.098> 3   G        T
>>> > -0.789",> header=TRUE,stringsAsFactors=FALSE)> # check it> mydf>
>>> > IDnumber OA
>>> > EA   beta> 1        1  C  A -0.050> 2        2  G  A  0.098> 3
>>> > 3
>>> > G
>>> > T -0.789> # change values of mydf$beta to absolute values>
>>> > mydf$beta<-abs(mydf$beta)> mydf> IDnumber OA EA  beta> 1        1  C
>>> > A
>>> > 0.050> 2        2  G  A 0.098> 3        3  G  T 0.789>> Jim>> On Sun,
>>> > Jun
>>> > 12, 2022 at 8:27 AM hanatezera <hanatezera at gmail.com> wrote:> >> > I
>>> > have
>>> > the following data set in data frameIDnumber  OA  EA
>>> > beta1                         C       A       -0.052
>>> > G        A        0.0983                          G        T
>>> > -0.789....I want to change the sign of negative beta. If the negative
>>> > value
>>> > change to postive i want to switch its EA and OA.My desire result will
>>> > beIDnumber  OA  EA  beta1                         A      C
>>> > 0.052                         G        A
>>> > 0.0983                          T         G        0.789....Any one
>>> > can
>>> > help
>>> > me with r codes? kind regards,Hana> >         [[alternative HTML
>>> > version
>>> > deleted]]> >> > ______________________________________________> >
>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see> >
>>> > https://stat.ethz.ch/mailman/listinfo/r-help> > PLEASE do read the
>>> > posting
>>> > guide http://www.R-project.org/posting-guide.html> > and provide
>>> > commented,
>>> > minimal, self-contained, reproducible code.
>>
>


From h@n@tezer@ @end|ng |rom gm@||@com  Mon Jun 13 21:48:02 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Mon, 13 Jun 2022 22:48:02 +0300
Subject: [R] Combining Differnt columns in one single column
Message-ID: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>

Dear All, I have the following column name in different data frame and
different size
ID_names1 <- paste0("id0000",1:9)
ID_names2 <- paste0("id000",10:99)
ID_names3 <- paste0("id00",100:999)
ID_names4 <- paste0("id0",1000:9999)
ID_names5 <- paste0("id",10000:50000)
Dose it possible to combine in to a single column in r?
Best,
Hana


From tebert @end|ng |rom u||@edu  Mon Jun 13 22:21:26 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Mon, 13 Jun 2022 20:21:26 +0000
Subject: [R] Combining Differnt columns in one single column
In-Reply-To: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>
References: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>
Message-ID: <BN6PR2201MB1553CED7003E70B0A65F2E42CFAB9@BN6PR2201MB1553.namprd22.prod.outlook.com>

ID5 <- as.data.frame(ID_names1)
colnames(ID5)<-"val" #rename the columns so they have the same name.
ID6 <- as.data.frame(ID_names2)
colnames(ID6)<-"val"
rbind(ID5, ID6)

This works for the first two, just keep doing the same thing for the others.
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of anteneh asmare
Sent: Monday, June 13, 2022 3:48 PM
To: r-help at r-project.org
Subject: [R] Combining Differnt columns in one single column

[External Email]

Dear All, I have the following column name in different data frame and different size
ID_names1 <- paste0("id0000",1:9)
ID_names2 <- paste0("id000",10:99)
ID_names3 <- paste0("id00",100:999)
ID_names4 <- paste0("id0",1000:9999)
ID_names5 <- paste0("id",10000:50000)
Dose it possible to combine in to a single column in r?
Best,
Hana

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=cjCY0UmRewiKkMVVxNu2riO8k4tZHD7s7R3R6SC-bwEM5u77LyExCSIzwI-q7Xu_&s=xrZArAFWnJf7ja13EEgxwBj-pMBEXBYSQgc6BhtNPAA&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=cjCY0UmRewiKkMVVxNu2riO8k4tZHD7s7R3R6SC-bwEM5u77LyExCSIzwI-q7Xu_&s=oUlYLeH_EF5rwea10resAheSjjvJ6nhUl8wT84OVXZM&e=
and provide commented, minimal, self-contained, reproducible code.


From h@n@tezer@ @end|ng |rom gm@||@com  Mon Jun 13 22:48:32 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Mon, 13 Jun 2022 23:48:32 +0300
Subject: [R] Combining Differnt columns in one single column
In-Reply-To: <BN6PR2201MB1553CED7003E70B0A65F2E42CFAB9@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>
 <BN6PR2201MB1553CED7003E70B0A65F2E42CFAB9@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <CAEuJ0VB2qa+gM3w=RrW9SrS7eN+Njf6GB-Yz6s++zwUo7Obu7w@mail.gmail.com>

Dear Tim it works, But i want to treat them as a column name. Is it
possible to treat as a column name ?

Best,
Hana
On 6/13/22, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> ID5 <- as.data.frame(ID_names1)
> colnames(ID5)<-"val" #rename the columns so they have the same name.
> ID6 <- as.data.frame(ID_names2)
> colnames(ID6)<-"val"
> rbind(ID5, ID6)
>
> This works for the first two, just keep doing the same thing for the
> others.
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of anteneh asmare
> Sent: Monday, June 13, 2022 3:48 PM
> To: r-help at r-project.org
> Subject: [R] Combining Differnt columns in one single column
>
> [External Email]
>
> Dear All, I have the following column name in different data frame and
> different size
> ID_names1 <- paste0("id0000",1:9)
> ID_names2 <- paste0("id000",10:99)
> ID_names3 <- paste0("id00",100:999)
> ID_names4 <- paste0("id0",1000:9999)
> ID_names5 <- paste0("id",10000:50000)
> Dose it possible to combine in to a single column in r?
> Best,
> Hana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=cjCY0UmRewiKkMVVxNu2riO8k4tZHD7s7R3R6SC-bwEM5u77LyExCSIzwI-q7Xu_&s=xrZArAFWnJf7ja13EEgxwBj-pMBEXBYSQgc6BhtNPAA&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=cjCY0UmRewiKkMVVxNu2riO8k4tZHD7s7R3R6SC-bwEM5u77LyExCSIzwI-q7Xu_&s=oUlYLeH_EF5rwea10resAheSjjvJ6nhUl8wT84OVXZM&e=
> and provide commented, minimal, self-contained, reproducible code.
>


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon Jun 13 23:12:53 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 13 Jun 2022 22:12:53 +0100
Subject: [R] Combining Differnt columns in one single column
In-Reply-To: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>
References: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>
Message-ID: <c9f6cef7-b932-b5bf-6396-6a73f9dbd7fe@sapo.pt>

Hello,

Are you looking for this?


ID_names <- sprintf("id%05d", 1:50000)
head(ID_names)
#> [1] "id00001" "id00002" "id00003" "id00004" "id00005" "id00006"
tail(ID_names)
#> [1] "id49995" "id49996" "id49997" "id49998" "id49999" "id50000"


sprintf with the format %05d pads the integers with zeros to length 5.
And you won't need to combine the 5 vectors, it already is only one.

Hope this helps,

Rui Barradas

?s 20:48 de 13/06/2022, anteneh asmare escreveu:
> Dear All, I have the following column name in different data frame and
> different size
> ID_names1 <- paste0("id0000",1:9)
> ID_names2 <- paste0("id000",10:99)
> ID_names3 <- paste0("id00",100:999)
> ID_names4 <- paste0("id0",1000:9999)
> ID_names5 <- paste0("id",10000:50000)
> Dose it possible to combine in to a single column in r?
> Best,
> Hana
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From h@n@tezer@ @end|ng |rom gm@||@com  Mon Jun 13 23:38:08 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Tue, 14 Jun 2022 00:38:08 +0300
Subject: [R] Combining Differnt columns in one single column
In-Reply-To: <c9f6cef7-b932-b5bf-6396-6a73f9dbd7fe@sapo.pt>
References: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>
 <c9f6cef7-b932-b5bf-6396-6a73f9dbd7fe@sapo.pt>
Message-ID: <CAEuJ0VBft360793LRG_G6hw_U6qTOP9M_CF4c=rADNZcwsEDYA@mail.gmail.com>

Dear Rui,  Thanks a lot! This exactly what I was looking for it.
Kind regads,
Hana

On 6/14/22, Rui Barradas <ruipbarradas at sapo.pt> wrote:
> Hello,
>
> Are you looking for this?
>
>
> ID_names <- sprintf("id%05d", 1:50000)
> head(ID_names)
> #> [1] "id00001" "id00002" "id00003" "id00004" "id00005" "id00006"
> tail(ID_names)
> #> [1] "id49995" "id49996" "id49997" "id49998" "id49999" "id50000"
>
>
> sprintf with the format %05d pads the integers with zeros to length 5.
> And you won't need to combine the 5 vectors, it already is only one.
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 20:48 de 13/06/2022, anteneh asmare escreveu:
>> Dear All, I have the following column name in different data frame and
>> different size
>> ID_names1 <- paste0("id0000",1:9)
>> ID_names2 <- paste0("id000",10:99)
>> ID_names3 <- paste0("id00",100:999)
>> ID_names4 <- paste0("id0",1000:9999)
>> ID_names5 <- paste0("id",10000:50000)
>> Dose it possible to combine in to a single column in r?
>> Best,
>> Hana
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From drj|m|emon @end|ng |rom gm@||@com  Tue Jun 14 00:26:11 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Tue, 14 Jun 2022 08:26:11 +1000
Subject: [R] Changing sign of columns and values
In-Reply-To: <CAEuJ0VB3zNoXae8Y4Dyi9o5Ou+crGn0XYg3ExD8ako144LDtQA@mail.gmail.com>
References: <CA+8X3fUB-vzBi0Jfr66tDUXQT4tctENg4=4sBvWxsiKBaEBUvg@mail.gmail.com>
 <62a5ca9c.1c69fb81.e10ce.4c1d@mx.google.com>
 <CAEuJ0VAqKdBpQJ=Bih=9V3=bDQBL05d=okpfRxSa0fqTK4EvBg@mail.gmail.com>
 <CA+8X3fWFE8zShP+depx-TeeFWZaYy=ySZpqDmcJkJwg95BTYcw@mail.gmail.com>
 <CAEuJ0VA6B+LJJRbZGp6D+HP33nmM9L+wdnPakVBg=-CxGkfEig@mail.gmail.com>
 <CAEuJ0VB3zNoXae8Y4Dyi9o5Ou+crGn0XYg3ExD8ako144LDtQA@mail.gmail.com>
Message-ID: <CA+8X3fWzwF-U0xYX_wP4_AzhbOc6RCaX9Rw3BabL6z7BXz5z4g@mail.gmail.com>

Hi Hana,
The reason that I didn't answer your last request was that I couldn't
understand it. You attached a spreadsheet that had been pasted
together from other documents. Point 1 promised a formula for
calculating PRS, but didn't show it. Point 2 was similarly lacking a
symbolic formula that could be translated into R code. It was clear to
me that the original documents that had been cut and pasted into the
spreadsheet looked very much like homework assignments. As others have
told you, we avoid doing homework for posters.

Jim

On Tue, Jun 14, 2022 at 12:38 AM anteneh asmare <hanatezera at gmail.com> wrote:
>
> Dear, Jim I try the following codes but I got an error
> Dfm1<-data.table(Dfm1)
> # the names of the columns to be multiplied by BETA
> ID_names <- paste0("ID",4:8)
> # the names of the new columns that have been multiplied by beta
> new_ID_names <- paste0("new_",ID_names)
> ##### Now with a simple command we get the desired multiplication
> datfm[, (new_ID_names) := .SD *BETA, .SDcols = ID_names]
> datfm
> str(datfm)
> ##### The PRS for each individual can be calculated using the colSums
> function in base R:
> PRS<-Dfm1[, colSums(.SD), .SDcols = new_ID_names]
> PRS
>


From tebert @end|ng |rom u||@edu  Tue Jun 14 02:31:54 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 14 Jun 2022 00:31:54 +0000
Subject: [R] Combining Differnt columns in one single column
In-Reply-To: <CAEuJ0VB2qa+gM3w=RrW9SrS7eN+Njf6GB-Yz6s++zwUo7Obu7w@mail.gmail.com>
References: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>
 <BN6PR2201MB1553CED7003E70B0A65F2E42CFAB9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAEuJ0VB2qa+gM3w=RrW9SrS7eN+Njf6GB-Yz6s++zwUo7Obu7w@mail.gmail.com>
Message-ID: <BN6PR2201MB1553A5FB7E83717DE7AACE2ECFAA9@BN6PR2201MB1553.namprd22.prod.outlook.com>

Yes, It just takes another step. Here is the new pattern:
ID_names1 <- paste0("id0000",1:9)
ID_names2 <- paste0("id000",10:99)
ID5 <-as.data.frame(ID_names1) #convert to a dataframe.
ID5$name<-colnames(ID5) #make a new variable from column name.
colnames(ID5)<-c("name","val") #rename columns to be consistent for all dataframes.
ID6 <-as.data.frame(ID_names2)
ID6$name<-colnames(ID6)
colnames(ID6)<-c("name","val")
IDall<-rbind(ID5,ID6) 


Tim

-----Original Message-----
From: anteneh asmare <hanatezera at gmail.com> 
Sent: Monday, June 13, 2022 4:49 PM
To: Ebert,Timothy Aaron <tebert at ufl.edu>
Cc: r-help at r-project.org
Subject: Re: [R] Combining Differnt columns in one single column

[External Email]

Dear Tim it works, But i want to treat them as a column name. Is it possible to treat as a column name ?

Best,
Hana
On 6/13/22, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> ID5 <- as.data.frame(ID_names1)
> colnames(ID5)<-"val" #rename the columns so they have the same name.
> ID6 <- as.data.frame(ID_names2)
> colnames(ID6)<-"val"
> rbind(ID5, ID6)
>
> This works for the first two, just keep doing the same thing for the 
> others.
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of anteneh 
> asmare
> Sent: Monday, June 13, 2022 3:48 PM
> To: r-help at r-project.org
> Subject: [R] Combining Differnt columns in one single column
>
> [External Email]
>
> Dear All, I have the following column name in different data frame and 
> different size
> ID_names1 <- paste0("id0000",1:9)
> ID_names2 <- paste0("id000",10:99)
> ID_names3 <- paste0("id00",100:999)
> ID_names4 <- paste0("id0",1000:9999)
> ID_names5 <- paste0("id",10000:50000)
> Dose it possible to combine in to a single column in r?
> Best,
> Hana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=cjCY0UmRewiKkMVVxNu2riO8k4tZHD7s7R3R6SC-bwEM5u77LyExCSIzw
> I-q7Xu_&s=xrZArAFWnJf7ja13EEgxwBj-pMBEXBYSQgc6BhtNPAA&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=cjCY0UmRewiKkMVVxNu2riO8k4tZHD7s7R3R6SC-bwEM5u77LyExCSIz
> wI-q7Xu_&s=oUlYLeH_EF5rwea10resAheSjjvJ6nhUl8wT84OVXZM&e=
> and provide commented, minimal, self-contained, reproducible code.
>

From tebert @end|ng |rom u||@edu  Tue Jun 14 02:43:26 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 14 Jun 2022 00:43:26 +0000
Subject: [R] Combining Differnt columns in one single column
In-Reply-To: <CAEuJ0VB2qa+gM3w=RrW9SrS7eN+Njf6GB-Yz6s++zwUo7Obu7w@mail.gmail.com>
References: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>
 <BN6PR2201MB1553CED7003E70B0A65F2E42CFAB9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAEuJ0VB2qa+gM3w=RrW9SrS7eN+Njf6GB-Yz6s++zwUo7Obu7w@mail.gmail.com>
Message-ID: <BN6PR2201MB1553CBBF7B2C5BB4B31CF6C6CFAA9@BN6PR2201MB1553.namprd22.prod.outlook.com>

Sorry, the last post had a small mistake. I got name and val switched. Here they are in the correct order.

ID_names1 <- paste0("id0000",1:9)
ID_names2 <- paste0("id000",10:99)
ID5 <-as.data.frame(ID_names1)
ID5$name<-colnames(ID5)
colnames(ID5)<-c("val","name")
ID6 <-as.data.frame(ID_names2)
ID6$name<-colnames(ID6)
colnames(ID6)<-c("val","name")
IDall<-rbind(ID5,ID6)


Tim
-----Original Message-----
From: anteneh asmare <hanatezera at gmail.com> 
Sent: Monday, June 13, 2022 4:49 PM
To: Ebert,Timothy Aaron <tebert at ufl.edu>
Cc: r-help at r-project.org
Subject: Re: [R] Combining Differnt columns in one single column

[External Email]

Dear Tim it works, But i want to treat them as a column name. Is it possible to treat as a column name ?

Best,
Hana
On 6/13/22, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> ID5 <- as.data.frame(ID_names1)
> colnames(ID5)<-"val" #rename the columns so they have the same name.
> ID6 <- as.data.frame(ID_names2)
> colnames(ID6)<-"val"
> rbind(ID5, ID6)
>
> This works for the first two, just keep doing the same thing for the 
> others.
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of anteneh 
> asmare
> Sent: Monday, June 13, 2022 3:48 PM
> To: r-help at r-project.org
> Subject: [R] Combining Differnt columns in one single column
>
> [External Email]
>
> Dear All, I have the following column name in different data frame and 
> different size
> ID_names1 <- paste0("id0000",1:9)
> ID_names2 <- paste0("id000",10:99)
> ID_names3 <- paste0("id00",100:999)
> ID_names4 <- paste0("id0",1000:9999)
> ID_names5 <- paste0("id",10000:50000)
> Dose it possible to combine in to a single column in r?
> Best,
> Hana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=cjCY0UmRewiKkMVVxNu2riO8k4tZHD7s7R3R6SC-bwEM5u77LyExCSIzw
> I-q7Xu_&s=xrZArAFWnJf7ja13EEgxwBj-pMBEXBYSQgc6BhtNPAA&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=cjCY0UmRewiKkMVVxNu2riO8k4tZHD7s7R3R6SC-bwEM5u77LyExCSIz
> wI-q7Xu_&s=oUlYLeH_EF5rwea10resAheSjjvJ6nhUl8wT84OVXZM&e=
> and provide commented, minimal, self-contained, reproducible code.
>

From @tephen@berm@n @end|ng |rom gmx@net  Mon Jun 13 23:47:29 2022
From: @tephen@berm@n @end|ng |rom gmx@net (Stephen Berman)
Date: Mon, 13 Jun 2022 23:47:29 +0200
Subject: [R] Why does qt() return Inf with certain negative ncp values?
Message-ID: <874k0oxmku.fsf@rub.de>

Can anyone explain why Inf appears in the following results?

> sapply(-1:-10, \(ncp) qt(1-1*(10^(-4+ncp)), 35, ncp))
 [1]  3.6527153  3.0627759  2.4158355  1.7380812  1.0506904  0.3700821
 [7]        Inf -0.9279783 -1.5341759 -2.1085213

> sapply(seq(-6.9, -7.9, -0.1), \(ncp) qt(1-1*(10^(-4+ncp)), 35, ncp))
 [1] -0.2268386        Inf        Inf        Inf -0.4857400 -0.5497784
 [7] -0.6135402 -0.6770143 -0.7401974 -0.8030853 -0.8656810

In case it matters:

> sessionInfo()
R Under development (unstable) (2022-06-05 r82452)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Linux From Scratch r11.0-165

Matrix products: default
BLAS:   /usr/lib/R/lib/libRblas.so
LAPACK: /usr/lib/R/lib/libRlapack.so

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_4.3.0 tools_4.3.0

Thanks.
Steve Berman


From bgunter@4567 @end|ng |rom gm@||@com  Tue Jun 14 04:28:43 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 13 Jun 2022 19:28:43 -0700
Subject: [R] Why does qt() return Inf with certain negative ncp values?
In-Reply-To: <874k0oxmku.fsf@rub.de>
References: <874k0oxmku.fsf@rub.de>
Message-ID: <CAGxFJbTXj_+pisvdmMXyTMiqokPAVx5GKwiA+j_tTrECzoR7dw@mail.gmail.com>

Well, this will likely require close numerical analysis of the algorithm
used for the calculation, which I can't help with, but I do note for the
first case (I didn't bother checking the second) it may be useful to note
that besides returning Inf, I also got with -7 as ncp:

"There were 50 or more warnings"
all of which were:
"In qt(1 - 1 * (10^(-11)), 35, -7) :
  full precision may not have been achieved in 'pnt{final}' "

I got this warning for all the other values of ncp also.

So it looks like: Congratulations, you seem to have defeated the algorithm.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Jun 13, 2022 at 6:14 PM Stephen Berman <stephen.berman at gmx.net>
wrote:

> Can anyone explain why Inf appears in the following results?
>
> > sapply(-1:-10, \(ncp) qt(1-1*(10^(-4+ncp)), 35, ncp))
>  [1]  3.6527153  3.0627759  2.4158355  1.7380812  1.0506904  0.3700821
>  [7]        Inf -0.9279783 -1.5341759 -2.1085213
>
> > sapply(seq(-6.9, -7.9, -0.1), \(ncp) qt(1-1*(10^(-4+ncp)), 35, ncp))
>  [1] -0.2268386        Inf        Inf        Inf -0.4857400 -0.5497784
>  [7] -0.6135402 -0.6770143 -0.7401974 -0.8030853 -0.8656810
>
> In case it matters:
>
> > sessionInfo()
> R Under development (unstable) (2022-06-05 r82452)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Linux From Scratch r11.0-165
>
> Matrix products: default
> BLAS:   /usr/lib/R/lib/libRblas.so
> LAPACK: /usr/lib/R/lib/libRlapack.so
>
> locale:
>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>  [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> loaded via a namespace (and not attached):
> [1] compiler_4.3.0 tools_4.3.0
>
> Thanks.
> Steve Berman
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Tue Jun 14 12:43:09 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Tue, 14 Jun 2022 13:43:09 +0300
Subject: [R] Install OpenCL
In-Reply-To: <5e9ca34f-9569-0b7d-e070-d7036f7801b2@gmx.de>
References: <1e3ff528-1d77-6a66-3184-0f94ce826f62@gmx.de>
 <20220602184655.1c840903@arachnoid>
 <5e9ca34f-9569-0b7d-e070-d7036f7801b2@gmx.de>
Message-ID: <20220614134309.0b390d14@arachnoid>

? Tue, 14 Jun 2022 10:58:59 +0200
Quirin Stier <Quirin_Stier at gmx.de> ?????:

> Sys.env(OCL="C:/Program Files/NVIDIA GPU Computing
> Toolkit/CUDA/v11.7/include/CL")

> In file included from buffer.c:2:
> ocl.h:7:10: fatal error: CL/opencl.h: No such file or directory
>  ??? 7 | #include <CL/opencl.h>
>  ????? |????????? ^~~~~~~~~~~~~

Almost there. If you download the package and read the configure.win
file, or take a look at the GitHub mirror [1], you can see that the
path it uses by default for the include files is equivalent to the
following R expression: file.path(Sys.getenv('OCL'), 'include'). On the
other hand, the code assumes that the include files are available under
file.path(include_path, 'CL', 'opencl.h').

You can probably find the file C:/Program Files/NVIDIA GPU Computing
Toolkit/CUDA/v11.7/include/CL/opencl.h, so the proper path to set the
OCL variable to is "C:/Program Files/NVIDIA GPU Computing
Toolkit/CUDA/v11.7", without the "include/CL" at the end.

The configure script also assumes that there are library files under
file.path(Sys.getenv('OCL'), 'lib', architecture, 'OpenCL.lib'). If
that's not correct, you have to find them and set the OCL32LIB or
OCL64LIB environment variable to the correct path.

> How can I make sure, that the OpenCL DLLs are available via the
> %PATH%?

The expression strsplit(Sys.getenv('PATH'), ';')[[1]] will return a
vector containing paths to directories. One of them must contain the
OpenCL.dll file in order for everything to work.

-- 
Best regards,
Ivan

[1] https://github.com/cran/OpenCL/blob/master/configure.win


From h@n@tezer@ @end|ng |rom gm@||@com  Tue Jun 14 13:28:37 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Tue, 14 Jun 2022 14:28:37 +0300
Subject: [R] Create a categorical variable using the deciles of data
Message-ID: <CAEuJ0VD66249iN6mAMJcYf=xf_2aktmB-31kx16-su4AT+P6kA@mail.gmail.com>

I want Create a categorical variable using the deciles of the
following data frame to divide the individuals into 10 groups equally.
I try the following codes
data_catigocal<-data.frame(c(1:50000))
# create categorical vector using deciles
group_vector <-
c('0-10','11-20','21-30','31-40','41-50','51-60','61-70','71-80','81-90','91-100')
# Add categorical variable to the data_catigocal
data_catigocal$decile <- factor(group_vector)
# print data frame
data_catigocal

can any one help me with the r code
Kind regards,
Hana


From tebert @end|ng |rom u||@edu  Tue Jun 14 14:28:49 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 14 Jun 2022 12:28:49 +0000
Subject: [R] Create a categorical variable using the deciles of data
In-Reply-To: <CAEuJ0VD66249iN6mAMJcYf=xf_2aktmB-31kx16-su4AT+P6kA@mail.gmail.com>
References: <CAEuJ0VD66249iN6mAMJcYf=xf_2aktmB-31kx16-su4AT+P6kA@mail.gmail.com>
Message-ID: <BN6PR2201MB15534461FC01AAE729FE92D7CFAA9@BN6PR2201MB1553.namprd22.prod.outlook.com>

Hana, the "right" answer depends on exactly what you need. Here are three correct solutions. They use the same basic strategy to give different results. There are also other approaches in R to get the same outcome. You could use data_catigocal[i,j] and some for loops. 

size1 <-50000
ngroup <- 10 # note that size1 must be evenly divisible by ngroup
group_size <- size1/ngroup
data_catigocal <-data.frame(c(1:size1))
data_categorical1<-data_catigocal
# create categorical vector using deciles 
group_vector <- c('0-10','11-20','21-30','31-40','41-50','51-60','61-70','71-80','81-90','91-100')
data_categorical1$group_vn <-rep(group_vector,group_size)

option2 <- rep(group_vector, group_size)
option2 <- sort(option2, decreasing=FALSE)
data_categorical2 <- cbind(option2, data_catigocal)

option3 <- rep(group_vector, group_size)
option3a <- sample(option3, size1, replace=FALSE)
data_categorical3 <- cbind(option3a, data_catigocal)



Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of anteneh asmare
Sent: Tuesday, June 14, 2022 7:29 AM
To: r-help at r-project.org
Subject: [R] Create a categorical variable using the deciles of data

[External Email]

I want Create a categorical variable using the deciles of the following data frame to divide the individuals into 10 groups equally.
I try the following codes
data_catigocal<-data.frame(c(1:50000))
# create categorical vector using deciles group_vector <-
c('0-10','11-20','21-30','31-40','41-50','51-60','61-70','71-80','81-90','91-100')
# Add categorical variable to the data_catigocal data_catigocal$decile <- factor(group_vector) # print data frame data_catigocal

can any one help me with the r code
Kind regards,
Hana

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=iJ1M9ZDgTrZDuxyw_CUg03Mb6JmtrOaSF0JqAl-1pdmgbKG3AWiI6hMbv9LVOjKN&s=eUb_8T4KZRbFW_poDuhkWwPvNKQdkI6fm0MMTsOyh-A&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=iJ1M9ZDgTrZDuxyw_CUg03Mb6JmtrOaSF0JqAl-1pdmgbKG3AWiI6hMbv9LVOjKN&s=tnk4qRX6T6SZuapvkrNEZOtHmOVlKGS-02yHEzajqS8&e=
and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Tue Jun 14 15:00:14 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Tue, 14 Jun 2022 14:00:14 +0100
Subject: [R] Create a categorical variable using the deciles of data
In-Reply-To: <CAEuJ0VD66249iN6mAMJcYf=xf_2aktmB-31kx16-su4AT+P6kA@mail.gmail.com>
References: <CAEuJ0VD66249iN6mAMJcYf=xf_2aktmB-31kx16-su4AT+P6kA@mail.gmail.com>
Message-ID: <a4fb15d7-ca3c-5706-fc3c-533b1e81aeda@sapo.pt>

Hello,

I have recreated the data.frame giving the column a name.
Here are two ways, both based on ?pretty:


data_catigocal <- data.frame(X = 1:50000)
pretty(data_catigocal$X, n = 10)
#>  [1]     0  5000 10000 15000 20000 25000 30000 35000 40000 45000 50000


1. Use ?cut to create a factor with 10 levels then assign the labels.


group_vector <-
c('0-10','11-20','21-30','31-40','41-50','51-60','61-70','71-80','81-90','91-100')

data_catigocal$decile <- with(data_catigocal, cut(X, breaks = pretty(X, 
n = 10), include.lowest = TRUE))
data_catigocal$decile <- factor(data_catigocal$decile, labels = 
group_vector)

head(data_catigocal)
#>   X decile
#> 1 1   0-10
#> 2 2   0-10
#> 3 3   0-10
#> 4 4   0-10
#> 5 5   0-10
#> 6 6   0-10
tail(data_catigocal)
#>           X decile
#> 49995 49995 91-100
#> 49996 49996 91-100
#> 49997 49997 91-100
#> 49998 49998 91-100
#> 49999 49999 91-100
#> 50000 50000 91-100


2. Use ?findInterval to bin the data and coerce to factor with the 
appropriate levels.



data_catigocal$decile <- findInterval(data_catigocal$X, 
pretty(data_catigocal$X, n = 10), rightmost.closed = TRUE)
data_catigocal$decile <- factor(data_catigocal$decile, labels = 
group_vector)


The results are the same.

Hope this helps,

Rui Barradas



?s 12:28 de 14/06/2022, anteneh asmare escreveu:
> I want Create a categorical variable using the deciles of the
> following data frame to divide the individuals into 10 groups equally.
> I try the following codes
> data_catigocal<-data.frame(c(1:50000))
> # create categorical vector using deciles
> group_vector <-
> c('0-10','11-20','21-30','31-40','41-50','51-60','61-70','71-80','81-90','91-100')
> # Add categorical variable to the data_catigocal
> data_catigocal$decile <- factor(group_vector)
> # print data frame
> data_catigocal
> 
> can any one help me with the r code
> Kind regards,
> Hana
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@oknz @end|ng |rom gm@||@com  Tue Jun 14 15:07:47 2022
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Wed, 15 Jun 2022 01:07:47 +1200
Subject: [R] Create a categorical variable using the deciles of data
In-Reply-To: <CAEuJ0VD66249iN6mAMJcYf=xf_2aktmB-31kx16-su4AT+P6kA@mail.gmail.com>
References: <CAEuJ0VD66249iN6mAMJcYf=xf_2aktmB-31kx16-su4AT+P6kA@mail.gmail.com>
Message-ID: <CABcYAdJO5nJ_cOXC6aGKKdQCQo4zCNSUMZQJoZ0v2Mq-bXJQdA@mail.gmail.com>

Can you explain why you are not using
?quantile
to find the deciles then
?cut
to construct the factor?
What have I misunderstood?

On Tue, 14 Jun 2022 at 23:29, anteneh asmare <hanatezera at gmail.com> wrote:

> I want Create a categorical variable using the deciles of the
> following data frame to divide the individuals into 10 groups equally.
> I try the following codes
> data_catigocal<-data.frame(c(1:50000))
> # create categorical vector using deciles
> group_vector <-
>
> c('0-10','11-20','21-30','31-40','41-50','51-60','61-70','71-80','81-90','91-100')
> # Add categorical variable to the data_catigocal
> data_catigocal$decile <- factor(group_vector)
> # print data frame
> data_catigocal
>
> can any one help me with the r code
> Kind regards,
> Hana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From h@n@tezer@ @end|ng |rom gm@||@com  Tue Jun 14 15:12:00 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Tue, 14 Jun 2022 16:12:00 +0300
Subject: [R] Create a categorical variable using the deciles of data
In-Reply-To: <BN6PR2201MB15534461FC01AAE729FE92D7CFAA9@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAEuJ0VD66249iN6mAMJcYf=xf_2aktmB-31kx16-su4AT+P6kA@mail.gmail.com>
 <BN6PR2201MB15534461FC01AAE729FE92D7CFAA9@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <CAEuJ0VDRB_njDcChWBeSYagtQcmcSf2pyVFimHyky0mp00q6iw@mail.gmail.com>

Dear, Tim, Thanks a lot. It works perfectly.
Best,
Hana
On 6/14/22, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> Hana, the "right" answer depends on exactly what you need. Here are three
> correct solutions. They use the same basic strategy to give different
> results. There are also other approaches in R to get the same outcome. You
> could use data_catigocal[i,j] and some for loops.
>
> size1 <-50000
> ngroup <- 10 # note that size1 must be evenly divisible by ngroup
> group_size <- size1/ngroup
> data_catigocal <-data.frame(c(1:size1))
> data_categorical1<-data_catigocal
> # create categorical vector using deciles
> group_vector <-
> c('0-10','11-20','21-30','31-40','41-50','51-60','61-70','71-80','81-90','91-100')
> data_categorical1$group_vn <-rep(group_vector,group_size)
>
> option2 <- rep(group_vector, group_size)
> option2 <- sort(option2, decreasing=FALSE)
> data_categorical2 <- cbind(option2, data_catigocal)
>
> option3 <- rep(group_vector, group_size)
> option3a <- sample(option3, size1, replace=FALSE)
> data_categorical3 <- cbind(option3a, data_catigocal)
>
>
>
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of anteneh asmare
> Sent: Tuesday, June 14, 2022 7:29 AM
> To: r-help at r-project.org
> Subject: [R] Create a categorical variable using the deciles of data
>
> [External Email]
>
> I want Create a categorical variable using the deciles of the following data
> frame to divide the individuals into 10 groups equally.
> I try the following codes
> data_catigocal<-data.frame(c(1:50000))
> # create categorical vector using deciles group_vector <-
> c('0-10','11-20','21-30','31-40','41-50','51-60','61-70','71-80','81-90','91-100')
> # Add categorical variable to the data_catigocal data_catigocal$decile <-
> factor(group_vector) # print data frame data_catigocal
>
> can any one help me with the r code
> Kind regards,
> Hana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=iJ1M9ZDgTrZDuxyw_CUg03Mb6JmtrOaSF0JqAl-1pdmgbKG3AWiI6hMbv9LVOjKN&s=eUb_8T4KZRbFW_poDuhkWwPvNKQdkI6fm0MMTsOyh-A&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=iJ1M9ZDgTrZDuxyw_CUg03Mb6JmtrOaSF0JqAl-1pdmgbKG3AWiI6hMbv9LVOjKN&s=tnk4qRX6T6SZuapvkrNEZOtHmOVlKGS-02yHEzajqS8&e=
> and provide commented, minimal, self-contained, reproducible code.
>


From tebert @end|ng |rom u||@edu  Tue Jun 14 15:49:03 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 14 Jun 2022 13:49:03 +0000
Subject: [R] Create a categorical variable using the deciles of data
In-Reply-To: <CABcYAdJO5nJ_cOXC6aGKKdQCQo4zCNSUMZQJoZ0v2Mq-bXJQdA@mail.gmail.com>
References: <CAEuJ0VD66249iN6mAMJcYf=xf_2aktmB-31kx16-su4AT+P6kA@mail.gmail.com>
 <CABcYAdJO5nJ_cOXC6aGKKdQCQo4zCNSUMZQJoZ0v2Mq-bXJQdA@mail.gmail.com>
Message-ID: <BN6PR2201MB1553901C2C47BFA5B3523BCDCFAA9@BN6PR2201MB1553.namprd22.prod.outlook.com>

A problem in R is that there are several dozen ways to do any of these basic activities. I used the approach that I could get to work the fastest and tried to make it somewhat general. I do not know functions like ?pretty that Rui used, nor ?quantile or ?cut. It is a difficulty in learning R where the internet or sites like this one are the "teacher." There are plenty of books, but they too take one approach to solve a problem rather than "here is a problem" and "these are all possible solutions." I appreciate seeing alternative solutions.

Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Richard O'Keefe
Sent: Tuesday, June 14, 2022 9:08 AM
To: anteneh asmare <hanatezera at gmail.com>
Cc: R Project Help <r-help at r-project.org>
Subject: Re: [R] Create a categorical variable using the deciles of data

[External Email]

Can you explain why you are not using
?quantile
to find the deciles then
?cut
to construct the factor?
What have I misunderstood?

On Tue, 14 Jun 2022 at 23:29, anteneh asmare <hanatezera at gmail.com> wrote:

> I want Create a categorical variable using the deciles of the 
> following data frame to divide the individuals into 10 groups equally.
> I try the following codes
> data_catigocal<-data.frame(c(1:50000))
> # create categorical vector using deciles group_vector <-
>
> c('0-10','11-20','21-30','31-40','41-50','51-60','61-70','71-80','81-9
> 0','91-100') # Add categorical variable to the data_catigocal 
> data_catigocal$decile <- factor(group_vector) # print data frame 
> data_catigocal
>
> can any one help me with the r code
> Kind regards,
> Hana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=JNEsCwSpVwVmoMiEXA8K7ZWqg1GZK3Cx87LshtZ5gy5Y8SyDZrUSTuotO
> cQ44yzy&s=2x4gMg5K_GJPK-XUk3UfSB3hhFCziCOgqvxl7yJXTvA&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=JNEsCwSpVwVmoMiEXA8K7ZWqg1GZK3Cx87LshtZ5gy5Y8SyDZrUSTuot
> OcQ44yzy&s=50k59quZy2KmFsVBRxK4P-M7RyxDsPGieX6TiiY5or0&e=
> and provide commented, minimal, self-contained, reproducible code.
>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=JNEsCwSpVwVmoMiEXA8K7ZWqg1GZK3Cx87LshtZ5gy5Y8SyDZrUSTuotOcQ44yzy&s=2x4gMg5K_GJPK-XUk3UfSB3hhFCziCOgqvxl7yJXTvA&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=JNEsCwSpVwVmoMiEXA8K7ZWqg1GZK3Cx87LshtZ5gy5Y8SyDZrUSTuotOcQ44yzy&s=50k59quZy2KmFsVBRxK4P-M7RyxDsPGieX6TiiY5or0&e=
and provide commented, minimal, self-contained, reproducible code.


From kry|ov@r00t @end|ng |rom gm@||@com  Tue Jun 14 15:59:57 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Tue, 14 Jun 2022 16:59:57 +0300
Subject: [R] Install OpenCL
In-Reply-To: <3702aa6c-5d0e-c76a-c06f-6a870265d03c@gmx.de>
References: <1e3ff528-1d77-6a66-3184-0f94ce826f62@gmx.de>
 <20220602184655.1c840903@arachnoid>
 <5e9ca34f-9569-0b7d-e070-d7036f7801b2@gmx.de>
 <20220614134309.0b390d14@arachnoid>
 <3702aa6c-5d0e-c76a-c06f-6a870265d03c@gmx.de>
Message-ID: <20220614165957.73ff239f@arachnoid>

? Tue, 14 Jun 2022 15:37:35 +0200
Quirin Stier <Quirin_Stier at gmx.de> ?????:

> OCL=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7
> PKG_CPPFLAGS=-IC:/Program Files/NVIDIA GPU Computing
> Toolkit/CUDA/v11.7/include
> PKG_LIBS=C:/Program Files/NVIDIA GPU Computing
> Toolkit/CUDA/v11.7/lib/x64/OpenCL.lib

> Makevars.win:2: *** missing separator.? Stop.

Are there any newlines in the value you're setting for the OCL
environment variable? If you copied it from my e-mail, there might be
some because of line-wrapping, but it should fit on one line. If you
set it correctly, file.exists(Sys.getenv('OCL')) should return TRUE.

-- 
Best regards,
Ivan


From kry|ov@r00t @end|ng |rom gm@||@com  Tue Jun 14 16:08:13 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Tue, 14 Jun 2022 17:08:13 +0300
Subject: [R] Install OpenCL
In-Reply-To: <20220614165957.73ff239f@arachnoid>
References: <1e3ff528-1d77-6a66-3184-0f94ce826f62@gmx.de>
 <20220602184655.1c840903@arachnoid>
 <5e9ca34f-9569-0b7d-e070-d7036f7801b2@gmx.de>
 <20220614134309.0b390d14@arachnoid>
 <3702aa6c-5d0e-c76a-c06f-6a870265d03c@gmx.de>
 <20220614165957.73ff239f@arachnoid>
Message-ID: <20220614170813.68f188d9@arachnoid>

? Tue, 14 Jun 2022 16:59:57 +0300
Ivan Krylov <krylov.r00t at gmail.com> ?????:

> If you set it correctly, file.exists(Sys.getenv('OCL')) should return
> TRUE.

Actually, I'm wrong about this. Your path contains spaces, which means
that it needs to be quoted. Otherwise the compiler will see five
separate arguments "-IC:/Program" "Files/NVIDIA" "GPU" "Computing"
"Toolkit/CUDA/v11.7/include" instead of one and won't understand what's
needed from it.

Does it work if you use the following call to set the environment
variable in quotes? The file.exists() test will fail, but other parts
of the package build system should work with it.

Sys.setenv(OCL = '"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7"')

-- 
Best regards,
Ivan


From kry|ov@r00t @end|ng |rom gm@||@com  Tue Jun 14 16:37:01 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Tue, 14 Jun 2022 17:37:01 +0300
Subject: [R] Install OpenCL
In-Reply-To: <fec5dfaa-1529-2107-1d60-87d9be153ed2@gmx.de>
References: <1e3ff528-1d77-6a66-3184-0f94ce826f62@gmx.de>
 <20220602184655.1c840903@arachnoid>
 <5e9ca34f-9569-0b7d-e070-d7036f7801b2@gmx.de>
 <20220614134309.0b390d14@arachnoid>
 <3702aa6c-5d0e-c76a-c06f-6a870265d03c@gmx.de>
 <20220614165957.73ff239f@arachnoid>
 <20220614170813.68f188d9@arachnoid>
 <fec5dfaa-1529-2107-1d60-87d9be153ed2@gmx.de>
Message-ID: <20220614173701.4ed0e300@arachnoid>

On Tue, 14 Jun 2022 16:14:09 +0200
Quirin Stier <Quirin_Stier at gmx.de> wrote:

> Sys.setenv(OCL="C:/Program Files/NVIDIA GPU Computing
> Toolkit/CUDA/v11.7")
> 
> is working

Okay, sorry for the misleading follow-up message. I see now that the
configure.win script performs appropriate quoting by itself, with no
need to add extra quotes manually.

> ** testing if installed package can be loaded from temporary location
> Error: Laden von Paket oder Namensraum f?r 'OpenCL' in inDL(x,
> as.logical(local), as.logical(now), ...): fehlgeschlagen
>  ?kann shared object
> 'C:/Users/quiri/AppData/Local/R/win-library/4.2/00LOCK-OpenCL/00new/OpenCL/libs/x64/OpenCL.dll'
> nicht laden:
>  ? LoadLibrary failure:? Die angegebene Prozedur wurde nicht gefunden.

This might need the help of someone else from the R-help list who has
more experience solving Windows-related problems than me. It's probably
a good idea to keep the Cc: r-help at R-project.org header in our messages.

If you pass the INSTALL_opts = '--no-test-load' argument to
install.packages(), installation will "succeed", but it will be
impossible to use the package. Can you use Dependency Walker
<https://dependencywalker.com/> to see which function is
C:/Users/quiri/AppData/Local/R/win-library/4.2/OpenCL/libs/x64/OpenCL.dll
trying to load from C:/Windows/system32/OpenCL.dll and failing? Could
it be that C:/Windows/system32/OpenCL.dll is the wrong dll file? If you
know that the right dll file is somewhere under C:/Program Files/NVIDIA
GPU Computing Toolkit/CUDA/v11.7, it should be possible to copy it to
C:/Users/quiri/AppData/Local/R/win-library/4.2/OpenCL/libs/x64 to make
the installed package work correctly.

-- 
Best regards,
Ivan


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Tue Jun 14 17:09:33 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Tue, 14 Jun 2022 16:09:33 +0100
Subject: [R] Create a categorical variable using the deciles of data
In-Reply-To: <BN6PR2201MB1553901C2C47BFA5B3523BCDCFAA9@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAEuJ0VD66249iN6mAMJcYf=xf_2aktmB-31kx16-su4AT+P6kA@mail.gmail.com>
 <CABcYAdJO5nJ_cOXC6aGKKdQCQo4zCNSUMZQJoZ0v2Mq-bXJQdA@mail.gmail.com>
 <BN6PR2201MB1553901C2C47BFA5B3523BCDCFAA9@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <b9af9ae4-d010-7c6a-664d-364955fd806a@sapo.pt>

Hello,

Inline.

?s 14:49 de 14/06/2022, Ebert,Timothy Aaron escreveu:
> A problem in R is that there are several dozen ways to do any of these basic activities. I used the approach that I could get to work the fastest and tried to make it somewhat general. I do not know functions like ?pretty that Rui used, nor ?quantile or ?cut. It is a difficulty in learning R where the internet or sites like this one are the "teacher." 

Yes, that's unfortunately true.

In this particular case I've learned about pretty() because I used to 
use hist(., plot=FALSE) to get pretty breakpoints. And hist's 
documentation says it uses pretty().


There are plenty of books, but they too take one approach to solve a 
problem rather than "here is a problem" and "these are all possible 
solutions." I appreciate seeing alternative solutions.

This thread is a good example of a "teacher" with several solutions, 
there are many more threads like this.

Rui Barradas

> 
> Tim
> 
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Richard O'Keefe
> Sent: Tuesday, June 14, 2022 9:08 AM
> To: anteneh asmare <hanatezera at gmail.com>
> Cc: R Project Help <r-help at r-project.org>
> Subject: Re: [R] Create a categorical variable using the deciles of data
> 
> [External Email]
> 
> Can you explain why you are not using
> ?quantile
> to find the deciles then
> ?cut
> to construct the factor?
> What have I misunderstood?
> 
> On Tue, 14 Jun 2022 at 23:29, anteneh asmare <hanatezera at gmail.com> wrote:
> 
>> I want Create a categorical variable using the deciles of the
>> following data frame to divide the individuals into 10 groups equally.
>> I try the following codes
>> data_catigocal<-data.frame(c(1:50000))
>> # create categorical vector using deciles group_vector <-
>>
>> c('0-10','11-20','21-30','31-40','41-50','51-60','61-70','71-80','81-9
>> 0','91-100') # Add categorical variable to the data_catigocal
>> data_catigocal$decile <- factor(group_vector) # print data frame
>> data_catigocal
>>
>> can any one help me with the r code
>> Kind regards,
>> Hana
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
>> man_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
>> Rzsn7AkP-g&m=JNEsCwSpVwVmoMiEXA8K7ZWqg1GZK3Cx87LshtZ5gy5Y8SyDZrUSTuotO
>> cQ44yzy&s=2x4gMg5K_GJPK-XUk3UfSB3hhFCziCOgqvxl7yJXTvA&e=
>> PLEASE do read the posting guide
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
>> g_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
>> sRzsn7AkP-g&m=JNEsCwSpVwVmoMiEXA8K7ZWqg1GZK3Cx87LshtZ5gy5Y8SyDZrUSTuot
>> OcQ44yzy&s=50k59quZy2KmFsVBRxK4P-M7RyxDsPGieX6TiiY5or0&e=
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
>          [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=JNEsCwSpVwVmoMiEXA8K7ZWqg1GZK3Cx87LshtZ5gy5Y8SyDZrUSTuotOcQ44yzy&s=2x4gMg5K_GJPK-XUk3UfSB3hhFCziCOgqvxl7yJXTvA&e=
> PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=JNEsCwSpVwVmoMiEXA8K7ZWqg1GZK3Cx87LshtZ5gy5Y8SyDZrUSTuotOcQ44yzy&s=50k59quZy2KmFsVBRxK4P-M7RyxDsPGieX6TiiY5or0&e=
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Tue Jun 14 17:49:47 2022
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Tue, 14 Jun 2022 17:49:47 +0200
Subject: [R] Why does qt() return Inf with certain negative ncp values?
In-Reply-To: <CAGxFJbTXj_+pisvdmMXyTMiqokPAVx5GKwiA+j_tTrECzoR7dw@mail.gmail.com>
References: <874k0oxmku.fsf@rub.de>
 <CAGxFJbTXj_+pisvdmMXyTMiqokPAVx5GKwiA+j_tTrECzoR7dw@mail.gmail.com>
Message-ID: <25256.44571.781883.394844@stat.math.ethz.ch>

>>>>>     on Mon, 13 Jun 2022 19:28:43 -0700 writes:

    > Well, this will likely require close numerical analysis of
    > the algorithm used for the calculation, which I can't help
    > with, but I do note for the first case (I didn't bother
    > checking the second) it may be useful to note that besides
    > returning Inf, I also got with -7 as ncp:

    > "There were 50 or more warnings" all of which were: "In
    > qt(1 - 1 * (10^(-11)), 35, -7) : full precision may not
    > have been achieved in 'pnt{final}' "

    > I got this warning for all the other values of ncp also.

    > So it looks like: Congratulations, you seem to have
    > defeated the algorithm.

    > Bert Gunter

Thank you, Bert, for the extra information and Stephen  for
reporting.

[@Stephen:  no reason to use sapply(); qt() etc are all vectorized!]

Part of this is "known in principle" 
as the help page mentions in several places that the non-central t
algorithm for pt() has deficiencies and
that in the non-central case, qt() is defined by  inversion,
i.e., a (primitive !) version of uniroot(), using pt(*, ncp=.) :


> Note :
>      The code for non-zero ?ncp? is principally intended to be used for
>      moderate values of ?ncp?: it will not be highly accurate,
>      especially in the tails, for large values.

and indeed, you use values very much "in the tails".
and that *does* give a warning, as Bert mentioned.
[ You really should heed such warnings.
  {{careless people using  suppressWarnings() all the time is
    really very irresponsible programming/scientific practice!
  }}]
    
>>>>>     on Mon, 13 Jun 2022 19:28:43 -0700 writes:

    > Well, this will likely require close numerical analysis of
    > the algorithm used for the calculation, which I can't help
    > with, but I do note for the first case (I didn't bother
    > checking the second) it may be useful to note that besides
    > returning Inf, I also got with -7 as ncp:

    > "There were 50 or more warnings" all of which were: "In
    > qt(1 - 1 * (10^(-11)), 35, -7) : full precision may not
    > have been achieved in 'pnt{final}' "

    > I got this warning for all the other values of ncp also.

    > So it looks like: Congratulations, you seem to have
    > defeated the algorithm.

    > Bert Gunter

Thank you, Bert, for the extra information and Stephen  for
reporting.

[@Stephen:  no reason to use sapply(); qt() etc are all vectorized!]

Part of this is "known in principle" 
as the help page mentions in several places that the non-central t
algorithm for pt() has deficiencies and
that in the non-central case, qt() is defined by  inversion,
i.e., a (primitive !) version of uniroot(), using pt(*, ncp=.) :

    Note :
         The code for non-zero ?ncp? is principally intended to be used for
         moderate values of ?ncp?: it will not be highly accurate,
         especially in the tails, for large values.

and indeed, you use values very much "in the tails".
and the use *does* give warning, as Bert mentioned.
You really should heed such warnings.
{{careless people using  suppressWarnings() all the time is
  really very irresponsible programming/scientific practice!   }}
    
There's much more to be said here, but as I see  Stephen has in
the mean time diverted the topic to the  R-devel mailing list --
quite appropriately -- 
  --> https://stat.ethz.ch/pipermail/r-devel/2022-June/081785.html

I will answer there.

Best,
Martin

--
Martin Maechler
ETH Zurich  and  R Core team


From h@n@tezer@ @end|ng |rom gm@||@com  Tue Jun 14 19:47:26 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Tue, 14 Jun 2022 20:47:26 +0300
Subject: [R] Create a categorical variable using the deciles of data
In-Reply-To: <b9af9ae4-d010-7c6a-664d-364955fd806a@sapo.pt>
References: <CAEuJ0VD66249iN6mAMJcYf=xf_2aktmB-31kx16-su4AT+P6kA@mail.gmail.com>
 <CABcYAdJO5nJ_cOXC6aGKKdQCQo4zCNSUMZQJoZ0v2Mq-bXJQdA@mail.gmail.com>
 <BN6PR2201MB1553901C2C47BFA5B3523BCDCFAA9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <b9af9ae4-d010-7c6a-664d-364955fd806a@sapo.pt>
Message-ID: <CAEuJ0VDPf4A+T+NPkL-pkkgckgYWT8GnFeJRPUsjufbHPNbKng@mail.gmail.com>

Hello Tim, Rui
Thank you very much for your time and unreserved support.
Best,
Hana

On 6/14/22, Rui Barradas <ruipbarradas at sapo.pt> wrote:
> Hello,
>
> Inline.
>
> ?s 14:49 de 14/06/2022, Ebert,Timothy Aaron escreveu:
>> A problem in R is that there are several dozen ways to do any of these
>> basic activities. I used the approach that I could get to work the fastest
>> and tried to make it somewhat general. I do not know functions like
>> ?pretty that Rui used, nor ?quantile or ?cut. It is a difficulty in
>> learning R where the internet or sites like this one are the "teacher."
>
> Yes, that's unfortunately true.
>
> In this particular case I've learned about pretty() because I used to
> use hist(., plot=FALSE) to get pretty breakpoints. And hist's
> documentation says it uses pretty().
>
>
> There are plenty of books, but they too take one approach to solve a
> problem rather than "here is a problem" and "these are all possible
> solutions." I appreciate seeing alternative solutions.
>
> This thread is a good example of a "teacher" with several solutions,
> there are many more threads like this.
>
> Rui Barradas
>
>>
>> Tim
>>
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Richard O'Keefe
>> Sent: Tuesday, June 14, 2022 9:08 AM
>> To: anteneh asmare <hanatezera at gmail.com>
>> Cc: R Project Help <r-help at r-project.org>
>> Subject: Re: [R] Create a categorical variable using the deciles of data
>>
>> [External Email]
>>
>> Can you explain why you are not using
>> ?quantile
>> to find the deciles then
>> ?cut
>> to construct the factor?
>> What have I misunderstood?
>>
>> On Tue, 14 Jun 2022 at 23:29, anteneh asmare <hanatezera at gmail.com>
>> wrote:
>>
>>> I want Create a categorical variable using the deciles of the
>>> following data frame to divide the individuals into 10 groups equally.
>>> I try the following codes
>>> data_catigocal<-data.frame(c(1:50000))
>>> # create categorical vector using deciles group_vector <-
>>>
>>> c('0-10','11-20','21-30','31-40','41-50','51-60','61-70','71-80','81-9
>>> 0','91-100') # Add categorical variable to the data_catigocal
>>> data_catigocal$decile <- factor(group_vector) # print data frame
>>> data_catigocal
>>>
>>> can any one help me with the r code
>>> Kind regards,
>>> Hana
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
>>> man_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
>>> Rzsn7AkP-g&m=JNEsCwSpVwVmoMiEXA8K7ZWqg1GZK3Cx87LshtZ5gy5Y8SyDZrUSTuotO
>>> cQ44yzy&s=2x4gMg5K_GJPK-XUk3UfSB3hhFCziCOgqvxl7yJXTvA&e=
>>> PLEASE do read the posting guide
>>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
>>> g_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
>>> sRzsn7AkP-g&m=JNEsCwSpVwVmoMiEXA8K7ZWqg1GZK3Cx87LshtZ5gy5Y8SyDZrUSTuot
>>> OcQ44yzy&s=50k59quZy2KmFsVBRxK4P-M7RyxDsPGieX6TiiY5or0&e=
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=JNEsCwSpVwVmoMiEXA8K7ZWqg1GZK3Cx87LshtZ5gy5Y8SyDZrUSTuotOcQ44yzy&s=2x4gMg5K_GJPK-XUk3UfSB3hhFCziCOgqvxl7yJXTvA&e=
>> PLEASE do read the posting guide
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=JNEsCwSpVwVmoMiEXA8K7ZWqg1GZK3Cx87LshtZ5gy5Y8SyDZrUSTuotOcQ44yzy&s=50k59quZy2KmFsVBRxK4P-M7RyxDsPGieX6TiiY5or0&e=
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From gregco@t@ @end|ng |rom me@com  Wed Jun 15 03:08:17 2022
From: gregco@t@ @end|ng |rom me@com (Gregory Coats)
Date: Tue, 14 Jun 2022 21:08:17 -0400
Subject: [R] How do I "teach" R that columns 1, 2,
 and 3 are the Year-Month-Day
In-Reply-To: <c2251d5b-ae25-0292-8fe4-61e4768becc8@ign.fr>
References: <47096049.765872.1653053240121.ref@mail.yahoo.com>
 <47096049.765872.1653053240121@mail.yahoo.com>
 <c2251d5b-ae25-0292-8fe4-61e4768becc8@ign.fr>
Message-ID: <55CD3E40-FDAE-4A6E-B352-B60D1D98A236@me.com>

# Column 1 is the Year
# Column 2 is the Month
# Column 3 is the Day
# Column 4 is the Fuel
Fuel <- c(50.45, 61.48, 59.07, 55.40, 30.63, 41.35, 32.81, 49.86, 62.99, 89.37)
plot (Fuel)

2021  7 25   50.45  
2021  8 27   61.48  
2021  9 26   59.07 
2021 11  4   55.40  
2021 11 22   30.63 
2021 11 26   41.35  
2021 12  6   32.81  
2022  1 14   49.86  
2022  4 29   62.99  
2022  6 11   89.37  

How do I "teach" R that columns 1, 2, and 3 are the Year-Month-Day?
Greg Coats
	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Jun 15 03:47:19 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 14 Jun 2022 18:47:19 -0700
Subject: [R] How do I "teach" R that columns 1, 2,
 and 3 are the Year-Month-Day
In-Reply-To: <55CD3E40-FDAE-4A6E-B352-B60D1D98A236@me.com>
References: <47096049.765872.1653053240121.ref@mail.yahoo.com>
 <47096049.765872.1653053240121@mail.yahoo.com>
 <c2251d5b-ae25-0292-8fe4-61e4768becc8@ign.fr>
 <55CD3E40-FDAE-4A6E-B352-B60D1D98A236@me.com>
Message-ID: <F10D1848-0DAC-4985-8F79-E09622C85EC9@dcn.davis.ca.us>

Reprex:

dta <- read.table( text =
"Yr  Mo Dy   Fuel
2021  7 25   50.45  
2021  8 27   61.48  
2021  9 26   59.07 
2021 11  4   55.40  
2021 11 22   30.63 
2021 11 26   41.35  
2021 12  6   32.81  
2022  1 14   49.86  
2022  4 29   62.99  
2022  6 11   89.37
", header=TRUE )
dta$Dtm <- with( dta, as.Date( ISOdate( Yr, Mo, Dy ) ) )
with( dta, plot( Dtm, Fuel ) )

The ISOdate function returns a POSIXct which includes time-of-day. Analyses that don't need time can instead rely on the Date type to avoid issues with timezones.

On June 14, 2022 6:08:17 PM PDT, Gregory Coats via R-help <r-help at r-project.org> wrote:
># Column 1 is the Year
># Column 2 is the Month
># Column 3 is the Day
># Column 4 is the Fuel
>Fuel <- c(50.45, 61.48, 59.07, 55.40, 30.63, 41.35, 32.81, 49.86, 62.99, 89.37)
>plot (Fuel)
>
>2021  7 25   50.45  
>2021  8 27   61.48  
>2021  9 26   59.07 
>2021 11  4   55.40  
>2021 11 22   30.63 
>2021 11 26   41.35  
>2021 12  6   32.81  
>2022  1 14   49.86  
>2022  4 29   62.99  
>2022  6 11   89.37  
>
>How do I "teach" R that columns 1, 2, and 3 are the Year-Month-Day?
>Greg Coats
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @v|gro@@ @end|ng |rom ver|zon@net  Wed Jun 15 03:59:50 2022
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Wed, 15 Jun 2022 01:59:50 +0000 (UTC)
Subject: [R] How do I "teach" R that columns 1, 2,
 and 3 are the Year-Month-Day
In-Reply-To: <55CD3E40-FDAE-4A6E-B352-B60D1D98A236@me.com>
References: <47096049.765872.1653053240121.ref@mail.yahoo.com>
 <47096049.765872.1653053240121@mail.yahoo.com>
 <c2251d5b-ae25-0292-8fe4-61e4768becc8@ign.fr>
 <55CD3E40-FDAE-4A6E-B352-B60D1D98A236@me.com>
Message-ID: <1229027594.2224595.1655258390324@mail.yahoo.com>

Greg,
It looks like you mean to have a data.frame containing the year as a column then?Month, Day and something called Fuel.
You only made a vector called Fuel, with no name as part of it and plotted it.
So you did not expect any names, I assume.
You probably want to combine the first three columns into holding some kind of?date object with something like?as.Date("2021-07-25") for each row and make a?vector of those. Your data.frame might then contain two columns called Date and Fuel?and you might use whatever plotting method to put Fuel on the Y axis and dates on the?X axis or whatever makes sense. Plotting four independent columns will not work here.
And note the NAMES go in with something like:
mydata <- data.frame(Date=dates, Fuel=Fuel)
I have not done all the work but hopefully this is helpful.?

-----Original Message-----
From: Gregory Coats via R-help <r-help at r-project.org>
To: Greg Comcast Coats <gregcoats at me.com>
Cc: Marc Schwartz via R-help <r-help at r-project.org>
Sent: Tue, Jun 14, 2022 9:08 pm
Subject: [R] How do I "teach" R that columns 1, 2, and 3 are the Year-Month-Day

# Column 1 is the Year
# Column 2 is the Month
# Column 3 is the Day
# Column 4 is the Fuel
Fuel <- c(50.45, 61.48, 59.07, 55.40, 30.63, 41.35, 32.81, 49.86, 62.99, 89.37)
plot (Fuel)

2021? 7 25? 50.45? 
2021? 8 27? 61.48? 
2021? 9 26? 59.07 
2021 11? 4? 55.40? 
2021 11 22? 30.63 
2021 11 26? 41.35? 
2021 12? 6? 32.81? 
2022? 1 14? 49.86? 
2022? 4 29? 62.99? 
2022? 6 11? 89.37? 

How do I "teach" R that columns 1, 2, and 3 are the Year-Month-Day?
Greg Coats
??? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Wed Jun 15 04:01:44 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Wed, 15 Jun 2022 02:01:44 +0000
Subject: [R] How do I "teach" R that columns 1, 2,
 and 3 are the Year-Month-Day
In-Reply-To: <1229027594.2224595.1655258390324@mail.yahoo.com>
References: <47096049.765872.1653053240121.ref@mail.yahoo.com>
 <47096049.765872.1653053240121@mail.yahoo.com>
 <c2251d5b-ae25-0292-8fe4-61e4768becc8@ign.fr>
 <55CD3E40-FDAE-4A6E-B352-B60D1D98A236@me.com>
 <1229027594.2224595.1655258390324@mail.yahoo.com>
Message-ID: <BN6PR2201MB155373731CCFC48AB0C7A8C3CFAD9@BN6PR2201MB1553.namprd22.prod.outlook.com>

You don't, or not exactly. You could make the three columns strings, then add them together. I would then use the lubridate package to convert to a date.  The first four lines of code are just getting a bit of data into R. The lubridate package will also work with date-time objects if you have time. 

year<-rep("2021",4) 
month<-c(7,8,9,11)
day<-c(25,27,26,4)
fuel<-c(50.45, 61.48,59.07,55.40)
df<-data.frame(year,month,day,fuel)
df$day <-as.character(df$day)
df$month<-as.character(df$month)
df$date <- paste(year,"/",month,"/",day)
library(lubridate)
df$date<-ymd(df$date)

Regards,
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Avi Gross via R-help
Sent: Tuesday, June 14, 2022 10:00 PM
To: r-help at r-project.org
Cc: r-help at r-project.org
Subject: Re: [R] How do I "teach" R that columns 1, 2, and 3 are the Year-Month-Day

[External Email]

Greg,
It looks like you mean to have a data.frame containing the year as a column then Month, Day and something called Fuel.
You only made a vector called Fuel, with no name as part of it and plotted it.
So you did not expect any names, I assume.
You probably want to combine the first three columns into holding some kind of date object with something like as.Date("2021-07-25") for each row and make a vector of those. Your data.frame might then contain two columns called Date and Fuel and you might use whatever plotting method to put Fuel on the Y axis and dates on the X axis or whatever makes sense. Plotting four independent columns will not work here.
And note the NAMES go in with something like:
mydata <- data.frame(Date=dates, Fuel=Fuel) I have not done all the work but hopefully this is helpful.

-----Original Message-----
From: Gregory Coats via R-help <r-help at r-project.org>
To: Greg Comcast Coats <gregcoats at me.com>
Cc: Marc Schwartz via R-help <r-help at r-project.org>
Sent: Tue, Jun 14, 2022 9:08 pm
Subject: [R] How do I "teach" R that columns 1, 2, and 3 are the Year-Month-Day

# Column 1 is the Year
# Column 2 is the Month
# Column 3 is the Day
# Column 4 is the Fuel
Fuel <- c(50.45, 61.48, 59.07, 55.40, 30.63, 41.35, 32.81, 49.86, 62.99, 89.37) plot (Fuel)

2021  7 25  50.45
2021  8 27  61.48
2021  9 26  59.07
2021 11  4  55.40
2021 11 22  30.63
2021 11 26  41.35
2021 12  6  32.81
2022  1 14  49.86
2022  4 29  62.99
2022  6 11  89.37

How do I "teach" R that columns 1, 2, and 3 are the Year-Month-Day?
Greg Coats
    [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=5mwGPy3wD6UgdaNamboKRB8Oo4Yd9OIBPIhrRkniM0N0wUSsekHwK4HbHytbdscH&s=3iFRCZeUCCgixeFjZsrmkHARgeHp8CLb7dIAk7EHiSY&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=5mwGPy3wD6UgdaNamboKRB8Oo4Yd9OIBPIhrRkniM0N0wUSsekHwK4HbHytbdscH&s=BG0mynWESmDXi-6Z2LX7yRr51Gqw3B-Gzfg3Ye2cy8M&e=
and provide commented, minimal, self-contained, reproducible code.

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=5mwGPy3wD6UgdaNamboKRB8Oo4Yd9OIBPIhrRkniM0N0wUSsekHwK4HbHytbdscH&s=3iFRCZeUCCgixeFjZsrmkHARgeHp8CLb7dIAk7EHiSY&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=5mwGPy3wD6UgdaNamboKRB8Oo4Yd9OIBPIhrRkniM0N0wUSsekHwK4HbHytbdscH&s=BG0mynWESmDXi-6Z2LX7yRr51Gqw3B-Gzfg3Ye2cy8M&e=
and provide commented, minimal, self-contained, reproducible code.

From @tephen@berm@n @end|ng |rom gmx@net  Tue Jun 14 09:30:08 2022
From: @tephen@berm@n @end|ng |rom gmx@net (Stephen Berman)
Date: Tue, 14 Jun 2022 09:30:08 +0200
Subject: [R] Why does qt() return Inf with certain negative ncp values?
In-Reply-To: <CAGxFJbTXj_+pisvdmMXyTMiqokPAVx5GKwiA+j_tTrECzoR7dw@mail.gmail.com>
 (Bert Gunter's message of "Mon, 13 Jun 2022 19:28:43 -0700")
References: <874k0oxmku.fsf@rub.de>
 <CAGxFJbTXj_+pisvdmMXyTMiqokPAVx5GKwiA+j_tTrECzoR7dw@mail.gmail.com>
Message-ID: <875yl3n1mn.fsf@gmx.net>

On Mon, 13 Jun 2022 19:28:43 -0700 Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Well, this will likely require close numerical analysis of the algorithm
> used for the calculation, which I can't help with, but I do note for the
> first case (I didn't bother checking the second) it may be useful to note
> that besides returning Inf, I also got with -7 as ncp:
>
> "There were 50 or more warnings"
> all of which were:
> "In qt(1 - 1 * (10^(-11)), 35, -7) :
>   full precision may not have been achieved in 'pnt{final}' "
>
> I got this warning for all the other values of ncp also.

Hm, I don't get this warning with ncp = -1 through -4; I get it once
with each of -5 and -8 through -10, 32 times with -6, 50 times with -7.
In the range -6.9:-7.9 I get the warning twice with each of -6.9 and
-7.3 through -7.7, once with -7.8 and -7.9, and 50 times with each of
-7.0 through -7.2.

> So it looks like: Congratulations, you seem to have defeated the algorithm.

Then I think I'll report it on r-devel.

> Bert Gunter

Thanks.
Steve Berman


From h@n@tezer@ @end|ng |rom gm@||@com  Tue Jun 14 22:01:21 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Tue, 14 Jun 2022 23:01:21 +0300
Subject: [R] Extracting only some coefficients for the logistic regression
 model and its plot
Message-ID: <CAEuJ0VCm3qmRayAoSRYYKbkTYYS8oOnQGubEQkgHm8aRZXNi8w@mail.gmail.com>

sample_data = read.table("http://freakonometrics.free.fr/db.txt",header=TRUE,sep=";")
head(sample_data)
model = glm(Y~0+X1+X2+X3,family=binomial,data=sample_data)
summary(model)
exp(coef(model ))
exp(cbind(OR = coef(model ), confint(model )))
I have the aove sample data on logistic regression with categorical predictor
I try the above code i get the follwing  out put,
            OR       2.5 %     97.5 %
X1  1.67639337 1.352583976 2.09856514
X2  1.23377720 1.071959330 1.42496949
X3A 0.01157565 0.001429430 0.08726854
X3B 0.06627849 0.008011818 0.54419759
X3C 0.01118084 0.001339984 0.08721028
X3D 0.01254032 0.001545240 0.09539880
X3E 0.10654454 0.013141540 0.87369972
 but i am wondering i want to extract OR and  CI only for factors, My
desire out put will be
 OR       2.5 %     97.5 %
X3A 0.01157565 0.001429430 0.08726854
X3B 0.06627849 0.008011818 0.54419759
X3C 0.01118084 0.001339984 0.08721028
X3D 0.01254032 0.001545240 0.09539880
X3E 0.10654454 0.013141540 0.87369972
Can any one help me the code to extact it?
additionally I want to plot the above OR with confidence interval for
the extracted one
can you also help me the code with plot,or box plot
Kind Regards,
Hana


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed Jun 15 07:06:07 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 15 Jun 2022 06:06:07 +0100
Subject: [R] 
 Extracting only some coefficients for the logistic regression
 model and its plot
In-Reply-To: <CAEuJ0VCm3qmRayAoSRYYKbkTYYS8oOnQGubEQkgHm8aRZXNi8w@mail.gmail.com>
References: <CAEuJ0VCm3qmRayAoSRYYKbkTYYS8oOnQGubEQkgHm8aRZXNi8w@mail.gmail.com>
Message-ID: <f2a9f13a-51ac-a1a2-db8d-5030d85793cd@sapo.pt>

Hello,

To extract all but the first 2 rows, use a negative index on the rows.
I will also coerce to data.frame and add a id column, it will be needed 
to plot the confidence intervals.


ORCI <- exp(cbind(OR = coef(model ), confint(model )))[-(1:2), ]
ORCI <- cbind.data.frame(ORCI, id = row.names(ORCI))


Now the base R and ggplot plots. In both cases plot the bars first, then 
the points.

1. Base R


ymin <- min(apply(ORCI[2:3], 1, range)[1,])
ymax <- max(apply(ORCI[2:3], 1, range)[2,])

plot((ymin + ymax)/2,
      type = "n",
      xaxt = "n",
      xlim = c(0.5, 5.5),
      ylim = c(ymin, ymax),
      xlab = "X3",
      ylab = "Odds Ratio")
with(ORCI, arrows(x0 = seq_along(id),
                   y0 = `2.5 %`,
                   y1 = `97.5 %`,
                   code = 3,
                   angle = 90))
points(OR ~ seq_along(id), ORCI, pch = 16)
axis(1, at = seq_along(ORCI$id), labels = ORCI$id)



2. Package ggplot2


library(ggplot2)

ggplot(ORCI, aes(id, OR)) +
   geom_errorbar(aes(ymin = `2.5 %`, max = `97.5 %`)) +
   geom_point() +
   theme_bw()


Hope this helps,

Rui Barradas

?s 21:01 de 14/06/2022, anteneh asmare escreveu:
> sample_data = read.table("http://freakonometrics.free.fr/db.txt",header=TRUE,sep=";")
> head(sample_data)
> model = glm(Y~0+X1+X2+X3,family=binomial,data=sample_data)
> summary(model)
> exp(coef(model ))
> exp(cbind(OR = coef(model ), confint(model )))
> I have the aove sample data on logistic regression with categorical predictor
> I try the above code i get the follwing  out put,
>              OR       2.5 %     97.5 %
> X1  1.67639337 1.352583976 2.09856514
> X2  1.23377720 1.071959330 1.42496949
> X3A 0.01157565 0.001429430 0.08726854
> X3B 0.06627849 0.008011818 0.54419759
> X3C 0.01118084 0.001339984 0.08721028
> X3D 0.01254032 0.001545240 0.09539880
> X3E 0.10654454 0.013141540 0.87369972
>   but i am wondering i want to extract OR and  CI only for factors, My
> desire out put will be
>   OR       2.5 %     97.5 %
> X3A 0.01157565 0.001429430 0.08726854
> X3B 0.06627849 0.008011818 0.54419759
> X3C 0.01118084 0.001339984 0.08721028
> X3D 0.01254032 0.001545240 0.09539880
> X3E 0.10654454 0.013141540 0.87369972
> Can any one help me the code to extact it?
> additionally I want to plot the above OR with confidence interval for
> the extracted one
> can you also help me the code with plot,or box plot
> Kind Regards,
> Hana
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Wed Jun 15 07:07:49 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 15 Jun 2022 15:07:49 +1000
Subject: [R] 
 Extracting only some coefficients for the logistic regression
 model and its plot
In-Reply-To: <CAEuJ0VCm3qmRayAoSRYYKbkTYYS8oOnQGubEQkgHm8aRZXNi8w@mail.gmail.com>
References: <CAEuJ0VCm3qmRayAoSRYYKbkTYYS8oOnQGubEQkgHm8aRZXNi8w@mail.gmail.com>
Message-ID: <CA+8X3fUXJVdFvrYWLRm_FzBmTtySwRQixr+WKx3ZFuFH7Op5Aw@mail.gmail.com>

Hi Hana,
str(model) will display the structure of the returned object. However,
you want the summary of that object, so:

As you have already constructed the estimate and confidence interval
matrix, you only have to extract rows 3 to 7 and columns 1:3:

> exp(cbind(OR = coef(model ), confint(model )))[3:7,1:3]
Waiting for profiling to be done...
           OR       2.5 %     97.5 %
X3A 0.01157565 0.001429430 0.08726854
X3B 0.06627849 0.008011818 0.54419759
X3C 0.01118084 0.001339984 0.08721028
X3D 0.01254032 0.001545240 0.09539880
X3E 0.10654454 0.013141540 0.87369972

Jim

On Wed, Jun 15, 2022 at 1:38 PM anteneh asmare <hanatezera at gmail.com> wrote:
>
> sample_data = read.table("http://freakonometrics.free.fr/db.txt",header=TRUE,sep=";")
> head(sample_data)
> model = glm(Y~0+X1+X2+X3,family=binomial,data=sample_data)
> summary(model)
> exp(coef(model ))
> exp(cbind(OR = coef(model ), confint(model )))
> I have the aove sample data on logistic regression with categorical predictor
> I try the above code i get the follwing  out put,
>             OR       2.5 %     97.5 %
> X1  1.67639337 1.352583976 2.09856514
> X2  1.23377720 1.071959330 1.42496949
> X3A 0.01157565 0.001429430 0.08726854
> X3B 0.06627849 0.008011818 0.54419759
> X3C 0.01118084 0.001339984 0.08721028
> X3D 0.01254032 0.001545240 0.09539880
> X3E 0.10654454 0.013141540 0.87369972
>  but i am wondering i want to extract OR and  CI only for factors, My
> desire out put will be
>  OR       2.5 %     97.5 %
> X3A 0.01157565 0.001429430 0.08726854
> X3B 0.06627849 0.008011818 0.54419759
> X3C 0.01118084 0.001339984 0.08721028
> X3D 0.01254032 0.001545240 0.09539880
> X3E 0.10654454 0.013141540 0.87369972
> Can any one help me the code to extact it?
> additionally I want to plot the above OR with confidence interval for
> the extracted one
> can you also help me the code with plot,or box plot
> Kind Regards,
> Hana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From h@n@tezer@ @end|ng |rom gm@||@com  Wed Jun 15 09:15:27 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Wed, 15 Jun 2022 10:15:27 +0300
Subject: [R] 
 Extracting only some coefficients for the logistic regression
 model and its plot
In-Reply-To: <f2a9f13a-51ac-a1a2-db8d-5030d85793cd@sapo.pt>
References: <CAEuJ0VCm3qmRayAoSRYYKbkTYYS8oOnQGubEQkgHm8aRZXNi8w@mail.gmail.com>
 <f2a9f13a-51ac-a1a2-db8d-5030d85793cd@sapo.pt>
Message-ID: <CAEuJ0VD4vL=NjPjodod9FnH5shaqP0eos+ULkoFkZAbr8U1UQQ@mail.gmail.com>

Dear Rui,  thanks a lot, dose  it possible to have the horizontal line
for scale OR value on Y axis and different color for entire box plots
?
Best,
Hana
On 6/15/22, Rui Barradas <ruipbarradas at sapo.pt> wrote:
> Hello,
>
> To extract all but the first 2 rows, use a negative index on the rows.
> I will also coerce to data.frame and add a id column, it will be needed
> to plot the confidence intervals.
>
>
> ORCI <- exp(cbind(OR = coef(model ), confint(model )))[-(1:2), ]
> ORCI <- cbind.data.frame(ORCI, id = row.names(ORCI))
>
>
> Now the base R and ggplot plots. In both cases plot the bars first, then
> the points.
>
> 1. Base R
>
>
> ymin <- min(apply(ORCI[2:3], 1, range)[1,])
> ymax <- max(apply(ORCI[2:3], 1, range)[2,])
>
> plot((ymin + ymax)/2,
>       type = "n",
>       xaxt = "n",
>       xlim = c(0.5, 5.5),
>       ylim = c(ymin, ymax),
>       xlab = "X3",
>       ylab = "Odds Ratio")
> with(ORCI, arrows(x0 = seq_along(id),
>                    y0 = `2.5 %`,
>                    y1 = `97.5 %`,
>                    code = 3,
>                    angle = 90))
> points(OR ~ seq_along(id), ORCI, pch = 16)
> axis(1, at = seq_along(ORCI$id), labels = ORCI$id)
>
>
>
> 2. Package ggplot2
>
>
> library(ggplot2)
>
> ggplot(ORCI, aes(id, OR)) +
>    geom_errorbar(aes(ymin = `2.5 %`, max = `97.5 %`)) +
>    geom_point() +
>    theme_bw()
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 21:01 de 14/06/2022, anteneh asmare escreveu:
>> sample_data =
>> read.table("http://freakonometrics.free.fr/db.txt",header=TRUE,sep=";")
>> head(sample_data)
>> model = glm(Y~0+X1+X2+X3,family=binomial,data=sample_data)
>> summary(model)
>> exp(coef(model ))
>> exp(cbind(OR = coef(model ), confint(model )))
>> I have the aove sample data on logistic regression with categorical
>> predictor
>> I try the above code i get the follwing  out put,
>>              OR       2.5 %     97.5 %
>> X1  1.67639337 1.352583976 2.09856514
>> X2  1.23377720 1.071959330 1.42496949
>> X3A 0.01157565 0.001429430 0.08726854
>> X3B 0.06627849 0.008011818 0.54419759
>> X3C 0.01118084 0.001339984 0.08721028
>> X3D 0.01254032 0.001545240 0.09539880
>> X3E 0.10654454 0.013141540 0.87369972
>>   but i am wondering i want to extract OR and  CI only for factors, My
>> desire out put will be
>>   OR       2.5 %     97.5 %
>> X3A 0.01157565 0.001429430 0.08726854
>> X3B 0.06627849 0.008011818 0.54419759
>> X3C 0.01118084 0.001339984 0.08721028
>> X3D 0.01254032 0.001545240 0.09539880
>> X3E 0.10654454 0.013141540 0.87369972
>> Can any one help me the code to extact it?
>> additionally I want to plot the above OR with confidence interval for
>> the extracted one
>> can you also help me the code with plot,or box plot
>> Kind Regards,
>> Hana
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From Qu|r|n_St|er @end|ng |rom gmx@de  Wed Jun 15 08:54:43 2022
From: Qu|r|n_St|er @end|ng |rom gmx@de (Quirin Stier)
Date: Wed, 15 Jun 2022 08:54:43 +0200
Subject: [R] Install OpenCL
In-Reply-To: <20220614173701.4ed0e300@arachnoid>
References: <1e3ff528-1d77-6a66-3184-0f94ce826f62@gmx.de>
 <20220602184655.1c840903@arachnoid>
 <5e9ca34f-9569-0b7d-e070-d7036f7801b2@gmx.de>
 <20220614134309.0b390d14@arachnoid>
 <3702aa6c-5d0e-c76a-c06f-6a870265d03c@gmx.de>
 <20220614165957.73ff239f@arachnoid> <20220614170813.68f188d9@arachnoid>
 <fec5dfaa-1529-2107-1d60-87d9be153ed2@gmx.de>
 <20220614173701.4ed0e300@arachnoid>
Message-ID: <2eb52eed-bf9d-04e0-cc22-238e6edcae00@gmx.de>

Thanks a lot for your help again.

Installing the package with INSTALL_opts = '--no-test-load' enables
package installation. Doing that plus analyzing the OpenCL.dll with the
dependency walker software for both the original OpenCL.dll (which
RStudio apparently found while installing) and a second OpenCl.dll from
the www yields following output (which is the same for both OpenCl.dll
attempts):

Error: At least one required implicit or forwarded dependency was not found.
Error: At least one module has an unresolved import due to a missing
export function in an implicitly dependent module.
Error: A circular dependency was detected.
Warning: At least one delay-load dependency module was not found.
Warning: At least one module has an unresolved import due to a missing
export function in a delay-load dependent module.

Nota: NVIDIA GPU Computing Toolkit does not contain any OpenCL.dll, only
the OpenCL.h (required for the includes) and a OpenCL.lib (maybe not
important)

Output from the package installation

* installing *source* package 'OpenCL' ...
** Paket 'OpenCL' erfolgreich entpackt und MD5 Summen ?berpr?ft
** using staged installation

 ?=== configurig OpenCL for /x64 ==

 ?OCL=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7

OCL64LIB not set, assuming C:/Program Files/NVIDIA GPU Computing
Toolkit/CUDA/v11.7/lib/x64/OpenCL.lib
OCL32LIB not set, assuming C:/Program Files/NVIDIA GPU Computing
Toolkit/CUDA/v11.7/lib/x86/OpenCL.lib
OCLINC not set, assuming -IC:/Program Files/NVIDIA GPU Computing
Toolkit/CUDA/v11.7/include

 ?--- Compiling 64-bit ---
OCL=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7
PKG_CPPFLAGS=-IC:/Program Files/NVIDIA GPU Computing
Toolkit/CUDA/v11.7/include
PKG_LIBS=C:/Program Files/NVIDIA GPU Computing
Toolkit/CUDA/v11.7/lib/x64/OpenCL.lib

NOTE: make sure the run-time DLLs are on PATH!

** libs
Warnung: this package has a non-empty 'configure.win' file,
so building only the main architecture

gcc? -I"C:/PROGRA~1/R/R-42~1.0/include" -DNDEBUG '-IC:/Program
Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7/include'
-I"C:/rtools42/x86_64-w64-mingw32.static.posix/include"???? -O2 -Wall?
-std=gnu99 -mfpmath=sse -msse2 -mstackrealign? -c buffer.c -o buffer.o
gcc? -I"C:/PROGRA~1/R/R-42~1.0/include" -DNDEBUG '-IC:/Program
Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7/include'
-I"C:/rtools42/x86_64-w64-mingw32.static.posix/include"???? -O2 -Wall?
-std=gnu99 -mfpmath=sse -msse2 -mstackrealign? -c ocl.c -o ocl.o
gcc? -I"C:/PROGRA~1/R/R-42~1.0/include" -DNDEBUG '-IC:/Program
Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7/include'
-I"C:/rtools42/x86_64-w64-mingw32.static.posix/include"???? -O2 -Wall?
-std=gnu99 -mfpmath=sse -msse2 -mstackrealign? -c reg.c -o reg.o
gcc? -I"C:/PROGRA~1/R/R-42~1.0/include" -DNDEBUG '-IC:/Program
Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7/include'
-I"C:/rtools42/x86_64-w64-mingw32.static.posix/include"???? -O2 -Wall?
-std=gnu99 -mfpmath=sse -msse2 -mstackrealign? -c wrap.c -o wrap.o
gcc -shared -s -static-libgcc -o OpenCL.dll tmp.def buffer.o ocl.o reg.o
wrap.o C:/Program Files/NVIDIA GPU Computing
Toolkit/CUDA/v11.7/lib/x64/OpenCL.lib
-LC:/rtools42/x86_64-w64-mingw32.static.posix/lib/x64
-LC:/rtools42/x86_64-w64-mingw32.static.posix/lib
-LC:/PROGRA~1/R/R-42~1.0/bin/x64 -lR
installing to
C:/Users/quiri/AppData/Local/R/win-library/4.2/00LOCK-OpenCL/00new/OpenCL/libs/x64
** R
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
* DONE (OpenCL)


Am 14.06.2022 um 16:37 schrieb Ivan Krylov:
> On Tue, 14 Jun 2022 16:14:09 +0200
> Quirin Stier <Quirin_Stier at gmx.de> wrote:
>
>> Sys.setenv(OCL="C:/Program Files/NVIDIA GPU Computing
>> Toolkit/CUDA/v11.7")
>>
>> is working
> Okay, sorry for the misleading follow-up message. I see now that the
> configure.win script performs appropriate quoting by itself, with no
> need to add extra quotes manually.
>
>> ** testing if installed package can be loaded from temporary location
>> Error: Laden von Paket oder Namensraum f?r 'OpenCL' in inDL(x,
>> as.logical(local), as.logical(now), ...): fehlgeschlagen
>>   ?kann shared object
>> 'C:/Users/quiri/AppData/Local/R/win-library/4.2/00LOCK-OpenCL/00new/OpenCL/libs/x64/OpenCL.dll'
>> nicht laden:
>>   ? LoadLibrary failure:? Die angegebene Prozedur wurde nicht gefunden.
> This might need the help of someone else from the R-help list who has
> more experience solving Windows-related problems than me. It's probably
> a good idea to keep the Cc: r-help at R-project.org header in our messages.
>
> If you pass the INSTALL_opts = '--no-test-load' argument to
> install.packages(), installation will "succeed", but it will be
> impossible to use the package. Can you use Dependency Walker
> <https://dependencywalker.com/> to see which function is
> C:/Users/quiri/AppData/Local/R/win-library/4.2/OpenCL/libs/x64/OpenCL.dll
> trying to load from C:/Windows/system32/OpenCL.dll and failing? Could
> it be that C:/Windows/system32/OpenCL.dll is the wrong dll file? If you
> know that the right dll file is somewhere under C:/Program Files/NVIDIA
> GPU Computing Toolkit/CUDA/v11.7, it should be possible to copy it to
> C:/Users/quiri/AppData/Local/R/win-library/4.2/OpenCL/libs/x64 to make
> the installed package work correctly.
>


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed Jun 15 10:55:57 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 15 Jun 2022 09:55:57 +0100
Subject: [R] 
 Extracting only some coefficients for the logistic regression
 model and its plot
In-Reply-To: <CAEuJ0VD4vL=NjPjodod9FnH5shaqP0eos+ULkoFkZAbr8U1UQQ@mail.gmail.com>
References: <CAEuJ0VCm3qmRayAoSRYYKbkTYYS8oOnQGubEQkgHm8aRZXNi8w@mail.gmail.com>
 <f2a9f13a-51ac-a1a2-db8d-5030d85793cd@sapo.pt>
 <CAEuJ0VD4vL=NjPjodod9FnH5shaqP0eos+ULkoFkZAbr8U1UQQ@mail.gmail.com>
Message-ID: <b6a48273-b64d-4a5e-5047-6b32bdcb946d@sapo.pt>

Hello,

With ggplot it's easy, add color = id and coord_flip().


ggplot(ORCI, aes(id, OR, color = id)) +
   geom_point() +
   geom_errorbar(aes(ymin = `2.5 %`, max = `97.5 %`)) +
   coord_flip() +
   theme_bw()


Hope this helps,

Rui Barradas

?s 08:15 de 15/06/2022, anteneh asmare escreveu:
> Dear Rui,  thanks a lot, dose  it possible to have the horizontal line
> for scale OR value on Y axis and different color for entire box plots
> ?
> Best,
> Hana
> On 6/15/22, Rui Barradas <ruipbarradas at sapo.pt> wrote:
>> Hello,
>>
>> To extract all but the first 2 rows, use a negative index on the rows.
>> I will also coerce to data.frame and add a id column, it will be needed
>> to plot the confidence intervals.
>>
>>
>> ORCI <- exp(cbind(OR = coef(model ), confint(model )))[-(1:2), ]
>> ORCI <- cbind.data.frame(ORCI, id = row.names(ORCI))
>>
>>
>> Now the base R and ggplot plots. In both cases plot the bars first, then
>> the points.
>>
>> 1. Base R
>>
>>
>> ymin <- min(apply(ORCI[2:3], 1, range)[1,])
>> ymax <- max(apply(ORCI[2:3], 1, range)[2,])
>>
>> plot((ymin + ymax)/2,
>>        type = "n",
>>        xaxt = "n",
>>        xlim = c(0.5, 5.5),
>>        ylim = c(ymin, ymax),
>>        xlab = "X3",
>>        ylab = "Odds Ratio")
>> with(ORCI, arrows(x0 = seq_along(id),
>>                     y0 = `2.5 %`,
>>                     y1 = `97.5 %`,
>>                     code = 3,
>>                     angle = 90))
>> points(OR ~ seq_along(id), ORCI, pch = 16)
>> axis(1, at = seq_along(ORCI$id), labels = ORCI$id)
>>
>>
>>
>> 2. Package ggplot2
>>
>>
>> library(ggplot2)
>>
>> ggplot(ORCI, aes(id, OR)) +
>>     geom_errorbar(aes(ymin = `2.5 %`, max = `97.5 %`)) +
>>     geom_point() +
>>     theme_bw()
>>
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>> ?s 21:01 de 14/06/2022, anteneh asmare escreveu:
>>> sample_data =
>>> read.table("http://freakonometrics.free.fr/db.txt",header=TRUE,sep=";")
>>> head(sample_data)
>>> model = glm(Y~0+X1+X2+X3,family=binomial,data=sample_data)
>>> summary(model)
>>> exp(coef(model ))
>>> exp(cbind(OR = coef(model ), confint(model )))
>>> I have the aove sample data on logistic regression with categorical
>>> predictor
>>> I try the above code i get the follwing  out put,
>>>               OR       2.5 %     97.5 %
>>> X1  1.67639337 1.352583976 2.09856514
>>> X2  1.23377720 1.071959330 1.42496949
>>> X3A 0.01157565 0.001429430 0.08726854
>>> X3B 0.06627849 0.008011818 0.54419759
>>> X3C 0.01118084 0.001339984 0.08721028
>>> X3D 0.01254032 0.001545240 0.09539880
>>> X3E 0.10654454 0.013141540 0.87369972
>>>    but i am wondering i want to extract OR and  CI only for factors, My
>>> desire out put will be
>>>    OR       2.5 %     97.5 %
>>> X3A 0.01157565 0.001429430 0.08726854
>>> X3B 0.06627849 0.008011818 0.54419759
>>> X3C 0.01118084 0.001339984 0.08721028
>>> X3D 0.01254032 0.001545240 0.09539880
>>> X3E 0.10654454 0.013141540 0.87369972
>>> Can any one help me the code to extact it?
>>> additionally I want to plot the above OR with confidence interval for
>>> the extracted one
>>> can you also help me the code with plot,or box plot
>>> Kind Regards,
>>> Hana
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Wed Jun 15 12:02:12 2022
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Wed, 15 Jun 2022 12:02:12 +0200
Subject: [R] How do I "teach" R that columns 1, 2,
 and 3 are the Year-Month-Day
In-Reply-To: <BN6PR2201MB155373731CCFC48AB0C7A8C3CFAD9@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <47096049.765872.1653053240121.ref@mail.yahoo.com>
 <47096049.765872.1653053240121@mail.yahoo.com>
 <c2251d5b-ae25-0292-8fe4-61e4768becc8@ign.fr>
 <55CD3E40-FDAE-4A6E-B352-B60D1D98A236@me.com>
 <1229027594.2224595.1655258390324@mail.yahoo.com>
 <BN6PR2201MB155373731CCFC48AB0C7A8C3CFAD9@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <25257.44580.312226.669632@stat.math.ethz.ch>

>>>>> Ebert,Timothy Aaron 
>>>>>     on Wed, 15 Jun 2022 02:01:44 +0000 writes:

    > You don't, or not exactly. You could make the three
    > columns strings, then add them together. I would then use
    > the lubridate package to convert to a date.  The first
    > four lines of code are just getting a bit of data into
    > R. The lubridate package will also work with date-time
    > objects if you have time.

    > year<-rep("2021",4) 
    > month<-c(7,8,9,11)
    > day<-c(25,27,26,4)
    > fuel<-c(50.45, 61.48,59.07,55.40)
    > df<-data.frame(year,month,day,fuel)
    > df$day <-as.character(df$day)
    > df$month<-as.character(df$month)
    > df$date <- paste(year,"/",month,"/",day)
    > library(lubridate)
    > df$date<-ymd(df$date)

    > Regards,
    > Tim

Thank you, but
Jeff Newmiller's answer which was much shorter *and* only
used base R  instead of an extra package was much more convincing to me.

Martin

    > -----Original Message-----
    > From: R-help <r-help-bounces at r-project.org> On Behalf Of Avi Gross via R-help
    > Sent: Tuesday, June 14, 2022 10:00 PM
    > To: r-help at r-project.org
    > Cc: r-help at r-project.org
    > Subject: Re: [R] How do I "teach" R that columns 1, 2, and 3 are the Year-Month-Day

    > [External Email]

    > Greg,
    > It looks like you mean to have a data.frame containing the year as a column then Month, Day and something called Fuel.
    > You only made a vector called Fuel, with no name as part of it and plotted it.
    > So you did not expect any names, I assume.
    > You probably want to combine the first three columns into holding some kind of date object with something like as.Date("2021-07-25") for each row and make a vector of those. Your data.frame might then contain two columns called Date and Fuel and you might use whatever plotting method to put Fuel on the Y axis and dates on the X axis or whatever makes sense. Plotting four independent columns will not work here.
    > And note the NAMES go in with something like:
    > mydata <- data.frame(Date=dates, Fuel=Fuel) I have not done all the work but hopefully this is helpful.

    > -----Original Message-----
    > From: Gregory Coats via R-help <r-help at r-project.org>
    > To: Greg Comcast Coats <gregcoats at me.com>
    > Cc: Marc Schwartz via R-help <r-help at r-project.org>
    > Sent: Tue, Jun 14, 2022 9:08 pm
    > Subject: [R] How do I "teach" R that columns 1, 2, and 3 are the Year-Month-Day

    > # Column 1 is the Year
    > # Column 2 is the Month
    > # Column 3 is the Day
    > # Column 4 is the Fuel
    > Fuel <- c(50.45, 61.48, 59.07, 55.40, 30.63, 41.35, 32.81, 49.86, 62.99, 89.37) plot (Fuel)

    > 2021  7 25  50.45
    > 2021  8 27  61.48
    > 2021  9 26  59.07
    > 2021 11  4  55.40
    > 2021 11 22  30.63
    > 2021 11 26  41.35
    > 2021 12  6  32.81
    > 2022  1 14  49.86
    > 2022  4 29  62.99
    > 2022  6 11  89.37

    > How do I "teach" R that columns 1, 2, and 3 are the Year-Month-Day?
    > Greg Coats
    > [[alternative HTML version deleted]]

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=5mwGPy3wD6UgdaNamboKRB8Oo4Yd9OIBPIhrRkniM0N0wUSsekHwK4HbHytbdscH&s=3iFRCZeUCCgixeFjZsrmkHARgeHp8CLb7dIAk7EHiSY&e=
    > PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=5mwGPy3wD6UgdaNamboKRB8Oo4Yd9OIBPIhrRkniM0N0wUSsekHwK4HbHytbdscH&s=BG0mynWESmDXi-6Z2LX7yRr51Gqw3B-Gzfg3Ye2cy8M&e=
    > and provide commented, minimal, self-contained, reproducible code.

    > [[alternative HTML version deleted]]

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=5mwGPy3wD6UgdaNamboKRB8Oo4Yd9OIBPIhrRkniM0N0wUSsekHwK4HbHytbdscH&s=3iFRCZeUCCgixeFjZsrmkHARgeHp8CLb7dIAk7EHiSY&e=
    > PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=5mwGPy3wD6UgdaNamboKRB8Oo4Yd9OIBPIhrRkniM0N0wUSsekHwK4HbHytbdscH&s=BG0mynWESmDXi-6Z2LX7yRr51Gqw3B-Gzfg3Ye2cy8M&e=
    > and provide commented, minimal, self-contained, reproducible code.
    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From h@n@tezer@ @end|ng |rom gm@||@com  Wed Jun 15 13:28:12 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Wed, 15 Jun 2022 14:28:12 +0300
Subject: [R] 
 Extracting only some coefficients for the logistic regression
 model and its plot
In-Reply-To: <b6a48273-b64d-4a5e-5047-6b32bdcb946d@sapo.pt>
References: <CAEuJ0VCm3qmRayAoSRYYKbkTYYS8oOnQGubEQkgHm8aRZXNi8w@mail.gmail.com>
 <f2a9f13a-51ac-a1a2-db8d-5030d85793cd@sapo.pt>
 <CAEuJ0VD4vL=NjPjodod9FnH5shaqP0eos+ULkoFkZAbr8U1UQQ@mail.gmail.com>
 <b6a48273-b64d-4a5e-5047-6b32bdcb946d@sapo.pt>
Message-ID: <CAEuJ0VDYcsMbpjqRbEyJhv1JzQiNsHBXFEpp7SWAxeigNrwbDg@mail.gmail.com>

Dear Rui, Thanks it works!
Best,
Hana
On 6/15/22, Rui Barradas <ruipbarradas at sapo.pt> wrote:
> Hello,
>
> With ggplot it's easy, add color = id and coord_flip().
>
>
> ggplot(ORCI, aes(id, OR, color = id)) +
>    geom_point() +
>    geom_errorbar(aes(ymin = `2.5 %`, max = `97.5 %`)) +
>    coord_flip() +
>    theme_bw()
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 08:15 de 15/06/2022, anteneh asmare escreveu:
>> Dear Rui,  thanks a lot, dose  it possible to have the horizontal line
>> for scale OR value on Y axis and different color for entire box plots
>> ?
>> Best,
>> Hana
>> On 6/15/22, Rui Barradas <ruipbarradas at sapo.pt> wrote:
>>> Hello,
>>>
>>> To extract all but the first 2 rows, use a negative index on the rows.
>>> I will also coerce to data.frame and add a id column, it will be needed
>>> to plot the confidence intervals.
>>>
>>>
>>> ORCI <- exp(cbind(OR = coef(model ), confint(model )))[-(1:2), ]
>>> ORCI <- cbind.data.frame(ORCI, id = row.names(ORCI))
>>>
>>>
>>> Now the base R and ggplot plots. In both cases plot the bars first, then
>>> the points.
>>>
>>> 1. Base R
>>>
>>>
>>> ymin <- min(apply(ORCI[2:3], 1, range)[1,])
>>> ymax <- max(apply(ORCI[2:3], 1, range)[2,])
>>>
>>> plot((ymin + ymax)/2,
>>>        type = "n",
>>>        xaxt = "n",
>>>        xlim = c(0.5, 5.5),
>>>        ylim = c(ymin, ymax),
>>>        xlab = "X3",
>>>        ylab = "Odds Ratio")
>>> with(ORCI, arrows(x0 = seq_along(id),
>>>                     y0 = `2.5 %`,
>>>                     y1 = `97.5 %`,
>>>                     code = 3,
>>>                     angle = 90))
>>> points(OR ~ seq_along(id), ORCI, pch = 16)
>>> axis(1, at = seq_along(ORCI$id), labels = ORCI$id)
>>>
>>>
>>>
>>> 2. Package ggplot2
>>>
>>>
>>> library(ggplot2)
>>>
>>> ggplot(ORCI, aes(id, OR)) +
>>>     geom_errorbar(aes(ymin = `2.5 %`, max = `97.5 %`)) +
>>>     geom_point() +
>>>     theme_bw()
>>>
>>>
>>> Hope this helps,
>>>
>>> Rui Barradas
>>>
>>> ?s 21:01 de 14/06/2022, anteneh asmare escreveu:
>>>> sample_data =
>>>> read.table("http://freakonometrics.free.fr/db.txt",header=TRUE,sep=";")
>>>> head(sample_data)
>>>> model = glm(Y~0+X1+X2+X3,family=binomial,data=sample_data)
>>>> summary(model)
>>>> exp(coef(model ))
>>>> exp(cbind(OR = coef(model ), confint(model )))
>>>> I have the aove sample data on logistic regression with categorical
>>>> predictor
>>>> I try the above code i get the follwing  out put,
>>>>               OR       2.5 %     97.5 %
>>>> X1  1.67639337 1.352583976 2.09856514
>>>> X2  1.23377720 1.071959330 1.42496949
>>>> X3A 0.01157565 0.001429430 0.08726854
>>>> X3B 0.06627849 0.008011818 0.54419759
>>>> X3C 0.01118084 0.001339984 0.08721028
>>>> X3D 0.01254032 0.001545240 0.09539880
>>>> X3E 0.10654454 0.013141540 0.87369972
>>>>    but i am wondering i want to extract OR and  CI only for factors, My
>>>> desire out put will be
>>>>    OR       2.5 %     97.5 %
>>>> X3A 0.01157565 0.001429430 0.08726854
>>>> X3B 0.06627849 0.008011818 0.54419759
>>>> X3C 0.01118084 0.001339984 0.08721028
>>>> X3D 0.01254032 0.001545240 0.09539880
>>>> X3E 0.10654454 0.013141540 0.87369972
>>>> Can any one help me the code to extact it?
>>>> additionally I want to plot the above OR with confidence interval for
>>>> the extracted one
>>>> can you also help me the code with plot,or box plot
>>>> Kind Regards,
>>>> Hana
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>


From h@n@tezer@ @end|ng |rom gm@||@com  Wed Jun 15 13:29:28 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Wed, 15 Jun 2022 14:29:28 +0300
Subject: [R] Model Comparision for case control studies in R
Message-ID: <CAEuJ0VAtsC3nHkUOiwfv7d8hA6N8q3DozStcX6=O+1juOs4fhA@mail.gmail.com>

y<-c(0,1,1,0,0,1,0,0,1,1,1,0,1,1,1,0,0,0,0,1)
age<-c(45,23,56,67,23,23,28,56,45,47,36,37,33,35,38,39,43,28,39,41)
smoking<-c(0,1,1,1,0,0,0,0,0,1,1,0,0,1,0,1,1,1,0,1)
hypertension<-c(1,1,0,1,0,1,0,1,1,0,1,1,1,1,1,1,0,0,1,0)
data<-data.frame(y,age,smoking,hypertension)
data
model<-glm(y~age+factor(smoking)+factor(hypertension), data, family =
binomial(link = "logit"),na.action = na.omit)
summary(model)
from above sample data I want to study a case-control study on male
individuals with my response variable y, disease status (1=Case,
0=Control) with covariates age, smoking status(1=Yes, 0=No)  and
hypertension, hypertensive (1=Yes, 0=No). I want to fit the model to
predict the disease status using at least two different methods. And
to make model comparisons. I think logistic regression will be the
best fit for this case control study. Do we have other options in
addition to logistic regression? My objective is to fit the model to
predict the disease status using at least two different methods.
Kind regards,
Hana


From tebert @end|ng |rom u||@edu  Wed Jun 15 15:46:54 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Wed, 15 Jun 2022 13:46:54 +0000
Subject: [R] Model Comparision for case control studies in R
In-Reply-To: <CAEuJ0VAtsC3nHkUOiwfv7d8hA6N8q3DozStcX6=O+1juOs4fhA@mail.gmail.com>
References: <CAEuJ0VAtsC3nHkUOiwfv7d8hA6N8q3DozStcX6=O+1juOs4fhA@mail.gmail.com>
Message-ID: <BN6PR2201MB15539B16281BDD689C99EA01CFAD9@BN6PR2201MB1553.namprd22.prod.outlook.com>

Disease status is missing from the sample data.
Are age, disease, smoking, and/or hypertension correlated in any way or are they independent (correlation=0)?
Are the correlations large enough to adversely influence your model?
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of anteneh asmare
Sent: Wednesday, June 15, 2022 7:29 AM
To: r-help at r-project.org
Subject: [R] Model Comparision for case control studies in R

[External Email]

y<-c(0,1,1,0,0,1,0,0,1,1,1,0,1,1,1,0,0,0,0,1)
age<-c(45,23,56,67,23,23,28,56,45,47,36,37,33,35,38,39,43,28,39,41)
smoking<-c(0,1,1,1,0,0,0,0,0,1,1,0,0,1,0,1,1,1,0,1)
hypertension<-c(1,1,0,1,0,1,0,1,1,0,1,1,1,1,1,1,0,0,1,0)
data<-data.frame(y,age,smoking,hypertension)
data
model<-glm(y~age+factor(smoking)+factor(hypertension), data, family = binomial(link = "logit"),na.action = na.omit)
summary(model)
from above sample data I want to study a case-control study on male individuals with my response variable y, disease status (1=Case,
0=Control) with covariates age, smoking status(1=Yes, 0=No)  and hypertension, hypertensive (1=Yes, 0=No). I want to fit the model to predict the disease status using at least two different methods. And to make model comparisons. I think logistic regression will be the best fit for this case control study. Do we have other options in addition to logistic regression? My objective is to fit the model to predict the disease status using at least two different methods.
Kind regards,
Hana

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=l7afPQ_gGAoV2EsNoYSYul0qAISEiXLmTmu0IQ03nZO4rcAi9xHZGsWwwig4oYOB&s=ztyDthknydhlcM49F33Gz6xRl6G7U9s8aIhB1VN-EKY&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=l7afPQ_gGAoV2EsNoYSYul0qAISEiXLmTmu0IQ03nZO4rcAi9xHZGsWwwig4oYOB&s=tcsGkhvtVvoVvb1Ehah-vLRC6an40rJXQXqqfX2f0gI&e=
and provide commented, minimal, self-contained, reproducible code.


From kry|ov@r00t @end|ng |rom gm@||@com  Wed Jun 15 16:09:50 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Wed, 15 Jun 2022 17:09:50 +0300
Subject: [R] Install OpenCL
In-Reply-To: <2eb52eed-bf9d-04e0-cc22-238e6edcae00@gmx.de>
References: <1e3ff528-1d77-6a66-3184-0f94ce826f62@gmx.de>
 <20220602184655.1c840903@arachnoid>
 <5e9ca34f-9569-0b7d-e070-d7036f7801b2@gmx.de>
 <20220614134309.0b390d14@arachnoid>
 <3702aa6c-5d0e-c76a-c06f-6a870265d03c@gmx.de>
 <20220614165957.73ff239f@arachnoid>
 <20220614170813.68f188d9@arachnoid>
 <fec5dfaa-1529-2107-1d60-87d9be153ed2@gmx.de>
 <20220614173701.4ed0e300@arachnoid>
 <2eb52eed-bf9d-04e0-cc22-238e6edcae00@gmx.de>
Message-ID: <20220615170950.7a9042d3@arachnoid>

On Wed, 15 Jun 2022 08:54:43 +0200
Quirin Stier <Quirin_Stier at gmx.de> wrote:

> Nota: NVIDIA GPU Computing Toolkit does not contain any OpenCL.dll,
> only the OpenCL.h (required for the includes) and a OpenCL.lib (maybe
> not important)

Part of the problem seems to be that there are two files named
OpenCL.dll, with one of them (the package DLL built from the sources
available from CRAN) depending on the other (the DLL containing OpenCL
functions for the package to call).

Have you tried the workaround from
https://github.com/s-u/OpenCL/issues/6#issuecomment-919990137 ? This
person is also running NVIDIA GPU Computing Toolkit of a similar
version on Windows.

-- 
Best regards,
Ivan


From g@@@powe|| @end|ng |rom protonm@||@com  Wed Jun 15 16:57:37 2022
From: g@@@powe|| @end|ng |rom protonm@||@com (Gregg Powell)
Date: Wed, 15 Jun 2022 14:57:37 +0000
Subject: [R] Is there a package that can do Fuzzy name matching to
 standardize names in a single column
Message-ID: <iv_SjOdZQNvgeBosgdd2MIiLeiIBtIW9cM2amgAQj90Xe-4lpUoTqZ9ii9orPrVjkvoWnJrFgsOOOrx2tCnjr-gZPFbzD_WfqLPv73-gO2g=@protonmail.com>

Have data sets where there are names, in the first column, client names in the second, and Client start date in the third.?

There are thousands of these records with thousands of names/clients/client start dates. The name is entered each time the person begins with a new client such that each person has many entries in the name column. Often the names were not entered in a consistent way. With and without middle initial, middle name, or various abbreviations such as ",RN" at the end of the name.

Is there a package that can do fuzzy name matching so that the names in name column get replaced with a "standardized" format - where some type of machine learning can pick the most common spelling of each repeat name and replace the different variations with the common spelling?

I included an example below. First table includes the names with the various spellings. Second table depicts what I hope to achieve.

Again - this is on a large scale - there are something like 10,000 records with names that need to be standardized.


Name

Client

Client Start Date

John Good

Client 1

1/1/2020

Joe Jackson

Client 2

6/1/2020

Bob A. Barker

Client 3

8/1/2020

John B. Good

Client 4

10/1/2020

Joe J. Jackson

Client 5

12/1/2020

Bob Allen Barker

Client 6

1/1/2021

John Good

Client 7

5/1/2021

Joe Jack Jackson

Client 8

8/1/2021

Bob Barker

Client 9

12/1/2021

?

?

?

Name

Client

Client Start Date

John Good

Client 1

1/1/2020

Joe J. Jackson

Client 2

6/1/2020

Bob A. Barker

Client 3

8/1/2020

John Good

Client 4

10/1/2020

Joe J. Jackson

Client 5

12/1/2020

Bob A. Barker

Client 6

1/1/2021

John Good

Client 7

5/1/2021

Joe J. Jackson

Client 8

8/1/2021

Bob A. Barker

Client 9

12/1/2021



THANKS!

Gregg Powell

Arizona, USA
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 509 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220615/86c9cb17/attachment.sig>

From @@h|mk@poor @end|ng |rom gm@||@com  Wed Jun 15 17:04:52 2022
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Wed, 15 Jun 2022 20:34:52 +0530
Subject: [R] Is there a package that can do Fuzzy name matching to
 standardize names in a single column
In-Reply-To: <iv_SjOdZQNvgeBosgdd2MIiLeiIBtIW9cM2amgAQj90Xe-4lpUoTqZ9ii9orPrVjkvoWnJrFgsOOOrx2tCnjr-gZPFbzD_WfqLPv73-gO2g=@protonmail.com>
References: <iv_SjOdZQNvgeBosgdd2MIiLeiIBtIW9cM2amgAQj90Xe-4lpUoTqZ9ii9orPrVjkvoWnJrFgsOOOrx2tCnjr-gZPFbzD_WfqLPv73-gO2g=@protonmail.com>
Message-ID: <CAC8=1eqd6XbxtaQcqsa5tpg-VVce0yjXqwEhSpbEn1b+c_qwNg@mail.gmail.com>

Dear Gregg,

Check this out:

library(fuzzyjoin)
?stringdist_left_join

Best Regards,
Ashim

On Wed, Jun 15, 2022 at 8:28 PM Gregg Powell via R-help
<r-help at r-project.org> wrote:
>
> Have data sets where there are names, in the first column, client names in the second, and Client start date in the third.
>
> There are thousands of these records with thousands of names/clients/client start dates. The name is entered each time the person begins with a new client such that each person has many entries in the name column. Often the names were not entered in a consistent way. With and without middle initial, middle name, or various abbreviations such as ",RN" at the end of the name.
>
> Is there a package that can do fuzzy name matching so that the names in name column get replaced with a "standardized" format - where some type of machine learning can pick the most common spelling of each repeat name and replace the different variations with the common spelling?
>
> I included an example below. First table includes the names with the various spellings. Second table depicts what I hope to achieve.
>
> Again - this is on a large scale - there are something like 10,000 records with names that need to be standardized.
>
>
> Name
>
> Client
>
> Client Start Date
>
> John Good
>
> Client 1
>
> 1/1/2020
>
> Joe Jackson
>
> Client 2
>
> 6/1/2020
>
> Bob A. Barker
>
> Client 3
>
> 8/1/2020
>
> John B. Good
>
> Client 4
>
> 10/1/2020
>
> Joe J. Jackson
>
> Client 5
>
> 12/1/2020
>
> Bob Allen Barker
>
> Client 6
>
> 1/1/2021
>
> John Good
>
> Client 7
>
> 5/1/2021
>
> Joe Jack Jackson
>
> Client 8
>
> 8/1/2021
>
> Bob Barker
>
> Client 9
>
> 12/1/2021
>
>
>
>
>
>
>
> Name
>
> Client
>
> Client Start Date
>
> John Good
>
> Client 1
>
> 1/1/2020
>
> Joe J. Jackson
>
> Client 2
>
> 6/1/2020
>
> Bob A. Barker
>
> Client 3
>
> 8/1/2020
>
> John Good
>
> Client 4
>
> 10/1/2020
>
> Joe J. Jackson
>
> Client 5
>
> 12/1/2020
>
> Bob A. Barker
>
> Client 6
>
> 1/1/2021
>
> John Good
>
> Client 7
>
> 5/1/2021
>
> Joe J. Jackson
>
> Client 8
>
> 8/1/2021
>
> Bob A. Barker
>
> Client 9
>
> 12/1/2021
>
>
>
> THANKS!
>
> Gregg Powell
>
> Arizona, USA______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Wed Jun 15 17:38:23 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 15 Jun 2022 08:38:23 -0700
Subject: [R] Is there a package that can do Fuzzy name matching to
 standardize names in a single column
In-Reply-To: <iv_SjOdZQNvgeBosgdd2MIiLeiIBtIW9cM2amgAQj90Xe-4lpUoTqZ9ii9orPrVjkvoWnJrFgsOOOrx2tCnjr-gZPFbzD_WfqLPv73-gO2g=@protonmail.com>
References: <iv_SjOdZQNvgeBosgdd2MIiLeiIBtIW9cM2amgAQj90Xe-4lpUoTqZ9ii9orPrVjkvoWnJrFgsOOOrx2tCnjr-gZPFbzD_WfqLPv73-gO2g=@protonmail.com>
Message-ID: <CAGxFJbSPrTA2Rw7udeuiTNnUh3+dEWhY=8j0g3CKLVyCFr=cyQ@mail.gmail.com>

As these are English names and appear to be present always as **first
?? last** (you didn't specify but that's how your example shows it),
maybe something like the following might be a start:

1. Use strsplit() to split the names into their constituent parts.
2. Find the last *meaningful* part in each vector (e.g. Joe Smith Jr.
should exclude Jr. and choose Smith)
3. Split the names into the groups of identical unique last parts
4. Split each of the groups of last names into subgroups based on the
first one or more letters of first name so that, e.g. Joe and Joseph
would be in the same subgroup of Smith. Of course Joe and John would
be also, so you see the problem...

Other Issues:
Are Joe Smith and Joe Smith Jr. the same person?
Misspellings? Typos?  Is Arlene Smith the same as Alene Smith?

Some sort of clustering of the names might also be appropriate. See
https://cran.r-project.org/web/views/Cluster.html  for ideas.

Cheers,
Bert

On Wed, Jun 15, 2022 at 7:58 AM Gregg Powell via R-help
<r-help at r-project.org> wrote:
>
> Have data sets where there are names, in the first column, client names in the second, and Client start date in the third.
>
> There are thousands of these records with thousands of names/clients/client start dates. The name is entered each time the person begins with a new client such that each person has many entries in the name column. Often the names were not entered in a consistent way. With and without middle initial, middle name, or various abbreviations such as ",RN" at the end of the name.
>
> Is there a package that can do fuzzy name matching so that the names in name column get replaced with a "standardized" format - where some type of machine learning can pick the most common spelling of each repeat name and replace the different variations with the common spelling?
>
> I included an example below. First table includes the names with the various spellings. Second table depicts what I hope to achieve.
>
> Again - this is on a large scale - there are something like 10,000 records with names that need to be standardized.
>
>
> Name
>
> Client
>
> Client Start Date
>
> John Good
>
> Client 1
>
> 1/1/2020
>
> Joe Jackson
>
> Client 2
>
> 6/1/2020
>
> Bob A. Barker
>
> Client 3
>
> 8/1/2020
>
> John B. Good
>
> Client 4
>
> 10/1/2020
>
> Joe J. Jackson
>
> Client 5
>
> 12/1/2020
>
> Bob Allen Barker
>
> Client 6
>
> 1/1/2021
>
> John Good
>
> Client 7
>
> 5/1/2021
>
> Joe Jack Jackson
>
> Client 8
>
> 8/1/2021
>
> Bob Barker
>
> Client 9
>
> 12/1/2021
>
>
>
>
>
>
>
> Name
>
> Client
>
> Client Start Date
>
> John Good
>
> Client 1
>
> 1/1/2020
>
> Joe J. Jackson
>
> Client 2
>
> 6/1/2020
>
> Bob A. Barker
>
> Client 3
>
> 8/1/2020
>
> John Good
>
> Client 4
>
> 10/1/2020
>
> Joe J. Jackson
>
> Client 5
>
> 12/1/2020
>
> Bob A. Barker
>
> Client 6
>
> 1/1/2021
>
> John Good
>
> Client 7
>
> 5/1/2021
>
> Joe J. Jackson
>
> Client 8
>
> 8/1/2021
>
> Bob A. Barker
>
> Client 9
>
> 12/1/2021
>
>
>
> THANKS!
>
> Gregg Powell
>
> Arizona, USA______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From g@@@powe|| @end|ng |rom protonm@||@com  Wed Jun 15 17:43:14 2022
From: g@@@powe|| @end|ng |rom protonm@||@com (Gregg Powell)
Date: Wed, 15 Jun 2022 15:43:14 +0000
Subject: [R] Is there a package that can do Fuzzy name matching to
 standardize names in a single column
In-Reply-To: <CAC8=1eqd6XbxtaQcqsa5tpg-VVce0yjXqwEhSpbEn1b+c_qwNg@mail.gmail.com>
References: <iv_SjOdZQNvgeBosgdd2MIiLeiIBtIW9cM2amgAQj90Xe-4lpUoTqZ9ii9orPrVjkvoWnJrFgsOOOrx2tCnjr-gZPFbzD_WfqLPv73-gO2g=@protonmail.com>
 <CAC8=1eqd6XbxtaQcqsa5tpg-VVce0yjXqwEhSpbEn1b+c_qwNg@mail.gmail.com>
Message-ID: <0uDyUBwhwANlV2-TpB5S6FJ0331LSmlqPfItpOP8a_I-UCnOAXL-bPzUvFcrFvSLV157uzjHjhg_gIA2sn7GaeU2dsmld918i0THNm7m8U0=@protonmail.com>


Hello Ashim and kind regards for you taking the time to answer back.


> library(fuzzyjoin)
> ?stringdist_left_join

-this will join two tables, but what I am trying to do is just standardize the similarly spelled duplicate names in just the first column of a single table.

I don't think fuzzyjoin will help me in that regard.

Thanks.
Gregg
Arizona, USA

------- Original Message -------
On Wednesday, June 15th, 2022 at 8:04 AM, Ashim Kapoor <ashimkapoor at gmail.com> wrote:


> 

> 

> Dear Gregg,
> 

> Check this out:
> 

> library(fuzzyjoin)
> ?stringdist_left_join
> 

> Best Regards,
> Ashim
> 

> On Wed, Jun 15, 2022 at 8:28 PM Gregg Powell via R-help
> r-help at r-project.org wrote:
> 

> > Have data sets where there are names, in the first column, client names in the second, and Client start date in the third.
> > 

> > There are thousands of these records with thousands of names/clients/client start dates. The name is entered each time the person begins with a new client such that each person has many entries in the name column. Often the names were not entered in a consistent way. With and without middle initial, middle name, or various abbreviations such as ",RN" at the end of the name.
> > 

> > Is there a package that can do fuzzy name matching so that the names in name column get replaced with a "standardized" format - where some type of machine learning can pick the most common spelling of each repeat name and replace the different variations with the common spelling?
> > 

> > I included an example below. First table includes the names with the various spellings. Second table depicts what I hope to achieve.
> > 

> > Again - this is on a large scale - there are something like 10,000 records with names that need to be standardized.
> > 

> > Name
> > 

> > Client
> > 

> > Client Start Date
> > 

> > John Good
> > 

> > Client 1
> > 

> > 1/1/2020
> > 

> > Joe Jackson
> > 

> > Client 2
> > 

> > 6/1/2020
> > 

> > Bob A. Barker
> > 

> > Client 3
> > 

> > 8/1/2020
> > 

> > John B. Good
> > 

> > Client 4
> > 

> > 10/1/2020
> > 

> > Joe J. Jackson
> > 

> > Client 5
> > 

> > 12/1/2020
> > 

> > Bob Allen Barker
> > 

> > Client 6
> > 

> > 1/1/2021
> > 

> > John Good
> > 

> > Client 7
> > 

> > 5/1/2021
> > 

> > Joe Jack Jackson
> > 

> > Client 8
> > 

> > 8/1/2021
> > 

> > Bob Barker
> > 

> > Client 9
> > 

> > 12/1/2021
> > 

> > Name
> > 

> > Client
> > 

> > Client Start Date
> > 

> > John Good
> > 

> > Client 1
> > 

> > 1/1/2020
> > 

> > Joe J. Jackson
> > 

> > Client 2
> > 

> > 6/1/2020
> > 

> > Bob A. Barker
> > 

> > Client 3
> > 

> > 8/1/2020
> > 

> > John Good
> > 

> > Client 4
> > 

> > 10/1/2020
> > 

> > Joe J. Jackson
> > 

> > Client 5
> > 

> > 12/1/2020
> > 

> > Bob A. Barker
> > 

> > Client 6
> > 

> > 1/1/2021
> > 

> > John Good
> > 

> > Client 7
> > 

> > 5/1/2021
> > 

> > Joe J. Jackson
> > 

> > Client 8
> > 

> > 8/1/2021
> > 

> > Bob A. Barker
> > 

> > Client 9
> > 

> > 12/1/2021
> > 

> > THANKS!
> > 

> > Gregg Powell
> > 

> > Arizona, USA______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 509 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220615/940a006c/attachment.sig>

From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Jun 15 18:24:23 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 15 Jun 2022 09:24:23 -0700
Subject: [R] Is there a package that can do Fuzzy name matching to
 standardize names in a single column
In-Reply-To: <0uDyUBwhwANlV2-TpB5S6FJ0331LSmlqPfItpOP8a_I-UCnOAXL-bPzUvFcrFvSLV157uzjHjhg_gIA2sn7GaeU2dsmld918i0THNm7m8U0=@protonmail.com>
References: <iv_SjOdZQNvgeBosgdd2MIiLeiIBtIW9cM2amgAQj90Xe-4lpUoTqZ9ii9orPrVjkvoWnJrFgsOOOrx2tCnjr-gZPFbzD_WfqLPv73-gO2g=@protonmail.com>
 <CAC8=1eqd6XbxtaQcqsa5tpg-VVce0yjXqwEhSpbEn1b+c_qwNg@mail.gmail.com>
 <0uDyUBwhwANlV2-TpB5S6FJ0331LSmlqPfItpOP8a_I-UCnOAXL-bPzUvFcrFvSLV157uzjHjhg_gIA2sn7GaeU2dsmld918i0THNm7m8U0=@protonmail.com>
Message-ID: <3648F43F-7398-4353-8B4E-499830FA5695@dcn.davis.ca.us>

This is an intractable problem... you cannot know that "John Good" is the same person as "John B. Good"... and even when you augment their identity with information like which town they are in or what the first name of their spouse is you could be mislead by such information in multiple ways.

Most of the time such lists are managed by creating an internal "unique id" for each person. But since there is no definitive way to do this, it is handled as an ongoing process, with multiple algorithms applied with or without human supervision and always with some risk of increasing the error rate by any specific algorithm than was originally present in the data.

One common approach is using regular expressions or approximate match algorithms like base::agrep to filter possible like identities for people to check into further.

Another approach could be a clustering algorithm as Bert suggested. It has been awhile since I had to do this kind of work... I suppose neural nets might also be applied to this problem these days. But an agrep pre-filtering human augmentation should not be discounted... 10k entries is not _that_ many.

On June 15, 2022 8:43:14 AM PDT, Gregg Powell via R-help <r-help at r-project.org> wrote:
>
>Hello Ashim and kind regards for you taking the time to answer back.
>
>
>> library(fuzzyjoin)
>> ?stringdist_left_join
>
>-this will join two tables, but what I am trying to do is just standardize the similarly spelled duplicate names in just the first column of a single table.
>
>I don't think fuzzyjoin will help me in that regard.
>
>Thanks.
>Gregg
>Arizona, USA
>
>------- Original Message -------
>On Wednesday, June 15th, 2022 at 8:04 AM, Ashim Kapoor <ashimkapoor at gmail.com> wrote:
>
>
>> 
>
>> 
>
>> Dear Gregg,
>> 
>
>> Check this out:
>> 
>
>> library(fuzzyjoin)
>> ?stringdist_left_join
>> 
>
>> Best Regards,
>> Ashim
>> 
>
>> On Wed, Jun 15, 2022 at 8:28 PM Gregg Powell via R-help
>> r-help at r-project.org wrote:
>> 
>
>> > Have data sets where there are names, in the first column, client names in the second, and Client start date in the third.
>> > 
>
>> > There are thousands of these records with thousands of names/clients/client start dates. The name is entered each time the person begins with a new client such that each person has many entries in the name column. Often the names were not entered in a consistent way. With and without middle initial, middle name, or various abbreviations such as ",RN" at the end of the name.
>> > 
>
>> > Is there a package that can do fuzzy name matching so that the names in name column get replaced with a "standardized" format - where some type of machine learning can pick the most common spelling of each repeat name and replace the different variations with the common spelling?
>> > 
>
>> > I included an example below. First table includes the names with the various spellings. Second table depicts what I hope to achieve.
>> > 
>
>> > Again - this is on a large scale - there are something like 10,000 records with names that need to be standardized.
>> > 
>
>> > Name
>> > 
>
>> > Client
>> > 
>
>> > Client Start Date
>> > 
>
>> > John Good
>> > 
>
>> > Client 1
>> > 
>
>> > 1/1/2020
>> > 
>
>> > Joe Jackson
>> > 
>
>> > Client 2
>> > 
>
>> > 6/1/2020
>> > 
>
>> > Bob A. Barker
>> > 
>
>> > Client 3
>> > 
>
>> > 8/1/2020
>> > 
>
>> > John B. Good
>> > 
>
>> > Client 4
>> > 
>
>> > 10/1/2020
>> > 
>
>> > Joe J. Jackson
>> > 
>
>> > Client 5
>> > 
>
>> > 12/1/2020
>> > 
>
>> > Bob Allen Barker
>> > 
>
>> > Client 6
>> > 
>
>> > 1/1/2021
>> > 
>
>> > John Good
>> > 
>
>> > Client 7
>> > 
>
>> > 5/1/2021
>> > 
>
>> > Joe Jack Jackson
>> > 
>
>> > Client 8
>> > 
>
>> > 8/1/2021
>> > 
>
>> > Bob Barker
>> > 
>
>> > Client 9
>> > 
>
>> > 12/1/2021
>> > 
>
>> > Name
>> > 
>
>> > Client
>> > 
>
>> > Client Start Date
>> > 
>
>> > John Good
>> > 
>
>> > Client 1
>> > 
>
>> > 1/1/2020
>> > 
>
>> > Joe J. Jackson
>> > 
>
>> > Client 2
>> > 
>
>> > 6/1/2020
>> > 
>
>> > Bob A. Barker
>> > 
>
>> > Client 3
>> > 
>
>> > 8/1/2020
>> > 
>
>> > John Good
>> > 
>
>> > Client 4
>> > 
>
>> > 10/1/2020
>> > 
>
>> > Joe J. Jackson
>> > 
>
>> > Client 5
>> > 
>
>> > 12/1/2020
>> > 
>
>> > Bob A. Barker
>> > 
>
>> > Client 6
>> > 
>
>> > 1/1/2021
>> > 
>
>> > John Good
>> > 
>
>> > Client 7
>> > 
>
>> > 5/1/2021
>> > 
>
>> > Joe J. Jackson
>> > 
>
>> > Client 8
>> > 
>
>> > 8/1/2021
>> > 
>
>> > Bob A. Barker
>> > 
>
>> > Client 9
>> > 
>
>> > 12/1/2021
>> > 
>
>> > THANKS!
>> > 
>
>> > Gregg Powell
>> > 
>
>> > Arizona, USA______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
-- 
Sent from my phone. Please excuse my brevity.


From chr|@ho|d @end|ng |rom p@yctc@org  Wed Jun 15 18:39:12 2022
From: chr|@ho|d @end|ng |rom p@yctc@org (Chris Evans)
Date: Wed, 15 Jun 2022 16:39:12 +0000 (UTC)
Subject: [R] Is there a package that can do Fuzzy name matching to
 standardize names in a single column
In-Reply-To: <0uDyUBwhwANlV2-TpB5S6FJ0331LSmlqPfItpOP8a_I-UCnOAXL-bPzUvFcrFvSLV157uzjHjhg_gIA2sn7GaeU2dsmld918i0THNm7m8U0=@protonmail.com>
References: <iv_SjOdZQNvgeBosgdd2MIiLeiIBtIW9cM2amgAQj90Xe-4lpUoTqZ9ii9orPrVjkvoWnJrFgsOOOrx2tCnjr-gZPFbzD_WfqLPv73-gO2g=@protonmail.com>
 <CAC8=1eqd6XbxtaQcqsa5tpg-VVce0yjXqwEhSpbEn1b+c_qwNg@mail.gmail.com>
 <0uDyUBwhwANlV2-TpB5S6FJ0331LSmlqPfItpOP8a_I-UCnOAXL-bPzUvFcrFvSLV157uzjHjhg_gIA2sn7GaeU2dsmld918i0THNm7m8U0=@protonmail.com>
Message-ID: <1279022010.967901.1655311152771.JavaMail.zimbra@psyctc.org>

This isn't my expert area but I have at times encountered issues relating to it and I think this isn't "just"
(as in "just standardize the similarly spelled duplicate names"). I once thought about trying to work out how
many names I have in citations to my work. Over the years I have seen my name as:
Chris Evans
Evans, Chris
Christopher Evans
Evans, Christopher
C.D.H.Evans
Evans, C.D.H.
and a great one that a bank once gave me: DR CHRISTOPHE D EVANS (honestly ... why?)

Then there are all the misspellings as you say.  Back in the days of snail mail reprint requests I used to
get teased about getting a fair few addressed to "Christ Evans".

Then there are things that add permutations of my qualifications (OK, perhaps not in your data but you have 
the "Jr." and perhaps "III" or the like.  I think these are more common in the USA than the UK.)

I also suspect that having names of "non-English" origins in there may complicate things too.  I still get
Spanish naming conventions wrong and know that the default order of given name / family name is reversed
in Japanese but that many Japanese know that much of the world won't know this so reverse their name order
for things going outside Japan.

I think there's nothing trivial or "just" about doing this but I suspect there are established, accepted,
and always fallible ways of doing it but I have a nasty suspicion that some are proprietary and not at all
open source.

I think you may have to start with the issue of commas: are they being used before terminal qualifiers 
(", Jr.", ", Dr." ...) or are they reversing family name and given name ("Evans, Chris")?  I might start
by counting the numbers of commas in the entries and hoping it's always zero or one.   If it is, I would
then look at the parts after the commas and see if I could get a list of common terminal qualifiers so I
would know they were not being treated as names (I know a man called Doctor Ronnie Doctor, but I suspect
he is never typed in as "Ronnie Doctor, Doctor"!)

If you have reversed given/family names you might want to try generating all the reversals and looking
for matches.

Then I might start to drill into full stops abbreviating names ("C. Evans", "Evans, C.", "Evans, C.D.H.") 
and what about "Evans, CDH"?  Can you assume that text segments all in upper case can be split, i.e. 
always translate "CDH" into "C.D.H.".  But then you have to deal with "II", "III" and even "IV" I guess.
Good job you're not doing British or French monarchs: "Henry VIII" and "Louise XVI" (I am not sure if 
you can change your name to "Henry VIII" by deed poll in the UK.  I do know you can't change it to "Jesus
Christ".

One tangential thing that might help is if you have other demographics: you might want to see if gender
(though it can change), age (will change), d.o.b. (shouldn't) might help you disaggregate some matches.

Enough already! Challenging stuff.

Very best (all),

Chris


----- Original Message -----
> From: "Gregg Powell via R-help" <r-help at r-project.org>
> To: "Ashim Kapoor" <ashimkapoor at gmail.com>
> Cc: "r-help" <R-help at r-project.org>, help at r-project.org, "R-help-request at lists.R-project.org"
> <R-help-request at lists.r-project.org>
> Sent: Wednesday, 15 June, 2022 17:43:14
> Subject: Re: [R] Is there a package that can do Fuzzy name matching to standardize names in a single column

> Hello Ashim and kind regards for you taking the time to answer back.
> 
> 
>> library(fuzzyjoin)
>> ?stringdist_left_join
> 
> -this will join two tables, but what I am trying to do is just standardize the
> similarly spelled duplicate names in just the first column of a single table.
> 
> I don't think fuzzyjoin will help me in that regard.
> 
> Thanks.
> Gregg
> Arizona, USA
> 
> ------- Original Message -------
> On Wednesday, June 15th, 2022 at 8:04 AM, Ashim Kapoor <ashimkapoor at gmail.com>
> wrote:
> 
> 
>> 
> 
>> 
> 
>> Dear Gregg,
>> 
> 
>> Check this out:
>> 
> 
>> library(fuzzyjoin)
>> ?stringdist_left_join
>> 
> 
>> Best Regards,
>> Ashim
>> 
> 
>> On Wed, Jun 15, 2022 at 8:28 PM Gregg Powell via R-help
>> r-help at r-project.org wrote:
>> 
> 
>> > Have data sets where there are names, in the first column, client names in the
>> > second, and Client start date in the third.
>> > 
> 
>> > There are thousands of these records with thousands of names/clients/client
>> > start dates. The name is entered each time the person begins with a new client
>> > such that each person has many entries in the name column. Often the names were
>> > not entered in a consistent way. With and without middle initial, middle name,
>> > or various abbreviations such as ",RN" at the end of the name.
>> > 
> 
>> > Is there a package that can do fuzzy name matching so that the names in name
>> > column get replaced with a "standardized" format - where some type of machine
>> > learning can pick the most common spelling of each repeat name and replace the
>> > different variations with the common spelling?
>> > 
> 
>> > I included an example below. First table includes the names with the various
>> > spellings. Second table depicts what I hope to achieve.
>> > 
> 
>> > Again - this is on a large scale - there are something like 10,000 records with
>> > names that need to be standardized.
>> > 
> 
>> > Name
>> > 
> 
>> > Client
>> > 
> 
>> > Client Start Date
>> > 
> 
>> > John Good
>> > 
> 
>> > Client 1
>> > 
> 
>> > 1/1/2020
>> > 
> 
>> > Joe Jackson
>> > 
> 
>> > Client 2
>> > 
> 
>> > 6/1/2020
>> > 
> 
>> > Bob A. Barker
>> > 
> 
>> > Client 3
>> > 
> 
>> > 8/1/2020
>> > 
> 
>> > John B. Good
>> > 
> 
>> > Client 4
>> > 
> 
>> > 10/1/2020
>> > 
> 
>> > Joe J. Jackson
>> > 
> 
>> > Client 5
>> > 
> 
>> > 12/1/2020
>> > 
> 
>> > Bob Allen Barker
>> > 
> 
>> > Client 6
>> > 
> 
>> > 1/1/2021
>> > 
> 
>> > John Good
>> > 
> 
>> > Client 7
>> > 
> 
>> > 5/1/2021
>> > 
> 
>> > Joe Jack Jackson
>> > 
> 
>> > Client 8
>> > 
> 
>> > 8/1/2021
>> > 
> 
>> > Bob Barker
>> > 
> 
>> > Client 9
>> > 
> 
>> > 12/1/2021
>> > 
> 
>> > Name
>> > 
> 
>> > Client
>> > 
> 
>> > Client Start Date
>> > 
> 
>> > John Good
>> > 
> 
>> > Client 1
>> > 
> 
>> > 1/1/2020
>> > 
> 
>> > Joe J. Jackson
>> > 
> 
>> > Client 2
>> > 
> 
>> > 6/1/2020
>> > 
> 
>> > Bob A. Barker
>> > 
> 
>> > Client 3
>> > 
> 
>> > 8/1/2020
>> > 
> 
>> > John Good
>> > 
> 
>> > Client 4
>> > 
> 
>> > 10/1/2020
>> > 
> 
>> > Joe J. Jackson
>> > 
> 
>> > Client 5
>> > 
> 
>> > 12/1/2020
>> > 
> 
>> > Bob A. Barker
>> > 
> 
>> > Client 6
>> > 
> 
>> > 1/1/2021
>> > 
> 
>> > John Good
>> > 
> 
>> > Client 7
>> > 
> 
>> > 5/1/2021
>> > 
> 
>> > Joe J. Jackson
>> > 
> 
>> > Client 8
>> > 
> 
>> > 8/1/2021
>> > 
> 
>> > Bob A. Barker
>> > 
> 
>> > Client 9
>> > 
> 
>> > 12/1/2021
>> > 
> 
>> > THANKS!
>> > 
> 
>> > Gregg Powell
>> > 
> 
>> > Arizona, USA______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chris Evans (he/him) <chris at psyctc.org> 
Visiting Professor, UDLA, Quito, Ecuador & Honorary Professor, University of Roehampton, London, UK.
Work web site: https://www.psyctc.org/psyctc/ 
CORE site:     https://www.coresystemtrust.org.uk/
Personal site: https://www.psyctc.org/pelerinage2016/
OMbook:        https://ombook.psyctc.org/book/


From gregco@t@ @end|ng |rom me@com  Wed Jun 15 18:58:18 2022
From: gregco@t@ @end|ng |rom me@com (Gregory Coats)
Date: Wed, 15 Jun 2022 12:58:18 -0400
Subject: [R] How do I "teach" R that columns 1, 2,
 and 3 are the Year-Month-Day
In-Reply-To: <25257.44580.312226.669632@stat.math.ethz.ch>
References: <47096049.765872.1653053240121.ref@mail.yahoo.com>
 <47096049.765872.1653053240121@mail.yahoo.com>
 <c2251d5b-ae25-0292-8fe4-61e4768becc8@ign.fr>
 <55CD3E40-FDAE-4A6E-B352-B60D1D98A236@me.com>
 <1229027594.2224595.1655258390324@mail.yahoo.com>
 <BN6PR2201MB155373731CCFC48AB0C7A8C3CFAD9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <25257.44580.312226.669632@stat.math.ethz.ch>
Message-ID: <1F255A77-3927-4A97-99E9-937029CFA9E6@me.com>

I do not see any posting on this topic from Jeff Newmiller.
I seek a way to ?teach? R that "2021-07-25? represents a Year, Month, and Day.
Greg Coats

> Fuel <- c(50.45, 61.48, 59.07, 55.40, 30.63, 41.35, 32.81, 49.86, 62.99, 89.37)
> plot (Fuel)
> Dates <- c("2021-07-25", "2021-08-27", "2021-09-26", "2021-11-04", "2021-11-22", "2021-11-26", "2021-12-06", "2022-01-14", "2022-04-29", "2022-06-11")
> plot (Dates)
Error in plot.window(...) : need finite 'ylim' values
In addition: Warning messages:
1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion
2: In min(x) : no non-missing arguments to min; returning Inf
3: In max(x) : no non-missing arguments to max; returning -Inf

> xyplot (Dates, Fuel)
Error in xyplot(Dates, Fuel) : could not find function "xyplot"

> On Jun 15, 2022, at 6:02 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> Jeff Newmiller's answer which was much shorter *and* only
> used base R  instead of an extra package


	[[alternative HTML version deleted]]


From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Wed Jun 15 18:59:54 2022
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Wed, 15 Jun 2022 11:59:54 -0500
Subject: [R] Is there a package that can do Fuzzy name matching to
 standardize names in a single column
In-Reply-To: <3648F43F-7398-4353-8B4E-499830FA5695@dcn.davis.ca.us>
References: <iv_SjOdZQNvgeBosgdd2MIiLeiIBtIW9cM2amgAQj90Xe-4lpUoTqZ9ii9orPrVjkvoWnJrFgsOOOrx2tCnjr-gZPFbzD_WfqLPv73-gO2g=@protonmail.com>
 <CAC8=1eqd6XbxtaQcqsa5tpg-VVce0yjXqwEhSpbEn1b+c_qwNg@mail.gmail.com>
 <0uDyUBwhwANlV2-TpB5S6FJ0331LSmlqPfItpOP8a_I-UCnOAXL-bPzUvFcrFvSLV157uzjHjhg_gIA2sn7GaeU2dsmld918i0THNm7m8U0=@protonmail.com>
 <3648F43F-7398-4353-8B4E-499830FA5695@dcn.davis.ca.us>
Message-ID: <81cb7359-f0c7-618b-9c76-313f8d31bbda@effectivedefense.org>

	  The Ecfun package has functions matchName, matchName1, parseName, and 
subNonStandardNames that were designed to deal especially with the 
problem of accents, knowing that, for example, in "Anastasio Somoza 
Debayle", "Somoza" is a surname, not a middle name, and names that get 
mangled like, "Andr_ Bruce C_rdenas".


	  Regarding disambiguating names like "John Good", Wikidata is 
wonderful for that.  I have not spent much time trying to access 
Wikidata from R, but sos::findFn('Wikidata') identified 12 different 
packages that mention Wikidata in a help page.  One or more of those may 
help you do what you want.


	  Spencer Graves


On 6/15/22 11:24 AM, Jeff Newmiller wrote:
> This is an intractable problem... you cannot know that "John Good" is the same person as "John B. Good"... and even when you augment their identity with information like which town they are in or what the first name of their spouse is you could be mislead by such information in multiple ways.
> 
> Most of the time such lists are managed by creating an internal "unique id" for each person. But since there is no definitive way to do this, it is handled as an ongoing process, with multiple algorithms applied with or without human supervision and always with some risk of increasing the error rate by any specific algorithm than was originally present in the data.
> 
> One common approach is using regular expressions or approximate match algorithms like base::agrep to filter possible like identities for people to check into further.
> 
> Another approach could be a clustering algorithm as Bert suggested. It has been awhile since I had to do this kind of work... I suppose neural nets might also be applied to this problem these days. But an agrep pre-filtering human augmentation should not be discounted... 10k entries is not _that_ many.
> 
> On June 15, 2022 8:43:14 AM PDT, Gregg Powell via R-help <r-help at r-project.org> wrote:
>>
>> Hello Ashim and kind regards for you taking the time to answer back.
>>
>>
>>> library(fuzzyjoin)
>>> ?stringdist_left_join
>>
>> -this will join two tables, but what I am trying to do is just standardize the similarly spelled duplicate names in just the first column of a single table.
>>
>> I don't think fuzzyjoin will help me in that regard.
>>
>> Thanks.
>> Gregg
>> Arizona, USA
>>
>> ------- Original Message -------
>> On Wednesday, June 15th, 2022 at 8:04 AM, Ashim Kapoor <ashimkapoor at gmail.com> wrote:
>>
>>
>>>
>>
>>>
>>
>>> Dear Gregg,
>>>
>>
>>> Check this out:
>>>
>>
>>> library(fuzzyjoin)
>>> ?stringdist_left_join
>>>
>>
>>> Best Regards,
>>> Ashim
>>>
>>
>>> On Wed, Jun 15, 2022 at 8:28 PM Gregg Powell via R-help
>>> r-help at r-project.org wrote:
>>>
>>
>>>> Have data sets where there are names, in the first column, client names in the second, and Client start date in the third.
>>>>
>>
>>>> There are thousands of these records with thousands of names/clients/client start dates. The name is entered each time the person begins with a new client such that each person has many entries in the name column. Often the names were not entered in a consistent way. With and without middle initial, middle name, or various abbreviations such as ",RN" at the end of the name.
>>>>
>>
>>>> Is there a package that can do fuzzy name matching so that the names in name column get replaced with a "standardized" format - where some type of machine learning can pick the most common spelling of each repeat name and replace the different variations with the common spelling?
>>>>
>>
>>>> I included an example below. First table includes the names with the various spellings. Second table depicts what I hope to achieve.
>>>>
>>
>>>> Again - this is on a large scale - there are something like 10,000 records with names that need to be standardized.
>>>>
>>
>>>> Name
>>>>
>>
>>>> Client
>>>>
>>
>>>> Client Start Date
>>>>
>>
>>>> John Good
>>>>
>>
>>>> Client 1
>>>>
>>
>>>> 1/1/2020
>>>>
>>
>>>> Joe Jackson
>>>>
>>
>>>> Client 2
>>>>
>>
>>>> 6/1/2020
>>>>
>>
>>>> Bob A. Barker
>>>>
>>
>>>> Client 3
>>>>
>>
>>>> 8/1/2020
>>>>
>>
>>>> John B. Good
>>>>
>>
>>>> Client 4
>>>>
>>
>>>> 10/1/2020
>>>>
>>
>>>> Joe J. Jackson
>>>>
>>
>>>> Client 5
>>>>
>>
>>>> 12/1/2020
>>>>
>>
>>>> Bob Allen Barker
>>>>
>>
>>>> Client 6
>>>>
>>
>>>> 1/1/2021
>>>>
>>
>>>> John Good
>>>>
>>
>>>> Client 7
>>>>
>>
>>>> 5/1/2021
>>>>
>>
>>>> Joe Jack Jackson
>>>>
>>
>>>> Client 8
>>>>
>>
>>>> 8/1/2021
>>>>
>>
>>>> Bob Barker
>>>>
>>
>>>> Client 9
>>>>
>>
>>>> 12/1/2021
>>>>
>>
>>>> Name
>>>>
>>
>>>> Client
>>>>
>>
>>>> Client Start Date
>>>>
>>
>>>> John Good
>>>>
>>
>>>> Client 1
>>>>
>>
>>>> 1/1/2020
>>>>
>>
>>>> Joe J. Jackson
>>>>
>>
>>>> Client 2
>>>>
>>
>>>> 6/1/2020
>>>>
>>
>>>> Bob A. Barker
>>>>
>>
>>>> Client 3
>>>>
>>
>>>> 8/1/2020
>>>>
>>
>>>> John Good
>>>>
>>
>>>> Client 4
>>>>
>>
>>>> 10/1/2020
>>>>
>>
>>>> Joe J. Jackson
>>>>
>>
>>>> Client 5
>>>>
>>
>>>> 12/1/2020
>>>>
>>
>>>> Bob A. Barker
>>>>
>>
>>>> Client 6
>>>>
>>
>>>> 1/1/2021
>>>>
>>
>>>> John Good
>>>>
>>
>>>> Client 7
>>>>
>>
>>>> 5/1/2021
>>>>
>>
>>>> Joe J. Jackson
>>>>
>>
>>>> Client 8
>>>>
>>
>>>> 8/1/2021
>>>>
>>
>>>> Bob A. Barker
>>>>
>>
>>>> Client 9
>>>>
>>
>>>> 12/1/2021
>>>>
>>
>>>> THANKS!
>>>>
>>
>>>> Gregg Powell
>>>>
>>
>>>> Arizona, USA______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.


From h@n@tezer@ @end|ng |rom gm@||@com  Wed Jun 15 19:09:53 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Wed, 15 Jun 2022 20:09:53 +0300
Subject: [R] Model Comparision for case control studies in R
In-Reply-To: <BN6PR2201MB15539B16281BDD689C99EA01CFAD9@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAEuJ0VAtsC3nHkUOiwfv7d8hA6N8q3DozStcX6=O+1juOs4fhA@mail.gmail.com>
 <BN6PR2201MB15539B16281BDD689C99EA01CFAD9@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <CAEuJ0VAFmHj=Nk7++J296x6qth9t6OqHz_4h13K8Pxt_YK_b6w@mail.gmail.com>

Dear Tim, Thanks. the first vector
y<-c(0,1,1,0,0,1,0,0,1,1,1,0,1,1,1,0,0,0,0,1) is the disease status y=
(1=Case,0=Control). The covariate age, smoking status and hypertension
are independent(uncorrelated). The logistic regression (unconditional)
will used. But I need to compare other models with logistic regression
instead of fitting it directly to logistic regression.
There is no matching on the data to use conditional logistics regression.
Best,
Hana
On 6/15/22, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> Disease status is missing from the sample data.
> Are age, disease, smoking, and/or hypertension correlated in any way or are
> they independent (correlation=0)?
> Are the correlations large enough to adversely influence your model?
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of anteneh asmare
> Sent: Wednesday, June 15, 2022 7:29 AM
> To: r-help at r-project.org
> Subject: [R] Model Comparision for case control studies in R
>
> [External Email]
>
> y<-c(0,1,1,0,0,1,0,0,1,1,1,0,1,1,1,0,0,0,0,1)
> age<-c(45,23,56,67,23,23,28,56,45,47,36,37,33,35,38,39,43,28,39,41)
> smoking<-c(0,1,1,1,0,0,0,0,0,1,1,0,0,1,0,1,1,1,0,1)
> hypertension<-c(1,1,0,1,0,1,0,1,1,0,1,1,1,1,1,1,0,0,1,0)
> data<-data.frame(y,age,smoking,hypertension)
> data
> model<-glm(y~age+factor(smoking)+factor(hypertension), data, family =
> binomial(link = "logit"),na.action = na.omit)
> summary(model)
> from above sample data I want to study a case-control study on male
> individuals with my response variable y, disease status (1=Case,
> 0=Control) with covariates age, smoking status(1=Yes, 0=No)  and
> hypertension, hypertensive (1=Yes, 0=No). I want to fit the model to predict
> the disease status using at least two different methods. And to make model
> comparisons. I think logistic regression will be the best fit for this case
> control study. Do we have other options in addition to logistic regression?
> My objective is to fit the model to predict the disease status using at
> least two different methods.
> Kind regards,
> Hana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=l7afPQ_gGAoV2EsNoYSYul0qAISEiXLmTmu0IQ03nZO4rcAi9xHZGsWwwig4oYOB&s=ztyDthknydhlcM49F33Gz6xRl6G7U9s8aIhB1VN-EKY&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=l7afPQ_gGAoV2EsNoYSYul0qAISEiXLmTmu0IQ03nZO4rcAi9xHZGsWwwig4oYOB&s=tcsGkhvtVvoVvb1Ehah-vLRC6an40rJXQXqqfX2f0gI&e=
> and provide commented, minimal, self-contained, reproducible code.
>


From tebert @end|ng |rom u||@edu  Wed Jun 15 20:02:12 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Wed, 15 Jun 2022 18:02:12 +0000
Subject: [R] Model Comparision for case control studies in R
In-Reply-To: <CAEuJ0VAFmHj=Nk7++J296x6qth9t6OqHz_4h13K8Pxt_YK_b6w@mail.gmail.com>
References: <CAEuJ0VAtsC3nHkUOiwfv7d8hA6N8q3DozStcX6=O+1juOs4fhA@mail.gmail.com>
 <BN6PR2201MB15539B16281BDD689C99EA01CFAD9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAEuJ0VAFmHj=Nk7++J296x6qth9t6OqHz_4h13K8Pxt_YK_b6w@mail.gmail.com>
Message-ID: <BN6PR2201MB1553F3AB6427997A1C0A02DDCFAD9@BN6PR2201MB1553.namprd22.prod.outlook.com>

The uncorrelated nature of smoking and hypertension is a major medical breakthrough and in contrast to reports like this:  https://pubmed.ncbi.nlm.nih.gov/20550499/ and the literature indicates the possibility of a relationship between age and hypertension https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4768730/. Depending on the country, there might be a relationship between smoking and age as government programs against smoking are developed.

Are you looking at different models or different methods. I could have y = x + y + z as one model and y=x + z as another model. Alternatively I could be comparing ordinary least squares regression versus maximum likelihood versus Bayesian linear regression versus nonlinear regression. The former might use something like the Akaike information criterion. I am not sure the latter is useful (or possible). For example I could approximate an exponential function using a polynomial, but in this context I see no benefit in doing so even if I could compare the models.

I do not quite understand why this is being done. It feels like fishing statistical methods to get the answer that I know is correct. Generally, one should understand the system well enough to select an appropriate model rather than try every possible model in the hope something fits. Of course one sometimes collects extra data in the hope that we do not miss an important feature. Then forwards/backwards/stepwise methods are used to identify the "best" model but this is looking at similar models that differ only in the list of independent variables.

However the problem is solved, I would start by trying to determine if any one model was appropriate. Are the model assumptions satisfied? If the answer is no, then try another model until you find one that does satisfy the model assumptions. Alternatively, start with an understanding of the biology and use the best model. Comparing an biologically meaningless statistical model to a biologically meaningful one is an easy choice. 

Tim

-----Original Message-----
From: anteneh asmare <hanatezera at gmail.com> 
Sent: Wednesday, June 15, 2022 1:10 PM
To: Ebert,Timothy Aaron <tebert at ufl.edu>
Cc: r-help at r-project.org
Subject: Re: [R] Model Comparision for case control studies in R

[External Email]

Dear Tim, Thanks. the first vector
y<-c(0,1,1,0,0,1,0,0,1,1,1,0,1,1,1,0,0,0,0,1) is the disease status y= (1=Case,0=Control). The covariate age, smoking status and hypertension are independent(uncorrelated). The logistic regression (unconditional) will used. But I need to compare other models with logistic regression instead of fitting it directly to logistic regression.
There is no matching on the data to use conditional logistics regression.
Best,
Hana
On 6/15/22, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> Disease status is missing from the sample data.
> Are age, disease, smoking, and/or hypertension correlated in any way 
> or are they independent (correlation=0)?
> Are the correlations large enough to adversely influence your model?
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of anteneh 
> asmare
> Sent: Wednesday, June 15, 2022 7:29 AM
> To: r-help at r-project.org
> Subject: [R] Model Comparision for case control studies in R
>
> [External Email]
>
> y<-c(0,1,1,0,0,1,0,0,1,1,1,0,1,1,1,0,0,0,0,1)
> age<-c(45,23,56,67,23,23,28,56,45,47,36,37,33,35,38,39,43,28,39,41)
> smoking<-c(0,1,1,1,0,0,0,0,0,1,1,0,0,1,0,1,1,1,0,1)
> hypertension<-c(1,1,0,1,0,1,0,1,1,0,1,1,1,1,1,1,0,0,1,0)
> data<-data.frame(y,age,smoking,hypertension)
> data
> model<-glm(y~age+factor(smoking)+factor(hypertension), data, family = 
> binomial(link = "logit"),na.action = na.omit)
> summary(model)
> from above sample data I want to study a case-control study on male 
> individuals with my response variable y, disease status (1=Case,
> 0=Control) with covariates age, smoking status(1=Yes, 0=No)  and 
> hypertension, hypertensive (1=Yes, 0=No). I want to fit the model to 
> predict the disease status using at least two different methods. And 
> to make model comparisons. I think logistic regression will be the 
> best fit for this case control study. Do we have other options in addition to logistic regression?
> My objective is to fit the model to predict the disease status using 
> at least two different methods.
> Kind regards,
> Hana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=l7afPQ_gGAoV2EsNoYSYul0qAISEiXLmTmu0IQ03nZO4rcAi9xHZGsWww
> ig4oYOB&s=ztyDthknydhlcM49F33Gz6xRl6G7U9s8aIhB1VN-EKY&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=l7afPQ_gGAoV2EsNoYSYul0qAISEiXLmTmu0IQ03nZO4rcAi9xHZGsWw
> wig4oYOB&s=tcsGkhvtVvoVvb1Ehah-vLRC6an40rJXQXqqfX2f0gI&e=
> and provide commented, minimal, self-contained, reproducible code.
>

From bgunter@4567 @end|ng |rom gm@||@com  Wed Jun 15 20:07:34 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 15 Jun 2022 11:07:34 -0700
Subject: [R] How do I "teach" R that columns 1, 2,
 and 3 are the Year-Month-Day
In-Reply-To: <1F255A77-3927-4A97-99E9-937029CFA9E6@me.com>
References: <47096049.765872.1653053240121.ref@mail.yahoo.com>
 <47096049.765872.1653053240121@mail.yahoo.com>
 <c2251d5b-ae25-0292-8fe4-61e4768becc8@ign.fr>
 <55CD3E40-FDAE-4A6E-B352-B60D1D98A236@me.com>
 <1229027594.2224595.1655258390324@mail.yahoo.com>
 <BN6PR2201MB155373731CCFC48AB0C7A8C3CFAD9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <25257.44580.312226.669632@stat.math.ethz.ch>
 <1F255A77-3927-4A97-99E9-937029CFA9E6@me.com>
Message-ID: <CAGxFJbTOUYxmgC=vyvFRhwkauHd8X1P9bCWvjyduJzxWQtRhYw@mail.gmail.com>

Look again -- perhaps in your spam folder.

Here's what he said:

"
Reprex:

dta <- read.table( text =
"Yr  Mo Dy   Fuel
2021  7 25   50.45
2021  8 27   61.48
2021  9 26   59.07
2021 11  4   55.40
2021 11 22   30.63
2021 11 26   41.35
2021 12  6   32.81
2022  1 14   49.86
2022  4 29   62.99
2022  6 11   89.37
", header=TRUE )
dta$Dtm <- with( dta, as.Date( ISOdate( Yr, Mo, Dy ) ) )
with( dta, plot( Dtm, Fuel ) )

The ISOdate function returns a POSIXct which includes time-of-day.
Analyses that don't need time can instead rely on the Date type to
avoid issues with timezones. "


Bert Gunter

On Wed, Jun 15, 2022 at 9:58 AM Gregory Coats via R-help
<r-help at r-project.org> wrote:
>
> I do not see any posting on this topic from Jeff Newmiller.
> I seek a way to ?teach? R that "2021-07-25? represents a Year, Month, and Day.
> Greg Coats
>
> > Fuel <- c(50.45, 61.48, 59.07, 55.40, 30.63, 41.35, 32.81, 49.86, 62.99, 89.37)
> > plot (Fuel)
> > Dates <- c("2021-07-25", "2021-08-27", "2021-09-26", "2021-11-04", "2021-11-22", "2021-11-26", "2021-12-06", "2022-01-14", "2022-04-29", "2022-06-11")
> > plot (Dates)
> Error in plot.window(...) : need finite 'ylim' values
> In addition: Warning messages:
> 1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion
> 2: In min(x) : no non-missing arguments to min; returning Inf
> 3: In max(x) : no non-missing arguments to max; returning -Inf
>
> > xyplot (Dates, Fuel)
> Error in xyplot(Dates, Fuel) : could not find function "xyplot"
>
> > On Jun 15, 2022, at 6:02 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> > Jeff Newmiller's answer which was much shorter *and* only
> > used base R  instead of an extra package
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rhe|p @end|ng |rom eoo@@dd@@n|  Wed Jun 15 21:31:08 2022
From: rhe|p @end|ng |rom eoo@@dd@@n| (Jan van der Laan)
Date: Wed, 15 Jun 2022 21:31:08 +0200
Subject: [R] Is there a package that can do Fuzzy name matching to
 standardize names in a single column
In-Reply-To: <iv_SjOdZQNvgeBosgdd2MIiLeiIBtIW9cM2amgAQj90Xe-4lpUoTqZ9ii9orPrVjkvoWnJrFgsOOOrx2tCnjr-gZPFbzD_WfqLPv73-gO2g=@protonmail.com>
References: <iv_SjOdZQNvgeBosgdd2MIiLeiIBtIW9cM2amgAQj90Xe-4lpUoTqZ9ii9orPrVjkvoWnJrFgsOOOrx2tCnjr-gZPFbzD_WfqLPv73-gO2g=@protonmail.com>
Message-ID: <bef366cf-f95f-c47f-89be-f0e1209879c2@dds.nl>


The reclin2 package (by me)? also has functionality for finding 
duplicate records. So it should handle finding names that are likely to 
be the same. See here for a vignette with an example where different 
variants of town names are clustered: 
https://cran.r-project.org/web/packages/reclin2/vignettes/deduplication.html 
.

In some of the cases where I used this, the quality of the matches 
improved immensely with a large number of manual preprocessing of the 
names. These were mostly done using regular expressions, for example 
using gsub. For example removing accents, hyphens, replacing common 
variants of names with the most common one.

HTH,

Jan




On 15-06-2022 16:57, Gregg Powell via R-help wrote:
> Have data sets where there are names, in the first column, client names in the second, and Client start date in the third.
>
> There are thousands of these records with thousands of names/clients/client start dates. The name is entered each time the person begins with a new client such that each person has many entries in the name column. Often the names were not entered in a consistent way. With and without middle initial, middle name, or various abbreviations such as ",RN" at the end of the name.
>
> Is there a package that can do fuzzy name matching so that the names in name column get replaced with a "standardized" format - where some type of machine learning can pick the most common spelling of each repeat name and replace the different variations with the common spelling?
>
> I included an example below. First table includes the names with the various spellings. Second table depicts what I hope to achieve.
>
> Again - this is on a large scale - there are something like 10,000 records with names that need to be standardized.
>
>
> Name
>
> Client
>
> Client Start Date
>
> John Good
>
> Client 1
>
> 1/1/2020
>
> Joe Jackson
>
> Client 2
>
> 6/1/2020
>
> Bob A. Barker
>
> Client 3
>
> 8/1/2020
>
> John B. Good
>
> Client 4
>
> 10/1/2020
>
> Joe J. Jackson
>
> Client 5
>
> 12/1/2020
>
> Bob Allen Barker
>
> Client 6
>
> 1/1/2021
>
> John Good
>
> Client 7
>
> 5/1/2021
>
> Joe Jack Jackson
>
> Client 8
>
> 8/1/2021
>
> Bob Barker
>
> Client 9
>
> 12/1/2021
>
>   
>
>   
>
>   
>
> Name
>
> Client
>
> Client Start Date
>
> John Good
>
> Client 1
>
> 1/1/2020
>
> Joe J. Jackson
>
> Client 2
>
> 6/1/2020
>
> Bob A. Barker
>
> Client 3
>
> 8/1/2020
>
> John Good
>
> Client 4
>
> 10/1/2020
>
> Joe J. Jackson
>
> Client 5
>
> 12/1/2020
>
> Bob A. Barker
>
> Client 6
>
> 1/1/2021
>
> John Good
>
> Client 7
>
> 5/1/2021
>
> Joe J. Jackson
>
> Client 8
>
> 8/1/2021
>
> Bob A. Barker
>
> Client 9
>
> 12/1/2021
>
>
>
> THANKS!
>
> Gregg Powell
>
> Arizona, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From h@n@tezer@ @end|ng |rom gm@||@com  Wed Jun 15 22:52:43 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (Hana Tezera)
Date: Wed, 15 Jun 2022 23:52:43 +0300
Subject: [R] Model Comparision for case control studies in R
In-Reply-To: <BN6PR2201MB1553F3AB6427997A1C0A02DDCFAD9@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAEuJ0VAtsC3nHkUOiwfv7d8hA6N8q3DozStcX6=O+1juOs4fhA@mail.gmail.com>
 <BN6PR2201MB15539B16281BDD689C99EA01CFAD9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAEuJ0VAFmHj=Nk7++J296x6qth9t6OqHz_4h13K8Pxt_YK_b6w@mail.gmail.com>
 <BN6PR2201MB1553F3AB6427997A1C0A02DDCFAD9@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <CAEuJ0VBer1N_-gS3k6frcqfMFzrdz7JQeXwHed=kpx6-G82MBg@mail.gmail.com>

Dear Tim, Thanks a lot I am looking for different methods for each
method, I want to select the best predictors and I want to report some
measures of the accuracy. And I will compare the performance of the
models, by plotting their ROC curves.
Best,
Hana

On 6/15/22, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> The uncorrelated nature of smoking and hypertension is a major medical
> breakthrough and in contrast to reports like this:
> https://pubmed.ncbi.nlm.nih.gov/20550499/ and the literature indicates the
> possibility of a relationship between age and hypertension
> https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4768730/. Depending on the
> country, there might be a relationship between smoking and age as government
> programs against smoking are developed.
>
> Are you looking at different models or different methods. I could have y = x
> + y + z as one model and y=x + z as another model. Alternatively I could be
> comparing ordinary least squares regression versus maximum likelihood versus
> Bayesian linear regression versus nonlinear regression. The former might use
> something like the Akaike information criterion. I am not sure the latter is
> useful (or possible). For example I could approximate an exponential
> function using a polynomial, but in this context I see no benefit in doing
> so even if I could compare the models.
>
> I do not quite understand why this is being done. It feels like fishing
> statistical methods to get the answer that I know is correct. Generally, one
> should understand the system well enough to select an appropriate model
> rather than try every possible model in the hope something fits. Of course
> one sometimes collects extra data in the hope that we do not miss an
> important feature. Then forwards/backwards/stepwise methods are used to
> identify the "best" model but this is looking at similar models that differ
> only in the list of independent variables.
>
> However the problem is solved, I would start by trying to determine if any
> one model was appropriate. Are the model assumptions satisfied? If the
> answer is no, then try another model until you find one that does satisfy
> the model assumptions. Alternatively, start with an understanding of the
> biology and use the best model. Comparing an biologically meaningless
> statistical model to a biologically meaningful one is an easy choice.
>
> Tim
>
> -----Original Message-----
> From: anteneh asmare <hanatezera at gmail.com>
> Sent: Wednesday, June 15, 2022 1:10 PM
> To: Ebert,Timothy Aaron <tebert at ufl.edu>
> Cc: r-help at r-project.org
> Subject: Re: [R] Model Comparision for case control studies in R
>
> [External Email]
>
> Dear Tim, Thanks. the first vector
> y<-c(0,1,1,0,0,1,0,0,1,1,1,0,1,1,1,0,0,0,0,1) is the disease status y=
> (1=Case,0=Control). The covariate age, smoking status and hypertension are
> independent(uncorrelated). The logistic regression (unconditional) will
> used. But I need to compare other models with logistic regression instead of
> fitting it directly to logistic regression.
> There is no matching on the data to use conditional logistics regression.
> Best,
> Hana
> On 6/15/22, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>> Disease status is missing from the sample data.
>> Are age, disease, smoking, and/or hypertension correlated in any way
>> or are they independent (correlation=0)?
>> Are the correlations large enough to adversely influence your model?
>> Tim
>>
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of anteneh
>> asmare
>> Sent: Wednesday, June 15, 2022 7:29 AM
>> To: r-help at r-project.org
>> Subject: [R] Model Comparision for case control studies in R
>>
>> [External Email]
>>
>> y<-c(0,1,1,0,0,1,0,0,1,1,1,0,1,1,1,0,0,0,0,1)
>> age<-c(45,23,56,67,23,23,28,56,45,47,36,37,33,35,38,39,43,28,39,41)
>> smoking<-c(0,1,1,1,0,0,0,0,0,1,1,0,0,1,0,1,1,1,0,1)
>> hypertension<-c(1,1,0,1,0,1,0,1,1,0,1,1,1,1,1,1,0,0,1,0)
>> data<-data.frame(y,age,smoking,hypertension)
>> data
>> model<-glm(y~age+factor(smoking)+factor(hypertension), data, family =
>> binomial(link = "logit"),na.action = na.omit)
>> summary(model)
>> from above sample data I want to study a case-control study on male
>> individuals with my response variable y, disease status (1=Case,
>> 0=Control) with covariates age, smoking status(1=Yes, 0=No)  and
>> hypertension, hypertensive (1=Yes, 0=No). I want to fit the model to
>> predict the disease status using at least two different methods. And
>> to make model comparisons. I think logistic regression will be the
>> best fit for this case control study. Do we have other options in addition
>> to logistic regression?
>> My objective is to fit the model to predict the disease status using
>> at least two different methods.
>> Kind regards,
>> Hana
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
>> man_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
>> Rzsn7AkP-g&m=l7afPQ_gGAoV2EsNoYSYul0qAISEiXLmTmu0IQ03nZO4rcAi9xHZGsWww
>> ig4oYOB&s=ztyDthknydhlcM49F33Gz6xRl6G7U9s8aIhB1VN-EKY&e=
>> PLEASE do read the posting guide
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
>> g_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
>> sRzsn7AkP-g&m=l7afPQ_gGAoV2EsNoYSYul0qAISEiXLmTmu0IQ03nZO4rcAi9xHZGsWw
>> wig4oYOB&s=tcsGkhvtVvoVvb1Ehah-vLRC6an40rJXQXqqfX2f0gI&e=
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From @@h|mk@poor @end|ng |rom gm@||@com  Thu Jun 16 04:06:45 2022
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Thu, 16 Jun 2022 07:36:45 +0530
Subject: [R] Is there a package that can do Fuzzy name matching to
 standardize names in a single column
In-Reply-To: <0uDyUBwhwANlV2-TpB5S6FJ0331LSmlqPfItpOP8a_I-UCnOAXL-bPzUvFcrFvSLV157uzjHjhg_gIA2sn7GaeU2dsmld918i0THNm7m8U0=@protonmail.com>
References: <iv_SjOdZQNvgeBosgdd2MIiLeiIBtIW9cM2amgAQj90Xe-4lpUoTqZ9ii9orPrVjkvoWnJrFgsOOOrx2tCnjr-gZPFbzD_WfqLPv73-gO2g=@protonmail.com>
 <CAC8=1eqd6XbxtaQcqsa5tpg-VVce0yjXqwEhSpbEn1b+c_qwNg@mail.gmail.com>
 <0uDyUBwhwANlV2-TpB5S6FJ0331LSmlqPfItpOP8a_I-UCnOAXL-bPzUvFcrFvSLV157uzjHjhg_gIA2sn7GaeU2dsmld918i0THNm7m8U0=@protonmail.com>
Message-ID: <CAC8=1eoua3y45NYFwGZw_6Y52uFKy=2kr9VbLDAefg=+MqSByw@mail.gmail.com>

Dear Gregg,

This is what I  meant :-

> df1
             Names
1        John Good
2      Joe Jackson
3    Bob A. Barker
4     John B. Good
5   Joe J. Jackson
6 Bob Allen Barker
7        John Good
8 Joe Jack Johnson
9       Bob Barker

> stringdist_left_join(df1,df1,by="Names",max_dist = 3)
            Names.x          Names.y
1         John Good        John Good
2         John Good     John B. Good
3         John Good        John Good
4       Joe Jackson      Joe Jackson
5       Joe Jackson   Joe J. Jackson
6     Bob A. Barker    Bob A. Barker
7     Bob A. Barker       Bob Barker
8      John B. Good        John Good
9      John B. Good     John B. Good
10     John B. Good        John Good
11   Joe J. Jackson      Joe Jackson
12   Joe J. Jackson   Joe J. Jackson
13 Bob Allen Barker Bob Allen Barker
14        John Good        John Good
15        John Good     John B. Good
16        John Good        John Good
17 Joe Jack Johnson Joe Jack Johnson
18       Bob Barker    Bob A. Barker
19       Bob Barker       Bob Barker
>


You can join a table to itself while tinkering with the max_distance function..
Please notice the clusters that have formed. This has to be cleaned up.

This is similar to the answer by Jan van der Laan.

Best Regards,
Ashim

On Wed, Jun 15, 2022 at 9:13 PM Gregg Powell <g.a.powell at protonmail.com> wrote:
>
>
> Hello Ashim and kind regards for you taking the time to answer back.
>
>
> > library(fuzzyjoin)
> > ?stringdist_left_join
>
> -this will join two tables, but what I am trying to do is just standardize the similarly spelled duplicate names in just the first column of a single table.
>
> I don't think fuzzyjoin will help me in that regard.
>
> Thanks.
> Gregg
> Arizona, USA
>
> ------- Original Message -------
> On Wednesday, June 15th, 2022 at 8:04 AM, Ashim Kapoor <ashimkapoor at gmail.com> wrote:
>
>
> >
>
> >
>
> > Dear Gregg,
> >
>
> > Check this out:
> >
>
> > library(fuzzyjoin)
> > ?stringdist_left_join
> >
>
> > Best Regards,
> > Ashim
> >
>
> > On Wed, Jun 15, 2022 at 8:28 PM Gregg Powell via R-help
> > r-help at r-project.org wrote:
> >
>
> > > Have data sets where there are names, in the first column, client names in the second, and Client start date in the third.
> > >
>
> > > There are thousands of these records with thousands of names/clients/client start dates. The name is entered each time the person begins with a new client such that each person has many entries in the name column. Often the names were not entered in a consistent way. With and without middle initial, middle name, or various abbreviations such as ",RN" at the end of the name.
> > >
>
> > > Is there a package that can do fuzzy name matching so that the names in name column get replaced with a "standardized" format - where some type of machine learning can pick the most common spelling of each repeat name and replace the different variations with the common spelling?
> > >
>
> > > I included an example below. First table includes the names with the various spellings. Second table depicts what I hope to achieve.
> > >
>
> > > Again - this is on a large scale - there are something like 10,000 records with names that need to be standardized.
> > >
>
> > > Name
> > >
>
> > > Client
> > >
>
> > > Client Start Date
> > >
>
> > > John Good
> > >
>
> > > Client 1
> > >
>
> > > 1/1/2020
> > >
>
> > > Joe Jackson
> > >
>
> > > Client 2
> > >
>
> > > 6/1/2020
> > >
>
> > > Bob A. Barker
> > >
>
> > > Client 3
> > >
>
> > > 8/1/2020
> > >
>
> > > John B. Good
> > >
>
> > > Client 4
> > >
>
> > > 10/1/2020
> > >
>
> > > Joe J. Jackson
> > >
>
> > > Client 5
> > >
>
> > > 12/1/2020
> > >
>
> > > Bob Allen Barker
> > >
>
> > > Client 6
> > >
>
> > > 1/1/2021
> > >
>
> > > John Good
> > >
>
> > > Client 7
> > >
>
> > > 5/1/2021
> > >
>
> > > Joe Jack Jackson
> > >
>
> > > Client 8
> > >
>
> > > 8/1/2021
> > >
>
> > > Bob Barker
> > >
>
> > > Client 9
> > >
>
> > > 12/1/2021
> > >
>
> > > Name
> > >
>
> > > Client
> > >
>
> > > Client Start Date
> > >
>
> > > John Good
> > >
>
> > > Client 1
> > >
>
> > > 1/1/2020
> > >
>
> > > Joe J. Jackson
> > >
>
> > > Client 2
> > >
>
> > > 6/1/2020
> > >
>
> > > Bob A. Barker
> > >
>
> > > Client 3
> > >
>
> > > 8/1/2020
> > >
>
> > > John Good
> > >
>
> > > Client 4
> > >
>
> > > 10/1/2020
> > >
>
> > > Joe J. Jackson
> > >
>
> > > Client 5
> > >
>
> > > 12/1/2020
> > >
>
> > > Bob A. Barker
> > >
>
> > > Client 6
> > >
>
> > > 1/1/2021
> > >
>
> > > John Good
> > >
>
> > > Client 7
> > >
>
> > > 5/1/2021
> > >
>
> > > Joe J. Jackson
> > >
>
> > > Client 8
> > >
>
> > > 8/1/2021
> > >
>
> > > Bob A. Barker
> > >
>
> > > Client 9
> > >
>
> > > 12/1/2021
> > >
>
> > > THANKS!
> > >
>
> > > Gregg Powell
> > >
>
> > > Arizona, USA______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.


From r@oknz @end|ng |rom gm@||@com  Thu Jun 16 04:36:28 2022
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Thu, 16 Jun 2022 14:36:28 +1200
Subject: [R] Create a categorical variable using the deciles of data
In-Reply-To: <BN6PR2201MB1553901C2C47BFA5B3523BCDCFAA9@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAEuJ0VD66249iN6mAMJcYf=xf_2aktmB-31kx16-su4AT+P6kA@mail.gmail.com>
 <CABcYAdJO5nJ_cOXC6aGKKdQCQo4zCNSUMZQJoZ0v2Mq-bXJQdA@mail.gmail.com>
 <BN6PR2201MB1553901C2C47BFA5B3523BCDCFAA9@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <CABcYAd+LznrGfABiZROv4m7A3astwO02+=j9=kgT8EnJY8n44Q@mail.gmail.com>

I had the advantage of studying
   Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
   "The New S Language".  Wadsworth & Brooks/Cole.
before starting to use R.  That book was reissued by CRC Press
only a few years ago.  It's *still* a pretty darned good intro
to R.  cut, pretty, and quantile are all there.  They are
pretty basic.

I strongly recommend looking for a copy of that book and
skimming through appendix 1 repeatedly until you have a rough
idea of what's there.  A lot has been added since then, and R
never did srarch the current working directory as part of the
environment, but a lot has NOT changed.  category(..) has gone,
that's the main difference I recall.


On Wed, 15 Jun 2022 at 01:49, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:

> A problem in R is that there are several dozen ways to do any of these
> basic activities. I used the approach that I could get to work the fastest
> and tried to make it somewhat general. I do not know functions like ?pretty
> that Rui used, nor ?quantile or ?cut. It is a difficulty in learning R
> where the internet or sites like this one are the "teacher." There are
> plenty of books, but they too take one approach to solve a problem rather
> than "here is a problem" and "these are all possible solutions." I
> appreciate seeing alternative solutions.
>
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Richard O'Keefe
> Sent: Tuesday, June 14, 2022 9:08 AM
> To: anteneh asmare <hanatezera at gmail.com>
> Cc: R Project Help <r-help at r-project.org>
> Subject: Re: [R] Create a categorical variable using the deciles of data
>
> [External Email]
>
> Can you explain why you are not using
> ?quantile
> to find the deciles then
> ?cut
> to construct the factor?
> What have I misunderstood?
>
> On Tue, 14 Jun 2022 at 23:29, anteneh asmare <hanatezera at gmail.com> wrote:
>
> > I want Create a categorical variable using the deciles of the
> > following data frame to divide the individuals into 10 groups equally.
> > I try the following codes
> > data_catigocal<-data.frame(c(1:50000))
> > # create categorical vector using deciles group_vector <-
> >
> > c('0-10','11-20','21-30','31-40','41-50','51-60','61-70','71-80','81-9
> > 0','91-100') # Add categorical variable to the data_catigocal
> > data_catigocal$decile <- factor(group_vector) # print data frame
> > data_catigocal
> >
> > can any one help me with the r code
> > Kind regards,
> > Hana
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> > man_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> > Rzsn7AkP-g&m=JNEsCwSpVwVmoMiEXA8K7ZWqg1GZK3Cx87LshtZ5gy5Y8SyDZrUSTuotO
> > cQ44yzy&s=2x4gMg5K_GJPK-XUk3UfSB3hhFCziCOgqvxl7yJXTvA&e=
> > PLEASE do read the posting guide
> > https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> > g_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> > sRzsn7AkP-g&m=JNEsCwSpVwVmoMiEXA8K7ZWqg1GZK3Cx87LshtZ5gy5Y8SyDZrUSTuot
> > OcQ44yzy&s=50k59quZy2KmFsVBRxK4P-M7RyxDsPGieX6TiiY5or0&e=
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=JNEsCwSpVwVmoMiEXA8K7ZWqg1GZK3Cx87LshtZ5gy5Y8SyDZrUSTuotOcQ44yzy&s=2x4gMg5K_GJPK-XUk3UfSB3hhFCziCOgqvxl7yJXTvA&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=JNEsCwSpVwVmoMiEXA8K7ZWqg1GZK3Cx87LshtZ5gy5Y8SyDZrUSTuotOcQ44yzy&s=50k59quZy2KmFsVBRxK4P-M7RyxDsPGieX6TiiY5or0&e=
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From j|n||68 @end|ng |rom gm@||@com  Thu Jun 16 06:41:49 2022
From: j|n||68 @end|ng |rom gm@||@com (Jin Li)
Date: Thu, 16 Jun 2022 14:41:49 +1000
Subject: [R] Model Comparision for case control studies in R
In-Reply-To: <CAEuJ0VBer1N_-gS3k6frcqfMFzrdz7JQeXwHed=kpx6-G82MBg@mail.gmail.com>
References: <CAEuJ0VAtsC3nHkUOiwfv7d8hA6N8q3DozStcX6=O+1juOs4fhA@mail.gmail.com>
 <BN6PR2201MB15539B16281BDD689C99EA01CFAD9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAEuJ0VAFmHj=Nk7++J296x6qth9t6OqHz_4h13K8Pxt_YK_b6w@mail.gmail.com>
 <BN6PR2201MB1553F3AB6427997A1C0A02DDCFAD9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAEuJ0VBer1N_-gS3k6frcqfMFzrdz7JQeXwHed=kpx6-G82MBg@mail.gmail.com>
Message-ID: <CAGu_3ZK68Ai_vDhZ9kQxAuvUdug9kR0DnvegOT6Ws8TnQvNbbw@mail.gmail.com>

Hi Hana,

ROC (or AUC) is misleading and should not be used to assess model
performance. For details, please see the references in "Spatial Predictive
Modelign with R '' that also provides some methods (e.g., gbm, rf, svm and
glmlet) for 1/0 data along with accuracy-based variable selection and
parameter optimisation.

Hope this helps,
Jin

On Thu, Jun 16, 2022 at 6:53 AM Hana Tezera <hanatezera at gmail.com> wrote:

> Dear Tim, Thanks a lot I am looking for different methods for each
> method, I want to select the best predictors and I want to report some
> measures of the accuracy. And I will compare the performance of the
> models, by plotting their ROC curves.
> Best,
> Hana
>
> On 6/15/22, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> > The uncorrelated nature of smoking and hypertension is a major medical
> > breakthrough and in contrast to reports like this:
> > https://pubmed.ncbi.nlm.nih.gov/20550499/ and the literature indicates
> the
> > possibility of a relationship between age and hypertension
> > https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4768730/. Depending on the
> > country, there might be a relationship between smoking and age as
> government
> > programs against smoking are developed.
> >
> > Are you looking at different models or different methods. I could have y
> = x
> > + y + z as one model and y=x + z as another model. Alternatively I could
> be
> > comparing ordinary least squares regression versus maximum likelihood
> versus
> > Bayesian linear regression versus nonlinear regression. The former might
> use
> > something like the Akaike information criterion. I am not sure the
> latter is
> > useful (or possible). For example I could approximate an exponential
> > function using a polynomial, but in this context I see no benefit in
> doing
> > so even if I could compare the models.
> >
> > I do not quite understand why this is being done. It feels like fishing
> > statistical methods to get the answer that I know is correct. Generally,
> one
> > should understand the system well enough to select an appropriate model
> > rather than try every possible model in the hope something fits. Of
> course
> > one sometimes collects extra data in the hope that we do not miss an
> > important feature. Then forwards/backwards/stepwise methods are used to
> > identify the "best" model but this is looking at similar models that
> differ
> > only in the list of independent variables.
> >
> > However the problem is solved, I would start by trying to determine if
> any
> > one model was appropriate. Are the model assumptions satisfied? If the
> > answer is no, then try another model until you find one that does satisfy
> > the model assumptions. Alternatively, start with an understanding of the
> > biology and use the best model. Comparing an biologically meaningless
> > statistical model to a biologically meaningful one is an easy choice.
> >
> > Tim
> >
> > -----Original Message-----
> > From: anteneh asmare <hanatezera at gmail.com>
> > Sent: Wednesday, June 15, 2022 1:10 PM
> > To: Ebert,Timothy Aaron <tebert at ufl.edu>
> > Cc: r-help at r-project.org
> > Subject: Re: [R] Model Comparision for case control studies in R
> >
> > [External Email]
> >
> > Dear Tim, Thanks. the first vector
> > y<-c(0,1,1,0,0,1,0,0,1,1,1,0,1,1,1,0,0,0,0,1) is the disease status y=
> > (1=Case,0=Control). The covariate age, smoking status and hypertension
> are
> > independent(uncorrelated). The logistic regression (unconditional) will
> > used. But I need to compare other models with logistic regression
> instead of
> > fitting it directly to logistic regression.
> > There is no matching on the data to use conditional logistics regression.
> > Best,
> > Hana
> > On 6/15/22, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> >> Disease status is missing from the sample data.
> >> Are age, disease, smoking, and/or hypertension correlated in any way
> >> or are they independent (correlation=0)?
> >> Are the correlations large enough to adversely influence your model?
> >> Tim
> >>
> >> -----Original Message-----
> >> From: R-help <r-help-bounces at r-project.org> On Behalf Of anteneh
> >> asmare
> >> Sent: Wednesday, June 15, 2022 7:29 AM
> >> To: r-help at r-project.org
> >> Subject: [R] Model Comparision for case control studies in R
> >>
> >> [External Email]
> >>
> >> y<-c(0,1,1,0,0,1,0,0,1,1,1,0,1,1,1,0,0,0,0,1)
> >> age<-c(45,23,56,67,23,23,28,56,45,47,36,37,33,35,38,39,43,28,39,41)
> >> smoking<-c(0,1,1,1,0,0,0,0,0,1,1,0,0,1,0,1,1,1,0,1)
> >> hypertension<-c(1,1,0,1,0,1,0,1,1,0,1,1,1,1,1,1,0,0,1,0)
> >> data<-data.frame(y,age,smoking,hypertension)
> >> data
> >> model<-glm(y~age+factor(smoking)+factor(hypertension), data, family =
> >> binomial(link = "logit"),na.action = na.omit)
> >> summary(model)
> >> from above sample data I want to study a case-control study on male
> >> individuals with my response variable y, disease status (1=Case,
> >> 0=Control) with covariates age, smoking status(1=Yes, 0=No)  and
> >> hypertension, hypertensive (1=Yes, 0=No). I want to fit the model to
> >> predict the disease status using at least two different methods. And
> >> to make model comparisons. I think logistic regression will be the
> >> best fit for this case control study. Do we have other options in
> addition
> >> to logistic regression?
> >> My objective is to fit the model to predict the disease status using
> >> at least two different methods.
> >> Kind regards,
> >> Hana
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> >> man_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> >> Rzsn7AkP-g&m=l7afPQ_gGAoV2EsNoYSYul0qAISEiXLmTmu0IQ03nZO4rcAi9xHZGsWww
> >> ig4oYOB&s=ztyDthknydhlcM49F33Gz6xRl6G7U9s8aIhB1VN-EKY&e=
> >> PLEASE do read the posting guide
> >> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> >> g_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> >> sRzsn7AkP-g&m=l7afPQ_gGAoV2EsNoYSYul0qAISEiXLmTmu0IQ03nZO4rcAi9xHZGsWw
> >> wig4oYOB&s=tcsGkhvtVvoVvb1Ehah-vLRC6an40rJXQXqqfX2f0gI&e=
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jin
------------------------------------------
Jin Li, PhD
Founder, Data2action, Australia
https://www.researchgate.net/profile/Jin_Li32
https://scholar.google.com/citations?user=Jeot53EAAAAJ&hl=en

	[[alternative HTML version deleted]]


From h@n@tezer@ @end|ng |rom gm@||@com  Thu Jun 16 16:58:54 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (Hana Tezera)
Date: Thu, 16 Jun 2022 17:58:54 +0300
Subject: [R] Model Comparision for case control studies in R
In-Reply-To: <CAGu_3ZK68Ai_vDhZ9kQxAuvUdug9kR0DnvegOT6Ws8TnQvNbbw@mail.gmail.com>
References: <CAEuJ0VAtsC3nHkUOiwfv7d8hA6N8q3DozStcX6=O+1juOs4fhA@mail.gmail.com>
 <BN6PR2201MB15539B16281BDD689C99EA01CFAD9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAEuJ0VAFmHj=Nk7++J296x6qth9t6OqHz_4h13K8Pxt_YK_b6w@mail.gmail.com>
 <BN6PR2201MB1553F3AB6427997A1C0A02DDCFAD9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAEuJ0VBer1N_-gS3k6frcqfMFzrdz7JQeXwHed=kpx6-G82MBg@mail.gmail.com>
 <CAGu_3ZK68Ai_vDhZ9kQxAuvUdug9kR0DnvegOT6Ws8TnQvNbbw@mail.gmail.com>
Message-ID: <CAEuJ0VDS7DQ2vNdWU8LYPPs3Px8VWOkNjLwv0tBk12pRJDuzaw@mail.gmail.com>

Dear Jin, Thanks a lot!

On 6/16/22, Jin Li <jinli68 at gmail.com> wrote:
> Hi Hana,
>
> ROC (or AUC) is misleading and should not be used to assess model
> performance. For details, please see the references in "Spatial Predictive
> Modelign with R '' that also provides some methods (e.g., gbm, rf, svm and
> glmlet) for 1/0 data along with accuracy-based variable selection and
> parameter optimisation.
>
> Hope this helps,
> Jin
>
> On Thu, Jun 16, 2022 at 6:53 AM Hana Tezera <hanatezera at gmail.com> wrote:
>
>> Dear Tim, Thanks a lot I am looking for different methods for each
>> method, I want to select the best predictors and I want to report some
>> measures of the accuracy. And I will compare the performance of the
>> models, by plotting their ROC curves.
>> Best,
>> Hana
>>
>> On 6/15/22, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>> > The uncorrelated nature of smoking and hypertension is a major medical
>> > breakthrough and in contrast to reports like this:
>> > https://pubmed.ncbi.nlm.nih.gov/20550499/ and the literature indicates
>> the
>> > possibility of a relationship between age and hypertension
>> > https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4768730/. Depending on the
>> > country, there might be a relationship between smoking and age as
>> government
>> > programs against smoking are developed.
>> >
>> > Are you looking at different models or different methods. I could have
>> > y
>> = x
>> > + y + z as one model and y=x + z as another model. Alternatively I
>> > could
>> be
>> > comparing ordinary least squares regression versus maximum likelihood
>> versus
>> > Bayesian linear regression versus nonlinear regression. The former
>> > might
>> use
>> > something like the Akaike information criterion. I am not sure the
>> latter is
>> > useful (or possible). For example I could approximate an exponential
>> > function using a polynomial, but in this context I see no benefit in
>> doing
>> > so even if I could compare the models.
>> >
>> > I do not quite understand why this is being done. It feels like fishing
>> > statistical methods to get the answer that I know is correct.
>> > Generally,
>> one
>> > should understand the system well enough to select an appropriate model
>> > rather than try every possible model in the hope something fits. Of
>> course
>> > one sometimes collects extra data in the hope that we do not miss an
>> > important feature. Then forwards/backwards/stepwise methods are used to
>> > identify the "best" model but this is looking at similar models that
>> differ
>> > only in the list of independent variables.
>> >
>> > However the problem is solved, I would start by trying to determine if
>> any
>> > one model was appropriate. Are the model assumptions satisfied? If the
>> > answer is no, then try another model until you find one that does
>> > satisfy
>> > the model assumptions. Alternatively, start with an understanding of
>> > the
>> > biology and use the best model. Comparing an biologically meaningless
>> > statistical model to a biologically meaningful one is an easy choice.
>> >
>> > Tim
>> >
>> > -----Original Message-----
>> > From: anteneh asmare <hanatezera at gmail.com>
>> > Sent: Wednesday, June 15, 2022 1:10 PM
>> > To: Ebert,Timothy Aaron <tebert at ufl.edu>
>> > Cc: r-help at r-project.org
>> > Subject: Re: [R] Model Comparision for case control studies in R
>> >
>> > [External Email]
>> >
>> > Dear Tim, Thanks. the first vector
>> > y<-c(0,1,1,0,0,1,0,0,1,1,1,0,1,1,1,0,0,0,0,1) is the disease status y=
>> > (1=Case,0=Control). The covariate age, smoking status and hypertension
>> are
>> > independent(uncorrelated). The logistic regression (unconditional) will
>> > used. But I need to compare other models with logistic regression
>> instead of
>> > fitting it directly to logistic regression.
>> > There is no matching on the data to use conditional logistics
>> > regression.
>> > Best,
>> > Hana
>> > On 6/15/22, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>> >> Disease status is missing from the sample data.
>> >> Are age, disease, smoking, and/or hypertension correlated in any way
>> >> or are they independent (correlation=0)?
>> >> Are the correlations large enough to adversely influence your model?
>> >> Tim
>> >>
>> >> -----Original Message-----
>> >> From: R-help <r-help-bounces at r-project.org> On Behalf Of anteneh
>> >> asmare
>> >> Sent: Wednesday, June 15, 2022 7:29 AM
>> >> To: r-help at r-project.org
>> >> Subject: [R] Model Comparision for case control studies in R
>> >>
>> >> [External Email]
>> >>
>> >> y<-c(0,1,1,0,0,1,0,0,1,1,1,0,1,1,1,0,0,0,0,1)
>> >> age<-c(45,23,56,67,23,23,28,56,45,47,36,37,33,35,38,39,43,28,39,41)
>> >> smoking<-c(0,1,1,1,0,0,0,0,0,1,1,0,0,1,0,1,1,1,0,1)
>> >> hypertension<-c(1,1,0,1,0,1,0,1,1,0,1,1,1,1,1,1,0,0,1,0)
>> >> data<-data.frame(y,age,smoking,hypertension)
>> >> data
>> >> model<-glm(y~age+factor(smoking)+factor(hypertension), data, family =
>> >> binomial(link = "logit"),na.action = na.omit)
>> >> summary(model)
>> >> from above sample data I want to study a case-control study on male
>> >> individuals with my response variable y, disease status (1=Case,
>> >> 0=Control) with covariates age, smoking status(1=Yes, 0=No)  and
>> >> hypertension, hypertensive (1=Yes, 0=No). I want to fit the model to
>> >> predict the disease status using at least two different methods. And
>> >> to make model comparisons. I think logistic regression will be the
>> >> best fit for this case control study. Do we have other options in
>> addition
>> >> to logistic regression?
>> >> My objective is to fit the model to predict the disease status using
>> >> at least two different methods.
>> >> Kind regards,
>> >> Hana
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
>> >> man_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
>> >> Rzsn7AkP-g&m=l7afPQ_gGAoV2EsNoYSYul0qAISEiXLmTmu0IQ03nZO4rcAi9xHZGsWww
>> >> ig4oYOB&s=ztyDthknydhlcM49F33Gz6xRl6G7U9s8aIhB1VN-EKY&e=
>> >> PLEASE do read the posting guide
>> >> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
>> >> g_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
>> >> sRzsn7AkP-g&m=l7afPQ_gGAoV2EsNoYSYul0qAISEiXLmTmu0IQ03nZO4rcAi9xHZGsWw
>> >> wig4oYOB&s=tcsGkhvtVvoVvb1Ehah-vLRC6an40rJXQXqqfX2f0gI&e=
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
> --
> Jin
> ------------------------------------------
> Jin Li, PhD
> Founder, Data2action, Australia
> https://www.researchgate.net/profile/Jin_Li32
> https://scholar.google.com/citations?user=Jeot53EAAAAJ&hl=en
>


From m@n|k@|@|v@n| @end|ng |rom gm@||@com  Thu Jun 16 11:42:31 2022
From: m@n|k@|@|v@n| @end|ng |rom gm@||@com (Kalaivani Mani)
Date: Thu, 16 Jun 2022 15:12:31 +0530
Subject: [R] Query regarding R 'irr' package 'N.cohen.kappa'
Message-ID: <CAAnBoy9U+8aH4yEEL5+6gZFpiDmBCZB4yQtMUCwSOondfPXW_A@mail.gmail.com>

Dear R-help Team,

I am from India and have a query on 'N.cohen.kappa' Sample size
calculations for Cohen's Kappa Statistic. I have calculated manually the
sample size using the formula mentioned in "Cantor, A. B. (1996)
Sample-size calculation for Cohen?s kappa. Psychological Methods, 1, 150-
153". Later came to know that it can be done using the R 'irr' package. I
got a different number.

Let us consider the following two situations:

For situation 1, the sample size is 1370 using R
Testing H0: kappa = 0.81 vs. HA: kappa> 0.95 given that kappa = 0.95 and
both raters classify 1.5% of subjects as positive.
R command used is: N.cohen.kappa(0.015, 0.015, 0.95, 0.81, alpha=0.05,
power=0.8, twosided=FALSE ).

But for the same situation, the sample size is much higher by manual
calculation, which is 8580.

For situation 2, the sample size is 74 by using R and is matching with the
manual calculation too.
Testing H0: kappa = 0.81 vs. HA: kappa> 0.95 given that kappa = 0.95 and
rater1 classify 40% of subjects and rate2 classify 50% of subjects as
positive.
R command used is: N.cohen.kappa(0.40, 0.50, 0.95, 0.81, alpha=0.05,
power=0.8, twosided=FALSE ).

I am attaching both the 'Excel sheet formula-kappa sample size situation1 &
2').

Why is this so? Please help me to sort this out.

Looking forward to hearing from you.

Best,
Kalaivani

-- 
*Dr. Kalaivani Mani, *

*M.Sc., Biostatistics (CMC, Vellore), Ph.D. (AIIMS, Delhi)*

*Scientist-IV*

*Dept. of Biostatistics*

*All India Institute of Medical Sciences*
*New Delhi-110029, India.*
*Mobile:91-9717319082*

From bgunter@4567 @end|ng |rom gm@||@com  Thu Jun 16 18:29:44 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 16 Jun 2022 09:29:44 -0700
Subject: [R] Fwd:  Query regarding R 'irr' package 'N.cohen.kappa'
In-Reply-To: <CAGxFJbS2-uLK7-iDO_SOOZ1dQHqg31gAnEZraAr6nf_FgGxP7g@mail.gmail.com>
References: <CAAnBoy9U+8aH4yEEL5+6gZFpiDmBCZB4yQtMUCwSOondfPXW_A@mail.gmail.com>
 <CAGxFJbS2-uLK7-iDO_SOOZ1dQHqg31gAnEZraAr6nf_FgGxP7g@mail.gmail.com>
Message-ID: <CAGxFJbRRGNrK0FK4S-rdDWbkOEAMTTw02A9Jb2xt0gcUNWP2OQ@mail.gmail.com>

---------- Forwarded message ---------
From: Bert Gunter <bgunter.4567 at gmail.com>
Date: Thu, Jun 16, 2022 at 9:28 AM
Subject: Re: [R] Query regarding R 'irr' package 'N.cohen.kappa'
To: Kalaivani Mani <manikalaivani at gmail.com>


Please read and follow the posting guide (linked below). No attachments
came through, because most attachments are stripped due to security
concerns. The PG tells you which attachments are allowed.

We don't do Excel here. This is R-help. Do and show your manual
calculations in R.

Cheers,
Bert

On Thu, Jun 16, 2022 at 9:20 AM Kalaivani Mani <manikalaivani at gmail.com>
wrote:

> Dear R-help Team,
>
> I am from India and have a query on 'N.cohen.kappa' Sample size
> calculations for Cohen's Kappa Statistic. I have calculated manually the
> sample size using the formula mentioned in "Cantor, A. B. (1996)
> Sample-size calculation for Cohen?s kappa. Psychological Methods, 1, 150-
> 153". Later came to know that it can be done using the R 'irr' package. I
> got a different number.
>
> Let us consider the following two situations:
>
> For situation 1, the sample size is 1370 using R
> Testing H0: kappa = 0.81 vs. HA: kappa> 0.95 given that kappa = 0.95 and
> both raters classify 1.5% of subjects as positive.
> R command used is: N.cohen.kappa(0.015, 0.015, 0.95, 0.81, alpha=0.05,
> power=0.8, twosided=FALSE ).
>
> But for the same situation, the sample size is much higher by manual
> calculation, which is 8580.
>
> For situation 2, the sample size is 74 by using R and is matching with the
> manual calculation too.
> Testing H0: kappa = 0.81 vs. HA: kappa> 0.95 given that kappa = 0.95 and
> rater1 classify 40% of subjects and rate2 classify 50% of subjects as
> positive.
> R command used is: N.cohen.kappa(0.40, 0.50, 0.95, 0.81, alpha=0.05,
> power=0.8, twosided=FALSE ).
>
> I am attaching both the 'Excel sheet formula-kappa sample size situation1 &
> 2').
>
> Why is this so? Please help me to sort this out.
>
> Looking forward to hearing from you.
>
> Best,
> Kalaivani
>
> --
> *Dr. Kalaivani Mani, *
>
> *M.Sc., Biostatistics (CMC, Vellore), Ph.D. (AIIMS, Delhi)*
>
> *Scientist-IV*
>
> *Dept. of Biostatistics*
>
> *All India Institute of Medical Sciences*
> *New Delhi-110029, India.*
> *Mobile:91-9717319082*
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Jun 16 19:25:58 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 16 Jun 2022 10:25:58 -0700
Subject: [R] Query regarding R 'irr' package 'N.cohen.kappa'
In-Reply-To: <CAAnBoy9U+8aH4yEEL5+6gZFpiDmBCZB4yQtMUCwSOondfPXW_A@mail.gmail.com>
References: <CAAnBoy9U+8aH4yEEL5+6gZFpiDmBCZB4yQtMUCwSOondfPXW_A@mail.gmail.com>
Message-ID: <A6C73D4B-9243-46CD-BD81-37B657EF88FB@dcn.davis.ca.us>

R is a fully capable computation environment... you do not have to use any contributed package to implement an algorithm. It may be convenient to rely on such resources, but you must always be aware that such code may not be correct. If you think it is not correct, then you can certainly express your validation calculations in R, and if you share such information with the maintainer of that package then they may either correct their package or suggest what considerations you may have neglected.

However, we participants on this list who happen to use R may or may not know anything about your algorithm... chances are pretty high that we don't. If you express your concerns using a reproducible R code example [1] we might take an interest... but debugging your Excel file is pretty unlikely to be of interest here (in addition to being hard to share... the mailing list strips most attachments). As the Posting Guide says, you really should be corresponding with the package maintainer. [2]

[1] https://cran.r-project.org/web/packages/reprex/vignettes/reprex-dos-and-donts.html
[2] https://cran.r-project.org/web/packages/irr/index.html

On June 16, 2022 2:42:31 AM PDT, Kalaivani Mani <manikalaivani at gmail.com> wrote:
>Dear R-help Team,
>
>I am from India and have a query on 'N.cohen.kappa' Sample size
>calculations for Cohen's Kappa Statistic. I have calculated manually the
>sample size using the formula mentioned in "Cantor, A. B. (1996)
>Sample-size calculation for Cohen?s kappa. Psychological Methods, 1, 150-
>153". Later came to know that it can be done using the R 'irr' package. I
>got a different number.
>
>Let us consider the following two situations:
>
>For situation 1, the sample size is 1370 using R
>Testing H0: kappa = 0.81 vs. HA: kappa> 0.95 given that kappa = 0.95 and
>both raters classify 1.5% of subjects as positive.
>R command used is: N.cohen.kappa(0.015, 0.015, 0.95, 0.81, alpha=0.05,
>power=0.8, twosided=FALSE ).
>
>But for the same situation, the sample size is much higher by manual
>calculation, which is 8580.
>
>For situation 2, the sample size is 74 by using R and is matching with the
>manual calculation too.
>Testing H0: kappa = 0.81 vs. HA: kappa> 0.95 given that kappa = 0.95 and
>rater1 classify 40% of subjects and rate2 classify 50% of subjects as
>positive.
>R command used is: N.cohen.kappa(0.40, 0.50, 0.95, 0.81, alpha=0.05,
>power=0.8, twosided=FALSE ).
>
>I am attaching both the 'Excel sheet formula-kappa sample size situation1 &
>2').
>
>Why is this so? Please help me to sort this out.
>
>Looking forward to hearing from you.
>
>Best,
>Kalaivani
>

-- 
Sent from my phone. Please excuse my brevity.


From mch@@tko @end|ng |rom @mgen@com  Thu Jun 16 23:22:12 2022
From: mch@@tko @end|ng |rom @mgen@com (Chastkofsky, Michael)
Date: Thu, 16 Jun 2022 21:22:12 +0000
Subject: [R] 
 [R-pkgs] r2rtf: Easily Create Production-Ready Rich Text Format
 (RTF) Table and Figure
Message-ID: <CH2PR07MB6520DF284BC221DD5C51BDB7D1AC9@CH2PR07MB6520.namprd07.prod.outlook.com>

Good afternoon.

I have been using the r2rtf package to generate tables for regulatory submissions, but have encountered some difficulties in formatting. Whenever a table is longer than one page, opening the file gives an error stating that the table has become corrupted. The file then proceeds to open normally, and it doesn't look like there is a problem. Do you know what is causing this warning? Is there a way to resolve it? Thanks you for your help.

Michael Chastkofsky


	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Fri Jun 17 01:16:32 2022
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Thu, 16 Jun 2022 16:16:32 -0700
Subject: [R] 
 [R-pkgs] r2rtf: Easily Create Production-Ready Rich Text Format
 (RTF) Table and Figure
In-Reply-To: <CH2PR07MB6520DF284BC221DD5C51BDB7D1AC9@CH2PR07MB6520.namprd07.prod.outlook.com>
References: <CH2PR07MB6520DF284BC221DD5C51BDB7D1AC9@CH2PR07MB6520.namprd07.prod.outlook.com>
Message-ID: <f65f9c0e-d259-791f-9661-e8fe84da032e@comcast.net>


On 6/16/22 14:22, Chastkofsky, Michael via R-help wrote:
> Good afternoon.
>
> I have been using the r2rtf package to generate tables for regulatory submissions, but have encountered some difficulties in formatting. Whenever a table is longer than one page, opening the file gives an error stating that the table has become corrupted. The file then proceeds to open normally, and it doesn't look like there is a problem. Do you know what is causing this warning? Is there a way to resolve it? Thanks you for your help.


This would seem to be better addressed to the package maintainer and 
when you do it might help if you offered a reproducible example.

Also Rhelp is? a plain text-only mailing list. If didn't matter here but 
if you send messages with tabular data it will probably get mangled by 
the HTML stripping process done by the mail server.

>
> Michael Chastkofsky
>
>
> 	[[alternative HTML version deleted]]
> -- 
David


From bgunter@4567 @end|ng |rom gm@||@com  Fri Jun 17 01:19:24 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 16 Jun 2022 16:19:24 -0700
Subject: [R] 
 [R-pkgs] r2rtf: Easily Create Production-Ready Rich Text Format
 (RTF) Table and Figure
In-Reply-To: <CH2PR07MB6520DF284BC221DD5C51BDB7D1AC9@CH2PR07MB6520.namprd07.prod.outlook.com>
References: <CH2PR07MB6520DF284BC221DD5C51BDB7D1AC9@CH2PR07MB6520.namprd07.prod.outlook.com>
Message-ID: <CAGxFJbTexC5CFHs6fk9hAeiKfrUXfE-bw8+_cGycfrz9ycfpoQ@mail.gmail.com>

Specialized package questions like this generally should go to the package
maintainer:
Yilong Zhang <elong0527 at gmail.com>

Bert

On Thu, Jun 16, 2022 at 4:08 PM Chastkofsky, Michael via R-help <
r-help at r-project.org> wrote:

> Good afternoon.
>
> I have been using the r2rtf package to generate tables for regulatory
> submissions, but have encountered some difficulties in formatting. Whenever
> a table is longer than one page, opening the file gives an error stating
> that the table has become corrupted. The file then proceeds to open
> normally, and it doesn't look like there is a problem. Do you know what is
> causing this warning? Is there a way to resolve it? Thanks you for your
> help.
>
> Michael Chastkofsky
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Fri Jun 17 02:10:56 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 17 Jun 2022 10:10:56 +1000
Subject: [R] Query regarding R 'irr' package 'N.cohen.kappa'
In-Reply-To: <CAAnBoy9U+8aH4yEEL5+6gZFpiDmBCZB4yQtMUCwSOondfPXW_A@mail.gmail.com>
References: <CAAnBoy9U+8aH4yEEL5+6gZFpiDmBCZB4yQtMUCwSOondfPXW_A@mail.gmail.com>
Message-ID: <CA+8X3fVFF0S7kOrAOB39yD_0nUMimd1hB5vP-LJ+bt_cm5fp-g@mail.gmail.com>

Hi Kalaivani,
The N.cohen.kappa function was written by Matthais Gamer, the
maintainer of the irr package. Both that function and N2.cohen.kappa
(written by Puspendra Singh) involve corrections that are described in
the references on the respective help pages. It is likely that there
will be small differences in the estimates for large N in the
different methods of calculation. I cannot advise which would best
suit your purpose as I only did testing and refining code in the
N2.cohen.kappa function. Perhaps corresponding directly with Matthais
Gamer would be your best option.

Jim

On Fri, Jun 17, 2022 at 2:20 AM Kalaivani Mani <manikalaivani at gmail.com> wrote:
>
> Dear R-help Team,
>
> I am from India and have a query on 'N.cohen.kappa' Sample size
> calculations for Cohen's Kappa Statistic. I have calculated manually the
> sample size using the formula mentioned in "Cantor, A. B. (1996)
> Sample-size calculation for Cohen?s kappa. Psychological Methods, 1, 150-
> 153". Later came to know that it can be done using the R 'irr' package. I
> got a different number.
>
> Let us consider the following two situations:
>
> For situation 1, the sample size is 1370 using R
> Testing H0: kappa = 0.81 vs. HA: kappa> 0.95 given that kappa = 0.95 and
> both raters classify 1.5% of subjects as positive.
> R command used is: N.cohen.kappa(0.015, 0.015, 0.95, 0.81, alpha=0.05,
> power=0.8, twosided=FALSE ).
>
> But for the same situation, the sample size is much higher by manual
> calculation, which is 8580.
>
> For situation 2, the sample size is 74 by using R and is matching with the
> manual calculation too.
> Testing H0: kappa = 0.81 vs. HA: kappa> 0.95 given that kappa = 0.95 and
> rater1 classify 40% of subjects and rate2 classify 50% of subjects as
> positive.
> R command used is: N.cohen.kappa(0.40, 0.50, 0.95, 0.81, alpha=0.05,
> power=0.8, twosided=FALSE ).
>
> I am attaching both the 'Excel sheet formula-kappa sample size situation1 &
> 2').
>
> Why is this so? Please help me to sort this out.
>
> Looking forward to hearing from you.
>
> Best,
> Kalaivani
>
> --
> *Dr. Kalaivani Mani, *
>
> *M.Sc., Biostatistics (CMC, Vellore), Ph.D. (AIIMS, Delhi)*
>
> *Scientist-IV*
>
> *Dept. of Biostatistics*
>
> *All India Institute of Medical Sciences*
> *New Delhi-110029, India.*
> *Mobile:91-9717319082*
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From |@go@g|ne @end|ng |rom @jd@e@  Fri Jun 17 11:41:32 2022
From: |@go@g|ne @end|ng |rom @jd@e@ (=?Windows-1252?Q?IAGO_GIN=C9_V=C1ZQUEZ?=)
Date: Fri, 17 Jun 2022 09:41:32 +0000
Subject: [R] Is it possible to set a default working directory for all R
 consoles?
Message-ID: <AM6PR02MB44237CC0924C7D8FC2A3E8F694AF9@AM6PR02MB4423.eurprd02.prod.outlook.com>

Hi all,

Is there some way to set a default working directory each time R.exe is opened? I ask this because Always that I open R 4.2.o in Windows 10 I get the next warning messages

Warning message:
In normalizePath(path.expand(path), winslash, mustWork) :
  path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
Warning message:
In normalizePath(path.expand(path), winslash, mustWork) :
  path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
Warning message:
In normalizePath(path.expand(path), winslash, mustWork) :
  path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
Warning message:
In normalizePath(path.expand(path), winslash, mustWork) :
  path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied

Even from cmd.exe in a  C: location

C:\Users\me>R
Warning message:
In normalizePath(path.expand(path), winslash, mustWork) :                                                                                                                                                                                       path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
Warning message:
In normalizePath(path.expand(path), winslash, mustWork) :
     path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
Warning message:
In normalizePath(path.expand(path), winslash, mustWork) :
     path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
Warning message:
In normalizePath(path.expand(path), winslash, mustWork) :
     path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied

Reading https://cran.r-project.org/bin/windows/base/rw-FAQ.html#What-are-HOME-and-working-directories_003f, first I cannot apply the suggested solution (I cannot see such "shortcut?s properties"), and second, I am not interested just in Rgui, or even RStudio, but other terminals running R (like VSCode-radian or SublimeText-Terminus)

Thanks in advance.

Best wishes,

Iago



	[[alternative HTML version deleted]]


From w||||@mwdun|@p @end|ng |rom gm@||@com  Fri Jun 17 17:46:24 2022
From: w||||@mwdun|@p @end|ng |rom gm@||@com (Bill Dunlap)
Date: Fri, 17 Jun 2022 08:46:24 -0700
Subject: [R] Is it possible to set a default working directory for all R
 consoles?
In-Reply-To: <AM6PR02MB44237CC0924C7D8FC2A3E8F694AF9@AM6PR02MB4423.eurprd02.prod.outlook.com>
References: <AM6PR02MB44237CC0924C7D8FC2A3E8F694AF9@AM6PR02MB4423.eurprd02.prod.outlook.com>
Message-ID: <CAHqSRuRa2K-ug52B4s11gJepW4GF3Y-UKn_bhRnS_iMbrkr9LA@mail.gmail.com>

Is there an environment variable containing that IP address?

     as.list(grep(value=TRUE, "172", Sys.getenv())) # as.list to make
printing nicer

If you know which variable is causing the problem you may be able to
override it by setting an R-specific one.

-Bill

On Fri, Jun 17, 2022 at 8:28 AM IAGO GIN? V?ZQUEZ <iago.gine at sjd.es> wrote:

> Hi all,
>
> Is there some way to set a default working directory each time R.exe is
> opened? I ask this because Always that I open R 4.2.o in Windows 10 I get
> the next warning messages
>
> Warning message:
> In normalizePath(path.expand(path), winslash, mustWork) :
>   path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
> Warning message:
> In normalizePath(path.expand(path), winslash, mustWork) :
>   path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
> Warning message:
> In normalizePath(path.expand(path), winslash, mustWork) :
>   path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
> Warning message:
> In normalizePath(path.expand(path), winslash, mustWork) :
>   path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
>
> Even from cmd.exe in a  C: location
>
> C:\Users\me>R
> Warning message:
> In normalizePath(path.expand(path), winslash, mustWork) :
>
>
>              path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access
> is denied
> Warning message:
> In normalizePath(path.expand(path), winslash, mustWork) :
>      path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
> denied
> Warning message:
> In normalizePath(path.expand(path), winslash, mustWork) :
>      path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
> denied
> Warning message:
> In normalizePath(path.expand(path), winslash, mustWork) :
>      path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
> denied
>
> Reading
> https://cran.r-project.org/bin/windows/base/rw-FAQ.html#What-are-HOME-and-working-directories_003f,
> first I cannot apply the suggested solution (I cannot see such "shortcut?s
> properties"), and second, I am not interested just in Rgui, or even
> RStudio, but other terminals running R (like VSCode-radian or
> SublimeText-Terminus)
>
> Thanks in advance.
>
> Best wishes,
>
> Iago
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Fri Jun 17 18:17:01 2022
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Fri, 17 Jun 2022 11:17:01 -0500
Subject: [R] Is it possible to set a default working directory for all R
 consoles?
In-Reply-To: <CAHqSRuRa2K-ug52B4s11gJepW4GF3Y-UKn_bhRnS_iMbrkr9LA@mail.gmail.com>
References: <AM6PR02MB44237CC0924C7D8FC2A3E8F694AF9@AM6PR02MB4423.eurprd02.prod.outlook.com>
 <CAHqSRuRa2K-ug52B4s11gJepW4GF3Y-UKn_bhRnS_iMbrkr9LA@mail.gmail.com>
Message-ID: <52b09cf4-6569-f0b1-e414-ff36a26619f0@effectivedefense.org>

	  I use the free version of RStudio, and I routinely work with 
"Projects".  For new projects, I first I create a new project directory 
in Finder (in a Mac) or Windows Explorer if it does not already exist. 
Then in RStudio, I do File > "New Project..." > in "Existing Directory".


	  Then when I want to work on an existing project, I can do File > 
"Recent Projects" in RStudio.  Or I can double click on the appropriate 
*.Rproj file in Finder or Windows Explorer.


	  Hope this helps.
	  Spencer Graves


On 6/17/22 10:46 AM, Bill Dunlap wrote:
> Is there an environment variable containing that IP address?
> 
>       as.list(grep(value=TRUE, "172", Sys.getenv())) # as.list to make
> printing nicer
> 
> If you know which variable is causing the problem you may be able to
> override it by setting an R-specific one.
> 
> -Bill
> 
> On Fri, Jun 17, 2022 at 8:28 AM IAGO GIN? V?ZQUEZ <iago.gine at sjd.es> wrote:
> 
>> Hi all,
>>
>> Is there some way to set a default working directory each time R.exe is
>> opened? I ask this because Always that I open R 4.2.o in Windows 10 I get
>> the next warning messages
>>
>> Warning message:
>> In normalizePath(path.expand(path), winslash, mustWork) :
>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
>> Warning message:
>> In normalizePath(path.expand(path), winslash, mustWork) :
>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
>> Warning message:
>> In normalizePath(path.expand(path), winslash, mustWork) :
>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
>> Warning message:
>> In normalizePath(path.expand(path), winslash, mustWork) :
>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
>>
>> Even from cmd.exe in a  C: location
>>
>> C:\Users\me>R
>> Warning message:
>> In normalizePath(path.expand(path), winslash, mustWork) :
>>
>>
>>               path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access
>> is denied
>> Warning message:
>> In normalizePath(path.expand(path), winslash, mustWork) :
>>       path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
>> denied
>> Warning message:
>> In normalizePath(path.expand(path), winslash, mustWork) :
>>       path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
>> denied
>> Warning message:
>> In normalizePath(path.expand(path), winslash, mustWork) :
>>       path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
>> denied
>>
>> Reading
>> https://cran.r-project.org/bin/windows/base/rw-FAQ.html#What-are-HOME-and-working-directories_003f,
>> first I cannot apply the suggested solution (I cannot see such "shortcut?s
>> properties"), and second, I am not interested just in Rgui, or even
>> RStudio, but other terminals running R (like VSCode-radian or
>> SublimeText-Terminus)
>>
>> Thanks in advance.
>>
>> Best wishes,
>>
>> Iago
>>
>>
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Jun 17 18:46:15 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 17 Jun 2022 09:46:15 -0700
Subject: [R] Is it possible to set a default working directory for all R
 consoles?
In-Reply-To: <52b09cf4-6569-f0b1-e414-ff36a26619f0@effectivedefense.org>
References: <AM6PR02MB44237CC0924C7D8FC2A3E8F694AF9@AM6PR02MB4423.eurprd02.prod.outlook.com>
 <CAHqSRuRa2K-ug52B4s11gJepW4GF3Y-UKn_bhRnS_iMbrkr9LA@mail.gmail.com>
 <52b09cf4-6569-f0b1-e414-ff36a26619f0@effectivedefense.org>
Message-ID: <6498D67D-4549-49CE-9A1F-7714C44E00CA@dcn.davis.ca.us>

While a discussion of working directories appears to address the thoughts expressed by OP, this error seems to be affecting R's ability to find a HOME directory from which to retrieve such items as the .Rprofile file and the default user directory. As such, it needs to be solved before worrying about which IDE to use or whether setting a universal working directory is a good idea.

I don't know the specific answer, but Bill's suggestion seems promising to me.

I did notice that the erroneous path also contained two nested "profile" directories... I have never seen this before, but that doesn't necessarily mean it is wrong... just seems odd.

On June 17, 2022 9:17:01 AM PDT, Spencer Graves <spencer.graves at effectivedefense.org> wrote:
>	  I use the free version of RStudio, and I routinely work with "Projects".  For new projects, I first I create a new project directory in Finder (in a Mac) or Windows Explorer if it does not already exist. Then in RStudio, I do File > "New Project..." > in "Existing Directory".
>
>
>	  Then when I want to work on an existing project, I can do File > "Recent Projects" in RStudio.  Or I can double click on the appropriate *.Rproj file in Finder or Windows Explorer.
>
>
>	  Hope this helps.
>	  Spencer Graves
>
>
>On 6/17/22 10:46 AM, Bill Dunlap wrote:
>> Is there an environment variable containing that IP address?
>> 
>>       as.list(grep(value=TRUE, "172", Sys.getenv())) # as.list to make
>> printing nicer
>> 
>> If you know which variable is causing the problem you may be able to
>> override it by setting an R-specific one.
>> 
>> -Bill
>> 
>> On Fri, Jun 17, 2022 at 8:28 AM IAGO GIN? V?ZQUEZ <iago.gine at sjd.es> wrote:
>> 
>>> Hi all,
>>> 
>>> Is there some way to set a default working directory each time R.exe is
>>> opened? I ask this because Always that I open R 4.2.o in Windows 10 I get
>>> the next warning messages
>>> 
>>> Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
>>> Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
>>> Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
>>> Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is denied
>>> 
>>> Even from cmd.exe in a  C: location
>>> 
>>> C:\Users\me>R
>>> Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>> 
>>> 
>>>               path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access
>>> is denied
>>> Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>       path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
>>> denied
>>> Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>       path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
>>> denied
>>> Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>       path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
>>> denied
>>> 
>>> Reading
>>> https://cran.r-project.org/bin/windows/base/rw-FAQ.html#What-are-HOME-and-working-directories_003f,
>>> first I cannot apply the suggested solution (I cannot see such "shortcut?s
>>> properties"), and second, I am not interested just in Rgui, or even
>>> RStudio, but other terminals running R (like VSCode-radian or
>>> SublimeText-Terminus)
>>> 
>>> Thanks in advance.
>>> 
>>> Best wishes,
>>> 
>>> Iago
>>> 
>>> 
>>> 
>>>          [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From tebert @end|ng |rom u||@edu  Fri Jun 17 19:40:28 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Fri, 17 Jun 2022 17:40:28 +0000
Subject: [R] Is it possible to set a default working directory for all R
 consoles?
In-Reply-To: <6498D67D-4549-49CE-9A1F-7714C44E00CA@dcn.davis.ca.us>
References: <AM6PR02MB44237CC0924C7D8FC2A3E8F694AF9@AM6PR02MB4423.eurprd02.prod.outlook.com>
 <CAHqSRuRa2K-ug52B4s11gJepW4GF3Y-UKn_bhRnS_iMbrkr9LA@mail.gmail.com>
 <52b09cf4-6569-f0b1-e414-ff36a26619f0@effectivedefense.org>
 <6498D67D-4549-49CE-9A1F-7714C44E00CA@dcn.davis.ca.us>
Message-ID: <BN6PR2201MB15538BA3040EE4B7D11958D2CFAF9@BN6PR2201MB1553.namprd22.prod.outlook.com>

At least in Windows having nested directories of the same name is valid. I think it unwise because of the potential for confusion, especially if multiple people will be using the directory. The other issue is that having nested directories of the same name increases path length. Some programs have trouble when a file name and path get past some character limit. Finally, long path names are more prone to typing errors. At least in R, I copy and paste path names from Explorer but then have to go back and change \ into / to get things to work.
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Jeff Newmiller
Sent: Friday, June 17, 2022 12:46 PM
To: r-help at r-project.org; Spencer Graves <spencer.graves at effectivedefense.org>
Subject: Re: [R] Is it possible to set a default working directory for all R consoles?

[External Email]

While a discussion of working directories appears to address the thoughts expressed by OP, this error seems to be affecting R's ability to find a HOME directory from which to retrieve such items as the .Rprofile file and the default user directory. As such, it needs to be solved before worrying about which IDE to use or whether setting a universal working directory is a good idea.

I don't know the specific answer, but Bill's suggestion seems promising to me.

I did notice that the erroneous path also contained two nested "profile" directories... I have never seen this before, but that doesn't necessarily mean it is wrong... just seems odd.

On June 17, 2022 9:17:01 AM PDT, Spencer Graves <spencer.graves at effectivedefense.org> wrote:
>         I use the free version of RStudio, and I routinely work with "Projects".  For new projects, I first I create a new project directory in Finder (in a Mac) or Windows Explorer if it does not already exist. Then in RStudio, I do File > "New Project..." > in "Existing Directory".
>
>
>         Then when I want to work on an existing project, I can do File > "Recent Projects" in RStudio.  Or I can double click on the appropriate *.Rproj file in Finder or Windows Explorer.
>
>
>         Hope this helps.
>         Spencer Graves
>
>
>On 6/17/22 10:46 AM, Bill Dunlap wrote:
>> Is there an environment variable containing that IP address?
>>
>>       as.list(grep(value=TRUE, "172", Sys.getenv())) # as.list to 
>> make printing nicer
>>
>> If you know which variable is causing the problem you may be able to 
>> override it by setting an R-specific one.
>>
>> -Bill
>>
>> On Fri, Jun 17, 2022 at 8:28 AM IAGO GIN? V?ZQUEZ <iago.gine at sjd.es> wrote:
>>
>>> Hi all,
>>>
>>> Is there some way to set a default working directory each time R.exe 
>>> is opened? I ask this because Always that I open R 4.2.o in Windows 
>>> 10 I get the next warning messages
>>>
>>> Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is 
>>> denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is 
>>> denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is 
>>> denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is 
>>> denied
>>>
>>> Even from cmd.exe in a  C: location
>>>
>>> C:\Users\me>R
>>> Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>
>>>
>>>               
>>> path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is 
>>> denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>       path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access 
>>> is denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>       path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access 
>>> is denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>       path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access 
>>> is denied
>>>
>>> Reading
>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dprojec
>>> t.org_bin_windows_base_rw-2DFAQ.html-23What-2Dare-2DHOME-2Dand-2Dwor
>>> king-2Ddirectories-5F003f&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh
>>> 2kVeAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrg
>>> b8AvEp2RG5r9mRz&s=ewUVpVtCgppOGT8w9h7W2lPKvYQuxVuFA9AQZo0bJ3M&e= , 
>>> first I cannot apply the suggested solution (I cannot see such 
>>> "shortcut?s properties"), and second, I am not interested just in 
>>> Rgui, or even RStudio, but other terminals running R (like 
>>> VSCode-radian or
>>> SublimeText-Terminus)
>>>
>>> Thanks in advance.
>>>
>>> Best wishes,
>>>
>>> Iago
>>>
>>>
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_ma
>>> ilman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2k
>>> VeAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8
>>> AvEp2RG5r9mRz&s=34YJoiQXMGD5d-AxYVPWqbe_AwYwHKFqj-skRXnbNo4&e=
>>> PLEASE do read the posting guide
>>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.
>>> org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2
>>> kVeAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb
>>> 8AvEp2RG5r9mRz&s=FPSw6wA0aGqiMUaFUAVMewdYBcxozs6UvBniYIe4bgw&e=
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>      [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
>> lman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVe
>> AsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8AvE
>> p2RG5r9mRz&s=34YJoiQXMGD5d-AxYVPWqbe_AwYwHKFqj-skRXnbNo4&e=
>> PLEASE do read the posting guide 
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
>> rg_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kV
>> eAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8Av
>> Ep2RG5r9mRz&s=FPSw6wA0aGqiMUaFUAVMewdYBcxozs6UvBniYIe4bgw&e=
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailm
>an_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRz
>sn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8AvEp2RG5r
>9mRz&s=34YJoiQXMGD5d-AxYVPWqbe_AwYwHKFqj-skRXnbNo4&e=
>PLEASE do read the posting guide 
>https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org
>_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsR
>zsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8AvEp2RG5
>r9mRz&s=FPSw6wA0aGqiMUaFUAVMewdYBcxozs6UvBniYIe4bgw&e=
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8AvEp2RG5r9mRz&s=34YJoiQXMGD5d-AxYVPWqbe_AwYwHKFqj-skRXnbNo4&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8AvEp2RG5r9mRz&s=FPSw6wA0aGqiMUaFUAVMewdYBcxozs6UvBniYIe4bgw&e=
and provide commented, minimal, self-contained, reproducible code.

From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Jun 17 21:01:19 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 17 Jun 2022 12:01:19 -0700
Subject: [R] 
 [R-pkgs] r2rtf: Easily Create Production-Ready Rich Text Format
 (RTF) Table and Figure
In-Reply-To: <CH2PR07MB6520DF284BC221DD5C51BDB7D1AC9@CH2PR07MB6520.namprd07.prod.outlook.com>
References: <CH2PR07MB6520DF284BC221DD5C51BDB7D1AC9@CH2PR07MB6520.namprd07.prod.outlook.com>
Message-ID: <6410A874-990A-44D3-8086-48D3FD9FA093@dcn.davis.ca.us>

You may find using the flextable package to  output to docx via officer helpful. If rtf is actually a requirement then pandoc may help.

On June 16, 2022 2:22:12 PM PDT, "Chastkofsky, Michael via R-help" <r-help at r-project.org> wrote:
>Good afternoon.
>
>I have been using the r2rtf package to generate tables for regulatory submissions, but have encountered some difficulties in formatting. Whenever a table is longer than one page, opening the file gives an error stating that the table has become corrupted. The file then proceeds to open normally, and it doesn't look like there is a problem. Do you know what is causing this warning? Is there a way to resolve it? Thanks you for your help.
>
>Michael Chastkofsky
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From cry@n @end|ng |rom b|ngh@mton@edu  Sat Jun 18 00:46:00 2022
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher W. Ryan)
Date: Fri, 17 Jun 2022 18:46:00 -0400
Subject: [R] trouble understanding syntax for glht methods
Message-ID: <beefed0a-31a7-7ac1-d450-7151d98540e9@binghamton.edu>

I'm running R 4.2.0 in Windows 10.

What is the proper syntax for plotting simultaneous CIs from a glht
object, choosing one's method of multiplicity correction?  I seem to be
able to specify a method in summary(some_glht) that but my attempts to
do so in confint() or plot() or plot(confint()) seem to have little or
no effect on the output or on the plot produced.

library(nlme)
library(multcomp)

## subset of 20 of my observations
## call it data.temp
data.temp <- structure(list(false.date = structure(c(-7915, -9772, -10168,
-7519, -9923, -8766, -8097, -10319, -10227, -8735, -10592, -10288,
-9831, -8370, -8220, -8158, -10380, -8493, -7762, -8067), class = "Date"),
    md = c(67, 252, 368, 36, 215, 170, 225, 158, 388, 166, 239,
    136, 234, 108, 27, 70, 28, 136, 128, 100), ad = c(1490, 1567,
    1541, 857, 1412, 1660, 1252, 1672, 1484, 1539, 1027, 1098,
    1413, 1575, 1175, 1620, 707, 1796, 1575, 1067), guv = c(1,
    0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1),
    season = structure(c(4L, 4L, 4L, 1L, 2L, 3L, 2L, 2L, 3L,
    3L, 3L, 2L, 3L, 3L, 1L, 2L, 1L, 2L, 2L, 3L), levels = c("summer",
    "fall", "winter", "spring"), class = "factor"), md.prop =
c(0.0449664429530201,
    0.16081684747926, 0.238805970149254, 0.0420070011668611,
    0.152266288951841, 0.102409638554217, 0.179712460063898,
    0.0944976076555024, 0.261455525606469, 0.107862248213125,
    0.232716650438169, 0.123861566484517, 0.165605095541401,
    0.0685714285714286, 0.0229787234042553, 0.0432098765432099,
    0.0396039603960396, 0.0757238307349666, 0.0812698412698413,
    0.0937207122774133), month = c(5, 4, 3, 6, 11, 1, 11, 10,
    1, 2, 1, 11, 2, 2, 7, 9, 8, 10, 10, 12), time.index = c(89L,
    28L, 15L, 102L, 23L, 61L, 83L, 10L, 13L, 62L, 1L, 11L, 26L,
    74L, 79L, 81L, 8L, 70L, 94L, 84L), guv.int = c(1L, 0L, 0L,
    1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L,
    1L, 1L), guv.fac = structure(c(2L, 1L, 1L, 2L, 1L, 2L, 2L,
    1L, 1L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L), levels = c("0",
    "1"), class = "factor"), trend.temp = c(89L, 28L, 15L, 102L,
    23L, 61L, 83L, 10L, 13L, 62L, 1L, 11L, 26L, 74L, 79L, 81L,
    8L, 70L, 94L, 84L), my.logit = c(-3.05582997869889, -1.65216285710044,
    -1.15923691048454, -3.12700417099632, -1.71693567743031,
    -2.17073296188924, -1.51829680772414, -2.25991540097043,
    -1.03841712788469, -2.11276561741143, -1.19303453792637,
    -1.95635956492965, -1.61710078517967, -2.60884355101876,
    -3.74993971087518, -3.09751496786393, -3.18841661738349,
    -2.50191799561454, -2.42521746271187, -2.2690283094652)), row.names
= c(NA,
-20L), class = c("tbl_df", "tbl", "data.frame"))

## fit an AR(1) model via gls() in nlme
m.full.temp <- gls(my.logit ~ season + guv.fac + season:guv.fac +
time.index, weights = varWeights(varFunc(~guv.fac)), correlation =
corAR1(form = ~ time.index), data = data.temp, method = "ML")

set.seed(42)

temp <- glht(m.full.temp, linfct = c("guv.fac1 >= 0", ## summer
                                "guv.fac1 + seasonfall:guv.fac1 >= 0",
                                "guv.fac1 + seasonwinter:guv.fac1 >= 0",
                               "guv.fac1 + seasonspring:guv.fac1 >= 0"))



Without correction for multiplicity, versus with e.g. bonferroni
correction, the p-values from summary() behave more or less as I'd expect:

> summary(temp, adjusted(type = "none"))
[some output truncated]

Linear Hypotheses:
                                      Estimate Std. Error z value Pr(<z)
guv.fac1 >= 0                          -0.2828     1.0911  -0.259 0.3978
guv.fac1 + seasonfall:guv.fac1 >= 0    -0.4380     0.8441  -0.519 0.3019
guv.fac1 + seasonwinter:guv.fac1 >= 0  -1.0424     0.7388  -1.411 0.0791 .
guv.fac1 + seasonspring:guv.fac1 >= 0  -1.6771     0.9436  -1.777 0.0378
(Adjusted p values reported -- none method)

> summary(temp, adjusted(type = "bonferroni"))

Linear Hypotheses:
                                      Estimate Std. Error z value Pr(<z)
guv.fac1 >= 0                          -0.2828     1.0911  -0.259  1.000
guv.fac1 + seasonfall:guv.fac1 >= 0    -0.4380     0.8441  -0.519  1.000
guv.fac1 + seasonwinter:guv.fac1 >= 0  -1.0424     0.7388  -1.411  0.317
guv.fac1 + seasonspring:guv.fac1 >= 0  -1.6771     0.9436  -1.777  0.151
(Adjusted p values reported -- bonferroni method)


But confint does not:

> confint(temp, adjusted(type = "none"))
	
Quantile = 2.0424
95% family-wise confidence level

Linear Hypotheses:
                                      Estimate lwr     upr
guv.fac1 >= 0                         -0.2828     -Inf  1.9457
guv.fac1 + seasonfall:guv.fac1 >= 0   -0.4380     -Inf  1.2858
guv.fac1 + seasonwinter:guv.fac1 >= 0 -1.0424     -Inf  0.4664
guv.fac1 + seasonspring:guv.fac1 >= 0 -1.6771     -Inf  0.2500


> confint(temp, adjusted(type = "bonferroni"))

Quantile = 2.0381
95% family-wise confidence level

Linear Hypotheses:
                                      Estimate lwr     upr
guv.fac1 >= 0                         -0.2828     -Inf  1.9411
guv.fac1 + seasonfall:guv.fac1 >= 0   -0.4380     -Inf  1.2823
guv.fac1 + seasonwinter:guv.fac1 >= 0 -1.0424     -Inf  0.4633
guv.fac1 + seasonspring:guv.fac1 >= 0 -1.6771     -Inf  0.2460

Same thing with plot(confint()). Trying to use adjusted(type =
"some_method_from_p.adjust") yields identical graphs regardless of the
adjustment method I type, or "none". What is the correct syntax for
specifying the multiplicity adjustment for displaying and plotting CIs?

Thanks.

--Chris Ryan


From h@n@tezer@ @end|ng |rom gm@||@com  Sat Jun 18 10:24:48 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (Hana Tezera)
Date: Sat, 18 Jun 2022 11:24:48 +0300
Subject: [R] error on fitting bayesian logistic in r
Message-ID: <CAEuJ0VCMwwYmPW2Re6XOGLder0ihVquixoD+uVsBZ0cnrMUs2Q@mail.gmail.com>

I try to fit Bayesian logistic regression using rstan but i got the
following error
### Model 2 Bayesian Logistic Regression  model using rstan
###the follwing package must be loded or install inorder to reproduce the code
library(dplyr)
library(rstan)
##install.packages("rstan", dependecies=TRUE)
library(readr)
setwd("C:/Users/hp/Desktop/exercise
Data/challenges_part_2/challenges_part_2/1_model_comparison/")# Change
working directory
Data <- read.table("simdata_covar.txt",colClasses=c(rep("numeric",6),rep("factor",2),rep("numeric",2)),as.is=T,na.strings=".",quote="",
header=T, sep="\t")
str(Data)
head(Data)
tail(Data)
dim(Data)
summary(Data)
summary(Data$y)
### when we look the summary of the data there is an outlies in bmi and age
### methods of detecting and removing outliers from the data.
## Using inter quartle rage
#find Q1, Q3, and interquartile range for values in column Age
Q1 <- quantile(Data$age, .25)
Q3 <- quantile(Data$age, .75)
IQR <- IQR(Data$age)
#only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3
Data_without_outliers <- subset(Data, Data$age> (Q1 - 1.5*IQR) &
Data$age< (Q3 + 1.5*IQR))
dim(Data_without_outliers)
str(Data_without_outliers)
head(Data_without_outliers)
tail(Data_without_outliers)
summary(Data_without_outliers)
sapply(Data_without_outliers, levels)##to see the level of the variables
Data1<-Data_without_outliers
dim(Data1)
str(Data1)
head(Data1)
tail(Data1)
summary(Data1)
Data_list<-list(N=dim(Data1)[1],No_pred=9,x=Data1 %>% select(age, bmi,
smoke,hypertension,diabetes ,edlevel ,region ,score ,score2
),y=Data1$y)
str(Data_list)
## returning the stan code
library(rstan)
require(rstan)
options(mc.Cores=parallel:: detectCores())
rstan_options(auto_write=TRUE)
setwd("C:/Users/hp/Desktop/exercise
Data/challenges_part_2/challenges_part_2/1_model_comparison/")#
working directory for stan code
### define the stan model
hfit<-stan(file= "logit_sta_code.stan", data=Data_list, iter=10,000, chains=4)
Error in stan_model(file, model_name = model_name, model_code = model_code,  :
  model name must match (^[[:alnum:]]{2,}.*$)|(^[A-E,G-S,U-Z,a-z].*$)|(^[F,T].+)
any one can help me with this error
Best,
Hana


From neh@@bo|ogn@90 @end|ng |rom gm@||@com  Sun Jun 19 16:53:36 2022
From: neh@@bo|ogn@90 @end|ng |rom gm@||@com (Neha gupta)
Date: Sun, 19 Jun 2022 16:53:36 +0200
Subject: [R] Rstudio cannot be loaded
Message-ID: <CA+nrPnuup+WOwgA-dR=zsu3_1OE9Y7ogNTt5LyzfBXp84V9MOw@mail.gmail.com>

Recently I updated R and now when I try to open R studio, I get

The R session failed to start.

ERROR MESSAGE
[No error available]
PROCESS OUTPUT
The R session process exited with code -1073740791.

ERRORS
[No errors emitted]
OUTPUT
[No output emitted]



22 Nov 2020 18:35:26 [rsession-] ERROR system error 10053 (An established
connection was aborted by the software in your host machine) [request-uri:
/events/get_events]; OCCURRED AT void __cdecl
rstudio::session::HttpConnectionImpl<class
rstudio_boost::asio::ip::tcp>::sendResponse(const class
rstudio::core::http::Response &)
src/cpp/session/http/SessionWin32HttpConnectionListener.cpp:113; LOGGED
FROM: void __cdecl rstudio::session::HttpConnectionImpl<class
rstudio_boost::asio::ip::tcp>::sendResponse(const class
rstudio::core::http::Response &)
src/cpp/session/http/SessionWin32HttpConnectionListener.cpp:118

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun Jun 19 17:03:18 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 19 Jun 2022 08:03:18 -0700
Subject: [R] Rstudio cannot be loaded
In-Reply-To: <CA+nrPnuup+WOwgA-dR=zsu3_1OE9Y7ogNTt5LyzfBXp84V9MOw@mail.gmail.com>
References: <CA+nrPnuup+WOwgA-dR=zsu3_1OE9Y7ogNTt5LyzfBXp84V9MOw@mail.gmail.com>
Message-ID: <CAGxFJbQNfBBB74moNM5zzbm3CD4Dsxh4wbrtb8_uLa6k0iVo3Q@mail.gmail.com>

1. This would be better posted on the RStudio help site:
https://community.rstudio.com/
(This is R-Help, not RStudio Help. RStudio and R are separate entities).

2. A standard recommendation, which may not be necessary though, would be
to delete and reload the latest version of RStudio so it can find your new
version of R.

Bert

On Sun, Jun 19, 2022 at 7:54 AM Neha gupta <neha.bologna90 at gmail.com> wrote:

> Recently I updated R and now when I try to open R studio, I get
>
> The R session failed to start.
>
> ERROR MESSAGE
> [No error available]
> PROCESS OUTPUT
> The R session process exited with code -1073740791.
>
> ERRORS
> [No errors emitted]
> OUTPUT
> [No output emitted]
>
>
>
> 22 Nov 2020 18:35:26 [rsession-] ERROR system error 10053 (An established
> connection was aborted by the software in your host machine) [request-uri:
> /events/get_events]; OCCURRED AT void __cdecl
> rstudio::session::HttpConnectionImpl<class
> rstudio_boost::asio::ip::tcp>::sendResponse(const class
> rstudio::core::http::Response &)
> src/cpp/session/http/SessionWin32HttpConnectionListener.cpp:113; LOGGED
> FROM: void __cdecl rstudio::session::HttpConnectionImpl<class
> rstudio_boost::asio::ip::tcp>::sendResponse(const class
> rstudio::core::http::Response &)
> src/cpp/session/http/SessionWin32HttpConnectionListener.cpp:118
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From neh@@bo|ogn@90 @end|ng |rom gm@||@com  Sun Jun 19 17:27:43 2022
From: neh@@bo|ogn@90 @end|ng |rom gm@||@com (Neha gupta)
Date: Sun, 19 Jun 2022 17:27:43 +0200
Subject: [R] Rstudio cannot be loaded
In-Reply-To: <CAGxFJbQNfBBB74moNM5zzbm3CD4Dsxh4wbrtb8_uLa6k0iVo3Q@mail.gmail.com>
References: <CA+nrPnuup+WOwgA-dR=zsu3_1OE9Y7ogNTt5LyzfBXp84V9MOw@mail.gmail.com>
 <CAGxFJbQNfBBB74moNM5zzbm3CD4Dsxh4wbrtb8_uLa6k0iVo3Q@mail.gmail.com>
Message-ID: <CA+nrPns6i8Rx-uNSEEEzCLN-scX3d53HRYtkpf1=_AFXhg-Kpg@mail.gmail.com>

Thank you for your answer.

I installed the new version of R and not the R studio. Should I delete R
studio and reinstall it? It will vanish all my installed packages.

On Sunday, June 19, 2022, Bert Gunter <bgunter.4567 at gmail.com> wrote:

> 1. This would be better posted on the RStudio help site:
> https://community.rstudio.com/
> (This is R-Help, not RStudio Help. RStudio and R are separate entities).
>
> 2. A standard recommendation, which may not be necessary though, would be
> to delete and reload the latest version of RStudio so it can find your new
> version of R.
>
> Bert
>
> On Sun, Jun 19, 2022 at 7:54 AM Neha gupta <neha.bologna90 at gmail.com>
> wrote:
>
>> Recently I updated R and now when I try to open R studio, I get
>>
>> The R session failed to start.
>>
>> ERROR MESSAGE
>> [No error available]
>> PROCESS OUTPUT
>> The R session process exited with code -1073740791.
>>
>> ERRORS
>> [No errors emitted]
>> OUTPUT
>> [No output emitted]
>>
>>
>>
>> 22 Nov 2020 18:35:26 [rsession-] ERROR system error 10053 (An established
>> connection was aborted by the software in your host machine) [request-uri:
>> /events/get_events]; OCCURRED AT void __cdecl
>> rstudio::session::HttpConnectionImpl<class
>> rstudio_boost::asio::ip::tcp>::sendResponse(const class
>> rstudio::core::http::Response &)
>> src/cpp/session/http/SessionWin32HttpConnectionListener.cpp:113; LOGGED
>> FROM: void __cdecl rstudio::session::HttpConnectionImpl<class
>> rstudio_boost::asio::ip::tcp>::sendResponse(const class
>> rstudio::core::http::Response &)
>> src/cpp/session/http/SessionWin32HttpConnectionListener.cpp:118
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Jun 19 17:31:18 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 19 Jun 2022 16:31:18 +0100
Subject: [R] Rstudio cannot be loaded
In-Reply-To: <CA+nrPns6i8Rx-uNSEEEzCLN-scX3d53HRYtkpf1=_AFXhg-Kpg@mail.gmail.com>
References: <CA+nrPnuup+WOwgA-dR=zsu3_1OE9Y7ogNTt5LyzfBXp84V9MOw@mail.gmail.com>
 <CAGxFJbQNfBBB74moNM5zzbm3CD4Dsxh4wbrtb8_uLa6k0iVo3Q@mail.gmail.com>
 <CA+nrPns6i8Rx-uNSEEEzCLN-scX3d53HRYtkpf1=_AFXhg-Kpg@mail.gmail.com>
Message-ID: <0b5f17c1-b1f1-5935-d337-14d435bfbdc0@sapo.pt>

Hello,

No, do not delete RStudio, just donwload and install the most up-to-date 
version.

Hope this helps,

Rui Barradas

?s 16:27 de 19/06/2022, Neha gupta escreveu:
> Thank you for your answer.
> 
> I installed the new version of R and not the R studio. Should I delete R
> studio and reinstall it? It will vanish all my installed packages.
> 
> On Sunday, June 19, 2022, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
>> 1. This would be better posted on the RStudio help site:
>> https://community.rstudio.com/
>> (This is R-Help, not RStudio Help. RStudio and R are separate entities).
>>
>> 2. A standard recommendation, which may not be necessary though, would be
>> to delete and reload the latest version of RStudio so it can find your new
>> version of R.
>>
>> Bert
>>
>> On Sun, Jun 19, 2022 at 7:54 AM Neha gupta <neha.bologna90 at gmail.com>
>> wrote:
>>
>>> Recently I updated R and now when I try to open R studio, I get
>>>
>>> The R session failed to start.
>>>
>>> ERROR MESSAGE
>>> [No error available]
>>> PROCESS OUTPUT
>>> The R session process exited with code -1073740791.
>>>
>>> ERRORS
>>> [No errors emitted]
>>> OUTPUT
>>> [No output emitted]
>>>
>>>
>>>
>>> 22 Nov 2020 18:35:26 [rsession-] ERROR system error 10053 (An established
>>> connection was aborted by the software in your host machine) [request-uri:
>>> /events/get_events]; OCCURRED AT void __cdecl
>>> rstudio::session::HttpConnectionImpl<class
>>> rstudio_boost::asio::ip::tcp>::sendResponse(const class
>>> rstudio::core::http::Response &)
>>> src/cpp/session/http/SessionWin32HttpConnectionListener.cpp:113; LOGGED
>>> FROM: void __cdecl rstudio::session::HttpConnectionImpl<class
>>> rstudio_boost::asio::ip::tcp>::sendResponse(const class
>>> rstudio::core::http::Response &)
>>> src/cpp/session/http/SessionWin32HttpConnectionListener.cpp:118
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/
>>> posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From neh@@bo|ogn@90 @end|ng |rom gm@||@com  Sun Jun 19 17:37:46 2022
From: neh@@bo|ogn@90 @end|ng |rom gm@||@com (Neha gupta)
Date: Sun, 19 Jun 2022 17:37:46 +0200
Subject: [R] Rstudio cannot be loaded
In-Reply-To: <0b5f17c1-b1f1-5935-d337-14d435bfbdc0@sapo.pt>
References: <CA+nrPnuup+WOwgA-dR=zsu3_1OE9Y7ogNTt5LyzfBXp84V9MOw@mail.gmail.com>
 <CAGxFJbQNfBBB74moNM5zzbm3CD4Dsxh4wbrtb8_uLa6k0iVo3Q@mail.gmail.com>
 <CA+nrPns6i8Rx-uNSEEEzCLN-scX3d53HRYtkpf1=_AFXhg-Kpg@mail.gmail.com>
 <0b5f17c1-b1f1-5935-d337-14d435bfbdc0@sapo.pt>
Message-ID: <CA+nrPnurY4r7+p=PWq8Gn_q6Sm_gSaycU1G06eadexTfQNnKeg@mail.gmail.com>

Okay, thank you. I will do that.

Best regards

On Sunday, June 19, 2022, Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> No, do not delete RStudio, just donwload and install the most up-to-date
> version.
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 16:27 de 19/06/2022, Neha gupta escreveu:
>
>> Thank you for your answer.
>>
>> I installed the new version of R and not the R studio. Should I delete R
>> studio and reinstall it? It will vanish all my installed packages.
>>
>> On Sunday, June 19, 2022, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>
>> 1. This would be better posted on the RStudio help site:
>>> https://community.rstudio.com/
>>> (This is R-Help, not RStudio Help. RStudio and R are separate entities).
>>>
>>> 2. A standard recommendation, which may not be necessary though, would be
>>> to delete and reload the latest version of RStudio so it can find your
>>> new
>>> version of R.
>>>
>>> Bert
>>>
>>> On Sun, Jun 19, 2022 at 7:54 AM Neha gupta <neha.bologna90 at gmail.com>
>>> wrote:
>>>
>>> Recently I updated R and now when I try to open R studio, I get
>>>>
>>>> The R session failed to start.
>>>>
>>>> ERROR MESSAGE
>>>> [No error available]
>>>> PROCESS OUTPUT
>>>> The R session process exited with code -1073740791.
>>>>
>>>> ERRORS
>>>> [No errors emitted]
>>>> OUTPUT
>>>> [No output emitted]
>>>>
>>>>
>>>>
>>>> 22 Nov 2020 18:35:26 [rsession-] ERROR system error 10053 (An
>>>> established
>>>> connection was aborted by the software in your host machine)
>>>> [request-uri:
>>>> /events/get_events]; OCCURRED AT void __cdecl
>>>> rstudio::session::HttpConnectionImpl<class
>>>> rstudio_boost::asio::ip::tcp>::sendResponse(const class
>>>> rstudio::core::http::Response &)
>>>> src/cpp/session/http/SessionWin32HttpConnectionListener.cpp:113; LOGGED
>>>> FROM: void __cdecl rstudio::session::HttpConnectionImpl<class
>>>> rstudio_boost::asio::ip::tcp>::sendResponse(const class
>>>> rstudio::core::http::Response &)
>>>> src/cpp/session/http/SessionWin32HttpConnectionListener.cpp:118
>>>>
>>>>          [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/
>>>> posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>
>>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From neh@@bo|ogn@90 @end|ng |rom gm@||@com  Sun Jun 19 18:18:49 2022
From: neh@@bo|ogn@90 @end|ng |rom gm@||@com (Neha gupta)
Date: Sun, 19 Jun 2022 18:18:49 +0200
Subject: [R] Rstudio cannot be loaded
In-Reply-To: <CA+nrPnurY4r7+p=PWq8Gn_q6Sm_gSaycU1G06eadexTfQNnKeg@mail.gmail.com>
References: <CA+nrPnuup+WOwgA-dR=zsu3_1OE9Y7ogNTt5LyzfBXp84V9MOw@mail.gmail.com>
 <CAGxFJbQNfBBB74moNM5zzbm3CD4Dsxh4wbrtb8_uLa6k0iVo3Q@mail.gmail.com>
 <CA+nrPns6i8Rx-uNSEEEzCLN-scX3d53HRYtkpf1=_AFXhg-Kpg@mail.gmail.com>
 <0b5f17c1-b1f1-5935-d337-14d435bfbdc0@sapo.pt>
 <CA+nrPnurY4r7+p=PWq8Gn_q6Sm_gSaycU1G06eadexTfQNnKeg@mail.gmail.com>
Message-ID: <CA+nrPnuZtiHAtqNRDTTFerH9LMDUnv4qhE8COdW-uCMMaSTzTw@mail.gmail.com>

@Rui Barradas <ruipbarradas at sapo.pt>

Thank you sir, I reinstalled R studio (with a newer version) and it now
works.

Best regards

On Sun, Jun 19, 2022 at 5:37 PM Neha gupta <neha.bologna90 at gmail.com> wrote:

> Okay, thank you. I will do that.
>
> Best regards
>
> On Sunday, June 19, 2022, Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
>> Hello,
>>
>> No, do not delete RStudio, just donwload and install the most up-to-date
>> version.
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>> ?s 16:27 de 19/06/2022, Neha gupta escreveu:
>>
>>> Thank you for your answer.
>>>
>>> I installed the new version of R and not the R studio. Should I delete R
>>> studio and reinstall it? It will vanish all my installed packages.
>>>
>>> On Sunday, June 19, 2022, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>>
>>> 1. This would be better posted on the RStudio help site:
>>>> https://community.rstudio.com/
>>>> (This is R-Help, not RStudio Help. RStudio and R are separate entities).
>>>>
>>>> 2. A standard recommendation, which may not be necessary though, would
>>>> be
>>>> to delete and reload the latest version of RStudio so it can find your
>>>> new
>>>> version of R.
>>>>
>>>> Bert
>>>>
>>>> On Sun, Jun 19, 2022 at 7:54 AM Neha gupta <neha.bologna90 at gmail.com>
>>>> wrote:
>>>>
>>>> Recently I updated R and now when I try to open R studio, I get
>>>>>
>>>>> The R session failed to start.
>>>>>
>>>>> ERROR MESSAGE
>>>>> [No error available]
>>>>> PROCESS OUTPUT
>>>>> The R session process exited with code -1073740791.
>>>>>
>>>>> ERRORS
>>>>> [No errors emitted]
>>>>> OUTPUT
>>>>> [No output emitted]
>>>>>
>>>>>
>>>>>
>>>>> 22 Nov 2020 18:35:26 [rsession-] ERROR system error 10053 (An
>>>>> established
>>>>> connection was aborted by the software in your host machine)
>>>>> [request-uri:
>>>>> /events/get_events]; OCCURRED AT void __cdecl
>>>>> rstudio::session::HttpConnectionImpl<class
>>>>> rstudio_boost::asio::ip::tcp>::sendResponse(const class
>>>>> rstudio::core::http::Response &)
>>>>> src/cpp/session/http/SessionWin32HttpConnectionListener.cpp:113; LOGGED
>>>>> FROM: void __cdecl rstudio::session::HttpConnectionImpl<class
>>>>> rstudio_boost::asio::ip::tcp>::sendResponse(const class
>>>>> rstudio::core::http::Response &)
>>>>> src/cpp/session/http/SessionWin32HttpConnectionListener.cpp:118
>>>>>
>>>>>          [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/
>>>>> posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>>
>>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>

	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Sun Jun 19 18:52:47 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Sun, 19 Jun 2022 16:52:47 +0000
Subject: [R] Rstudio cannot be loaded
In-Reply-To: <CA+nrPns6i8Rx-uNSEEEzCLN-scX3d53HRYtkpf1=_AFXhg-Kpg@mail.gmail.com>
References: <CA+nrPnuup+WOwgA-dR=zsu3_1OE9Y7ogNTt5LyzfBXp84V9MOw@mail.gmail.com>
 <CAGxFJbQNfBBB74moNM5zzbm3CD4Dsxh4wbrtb8_uLa6k0iVo3Q@mail.gmail.com>
 <CA+nrPns6i8Rx-uNSEEEzCLN-scX3d53HRYtkpf1=_AFXhg-Kpg@mail.gmail.com>
Message-ID: <BN6PR2201MB1553EB1FC4CE5555F6DC47FDCFB19@BN6PR2201MB1553.namprd22.prod.outlook.com>

Update RStudio through RStudio. In the "help" tab there is a choice "check for updates." This approach will preserve all your files. I use if(!require(package)){install.packages("package")}
So that I always install a package I need but only if it is not already installed.

Tim


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Neha gupta
Sent: Sunday, June 19, 2022 11:28 AM
To: Bert Gunter <bgunter.4567 at gmail.com>
Cc: r-help mailing list <r-help at r-project.org>
Subject: Re: [R] Rstudio cannot be loaded

[External Email]

Thank you for your answer.

I installed the new version of R and not the R studio. Should I delete R studio and reinstall it? It will vanish all my installed packages.

On Sunday, June 19, 2022, Bert Gunter <bgunter.4567 at gmail.com> wrote:

> 1. This would be better posted on the RStudio help site:
> https://urldefense.proofpoint.com/v2/url?u=https-3A__community.rstudio
> .com_&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=hBF
> mb0Qv_oQQnQxsO4Pt3QXe99R9mHafWJnOJQ8fn-lvk9dBvX8FHU5-QQyD5GT6&s=m1OLv6
> TEBTF1_qs5_It8GNdL9O7H3C-1K74GC4kVlnY&e=
> (This is R-Help, not RStudio Help. RStudio and R are separate entities).
>
> 2. A standard recommendation, which may not be necessary though, would 
> be to delete and reload the latest version of RStudio so it can find 
> your new version of R.
>
> Bert
>
> On Sun, Jun 19, 2022 at 7:54 AM Neha gupta <neha.bologna90 at gmail.com>
> wrote:
>
>> Recently I updated R and now when I try to open R studio, I get
>>
>> The R session failed to start.
>>
>> ERROR MESSAGE
>> [No error available]
>> PROCESS OUTPUT
>> The R session process exited with code -1073740791.
>>
>> ERRORS
>> [No errors emitted]
>> OUTPUT
>> [No output emitted]
>>
>>
>>
>> 22 Nov 2020 18:35:26 [rsession-] ERROR system error 10053 (An 
>> established connection was aborted by the software in your host machine) [request-uri:
>> /events/get_events]; OCCURRED AT void __cdecl 
>> rstudio::session::HttpConnectionImpl<class
>> rstudio_boost::asio::ip::tcp>::sendResponse(const class 
>> rstudio::core::http::Response &) 
>> src/cpp/session/http/SessionWin32HttpConnectionListener.cpp:113; 
>> LOGGED
>> FROM: void __cdecl rstudio::session::HttpConnectionImpl<class
>> rstudio_boost::asio::ip::tcp>::sendResponse(const class 
>> rstudio::core::http::Response &)
>> src/cpp/session/http/SessionWin32HttpConnectionListener.cpp:118
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
>> lman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVe
>> AsRzsn7AkP-g&m=hBFmb0Qv_oQQnQxsO4Pt3QXe99R9mHafWJnOJQ8fn-lvk9dBvX8FHU
>> 5-QQyD5GT6&s=QJZc8GHayEQv-3Zd2zLp_t3VMrFpLnxZ6Pky7TRRuKg&e=
>> PLEASE do read the posting guide 
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
>> rg_&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=hBFm
>> b0Qv_oQQnQxsO4Pt3QXe99R9mHafWJnOJQ8fn-lvk9dBvX8FHU5-QQyD5GT6&s=UaZECA
>> EKTxKV2WfnwFGMKmekyC5i1G5gJaSUeW015RE&e=
>> posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=hBFmb0Qv_oQQnQxsO4Pt3QXe99R9mHafWJnOJQ8fn-lvk9dBvX8FHU5-QQyD5GT6&s=QJZc8GHayEQv-3Zd2zLp_t3VMrFpLnxZ6Pky7TRRuKg&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=hBFmb0Qv_oQQnQxsO4Pt3QXe99R9mHafWJnOJQ8fn-lvk9dBvX8FHU5-QQyD5GT6&s=WNUx1emVL5m9CjR_q4n5rCB41XeJpCoK62-nE_ftSXI&e=
and provide commented, minimal, self-contained, reproducible code.


From bog@@o@chr|@to|er @end|ng |rom gm@||@com  Sun Jun 19 19:23:30 2022
From: bog@@o@chr|@to|er @end|ng |rom gm@||@com (Christofer Bogaso)
Date: Sun, 19 Jun 2022 22:53:30 +0530
Subject: [R] Obtaining the source code
Message-ID: <CA+dpOJ=Fw47kmin7eO51dTOqh1W+g8XzidRYp6YPage6d1W42Q@mail.gmail.com>

Hi,

I am trying to see the source code of rstandard function. I tried below,

> methods('rstandard')

[1] rstandard.glm* rstandard.lm*

What do I need to do if I want to see the source code of rstandard.lm*?

Thanks for your help.


From j|ox @end|ng |rom mcm@@ter@c@  Sun Jun 19 19:34:32 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Sun, 19 Jun 2022 13:34:32 -0400
Subject: [R] Obtaining the source code
In-Reply-To: <4589_1655659441_25JHO1h1028824_CA+dpOJ=Fw47kmin7eO51dTOqh1W+g8XzidRYp6YPage6d1W42Q@mail.gmail.com>
References: <4589_1655659441_25JHO1h1028824_CA+dpOJ=Fw47kmin7eO51dTOqh1W+g8XzidRYp6YPage6d1W42Q@mail.gmail.com>
Message-ID: <ae741fba-2f8f-e733-e26e-2d6485af9d0f@mcmaster.ca>

Dear Cristofer,

 > stats:::rstandard.lm
function (model, infl = lm.influence(model, do.coef = FALSE),
     sd = sqrt(deviance(model)/df.residual(model)), type = c("sd.1",
         "predictive"), ...)
{
     type <- match.arg(type)
     res <- infl$wt.res/switch(type, sd.1 = c(outer(sqrt(1 - infl$hat),
         sd)), predictive = 1 - infl$hat)
     res[is.infinite(res)] <- NaN
     res
}
<bytecode: 0x114ed5740>
<environment: namespace:stats>

More generally, use ::: for an object that's hidden in a package namespace.

I hope this helps,
  John

On 2022-06-19 1:23 p.m., Christofer Bogaso wrote:
> Hi,
> 
> I am trying to see the source code of rstandard function. I tried below,
> 
>> methods('rstandard')
> 
> [1] rstandard.glm* rstandard.lm*
> 
> What do I need to do if I want to see the source code of rstandard.lm*?
> 
> Thanks for your help.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


From @kw@|mmo @end|ng |rom gm@||@com  Sun Jun 19 19:37:41 2022
From: @kw@|mmo @end|ng |rom gm@||@com (Andrew Simmons)
Date: Sun, 19 Jun 2022 13:37:41 -0400
Subject: [R] Obtaining the source code
In-Reply-To: <CA+dpOJ=Fw47kmin7eO51dTOqh1W+g8XzidRYp6YPage6d1W42Q@mail.gmail.com>
References: <CA+dpOJ=Fw47kmin7eO51dTOqh1W+g8XzidRYp6YPage6d1W42Q@mail.gmail.com>
Message-ID: <CAPcHnpQq14esPtVpJ=frKuyjizcvQXD-12+04pBOpCkPAoVnug@mail.gmail.com>

You can use getAnywhere

On Sun, Jun 19, 2022, 13:23 Christofer Bogaso <bogaso.christofer at gmail.com>
wrote:

> Hi,
>
> I am trying to see the source code of rstandard function. I tried below,
>
> > methods('rstandard')
>
> [1] rstandard.glm* rstandard.lm*
>
> What do I need to do if I want to see the source code of rstandard.lm*?
>
> Thanks for your help.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From e@ @end|ng |rom enr|co@chum@nn@net  Sun Jun 19 19:41:43 2022
From: e@ @end|ng |rom enr|co@chum@nn@net (Enrico Schumann)
Date: Sun, 19 Jun 2022 19:41:43 +0200
Subject: [R] Obtaining the source code
In-Reply-To: <CA+dpOJ=Fw47kmin7eO51dTOqh1W+g8XzidRYp6YPage6d1W42Q@mail.gmail.com>
 (Christofer Bogaso's message of "Sun, 19 Jun 2022 22:53:30 +0530")
References: <CA+dpOJ=Fw47kmin7eO51dTOqh1W+g8XzidRYp6YPage6d1W42Q@mail.gmail.com>
Message-ID: <87czf4zh2g.fsf@enricoschumann.net>

On Sun, 19 Jun 2022, Christofer Bogaso writes:

> Hi,
>
> I am trying to see the source code of rstandard function. I tried below,
>
>> methods('rstandard')
>
> [1] rstandard.glm* rstandard.lm*
>
> What do I need to do if I want to see the source code of rstandard.lm*?
>
> Thanks for your help.

Since someone took the time to add this topic to the
FAQs, I think the answer deserves being mentioned:

  https://cran.r-project.org/doc/FAQ/R-FAQ.html#How-do-I-access-the-source-code-for-a-function_003f


-- 
Enrico Schumann
Lucerne, Switzerland
http://enricoschumann.net


From neh@@bo|ogn@90 @end|ng |rom gm@||@com  Sun Jun 19 22:52:33 2022
From: neh@@bo|ogn@90 @end|ng |rom gm@||@com (Neha gupta)
Date: Sun, 19 Jun 2022 22:52:33 +0200
Subject: [R] Rstudio cannot be loaded
In-Reply-To: <BN6PR2201MB1553EB1FC4CE5555F6DC47FDCFB19@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CA+nrPnuup+WOwgA-dR=zsu3_1OE9Y7ogNTt5LyzfBXp84V9MOw@mail.gmail.com>
 <CAGxFJbQNfBBB74moNM5zzbm3CD4Dsxh4wbrtb8_uLa6k0iVo3Q@mail.gmail.com>
 <CA+nrPns6i8Rx-uNSEEEzCLN-scX3d53HRYtkpf1=_AFXhg-Kpg@mail.gmail.com>
 <BN6PR2201MB1553EB1FC4CE5555F6DC47FDCFB19@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <CA+nrPnvMcE2v_vLz4=WLwbhVvL6K2o2aLtaap7AgHpoJvmtQbQ@mail.gmail.com>

@Elbert

Thank you so much, I got your point.

Best regards

On Sun, Jun 19, 2022 at 6:54 PM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:

> Update RStudio through RStudio. In the "help" tab there is a choice "check
> for updates." This approach will preserve all your files. I use
> if(!require(package)){install.packages("package")}
> So that I always install a package I need but only if it is not already
> installed.
>
> Tim
>
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Neha gupta
> Sent: Sunday, June 19, 2022 11:28 AM
> To: Bert Gunter <bgunter.4567 at gmail.com>
> Cc: r-help mailing list <r-help at r-project.org>
> Subject: Re: [R] Rstudio cannot be loaded
>
> [External Email]
>
> Thank you for your answer.
>
> I installed the new version of R and not the R studio. Should I delete R
> studio and reinstall it? It will vanish all my installed packages.
>
> On Sunday, June 19, 2022, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> > 1. This would be better posted on the RStudio help site:
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__community.rstudio
> > .com_&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=hBF
> > mb0Qv_oQQnQxsO4Pt3QXe99R9mHafWJnOJQ8fn-lvk9dBvX8FHU5-QQyD5GT6&s=m1OLv6
> > TEBTF1_qs5_It8GNdL9O7H3C-1K74GC4kVlnY&e=
> > (This is R-Help, not RStudio Help. RStudio and R are separate entities).
> >
> > 2. A standard recommendation, which may not be necessary though, would
> > be to delete and reload the latest version of RStudio so it can find
> > your new version of R.
> >
> > Bert
> >
> > On Sun, Jun 19, 2022 at 7:54 AM Neha gupta <neha.bologna90 at gmail.com>
> > wrote:
> >
> >> Recently I updated R and now when I try to open R studio, I get
> >>
> >> The R session failed to start.
> >>
> >> ERROR MESSAGE
> >> [No error available]
> >> PROCESS OUTPUT
> >> The R session process exited with code -1073740791.
> >>
> >> ERRORS
> >> [No errors emitted]
> >> OUTPUT
> >> [No output emitted]
> >>
> >>
> >>
> >> 22 Nov 2020 18:35:26 [rsession-] ERROR system error 10053 (An
> >> established connection was aborted by the software in your host
> machine) [request-uri:
> >> /events/get_events]; OCCURRED AT void __cdecl
> >> rstudio::session::HttpConnectionImpl<class
> >> rstudio_boost::asio::ip::tcp>::sendResponse(const class
> >> rstudio::core::http::Response &)
> >> src/cpp/session/http/SessionWin32HttpConnectionListener.cpp:113;
> >> LOGGED
> >> FROM: void __cdecl rstudio::session::HttpConnectionImpl<class
> >> rstudio_boost::asio::ip::tcp>::sendResponse(const class
> >> rstudio::core::http::Response &)
> >> src/cpp/session/http/SessionWin32HttpConnectionListener.cpp:118
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
> >> lman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVe
> >> AsRzsn7AkP-g&m=hBFmb0Qv_oQQnQxsO4Pt3QXe99R9mHafWJnOJQ8fn-lvk9dBvX8FHU
> >> 5-QQyD5GT6&s=QJZc8GHayEQv-3Zd2zLp_t3VMrFpLnxZ6Pky7TRRuKg&e=
> >> PLEASE do read the posting guide
> >> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
> >> rg_&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=hBFm
> >> b0Qv_oQQnQxsO4Pt3QXe99R9mHafWJnOJQ8fn-lvk9dBvX8FHU5-QQyD5GT6&s=UaZECA
> >> EKTxKV2WfnwFGMKmekyC5i1G5gJaSUeW015RE&e=
> >> posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=hBFmb0Qv_oQQnQxsO4Pt3QXe99R9mHafWJnOJQ8fn-lvk9dBvX8FHU5-QQyD5GT6&s=QJZc8GHayEQv-3Zd2zLp_t3VMrFpLnxZ6Pky7TRRuKg&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=hBFmb0Qv_oQQnQxsO4Pt3QXe99R9mHafWJnOJQ8fn-lvk9dBvX8FHU5-QQyD5GT6&s=WNUx1emVL5m9CjR_q4n5rCB41XeJpCoK62-nE_ftSXI&e=
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From |@go@g|ne @end|ng |rom @jd@e@  Mon Jun 20 09:22:57 2022
From: |@go@g|ne @end|ng |rom @jd@e@ (=?Windows-1252?Q?IAGO_GIN=C9_V=C1ZQUEZ?=)
Date: Mon, 20 Jun 2022 07:22:57 +0000
Subject: [R] Is it possible to set a default working directory for all R
 consoles?
In-Reply-To: <BN6PR2201MB15538BA3040EE4B7D11958D2CFAF9@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <AM6PR02MB44237CC0924C7D8FC2A3E8F694AF9@AM6PR02MB4423.eurprd02.prod.outlook.com>
 <CAHqSRuRa2K-ug52B4s11gJepW4GF3Y-UKn_bhRnS_iMbrkr9LA@mail.gmail.com>
 <52b09cf4-6569-f0b1-e414-ff36a26619f0@effectivedefense.org>
 <6498D67D-4549-49CE-9A1F-7714C44E00CA@dcn.davis.ca.us>
 <BN6PR2201MB15538BA3040EE4B7D11958D2CFAF9@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <AM6PR02MB4423A41909CBEC859BD4FFC394B09@AM6PR02MB4423.eurprd02.prod.outlook.com>

Hi all,

Thanks for your answers.

Bill, indeed I get two environment variables containing that IP address, HOME and R_USER. I do not know how may I change them. If I go to "Control Panel > User accounts > Change my environment variables" neither of those variables appear to be defined (I assume the HOME variable does not appear among the environment variables of the computer, as it relates to an address seemingly outside the PC). Beyond that, I see that there is a .Renviron file in that folder ("\\172.19.2.44\profiles\profiles\me\Documents"), while there is a Rprofile.site (only comments) file in R_HOME\etc (and there is no .Rprofile file).

Regarding the 'two nested "profile" directories', I cannot do anything on that, as it relates to an address I cannot modify as it depends not on me but on the IT department at my workplace.

How should I proceed?

Thanks!
________________________________
De: R-help <r-help-bounces at r-project.org> de part de Ebert,Timothy Aaron <tebert at ufl.edu>
Enviat el: divendres, 17 de juny de 2022 19:40
Per a: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>; r-help at r-project.org <r-help at r-project.org>; Spencer Graves <spencer.graves at effectivedefense.org>
Tema: Re: [R] Is it possible to set a default working directory for all R consoles?

At least in Windows having nested directories of the same name is valid. I think it unwise because of the potential for confusion, especially if multiple people will be using the directory. The other issue is that having nested directories of the same name increases path length. Some programs have trouble when a file name and path get past some character limit. Finally, long path names are more prone to typing errors. At least in R, I copy and paste path names from Explorer but then have to go back and change \ into / to get things to work.
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Jeff Newmiller
Sent: Friday, June 17, 2022 12:46 PM
To: r-help at r-project.org; Spencer Graves <spencer.graves at effectivedefense.org>
Subject: Re: [R] Is it possible to set a default working directory for all R consoles?

[External Email]

While a discussion of working directories appears to address the thoughts expressed by OP, this error seems to be affecting R's ability to find a HOME directory from which to retrieve such items as the .Rprofile file and the default user directory. As such, it needs to be solved before worrying about which IDE to use or whether setting a universal working directory is a good idea.

I don't know the specific answer, but Bill's suggestion seems promising to me.

I did notice that the erroneous path also contained two nested "profile" directories... I have never seen this before, but that doesn't necessarily mean it is wrong... just seems odd.

On June 17, 2022 9:17:01 AM PDT, Spencer Graves <spencer.graves at effectivedefense.org> wrote:
>         I use the free version of RStudio, and I routinely work with "Projects".  For new projects, I first I create a new project directory in Finder (in a Mac) or Windows Explorer if it does not already exist. Then in RStudio, I do File > "New Project..." > in "Existing Directory".
>
>
>         Then when I want to work on an existing project, I can do File > "Recent Projects" in RStudio.  Or I can double click on the appropriate *.Rproj file in Finder or Windows Explorer.
>
>
>         Hope this helps.
>         Spencer Graves
>
>
>On 6/17/22 10:46 AM, Bill Dunlap wrote:
>> Is there an environment variable containing that IP address?
>>
>>       as.list(grep(value=TRUE, "172", Sys.getenv())) # as.list to
>> make printing nicer
>>
>> If you know which variable is causing the problem you may be able to
>> override it by setting an R-specific one.
>>
>> -Bill
>>
>> On Fri, Jun 17, 2022 at 8:28 AM IAGO GIN? V?ZQUEZ <iago.gine at sjd.es> wrote:
>>
>>> Hi all,
>>>
>>> Is there some way to set a default working directory each time R.exe
>>> is opened? I ask this because Always that I open R 4.2.o in Windows
>>> 10 I get the next warning messages
>>>
>>> Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
>>> denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
>>> denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
>>> denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
>>> denied
>>>
>>> Even from cmd.exe in a  C: location
>>>
>>> C:\Users\me>R
>>> Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>
>>>
>>>
>>> path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
>>> denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>       path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access
>>> is denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>       path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access
>>> is denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>       path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access
>>> is denied
>>>
>>> Reading
>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dprojec
>>> t.org_bin_windows_base_rw-2DFAQ.html-23What-2Dare-2DHOME-2Dand-2Dwor
>>> king-2Ddirectories-5F003f&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh
>>> 2kVeAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrg
>>> b8AvEp2RG5r9mRz&s=ewUVpVtCgppOGT8w9h7W2lPKvYQuxVuFA9AQZo0bJ3M&e= ,
>>> first I cannot apply the suggested solution (I cannot see such
>>> "shortcut?s properties"), and second, I am not interested just in
>>> Rgui, or even RStudio, but other terminals running R (like
>>> VSCode-radian or
>>> SublimeText-Terminus)
>>>
>>> Thanks in advance.
>>>
>>> Best wishes,
>>>
>>> Iago
>>>
>>>
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_ma
>>> ilman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2k
>>> VeAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8
>>> AvEp2RG5r9mRz&s=34YJoiQXMGD5d-AxYVPWqbe_AwYwHKFqj-skRXnbNo4&e=
>>> PLEASE do read the posting guide
>>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.
>>> org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2
>>> kVeAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb
>>> 8AvEp2RG5r9mRz&s=FPSw6wA0aGqiMUaFUAVMewdYBcxozs6UvBniYIe4bgw&e=
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>      [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
>> lman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVe
>> AsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8AvE
>> p2RG5r9mRz&s=34YJoiQXMGD5d-AxYVPWqbe_AwYwHKFqj-skRXnbNo4&e=
>> PLEASE do read the posting guide
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
>> rg_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kV
>> eAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8Av
>> Ep2RG5r9mRz&s=FPSw6wA0aGqiMUaFUAVMewdYBcxozs6UvBniYIe4bgw&e=
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailm
>an_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRz
>sn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8AvEp2RG5r
>9mRz&s=34YJoiQXMGD5d-AxYVPWqbe_AwYwHKFqj-skRXnbNo4&e=
>PLEASE do read the posting guide
>https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org
>_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsR
>zsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8AvEp2RG5
>r9mRz&s=FPSw6wA0aGqiMUaFUAVMewdYBcxozs6UvBniYIe4bgw&e=
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8AvEp2RG5r9mRz&s=34YJoiQXMGD5d-AxYVPWqbe_AwYwHKFqj-skRXnbNo4&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8AvEp2RG5r9mRz&s=FPSw6wA0aGqiMUaFUAVMewdYBcxozs6UvBniYIe4bgw&e=
and provide commented, minimal, self-contained, reproducible code.
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From |@go@g|ne @end|ng |rom @jd@e@  Mon Jun 20 09:34:22 2022
From: |@go@g|ne @end|ng |rom @jd@e@ (=?utf-8?B?SUFHTyBHSU7DiSBWw4FaUVVFWg==?=)
Date: Mon, 20 Jun 2022 07:34:22 +0000
Subject: [R] Is it possible to set a default working directory for all R
 consoles?
In-Reply-To: <AM6PR02MB4423A41909CBEC859BD4FFC394B09@AM6PR02MB4423.eurprd02.prod.outlook.com>
References: <AM6PR02MB44237CC0924C7D8FC2A3E8F694AF9@AM6PR02MB4423.eurprd02.prod.outlook.com>
 <CAHqSRuRa2K-ug52B4s11gJepW4GF3Y-UKn_bhRnS_iMbrkr9LA@mail.gmail.com>
 <52b09cf4-6569-f0b1-e414-ff36a26619f0@effectivedefense.org>
 <6498D67D-4549-49CE-9A1F-7714C44E00CA@dcn.davis.ca.us>
 <BN6PR2201MB15538BA3040EE4B7D11958D2CFAF9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <AM6PR02MB4423A41909CBEC859BD4FFC394B09@AM6PR02MB4423.eurprd02.prod.outlook.com>
Message-ID: <AM6PR02MB4423F9AD70CC33EA9062B5D994B09@AM6PR02MB4423.eurprd02.prod.outlook.com>

Just a remark/correction. Sometimes the message I get after the addres is "The filename, directory name, or volume label syntax is incorrect" instead of "Access is denied".

________________________________
De: R-help <r-help-bounces at r-project.org> de part de IAGO GIN? V?ZQUEZ <iago.gine at sjd.es>
Enviat el: dilluns, 20 de juny de 2022 9:22
Per a: Ebert,Timothy Aaron <tebert at ufl.edu>; Jeff Newmiller <jdnewmil at dcn.davis.ca.us>; r-help at r-project.org <r-help at r-project.org>; Spencer Graves <spencer.graves at effectivedefense.org>
Tema: Re: [R] Is it possible to set a default working directory for all R consoles?

Hi all,

Thanks for your answers.

Bill, indeed I get two environment variables containing that IP address, HOME and R_USER. I do not know how may I change them. If I go to "Control Panel > User accounts > Change my environment variables" neither of those variables appear to be defined (I assume the HOME variable does not appear among the environment variables of the computer, as it relates to an address seemingly outside the PC). Beyond that, I see that there is a .Renviron file in that folder ("\\172.19.2.44\profiles\profiles\me\Documents<file://\\172.19.2.44\profiles\profiles\me\Documents>"), while there is a Rprofile.site (only comments) file in R_HOME\etc (and there is no .Rprofile file).

Regarding the 'two nested "profile" directories', I cannot do anything on that, as it relates to an address I cannot modify as it depends not on me but on the IT department at my workplace.

How should I proceed?

Thanks!
________________________________
De: R-help <r-help-bounces at r-project.org> de part de Ebert,Timothy Aaron <tebert at ufl.edu>
Enviat el: divendres, 17 de juny de 2022 19:40
Per a: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>; r-help at r-project.org <r-help at r-project.org>; Spencer Graves <spencer.graves at effectivedefense.org>
Tema: Re: [R] Is it possible to set a default working directory for all R consoles?

At least in Windows having nested directories of the same name is valid. I think it unwise because of the potential for confusion, especially if multiple people will be using the directory. The other issue is that having nested directories of the same name increases path length. Some programs have trouble when a file name and path get past some character limit. Finally, long path names are more prone to typing errors. At least in R, I copy and paste path names from Explorer but then have to go back and change \ into / to get things to work.
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Jeff Newmiller
Sent: Friday, June 17, 2022 12:46 PM
To: r-help at r-project.org; Spencer Graves <spencer.graves at effectivedefense.org>
Subject: Re: [R] Is it possible to set a default working directory for all R consoles?

[External Email]

While a discussion of working directories appears to address the thoughts expressed by OP, this error seems to be affecting R's ability to find a HOME directory from which to retrieve such items as the .Rprofile file and the default user directory. As such, it needs to be solved before worrying about which IDE to use or whether setting a universal working directory is a good idea.

I don't know the specific answer, but Bill's suggestion seems promising to me.

I did notice that the erroneous path also contained two nested "profile" directories... I have never seen this before, but that doesn't necessarily mean it is wrong... just seems odd.

On June 17, 2022 9:17:01 AM PDT, Spencer Graves <spencer.graves at effectivedefense.org> wrote:
>         I use the free version of RStudio, and I routinely work with "Projects".  For new projects, I first I create a new project directory in Finder (in a Mac) or Windows Explorer if it does not already exist. Then in RStudio, I do File > "New Project..." > in "Existing Directory".
>
>
>         Then when I want to work on an existing project, I can do File > "Recent Projects" in RStudio.  Or I can double click on the appropriate *.Rproj file in Finder or Windows Explorer.
>
>
>         Hope this helps.
>         Spencer Graves
>
>
>On 6/17/22 10:46 AM, Bill Dunlap wrote:
>> Is there an environment variable containing that IP address?
>>
>>       as.list(grep(value=TRUE, "172", Sys.getenv())) # as.list to
>> make printing nicer
>>
>> If you know which variable is causing the problem you may be able to
>> override it by setting an R-specific one.
>>
>> -Bill
>>
>> On Fri, Jun 17, 2022 at 8:28 AM IAGO GIN? V?ZQUEZ <iago.gine at sjd.es> wrote:
>>
>>> Hi all,
>>>
>>> Is there some way to set a default working directory each time R.exe
>>> is opened? I ask this because Always that I open R 4.2.o in Windows
>>> 10 I get the next warning messages
>>>
>>> Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
>>> denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
>>> denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
>>> denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>    path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
>>> denied
>>>
>>> Even from cmd.exe in a  C: location
>>>
>>> C:\Users\me>R
>>> Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>
>>>
>>>
>>> path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access is
>>> denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>       path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access
>>> is denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>       path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access
>>> is denied Warning message:
>>> In normalizePath(path.expand(path), winslash, mustWork) :
>>>       path[1]="\\172.19.2.44\profiles\profiles\me\Documents": Access
>>> is denied
>>>
>>> Reading
>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dprojec
>>> t.org_bin_windows_base_rw-2DFAQ.html-23What-2Dare-2DHOME-2Dand-2Dwor
>>> king-2Ddirectories-5F003f&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh
>>> 2kVeAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrg
>>> b8AvEp2RG5r9mRz&s=ewUVpVtCgppOGT8w9h7W2lPKvYQuxVuFA9AQZo0bJ3M&e= ,
>>> first I cannot apply the suggested solution (I cannot see such
>>> "shortcut?s properties"), and second, I am not interested just in
>>> Rgui, or even RStudio, but other terminals running R (like
>>> VSCode-radian or
>>> SublimeText-Terminus)
>>>
>>> Thanks in advance.
>>>
>>> Best wishes,
>>>
>>> Iago
>>>
>>>
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_ma
>>> ilman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2k
>>> VeAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8
>>> AvEp2RG5r9mRz&s=34YJoiQXMGD5d-AxYVPWqbe_AwYwHKFqj-skRXnbNo4&e=
>>> PLEASE do read the posting guide
>>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.
>>> org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2
>>> kVeAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb
>>> 8AvEp2RG5r9mRz&s=FPSw6wA0aGqiMUaFUAVMewdYBcxozs6UvBniYIe4bgw&e=
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>      [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
>> lman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVe
>> AsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8AvE
>> p2RG5r9mRz&s=34YJoiQXMGD5d-AxYVPWqbe_AwYwHKFqj-skRXnbNo4&e=
>> PLEASE do read the posting guide
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
>> rg_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kV
>> eAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8Av
>> Ep2RG5r9mRz&s=FPSw6wA0aGqiMUaFUAVMewdYBcxozs6UvBniYIe4bgw&e=
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailm
>an_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRz
>sn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8AvEp2RG5r
>9mRz&s=34YJoiQXMGD5d-AxYVPWqbe_AwYwHKFqj-skRXnbNo4&e=
>PLEASE do read the posting guide
>https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org
>_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsR
>zsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8AvEp2RG5
>r9mRz&s=FPSw6wA0aGqiMUaFUAVMewdYBcxozs6UvBniYIe4bgw&e=
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8AvEp2RG5r9mRz&s=34YJoiQXMGD5d-AxYVPWqbe_AwYwHKFqj-skRXnbNo4&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=BQr4uevYaXrkdUBxnwQAsvj8tBoLWJWrxywmRAh9UFty0XNrgb8AvEp2RG5r9mRz&s=FPSw6wA0aGqiMUaFUAVMewdYBcxozs6UvBniYIe4bgw&e=
and provide commented, minimal, self-contained, reproducible code.
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

        [[alternative HTML version deleted]]


[https://www.pssjd.org/wp-content/uploads/2021/09/firmacorreo125.png]

	[[alternative HTML version deleted]]


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Tue Jun 21 15:09:10 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Tue, 21 Jun 2022 18:09:10 +0500
Subject: [R] A Request
Message-ID: <CAMfKi3+An0ryAseYY1uyPh22w+c2YrsT5M0E9x+65A3P9fDt3w@mail.gmail.com>

Hi, Dear Professor,
When I run a code in R, I face the following error:
Error in dimnames(x) <- dn :
  length of 'dimnames' [2] not equal to array extent

Kindly help how to handle this.

Regards
Chishti

	[[alternative HTML version deleted]]


From m@rc@g|rondot @end|ng |rom un|ver@|te-p@r|@-@@c|@y@|r  Tue Jun 21 15:52:03 2022
From: m@rc@g|rondot @end|ng |rom un|ver@|te-p@r|@-@@c|@y@|r (Marc Girondot)
Date: Tue, 21 Jun 2022 15:52:03 +0200
Subject: [R] A Request
In-Reply-To: <CAMfKi3+An0ryAseYY1uyPh22w+c2YrsT5M0E9x+65A3P9fDt3w@mail.gmail.com>
References: <CAMfKi3+An0ryAseYY1uyPh22w+c2YrsT5M0E9x+65A3P9fDt3w@mail.gmail.com>
Message-ID: <10d12e72-f23f-7954-0cf3-0ef91eaa001b@universite-paris-saclay.fr>

Hi, you should post a reproducible example if you want to have an answer.

This error is generated when you try to copy an object in another of the 
wrong size.

 > a <- data.frame(A=1:2)
 > dimnames(a)
[[1]]
[1] "1" "2"

[[2]]
[1] "A"

 > dn <- list(c("3", "4"), c("B", "D"))
 > dimnames(a) <- dn
Erreur dans `dimnames<-.data.frame`(`*tmp*`, value = list(c("3", "4"), 
c("B",? :
 ? 'dimnames' incorrect pour ce tableau de donn?es

Marc

Le 21/06/2022 ? 15:09, Muhammad Zubair Chishti a ?crit?:
> Hi, Dear Professor,
> When I run a code in R, I face the following error:
> Error in dimnames(x) <- dn :
>    length of 'dimnames' [2] not equal to array extent
>
> Kindly help how to handle this.
>
> Regards
> Chishti
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

__________________________________________________________
Marc Girondot, Pr

Laboratoire Ecologie, Syst?matique et Evolution
Equipe de Processus Ecologiques et Pressions Anthropiques
CNRS, AgroParisTech et Universit? Paris-Saclay, UMR 8079
B?timent 362
91405 Orsay Cedex, France

Tel:  +33 (0)1.69.15.72.30   Mobile: +33 (0)6.20.18.22.16
e-mail: marc.girondot at universite-paris-saclay.fr
         marc.girondot at gmail.com
Web: https://www.ese.universite-paris-saclay.fr/epc/conservation/index.html
Skype: girondot


From thom@@@@ub|@ @end|ng |rom |m|ndu@tr|e@@com  Tue Jun 21 18:22:48 2022
From: thom@@@@ub|@ @end|ng |rom |m|ndu@tr|e@@com (Thomas Subia)
Date: Tue, 21 Jun 2022 16:22:48 +0000
Subject: [R] Dplyr question
Message-ID: <BY5PR12MB55127EBF4CA7400D715A4A07ECB39@BY5PR12MB5512.namprd12.prod.outlook.com>

Colleagues:

The header of my data set is:
Time_stamp	P1A0B0D	P190-90D
Jun-10 10:34	-0.000208	-0.000195
Jun-10 10:51	-0.000228	-0.000188
Jun-10 11:02	-0.000234	-0.000204
Jun-10 11:17	-0.00022	-0.000205
Jun-10 11:25	-0.000238	-0.000195

I want my data set to resemble:

Time_stamp	Location	Measurement
Jun-10 10:34	P1A0B0D	-0.000208
Jun-10 10:51	P1A0B0D	-0.000228
Jun-10 11:02	P1A0B0D	-0.000234
Jun-10 11:17	P1A0B0D	-0.00022
Jun-10 11:25	P1A0B0D	-0.000238
Jun-10 10:34	P190-90D	-0.000195
Jun-10 10:51	P190-90D	-0.000188
Jun-10 11:02	P190-90D	-0.000204
Jun-10 11:17	P190-90D	-0.000205
Jun-10 11:25	P190-90D	-0.000195

I need some advice on how to do this using dplyr.

V/R
Thomas Subia

FM Industries, Inc. - NGK Electronics, USA | www.fmindustries.com 
221 Warren Ave, Fremont, CA 94539 

"En Dieu nous avons confiance, tous les autres doivent apporter des donnees"


From @v|gro@@ @end|ng |rom ver|zon@net  Tue Jun 21 18:36:08 2022
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Tue, 21 Jun 2022 16:36:08 +0000 (UTC)
Subject: [R] A Request
In-Reply-To: <CAMfKi3+An0ryAseYY1uyPh22w+c2YrsT5M0E9x+65A3P9fDt3w@mail.gmail.com>
References: <CAMfKi3+An0ryAseYY1uyPh22w+c2YrsT5M0E9x+65A3P9fDt3w@mail.gmail.com>
Message-ID: <1523366683.3740722.1655829368797@mail.yahoo.com>


The code used is not shown but the error message seems fairly obvious.
Something is trying to change the attribute containing names for the object and is not?changing the same number of items as the object contains.
What that is might be clear if you showed some code and perhaps printed out info about?your variable before calling what caused the error and showing what you wanted to change it to.
And if you did that in advance, you might see what the error is and fix it without asking here.

-----Original Message-----
From: Muhammad Zubair Chishti <mzchishti at eco.qau.edu.pk>
To: r-help at r-project.org
Sent: Tue, Jun 21, 2022 9:09 am
Subject: [R] A Request

Hi, Dear Professor,
When I run a code in R, I face the following error:
Error in dimnames(x) <- dn :
? length of 'dimnames' [2] not equal to array extent

Kindly help how to handle this.

Regards
Chishti

??? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Tue Jun 21 18:36:14 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 21 Jun 2022 16:36:14 +0000
Subject: [R] Dplyr question
In-Reply-To: <BY5PR12MB55127EBF4CA7400D715A4A07ECB39@BY5PR12MB5512.namprd12.prod.outlook.com>
References: <BY5PR12MB55127EBF4CA7400D715A4A07ECB39@BY5PR12MB5512.namprd12.prod.outlook.com>
Message-ID: <DM5PR2201MB156472D062AE205CB938DC17CFB39@DM5PR2201MB1564.namprd22.prod.outlook.com>

I do not see how to do this with exclusively dplyr commands. I might be able to make it work by including commands outside of dplyr, but at that point I would use pivot_longer() from the tidyr package.

Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Thomas Subia
Sent: Tuesday, June 21, 2022 12:23 PM
To: r-help at r-project.org
Subject: [R] Dplyr question

[External Email]

Colleagues:

The header of my data set is:
Time_stamp      P1A0B0D P190-90D
Jun-10 10:34    -0.000208       -0.000195
Jun-10 10:51    -0.000228       -0.000188
Jun-10 11:02    -0.000234       -0.000204
Jun-10 11:17    -0.00022        -0.000205
Jun-10 11:25    -0.000238       -0.000195

I want my data set to resemble:

Time_stamp      Location        Measurement
Jun-10 10:34    P1A0B0D -0.000208
Jun-10 10:51    P1A0B0D -0.000228
Jun-10 11:02    P1A0B0D -0.000234
Jun-10 11:17    P1A0B0D -0.00022
Jun-10 11:25    P1A0B0D -0.000238
Jun-10 10:34    P190-90D        -0.000195
Jun-10 10:51    P190-90D        -0.000188
Jun-10 11:02    P190-90D        -0.000204
Jun-10 11:17    P190-90D        -0.000205
Jun-10 11:25    P190-90D        -0.000195

I need some advice on how to do this using dplyr.

V/R
Thomas Subia

FM Industries, Inc. - NGK Electronics, USA | https://urldefense.proofpoint.com/v2/url?u=http-3A__www.fmindustries.com&d=DwIFAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=2wtqsRoT5-Y52Nm7oSjNuHNQHkSq2Eau1NqsSFofmKxuHYYVfjvnWxMdz4-ihkH_&s=wC7lz5W60gl0hAYdOMsXi6BiQ7LmX1r-vSkIHY_78IY&e=
221 Warren Ave, Fremont, CA 94539

"En Dieu nous avons confiance, tous les autres doivent apporter des donnees"

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=2wtqsRoT5-Y52Nm7oSjNuHNQHkSq2Eau1NqsSFofmKxuHYYVfjvnWxMdz4-ihkH_&s=Xb1jmlqPlfkqfb6U6vZjMhHn0dWhTZ_Tj-k4ZHpGXFY&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=2wtqsRoT5-Y52Nm7oSjNuHNQHkSq2Eau1NqsSFofmKxuHYYVfjvnWxMdz4-ihkH_&s=D8A_JX8FXGErCjS4rD4k9S200L_F33HHrt0jIKJdRGM&e=
and provide commented, minimal, self-contained, reproducible code.


From @v|gro@@ @end|ng |rom ver|zon@net  Tue Jun 21 18:41:39 2022
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Tue, 21 Jun 2022 16:41:39 +0000 (UTC)
Subject: [R] Dplyr question
In-Reply-To: <BY5PR12MB55127EBF4CA7400D715A4A07ECB39@BY5PR12MB5512.namprd12.prod.outlook.com>
References: <BY5PR12MB55127EBF4CA7400D715A4A07ECB39@BY5PR12MB5512.namprd12.prod.outlook.com>
Message-ID: <378201264.3735807.1655829699254@mail.yahoo.com>

Thomas,
Your need is quite common and base R has ways to do it too. This forum supposedly?focuses on more basic base R. I am sure someone will gladly tell you how to do it?that way.
For dplyr just go long:
https://tidyr.tidyverse.org/reference/pivot_longer.html
The vignette?may be an easier read:
https://tidyr.tidyverse.org/articles/pivot.html



-----Original Message-----
From: Thomas Subia <thomas.subia at fmindustries.com>
To: r-help at r-project.org <r-help at r-project.org>
Sent: Tue, Jun 21, 2022 12:22 pm
Subject: [R] Dplyr question

Colleagues:

The header of my data set is:
Time_stamp??? P1A0B0D??? P190-90D
Jun-10 10:34??? -0.000208??? -0.000195
Jun-10 10:51??? -0.000228??? -0.000188
Jun-10 11:02??? -0.000234??? -0.000204
Jun-10 11:17??? -0.00022??? -0.000205
Jun-10 11:25??? -0.000238??? -0.000195

I want my data set to resemble:

Time_stamp??? Location??? Measurement
Jun-10 10:34??? P1A0B0D??? -0.000208
Jun-10 10:51??? P1A0B0D??? -0.000228
Jun-10 11:02??? P1A0B0D??? -0.000234
Jun-10 11:17??? P1A0B0D??? -0.00022
Jun-10 11:25??? P1A0B0D??? -0.000238
Jun-10 10:34??? P190-90D??? -0.000195
Jun-10 10:51??? P190-90D??? -0.000188
Jun-10 11:02??? P190-90D??? -0.000204
Jun-10 11:17??? P190-90D??? -0.000205
Jun-10 11:25??? P190-90D??? -0.000195

I need some advice on how to do this using dplyr.

V/R
Thomas Subia

FM Industries, Inc. - NGK Electronics, USA | www.fmindustries.com 
221 Warren Ave, Fremont, CA 94539 

"En Dieu nous avons confiance, tous les autres doivent apporter des donnees"

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Tue Jun 21 18:54:08 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Tue, 21 Jun 2022 12:54:08 -0400
Subject: [R] Dplyr question
In-Reply-To: <27619_1655828599_25LGNIhm017407_BY5PR12MB55127EBF4CA7400D715A4A07ECB39@BY5PR12MB5512.namprd12.prod.outlook.com>
References: <27619_1655828599_25LGNIhm017407_BY5PR12MB55127EBF4CA7400D715A4A07ECB39@BY5PR12MB5512.namprd12.prod.outlook.com>
Message-ID: <6c001cdd-c75e-e3ef-00c1-29e8b446d921@mcmaster.ca>

Dear Thomas,

As others have noted, it's not obvious why one needs to use dplyr. 
Here's a solution in base R (with your original data in D):

 > D1 <- cbind(Location="P1A0B0D", D[, 1:2])
 > D2 <- cbind(Location="P190.90D", D[, c(1, 3)])
 > colnames(D1)[3] <- colnames(D2)[3] <- "Measurement"
 > rbind(D1[, c(2, 1, 3)], D2[, c(2, 1, 3)])
      Time_stamp Location Measurement
1  Jun-10 10:34  P1A0B0D   -0.000208
2  Jun-10 10:51  P1A0B0D   -0.000228
3  Jun-10 11:02  P1A0B0D   -0.000234
4  Jun-10 11:17  P1A0B0D   -0.000220
5  Jun-10 11:25  P1A0B0D   -0.000238
6  Jun-10 10:34 P190.90D   -0.000195
7  Jun-10 10:51 P190.90D   -0.000188
8  Jun-10 11:02 P190.90D   -0.000204
9  Jun-10 11:17 P190.90D   -0.000205
10 Jun-10 11:25 P190.90D   -0.000195

It would be simple to generalize this as a function.

I hope this helps,
  John

On 2022-06-21 12:22 p.m., Thomas Subia wrote:
> Colleagues:
> 
> The header of my data set is:
> Time_stamp	P1A0B0D	P190-90D
> Jun-10 10:34	-0.000208	-0.000195
> Jun-10 10:51	-0.000228	-0.000188
> Jun-10 11:02	-0.000234	-0.000204
> Jun-10 11:17	-0.00022	-0.000205
> Jun-10 11:25	-0.000238	-0.000195
> 
> I want my data set to resemble:
> 
> Time_stamp	Location	Measurement
> Jun-10 10:34	P1A0B0D	-0.000208
> Jun-10 10:51	P1A0B0D	-0.000228
> Jun-10 11:02	P1A0B0D	-0.000234
> Jun-10 11:17	P1A0B0D	-0.00022
> Jun-10 11:25	P1A0B0D	-0.000238
> Jun-10 10:34	P190-90D	-0.000195
> Jun-10 10:51	P190-90D	-0.000188
> Jun-10 11:02	P190-90D	-0.000204
> Jun-10 11:17	P190-90D	-0.000205
> Jun-10 11:25	P190-90D	-0.000195
> 
> I need some advice on how to do this using dplyr.
> 
> V/R
> Thomas Subia
> 
> FM Industries, Inc. - NGK Electronics, USA | www.fmindustries.com
> 221 Warren Ave, Fremont, CA 94539
> 
> "En Dieu nous avons confiance, tous les autres doivent apporter des donnees"
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Jun 21 19:02:59 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 21 Jun 2022 10:02:59 -0700
Subject: [R] Dplyr question
In-Reply-To: <BY5PR12MB55127EBF4CA7400D715A4A07ECB39@BY5PR12MB5512.namprd12.prod.outlook.com>
References: <BY5PR12MB55127EBF4CA7400D715A4A07ECB39@BY5PR12MB5512.namprd12.prod.outlook.com>
Message-ID: <D33C4ADD-A392-4EC2-9F9D-514C27DE460F@dcn.davis.ca.us>

Beware of being too specific about how you want something solved... not just here, but in all contexts. Your question is like "how do I slice this apple with this potholder"... dplyr actually doesn't do that, and you can benefit from learning how to do things in general, not just in your preferred idiom.

ans <- 
do.call( rbind
       , lapply( names(dta[ -1 ])
               , function( nm )
                   data.frame( dta[1]
                             , Location = nm
                             , Measurement = dta[[nm]]
                             , stringsAsFactors = FALSE ) ) )

On June 21, 2022 9:22:48 AM PDT, Thomas Subia <thomas.subia at fmindustries.com> wrote:
>Colleagues:
>
>The header of my data set is:
>Time_stamp	P1A0B0D	P190-90D
>Jun-10 10:34	-0.000208	-0.000195
>Jun-10 10:51	-0.000228	-0.000188
>Jun-10 11:02	-0.000234	-0.000204
>Jun-10 11:17	-0.00022	-0.000205
>Jun-10 11:25	-0.000238	-0.000195
>
>I want my data set to resemble:
>
>Time_stamp	Location	Measurement
>Jun-10 10:34	P1A0B0D	-0.000208
>Jun-10 10:51	P1A0B0D	-0.000228
>Jun-10 11:02	P1A0B0D	-0.000234
>Jun-10 11:17	P1A0B0D	-0.00022
>Jun-10 11:25	P1A0B0D	-0.000238
>Jun-10 10:34	P190-90D	-0.000195
>Jun-10 10:51	P190-90D	-0.000188
>Jun-10 11:02	P190-90D	-0.000204
>Jun-10 11:17	P190-90D	-0.000205
>Jun-10 11:25	P190-90D	-0.000195
>
>I need some advice on how to do this using dplyr.
>
>V/R
>Thomas Subia
>
>FM Industries, Inc. - NGK Electronics, USA | www.fmindustries.com 
>221 Warren Ave, Fremont, CA 94539 
>
>"En Dieu nous avons confiance, tous les autres doivent apporter des donnees"
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Tue Jun 21 19:49:38 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Tue, 21 Jun 2022 18:49:38 +0100
Subject: [R] Dplyr question
In-Reply-To: <BY5PR12MB55127EBF4CA7400D715A4A07ECB39@BY5PR12MB5512.namprd12.prod.outlook.com>
References: <BY5PR12MB55127EBF4CA7400D715A4A07ECB39@BY5PR12MB5512.namprd12.prod.outlook.com>
Message-ID: <1e047cf3-c4f7-8b8f-4e08-95acbc01d40b@sapo.pt>

Hello,

pivot_longer is a package tidyr function, not dplyr. I find its syntax 
very intuitive. Here is a solution.



x <- "Time_stamp    P1A0B0D P190-90D
'Jun-10 10:34'  -0.000208   -0.000195
'Jun-10 10:51'  -0.000228   -0.000188
'Jun-10 11:02'  -0.000234   -0.000204
'Jun-10 11:17'  -0.00022    -0.000205
'Jun-10 11:25'  -0.000238   -0.000195"
df1 <- read.table(textConnection(x), header = TRUE, check.names = FALSE)

suppressPackageStartupMessages({
   library(dplyr)
   library(tidyr)
})

df1 %>%
   pivot_longer(
     cols = -Time_stamp,     # or starts_with("P1")
     names_to = "Location",
     values_to = "Measurement"
   ) %>%
   arrange(desc(Location), Time_stamp)
#> # A tibble: 10 ? 3
#>    Time_stamp   Location Measurement
#>    <chr>        <chr>          <dbl>
#>  1 Jun-10 10:34 P1A0B0D    -0.000208
#>  2 Jun-10 10:51 P1A0B0D    -0.000228
#>  3 Jun-10 11:02 P1A0B0D    -0.000234
#>  4 Jun-10 11:17 P1A0B0D    -0.00022
#>  5 Jun-10 11:25 P1A0B0D    -0.000238
#>  6 Jun-10 10:34 P190-90D   -0.000195
#>  7 Jun-10 10:51 P190-90D   -0.000188
#>  8 Jun-10 11:02 P190-90D   -0.000204
#>  9 Jun-10 11:17 P190-90D   -0.000205
#> 10 Jun-10 11:25 P190-90D   -0.000195



Hope this helps,

Rui Barradas

?s 17:22 de 21/06/2022, Thomas Subia escreveu:
> Colleagues:
> 
> The header of my data set is:
> Time_stamp	P1A0B0D	P190-90D
> Jun-10 10:34	-0.000208	-0.000195
> Jun-10 10:51	-0.000228	-0.000188
> Jun-10 11:02	-0.000234	-0.000204
> Jun-10 11:17	-0.00022	-0.000205
> Jun-10 11:25	-0.000238	-0.000195
> 
> I want my data set to resemble:
> 
> Time_stamp	Location	Measurement
> Jun-10 10:34	P1A0B0D	-0.000208
> Jun-10 10:51	P1A0B0D	-0.000228
> Jun-10 11:02	P1A0B0D	-0.000234
> Jun-10 11:17	P1A0B0D	-0.00022
> Jun-10 11:25	P1A0B0D	-0.000238
> Jun-10 10:34	P190-90D	-0.000195
> Jun-10 10:51	P190-90D	-0.000188
> Jun-10 11:02	P190-90D	-0.000204
> Jun-10 11:17	P190-90D	-0.000205
> Jun-10 11:25	P190-90D	-0.000195
> 
> I need some advice on how to do this using dplyr.
> 
> V/R
> Thomas Subia
> 
> FM Industries, Inc. - NGK Electronics, USA | www.fmindustries.com
> 221 Warren Ave, Fremont, CA 94539
> 
> "En Dieu nous avons confiance, tous les autres doivent apporter des donnees"
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Tue Jun 21 20:25:29 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 21 Jun 2022 11:25:29 -0700
Subject: [R] Dplyr question
In-Reply-To: <1e047cf3-c4f7-8b8f-4e08-95acbc01d40b@sapo.pt>
References: <BY5PR12MB55127EBF4CA7400D715A4A07ECB39@BY5PR12MB5512.namprd12.prod.outlook.com>
 <1e047cf3-c4f7-8b8f-4e08-95acbc01d40b@sapo.pt>
Message-ID: <CAGxFJbTTwWiuUFdGgCzpGL_iMA-Pd=HjS2H_mOKO25i_cvRDDA@mail.gmail.com>

Heh heh. Well "intuitiveness" is in the mind of the intuiter. ;-)
One might even say that Jeff's and John's solutions were the most
"intuitive" as they involved nothing more than the "straightforward"
application of standard base R functionality. (Do note the scare quotes
around 'straightforward'.) Of course, other factors may well be decisive,
such as efficiency, generalizability to the *real* problem and data, and so
forth.

Best to all,
Bert

On Tue, Jun 21, 2022 at 10:50 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> pivot_longer is a package tidyr function, not dplyr. I find its syntax
> very intuitive. Here is a solution.
>
>
>
> x <- "Time_stamp    P1A0B0D P190-90D
> 'Jun-10 10:34'  -0.000208   -0.000195
> 'Jun-10 10:51'  -0.000228   -0.000188
> 'Jun-10 11:02'  -0.000234   -0.000204
> 'Jun-10 11:17'  -0.00022    -0.000205
> 'Jun-10 11:25'  -0.000238   -0.000195"
> df1 <- read.table(textConnection(x), header = TRUE, check.names = FALSE)
>
> suppressPackageStartupMessages({
>    library(dplyr)
>    library(tidyr)
> })
>
> df1 %>%
>    pivot_longer(
>      cols = -Time_stamp,     # or starts_with("P1")
>      names_to = "Location",
>      values_to = "Measurement"
>    ) %>%
>    arrange(desc(Location), Time_stamp)
> #> # A tibble: 10 ? 3
> #>    Time_stamp   Location Measurement
> #>    <chr>        <chr>          <dbl>
> #>  1 Jun-10 10:34 P1A0B0D    -0.000208
> #>  2 Jun-10 10:51 P1A0B0D    -0.000228
> #>  3 Jun-10 11:02 P1A0B0D    -0.000234
> #>  4 Jun-10 11:17 P1A0B0D    -0.00022
> #>  5 Jun-10 11:25 P1A0B0D    -0.000238
> #>  6 Jun-10 10:34 P190-90D   -0.000195
> #>  7 Jun-10 10:51 P190-90D   -0.000188
> #>  8 Jun-10 11:02 P190-90D   -0.000204
> #>  9 Jun-10 11:17 P190-90D   -0.000205
> #> 10 Jun-10 11:25 P190-90D   -0.000195
>
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 17:22 de 21/06/2022, Thomas Subia escreveu:
> > Colleagues:
> >
> > The header of my data set is:
> > Time_stamp    P1A0B0D P190-90D
> > Jun-10 10:34  -0.000208       -0.000195
> > Jun-10 10:51  -0.000228       -0.000188
> > Jun-10 11:02  -0.000234       -0.000204
> > Jun-10 11:17  -0.00022        -0.000205
> > Jun-10 11:25  -0.000238       -0.000195
> >
> > I want my data set to resemble:
> >
> > Time_stamp    Location        Measurement
> > Jun-10 10:34  P1A0B0D -0.000208
> > Jun-10 10:51  P1A0B0D -0.000228
> > Jun-10 11:02  P1A0B0D -0.000234
> > Jun-10 11:17  P1A0B0D -0.00022
> > Jun-10 11:25  P1A0B0D -0.000238
> > Jun-10 10:34  P190-90D        -0.000195
> > Jun-10 10:51  P190-90D        -0.000188
> > Jun-10 11:02  P190-90D        -0.000204
> > Jun-10 11:17  P190-90D        -0.000205
> > Jun-10 11:25  P190-90D        -0.000195
> >
> > I need some advice on how to do this using dplyr.
> >
> > V/R
> > Thomas Subia
> >
> > FM Industries, Inc. - NGK Electronics, USA | www.fmindustries.com
> > 221 Warren Ave, Fremont, CA 94539
> >
> > "En Dieu nous avons confiance, tous les autres doivent apporter des
> donnees"
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Tue Jun 21 20:46:05 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Tue, 21 Jun 2022 19:46:05 +0100
Subject: [R] Dplyr question
In-Reply-To: <CAGxFJbTTwWiuUFdGgCzpGL_iMA-Pd=HjS2H_mOKO25i_cvRDDA@mail.gmail.com>
References: <BY5PR12MB55127EBF4CA7400D715A4A07ECB39@BY5PR12MB5512.namprd12.prod.outlook.com>
 <1e047cf3-c4f7-8b8f-4e08-95acbc01d40b@sapo.pt>
 <CAGxFJbTTwWiuUFdGgCzpGL_iMA-Pd=HjS2H_mOKO25i_cvRDDA@mail.gmail.com>
Message-ID: <8e477201-0cc9-bafb-6027-009adf3a9346@sapo.pt>

Hello,

Right, intuitive is (very) relative. I was thinking of base function 
stats::reshape. Its main difficulty is, imho, to reshape to both wide 
and long formats. Compared to it, tidyr::pivot_* are (much?) easier to 
understand.

Here is a stats::reshape solution.


df_long <- reshape(
   data = df1,
   idvar = "Time_stamp",
   varying = list(2:3),
   v.names = "Measurement",
   direction = "long")

df_long$time <- sort(names(df1)[-1])[df_long$time]
names(df_long)[2] <- "Location"
df_long
#>                  Time_stamp Location Measurement
#> Jun-10 10:34.1 Jun-10 10:34 P190-90D   -0.000208
#> Jun-10 10:51.1 Jun-10 10:51 P190-90D   -0.000228
#> Jun-10 11:02.1 Jun-10 11:02 P190-90D   -0.000234
#> Jun-10 11:17.1 Jun-10 11:17 P190-90D   -0.000220
#> Jun-10 11:25.1 Jun-10 11:25 P190-90D   -0.000238
#> Jun-10 10:34.2 Jun-10 10:34  P1A0B0D   -0.000195
#> Jun-10 10:51.2 Jun-10 10:51  P1A0B0D   -0.000188
#> Jun-10 11:02.2 Jun-10 11:02  P1A0B0D   -0.000204
#> Jun-10 11:17.2 Jun-10 11:17  P1A0B0D   -0.000205
#> Jun-10 11:25.2 Jun-10 11:25  P1A0B0D   -0.000195


Hope this helps,

Rui Barradas

?s 19:25 de 21/06/2022, Bert Gunter escreveu:
> Heh heh. Well "intuitiveness" is in the mind of the intuiter. ;-)
> One might even say that Jeff's and John's solutions were the most 
> "intuitive" as they involved nothing more than the "straightforward" 
> application of standard base R functionality. (Do note the scare quotes 
> around 'straightforward'.) Of course, other factors may well be 
> decisive, such as efficiency, generalizability to the *real* problem and 
> data, and so forth.
> 
> Best to all,
> Bert
> 
> On Tue, Jun 21, 2022 at 10:50 AM Rui Barradas <ruipbarradas at sapo.pt 
> <mailto:ruipbarradas at sapo.pt>> wrote:
> 
>     Hello,
> 
>     pivot_longer is a package tidyr function, not dplyr. I find its syntax
>     very intuitive. Here is a solution.
> 
> 
> 
>     x <- "Time_stamp? ? P1A0B0D P190-90D
>     'Jun-10 10:34'? -0.000208? ?-0.000195
>     'Jun-10 10:51'? -0.000228? ?-0.000188
>     'Jun-10 11:02'? -0.000234? ?-0.000204
>     'Jun-10 11:17'? -0.00022? ? -0.000205
>     'Jun-10 11:25'? -0.000238? ?-0.000195"
>     df1 <- read.table(textConnection(x), header = TRUE, check.names = FALSE)
> 
>     suppressPackageStartupMessages({
>      ? ?library(dplyr)
>      ? ?library(tidyr)
>     })
> 
>     df1 %>%
>      ? ?pivot_longer(
>      ? ? ?cols = -Time_stamp,? ? ?# or starts_with("P1")
>      ? ? ?names_to = "Location",
>      ? ? ?values_to = "Measurement"
>      ? ?) %>%
>      ? ?arrange(desc(Location), Time_stamp)
>     #> # A tibble: 10 ? 3
>     #>? ? Time_stamp? ?Location Measurement
>     #>? ? <chr>? ? ? ? <chr>? ? ? ? ? <dbl>
>     #>? 1 Jun-10 10:34 P1A0B0D? ? -0.000208
>     #>? 2 Jun-10 10:51 P1A0B0D? ? -0.000228
>     #>? 3 Jun-10 11:02 P1A0B0D? ? -0.000234
>     #>? 4 Jun-10 11:17 P1A0B0D? ? -0.00022
>     #>? 5 Jun-10 11:25 P1A0B0D? ? -0.000238
>     #>? 6 Jun-10 10:34 P190-90D? ?-0.000195
>     #>? 7 Jun-10 10:51 P190-90D? ?-0.000188
>     #>? 8 Jun-10 11:02 P190-90D? ?-0.000204
>     #>? 9 Jun-10 11:17 P190-90D? ?-0.000205
>     #> 10 Jun-10 11:25 P190-90D? ?-0.000195
> 
> 
> 
>     Hope this helps,
> 
>     Rui Barradas
> 
>     ?s 17:22 de 21/06/2022, Thomas Subia escreveu:
>      > Colleagues:
>      >
>      > The header of my data set is:
>      > Time_stamp? ? P1A0B0D P190-90D
>      > Jun-10 10:34? -0.000208? ? ? ?-0.000195
>      > Jun-10 10:51? -0.000228? ? ? ?-0.000188
>      > Jun-10 11:02? -0.000234? ? ? ?-0.000204
>      > Jun-10 11:17? -0.00022? ? ? ? -0.000205
>      > Jun-10 11:25? -0.000238? ? ? ?-0.000195
>      >
>      > I want my data set to resemble:
>      >
>      > Time_stamp? ? Location? ? ? ? Measurement
>      > Jun-10 10:34? P1A0B0D -0.000208
>      > Jun-10 10:51? P1A0B0D -0.000228
>      > Jun-10 11:02? P1A0B0D -0.000234
>      > Jun-10 11:17? P1A0B0D -0.00022
>      > Jun-10 11:25? P1A0B0D -0.000238
>      > Jun-10 10:34? P190-90D? ? ? ? -0.000195
>      > Jun-10 10:51? P190-90D? ? ? ? -0.000188
>      > Jun-10 11:02? P190-90D? ? ? ? -0.000204
>      > Jun-10 11:17? P190-90D? ? ? ? -0.000205
>      > Jun-10 11:25? P190-90D? ? ? ? -0.000195
>      >
>      > I need some advice on how to do this using dplyr.
>      >
>      > V/R
>      > Thomas Subia
>      >
>      > FM Industries, Inc. - NGK Electronics, USA | www.fmindustries.com
>     <http://www.fmindustries.com>
>      > 221 Warren Ave, Fremont, CA 94539
>      >
>      > "En Dieu nous avons confiance, tous les autres doivent apporter
>     des donnees"
>      >
>      > ______________________________________________
>      > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>     -- To UNSUBSCRIBE and more, see
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>      > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>      > and provide commented, minimal, self-contained, reproducible code.
> 
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>     and provide commented, minimal, self-contained, reproducible code.
>


From @v|gro@@ @end|ng |rom ver|zon@net  Wed Jun 22 01:13:50 2022
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Tue, 21 Jun 2022 23:13:50 +0000 (UTC)
Subject: [R] Dplyr question
In-Reply-To: <CAGxFJbTTwWiuUFdGgCzpGL_iMA-Pd=HjS2H_mOKO25i_cvRDDA@mail.gmail.com>
References: <BY5PR12MB55127EBF4CA7400D715A4A07ECB39@BY5PR12MB5512.namprd12.prod.outlook.com>
 <1e047cf3-c4f7-8b8f-4e08-95acbc01d40b@sapo.pt>
 <CAGxFJbTTwWiuUFdGgCzpGL_iMA-Pd=HjS2H_mOKO25i_cvRDDA@mail.gmail.com>
Message-ID: <272924041.3859418.1655853230049@mail.yahoo.com>

Bert and Others,

Now that newer versions of R support a reasonable pipeline method, I think?there may be more interest in using functions designed to be easy to use?in pipelines, including wrappers that just re-arrange the order for existing?functions to make the first argument the one passed along the pipeline.

When people say "dplyr' now it is indeed a specific package but? some?use it to mean more like the "tidyverse" group of packages that are meant?to operate well together and they includes the "tidyr" package.

The intuitively OBVIOUS solution using base R that is shown is actually?a bit restricted and does not trivially scale up to deal with lots more columns?that are to be consolidated, perhaps in multiple batches and based on?things like suffixes in the names and so on that the tidyverse functions?are able to handle. And if it matters, you may want to keep the order?of the rows relatively intact and the solution offered does not.

But packages like dplyr are not a full solution and most people would?be better off learning all about what core R offers and only supplementing?it here and there with selected packages. If you ever have to read code others?wrote or modified, ...

In any case, THIS forum seems dedicated for a purpose that precludes more?than an aside about packages. Very little that is in packages cannot in theory?be done using mostly regular R but I am not sure if that is any longer true?or wise. Many packages re-write R functionality as something like much?faster code in C or C++ or make use of some R code that is more efficient?than you might cobble together on your own. Some is also very general?and allows programming at higher levels of abstraction and I specifically?include the pipeline methods (now also in R) as such a level of?abstraction.
The topic, loosely, was how to transform your data.frame (or equivalent)?from what some call WIDE form to LONG form. That is often done in?pipelines where after some steps, the resulting data has to be transformed?before being given to a program like one doing graphics with ggplot() and no?amount of lecturing suggesting we use native R graphics for everything will?in the slightest bit convince me.

So the supplied method, unless suitably placed in a function that takes a?data.frame as a first argument and returns the modified new one as a result,?will only help for some purposes and be a pain for others as you pause and?leave any pipeline to make the change and then ...

As was said, intuitive is fairly meaningless as my personal intuition often?intuits multiple ways of looking at something, each one being its own intuitive way?and the task often is simply to pick one based on additional factors. It may be?intuitively obvious to do it the shortest and easiest way imaginable but also?obvious if you will need this again, to make it properly commented and documented?and even at times do more error checking or do more general tasks ...
At MY stage I think I know enough but also see no reason to waste lots of?time doing things in many steps with lots of possible mistakes on my part?when a few well-coordinated and tested packages make it easy.
To each their own. But I am NOT suggesting this forum should change, there are?others that can accommodate people. And there are way more packages out?there that most of us are not even aware of exist!


-----Original Message-----
From: Bert Gunter <bgunter.4567 at gmail.com>
To: Rui Barradas <ruipbarradas at sapo.pt>
Cc: r-help at r-project.org <r-help at r-project.org>; Thomas Subia <thomas.subia at fmindustries.com>
Sent: Tue, Jun 21, 2022 2:25 pm
Subject: Re: [R] Dplyr question

Heh heh. Well "intuitiveness" is in the mind of the intuiter. ;-)
One might even say that Jeff's and John's solutions were the most
"intuitive" as they involved nothing more than the "straightforward"
application of standard base R functionality. (Do note the scare quotes
around 'straightforward'.) Of course, other factors may well be decisive,
such as efficiency, generalizability to the *real* problem and data, and so
forth.

Best to all,
Bert

On Tue, Jun 21, 2022 at 10:50 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> pivot_longer is a package tidyr function, not dplyr. I find its syntax
> very intuitive. Here is a solution.
>
>
>
> x <- "Time_stamp? ? P1A0B0D P190-90D
> 'Jun-10 10:34'? -0.000208? -0.000195
> 'Jun-10 10:51'? -0.000228? -0.000188
> 'Jun-10 11:02'? -0.000234? -0.000204
> 'Jun-10 11:17'? -0.00022? ? -0.000205
> 'Jun-10 11:25'? -0.000238? -0.000195"
> df1 <- read.table(textConnection(x), header = TRUE, check.names = FALSE)
>
> suppressPackageStartupMessages({
>? ? library(dplyr)
>? ? library(tidyr)
> })
>
> df1 %>%
>? ? pivot_longer(
>? ? ? cols = -Time_stamp,? ? # or starts_with("P1")
>? ? ? names_to = "Location",
>? ? ? values_to = "Measurement"
>? ? ) %>%
>? ? arrange(desc(Location), Time_stamp)
> #> # A tibble: 10 ? 3
> #>? ? Time_stamp? Location Measurement
> #>? ? <chr>? ? ? ? <chr>? ? ? ? ? <dbl>
> #>? 1 Jun-10 10:34 P1A0B0D? ? -0.000208
> #>? 2 Jun-10 10:51 P1A0B0D? ? -0.000228
> #>? 3 Jun-10 11:02 P1A0B0D? ? -0.000234
> #>? 4 Jun-10 11:17 P1A0B0D? ? -0.00022
> #>? 5 Jun-10 11:25 P1A0B0D? ? -0.000238
> #>? 6 Jun-10 10:34 P190-90D? -0.000195
> #>? 7 Jun-10 10:51 P190-90D? -0.000188
> #>? 8 Jun-10 11:02 P190-90D? -0.000204
> #>? 9 Jun-10 11:17 P190-90D? -0.000205
> #> 10 Jun-10 11:25 P190-90D? -0.000195
>
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 17:22 de 21/06/2022, Thomas Subia escreveu:
> > Colleagues:
> >
> > The header of my data set is:
> > Time_stamp? ? P1A0B0D P190-90D
> > Jun-10 10:34? -0.000208? ? ? -0.000195
> > Jun-10 10:51? -0.000228? ? ? -0.000188
> > Jun-10 11:02? -0.000234? ? ? -0.000204
> > Jun-10 11:17? -0.00022? ? ? ? -0.000205
> > Jun-10 11:25? -0.000238? ? ? -0.000195
> >
> > I want my data set to resemble:
> >
> > Time_stamp? ? Location? ? ? ? Measurement
> > Jun-10 10:34? P1A0B0D -0.000208
> > Jun-10 10:51? P1A0B0D -0.000228
> > Jun-10 11:02? P1A0B0D -0.000234
> > Jun-10 11:17? P1A0B0D -0.00022
> > Jun-10 11:25? P1A0B0D -0.000238
> > Jun-10 10:34? P190-90D? ? ? ? -0.000195
> > Jun-10 10:51? P190-90D? ? ? ? -0.000188
> > Jun-10 11:02? P190-90D? ? ? ? -0.000204
> > Jun-10 11:17? P190-90D? ? ? ? -0.000205
> > Jun-10 11:25? P190-90D? ? ? ? -0.000195
> >
> > I need some advice on how to do this using dplyr.
> >
> > V/R
> > Thomas Subia
> >
> > FM Industries, Inc. - NGK Electronics, USA | www.fmindustries.com
> > 221 Warren Ave, Fremont, CA 94539
> >
> > "En Dieu nous avons confiance, tous les autres doivent apporter des
> donnees"
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

??? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Wed Jun 22 01:36:15 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 22 Jun 2022 09:36:15 +1000
Subject: [R] A Request
In-Reply-To: <CAMfKi3+An0ryAseYY1uyPh22w+c2YrsT5M0E9x+65A3P9fDt3w@mail.gmail.com>
References: <CAMfKi3+An0ryAseYY1uyPh22w+c2YrsT5M0E9x+65A3P9fDt3w@mail.gmail.com>
Message-ID: <CA+8X3fWhR2u_2_wZ8Qjw64y_hDe99_2vqhnSgw2wrW8567unTA@mail.gmail.com>

Hi Chishti,
Try this:

dim(x)[2]
length(dn)

>From your error message, the two will be different. They should be the
same. A wild guess is that the offending line of code should be:

dimnames[2]<-1:dn

Jim

On Tue, Jun 21, 2022 at 11:10 PM Muhammad Zubair Chishti
<mzchishti at eco.qau.edu.pk> wrote:
>
> Hi, Dear Professor,
> When I run a code in R, I face the following error:
> Error in dimnames(x) <- dn :
>   length of 'dimnames' [2] not equal to array extent
>
> Kindly help how to handle this.
>
> Regards
> Chishti
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Jun 22 01:38:41 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 21 Jun 2022 16:38:41 -0700
Subject: [R] Dplyr question
In-Reply-To: <272924041.3859418.1655853230049@mail.yahoo.com>
References: <BY5PR12MB55127EBF4CA7400D715A4A07ECB39@BY5PR12MB5512.namprd12.prod.outlook.com>
 <1e047cf3-c4f7-8b8f-4e08-95acbc01d40b@sapo.pt>
 <CAGxFJbTTwWiuUFdGgCzpGL_iMA-Pd=HjS2H_mOKO25i_cvRDDA@mail.gmail.com>
 <272924041.3859418.1655853230049@mail.yahoo.com>
Message-ID: <2DBD12C5-A847-47C7-AC84-046103A7B379@dcn.davis.ca.us>

dplyr != tidyverse.

And questions that presuppose a solution method demonstrate closed-mindedness, and reduce the chance that novel solutions will be put forth, or that the questioner will actually learn something instead of using the list as a glorified search engine.

It is not that answers should avoid contributed packages, but questioners should be open to all answers. This is not the tidyverse help list, it is the R-help list, and telling people to stay quiet if they cannot conform to the biases of the questioner is self-defeating, since the point of the list is to learn how problems can be solved. An open mind can learn for instance that searching for functions that reshape dataframes in dplyr is doomed and that those functions are found elsewhere can use that info to successfully form Google queries later without coming back to the list later for the same question.

FWIW I use certain tidyverse packages frequently. But solutions using them are not always as useful for learning opportunities as base R solutions are.

Oh, and pipes are orthogonal to reshaping... searching for magrittr and reshaping isn't a productive association for searches.

On June 21, 2022 4:13:50 PM PDT, Avi Gross via R-help <r-help at r-project.org> wrote:
>Bert and Others,
>
>Now that newer versions of R support a reasonable pipeline method, I think?there may be more interest in using functions designed to be easy to use?in pipelines, including wrappers that just re-arrange the order for existing?functions to make the first argument the one passed along the pipeline.
>
>When people say "dplyr' now it is indeed a specific package but? some?use it to mean more like the "tidyverse" group of packages that are meant?to operate well together and they includes the "tidyr" package.
>
>The intuitively OBVIOUS solution using base R that is shown is actually?a bit restricted and does not trivially scale up to deal with lots more columns?that are to be consolidated, perhaps in multiple batches and based on?things like suffixes in the names and so on that the tidyverse functions?are able to handle. And if it matters, you may want to keep the order?of the rows relatively intact and the solution offered does not.
>
>But packages like dplyr are not a full solution and most people would?be better off learning all about what core R offers and only supplementing?it here and there with selected packages. If you ever have to read code others?wrote or modified, ...
>
>In any case, THIS forum seems dedicated for a purpose that precludes more?than an aside about packages. Very little that is in packages cannot in theory?be done using mostly regular R but I am not sure if that is any longer true?or wise. Many packages re-write R functionality as something like much?faster code in C or C++ or make use of some R code that is more efficient?than you might cobble together on your own. Some is also very general?and allows programming at higher levels of abstraction and I specifically?include the pipeline methods (now also in R) as such a level of?abstraction.
>The topic, loosely, was how to transform your data.frame (or equivalent)?from what some call WIDE form to LONG form. That is often done in?pipelines where after some steps, the resulting data has to be transformed?before being given to a program like one doing graphics with ggplot() and no?amount of lecturing suggesting we use native R graphics for everything will?in the slightest bit convince me.
>
>So the supplied method, unless suitably placed in a function that takes a?data.frame as a first argument and returns the modified new one as a result,?will only help for some purposes and be a pain for others as you pause and?leave any pipeline to make the change and then ...
>
>As was said, intuitive is fairly meaningless as my personal intuition often?intuits multiple ways of looking at something, each one being its own intuitive way?and the task often is simply to pick one based on additional factors. It may be?intuitively obvious to do it the shortest and easiest way imaginable but also?obvious if you will need this again, to make it properly commented and documented?and even at times do more error checking or do more general tasks ...
>At MY stage I think I know enough but also see no reason to waste lots of?time doing things in many steps with lots of possible mistakes on my part?when a few well-coordinated and tested packages make it easy.
>To each their own. But I am NOT suggesting this forum should change, there are?others that can accommodate people. And there are way more packages out?there that most of us are not even aware of exist!
>
>
>-----Original Message-----
>From: Bert Gunter <bgunter.4567 at gmail.com>
>To: Rui Barradas <ruipbarradas at sapo.pt>
>Cc: r-help at r-project.org <r-help at r-project.org>; Thomas Subia <thomas.subia at fmindustries.com>
>Sent: Tue, Jun 21, 2022 2:25 pm
>Subject: Re: [R] Dplyr question
>
>Heh heh. Well "intuitiveness" is in the mind of the intuiter. ;-)
>One might even say that Jeff's and John's solutions were the most
>"intuitive" as they involved nothing more than the "straightforward"
>application of standard base R functionality. (Do note the scare quotes
>around 'straightforward'.) Of course, other factors may well be decisive,
>such as efficiency, generalizability to the *real* problem and data, and so
>forth.
>
>Best to all,
>Bert
>
>On Tue, Jun 21, 2022 at 10:50 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
>> Hello,
>>
>> pivot_longer is a package tidyr function, not dplyr. I find its syntax
>> very intuitive. Here is a solution.
>>
>>
>>
>> x <- "Time_stamp? ? P1A0B0D P190-90D
>> 'Jun-10 10:34'? -0.000208? -0.000195
>> 'Jun-10 10:51'? -0.000228? -0.000188
>> 'Jun-10 11:02'? -0.000234? -0.000204
>> 'Jun-10 11:17'? -0.00022? ? -0.000205
>> 'Jun-10 11:25'? -0.000238? -0.000195"
>> df1 <- read.table(textConnection(x), header = TRUE, check.names = FALSE)
>>
>> suppressPackageStartupMessages({
>>? ? library(dplyr)
>>? ? library(tidyr)
>> })
>>
>> df1 %>%
>>? ? pivot_longer(
>>? ? ? cols = -Time_stamp,? ? # or starts_with("P1")
>>? ? ? names_to = "Location",
>>? ? ? values_to = "Measurement"
>>? ? ) %>%
>>? ? arrange(desc(Location), Time_stamp)
>> #> # A tibble: 10 ? 3
>> #>? ? Time_stamp? Location Measurement
>> #>? ? <chr>? ? ? ? <chr>? ? ? ? ? <dbl>
>> #>? 1 Jun-10 10:34 P1A0B0D? ? -0.000208
>> #>? 2 Jun-10 10:51 P1A0B0D? ? -0.000228
>> #>? 3 Jun-10 11:02 P1A0B0D? ? -0.000234
>> #>? 4 Jun-10 11:17 P1A0B0D? ? -0.00022
>> #>? 5 Jun-10 11:25 P1A0B0D? ? -0.000238
>> #>? 6 Jun-10 10:34 P190-90D? -0.000195
>> #>? 7 Jun-10 10:51 P190-90D? -0.000188
>> #>? 8 Jun-10 11:02 P190-90D? -0.000204
>> #>? 9 Jun-10 11:17 P190-90D? -0.000205
>> #> 10 Jun-10 11:25 P190-90D? -0.000195
>>
>>
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>> ?s 17:22 de 21/06/2022, Thomas Subia escreveu:
>> > Colleagues:
>> >
>> > The header of my data set is:
>> > Time_stamp? ? P1A0B0D P190-90D
>> > Jun-10 10:34? -0.000208? ? ? -0.000195
>> > Jun-10 10:51? -0.000228? ? ? -0.000188
>> > Jun-10 11:02? -0.000234? ? ? -0.000204
>> > Jun-10 11:17? -0.00022? ? ? ? -0.000205
>> > Jun-10 11:25? -0.000238? ? ? -0.000195
>> >
>> > I want my data set to resemble:
>> >
>> > Time_stamp? ? Location? ? ? ? Measurement
>> > Jun-10 10:34? P1A0B0D -0.000208
>> > Jun-10 10:51? P1A0B0D -0.000228
>> > Jun-10 11:02? P1A0B0D -0.000234
>> > Jun-10 11:17? P1A0B0D -0.00022
>> > Jun-10 11:25? P1A0B0D -0.000238
>> > Jun-10 10:34? P190-90D? ? ? ? -0.000195
>> > Jun-10 10:51? P190-90D? ? ? ? -0.000188
>> > Jun-10 11:02? P190-90D? ? ? ? -0.000204
>> > Jun-10 11:17? P190-90D? ? ? ? -0.000205
>> > Jun-10 11:25? P190-90D? ? ? ? -0.000195
>> >
>> > I need some advice on how to do this using dplyr.
>> >
>> > V/R
>> > Thomas Subia
>> >
>> > FM Industries, Inc. - NGK Electronics, USA | www.fmindustries.com
>> > 221 Warren Ave, Fremont, CA 94539
>> >
>> > "En Dieu nous avons confiance, tous les autres doivent apporter des
>> donnees"
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>??? [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From cry@n @end|ng |rom b|ngh@mton@edu  Wed Jun 22 05:16:11 2022
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher W Ryan)
Date: Tue, 21 Jun 2022 23:16:11 -0400
Subject: [R] proper syntax for testing linear combination of coefficients
 with glht() from multcomp package
Message-ID: <CAM+rpYkaDVCiHwcEGBGigf6qsr+e=GgdHuGdQQhdHVnZWc6Wgw@mail.gmail.com>

I have a linear model with month, guv, and month:guv as predictors, and the
logit of the monthly absenteeism rate as response. I have 9 years of
monthly data, 107 observations altogether. I believe I've accounted for
autocorrelation  and heteroskedasticity properly. But I have a question
about hypothesis tests of linear combinations of predictors.

month is a factor, 12 levels, "january" as the baseline. guv is also a
factor with two levels: 0 until the 59th observation, and 1 thereafter.

I'd like to use the glht() function in the multcomp package for a
simultaneous one-sided hypothesis test of the effect of guv not in each
month, but in each season. So the effect of guv averaged across the 3
months in a season. So for example I define "fall" as comprising september,
october, and november.  Omitting the non-salient parts of the glht()
command, is this appropriate syntax for testing the average effect of guv
in the fall:

linfct = c("guv.fac1 + (1/3)*(guv.fac1:monthseptember) +
(1/3)*(guv.fac1:monthoctober) + (1/3)*(guv.fac1:monthnovember) >= 0")

And if winter comprises december, january, and february, and since january
is the baseline month, would this be the syntax to set up a hypothesis test
for effect of guv in the winter:

linfct= c("(1/3)*(guv.fac1:monthdecember) + (1)*(guv.fac1) +
(1/3)*(guv.fac1:monthfebruary) >= 0")

A glht() built like this runs without error and yields results that make
sense, but am I testing what I think I am testing?

Thanks.

--Chris Ryan

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Wed Jun 22 21:00:38 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Wed, 22 Jun 2022 19:00:38 +0000
Subject: [R] inconsistency in tryCatch...
Message-ID: <PU4P216MB1568E4236607A9C66467518CC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear members,
                          I have the following code:

        > tryCatch(function() print("fred"),  error = function(e) sum(1:3), finally = sum(1:3))
         function() print("fred")

The expected output from the tryCatch call should be to print "fred" to the console, and exit, but as seen above, it is outputting
          function() print("fred")

Can you people please shed some light on what is happening?

thanking you,
Yours sincerely,
AKSHAY M KULKARNI

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed Jun 22 21:21:43 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 22 Jun 2022 12:21:43 -0700
Subject: [R] inconsistency in tryCatch...
In-Reply-To: <PU4P216MB1568E4236607A9C66467518CC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <PU4P216MB1568E4236607A9C66467518CC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <CAGxFJbT3e0xD4yJjqmXDACYCa2sfuR7DC5mLxtpJ2hBQsCxBAA@mail.gmail.com>

inline

On Wed, Jun 22, 2022 at 12:01 PM akshay kulkarni <akshay_e4 at hotmail.com>
wrote:

> Dear members,
>                           I have the following code:
>
>         > tryCatch(function() print("fred"),  error = function(e)
> sum(1:3), finally = sum(1:3))
>          function() print("fred")
>
> The expected output from the tryCatch call should be to print "fred" to
> the console,

NO! You have simple defined the function -- you have not called/executed
it. This defines and calls it:

> tryCatch({function() print("fred")}(),  error = function(e) sum(1:3),
finally = sum(1:3))
[1] "fred"

Cheers,
Bert





> and exit, but as seen above, it is outputting
>           function() print("fred")
>
> Can you people please shed some light on what is happening?
>
> thanking you,
> Yours sincerely,
> AKSHAY M KULKARNI
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Jun 22 21:23:46 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 22 Jun 2022 12:23:46 -0700
Subject: [R] inconsistency in tryCatch...
In-Reply-To: <PU4P216MB1568E4236607A9C66467518CC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <PU4P216MB1568E4236607A9C66467518CC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <3A0BB9FC-E0B5-4C9C-AB63-5EB715842983@dcn.davis.ca.us>

You defined a function. You did not call the function. tryCatch returned the object you defined. So the interactive console printed the object returned.

Invoking the "function" function does not call the defined function for you. Try:

tryCatch((function() print("fred"))(), error = function(e) sum(1:3), finally = sum(1:3))

On June 22, 2022 12:00:38 PM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>Dear members,
>                          I have the following code:
>
>        > tryCatch(function() print("fred"),  error = function(e) sum(1:3), finally = sum(1:3))
>         function() print("fred")
>
>The expected output from the tryCatch call should be to print "fred" to the console, and exit, but as seen above, it is outputting
>          function() print("fred")
>
>Can you people please shed some light on what is happening?
>
>thanking you,
>Yours sincerely,
>AKSHAY M KULKARNI
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @k@h@y_e4 @end|ng |rom hotm@||@com  Wed Jun 22 21:31:58 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Wed, 22 Jun 2022 19:31:58 +0000
Subject: [R] inconsistency in tryCatch...
In-Reply-To: <CAGxFJbT3e0xD4yJjqmXDACYCa2sfuR7DC5mLxtpJ2hBQsCxBAA@mail.gmail.com>
References: <PU4P216MB1568E4236607A9C66467518CC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <CAGxFJbT3e0xD4yJjqmXDACYCa2sfuR7DC5mLxtpJ2hBQsCxBAA@mail.gmail.com>
Message-ID: <PU4P216MB15687D3F7431C87EC609CD62C8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear Bert,
                 THanks a lot! I don't know how I escaped such a simple reasoning...

Your sincerely,
AKSHAY M KULKARNI
________________________________
From: Bert Gunter <bgunter.4567 at gmail.com>
Sent: Thursday, June 23, 2022 12:51 AM
To: akshay kulkarni <akshay_e4 at hotmail.com>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] inconsistency in tryCatch...

inline

On Wed, Jun 22, 2022 at 12:01 PM akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:
Dear members,
                          I have the following code:

        > tryCatch(function() print("fred"),  error = function(e) sum(1:3), finally = sum(1:3))
         function() print("fred")

The expected output from the tryCatch call should be to print "fred" to the console,
NO! You have simple defined the function -- you have not called/executed it. This defines and calls it:

> tryCatch({function() print("fred")}(),  error = function(e) sum(1:3), finally = sum(1:3))
[1] "fred"

Cheers,
Bert




and exit, but as seen above, it is outputting
          function() print("fred")

Can you people please shed some light on what is happening?

thanking you,
Yours sincerely,
AKSHAY M KULKARNI

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Wed Jun 22 21:36:27 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Wed, 22 Jun 2022 19:36:27 +0000
Subject: [R] inconsistency in tryCatch...
In-Reply-To: <3A0BB9FC-E0B5-4C9C-AB63-5EB715842983@dcn.davis.ca.us>
References: <PU4P216MB1568E4236607A9C66467518CC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <3A0BB9FC-E0B5-4C9C-AB63-5EB715842983@dcn.davis.ca.us>
Message-ID: <PU4P216MB1568D8DDDBCACFE50F0397ACC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear Jeff,
               Thanks! I think it is an idiosyncrasy of tryCatch? The other arguments like "error" doesn't need to be assigned to a call right? Just the definition would be sufficient, i think?

Yours sincerely,
AKSHAY M KULKARNI
________________________________
From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Sent: Thursday, June 23, 2022 12:53 AM
To: r-help at r-project.org <r-help at r-project.org>; akshay kulkarni <akshay_e4 at hotmail.com>; R help Mailing list <r-help at r-project.org>
Subject: Re: [R] inconsistency in tryCatch...

You defined a function. You did not call the function. tryCatch returned the object you defined. So the interactive console printed the object returned.

Invoking the "function" function does not call the defined function for you. Try:

tryCatch((function() print("fred"))(), error = function(e) sum(1:3), finally = sum(1:3))

On June 22, 2022 12:00:38 PM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>Dear members,
>                          I have the following code:
>
>        > tryCatch(function() print("fred"),  error = function(e) sum(1:3), finally = sum(1:3))
>         function() print("fred")
>
>The expected output from the tryCatch call should be to print "fred" to the console, and exit, but as seen above, it is outputting
>          function() print("fred")
>
>Can you people please shed some light on what is happening?
>
>thanking you,
>Yours sincerely,
>AKSHAY M KULKARNI
>
>       [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Wed Jun 22 21:43:55 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Wed, 22 Jun 2022 19:43:55 +0000
Subject: [R] inconsistency in tryCatch...
In-Reply-To: <3A0BB9FC-E0B5-4C9C-AB63-5EB715842983@dcn.davis.ca.us>
References: <PU4P216MB1568E4236607A9C66467518CC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <3A0BB9FC-E0B5-4C9C-AB63-5EB715842983@dcn.davis.ca.us>
Message-ID: <BN6PR2201MB155388D83DF997111A76E3EDCFB29@BN6PR2201MB1553.namprd22.prod.outlook.com>

I am used to seeing a function like this:
> function_name <- function(arg_1, arg_2, ...) {
Function body
}

Where would I find documentation on building functions of the form of tryCatch?
In the form I am used to, the function would look like this:

tryCatch <- function(){
  print("fred")
}
tryCatch()



Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Jeff Newmiller
Sent: Wednesday, June 22, 2022 3:24 PM
To: r-help at r-project.org; akshay kulkarni <akshay_e4 at hotmail.com>; R help Mailing list <r-help at r-project.org>
Subject: Re: [R] inconsistency in tryCatch...

[External Email]

You defined a function. You did not call the function. tryCatch returned the object you defined. So the interactive console printed the object returned.

Invoking the "function" function does not call the defined function for you. Try:

tryCatch((function() print("fred"))(), error = function(e) sum(1:3), finally = sum(1:3))

On June 22, 2022 12:00:38 PM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>Dear members,
>                          I have the following code:
>
>        > tryCatch(function() print("fred"),  error = function(e) sum(1:3), finally = sum(1:3))
>         function() print("fred")
>
>The expected output from the tryCatch call should be to print "fred" to the console, and exit, but as seen above, it is outputting
>          function() print("fred")
>
>Can you people please shed some light on what is happening?
>
>thanking you,
>Yours sincerely,
>AKSHAY M KULKARNI
>
>       [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailm
>an_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRz
>sn7AkP-g&m=4GOIwQQ_5zpqVjTnLYhmyFiVnTjVbTaENrDP6E3yyYsFz0UsfGP3b8k4vIW5
>G8Od&s=miLGRDrHT2Mq-cL9E3qEBX6MDiNcxeNASJxGBFYRFRA&e=
>PLEASE do read the posting guide 
>https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org
>_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsR
>zsn7AkP-g&m=4GOIwQQ_5zpqVjTnLYhmyFiVnTjVbTaENrDP6E3yyYsFz0UsfGP3b8k4vIW
>5G8Od&s=rwDSAcNw3na2O_NBsF6rl4XQL03_s3n5dXuEnpxcYRM&e=
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=4GOIwQQ_5zpqVjTnLYhmyFiVnTjVbTaENrDP6E3yyYsFz0UsfGP3b8k4vIW5G8Od&s=miLGRDrHT2Mq-cL9E3qEBX6MDiNcxeNASJxGBFYRFRA&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=4GOIwQQ_5zpqVjTnLYhmyFiVnTjVbTaENrDP6E3yyYsFz0UsfGP3b8k4vIW5G8Od&s=rwDSAcNw3na2O_NBsF6rl4XQL03_s3n5dXuEnpxcYRM&e=
and provide commented, minimal, self-contained, reproducible code.


From @k@h@y_e4 @end|ng |rom hotm@||@com  Wed Jun 22 21:49:47 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Wed, 22 Jun 2022 19:49:47 +0000
Subject: [R] inconsistency in tryCatch...
In-Reply-To: <BN6PR2201MB155388D83DF997111A76E3EDCFB29@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <PU4P216MB1568E4236607A9C66467518CC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <3A0BB9FC-E0B5-4C9C-AB63-5EB715842983@dcn.davis.ca.us>
 <BN6PR2201MB155388D83DF997111A76E3EDCFB29@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <PU4P216MB1568E4CF260E653118A2DDABC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

dear Tim,
                Thanks...
________________________________
From: Ebert,Timothy Aaron <tebert at ufl.edu>
Sent: Thursday, June 23, 2022 1:13 AM
To: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>; r-help at r-project.org <r-help at r-project.org>; akshay kulkarni <akshay_e4 at hotmail.com>; R help Mailing list <r-help at r-project.org>
Subject: RE: [R] inconsistency in tryCatch...

I am used to seeing a function like this:
> function_name <- function(arg_1, arg_2, ...) {
Function body
}

Where would I find documentation on building functions of the form of tryCatch?
In the form I am used to, the function would look like this:

tryCatch <- function(){
  print("fred")
}
tryCatch()



Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Jeff Newmiller
Sent: Wednesday, June 22, 2022 3:24 PM
To: r-help at r-project.org; akshay kulkarni <akshay_e4 at hotmail.com>; R help Mailing list <r-help at r-project.org>
Subject: Re: [R] inconsistency in tryCatch...

[External Email]

You defined a function. You did not call the function. tryCatch returned the object you defined. So the interactive console printed the object returned.

Invoking the "function" function does not call the defined function for you. Try:

tryCatch((function() print("fred"))(), error = function(e) sum(1:3), finally = sum(1:3))

On June 22, 2022 12:00:38 PM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>Dear members,
>                          I have the following code:
>
>        > tryCatch(function() print("fred"),  error = function(e) sum(1:3), finally = sum(1:3))
>         function() print("fred")
>
>The expected output from the tryCatch call should be to print "fred" to the console, and exit, but as seen above, it is outputting
>          function() print("fred")
>
>Can you people please shed some light on what is happening?
>
>thanking you,
>Yours sincerely,
>AKSHAY M KULKARNI
>
>       [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailm
>an_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRz
>sn7AkP-g&m=4GOIwQQ_5zpqVjTnLYhmyFiVnTjVbTaENrDP6E3yyYsFz0UsfGP3b8k4vIW5
>G8Od&s=miLGRDrHT2Mq-cL9E3qEBX6MDiNcxeNASJxGBFYRFRA&e=
>PLEASE do read the posting guide
>https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org
>_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsR
>zsn7AkP-g&m=4GOIwQQ_5zpqVjTnLYhmyFiVnTjVbTaENrDP6E3yyYsFz0UsfGP3b8k4vIW
>5G8Od&s=rwDSAcNw3na2O_NBsF6rl4XQL03_s3n5dXuEnpxcYRM&e=
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=4GOIwQQ_5zpqVjTnLYhmyFiVnTjVbTaENrDP6E3yyYsFz0UsfGP3b8k4vIW5G8Od&s=miLGRDrHT2Mq-cL9E3qEBX6MDiNcxeNASJxGBFYRFRA&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=4GOIwQQ_5zpqVjTnLYhmyFiVnTjVbTaENrDP6E3yyYsFz0UsfGP3b8k4vIW5G8Od&s=rwDSAcNw3na2O_NBsF6rl4XQL03_s3n5dXuEnpxcYRM&e=
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Jun 22 21:55:38 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 22 Jun 2022 12:55:38 -0700
Subject: [R] inconsistency in tryCatch...
In-Reply-To: <PU4P216MB1568D8DDDBCACFE50F0397ACC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <PU4P216MB1568E4236607A9C66467518CC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <3A0BB9FC-E0B5-4C9C-AB63-5EB715842983@dcn.davis.ca.us>
 <PU4P216MB1568D8DDDBCACFE50F0397ACC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <42F293F6-81C4-4D96-9EBA-820434EBE60C@dcn.davis.ca.us>

I don't think it is at all idiosyncratic... tryCatch doesn't expect the first argument to be a function... it is supposed to be the actual code that might break and raise an error.

There are lots of functions (like lapply) that do expect you to provide a function, and even the other parameters to tryCatch expect a function, but you would not write a for loop and have a function definition in the body of the loop and expect the for loop to know it was supposed to call that function, would you? Think of tryCatch like a for loop or an if statement.

On June 22, 2022 12:36:27 PM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>Dear Jeff,
>               Thanks! I think it is an idiosyncrasy of tryCatch? The other arguments like "error" doesn't need to be assigned to a call right? Just the definition would be sufficient, i think?
>
>Yours sincerely,
>AKSHAY M KULKARNI
>________________________________
>From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>Sent: Thursday, June 23, 2022 12:53 AM
>To: r-help at r-project.org <r-help at r-project.org>; akshay kulkarni <akshay_e4 at hotmail.com>; R help Mailing list <r-help at r-project.org>
>Subject: Re: [R] inconsistency in tryCatch...
>
>You defined a function. You did not call the function. tryCatch returned the object you defined. So the interactive console printed the object returned.
>
>Invoking the "function" function does not call the defined function for you. Try:
>
>tryCatch((function() print("fred"))(), error = function(e) sum(1:3), finally = sum(1:3))
>
>On June 22, 2022 12:00:38 PM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>>Dear members,
>>                          I have the following code:
>>
>>        > tryCatch(function() print("fred"),  error = function(e) sum(1:3), finally = sum(1:3))
>>         function() print("fred")
>>
>>The expected output from the tryCatch call should be to print "fred" to the console, and exit, but as seen above, it is outputting
>>          function() print("fred")
>>
>>Can you people please shed some light on what is happening?
>>
>>thanking you,
>>Yours sincerely,
>>AKSHAY M KULKARNI
>>
>>       [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>--
>Sent from my phone. Please excuse my brevity.

-- 
Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Jun 22 22:04:04 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 22 Jun 2022 13:04:04 -0700
Subject: [R] inconsistency in tryCatch...
In-Reply-To: <BN6PR2201MB155388D83DF997111A76E3EDCFB29@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <PU4P216MB1568E4236607A9C66467518CC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <3A0BB9FC-E0B5-4C9C-AB63-5EB715842983@dcn.davis.ca.us>
 <BN6PR2201MB155388D83DF997111A76E3EDCFB29@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <DF3EC6FF-B7CE-4995-86B8-A3490F016B09@dcn.davis.ca.us>

I would type

  tryCatch

at the R console and start reading code. It looks like it uses some internals, though.

On June 22, 2022 12:43:55 PM PDT, "Ebert,Timothy Aaron" <tebert at ufl.edu> wrote:
>I am used to seeing a function like this:
>> function_name <- function(arg_1, arg_2, ...) {
>Function body
>}
>
>Where would I find documentation on building functions of the form of tryCatch?
>In the form I am used to, the function would look like this:
>
>tryCatch <- function(){
>  print("fred")
>}
>tryCatch()
>
>
>
>Tim
>
>-----Original Message-----
>From: R-help <r-help-bounces at r-project.org> On Behalf Of Jeff Newmiller
>Sent: Wednesday, June 22, 2022 3:24 PM
>To: r-help at r-project.org; akshay kulkarni <akshay_e4 at hotmail.com>; R help Mailing list <r-help at r-project.org>
>Subject: Re: [R] inconsistency in tryCatch...
>
>[External Email]
>
>You defined a function. You did not call the function. tryCatch returned the object you defined. So the interactive console printed the object returned.
>
>Invoking the "function" function does not call the defined function for you. Try:
>
>tryCatch((function() print("fred"))(), error = function(e) sum(1:3), finally = sum(1:3))
>
>On June 22, 2022 12:00:38 PM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>>Dear members,
>>                          I have the following code:
>>
>>        > tryCatch(function() print("fred"),  error = function(e) sum(1:3), finally = sum(1:3))
>>         function() print("fred")
>>
>>The expected output from the tryCatch call should be to print "fred" to the console, and exit, but as seen above, it is outputting
>>          function() print("fred")
>>
>>Can you people please shed some light on what is happening?
>>
>>thanking you,
>>Yours sincerely,
>>AKSHAY M KULKARNI
>>
>>       [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>>https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailm
>>an_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRz
>>sn7AkP-g&m=4GOIwQQ_5zpqVjTnLYhmyFiVnTjVbTaENrDP6E3yyYsFz0UsfGP3b8k4vIW5
>>G8Od&s=miLGRDrHT2Mq-cL9E3qEBX6MDiNcxeNASJxGBFYRFRA&e=
>>PLEASE do read the posting guide 
>>https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org
>>_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsR
>>zsn7AkP-g&m=4GOIwQQ_5zpqVjTnLYhmyFiVnTjVbTaENrDP6E3yyYsFz0UsfGP3b8k4vIW
>>5G8Od&s=rwDSAcNw3na2O_NBsF6rl4XQL03_s3n5dXuEnpxcYRM&e=
>>and provide commented, minimal, self-contained, reproducible code.
>
>--
>Sent from my phone. Please excuse my brevity.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=4GOIwQQ_5zpqVjTnLYhmyFiVnTjVbTaENrDP6E3yyYsFz0UsfGP3b8k4vIW5G8Od&s=miLGRDrHT2Mq-cL9E3qEBX6MDiNcxeNASJxGBFYRFRA&e=
>PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=4GOIwQQ_5zpqVjTnLYhmyFiVnTjVbTaENrDP6E3yyYsFz0UsfGP3b8k4vIW5G8Od&s=rwDSAcNw3na2O_NBsF6rl4XQL03_s3n5dXuEnpxcYRM&e=
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @k@h@y_e4 @end|ng |rom hotm@||@com  Wed Jun 22 22:08:52 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Wed, 22 Jun 2022 20:08:52 +0000
Subject: [R] inconsistency in tryCatch...
In-Reply-To: <42F293F6-81C4-4D96-9EBA-820434EBE60C@dcn.davis.ca.us>
References: <PU4P216MB1568E4236607A9C66467518CC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <3A0BB9FC-E0B5-4C9C-AB63-5EB715842983@dcn.davis.ca.us>
 <PU4P216MB1568D8DDDBCACFE50F0397ACC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <42F293F6-81C4-4D96-9EBA-820434EBE60C@dcn.davis.ca.us>
Message-ID: <PU4P216MB1568938026618A09881D3BB2C8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

dear Jeff,
               Thanks a lot for the informative reply....

Yours sinecrely,
AKSHAY M KULKARNI
________________________________
From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Sent: Thursday, June 23, 2022 1:25 AM
To: akshay kulkarni <akshay_e4 at hotmail.com>; r-help at r-project.org <r-help at r-project.org>
Subject: Re: [R] inconsistency in tryCatch...

I don't think it is at all idiosyncratic... tryCatch doesn't expect the first argument to be a function... it is supposed to be the actual code that might break and raise an error.

There are lots of functions (like lapply) that do expect you to provide a function, and even the other parameters to tryCatch expect a function, but you would not write a for loop and have a function definition in the body of the loop and expect the for loop to know it was supposed to call that function, would you? Think of tryCatch like a for loop or an if statement.

On June 22, 2022 12:36:27 PM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>Dear Jeff,
>               Thanks! I think it is an idiosyncrasy of tryCatch? The other arguments like "error" doesn't need to be assigned to a call right? Just the definition would be sufficient, i think?
>
>Yours sincerely,
>AKSHAY M KULKARNI
>________________________________
>From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>Sent: Thursday, June 23, 2022 12:53 AM
>To: r-help at r-project.org <r-help at r-project.org>; akshay kulkarni <akshay_e4 at hotmail.com>; R help Mailing list <r-help at r-project.org>
>Subject: Re: [R] inconsistency in tryCatch...
>
>You defined a function. You did not call the function. tryCatch returned the object you defined. So the interactive console printed the object returned.
>
>Invoking the "function" function does not call the defined function for you. Try:
>
>tryCatch((function() print("fred"))(), error = function(e) sum(1:3), finally = sum(1:3))
>
>On June 22, 2022 12:00:38 PM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>>Dear members,
>>                          I have the following code:
>>
>>        > tryCatch(function() print("fred"),  error = function(e) sum(1:3), finally = sum(1:3))
>>         function() print("fred")
>>
>>The expected output from the tryCatch call should be to print "fred" to the console, and exit, but as seen above, it is outputting
>>          function() print("fred")
>>
>>Can you people please shed some light on what is happening?
>>
>>thanking you,
>>Yours sincerely,
>>AKSHAY M KULKARNI
>>
>>       [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>--
>Sent from my phone. Please excuse my brevity.

--
Sent from my phone. Please excuse my brevity.

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Jun 23 01:39:41 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 23 Jun 2022 00:39:41 +0100
Subject: [R] inconsistency in tryCatch...
In-Reply-To: <PU4P216MB1568938026618A09881D3BB2C8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <PU4P216MB1568E4236607A9C66467518CC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <3A0BB9FC-E0B5-4C9C-AB63-5EB715842983@dcn.davis.ca.us>
 <PU4P216MB1568D8DDDBCACFE50F0397ACC8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <42F293F6-81C4-4D96-9EBA-820434EBE60C@dcn.davis.ca.us>
 <PU4P216MB1568938026618A09881D3BB2C8B29@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <f5aca384-6196-ddc6-b892-b29d48d77550@sapo.pt>

Hello,

You don't need function() at all.
Unless you want a function to be called later, this also prints "fred":


tryCatch(print("fred"), error = function(e) sum(1:3), finally = sum(1:3))
#[1] "fred"


tryCatch's first argument name is 'expr'. In this case the expression is 
print("fred"), the code Jeff mentions and that you believe might break. 
Wrap tryCatch around it to prevent your code to stop running, for 
instance, when reading files in a loop or in a long simulation. The 
error is trapped and the code continues to the next instruction or loop 
iteration.

Or do you need the function, something along the following lines?


f <- tryCatch(function() print("fred"),  error = function(e) sum(1:3), 
finally = sum(1:3))

f()
#[1] "fred"


Hope this helps,

Rui Barradas

?s 21:08 de 22/06/2022, akshay kulkarni escreveu:
> dear Jeff,
>                 Thanks a lot for the informative reply....
> 
> Yours sinecrely,
> AKSHAY M KULKARNI
> ________________________________
> From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> Sent: Thursday, June 23, 2022 1:25 AM
> To: akshay kulkarni <akshay_e4 at hotmail.com>; r-help at r-project.org <r-help at r-project.org>
> Subject: Re: [R] inconsistency in tryCatch...
> 
> I don't think it is at all idiosyncratic... tryCatch doesn't expect the first argument to be a function... it is supposed to be the actual code that might break and raise an error.
> 
> There are lots of functions (like lapply) that do expect you to provide a function, and even the other parameters to tryCatch expect a function, but you would not write a for loop and have a function definition in the body of the loop and expect the for loop to know it was supposed to call that function, would you? Think of tryCatch like a for loop or an if statement.
> 
> On June 22, 2022 12:36:27 PM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>> Dear Jeff,
>>                Thanks! I think it is an idiosyncrasy of tryCatch? The other arguments like "error" doesn't need to be assigned to a call right? Just the definition would be sufficient, i think?
>>
>> Yours sincerely,
>> AKSHAY M KULKARNI
>> ________________________________
>> From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>> Sent: Thursday, June 23, 2022 12:53 AM
>> To: r-help at r-project.org <r-help at r-project.org>; akshay kulkarni <akshay_e4 at hotmail.com>; R help Mailing list <r-help at r-project.org>
>> Subject: Re: [R] inconsistency in tryCatch...
>>
>> You defined a function. You did not call the function. tryCatch returned the object you defined. So the interactive console printed the object returned.
>>
>> Invoking the "function" function does not call the defined function for you. Try:
>>
>> tryCatch((function() print("fred"))(), error = function(e) sum(1:3), finally = sum(1:3))
>>
>> On June 22, 2022 12:00:38 PM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>>> Dear members,
>>>                           I have the following code:
>>>
>>>         > tryCatch(function() print("fred"),  error = function(e) sum(1:3), finally = sum(1:3))
>>>          function() print("fred")
>>>
>>> The expected output from the tryCatch call should be to print "fred" to the console, and exit, but as seen above, it is outputting
>>>           function() print("fred")
>>>
>>> Can you people please shed some light on what is happening?
>>>
>>> thanking you,
>>> Yours sincerely,
>>> AKSHAY M KULKARNI
>>>
>>>        [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Sent from my phone. Please excuse my brevity.
> 
> --
> Sent from my phone. Please excuse my brevity.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@oknz @end|ng |rom gm@||@com  Thu Jun 23 02:28:32 2022
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Thu, 23 Jun 2022 12:28:32 +1200
Subject: [R] Dplyr question
In-Reply-To: <BY5PR12MB55127EBF4CA7400D715A4A07ECB39@BY5PR12MB5512.namprd12.prod.outlook.com>
References: <BY5PR12MB55127EBF4CA7400D715A4A07ECB39@BY5PR12MB5512.namprd12.prod.outlook.com>
Message-ID: <CABcYAdJ5XkJ0Uzbc3g=86yb-XtKaPPkP0L52Zbs9h+znMg0eaw@mail.gmail.com>

Why do you want to use dplyr?
It's easy using base R.

original <- ...
a <- cbind(original[,-3], Location=colnames(original)[2])
colnames(a)[2] <- "Measurement"
b <- cbind(original[,-2], Location=colnames(original)[3])
colnames(b)[2] <- "Measurement"
result <- rbind(a, b)[,c(1,3,2)]




On Wed, 22 Jun 2022 at 04:23, Thomas Subia <thomas.subia at fmindustries.com>
wrote:

> Colleagues:
>
> The header of my data set is:
> Time_stamp      P1A0B0D P190-90D
> Jun-10 10:34    -0.000208       -0.000195
> Jun-10 10:51    -0.000228       -0.000188
> Jun-10 11:02    -0.000234       -0.000204
> Jun-10 11:17    -0.00022        -0.000205
> Jun-10 11:25    -0.000238       -0.000195
>
> I want my data set to resemble:
>
> Time_stamp      Location        Measurement
> Jun-10 10:34    P1A0B0D -0.000208
> Jun-10 10:51    P1A0B0D -0.000228
> Jun-10 11:02    P1A0B0D -0.000234
> Jun-10 11:17    P1A0B0D -0.00022
> Jun-10 11:25    P1A0B0D -0.000238
> Jun-10 10:34    P190-90D        -0.000195
> Jun-10 10:51    P190-90D        -0.000188
> Jun-10 11:02    P190-90D        -0.000204
> Jun-10 11:17    P190-90D        -0.000205
> Jun-10 11:25    P190-90D        -0.000195
>
> I need some advice on how to do this using dplyr.
>
> V/R
> Thomas Subia
>
> FM Industries, Inc. - NGK Electronics, USA | www.fmindustries.com
> 221 Warren Ave, Fremont, CA 94539
>
> "En Dieu nous avons confiance, tous les autres doivent apporter des
> donnees"
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


