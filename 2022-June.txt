From @@h|mk@poor @end|ng |rom gm@||@com  Wed Jun  1 07:54:21 2022
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Wed, 1 Jun 2022 11:24:21 +0530
Subject: [R] R_LIBS var needed to be set after upgrade to R 4.2.2
Message-ID: <CAC8=1eppHfgnZkhE2tS_ko3Db0DNSMcs0fbYR++nYNsWqMLpvQ@mail.gmail.com>

Dear All,

I upgraded to R 4.2.2 on Debian 10 today.

The R shell incantation worked fine and all libraries would load but,
I needed to point the R_LIBS variable to
/usr/local/lib/R/site-library/ in order for the R --vanilla < myfile.R
incantation to find the libraries.

May I ask, why was this ? I never needed to do this on any previous
upgrade to R.

Many thanks,
Ashim


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Wed Jun  1 10:35:48 2022
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Wed, 1 Jun 2022 10:35:48 +0200
Subject: [R] R_LIBS var needed to be set after upgrade to R 4.2.2
In-Reply-To: <CAC8=1eppHfgnZkhE2tS_ko3Db0DNSMcs0fbYR++nYNsWqMLpvQ@mail.gmail.com>
References: <CAC8=1eppHfgnZkhE2tS_ko3Db0DNSMcs0fbYR++nYNsWqMLpvQ@mail.gmail.com>
Message-ID: <25239.9444.688970.964168@stat.math.ethz.ch>

>>>>> Ashim Kapoor 
>>>>>     on Wed, 1 Jun 2022 11:24:21 +0530 writes:

    > Dear All,

> I upgraded to R 4.2.2  on Debian 10 today.

Well, I assume you mean R 4.2.0 .. at least that one exists.

    > The R shell incantation worked fine and all libraries would load but,
    > I needed to point the R_LIBS variable to
    > /usr/local/lib/R/site-library/ in order for the R --vanilla < myfile.R
    > incantation to find the libraries.

you mean other installed *packages*

    > May I ask, why was this ? I never needed to do this on any previous
    > upgrade to R.

Well,  for me, the     R --vanilla   form also only sees the
29 (14 "base" + 15 "Recommended") packages that come with R.

Debian (& Ubuntu etc)  have used a similar setup where the
default {R-level}  .libPaths() has contained three libraries,
via

R_LIBS_SITE=${R_LIBS_SITE-'/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library'}

see

   https://cloud.r-project.org/bin/linux/debian/#pathways-to-r-packages

also for much more.
Note (also from the above CRAN page
      https://cloud.r-project.org/bin/linux/debian/
   [ Remember "menu"  "Binaries" -> "Linux" -> "Debian" ]
   
The good thing about the Debian (and derivatives) setup is that
it also separates (as I do) the  "packages that come with R" in
one library (= /usr/lib/R/library) from packages that are
installed differently.


    > Many thanks,
    > Ashim


From @@h|mk@poor @end|ng |rom gm@||@com  Wed Jun  1 11:00:58 2022
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Wed, 1 Jun 2022 14:30:58 +0530
Subject: [R] R_LIBS var needed to be set after upgrade to R 4.2.2
In-Reply-To: <25239.9444.688970.964168@stat.math.ethz.ch>
References: <CAC8=1eppHfgnZkhE2tS_ko3Db0DNSMcs0fbYR++nYNsWqMLpvQ@mail.gmail.com>
 <25239.9444.688970.964168@stat.math.ethz.ch>
Message-ID: <CAC8=1er4BwEJ0zousw0rC-g75G8HviGxqZ+1jB0heD2379x-wA@mail.gmail.com>

Dear Sir,

> > I upgraded to R 4.2.2  on Debian 10 today.
>
> Well, I assume you mean R 4.2.0 .. at least that one exists.

My bad, yes I made a typo. I did mean R 4.2.0.

>     > The R shell incantation worked fine and all libraries would load but,
>     > I needed to point the R_LIBS variable to
>     > /usr/local/lib/R/site-library/ in order for the R --vanilla < myfile.R
>     > incantation to find the libraries.
>
> you mean other installed *packages*
>
>     > May I ask, why was this ? I never needed to do this on any previous
>     > upgrade to R.
>
> Well,  for me, the     R --vanilla   form also only sees the
> 29 (14 "base" + 15 "Recommended") packages that come with R.

Has this ALWAYS been the case for you ? Even with prior versions of R ?

> Debian (& Ubuntu etc)  have used a similar setup where the
> default {R-level}  .libPaths() has contained three libraries,
> via
>
> R_LIBS_SITE=${R_LIBS_SITE-'/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library'}
>
> see
>
>    https://cloud.r-project.org/bin/linux/debian/#pathways-to-r-packages
>
> also for much more.
> Note (also from the above CRAN page
>       https://cloud.r-project.org/bin/linux/debian/
>    [ Remember "menu"  "Binaries" -> "Linux" -> "Debian" ]
>
> The good thing about the Debian (and derivatives) setup is that
> it also separates (as I do) the  "packages that come with R" in
> one library (= /usr/lib/R/library) from packages that are
> installed differently.

My confusion is : Earlier R --vanilla incantation was working fine,
even without my intervening and
adding to R_LIBS. That is why I was confused.

My main query is : Is there anything special to R 4.2.0 which needs
R_LIBS to be setup seperately?

THAT is my query.

Best Regards,
Ashim


From j@de@shod@@ m@iii@g oii googiem@ii@com  Wed Jun  1 10:12:25 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Wed, 1 Jun 2022 09:12:25 +0100
Subject: [R] mgcv: concurvity/ auto-correlation in GAM predicting deaths
 from weather time series with distributed lag
Message-ID: <CANg3_k_YSkTKLgSkWkEuxuFe_3tOaC0vBxBAUSt7KucbGZ5h=w@mail.gmail.com>

Hello all,

First posting to this list, hope you can bear with me - not a
statistician here... (my field is public health).

Have tried posting on Cross Validated (stack exchange) but it seems
that there are not many people there working with GAMs. Have been
doing quite a bit of reading/ googling, but am being halted by my lack
of knowledge of matrix algebra (not a mathematician here either) and
don't know anyone who works with GAMs, so this list really is my last
hope in my struggles.

My question is: what should I be more worried about in a time series
GAM with distributed lag? Highly significant k-index for time terms or
high concurvity between time and explanatory variables? Or are both
two sides of the same coin?  And what should/ can I do about either?
For description of the problem, please see below.

I am running various GAM models (using mgcv) to estimate daily number
of deaths from daily time series of several weather variables such as
temperature, humidity and precipitation among others. The primary aim
is to get more insight into the (complex) relationship between these
variables (rather than pure prediction performance). The models have
distributed lag terms because deaths may occur over several days
following exposure. Modelling lag is based on the  example in Simon
Wood's 2017 book, p. 349 (and the gamair package documentation
(https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)).
Code is listed at the bottom of this post. Data (as well as R script,
and model output files) are available on Github:
https://github.com/JadeShodan/heat-mortality/tree/main/social_media_posts/r_help/concurvity

Model m1 below is relatively simple with just maximum temperature,
total daily precipitation and lag, and default k for (year) and
seasonality (doy = day of year). (Heap is a categorical variable
coding for the fact that all deaths for which no specific date was
known (data is from a low income country with problematic death
registration) were assigned to the 15th day of the month in which the
deaths were thought to have occurred. This categorical variable has
169 levels (0 for all non-heaping days, 1 for the first heaping day, 2
for the second ? 168 for the final heaping day over the 14 year
period).

When I run this model:

m1 <- gam(deaths~te(year, doy, bs = c("cr", "cc")) + heap +
                                te(temp_max, lag, k=c(10,4))+
te(precip_daily_total, lag, k=c(10,4)),
                                data = dat, family = nb, method =
'REML', select = TRUE, knots = knots)

I get a highly significant k-index for (year, doy) - not sure why I am
getting NAs either:

                                         k'               edf
k-index   p-value
te(year,doy)                     19.00000 13.17462    0.93     <2e-16 ***
te(temp_max,lag)            39.00000  4.23584      NA      NA
te(precip_daily_total,lag) 36.00000  0.00929      NA      NA

Concurvity between (year, doy) and (temp_max, lag) is very high too (0.83):

                                              para
te(year,doy)      te(temp_max,lag)      te(precip_daily_total,lag)
para                                 1.000000e+00     2.437000e-31
    0.3324461                 0.6666532
te(year,doy)                      2.149940e-31     1.000000e+00
0.8285219                  0.5601749
te(temp_max,lag)            3.329829e-01      8.268335e-01
1.0000000                  0.5861324
te(precip_daily_total,lag) 6.666532e-01      5.601749e-01
0.5975987                 1.0000000

To reduce significance of p for the k-index, increasing k helps
somewhat, but only up to a point:

m2 <- gam(deaths~te(year, doy, bs = c("cr", "cc"), k=c(7, 20)) + heap
+ te(temp_max, lag, k=c(10,4))+ te(precip_daily_total, lag,
k=c(10,4)), data = dat, family = nb, method = 'REML', select = TRUE,
knots = knots)

                                                k'      edf
k-index p-value
te(year,doy)                     132.0000  24.9619    0.94    0.01 **
te(temp_max,lag)            39.0000   3.8914      NA      NA
te(precip_daily_total,lag)  36.0000   0.0128      NA      NA

However, with increasing k for (year, doy) concurvity for (temp_max,
lag) and (precipitation, lag) with  (year, doy) increases even more
(which also happens in models with predictors other than temp_max):

                                           para
te(year,doy)           te(temp_max,lag)     te(precip_daily_total,lag)
para                                   1.000000e+00    3.260062e-32
    0.3235908                  0.6666532
te(year,doy)                       4.530791e-32     1.000000e+00
 0.9206842                  0.7261335
te(temp_max,lag)              3.298028e-01     9.157306e-01
1.0000000                  0.5831089
te(precip_daily_total,lag)   6.666532e-01     7.261335e-01
0.5805672                  1.0000000

Is this happening because of the lag term? Should I be worried about
this? Is there anything I can do about this? I saw lecture slides by
Simon Wood where he advises that the functions sp.vcov and gam.vcomp
can help shine light on the problem, but I don't know how to interpret
the output from these functions.

In models with more explanatory weather variables I also have high
concurvity - over 0.80 - between e.g. lagged humidity and lagged
temperature. Both variables are of interest (in fact the purpose of
this study) and should not be dropped, but I guess that's another
story. I posted about it on Cross Validated:
https://stats.stackexchange.com/questions/576688/assessing-impact-of-concurvity-in-gam-using-time-series-of-weather-and-mortality
and any thoughts about this are welcome too.

Posting output from summary() and gam.check() would make this post too
long, so instead I've put text files on Github:

output model 1:
https://github.com/JadeShodan/heat-mortality/blob/main/social_media_posts/r_help/concurvity/model_m1_output.txt

ouput model 2:

https://github.com/JadeShodan/heat-mortality/blob/main/social_media_posts/r_help/concurvity/model_m2_output.txt


I'd be very grateful for any help anyone might be able to offer!

Jade


###############################  Model code  ################################
library(readr)
library(mgcv)

df <- read_rds("data_crossvalidated_post2.rds")
# data available on github: https://github.com/JadeShodan/heat-mortality

# Create matrices for lagged weather variables (6 day lags)
lagard <- function(x,n.lag=7) {
n <- length(x); X <- matrix(NA,n,n.lag)
for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
X
}

dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
deaths=df$deaths_total, doy=df$doy, year = df$year, month = df$month,
weekday = df$weekday, week = df$week, monthday = df$monthday, time =
df$time, heap=df$heap, heap_bin = df$heap_bin)
dat$temp_max <- lagard(df$temp_max)
dat$precip_daily_total <- lagard( df$precip_daily_total)


knots <- list(doy=c(0.5, 366.5)) # set knots for cyclic spline for doy

m1 <- gam(deaths~te(year, doy, bs = c("cr", "cc")) + heap +
                                te(temp_max, lag, k=c(10,4))+
                                te(precip_daily_total, lag, k=c(10,4)),
                               data = dat, family = nb, method =
'REML', select = TRUE, knots =knots)

# now increase k for (year, doy)
m2 <- gam(deaths~te(year, doy, bs = c("cr", "cc"), k=c(7, 20)) + heap +
                                te(temp_max, lag, k=c(10,4))+
                                te(precip_daily_total, lag, k=c(10,4)),
                                data = dat, family = nb, method =
'REML', select = TRUE, knots = knots)

gam.check(m1, rep=1000)
gam.check(m2, rep=1000)
concurvity(m1, full=FALSE)$worst
concurvity(m2, full=FALSE)$worst


From m@rek@h|@v@c @end|ng |rom gm@||@com  Wed Jun  1 13:33:25 2022
From: m@rek@h|@v@c @end|ng |rom gm@||@com (Marek Hlavac)
Date: Wed, 1 Jun 2022 13:33:25 +0200
Subject: [R] Calling stargazer() with do.call() in R 4.2.0
In-Reply-To: <CAMTWbJgxm7DmWr-agv5HLyg15wj_qu0055XQ0fLAe=q9hMzjTw@mail.gmail.com>
References: <CAMTWbJiVBXDA5ABx2zGOueDk3OOPLCvWcjqJhY21tCSm5YWHMw@mail.gmail.com>
 <af234546-216c-52d0-060e-d6dbc47dbc7c@statistik.tu-dortmund.de>
 <CAMTWbJjb+XDS2zL18vScsYJa8+B5SRQdiEaSGUVbatWFYoburQ@mail.gmail.com>
 <CAPcHnpSe8Q0C5cMPfbVtJe8rZeHtYTpG16otS4=zYqjtraVEmA@mail.gmail.com>
 <CAMTWbJgxm7DmWr-agv5HLyg15wj_qu0055XQ0fLAe=q9hMzjTw@mail.gmail.com>
Message-ID: <CAP7TaCovurYpGXQ0M2KdMhansUfLq_xFPrgh-5hh8S3E1ixbdA@mail.gmail.com>

Thanks, everyone, will work on it this weekend.

On Sun, May 29, 2022 at 6:52 AM Arne Henningsen
<arne.henningsen at gmail.com> wrote:
>
> Dear Andrew
>
> Thanks a lot for investigating the problem and for suggesting a
> solution and a workaround! I have asked the maintainer of the
> 'stargazer' package to fix the problem. Until then, we can use the
> workaround that you suggested.
>
> Best regards,
> Arne
>
> On Sat, 28 May 2022 at 23:13, Andrew Simmons <akwsimmo at gmail.com> wrote:
> >
> > Hello,
> >
> >
> > I don't have the slightest clue what stargazer is supposed to be
> > doing, but it seems as though it's trying to create names for the ...
> > list using:
> > object.names.string <- deparse(substitute(list(...)))
> >
> > It makes an error in assuming that the return value of deparse will be
> > a character string. For stargazer(res), object.names.string becomes:
> > "list(res)"
> >
> > while for do.call(stargazer, list(res)), object.names.string becomes:
> > [1] "list(structure(list(coefficients = c(`(Intercept)` =
> > 6.41594246095523, "
> > [2] "UrbanPop = 0.0209346588197249), residuals = c(Alabama =
> > 5.56984732750071, "
> > [3] "Alaska = 2.57919391569797, Arizona = 0.009284833466778, Arkansas
> > = 1.33732459805853, "
> > [4] "California = 0.679003586449805, Colorado = -0.148845848893771, "
> > [5] "Connecticut = -4.72791119007405, Delaware = -2.02323789597542, "
> > [6] "Florida = 7.30928483346678, Georgia = 9.72797800986128, Hawaii =
> > -2.8535191429924, "
> > [7] "Idaho = -4.94641403722037, Illinois = 2.2464808570076, Indiana =
> > -0.576695284237348, "
> > [8] "Iowa = -5.40921801367955, Kansas = -1.79762994305707, Kentucky =
> > 2.19545528041907, "
> > [9] "Louisiana = 7.60237005694293, Maine = -5.3836100607612, Maryland
> > = 3.4814353981232, "
> >  [ reached getOption("max.print") -- omitted 110 entries ]
> >
> > perhaps the package maintainer could change the line from:
> > object.names.string <- deparse(substitute(list(...)))
> > to:
> > object.names.string <- deparse1(substitute(list(...)), collapse = "")
> >
> > or you could change your code to:
> > do.call(stargazer, alist(res))
> >
> > please note that using alist instead of list is only a workaround, you
> > should still let the package maintainer know of this bug. If the
> > maintainer asks, this is what I used to get the strings above:
> > fun <- \(...) deparse(substitute(list(...)))
> > data("USArrests")
> > res <- lm( Murder ~ UrbanPop, data = USArrests)
> > fun(res)
> > print(do.call("fun", list(res)), max = 9)
> >
> > On Sat, May 28, 2022 at 4:41 PM Arne Henningsen
> > <arne.henningsen at gmail.com> wrote:
> > >
> > > On Sat, 28 May 2022 at 01:21, Uwe Ligges
> > > <ligges at statistik.tu-dortmund.de> wrote:
> > > > On 27.05.2022 17:29, Arne Henningsen wrote:
> > > >> Dear all  (cc Marek = maintainer of the stargazer package)
> > > >>
> > > >> We use do.call() to automatically create many LaTeX tables with
> > > >> stargazer but after upgrading to R 4.2.0, this no longer works. I
> > > >> illustrate this with a simple reproducible example:
> > > >>
> > > >> R> data("USArrests")
> > > >> R> res <- lm( Murder ~ UrbanPop, data = USArrests )
> > > >> R> library(stargazer)
> > > >> R> stargazer(res)  # works as expected
> > > >> R> do.call( stargazer, list(res) )
> > > >> Error in if (is.na(s)) { : the condition has length > 1
> > > >
> > > > Without looking at the code in detail: The line aboce suggests the code
> > > > needs an any():    if(any(is.na(x))) raher than if(is.na(x)).
> > >
> > > Yes, this is likely a problem in the 'stargazer' package.
> > >
> > > ... but why does the problem occur when using do.call( stargazer, )
> > > but the problem does *not* occur when using stargazer() directly?
> > >
> > > Best regards,
> > > Arne
> > >
> > > >> Any ideas what we can do so that the last command works with R 4.2.0?
> > > >>
> > > >> /Arne
> > >
> > > --
> > > Arne Henningsen
> > > http://www.arne-henningsen.name
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Arne Henningsen
> http://www.arne-henningsen.name


From jeroen @end|ng |rom berke|ey@edu  Wed Jun  1 14:05:51 2022
From: jeroen @end|ng |rom berke|ey@edu (Jeroen Ooms)
Date: Wed, 1 Jun 2022 14:05:51 +0200
Subject: [R] R_LIBS var needed to be set after upgrade to R 4.2.2
In-Reply-To: <CAC8=1er4BwEJ0zousw0rC-g75G8HviGxqZ+1jB0heD2379x-wA@mail.gmail.com>
References: <CAC8=1eppHfgnZkhE2tS_ko3Db0DNSMcs0fbYR++nYNsWqMLpvQ@mail.gmail.com>
 <25239.9444.688970.964168@stat.math.ethz.ch>
 <CAC8=1er4BwEJ0zousw0rC-g75G8HviGxqZ+1jB0heD2379x-wA@mail.gmail.com>
Message-ID: <CABFfbXu4eLha3rUaLFk=8N2c2xEO2B+abHSnZOePfCrCwEMuxg@mail.gmail.com>

On Wed, Jun 1, 2022 at 11:02 AM Ashim Kapoor <ashimkapoor at gmail.com> wrote:
>
> My confusion is : Earlier R --vanilla incantation was working fine,
> even without my intervening and
> adding to R_LIBS. That is why I was confused.
>
> My main query is : Is there anything special to R 4.2.0 which needs
> R_LIBS to be setup seperately?

I was hit by this problem as well a few weeks ago. You may get a more
detailed answer in r-sig-debian or from Dirk directly, but from what I
understood, this is now expected behavior: indeed if you start R
--vanilla then /usr/local/lib is no longer included in the library
path.

The reason is that R-core made a change in 4.2.0 to pre-set values for
R_LIBS_USER and R_LIBS_SITE in Renviron [1] which would take
precedence over the proper distro defaults (that include
/usr/local/lib) as configured by the r-base deb/rpm packages. To
mitigate the problem the the r-base deb package moved the appropriate
R_LIBS_USER and R_LIBS_SITE definitions into Renviron.site, which
takes precedence over Renviron. However a side effect of this solution
is that Renviron.site is ignored in --vanilla mode. At least this is
my best understanding of the problem.

[1] https://cran.r-project.org/doc/manuals/r-release/NEWS.html


From bh@@k@r@ko|k@t@ @end|ng |rom gm@||@com  Wed Jun  1 18:22:14 2022
From: bh@@k@r@ko|k@t@ @end|ng |rom gm@||@com (Bhaskar Mitra)
Date: Wed, 1 Jun 2022 09:22:14 -0700
Subject: [R] Request for some help about uncertainty analysis using
 bootstrap approach
In-Reply-To: <dae65208-0756-29b9-fbe0-e4c080c95a83@sapo.pt>
References: <CAEGXkYUo49WaArytfRffwxg87ZxNph1w-RNaZyi0Dgna=YKTbg@mail.gmail.com>
 <dae65208-0756-29b9-fbe0-e4c080c95a83@sapo.pt>
Message-ID: <CAEGXkYUnzyNMFuM527X9HGj2d0hJwb1F7M5TLj1oqNToz2NF2w@mail.gmail.com>

Thanks Rui. This is really helpful.

Regards,
Bhaskar

On Tue, May 31, 2022 at 4:03 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> You can use package boot to bootstrap the statistic for you.
> Write a function to compute the new column and assign the column means
> to the new variable Z or, like in the code below Z2 (so that you can
> compare to the Z column of simple averages).
>
> library(dplyr)
> library(boot)
>
> boot_uncert <- function(data, indices) {
>    data[indices, ] %>%
>      group_by(Group) %>%
>      mutate(Y = mean(X, na.rm = TRUE),
>             Z = coalesce(X, Y)) %>%
>      pull(Z)
> }
>
> Df1 <- Df1 %>%
>    group_by(Group) %>%
>    mutate(Y = mean(X, na.rm = TRUE),
>           Z = coalesce(X, Y)) %>%
>    ungroup()
>
> set.seed(2022)
> R <- 1e3
>
> Df1 %>%
>    mutate(Z2 = colMeans(boot(., boot_uncert, R = R)$t, na.rm = TRUE),
>           Z2 = coalesce(X, Z2))
>
> Hope this helps,
>
> Rui Barradas
>
>
> ?s 22:23 de 29/05/2022, Bhaskar Mitra escreveu:
> > Hello Everyone,
> >
> > I have a query about uncertainty analysis and would really appreciate
> some
> > help  in this regard.
> >
> > I intend to gapfill the NAs in the ?X? column of the dataframe (Df1). I
> > have grouped the data using the column ?Group? ,
> > determined the mean and generated the ?Z? column.
> >
> > While I am using the mean and standard error approach to generate the
> > uncertainty analysis, can we use the bootstrap approach to
> > generate the uncertainty for the ?Z? column? Any help in this regard will
> > be really appreciated.
> >
> > Regards,
> > Bhaskar
> > ---------------------------------------------------------------
> >
> > Df1 <-
> >
> > Group     X            Y    Z
> > 1           2              3     2
> > 1          NA            3     3
> > 1            3             3     3
> > 1           4              3    4
> > 2            2             2    1
> > 2          NA            2    3
> > 2           NA           2    3
> > 2            4             2    4
> > 3             2            2    2
> > 3         NA             2     2
> > 3              2           2     2
> >
> >
> -------------------------------------------------------------------------------
> > Codes:
> >
> > Df1 <- Df1 %>% group_by(Group) %>% summarise(Y= mean(X), na.rm=T)
> >
> > Df1  <- Df1%>% mutate(Z= coalesce(X,Y))
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @@h|mk@poor @end|ng |rom gm@||@com  Thu Jun  2 06:09:02 2022
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Thu, 2 Jun 2022 09:39:02 +0530
Subject: [R] R_LIBS var needed to be set after upgrade to R 4.2.2
In-Reply-To: <CABFfbXu4eLha3rUaLFk=8N2c2xEO2B+abHSnZOePfCrCwEMuxg@mail.gmail.com>
References: <CAC8=1eppHfgnZkhE2tS_ko3Db0DNSMcs0fbYR++nYNsWqMLpvQ@mail.gmail.com>
 <25239.9444.688970.964168@stat.math.ethz.ch>
 <CAC8=1er4BwEJ0zousw0rC-g75G8HviGxqZ+1jB0heD2379x-wA@mail.gmail.com>
 <CABFfbXu4eLha3rUaLFk=8N2c2xEO2B+abHSnZOePfCrCwEMuxg@mail.gmail.com>
Message-ID: <CAC8=1eoTy7abPxRWeLk4EbvBVb3_gX0QTjx=1j9iXjGdP2113Q@mail.gmail.com>

Dear Sir,

Many thanks.

Best Regards,
Ashim

On Wed, Jun 1, 2022 at 5:36 PM Jeroen Ooms <jeroen at berkeley.edu> wrote:
>
> On Wed, Jun 1, 2022 at 11:02 AM Ashim Kapoor <ashimkapoor at gmail.com> wrote:
> >
> > My confusion is : Earlier R --vanilla incantation was working fine,
> > even without my intervening and
> > adding to R_LIBS. That is why I was confused.
> >
> > My main query is : Is there anything special to R 4.2.0 which needs
> > R_LIBS to be setup seperately?
>
> I was hit by this problem as well a few weeks ago. You may get a more
> detailed answer in r-sig-debian or from Dirk directly, but from what I
> understood, this is now expected behavior: indeed if you start R
> --vanilla then /usr/local/lib is no longer included in the library
> path.
>
> The reason is that R-core made a change in 4.2.0 to pre-set values for
> R_LIBS_USER and R_LIBS_SITE in Renviron [1] which would take
> precedence over the proper distro defaults (that include
> /usr/local/lib) as configured by the r-base deb/rpm packages. To
> mitigate the problem the the r-base deb package moved the appropriate
> R_LIBS_USER and R_LIBS_SITE definitions into Renviron.site, which
> takes precedence over Renviron. However a side effect of this solution
> is that Renviron.site is ignored in --vanilla mode. At least this is
> my best understanding of the problem.
>
> [1] https://cran.r-project.org/doc/manuals/r-release/NEWS.html


From @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t  Thu Jun  2 08:12:44 2022
From: @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t (Stefano Sofia)
Date: Thu, 2 Jun 2022 06:12:44 +0000
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
Message-ID: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>

Dear R-list users,

for each winter season from 2000 to 2022 I have a data frame collecting for different weather stations snowpack height (Hs), snowfall in the last 24h (Hn) and a validation flag.

Suppose I have these three following data frames


df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 40, 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0, 0, 0, NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10, 0, 5), Station2_flag = c(0, 0, 0, 1, 0))


df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 60, 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))


df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station2_Hs = c(50, 60, 70, NA, NA), Station2_Hn = c(20, 20, 20, NA, NA), Station2_flag = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))


As you can see, each data frame can have different stations loaded.

I would need to call rbind matching data frames by column name (i.e. by station name), keeping in mind that the number of stations loaded in each data frame may differ. The result should be

data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs Station2_Hn Station2_flag Station3_Hs Station3_Hn Station3_flag
2000-12-01 30 10 0 20 0 0 NA NA NA
2000-12-02 40 20 0 20 0 0 NA NA NA
2000-12-03 50 10 0 30 10 0 NA NA NA
2000-12-04 NA NA NA 30 0 0 NA NA NA
2000-12-05 55 5 0 0 5 0 NA NA NA
2001-12-01 50 20 0 NA NA NA 20 0 0
2001-12-02 60 20 0 NA NA NA 20 0 0
2001-12-03 70 20 0 NA NA NA 30 10 0
2001-12-04 NA NA NA NA NA NA 30 0 1
2001-12-05 NA NA NA NA NA NA 0 5 0
2002-12-01 NA NA NA 50 20 0 20 0 0
2002-12-02 NA NA NA 60 20 0 20 0 0
2002-12-03 NA NA NA 70 20 0 30 10 0
2002-12-04 NA NA NA NA NA NA 30 0 1
2002-12-05 NA NA NA NA NA NA 0 5 0

I tried this code

df_list <- list(df1, df2, df3)
allNms <- unique(unlist(lapply(df_list, names)))
do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x, sapply(setdiff(allNms, names(x)), function(y) NA)))), make.row.names=FALSE))

but I get this error:
Error in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE,  :
  arguments imply differing number of rows

Could someone please help me?


Thank you for your attention

Stefano


         (oo)
--oOO--( )--OOo--------------------------------------
Stefano Sofia PhD
Civil Protection - Marche Region - Italy
Meteo Section
Snow Section
Via del Colle Ameno 5
60126 Torrette di Ancona, Ancona (AN)
Uff: +39 071 806 7743
E-mail: stefano.sofia at regione.marche.it
---Oo---------oO----------------------------------------

________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

--
Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non infetto.
This message was scanned by Libraesva ESG and is believed to be clean.


	[[alternative HTML version deleted]]


From bh@@k@r@ko|k@t@ @end|ng |rom gm@||@com  Thu Jun  2 01:17:18 2022
From: bh@@k@r@ko|k@t@ @end|ng |rom gm@||@com (Bhaskar Mitra)
Date: Wed, 1 Jun 2022 16:17:18 -0700
Subject: [R] Request for some help: Error when joining multiple csv files
 together
Message-ID: <CAEGXkYV-F6sRgYaq=57ic-iXF52r-4GyLWW4485f+Rzz3wq_KA@mail.gmail.com>

Hello Everyone,

I have a bunch of csv files, all with common headers (d1, d2, d3, d4).
When I am trying to join the csv files together, with the following code
(shown below),
I am getting a warning  when the files are joined. The values under columns
d3 and d4 are not joined properly.

The code and the error are given below. I would really appreciate some help
in this regard.

regards,
bhaskar


#---code--------------------------------

df <- list.files(pattern = "*.csv") %>%
  lapply(read_csv) %>%
  bind_rows

#---code--------------------------------

Header of each file:

d1  d2  d3  d4
3   4    NA   NA
4   5   NA   7
5   6   8   8
6   7   8   NA

#--------------------------------------------------------

#Warning when the codes run --------------------------------------

Column specification
?????????????????????????????????????????????????????????????????????????????????????????????????????????????????
cols(
  .default = col_double(),
  name = col_character(),
  d1 = col_datetime(format = ""),
  d2 = col_character(),
  d3 = col_logical(),
  d4 = col_logical()
)

Warning: 48766 parsing failures.
* row*    *col*          * expected  *                         *actual*
     *file*
3529   d3           1/0/T/F/TRUE/FALSE       100             'file1.csv'
3529   d4            1/0/T/F/TRUE/FALSE      100             'file1.csv'

.... ..... .................. ...... ................................

	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Thu Jun  2 09:26:02 2022
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Thu, 2 Jun 2022 09:26:02 +0200
Subject: [R] R_LIBS var needed to be set after upgrade to R 4.2.2
In-Reply-To: <CAC8=1er4BwEJ0zousw0rC-g75G8HviGxqZ+1jB0heD2379x-wA@mail.gmail.com>
References: <CAC8=1eppHfgnZkhE2tS_ko3Db0DNSMcs0fbYR++nYNsWqMLpvQ@mail.gmail.com>
 <25239.9444.688970.964168@stat.math.ethz.ch>
 <CAC8=1er4BwEJ0zousw0rC-g75G8HviGxqZ+1jB0heD2379x-wA@mail.gmail.com>
Message-ID: <25240.26122.534907.343899@stat.math.ethz.ch>

>>>>> Ashim Kapoor 
>>>>>     on Wed, 1 Jun 2022 14:30:58 +0530 writes:

    > Dear Sir,
    >> > I upgraded to R 4.2.2  on Debian 10 today.
    >> 
    >> Well, I assume you mean R 4.2.0 .. at least that one exists.

    > My bad, yes I made a typo. I did mean R 4.2.0.

    >> > The R shell incantation worked fine and all libraries would load but,
    >> > I needed to point the R_LIBS variable to
    >> > /usr/local/lib/R/site-library/ in order for the R --vanilla < myfile.R
    >> > incantation to find the libraries.
    >> 
    >> you mean other installed *packages*
    >> 
    >> > May I ask, why was this ? I never needed to do this on any previous
    >> > upgrade to R.
    >> 
    >> Well,  for me, the     R --vanilla   form also only sees the
    >> 29 (14 "base" + 15 "Recommended") packages that come with R.

    > Has this ALWAYS been the case for you ? Even with prior versions of R ?

Yes; I'm sorry this was unclear.

    >> Debian (& Ubuntu etc)  have used a similar setup where the
    >> default {R-level}  .libPaths() has contained three libraries,
    >> via
    >> 
    >> R_LIBS_SITE=${R_LIBS_SITE-'/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library'}
    >> 
    >> see
    >> 
    >> https://cloud.r-project.org/bin/linux/debian/#pathways-to-r-packages
    >> 
    >> also for much more.
    >> Note (also from the above CRAN page
    >> https://cloud.r-project.org/bin/linux/debian/
    >> [ Remember "menu"  "Binaries" -> "Linux" -> "Debian" ]
    >> 
    >> The good thing about the Debian (and derivatives) setup is that
    >> it also separates (as I do) the  "packages that come with R" in
    >> one library (= /usr/lib/R/library) from packages that are
    >> installed differently.

    > My confusion is : Earlier R --vanilla incantation was working fine,
    > even without my intervening and
    > adding to R_LIBS. That is why I was confused.

    > My main query is : Is there anything special to R 4.2.0 which needs
    > R_LIBS to be setup seperately?

to which Jeroen Ooms answered nicely -- thank you, Jeroen!

    > I was hit by this problem as well a few weeks ago. You may get a more
    > detailed answer in r-sig-debian or from Dirk directly, but from what I
    > understood, this is now expected behavior: indeed if you start R
    > --vanilla then /usr/local/lib is no longer included in the library path.

    > The reason is that R-core made a change in 4.2.0 to pre-set values for
    > R_LIBS_USER and R_LIBS_SITE in Renviron [1] which would take
    > precedence over the proper distro defaults (that include
    > /usr/local/lib) as configured by the r-base deb/rpm packages. To
    > mitigate the problem the r-base deb package moved the appropriate
    > R_LIBS_USER and R_LIBS_SITE definitions into Renviron.site, which
    > takes precedence over Renviron. However a side effect of this solution
    > is that Renviron.site is ignored in --vanilla mode. At least this is
    > my best understanding of the problem.

    > [1] https://cran.r-project.org/doc/manuals/r-release/NEWS.html

Yes, as

     R --help | grep -A1 -e --vanilla | head -2

has given
 
  --vanilla		Combine --no-save, --no-restore, --no-site-file,
			--no-init-file and --no-environ

(unchanged for many years).


Being a Linux user of more than 25 years, I've not been much of a
Debian-or-derivatives user in recent years (apart from setting up
and maintaining Ubuntu LTS on my wife's computer) mainly because
of lazyness as our IT staff helps me solve all problems with
Fedora quickly, including lowelevel device-related ones,
I think that Debian(+derivatives) has always been the exception
among the Linux distros and for all the others, '--vanilla'
really meant "vanilla" in a loose sense, i.e., "just base R".

I agree that I had wanted a "vanilla+strawberry" version myself, really.
which would be just not have the  --no-site-file  switch,
so we could call it
   --vanilla+site

{Others may hate  "feature creep" and yet more output from
'R --help'  ..}

Martin


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Jun  2 09:30:01 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 2 Jun 2022 08:30:01 +0100
Subject: [R] 
 Request for some help: Error when joining multiple csv files
 together
In-Reply-To: <CAEGXkYV-F6sRgYaq=57ic-iXF52r-4GyLWW4485f+Rzz3wq_KA@mail.gmail.com>
References: <CAEGXkYV-F6sRgYaq=57ic-iXF52r-4GyLWW4485f+Rzz3wq_KA@mail.gmail.com>
Message-ID: <f040297a-71b6-dadd-8e34-b1a523d105e9@sapo.pt>

Hello,

I'm seeing two obvious errors, those are not csv files and the columns 
spec is wrong, you have a spec of 6 columns but the posted data only has 
4 and of different classes.

If the data is in comma separated values (CSV) files the following 
worked without errors.


library(readr)

col_spec <- cols(
   d1 = col_double(),
   d2 = col_double(),
   d3 = col_double(),
   d4 = col_double()
)

list.files(pattern = "*\\.csv$") |>
   lapply(read_csv, col_types = col_spec) |>
   dplyr::bind_rows()


If the data is like in the question, try instead


list.files(pattern = "*\\.csv$") |>
   lapply(read_delim, delim = " ", col_types = col_spec) |>
   dplyr::bind_rows()


Hope this helps,

Rui Barradas

?s 00:17 de 02/06/2022, Bhaskar Mitra escreveu:
> Hello Everyone,
> 
> I have a bunch of csv files, all with common headers (d1, d2, d3, d4).
> When I am trying to join the csv files together, with the following code
> (shown below),
> I am getting a warning  when the files are joined. The values under columns
> d3 and d4 are not joined properly.
> 
> The code and the error are given below. I would really appreciate some help
> in this regard.
> 
> regards,
> bhaskar
> 
> 
> #---code--------------------------------
> 
> df <- list.files(pattern = "*.csv") %>%
>    lapply(read_csv) %>%
>    bind_rows
> 
> #---code--------------------------------
> 
> Header of each file:
> 
> d1  d2  d3  d4
> 3   4    NA   NA
> 4   5   NA   7
> 5   6   8   8
> 6   7   8   NA
> 
> #--------------------------------------------------------
> 
> #Warning when the codes run --------------------------------------
> 
> Column specification
> ?????????????????????????????????????????????????????????????????????????????????????????????????????????????????
> cols(
>    .default = col_double(),
>    name = col_character(),
>    d1 = col_datetime(format = ""),
>    d2 = col_character(),
>    d3 = col_logical(),
>    d4 = col_logical()
> )
> 
> Warning: 48766 parsing failures.
> * row*    *col*          * expected  *                         *actual*
>       *file*
> 3529   d3           1/0/T/F/TRUE/FALSE       100             'file1.csv'
> 3529   d4            1/0/T/F/TRUE/FALSE      100             'file1.csv'
> 
> .... ..... .................. ...... ................................
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Jun  2 09:37:50 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 2 Jun 2022 08:37:50 +0100
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
References: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
Message-ID: <ca06d276-9e3a-dd6b-da51-30b07b7e7423@sapo.pt>

Hello,

Here are two ways, both with dplyr::bind_rows.


res1 <- Reduce(dplyr::bind_rows, df_list)
res2 <- do.call(dplyr::bind_rows, df_list)
identical(res1, res2)
# [1] TRUE


Hope this helps,

Rui Barradas

?s 07:12 de 02/06/2022, Stefano Sofia escreveu:
> Dear R-list users,
> 
> for each winter season from 2000 to 2022 I have a data frame collecting for different weather stations snowpack height (Hs), snowfall in the last 24h (Hn) and a validation flag.
> 
> Suppose I have these three following data frames
> 
> 
> df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 40, 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0, 0, 0, NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10, 0, 5), Station2_flag = c(0, 0, 0, 1, 0))
> 
> 
> df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 60, 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> 
> 
> df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station2_Hs = c(50, 60, 70, NA, NA), Station2_Hn = c(20, 20, 20, NA, NA), Station2_flag = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> 
> 
> As you can see, each data frame can have different stations loaded.
> 
> I would need to call rbind matching data frames by column name (i.e. by station name), keeping in mind that the number of stations loaded in each data frame may differ. The result should be
> 
> data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs Station2_Hn Station2_flag Station3_Hs Station3_Hn Station3_flag
> 2000-12-01 30 10 0 20 0 0 NA NA NA
> 2000-12-02 40 20 0 20 0 0 NA NA NA
> 2000-12-03 50 10 0 30 10 0 NA NA NA
> 2000-12-04 NA NA NA 30 0 0 NA NA NA
> 2000-12-05 55 5 0 0 5 0 NA NA NA
> 2001-12-01 50 20 0 NA NA NA 20 0 0
> 2001-12-02 60 20 0 NA NA NA 20 0 0
> 2001-12-03 70 20 0 NA NA NA 30 10 0
> 2001-12-04 NA NA NA NA NA NA 30 0 1
> 2001-12-05 NA NA NA NA NA NA 0 5 0
> 2002-12-01 NA NA NA 50 20 0 20 0 0
> 2002-12-02 NA NA NA 60 20 0 20 0 0
> 2002-12-03 NA NA NA 70 20 0 30 10 0
> 2002-12-04 NA NA NA NA NA NA 30 0 1
> 2002-12-05 NA NA NA NA NA NA 0 5 0
> 
> I tried this code
> 
> df_list <- list(df1, df2, df3)
> allNms <- unique(unlist(lapply(df_list, names)))
> do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x, sapply(setdiff(allNms, names(x)), function(y) NA)))), make.row.names=FALSE))
> 
> but I get this error:
> Error in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE,  :
>    arguments imply differing number of rows
> 
> Could someone please help me?
> 
> 
> Thank you for your attention
> 
> Stefano
> 
> 
>           (oo)
> --oOO--( )--OOo--------------------------------------
> Stefano Sofia PhD
> Civil Protection - Marche Region - Italy
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona (AN)
> Uff: +39 071 806 7743
> E-mail: stefano.sofia at regione.marche.it
> ---Oo---------oO----------------------------------------
> 
> ________________________________
> 
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> 
> --
> 
> Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non infetto.
> 
> This message was scanned by Libraesva ESG and is believed to be clean.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Jun  2 09:46:02 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 2 Jun 2022 08:46:02 +0100
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <ca06d276-9e3a-dd6b-da51-30b07b7e7423@sapo.pt>
References: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
 <ca06d276-9e3a-dd6b-da51-30b07b7e7423@sapo.pt>
Message-ID: <bc824824-6873-b9ea-a007-6785ad2bde84@sapo.pt>

Hello,

And a base R only version.
Row binding code taken from StackOverflow [1].


rowbind <- function(x, y, all_cols = FALSE) {
   if(all_cols) {
     x[setdiff(names(y), names(x))] <- NA
     y[setdiff(names(x), names(y))] <- NA
   }
   rbind(x, y)
}

res3 <- Reduce(\(x, y) rowbind(x, y, all_cols = TRUE), df_list)
identical(res1, res3)
# [1] TRUE


[1] https://stackoverflow.com/a/46635610/8245406

Hope this helps,

Rui Barradas

?s 08:37 de 02/06/2022, Rui Barradas escreveu:
> Hello,
> 
> Here are two ways, both with dplyr::bind_rows.
> 
> 
> res1 <- Reduce(dplyr::bind_rows, df_list)
> res2 <- do.call(dplyr::bind_rows, df_list)
> identical(res1, res2)
> # [1] TRUE
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 07:12 de 02/06/2022, Stefano Sofia escreveu:
>> Dear R-list users,
>>
>> for each winter season from 2000 to 2022 I have a data frame 
>> collecting for different weather stations snowpack height (Hs), 
>> snowfall in the last 24h (Hn) and a validation flag.
>>
>> Suppose I have these three following data frames
>>
>>
>> df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01", 
>> format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05", 
>> format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 
>> 40, 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = 
>> c(0, 0, 0, NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = 
>> c(0, 0, 10, 0, 5), Station2_flag = c(0, 0, 0, 1, 0))
>>
>>
>> df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01", 
>> format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05", 
>> format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 
>> 60, 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = 
>> c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = 
>> c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
>>
>>
>> df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01", 
>> format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05", 
>> format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station2_Hs = c(50, 
>> 60, 70, NA, NA), Station2_Hn = c(20, 20, 20, NA, NA), Station2_flag = 
>> c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = 
>> c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
>>
>>
>> As you can see, each data frame can have different stations loaded.
>>
>> I would need to call rbind matching data frames by column name (i.e. 
>> by station name), keeping in mind that the number of stations loaded 
>> in each data frame may differ. The result should be
>>
>> data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs 
>> Station2_Hn Station2_flag Station3_Hs Station3_Hn Station3_flag
>> 2000-12-01 30 10 0 20 0 0 NA NA NA
>> 2000-12-02 40 20 0 20 0 0 NA NA NA
>> 2000-12-03 50 10 0 30 10 0 NA NA NA
>> 2000-12-04 NA NA NA 30 0 0 NA NA NA
>> 2000-12-05 55 5 0 0 5 0 NA NA NA
>> 2001-12-01 50 20 0 NA NA NA 20 0 0
>> 2001-12-02 60 20 0 NA NA NA 20 0 0
>> 2001-12-03 70 20 0 NA NA NA 30 10 0
>> 2001-12-04 NA NA NA NA NA NA 30 0 1
>> 2001-12-05 NA NA NA NA NA NA 0 5 0
>> 2002-12-01 NA NA NA 50 20 0 20 0 0
>> 2002-12-02 NA NA NA 60 20 0 20 0 0
>> 2002-12-03 NA NA NA 70 20 0 30 10 0
>> 2002-12-04 NA NA NA NA NA NA 30 0 1
>> 2002-12-05 NA NA NA NA NA NA 0 5 0
>>
>> I tried this code
>>
>> df_list <- list(df1, df2, df3)
>> allNms <- unique(unlist(lapply(df_list, names)))
>> do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x, 
>> sapply(setdiff(allNms, names(x)), function(y) NA)))), 
>> make.row.names=FALSE))
>>
>> but I get this error:
>> Error in (function (..., row.names = NULL, check.rows = FALSE, 
>> check.names = TRUE,? :
>> ?? arguments imply differing number of rows
>>
>> Could someone please help me?
>>
>>
>> Thank you for your attention
>>
>> Stefano
>>
>>
>> ????????? (oo)
>> --oOO--( )--OOo--------------------------------------
>> Stefano Sofia PhD
>> Civil Protection - Marche Region - Italy
>> Meteo Section
>> Snow Section
>> Via del Colle Ameno 5
>> 60126 Torrette di Ancona, Ancona (AN)
>> Uff: +39 071 806 7743
>> E-mail: stefano.sofia at regione.marche.it
>> ---Oo---------oO----------------------------------------
>>
>> ________________________________
>>
>> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere 
>> informazioni confidenziali, pertanto ? destinato solo a persone 
>> autorizzate alla ricezione. I messaggi di posta elettronica per i 
>> client di Regione Marche possono contenere informazioni confidenziali 
>> e con privilegi legali. Se non si ? il destinatario specificato, non 
>> leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? 
>> ricevuto questo messaggio per errore, inoltrarlo al mittente ed 
>> eliminarlo completamente dal sistema del proprio computer. Ai sensi 
>> dell'art. 6 della DGR n. 1394/2008 si segnala che, in caso di 
>> necessit? ed urgenza, la risposta al presente messaggio di posta 
>> elettronica pu? essere visionata da persone estranee al destinatario.
>> IMPORTANT NOTICE: This e-mail message is intended to be received only 
>> by persons entitled to receive the confidential information it may 
>> contain. E-mail messages to clients of Regione Marche may contain 
>> information that is confidential and legally privileged. Please do not 
>> read, copy, forward, or store this message unless you are an intended 
>> recipient of it. If you have received this message in error, please 
>> forward it to the sender and delete it completely from your computer 
>> system.
>>
>> -- 
>>
>> Questo messaggio? stato analizzato da Libraesva ESG ed? risultato non 
>> infetto.
>>
>> This message was scanned by Libraesva ESG and is believed to be clean.
>>
>>
>> ????[[alternative HTML version deleted]]
>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @@h|mk@poor @end|ng |rom gm@||@com  Thu Jun  2 10:00:27 2022
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Thu, 2 Jun 2022 13:30:27 +0530
Subject: [R] R_LIBS var needed to be set after upgrade to R 4.2.2
In-Reply-To: <25240.26122.534907.343899@stat.math.ethz.ch>
References: <CAC8=1eppHfgnZkhE2tS_ko3Db0DNSMcs0fbYR++nYNsWqMLpvQ@mail.gmail.com>
 <25239.9444.688970.964168@stat.math.ethz.ch>
 <CAC8=1er4BwEJ0zousw0rC-g75G8HviGxqZ+1jB0heD2379x-wA@mail.gmail.com>
 <25240.26122.534907.343899@stat.math.ethz.ch>
Message-ID: <CAC8=1epPYOT1j9arYvOVBOv=ehm78WJ2gc4AL296EXPqak-bqA@mail.gmail.com>

Dear Sir,

Many thanks for the reply.

Best,
Ashim

On Thu, Jun 2, 2022 at 12:56 PM Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>
> >>>>> Ashim Kapoor
> >>>>>     on Wed, 1 Jun 2022 14:30:58 +0530 writes:
>
>     > Dear Sir,
>     >> > I upgraded to R 4.2.2  on Debian 10 today.
>     >>
>     >> Well, I assume you mean R 4.2.0 .. at least that one exists.
>
>     > My bad, yes I made a typo. I did mean R 4.2.0.
>
>     >> > The R shell incantation worked fine and all libraries would load but,
>     >> > I needed to point the R_LIBS variable to
>     >> > /usr/local/lib/R/site-library/ in order for the R --vanilla < myfile.R
>     >> > incantation to find the libraries.
>     >>
>     >> you mean other installed *packages*
>     >>
>     >> > May I ask, why was this ? I never needed to do this on any previous
>     >> > upgrade to R.
>     >>
>     >> Well,  for me, the     R --vanilla   form also only sees the
>     >> 29 (14 "base" + 15 "Recommended") packages that come with R.
>
>     > Has this ALWAYS been the case for you ? Even with prior versions of R ?
>
> Yes; I'm sorry this was unclear.
>
>     >> Debian (& Ubuntu etc)  have used a similar setup where the
>     >> default {R-level}  .libPaths() has contained three libraries,
>     >> via
>     >>
>     >> R_LIBS_SITE=${R_LIBS_SITE-'/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library'}
>     >>
>     >> see
>     >>
>     >> https://cloud.r-project.org/bin/linux/debian/#pathways-to-r-packages
>     >>
>     >> also for much more.
>     >> Note (also from the above CRAN page
>     >> https://cloud.r-project.org/bin/linux/debian/
>     >> [ Remember "menu"  "Binaries" -> "Linux" -> "Debian" ]
>     >>
>     >> The good thing about the Debian (and derivatives) setup is that
>     >> it also separates (as I do) the  "packages that come with R" in
>     >> one library (= /usr/lib/R/library) from packages that are
>     >> installed differently.
>
>     > My confusion is : Earlier R --vanilla incantation was working fine,
>     > even without my intervening and
>     > adding to R_LIBS. That is why I was confused.
>
>     > My main query is : Is there anything special to R 4.2.0 which needs
>     > R_LIBS to be setup seperately?
>
> to which Jeroen Ooms answered nicely -- thank you, Jeroen!
>
>     > I was hit by this problem as well a few weeks ago. You may get a more
>     > detailed answer in r-sig-debian or from Dirk directly, but from what I
>     > understood, this is now expected behavior: indeed if you start R
>     > --vanilla then /usr/local/lib is no longer included in the library path.
>
>     > The reason is that R-core made a change in 4.2.0 to pre-set values for
>     > R_LIBS_USER and R_LIBS_SITE in Renviron [1] which would take
>     > precedence over the proper distro defaults (that include
>     > /usr/local/lib) as configured by the r-base deb/rpm packages. To
>     > mitigate the problem the r-base deb package moved the appropriate
>     > R_LIBS_USER and R_LIBS_SITE definitions into Renviron.site, which
>     > takes precedence over Renviron. However a side effect of this solution
>     > is that Renviron.site is ignored in --vanilla mode. At least this is
>     > my best understanding of the problem.
>
>     > [1] https://cran.r-project.org/doc/manuals/r-release/NEWS.html
>
> Yes, as
>
>      R --help | grep -A1 -e --vanilla | head -2
>
> has given
>
>   --vanilla             Combine --no-save, --no-restore, --no-site-file,
>                         --no-init-file and --no-environ
>
> (unchanged for many years).
>
>
> Being a Linux user of more than 25 years, I've not been much of a
> Debian-or-derivatives user in recent years (apart from setting up
> and maintaining Ubuntu LTS on my wife's computer) mainly because
> of lazyness as our IT staff helps me solve all problems with
> Fedora quickly, including lowelevel device-related ones,
> I think that Debian(+derivatives) has always been the exception
> among the Linux distros and for all the others, '--vanilla'
> really meant "vanilla" in a loose sense, i.e., "just base R".
>
> I agree that I had wanted a "vanilla+strawberry" version myself, really.
> which would be just not have the  --no-site-file  switch,
> so we could call it
>    --vanilla+site
>
> {Others may hate  "feature creep" and yet more output from
> 'R --help'  ..}
>
> Martin


From Qu|r|n_St|er @end|ng |rom gmx@de  Thu Jun  2 16:49:27 2022
From: Qu|r|n_St|er @end|ng |rom gmx@de (Quirin Stier)
Date: Thu, 2 Jun 2022 16:49:27 +0200
Subject: [R] Install OpenCL
Message-ID: <1e3ff528-1d77-6a66-3184-0f94ce826f62@gmx.de>

Hi everyone,

the installation of OpenCL on windows with CUDA 11.7 failed. I can set
up tensorflow in python and communicate with the GPU, so I assume CUDA
is set up correctly. OpenCL headers are also available through the CUDA
toolkit.

However, the package installation of OpenCL in R fails. There is not
much documentation available. Can anyone help with that please?

 > install.packages("OpenCL")
Installiere Paket nach ?C:/Users/quiri/AppData/Local/R/win-library/4.2?
(da ?lib? nicht spezifiziert)
Paket, das nur als Quelltext vorliegt und eventuell ?bersetzung von
C/C++/Fortran ben?tigt.: ?OpenCL?
installiere Quellpaket ?OpenCL?

trying URL 'https://cran.rstudio.com/src/contrib/OpenCL_0.2-2.tar.gz'
Content type 'application/x-gzip' length 20881 bytes (20 KB)
downloaded 20 KB

* installing *source* package 'OpenCL' ...
** Paket 'OpenCL' erfolgreich entpackt und MD5 Summen ?berpr?ft
** using staged installation

ERROR: OCL not set!

You will need a working OpenCL SDK with headers
and libraries for both i386 and x64

Set OCL to the root of the SDK

You can also set individial variables OCLINC,
OCL32LIB and OCL64LIB. If they are not set,
the default layout will be assumed.

ERROR: configuration failed for package 'OpenCL'
* removing 'C:/Users/quiri/AppData/Local/R/win-library/4.2/OpenCL'
Warning in install.packages :
 ? Installation des Pakets ?OpenCL? hatte Exit-Status ungleich 0

Die heruntergeladenen Quellpakete sind in
 ??C:\Users\quiri\AppData\Local\Temp\RtmpeepQng\downloaded_packages?


Best regards,

Quirin


From jr@| @end|ng |rom po@teo@no  Thu Jun  2 17:43:32 2022
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Thu,  2 Jun 2022 15:43:32 +0000
Subject: [R] Install OpenCL
In-Reply-To: <1e3ff528-1d77-6a66-3184-0f94ce826f62@gmx.de>
References: <1e3ff528-1d77-6a66-3184-0f94ce826f62@gmx.de>
Message-ID: <YpjapBDJz46YX7Fy@posteo.no>

Dear Quirin,

To be able to install OpenCL on 
ArchLinux, I needed to install 
opencl-headers first, because my system 
complained about CL/opencl.h ...  Maybe 
there is something like that on Windows 
... 

Best,
Rasmus

[1] https://archlinux.org/packages/extra/any/opencl-headers/


From kry|ov@r00t @end|ng |rom gm@||@com  Thu Jun  2 17:46:55 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Thu, 2 Jun 2022 18:46:55 +0300
Subject: [R] Install OpenCL
In-Reply-To: <1e3ff528-1d77-6a66-3184-0f94ce826f62@gmx.de>
References: <1e3ff528-1d77-6a66-3184-0f94ce826f62@gmx.de>
Message-ID: <20220602184655.1c840903@arachnoid>

? Thu, 2 Jun 2022 16:49:27 +0200
Quirin Stier <Quirin_Stier at gmx.de> ?????:

> ERROR: OCL not set!
> 
> You will need a working OpenCL SDK with headers
> and libraries for both i386 and x64
> 
> Set OCL to the root of the SDK

Admittedly, it's not mentioned in the package's INSTALL file, but as
the configure script says, you need to set the "OCL" environment
variable to the path where you have OpenCL SDK installed. Use
Sys.setenv() to set the variable before running install.packages().

Also, make sure that the OpenCL DLLs are available via the %PATH%
variable.

-- 
Best regards,
Ivan


From bgunter@4567 @end|ng |rom gm@||@com  Thu Jun  2 22:04:15 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 2 Jun 2022 13:04:15 -0700
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
References: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
Message-ID: <CAGxFJbS9+zZMRSaRU7KhBEJUWb8kiKjy51xe5TtjyDboKDafxQ@mail.gmail.com>

Well, it seems better to me to put all the data frames in long format and
then rbind them instead of the other way round, which results in the piles
of NA's you see. I note also, FWIW, that this accords with the so-called
"tidy" format that many advocate these days. You can always subset (rows)
and choose by station, date, etc. as needed, of course from the long format.

Because of the regularity of your data frames, it is easy to do this. Here
is a little base R function that "reforms" each data frame (I suspect Rui
may well provide a more elegant version, though):


reform<- function(dat){
   nm <- names(dat)
   stanums <- unique(gsub("[^[:digit:]]","", nm[-1])) ## station numbers
present
    z <- do.call(rbind, lapply(stanums,
      \(i)
      structure(dat[,grep(i, nm, fixed = TRUE)],
                names = c("Hs", "Hn", "flag"))))
   data.frame(POSIX =rep(dat[,1], length(stanums)),
              Station = rep(stanums, e = nrow(dat)),
              z)
}

e.g.
> reform(df2)
        POSIX Station Hs Hn flag
1  2001-12-01       1 50 20    0
2  2001-12-02       1 60 20    0
3  2001-12-03       1 70 20    0
4  2001-12-04       1 NA NA   NA
5  2001-12-05       1 NA NA   NA
6  2001-12-01       3 20  0    0
7  2001-12-02       3 20  0    0
8  2001-12-03       3 30 10    0
9  2001-12-04       3 30  0    1
10 2001-12-05       3  0  5    0

A call to rbind() of the following form then gives you all your data in
long form(you may wish to use some shortcuts to form the list of frames):

> do.call(rbind, lapply(list(df1, df2, df3), reform))
        POSIX Station Hs Hn flag
1  2000-12-01       1 30 10    0
2  2000-12-02       1 40 20    0
3  2000-12-03       1 50 10    0
4  2000-12-04       1 NA NA   NA
5  2000-12-05       1 55  5    0
6  2000-12-01       2 20  0    0
7  2000-12-02       2 20  0    0
8  2000-12-03       2 30 10    0
9  2000-12-04       2 30  0    1
10 2000-12-05       2  0  5    0
11 2001-12-01       1 50 20    0
12 2001-12-02       1 60 20    0
13 2001-12-03       1 70 20    0
14 2001-12-04       1 NA NA   NA
15 2001-12-05       1 NA NA   NA
16 2001-12-01       3 20  0    0
17 2001-12-02       3 20  0    0
18 2001-12-03       3 30 10    0
19 2001-12-04       3 30  0    1
20 2001-12-05       3  0  5    0
21 2002-12-01       2 50 20    0
22 2002-12-02       2 60 20    0
23 2002-12-03       2 70 20    0
24 2002-12-04       2 NA NA   NA
25 2002-12-05       2 NA NA   NA
26 2002-12-01       3 20  0    0
27 2002-12-02       3 20  0    0
28 2002-12-03       3 30 10    0
29 2002-12-04       3 30  0    1
30 2002-12-05       3  0  5    0


Cheers,
Bert Gunter




On Wed, Jun 1, 2022 at 11:13 PM Stefano Sofia <
stefano.sofia at regione.marche.it> wrote:

> Dear R-list users,
>
> for each winter season from 2000 to 2022 I have a data frame collecting
> for different weather stations snowpack height (Hs), snowfall in the last
> 24h (Hn) and a validation flag.
>
> Suppose I have these three following data frames
>
>
> df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01",
> format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05",
> format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 40,
> 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0, 0, 0,
> NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10, 0,
> 5), Station2_flag = c(0, 0, 0, 1, 0))
>
>
> df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01",
> format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05",
> format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 60,
> 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0, 0,
> 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10,
> 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
>
>
> df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01",
> format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05",
> format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station2_Hs = c(50, 60,
> 70, NA, NA), Station2_Hn = c(20, 20, 20, NA, NA), Station2_flag = c(0, 0,
> 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10,
> 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
>
>
> As you can see, each data frame can have different stations loaded.
>
> I would need to call rbind matching data frames by column name (i.e. by
> station name), keeping in mind that the number of stations loaded in each
> data frame may differ. The result should be
>
> data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs Station2_Hn
> Station2_flag Station3_Hs Station3_Hn Station3_flag
> 2000-12-01 30 10 0 20 0 0 NA NA NA
> 2000-12-02 40 20 0 20 0 0 NA NA NA
> 2000-12-03 50 10 0 30 10 0 NA NA NA
> 2000-12-04 NA NA NA 30 0 0 NA NA NA
> 2000-12-05 55 5 0 0 5 0 NA NA NA
> 2001-12-01 50 20 0 NA NA NA 20 0 0
> 2001-12-02 60 20 0 NA NA NA 20 0 0
> 2001-12-03 70 20 0 NA NA NA 30 10 0
> 2001-12-04 NA NA NA NA NA NA 30 0 1
> 2001-12-05 NA NA NA NA NA NA 0 5 0
> 2002-12-01 NA NA NA 50 20 0 20 0 0
> 2002-12-02 NA NA NA 60 20 0 20 0 0
> 2002-12-03 NA NA NA 70 20 0 30 10 0
> 2002-12-04 NA NA NA NA NA NA 30 0 1
> 2002-12-05 NA NA NA NA NA NA 0 5 0
>
> I tried this code
>
> df_list <- list(df1, df2, df3)
> allNms <- unique(unlist(lapply(df_list, names)))
> do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x,
> sapply(setdiff(allNms, names(x)), function(y) NA)))), make.row.names=FALSE))
>
> but I get this error:
> Error in (function (..., row.names = NULL, check.rows = FALSE, check.names
> = TRUE,  :
>   arguments imply differing number of rows
>
> Could someone please help me?
>
>
> Thank you for your attention
>
> Stefano
>
>
>          (oo)
> --oOO--( )--OOo--------------------------------------
> Stefano Sofia PhD
> Civil Protection - Marche Region - Italy
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona (AN)
> Uff: +39 071 806 7743
> E-mail: stefano.sofia at regione.marche.it
> ---Oo---------oO----------------------------------------
>
> ________________________________
>
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
> informazioni confidenziali, pertanto ? destinato solo a persone autorizzate
> alla ricezione. I messaggi di posta elettronica per i client di Regione
> Marche possono contenere informazioni confidenziali e con privilegi legali.
> Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o
> archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore,
> inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio
> computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che, in
> caso di necessit? ed urgenza, la risposta al presente messaggio di posta
> elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by
> persons entitled to receive the confidential information it may contain.
> E-mail messages to clients of Regione Marche may contain information that
> is confidential and legally privileged. Please do not read, copy, forward,
> or store this message unless you are an intended recipient of it. If you
> have received this message in error, please forward it to the sender and
> delete it completely from your computer system.
>
> --
> Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non
> infetto.
> This message was scanned by Libraesva ESG and is believed to be clean.
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Fri Jun  3 02:26:53 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Thu, 2 Jun 2022 20:26:53 -0400
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <18788_1654150401_2526DLjR006367_53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
References: <18788_1654150401_2526DLjR006367_53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
Message-ID: <f453cd34-b3bd-43f5-755b-bf01f7c78fe7@mcmaster.ca>

Dear Stefano,

I don't believe that your question has been answered.

You can use merge(), twice:

------- snip --------

 > merge(merge(df1, df2, all=TRUE), df3, all=TRUE)
    data_POSIX Station2_Hs Station2_Hn Station2_flag Station3_Hs Station3_Hn
1  2000-12-01          20           0             0          NA          NA
2  2000-12-02          20           0             0          NA          NA
3  2000-12-03          30          10             0          NA          NA
4  2000-12-04          30           0             1          NA          NA
5  2000-12-05           0           5             0          NA          NA
6  2001-12-01          NA          NA            NA          20           0
7  2001-12-02          NA          NA            NA          20           0
8  2001-12-03          NA          NA            NA          30          10
9  2001-12-04          NA          NA            NA          30           0
10 2001-12-05          NA          NA            NA           0           5
11 2002-12-01          50          20             0          20           0
12 2002-12-02          60          20             0          20           0
13 2002-12-03          70          20             0          30          10
14 2002-12-04          NA          NA            NA          30           0
15 2002-12-05          NA          NA            NA           0           5
    Station3_flag Station1_Hs Station1_Hn Station1_flag
1             NA          30          10             0
2             NA          40          20             0
3             NA          50          10             0
4             NA          NA          NA            NA
5             NA          55           5             0
6              0          50          20             0
7              0          60          20             0
8              0          70          20             0
9              1          NA          NA            NA
10             0          NA          NA            NA
11             0          NA          NA            NA
12             0          NA          NA            NA
13             0          NA          NA            NA
14             1          NA          NA            NA
15             0          NA          NA            NA

------- snip --------

The columns aren't in the order that you specified but, if that's 
important, you can simply reorder them.

I hope this helps,
  John

-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/

On 2022-06-02 2:12 a.m., Stefano Sofia wrote:
> Dear R-list users,
> 
> for each winter season from 2000 to 2022 I have a data frame collecting for different weather stations snowpack height (Hs), snowfall in the last 24h (Hn) and a validation flag.
> 
> Suppose I have these three following data frames
> 
> 
> df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 40, 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0, 0, 0, NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10, 0, 5), Station2_flag = c(0, 0, 0, 1, 0))
> 
> 
> df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 60, 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> 
> 
> df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station2_Hs = c(50, 60, 70, NA, NA), Station2_Hn = c(20, 20, 20, NA, NA), Station2_flag = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> 
> 
> As you can see, each data frame can have different stations loaded.
> 
> I would need to call rbind matching data frames by column name (i.e. by station name), keeping in mind that the number of stations loaded in each data frame may differ. The result should be
> 
> data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs Station2_Hn Station2_flag Station3_Hs Station3_Hn Station3_flag
> 2000-12-01 30 10 0 20 0 0 NA NA NA
> 2000-12-02 40 20 0 20 0 0 NA NA NA
> 2000-12-03 50 10 0 30 10 0 NA NA NA
> 2000-12-04 NA NA NA 30 0 0 NA NA NA
> 2000-12-05 55 5 0 0 5 0 NA NA NA
> 2001-12-01 50 20 0 NA NA NA 20 0 0
> 2001-12-02 60 20 0 NA NA NA 20 0 0
> 2001-12-03 70 20 0 NA NA NA 30 10 0
> 2001-12-04 NA NA NA NA NA NA 30 0 1
> 2001-12-05 NA NA NA NA NA NA 0 5 0
> 2002-12-01 NA NA NA 50 20 0 20 0 0
> 2002-12-02 NA NA NA 60 20 0 20 0 0
> 2002-12-03 NA NA NA 70 20 0 30 10 0
> 2002-12-04 NA NA NA NA NA NA 30 0 1
> 2002-12-05 NA NA NA NA NA NA 0 5 0
> 
> I tried this code
> 
> df_list <- list(df1, df2, df3)
> allNms <- unique(unlist(lapply(df_list, names)))
> do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x, sapply(setdiff(allNms, names(x)), function(y) NA)))), make.row.names=FALSE))
> 
> but I get this error:
> Error in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE,  :
>    arguments imply differing number of rows
> 
> Could someone please help me?
> 
> 
> Thank you for your attention
> 
> Stefano
> 
> 
>           (oo)
> --oOO--( )--OOo--------------------------------------
> Stefano Sofia PhD
> Civil Protection - Marche Region - Italy
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona (AN)
> Uff: +39 071 806 7743
> E-mail: stefano.sofia at regione.marche.it
> ---Oo---------oO----------------------------------------
> 
> ________________________________
> 
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> 
> --
> Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non infetto.
> This message was scanned by Libraesva ESG and is believed to be clean.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t  Fri Jun  3 14:43:56 2022
From: @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t (Stefano Sofia)
Date: Fri, 3 Jun 2022 12:43:56 +0000
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <CAPcHnpT_qj4UONt=kgOn9g+x4Od6AgabMR8+ypRNWe3Uy=mD1Q@mail.gmail.com>
References: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
 <CAPcHnpQARh_MdZhkCi5w_bkx8-2NrT3Vukr6tu6L_8R1MczBfQ@mail.gmail.com>
 <602a61e019da43a69d37c4d59f630f6b@regione.marche.it>,
 <CAPcHnpT_qj4UONt=kgOn9g+x4Od6AgabMR8+ypRNWe3Uy=mD1Q@mail.gmail.com>
Message-ID: <a06135d4a46244d283a6f5eb30d8aae1@regione.marche.it>

Thank you to all who provided useful hints, great as always.

In my opinion the solution given by Andrew is perfect, exactly what I wanted to do without changing the format of my data frames:


df_list <- list(df1, df2, df3)

allNms <- unique(unlist(lapply(df_list, names)))

do.call(rbind, c(lapply(df_list, function(x) data.frame(x, sapply(setdiff(allNms, names(x)), function(y) NA, simplify = FALSE))), make.row.names=FALSE))


Now I encountered this final problem: when I load my data frames in R through


df1 <- read.table(file="/mypath/df1.csv", header = TRUE, sep=" ", dec = ".", stringsAsFactors = TRUE)

df1$data_POSIX <- as.POSIXct(df1$data_POSIX, format = "%Y-%m-%d", tz="Etc/GMT-1")


I get the following error:


Error in data.frame(x, sapply(setdiff(allNms, names(x)), function(y) NA,  :
  arguments imply differing number of rows:5, 0


Why? df1 is a data frame correctly filled in.


Thank you again

Stefano


         (oo)
--oOO--( )--OOo--------------------------------------
Stefano Sofia PhD
Civil Protection - Marche Region - Italy
Meteo Section
Snow Section
Via del Colle Ameno 5
60126 Torrette di Ancona, Ancona (AN)
Uff: +39 071 806 7743
E-mail: stefano.sofia at regione.marche.it
---Oo---------oO----------------------------------------


________________________________
Da: Andrew Simmons <akwsimmo at gmail.com>
Inviato: venerd? 3 giugno 2022 11:06
A: Stefano Sofia
Oggetto: Re: [R] rbind of multiple data frames by column name, when each data frames can contain different columns

I think I see the problem. I forgot that lapply doesn't assign names where sapply does. I think you might want to use that instead, but also supply argument simplify as FALSE

sapply(setdiff(allNms, names(x)), function(y) NA, simplify = FALSE)

It should produce something more like

$Station2_Hs
[1] NA

$Station2_Hn
[1] NA

$Station2_flag
[1] NA

which should combine nicely with data.frame(x,)


On Fri, Jun 3, 2022, 03:25 Stefano Sofia <stefano.sofia at regione.marche.it<mailto:stefano.sofia at regione.marche.it>> wrote:

Good morning Andrew.

Thank you for your help.

Unfortunately your suggestion does not work, and I got different errors depending on the use of the three small examples or real data frames.

If I apply your code to the three small examples I gave you (df1, df2 and df3) I get:


Error in match.names(clabs, names(xi)) :
  names do not match previous names


I tried to understand the origin of the problem.


lapply(setdiff(allNms, names(x)), function(y) NA)

gives


[[1]]
[1] NA

[[2]]
[1] NA

[[3]]
[1] NA

[[4]]
[1] NA

[[5]]
[1] NA

[[6]]
[1] NA

[[7]]
[1] NA

[[8]]
[1] NA

[[9]]
[1] NA

[[10]]
[1] NA


and

data.frame(x, lapply(setdiff(allNms, names(x)), function(y) NA))

gives

   x NA. NA..1 NA..2 NA..3 NA..4 NA..5 NA..6 NA..7 NA..8 NA..9
1  1  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
2  1  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
3  1  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
4  2  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
5  2  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
6  3  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
7  4  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
8  4  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
9  4  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
10 5  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
11 6  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
12 6  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
13 6  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA


Why?

Unfortunately this code is too difficult for me.

Sorry for bothering you, thank you for what you have already done.

Stefano


         (oo)
--oOO--( )--OOo--------------------------------------
Stefano Sofia PhD
Civil Protection - Marche Region - Italy
Meteo Section
Snow Section
Via del Colle Ameno 5
60126 Torrette di Ancona, Ancona (AN)
Uff: +39 071 806 7743
E-mail: stefano.sofia at regione.marche.it<mailto:stefano.sofia at regione.marche.it>
---Oo---------oO----------------------------------------


________________________________
Da: Andrew Simmons <akwsimmo at gmail.com<mailto:akwsimmo at gmail.com>>
Inviato: gioved? 2 giugno 2022 08:21
A: Stefano Sofia
Oggetto: Re: [R] rbind of multiple data frames by column name, when each data frames can contain different columns

I would change this:
do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x, sapply(setdiff(allNms, names(x)), function(y) NA)))), make.row.names=FALSE))
to:
do.call(rbind, c(lapply(df_list, function(x) data.frame(x, lapply(setdiff(allNms, names(x)), function(y) NA))), make.row.names=FALSE))


On Thu, Jun 2, 2022, 02:13 Stefano Sofia <stefano.sofia at regione.marche.it<mailto:stefano.sofia at regione.marche.it>> wrote:
Dear R-list users,

for each winter season from 2000 to 2022 I have a data frame collecting for different weather stations snowpack height (Hs), snowfall in the last 24h (Hn) and a validation flag.

Suppose I have these three following data frames


df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 40, 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0, 0, 0, NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10, 0, 5), Station2_flag = c(0, 0, 0, 1, 0))


df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 60, 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))


df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"),   = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))


As you can see, each data frame can have different stations loaded.

I would need to call rbind matching data frames by column name (i.e. by station name), keeping in mind that the number of stations loaded in each data frame may differ. The result should be

data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs Station2_Hn Station2_flag Station3_Hs Station3_Hn Station3_flag
2000-12-01 30 10 0 20 0 0 NA NA NA
2000-12-02 40 20 0 20 0 0 NA NA NA
2000-12-03 50 10 0 30 10 0 NA NA NA
2000-12-04 NA NA NA 30 0 0 NA NA NA
2000-12-05 55 5 0 0 5 0 NA NA NA
2001-12-01 50 20 0 NA NA NA 20 0 0
2001-12-02 60 20 0 NA NA NA 20 0 0
2001-12-03 70 20 0 NA NA NA 30 10 0
2001-12-04 NA NA NA NA NA NA 30 0 1
2001-12-05 NA NA NA NA NA NA 0 5 0
2002-12-01 NA NA NA 50 20 0 20 0 0
2002-12-02 NA NA NA 60 20 0 20 0 0
2002-12-03 NA NA NA 70 20 0 30 10 0
2002-12-04 NA NA NA NA NA NA 30 0 1
2002-12-05 NA NA NA NA NA NA 0 5 0

I tried this code

df_list <- list(df1, df2, df3)
allNms <- unique(unlist(lapply(df_list, names)))
do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x, sapply(setdiff(allNms, names(x)), function(y) NA)))), make.row.names=FALSE))

but I get this error:
Error in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE,  :
  arguments imply differing number of rows

Could someone please help me?


Thank you for your attention

Stefano


         (oo)
--oOO--( )--OOo--------------------------------------
Stefano Sofia PhD
Civil Protection - Marche Region - Italy
Meteo Section
Snow Section
Via del Colle Ameno 5
60126 Torrette di Ancona, Ancona (AN)
Uff: +39 071 806 7743
E-mail: stefano.sofia at regione.marche.it<mailto:stefano.sofia at regione.marche.it>
---Oo---------oO----------------------------------------

________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

--
Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non infetto.
This message was scanned by Libraesva ESG and is believed to be clean.


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help<https://urlsand.esvalabs.com/?u=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&e=5a635173&h=06ff70f3&f=y&p=y>
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html<https://urlsand.esvalabs.com/?u=http%3A%2F%2Fwww.R-project.org%2Fposting-guide.html&e=5a635173&h=e12f63e8&f=y&p=y>
and provide commented, minimal, self-contained, reproducible code.

--
Questo messaggio ? stato analizzato con Libraesva ESG ed ? risultato non infetto

________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

--
Questo messaggio ? stato analizzato con Libraesva ESG ed ? risultato non infetto.
This message has been checked by Libraesva ESG and is believed to be clean.

--
Questo messaggio ? stato analizzato con Libraesva ESG ed ? risultato non infetto

________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

--
Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non infetto.
This message was scanned by Libraesva ESG and is believed to be clean.


	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Fri Jun  3 15:36:45 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Fri, 3 Jun 2022 09:36:45 -0400
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <25376_1654260288_253CimUt010543_a06135d4a46244d283a6f5eb30d8aae1@regione.marche.it>
References: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
 <CAPcHnpQARh_MdZhkCi5w_bkx8-2NrT3Vukr6tu6L_8R1MczBfQ@mail.gmail.com>
 <602a61e019da43a69d37c4d59f630f6b@regione.marche.it>
 <CAPcHnpT_qj4UONt=kgOn9g+x4Od6AgabMR8+ypRNWe3Uy=mD1Q@mail.gmail.com>
 <25376_1654260288_253CimUt010543_a06135d4a46244d283a6f5eb30d8aae1@regione.marche.it>
Message-ID: <1adea36c-72cc-fafd-e887-93c016283927@mcmaster.ca>

Dear Stefano,

On 2022-06-03 8:43 a.m., Stefano Sofia wrote:
> Thank you to all who provided useful hints, great as always.
> 
> In my opinion the solution given by Andrew is perfect, exactly what I wanted to do without changing the format of my data frames:
> 
> 
> df_list <- list(df1, df2, df3)
> 
> allNms <- unique(unlist(lapply(df_list, names)))
> 
> do.call(rbind, c(lapply(df_list, function(x) data.frame(x, sapply(setdiff(allNms, names(x)), function(y) NA, simplify = FALSE))), make.row.names=FALSE))

I'm moderately surprised that you find this simpler than, say,

	merge(merge(df1, df2, all=TRUE), df3, all=TRUE)[allNms]

> 
> 
> Now I encountered this final problem: when I load my data frames in R through
> 
> 
> df1 <- read.table(file="/mypath/df1.csv", header = TRUE, sep=" ", dec = ".", stringsAsFactors = TRUE)
> 
> df1$data_POSIX <- as.POSIXct(df1$data_POSIX, format = "%Y-%m-%d", tz="Etc/GMT-1")
> 
> 
> I get the following error:
> 
> 
> Error in data.frame(x, sapply(setdiff(allNms, names(x)), function(y) NA,  :
>    arguments imply differing number of rows:5, 0
> 
> 
> Why? df1 is a data frame correctly filled in.

I can't duplicate this error (and you don't provide the data files). A 
couple of thoughts:

(1) Why set stringsAsFactors = TRUE when you plan to convert the first 
column to dates (though that doesn't account for the error)?

(2) If df1.csv is really a csv file (and why use the extension .csv if 
it isn't?), why specify sep=" " (though if the data file is really a csv 
file, the resulting df1 would be corrupted, and you say it isn't).

Best,
John

> 
> 
> Thank you again
> 
> Stefano
> 
> 
>           (oo)
> --oOO--( )--OOo--------------------------------------
> Stefano Sofia PhD
> Civil Protection - Marche Region - Italy
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona (AN)
> Uff: +39 071 806 7743
> E-mail: stefano.sofia at regione.marche.it
> ---Oo---------oO----------------------------------------
> 
> 
> ________________________________
> Da: Andrew Simmons <akwsimmo at gmail.com>
> Inviato: venerd? 3 giugno 2022 11:06
> A: Stefano Sofia
> Oggetto: Re: [R] rbind of multiple data frames by column name, when each data frames can contain different columns
> 
> I think I see the problem. I forgot that lapply doesn't assign names where sapply does. I think you might want to use that instead, but also supply argument simplify as FALSE
> 
> sapply(setdiff(allNms, names(x)), function(y) NA, simplify = FALSE)
> 
> It should produce something more like
> 
> $Station2_Hs
> [1] NA
> 
> $Station2_Hn
> [1] NA
> 
> $Station2_flag
> [1] NA
> 
> which should combine nicely with data.frame(x,)
> 
> 
> On Fri, Jun 3, 2022, 03:25 Stefano Sofia <stefano.sofia at regione.marche.it<mailto:stefano.sofia at regione.marche.it>> wrote:
> 
> Good morning Andrew.
> 
> Thank you for your help.
> 
> Unfortunately your suggestion does not work, and I got different errors depending on the use of the three small examples or real data frames.
> 
> If I apply your code to the three small examples I gave you (df1, df2 and df3) I get:
> 
> 
> Error in match.names(clabs, names(xi)) :
>    names do not match previous names
> 
> 
> I tried to understand the origin of the problem.
> 
> 
> lapply(setdiff(allNms, names(x)), function(y) NA)
> 
> gives
> 
> 
> [[1]]
> [1] NA
> 
> [[2]]
> [1] NA
> 
> [[3]]
> [1] NA
> 
> [[4]]
> [1] NA
> 
> [[5]]
> [1] NA
> 
> [[6]]
> [1] NA
> 
> [[7]]
> [1] NA
> 
> [[8]]
> [1] NA
> 
> [[9]]
> [1] NA
> 
> [[10]]
> [1] NA
> 
> 
> and
> 
> data.frame(x, lapply(setdiff(allNms, names(x)), function(y) NA))
> 
> gives
> 
>     x NA. NA..1 NA..2 NA..3 NA..4 NA..5 NA..6 NA..7 NA..8 NA..9
> 1  1  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 2  1  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 3  1  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 4  2  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 5  2  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 6  3  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 7  4  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 8  4  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 9  4  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 10 5  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 11 6  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 12 6  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 13 6  NA    NA    NA    NA    NA    NA    NA    NA    NA    NA
> 
> 
> Why?
> 
> Unfortunately this code is too difficult for me.
> 
> Sorry for bothering you, thank you for what you have already done.
> 
> Stefano
> 
> 
>           (oo)
> --oOO--( )--OOo--------------------------------------
> Stefano Sofia PhD
> Civil Protection - Marche Region - Italy
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona (AN)
> Uff: +39 071 806 7743
> E-mail: stefano.sofia at regione.marche.it<mailto:stefano.sofia at regione.marche.it>
> ---Oo---------oO----------------------------------------
> 
> 
> ________________________________
> Da: Andrew Simmons <akwsimmo at gmail.com<mailto:akwsimmo at gmail.com>>
> Inviato: gioved? 2 giugno 2022 08:21
> A: Stefano Sofia
> Oggetto: Re: [R] rbind of multiple data frames by column name, when each data frames can contain different columns
> 
> I would change this:
> do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x, sapply(setdiff(allNms, names(x)), function(y) NA)))), make.row.names=FALSE))
> to:
> do.call(rbind, c(lapply(df_list, function(x) data.frame(x, lapply(setdiff(allNms, names(x)), function(y) NA))), make.row.names=FALSE))
> 
> 
> On Thu, Jun 2, 2022, 02:13 Stefano Sofia <stefano.sofia at regione.marche.it<mailto:stefano.sofia at regione.marche.it>> wrote:
> Dear R-list users,
> 
> for each winter season from 2000 to 2022 I have a data frame collecting for different weather stations snowpack height (Hs), snowfall in the last 24h (Hn) and a validation flag.
> 
> Suppose I have these three following data frames
> 
> 
> df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 40, 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0, 0, 0, NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10, 0, 5), Station2_flag = c(0, 0, 0, 1, 0))
> 
> 
> df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 60, 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> 
> 
> df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01", format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05", format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"),   = c(0, 0, 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10, 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> 
> 
> As you can see, each data frame can have different stations loaded.
> 
> I would need to call rbind matching data frames by column name (i.e. by station name), keeping in mind that the number of stations loaded in each data frame may differ. The result should be
> 
> data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs Station2_Hn Station2_flag Station3_Hs Station3_Hn Station3_flag
> 2000-12-01 30 10 0 20 0 0 NA NA NA
> 2000-12-02 40 20 0 20 0 0 NA NA NA
> 2000-12-03 50 10 0 30 10 0 NA NA NA
> 2000-12-04 NA NA NA 30 0 0 NA NA NA
> 2000-12-05 55 5 0 0 5 0 NA NA NA
> 2001-12-01 50 20 0 NA NA NA 20 0 0
> 2001-12-02 60 20 0 NA NA NA 20 0 0
> 2001-12-03 70 20 0 NA NA NA 30 10 0
> 2001-12-04 NA NA NA NA NA NA 30 0 1
> 2001-12-05 NA NA NA NA NA NA 0 5 0
> 2002-12-01 NA NA NA 50 20 0 20 0 0
> 2002-12-02 NA NA NA 60 20 0 20 0 0
> 2002-12-03 NA NA NA 70 20 0 30 10 0
> 2002-12-04 NA NA NA NA NA NA 30 0 1
> 2002-12-05 NA NA NA NA NA NA 0 5 0
> 
> I tried this code
> 
> df_list <- list(df1, df2, df3)
> allNms <- unique(unlist(lapply(df_list, names)))
> do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x, sapply(setdiff(allNms, names(x)), function(y) NA)))), make.row.names=FALSE))
> 
> but I get this error:
> Error in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE,  :
>    arguments imply differing number of rows
> 
> Could someone please help me?
> 
> 
> Thank you for your attention
> 
> Stefano
> 
> 
>           (oo)
> --oOO--( )--OOo--------------------------------------
> Stefano Sofia PhD
> Civil Protection - Marche Region - Italy
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona (AN)
> Uff: +39 071 806 7743
> E-mail: stefano.sofia at regione.marche.it<mailto:stefano.sofia at regione.marche.it>
> ---Oo---------oO----------------------------------------
> 
> ________________________________
> 
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> 
> --
> Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non infetto.
> This message was scanned by Libraesva ESG and is believed to be clean.
> 
> 
>          [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help<https://urlsand.esvalabs.com/?u=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&e=5a635173&h=06ff70f3&f=y&p=y>
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html<https://urlsand.esvalabs.com/?u=http%3A%2F%2Fwww.R-project.org%2Fposting-guide.html&e=5a635173&h=e12f63e8&f=y&p=y>
> and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Questo messaggio ? stato analizzato con Libraesva ESG ed ? risultato non infetto
> 
> ________________________________
> 
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> 
> --
> Questo messaggio ? stato analizzato con Libraesva ESG ed ? risultato non infetto.
> This message has been checked by Libraesva ESG and is believed to be clean.
> 
> --
> Questo messaggio ? stato analizzato con Libraesva ESG ed ? risultato non infetto
> 
> ________________________________
> 
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> 
> --
> Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non infetto.
> This message was scanned by Libraesva ESG and is believed to be clean.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


From po|c1410 @end|ng |rom gm@||@com  Fri Jun  3 09:48:23 2022
From: po|c1410 @end|ng |rom gm@||@com (CALUM POLWART)
Date: Fri, 3 Jun 2022 08:48:23 +0100
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <CAGxFJbS9+zZMRSaRU7KhBEJUWb8kiKjy51xe5TtjyDboKDafxQ@mail.gmail.com>
References: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
 <CAGxFJbS9+zZMRSaRU7KhBEJUWb8kiKjy51xe5TtjyDboKDafxQ@mail.gmail.com>
Message-ID: <CA+etgPmdMNTxovwcU+Drrhi-6-Wua_dVO+fiJht7yZNVEt4QeQ@mail.gmail.com>

Bert! It sounds like you are warming to the the tidyverse! ;-)

I completed agree with your analysis this data would be best served long as
you have shown.

If the OP was going to use tidy to manipulate it, they can do the same with
tidyr::pivot_ functions (pivot_longer and pivot_wider). The result will be
the same - but understanding how it happened may be easier!

On Thu, 2 Jun 2022, 21:04 Bert Gunter, <bgunter.4567 at gmail.com> wrote:

> Well, it seems better to me to put all the data frames in long format and
> then rbind them instead of the other way round, which results in the piles
> of NA's you see. I note also, FWIW, that this accords with the so-called
> "tidy" format that many advocate these days. You can always subset (rows)
> and choose by station, date, etc. as needed, of course from the long
> format.
>
> Because of the regularity of your data frames, it is easy to do this. Here
> is a little base R function that "reforms" each data frame (I suspect Rui
> may well provide a more elegant version, though):
>
>
> reform<- function(dat){
>    nm <- names(dat)
>    stanums <- unique(gsub("[^[:digit:]]","", nm[-1])) ## station numbers
> present
>     z <- do.call(rbind, lapply(stanums,
>       \(i)
>       structure(dat[,grep(i, nm, fixed = TRUE)],
>                 names = c("Hs", "Hn", "flag"))))
>    data.frame(POSIX =rep(dat[,1], length(stanums)),
>               Station = rep(stanums, e = nrow(dat)),
>               z)
> }
>
> e.g.
> > reform(df2)
>         POSIX Station Hs Hn flag
> 1  2001-12-01       1 50 20    0
> 2  2001-12-02       1 60 20    0
> 3  2001-12-03       1 70 20    0
> 4  2001-12-04       1 NA NA   NA
> 5  2001-12-05       1 NA NA   NA
> 6  2001-12-01       3 20  0    0
> 7  2001-12-02       3 20  0    0
> 8  2001-12-03       3 30 10    0
> 9  2001-12-04       3 30  0    1
> 10 2001-12-05       3  0  5    0
>
> A call to rbind() of the following form then gives you all your data in
> long form(you may wish to use some shortcuts to form the list of frames):
>
> > do.call(rbind, lapply(list(df1, df2, df3), reform))
>         POSIX Station Hs Hn flag
> 1  2000-12-01       1 30 10    0
> 2  2000-12-02       1 40 20    0
> 3  2000-12-03       1 50 10    0
> 4  2000-12-04       1 NA NA   NA
> 5  2000-12-05       1 55  5    0
> 6  2000-12-01       2 20  0    0
> 7  2000-12-02       2 20  0    0
> 8  2000-12-03       2 30 10    0
> 9  2000-12-04       2 30  0    1
> 10 2000-12-05       2  0  5    0
> 11 2001-12-01       1 50 20    0
> 12 2001-12-02       1 60 20    0
> 13 2001-12-03       1 70 20    0
> 14 2001-12-04       1 NA NA   NA
> 15 2001-12-05       1 NA NA   NA
> 16 2001-12-01       3 20  0    0
> 17 2001-12-02       3 20  0    0
> 18 2001-12-03       3 30 10    0
> 19 2001-12-04       3 30  0    1
> 20 2001-12-05       3  0  5    0
> 21 2002-12-01       2 50 20    0
> 22 2002-12-02       2 60 20    0
> 23 2002-12-03       2 70 20    0
> 24 2002-12-04       2 NA NA   NA
> 25 2002-12-05       2 NA NA   NA
> 26 2002-12-01       3 20  0    0
> 27 2002-12-02       3 20  0    0
> 28 2002-12-03       3 30 10    0
> 29 2002-12-04       3 30  0    1
> 30 2002-12-05       3  0  5    0
>
>
> Cheers,
> Bert Gunter
>
>
>
>
> On Wed, Jun 1, 2022 at 11:13 PM Stefano Sofia <
> stefano.sofia at regione.marche.it> wrote:
>
> > Dear R-list users,
> >
> > for each winter season from 2000 to 2022 I have a data frame collecting
> > for different weather stations snowpack height (Hs), snowfall in the last
> > 24h (Hn) and a validation flag.
> >
> > Suppose I have these three following data frames
> >
> >
> > df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 40,
> > 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0, 0,
> 0,
> > NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10, 0,
> > 5), Station2_flag = c(0, 0, 0, 1, 0))
> >
> >
> > df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 60,
> > 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0, 0,
> > 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10,
> > 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> >
> >
> > df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station2_Hs = c(50, 60,
> > 70, NA, NA), Station2_Hn = c(20, 20, 20, NA, NA), Station2_flag = c(0, 0,
> > 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10,
> > 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> >
> >
> > As you can see, each data frame can have different stations loaded.
> >
> > I would need to call rbind matching data frames by column name (i.e. by
> > station name), keeping in mind that the number of stations loaded in each
> > data frame may differ. The result should be
> >
> > data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs Station2_Hn
> > Station2_flag Station3_Hs Station3_Hn Station3_flag
> > 2000-12-01 30 10 0 20 0 0 NA NA NA
> > 2000-12-02 40 20 0 20 0 0 NA NA NA
> > 2000-12-03 50 10 0 30 10 0 NA NA NA
> > 2000-12-04 NA NA NA 30 0 0 NA NA NA
> > 2000-12-05 55 5 0 0 5 0 NA NA NA
> > 2001-12-01 50 20 0 NA NA NA 20 0 0
> > 2001-12-02 60 20 0 NA NA NA 20 0 0
> > 2001-12-03 70 20 0 NA NA NA 30 10 0
> > 2001-12-04 NA NA NA NA NA NA 30 0 1
> > 2001-12-05 NA NA NA NA NA NA 0 5 0
> > 2002-12-01 NA NA NA 50 20 0 20 0 0
> > 2002-12-02 NA NA NA 60 20 0 20 0 0
> > 2002-12-03 NA NA NA 70 20 0 30 10 0
> > 2002-12-04 NA NA NA NA NA NA 30 0 1
> > 2002-12-05 NA NA NA NA NA NA 0 5 0
> >
> > I tried this code
> >
> > df_list <- list(df1, df2, df3)
> > allNms <- unique(unlist(lapply(df_list, names)))
> > do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x,
> > sapply(setdiff(allNms, names(x)), function(y) NA)))),
> make.row.names=FALSE))
> >
> > but I get this error:
> > Error in (function (..., row.names = NULL, check.rows = FALSE,
> check.names
> > = TRUE,  :
> >   arguments imply differing number of rows
> >
> > Could someone please help me?
> >
> >
> > Thank you for your attention
> >
> > Stefano
> >
> >
> >          (oo)
> > --oOO--( )--OOo--------------------------------------
> > Stefano Sofia PhD
> > Civil Protection - Marche Region - Italy
> > Meteo Section
> > Snow Section
> > Via del Colle Ameno 5
> > 60126 Torrette di Ancona, Ancona (AN)
> > Uff: +39 071 806 7743
> > E-mail: stefano.sofia at regione.marche.it
> > ---Oo---------oO----------------------------------------
> >
> > ________________________________
> >
> > AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
> > informazioni confidenziali, pertanto ? destinato solo a persone
> autorizzate
> > alla ricezione. I messaggi di posta elettronica per i client di Regione
> > Marche possono contenere informazioni confidenziali e con privilegi
> legali.
> > Se non si ? il destinatario specificato, non leggere, copiare, inoltrare
> o
> > archiviare questo messaggio. Se si ? ricevuto questo messaggio per
> errore,
> > inoltrarlo al mittente ed eliminarlo completamente dal sistema del
> proprio
> > computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che, in
> > caso di necessit? ed urgenza, la risposta al presente messaggio di posta
> > elettronica pu? essere visionata da persone estranee al destinatario.
> > IMPORTANT NOTICE: This e-mail message is intended to be received only by
> > persons entitled to receive the confidential information it may contain.
> > E-mail messages to clients of Regione Marche may contain information that
> > is confidential and legally privileged. Please do not read, copy,
> forward,
> > or store this message unless you are an intended recipient of it. If you
> > have received this message in error, please forward it to the sender and
> > delete it completely from your computer system.
> >
> > --
> > Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non
> > infetto.
> > This message was scanned by Libraesva ESG and is believed to be clean.
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From d@vidwk@tz m@iii@g oii gm@ii@com  Fri Jun  3 20:12:31 2022
From: d@vidwk@tz m@iii@g oii gm@ii@com (d@vidwk@tz m@iii@g oii gm@ii@com)
Date: Fri, 3 Jun 2022 11:12:31 -0700
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <CA+etgPmdMNTxovwcU+Drrhi-6-Wua_dVO+fiJht7yZNVEt4QeQ@mail.gmail.com>
References: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
 <CAGxFJbS9+zZMRSaRU7KhBEJUWb8kiKjy51xe5TtjyDboKDafxQ@mail.gmail.com>
 <CA+etgPmdMNTxovwcU+Drrhi-6-Wua_dVO+fiJht7yZNVEt4QeQ@mail.gmail.com>
Message-ID: <CAL+5cujqHe15QWA+fqw+K1AYvSCe90FRTMUYqp_83G=PMzdccw@mail.gmail.com>

library(data.table)
?rbindlist

On Fri, Jun 3, 2022 at 11:06 AM CALUM POLWART <polc1410 at gmail.com> wrote:

> Bert! It sounds like you are warming to the the tidyverse! ;-)
>
> I completed agree with your analysis this data would be best served long as
> you have shown.
>
> If the OP was going to use tidy to manipulate it, they can do the same with
> tidyr::pivot_ functions (pivot_longer and pivot_wider). The result will be
> the same - but understanding how it happened may be easier!
>
> On Thu, 2 Jun 2022, 21:04 Bert Gunter, <bgunter.4567 at gmail.com> wrote:
>
> > Well, it seems better to me to put all the data frames in long format and
> > then rbind them instead of the other way round, which results in the
> piles
> > of NA's you see. I note also, FWIW, that this accords with the so-called
> > "tidy" format that many advocate these days. You can always subset (rows)
> > and choose by station, date, etc. as needed, of course from the long
> > format.
> >
> > Because of the regularity of your data frames, it is easy to do this.
> Here
> > is a little base R function that "reforms" each data frame (I suspect Rui
> > may well provide a more elegant version, though):
> >
> >
> > reform<- function(dat){
> >    nm <- names(dat)
> >    stanums <- unique(gsub("[^[:digit:]]","", nm[-1])) ## station numbers
> > present
> >     z <- do.call(rbind, lapply(stanums,
> >       \(i)
> >       structure(dat[,grep(i, nm, fixed = TRUE)],
> >                 names = c("Hs", "Hn", "flag"))))
> >    data.frame(POSIX =rep(dat[,1], length(stanums)),
> >               Station = rep(stanums, e = nrow(dat)),
> >               z)
> > }
> >
> > e.g.
> > > reform(df2)
> >         POSIX Station Hs Hn flag
> > 1  2001-12-01       1 50 20    0
> > 2  2001-12-02       1 60 20    0
> > 3  2001-12-03       1 70 20    0
> > 4  2001-12-04       1 NA NA   NA
> > 5  2001-12-05       1 NA NA   NA
> > 6  2001-12-01       3 20  0    0
> > 7  2001-12-02       3 20  0    0
> > 8  2001-12-03       3 30 10    0
> > 9  2001-12-04       3 30  0    1
> > 10 2001-12-05       3  0  5    0
> >
> > A call to rbind() of the following form then gives you all your data in
> > long form(you may wish to use some shortcuts to form the list of frames):
> >
> > > do.call(rbind, lapply(list(df1, df2, df3), reform))
> >         POSIX Station Hs Hn flag
> > 1  2000-12-01       1 30 10    0
> > 2  2000-12-02       1 40 20    0
> > 3  2000-12-03       1 50 10    0
> > 4  2000-12-04       1 NA NA   NA
> > 5  2000-12-05       1 55  5    0
> > 6  2000-12-01       2 20  0    0
> > 7  2000-12-02       2 20  0    0
> > 8  2000-12-03       2 30 10    0
> > 9  2000-12-04       2 30  0    1
> > 10 2000-12-05       2  0  5    0
> > 11 2001-12-01       1 50 20    0
> > 12 2001-12-02       1 60 20    0
> > 13 2001-12-03       1 70 20    0
> > 14 2001-12-04       1 NA NA   NA
> > 15 2001-12-05       1 NA NA   NA
> > 16 2001-12-01       3 20  0    0
> > 17 2001-12-02       3 20  0    0
> > 18 2001-12-03       3 30 10    0
> > 19 2001-12-04       3 30  0    1
> > 20 2001-12-05       3  0  5    0
> > 21 2002-12-01       2 50 20    0
> > 22 2002-12-02       2 60 20    0
> > 23 2002-12-03       2 70 20    0
> > 24 2002-12-04       2 NA NA   NA
> > 25 2002-12-05       2 NA NA   NA
> > 26 2002-12-01       3 20  0    0
> > 27 2002-12-02       3 20  0    0
> > 28 2002-12-03       3 30 10    0
> > 29 2002-12-04       3 30  0    1
> > 30 2002-12-05       3  0  5    0
> >
> >
> > Cheers,
> > Bert Gunter
> >
> >
> >
> >
> > On Wed, Jun 1, 2022 at 11:13 PM Stefano Sofia <
> > stefano.sofia at regione.marche.it> wrote:
> >
> > > Dear R-list users,
> > >
> > > for each winter season from 2000 to 2022 I have a data frame collecting
> > > for different weather stations snowpack height (Hs), snowfall in the
> last
> > > 24h (Hn) and a validation flag.
> > >
> > > Suppose I have these three following data frames
> > >
> > >
> > > df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01",
> > > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05",
> > > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30,
> 40,
> > > 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0,
> 0,
> > 0,
> > > NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10,
> 0,
> > > 5), Station2_flag = c(0, 0, 0, 1, 0))
> > >
> > >
> > > df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01",
> > > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05",
> > > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50,
> 60,
> > > 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0,
> 0,
> > > 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0,
> 10,
> > > 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> > >
> > >
> > > df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01",
> > > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05",
> > > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station2_Hs = c(50,
> 60,
> > > 70, NA, NA), Station2_Hn = c(20, 20, 20, NA, NA), Station2_flag = c(0,
> 0,
> > > 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0,
> 10,
> > > 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> > >
> > >
> > > As you can see, each data frame can have different stations loaded.
> > >
> > > I would need to call rbind matching data frames by column name (i.e. by
> > > station name), keeping in mind that the number of stations loaded in
> each
> > > data frame may differ. The result should be
> > >
> > > data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs
> Station2_Hn
> > > Station2_flag Station3_Hs Station3_Hn Station3_flag
> > > 2000-12-01 30 10 0 20 0 0 NA NA NA
> > > 2000-12-02 40 20 0 20 0 0 NA NA NA
> > > 2000-12-03 50 10 0 30 10 0 NA NA NA
> > > 2000-12-04 NA NA NA 30 0 0 NA NA NA
> > > 2000-12-05 55 5 0 0 5 0 NA NA NA
> > > 2001-12-01 50 20 0 NA NA NA 20 0 0
> > > 2001-12-02 60 20 0 NA NA NA 20 0 0
> > > 2001-12-03 70 20 0 NA NA NA 30 10 0
> > > 2001-12-04 NA NA NA NA NA NA 30 0 1
> > > 2001-12-05 NA NA NA NA NA NA 0 5 0
> > > 2002-12-01 NA NA NA 50 20 0 20 0 0
> > > 2002-12-02 NA NA NA 60 20 0 20 0 0
> > > 2002-12-03 NA NA NA 70 20 0 30 10 0
> > > 2002-12-04 NA NA NA NA NA NA 30 0 1
> > > 2002-12-05 NA NA NA NA NA NA 0 5 0
> > >
> > > I tried this code
> > >
> > > df_list <- list(df1, df2, df3)
> > > allNms <- unique(unlist(lapply(df_list, names)))
> > > do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x,
> > > sapply(setdiff(allNms, names(x)), function(y) NA)))),
> > make.row.names=FALSE))
> > >
> > > but I get this error:
> > > Error in (function (..., row.names = NULL, check.rows = FALSE,
> > check.names
> > > = TRUE,  :
> > >   arguments imply differing number of rows
> > >
> > > Could someone please help me?
> > >
> > >
> > > Thank you for your attention
> > >
> > > Stefano
> > >
> > >
> > >          (oo)
> > > --oOO--( )--OOo--------------------------------------
> > > Stefano Sofia PhD
> > > Civil Protection - Marche Region - Italy
> > > Meteo Section
> > > Snow Section
> > > Via del Colle Ameno 5
> > > 60126 Torrette di Ancona, Ancona (AN)
> > > Uff: +39 071 806 7743
> > > E-mail: stefano.sofia at regione.marche.it
> > > ---Oo---------oO----------------------------------------
> > >
> > > ________________________________
> > >
> > > AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
> > > informazioni confidenziali, pertanto ? destinato solo a persone
> > autorizzate
> > > alla ricezione. I messaggi di posta elettronica per i client di Regione
> > > Marche possono contenere informazioni confidenziali e con privilegi
> > legali.
> > > Se non si ? il destinatario specificato, non leggere, copiare,
> inoltrare
> > o
> > > archiviare questo messaggio. Se si ? ricevuto questo messaggio per
> > errore,
> > > inoltrarlo al mittente ed eliminarlo completamente dal sistema del
> > proprio
> > > computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che,
> in
> > > caso di necessit? ed urgenza, la risposta al presente messaggio di
> posta
> > > elettronica pu? essere visionata da persone estranee al destinatario.
> > > IMPORTANT NOTICE: This e-mail message is intended to be received only
> by
> > > persons entitled to receive the confidential information it may
> contain.
> > > E-mail messages to clients of Regione Marche may contain information
> that
> > > is confidential and legally privileged. Please do not read, copy,
> > forward,
> > > or store this message unless you are an intended recipient of it. If
> you
> > > have received this message in error, please forward it to the sender
> and
> > > delete it completely from your computer system.
> > >
> > > --
> > > Questo messaggio  stato analizzato da Libraesva ESG ed  risultato non
> > > infetto.
> > > This message was scanned by Libraesva ESG and is believed to be clean.
> > >
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @v|gro@@ @end|ng |rom ver|zon@net  Fri Jun  3 20:51:15 2022
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Fri, 3 Jun 2022 18:51:15 +0000 (UTC)
Subject: [R] rbind of multiple data frames by column name,
 when each data frames can contain different columns
In-Reply-To: <CA+etgPmdMNTxovwcU+Drrhi-6-Wua_dVO+fiJht7yZNVEt4QeQ@mail.gmail.com>
References: <53bbac255bc84b64beecadbe62c4d092@regione.marche.it>
 <CAGxFJbS9+zZMRSaRU7KhBEJUWb8kiKjy51xe5TtjyDboKDafxQ@mail.gmail.com>
 <CA+etgPmdMNTxovwcU+Drrhi-6-Wua_dVO+fiJht7yZNVEt4QeQ@mail.gmail.com>
Message-ID: <1385052253.3823207.1654282275962@mail.yahoo.com>

Complexity has all kinds of costs and tradeoffs. In one sense?you are doing a conceptual merge between related but not?identical database tables albeit from a set of .CSV files.

You do have some??conceptually direct ways using base R or?packages like dplyr that??possibly do it in a more complex way?outside your view. Hypothetically, you could even work?sideways and open a connection to a database and store the?contents of each .CSV files into a relational table and then?issue some SQL commands asking to do the merge and read them?back but that would be weird and overkill as R has similar abilities.

But underneath the covers, as shown by the longer code, the R?functionality has to determine what field/column names are common?between pairs of data.frames and fill any missing ones with NA and?so on, perhaps faster than the code being shown and perhaps not.?Much depends on the code used and how general it is as it is?often weighted down by lots of additional functionality you are?not using, and whether it has big chunks being used that were?rewritten in C or C++ or other libraries from FORTRAN ...

But unless the amount of data gets huge and it takes too much?time or memory, I would opt for the conceptually simpler method of?using merge() or the full_join() in dplyr, or as pointed out,?other techniques by changing the formats. I can think of other ways?such as getting the combined column names from all the files,?then adding missing ones to each data.frame full of NA as needed, then?reordering the columns so they all are identical, and finally vertically?combining all the resulting rows. Again note the internal methods used?by any merge method (including in SQL) may simply automate tasks.

The more complex methods shown strike me as a tad painful to read, let?alone to modify when your needs change. Just because some people can?program all kinds of things in low-level languages like assembler, that does?not justify spending months on what can be done in days or even minutes?using pre-built and pre-tested methods layered one upon another.

If there is already an acceptable function that already accepts a list of such?overlapping data.frames, then other than HW problems, that might be even better.?It looks fairly easy to construct such a function based on the discussion so I wonder?if some package already has one.


-----Original Message-----
From: CALUM POLWART <polc1410 at gmail.com>
To: Bert Gunter <bgunter.4567 at gmail.com>
Cc: r-help at R-project.org <r-help at r-project.org>; Stefano Sofia <stefano.sofia at regione.marche.it>
Sent: Fri, Jun 3, 2022 3:48 am
Subject: Re: [R] rbind of multiple data frames by column name, when each data frames can contain different columns

Bert! It sounds like you are warming to the the tidyverse! ;-)

I completed agree with your analysis this data would be best served long as
you have shown.

If the OP was going to use tidy to manipulate it, they can do the same with
tidyr::pivot_ functions (pivot_longer and pivot_wider). The result will be
the same - but understanding how it happened may be easier!

On Thu, 2 Jun 2022, 21:04 Bert Gunter, <bgunter.4567 at gmail.com> wrote:

> Well, it seems better to me to put all the data frames in long format and
> then rbind them instead of the other way round, which results in the piles
> of NA's you see. I note also, FWIW, that this accords with the so-called
> "tidy" format that many advocate these days. You can always subset (rows)
> and choose by station, date, etc. as needed, of course from the long
> format.
>
> Because of the regularity of your data frames, it is easy to do this. Here
> is a little base R function that "reforms" each data frame (I suspect Rui
> may well provide a more elegant version, though):
>
>
> reform<- function(dat){
>? ? nm <- names(dat)
>? ? stanums <- unique(gsub("[^[:digit:]]","", nm[-1])) ## station numbers
> present
>? ? z <- do.call(rbind, lapply(stanums,
>? ? ? \(i)
>? ? ? structure(dat[,grep(i, nm, fixed = TRUE)],
>? ? ? ? ? ? ? ? names = c("Hs", "Hn", "flag"))))
>? ? data.frame(POSIX =rep(dat[,1], length(stanums)),
>? ? ? ? ? ? ? Station = rep(stanums, e = nrow(dat)),
>? ? ? ? ? ? ? z)
> }
>
> e.g.
> > reform(df2)
>? ? ? ? POSIX Station Hs Hn flag
> 1? 2001-12-01? ? ? 1 50 20? ? 0
> 2? 2001-12-02? ? ? 1 60 20? ? 0
> 3? 2001-12-03? ? ? 1 70 20? ? 0
> 4? 2001-12-04? ? ? 1 NA NA? NA
> 5? 2001-12-05? ? ? 1 NA NA? NA
> 6? 2001-12-01? ? ? 3 20? 0? ? 0
> 7? 2001-12-02? ? ? 3 20? 0? ? 0
> 8? 2001-12-03? ? ? 3 30 10? ? 0
> 9? 2001-12-04? ? ? 3 30? 0? ? 1
> 10 2001-12-05? ? ? 3? 0? 5? ? 0
>
> A call to rbind() of the following form then gives you all your data in
> long form(you may wish to use some shortcuts to form the list of frames):
>
> > do.call(rbind, lapply(list(df1, df2, df3), reform))
>? ? ? ? POSIX Station Hs Hn flag
> 1? 2000-12-01? ? ? 1 30 10? ? 0
> 2? 2000-12-02? ? ? 1 40 20? ? 0
> 3? 2000-12-03? ? ? 1 50 10? ? 0
> 4? 2000-12-04? ? ? 1 NA NA? NA
> 5? 2000-12-05? ? ? 1 55? 5? ? 0
> 6? 2000-12-01? ? ? 2 20? 0? ? 0
> 7? 2000-12-02? ? ? 2 20? 0? ? 0
> 8? 2000-12-03? ? ? 2 30 10? ? 0
> 9? 2000-12-04? ? ? 2 30? 0? ? 1
> 10 2000-12-05? ? ? 2? 0? 5? ? 0
> 11 2001-12-01? ? ? 1 50 20? ? 0
> 12 2001-12-02? ? ? 1 60 20? ? 0
> 13 2001-12-03? ? ? 1 70 20? ? 0
> 14 2001-12-04? ? ? 1 NA NA? NA
> 15 2001-12-05? ? ? 1 NA NA? NA
> 16 2001-12-01? ? ? 3 20? 0? ? 0
> 17 2001-12-02? ? ? 3 20? 0? ? 0
> 18 2001-12-03? ? ? 3 30 10? ? 0
> 19 2001-12-04? ? ? 3 30? 0? ? 1
> 20 2001-12-05? ? ? 3? 0? 5? ? 0
> 21 2002-12-01? ? ? 2 50 20? ? 0
> 22 2002-12-02? ? ? 2 60 20? ? 0
> 23 2002-12-03? ? ? 2 70 20? ? 0
> 24 2002-12-04? ? ? 2 NA NA? NA
> 25 2002-12-05? ? ? 2 NA NA? NA
> 26 2002-12-01? ? ? 3 20? 0? ? 0
> 27 2002-12-02? ? ? 3 20? 0? ? 0
> 28 2002-12-03? ? ? 3 30 10? ? 0
> 29 2002-12-04? ? ? 3 30? 0? ? 1
> 30 2002-12-05? ? ? 3? 0? 5? ? 0
>
>
> Cheers,
> Bert Gunter
>
>
>
>
> On Wed, Jun 1, 2022 at 11:13 PM Stefano Sofia <
> stefano.sofia at regione.marche.it> wrote:
>
> > Dear R-list users,
> >
> > for each winter season from 2000 to 2022 I have a data frame collecting
> > for different weather stations snowpack height (Hs), snowfall in the last
> > 24h (Hn) and a validation flag.
> >
> > Suppose I have these three following data frames
> >
> >
> > df1 <- data.frame(data_POSIX=seq(as.POSIXct("2000-12-01",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2000-12-05",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(30, 40,
> > 50, NA, 55), Station1_Hn = c(10, 20, 10, NA, 5), Station1_flag = c(0, 0,
> 0,
> > NA, 0), Station2_Hs = c(20, 20, 30, 30, 0), Station2_Hn = c(0, 0, 10, 0,
> > 5), Station2_flag = c(0, 0, 0, 1, 0))
> >
> >
> > df2 <- data.frame(data_POSIX=seq(as.POSIXct("2001-12-01",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2001-12-05",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station1_Hs = c(50, 60,
> > 70, NA, NA), Station1_Hn = c(20, 20, 20, NA, NA), Station1_flag = c(0, 0,
> > 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10,
> > 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> >
> >
> > df3 <- data.frame(data_POSIX=seq(as.POSIXct("2002-12-01",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), as.POSIXct("2002-12-05",
> > format="%Y-%m-%d", tz="Etc/GMT-1"), by="1 days"), Station2_Hs = c(50, 60,
> > 70, NA, NA), Station2_Hn = c(20, 20, 20, NA, NA), Station2_flag = c(0, 0,
> > 0, NA, NA), Station3_Hs = c(20, 20, 30, 30, 0), Station3_Hn = c(0, 0, 10,
> > 0, 5), Station3_flag = c(0, 0, 0, 1, 0))
> >
> >
> > As you can see, each data frame can have different stations loaded.
> >
> > I would need to call rbind matching data frames by column name (i.e. by
> > station name), keeping in mind that the number of stations loaded in each
> > data frame may differ. The result should be
> >
> > data_POSIX Station1_Hs Station1_Hn Station1_flag Station2_Hs Station2_Hn
> > Station2_flag Station3_Hs Station3_Hn Station3_flag
> > 2000-12-01 30 10 0 20 0 0 NA NA NA
> > 2000-12-02 40 20 0 20 0 0 NA NA NA
> > 2000-12-03 50 10 0 30 10 0 NA NA NA
> > 2000-12-04 NA NA NA 30 0 0 NA NA NA
> > 2000-12-05 55 5 0 0 5 0 NA NA NA
> > 2001-12-01 50 20 0 NA NA NA 20 0 0
> > 2001-12-02 60 20 0 NA NA NA 20 0 0
> > 2001-12-03 70 20 0 NA NA NA 30 10 0
> > 2001-12-04 NA NA NA NA NA NA 30 0 1
> > 2001-12-05 NA NA NA NA NA NA 0 5 0
> > 2002-12-01 NA NA NA 50 20 0 20 0 0
> > 2002-12-02 NA NA NA 60 20 0 20 0 0
> > 2002-12-03 NA NA NA 70 20 0 30 10 0
> > 2002-12-04 NA NA NA NA NA NA 30 0 1
> > 2002-12-05 NA NA NA NA NA NA 0 5 0
> >
> > I tried this code
> >
> > df_list <- list(df1, df2, df3)
> > allNms <- unique(unlist(lapply(df_list, names)))
> > do.call(rbind, c(lapply(df_list, function(x) data.frame(c(x,
> > sapply(setdiff(allNms, names(x)), function(y) NA)))),
> make.row.names=FALSE))
> >
> > but I get this error:
> > Error in (function (..., row.names = NULL, check.rows = FALSE,
> check.names
> > = TRUE,? :
> >? arguments imply differing number of rows
> >
> > Could someone please help me?
> >
> >
> > Thank you for your attention
> >
> > Stefano
> >
> >
> >? ? ? ? ? (oo)
> > --oOO--( )--OOo--------------------------------------
> > Stefano Sofia PhD
> > Civil Protection - Marche Region - Italy
> > Meteo Section
> > Snow Section
> > Via del Colle Ameno 5
> > 60126 Torrette di Ancona, Ancona (AN)
> > Uff: +39 071 806 7743
> > E-mail: stefano.sofia at regione.marche.it
> > ---Oo---------oO----------------------------------------
> >
> > ________________________________
> >
> > AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
> > informazioni confidenziali, pertanto ? destinato solo a persone
> autorizzate
> > alla ricezione. I messaggi di posta elettronica per i client di Regione
> > Marche possono contenere informazioni confidenziali e con privilegi
> legali.
> > Se non si ? il destinatario specificato, non leggere, copiare, inoltrare
> o
> > archiviare questo messaggio. Se si ? ricevuto questo messaggio per
> errore,
> > inoltrarlo al mittente ed eliminarlo completamente dal sistema del
> proprio
> > computer. Ai sensi dell'art. 6 della DGR n. 1394/2008 si segnala che, in
> > caso di necessit? ed urgenza, la risposta al presente messaggio di posta
> > elettronica pu? essere visionata da persone estranee al destinatario.
> > IMPORTANT NOTICE: This e-mail message is intended to be received only by
> > persons entitled to receive the confidential information it may contain.
> > E-mail messages to clients of Regione Marche may contain information that
> > is confidential and legally privileged. Please do not read, copy,
> forward,
> > or store this message unless you are an intended recipient of it. If you
> > have received this message in error, please forward it to the sender and
> > delete it completely from your computer system.
> >
> > --
> > Questo messaggio? stato analizzato da Libraesva ESG ed? risultato non
> > infetto.
> > This message was scanned by Libraesva ESG and is believed to be clean.
> >
> >
> >? ? ? ? [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

??? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From djnord|und @end|ng |rom gm@||@com  Sat Jun  4 09:31:29 2022
From: djnord|und @end|ng |rom gm@||@com (Daniel Nordlund)
Date: Sat, 4 Jun 2022 00:31:29 -0700
Subject: [R] bootstrap CI of the difference between 2 Cramer's V
In-Reply-To: <2109899925.6516974.1653762082722@mail.yahoo.com>
References: <2109899925.6516974.1653762082722.ref@mail.yahoo.com>
 <2109899925.6516974.1653762082722@mail.yahoo.com>
Message-ID: <a5972539-71a0-864e-4843-5144698ac60b@gmail.com>

On 5/28/2022 11:21 AM, varin sacha via R-help wrote:
> Dear R-experts,
>
> While comparing groups, it is better to assess confidence intervals of those differences rather than comparing confidence intervals for each group.
> I am trying to calculate the CIs of the difference between the two Cramer's V and not the CI to the estimate of each group?s Cramer's V.
>
> Here below my toy R example. There are error messages. Any help would be highly appreciated.
>
> ##############################
> library(questionr)
> library(boot)
>
> gender1<-c("M","F","F","F","M","M","F","F","F","M","M","F","M","M","F","M","M","F","M","F","F","F","M","M","M","F","F","M","M","M","F","M","F","F","F","M","M","F","M","F")
> color1<-c("blue","green","black","black","green","green","blue","blue","green","black","blue","green","blue","black","black","blue","green","blue","green","black","blue","blue","black","black","green","green","blue","green","black","green","blue","black","black","blue","green","green","green","blue","blue","black")
>
> gender2<-c("F","F","F","M","M","F","M","M","M","F","F","M","F","M","F","F","M","M","M","F","M","M","M","F","F","F","M","M","M","F","M","M","M","F","F","F","M","F","F","F")
> color2<-c("green","blue","black","blue","blue","blue","green","blue","green","black","blue","black","blue","blue","black","blue","blue","green","blue","black","blue","blue","black","black","green","blue","black","green","blue","green","black","blue","black","blue","green","blue","green","green","blue","black")
>
> f1=data.frame(gender1,color1)
> tab1<-table(gender1,color1)
> e1<-cramer.v(tab1)
>
> f2=data.frame(gender2,color2)
> tab2<-table(gender2,color2)
> e2<-cramer.v(tab2)
>
> f3<-data.frame(e1-e2)
>
> cramerdiff=function(x,w){
> y<-tapply(x[w,1], x[w,2],cramer.v)
> y[1]-y[2]
> }
>
> results<-boot(data=f3,statistic=cramerdiff,R=2000)
> results
>
> boot.ci(results,type="all")
> ##############################
>
>   
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

I don't know if someone responded offline, but if not, there are a 
couple of problems with your code. ? First, the f3 dataframe is not what 
you think it is.? Second, your cramerdiff function isn't going to 
produce the results that you want.

I would put your data into a single dataframe with a variable 
designating which group data came from.? Then use that variable as the 
strata variable in the boot function to resample within groups.? So 
something like this:

f1 <- data.frame(gender=gender1,color=color1,group='grp1')
f2 <- data.frame(gender=gender2,color=color2,group='grp2')
f3 <- rbind(f1,f2)

cramerdiff <- function(x, ndx) {
 ?? # calculate cramer.v for group 1 bootstrap sample
 ?? g1 <-x[ndx,][x[,3]=='grp1',]
 ?? cramer_g1 <- cramer.v(table(g1[,1:2]))
 ?? # calculate cramer.v for group 2 bootstrap sample
 ?? g2 <-x[ndx,][x[,3]=='grp2',]
 ?? cramer_g2 <- cramer.v(table(g2[,1:2]))
 ?? # calculate difference
 ?? cramer_g1-cramer_g2
 ?? }
# use strata parameter in function boot to resample within each group
results <- boot(data=f3,statistic=cramerdiff, 
strata=as.factor(f3$group),R=2000)
results
boot.ci(results)


Hope this is helpful,

Dan

-- 
Daniel Nordlund
Port Townsend, WA  USA


-- 
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus


From tebert @end|ng |rom u||@edu  Sat Jun  4 13:11:42 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Sat, 4 Jun 2022 11:11:42 +0000
Subject: [R] bootstrap CI of the difference between 2 Cramer's V
In-Reply-To: <a5972539-71a0-864e-4843-5144698ac60b@gmail.com>
References: <2109899925.6516974.1653762082722.ref@mail.yahoo.com>
 <2109899925.6516974.1653762082722@mail.yahoo.com>
 <a5972539-71a0-864e-4843-5144698ac60b@gmail.com>
Message-ID: <BN6PR2201MB15530226780B90DD66CFD955CFA09@BN6PR2201MB1553.namprd22.prod.outlook.com>

I would calculate the difference and the CI about that difference. You would not get the same thing by comparing the bootstrap CI of the group means.
One use for this is to determine if the confidence interval for the difference in means includes zero. An alternative would be to use a more conventional test (rather than calculate a difference) and then find a mean p-value and a confidence interval about the p-value. This gives a better assessment of the p-value but is harder to decide if the test outcome is "significant." 

You might also consider whether you want a permutation test, a randomization test, or a bootstrap. A permutation test will look at all possible combinations of the data once. Use this approach when computationally reasonable. A randomization test will look at a random subset of all possible combinations, but may include repeats of some combinations. Both of these do not replace values. The bootstrap replaces values and will therefore tend to minimize the effects of outliers in the data. With small datasets a risk is that there are few permutations and performing a randomization test with 1,000,000 randomizations on data with 4000 permutations is not good.

Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Daniel Nordlund
Sent: Saturday, June 4, 2022 3:31 AM
To: varin sacha <varinsacha at yahoo.fr>; r-help at r-project.org
Subject: Re: [R] bootstrap CI of the difference between 2 Cramer's V

[External Email]

On 5/28/2022 11:21 AM, varin sacha via R-help wrote:
> Dear R-experts,
>
> While comparing groups, it is better to assess confidence intervals of those differences rather than comparing confidence intervals for each group.
> I am trying to calculate the CIs of the difference between the two Cramer's V and not the CI to the estimate of each group?s Cramer's V.
>
> Here below my toy R example. There are error messages. Any help would be highly appreciated.
>
> ##############################
> library(questionr)
> library(boot)
>
> gender1<-c("M","F","F","F","M","M","F","F","F","M","M","F","M","M","F"
> ,"M","M","F","M","F","F","F","M","M","M","F","F","M","M","M","F","M","
> F","F","F","M","M","F","M","F")
> color1<-c("blue","green","black","black","green","green","blue","blue"
> ,"green","black","blue","green","blue","black","black","blue","green",
> "blue","green","black","blue","blue","black","black","green","green","
> blue","green","black","green","blue","black","black","blue","green","g
> reen","green","blue","blue","black")
>
> gender2<-c("F","F","F","M","M","F","M","M","M","F","F","M","F","M","F"
> ,"F","M","M","M","F","M","M","M","F","F","F","M","M","M","F","M","M","
> M","F","F","F","M","F","F","F")
> color2<-c("green","blue","black","blue","blue","blue","green","blue","
> green","black","blue","black","blue","blue","black","blue","blue","gre
> en","blue","black","blue","blue","black","black","green","blue","black
> ","green","blue","green","black","blue","black","blue","green","blue",
> "green","green","blue","black")
>
> f1=data.frame(gender1,color1)
> tab1<-table(gender1,color1)
> e1<-cramer.v(tab1)
>
> f2=data.frame(gender2,color2)
> tab2<-table(gender2,color2)
> e2<-cramer.v(tab2)
>
> f3<-data.frame(e1-e2)
>
> cramerdiff=function(x,w){
> y<-tapply(x[w,1], x[w,2],cramer.v)
> y[1]-y[2]
> }
>
> results<-boot(data=f3,statistic=cramerdiff,R=2000)
> results
>
> boot.ci(results,type="all")
> ##############################
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=9NrsizTQzUnuqLRvbQaINvkX7iBIqmQgfbus-vohqP_KZnrkn_b1iH1ma
> wVqPzLz&s=a-dJNz_c6kgMANbI7VZ9N96pRhcKodeukMsVJ0Ol2qc&e=
> PLEASE do read the posting guide 
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=9NrsizTQzUnuqLRvbQaINvkX7iBIqmQgfbus-vohqP_KZnrkn_b1iH1m
> awVqPzLz&s=iLlqKhwHsqxsBYuq5S1ooeyH3sepv85k8fhSi27sOG8&e=
> and provide commented, minimal, self-contained, reproducible code.

I don't know if someone responded offline, but if not, there are a
couple of problems with your code.   First, the f3 dataframe is not what
you think it is.  Second, your cramerdiff function isn't going to produce the results that you want.

I would put your data into a single dataframe with a variable designating which group data came from.  Then use that variable as the strata variable in the boot function to resample within groups.  So something like this:

f1 <- data.frame(gender=gender1,color=color1,group='grp1')
f2 <- data.frame(gender=gender2,color=color2,group='grp2')
f3 <- rbind(f1,f2)

cramerdiff <- function(x, ndx) {
    # calculate cramer.v for group 1 bootstrap sample
    g1 <-x[ndx,][x[,3]=='grp1',]
    cramer_g1 <- cramer.v(table(g1[,1:2]))
    # calculate cramer.v for group 2 bootstrap sample
    g2 <-x[ndx,][x[,3]=='grp2',]
    cramer_g2 <- cramer.v(table(g2[,1:2]))
    # calculate difference
    cramer_g1-cramer_g2
    }
# use strata parameter in function boot to resample within each group results <- boot(data=f3,statistic=cramerdiff,
strata=as.factor(f3$group),R=2000)
results
boot.ci(results)


Hope this is helpful,

Dan

--
Daniel Nordlund
Port Townsend, WA  USA


--
This email has been checked for viruses by Avast antivirus software.
https://urldefense.proofpoint.com/v2/url?u=https-3A__www.avast.com_antivirus&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=9NrsizTQzUnuqLRvbQaINvkX7iBIqmQgfbus-vohqP_KZnrkn_b1iH1mawVqPzLz&s=hO-ovpt1HbZ1YM4mIaOCGdPXuVtxnWfAk8ro5PgkZvw&e=

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=9NrsizTQzUnuqLRvbQaINvkX7iBIqmQgfbus-vohqP_KZnrkn_b1iH1mawVqPzLz&s=a-dJNz_c6kgMANbI7VZ9N96pRhcKodeukMsVJ0Ol2qc&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=9NrsizTQzUnuqLRvbQaINvkX7iBIqmQgfbus-vohqP_KZnrkn_b1iH1mawVqPzLz&s=iLlqKhwHsqxsBYuq5S1ooeyH3sepv85k8fhSi27sOG8&e=
and provide commented, minimal, self-contained, reproducible code.

From bh@@k@r@ko|k@t@ @end|ng |rom gm@||@com  Sat Jun  4 23:19:28 2022
From: bh@@k@r@ko|k@t@ @end|ng |rom gm@||@com (Bhaskar Mitra)
Date: Sat, 4 Jun 2022 14:19:28 -0700
Subject: [R] 
 Request for some help: Error when joining multiple csv files
 together
In-Reply-To: <f040297a-71b6-dadd-8e34-b1a523d105e9@sapo.pt>
References: <CAEGXkYV-F6sRgYaq=57ic-iXF52r-4GyLWW4485f+Rzz3wq_KA@mail.gmail.com>
 <f040297a-71b6-dadd-8e34-b1a523d105e9@sapo.pt>
Message-ID: <CAEGXkYVWcGsFMOdNBuKne2oVDCO-6j17R6cCcdtyVUeq_f0rbw@mail.gmail.com>

Thanks Rui.
The first approach suggested by you worked perfectly.

best,
bhaskar

On Thu, Jun 2, 2022 at 12:30 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> I'm seeing two obvious errors, those are not csv files and the columns
> spec is wrong, you have a spec of 6 columns but the posted data only has
> 4 and of different classes.
>
> If the data is in comma separated values (CSV) files the following
> worked without errors.
>
>
> library(readr)
>
> col_spec <- cols(
>    d1 = col_double(),
>    d2 = col_double(),
>    d3 = col_double(),
>    d4 = col_double()
> )
>
> list.files(pattern = "*\\.csv$") |>
>    lapply(read_csv, col_types = col_spec) |>
>    dplyr::bind_rows()
>
>
> If the data is like in the question, try instead
>
>
> list.files(pattern = "*\\.csv$") |>
>    lapply(read_delim, delim = " ", col_types = col_spec) |>
>    dplyr::bind_rows()
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 00:17 de 02/06/2022, Bhaskar Mitra escreveu:
> > Hello Everyone,
> >
> > I have a bunch of csv files, all with common headers (d1, d2, d3, d4).
> > When I am trying to join the csv files together, with the following code
> > (shown below),
> > I am getting a warning  when the files are joined. The values under
> columns
> > d3 and d4 are not joined properly.
> >
> > The code and the error are given below. I would really appreciate some
> help
> > in this regard.
> >
> > regards,
> > bhaskar
> >
> >
> > #---code--------------------------------
> >
> > df <- list.files(pattern = "*.csv") %>%
> >    lapply(read_csv) %>%
> >    bind_rows
> >
> > #---code--------------------------------
> >
> > Header of each file:
> >
> > d1  d2  d3  d4
> > 3   4    NA   NA
> > 4   5   NA   7
> > 5   6   8   8
> > 6   7   8   NA
> >
> > #--------------------------------------------------------
> >
> > #Warning when the codes run --------------------------------------
> >
> > Column specification
> >
> ?????????????????????????????????????????????????????????????????????????????????????????????????????????????????
> > cols(
> >    .default = col_double(),
> >    name = col_character(),
> >    d1 = col_datetime(format = ""),
> >    d2 = col_character(),
> >    d3 = col_logical(),
> >    d4 = col_logical()
> > )
> >
> > Warning: 48766 parsing failures.
> > * row*    *col*          * expected  *                         *actual*
> >       *file*
> > 3529   d3           1/0/T/F/TRUE/FALSE       100             'file1.csv'
> > 3529   d4            1/0/T/F/TRUE/FALSE      100             'file1.csv'
> >
> > .... ..... .................. ...... ................................
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Sun Jun  5 18:21:08 2022
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Sun, 5 Jun 2022 16:21:08 +0000 (UTC)
Subject: [R] bootstrap CI of the difference between 2 Cramer's V
In-Reply-To: <a5972539-71a0-864e-4843-5144698ac60b@gmail.com>
References: <2109899925.6516974.1653762082722.ref@mail.yahoo.com>
 <2109899925.6516974.1653762082722@mail.yahoo.com>
 <a5972539-71a0-864e-4843-5144698ac60b@gmail.com>
Message-ID: <317595679.11661946.1654446068116@mail.yahoo.com>

Dear Daniel,
Dear R-experts,

I really thank you a lot Daniel. Nobody had answered to me offline. So, thanks.
I have tried in the same vein for the Goodman-Kruskal gamma for ordinal data. There is an error message at the end of the code. Thanks for your help.


##############################
library(ryouready)
library(boot)

shopping1<-c("tr?s important","important","pas important","pas important","important","tr?s important","important","pas important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","important")

statut1<-c("riche","pas riche","moyennement riche","moyennement riche","riche","pas riche","moyennement riche","moyennement riche","riche","pas riche","moyennement riche","riche","pas riche","pas riche","riche","moyennement riche","riche","pas riche","pas riche","pas riche","riche","riche","moyennement riche","riche","riche","moyennement riche","moyennement riche","moyennement riche","pas riche","pas riche","riche","pas riche","riche","pas riche","riche","moyennement riche","riche","pas riche","moyennement riche","riche")

shopping2<-c("important","pas important","tr?s important","tr?s important","important","tr?s important","pas important","important","pas important","tr?s important","important","important","important","important","pas important","tr?s important","tr?s important","important","pas important","tr?s important","pas important","tr?s important","pas important","tr?s important","important","tr?s important","important","pas important","pas important","important","pas important","tr?s important","pas important","pas important","important","important","tr?s important","tr?s important","pas important","pas important")

statut2<-c("moyennement riche","pas riche","riche","moyennement riche","moyennement riche","moyennement riche","pas riche","riche","riche","pas riche","moyennement riche","riche","riche","riche","riche","riche","pas riche","moyennement riche","moyennement riche","pas riche","moyennement riche","pas riche","pas riche","pas riche","moyennement riche","riche","moyennement riche","riche","pas riche","riche","moyennement riche","blue","moyennement riche","pas riche","pas riche","riche","riche","pas riche","pas riche","pas riche")

f1 <- data.frame(shopping=shopping1,statut=statut1,group='grp1')
f2 <- data.frame(shopping=shopping2,statut=statut2,group='grp2')
f3 <- rbind(f1,f2)

G <- function(x, index) {
?? 
# calculate goodman for group 1 bootstrap sample
?? g1 <-x[index,][x[,3]=='grp1',]
?? goodman_g1 <- cor(data[index,][1,2])
??
?# calculate goodman for group 2 bootstrap sample
?? g2 <-x[index,][x[,3]=='grp2',]
?? goodman_g2 <- cor(data[index,][3,4])
??
?# calculate difference
?? goodman_g1-goodman_g2
?? }
?

# use strata parameter in function boot to resample within each group
results <- boot(data=f3,statistic=G, strata=as.factor(f3$group),R=2000)

results
boot.ci(results)
##############################



Le samedi 4 juin 2022 ? 09:31:36 UTC+2, Daniel Nordlund <djnordlund at gmail.com> a ?crit : 





On 5/28/2022 11:21 AM, varin sacha via R-help wrote:
> Dear R-experts,
>
> While comparing groups, it is better to assess confidence intervals of those differences rather than comparing confidence intervals for each group.
> I am trying to calculate the CIs of the difference between the two Cramer's V and not the CI to the estimate of each group?s Cramer's V.
>
> Here below my toy R example. There are error messages. Any help would be highly appreciated.
>
> ##############################
> library(questionr)
> library(boot)
>
> gender1<-c("M","F","F","F","M","M","F","F","F","M","M","F","M","M","F","M","M","F","M","F","F","F","M","M","M","F","F","M","M","M","F","M","F","F","F","M","M","F","M","F")
> color1<-c("blue","green","black","black","green","green","blue","blue","green","black","blue","green","blue","black","black","blue","green","blue","green","black","blue","blue","black","black","green","green","blue","green","black","green","blue","black","black","blue","green","green","green","blue","blue","black")
>
> gender2<-c("F","F","F","M","M","F","M","M","M","F","F","M","F","M","F","F","M","M","M","F","M","M","M","F","F","F","M","M","M","F","M","M","M","F","F","F","M","F","F","F")
> color2<-c("green","blue","black","blue","blue","blue","green","blue","green","black","blue","black","blue","blue","black","blue","blue","green","blue","black","blue","blue","black","black","green","blue","black","green","blue","green","black","blue","black","blue","green","blue","green","green","blue","black")
>
> f1=data.frame(gender1,color1)
> tab1<-table(gender1,color1)
> e1<-cramer.v(tab1)
>
> f2=data.frame(gender2,color2)
> tab2<-table(gender2,color2)
> e2<-cramer.v(tab2)
>
> f3<-data.frame(e1-e2)
>
> cramerdiff=function(x,w){
> y<-tapply(x[w,1], x[w,2],cramer.v)
> y[1]-y[2]
> }
>
> results<-boot(data=f3,statistic=cramerdiff,R=2000)
> results
>
> boot.ci(results,type="all")
> ##############################
>
>? 
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

I don't know if someone responded offline, but if not, there are a 
couple of problems with your code. ? First, the f3 dataframe is not what 
you think it is.? Second, your cramerdiff function isn't going to 
produce the results that you want.

I would put your data into a single dataframe with a variable 
designating which group data came from.? Then use that variable as the
strata variable in the boot function to resample within groups.? So 
something like this:

f1 <- data.frame(gender=gender1,color=color1,group='grp1')
f2 <- data.frame(gender=gender2,color=color2,group='grp2')
f3 <- rbind(f1,f2)

cramerdiff <- function(x, ndx) {
?? # calculate cramer.v for group 1 bootstrap sample
?? g1 <-x[ndx,][x[,3]=='grp1',]
?? cramer_g1 <- cramer.v(table(g1[,1:2]))
?? # calculate cramer.v for group 2 bootstrap sample
?? g2 <-x[ndx,][x[,3]=='grp2',]
?? cramer_g2 <- cramer.v(table(g2[,1:2]))
?? # calculate difference
?? cramer_g1-cramer_g2
?? }
# use strata parameter in function boot to resample within each group
results <- boot(data=f3,statistic=cramerdiff, 
strata=as.factor(f3$group),R=2000)

results
boot.ci(results)



Hope this is helpful,

Dan

-- 
Daniel Nordlund
Port Townsend, WA? USA


-- 
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus


From j@de@shod@@ m@iii@g oii googiem@ii@com  Sun Jun  5 21:01:38 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Sun, 5 Jun 2022 20:01:38 +0100
Subject: [R] High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
Message-ID: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>

Hello everyone,

A few days ago I asked a question about concurvity in a GAM (the
anologue of collinearity in a GLM) implemented in mgcv. I think my
question was a bit unfocussed, so I am retrying again, but with
additional information included about the autocorrelation function. I
have also posted about this on Cross Validated. Given all the model
output, it might make for easier
reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea

As mentioned previously, I have problems with concurvity in my thesis
research, and don't have access to a statistician who works with time
series, GAMs or R. I'd be very grateful for any (partial) answer,
however short. I'll gladly return the favour where I can! For really
helpful input I'd be more than happy to offer co-authorship on
publication. Deadlines are very close, and I'm heading towards having
no results at all if I can't solve this concurvity issue :(

I'm using GAMs to try to understand the relationship between deaths
and heat-related variables (e.g. temperature and humidity), using
daily time series over a 14-year period from a tropical, low-income
country. My aim is to understand the relationship between these
variables and deaths, rather than pure prediction performance.

The GAMs include distributed lag models (set up as 7-column matrices,
see code at bottom of post), since deaths may occur over several days
following exposure.

Simple GAMs with just time, lagged temperature and lagged
precipitation (a potential confounder) show very high concurvity
between lagged temperature and time, regardless of the many different
ways I have tried to decompose time. The autocorrelation functions
(ACF) however, shows values close to zero, only just about breaching
the 'significance line' in a few instances. It does show patterning
though, although the regularity is difficult to define.

My questions are:
1) Should I be worried about the high concurvity, or can I ignore it
given the mostly non-significant ACF? I've read dozens of
heat-mortality modelling studies and none report on concurvity between
weather variables and time (though one 2012 paper discussed
autocorrelation).

2) If I cannot ignore it, what should I do to resolve it? Would
including an autoregressive term be appropriate, and if so, where can
I find a coded example of how to do this? I've also come across
sequential regression][1]. Would this be more or less appropriate? If
appropriate, a pointer to an example would be really appreciated!

Some example GAMs are specified as follows:
```r
conc38b <- gam(deaths~te(year, month, week, weekday,
bs=c("cr","cc","cc","cc")) + heap +
                      te(temp_max, lag, k=c(10, 3)) +
                      te(precip_daily_total, lag, k=c(10, 3)),
                      data = dat, family = nb, method = 'REML', select = TRUE,
                      knots = list(month = c(0.5, 12.5), week = c(0.5,
52.5), weekday = c(0, 6.5)))
```
Concurvity for the above model between (temp_max, lag) and (year,
month, week, weekday) is 0.91:

```r
$worst
                                    para te(year,month,week,weekday)
te(temp_max,lag) te(precip_daily_total,lag)
para                        1.000000e+00                1.125625e-29
     0.3150073                  0.6666348
te(year,month,week,weekday) 1.400648e-29                1.000000e+00
     0.9060552                  0.6652313
te(temp_max,lag)            3.152795e-01                8.998113e-01
     1.0000000                  0.5781015
te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
     0.5805159                  1.0000000
```

Output from ```gam.check()```:
```r
Method: REML   Optimizer: outer newton
full convergence after 16 iterations.
Gradient range [-0.01467332,0.003096643]
(score 8915.994 & scale 1).
Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
Model rank =  544 / 544

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

                                  k'      edf k-index p-value
te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
te(temp_max,lag)             29.0000   3.3681      NA      NA
te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
```

Some output from ```summary(conc38b)```:
```r
Approximate significance of smooth terms:
                                  edf Ref.df  Chi.sq p-value
te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
te(temp_max,lag)             3.368129     27  11.130 0.00145 **
te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

R-sq.(adj) =  0.839   Deviance explained = 53.3%
-REML =   8916  Scale est. = 1         n = 5107
```


Below are the ACF plots (note limit y-axis = 0.1 for clarity of
pattern). They show peaks at 5 and 15, and then there seems to be a
recurring pattern at multiples of approx. 30 (suggesting month is not
modelled adequately?). Not sure what would cause the spikes at 5 and
15. There is heaping of deaths on the 15th day of each month, to which
deaths with unknown date were allocated. This heaping was modelled
with categorical variable/ factor ```heap``` with 169 levels (0 for
all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
heaping day over the 14-year period):

  [2]: https://i.stack.imgur.com/FzKyM.png
  [3]: https://i.stack.imgur.com/fE3aL.png


I get an identical looking ACF when I decompose time into (year,
month, monthday) as in model conc39 below, although concurvity between
(temp_max, lag) and the time term has now dropped somewhat to 0.83:

```r
conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
                     te(temp_max, lag, k=c(10, 4)) +
                     te(precip_daily_total, lag, k=c(10, 4)),
                     data = dat, family = nb, method = 'REML', select = TRUE,
                     knots = list(month = c(0.5, 12.5)))
```
```r

Method: REML   Optimizer: outer newton
full convergence after 14 iterations.
Gradient range [-0.001578187,6.155096e-05]
(score 8915.763 & scale 1).
Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
Model rank =  323 / 323

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

                                k'     edf k-index p-value
te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
te(temp_max,lag)           39.0000  4.0875      NA      NA
te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
```
Some output from ```summary(conc39)```:
```r
Approximate significance of smooth terms:
                                edf Ref.df  Chi.sq  p-value
te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
te(precip_daily_total,lag)  0.01173     36   0.008    0.557
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

R-sq.(adj) =  0.839   Deviance explained = 53.8%
-REML =   8915  Scale est. = 1         n = 5107
```


```r
$worst
                                   para te(year,month,monthday)
te(temp_max,lag) te(precip_daily_total,lag)
para                       1.000000e+00            3.261007e-31
0.3313549                  0.6666532
te(year,month,monthday)    3.060763e-31            1.000000e+00
0.8266086                  0.5670777
te(temp_max,lag)           3.331014e-01            8.225942e-01
1.0000000                  0.5840875
te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
0.5939380                  1.0000000
```

Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
various choices for k or as ```s(time)``` with various k does not
reduce concurvity either.


The default approach in time series studies of heat-mortality is to
model time with fixed df, generally between 7-10 df per year of data.
I am, however, apprehensive about this approach because a) mortality
profiles vary with locality due to sociodemographic and environmental
characteristics and b) the choice of df is based on higher income
countries (where nearly all these studies have been done) with
different mortality profiles and so may not be appropriate for
tropical, low-income countries.

Although the approach of fixing (high) df does remove more temporal
patterns from the ACF (see model and output below), concurvity between
time and lagged temperature has now risen to 0.99! Moreover,
temperature (which has been a consistent, highly significant predictor
in every model of the tens (hundreds?) I have run, has now turned
non-significant. I am guessing this is because time is now a very
wiggly function that not only models/ removes seasonal variation, but
also some of the day-to-day variation that is needed for the
temperature smooth  :

```r
conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
                      te(temp_max, lag, k=c(10,3)) +
                      te(precip_daily_total, lag, k=c(10,3)),
                      data = dat, family = nb, method = 'REML', select = TRUE)
```
Output from ```gam.check(conc20a, rep = 1000)```:

```r
Method: REML   Optimizer: outer newton
full convergence after 9 iterations.
Gradient range [-0.0008983099,9.546022e-05]
(score 8750.13 & scale 1).
Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
Model rank =  336 / 336

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

                                 k'      edf k-index p-value
s(time)                    111.0000 111.0000    0.98    0.56
te(temp_max,lag)            29.0000   0.6548      NA      NA
te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
```
Output from ```concurvity(conc20a, full=FALSE)$worst```:

```r
                                   para      s(time) te(temp_max,lag)
te(precip_daily_total,lag)
para                       1.000000e+00 2.462064e-19        0.3165236
                0.6666348
s(time)                    2.462398e-19 1.000000e+00        0.9930674
                0.6879284
te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
                0.5788711
te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
                1.0000000

```

Some output from ```summary(conc20a)```:
```r
Approximate significance of smooth terms:
                                 edf Ref.df  Chi.sq p-value
s(time)                    1.110e+02    111 419.375  <2e-16 ***
te(temp_max,lag)           6.548e-01     27   0.895   0.249
te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

R-sq.(adj) =  0.843   Deviance explained = 56.1%
-REML = 8750.1  Scale est. = 1         n = 5107
```

ACF functions:

[4]: https://i.stack.imgur.com/7nbXS.png
[5]: https://i.stack.imgur.com/pNnZU.png

Data can be found on my [GitHub][6] site in the file
[data_cross_validated_post2.rds][7]. A csv version is also available.
This is my code:

```r
library(readr)
library(mgcv)

df <- read_rds("data_crossvalidated_post2.rds")

# Create matrices for lagged weather variables (6 day lags) based on
example by Simon Wood
# in his 2017 book ("Generalized additive models: an introduction with
R", p. 349) and
# gamair package documentation
(https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)

lagard <- function(x,n.lag=7) {
n <- length(x); X <- matrix(NA,n,n.lag)
for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
X
}

dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
weekday = df$weekday, week = df$week, monthday = df$monthday, time =
df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
= df$precip_hourly_dailysum)
dat$temp_max <- lagard(df$temp_max)
dat$temp_min <- lagard(df$temp_min)
dat$temp_mean <- lagard(df$temp_mean)
dat$wbgt_max <- lagard(df$wbgt_max)
dat$wbgt_mean <- lagard(df$wbgt_mean)
dat$wbgt_min <- lagard(df$wbgt_min)
dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
dat$sh_mean <- lagard(df$sh_mean)
dat$solar_mean <- lagard(df$solar_mean)
dat$wind2m_mean <- lagard(df$wind2m_mean)
dat$sh_max <- lagard(df$sh_max)
dat$solar_max <- lagard(df$solar_max)
dat$wind2m_max <- lagard(df$wind2m_max)
dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
dat$precip_hourly <- lagard(df$precip_hourly)
dat$precip_daily_total <- lagard( df$precip_daily_total)
dat$temp <- lagard(df$temp)
dat$sh <- lagard(df$sh)
dat$rh <- lagard(df$rh)
dat$solar <- lagard(df$solar)
dat$wind2m <- lagard(df$wind2m)


conc38b <- gam(deaths~te(year, month, week, weekday,
bs=c("cr","cc","cc","cc")) + heap +
                      te(temp_max, lag, k=c(10, 3)) +
                      te(precip_daily_total, lag, k=c(10, 3)),
                      data = dat, family = nb, method = 'REML', select = TRUE,
                      knots = list(month = c(0.5, 12.5), week = c(0.5,
52.5), weekday = c(0, 6.5)))

conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
                     te(temp_max, lag, k=c(10, 4)) +
                     te(precip_daily_total, lag, k=c(10, 4)),
                     data = dat, family = nb, method = 'REML', select = TRUE,
                     knots = list(month = c(0.5, 12.5)))

conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
                      te(temp_max, lag, k=c(10,3)) +
                      te(precip_daily_total, lag, k=c(10,3)),
                      data = dat, family = nb, method = 'REML', select = TRUE)

```
Thank you if you've read this far!! :-))

  [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
  [2]: https://i.stack.imgur.com/FzKyM.png
  [3]: https://i.stack.imgur.com/fE3aL.png
  [4]: https://i.stack.imgur.com/7nbXS.png
  [5]: https://i.stack.imgur.com/pNnZU.png
  [6]: https://github.com/JadeShodan/heat-mortality
  [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds


From djnord|und @end|ng |rom gm@||@com  Mon Jun  6 02:40:59 2022
From: djnord|und @end|ng |rom gm@||@com (Daniel Nordlund)
Date: Sun, 5 Jun 2022 17:40:59 -0700
Subject: [R] bootstrap CI of the difference between 2 Cramer's V
In-Reply-To: <317595679.11661946.1654446068116@mail.yahoo.com>
References: <2109899925.6516974.1653762082722.ref@mail.yahoo.com>
 <2109899925.6516974.1653762082722@mail.yahoo.com>
 <a5972539-71a0-864e-4843-5144698ac60b@gmail.com>
 <317595679.11661946.1654446068116@mail.yahoo.com>
Message-ID: <9bc0d7b2-04fc-8e7a-72bd-2eaaadd91ccf@gmail.com>

There are a few problems with the "rewrite" of the code, both 
syntactically and conceptually.
1. Goodman-Kruskal gamma is for ordinal data.? You should create your 
"shopping" and "statut" variables as factors, ordered from lowest to 
highest using the levels= parameter in? the function, factor
2. In your function, G, you use "data[index,][1,2]"? where you should 
have used either "g1[,c(1,2)]", or "g2[,c(1,2)]".? You should read up on 
Indexing using [] on data frames, to make sure you understand what the 
original code was doing.
3.? The base cor function does not calculate a Goodman-Kruskal gamma 
(unless somebody has written a new version).? So you need to find an 
appropriate function and you may need to structure your data differently 
for calculating gamma, depending on what parameters the function 
demands.? Google is your friend here, search for??? "R Goodman Kruskal 
gamma"

Since this is looking like homework to me, I suggest you ask your 
instructor about some of this.

Best of luck,

Dan


On 6/5/2022 9:21 AM, varin sacha wrote:
> Dear Daniel,
> Dear R-experts,
>
> I really thank you a lot Daniel. Nobody had answered to me offline. So, thanks.
> I have tried in the same vein for the Goodman-Kruskal gamma for ordinal data. There is an error message at the end of the code. Thanks for your help.
>
>
> ##############################
> library(ryouready)
> library(boot)
>
> shopping1<-c("tr?s important","important","pas important","pas important","important","tr?s important","important","pas important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","important")
>
> statut1<-c("riche","pas riche","moyennement riche","moyennement riche","riche","pas riche","moyennement riche","moyennement riche","riche","pas riche","moyennement riche","riche","pas riche","pas riche","riche","moyennement riche","riche","pas riche","pas riche","pas riche","riche","riche","moyennement riche","riche","riche","moyennement riche","moyennement riche","moyennement riche","pas riche","pas riche","riche","pas riche","riche","pas riche","riche","moyennement riche","riche","pas riche","moyennement riche","riche")
>
> shopping2<-c("important","pas important","tr?s important","tr?s important","important","tr?s important","pas important","important","pas important","tr?s important","important","important","important","important","pas important","tr?s important","tr?s important","important","pas important","tr?s important","pas important","tr?s important","pas important","tr?s important","important","tr?s important","important","pas important","pas important","important","pas important","tr?s important","pas important","pas important","important","important","tr?s important","tr?s important","pas important","pas important")
>
> statut2<-c("moyennement riche","pas riche","riche","moyennement riche","moyennement riche","moyennement riche","pas riche","riche","riche","pas riche","moyennement riche","riche","riche","riche","riche","riche","pas riche","moyennement riche","moyennement riche","pas riche","moyennement riche","pas riche","pas riche","pas riche","moyennement riche","riche","moyennement riche","riche","pas riche","riche","moyennement riche","blue","moyennement riche","pas riche","pas riche","riche","riche","pas riche","pas riche","pas riche")
>
> f1 <- data.frame(shopping=shopping1,statut=statut1,group='grp1')
> f2 <- data.frame(shopping=shopping2,statut=statut2,group='grp2')
> f3 <- rbind(f1,f2)
>
> G <- function(x, index) {
>     
> # calculate goodman for group 1 bootstrap sample
>  ?? g1 <-x[index,][x[,3]=='grp1',]
>  ?? goodman_g1 <- cor(data[index,][1,2])
>    
>  ?# calculate goodman for group 2 bootstrap sample
>  ?? g2 <-x[index,][x[,3]=='grp2',]
>  ?? goodman_g2 <- cor(data[index,][3,4])
>    
>  ?# calculate difference
>  ?? goodman_g1-goodman_g2
>  ?? }
>   
>
> # use strata parameter in function boot to resample within each group
> results <- boot(data=f3,statistic=G, strata=as.factor(f3$group),R=2000)
>
> results
> boot.ci(results)
> ##############################
>
>
>
> Le samedi 4 juin 2022 ? 09:31:36 UTC+2, Daniel Nordlund <djnordlund at gmail.com> a ?crit :
>
>
>
>
>
> On 5/28/2022 11:21 AM, varin sacha via R-help wrote:
>> Dear R-experts,
>>
>> While comparing groups, it is better to assess confidence intervals of those differences rather than comparing confidence intervals for each group.
>> I am trying to calculate the CIs of the difference between the two Cramer's V and not the CI to the estimate of each group?s Cramer's V.
>>
>> Here below my toy R example. There are error messages. Any help would be highly appreciated.
>>
>> ##############################
>> library(questionr)
>> library(boot)
>>
>> gender1<-c("M","F","F","F","M","M","F","F","F","M","M","F","M","M","F","M","M","F","M","F","F","F","M","M","M","F","F","M","M","M","F","M","F","F","F","M","M","F","M","F")
>> color1<-c("blue","green","black","black","green","green","blue","blue","green","black","blue","green","blue","black","black","blue","green","blue","green","black","blue","blue","black","black","green","green","blue","green","black","green","blue","black","black","blue","green","green","green","blue","blue","black")
>>
>> gender2<-c("F","F","F","M","M","F","M","M","M","F","F","M","F","M","F","F","M","M","M","F","M","M","M","F","F","F","M","M","M","F","M","M","M","F","F","F","M","F","F","F")
>> color2<-c("green","blue","black","blue","blue","blue","green","blue","green","black","blue","black","blue","blue","black","blue","blue","green","blue","black","blue","blue","black","black","green","blue","black","green","blue","green","black","blue","black","blue","green","blue","green","green","blue","black")
>>
>> f1=data.frame(gender1,color1)
>> tab1<-table(gender1,color1)
>> e1<-cramer.v(tab1)
>>
>> f2=data.frame(gender2,color2)
>> tab2<-table(gender2,color2)
>> e2<-cramer.v(tab2)
>>
>> f3<-data.frame(e1-e2)
>>
>> cramerdiff=function(x,w){
>> y<-tapply(x[w,1], x[w,2],cramer.v)
>> y[1]-y[2]
>> }
>>
>> results<-boot(data=f3,statistic=cramerdiff,R=2000)
>> results
>>
>> boot.ci(results,type="all")
>> ##############################
>>
>>    
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> I don't know if someone responded offline, but if not, there are a
> couple of problems with your code. ? First, the f3 dataframe is not what
> you think it is.? Second, your cramerdiff function isn't going to
> produce the results that you want.
>
> I would put your data into a single dataframe with a variable
> designating which group data came from.? Then use that variable as the
> strata variable in the boot function to resample within groups.? So
> something like this:
>
> f1 <- data.frame(gender=gender1,color=color1,group='grp1')
> f2 <- data.frame(gender=gender2,color=color2,group='grp2')
> f3 <- rbind(f1,f2)
>
> cramerdiff <- function(x, ndx) {
>  ?? # calculate cramer.v for group 1 bootstrap sample
>  ?? g1 <-x[ndx,][x[,3]=='grp1',]
>  ?? cramer_g1 <- cramer.v(table(g1[,1:2]))
>  ?? # calculate cramer.v for group 2 bootstrap sample
>  ?? g2 <-x[ndx,][x[,3]=='grp2',]
>  ?? cramer_g2 <- cramer.v(table(g2[,1:2]))
>  ?? # calculate difference
>  ?? cramer_g1-cramer_g2
>  ?? }
> # use strata parameter in function boot to resample within each group
> results <- boot(data=f3,statistic=cramerdiff,
> strata=as.factor(f3$group),R=2000)
>
> results
> boot.ci(results)
>
>
>
> Hope this is helpful,
>
> Dan
>

-- 
Daniel Nordlund
Port Townsend, WA  USA


-- 
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus


From tebert @end|ng |rom u||@edu  Mon Jun  6 02:54:35 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Mon, 6 Jun 2022 00:54:35 +0000
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
Message-ID: <BN6PR2201MB1553A755FBD07E5B894D28B4CFA29@BN6PR2201MB1553.namprd22.prod.outlook.com>

You are welcome to ask here. However, you should try contacting the authors of the gam package. Package authors are often extraordinarily helpful.
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of jade.shodan--- via R-help
Sent: Sunday, June 5, 2022 3:02 PM
To: r-help at r-project.org
Subject: [R] High concurvity/ collinearity between time and temperature in GAM predicting deaths but low ACF. Does this matter?

[External Email]

Hello everyone,

A few days ago I asked a question about concurvity in a GAM (the anologue of collinearity in a GLM) implemented in mgcv. I think my question was a bit unfocussed, so I am retrying again, but with additional information included about the autocorrelation function. I have also posted about this on Cross Validated. Given all the model output, it might make for easier reading:https://urldefense.proofpoint.com/v2/url?u=https-3A__stats.stackexchange.com_questions_577790_high-2Dconcurvity-2Dcollinearity-2Dbetween-2Dtime-2Dand-2Dtemperature-2Din-2Dgam-2Dpredicting-2Ddea&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=UnxUj1p0u7yNcAwVR0Na6FmRBibHxhuhscEcpFA2qRQ&e=

As mentioned previously, I have problems with concurvity in my thesis research, and don't have access to a statistician who works with time series, GAMs or R. I'd be very grateful for any (partial) answer, however short. I'll gladly return the favour where I can! For really helpful input I'd be more than happy to offer co-authorship on publication. Deadlines are very close, and I'm heading towards having no results at all if I can't solve this concurvity issue :(

I'm using GAMs to try to understand the relationship between deaths and heat-related variables (e.g. temperature and humidity), using daily time series over a 14-year period from a tropical, low-income country. My aim is to understand the relationship between these variables and deaths, rather than pure prediction performance.

The GAMs include distributed lag models (set up as 7-column matrices, see code at bottom of post), since deaths may occur over several days following exposure.

Simple GAMs with just time, lagged temperature and lagged precipitation (a potential confounder) show very high concurvity between lagged temperature and time, regardless of the many different ways I have tried to decompose time. The autocorrelation functions
(ACF) however, shows values close to zero, only just about breaching the 'significance line' in a few instances. It does show patterning though, although the regularity is difficult to define.

My questions are:
1) Should I be worried about the high concurvity, or can I ignore it given the mostly non-significant ACF? I've read dozens of heat-mortality modelling studies and none report on concurvity between weather variables and time (though one 2012 paper discussed autocorrelation).

2) If I cannot ignore it, what should I do to resolve it? Would including an autoregressive term be appropriate, and if so, where can I find a coded example of how to do this? I've also come across sequential regression][1]. Would this be more or less appropriate? If appropriate, a pointer to an example would be really appreciated!

Some example GAMs are specified as follows:
```r
conc38b <- gam(deaths~te(year, month, week, weekday,
bs=c("cr","cc","cc","cc")) + heap +
                      te(temp_max, lag, k=c(10, 3)) +
                      te(precip_daily_total, lag, k=c(10, 3)),
                      data = dat, family = nb, method = 'REML', select = TRUE,
                      knots = list(month = c(0.5, 12.5), week = c(0.5, 52.5), weekday = c(0, 6.5))) ``` Concurvity for the above model between (temp_max, lag) and (year, month, week, weekday) is 0.91:

```r
$worst
                                    para te(year,month,week,weekday)
te(temp_max,lag) te(precip_daily_total,lag)
para                        1.000000e+00                1.125625e-29
     0.3150073                  0.6666348
te(year,month,week,weekday) 1.400648e-29                1.000000e+00
     0.9060552                  0.6652313
te(temp_max,lag)            3.152795e-01                8.998113e-01
     1.0000000                  0.5781015
te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
     0.5805159                  1.0000000
```

Output from ```gam.check()```:
```r
Method: REML   Optimizer: outer newton
full convergence after 16 iterations.
Gradient range [-0.01467332,0.003096643] (score 8915.994 & scale 1).
Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
Model rank =  544 / 544

Basis dimension (k) checking results. Low p-value (k-index<1) may indicate that k is too low, especially if edf is close to k'.

                                  k'      edf k-index p-value
te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
te(temp_max,lag)             29.0000   3.3681      NA      NA
te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 ```

Some output from ```summary(conc38b)```:
```r
Approximate significance of smooth terms:
                                  edf Ref.df  Chi.sq p-value
te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
te(temp_max,lag)             3.368129     27  11.130 0.00145 **
te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

R-sq.(adj) =  0.839   Deviance explained = 53.3%
-REML =   8916  Scale est. = 1         n = 5107
```


Below are the ACF plots (note limit y-axis = 0.1 for clarity of pattern). They show peaks at 5 and 15, and then there seems to be a recurring pattern at multiples of approx. 30 (suggesting month is not modelled adequately?). Not sure what would cause the spikes at 5 and 15. There is heaping of deaths on the 15th day of each month, to which deaths with unknown date were allocated. This heaping was modelled with categorical variable/ factor ```heap``` with 169 levels (0 for all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent heaping day over the 14-year period):

  [2]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_FzKyM.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=rhd6ZkNNDyYd1zntgAjnNzZFkYPFica9xzx9ruBHG9g&e=
  [3]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_fE3aL.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=DUqm7oXz2zc3oaDR6ESbWGKZdinIsZf-ULGgDsyIOfM&e=


I get an identical looking ACF when I decompose time into (year, month, monthday) as in model conc39 below, although concurvity between (temp_max, lag) and the time term has now dropped somewhat to 0.83:

```r
conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
                     te(temp_max, lag, k=c(10, 4)) +
                     te(precip_daily_total, lag, k=c(10, 4)),
                     data = dat, family = nb, method = 'REML', select = TRUE,
                     knots = list(month = c(0.5, 12.5))) ``` ```r

Method: REML   Optimizer: outer newton
full convergence after 14 iterations.
Gradient range [-0.001578187,6.155096e-05] (score 8915.763 & scale 1).
Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
Model rank =  323 / 323

Basis dimension (k) checking results. Low p-value (k-index<1) may indicate that k is too low, especially if edf is close to k'.

                                k'     edf k-index p-value
te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
te(temp_max,lag)           39.0000  4.0875      NA      NA
te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 ``` Some output from ```summary(conc39)```:
```r
Approximate significance of smooth terms:
                                edf Ref.df  Chi.sq  p-value
te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
te(precip_daily_total,lag)  0.01173     36   0.008    0.557
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

R-sq.(adj) =  0.839   Deviance explained = 53.8%
-REML =   8915  Scale est. = 1         n = 5107
```


```r
$worst
                                   para te(year,month,monthday)
te(temp_max,lag) te(precip_daily_total,lag)
para                       1.000000e+00            3.261007e-31
0.3313549                  0.6666532
te(year,month,monthday)    3.060763e-31            1.000000e+00
0.8266086                  0.5670777
te(temp_max,lag)           3.331014e-01            8.225942e-01
1.0000000                  0.5840875
te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
0.5939380                  1.0000000
```

Modelling time as ```te(year, doy)``` with a cyclic spline for doy and various choices for k or as ```s(time)``` with various k does not reduce concurvity either.


The default approach in time series studies of heat-mortality is to model time with fixed df, generally between 7-10 df per year of data.
I am, however, apprehensive about this approach because a) mortality profiles vary with locality due to sociodemographic and environmental characteristics and b) the choice of df is based on higher income countries (where nearly all these studies have been done) with different mortality profiles and so may not be appropriate for tropical, low-income countries.

Although the approach of fixing (high) df does remove more temporal patterns from the ACF (see model and output below), concurvity between time and lagged temperature has now risen to 0.99! Moreover, temperature (which has been a consistent, highly significant predictor in every model of the tens (hundreds?) I have run, has now turned non-significant. I am guessing this is because time is now a very wiggly function that not only models/ removes seasonal variation, but also some of the day-to-day variation that is needed for the temperature smooth  :

```r
conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
                      te(temp_max, lag, k=c(10,3)) +
                      te(precip_daily_total, lag, k=c(10,3)),
                      data = dat, family = nb, method = 'REML', select = TRUE) ``` Output from ```gam.check(conc20a, rep = 1000)```:

```r
Method: REML   Optimizer: outer newton
full convergence after 9 iterations.
Gradient range [-0.0008983099,9.546022e-05] (score 8750.13 & scale 1).
Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
Model rank =  336 / 336

Basis dimension (k) checking results. Low p-value (k-index<1) may indicate that k is too low, especially if edf is close to k'.

                                 k'      edf k-index p-value
s(time)                    111.0000 111.0000    0.98    0.56
te(temp_max,lag)            29.0000   0.6548      NA      NA
te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
```
Output from ```concurvity(conc20a, full=FALSE)$worst```:

```r
                                   para      s(time) te(temp_max,lag)
te(precip_daily_total,lag)
para                       1.000000e+00 2.462064e-19        0.3165236
                0.6666348
s(time)                    2.462398e-19 1.000000e+00        0.9930674
                0.6879284
te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
                0.5788711
te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
                1.0000000

```

Some output from ```summary(conc20a)```:
```r
Approximate significance of smooth terms:
                                 edf Ref.df  Chi.sq p-value
s(time)                    1.110e+02    111 419.375  <2e-16 ***
te(temp_max,lag)           6.548e-01     27   0.895   0.249
te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

R-sq.(adj) =  0.843   Deviance explained = 56.1%
-REML = 8750.1  Scale est. = 1         n = 5107
```

ACF functions:

[4]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_7nbXS.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=hXhWH-VySi9i27hNDKK184WiLooYmhdni7_7JOLhRcI&e=
[5]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_pNnZU.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=HV6sMbzNkG-NdJZBjZRAjrvCDl2VtMPfl5rY2Ss8muM&e=

Data can be found on my [GitHub][6] site in the file [data_cross_validated_post2.rds][7]. A csv version is also available.
This is my code:

```r
library(readr)
library(mgcv)

df <- read_rds("data_crossvalidated_post2.rds")

# Create matrices for lagged weather variables (6 day lags) based on example by Simon Wood # in his 2017 book ("Generalized additive models: an introduction with R", p. 349) and # gamair package documentation (https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dproject.org_web_packages_gamair_gamair.pdf&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=RmLMXgEhg76PLgzNq7CbZkD29EGQili4Pkd7ESctrJ0&e= , p. 54)

lagard <- function(x,n.lag=7) {
n <- length(x); X <- matrix(NA,n,n.lag)
for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1] X }

dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month, weekday = df$weekday, week = df$week, monthday = df$monthday, time = df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum = df$precip_hourly_dailysum) dat$temp_max <- lagard(df$temp_max) dat$temp_min <- lagard(df$temp_min) dat$temp_mean <- lagard(df$temp_mean) dat$wbgt_max <- lagard(df$wbgt_max) dat$wbgt_mean <- lagard(df$wbgt_mean) dat$wbgt_min <- lagard(df$wbgt_min) dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max) dat$sh_mean <- lagard(df$sh_mean) dat$solar_mean <- lagard(df$solar_mean) dat$wind2m_mean <- lagard(df$wind2m_mean) dat$sh_max <- lagard(df$sh_max) dat$solar_max <- lagard(df$solar_max) dat$wind2m_max <- lagard(df$wind2m_max) dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean) dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum) dat$precip_hourly <- lagard(df$precip_hourly) dat$precip_daily_total <- lagard( df$precip_daily_total) dat$temp <- lagard(df$temp) dat$sh <- lagard(df$sh) dat$rh <- lagard(df$rh) dat$solar <- lagard(df$solar) dat$wind2m <- lagard(df$wind2m)


conc38b <- gam(deaths~te(year, month, week, weekday,
bs=c("cr","cc","cc","cc")) + heap +
                      te(temp_max, lag, k=c(10, 3)) +
                      te(precip_daily_total, lag, k=c(10, 3)),
                      data = dat, family = nb, method = 'REML', select = TRUE,
                      knots = list(month = c(0.5, 12.5), week = c(0.5, 52.5), weekday = c(0, 6.5)))

conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
                     te(temp_max, lag, k=c(10, 4)) +
                     te(precip_daily_total, lag, k=c(10, 4)),
                     data = dat, family = nb, method = 'REML', select = TRUE,
                     knots = list(month = c(0.5, 12.5)))

conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
                      te(temp_max, lag, k=c(10,3)) +
                      te(precip_daily_total, lag, k=c(10,3)),
                      data = dat, family = nb, method = 'REML', select = TRUE)

```
Thank you if you've read this far!! :-))

  [1]: https://urldefense.proofpoint.com/v2/url?u=https-3A__scholar.google.co.uk_scholar-3Foutput-3Dinstlink-26q-3Dinfo-3APKdjq7ZwozEJ-3Ascholar.google.com_-26hl-3Den-26as-5Fsdt-3D0-2C5-26scillfp-3D17865929886710916120-26oi-3Dlle&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=05YTFJr01J0QkoraKsVufRrfVvevvADTCCCjxskRbfY&e=
  [2]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_FzKyM.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=rhd6ZkNNDyYd1zntgAjnNzZFkYPFica9xzx9ruBHG9g&e=
  [3]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_fE3aL.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=DUqm7oXz2zc3oaDR6ESbWGKZdinIsZf-ULGgDsyIOfM&e=
  [4]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_7nbXS.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=hXhWH-VySi9i27hNDKK184WiLooYmhdni7_7JOLhRcI&e=
  [5]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_pNnZU.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=HV6sMbzNkG-NdJZBjZRAjrvCDl2VtMPfl5rY2Ss8muM&e=
  [6]: https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_JadeShodan_heat-2Dmortality&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=zxKWkmFT-DWpRAx6t0DRbV9ldSbgyIE6V3LdJBm4ULU&e=
  [7]: https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_JadeShodan_heat-2Dmortality_blob_main_data-5Fcross-5Fvalidated-5Fpost2.rds&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=VIm9gkrPHFVXJFZeg6sMnKYGcLD5HR3BKqh4z8iIQjc&e=

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=QP0xRvlj8tppy7hmTxhzrD62Hd1N4mXYmPKW48XiiRg&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=M7FO2Q09m7Or14GXa16BtzX1E8Xa5AWB-eN9ZTGlcjA&e=
and provide commented, minimal, self-contained, reproducible code.

From j@de@shod@@ m@iii@g oii googiem@ii@com  Mon Jun  6 11:40:39 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Mon, 6 Jun 2022 10:40:39 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <BN6PR2201MB1553A755FBD07E5B894D28B4CFA29@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <BN6PR2201MB1553A755FBD07E5B894D28B4CFA29@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <CANg3_k9POxE3n982OjiUy9gVh7MbvKXFWWOiJVRQaUxta-EKdA@mail.gmail.com>

Hi Tim,

Thanks for the suggestion. I think the author of the mgcv package
(Simon Wood) is/ was (?) on this list, so I thought I'd give it a try
here first. Plus I already emailed him recently about a different
issue, so I didn't want to come on too strongly by emailing him again
so shortly after. But I will try that if no luck here. Thanks!

On Mon, 6 Jun 2022 at 01:54, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>
> You are welcome to ask here. However, you should try contacting the authors of the gam package. Package authors are often extraordinarily helpful.
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of jade.shodan--- via R-help
> Sent: Sunday, June 5, 2022 3:02 PM
> To: r-help at r-project.org
> Subject: [R] High concurvity/ collinearity between time and temperature in GAM predicting deaths but low ACF. Does this matter?
>
> [External Email]
>
> Hello everyone,
>
> A few days ago I asked a question about concurvity in a GAM (the anologue of collinearity in a GLM) implemented in mgcv. I think my question was a bit unfocussed, so I am retrying again, but with additional information included about the autocorrelation function. I have also posted about this on Cross Validated. Given all the model output, it might make for easier reading:https://urldefense.proofpoint.com/v2/url?u=https-3A__stats.stackexchange.com_questions_577790_high-2Dconcurvity-2Dcollinearity-2Dbetween-2Dtime-2Dand-2Dtemperature-2Din-2Dgam-2Dpredicting-2Ddea&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=UnxUj1p0u7yNcAwVR0Na6FmRBibHxhuhscEcpFA2qRQ&e=
>
> As mentioned previously, I have problems with concurvity in my thesis research, and don't have access to a statistician who works with time series, GAMs or R. I'd be very grateful for any (partial) answer, however short. I'll gladly return the favour where I can! For really helpful input I'd be more than happy to offer co-authorship on publication. Deadlines are very close, and I'm heading towards having no results at all if I can't solve this concurvity issue :(
>
> I'm using GAMs to try to understand the relationship between deaths and heat-related variables (e.g. temperature and humidity), using daily time series over a 14-year period from a tropical, low-income country. My aim is to understand the relationship between these variables and deaths, rather than pure prediction performance.
>
> The GAMs include distributed lag models (set up as 7-column matrices, see code at bottom of post), since deaths may occur over several days following exposure.
>
> Simple GAMs with just time, lagged temperature and lagged precipitation (a potential confounder) show very high concurvity between lagged temperature and time, regardless of the many different ways I have tried to decompose time. The autocorrelation functions
> (ACF) however, shows values close to zero, only just about breaching the 'significance line' in a few instances. It does show patterning though, although the regularity is difficult to define.
>
> My questions are:
> 1) Should I be worried about the high concurvity, or can I ignore it given the mostly non-significant ACF? I've read dozens of heat-mortality modelling studies and none report on concurvity between weather variables and time (though one 2012 paper discussed autocorrelation).
>
> 2) If I cannot ignore it, what should I do to resolve it? Would including an autoregressive term be appropriate, and if so, where can I find a coded example of how to do this? I've also come across sequential regression][1]. Would this be more or less appropriate? If appropriate, a pointer to an example would be really appreciated!
>
> Some example GAMs are specified as follows:
> ```r
> conc38b <- gam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc")) + heap +
>                       te(temp_max, lag, k=c(10, 3)) +
>                       te(precip_daily_total, lag, k=c(10, 3)),
>                       data = dat, family = nb, method = 'REML', select = TRUE,
>                       knots = list(month = c(0.5, 12.5), week = c(0.5, 52.5), weekday = c(0, 6.5))) ``` Concurvity for the above model between (temp_max, lag) and (year, month, week, weekday) is 0.91:
>
> ```r
> $worst
>                                     para te(year,month,week,weekday)
> te(temp_max,lag) te(precip_daily_total,lag)
> para                        1.000000e+00                1.125625e-29
>      0.3150073                  0.6666348
> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
>      0.9060552                  0.6652313
> te(temp_max,lag)            3.152795e-01                8.998113e-01
>      1.0000000                  0.5781015
> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
>      0.5805159                  1.0000000
> ```
>
> Output from ```gam.check()```:
> ```r
> Method: REML   Optimizer: outer newton
> full convergence after 16 iterations.
> Gradient range [-0.01467332,0.003096643] (score 8915.994 & scale 1).
> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> Model rank =  544 / 544
>
> Basis dimension (k) checking results. Low p-value (k-index<1) may indicate that k is too low, especially if edf is close to k'.
>
>                                   k'      edf k-index p-value
> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> te(temp_max,lag)             29.0000   3.3681      NA      NA
> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 ```
>
> Some output from ```summary(conc38b)```:
> ```r
> Approximate significance of smooth terms:
>                                   edf Ref.df  Chi.sq p-value
> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> -REML =   8916  Scale est. = 1         n = 5107
> ```
>
>
> Below are the ACF plots (note limit y-axis = 0.1 for clarity of pattern). They show peaks at 5 and 15, and then there seems to be a recurring pattern at multiples of approx. 30 (suggesting month is not modelled adequately?). Not sure what would cause the spikes at 5 and 15. There is heaping of deaths on the 15th day of each month, to which deaths with unknown date were allocated. This heaping was modelled with categorical variable/ factor ```heap``` with 169 levels (0 for all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent heaping day over the 14-year period):
>
>   [2]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_FzKyM.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=rhd6ZkNNDyYd1zntgAjnNzZFkYPFica9xzx9ruBHG9g&e=
>   [3]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_fE3aL.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=DUqm7oXz2zc3oaDR6ESbWGKZdinIsZf-ULGgDsyIOfM&e=
>
>
> I get an identical looking ACF when I decompose time into (year, month, monthday) as in model conc39 below, although concurvity between (temp_max, lag) and the time term has now dropped somewhat to 0.83:
>
> ```r
> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>                      te(temp_max, lag, k=c(10, 4)) +
>                      te(precip_daily_total, lag, k=c(10, 4)),
>                      data = dat, family = nb, method = 'REML', select = TRUE,
>                      knots = list(month = c(0.5, 12.5))) ``` ```r
>
> Method: REML   Optimizer: outer newton
> full convergence after 14 iterations.
> Gradient range [-0.001578187,6.155096e-05] (score 8915.763 & scale 1).
> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> Model rank =  323 / 323
>
> Basis dimension (k) checking results. Low p-value (k-index<1) may indicate that k is too low, especially if edf is close to k'.
>
>                                 k'     edf k-index p-value
> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> te(temp_max,lag)           39.0000  4.0875      NA      NA
> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 ``` Some output from ```summary(conc39)```:
> ```r
> Approximate significance of smooth terms:
>                                 edf Ref.df  Chi.sq  p-value
> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> -REML =   8915  Scale est. = 1         n = 5107
> ```
>
>
> ```r
> $worst
>                                    para te(year,month,monthday)
> te(temp_max,lag) te(precip_daily_total,lag)
> para                       1.000000e+00            3.261007e-31
> 0.3313549                  0.6666532
> te(year,month,monthday)    3.060763e-31            1.000000e+00
> 0.8266086                  0.5670777
> te(temp_max,lag)           3.331014e-01            8.225942e-01
> 1.0000000                  0.5840875
> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> 0.5939380                  1.0000000
> ```
>
> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and various choices for k or as ```s(time)``` with various k does not reduce concurvity either.
>
>
> The default approach in time series studies of heat-mortality is to model time with fixed df, generally between 7-10 df per year of data.
> I am, however, apprehensive about this approach because a) mortality profiles vary with locality due to sociodemographic and environmental characteristics and b) the choice of df is based on higher income countries (where nearly all these studies have been done) with different mortality profiles and so may not be appropriate for tropical, low-income countries.
>
> Although the approach of fixing (high) df does remove more temporal patterns from the ACF (see model and output below), concurvity between time and lagged temperature has now risen to 0.99! Moreover, temperature (which has been a consistent, highly significant predictor in every model of the tens (hundreds?) I have run, has now turned non-significant. I am guessing this is because time is now a very wiggly function that not only models/ removes seasonal variation, but also some of the day-to-day variation that is needed for the temperature smooth  :
>
> ```r
> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>                       te(temp_max, lag, k=c(10,3)) +
>                       te(precip_daily_total, lag, k=c(10,3)),
>                       data = dat, family = nb, method = 'REML', select = TRUE) ``` Output from ```gam.check(conc20a, rep = 1000)```:
>
> ```r
> Method: REML   Optimizer: outer newton
> full convergence after 9 iterations.
> Gradient range [-0.0008983099,9.546022e-05] (score 8750.13 & scale 1).
> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> Model rank =  336 / 336
>
> Basis dimension (k) checking results. Low p-value (k-index<1) may indicate that k is too low, especially if edf is close to k'.
>
>                                  k'      edf k-index p-value
> s(time)                    111.0000 111.0000    0.98    0.56
> te(temp_max,lag)            29.0000   0.6548      NA      NA
> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> ```
> Output from ```concurvity(conc20a, full=FALSE)$worst```:
>
> ```r
>                                    para      s(time) te(temp_max,lag)
> te(precip_daily_total,lag)
> para                       1.000000e+00 2.462064e-19        0.3165236
>                 0.6666348
> s(time)                    2.462398e-19 1.000000e+00        0.9930674
>                 0.6879284
> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
>                 0.5788711
> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
>                 1.0000000
>
> ```
>
> Some output from ```summary(conc20a)```:
> ```r
> Approximate significance of smooth terms:
>                                  edf Ref.df  Chi.sq p-value
> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> -REML = 8750.1  Scale est. = 1         n = 5107
> ```
>
> ACF functions:
>
> [4]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_7nbXS.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=hXhWH-VySi9i27hNDKK184WiLooYmhdni7_7JOLhRcI&e=
> [5]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_pNnZU.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=HV6sMbzNkG-NdJZBjZRAjrvCDl2VtMPfl5rY2Ss8muM&e=
>
> Data can be found on my [GitHub][6] site in the file [data_cross_validated_post2.rds][7]. A csv version is also available.
> This is my code:
>
> ```r
> library(readr)
> library(mgcv)
>
> df <- read_rds("data_crossvalidated_post2.rds")
>
> # Create matrices for lagged weather variables (6 day lags) based on example by Simon Wood # in his 2017 book ("Generalized additive models: an introduction with R", p. 349) and # gamair package documentation (https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dproject.org_web_packages_gamair_gamair.pdf&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=RmLMXgEhg76PLgzNq7CbZkD29EGQili4Pkd7ESctrJ0&e= , p. 54)
>
> lagard <- function(x,n.lag=7) {
> n <- length(x); X <- matrix(NA,n,n.lag)
> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1] X }
>
> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month, weekday = df$weekday, week = df$week, monthday = df$monthday, time = df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum = df$precip_hourly_dailysum) dat$temp_max <- lagard(df$temp_max) dat$temp_min <- lagard(df$temp_min) dat$temp_mean <- lagard(df$temp_mean) dat$wbgt_max <- lagard(df$wbgt_max) dat$wbgt_mean <- lagard(df$wbgt_mean) dat$wbgt_min <- lagard(df$wbgt_min) dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max) dat$sh_mean <- lagard(df$sh_mean) dat$solar_mean <- lagard(df$solar_mean) dat$wind2m_mean <- lagard(df$wind2m_mean) dat$sh_max <- lagard(df$sh_max) dat$solar_max <- lagard(df$solar_max) dat$wind2m_max <- lagard(df$wind2m_max) dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean) dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum) dat$precip_hourly <- lagard(df$precip_hourly) dat$precip_daily_total <- lagard( df$precip_daily_total) dat$temp <- lagard(df$temp) dat$sh <- lagard(df$sh) dat$rh <- lagard(df$rh) dat$solar <- lagard(df$solar) dat$wind2m <- lagard(df$wind2m)
>
>
> conc38b <- gam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc")) + heap +
>                       te(temp_max, lag, k=c(10, 3)) +
>                       te(precip_daily_total, lag, k=c(10, 3)),
>                       data = dat, family = nb, method = 'REML', select = TRUE,
>                       knots = list(month = c(0.5, 12.5), week = c(0.5, 52.5), weekday = c(0, 6.5)))
>
> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>                      te(temp_max, lag, k=c(10, 4)) +
>                      te(precip_daily_total, lag, k=c(10, 4)),
>                      data = dat, family = nb, method = 'REML', select = TRUE,
>                      knots = list(month = c(0.5, 12.5)))
>
> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>                       te(temp_max, lag, k=c(10,3)) +
>                       te(precip_daily_total, lag, k=c(10,3)),
>                       data = dat, family = nb, method = 'REML', select = TRUE)
>
> ```
> Thank you if you've read this far!! :-))
>
>   [1]: https://urldefense.proofpoint.com/v2/url?u=https-3A__scholar.google.co.uk_scholar-3Foutput-3Dinstlink-26q-3Dinfo-3APKdjq7ZwozEJ-3Ascholar.google.com_-26hl-3Den-26as-5Fsdt-3D0-2C5-26scillfp-3D17865929886710916120-26oi-3Dlle&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=05YTFJr01J0QkoraKsVufRrfVvevvADTCCCjxskRbfY&e=
>   [2]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_FzKyM.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=rhd6ZkNNDyYd1zntgAjnNzZFkYPFica9xzx9ruBHG9g&e=
>   [3]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_fE3aL.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=DUqm7oXz2zc3oaDR6ESbWGKZdinIsZf-ULGgDsyIOfM&e=
>   [4]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_7nbXS.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=hXhWH-VySi9i27hNDKK184WiLooYmhdni7_7JOLhRcI&e=
>   [5]: https://urldefense.proofpoint.com/v2/url?u=https-3A__i.stack.imgur.com_pNnZU.png&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=HV6sMbzNkG-NdJZBjZRAjrvCDl2VtMPfl5rY2Ss8muM&e=
>   [6]: https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_JadeShodan_heat-2Dmortality&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=zxKWkmFT-DWpRAx6t0DRbV9ldSbgyIE6V3LdJBm4ULU&e=
>   [7]: https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_JadeShodan_heat-2Dmortality_blob_main_data-5Fcross-5Fvalidated-5Fpost2.rds&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=VIm9gkrPHFVXJFZeg6sMnKYGcLD5HR3BKqh4z8iIQjc&e=
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=QP0xRvlj8tppy7hmTxhzrD62Hd1N4mXYmPKW48XiiRg&e=
> PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=OcAaOvxwBr8hQWb7DaBa1490imjSyKQGzC2TbT3r6Xq3uAna_UgeAo-SNr5SwaYD&s=M7FO2Q09m7Or14GXa16BtzX1E8Xa5AWB-eN9ZTGlcjA&e=
> and provide commented, minimal, self-contained, reproducible code.


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Sun Jun  5 11:52:20 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Sun, 5 Jun 2022 14:52:20 +0500
Subject: [R] A humble request
Message-ID: <CAMfKi3KBxpQTdP=EX6vF5hcfM76MbhbdR_D6G-8Ly3myve1vOw@mail.gmail.com>

*Hi, Dear Respected Professors! I hope that you are doing well. *
Kindly help me regarding the following:
When I run the following code in R
d1=as.data.frame(mra(valq, wf = "la8", J = 8, method = "modwt", boundary =
"periodic"))
I face the following error
Error in modwt(x, wf, J, "periodic") : wavelet transform exceeds sample
size in modwt

Kindly help me to fix it.

Regards
Muhammad Zubair Chishti

	[[alternative HTML version deleted]]


From john@@t@p|e@ @end|ng |rom me@com  Mon Jun  6 15:05:29 2022
From: john@@t@p|e@ @end|ng |rom me@com (John RF Staples)
Date: Mon, 6 Jun 2022 14:05:29 +0100
Subject: [R] Build a data.table via two string lists
Message-ID: <6847A6BF-AEBA-47D0-9F78-1680A3F2BFA0@me.com>

Hello everyone,

I have the following two structures:

# str(cl)
# chr [1:5] "timestamp" "assess" "catheter" "service" "ref"

# str(rw)
# chr [1:5] "1654508112" "3" "gel" "2785" ?16545081121a8d8f956cb871053a3f56b782b70a76"

From which I would like to generate a 5 col x 1 row data.table; the question is ?how??

?cl? represents the column names; ?rw? the row values.

With anticipatory thanks ?
John

From w||||@mwdun|@p @end|ng |rom gm@||@com  Mon Jun  6 19:08:24 2022
From: w||||@mwdun|@p @end|ng |rom gm@||@com (Bill Dunlap)
Date: Mon, 6 Jun 2022 10:08:24 -0700
Subject: [R] Build a data.table via two string lists
In-Reply-To: <6847A6BF-AEBA-47D0-9F78-1680A3F2BFA0@me.com>
References: <6847A6BF-AEBA-47D0-9F78-1680A3F2BFA0@me.com>
Message-ID: <CAHqSRuQOgKDBjstV3RNhNycbYOsA9V7iJREKFFSYyPZOZ8Z3Qg@mail.gmail.com>

You could turn your row values in a list, attach names to that list, and
pass the list to data.frame():
> cl <- c("timestamp", "assess", "catheter", "service", "ref")
> rw <- c("1654508112", "3", "gel", "2785",
"16545081121a8d8f956cb871053a3f56b782b70a76")
> df <- data.frame(setNames(as.list(rw), nm=cl))
> df
   timestamp assess catheter service
 ref
1 1654508112      3      gel    2785
16545081121a8d8f956cb871053a3f56b782b70a76

At this point all the columns have type "character".  You may want to
convert some columns to more meaningful types, e.g., "timestamp" to a
date/time class and "assess" to a number.  E.g.,
> df[["timestamp"]] <- as.POSIXct(as.numeric(df[["timestamp"]]),
origin=as.POSIXct("1970/01/01"))
> df[["assess"]] <- as.numeric(df[["assess"]])
> df
            timestamp assess catheter service
         ref
1 2022-06-06 10:35:12      3      gel    2785
16545081121a8d8f956cb871053a3f56b782b70a76
> str(df)
'data.frame':   1 obs. of  5 variables:
 $ timestamp: POSIXct, format: "2022-06-06 10:35:12"
 $ assess   : num 3
 $ catheter : chr "gel"
 $ service  : chr "2785"
 $ ref      : chr "16545081121a8d8f956cb871053a3f56b782b70a76"

-Bill


On Mon, Jun 6, 2022 at 9:51 AM John RF Staples via R-help <
r-help at r-project.org> wrote:

> Hello everyone,
>
> I have the following two structures:
>
> # str(cl)
> # chr [1:5] "timestamp" "assess" "catheter" "service" "ref"
>
> # str(rw)
> # chr [1:5] "1654508112" "3" "gel" "2785"
> ?16545081121a8d8f956cb871053a3f56b782b70a76"
>
> From which I would like to generate a 5 col x 1 row data.table; the
> question is ?how??
>
> ?cl? represents the column names; ?rw? the row values.
>
> With anticipatory thanks ?
> John
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon Jun  6 21:17:21 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 6 Jun 2022 20:17:21 +0100
Subject: [R] bootstrap CI of the difference between 2 Cramer's V
In-Reply-To: <317595679.11661946.1654446068116@mail.yahoo.com>
References: <2109899925.6516974.1653762082722.ref@mail.yahoo.com>
 <2109899925.6516974.1653762082722@mail.yahoo.com>
 <a5972539-71a0-864e-4843-5144698ac60b@gmail.com>
 <317595679.11661946.1654446068116@mail.yahoo.com>
Message-ID: <0e2dd5f6-c0e8-5b72-37f4-4115794319f8@sapo.pt>

Hello,

Here is your code, corrected. It uses a Goodman-Kruskal gamma function 
in package DescTools.
Function G probably doesn't need tryCatch but I had errors in some test 
runs.


library(boot)

G <- function(x, index, cols = 1:2) {
   y <- x[index, cols]    # bootstrapped data
   g <- x$group[index]    # and groups
   # calculate gamma for each group bootstrap sample
   # (trap errors in case one of the groups is empty)
   g <- tryCatch(
     lapply(split(y[1:2], g), \(x) {
       tbl <- table(x)
       DescTools::GoodmanKruskalGamma(tbl)
     }),
     error = function(e) list(NA_real_, NA_real_)
   )
   # calculate difference
   g[[1]] - g[[2]]
}

set.seed(2022)
# use strata parameter in function boot to resample within each group
results <- boot(
   data = f3,
   statistic = G,
   R = 2000,
   strata = as.factor(f3$group),
   cols = 1:2
)

results
boot.ci(results)


Hope this helps,

Rui Barradas


?s 17:21 de 05/06/2022, varin sacha via R-help escreveu:
> Dear Daniel,
> Dear R-experts,
> 
> I really thank you a lot Daniel. Nobody had answered to me offline. So, thanks.
> I have tried in the same vein for the Goodman-Kruskal gamma for ordinal data. There is an error message at the end of the code. Thanks for your help.
> 
> 
> ##############################
> library(ryouready)
> library(boot)
> 
> shopping1<-c("tr?s important","important","pas important","pas important","important","tr?s important","important","pas important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","important")
> 
> statut1<-c("riche","pas riche","moyennement riche","moyennement riche","riche","pas riche","moyennement riche","moyennement riche","riche","pas riche","moyennement riche","riche","pas riche","pas riche","riche","moyennement riche","riche","pas riche","pas riche","pas riche","riche","riche","moyennement riche","riche","riche","moyennement riche","moyennement riche","moyennement riche","pas riche","pas riche","riche","pas riche","riche","pas riche","riche","moyennement riche","riche","pas riche","moyennement riche","riche")
> 
> shopping2<-c("important","pas important","tr?s important","tr?s important","important","tr?s important","pas important","important","pas important","tr?s important","important","important","important","important","pas important","tr?s important","tr?s important","important","pas important","tr?s important","pas important","tr?s important","pas important","tr?s important","important","tr?s important","important","pas important","pas important","important","pas important","tr?s important","pas important","pas important","important","important","tr?s important","tr?s important","pas important","pas important")
> 
> statut2<-c("moyennement riche","pas riche","riche","moyennement riche","moyennement riche","moyennement riche","pas riche","riche","riche","pas riche","moyennement riche","riche","riche","riche","riche","riche","pas riche","moyennement riche","moyennement riche","pas riche","moyennement riche","pas riche","pas riche","pas riche","moyennement riche","riche","moyennement riche","riche","pas riche","riche","moyennement riche","blue","moyennement riche","pas riche","pas riche","riche","riche","pas riche","pas riche","pas riche")
> 
> f1 <- data.frame(shopping=shopping1,statut=statut1,group='grp1')
> f2 <- data.frame(shopping=shopping2,statut=statut2,group='grp2')
> f3 <- rbind(f1,f2)
> 
> G <- function(x, index) {
>     
> # calculate goodman for group 1 bootstrap sample
>  ?? g1 <-x[index,][x[,3]=='grp1',]
>  ?? goodman_g1 <- cor(data[index,][1,2])
>    
>  ?# calculate goodman for group 2 bootstrap sample
>  ?? g2 <-x[index,][x[,3]=='grp2',]
>  ?? goodman_g2 <- cor(data[index,][3,4])
>    
>  ?# calculate difference
>  ?? goodman_g1-goodman_g2
>  ?? }
>   
> 
> # use strata parameter in function boot to resample within each group
> results <- boot(data=f3,statistic=G, strata=as.factor(f3$group),R=2000)
> 
> results
> boot.ci(results)
> ##############################
> 
> 
> 
> Le samedi 4 juin 2022 ? 09:31:36 UTC+2, Daniel Nordlund <djnordlund at gmail.com> a ?crit :
> 
> 
> 
> 
> 
> On 5/28/2022 11:21 AM, varin sacha via R-help wrote:
>> Dear R-experts,
>>
>> While comparing groups, it is better to assess confidence intervals of those differences rather than comparing confidence intervals for each group.
>> I am trying to calculate the CIs of the difference between the two Cramer's V and not the CI to the estimate of each group?s Cramer's V.
>>
>> Here below my toy R example. There are error messages. Any help would be highly appreciated.
>>
>> ##############################
>> library(questionr)
>> library(boot)
>>
>> gender1<-c("M","F","F","F","M","M","F","F","F","M","M","F","M","M","F","M","M","F","M","F","F","F","M","M","M","F","F","M","M","M","F","M","F","F","F","M","M","F","M","F")
>> color1<-c("blue","green","black","black","green","green","blue","blue","green","black","blue","green","blue","black","black","blue","green","blue","green","black","blue","blue","black","black","green","green","blue","green","black","green","blue","black","black","blue","green","green","green","blue","blue","black")
>>
>> gender2<-c("F","F","F","M","M","F","M","M","M","F","F","M","F","M","F","F","M","M","M","F","M","M","M","F","F","F","M","M","M","F","M","M","M","F","F","F","M","F","F","F")
>> color2<-c("green","blue","black","blue","blue","blue","green","blue","green","black","blue","black","blue","blue","black","blue","blue","green","blue","black","blue","blue","black","black","green","blue","black","green","blue","green","black","blue","black","blue","green","blue","green","green","blue","black")
>>
>> f1=data.frame(gender1,color1)
>> tab1<-table(gender1,color1)
>> e1<-cramer.v(tab1)
>>
>> f2=data.frame(gender2,color2)
>> tab2<-table(gender2,color2)
>> e2<-cramer.v(tab2)
>>
>> f3<-data.frame(e1-e2)
>>
>> cramerdiff=function(x,w){
>> y<-tapply(x[w,1], x[w,2],cramer.v)
>> y[1]-y[2]
>> }
>>
>> results<-boot(data=f3,statistic=cramerdiff,R=2000)
>> results
>>
>> boot.ci(results,type="all")
>> ##############################
>>
>>    
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> I don't know if someone responded offline, but if not, there are a
> couple of problems with your code. ? First, the f3 dataframe is not what
> you think it is.? Second, your cramerdiff function isn't going to
> produce the results that you want.
> 
> I would put your data into a single dataframe with a variable
> designating which group data came from.? Then use that variable as the
> strata variable in the boot function to resample within groups.? So
> something like this:
> 
> f1 <- data.frame(gender=gender1,color=color1,group='grp1')
> f2 <- data.frame(gender=gender2,color=color2,group='grp2')
> f3 <- rbind(f1,f2)
> 
> cramerdiff <- function(x, ndx) {
>  ?? # calculate cramer.v for group 1 bootstrap sample
>  ?? g1 <-x[ndx,][x[,3]=='grp1',]
>  ?? cramer_g1 <- cramer.v(table(g1[,1:2]))
>  ?? # calculate cramer.v for group 2 bootstrap sample
>  ?? g2 <-x[ndx,][x[,3]=='grp2',]
>  ?? cramer_g2 <- cramer.v(table(g2[,1:2]))
>  ?? # calculate difference
>  ?? cramer_g1-cramer_g2
>  ?? }
> # use strata parameter in function boot to resample within each group
> results <- boot(data=f3,statistic=cramerdiff,
> strata=as.factor(f3$group),R=2000)
> 
> results
> boot.ci(results)
> 
> 
> 
> Hope this is helpful,
> 
> Dan
>


From drj|m|emon @end|ng |rom gm@||@com  Mon Jun  6 23:27:00 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Tue, 7 Jun 2022 07:27:00 +1000
Subject: [R] A humble request
In-Reply-To: <CAMfKi3KBxpQTdP=EX6vF5hcfM76MbhbdR_D6G-8Ly3myve1vOw@mail.gmail.com>
References: <CAMfKi3KBxpQTdP=EX6vF5hcfM76MbhbdR_D6G-8Ly3myve1vOw@mail.gmail.com>
Message-ID: <CA+8X3fW1x77nPM9XssTYjGK8=j59tyGCmvceptpm7Xvu1W7myQ@mail.gmail.com>

Hi Muhammad,
You may be using the "mra" function in the "waveslim" package. At a
wild guess, if "valq" is a vector or time series, it is too short to
use J=8. Try smaller values for J to see if this is the problem.
Otherwise ask your teacher.

Jim

On Tue, Jun 7, 2022 at 2:50 AM Muhammad Zubair Chishti
<mzchishti at eco.qau.edu.pk> wrote:
>
> *Hi, Dear Respected Professors! I hope that you are doing well. *
> Kindly help me regarding the following:
> When I run the following code in R
> d1=as.data.frame(mra(valq, wf = "la8", J = 8, method = "modwt", boundary =
> "periodic"))
> I face the following error
> Error in modwt(x, wf, J, "periodic") : wavelet transform exceeds sample
> size in modwt
>
> Kindly help me to fix it.
>
> Regards
> Muhammad Zubair Chishti
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tebert @end|ng |rom u||@edu  Tue Jun  7 02:22:31 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 7 Jun 2022 00:22:31 +0000
Subject: [R] Build a data.table via two string lists
In-Reply-To: <CAHqSRuQOgKDBjstV3RNhNycbYOsA9V7iJREKFFSYyPZOZ8Z3Qg@mail.gmail.com>
References: <6847A6BF-AEBA-47D0-9F78-1680A3F2BFA0@me.com>
 <CAHqSRuQOgKDBjstV3RNhNycbYOsA9V7iJREKFFSYyPZOZ8Z3Qg@mail.gmail.com>
Message-ID: <BN6PR2201MB1553D19D2BDD95F3E9D8D54BCFA59@BN6PR2201MB1553.namprd22.prod.outlook.com>

Maybe I don't quite understand, but to get a 1 row data table the cl has to be column names.

colnames(df) <- cl


Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Bill Dunlap
Sent: Monday, June 6, 2022 1:08 PM
To: John RF Staples <john.staples at me.com>
Cc: r-help at R-project.org
Subject: Re: [R] Build a data.table via two string lists

[External Email]

You could turn your row values in a list, attach names to that list, and pass the list to data.frame():
> cl <- c("timestamp", "assess", "catheter", "service", "ref") rw <- 
> c("1654508112", "3", "gel", "2785",
"16545081121a8d8f956cb871053a3f56b782b70a76")
> df <- data.frame(setNames(as.list(rw), nm=cl)) df
   timestamp assess catheter service
 ref
1 1654508112      3      gel    2785
16545081121a8d8f956cb871053a3f56b782b70a76

At this point all the columns have type "character".  You may want to convert some columns to more meaningful types, e.g., "timestamp" to a date/time class and "assess" to a number.  E.g.,
> df[["timestamp"]] <- as.POSIXct(as.numeric(df[["timestamp"]]),
origin=as.POSIXct("1970/01/01"))
> df[["assess"]] <- as.numeric(df[["assess"]]) df
            timestamp assess catheter service
         ref
1 2022-06-06 10:35:12      3      gel    2785
16545081121a8d8f956cb871053a3f56b782b70a76
> str(df)
'data.frame':   1 obs. of  5 variables:
 $ timestamp: POSIXct, format: "2022-06-06 10:35:12"
 $ assess   : num 3
 $ catheter : chr "gel"
 $ service  : chr "2785"
 $ ref      : chr "16545081121a8d8f956cb871053a3f56b782b70a76"

-Bill


On Mon, Jun 6, 2022 at 9:51 AM John RF Staples via R-help < r-help at r-project.org> wrote:

> Hello everyone,
>
> I have the following two structures:
>
> # str(cl)
> # chr [1:5] "timestamp" "assess" "catheter" "service" "ref"
>
> # str(rw)
> # chr [1:5] "1654508112" "3" "gel" "2785"
> ?16545081121a8d8f956cb871053a3f56b782b70a76"
>
> From which I would like to generate a 5 col x 1 row data.table; the 
> question is ?how??
>
> ?cl? represents the column names; ?rw? the row values.
>
> With anticipatory thanks ?
> John
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=K1a_rA-Z0ef6qHvn7vTcCIiwqM3-TiLHpEkHxPn9eW2bhsutM9gsG6l7Z
> IygZrrc&s=Kv3c5xefTDI4vqOIrrz8s6kS-9Yt30ttEFpzh6uMVq8&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=K1a_rA-Z0ef6qHvn7vTcCIiwqM3-TiLHpEkHxPn9eW2bhsutM9gsG6l7
> ZIygZrrc&s=iXx9A0KYXW8fUfuPHkg2NpKITg-FGyQuYTU5vSA4zOo&e=
> and provide commented, minimal, self-contained, reproducible code.
>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=K1a_rA-Z0ef6qHvn7vTcCIiwqM3-TiLHpEkHxPn9eW2bhsutM9gsG6l7ZIygZrrc&s=Kv3c5xefTDI4vqOIrrz8s6kS-9Yt30ttEFpzh6uMVq8&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=K1a_rA-Z0ef6qHvn7vTcCIiwqM3-TiLHpEkHxPn9eW2bhsutM9gsG6l7ZIygZrrc&s=iXx9A0KYXW8fUfuPHkg2NpKITg-FGyQuYTU5vSA4zOo&e=
and provide commented, minimal, self-contained, reproducible code.

From john@@t@p|e@ @end|ng |rom |c|oud@com  Mon Jun  6 20:53:15 2022
From: john@@t@p|e@ @end|ng |rom |c|oud@com (John RF Staples)
Date: Mon, 6 Jun 2022 19:53:15 +0100
Subject: [R] Build a data.table via two string lists
In-Reply-To: <6847A6BF-AEBA-47D0-9F78-1680A3F2BFA0@me.com>
References: <6847A6BF-AEBA-47D0-9F78-1680A3F2BFA0@me.com>
Message-ID: <2AC30BF4-A9A2-4A9E-B50D-5909CAD85DE5@icloud.com>

Hi, thanks for the comprehensive reply; it is appreciated.
John Staples

Sent from my iPad

> On 6 Jun 2022, at 14:05, John RF Staples <john.staples at me.com> wrote:
> 
> ?Hello everyone,
> 
> I have the following two structures:
> 
> # str(cl)
> # chr [1:5] "timestamp" "assess" "catheter" "service" "ref"
> 
> # str(rw)
> # chr [1:5] "1654508112" "3" "gel" "2785" ?16545081121a8d8f956cb871053a3f56b782b70a76"
> 
> From which I would like to generate a 5 col x 1 row data.table; the question is ?how??
> 
> ?cl? represents the column names; ?rw? the row values.
> 
> With anticipatory thanks ?
> John


From er|nm@hodge@@ @end|ng |rom gm@||@com  Tue Jun  7 21:38:43 2022
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Tue, 7 Jun 2022 15:38:43 -0400
Subject: [R] Mixing plotting symbols with text
Message-ID: <CACxE24=pgrsjNS2hq8U5pxdz6wSfh0+Ppn2y=dH32nG0znQgNg@mail.gmail.com>

Hello!

Hope you?re having a great day!

I would like to combine plotting symbols with text.

I have tried
xp2a <- expression(paste(pch =3, ?my stuff?))
But when I use that as a plot title, it just shows as ?3 my stuff?.

I have a feeling that it?s going to be something very straightforward that
I am missing.

Thanks for your help,
Sincerely,
Erin
-- 
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Tue Jun  7 22:08:38 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 7 Jun 2022 20:08:38 +0000
Subject: [R] Mixing plotting symbols with text
In-Reply-To: <CACxE24=pgrsjNS2hq8U5pxdz6wSfh0+Ppn2y=dH32nG0znQgNg@mail.gmail.com>
References: <CACxE24=pgrsjNS2hq8U5pxdz6wSfh0+Ppn2y=dH32nG0znQgNg@mail.gmail.com>
Message-ID: <BN6PR2201MB15536FDF31C74EDF7BBF7D77CFA59@BN6PR2201MB1553.namprd22.prod.outlook.com>

What is the result that you want?
If you cannot print the symbol that you want in a text format for this email, can you give us the Unicode?
You can find that here: https://en.wikipedia.org/wiki/List_of_Unicode_characters
But you have to scroll down a bit to get the list.


Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Erin Hodgess
Sent: Tuesday, June 7, 2022 3:39 PM
To: r-help at r-project.org
Subject: [R] Mixing plotting symbols with text

[External Email]

Hello!

Hope you?re having a great day!

I would like to combine plotting symbols with text.

I have tried
xp2a <- expression(paste(pch =3, ?my stuff?)) But when I use that as a plot title, it just shows as ?3 my stuff?.

I have a feeling that it?s going to be something very straightforward that I am missing.

Thanks for your help,
Sincerely,
Erin
--
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=Ca24Bry9icNvM5nMbklTt79BL8XEZwCr23gdqmPhbctGT3LAmDHnbSQVZbTQzIsJ&s=6B6Z3cxtD3WY5Aj9Pn0JncHo45ujheBej5uQytJmwig&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=Ca24Bry9icNvM5nMbklTt79BL8XEZwCr23gdqmPhbctGT3LAmDHnbSQVZbTQzIsJ&s=nxJBNhFueZylG4iHMRAjipCAASj2UFNzde6qi7tM7Bk&e=
and provide commented, minimal, self-contained, reproducible code.

From er|nm@hodge@@ @end|ng |rom gm@||@com  Tue Jun  7 22:14:22 2022
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Tue, 7 Jun 2022 16:14:22 -0400
Subject: [R] Mixing plotting symbols with text
In-Reply-To: <CAGxFJbQwaLUKe58D3LRBMRUDUBaoGnU45P75sdVXfyawZ=mYUA@mail.gmail.com>
References: <CACxE24=pgrsjNS2hq8U5pxdz6wSfh0+Ppn2y=dH32nG0znQgNg@mail.gmail.com>
 <CAGxFJbQwaLUKe58D3LRBMRUDUBaoGnU45P75sdVXfyawZ=mYUA@mail.gmail.com>
Message-ID: <CACxE24nyfLcYETABp0L386eCg7bf18JyPytDvo=UTi2Big-MtA@mail.gmail.com>

Thank you everyone, for your help!!

Sincerely,
Erin


On Tue, Jun 7, 2022 at 4:13 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> I assume this is using R's basic plotting engine.
>
> If so, you **might** be able to do this nicely with legend(). Jim Lemon's
> plotrix package might also have something for you.
>
> Otherwise you can set plot limits explicitly, and then plot the point
> outside those limits with your desired pch by setting xpd = TRUE. Something
> like:
>
> plot(c(1,1.5,2), c(3,5.5,4), pch = c(1,3,1), ylim = c(2.5,4.5),
>      xpd = TRUE)
>
> Then use mtext() to add your text nearby.
> Inelegant, but it should work.
>
> Cheers,
> Bert Gunter
>
>
> On Tue, Jun 7, 2022 at 12:39 PM Erin Hodgess <erinm.hodgess at gmail.com>
> wrote:
>
>> Hello!
>>
>> Hope you?re having a great day!
>>
>> I would like to combine plotting symbols with text.
>>
>> I have tried
>> xp2a <- expression(paste(pch =3, ?my stuff?))
>> But when I use that as a plot title, it just shows as ?3 my stuff?.
>>
>> I have a feeling that it?s going to be something very straightforward that
>> I am missing.
>>
>> Thanks for your help,
>> Sincerely,
>> Erin
>> --
>> Erin Hodgess, PhD
>> mailto: erinm.hodgess at gmail.com
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>
>
>> and provide commented, minimal, self-contained, reproducible code.
>>
> --
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Tue Jun  7 22:12:56 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 7 Jun 2022 13:12:56 -0700
Subject: [R] Mixing plotting symbols with text
In-Reply-To: <CACxE24=pgrsjNS2hq8U5pxdz6wSfh0+Ppn2y=dH32nG0znQgNg@mail.gmail.com>
References: <CACxE24=pgrsjNS2hq8U5pxdz6wSfh0+Ppn2y=dH32nG0znQgNg@mail.gmail.com>
Message-ID: <CAGxFJbQwaLUKe58D3LRBMRUDUBaoGnU45P75sdVXfyawZ=mYUA@mail.gmail.com>

I assume this is using R's basic plotting engine.

If so, you **might** be able to do this nicely with legend(). Jim Lemon's
plotrix package might also have something for you.

Otherwise you can set plot limits explicitly, and then plot the point
outside those limits with your desired pch by setting xpd = TRUE. Something
like:

plot(c(1,1.5,2), c(3,5.5,4), pch = c(1,3,1), ylim = c(2.5,4.5),
     xpd = TRUE)

Then use mtext() to add your text nearby.
Inelegant, but it should work.

Cheers,
Bert Gunter


On Tue, Jun 7, 2022 at 12:39 PM Erin Hodgess <erinm.hodgess at gmail.com>
wrote:

> Hello!
>
> Hope you?re having a great day!
>
> I would like to combine plotting symbols with text.
>
> I have tried
> xp2a <- expression(paste(pch =3, ?my stuff?))
> But when I use that as a plot title, it just shows as ?3 my stuff?.
>
> I have a feeling that it?s going to be something very straightforward that
> I am missing.
>
> Thanks for your help,
> Sincerely,
> Erin
> --
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Wed Jun  8 00:41:23 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 8 Jun 2022 08:41:23 +1000
Subject: [R] Mixing plotting symbols with text
In-Reply-To: <CACxE24=pgrsjNS2hq8U5pxdz6wSfh0+Ppn2y=dH32nG0znQgNg@mail.gmail.com>
References: <CACxE24=pgrsjNS2hq8U5pxdz6wSfh0+Ppn2y=dH32nG0znQgNg@mail.gmail.com>
Message-ID: <CA+8X3fV+a1wFq0mU6eib7KU1HDerEO=c7DkUf9yqbXVDSNBqBg@mail.gmail.com>

Hi Erin,
Here's a kinda clunky way to get your plot title. It can be modified
to do fancier things like intersperse symbols with letters but you may
need no more than this. Have fun.

par(mar=c(5,4,5,2))
plot(0,xlim=c(-1,1),ylim=c(-1,1),type="n",xlab="Nonsense",ylab="Intensity")
points(runif(4,-1,1),runif(4,-1,1),pch=3)
points(runif(4,-1,1),runif(4,-1,1),pch=4)
symbol_n_text<-function(x,y,pch,txt,adj=0,cex=1) {
 oldcex<-par(cex=cex)
 x<-x-(strwidth("M ")+strwidth(txt))/2
 points(x,y,pch=pch)
 x<-x+strwidth("M ")
 text(x,y,txt,adj=adj)
 par(oldcex)
}
par(xpd=TRUE)
symbol_n_text(0,1.35,3,"your stuff",cex=1.5)
symbol_n_text(0,1.2,4,"my stuff",cex=1.5)

Thanks for the nod, Bert.

Jim

On Wed, Jun 8, 2022 at 5:39 AM Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>
> Hello!
>
> Hope you?re having a great day!
>
> I would like to combine plotting symbols with text.
>
> I have tried
> xp2a <- expression(paste(pch =3, ?my stuff?))
> But when I use that as a plot title, it just shows as ?3 my stuff?.
>
> I have a feeling that it?s going to be something very straightforward that
> I am missing.
>
> Thanks for your help,
> Sincerely,
> Erin
> --
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @|mon@wood @end|ng |rom b@th@edu  Wed Jun  8 13:04:59 2022
From: @|mon@wood @end|ng |rom b@th@edu (Simon Wood)
Date: Wed, 8 Jun 2022 12:04:59 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
Message-ID: <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>

I would not worry too much about high concurvity between variables like 
temperature and time. This just reflects the fact that temperature has a 
strong temporal pattern.

I would also not be too worried about the low p-values on the k check. 
The check only looks for pattern in the residuals when they are ordered 
with respect to the variables of a smooth. When you have time series 
data and some smooths involve time then it's hard not to pick up some 
degree of residual auto-correlation, which you often would not want to 
model with a higher rank smoother.

The NAs? for the distributed lag terms just reflect the fact that there 
is no obvious way to order the residuals w.r.t. the covariates for such 
terms, so the simple check for residual pattern is not really possible.

One simple approach is to fit using bam(...,discrete=TRUE) which will 
let you specify an AR1 parameter to mop up some of the residual 
auto-correlation without resorting to a high rank smooth that then does 
all the work of the covariates as well. The AR1 parameter can be set by 
looking at the ACF of the residuals of the model without this. You need 
to look at the ACF of suitably standardized residuals to check how well 
this has worked.

best,

Simon

On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> Hello everyone,
>
> A few days ago I asked a question about concurvity in a GAM (the
> anologue of collinearity in a GLM) implemented in mgcv. I think my
> question was a bit unfocussed, so I am retrying again, but with
> additional information included about the autocorrelation function. I
> have also posted about this on Cross Validated. Given all the model
> output, it might make for easier
> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
>
> As mentioned previously, I have problems with concurvity in my thesis
> research, and don't have access to a statistician who works with time
> series, GAMs or R. I'd be very grateful for any (partial) answer,
> however short. I'll gladly return the favour where I can! For really
> helpful input I'd be more than happy to offer co-authorship on
> publication. Deadlines are very close, and I'm heading towards having
> no results at all if I can't solve this concurvity issue :(
>
> I'm using GAMs to try to understand the relationship between deaths
> and heat-related variables (e.g. temperature and humidity), using
> daily time series over a 14-year period from a tropical, low-income
> country. My aim is to understand the relationship between these
> variables and deaths, rather than pure prediction performance.
>
> The GAMs include distributed lag models (set up as 7-column matrices,
> see code at bottom of post), since deaths may occur over several days
> following exposure.
>
> Simple GAMs with just time, lagged temperature and lagged
> precipitation (a potential confounder) show very high concurvity
> between lagged temperature and time, regardless of the many different
> ways I have tried to decompose time. The autocorrelation functions
> (ACF) however, shows values close to zero, only just about breaching
> the 'significance line' in a few instances. It does show patterning
> though, although the regularity is difficult to define.
>
> My questions are:
> 1) Should I be worried about the high concurvity, or can I ignore it
> given the mostly non-significant ACF? I've read dozens of
> heat-mortality modelling studies and none report on concurvity between
> weather variables and time (though one 2012 paper discussed
> autocorrelation).
>
> 2) If I cannot ignore it, what should I do to resolve it? Would
> including an autoregressive term be appropriate, and if so, where can
> I find a coded example of how to do this? I've also come across
> sequential regression][1]. Would this be more or less appropriate? If
> appropriate, a pointer to an example would be really appreciated!
>
> Some example GAMs are specified as follows:
> ```r
> conc38b <- gam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc")) + heap +
>                        te(temp_max, lag, k=c(10, 3)) +
>                        te(precip_daily_total, lag, k=c(10, 3)),
>                        data = dat, family = nb, method = 'REML', select = TRUE,
>                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> 52.5), weekday = c(0, 6.5)))
> ```
> Concurvity for the above model between (temp_max, lag) and (year,
> month, week, weekday) is 0.91:
>
> ```r
> $worst
>                                      para te(year,month,week,weekday)
> te(temp_max,lag) te(precip_daily_total,lag)
> para                        1.000000e+00                1.125625e-29
>       0.3150073                  0.6666348
> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
>       0.9060552                  0.6652313
> te(temp_max,lag)            3.152795e-01                8.998113e-01
>       1.0000000                  0.5781015
> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
>       0.5805159                  1.0000000
> ```
>
> Output from ```gam.check()```:
> ```r
> Method: REML   Optimizer: outer newton
> full convergence after 16 iterations.
> Gradient range [-0.01467332,0.003096643]
> (score 8915.994 & scale 1).
> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> Model rank =  544 / 544
>
> Basis dimension (k) checking results. Low p-value (k-index<1) may
> indicate that k is too low, especially if edf is close to k'.
>
>                                    k'      edf k-index p-value
> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> te(temp_max,lag)             29.0000   3.3681      NA      NA
> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> ```
>
> Some output from ```summary(conc38b)```:
> ```r
> Approximate significance of smooth terms:
>                                    edf Ref.df  Chi.sq p-value
> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> -REML =   8916  Scale est. = 1         n = 5107
> ```
>
>
> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> pattern). They show peaks at 5 and 15, and then there seems to be a
> recurring pattern at multiples of approx. 30 (suggesting month is not
> modelled adequately?). Not sure what would cause the spikes at 5 and
> 15. There is heaping of deaths on the 15th day of each month, to which
> deaths with unknown date were allocated. This heaping was modelled
> with categorical variable/ factor ```heap``` with 169 levels (0 for
> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> heaping day over the 14-year period):
>
>    [2]: https://i.stack.imgur.com/FzKyM.png
>    [3]: https://i.stack.imgur.com/fE3aL.png
>
>
> I get an identical looking ACF when I decompose time into (year,
> month, monthday) as in model conc39 below, although concurvity between
> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
>
> ```r
> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>                       te(temp_max, lag, k=c(10, 4)) +
>                       te(precip_daily_total, lag, k=c(10, 4)),
>                       data = dat, family = nb, method = 'REML', select = TRUE,
>                       knots = list(month = c(0.5, 12.5)))
> ```
> ```r
>
> Method: REML   Optimizer: outer newton
> full convergence after 14 iterations.
> Gradient range [-0.001578187,6.155096e-05]
> (score 8915.763 & scale 1).
> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> Model rank =  323 / 323
>
> Basis dimension (k) checking results. Low p-value (k-index<1) may
> indicate that k is too low, especially if edf is close to k'.
>
>                                  k'     edf k-index p-value
> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> te(temp_max,lag)           39.0000  4.0875      NA      NA
> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> ```
> Some output from ```summary(conc39)```:
> ```r
> Approximate significance of smooth terms:
>                                  edf Ref.df  Chi.sq  p-value
> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> -REML =   8915  Scale est. = 1         n = 5107
> ```
>
>
> ```r
> $worst
>                                     para te(year,month,monthday)
> te(temp_max,lag) te(precip_daily_total,lag)
> para                       1.000000e+00            3.261007e-31
> 0.3313549                  0.6666532
> te(year,month,monthday)    3.060763e-31            1.000000e+00
> 0.8266086                  0.5670777
> te(temp_max,lag)           3.331014e-01            8.225942e-01
> 1.0000000                  0.5840875
> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> 0.5939380                  1.0000000
> ```
>
> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> various choices for k or as ```s(time)``` with various k does not
> reduce concurvity either.
>
>
> The default approach in time series studies of heat-mortality is to
> model time with fixed df, generally between 7-10 df per year of data.
> I am, however, apprehensive about this approach because a) mortality
> profiles vary with locality due to sociodemographic and environmental
> characteristics and b) the choice of df is based on higher income
> countries (where nearly all these studies have been done) with
> different mortality profiles and so may not be appropriate for
> tropical, low-income countries.
>
> Although the approach of fixing (high) df does remove more temporal
> patterns from the ACF (see model and output below), concurvity between
> time and lagged temperature has now risen to 0.99! Moreover,
> temperature (which has been a consistent, highly significant predictor
> in every model of the tens (hundreds?) I have run, has now turned
> non-significant. I am guessing this is because time is now a very
> wiggly function that not only models/ removes seasonal variation, but
> also some of the day-to-day variation that is needed for the
> temperature smooth  :
>
> ```r
> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>                        te(temp_max, lag, k=c(10,3)) +
>                        te(precip_daily_total, lag, k=c(10,3)),
>                        data = dat, family = nb, method = 'REML', select = TRUE)
> ```
> Output from ```gam.check(conc20a, rep = 1000)```:
>
> ```r
> Method: REML   Optimizer: outer newton
> full convergence after 9 iterations.
> Gradient range [-0.0008983099,9.546022e-05]
> (score 8750.13 & scale 1).
> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> Model rank =  336 / 336
>
> Basis dimension (k) checking results. Low p-value (k-index<1) may
> indicate that k is too low, especially if edf is close to k'.
>
>                                   k'      edf k-index p-value
> s(time)                    111.0000 111.0000    0.98    0.56
> te(temp_max,lag)            29.0000   0.6548      NA      NA
> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> ```
> Output from ```concurvity(conc20a, full=FALSE)$worst```:
>
> ```r
>                                     para      s(time) te(temp_max,lag)
> te(precip_daily_total,lag)
> para                       1.000000e+00 2.462064e-19        0.3165236
>                  0.6666348
> s(time)                    2.462398e-19 1.000000e+00        0.9930674
>                  0.6879284
> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
>                  0.5788711
> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
>                  1.0000000
>
> ```
>
> Some output from ```summary(conc20a)```:
> ```r
> Approximate significance of smooth terms:
>                                   edf Ref.df  Chi.sq p-value
> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> -REML = 8750.1  Scale est. = 1         n = 5107
> ```
>
> ACF functions:
>
> [4]: https://i.stack.imgur.com/7nbXS.png
> [5]: https://i.stack.imgur.com/pNnZU.png
>
> Data can be found on my [GitHub][6] site in the file
> [data_cross_validated_post2.rds][7]. A csv version is also available.
> This is my code:
>
> ```r
> library(readr)
> library(mgcv)
>
> df <- read_rds("data_crossvalidated_post2.rds")
>
> # Create matrices for lagged weather variables (6 day lags) based on
> example by Simon Wood
> # in his 2017 book ("Generalized additive models: an introduction with
> R", p. 349) and
> # gamair package documentation
> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
>
> lagard <- function(x,n.lag=7) {
> n <- length(x); X <- matrix(NA,n,n.lag)
> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> X
> }
>
> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> = df$precip_hourly_dailysum)
> dat$temp_max <- lagard(df$temp_max)
> dat$temp_min <- lagard(df$temp_min)
> dat$temp_mean <- lagard(df$temp_mean)
> dat$wbgt_max <- lagard(df$wbgt_max)
> dat$wbgt_mean <- lagard(df$wbgt_mean)
> dat$wbgt_min <- lagard(df$wbgt_min)
> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> dat$sh_mean <- lagard(df$sh_mean)
> dat$solar_mean <- lagard(df$solar_mean)
> dat$wind2m_mean <- lagard(df$wind2m_mean)
> dat$sh_max <- lagard(df$sh_max)
> dat$solar_max <- lagard(df$solar_max)
> dat$wind2m_max <- lagard(df$wind2m_max)
> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> dat$precip_hourly <- lagard(df$precip_hourly)
> dat$precip_daily_total <- lagard( df$precip_daily_total)
> dat$temp <- lagard(df$temp)
> dat$sh <- lagard(df$sh)
> dat$rh <- lagard(df$rh)
> dat$solar <- lagard(df$solar)
> dat$wind2m <- lagard(df$wind2m)
>
>
> conc38b <- gam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc")) + heap +
>                        te(temp_max, lag, k=c(10, 3)) +
>                        te(precip_daily_total, lag, k=c(10, 3)),
>                        data = dat, family = nb, method = 'REML', select = TRUE,
>                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> 52.5), weekday = c(0, 6.5)))
>
> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>                       te(temp_max, lag, k=c(10, 4)) +
>                       te(precip_daily_total, lag, k=c(10, 4)),
>                       data = dat, family = nb, method = 'REML', select = TRUE,
>                       knots = list(month = c(0.5, 12.5)))
>
> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>                        te(temp_max, lag, k=c(10,3)) +
>                        te(precip_daily_total, lag, k=c(10,3)),
>                        data = dat, family = nb, method = 'REML', select = TRUE)
>
> ```
> Thank you if you've read this far!! :-))
>
>    [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
>    [2]: https://i.stack.imgur.com/FzKyM.png
>    [3]: https://i.stack.imgur.com/fE3aL.png
>    [4]: https://i.stack.imgur.com/7nbXS.png
>    [5]: https://i.stack.imgur.com/pNnZU.png
>    [6]: https://github.com/JadeShodan/heat-mortality
>    [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Simon Wood, School of Mathematics, University of Edinburgh,
https://www.maths.ed.ac.uk/~swood34/


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Wed Jun  8 20:35:46 2022
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Wed, 8 Jun 2022 20:35:46 +0200
Subject: [R] bootstrap CI of the difference between 2 Cramer's V
In-Reply-To: <0e2dd5f6-c0e8-5b72-37f4-4115794319f8@sapo.pt>
References: <0e2dd5f6-c0e8-5b72-37f4-4115794319f8@sapo.pt>
Message-ID: <BAF5F99C-16A6-4705-B651-F7CD16140302@yahoo.fr>

Dear Rui, 
Dear Daniel,

Many thanks for your code. It perfectly works.
Thanks for your answer.

Best,
S.

Envoy? de mon iPhone

> Le 6 juin 2022 ? 21:17, Rui Barradas <ruipbarradas at sapo.pt> a ?crit :
> 
> ?Hello,
> 
> Here is your code, corrected. It uses a Goodman-Kruskal gamma function in package DescTools.
> Function G probably doesn't need tryCatch but I had errors in some test runs.
> 
> 
> library(boot)
> 
> G <- function(x, index, cols = 1:2) {
>  y <- x[index, cols]    # bootstrapped data
>  g <- x$group[index]    # and groups
>  # calculate gamma for each group bootstrap sample
>  # (trap errors in case one of the groups is empty)
>  g <- tryCatch(
>    lapply(split(y[1:2], g), \(x) {
>      tbl <- table(x)
>      DescTools::GoodmanKruskalGamma(tbl)
>    }),
>    error = function(e) list(NA_real_, NA_real_)
>  )
>  # calculate difference
>  g[[1]] - g[[2]]
> }
> 
> set.seed(2022)
> # use strata parameter in function boot to resample within each group
> results <- boot(
>  data = f3,
>  statistic = G,
>  R = 2000,
>  strata = as.factor(f3$group),
>  cols = 1:2
> )
> 
> results
> boot.ci(results)
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> 
> ?s 17:21 de 05/06/2022, varin sacha via R-help escreveu:
>> Dear Daniel,
>> Dear R-experts,
>> I really thank you a lot Daniel. Nobody had answered to me offline. So, thanks.
>> I have tried in the same vein for the Goodman-Kruskal gamma for ordinal data. There is an error message at the end of the code. Thanks for your help.
>> ##############################
>> library(ryouready)
>> library(boot)
>> shopping1<-c("tr?s important","important","pas important","pas important","important","tr?s important","important","pas important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","tr?s important","important","pas important","pas important","important","tr?s important","important")
>> statut1<-c("riche","pas riche","moyennement riche","moyennement riche","riche","pas riche","moyennement riche","moyennement riche","riche","pas riche","moyennement riche","riche","pas riche","pas riche","riche","moyennement riche","riche","pas riche","pas riche","pas riche","riche","riche","moyennement riche","riche","riche","moyennement riche","moyennement riche","moyennement riche","pas riche","pas riche","riche","pas riche","riche","pas riche","riche","moyennement riche","riche","pas riche","moyennement riche","riche")
>> shopping2<-c("important","pas important","tr?s important","tr?s important","important","tr?s important","pas important","important","pas important","tr?s important","important","important","important","important","pas important","tr?s important","tr?s important","important","pas important","tr?s important","pas important","tr?s important","pas important","tr?s important","important","tr?s important","important","pas important","pas important","important","pas important","tr?s important","pas important","pas important","important","important","tr?s important","tr?s important","pas important","pas important")
>> statut2<-c("moyennement riche","pas riche","riche","moyennement riche","moyennement riche","moyennement riche","pas riche","riche","riche","pas riche","moyennement riche","riche","riche","riche","riche","riche","pas riche","moyennement riche","moyennement riche","pas riche","moyennement riche","pas riche","pas riche","pas riche","moyennement riche","riche","moyennement riche","riche","pas riche","riche","moyennement riche","blue","moyennement riche","pas riche","pas riche","riche","riche","pas riche","pas riche","pas riche")
>> f1 <- data.frame(shopping=shopping1,statut=statut1,group='grp1')
>> f2 <- data.frame(shopping=shopping2,statut=statut2,group='grp2')
>> f3 <- rbind(f1,f2)
>> G <- function(x, index) {
>>    # calculate goodman for group 1 bootstrap sample
>>    g1 <-x[index,][x[,3]=='grp1',]
>>    goodman_g1 <- cor(data[index,][1,2])
>>     # calculate goodman for group 2 bootstrap sample
>>    g2 <-x[index,][x[,3]=='grp2',]
>>    goodman_g2 <- cor(data[index,][3,4])
>>     # calculate difference
>>    goodman_g1-goodman_g2
>>    }
>>  # use strata parameter in function boot to resample within each group
>> results <- boot(data=f3,statistic=G, strata=as.factor(f3$group),R=2000)
>> results
>> boot.ci(results)
>> ##############################
>> Le samedi 4 juin 2022 ? 09:31:36 UTC+2, Daniel Nordlund <djnordlund at gmail.com> a ?crit :
>>> On 5/28/2022 11:21 AM, varin sacha via R-help wrote:
>>> Dear R-experts,
>>> 
>>> While comparing groups, it is better to assess confidence intervals of those differences rather than comparing confidence intervals for each group.
>>> I am trying to calculate the CIs of the difference between the two Cramer's V and not the CI to the estimate of each group?s Cramer's V.
>>> 
>>> Here below my toy R example. There are error messages. Any help would be highly appreciated.
>>> 
>>> ##############################
>>> library(questionr)
>>> library(boot)
>>> 
>>> gender1<-c("M","F","F","F","M","M","F","F","F","M","M","F","M","M","F","M","M","F","M","F","F","F","M","M","M","F","F","M","M","M","F","M","F","F","F","M","M","F","M","F")
>>> color1<-c("blue","green","black","black","green","green","blue","blue","green","black","blue","green","blue","black","black","blue","green","blue","green","black","blue","blue","black","black","green","green","blue","green","black","green","blue","black","black","blue","green","green","green","blue","blue","black")
>>> 
>>> gender2<-c("F","F","F","M","M","F","M","M","M","F","F","M","F","M","F","F","M","M","M","F","M","M","M","F","F","F","M","M","M","F","M","M","M","F","F","F","M","F","F","F")
>>> color2<-c("green","blue","black","blue","blue","blue","green","blue","green","black","blue","black","blue","blue","black","blue","blue","green","blue","black","blue","blue","black","black","green","blue","black","green","blue","green","black","blue","black","blue","green","blue","green","green","blue","black")
>>> 
>>> f1=data.frame(gender1,color1)
>>> tab1<-table(gender1,color1)
>>> e1<-cramer.v(tab1)
>>> 
>>> f2=data.frame(gender2,color2)
>>> tab2<-table(gender2,color2)
>>> e2<-cramer.v(tab2)
>>> 
>>> f3<-data.frame(e1-e2)
>>> 
>>> cramerdiff=function(x,w){
>>> y<-tapply(x[w,1], x[w,2],cramer.v)
>>> y[1]-y[2]
>>> }
>>> 
>>> results<-boot(data=f3,statistic=cramerdiff,R=2000)
>>> results
>>> 
>>> boot.ci(results,type="all")
>>> ##############################
>>> 
>>>   
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> I don't know if someone responded offline, but if not, there are a
>> couple of problems with your code.   First, the f3 dataframe is not what
>> you think it is.  Second, your cramerdiff function isn't going to
>> produce the results that you want.
>> I would put your data into a single dataframe with a variable
>> designating which group data came from.  Then use that variable as the
>> strata variable in the boot function to resample within groups.  So
>> something like this:
>> f1 <- data.frame(gender=gender1,color=color1,group='grp1')
>> f2 <- data.frame(gender=gender2,color=color2,group='grp2')
>> f3 <- rbind(f1,f2)
>> cramerdiff <- function(x, ndx) {
>>    # calculate cramer.v for group 1 bootstrap sample
>>    g1 <-x[ndx,][x[,3]=='grp1',]
>>    cramer_g1 <- cramer.v(table(g1[,1:2]))
>>    # calculate cramer.v for group 2 bootstrap sample
>>    g2 <-x[ndx,][x[,3]=='grp2',]
>>    cramer_g2 <- cramer.v(table(g2[,1:2]))
>>    # calculate difference
>>    cramer_g1-cramer_g2
>>    }
>> # use strata parameter in function boot to resample within each group
>> results <- boot(data=f3,statistic=cramerdiff,
>> strata=as.factor(f3$group),R=2000)
>> results
>> boot.ci(results)
>> Hope this is helpful,
>> Dan


From j@de@shod@@ m@iii@g oii googiem@ii@com  Wed Jun  8 19:15:11 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Wed, 8 Jun 2022 18:15:11 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
Message-ID: <CANg3_k-EXdbmLVNbEH9nN=FE3FYs4uqXgVtH3V35o6BU0z=rTw@mail.gmail.com>

Hi Simon,

Thanks so much for this!! I have two follow up questions, if you don't mind.

1. Does including an autoregressive term not adjust away part of the
effect of the response in a distributed lag model (where the outcome
accumulates over time)?
2. I've tried to fit a model using bam (just a first attempt without
AR term), but including the factor variable heap creates errors:

bam0 <- bam(deaths~te(year, month, week, weekday,
bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
                      te(temp_max, lag, k=c(8, 3)) +
                      te(precip_daily_total, lag, k=c(8, 3)),
                      data = dat, family = nb, method = 'fREML',
select = TRUE, discrete = TRUE,
                      knots = list(month = c(0.5, 12.5), week = c(0.5,
52.5), weekday = c(0, 6.5)))

This model results in errors:

Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
  step failure in theta estimation
Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
object$prior.weights)) :
  NaNs produced


Including heap as as.numeric(heap) runs the model without error
messages or warnings, but model diagnostics look terrible, and it also
doesn't make sense (to me) to make heap a numeric. The factor variable
heap (with 169 levels) codes the fact that all deaths for which no
date was known, were registered on the 15th day of each month. I've
coded all non-heaping days as 0. All heaping days were coded as a
value between 1-168. The time series spans 14 years, so a heaping day
in each month results in 14*12 levels = 168, plus one level for
non-heaping days.

So my second question is: Does bam allow factor variables? And if not,
how should I model this heaping on the 15th day of the month instead?

With thanks,

Jade

On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
>
> I would not worry too much about high concurvity between variables like
> temperature and time. This just reflects the fact that temperature has a
> strong temporal pattern.
>
> I would also not be too worried about the low p-values on the k check.
> The check only looks for pattern in the residuals when they are ordered
> with respect to the variables of a smooth. When you have time series
> data and some smooths involve time then it's hard not to pick up some
> degree of residual auto-correlation, which you often would not want to
> model with a higher rank smoother.
>
> The NAs  for the distributed lag terms just reflect the fact that there
> is no obvious way to order the residuals w.r.t. the covariates for such
> terms, so the simple check for residual pattern is not really possible.
>
> One simple approach is to fit using bam(...,discrete=TRUE) which will
> let you specify an AR1 parameter to mop up some of the residual
> auto-correlation without resorting to a high rank smooth that then does
> all the work of the covariates as well. The AR1 parameter can be set by
> looking at the ACF of the residuals of the model without this. You need
> to look at the ACF of suitably standardized residuals to check how well
> this has worked.
>
> best,
>
> Simon
>
> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> > Hello everyone,
> >
> > A few days ago I asked a question about concurvity in a GAM (the
> > anologue of collinearity in a GLM) implemented in mgcv. I think my
> > question was a bit unfocussed, so I am retrying again, but with
> > additional information included about the autocorrelation function. I
> > have also posted about this on Cross Validated. Given all the model
> > output, it might make for easier
> > reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> >
> > As mentioned previously, I have problems with concurvity in my thesis
> > research, and don't have access to a statistician who works with time
> > series, GAMs or R. I'd be very grateful for any (partial) answer,
> > however short. I'll gladly return the favour where I can! For really
> > helpful input I'd be more than happy to offer co-authorship on
> > publication. Deadlines are very close, and I'm heading towards having
> > no results at all if I can't solve this concurvity issue :(
> >
> > I'm using GAMs to try to understand the relationship between deaths
> > and heat-related variables (e.g. temperature and humidity), using
> > daily time series over a 14-year period from a tropical, low-income
> > country. My aim is to understand the relationship between these
> > variables and deaths, rather than pure prediction performance.
> >
> > The GAMs include distributed lag models (set up as 7-column matrices,
> > see code at bottom of post), since deaths may occur over several days
> > following exposure.
> >
> > Simple GAMs with just time, lagged temperature and lagged
> > precipitation (a potential confounder) show very high concurvity
> > between lagged temperature and time, regardless of the many different
> > ways I have tried to decompose time. The autocorrelation functions
> > (ACF) however, shows values close to zero, only just about breaching
> > the 'significance line' in a few instances. It does show patterning
> > though, although the regularity is difficult to define.
> >
> > My questions are:
> > 1) Should I be worried about the high concurvity, or can I ignore it
> > given the mostly non-significant ACF? I've read dozens of
> > heat-mortality modelling studies and none report on concurvity between
> > weather variables and time (though one 2012 paper discussed
> > autocorrelation).
> >
> > 2) If I cannot ignore it, what should I do to resolve it? Would
> > including an autoregressive term be appropriate, and if so, where can
> > I find a coded example of how to do this? I've also come across
> > sequential regression][1]. Would this be more or less appropriate? If
> > appropriate, a pointer to an example would be really appreciated!
> >
> > Some example GAMs are specified as follows:
> > ```r
> > conc38b <- gam(deaths~te(year, month, week, weekday,
> > bs=c("cr","cc","cc","cc")) + heap +
> >                        te(temp_max, lag, k=c(10, 3)) +
> >                        te(precip_daily_total, lag, k=c(10, 3)),
> >                        data = dat, family = nb, method = 'REML', select = TRUE,
> >                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> > 52.5), weekday = c(0, 6.5)))
> > ```
> > Concurvity for the above model between (temp_max, lag) and (year,
> > month, week, weekday) is 0.91:
> >
> > ```r
> > $worst
> >                                      para te(year,month,week,weekday)
> > te(temp_max,lag) te(precip_daily_total,lag)
> > para                        1.000000e+00                1.125625e-29
> >       0.3150073                  0.6666348
> > te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> >       0.9060552                  0.6652313
> > te(temp_max,lag)            3.152795e-01                8.998113e-01
> >       1.0000000                  0.5781015
> > te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> >       0.5805159                  1.0000000
> > ```
> >
> > Output from ```gam.check()```:
> > ```r
> > Method: REML   Optimizer: outer newton
> > full convergence after 16 iterations.
> > Gradient range [-0.01467332,0.003096643]
> > (score 8915.994 & scale 1).
> > Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> > Model rank =  544 / 544
> >
> > Basis dimension (k) checking results. Low p-value (k-index<1) may
> > indicate that k is too low, especially if edf is close to k'.
> >
> >                                    k'      edf k-index p-value
> > te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> > te(temp_max,lag)             29.0000   3.3681      NA      NA
> > te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > ```
> >
> > Some output from ```summary(conc38b)```:
> > ```r
> > Approximate significance of smooth terms:
> >                                    edf Ref.df  Chi.sq p-value
> > te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> > te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> > te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >
> > R-sq.(adj) =  0.839   Deviance explained = 53.3%
> > -REML =   8916  Scale est. = 1         n = 5107
> > ```
> >
> >
> > Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> > pattern). They show peaks at 5 and 15, and then there seems to be a
> > recurring pattern at multiples of approx. 30 (suggesting month is not
> > modelled adequately?). Not sure what would cause the spikes at 5 and
> > 15. There is heaping of deaths on the 15th day of each month, to which
> > deaths with unknown date were allocated. This heaping was modelled
> > with categorical variable/ factor ```heap``` with 169 levels (0 for
> > all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> > heaping day over the 14-year period):
> >
> >    [2]: https://i.stack.imgur.com/FzKyM.png
> >    [3]: https://i.stack.imgur.com/fE3aL.png
> >
> >
> > I get an identical looking ACF when I decompose time into (year,
> > month, monthday) as in model conc39 below, although concurvity between
> > (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> >
> > ```r
> > conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >                       te(temp_max, lag, k=c(10, 4)) +
> >                       te(precip_daily_total, lag, k=c(10, 4)),
> >                       data = dat, family = nb, method = 'REML', select = TRUE,
> >                       knots = list(month = c(0.5, 12.5)))
> > ```
> > ```r
> >
> > Method: REML   Optimizer: outer newton
> > full convergence after 14 iterations.
> > Gradient range [-0.001578187,6.155096e-05]
> > (score 8915.763 & scale 1).
> > Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> > Model rank =  323 / 323
> >
> > Basis dimension (k) checking results. Low p-value (k-index<1) may
> > indicate that k is too low, especially if edf is close to k'.
> >
> >                                  k'     edf k-index p-value
> > te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> > te(temp_max,lag)           39.0000  4.0875      NA      NA
> > te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > ```
> > Some output from ```summary(conc39)```:
> > ```r
> > Approximate significance of smooth terms:
> >                                  edf Ref.df  Chi.sq  p-value
> > te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> > te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> > te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >
> > R-sq.(adj) =  0.839   Deviance explained = 53.8%
> > -REML =   8915  Scale est. = 1         n = 5107
> > ```
> >
> >
> > ```r
> > $worst
> >                                     para te(year,month,monthday)
> > te(temp_max,lag) te(precip_daily_total,lag)
> > para                       1.000000e+00            3.261007e-31
> > 0.3313549                  0.6666532
> > te(year,month,monthday)    3.060763e-31            1.000000e+00
> > 0.8266086                  0.5670777
> > te(temp_max,lag)           3.331014e-01            8.225942e-01
> > 1.0000000                  0.5840875
> > te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> > 0.5939380                  1.0000000
> > ```
> >
> > Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> > various choices for k or as ```s(time)``` with various k does not
> > reduce concurvity either.
> >
> >
> > The default approach in time series studies of heat-mortality is to
> > model time with fixed df, generally between 7-10 df per year of data.
> > I am, however, apprehensive about this approach because a) mortality
> > profiles vary with locality due to sociodemographic and environmental
> > characteristics and b) the choice of df is based on higher income
> > countries (where nearly all these studies have been done) with
> > different mortality profiles and so may not be appropriate for
> > tropical, low-income countries.
> >
> > Although the approach of fixing (high) df does remove more temporal
> > patterns from the ACF (see model and output below), concurvity between
> > time and lagged temperature has now risen to 0.99! Moreover,
> > temperature (which has been a consistent, highly significant predictor
> > in every model of the tens (hundreds?) I have run, has now turned
> > non-significant. I am guessing this is because time is now a very
> > wiggly function that not only models/ removes seasonal variation, but
> > also some of the day-to-day variation that is needed for the
> > temperature smooth  :
> >
> > ```r
> > conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >                        te(temp_max, lag, k=c(10,3)) +
> >                        te(precip_daily_total, lag, k=c(10,3)),
> >                        data = dat, family = nb, method = 'REML', select = TRUE)
> > ```
> > Output from ```gam.check(conc20a, rep = 1000)```:
> >
> > ```r
> > Method: REML   Optimizer: outer newton
> > full convergence after 9 iterations.
> > Gradient range [-0.0008983099,9.546022e-05]
> > (score 8750.13 & scale 1).
> > Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> > Model rank =  336 / 336
> >
> > Basis dimension (k) checking results. Low p-value (k-index<1) may
> > indicate that k is too low, especially if edf is close to k'.
> >
> >                                   k'      edf k-index p-value
> > s(time)                    111.0000 111.0000    0.98    0.56
> > te(temp_max,lag)            29.0000   0.6548      NA      NA
> > te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> > ```
> > Output from ```concurvity(conc20a, full=FALSE)$worst```:
> >
> > ```r
> >                                     para      s(time) te(temp_max,lag)
> > te(precip_daily_total,lag)
> > para                       1.000000e+00 2.462064e-19        0.3165236
> >                  0.6666348
> > s(time)                    2.462398e-19 1.000000e+00        0.9930674
> >                  0.6879284
> > te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> >                  0.5788711
> > te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> >                  1.0000000
> >
> > ```
> >
> > Some output from ```summary(conc20a)```:
> > ```r
> > Approximate significance of smooth terms:
> >                                   edf Ref.df  Chi.sq p-value
> > s(time)                    1.110e+02    111 419.375  <2e-16 ***
> > te(temp_max,lag)           6.548e-01     27   0.895   0.249
> > te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >
> > R-sq.(adj) =  0.843   Deviance explained = 56.1%
> > -REML = 8750.1  Scale est. = 1         n = 5107
> > ```
> >
> > ACF functions:
> >
> > [4]: https://i.stack.imgur.com/7nbXS.png
> > [5]: https://i.stack.imgur.com/pNnZU.png
> >
> > Data can be found on my [GitHub][6] site in the file
> > [data_cross_validated_post2.rds][7]. A csv version is also available.
> > This is my code:
> >
> > ```r
> > library(readr)
> > library(mgcv)
> >
> > df <- read_rds("data_crossvalidated_post2.rds")
> >
> > # Create matrices for lagged weather variables (6 day lags) based on
> > example by Simon Wood
> > # in his 2017 book ("Generalized additive models: an introduction with
> > R", p. 349) and
> > # gamair package documentation
> > (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> >
> > lagard <- function(x,n.lag=7) {
> > n <- length(x); X <- matrix(NA,n,n.lag)
> > for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > X
> > }
> >
> > dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> > deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> > weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> > df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> > = df$precip_hourly_dailysum)
> > dat$temp_max <- lagard(df$temp_max)
> > dat$temp_min <- lagard(df$temp_min)
> > dat$temp_mean <- lagard(df$temp_mean)
> > dat$wbgt_max <- lagard(df$wbgt_max)
> > dat$wbgt_mean <- lagard(df$wbgt_mean)
> > dat$wbgt_min <- lagard(df$wbgt_min)
> > dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> > dat$sh_mean <- lagard(df$sh_mean)
> > dat$solar_mean <- lagard(df$solar_mean)
> > dat$wind2m_mean <- lagard(df$wind2m_mean)
> > dat$sh_max <- lagard(df$sh_max)
> > dat$solar_max <- lagard(df$solar_max)
> > dat$wind2m_max <- lagard(df$wind2m_max)
> > dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> > dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> > dat$precip_hourly <- lagard(df$precip_hourly)
> > dat$precip_daily_total <- lagard( df$precip_daily_total)
> > dat$temp <- lagard(df$temp)
> > dat$sh <- lagard(df$sh)
> > dat$rh <- lagard(df$rh)
> > dat$solar <- lagard(df$solar)
> > dat$wind2m <- lagard(df$wind2m)
> >
> >
> > conc38b <- gam(deaths~te(year, month, week, weekday,
> > bs=c("cr","cc","cc","cc")) + heap +
> >                        te(temp_max, lag, k=c(10, 3)) +
> >                        te(precip_daily_total, lag, k=c(10, 3)),
> >                        data = dat, family = nb, method = 'REML', select = TRUE,
> >                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> > 52.5), weekday = c(0, 6.5)))
> >
> > conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >                       te(temp_max, lag, k=c(10, 4)) +
> >                       te(precip_daily_total, lag, k=c(10, 4)),
> >                       data = dat, family = nb, method = 'REML', select = TRUE,
> >                       knots = list(month = c(0.5, 12.5)))
> >
> > conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >                        te(temp_max, lag, k=c(10,3)) +
> >                        te(precip_daily_total, lag, k=c(10,3)),
> >                        data = dat, family = nb, method = 'REML', select = TRUE)
> >
> > ```
> > Thank you if you've read this far!! :-))
> >
> >    [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> >    [2]: https://i.stack.imgur.com/FzKyM.png
> >    [3]: https://i.stack.imgur.com/fE3aL.png
> >    [4]: https://i.stack.imgur.com/7nbXS.png
> >    [5]: https://i.stack.imgur.com/pNnZU.png
> >    [6]: https://github.com/JadeShodan/heat-mortality
> >    [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Simon Wood, School of Mathematics, University of Edinburgh,
> https://www.maths.ed.ac.uk/~swood34/
>


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Thu Jun  9 15:52:35 2022
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Thu, 9 Jun 2022 14:52:35 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k-EXdbmLVNbEH9nN=FE3FYs4uqXgVtH3V35o6BU0z=rTw@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-EXdbmLVNbEH9nN=FE3FYs4uqXgVtH3V35o6BU0z=rTw@mail.gmail.com>
Message-ID: <91a30ffd-1aec-0f44-17a0-b632fae91014@dewey.myzen.co.uk>

Dear Jade

Do you really need to fit a separate parameter for each heaping day? Can 
you not just make it a binary predictor or a categorical one with fewer 
levels, perhaps 14 (for heaping in each year) or 12 (for each calendar 
month). I have no idea whether that would help but it seems worth a try.

Michael

On 08/06/2022 18:15, jade.shodan--- via R-help wrote:
> Hi Simon,
> 
> Thanks so much for this!! I have two follow up questions, if you don't mind.
> 
> 1. Does including an autoregressive term not adjust away part of the
> effect of the response in a distributed lag model (where the outcome
> accumulates over time)?
> 2. I've tried to fit a model using bam (just a first attempt without
> AR term), but including the factor variable heap creates errors:
> 
> bam0 <- bam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
>                        te(temp_max, lag, k=c(8, 3)) +
>                        te(precip_daily_total, lag, k=c(8, 3)),
>                        data = dat, family = nb, method = 'fREML',
> select = TRUE, discrete = TRUE,
>                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> 52.5), weekday = c(0, 6.5)))
> 
> This model results in errors:
> 
> Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
>    step failure in theta estimation
> Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> object$prior.weights)) :
>    NaNs produced
> 
> 
> Including heap as as.numeric(heap) runs the model without error
> messages or warnings, but model diagnostics look terrible, and it also
> doesn't make sense (to me) to make heap a numeric. The factor variable
> heap (with 169 levels) codes the fact that all deaths for which no
> date was known, were registered on the 15th day of each month. I've
> coded all non-heaping days as 0. All heaping days were coded as a
> value between 1-168. The time series spans 14 years, so a heaping day
> in each month results in 14*12 levels = 168, plus one level for
> non-heaping days.
> 
> So my second question is: Does bam allow factor variables? And if not,
> how should I model this heaping on the 15th day of the month instead?
> 
> With thanks,
> 
> Jade
> 
> On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
>>
>> I would not worry too much about high concurvity between variables like
>> temperature and time. This just reflects the fact that temperature has a
>> strong temporal pattern.
>>
>> I would also not be too worried about the low p-values on the k check.
>> The check only looks for pattern in the residuals when they are ordered
>> with respect to the variables of a smooth. When you have time series
>> data and some smooths involve time then it's hard not to pick up some
>> degree of residual auto-correlation, which you often would not want to
>> model with a higher rank smoother.
>>
>> The NAs  for the distributed lag terms just reflect the fact that there
>> is no obvious way to order the residuals w.r.t. the covariates for such
>> terms, so the simple check for residual pattern is not really possible.
>>
>> One simple approach is to fit using bam(...,discrete=TRUE) which will
>> let you specify an AR1 parameter to mop up some of the residual
>> auto-correlation without resorting to a high rank smooth that then does
>> all the work of the covariates as well. The AR1 parameter can be set by
>> looking at the ACF of the residuals of the model without this. You need
>> to look at the ACF of suitably standardized residuals to check how well
>> this has worked.
>>
>> best,
>>
>> Simon
>>
>> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
>>> Hello everyone,
>>>
>>> A few days ago I asked a question about concurvity in a GAM (the
>>> anologue of collinearity in a GLM) implemented in mgcv. I think my
>>> question was a bit unfocussed, so I am retrying again, but with
>>> additional information included about the autocorrelation function. I
>>> have also posted about this on Cross Validated. Given all the model
>>> output, it might make for easier
>>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
>>>
>>> As mentioned previously, I have problems with concurvity in my thesis
>>> research, and don't have access to a statistician who works with time
>>> series, GAMs or R. I'd be very grateful for any (partial) answer,
>>> however short. I'll gladly return the favour where I can! For really
>>> helpful input I'd be more than happy to offer co-authorship on
>>> publication. Deadlines are very close, and I'm heading towards having
>>> no results at all if I can't solve this concurvity issue :(
>>>
>>> I'm using GAMs to try to understand the relationship between deaths
>>> and heat-related variables (e.g. temperature and humidity), using
>>> daily time series over a 14-year period from a tropical, low-income
>>> country. My aim is to understand the relationship between these
>>> variables and deaths, rather than pure prediction performance.
>>>
>>> The GAMs include distributed lag models (set up as 7-column matrices,
>>> see code at bottom of post), since deaths may occur over several days
>>> following exposure.
>>>
>>> Simple GAMs with just time, lagged temperature and lagged
>>> precipitation (a potential confounder) show very high concurvity
>>> between lagged temperature and time, regardless of the many different
>>> ways I have tried to decompose time. The autocorrelation functions
>>> (ACF) however, shows values close to zero, only just about breaching
>>> the 'significance line' in a few instances. It does show patterning
>>> though, although the regularity is difficult to define.
>>>
>>> My questions are:
>>> 1) Should I be worried about the high concurvity, or can I ignore it
>>> given the mostly non-significant ACF? I've read dozens of
>>> heat-mortality modelling studies and none report on concurvity between
>>> weather variables and time (though one 2012 paper discussed
>>> autocorrelation).
>>>
>>> 2) If I cannot ignore it, what should I do to resolve it? Would
>>> including an autoregressive term be appropriate, and if so, where can
>>> I find a coded example of how to do this? I've also come across
>>> sequential regression][1]. Would this be more or less appropriate? If
>>> appropriate, a pointer to an example would be really appreciated!
>>>
>>> Some example GAMs are specified as follows:
>>> ```r
>>> conc38b <- gam(deaths~te(year, month, week, weekday,
>>> bs=c("cr","cc","cc","cc")) + heap +
>>>                         te(temp_max, lag, k=c(10, 3)) +
>>>                         te(precip_daily_total, lag, k=c(10, 3)),
>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
>>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
>>> 52.5), weekday = c(0, 6.5)))
>>> ```
>>> Concurvity for the above model between (temp_max, lag) and (year,
>>> month, week, weekday) is 0.91:
>>>
>>> ```r
>>> $worst
>>>                                       para te(year,month,week,weekday)
>>> te(temp_max,lag) te(precip_daily_total,lag)
>>> para                        1.000000e+00                1.125625e-29
>>>        0.3150073                  0.6666348
>>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
>>>        0.9060552                  0.6652313
>>> te(temp_max,lag)            3.152795e-01                8.998113e-01
>>>        1.0000000                  0.5781015
>>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
>>>        0.5805159                  1.0000000
>>> ```
>>>
>>> Output from ```gam.check()```:
>>> ```r
>>> Method: REML   Optimizer: outer newton
>>> full convergence after 16 iterations.
>>> Gradient range [-0.01467332,0.003096643]
>>> (score 8915.994 & scale 1).
>>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
>>> Model rank =  544 / 544
>>>
>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>> indicate that k is too low, especially if edf is close to k'.
>>>
>>>                                     k'      edf k-index p-value
>>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
>>> te(temp_max,lag)             29.0000   3.3681      NA      NA
>>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>> ```
>>>
>>> Some output from ```summary(conc38b)```:
>>> ```r
>>> Approximate significance of smooth terms:
>>>                                     edf Ref.df  Chi.sq p-value
>>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
>>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
>>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>
>>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
>>> -REML =   8916  Scale est. = 1         n = 5107
>>> ```
>>>
>>>
>>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
>>> pattern). They show peaks at 5 and 15, and then there seems to be a
>>> recurring pattern at multiples of approx. 30 (suggesting month is not
>>> modelled adequately?). Not sure what would cause the spikes at 5 and
>>> 15. There is heaping of deaths on the 15th day of each month, to which
>>> deaths with unknown date were allocated. This heaping was modelled
>>> with categorical variable/ factor ```heap``` with 169 levels (0 for
>>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
>>> heaping day over the 14-year period):
>>>
>>>     [2]: https://i.stack.imgur.com/FzKyM.png
>>>     [3]: https://i.stack.imgur.com/fE3aL.png
>>>
>>>
>>> I get an identical looking ACF when I decompose time into (year,
>>> month, monthday) as in model conc39 below, although concurvity between
>>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
>>>
>>> ```r
>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>>>                        te(temp_max, lag, k=c(10, 4)) +
>>>                        te(precip_daily_total, lag, k=c(10, 4)),
>>>                        data = dat, family = nb, method = 'REML', select = TRUE,
>>>                        knots = list(month = c(0.5, 12.5)))
>>> ```
>>> ```r
>>>
>>> Method: REML   Optimizer: outer newton
>>> full convergence after 14 iterations.
>>> Gradient range [-0.001578187,6.155096e-05]
>>> (score 8915.763 & scale 1).
>>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
>>> Model rank =  323 / 323
>>>
>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>> indicate that k is too low, especially if edf is close to k'.
>>>
>>>                                   k'     edf k-index p-value
>>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
>>> te(temp_max,lag)           39.0000  4.0875      NA      NA
>>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>> ```
>>> Some output from ```summary(conc39)```:
>>> ```r
>>> Approximate significance of smooth terms:
>>>                                   edf Ref.df  Chi.sq  p-value
>>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
>>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
>>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>
>>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
>>> -REML =   8915  Scale est. = 1         n = 5107
>>> ```
>>>
>>>
>>> ```r
>>> $worst
>>>                                      para te(year,month,monthday)
>>> te(temp_max,lag) te(precip_daily_total,lag)
>>> para                       1.000000e+00            3.261007e-31
>>> 0.3313549                  0.6666532
>>> te(year,month,monthday)    3.060763e-31            1.000000e+00
>>> 0.8266086                  0.5670777
>>> te(temp_max,lag)           3.331014e-01            8.225942e-01
>>> 1.0000000                  0.5840875
>>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
>>> 0.5939380                  1.0000000
>>> ```
>>>
>>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
>>> various choices for k or as ```s(time)``` with various k does not
>>> reduce concurvity either.
>>>
>>>
>>> The default approach in time series studies of heat-mortality is to
>>> model time with fixed df, generally between 7-10 df per year of data.
>>> I am, however, apprehensive about this approach because a) mortality
>>> profiles vary with locality due to sociodemographic and environmental
>>> characteristics and b) the choice of df is based on higher income
>>> countries (where nearly all these studies have been done) with
>>> different mortality profiles and so may not be appropriate for
>>> tropical, low-income countries.
>>>
>>> Although the approach of fixing (high) df does remove more temporal
>>> patterns from the ACF (see model and output below), concurvity between
>>> time and lagged temperature has now risen to 0.99! Moreover,
>>> temperature (which has been a consistent, highly significant predictor
>>> in every model of the tens (hundreds?) I have run, has now turned
>>> non-significant. I am guessing this is because time is now a very
>>> wiggly function that not only models/ removes seasonal variation, but
>>> also some of the day-to-day variation that is needed for the
>>> temperature smooth  :
>>>
>>> ```r
>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>>>                         te(temp_max, lag, k=c(10,3)) +
>>>                         te(precip_daily_total, lag, k=c(10,3)),
>>>                         data = dat, family = nb, method = 'REML', select = TRUE)
>>> ```
>>> Output from ```gam.check(conc20a, rep = 1000)```:
>>>
>>> ```r
>>> Method: REML   Optimizer: outer newton
>>> full convergence after 9 iterations.
>>> Gradient range [-0.0008983099,9.546022e-05]
>>> (score 8750.13 & scale 1).
>>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
>>> Model rank =  336 / 336
>>>
>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>> indicate that k is too low, especially if edf is close to k'.
>>>
>>>                                    k'      edf k-index p-value
>>> s(time)                    111.0000 111.0000    0.98    0.56
>>> te(temp_max,lag)            29.0000   0.6548      NA      NA
>>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
>>> ```
>>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
>>>
>>> ```r
>>>                                      para      s(time) te(temp_max,lag)
>>> te(precip_daily_total,lag)
>>> para                       1.000000e+00 2.462064e-19        0.3165236
>>>                   0.6666348
>>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
>>>                   0.6879284
>>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
>>>                   0.5788711
>>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
>>>                   1.0000000
>>>
>>> ```
>>>
>>> Some output from ```summary(conc20a)```:
>>> ```r
>>> Approximate significance of smooth terms:
>>>                                    edf Ref.df  Chi.sq p-value
>>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
>>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
>>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>
>>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
>>> -REML = 8750.1  Scale est. = 1         n = 5107
>>> ```
>>>
>>> ACF functions:
>>>
>>> [4]: https://i.stack.imgur.com/7nbXS.png
>>> [5]: https://i.stack.imgur.com/pNnZU.png
>>>
>>> Data can be found on my [GitHub][6] site in the file
>>> [data_cross_validated_post2.rds][7]. A csv version is also available.
>>> This is my code:
>>>
>>> ```r
>>> library(readr)
>>> library(mgcv)
>>>
>>> df <- read_rds("data_crossvalidated_post2.rds")
>>>
>>> # Create matrices for lagged weather variables (6 day lags) based on
>>> example by Simon Wood
>>> # in his 2017 book ("Generalized additive models: an introduction with
>>> R", p. 349) and
>>> # gamair package documentation
>>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
>>>
>>> lagard <- function(x,n.lag=7) {
>>> n <- length(x); X <- matrix(NA,n,n.lag)
>>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
>>> X
>>> }
>>>
>>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
>>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
>>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
>>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
>>> = df$precip_hourly_dailysum)
>>> dat$temp_max <- lagard(df$temp_max)
>>> dat$temp_min <- lagard(df$temp_min)
>>> dat$temp_mean <- lagard(df$temp_mean)
>>> dat$wbgt_max <- lagard(df$wbgt_max)
>>> dat$wbgt_mean <- lagard(df$wbgt_mean)
>>> dat$wbgt_min <- lagard(df$wbgt_min)
>>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
>>> dat$sh_mean <- lagard(df$sh_mean)
>>> dat$solar_mean <- lagard(df$solar_mean)
>>> dat$wind2m_mean <- lagard(df$wind2m_mean)
>>> dat$sh_max <- lagard(df$sh_max)
>>> dat$solar_max <- lagard(df$solar_max)
>>> dat$wind2m_max <- lagard(df$wind2m_max)
>>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
>>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
>>> dat$precip_hourly <- lagard(df$precip_hourly)
>>> dat$precip_daily_total <- lagard( df$precip_daily_total)
>>> dat$temp <- lagard(df$temp)
>>> dat$sh <- lagard(df$sh)
>>> dat$rh <- lagard(df$rh)
>>> dat$solar <- lagard(df$solar)
>>> dat$wind2m <- lagard(df$wind2m)
>>>
>>>
>>> conc38b <- gam(deaths~te(year, month, week, weekday,
>>> bs=c("cr","cc","cc","cc")) + heap +
>>>                         te(temp_max, lag, k=c(10, 3)) +
>>>                         te(precip_daily_total, lag, k=c(10, 3)),
>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
>>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
>>> 52.5), weekday = c(0, 6.5)))
>>>
>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>>>                        te(temp_max, lag, k=c(10, 4)) +
>>>                        te(precip_daily_total, lag, k=c(10, 4)),
>>>                        data = dat, family = nb, method = 'REML', select = TRUE,
>>>                        knots = list(month = c(0.5, 12.5)))
>>>
>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>>>                         te(temp_max, lag, k=c(10,3)) +
>>>                         te(precip_daily_total, lag, k=c(10,3)),
>>>                         data = dat, family = nb, method = 'REML', select = TRUE)
>>>
>>> ```
>>> Thank you if you've read this far!! :-))
>>>
>>>     [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
>>>     [2]: https://i.stack.imgur.com/FzKyM.png
>>>     [3]: https://i.stack.imgur.com/fE3aL.png
>>>     [4]: https://i.stack.imgur.com/7nbXS.png
>>>     [5]: https://i.stack.imgur.com/pNnZU.png
>>>     [6]: https://github.com/JadeShodan/heat-mortality
>>>     [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Simon Wood, School of Mathematics, University of Edinburgh,
>> https://www.maths.ed.ac.uk/~swood34/
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From @|mon@wood @end|ng |rom b@th@edu  Thu Jun  9 16:24:51 2022
From: @|mon@wood @end|ng |rom b@th@edu (Simon Wood)
Date: Thu, 9 Jun 2022 15:24:51 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k-g=M41cxom6DVdTunFjgZ6VfKobqW__Mq351pDpY22PQ@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-g=M41cxom6DVdTunFjgZ6VfKobqW__Mq351pDpY22PQ@mail.gmail.com>
Message-ID: <f371005e-6f57-2007-f859-d66595a19c0f@bath.edu>


On 09/06/2022 12:30, jade.shodan at googlemail.com wrote:
> Hi Simon,
>
> Thanks so much for this!! (Apologies if this is a double posting. I
> seem to have a problem getting messages through to the list).
>
> I have two follow up questions, if you don't mind.
>
> 1. Does including an autoregressive term not adjust away part of the
> effect of the response in a distributed lag model (where the outcome
> accumulates over time)?

- the hope is that it approximately deals with short timescale stuff 
without interfering with the longer timescales to the same extent as a 
high rank smooth would.

> 2. I've tried to fit a model using bam (just a first attempt without
> AR term), but including the factor variable heap creates errors:
>
> bam0 <- bam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
>                                       te(temp_max, lag, k=c(8, 3)) +
>                                       te(precip_daily_total, lag, k=c(8, 3)),
>                                      data = dat, family = nb, method =
> 'fREML', select = TRUE, discrete = TRUE,
>                                      knots = list(month = c(0.5, 12.5),
> week = c(0.5, 52.5), weekday = c(0, 6.5)))
>
> This model results in errors:
>
> Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
>    step failure in theta estimation
> Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> object$prior.weights)) :
>    NaNs produced
>
Did it actually fail, or simply generate warnings?

'bam' handles factors, but if I understand your model right there is one 
free parameter for each observation falling on the 15th, so that you 
will fit data for those days exactly (and might just as well have 
dropped them, for all the information they contribute to the rest of the 
model). If you want a structure like this, I'd be inclined to make the 
heap variable random, something like...

aheap <- heap!="0"

heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap day 
+ a r.e. for each heap day - no effect on non-heap days

best,

Simon

> Including heap as as.numeric(heap) runs the model without error
> messages or warnings, but model diagnostics look terrible, and it also
> doesn't make sense (to me) to make heap a numeric. The factor variable
> heap (with 169 levels) codes the fact that all deaths for which no
> date was known, were registered on the 15th day of each month. I've
> coded all non-heaping days as 0. All heaping days were coded as a
> value between 1-168. The time series spans 14 years, so a heaping day
> in each month results in 14*12 levels = 168, plus one level for
> non-heaping days.
>
> So my second question is: Does bam allow factor variables? And if not,
> how should I model this heaping on the 15th day of the month instead?
>
> With thanks,
>
> Jade
>
> On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
>> I would not worry too much about high concurvity between variables like
>> temperature and time. This just reflects the fact that temperature has a
>> strong temporal pattern.
>>
>> I would also not be too worried about the low p-values on the k check.
>> The check only looks for pattern in the residuals when they are ordered
>> with respect to the variables of a smooth. When you have time series
>> data and some smooths involve time then it's hard not to pick up some
>> degree of residual auto-correlation, which you often would not want to
>> model with a higher rank smoother.
>>
>> The NAs  for the distributed lag terms just reflect the fact that there
>> is no obvious way to order the residuals w.r.t. the covariates for such
>> terms, so the simple check for residual pattern is not really possible.
>>
>> One simple approach is to fit using bam(...,discrete=TRUE) which will
>> let you specify an AR1 parameter to mop up some of the residual
>> auto-correlation without resorting to a high rank smooth that then does
>> all the work of the covariates as well. The AR1 parameter can be set by
>> looking at the ACF of the residuals of the model without this. You need
>> to look at the ACF of suitably standardized residuals to check how well
>> this has worked.
>>
>> best,
>>
>> Simon
>>
>> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
>>> Hello everyone,
>>>
>>> A few days ago I asked a question about concurvity in a GAM (the
>>> anologue of collinearity in a GLM) implemented in mgcv. I think my
>>> question was a bit unfocussed, so I am retrying again, but with
>>> additional information included about the autocorrelation function. I
>>> have also posted about this on Cross Validated. Given all the model
>>> output, it might make for easier
>>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
>>>
>>> As mentioned previously, I have problems with concurvity in my thesis
>>> research, and don't have access to a statistician who works with time
>>> series, GAMs or R. I'd be very grateful for any (partial) answer,
>>> however short. I'll gladly return the favour where I can! For really
>>> helpful input I'd be more than happy to offer co-authorship on
>>> publication. Deadlines are very close, and I'm heading towards having
>>> no results at all if I can't solve this concurvity issue :(
>>>
>>> I'm using GAMs to try to understand the relationship between deaths
>>> and heat-related variables (e.g. temperature and humidity), using
>>> daily time series over a 14-year period from a tropical, low-income
>>> country. My aim is to understand the relationship between these
>>> variables and deaths, rather than pure prediction performance.
>>>
>>> The GAMs include distributed lag models (set up as 7-column matrices,
>>> see code at bottom of post), since deaths may occur over several days
>>> following exposure.
>>>
>>> Simple GAMs with just time, lagged temperature and lagged
>>> precipitation (a potential confounder) show very high concurvity
>>> between lagged temperature and time, regardless of the many different
>>> ways I have tried to decompose time. The autocorrelation functions
>>> (ACF) however, shows values close to zero, only just about breaching
>>> the 'significance line' in a few instances. It does show patterning
>>> though, although the regularity is difficult to define.
>>>
>>> My questions are:
>>> 1) Should I be worried about the high concurvity, or can I ignore it
>>> given the mostly non-significant ACF? I've read dozens of
>>> heat-mortality modelling studies and none report on concurvity between
>>> weather variables and time (though one 2012 paper discussed
>>> autocorrelation).
>>>
>>> 2) If I cannot ignore it, what should I do to resolve it? Would
>>> including an autoregressive term be appropriate, and if so, where can
>>> I find a coded example of how to do this? I've also come across
>>> sequential regression][1]. Would this be more or less appropriate? If
>>> appropriate, a pointer to an example would be really appreciated!
>>>
>>> Some example GAMs are specified as follows:
>>> ```r
>>> conc38b <- gam(deaths~te(year, month, week, weekday,
>>> bs=c("cr","cc","cc","cc")) + heap +
>>>                         te(temp_max, lag, k=c(10, 3)) +
>>>                         te(precip_daily_total, lag, k=c(10, 3)),
>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
>>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
>>> 52.5), weekday = c(0, 6.5)))
>>> ```
>>> Concurvity for the above model between (temp_max, lag) and (year,
>>> month, week, weekday) is 0.91:
>>>
>>> ```r
>>> $worst
>>>                                       para te(year,month,week,weekday)
>>> te(temp_max,lag) te(precip_daily_total,lag)
>>> para                        1.000000e+00                1.125625e-29
>>>        0.3150073                  0.6666348
>>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
>>>        0.9060552                  0.6652313
>>> te(temp_max,lag)            3.152795e-01                8.998113e-01
>>>        1.0000000                  0.5781015
>>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
>>>        0.5805159                  1.0000000
>>> ```
>>>
>>> Output from ```gam.check()```:
>>> ```r
>>> Method: REML   Optimizer: outer newton
>>> full convergence after 16 iterations.
>>> Gradient range [-0.01467332,0.003096643]
>>> (score 8915.994 & scale 1).
>>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
>>> Model rank =  544 / 544
>>>
>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>> indicate that k is too low, especially if edf is close to k'.
>>>
>>>                                     k'      edf k-index p-value
>>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
>>> te(temp_max,lag)             29.0000   3.3681      NA      NA
>>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>> ```
>>>
>>> Some output from ```summary(conc38b)```:
>>> ```r
>>> Approximate significance of smooth terms:
>>>                                     edf Ref.df  Chi.sq p-value
>>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
>>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
>>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>
>>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
>>> -REML =   8916  Scale est. = 1         n = 5107
>>> ```
>>>
>>>
>>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
>>> pattern). They show peaks at 5 and 15, and then there seems to be a
>>> recurring pattern at multiples of approx. 30 (suggesting month is not
>>> modelled adequately?). Not sure what would cause the spikes at 5 and
>>> 15. There is heaping of deaths on the 15th day of each month, to which
>>> deaths with unknown date were allocated. This heaping was modelled
>>> with categorical variable/ factor ```heap``` with 169 levels (0 for
>>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
>>> heaping day over the 14-year period):
>>>
>>>     [2]: https://i.stack.imgur.com/FzKyM.png
>>>     [3]: https://i.stack.imgur.com/fE3aL.png
>>>
>>>
>>> I get an identical looking ACF when I decompose time into (year,
>>> month, monthday) as in model conc39 below, although concurvity between
>>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
>>>
>>> ```r
>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>>>                        te(temp_max, lag, k=c(10, 4)) +
>>>                        te(precip_daily_total, lag, k=c(10, 4)),
>>>                        data = dat, family = nb, method = 'REML', select = TRUE,
>>>                        knots = list(month = c(0.5, 12.5)))
>>> ```
>>> ```r
>>>
>>> Method: REML   Optimizer: outer newton
>>> full convergence after 14 iterations.
>>> Gradient range [-0.001578187,6.155096e-05]
>>> (score 8915.763 & scale 1).
>>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
>>> Model rank =  323 / 323
>>>
>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>> indicate that k is too low, especially if edf is close to k'.
>>>
>>>                                   k'     edf k-index p-value
>>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
>>> te(temp_max,lag)           39.0000  4.0875      NA      NA
>>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>> ```
>>> Some output from ```summary(conc39)```:
>>> ```r
>>> Approximate significance of smooth terms:
>>>                                   edf Ref.df  Chi.sq  p-value
>>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
>>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
>>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>
>>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
>>> -REML =   8915  Scale est. = 1         n = 5107
>>> ```
>>>
>>>
>>> ```r
>>> $worst
>>>                                      para te(year,month,monthday)
>>> te(temp_max,lag) te(precip_daily_total,lag)
>>> para                       1.000000e+00            3.261007e-31
>>> 0.3313549                  0.6666532
>>> te(year,month,monthday)    3.060763e-31            1.000000e+00
>>> 0.8266086                  0.5670777
>>> te(temp_max,lag)           3.331014e-01            8.225942e-01
>>> 1.0000000                  0.5840875
>>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
>>> 0.5939380                  1.0000000
>>> ```
>>>
>>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
>>> various choices for k or as ```s(time)``` with various k does not
>>> reduce concurvity either.
>>>
>>>
>>> The default approach in time series studies of heat-mortality is to
>>> model time with fixed df, generally between 7-10 df per year of data.
>>> I am, however, apprehensive about this approach because a) mortality
>>> profiles vary with locality due to sociodemographic and environmental
>>> characteristics and b) the choice of df is based on higher income
>>> countries (where nearly all these studies have been done) with
>>> different mortality profiles and so may not be appropriate for
>>> tropical, low-income countries.
>>>
>>> Although the approach of fixing (high) df does remove more temporal
>>> patterns from the ACF (see model and output below), concurvity between
>>> time and lagged temperature has now risen to 0.99! Moreover,
>>> temperature (which has been a consistent, highly significant predictor
>>> in every model of the tens (hundreds?) I have run, has now turned
>>> non-significant. I am guessing this is because time is now a very
>>> wiggly function that not only models/ removes seasonal variation, but
>>> also some of the day-to-day variation that is needed for the
>>> temperature smooth  :
>>>
>>> ```r
>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>>>                         te(temp_max, lag, k=c(10,3)) +
>>>                         te(precip_daily_total, lag, k=c(10,3)),
>>>                         data = dat, family = nb, method = 'REML', select = TRUE)
>>> ```
>>> Output from ```gam.check(conc20a, rep = 1000)```:
>>>
>>> ```r
>>> Method: REML   Optimizer: outer newton
>>> full convergence after 9 iterations.
>>> Gradient range [-0.0008983099,9.546022e-05]
>>> (score 8750.13 & scale 1).
>>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
>>> Model rank =  336 / 336
>>>
>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>> indicate that k is too low, especially if edf is close to k'.
>>>
>>>                                    k'      edf k-index p-value
>>> s(time)                    111.0000 111.0000    0.98    0.56
>>> te(temp_max,lag)            29.0000   0.6548      NA      NA
>>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
>>> ```
>>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
>>>
>>> ```r
>>>                                      para      s(time) te(temp_max,lag)
>>> te(precip_daily_total,lag)
>>> para                       1.000000e+00 2.462064e-19        0.3165236
>>>                   0.6666348
>>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
>>>                   0.6879284
>>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
>>>                   0.5788711
>>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
>>>                   1.0000000
>>>
>>> ```
>>>
>>> Some output from ```summary(conc20a)```:
>>> ```r
>>> Approximate significance of smooth terms:
>>>                                    edf Ref.df  Chi.sq p-value
>>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
>>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
>>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>
>>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
>>> -REML = 8750.1  Scale est. = 1         n = 5107
>>> ```
>>>
>>> ACF functions:
>>>
>>> [4]: https://i.stack.imgur.com/7nbXS.png
>>> [5]: https://i.stack.imgur.com/pNnZU.png
>>>
>>> Data can be found on my [GitHub][6] site in the file
>>> [data_cross_validated_post2.rds][7]. A csv version is also available.
>>> This is my code:
>>>
>>> ```r
>>> library(readr)
>>> library(mgcv)
>>>
>>> df <- read_rds("data_crossvalidated_post2.rds")
>>>
>>> # Create matrices for lagged weather variables (6 day lags) based on
>>> example by Simon Wood
>>> # in his 2017 book ("Generalized additive models: an introduction with
>>> R", p. 349) and
>>> # gamair package documentation
>>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
>>>
>>> lagard <- function(x,n.lag=7) {
>>> n <- length(x); X <- matrix(NA,n,n.lag)
>>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
>>> X
>>> }
>>>
>>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
>>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
>>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
>>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
>>> = df$precip_hourly_dailysum)
>>> dat$temp_max <- lagard(df$temp_max)
>>> dat$temp_min <- lagard(df$temp_min)
>>> dat$temp_mean <- lagard(df$temp_mean)
>>> dat$wbgt_max <- lagard(df$wbgt_max)
>>> dat$wbgt_mean <- lagard(df$wbgt_mean)
>>> dat$wbgt_min <- lagard(df$wbgt_min)
>>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
>>> dat$sh_mean <- lagard(df$sh_mean)
>>> dat$solar_mean <- lagard(df$solar_mean)
>>> dat$wind2m_mean <- lagard(df$wind2m_mean)
>>> dat$sh_max <- lagard(df$sh_max)
>>> dat$solar_max <- lagard(df$solar_max)
>>> dat$wind2m_max <- lagard(df$wind2m_max)
>>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
>>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
>>> dat$precip_hourly <- lagard(df$precip_hourly)
>>> dat$precip_daily_total <- lagard( df$precip_daily_total)
>>> dat$temp <- lagard(df$temp)
>>> dat$sh <- lagard(df$sh)
>>> dat$rh <- lagard(df$rh)
>>> dat$solar <- lagard(df$solar)
>>> dat$wind2m <- lagard(df$wind2m)
>>>
>>>
>>> conc38b <- gam(deaths~te(year, month, week, weekday,
>>> bs=c("cr","cc","cc","cc")) + heap +
>>>                         te(temp_max, lag, k=c(10, 3)) +
>>>                         te(precip_daily_total, lag, k=c(10, 3)),
>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
>>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
>>> 52.5), weekday = c(0, 6.5)))
>>>
>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>>>                        te(temp_max, lag, k=c(10, 4)) +
>>>                        te(precip_daily_total, lag, k=c(10, 4)),
>>>                        data = dat, family = nb, method = 'REML', select = TRUE,
>>>                        knots = list(month = c(0.5, 12.5)))
>>>
>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>>>                         te(temp_max, lag, k=c(10,3)) +
>>>                         te(precip_daily_total, lag, k=c(10,3)),
>>>                         data = dat, family = nb, method = 'REML', select = TRUE)
>>>
>>> ```
>>> Thank you if you've read this far!! :-))
>>>
>>>     [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
>>>     [2]: https://i.stack.imgur.com/FzKyM.png
>>>     [3]: https://i.stack.imgur.com/fE3aL.png
>>>     [4]: https://i.stack.imgur.com/7nbXS.png
>>>     [5]: https://i.stack.imgur.com/pNnZU.png
>>>     [6]: https://github.com/JadeShodan/heat-mortality
>>>     [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> --
>> Simon Wood, School of Mathematics, University of Edinburgh,
>> https://www.maths.ed.ac.uk/~swood34/
>>
-- 
Simon Wood, School of Mathematics, University of Edinburgh,
https://www.maths.ed.ac.uk/~swood34/


From j@de@shod@@ m@iii@g oii googiem@ii@com  Thu Jun  9 13:30:59 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Thu, 9 Jun 2022 12:30:59 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
Message-ID: <CANg3_k-g=M41cxom6DVdTunFjgZ6VfKobqW__Mq351pDpY22PQ@mail.gmail.com>

Hi Simon,

Thanks so much for this!! (Apologies if this is a double posting. I
seem to have a problem getting messages through to the list).

I have two follow up questions, if you don't mind.

1. Does including an autoregressive term not adjust away part of the
effect of the response in a distributed lag model (where the outcome
accumulates over time)?
2. I've tried to fit a model using bam (just a first attempt without
AR term), but including the factor variable heap creates errors:

bam0 <- bam(deaths~te(year, month, week, weekday,
bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
                                     te(temp_max, lag, k=c(8, 3)) +
                                     te(precip_daily_total, lag, k=c(8, 3)),
                                    data = dat, family = nb, method =
'fREML', select = TRUE, discrete = TRUE,
                                    knots = list(month = c(0.5, 12.5),
week = c(0.5, 52.5), weekday = c(0, 6.5)))

This model results in errors:

Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
  step failure in theta estimation
Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
object$prior.weights)) :
  NaNs produced


Including heap as as.numeric(heap) runs the model without error
messages or warnings, but model diagnostics look terrible, and it also
doesn't make sense (to me) to make heap a numeric. The factor variable
heap (with 169 levels) codes the fact that all deaths for which no
date was known, were registered on the 15th day of each month. I've
coded all non-heaping days as 0. All heaping days were coded as a
value between 1-168. The time series spans 14 years, so a heaping day
in each month results in 14*12 levels = 168, plus one level for
non-heaping days.

So my second question is: Does bam allow factor variables? And if not,
how should I model this heaping on the 15th day of the month instead?

With thanks,

Jade

On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
>
> I would not worry too much about high concurvity between variables like
> temperature and time. This just reflects the fact that temperature has a
> strong temporal pattern.
>
> I would also not be too worried about the low p-values on the k check.
> The check only looks for pattern in the residuals when they are ordered
> with respect to the variables of a smooth. When you have time series
> data and some smooths involve time then it's hard not to pick up some
> degree of residual auto-correlation, which you often would not want to
> model with a higher rank smoother.
>
> The NAs  for the distributed lag terms just reflect the fact that there
> is no obvious way to order the residuals w.r.t. the covariates for such
> terms, so the simple check for residual pattern is not really possible.
>
> One simple approach is to fit using bam(...,discrete=TRUE) which will
> let you specify an AR1 parameter to mop up some of the residual
> auto-correlation without resorting to a high rank smooth that then does
> all the work of the covariates as well. The AR1 parameter can be set by
> looking at the ACF of the residuals of the model without this. You need
> to look at the ACF of suitably standardized residuals to check how well
> this has worked.
>
> best,
>
> Simon
>
> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> > Hello everyone,
> >
> > A few days ago I asked a question about concurvity in a GAM (the
> > anologue of collinearity in a GLM) implemented in mgcv. I think my
> > question was a bit unfocussed, so I am retrying again, but with
> > additional information included about the autocorrelation function. I
> > have also posted about this on Cross Validated. Given all the model
> > output, it might make for easier
> > reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> >
> > As mentioned previously, I have problems with concurvity in my thesis
> > research, and don't have access to a statistician who works with time
> > series, GAMs or R. I'd be very grateful for any (partial) answer,
> > however short. I'll gladly return the favour where I can! For really
> > helpful input I'd be more than happy to offer co-authorship on
> > publication. Deadlines are very close, and I'm heading towards having
> > no results at all if I can't solve this concurvity issue :(
> >
> > I'm using GAMs to try to understand the relationship between deaths
> > and heat-related variables (e.g. temperature and humidity), using
> > daily time series over a 14-year period from a tropical, low-income
> > country. My aim is to understand the relationship between these
> > variables and deaths, rather than pure prediction performance.
> >
> > The GAMs include distributed lag models (set up as 7-column matrices,
> > see code at bottom of post), since deaths may occur over several days
> > following exposure.
> >
> > Simple GAMs with just time, lagged temperature and lagged
> > precipitation (a potential confounder) show very high concurvity
> > between lagged temperature and time, regardless of the many different
> > ways I have tried to decompose time. The autocorrelation functions
> > (ACF) however, shows values close to zero, only just about breaching
> > the 'significance line' in a few instances. It does show patterning
> > though, although the regularity is difficult to define.
> >
> > My questions are:
> > 1) Should I be worried about the high concurvity, or can I ignore it
> > given the mostly non-significant ACF? I've read dozens of
> > heat-mortality modelling studies and none report on concurvity between
> > weather variables and time (though one 2012 paper discussed
> > autocorrelation).
> >
> > 2) If I cannot ignore it, what should I do to resolve it? Would
> > including an autoregressive term be appropriate, and if so, where can
> > I find a coded example of how to do this? I've also come across
> > sequential regression][1]. Would this be more or less appropriate? If
> > appropriate, a pointer to an example would be really appreciated!
> >
> > Some example GAMs are specified as follows:
> > ```r
> > conc38b <- gam(deaths~te(year, month, week, weekday,
> > bs=c("cr","cc","cc","cc")) + heap +
> >                        te(temp_max, lag, k=c(10, 3)) +
> >                        te(precip_daily_total, lag, k=c(10, 3)),
> >                        data = dat, family = nb, method = 'REML', select = TRUE,
> >                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> > 52.5), weekday = c(0, 6.5)))
> > ```
> > Concurvity for the above model between (temp_max, lag) and (year,
> > month, week, weekday) is 0.91:
> >
> > ```r
> > $worst
> >                                      para te(year,month,week,weekday)
> > te(temp_max,lag) te(precip_daily_total,lag)
> > para                        1.000000e+00                1.125625e-29
> >       0.3150073                  0.6666348
> > te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> >       0.9060552                  0.6652313
> > te(temp_max,lag)            3.152795e-01                8.998113e-01
> >       1.0000000                  0.5781015
> > te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> >       0.5805159                  1.0000000
> > ```
> >
> > Output from ```gam.check()```:
> > ```r
> > Method: REML   Optimizer: outer newton
> > full convergence after 16 iterations.
> > Gradient range [-0.01467332,0.003096643]
> > (score 8915.994 & scale 1).
> > Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> > Model rank =  544 / 544
> >
> > Basis dimension (k) checking results. Low p-value (k-index<1) may
> > indicate that k is too low, especially if edf is close to k'.
> >
> >                                    k'      edf k-index p-value
> > te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> > te(temp_max,lag)             29.0000   3.3681      NA      NA
> > te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > ```
> >
> > Some output from ```summary(conc38b)```:
> > ```r
> > Approximate significance of smooth terms:
> >                                    edf Ref.df  Chi.sq p-value
> > te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> > te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> > te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >
> > R-sq.(adj) =  0.839   Deviance explained = 53.3%
> > -REML =   8916  Scale est. = 1         n = 5107
> > ```
> >
> >
> > Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> > pattern). They show peaks at 5 and 15, and then there seems to be a
> > recurring pattern at multiples of approx. 30 (suggesting month is not
> > modelled adequately?). Not sure what would cause the spikes at 5 and
> > 15. There is heaping of deaths on the 15th day of each month, to which
> > deaths with unknown date were allocated. This heaping was modelled
> > with categorical variable/ factor ```heap``` with 169 levels (0 for
> > all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> > heaping day over the 14-year period):
> >
> >    [2]: https://i.stack.imgur.com/FzKyM.png
> >    [3]: https://i.stack.imgur.com/fE3aL.png
> >
> >
> > I get an identical looking ACF when I decompose time into (year,
> > month, monthday) as in model conc39 below, although concurvity between
> > (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> >
> > ```r
> > conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >                       te(temp_max, lag, k=c(10, 4)) +
> >                       te(precip_daily_total, lag, k=c(10, 4)),
> >                       data = dat, family = nb, method = 'REML', select = TRUE,
> >                       knots = list(month = c(0.5, 12.5)))
> > ```
> > ```r
> >
> > Method: REML   Optimizer: outer newton
> > full convergence after 14 iterations.
> > Gradient range [-0.001578187,6.155096e-05]
> > (score 8915.763 & scale 1).
> > Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> > Model rank =  323 / 323
> >
> > Basis dimension (k) checking results. Low p-value (k-index<1) may
> > indicate that k is too low, especially if edf is close to k'.
> >
> >                                  k'     edf k-index p-value
> > te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> > te(temp_max,lag)           39.0000  4.0875      NA      NA
> > te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > ```
> > Some output from ```summary(conc39)```:
> > ```r
> > Approximate significance of smooth terms:
> >                                  edf Ref.df  Chi.sq  p-value
> > te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> > te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> > te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >
> > R-sq.(adj) =  0.839   Deviance explained = 53.8%
> > -REML =   8915  Scale est. = 1         n = 5107
> > ```
> >
> >
> > ```r
> > $worst
> >                                     para te(year,month,monthday)
> > te(temp_max,lag) te(precip_daily_total,lag)
> > para                       1.000000e+00            3.261007e-31
> > 0.3313549                  0.6666532
> > te(year,month,monthday)    3.060763e-31            1.000000e+00
> > 0.8266086                  0.5670777
> > te(temp_max,lag)           3.331014e-01            8.225942e-01
> > 1.0000000                  0.5840875
> > te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> > 0.5939380                  1.0000000
> > ```
> >
> > Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> > various choices for k or as ```s(time)``` with various k does not
> > reduce concurvity either.
> >
> >
> > The default approach in time series studies of heat-mortality is to
> > model time with fixed df, generally between 7-10 df per year of data.
> > I am, however, apprehensive about this approach because a) mortality
> > profiles vary with locality due to sociodemographic and environmental
> > characteristics and b) the choice of df is based on higher income
> > countries (where nearly all these studies have been done) with
> > different mortality profiles and so may not be appropriate for
> > tropical, low-income countries.
> >
> > Although the approach of fixing (high) df does remove more temporal
> > patterns from the ACF (see model and output below), concurvity between
> > time and lagged temperature has now risen to 0.99! Moreover,
> > temperature (which has been a consistent, highly significant predictor
> > in every model of the tens (hundreds?) I have run, has now turned
> > non-significant. I am guessing this is because time is now a very
> > wiggly function that not only models/ removes seasonal variation, but
> > also some of the day-to-day variation that is needed for the
> > temperature smooth  :
> >
> > ```r
> > conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >                        te(temp_max, lag, k=c(10,3)) +
> >                        te(precip_daily_total, lag, k=c(10,3)),
> >                        data = dat, family = nb, method = 'REML', select = TRUE)
> > ```
> > Output from ```gam.check(conc20a, rep = 1000)```:
> >
> > ```r
> > Method: REML   Optimizer: outer newton
> > full convergence after 9 iterations.
> > Gradient range [-0.0008983099,9.546022e-05]
> > (score 8750.13 & scale 1).
> > Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> > Model rank =  336 / 336
> >
> > Basis dimension (k) checking results. Low p-value (k-index<1) may
> > indicate that k is too low, especially if edf is close to k'.
> >
> >                                   k'      edf k-index p-value
> > s(time)                    111.0000 111.0000    0.98    0.56
> > te(temp_max,lag)            29.0000   0.6548      NA      NA
> > te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> > ```
> > Output from ```concurvity(conc20a, full=FALSE)$worst```:
> >
> > ```r
> >                                     para      s(time) te(temp_max,lag)
> > te(precip_daily_total,lag)
> > para                       1.000000e+00 2.462064e-19        0.3165236
> >                  0.6666348
> > s(time)                    2.462398e-19 1.000000e+00        0.9930674
> >                  0.6879284
> > te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> >                  0.5788711
> > te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> >                  1.0000000
> >
> > ```
> >
> > Some output from ```summary(conc20a)```:
> > ```r
> > Approximate significance of smooth terms:
> >                                   edf Ref.df  Chi.sq p-value
> > s(time)                    1.110e+02    111 419.375  <2e-16 ***
> > te(temp_max,lag)           6.548e-01     27   0.895   0.249
> > te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >
> > R-sq.(adj) =  0.843   Deviance explained = 56.1%
> > -REML = 8750.1  Scale est. = 1         n = 5107
> > ```
> >
> > ACF functions:
> >
> > [4]: https://i.stack.imgur.com/7nbXS.png
> > [5]: https://i.stack.imgur.com/pNnZU.png
> >
> > Data can be found on my [GitHub][6] site in the file
> > [data_cross_validated_post2.rds][7]. A csv version is also available.
> > This is my code:
> >
> > ```r
> > library(readr)
> > library(mgcv)
> >
> > df <- read_rds("data_crossvalidated_post2.rds")
> >
> > # Create matrices for lagged weather variables (6 day lags) based on
> > example by Simon Wood
> > # in his 2017 book ("Generalized additive models: an introduction with
> > R", p. 349) and
> > # gamair package documentation
> > (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> >
> > lagard <- function(x,n.lag=7) {
> > n <- length(x); X <- matrix(NA,n,n.lag)
> > for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > X
> > }
> >
> > dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> > deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> > weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> > df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> > = df$precip_hourly_dailysum)
> > dat$temp_max <- lagard(df$temp_max)
> > dat$temp_min <- lagard(df$temp_min)
> > dat$temp_mean <- lagard(df$temp_mean)
> > dat$wbgt_max <- lagard(df$wbgt_max)
> > dat$wbgt_mean <- lagard(df$wbgt_mean)
> > dat$wbgt_min <- lagard(df$wbgt_min)
> > dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> > dat$sh_mean <- lagard(df$sh_mean)
> > dat$solar_mean <- lagard(df$solar_mean)
> > dat$wind2m_mean <- lagard(df$wind2m_mean)
> > dat$sh_max <- lagard(df$sh_max)
> > dat$solar_max <- lagard(df$solar_max)
> > dat$wind2m_max <- lagard(df$wind2m_max)
> > dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> > dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> > dat$precip_hourly <- lagard(df$precip_hourly)
> > dat$precip_daily_total <- lagard( df$precip_daily_total)
> > dat$temp <- lagard(df$temp)
> > dat$sh <- lagard(df$sh)
> > dat$rh <- lagard(df$rh)
> > dat$solar <- lagard(df$solar)
> > dat$wind2m <- lagard(df$wind2m)
> >
> >
> > conc38b <- gam(deaths~te(year, month, week, weekday,
> > bs=c("cr","cc","cc","cc")) + heap +
> >                        te(temp_max, lag, k=c(10, 3)) +
> >                        te(precip_daily_total, lag, k=c(10, 3)),
> >                        data = dat, family = nb, method = 'REML', select = TRUE,
> >                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> > 52.5), weekday = c(0, 6.5)))
> >
> > conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >                       te(temp_max, lag, k=c(10, 4)) +
> >                       te(precip_daily_total, lag, k=c(10, 4)),
> >                       data = dat, family = nb, method = 'REML', select = TRUE,
> >                       knots = list(month = c(0.5, 12.5)))
> >
> > conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >                        te(temp_max, lag, k=c(10,3)) +
> >                        te(precip_daily_total, lag, k=c(10,3)),
> >                        data = dat, family = nb, method = 'REML', select = TRUE)
> >
> > ```
> > Thank you if you've read this far!! :-))
> >
> >    [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> >    [2]: https://i.stack.imgur.com/FzKyM.png
> >    [3]: https://i.stack.imgur.com/fE3aL.png
> >    [4]: https://i.stack.imgur.com/7nbXS.png
> >    [5]: https://i.stack.imgur.com/pNnZU.png
> >    [6]: https://github.com/JadeShodan/heat-mortality
> >    [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Simon Wood, School of Mathematics, University of Edinburgh,
> https://www.maths.ed.ac.uk/~swood34/
>


From j@de@shod@@ m@iii@g oii googiem@ii@com  Thu Jun  9 16:27:04 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Thu, 9 Jun 2022 15:27:04 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <91a30ffd-1aec-0f44-17a0-b632fae91014@dewey.myzen.co.uk>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-EXdbmLVNbEH9nN=FE3FYs4uqXgVtH3V35o6BU0z=rTw@mail.gmail.com>
 <91a30ffd-1aec-0f44-17a0-b632fae91014@dewey.myzen.co.uk>
Message-ID: <CANg3_k-0=Z4KUE52mxHrSj4c+9bhJ+C4HDqxM2HqqVJ9um0WBQ@mail.gmail.com>

Hi Michael,

Thanks for the reply! When I ran the gam  with the gam() function, the
model worked fine with heap having 169 levels. The same model with
bam() however, fails.  I don't understand the difference between bam()
and gam() at all (other than computational efficiency), but could the
fact that each level only has 1 data point be the reason for it?

These heaping days are very large measurement errors I need to get rid
off, so I just want to take them out of the model altogether. (My data
is quite noisy already, because date of death is based on memory
recall, rather than formal death registration, due to the data being
from a low income country). Median of deaths is approx. 2 per day, but
on heaping days it can be as high as 50 or so.

My understanding was that coding with 169 levels would effectively
take these measurements out of the model (but do correct me if I'm
wrong!)  I originally coded heap as a binary variable with 0 for
non-heaping days and 1 for heaping days, but was told that meant that
I was assuming the effect was the same for all heaping days. If I
coded with 12 or 14 levels, wouldn't that leave a lot of noise in the
data?

Jade

On Thu, 9 Jun 2022 at 14:52, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>
> Dear Jade
>
> Do you really need to fit a separate parameter for each heaping day? Can
> you not just make it a binary predictor or a categorical one with fewer
> levels, perhaps 14 (for heaping in each year) or 12 (for each calendar
> month). I have no idea whether that would help but it seems worth a try.
>
> Michael
>
> On 08/06/2022 18:15, jade.shodan--- via R-help wrote:
> > Hi Simon,
> >
> > Thanks so much for this!! I have two follow up questions, if you don't mind.
> >
> > 1. Does including an autoregressive term not adjust away part of the
> > effect of the response in a distributed lag model (where the outcome
> > accumulates over time)?
> > 2. I've tried to fit a model using bam (just a first attempt without
> > AR term), but including the factor variable heap creates errors:
> >
> > bam0 <- bam(deaths~te(year, month, week, weekday,
> > bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> >                        te(temp_max, lag, k=c(8, 3)) +
> >                        te(precip_daily_total, lag, k=c(8, 3)),
> >                        data = dat, family = nb, method = 'fREML',
> > select = TRUE, discrete = TRUE,
> >                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> > 52.5), weekday = c(0, 6.5)))
> >
> > This model results in errors:
> >
> > Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
> >    step failure in theta estimation
> > Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> > object$prior.weights)) :
> >    NaNs produced
> >
> >
> > Including heap as as.numeric(heap) runs the model without error
> > messages or warnings, but model diagnostics look terrible, and it also
> > doesn't make sense (to me) to make heap a numeric. The factor variable
> > heap (with 169 levels) codes the fact that all deaths for which no
> > date was known, were registered on the 15th day of each month. I've
> > coded all non-heaping days as 0. All heaping days were coded as a
> > value between 1-168. The time series spans 14 years, so a heaping day
> > in each month results in 14*12 levels = 168, plus one level for
> > non-heaping days.
> >
> > So my second question is: Does bam allow factor variables? And if not,
> > how should I model this heaping on the 15th day of the month instead?
> >
> > With thanks,
> >
> > Jade
> >
> > On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
> >>
> >> I would not worry too much about high concurvity between variables like
> >> temperature and time. This just reflects the fact that temperature has a
> >> strong temporal pattern.
> >>
> >> I would also not be too worried about the low p-values on the k check.
> >> The check only looks for pattern in the residuals when they are ordered
> >> with respect to the variables of a smooth. When you have time series
> >> data and some smooths involve time then it's hard not to pick up some
> >> degree of residual auto-correlation, which you often would not want to
> >> model with a higher rank smoother.
> >>
> >> The NAs  for the distributed lag terms just reflect the fact that there
> >> is no obvious way to order the residuals w.r.t. the covariates for such
> >> terms, so the simple check for residual pattern is not really possible.
> >>
> >> One simple approach is to fit using bam(...,discrete=TRUE) which will
> >> let you specify an AR1 parameter to mop up some of the residual
> >> auto-correlation without resorting to a high rank smooth that then does
> >> all the work of the covariates as well. The AR1 parameter can be set by
> >> looking at the ACF of the residuals of the model without this. You need
> >> to look at the ACF of suitably standardized residuals to check how well
> >> this has worked.
> >>
> >> best,
> >>
> >> Simon
> >>
> >> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> >>> Hello everyone,
> >>>
> >>> A few days ago I asked a question about concurvity in a GAM (the
> >>> anologue of collinearity in a GLM) implemented in mgcv. I think my
> >>> question was a bit unfocussed, so I am retrying again, but with
> >>> additional information included about the autocorrelation function. I
> >>> have also posted about this on Cross Validated. Given all the model
> >>> output, it might make for easier
> >>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> >>>
> >>> As mentioned previously, I have problems with concurvity in my thesis
> >>> research, and don't have access to a statistician who works with time
> >>> series, GAMs or R. I'd be very grateful for any (partial) answer,
> >>> however short. I'll gladly return the favour where I can! For really
> >>> helpful input I'd be more than happy to offer co-authorship on
> >>> publication. Deadlines are very close, and I'm heading towards having
> >>> no results at all if I can't solve this concurvity issue :(
> >>>
> >>> I'm using GAMs to try to understand the relationship between deaths
> >>> and heat-related variables (e.g. temperature and humidity), using
> >>> daily time series over a 14-year period from a tropical, low-income
> >>> country. My aim is to understand the relationship between these
> >>> variables and deaths, rather than pure prediction performance.
> >>>
> >>> The GAMs include distributed lag models (set up as 7-column matrices,
> >>> see code at bottom of post), since deaths may occur over several days
> >>> following exposure.
> >>>
> >>> Simple GAMs with just time, lagged temperature and lagged
> >>> precipitation (a potential confounder) show very high concurvity
> >>> between lagged temperature and time, regardless of the many different
> >>> ways I have tried to decompose time. The autocorrelation functions
> >>> (ACF) however, shows values close to zero, only just about breaching
> >>> the 'significance line' in a few instances. It does show patterning
> >>> though, although the regularity is difficult to define.
> >>>
> >>> My questions are:
> >>> 1) Should I be worried about the high concurvity, or can I ignore it
> >>> given the mostly non-significant ACF? I've read dozens of
> >>> heat-mortality modelling studies and none report on concurvity between
> >>> weather variables and time (though one 2012 paper discussed
> >>> autocorrelation).
> >>>
> >>> 2) If I cannot ignore it, what should I do to resolve it? Would
> >>> including an autoregressive term be appropriate, and if so, where can
> >>> I find a coded example of how to do this? I've also come across
> >>> sequential regression][1]. Would this be more or less appropriate? If
> >>> appropriate, a pointer to an example would be really appreciated!
> >>>
> >>> Some example GAMs are specified as follows:
> >>> ```r
> >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> >>> bs=c("cr","cc","cc","cc")) + heap +
> >>>                         te(temp_max, lag, k=c(10, 3)) +
> >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>> 52.5), weekday = c(0, 6.5)))
> >>> ```
> >>> Concurvity for the above model between (temp_max, lag) and (year,
> >>> month, week, weekday) is 0.91:
> >>>
> >>> ```r
> >>> $worst
> >>>                                       para te(year,month,week,weekday)
> >>> te(temp_max,lag) te(precip_daily_total,lag)
> >>> para                        1.000000e+00                1.125625e-29
> >>>        0.3150073                  0.6666348
> >>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> >>>        0.9060552                  0.6652313
> >>> te(temp_max,lag)            3.152795e-01                8.998113e-01
> >>>        1.0000000                  0.5781015
> >>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> >>>        0.5805159                  1.0000000
> >>> ```
> >>>
> >>> Output from ```gam.check()```:
> >>> ```r
> >>> Method: REML   Optimizer: outer newton
> >>> full convergence after 16 iterations.
> >>> Gradient range [-0.01467332,0.003096643]
> >>> (score 8915.994 & scale 1).
> >>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> >>> Model rank =  544 / 544
> >>>
> >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>> indicate that k is too low, especially if edf is close to k'.
> >>>
> >>>                                     k'      edf k-index p-value
> >>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> >>> te(temp_max,lag)             29.0000   3.3681      NA      NA
> >>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>> ```
> >>>
> >>> Some output from ```summary(conc38b)```:
> >>> ```r
> >>> Approximate significance of smooth terms:
> >>>                                     edf Ref.df  Chi.sq p-value
> >>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> >>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> >>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>
> >>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> >>> -REML =   8916  Scale est. = 1         n = 5107
> >>> ```
> >>>
> >>>
> >>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> >>> pattern). They show peaks at 5 and 15, and then there seems to be a
> >>> recurring pattern at multiples of approx. 30 (suggesting month is not
> >>> modelled adequately?). Not sure what would cause the spikes at 5 and
> >>> 15. There is heaping of deaths on the 15th day of each month, to which
> >>> deaths with unknown date were allocated. This heaping was modelled
> >>> with categorical variable/ factor ```heap``` with 169 levels (0 for
> >>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> >>> heaping day over the 14-year period):
> >>>
> >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> >>>
> >>>
> >>> I get an identical looking ACF when I decompose time into (year,
> >>> month, monthday) as in model conc39 below, although concurvity between
> >>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> >>>
> >>> ```r
> >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >>>                        te(temp_max, lag, k=c(10, 4)) +
> >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> >>>                        knots = list(month = c(0.5, 12.5)))
> >>> ```
> >>> ```r
> >>>
> >>> Method: REML   Optimizer: outer newton
> >>> full convergence after 14 iterations.
> >>> Gradient range [-0.001578187,6.155096e-05]
> >>> (score 8915.763 & scale 1).
> >>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> >>> Model rank =  323 / 323
> >>>
> >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>> indicate that k is too low, especially if edf is close to k'.
> >>>
> >>>                                   k'     edf k-index p-value
> >>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> >>> te(temp_max,lag)           39.0000  4.0875      NA      NA
> >>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>> ```
> >>> Some output from ```summary(conc39)```:
> >>> ```r
> >>> Approximate significance of smooth terms:
> >>>                                   edf Ref.df  Chi.sq  p-value
> >>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> >>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> >>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>
> >>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> >>> -REML =   8915  Scale est. = 1         n = 5107
> >>> ```
> >>>
> >>>
> >>> ```r
> >>> $worst
> >>>                                      para te(year,month,monthday)
> >>> te(temp_max,lag) te(precip_daily_total,lag)
> >>> para                       1.000000e+00            3.261007e-31
> >>> 0.3313549                  0.6666532
> >>> te(year,month,monthday)    3.060763e-31            1.000000e+00
> >>> 0.8266086                  0.5670777
> >>> te(temp_max,lag)           3.331014e-01            8.225942e-01
> >>> 1.0000000                  0.5840875
> >>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> >>> 0.5939380                  1.0000000
> >>> ```
> >>>
> >>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> >>> various choices for k or as ```s(time)``` with various k does not
> >>> reduce concurvity either.
> >>>
> >>>
> >>> The default approach in time series studies of heat-mortality is to
> >>> model time with fixed df, generally between 7-10 df per year of data.
> >>> I am, however, apprehensive about this approach because a) mortality
> >>> profiles vary with locality due to sociodemographic and environmental
> >>> characteristics and b) the choice of df is based on higher income
> >>> countries (where nearly all these studies have been done) with
> >>> different mortality profiles and so may not be appropriate for
> >>> tropical, low-income countries.
> >>>
> >>> Although the approach of fixing (high) df does remove more temporal
> >>> patterns from the ACF (see model and output below), concurvity between
> >>> time and lagged temperature has now risen to 0.99! Moreover,
> >>> temperature (which has been a consistent, highly significant predictor
> >>> in every model of the tens (hundreds?) I have run, has now turned
> >>> non-significant. I am guessing this is because time is now a very
> >>> wiggly function that not only models/ removes seasonal variation, but
> >>> also some of the day-to-day variation that is needed for the
> >>> temperature smooth  :
> >>>
> >>> ```r
> >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >>>                         te(temp_max, lag, k=c(10,3)) +
> >>>                         te(precip_daily_total, lag, k=c(10,3)),
> >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> >>> ```
> >>> Output from ```gam.check(conc20a, rep = 1000)```:
> >>>
> >>> ```r
> >>> Method: REML   Optimizer: outer newton
> >>> full convergence after 9 iterations.
> >>> Gradient range [-0.0008983099,9.546022e-05]
> >>> (score 8750.13 & scale 1).
> >>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> >>> Model rank =  336 / 336
> >>>
> >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>> indicate that k is too low, especially if edf is close to k'.
> >>>
> >>>                                    k'      edf k-index p-value
> >>> s(time)                    111.0000 111.0000    0.98    0.56
> >>> te(temp_max,lag)            29.0000   0.6548      NA      NA
> >>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> >>> ```
> >>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
> >>>
> >>> ```r
> >>>                                      para      s(time) te(temp_max,lag)
> >>> te(precip_daily_total,lag)
> >>> para                       1.000000e+00 2.462064e-19        0.3165236
> >>>                   0.6666348
> >>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
> >>>                   0.6879284
> >>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> >>>                   0.5788711
> >>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> >>>                   1.0000000
> >>>
> >>> ```
> >>>
> >>> Some output from ```summary(conc20a)```:
> >>> ```r
> >>> Approximate significance of smooth terms:
> >>>                                    edf Ref.df  Chi.sq p-value
> >>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> >>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> >>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>
> >>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> >>> -REML = 8750.1  Scale est. = 1         n = 5107
> >>> ```
> >>>
> >>> ACF functions:
> >>>
> >>> [4]: https://i.stack.imgur.com/7nbXS.png
> >>> [5]: https://i.stack.imgur.com/pNnZU.png
> >>>
> >>> Data can be found on my [GitHub][6] site in the file
> >>> [data_cross_validated_post2.rds][7]. A csv version is also available.
> >>> This is my code:
> >>>
> >>> ```r
> >>> library(readr)
> >>> library(mgcv)
> >>>
> >>> df <- read_rds("data_crossvalidated_post2.rds")
> >>>
> >>> # Create matrices for lagged weather variables (6 day lags) based on
> >>> example by Simon Wood
> >>> # in his 2017 book ("Generalized additive models: an introduction with
> >>> R", p. 349) and
> >>> # gamair package documentation
> >>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> >>>
> >>> lagard <- function(x,n.lag=7) {
> >>> n <- length(x); X <- matrix(NA,n,n.lag)
> >>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> >>> X
> >>> }
> >>>
> >>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> >>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> >>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> >>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> >>> = df$precip_hourly_dailysum)
> >>> dat$temp_max <- lagard(df$temp_max)
> >>> dat$temp_min <- lagard(df$temp_min)
> >>> dat$temp_mean <- lagard(df$temp_mean)
> >>> dat$wbgt_max <- lagard(df$wbgt_max)
> >>> dat$wbgt_mean <- lagard(df$wbgt_mean)
> >>> dat$wbgt_min <- lagard(df$wbgt_min)
> >>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> >>> dat$sh_mean <- lagard(df$sh_mean)
> >>> dat$solar_mean <- lagard(df$solar_mean)
> >>> dat$wind2m_mean <- lagard(df$wind2m_mean)
> >>> dat$sh_max <- lagard(df$sh_max)
> >>> dat$solar_max <- lagard(df$solar_max)
> >>> dat$wind2m_max <- lagard(df$wind2m_max)
> >>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> >>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> >>> dat$precip_hourly <- lagard(df$precip_hourly)
> >>> dat$precip_daily_total <- lagard( df$precip_daily_total)
> >>> dat$temp <- lagard(df$temp)
> >>> dat$sh <- lagard(df$sh)
> >>> dat$rh <- lagard(df$rh)
> >>> dat$solar <- lagard(df$solar)
> >>> dat$wind2m <- lagard(df$wind2m)
> >>>
> >>>
> >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> >>> bs=c("cr","cc","cc","cc")) + heap +
> >>>                         te(temp_max, lag, k=c(10, 3)) +
> >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>> 52.5), weekday = c(0, 6.5)))
> >>>
> >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >>>                        te(temp_max, lag, k=c(10, 4)) +
> >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> >>>                        knots = list(month = c(0.5, 12.5)))
> >>>
> >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >>>                         te(temp_max, lag, k=c(10,3)) +
> >>>                         te(precip_daily_total, lag, k=c(10,3)),
> >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> >>>
> >>> ```
> >>> Thank you if you've read this far!! :-))
> >>>
> >>>     [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> >>>     [4]: https://i.stack.imgur.com/7nbXS.png
> >>>     [5]: https://i.stack.imgur.com/pNnZU.png
> >>>     [6]: https://github.com/JadeShodan/heat-mortality
> >>>     [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Simon Wood, School of Mathematics, University of Edinburgh,
> >> https://www.maths.ed.ac.uk/~swood34/
> >>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html


From j@de@shod@@ m@iii@g oii googiem@ii@com  Thu Jun  9 17:57:45 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Thu, 9 Jun 2022 16:57:45 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <f371005e-6f57-2007-f859-d66595a19c0f@bath.edu>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-g=M41cxom6DVdTunFjgZ6VfKobqW__Mq351pDpY22PQ@mail.gmail.com>
 <f371005e-6f57-2007-f859-d66595a19c0f@bath.edu>
Message-ID: <CANg3_k_qMX24_a-Hppk6knrH-nt1QMLj2FFSJbkMJc0TBxqzQA@mail.gmail.com>

Hi Simon,

(Sorry, replies and answers are out of sync due to my problems posting
to the list/ messages being held for moderation)

> Did it actually fail, or simply generate warnings?
The model was computed (you're right, not a model failure), but
resulted in warnings (as per previous post) which, frankly, I didn't
understand.

> if I understand your model right there is one free parameter for each observation falling on the 15th
That's right, one observation on each 15th day of the month.

Thank you for the suggestion about the random effects! I had been
wondering about how I could model this heaping with a smooth!

Quick question:

You proposed:

aheap <- heap!="0"
heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap
day + a r.e. for each heap day - no effect on non-heap days

Is there a typo in the code above? I don't see the newly created
variable aheap in the model?
Should it read "by = aheap" as follows:

heap + s(heap,bs="re",by=aheap)   ?

So a full model might then look like the one below?

 bam0 <- bam(deaths~te(year, month, week, weekday,
bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
s(heap,bs="re",by=aheap)
                                     te(temp_max, lag, k=c(8, 3)) +
                                     te(precip_daily_total, lag, k=c(8, 3)),
                                     data = dat, family = nb, method =
'fREML', select = TRUE, discrete = TRUE,
                                   knots = list(month = c(0.5, 12.5),
week = c(0.5, 52.5), weekday = c(0, 6.5)))

One, hopefully final (!) question:

Is it actually useful at all to keep these observations on the 15th
day of each month (which are huge errors), or am I better off removing
them from the data set (or replacing them with e.g. median values)?
For temperature-mortality modelling it is the day-to-day variation in
deaths and temperature that is of interest. So is modelling heaping
actually useful at all, given that this variable changes on a monthly
basis? (I think you are alluding to this, but I just want to make
sure).

If I take them out altogether, would I be best off removing all data
for these dates, so that the time series jumps from day 14 to day 16?
Or would this create problems with e.g. the distributed lag model?

Sorry for all these questions! Have been struggling with this for
months (posted on Cross Validated about the heaping issue too), and
feel I am finally getting somewhere with your help!

Jade

On Thu, 9 Jun 2022 at 15:24, Simon Wood <simon.wood at bath.edu> wrote:
>
>
> On 09/06/2022 12:30, jade.shodan at googlemail.com wrote:
> > Hi Simon,
> >
> > Thanks so much for this!! (Apologies if this is a double posting. I
> > seem to have a problem getting messages through to the list).
> >
> > I have two follow up questions, if you don't mind.
> >
> > 1. Does including an autoregressive term not adjust away part of the
> > effect of the response in a distributed lag model (where the outcome
> > accumulates over time)?
>
> - the hope is that it approximately deals with short timescale stuff
> without interfering with the longer timescales to the same extent as a
> high rank smooth would.
>
> > 2. I've tried to fit a model using bam (just a first attempt without
> > AR term), but including the factor variable heap creates errors:
> >
> > bam0 <- bam(deaths~te(year, month, week, weekday,
> > bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> >                                       te(temp_max, lag, k=c(8, 3)) +
> >                                       te(precip_daily_total, lag, k=c(8, 3)),
> >                                      data = dat, family = nb, method =
> > 'fREML', select = TRUE, discrete = TRUE,
> >                                      knots = list(month = c(0.5, 12.5),
> > week = c(0.5, 52.5), weekday = c(0, 6.5)))
> >
> > This model results in errors:
> >
> > Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
> >    step failure in theta estimation
> > Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> > object$prior.weights)) :
> >    NaNs produced
> >
> Did it actually fail, or simply generate warnings?
>
> 'bam' handles factors, but if I understand your model right there is one
> free parameter for each observation falling on the 15th, so that you
> will fit data for those days exactly (and might just as well have
> dropped them, for all the information they contribute to the rest of the
> model). If you want a structure like this, I'd be inclined to make the
> heap variable random, something like...
>
> aheap <- heap!="0"
>
> heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap day
> + a r.e. for each heap day - no effect on non-heap days
>
> best,
>
> Simon
>
> > Including heap as as.numeric(heap) runs the model without error
> > messages or warnings, but model diagnostics look terrible, and it also
> > doesn't make sense (to me) to make heap a numeric. The factor variable
> > heap (with 169 levels) codes the fact that all deaths for which no
> > date was known, were registered on the 15th day of each month. I've
> > coded all non-heaping days as 0. All heaping days were coded as a
> > value between 1-168. The time series spans 14 years, so a heaping day
> > in each month results in 14*12 levels = 168, plus one level for
> > non-heaping days.
> >
> > So my second question is: Does bam allow factor variables? And if not,
> > how should I model this heaping on the 15th day of the month instead?
> >
> > With thanks,
> >
> > Jade
> >
> > On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
> >> I would not worry too much about high concurvity between variables like
> >> temperature and time. This just reflects the fact that temperature has a
> >> strong temporal pattern.
> >>
> >> I would also not be too worried about the low p-values on the k check.
> >> The check only looks for pattern in the residuals when they are ordered
> >> with respect to the variables of a smooth. When you have time series
> >> data and some smooths involve time then it's hard not to pick up some
> >> degree of residual auto-correlation, which you often would not want to
> >> model with a higher rank smoother.
> >>
> >> The NAs  for the distributed lag terms just reflect the fact that there
> >> is no obvious way to order the residuals w.r.t. the covariates for such
> >> terms, so the simple check for residual pattern is not really possible.
> >>
> >> One simple approach is to fit using bam(...,discrete=TRUE) which will
> >> let you specify an AR1 parameter to mop up some of the residual
> >> auto-correlation without resorting to a high rank smooth that then does
> >> all the work of the covariates as well. The AR1 parameter can be set by
> >> looking at the ACF of the residuals of the model without this. You need
> >> to look at the ACF of suitably standardized residuals to check how well
> >> this has worked.
> >>
> >> best,
> >>
> >> Simon
> >>
> >> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> >>> Hello everyone,
> >>>
> >>> A few days ago I asked a question about concurvity in a GAM (the
> >>> anologue of collinearity in a GLM) implemented in mgcv. I think my
> >>> question was a bit unfocussed, so I am retrying again, but with
> >>> additional information included about the autocorrelation function. I
> >>> have also posted about this on Cross Validated. Given all the model
> >>> output, it might make for easier
> >>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> >>>
> >>> As mentioned previously, I have problems with concurvity in my thesis
> >>> research, and don't have access to a statistician who works with time
> >>> series, GAMs or R. I'd be very grateful for any (partial) answer,
> >>> however short. I'll gladly return the favour where I can! For really
> >>> helpful input I'd be more than happy to offer co-authorship on
> >>> publication. Deadlines are very close, and I'm heading towards having
> >>> no results at all if I can't solve this concurvity issue :(
> >>>
> >>> I'm using GAMs to try to understand the relationship between deaths
> >>> and heat-related variables (e.g. temperature and humidity), using
> >>> daily time series over a 14-year period from a tropical, low-income
> >>> country. My aim is to understand the relationship between these
> >>> variables and deaths, rather than pure prediction performance.
> >>>
> >>> The GAMs include distributed lag models (set up as 7-column matrices,
> >>> see code at bottom of post), since deaths may occur over several days
> >>> following exposure.
> >>>
> >>> Simple GAMs with just time, lagged temperature and lagged
> >>> precipitation (a potential confounder) show very high concurvity
> >>> between lagged temperature and time, regardless of the many different
> >>> ways I have tried to decompose time. The autocorrelation functions
> >>> (ACF) however, shows values close to zero, only just about breaching
> >>> the 'significance line' in a few instances. It does show patterning
> >>> though, although the regularity is difficult to define.
> >>>
> >>> My questions are:
> >>> 1) Should I be worried about the high concurvity, or can I ignore it
> >>> given the mostly non-significant ACF? I've read dozens of
> >>> heat-mortality modelling studies and none report on concurvity between
> >>> weather variables and time (though one 2012 paper discussed
> >>> autocorrelation).
> >>>
> >>> 2) If I cannot ignore it, what should I do to resolve it? Would
> >>> including an autoregressive term be appropriate, and if so, where can
> >>> I find a coded example of how to do this? I've also come across
> >>> sequential regression][1]. Would this be more or less appropriate? If
> >>> appropriate, a pointer to an example would be really appreciated!
> >>>
> >>> Some example GAMs are specified as follows:
> >>> ```r
> >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> >>> bs=c("cr","cc","cc","cc")) + heap +
> >>>                         te(temp_max, lag, k=c(10, 3)) +
> >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>> 52.5), weekday = c(0, 6.5)))
> >>> ```
> >>> Concurvity for the above model between (temp_max, lag) and (year,
> >>> month, week, weekday) is 0.91:
> >>>
> >>> ```r
> >>> $worst
> >>>                                       para te(year,month,week,weekday)
> >>> te(temp_max,lag) te(precip_daily_total,lag)
> >>> para                        1.000000e+00                1.125625e-29
> >>>        0.3150073                  0.6666348
> >>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> >>>        0.9060552                  0.6652313
> >>> te(temp_max,lag)            3.152795e-01                8.998113e-01
> >>>        1.0000000                  0.5781015
> >>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> >>>        0.5805159                  1.0000000
> >>> ```
> >>>
> >>> Output from ```gam.check()```:
> >>> ```r
> >>> Method: REML   Optimizer: outer newton
> >>> full convergence after 16 iterations.
> >>> Gradient range [-0.01467332,0.003096643]
> >>> (score 8915.994 & scale 1).
> >>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> >>> Model rank =  544 / 544
> >>>
> >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>> indicate that k is too low, especially if edf is close to k'.
> >>>
> >>>                                     k'      edf k-index p-value
> >>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> >>> te(temp_max,lag)             29.0000   3.3681      NA      NA
> >>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>> ```
> >>>
> >>> Some output from ```summary(conc38b)```:
> >>> ```r
> >>> Approximate significance of smooth terms:
> >>>                                     edf Ref.df  Chi.sq p-value
> >>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> >>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> >>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>
> >>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> >>> -REML =   8916  Scale est. = 1         n = 5107
> >>> ```
> >>>
> >>>
> >>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> >>> pattern). They show peaks at 5 and 15, and then there seems to be a
> >>> recurring pattern at multiples of approx. 30 (suggesting month is not
> >>> modelled adequately?). Not sure what would cause the spikes at 5 and
> >>> 15. There is heaping of deaths on the 15th day of each month, to which
> >>> deaths with unknown date were allocated. This heaping was modelled
> >>> with categorical variable/ factor ```heap``` with 169 levels (0 for
> >>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> >>> heaping day over the 14-year period):
> >>>
> >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> >>>
> >>>
> >>> I get an identical looking ACF when I decompose time into (year,
> >>> month, monthday) as in model conc39 below, although concurvity between
> >>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> >>>
> >>> ```r
> >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >>>                        te(temp_max, lag, k=c(10, 4)) +
> >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> >>>                        knots = list(month = c(0.5, 12.5)))
> >>> ```
> >>> ```r
> >>>
> >>> Method: REML   Optimizer: outer newton
> >>> full convergence after 14 iterations.
> >>> Gradient range [-0.001578187,6.155096e-05]
> >>> (score 8915.763 & scale 1).
> >>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> >>> Model rank =  323 / 323
> >>>
> >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>> indicate that k is too low, especially if edf is close to k'.
> >>>
> >>>                                   k'     edf k-index p-value
> >>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> >>> te(temp_max,lag)           39.0000  4.0875      NA      NA
> >>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>> ```
> >>> Some output from ```summary(conc39)```:
> >>> ```r
> >>> Approximate significance of smooth terms:
> >>>                                   edf Ref.df  Chi.sq  p-value
> >>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> >>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> >>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>
> >>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> >>> -REML =   8915  Scale est. = 1         n = 5107
> >>> ```
> >>>
> >>>
> >>> ```r
> >>> $worst
> >>>                                      para te(year,month,monthday)
> >>> te(temp_max,lag) te(precip_daily_total,lag)
> >>> para                       1.000000e+00            3.261007e-31
> >>> 0.3313549                  0.6666532
> >>> te(year,month,monthday)    3.060763e-31            1.000000e+00
> >>> 0.8266086                  0.5670777
> >>> te(temp_max,lag)           3.331014e-01            8.225942e-01
> >>> 1.0000000                  0.5840875
> >>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> >>> 0.5939380                  1.0000000
> >>> ```
> >>>
> >>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> >>> various choices for k or as ```s(time)``` with various k does not
> >>> reduce concurvity either.
> >>>
> >>>
> >>> The default approach in time series studies of heat-mortality is to
> >>> model time with fixed df, generally between 7-10 df per year of data.
> >>> I am, however, apprehensive about this approach because a) mortality
> >>> profiles vary with locality due to sociodemographic and environmental
> >>> characteristics and b) the choice of df is based on higher income
> >>> countries (where nearly all these studies have been done) with
> >>> different mortality profiles and so may not be appropriate for
> >>> tropical, low-income countries.
> >>>
> >>> Although the approach of fixing (high) df does remove more temporal
> >>> patterns from the ACF (see model and output below), concurvity between
> >>> time and lagged temperature has now risen to 0.99! Moreover,
> >>> temperature (which has been a consistent, highly significant predictor
> >>> in every model of the tens (hundreds?) I have run, has now turned
> >>> non-significant. I am guessing this is because time is now a very
> >>> wiggly function that not only models/ removes seasonal variation, but
> >>> also some of the day-to-day variation that is needed for the
> >>> temperature smooth  :
> >>>
> >>> ```r
> >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >>>                         te(temp_max, lag, k=c(10,3)) +
> >>>                         te(precip_daily_total, lag, k=c(10,3)),
> >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> >>> ```
> >>> Output from ```gam.check(conc20a, rep = 1000)```:
> >>>
> >>> ```r
> >>> Method: REML   Optimizer: outer newton
> >>> full convergence after 9 iterations.
> >>> Gradient range [-0.0008983099,9.546022e-05]
> >>> (score 8750.13 & scale 1).
> >>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> >>> Model rank =  336 / 336
> >>>
> >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>> indicate that k is too low, especially if edf is close to k'.
> >>>
> >>>                                    k'      edf k-index p-value
> >>> s(time)                    111.0000 111.0000    0.98    0.56
> >>> te(temp_max,lag)            29.0000   0.6548      NA      NA
> >>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> >>> ```
> >>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
> >>>
> >>> ```r
> >>>                                      para      s(time) te(temp_max,lag)
> >>> te(precip_daily_total,lag)
> >>> para                       1.000000e+00 2.462064e-19        0.3165236
> >>>                   0.6666348
> >>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
> >>>                   0.6879284
> >>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> >>>                   0.5788711
> >>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> >>>                   1.0000000
> >>>
> >>> ```
> >>>
> >>> Some output from ```summary(conc20a)```:
> >>> ```r
> >>> Approximate significance of smooth terms:
> >>>                                    edf Ref.df  Chi.sq p-value
> >>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> >>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> >>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> >>> ---
> >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>
> >>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> >>> -REML = 8750.1  Scale est. = 1         n = 5107
> >>> ```
> >>>
> >>> ACF functions:
> >>>
> >>> [4]: https://i.stack.imgur.com/7nbXS.png
> >>> [5]: https://i.stack.imgur.com/pNnZU.png
> >>>
> >>> Data can be found on my [GitHub][6] site in the file
> >>> [data_cross_validated_post2.rds][7]. A csv version is also available.
> >>> This is my code:
> >>>
> >>> ```r
> >>> library(readr)
> >>> library(mgcv)
> >>>
> >>> df <- read_rds("data_crossvalidated_post2.rds")
> >>>
> >>> # Create matrices for lagged weather variables (6 day lags) based on
> >>> example by Simon Wood
> >>> # in his 2017 book ("Generalized additive models: an introduction with
> >>> R", p. 349) and
> >>> # gamair package documentation
> >>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> >>>
> >>> lagard <- function(x,n.lag=7) {
> >>> n <- length(x); X <- matrix(NA,n,n.lag)
> >>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> >>> X
> >>> }
> >>>
> >>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> >>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> >>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> >>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> >>> = df$precip_hourly_dailysum)
> >>> dat$temp_max <- lagard(df$temp_max)
> >>> dat$temp_min <- lagard(df$temp_min)
> >>> dat$temp_mean <- lagard(df$temp_mean)
> >>> dat$wbgt_max <- lagard(df$wbgt_max)
> >>> dat$wbgt_mean <- lagard(df$wbgt_mean)
> >>> dat$wbgt_min <- lagard(df$wbgt_min)
> >>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> >>> dat$sh_mean <- lagard(df$sh_mean)
> >>> dat$solar_mean <- lagard(df$solar_mean)
> >>> dat$wind2m_mean <- lagard(df$wind2m_mean)
> >>> dat$sh_max <- lagard(df$sh_max)
> >>> dat$solar_max <- lagard(df$solar_max)
> >>> dat$wind2m_max <- lagard(df$wind2m_max)
> >>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> >>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> >>> dat$precip_hourly <- lagard(df$precip_hourly)
> >>> dat$precip_daily_total <- lagard( df$precip_daily_total)
> >>> dat$temp <- lagard(df$temp)
> >>> dat$sh <- lagard(df$sh)
> >>> dat$rh <- lagard(df$rh)
> >>> dat$solar <- lagard(df$solar)
> >>> dat$wind2m <- lagard(df$wind2m)
> >>>
> >>>
> >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> >>> bs=c("cr","cc","cc","cc")) + heap +
> >>>                         te(temp_max, lag, k=c(10, 3)) +
> >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>> 52.5), weekday = c(0, 6.5)))
> >>>
> >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >>>                        te(temp_max, lag, k=c(10, 4)) +
> >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> >>>                        knots = list(month = c(0.5, 12.5)))
> >>>
> >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >>>                         te(temp_max, lag, k=c(10,3)) +
> >>>                         te(precip_daily_total, lag, k=c(10,3)),
> >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> >>>
> >>> ```
> >>> Thank you if you've read this far!! :-))
> >>>
> >>>     [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> >>>     [4]: https://i.stack.imgur.com/7nbXS.png
> >>>     [5]: https://i.stack.imgur.com/pNnZU.png
> >>>     [6]: https://github.com/JadeShodan/heat-mortality
> >>>     [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >> --
> >> Simon Wood, School of Mathematics, University of Edinburgh,
> >> https://www.maths.ed.ac.uk/~swood34/
> >>
> --
> Simon Wood, School of Mathematics, University of Edinburgh,
> https://www.maths.ed.ac.uk/~swood34/
>


From j@de@shod@@ m@iii@g oii googiem@ii@com  Fri Jun 10 11:30:02 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Fri, 10 Jun 2022 10:30:02 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k-0=Z4KUE52mxHrSj4c+9bhJ+C4HDqxM2HqqVJ9um0WBQ@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-EXdbmLVNbEH9nN=FE3FYs4uqXgVtH3V35o6BU0z=rTw@mail.gmail.com>
 <91a30ffd-1aec-0f44-17a0-b632fae91014@dewey.myzen.co.uk>
 <CANg3_k-0=Z4KUE52mxHrSj4c+9bhJ+C4HDqxM2HqqVJ9um0WBQ@mail.gmail.com>
Message-ID: <CANg3_k-1=WsD71xcSa7WV0Ef=tiY3_Gz2KxvmskY8QuJso+EYA@mail.gmail.com>

Hi Michael,

I don't think my reply to your email came through to the list, so am
resending (see below). Problems with subscription have now hopefully
been resolved. Apologies if this is a double posting!

On Thu, 9 Jun 2022 at 15:27, jade.shodan at googlemail.com
<jade.shodan at googlemail.com> wrote:
>
> Hi Michael,
>
> Thanks for the reply! When I ran the gam  with the gam() function, the
> model worked fine with heap having 169 levels. The same model with
> bam() however, fails.  I don't understand the difference between bam()
> and gam() at all (other than computational efficiency), but could the
> fact that each level only has 1 data point be the reason for it?
>
> These heaping days are very large measurement errors I need to get rid
> off, so I just want to take them out of the model altogether. (My data
> is quite noisy already, because date of death is based on memory
> recall, rather than formal death registration, due to the data being
> from a low income country). Median of deaths is approx. 2 per day, but
> on heaping days it can be as high as 50 or so.
>
> My understanding was that coding with 169 levels would effectively
> take these measurements out of the model (but do correct me if I'm
> wrong!)  I originally coded heap as a binary variable with 0 for
> non-heaping days and 1 for heaping days, but was told that meant that
> I was assuming the effect was the same for all heaping days. If I
> coded with 12 or 14 levels, wouldn't that leave a lot of noise in the
> data?
>
> Jade
>
> On Thu, 9 Jun 2022 at 14:52, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
> >
> > Dear Jade
> >
> > Do you really need to fit a separate parameter for each heaping day? Can
> > you not just make it a binary predictor or a categorical one with fewer
> > levels, perhaps 14 (for heaping in each year) or 12 (for each calendar
> > month). I have no idea whether that would help but it seems worth a try.
> >
> > Michael
> >
> > On 08/06/2022 18:15, jade.shodan--- via R-help wrote:
> > > Hi Simon,
> > >
> > > Thanks so much for this!! I have two follow up questions, if you don't mind.
> > >
> > > 1. Does including an autoregressive term not adjust away part of the
> > > effect of the response in a distributed lag model (where the outcome
> > > accumulates over time)?
> > > 2. I've tried to fit a model using bam (just a first attempt without
> > > AR term), but including the factor variable heap creates errors:
> > >
> > > bam0 <- bam(deaths~te(year, month, week, weekday,
> > > bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> > >                        te(temp_max, lag, k=c(8, 3)) +
> > >                        te(precip_daily_total, lag, k=c(8, 3)),
> > >                        data = dat, family = nb, method = 'fREML',
> > > select = TRUE, discrete = TRUE,
> > >                        knots = list(month = c(0.5, 12.5), week = c(0.5,
> > > 52.5), weekday = c(0, 6.5)))
> > >
> > > This model results in errors:
> > >
> > > Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
> > >    step failure in theta estimation
> > > Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> > > object$prior.weights)) :
> > >    NaNs produced
> > >
> > >
> > > Including heap as as.numeric(heap) runs the model without error
> > > messages or warnings, but model diagnostics look terrible, and it also
> > > doesn't make sense (to me) to make heap a numeric. The factor variable
> > > heap (with 169 levels) codes the fact that all deaths for which no
> > > date was known, were registered on the 15th day of each month. I've
> > > coded all non-heaping days as 0. All heaping days were coded as a
> > > value between 1-168. The time series spans 14 years, so a heaping day
> > > in each month results in 14*12 levels = 168, plus one level for
> > > non-heaping days.
> > >
> > > So my second question is: Does bam allow factor variables? And if not,
> > > how should I model this heaping on the 15th day of the month instead?
> > >
> > > With thanks,
> > >
> > > Jade
> > >
> > > On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
> > >>
> > >> I would not worry too much about high concurvity between variables like
> > >> temperature and time. This just reflects the fact that temperature has a
> > >> strong temporal pattern.
> > >>
> > >> I would also not be too worried about the low p-values on the k check.
> > >> The check only looks for pattern in the residuals when they are ordered
> > >> with respect to the variables of a smooth. When you have time series
> > >> data and some smooths involve time then it's hard not to pick up some
> > >> degree of residual auto-correlation, which you often would not want to
> > >> model with a higher rank smoother.
> > >>
> > >> The NAs  for the distributed lag terms just reflect the fact that there
> > >> is no obvious way to order the residuals w.r.t. the covariates for such
> > >> terms, so the simple check for residual pattern is not really possible.
> > >>
> > >> One simple approach is to fit using bam(...,discrete=TRUE) which will
> > >> let you specify an AR1 parameter to mop up some of the residual
> > >> auto-correlation without resorting to a high rank smooth that then does
> > >> all the work of the covariates as well. The AR1 parameter can be set by
> > >> looking at the ACF of the residuals of the model without this. You need
> > >> to look at the ACF of suitably standardized residuals to check how well
> > >> this has worked.
> > >>
> > >> best,
> > >>
> > >> Simon
> > >>
> > >> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> > >>> Hello everyone,
> > >>>
> > >>> A few days ago I asked a question about concurvity in a GAM (the
> > >>> anologue of collinearity in a GLM) implemented in mgcv. I think my
> > >>> question was a bit unfocussed, so I am retrying again, but with
> > >>> additional information included about the autocorrelation function. I
> > >>> have also posted about this on Cross Validated. Given all the model
> > >>> output, it might make for easier
> > >>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> > >>>
> > >>> As mentioned previously, I have problems with concurvity in my thesis
> > >>> research, and don't have access to a statistician who works with time
> > >>> series, GAMs or R. I'd be very grateful for any (partial) answer,
> > >>> however short. I'll gladly return the favour where I can! For really
> > >>> helpful input I'd be more than happy to offer co-authorship on
> > >>> publication. Deadlines are very close, and I'm heading towards having
> > >>> no results at all if I can't solve this concurvity issue :(
> > >>>
> > >>> I'm using GAMs to try to understand the relationship between deaths
> > >>> and heat-related variables (e.g. temperature and humidity), using
> > >>> daily time series over a 14-year period from a tropical, low-income
> > >>> country. My aim is to understand the relationship between these
> > >>> variables and deaths, rather than pure prediction performance.
> > >>>
> > >>> The GAMs include distributed lag models (set up as 7-column matrices,
> > >>> see code at bottom of post), since deaths may occur over several days
> > >>> following exposure.
> > >>>
> > >>> Simple GAMs with just time, lagged temperature and lagged
> > >>> precipitation (a potential confounder) show very high concurvity
> > >>> between lagged temperature and time, regardless of the many different
> > >>> ways I have tried to decompose time. The autocorrelation functions
> > >>> (ACF) however, shows values close to zero, only just about breaching
> > >>> the 'significance line' in a few instances. It does show patterning
> > >>> though, although the regularity is difficult to define.
> > >>>
> > >>> My questions are:
> > >>> 1) Should I be worried about the high concurvity, or can I ignore it
> > >>> given the mostly non-significant ACF? I've read dozens of
> > >>> heat-mortality modelling studies and none report on concurvity between
> > >>> weather variables and time (though one 2012 paper discussed
> > >>> autocorrelation).
> > >>>
> > >>> 2) If I cannot ignore it, what should I do to resolve it? Would
> > >>> including an autoregressive term be appropriate, and if so, where can
> > >>> I find a coded example of how to do this? I've also come across
> > >>> sequential regression][1]. Would this be more or less appropriate? If
> > >>> appropriate, a pointer to an example would be really appreciated!
> > >>>
> > >>> Some example GAMs are specified as follows:
> > >>> ```r
> > >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> > >>> bs=c("cr","cc","cc","cc")) + heap +
> > >>>                         te(temp_max, lag, k=c(10, 3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> > >>> 52.5), weekday = c(0, 6.5)))
> > >>> ```
> > >>> Concurvity for the above model between (temp_max, lag) and (year,
> > >>> month, week, weekday) is 0.91:
> > >>>
> > >>> ```r
> > >>> $worst
> > >>>                                       para te(year,month,week,weekday)
> > >>> te(temp_max,lag) te(precip_daily_total,lag)
> > >>> para                        1.000000e+00                1.125625e-29
> > >>>        0.3150073                  0.6666348
> > >>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> > >>>        0.9060552                  0.6652313
> > >>> te(temp_max,lag)            3.152795e-01                8.998113e-01
> > >>>        1.0000000                  0.5781015
> > >>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> > >>>        0.5805159                  1.0000000
> > >>> ```
> > >>>
> > >>> Output from ```gam.check()```:
> > >>> ```r
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 16 iterations.
> > >>> Gradient range [-0.01467332,0.003096643]
> > >>> (score 8915.994 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> > >>> Model rank =  544 / 544
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                     k'      edf k-index p-value
> > >>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> > >>> te(temp_max,lag)             29.0000   3.3681      NA      NA
> > >>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>> ```
> > >>>
> > >>> Some output from ```summary(conc38b)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                     edf Ref.df  Chi.sq p-value
> > >>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> > >>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> > >>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> > >>> -REML =   8916  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>>
> > >>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> > >>> pattern). They show peaks at 5 and 15, and then there seems to be a
> > >>> recurring pattern at multiples of approx. 30 (suggesting month is not
> > >>> modelled adequately?). Not sure what would cause the spikes at 5 and
> > >>> 15. There is heaping of deaths on the 15th day of each month, to which
> > >>> deaths with unknown date were allocated. This heaping was modelled
> > >>> with categorical variable/ factor ```heap``` with 169 levels (0 for
> > >>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> > >>> heaping day over the 14-year period):
> > >>>
> > >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> > >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> > >>>
> > >>>
> > >>> I get an identical looking ACF when I decompose time into (year,
> > >>> month, monthday) as in model conc39 below, although concurvity between
> > >>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> > >>>
> > >>> ```r
> > >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> > >>>                        te(temp_max, lag, k=c(10, 4)) +
> > >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> > >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                        knots = list(month = c(0.5, 12.5)))
> > >>> ```
> > >>> ```r
> > >>>
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 14 iterations.
> > >>> Gradient range [-0.001578187,6.155096e-05]
> > >>> (score 8915.763 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> > >>> Model rank =  323 / 323
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                   k'     edf k-index p-value
> > >>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> > >>> te(temp_max,lag)           39.0000  4.0875      NA      NA
> > >>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>> ```
> > >>> Some output from ```summary(conc39)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                   edf Ref.df  Chi.sq  p-value
> > >>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> > >>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> > >>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> > >>> -REML =   8915  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>>
> > >>> ```r
> > >>> $worst
> > >>>                                      para te(year,month,monthday)
> > >>> te(temp_max,lag) te(precip_daily_total,lag)
> > >>> para                       1.000000e+00            3.261007e-31
> > >>> 0.3313549                  0.6666532
> > >>> te(year,month,monthday)    3.060763e-31            1.000000e+00
> > >>> 0.8266086                  0.5670777
> > >>> te(temp_max,lag)           3.331014e-01            8.225942e-01
> > >>> 1.0000000                  0.5840875
> > >>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> > >>> 0.5939380                  1.0000000
> > >>> ```
> > >>>
> > >>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> > >>> various choices for k or as ```s(time)``` with various k does not
> > >>> reduce concurvity either.
> > >>>
> > >>>
> > >>> The default approach in time series studies of heat-mortality is to
> > >>> model time with fixed df, generally between 7-10 df per year of data.
> > >>> I am, however, apprehensive about this approach because a) mortality
> > >>> profiles vary with locality due to sociodemographic and environmental
> > >>> characteristics and b) the choice of df is based on higher income
> > >>> countries (where nearly all these studies have been done) with
> > >>> different mortality profiles and so may not be appropriate for
> > >>> tropical, low-income countries.
> > >>>
> > >>> Although the approach of fixing (high) df does remove more temporal
> > >>> patterns from the ACF (see model and output below), concurvity between
> > >>> time and lagged temperature has now risen to 0.99! Moreover,
> > >>> temperature (which has been a consistent, highly significant predictor
> > >>> in every model of the tens (hundreds?) I have run, has now turned
> > >>> non-significant. I am guessing this is because time is now a very
> > >>> wiggly function that not only models/ removes seasonal variation, but
> > >>> also some of the day-to-day variation that is needed for the
> > >>> temperature smooth  :
> > >>>
> > >>> ```r
> > >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> > >>>                         te(temp_max, lag, k=c(10,3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10,3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> > >>> ```
> > >>> Output from ```gam.check(conc20a, rep = 1000)```:
> > >>>
> > >>> ```r
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 9 iterations.
> > >>> Gradient range [-0.0008983099,9.546022e-05]
> > >>> (score 8750.13 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> > >>> Model rank =  336 / 336
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                    k'      edf k-index p-value
> > >>> s(time)                    111.0000 111.0000    0.98    0.56
> > >>> te(temp_max,lag)            29.0000   0.6548      NA      NA
> > >>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> > >>> ```
> > >>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
> > >>>
> > >>> ```r
> > >>>                                      para      s(time) te(temp_max,lag)
> > >>> te(precip_daily_total,lag)
> > >>> para                       1.000000e+00 2.462064e-19        0.3165236
> > >>>                   0.6666348
> > >>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
> > >>>                   0.6879284
> > >>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> > >>>                   0.5788711
> > >>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> > >>>                   1.0000000
> > >>>
> > >>> ```
> > >>>
> > >>> Some output from ```summary(conc20a)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                    edf Ref.df  Chi.sq p-value
> > >>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> > >>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> > >>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> > >>> -REML = 8750.1  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>> ACF functions:
> > >>>
> > >>> [4]: https://i.stack.imgur.com/7nbXS.png
> > >>> [5]: https://i.stack.imgur.com/pNnZU.png
> > >>>
> > >>> Data can be found on my [GitHub][6] site in the file
> > >>> [data_cross_validated_post2.rds][7]. A csv version is also available.
> > >>> This is my code:
> > >>>
> > >>> ```r
> > >>> library(readr)
> > >>> library(mgcv)
> > >>>
> > >>> df <- read_rds("data_crossvalidated_post2.rds")
> > >>>
> > >>> # Create matrices for lagged weather variables (6 day lags) based on
> > >>> example by Simon Wood
> > >>> # in his 2017 book ("Generalized additive models: an introduction with
> > >>> R", p. 349) and
> > >>> # gamair package documentation
> > >>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> > >>>
> > >>> lagard <- function(x,n.lag=7) {
> > >>> n <- length(x); X <- matrix(NA,n,n.lag)
> > >>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > >>> X
> > >>> }
> > >>>
> > >>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> > >>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> > >>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> > >>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> > >>> = df$precip_hourly_dailysum)
> > >>> dat$temp_max <- lagard(df$temp_max)
> > >>> dat$temp_min <- lagard(df$temp_min)
> > >>> dat$temp_mean <- lagard(df$temp_mean)
> > >>> dat$wbgt_max <- lagard(df$wbgt_max)
> > >>> dat$wbgt_mean <- lagard(df$wbgt_mean)
> > >>> dat$wbgt_min <- lagard(df$wbgt_min)
> > >>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> > >>> dat$sh_mean <- lagard(df$sh_mean)
> > >>> dat$solar_mean <- lagard(df$solar_mean)
> > >>> dat$wind2m_mean <- lagard(df$wind2m_mean)
> > >>> dat$sh_max <- lagard(df$sh_max)
> > >>> dat$solar_max <- lagard(df$solar_max)
> > >>> dat$wind2m_max <- lagard(df$wind2m_max)
> > >>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> > >>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> > >>> dat$precip_hourly <- lagard(df$precip_hourly)
> > >>> dat$precip_daily_total <- lagard( df$precip_daily_total)
> > >>> dat$temp <- lagard(df$temp)
> > >>> dat$sh <- lagard(df$sh)
> > >>> dat$rh <- lagard(df$rh)
> > >>> dat$solar <- lagard(df$solar)
> > >>> dat$wind2m <- lagard(df$wind2m)
> > >>>
> > >>>
> > >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> > >>> bs=c("cr","cc","cc","cc")) + heap +
> > >>>                         te(temp_max, lag, k=c(10, 3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> > >>> 52.5), weekday = c(0, 6.5)))
> > >>>
> > >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> > >>>                        te(temp_max, lag, k=c(10, 4)) +
> > >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> > >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                        knots = list(month = c(0.5, 12.5)))
> > >>>
> > >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> > >>>                         te(temp_max, lag, k=c(10,3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10,3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> > >>>
> > >>> ```
> > >>> Thank you if you've read this far!! :-))
> > >>>
> > >>>     [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> > >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> > >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> > >>>     [4]: https://i.stack.imgur.com/7nbXS.png
> > >>>     [5]: https://i.stack.imgur.com/pNnZU.png
> > >>>     [6]: https://github.com/JadeShodan/heat-mortality
> > >>>     [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> > >>>
> > >>> ______________________________________________
> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >>> and provide commented, minimal, self-contained, reproducible code.
> > >>
> > >> --
> > >> Simon Wood, School of Mathematics, University of Edinburgh,
> > >> https://www.maths.ed.ac.uk/~swood34/
> > >>
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > --
> > Michael
> > http://www.dewey.myzen.co.uk/home.html


From j@de@shod@@ m@iii@g oii googiem@ii@com  Fri Jun 10 11:30:39 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Fri, 10 Jun 2022 10:30:39 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k_qMX24_a-Hppk6knrH-nt1QMLj2FFSJbkMJc0TBxqzQA@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-g=M41cxom6DVdTunFjgZ6VfKobqW__Mq351pDpY22PQ@mail.gmail.com>
 <f371005e-6f57-2007-f859-d66595a19c0f@bath.edu>
 <CANg3_k_qMX24_a-Hppk6knrH-nt1QMLj2FFSJbkMJc0TBxqzQA@mail.gmail.com>
Message-ID: <CANg3_k9pNAfJ8eC+XT3v8ugA6aXUcenmAQv7awHiGDOL_ytyHg@mail.gmail.com>

Hi Simon,

I don't think my reply to your email came through to the list, so am
resending (see below). Problems with subscription have now hopefully
been resolved. Apologies if this is a double posting!

On Thu, 9 Jun 2022 at 16:57, jade.shodan at googlemail.com
<jade.shodan at googlemail.com> wrote:
>
> Hi Simon,
>
> (Sorry, replies and answers are out of sync due to my problems posting
> to the list/ messages being held for moderation)
>
> > Did it actually fail, or simply generate warnings?
> The model was computed (you're right, not a model failure), but
> resulted in warnings (as per previous post) which, frankly, I didn't
> understand.
>
> > if I understand your model right there is one free parameter for each observation falling on the 15th
> That's right, one observation on each 15th day of the month.
>
> Thank you for the suggestion about the random effects! I had been
> wondering about how I could model this heaping with a smooth!
>
> Quick question:
>
> You proposed:
>
> aheap <- heap!="0"
> heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap
> day + a r.e. for each heap day - no effect on non-heap days
>
> Is there a typo in the code above? I don't see the newly created
> variable aheap in the model?
> Should it read "by = aheap" as follows:
>
> heap + s(heap,bs="re",by=aheap)   ?
>
> So a full model might then look like the one below?
>
>  bam0 <- bam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> s(heap,bs="re",by=aheap)
>                                      te(temp_max, lag, k=c(8, 3)) +
>                                      te(precip_daily_total, lag, k=c(8, 3)),
>                                      data = dat, family = nb, method =
> 'fREML', select = TRUE, discrete = TRUE,
>                                    knots = list(month = c(0.5, 12.5),
> week = c(0.5, 52.5), weekday = c(0, 6.5)))
>
> One, hopefully final (!) question:
>
> Is it actually useful at all to keep these observations on the 15th
> day of each month (which are huge errors), or am I better off removing
> them from the data set (or replacing them with e.g. median values)?
> For temperature-mortality modelling it is the day-to-day variation in
> deaths and temperature that is of interest. So is modelling heaping
> actually useful at all, given that this variable changes on a monthly
> basis? (I think you are alluding to this, but I just want to make
> sure).
>
> If I take them out altogether, would I be best off removing all data
> for these dates, so that the time series jumps from day 14 to day 16?
> Or would this create problems with e.g. the distributed lag model?
>
> Sorry for all these questions! Have been struggling with this for
> months (posted on Cross Validated about the heaping issue too), and
> feel I am finally getting somewhere with your help!
>
> Jade
>
> On Thu, 9 Jun 2022 at 15:24, Simon Wood <simon.wood at bath.edu> wrote:
> >
> >
> > On 09/06/2022 12:30, jade.shodan at googlemail.com wrote:
> > > Hi Simon,
> > >
> > > Thanks so much for this!! (Apologies if this is a double posting. I
> > > seem to have a problem getting messages through to the list).
> > >
> > > I have two follow up questions, if you don't mind.
> > >
> > > 1. Does including an autoregressive term not adjust away part of the
> > > effect of the response in a distributed lag model (where the outcome
> > > accumulates over time)?
> >
> > - the hope is that it approximately deals with short timescale stuff
> > without interfering with the longer timescales to the same extent as a
> > high rank smooth would.
> >
> > > 2. I've tried to fit a model using bam (just a first attempt without
> > > AR term), but including the factor variable heap creates errors:
> > >
> > > bam0 <- bam(deaths~te(year, month, week, weekday,
> > > bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> > >                                       te(temp_max, lag, k=c(8, 3)) +
> > >                                       te(precip_daily_total, lag, k=c(8, 3)),
> > >                                      data = dat, family = nb, method =
> > > 'fREML', select = TRUE, discrete = TRUE,
> > >                                      knots = list(month = c(0.5, 12.5),
> > > week = c(0.5, 52.5), weekday = c(0, 6.5)))
> > >
> > > This model results in errors:
> > >
> > > Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
> > >    step failure in theta estimation
> > > Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> > > object$prior.weights)) :
> > >    NaNs produced
> > >
> > Did it actually fail, or simply generate warnings?
> >
> > 'bam' handles factors, but if I understand your model right there is one
> > free parameter for each observation falling on the 15th, so that you
> > will fit data for those days exactly (and might just as well have
> > dropped them, for all the information they contribute to the rest of the
> > model). If you want a structure like this, I'd be inclined to make the
> > heap variable random, something like...
> >
> > aheap <- heap!="0"
> >
> > heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap day
> > + a r.e. for each heap day - no effect on non-heap days
> >
> > best,
> >
> > Simon
> >
> > > Including heap as as.numeric(heap) runs the model without error
> > > messages or warnings, but model diagnostics look terrible, and it also
> > > doesn't make sense (to me) to make heap a numeric. The factor variable
> > > heap (with 169 levels) codes the fact that all deaths for which no
> > > date was known, were registered on the 15th day of each month. I've
> > > coded all non-heaping days as 0. All heaping days were coded as a
> > > value between 1-168. The time series spans 14 years, so a heaping day
> > > in each month results in 14*12 levels = 168, plus one level for
> > > non-heaping days.
> > >
> > > So my second question is: Does bam allow factor variables? And if not,
> > > how should I model this heaping on the 15th day of the month instead?
> > >
> > > With thanks,
> > >
> > > Jade
> > >
> > > On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
> > >> I would not worry too much about high concurvity between variables like
> > >> temperature and time. This just reflects the fact that temperature has a
> > >> strong temporal pattern.
> > >>
> > >> I would also not be too worried about the low p-values on the k check.
> > >> The check only looks for pattern in the residuals when they are ordered
> > >> with respect to the variables of a smooth. When you have time series
> > >> data and some smooths involve time then it's hard not to pick up some
> > >> degree of residual auto-correlation, which you often would not want to
> > >> model with a higher rank smoother.
> > >>
> > >> The NAs  for the distributed lag terms just reflect the fact that there
> > >> is no obvious way to order the residuals w.r.t. the covariates for such
> > >> terms, so the simple check for residual pattern is not really possible.
> > >>
> > >> One simple approach is to fit using bam(...,discrete=TRUE) which will
> > >> let you specify an AR1 parameter to mop up some of the residual
> > >> auto-correlation without resorting to a high rank smooth that then does
> > >> all the work of the covariates as well. The AR1 parameter can be set by
> > >> looking at the ACF of the residuals of the model without this. You need
> > >> to look at the ACF of suitably standardized residuals to check how well
> > >> this has worked.
> > >>
> > >> best,
> > >>
> > >> Simon
> > >>
> > >> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> > >>> Hello everyone,
> > >>>
> > >>> A few days ago I asked a question about concurvity in a GAM (the
> > >>> anologue of collinearity in a GLM) implemented in mgcv. I think my
> > >>> question was a bit unfocussed, so I am retrying again, but with
> > >>> additional information included about the autocorrelation function. I
> > >>> have also posted about this on Cross Validated. Given all the model
> > >>> output, it might make for easier
> > >>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> > >>>
> > >>> As mentioned previously, I have problems with concurvity in my thesis
> > >>> research, and don't have access to a statistician who works with time
> > >>> series, GAMs or R. I'd be very grateful for any (partial) answer,
> > >>> however short. I'll gladly return the favour where I can! For really
> > >>> helpful input I'd be more than happy to offer co-authorship on
> > >>> publication. Deadlines are very close, and I'm heading towards having
> > >>> no results at all if I can't solve this concurvity issue :(
> > >>>
> > >>> I'm using GAMs to try to understand the relationship between deaths
> > >>> and heat-related variables (e.g. temperature and humidity), using
> > >>> daily time series over a 14-year period from a tropical, low-income
> > >>> country. My aim is to understand the relationship between these
> > >>> variables and deaths, rather than pure prediction performance.
> > >>>
> > >>> The GAMs include distributed lag models (set up as 7-column matrices,
> > >>> see code at bottom of post), since deaths may occur over several days
> > >>> following exposure.
> > >>>
> > >>> Simple GAMs with just time, lagged temperature and lagged
> > >>> precipitation (a potential confounder) show very high concurvity
> > >>> between lagged temperature and time, regardless of the many different
> > >>> ways I have tried to decompose time. The autocorrelation functions
> > >>> (ACF) however, shows values close to zero, only just about breaching
> > >>> the 'significance line' in a few instances. It does show patterning
> > >>> though, although the regularity is difficult to define.
> > >>>
> > >>> My questions are:
> > >>> 1) Should I be worried about the high concurvity, or can I ignore it
> > >>> given the mostly non-significant ACF? I've read dozens of
> > >>> heat-mortality modelling studies and none report on concurvity between
> > >>> weather variables and time (though one 2012 paper discussed
> > >>> autocorrelation).
> > >>>
> > >>> 2) If I cannot ignore it, what should I do to resolve it? Would
> > >>> including an autoregressive term be appropriate, and if so, where can
> > >>> I find a coded example of how to do this? I've also come across
> > >>> sequential regression][1]. Would this be more or less appropriate? If
> > >>> appropriate, a pointer to an example would be really appreciated!
> > >>>
> > >>> Some example GAMs are specified as follows:
> > >>> ```r
> > >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> > >>> bs=c("cr","cc","cc","cc")) + heap +
> > >>>                         te(temp_max, lag, k=c(10, 3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> > >>> 52.5), weekday = c(0, 6.5)))
> > >>> ```
> > >>> Concurvity for the above model between (temp_max, lag) and (year,
> > >>> month, week, weekday) is 0.91:
> > >>>
> > >>> ```r
> > >>> $worst
> > >>>                                       para te(year,month,week,weekday)
> > >>> te(temp_max,lag) te(precip_daily_total,lag)
> > >>> para                        1.000000e+00                1.125625e-29
> > >>>        0.3150073                  0.6666348
> > >>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> > >>>        0.9060552                  0.6652313
> > >>> te(temp_max,lag)            3.152795e-01                8.998113e-01
> > >>>        1.0000000                  0.5781015
> > >>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> > >>>        0.5805159                  1.0000000
> > >>> ```
> > >>>
> > >>> Output from ```gam.check()```:
> > >>> ```r
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 16 iterations.
> > >>> Gradient range [-0.01467332,0.003096643]
> > >>> (score 8915.994 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> > >>> Model rank =  544 / 544
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                     k'      edf k-index p-value
> > >>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> > >>> te(temp_max,lag)             29.0000   3.3681      NA      NA
> > >>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>> ```
> > >>>
> > >>> Some output from ```summary(conc38b)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                     edf Ref.df  Chi.sq p-value
> > >>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> > >>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> > >>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> > >>> -REML =   8916  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>>
> > >>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> > >>> pattern). They show peaks at 5 and 15, and then there seems to be a
> > >>> recurring pattern at multiples of approx. 30 (suggesting month is not
> > >>> modelled adequately?). Not sure what would cause the spikes at 5 and
> > >>> 15. There is heaping of deaths on the 15th day of each month, to which
> > >>> deaths with unknown date were allocated. This heaping was modelled
> > >>> with categorical variable/ factor ```heap``` with 169 levels (0 for
> > >>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> > >>> heaping day over the 14-year period):
> > >>>
> > >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> > >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> > >>>
> > >>>
> > >>> I get an identical looking ACF when I decompose time into (year,
> > >>> month, monthday) as in model conc39 below, although concurvity between
> > >>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> > >>>
> > >>> ```r
> > >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> > >>>                        te(temp_max, lag, k=c(10, 4)) +
> > >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> > >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                        knots = list(month = c(0.5, 12.5)))
> > >>> ```
> > >>> ```r
> > >>>
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 14 iterations.
> > >>> Gradient range [-0.001578187,6.155096e-05]
> > >>> (score 8915.763 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> > >>> Model rank =  323 / 323
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                   k'     edf k-index p-value
> > >>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> > >>> te(temp_max,lag)           39.0000  4.0875      NA      NA
> > >>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>> ```
> > >>> Some output from ```summary(conc39)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                   edf Ref.df  Chi.sq  p-value
> > >>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> > >>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> > >>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> > >>> -REML =   8915  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>>
> > >>> ```r
> > >>> $worst
> > >>>                                      para te(year,month,monthday)
> > >>> te(temp_max,lag) te(precip_daily_total,lag)
> > >>> para                       1.000000e+00            3.261007e-31
> > >>> 0.3313549                  0.6666532
> > >>> te(year,month,monthday)    3.060763e-31            1.000000e+00
> > >>> 0.8266086                  0.5670777
> > >>> te(temp_max,lag)           3.331014e-01            8.225942e-01
> > >>> 1.0000000                  0.5840875
> > >>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> > >>> 0.5939380                  1.0000000
> > >>> ```
> > >>>
> > >>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> > >>> various choices for k or as ```s(time)``` with various k does not
> > >>> reduce concurvity either.
> > >>>
> > >>>
> > >>> The default approach in time series studies of heat-mortality is to
> > >>> model time with fixed df, generally between 7-10 df per year of data.
> > >>> I am, however, apprehensive about this approach because a) mortality
> > >>> profiles vary with locality due to sociodemographic and environmental
> > >>> characteristics and b) the choice of df is based on higher income
> > >>> countries (where nearly all these studies have been done) with
> > >>> different mortality profiles and so may not be appropriate for
> > >>> tropical, low-income countries.
> > >>>
> > >>> Although the approach of fixing (high) df does remove more temporal
> > >>> patterns from the ACF (see model and output below), concurvity between
> > >>> time and lagged temperature has now risen to 0.99! Moreover,
> > >>> temperature (which has been a consistent, highly significant predictor
> > >>> in every model of the tens (hundreds?) I have run, has now turned
> > >>> non-significant. I am guessing this is because time is now a very
> > >>> wiggly function that not only models/ removes seasonal variation, but
> > >>> also some of the day-to-day variation that is needed for the
> > >>> temperature smooth  :
> > >>>
> > >>> ```r
> > >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> > >>>                         te(temp_max, lag, k=c(10,3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10,3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> > >>> ```
> > >>> Output from ```gam.check(conc20a, rep = 1000)```:
> > >>>
> > >>> ```r
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 9 iterations.
> > >>> Gradient range [-0.0008983099,9.546022e-05]
> > >>> (score 8750.13 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> > >>> Model rank =  336 / 336
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                    k'      edf k-index p-value
> > >>> s(time)                    111.0000 111.0000    0.98    0.56
> > >>> te(temp_max,lag)            29.0000   0.6548      NA      NA
> > >>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> > >>> ```
> > >>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
> > >>>
> > >>> ```r
> > >>>                                      para      s(time) te(temp_max,lag)
> > >>> te(precip_daily_total,lag)
> > >>> para                       1.000000e+00 2.462064e-19        0.3165236
> > >>>                   0.6666348
> > >>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
> > >>>                   0.6879284
> > >>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> > >>>                   0.5788711
> > >>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> > >>>                   1.0000000
> > >>>
> > >>> ```
> > >>>
> > >>> Some output from ```summary(conc20a)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                    edf Ref.df  Chi.sq p-value
> > >>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> > >>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> > >>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> > >>> -REML = 8750.1  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>> ACF functions:
> > >>>
> > >>> [4]: https://i.stack.imgur.com/7nbXS.png
> > >>> [5]: https://i.stack.imgur.com/pNnZU.png
> > >>>
> > >>> Data can be found on my [GitHub][6] site in the file
> > >>> [data_cross_validated_post2.rds][7]. A csv version is also available.
> > >>> This is my code:
> > >>>
> > >>> ```r
> > >>> library(readr)
> > >>> library(mgcv)
> > >>>
> > >>> df <- read_rds("data_crossvalidated_post2.rds")
> > >>>
> > >>> # Create matrices for lagged weather variables (6 day lags) based on
> > >>> example by Simon Wood
> > >>> # in his 2017 book ("Generalized additive models: an introduction with
> > >>> R", p. 349) and
> > >>> # gamair package documentation
> > >>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> > >>>
> > >>> lagard <- function(x,n.lag=7) {
> > >>> n <- length(x); X <- matrix(NA,n,n.lag)
> > >>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > >>> X
> > >>> }
> > >>>
> > >>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> > >>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> > >>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> > >>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> > >>> = df$precip_hourly_dailysum)
> > >>> dat$temp_max <- lagard(df$temp_max)
> > >>> dat$temp_min <- lagard(df$temp_min)
> > >>> dat$temp_mean <- lagard(df$temp_mean)
> > >>> dat$wbgt_max <- lagard(df$wbgt_max)
> > >>> dat$wbgt_mean <- lagard(df$wbgt_mean)
> > >>> dat$wbgt_min <- lagard(df$wbgt_min)
> > >>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> > >>> dat$sh_mean <- lagard(df$sh_mean)
> > >>> dat$solar_mean <- lagard(df$solar_mean)
> > >>> dat$wind2m_mean <- lagard(df$wind2m_mean)
> > >>> dat$sh_max <- lagard(df$sh_max)
> > >>> dat$solar_max <- lagard(df$solar_max)
> > >>> dat$wind2m_max <- lagard(df$wind2m_max)
> > >>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> > >>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> > >>> dat$precip_hourly <- lagard(df$precip_hourly)
> > >>> dat$precip_daily_total <- lagard( df$precip_daily_total)
> > >>> dat$temp <- lagard(df$temp)
> > >>> dat$sh <- lagard(df$sh)
> > >>> dat$rh <- lagard(df$rh)
> > >>> dat$solar <- lagard(df$solar)
> > >>> dat$wind2m <- lagard(df$wind2m)
> > >>>
> > >>>
> > >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> > >>> bs=c("cr","cc","cc","cc")) + heap +
> > >>>                         te(temp_max, lag, k=c(10, 3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> > >>> 52.5), weekday = c(0, 6.5)))
> > >>>
> > >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> > >>>                        te(temp_max, lag, k=c(10, 4)) +
> > >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> > >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                        knots = list(month = c(0.5, 12.5)))
> > >>>
> > >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> > >>>                         te(temp_max, lag, k=c(10,3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10,3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> > >>>
> > >>> ```
> > >>> Thank you if you've read this far!! :-))
> > >>>
> > >>>     [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> > >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> > >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> > >>>     [4]: https://i.stack.imgur.com/7nbXS.png
> > >>>     [5]: https://i.stack.imgur.com/pNnZU.png
> > >>>     [6]: https://github.com/JadeShodan/heat-mortality
> > >>>     [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> > >>>
> > >>> ______________________________________________
> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >>> and provide commented, minimal, self-contained, reproducible code.
> > >> --
> > >> Simon Wood, School of Mathematics, University of Edinburgh,
> > >> https://www.maths.ed.ac.uk/~swood34/
> > >>
> > --
> > Simon Wood, School of Mathematics, University of Edinburgh,
> > https://www.maths.ed.ac.uk/~swood34/
> >


From j@de@shod@@ m@iii@g oii googiem@ii@com  Fri Jun 10 12:01:24 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Fri, 10 Jun 2022 11:01:24 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k_qMX24_a-Hppk6knrH-nt1QMLj2FFSJbkMJc0TBxqzQA@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-g=M41cxom6DVdTunFjgZ6VfKobqW__Mq351pDpY22PQ@mail.gmail.com>
 <f371005e-6f57-2007-f859-d66595a19c0f@bath.edu>
 <CANg3_k_qMX24_a-Hppk6knrH-nt1QMLj2FFSJbkMJc0TBxqzQA@mail.gmail.com>
Message-ID: <CANg3_k8Z_D63TTiWPPS24PhRHP_86=TUoxETn-2GGTg0VDqopg@mail.gmail.com>

Hi Simon,

I don't think my reply to your email came through to the list, so am
resending (see below). I hope that problems with subscription and
posting have now
been resolved. Apologies to anyone who is receiving multiple copies!

On Thu, 9 Jun 2022 at 16:57, jade.shodan at googlemail.com
<jade.shodan at googlemail.com> wrote:
>
> Hi Simon,
>
> (Sorry, replies and answers are out of sync due to my problems posting
> to the list/ messages being held for moderation)
>
> > Did it actually fail, or simply generate warnings?
> The model was computed (you're right, not a model failure), but
> resulted in warnings (as per previous post) which, frankly, I didn't
> understand.
>
> > if I understand your model right there is one free parameter for each observation falling on the 15th
> That's right, one observation on each 15th day of the month.
>
> Thank you for the suggestion about the random effects! I had been
> wondering about how I could model this heaping with a smooth!
>
> Quick question:
>
> You proposed:
>
> aheap <- heap!="0"
> heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap
> day + a r.e. for each heap day - no effect on non-heap days
>
> Is there a typo in the code above? I don't see the newly created
> variable aheap in the model?
> Should it read "by = aheap" as follows:
>
> heap + s(heap,bs="re",by=aheap)   ?
>
> So a full model might then look like the one below?
>
>  bam0 <- bam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> s(heap,bs="re",by=aheap)
>                                      te(temp_max, lag, k=c(8, 3)) +
>                                      te(precip_daily_total, lag, k=c(8, 3)),
>                                      data = dat, family = nb, method =
> 'fREML', select = TRUE, discrete = TRUE,
>                                    knots = list(month = c(0.5, 12.5),
> week = c(0.5, 52.5), weekday = c(0, 6.5)))
>
> One, hopefully final (!) question:
>
> Is it actually useful at all to keep these observations on the 15th
> day of each month (which are huge errors), or am I better off removing
> them from the data set (or replacing them with e.g. median values)?
> For temperature-mortality modelling it is the day-to-day variation in
> deaths and temperature that is of interest. So is modelling heaping
> actually useful at all, given that this variable changes on a monthly
> basis? (I think you are alluding to this, but I just want to make
> sure).
>
> If I take them out altogether, would I be best off removing all data
> for these dates, so that the time series jumps from day 14 to day 16?
> Or would this create problems with e.g. the distributed lag model?
>
> Sorry for all these questions! Have been struggling with this for
> months (posted on Cross Validated about the heaping issue too), and
> feel I am finally getting somewhere with your help!
>
> Jade
>
> On Thu, 9 Jun 2022 at 15:24, Simon Wood <simon.wood at bath.edu> wrote:
> >
> >
> > On 09/06/2022 12:30, jade.shodan at googlemail.com wrote:
> > > Hi Simon,
> > >
> > > Thanks so much for this!! (Apologies if this is a double posting. I
> > > seem to have a problem getting messages through to the list).
> > >
> > > I have two follow up questions, if you don't mind.
> > >
> > > 1. Does including an autoregressive term not adjust away part of the
> > > effect of the response in a distributed lag model (where the outcome
> > > accumulates over time)?
> >
> > - the hope is that it approximately deals with short timescale stuff
> > without interfering with the longer timescales to the same extent as a
> > high rank smooth would.
> >
> > > 2. I've tried to fit a model using bam (just a first attempt without
> > > AR term), but including the factor variable heap creates errors:
> > >
> > > bam0 <- bam(deaths~te(year, month, week, weekday,
> > > bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> > >                                       te(temp_max, lag, k=c(8, 3)) +
> > >                                       te(precip_daily_total, lag, k=c(8, 3)),
> > >                                      data = dat, family = nb, method =
> > > 'fREML', select = TRUE, discrete = TRUE,
> > >                                      knots = list(month = c(0.5, 12.5),
> > > week = c(0.5, 52.5), weekday = c(0, 6.5)))
> > >
> > > This model results in errors:
> > >
> > > Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
> > >    step failure in theta estimation
> > > Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> > > object$prior.weights)) :
> > >    NaNs produced
> > >
> > Did it actually fail, or simply generate warnings?
> >
> > 'bam' handles factors, but if I understand your model right there is one
> > free parameter for each observation falling on the 15th, so that you
> > will fit data for those days exactly (and might just as well have
> > dropped them, for all the information they contribute to the rest of the
> > model). If you want a structure like this, I'd be inclined to make the
> > heap variable random, something like...
> >
> > aheap <- heap!="0"
> >
> > heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap day
> > + a r.e. for each heap day - no effect on non-heap days
> >
> > best,
> >
> > Simon
> >
> > > Including heap as as.numeric(heap) runs the model without error
> > > messages or warnings, but model diagnostics look terrible, and it also
> > > doesn't make sense (to me) to make heap a numeric. The factor variable
> > > heap (with 169 levels) codes the fact that all deaths for which no
> > > date was known, were registered on the 15th day of each month. I've
> > > coded all non-heaping days as 0. All heaping days were coded as a
> > > value between 1-168. The time series spans 14 years, so a heaping day
> > > in each month results in 14*12 levels = 168, plus one level for
> > > non-heaping days.
> > >
> > > So my second question is: Does bam allow factor variables? And if not,
> > > how should I model this heaping on the 15th day of the month instead?
> > >
> > > With thanks,
> > >
> > > Jade
> > >
> > > On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
> > >> I would not worry too much about high concurvity between variables like
> > >> temperature and time. This just reflects the fact that temperature has a
> > >> strong temporal pattern.
> > >>
> > >> I would also not be too worried about the low p-values on the k check.
> > >> The check only looks for pattern in the residuals when they are ordered
> > >> with respect to the variables of a smooth. When you have time series
> > >> data and some smooths involve time then it's hard not to pick up some
> > >> degree of residual auto-correlation, which you often would not want to
> > >> model with a higher rank smoother.
> > >>
> > >> The NAs  for the distributed lag terms just reflect the fact that there
> > >> is no obvious way to order the residuals w.r.t. the covariates for such
> > >> terms, so the simple check for residual pattern is not really possible.
> > >>
> > >> One simple approach is to fit using bam(...,discrete=TRUE) which will
> > >> let you specify an AR1 parameter to mop up some of the residual
> > >> auto-correlation without resorting to a high rank smooth that then does
> > >> all the work of the covariates as well. The AR1 parameter can be set by
> > >> looking at the ACF of the residuals of the model without this. You need
> > >> to look at the ACF of suitably standardized residuals to check how well
> > >> this has worked.
> > >>
> > >> best,
> > >>
> > >> Simon
> > >>
> > >> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> > >>> Hello everyone,
> > >>>
> > >>> A few days ago I asked a question about concurvity in a GAM (the
> > >>> anologue of collinearity in a GLM) implemented in mgcv. I think my
> > >>> question was a bit unfocussed, so I am retrying again, but with
> > >>> additional information included about the autocorrelation function. I
> > >>> have also posted about this on Cross Validated. Given all the model
> > >>> output, it might make for easier
> > >>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> > >>>
> > >>> As mentioned previously, I have problems with concurvity in my thesis
> > >>> research, and don't have access to a statistician who works with time
> > >>> series, GAMs or R. I'd be very grateful for any (partial) answer,
> > >>> however short. I'll gladly return the favour where I can! For really
> > >>> helpful input I'd be more than happy to offer co-authorship on
> > >>> publication. Deadlines are very close, and I'm heading towards having
> > >>> no results at all if I can't solve this concurvity issue :(
> > >>>
> > >>> I'm using GAMs to try to understand the relationship between deaths
> > >>> and heat-related variables (e.g. temperature and humidity), using
> > >>> daily time series over a 14-year period from a tropical, low-income
> > >>> country. My aim is to understand the relationship between these
> > >>> variables and deaths, rather than pure prediction performance.
> > >>>
> > >>> The GAMs include distributed lag models (set up as 7-column matrices,
> > >>> see code at bottom of post), since deaths may occur over several days
> > >>> following exposure.
> > >>>
> > >>> Simple GAMs with just time, lagged temperature and lagged
> > >>> precipitation (a potential confounder) show very high concurvity
> > >>> between lagged temperature and time, regardless of the many different
> > >>> ways I have tried to decompose time. The autocorrelation functions
> > >>> (ACF) however, shows values close to zero, only just about breaching
> > >>> the 'significance line' in a few instances. It does show patterning
> > >>> though, although the regularity is difficult to define.
> > >>>
> > >>> My questions are:
> > >>> 1) Should I be worried about the high concurvity, or can I ignore it
> > >>> given the mostly non-significant ACF? I've read dozens of
> > >>> heat-mortality modelling studies and none report on concurvity between
> > >>> weather variables and time (though one 2012 paper discussed
> > >>> autocorrelation).
> > >>>
> > >>> 2) If I cannot ignore it, what should I do to resolve it? Would
> > >>> including an autoregressive term be appropriate, and if so, where can
> > >>> I find a coded example of how to do this? I've also come across
> > >>> sequential regression][1]. Would this be more or less appropriate? If
> > >>> appropriate, a pointer to an example would be really appreciated!
> > >>>
> > >>> Some example GAMs are specified as follows:
> > >>> ```r
> > >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> > >>> bs=c("cr","cc","cc","cc")) + heap +
> > >>>                         te(temp_max, lag, k=c(10, 3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> > >>> 52.5), weekday = c(0, 6.5)))
> > >>> ```
> > >>> Concurvity for the above model between (temp_max, lag) and (year,
> > >>> month, week, weekday) is 0.91:
> > >>>
> > >>> ```r
> > >>> $worst
> > >>>                                       para te(year,month,week,weekday)
> > >>> te(temp_max,lag) te(precip_daily_total,lag)
> > >>> para                        1.000000e+00                1.125625e-29
> > >>>        0.3150073                  0.6666348
> > >>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> > >>>        0.9060552                  0.6652313
> > >>> te(temp_max,lag)            3.152795e-01                8.998113e-01
> > >>>        1.0000000                  0.5781015
> > >>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> > >>>        0.5805159                  1.0000000
> > >>> ```
> > >>>
> > >>> Output from ```gam.check()```:
> > >>> ```r
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 16 iterations.
> > >>> Gradient range [-0.01467332,0.003096643]
> > >>> (score 8915.994 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> > >>> Model rank =  544 / 544
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                     k'      edf k-index p-value
> > >>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> > >>> te(temp_max,lag)             29.0000   3.3681      NA      NA
> > >>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>> ```
> > >>>
> > >>> Some output from ```summary(conc38b)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                     edf Ref.df  Chi.sq p-value
> > >>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> > >>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> > >>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> > >>> -REML =   8916  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>>
> > >>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> > >>> pattern). They show peaks at 5 and 15, and then there seems to be a
> > >>> recurring pattern at multiples of approx. 30 (suggesting month is not
> > >>> modelled adequately?). Not sure what would cause the spikes at 5 and
> > >>> 15. There is heaping of deaths on the 15th day of each month, to which
> > >>> deaths with unknown date were allocated. This heaping was modelled
> > >>> with categorical variable/ factor ```heap``` with 169 levels (0 for
> > >>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> > >>> heaping day over the 14-year period):
> > >>>
> > >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> > >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> > >>>
> > >>>
> > >>> I get an identical looking ACF when I decompose time into (year,
> > >>> month, monthday) as in model conc39 below, although concurvity between
> > >>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> > >>>
> > >>> ```r
> > >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> > >>>                        te(temp_max, lag, k=c(10, 4)) +
> > >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> > >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                        knots = list(month = c(0.5, 12.5)))
> > >>> ```
> > >>> ```r
> > >>>
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 14 iterations.
> > >>> Gradient range [-0.001578187,6.155096e-05]
> > >>> (score 8915.763 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> > >>> Model rank =  323 / 323
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                   k'     edf k-index p-value
> > >>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> > >>> te(temp_max,lag)           39.0000  4.0875      NA      NA
> > >>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>> ```
> > >>> Some output from ```summary(conc39)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                   edf Ref.df  Chi.sq  p-value
> > >>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> > >>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> > >>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> > >>> -REML =   8915  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>>
> > >>> ```r
> > >>> $worst
> > >>>                                      para te(year,month,monthday)
> > >>> te(temp_max,lag) te(precip_daily_total,lag)
> > >>> para                       1.000000e+00            3.261007e-31
> > >>> 0.3313549                  0.6666532
> > >>> te(year,month,monthday)    3.060763e-31            1.000000e+00
> > >>> 0.8266086                  0.5670777
> > >>> te(temp_max,lag)           3.331014e-01            8.225942e-01
> > >>> 1.0000000                  0.5840875
> > >>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> > >>> 0.5939380                  1.0000000
> > >>> ```
> > >>>
> > >>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> > >>> various choices for k or as ```s(time)``` with various k does not
> > >>> reduce concurvity either.
> > >>>
> > >>>
> > >>> The default approach in time series studies of heat-mortality is to
> > >>> model time with fixed df, generally between 7-10 df per year of data.
> > >>> I am, however, apprehensive about this approach because a) mortality
> > >>> profiles vary with locality due to sociodemographic and environmental
> > >>> characteristics and b) the choice of df is based on higher income
> > >>> countries (where nearly all these studies have been done) with
> > >>> different mortality profiles and so may not be appropriate for
> > >>> tropical, low-income countries.
> > >>>
> > >>> Although the approach of fixing (high) df does remove more temporal
> > >>> patterns from the ACF (see model and output below), concurvity between
> > >>> time and lagged temperature has now risen to 0.99! Moreover,
> > >>> temperature (which has been a consistent, highly significant predictor
> > >>> in every model of the tens (hundreds?) I have run, has now turned
> > >>> non-significant. I am guessing this is because time is now a very
> > >>> wiggly function that not only models/ removes seasonal variation, but
> > >>> also some of the day-to-day variation that is needed for the
> > >>> temperature smooth  :
> > >>>
> > >>> ```r
> > >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> > >>>                         te(temp_max, lag, k=c(10,3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10,3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> > >>> ```
> > >>> Output from ```gam.check(conc20a, rep = 1000)```:
> > >>>
> > >>> ```r
> > >>> Method: REML   Optimizer: outer newton
> > >>> full convergence after 9 iterations.
> > >>> Gradient range [-0.0008983099,9.546022e-05]
> > >>> (score 8750.13 & scale 1).
> > >>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> > >>> Model rank =  336 / 336
> > >>>
> > >>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> > >>> indicate that k is too low, especially if edf is close to k'.
> > >>>
> > >>>                                    k'      edf k-index p-value
> > >>> s(time)                    111.0000 111.0000    0.98    0.56
> > >>> te(temp_max,lag)            29.0000   0.6548      NA      NA
> > >>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> > >>> ```
> > >>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
> > >>>
> > >>> ```r
> > >>>                                      para      s(time) te(temp_max,lag)
> > >>> te(precip_daily_total,lag)
> > >>> para                       1.000000e+00 2.462064e-19        0.3165236
> > >>>                   0.6666348
> > >>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
> > >>>                   0.6879284
> > >>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> > >>>                   0.5788711
> > >>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> > >>>                   1.0000000
> > >>>
> > >>> ```
> > >>>
> > >>> Some output from ```summary(conc20a)```:
> > >>> ```r
> > >>> Approximate significance of smooth terms:
> > >>>                                    edf Ref.df  Chi.sq p-value
> > >>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> > >>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> > >>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> > >>> ---
> > >>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >>>
> > >>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> > >>> -REML = 8750.1  Scale est. = 1         n = 5107
> > >>> ```
> > >>>
> > >>> ACF functions:
> > >>>
> > >>> [4]: https://i.stack.imgur.com/7nbXS.png
> > >>> [5]: https://i.stack.imgur.com/pNnZU.png
> > >>>
> > >>> Data can be found on my [GitHub][6] site in the file
> > >>> [data_cross_validated_post2.rds][7]. A csv version is also available.
> > >>> This is my code:
> > >>>
> > >>> ```r
> > >>> library(readr)
> > >>> library(mgcv)
> > >>>
> > >>> df <- read_rds("data_crossvalidated_post2.rds")
> > >>>
> > >>> # Create matrices for lagged weather variables (6 day lags) based on
> > >>> example by Simon Wood
> > >>> # in his 2017 book ("Generalized additive models: an introduction with
> > >>> R", p. 349) and
> > >>> # gamair package documentation
> > >>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> > >>>
> > >>> lagard <- function(x,n.lag=7) {
> > >>> n <- length(x); X <- matrix(NA,n,n.lag)
> > >>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > >>> X
> > >>> }
> > >>>
> > >>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> > >>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> > >>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> > >>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> > >>> = df$precip_hourly_dailysum)
> > >>> dat$temp_max <- lagard(df$temp_max)
> > >>> dat$temp_min <- lagard(df$temp_min)
> > >>> dat$temp_mean <- lagard(df$temp_mean)
> > >>> dat$wbgt_max <- lagard(df$wbgt_max)
> > >>> dat$wbgt_mean <- lagard(df$wbgt_mean)
> > >>> dat$wbgt_min <- lagard(df$wbgt_min)
> > >>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> > >>> dat$sh_mean <- lagard(df$sh_mean)
> > >>> dat$solar_mean <- lagard(df$solar_mean)
> > >>> dat$wind2m_mean <- lagard(df$wind2m_mean)
> > >>> dat$sh_max <- lagard(df$sh_max)
> > >>> dat$solar_max <- lagard(df$solar_max)
> > >>> dat$wind2m_max <- lagard(df$wind2m_max)
> > >>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> > >>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> > >>> dat$precip_hourly <- lagard(df$precip_hourly)
> > >>> dat$precip_daily_total <- lagard( df$precip_daily_total)
> > >>> dat$temp <- lagard(df$temp)
> > >>> dat$sh <- lagard(df$sh)
> > >>> dat$rh <- lagard(df$rh)
> > >>> dat$solar <- lagard(df$solar)
> > >>> dat$wind2m <- lagard(df$wind2m)
> > >>>
> > >>>
> > >>> conc38b <- gam(deaths~te(year, month, week, weekday,
> > >>> bs=c("cr","cc","cc","cc")) + heap +
> > >>>                         te(temp_max, lag, k=c(10, 3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10, 3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> > >>> 52.5), weekday = c(0, 6.5)))
> > >>>
> > >>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> > >>>                        te(temp_max, lag, k=c(10, 4)) +
> > >>>                        te(precip_daily_total, lag, k=c(10, 4)),
> > >>>                        data = dat, family = nb, method = 'REML', select = TRUE,
> > >>>                        knots = list(month = c(0.5, 12.5)))
> > >>>
> > >>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> > >>>                         te(temp_max, lag, k=c(10,3)) +
> > >>>                         te(precip_daily_total, lag, k=c(10,3)),
> > >>>                         data = dat, family = nb, method = 'REML', select = TRUE)
> > >>>
> > >>> ```
> > >>> Thank you if you've read this far!! :-))
> > >>>
> > >>>     [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> > >>>     [2]: https://i.stack.imgur.com/FzKyM.png
> > >>>     [3]: https://i.stack.imgur.com/fE3aL.png
> > >>>     [4]: https://i.stack.imgur.com/7nbXS.png
> > >>>     [5]: https://i.stack.imgur.com/pNnZU.png
> > >>>     [6]: https://github.com/JadeShodan/heat-mortality
> > >>>     [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> > >>>
> > >>> ______________________________________________
> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >>> and provide commented, minimal, self-contained, reproducible code.
> > >> --
> > >> Simon Wood, School of Mathematics, University of Edinburgh,
> > >> https://www.maths.ed.ac.uk/~swood34/
> > >>
> > --
> > Simon Wood, School of Mathematics, University of Edinburgh,
> > https://www.maths.ed.ac.uk/~swood34/
> >


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Fri Jun 10 12:19:57 2022
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Fri, 10 Jun 2022 12:19:57 +0200
Subject: [R] how to add comma to string vector?
Message-ID: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>

Hello,
I need to convert an R vector to a python array. Essentially, I got a
vector of strings where each element must be enclosed in single
quotes. The problem is that each closing single quote should contain a
comma. What is the regex trick to do that?
I tried with:
```
> (raws = c("field_1", "field_2",  "field_3"))
[1] "field_1" "field_2" "field_3"
> (items = sQuote(raws))
[1] "'field_1'" "'field_2'" "'field_3'"
> gsub("\'[:blank:]", "\',[:blank:]", items,  ignore.case = FALSE, perl = FALSE)
[1] "'field_1'" "'field_2'" "'field_3'"
> cat("python_Array = [", domain_list, "]\n")
python_Array = [ 'Index' 'Endpoints' 'Confounders' 'Arboexposure' ]
```
To note that `python_Array` does not have commas between elements and
`gsub` did not do anything...

Thank you


From jr@| @end|ng |rom po@teo@no  Fri Jun 10 12:31:56 2022
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Fri, 10 Jun 2022 10:31:56 +0000
Subject: [R] how to add comma to string vector?
In-Reply-To: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
References: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
Message-ID: <YqMdnKIYlOXIrw1X@posteo.no>

Hello ...  

raws <- c("field_1", "field_2",  "field_3")
paste0("['", paste0(raws, collapse="', '"), "']")

rasmus at eightforty ~ % python
Python 3.10.5 (main, Jun  6 2022, 18:49:26) [GCC 12.1.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> ['field_1', 'field_2', 'field_3']
['field_1', 'field_2', 'field_3']

?

R


From kry|ov@r00t @end|ng |rom gm@||@com  Fri Jun 10 12:34:00 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Fri, 10 Jun 2022 13:34:00 +0300
Subject: [R] how to add comma to string vector?
In-Reply-To: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
References: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
Message-ID: <20220610133400.14bdf8cf@arachnoid>

On Fri, 10 Jun 2022 12:19:57 +0200
Luigi Marongiu <marongiu.luigi at gmail.com> wrote:

> I need to convert an R vector to a python array.

Have you considered using the reticulate package on the R side or the
rpy2 package on the Python side? It's hard to cover all edge cases when
producing source code to be evaluated by another language. You may
encounter a corner case where you output a character special to Python
without properly escaping it and end up with an injection error (see
also: SQL injection vulnerability, the most widely known type of this
mistake). For producing R code from R values, there's deparse(), and
even that's not perfect:
https://bugs.r-project.org/show_bug.cgi?id=18232

Having said that, paste(sQuote(values, FALSE), collapse = ',') will do
the trick, but only if values are guaranteed not to contain single
quotes or other characters that have special meaning in Python. Note
the FALSE argument to sQuote: otherwise it could return ?Unicode
quotes?, `TeX quotes', or even ?guillemets?, depending on the options
set by the user.

-- 
Best regards,
Ivan


From tebert @end|ng |rom u||@edu  Fri Jun 10 12:48:26 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Fri, 10 Jun 2022 10:48:26 +0000
Subject: [R] how to add comma to string vector?
In-Reply-To: <20220610133400.14bdf8cf@arachnoid>
References: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
 <20220610133400.14bdf8cf@arachnoid>
Message-ID: <BN6PR2201MB15537DC973F89F73BDDBE727CFA69@BN6PR2201MB1553.namprd22.prod.outlook.com>

Have you considered saving it to a csv file in R and then reading the file in python?
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ivan Krylov
Sent: Friday, June 10, 2022 6:34 AM
To: Luigi Marongiu <marongiu.luigi at gmail.com>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] how to add comma to string vector?

[External Email]

On Fri, 10 Jun 2022 12:19:57 +0200
Luigi Marongiu <marongiu.luigi at gmail.com> wrote:

> I need to convert an R vector to a python array.

Have you considered using the reticulate package on the R side or the
rpy2 package on the Python side? It's hard to cover all edge cases when producing source code to be evaluated by another language. You may encounter a corner case where you output a character special to Python without properly escaping it and end up with an injection error (see
also: SQL injection vulnerability, the most widely known type of this mistake). For producing R code from R values, there's deparse(), and even that's not perfect:
https://urldefense.proofpoint.com/v2/url?u=https-3A__bugs.r-2Dproject.org_show-5Fbug.cgi-3Fid-3D18232&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=_jG6hqNCBjiWjIVFOx5Zm4cTTHAd-Rv2IBVjFZkWtwdR9QDc9qZI59xb7fZs6qtT&s=unYB3ajJ6LhkNKab1X8TGSNYaiYwhgEC7GZfknFfA9Q&e=

Having said that, paste(sQuote(values, FALSE), collapse = ',') will do the trick, but only if values are guaranteed not to contain single quotes or other characters that have special meaning in Python. Note the FALSE argument to sQuote: otherwise it could return ?Unicode quotes?, `TeX quotes', or even ?guillemets?, depending on the options set by the user.

--
Best regards,
Ivan

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=_jG6hqNCBjiWjIVFOx5Zm4cTTHAd-Rv2IBVjFZkWtwdR9QDc9qZI59xb7fZs6qtT&s=10-fNd6yyMRCfduzKbeG6MHUz5Tkf4rf6lbKc7OFuSM&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=_jG6hqNCBjiWjIVFOx5Zm4cTTHAd-Rv2IBVjFZkWtwdR9QDc9qZI59xb7fZs6qtT&s=j_RwC0Njzj5WVBbGGmslVCm2EoDQxs7iNbMXItIRobM&e=
and provide commented, minimal, self-contained, reproducible code.

From jr@| @end|ng |rom po@teo@no  Fri Jun 10 13:15:37 2022
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Fri, 10 Jun 2022 11:15:37 +0000
Subject: [R] how to add comma to string vector?
In-Reply-To: <BN6PR2201MB15537DC973F89F73BDDBE727CFA69@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
 <20220610133400.14bdf8cf@arachnoid>
 <BN6PR2201MB15537DC973F89F73BDDBE727CFA69@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <YqMn2StXjd7MvdxV@posteo.no>

> raws <- c("field_1", "field_2",  "field_3")
> reticulate::r_to_py(x=raws)
No non-system installation of Python could be found.
Would you like to download and install Miniconda?
Miniconda is an open source environment management system for Python.
See https://docs.conda.io/en/latest/miniconda.html for more details.

Would you like to install Miniconda? [Y/n]: n
Installation aborted.
['field_1', 'field_2', 'field_3']
> reticulate::py_config()
python:         /usr/bin/python3
libpython:      /usr/lib/libpython3.10.so
pythonhome:     //usr://usr
version:        3.10.5 (main, Jun  6 2022, 18:49:26) [GCC 12.1.0]
numpy:          /usr/lib/python3.10/site-packages/numpy
numpy_version:  1.22.4

python versions found:
 /usr/bin/python3
 /usr/bin/python
> reticulate::py_run_string('print(1+1)')
2
> reticulate::r_to_py(x=raws)
['field_1', 'field_2', 'field_3']


From pd@|gd @end|ng |rom gm@||@com  Fri Jun 10 14:01:41 2022
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Fri, 10 Jun 2022 14:01:41 +0200
Subject: [R] [Rd] R 4.2.1 scheduled for June 23
Message-ID: <492506A5-D562-4E87-BA6C-A25CB185AD57@gmail.com>

Full schedule available on developer.r-project.org in a short while.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

_______________________________________________
R-announce at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Fri Jun 10 14:09:12 2022
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Fri, 10 Jun 2022 14:09:12 +0200
Subject: [R] how to add comma to string vector?
In-Reply-To: <YqMdnKIYlOXIrw1X@posteo.no>
References: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
 <YqMdnKIYlOXIrw1X@posteo.no>
Message-ID: <CAMk+s2TkkPS-GOgbWJZ_tV2Y3pEUU1jZ43X2T6HO-h1AzimTDw@mail.gmail.com>

Thank you.
```
raws <- c("field_1", "field_2",  "field_3")
paste0("['", paste0(raws, collapse="', '"), "']")
```
gave the most fitting option...

On Fri, Jun 10, 2022 at 12:31 PM Rasmus Liland <jral at posteo.no> wrote:
>
> Hello ...
>
> raws <- c("field_1", "field_2",  "field_3")
> paste0("['", paste0(raws, collapse="', '"), "']")
>
> rasmus at eightforty ~ % python
> Python 3.10.5 (main, Jun  6 2022, 18:49:26) [GCC 12.1.0] on linux
> Type "help", "copyright", "credits" or "license" for more information.
> >>> ['field_1', 'field_2', 'field_3']
> ['field_1', 'field_2', 'field_3']
>
> ?
>
> R



-- 
Best regards,
Luigi


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri Jun 10 14:30:58 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 10 Jun 2022 13:30:58 +0100
Subject: [R] how to add comma to string vector?
In-Reply-To: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
References: <CAMk+s2SFO-PzS_R2KfAMBZMY-5GnopQ9EghRVMdsCTVpFA_bLw@mail.gmail.com>
Message-ID: <67183fd5-a04d-9ce5-a225-14fe985b07f3@sapo.pt>

Hello,

cat has a sep argument:

capture.output(cat(raws, sep = ","))
#[1] "field_1,field_2,field_3"


Instead of capture.output there is also cat argument file.

Hope this helps,

Rui Barradas


?s 11:19 de 10/06/2022, Luigi Marongiu escreveu:
> Hello,
> I need to convert an R vector to a python array. Essentially, I got a
> vector of strings where each element must be enclosed in single
> quotes. The problem is that each closing single quote should contain a
> comma. What is the regex trick to do that?
> I tried with:
> ```
>> (raws = c("field_1", "field_2",  "field_3"))
> [1] "field_1" "field_2" "field_3"
>> (items = sQuote(raws))
> [1] "'field_1'" "'field_2'" "'field_3'"
>> gsub("\'[:blank:]", "\',[:blank:]", items,  ignore.case = FALSE, perl = FALSE)
> [1] "'field_1'" "'field_2'" "'field_3'"
>> cat("python_Array = [", domain_list, "]\n")
> python_Array = [ 'Index' 'Endpoints' 'Confounders' 'Arboexposure' ]
> ```
> To note that `python_Array` does not have commas between elements and
> `gsub` did not do anything...
> 
> Thank you
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Wed Jun  8 06:44:39 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Wed, 8 Jun 2022 09:44:39 +0500
Subject: [R] A humble request regarding QAMG method in R
Message-ID: <CAMfKi3KtMG2cE6dNc6xYn--34XVNS-9tR7wr_h3KCuCeoP8VmA@mail.gmail.com>

Hi, Dear Respected Professors! I hope that you are doing well.
Kindly help me with the following:
I have the R-codes for the "Quantile Augmented Mean Group" method. The
relevant codes and papers are attached herewith.
My Issue:
When I run the given codes to estimate the results for my own data. I have
to face several errors. I humbly request the experts to help me to estimate
the results for my data.
Thank you so much for your precious time.

Regards

Muhammad Zubair Chishti
Ph.D. Student
School of Business,
Zhengzhou University, Henan, China.
My Google scholar link:
https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
My ResearchGate link: https://www.researchgate.net/profile/Muhammad-Chishti

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Journal of Applied Econometrics Volume issue 2020 [doi 10.1002_jae.2753] Harding, Matthew; Lamarche, Carlos; Pesaran, M. Hashem -- Common Correlated Effects Estimation of Heterogeneous Dynamic Panel_2.pdf
Type: application/pdf
Size: 1615687 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220608/1d9d2d17/attachment-0001.pdf>

From ||@t@ @end|ng |rom dewey@myzen@co@uk  Fri Jun 10 17:45:04 2022
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Fri, 10 Jun 2022 16:45:04 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k-1=WsD71xcSa7WV0Ef=tiY3_Gz2KxvmskY8QuJso+EYA@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-EXdbmLVNbEH9nN=FE3FYs4uqXgVtH3V35o6BU0z=rTw@mail.gmail.com>
 <91a30ffd-1aec-0f44-17a0-b632fae91014@dewey.myzen.co.uk>
 <CANg3_k-0=Z4KUE52mxHrSj4c+9bhJ+C4HDqxM2HqqVJ9um0WBQ@mail.gmail.com>
 <CANg3_k-1=WsD71xcSa7WV0Ef=tiY3_Gz2KxvmskY8QuJso+EYA@mail.gmail.com>
Message-ID: <bd555484-ba2c-024f-e5eb-9eb19b86761e@dewey.myzen.co.uk>

Dear Jade

It seems to me that there are several issues here.

1 - if you fit a separate parameter for each heaping day you efectively 
remove them from the model altogether. If they do carry meaningful 
information then that is undesirable.

2 - if the reason that there ae so many 15s is because people state, 
"Oh, it was in July" and either they or the interviewere impute 15 
because it si the month mid-point then it begins to look as though 
attempting to use daily data is fraught with problems and monthly might 
be better.

3 - if you fit a binary heap variable which is zero except for the 15 of 
the month and 1 for the 15 then you are taking out any general overall 
tendency to report 15 but you are allowing for the possibility that the 
relative size of heaping days can vary during the study period.

I am not an expert on this sort of model, that is Simon clearly, but it 
seems to me that, as usual, we have a mix of statistical and scientific 
questions here and you may need to rethink your sceientific model in the 
light of the issue with your data.

Michael

On 10/06/2022 10:30, jade.shodan at googlemail.com wrote:
> Hi Michael,
> 
> I don't think my reply to your email came through to the list, so am
> resending (see below). Problems with subscription have now hopefully
> been resolved. Apologies if this is a double posting!
> 
> On Thu, 9 Jun 2022 at 15:27, jade.shodan at googlemail.com
> <jade.shodan at googlemail.com> wrote:
>>
>> Hi Michael,
>>
>> Thanks for the reply! When I ran the gam  with the gam() function, the
>> model worked fine with heap having 169 levels. The same model with
>> bam() however, fails.  I don't understand the difference between bam()
>> and gam() at all (other than computational efficiency), but could the
>> fact that each level only has 1 data point be the reason for it?
>>
>> These heaping days are very large measurement errors I need to get rid
>> off, so I just want to take them out of the model altogether. (My data
>> is quite noisy already, because date of death is based on memory
>> recall, rather than formal death registration, due to the data being
>> from a low income country). Median of deaths is approx. 2 per day, but
>> on heaping days it can be as high as 50 or so.
>>
>> My understanding was that coding with 169 levels would effectively
>> take these measurements out of the model (but do correct me if I'm
>> wrong!)  I originally coded heap as a binary variable with 0 for
>> non-heaping days and 1 for heaping days, but was told that meant that
>> I was assuming the effect was the same for all heaping days. If I
>> coded with 12 or 14 levels, wouldn't that leave a lot of noise in the
>> data?
>>
>> Jade
>>
>> On Thu, 9 Jun 2022 at 14:52, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>>>
>>> Dear Jade
>>>
>>> Do you really need to fit a separate parameter for each heaping day? Can
>>> you not just make it a binary predictor or a categorical one with fewer
>>> levels, perhaps 14 (for heaping in each year) or 12 (for each calendar
>>> month). I have no idea whether that would help but it seems worth a try.
>>>
>>> Michael
>>>
>>> On 08/06/2022 18:15, jade.shodan--- via R-help wrote:
>>>> Hi Simon,
>>>>
>>>> Thanks so much for this!! I have two follow up questions, if you don't mind.
>>>>
>>>> 1. Does including an autoregressive term not adjust away part of the
>>>> effect of the response in a distributed lag model (where the outcome
>>>> accumulates over time)?
>>>> 2. I've tried to fit a model using bam (just a first attempt without
>>>> AR term), but including the factor variable heap creates errors:
>>>>
>>>> bam0 <- bam(deaths~te(year, month, week, weekday,
>>>> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
>>>>                         te(temp_max, lag, k=c(8, 3)) +
>>>>                         te(precip_daily_total, lag, k=c(8, 3)),
>>>>                         data = dat, family = nb, method = 'fREML',
>>>> select = TRUE, discrete = TRUE,
>>>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
>>>> 52.5), weekday = c(0, 6.5)))
>>>>
>>>> This model results in errors:
>>>>
>>>> Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
>>>>     step failure in theta estimation
>>>> Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
>>>> object$prior.weights)) :
>>>>     NaNs produced
>>>>
>>>>
>>>> Including heap as as.numeric(heap) runs the model without error
>>>> messages or warnings, but model diagnostics look terrible, and it also
>>>> doesn't make sense (to me) to make heap a numeric. The factor variable
>>>> heap (with 169 levels) codes the fact that all deaths for which no
>>>> date was known, were registered on the 15th day of each month. I've
>>>> coded all non-heaping days as 0. All heaping days were coded as a
>>>> value between 1-168. The time series spans 14 years, so a heaping day
>>>> in each month results in 14*12 levels = 168, plus one level for
>>>> non-heaping days.
>>>>
>>>> So my second question is: Does bam allow factor variables? And if not,
>>>> how should I model this heaping on the 15th day of the month instead?
>>>>
>>>> With thanks,
>>>>
>>>> Jade
>>>>
>>>> On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
>>>>>
>>>>> I would not worry too much about high concurvity between variables like
>>>>> temperature and time. This just reflects the fact that temperature has a
>>>>> strong temporal pattern.
>>>>>
>>>>> I would also not be too worried about the low p-values on the k check.
>>>>> The check only looks for pattern in the residuals when they are ordered
>>>>> with respect to the variables of a smooth. When you have time series
>>>>> data and some smooths involve time then it's hard not to pick up some
>>>>> degree of residual auto-correlation, which you often would not want to
>>>>> model with a higher rank smoother.
>>>>>
>>>>> The NAs  for the distributed lag terms just reflect the fact that there
>>>>> is no obvious way to order the residuals w.r.t. the covariates for such
>>>>> terms, so the simple check for residual pattern is not really possible.
>>>>>
>>>>> One simple approach is to fit using bam(...,discrete=TRUE) which will
>>>>> let you specify an AR1 parameter to mop up some of the residual
>>>>> auto-correlation without resorting to a high rank smooth that then does
>>>>> all the work of the covariates as well. The AR1 parameter can be set by
>>>>> looking at the ACF of the residuals of the model without this. You need
>>>>> to look at the ACF of suitably standardized residuals to check how well
>>>>> this has worked.
>>>>>
>>>>> best,
>>>>>
>>>>> Simon
>>>>>
>>>>> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
>>>>>> Hello everyone,
>>>>>>
>>>>>> A few days ago I asked a question about concurvity in a GAM (the
>>>>>> anologue of collinearity in a GLM) implemented in mgcv. I think my
>>>>>> question was a bit unfocussed, so I am retrying again, but with
>>>>>> additional information included about the autocorrelation function. I
>>>>>> have also posted about this on Cross Validated. Given all the model
>>>>>> output, it might make for easier
>>>>>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
>>>>>>
>>>>>> As mentioned previously, I have problems with concurvity in my thesis
>>>>>> research, and don't have access to a statistician who works with time
>>>>>> series, GAMs or R. I'd be very grateful for any (partial) answer,
>>>>>> however short. I'll gladly return the favour where I can! For really
>>>>>> helpful input I'd be more than happy to offer co-authorship on
>>>>>> publication. Deadlines are very close, and I'm heading towards having
>>>>>> no results at all if I can't solve this concurvity issue :(
>>>>>>
>>>>>> I'm using GAMs to try to understand the relationship between deaths
>>>>>> and heat-related variables (e.g. temperature and humidity), using
>>>>>> daily time series over a 14-year period from a tropical, low-income
>>>>>> country. My aim is to understand the relationship between these
>>>>>> variables and deaths, rather than pure prediction performance.
>>>>>>
>>>>>> The GAMs include distributed lag models (set up as 7-column matrices,
>>>>>> see code at bottom of post), since deaths may occur over several days
>>>>>> following exposure.
>>>>>>
>>>>>> Simple GAMs with just time, lagged temperature and lagged
>>>>>> precipitation (a potential confounder) show very high concurvity
>>>>>> between lagged temperature and time, regardless of the many different
>>>>>> ways I have tried to decompose time. The autocorrelation functions
>>>>>> (ACF) however, shows values close to zero, only just about breaching
>>>>>> the 'significance line' in a few instances. It does show patterning
>>>>>> though, although the regularity is difficult to define.
>>>>>>
>>>>>> My questions are:
>>>>>> 1) Should I be worried about the high concurvity, or can I ignore it
>>>>>> given the mostly non-significant ACF? I've read dozens of
>>>>>> heat-mortality modelling studies and none report on concurvity between
>>>>>> weather variables and time (though one 2012 paper discussed
>>>>>> autocorrelation).
>>>>>>
>>>>>> 2) If I cannot ignore it, what should I do to resolve it? Would
>>>>>> including an autoregressive term be appropriate, and if so, where can
>>>>>> I find a coded example of how to do this? I've also come across
>>>>>> sequential regression][1]. Would this be more or less appropriate? If
>>>>>> appropriate, a pointer to an example would be really appreciated!
>>>>>>
>>>>>> Some example GAMs are specified as follows:
>>>>>> ```r
>>>>>> conc38b <- gam(deaths~te(year, month, week, weekday,
>>>>>> bs=c("cr","cc","cc","cc")) + heap +
>>>>>>                          te(temp_max, lag, k=c(10, 3)) +
>>>>>>                          te(precip_daily_total, lag, k=c(10, 3)),
>>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE,
>>>>>>                          knots = list(month = c(0.5, 12.5), week = c(0.5,
>>>>>> 52.5), weekday = c(0, 6.5)))
>>>>>> ```
>>>>>> Concurvity for the above model between (temp_max, lag) and (year,
>>>>>> month, week, weekday) is 0.91:
>>>>>>
>>>>>> ```r
>>>>>> $worst
>>>>>>                                        para te(year,month,week,weekday)
>>>>>> te(temp_max,lag) te(precip_daily_total,lag)
>>>>>> para                        1.000000e+00                1.125625e-29
>>>>>>         0.3150073                  0.6666348
>>>>>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
>>>>>>         0.9060552                  0.6652313
>>>>>> te(temp_max,lag)            3.152795e-01                8.998113e-01
>>>>>>         1.0000000                  0.5781015
>>>>>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
>>>>>>         0.5805159                  1.0000000
>>>>>> ```
>>>>>>
>>>>>> Output from ```gam.check()```:
>>>>>> ```r
>>>>>> Method: REML   Optimizer: outer newton
>>>>>> full convergence after 16 iterations.
>>>>>> Gradient range [-0.01467332,0.003096643]
>>>>>> (score 8915.994 & scale 1).
>>>>>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
>>>>>> Model rank =  544 / 544
>>>>>>
>>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>>>>> indicate that k is too low, especially if edf is close to k'.
>>>>>>
>>>>>>                                      k'      edf k-index p-value
>>>>>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
>>>>>> te(temp_max,lag)             29.0000   3.3681      NA      NA
>>>>>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
>>>>>> ---
>>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>>> ```
>>>>>>
>>>>>> Some output from ```summary(conc38b)```:
>>>>>> ```r
>>>>>> Approximate significance of smooth terms:
>>>>>>                                      edf Ref.df  Chi.sq p-value
>>>>>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
>>>>>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
>>>>>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
>>>>>> ---
>>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>>>
>>>>>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
>>>>>> -REML =   8916  Scale est. = 1         n = 5107
>>>>>> ```
>>>>>>
>>>>>>
>>>>>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
>>>>>> pattern). They show peaks at 5 and 15, and then there seems to be a
>>>>>> recurring pattern at multiples of approx. 30 (suggesting month is not
>>>>>> modelled adequately?). Not sure what would cause the spikes at 5 and
>>>>>> 15. There is heaping of deaths on the 15th day of each month, to which
>>>>>> deaths with unknown date were allocated. This heaping was modelled
>>>>>> with categorical variable/ factor ```heap``` with 169 levels (0 for
>>>>>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
>>>>>> heaping day over the 14-year period):
>>>>>>
>>>>>>      [2]: https://i.stack.imgur.com/FzKyM.png
>>>>>>      [3]: https://i.stack.imgur.com/fE3aL.png
>>>>>>
>>>>>>
>>>>>> I get an identical looking ACF when I decompose time into (year,
>>>>>> month, monthday) as in model conc39 below, although concurvity between
>>>>>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
>>>>>>
>>>>>> ```r
>>>>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>>>>>>                         te(temp_max, lag, k=c(10, 4)) +
>>>>>>                         te(precip_daily_total, lag, k=c(10, 4)),
>>>>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
>>>>>>                         knots = list(month = c(0.5, 12.5)))
>>>>>> ```
>>>>>> ```r
>>>>>>
>>>>>> Method: REML   Optimizer: outer newton
>>>>>> full convergence after 14 iterations.
>>>>>> Gradient range [-0.001578187,6.155096e-05]
>>>>>> (score 8915.763 & scale 1).
>>>>>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
>>>>>> Model rank =  323 / 323
>>>>>>
>>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>>>>> indicate that k is too low, especially if edf is close to k'.
>>>>>>
>>>>>>                                    k'     edf k-index p-value
>>>>>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
>>>>>> te(temp_max,lag)           39.0000  4.0875      NA      NA
>>>>>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
>>>>>> ---
>>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>>> ```
>>>>>> Some output from ```summary(conc39)```:
>>>>>> ```r
>>>>>> Approximate significance of smooth terms:
>>>>>>                                    edf Ref.df  Chi.sq  p-value
>>>>>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
>>>>>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
>>>>>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
>>>>>> ---
>>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>>>
>>>>>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
>>>>>> -REML =   8915  Scale est. = 1         n = 5107
>>>>>> ```
>>>>>>
>>>>>>
>>>>>> ```r
>>>>>> $worst
>>>>>>                                       para te(year,month,monthday)
>>>>>> te(temp_max,lag) te(precip_daily_total,lag)
>>>>>> para                       1.000000e+00            3.261007e-31
>>>>>> 0.3313549                  0.6666532
>>>>>> te(year,month,monthday)    3.060763e-31            1.000000e+00
>>>>>> 0.8266086                  0.5670777
>>>>>> te(temp_max,lag)           3.331014e-01            8.225942e-01
>>>>>> 1.0000000                  0.5840875
>>>>>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
>>>>>> 0.5939380                  1.0000000
>>>>>> ```
>>>>>>
>>>>>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
>>>>>> various choices for k or as ```s(time)``` with various k does not
>>>>>> reduce concurvity either.
>>>>>>
>>>>>>
>>>>>> The default approach in time series studies of heat-mortality is to
>>>>>> model time with fixed df, generally between 7-10 df per year of data.
>>>>>> I am, however, apprehensive about this approach because a) mortality
>>>>>> profiles vary with locality due to sociodemographic and environmental
>>>>>> characteristics and b) the choice of df is based on higher income
>>>>>> countries (where nearly all these studies have been done) with
>>>>>> different mortality profiles and so may not be appropriate for
>>>>>> tropical, low-income countries.
>>>>>>
>>>>>> Although the approach of fixing (high) df does remove more temporal
>>>>>> patterns from the ACF (see model and output below), concurvity between
>>>>>> time and lagged temperature has now risen to 0.99! Moreover,
>>>>>> temperature (which has been a consistent, highly significant predictor
>>>>>> in every model of the tens (hundreds?) I have run, has now turned
>>>>>> non-significant. I am guessing this is because time is now a very
>>>>>> wiggly function that not only models/ removes seasonal variation, but
>>>>>> also some of the day-to-day variation that is needed for the
>>>>>> temperature smooth  :
>>>>>>
>>>>>> ```r
>>>>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>>>>>>                          te(temp_max, lag, k=c(10,3)) +
>>>>>>                          te(precip_daily_total, lag, k=c(10,3)),
>>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE)
>>>>>> ```
>>>>>> Output from ```gam.check(conc20a, rep = 1000)```:
>>>>>>
>>>>>> ```r
>>>>>> Method: REML   Optimizer: outer newton
>>>>>> full convergence after 9 iterations.
>>>>>> Gradient range [-0.0008983099,9.546022e-05]
>>>>>> (score 8750.13 & scale 1).
>>>>>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
>>>>>> Model rank =  336 / 336
>>>>>>
>>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>>>>> indicate that k is too low, especially if edf is close to k'.
>>>>>>
>>>>>>                                     k'      edf k-index p-value
>>>>>> s(time)                    111.0000 111.0000    0.98    0.56
>>>>>> te(temp_max,lag)            29.0000   0.6548      NA      NA
>>>>>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
>>>>>> ```
>>>>>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
>>>>>>
>>>>>> ```r
>>>>>>                                       para      s(time) te(temp_max,lag)
>>>>>> te(precip_daily_total,lag)
>>>>>> para                       1.000000e+00 2.462064e-19        0.3165236
>>>>>>                    0.6666348
>>>>>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
>>>>>>                    0.6879284
>>>>>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
>>>>>>                    0.5788711
>>>>>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
>>>>>>                    1.0000000
>>>>>>
>>>>>> ```
>>>>>>
>>>>>> Some output from ```summary(conc20a)```:
>>>>>> ```r
>>>>>> Approximate significance of smooth terms:
>>>>>>                                     edf Ref.df  Chi.sq p-value
>>>>>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
>>>>>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
>>>>>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
>>>>>> ---
>>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>>>
>>>>>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
>>>>>> -REML = 8750.1  Scale est. = 1         n = 5107
>>>>>> ```
>>>>>>
>>>>>> ACF functions:
>>>>>>
>>>>>> [4]: https://i.stack.imgur.com/7nbXS.png
>>>>>> [5]: https://i.stack.imgur.com/pNnZU.png
>>>>>>
>>>>>> Data can be found on my [GitHub][6] site in the file
>>>>>> [data_cross_validated_post2.rds][7]. A csv version is also available.
>>>>>> This is my code:
>>>>>>
>>>>>> ```r
>>>>>> library(readr)
>>>>>> library(mgcv)
>>>>>>
>>>>>> df <- read_rds("data_crossvalidated_post2.rds")
>>>>>>
>>>>>> # Create matrices for lagged weather variables (6 day lags) based on
>>>>>> example by Simon Wood
>>>>>> # in his 2017 book ("Generalized additive models: an introduction with
>>>>>> R", p. 349) and
>>>>>> # gamair package documentation
>>>>>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
>>>>>>
>>>>>> lagard <- function(x,n.lag=7) {
>>>>>> n <- length(x); X <- matrix(NA,n,n.lag)
>>>>>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
>>>>>> X
>>>>>> }
>>>>>>
>>>>>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
>>>>>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
>>>>>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
>>>>>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
>>>>>> = df$precip_hourly_dailysum)
>>>>>> dat$temp_max <- lagard(df$temp_max)
>>>>>> dat$temp_min <- lagard(df$temp_min)
>>>>>> dat$temp_mean <- lagard(df$temp_mean)
>>>>>> dat$wbgt_max <- lagard(df$wbgt_max)
>>>>>> dat$wbgt_mean <- lagard(df$wbgt_mean)
>>>>>> dat$wbgt_min <- lagard(df$wbgt_min)
>>>>>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
>>>>>> dat$sh_mean <- lagard(df$sh_mean)
>>>>>> dat$solar_mean <- lagard(df$solar_mean)
>>>>>> dat$wind2m_mean <- lagard(df$wind2m_mean)
>>>>>> dat$sh_max <- lagard(df$sh_max)
>>>>>> dat$solar_max <- lagard(df$solar_max)
>>>>>> dat$wind2m_max <- lagard(df$wind2m_max)
>>>>>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
>>>>>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
>>>>>> dat$precip_hourly <- lagard(df$precip_hourly)
>>>>>> dat$precip_daily_total <- lagard( df$precip_daily_total)
>>>>>> dat$temp <- lagard(df$temp)
>>>>>> dat$sh <- lagard(df$sh)
>>>>>> dat$rh <- lagard(df$rh)
>>>>>> dat$solar <- lagard(df$solar)
>>>>>> dat$wind2m <- lagard(df$wind2m)
>>>>>>
>>>>>>
>>>>>> conc38b <- gam(deaths~te(year, month, week, weekday,
>>>>>> bs=c("cr","cc","cc","cc")) + heap +
>>>>>>                          te(temp_max, lag, k=c(10, 3)) +
>>>>>>                          te(precip_daily_total, lag, k=c(10, 3)),
>>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE,
>>>>>>                          knots = list(month = c(0.5, 12.5), week = c(0.5,
>>>>>> 52.5), weekday = c(0, 6.5)))
>>>>>>
>>>>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>>>>>>                         te(temp_max, lag, k=c(10, 4)) +
>>>>>>                         te(precip_daily_total, lag, k=c(10, 4)),
>>>>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
>>>>>>                         knots = list(month = c(0.5, 12.5)))
>>>>>>
>>>>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>>>>>>                          te(temp_max, lag, k=c(10,3)) +
>>>>>>                          te(precip_daily_total, lag, k=c(10,3)),
>>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE)
>>>>>>
>>>>>> ```
>>>>>> Thank you if you've read this far!! :-))
>>>>>>
>>>>>>      [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
>>>>>>      [2]: https://i.stack.imgur.com/FzKyM.png
>>>>>>      [3]: https://i.stack.imgur.com/fE3aL.png
>>>>>>      [4]: https://i.stack.imgur.com/7nbXS.png
>>>>>>      [5]: https://i.stack.imgur.com/pNnZU.png
>>>>>>      [6]: https://github.com/JadeShodan/heat-mortality
>>>>>>      [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>> --
>>>>> Simon Wood, School of Mathematics, University of Edinburgh,
>>>>> https://www.maths.ed.ac.uk/~swood34/
>>>>>
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>> --
>>> Michael
>>> http://www.dewey.myzen.co.uk/home.html
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From @|mon@wood @end|ng |rom b@th@edu  Fri Jun 10 20:50:20 2022
From: @|mon@wood @end|ng |rom b@th@edu (Simon Wood)
Date: Fri, 10 Jun 2022 19:50:20 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <CANg3_k_qMX24_a-Hppk6knrH-nt1QMLj2FFSJbkMJc0TBxqzQA@mail.gmail.com>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-g=M41cxom6DVdTunFjgZ6VfKobqW__Mq351pDpY22PQ@mail.gmail.com>
 <f371005e-6f57-2007-f859-d66595a19c0f@bath.edu>
 <CANg3_k_qMX24_a-Hppk6knrH-nt1QMLj2FFSJbkMJc0TBxqzQA@mail.gmail.com>
Message-ID: <80abf9e4-d864-43d3-e8ef-488239d22931@bath.edu>

Apologies, I meant...

aheap <- heap!="0"
aheap + s(heap,bs="re",by=aheap)

- fixed effect for heap day or not, and then an extra random effect if it is a heap day

... bit difficult to give a generic answer to the usefulness question - 
I guess you probably have to decide on that yourself.

Simon

On 09/06/2022 16:57, jade.shodan at googlemail.com wrote:
> Hi Simon,
>
> (Sorry, replies and answers are out of sync due to my problems posting
> to the list/ messages being held for moderation)
>
>> Did it actually fail, or simply generate warnings?
> The model was computed (you're right, not a model failure), but
> resulted in warnings (as per previous post) which, frankly, I didn't
> understand.
>
>> if I understand your model right there is one free parameter for each observation falling on the 15th
> That's right, one observation on each 15th day of the month.
>
> Thank you for the suggestion about the random effects! I had been
> wondering about how I could model this heaping with a smooth!
>
> Quick question:
>
> You proposed:
>
> aheap <- heap!="0"
> heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap
> day + a r.e. for each heap day - no effect on non-heap days
>
> Is there a typo in the code above? I don't see the newly created
> variable aheap in the model?
> Should it read "by = aheap" as follows:
>
> heap + s(heap,bs="re",by=aheap)   ?
>
> So a full model might then look like the one below?
>
>   bam0 <- bam(deaths~te(year, month, week, weekday,
> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> s(heap,bs="re",by=aheap)
>                                       te(temp_max, lag, k=c(8, 3)) +
>                                       te(precip_daily_total, lag, k=c(8, 3)),
>                                       data = dat, family = nb, method =
> 'fREML', select = TRUE, discrete = TRUE,
>                                     knots = list(month = c(0.5, 12.5),
> week = c(0.5, 52.5), weekday = c(0, 6.5)))
>
> One, hopefully final (!) question:
>
> Is it actually useful at all to keep these observations on the 15th
> day of each month (which are huge errors), or am I better off removing
> them from the data set (or replacing them with e.g. median values)?
> For temperature-mortality modelling it is the day-to-day variation in
> deaths and temperature that is of interest. So is modelling heaping
> actually useful at all, given that this variable changes on a monthly
> basis? (I think you are alluding to this, but I just want to make
> sure).
>
> If I take them out altogether, would I be best off removing all data
> for these dates, so that the time series jumps from day 14 to day 16?
> Or would this create problems with e.g. the distributed lag model?
>
> Sorry for all these questions! Have been struggling with this for
> months (posted on Cross Validated about the heaping issue too), and
> feel I am finally getting somewhere with your help!
>
> Jade
>
> On Thu, 9 Jun 2022 at 15:24, Simon Wood <simon.wood at bath.edu> wrote:
>>
>> On 09/06/2022 12:30, jade.shodan at googlemail.com wrote:
>>> Hi Simon,
>>>
>>> Thanks so much for this!! (Apologies if this is a double posting. I
>>> seem to have a problem getting messages through to the list).
>>>
>>> I have two follow up questions, if you don't mind.
>>>
>>> 1. Does including an autoregressive term not adjust away part of the
>>> effect of the response in a distributed lag model (where the outcome
>>> accumulates over time)?
>> - the hope is that it approximately deals with short timescale stuff
>> without interfering with the longer timescales to the same extent as a
>> high rank smooth would.
>>
>>> 2. I've tried to fit a model using bam (just a first attempt without
>>> AR term), but including the factor variable heap creates errors:
>>>
>>> bam0 <- bam(deaths~te(year, month, week, weekday,
>>> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
>>>                                        te(temp_max, lag, k=c(8, 3)) +
>>>                                        te(precip_daily_total, lag, k=c(8, 3)),
>>>                                       data = dat, family = nb, method =
>>> 'fREML', select = TRUE, discrete = TRUE,
>>>                                       knots = list(month = c(0.5, 12.5),
>>> week = c(0.5, 52.5), weekday = c(0, 6.5)))
>>>
>>> This model results in errors:
>>>
>>> Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
>>>     step failure in theta estimation
>>> Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
>>> object$prior.weights)) :
>>>     NaNs produced
>>>
>> Did it actually fail, or simply generate warnings?
>>
>> 'bam' handles factors, but if I understand your model right there is one
>> free parameter for each observation falling on the 15th, so that you
>> will fit data for those days exactly (and might just as well have
>> dropped them, for all the information they contribute to the rest of the
>> model). If you want a structure like this, I'd be inclined to make the
>> heap variable random, something like...
>>
>> aheap <- heap!="0"
>>
>> heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap day
>> + a r.e. for each heap day - no effect on non-heap days
>>
>> best,
>>
>> Simon
>>
>>> Including heap as as.numeric(heap) runs the model without error
>>> messages or warnings, but model diagnostics look terrible, and it also
>>> doesn't make sense (to me) to make heap a numeric. The factor variable
>>> heap (with 169 levels) codes the fact that all deaths for which no
>>> date was known, were registered on the 15th day of each month. I've
>>> coded all non-heaping days as 0. All heaping days were coded as a
>>> value between 1-168. The time series spans 14 years, so a heaping day
>>> in each month results in 14*12 levels = 168, plus one level for
>>> non-heaping days.
>>>
>>> So my second question is: Does bam allow factor variables? And if not,
>>> how should I model this heaping on the 15th day of the month instead?
>>>
>>> With thanks,
>>>
>>> Jade
>>>
>>> On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
>>>> I would not worry too much about high concurvity between variables like
>>>> temperature and time. This just reflects the fact that temperature has a
>>>> strong temporal pattern.
>>>>
>>>> I would also not be too worried about the low p-values on the k check.
>>>> The check only looks for pattern in the residuals when they are ordered
>>>> with respect to the variables of a smooth. When you have time series
>>>> data and some smooths involve time then it's hard not to pick up some
>>>> degree of residual auto-correlation, which you often would not want to
>>>> model with a higher rank smoother.
>>>>
>>>> The NAs  for the distributed lag terms just reflect the fact that there
>>>> is no obvious way to order the residuals w.r.t. the covariates for such
>>>> terms, so the simple check for residual pattern is not really possible.
>>>>
>>>> One simple approach is to fit using bam(...,discrete=TRUE) which will
>>>> let you specify an AR1 parameter to mop up some of the residual
>>>> auto-correlation without resorting to a high rank smooth that then does
>>>> all the work of the covariates as well. The AR1 parameter can be set by
>>>> looking at the ACF of the residuals of the model without this. You need
>>>> to look at the ACF of suitably standardized residuals to check how well
>>>> this has worked.
>>>>
>>>> best,
>>>>
>>>> Simon
>>>>
>>>> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
>>>>> Hello everyone,
>>>>>
>>>>> A few days ago I asked a question about concurvity in a GAM (the
>>>>> anologue of collinearity in a GLM) implemented in mgcv. I think my
>>>>> question was a bit unfocussed, so I am retrying again, but with
>>>>> additional information included about the autocorrelation function. I
>>>>> have also posted about this on Cross Validated. Given all the model
>>>>> output, it might make for easier
>>>>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
>>>>>
>>>>> As mentioned previously, I have problems with concurvity in my thesis
>>>>> research, and don't have access to a statistician who works with time
>>>>> series, GAMs or R. I'd be very grateful for any (partial) answer,
>>>>> however short. I'll gladly return the favour where I can! For really
>>>>> helpful input I'd be more than happy to offer co-authorship on
>>>>> publication. Deadlines are very close, and I'm heading towards having
>>>>> no results at all if I can't solve this concurvity issue :(
>>>>>
>>>>> I'm using GAMs to try to understand the relationship between deaths
>>>>> and heat-related variables (e.g. temperature and humidity), using
>>>>> daily time series over a 14-year period from a tropical, low-income
>>>>> country. My aim is to understand the relationship between these
>>>>> variables and deaths, rather than pure prediction performance.
>>>>>
>>>>> The GAMs include distributed lag models (set up as 7-column matrices,
>>>>> see code at bottom of post), since deaths may occur over several days
>>>>> following exposure.
>>>>>
>>>>> Simple GAMs with just time, lagged temperature and lagged
>>>>> precipitation (a potential confounder) show very high concurvity
>>>>> between lagged temperature and time, regardless of the many different
>>>>> ways I have tried to decompose time. The autocorrelation functions
>>>>> (ACF) however, shows values close to zero, only just about breaching
>>>>> the 'significance line' in a few instances. It does show patterning
>>>>> though, although the regularity is difficult to define.
>>>>>
>>>>> My questions are:
>>>>> 1) Should I be worried about the high concurvity, or can I ignore it
>>>>> given the mostly non-significant ACF? I've read dozens of
>>>>> heat-mortality modelling studies and none report on concurvity between
>>>>> weather variables and time (though one 2012 paper discussed
>>>>> autocorrelation).
>>>>>
>>>>> 2) If I cannot ignore it, what should I do to resolve it? Would
>>>>> including an autoregressive term be appropriate, and if so, where can
>>>>> I find a coded example of how to do this? I've also come across
>>>>> sequential regression][1]. Would this be more or less appropriate? If
>>>>> appropriate, a pointer to an example would be really appreciated!
>>>>>
>>>>> Some example GAMs are specified as follows:
>>>>> ```r
>>>>> conc38b <- gam(deaths~te(year, month, week, weekday,
>>>>> bs=c("cr","cc","cc","cc")) + heap +
>>>>>                          te(temp_max, lag, k=c(10, 3)) +
>>>>>                          te(precip_daily_total, lag, k=c(10, 3)),
>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE,
>>>>>                          knots = list(month = c(0.5, 12.5), week = c(0.5,
>>>>> 52.5), weekday = c(0, 6.5)))
>>>>> ```
>>>>> Concurvity for the above model between (temp_max, lag) and (year,
>>>>> month, week, weekday) is 0.91:
>>>>>
>>>>> ```r
>>>>> $worst
>>>>>                                        para te(year,month,week,weekday)
>>>>> te(temp_max,lag) te(precip_daily_total,lag)
>>>>> para                        1.000000e+00                1.125625e-29
>>>>>         0.3150073                  0.6666348
>>>>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
>>>>>         0.9060552                  0.6652313
>>>>> te(temp_max,lag)            3.152795e-01                8.998113e-01
>>>>>         1.0000000                  0.5781015
>>>>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
>>>>>         0.5805159                  1.0000000
>>>>> ```
>>>>>
>>>>> Output from ```gam.check()```:
>>>>> ```r
>>>>> Method: REML   Optimizer: outer newton
>>>>> full convergence after 16 iterations.
>>>>> Gradient range [-0.01467332,0.003096643]
>>>>> (score 8915.994 & scale 1).
>>>>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
>>>>> Model rank =  544 / 544
>>>>>
>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>>>> indicate that k is too low, especially if edf is close to k'.
>>>>>
>>>>>                                      k'      edf k-index p-value
>>>>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
>>>>> te(temp_max,lag)             29.0000   3.3681      NA      NA
>>>>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
>>>>> ---
>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>> ```
>>>>>
>>>>> Some output from ```summary(conc38b)```:
>>>>> ```r
>>>>> Approximate significance of smooth terms:
>>>>>                                      edf Ref.df  Chi.sq p-value
>>>>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
>>>>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
>>>>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
>>>>> ---
>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>>
>>>>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
>>>>> -REML =   8916  Scale est. = 1         n = 5107
>>>>> ```
>>>>>
>>>>>
>>>>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
>>>>> pattern). They show peaks at 5 and 15, and then there seems to be a
>>>>> recurring pattern at multiples of approx. 30 (suggesting month is not
>>>>> modelled adequately?). Not sure what would cause the spikes at 5 and
>>>>> 15. There is heaping of deaths on the 15th day of each month, to which
>>>>> deaths with unknown date were allocated. This heaping was modelled
>>>>> with categorical variable/ factor ```heap``` with 169 levels (0 for
>>>>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
>>>>> heaping day over the 14-year period):
>>>>>
>>>>>      [2]: https://i.stack.imgur.com/FzKyM.png
>>>>>      [3]: https://i.stack.imgur.com/fE3aL.png
>>>>>
>>>>>
>>>>> I get an identical looking ACF when I decompose time into (year,
>>>>> month, monthday) as in model conc39 below, although concurvity between
>>>>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
>>>>>
>>>>> ```r
>>>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>>>>>                         te(temp_max, lag, k=c(10, 4)) +
>>>>>                         te(precip_daily_total, lag, k=c(10, 4)),
>>>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
>>>>>                         knots = list(month = c(0.5, 12.5)))
>>>>> ```
>>>>> ```r
>>>>>
>>>>> Method: REML   Optimizer: outer newton
>>>>> full convergence after 14 iterations.
>>>>> Gradient range [-0.001578187,6.155096e-05]
>>>>> (score 8915.763 & scale 1).
>>>>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
>>>>> Model rank =  323 / 323
>>>>>
>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>>>> indicate that k is too low, especially if edf is close to k'.
>>>>>
>>>>>                                    k'     edf k-index p-value
>>>>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
>>>>> te(temp_max,lag)           39.0000  4.0875      NA      NA
>>>>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
>>>>> ---
>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>> ```
>>>>> Some output from ```summary(conc39)```:
>>>>> ```r
>>>>> Approximate significance of smooth terms:
>>>>>                                    edf Ref.df  Chi.sq  p-value
>>>>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
>>>>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
>>>>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
>>>>> ---
>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>>
>>>>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
>>>>> -REML =   8915  Scale est. = 1         n = 5107
>>>>> ```
>>>>>
>>>>>
>>>>> ```r
>>>>> $worst
>>>>>                                       para te(year,month,monthday)
>>>>> te(temp_max,lag) te(precip_daily_total,lag)
>>>>> para                       1.000000e+00            3.261007e-31
>>>>> 0.3313549                  0.6666532
>>>>> te(year,month,monthday)    3.060763e-31            1.000000e+00
>>>>> 0.8266086                  0.5670777
>>>>> te(temp_max,lag)           3.331014e-01            8.225942e-01
>>>>> 1.0000000                  0.5840875
>>>>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
>>>>> 0.5939380                  1.0000000
>>>>> ```
>>>>>
>>>>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
>>>>> various choices for k or as ```s(time)``` with various k does not
>>>>> reduce concurvity either.
>>>>>
>>>>>
>>>>> The default approach in time series studies of heat-mortality is to
>>>>> model time with fixed df, generally between 7-10 df per year of data.
>>>>> I am, however, apprehensive about this approach because a) mortality
>>>>> profiles vary with locality due to sociodemographic and environmental
>>>>> characteristics and b) the choice of df is based on higher income
>>>>> countries (where nearly all these studies have been done) with
>>>>> different mortality profiles and so may not be appropriate for
>>>>> tropical, low-income countries.
>>>>>
>>>>> Although the approach of fixing (high) df does remove more temporal
>>>>> patterns from the ACF (see model and output below), concurvity between
>>>>> time and lagged temperature has now risen to 0.99! Moreover,
>>>>> temperature (which has been a consistent, highly significant predictor
>>>>> in every model of the tens (hundreds?) I have run, has now turned
>>>>> non-significant. I am guessing this is because time is now a very
>>>>> wiggly function that not only models/ removes seasonal variation, but
>>>>> also some of the day-to-day variation that is needed for the
>>>>> temperature smooth  :
>>>>>
>>>>> ```r
>>>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>>>>>                          te(temp_max, lag, k=c(10,3)) +
>>>>>                          te(precip_daily_total, lag, k=c(10,3)),
>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE)
>>>>> ```
>>>>> Output from ```gam.check(conc20a, rep = 1000)```:
>>>>>
>>>>> ```r
>>>>> Method: REML   Optimizer: outer newton
>>>>> full convergence after 9 iterations.
>>>>> Gradient range [-0.0008983099,9.546022e-05]
>>>>> (score 8750.13 & scale 1).
>>>>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
>>>>> Model rank =  336 / 336
>>>>>
>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
>>>>> indicate that k is too low, especially if edf is close to k'.
>>>>>
>>>>>                                     k'      edf k-index p-value
>>>>> s(time)                    111.0000 111.0000    0.98    0.56
>>>>> te(temp_max,lag)            29.0000   0.6548      NA      NA
>>>>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
>>>>> ```
>>>>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
>>>>>
>>>>> ```r
>>>>>                                       para      s(time) te(temp_max,lag)
>>>>> te(precip_daily_total,lag)
>>>>> para                       1.000000e+00 2.462064e-19        0.3165236
>>>>>                    0.6666348
>>>>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
>>>>>                    0.6879284
>>>>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
>>>>>                    0.5788711
>>>>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
>>>>>                    1.0000000
>>>>>
>>>>> ```
>>>>>
>>>>> Some output from ```summary(conc20a)```:
>>>>> ```r
>>>>> Approximate significance of smooth terms:
>>>>>                                     edf Ref.df  Chi.sq p-value
>>>>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
>>>>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
>>>>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
>>>>> ---
>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>>>
>>>>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
>>>>> -REML = 8750.1  Scale est. = 1         n = 5107
>>>>> ```
>>>>>
>>>>> ACF functions:
>>>>>
>>>>> [4]: https://i.stack.imgur.com/7nbXS.png
>>>>> [5]: https://i.stack.imgur.com/pNnZU.png
>>>>>
>>>>> Data can be found on my [GitHub][6] site in the file
>>>>> [data_cross_validated_post2.rds][7]. A csv version is also available.
>>>>> This is my code:
>>>>>
>>>>> ```r
>>>>> library(readr)
>>>>> library(mgcv)
>>>>>
>>>>> df <- read_rds("data_crossvalidated_post2.rds")
>>>>>
>>>>> # Create matrices for lagged weather variables (6 day lags) based on
>>>>> example by Simon Wood
>>>>> # in his 2017 book ("Generalized additive models: an introduction with
>>>>> R", p. 349) and
>>>>> # gamair package documentation
>>>>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
>>>>>
>>>>> lagard <- function(x,n.lag=7) {
>>>>> n <- length(x); X <- matrix(NA,n,n.lag)
>>>>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
>>>>> X
>>>>> }
>>>>>
>>>>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
>>>>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
>>>>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
>>>>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
>>>>> = df$precip_hourly_dailysum)
>>>>> dat$temp_max <- lagard(df$temp_max)
>>>>> dat$temp_min <- lagard(df$temp_min)
>>>>> dat$temp_mean <- lagard(df$temp_mean)
>>>>> dat$wbgt_max <- lagard(df$wbgt_max)
>>>>> dat$wbgt_mean <- lagard(df$wbgt_mean)
>>>>> dat$wbgt_min <- lagard(df$wbgt_min)
>>>>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
>>>>> dat$sh_mean <- lagard(df$sh_mean)
>>>>> dat$solar_mean <- lagard(df$solar_mean)
>>>>> dat$wind2m_mean <- lagard(df$wind2m_mean)
>>>>> dat$sh_max <- lagard(df$sh_max)
>>>>> dat$solar_max <- lagard(df$solar_max)
>>>>> dat$wind2m_max <- lagard(df$wind2m_max)
>>>>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
>>>>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
>>>>> dat$precip_hourly <- lagard(df$precip_hourly)
>>>>> dat$precip_daily_total <- lagard( df$precip_daily_total)
>>>>> dat$temp <- lagard(df$temp)
>>>>> dat$sh <- lagard(df$sh)
>>>>> dat$rh <- lagard(df$rh)
>>>>> dat$solar <- lagard(df$solar)
>>>>> dat$wind2m <- lagard(df$wind2m)
>>>>>
>>>>>
>>>>> conc38b <- gam(deaths~te(year, month, week, weekday,
>>>>> bs=c("cr","cc","cc","cc")) + heap +
>>>>>                          te(temp_max, lag, k=c(10, 3)) +
>>>>>                          te(precip_daily_total, lag, k=c(10, 3)),
>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE,
>>>>>                          knots = list(month = c(0.5, 12.5), week = c(0.5,
>>>>> 52.5), weekday = c(0, 6.5)))
>>>>>
>>>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
>>>>>                         te(temp_max, lag, k=c(10, 4)) +
>>>>>                         te(precip_daily_total, lag, k=c(10, 4)),
>>>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
>>>>>                         knots = list(month = c(0.5, 12.5)))
>>>>>
>>>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
>>>>>                          te(temp_max, lag, k=c(10,3)) +
>>>>>                          te(precip_daily_total, lag, k=c(10,3)),
>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE)
>>>>>
>>>>> ```
>>>>> Thank you if you've read this far!! :-))
>>>>>
>>>>>      [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
>>>>>      [2]: https://i.stack.imgur.com/FzKyM.png
>>>>>      [3]: https://i.stack.imgur.com/fE3aL.png
>>>>>      [4]: https://i.stack.imgur.com/7nbXS.png
>>>>>      [5]: https://i.stack.imgur.com/pNnZU.png
>>>>>      [6]: https://github.com/JadeShodan/heat-mortality
>>>>>      [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> --
>>>> Simon Wood, School of Mathematics, University of Edinburgh,
>>>> https://www.maths.ed.ac.uk/~swood34/
>>>>
>> --
>> Simon Wood, School of Mathematics, University of Edinburgh,
>> https://www.maths.ed.ac.uk/~swood34/
>>
-- 
Simon Wood, School of Mathematics, University of Edinburgh,
https://www.maths.ed.ac.uk/~swood34/


From j@de@shod@@ m@iii@g oii googiem@ii@com  Fri Jun 10 21:19:10 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Fri, 10 Jun 2022 20:19:10 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <bd555484-ba2c-024f-e5eb-9eb19b86761e@dewey.myzen.co.uk>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-EXdbmLVNbEH9nN=FE3FYs4uqXgVtH3V35o6BU0z=rTw@mail.gmail.com>
 <91a30ffd-1aec-0f44-17a0-b632fae91014@dewey.myzen.co.uk>
 <CANg3_k-0=Z4KUE52mxHrSj4c+9bhJ+C4HDqxM2HqqVJ9um0WBQ@mail.gmail.com>
 <CANg3_k-1=WsD71xcSa7WV0Ef=tiY3_Gz2KxvmskY8QuJso+EYA@mail.gmail.com>
 <bd555484-ba2c-024f-e5eb-9eb19b86761e@dewey.myzen.co.uk>
Message-ID: <CANg3_k8VqTpqCw-W7YCZ4kZ_+gnsq5XhEKBCz64HGVoyOUC_Dw@mail.gmail.com>

Hi Michael,

Thanks for your input on this. It's appreciated!

 I think I did not make this very clear in my post here, but I am
trying to model day-to-day variation in deaths and temperature. In
other words, if people get exposed to high temperature today, are
there increased numbers of deaths over the following 6 days? Because
I'm trying to model day-to-day variation, I'm not interested in
seasonal effects (it's a nuisance variable), so I am trying to remove
seasonality.There's a somewhat better post of my problem on Coss
Validated: https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea

The deaths on the 15th of each month are huge errors. The data is from
a low-income country with less than adequate registration of deaths.
This data was collected by interviewers visiting housholds at half
yearly, or even yearly (or larger) intervals and asking if any deaths
had occurred over this period. If people could not remember the
specific date on which a death had occurred, it was - as a rule -
registered on the 15th of the month in which they though the death had
occurred. So the number of deaths on the 15th of each month does not
contribute anything useful to the model. It bears no relation to the
temperature on that day. It carries seasonal information, but I'm not
interested in that, and in fact need to remove that.  Hence why I
modelled those days as a factor variable with 169 levels (14 years of
data x 12 months +1 for all non-heaping days). Because there is only
one data point for each day, the idea was that this would remove any
effect of heaping. Following on from Simon Wood's comments, I'm now
thinking about using interpolation of deaths on those days, rather
than trying to control for heaping in the model. Removing rows of data
for each 15th day of the month doesn't seem right to me, because the
model includes distributed lag. Because of this distributed lag I am
still interested in the temperature on the 15th of each month, because
that they may have lagged effect over the following few days. I just
want to get rid of the error in deaths on the 15th, which I think I
can do by interpolation.

The ACF showed a peak at 15 (albeit barely significant). I don't
understand why that would happen given that heap as a factor with 169
levels should have effectively removed the effect of heaping, but it's
too much of a coincidence, so hopefully interpolation will help with
this too.

As for the concurvity between time and temperature, am going to see if
Simon's suggestion of adding an autoregressive term AR1 term does
anything to reduce it.

Thanks to both you and Simon for your input!

Jade

On Fri, 10 Jun 2022 at 16:45, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>
> Dear Jade
>
> It seems to me that there are several issues here.
>
> 1 - if you fit a separate parameter for each heaping day you efectively
> remove them from the model altogether. If they do carry meaningful
> information then that is undesirable.
>
> 2 - if the reason that there ae so many 15s is because people state,
> "Oh, it was in July" and either they or the interviewere impute 15
> because it si the month mid-point then it begins to look as though
> attempting to use daily data is fraught with problems and monthly might
> be better.
>
> 3 - if you fit a binary heap variable which is zero except for the 15 of
> the month and 1 for the 15 then you are taking out any general overall
> tendency to report 15 but you are allowing for the possibility that the
> relative size of heaping days can vary during the study period.
>
> I am not an expert on this sort of model, that is Simon clearly, but it
> seems to me that, as usual, we have a mix of statistical and scientific
> questions here and you may need to rethink your sceientific model in the
> light of the issue with your data.
>
> Michael
>
> On 10/06/2022 10:30, jade.shodan at googlemail.com wrote:
> > Hi Michael,
> >
> > I don't think my reply to your email came through to the list, so am
> > resending (see below). Problems with subscription have now hopefully
> > been resolved. Apologies if this is a double posting!
> >
> > On Thu, 9 Jun 2022 at 15:27, jade.shodan at googlemail.com
> > <jade.shodan at googlemail.com> wrote:
> >>
> >> Hi Michael,
> >>
> >> Thanks for the reply! When I ran the gam  with the gam() function, the
> >> model worked fine with heap having 169 levels. The same model with
> >> bam() however, fails.  I don't understand the difference between bam()
> >> and gam() at all (other than computational efficiency), but could the
> >> fact that each level only has 1 data point be the reason for it?
> >>
> >> These heaping days are very large measurement errors I need to get rid
> >> off, so I just want to take them out of the model altogether. (My data
> >> is quite noisy already, because date of death is based on memory
> >> recall, rather than formal death registration, due to the data being
> >> from a low income country). Median of deaths is approx. 2 per day, but
> >> on heaping days it can be as high as 50 or so.
> >>
> >> My understanding was that coding with 169 levels would effectively
> >> take these measurements out of the model (but do correct me if I'm
> >> wrong!)  I originally coded heap as a binary variable with 0 for
> >> non-heaping days and 1 for heaping days, but was told that meant that
> >> I was assuming the effect was the same for all heaping days. If I
> >> coded with 12 or 14 levels, wouldn't that leave a lot of noise in the
> >> data?
> >>
> >> Jade
> >>
> >> On Thu, 9 Jun 2022 at 14:52, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
> >>>
> >>> Dear Jade
> >>>
> >>> Do you really need to fit a separate parameter for each heaping day? Can
> >>> you not just make it a binary predictor or a categorical one with fewer
> >>> levels, perhaps 14 (for heaping in each year) or 12 (for each calendar
> >>> month). I have no idea whether that would help but it seems worth a try.
> >>>
> >>> Michael
> >>>
> >>> On 08/06/2022 18:15, jade.shodan--- via R-help wrote:
> >>>> Hi Simon,
> >>>>
> >>>> Thanks so much for this!! I have two follow up questions, if you don't mind.
> >>>>
> >>>> 1. Does including an autoregressive term not adjust away part of the
> >>>> effect of the response in a distributed lag model (where the outcome
> >>>> accumulates over time)?
> >>>> 2. I've tried to fit a model using bam (just a first attempt without
> >>>> AR term), but including the factor variable heap creates errors:
> >>>>
> >>>> bam0 <- bam(deaths~te(year, month, week, weekday,
> >>>> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> >>>>                         te(temp_max, lag, k=c(8, 3)) +
> >>>>                         te(precip_daily_total, lag, k=c(8, 3)),
> >>>>                         data = dat, family = nb, method = 'fREML',
> >>>> select = TRUE, discrete = TRUE,
> >>>>                         knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>>> 52.5), weekday = c(0, 6.5)))
> >>>>
> >>>> This model results in errors:
> >>>>
> >>>> Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
> >>>>     step failure in theta estimation
> >>>> Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> >>>> object$prior.weights)) :
> >>>>     NaNs produced
> >>>>
> >>>>
> >>>> Including heap as as.numeric(heap) runs the model without error
> >>>> messages or warnings, but model diagnostics look terrible, and it also
> >>>> doesn't make sense (to me) to make heap a numeric. The factor variable
> >>>> heap (with 169 levels) codes the fact that all deaths for which no
> >>>> date was known, were registered on the 15th day of each month. I've
> >>>> coded all non-heaping days as 0. All heaping days were coded as a
> >>>> value between 1-168. The time series spans 14 years, so a heaping day
> >>>> in each month results in 14*12 levels = 168, plus one level for
> >>>> non-heaping days.
> >>>>
> >>>> So my second question is: Does bam allow factor variables? And if not,
> >>>> how should I model this heaping on the 15th day of the month instead?
> >>>>
> >>>> With thanks,
> >>>>
> >>>> Jade
> >>>>
> >>>> On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
> >>>>>
> >>>>> I would not worry too much about high concurvity between variables like
> >>>>> temperature and time. This just reflects the fact that temperature has a
> >>>>> strong temporal pattern.
> >>>>>
> >>>>> I would also not be too worried about the low p-values on the k check.
> >>>>> The check only looks for pattern in the residuals when they are ordered
> >>>>> with respect to the variables of a smooth. When you have time series
> >>>>> data and some smooths involve time then it's hard not to pick up some
> >>>>> degree of residual auto-correlation, which you often would not want to
> >>>>> model with a higher rank smoother.
> >>>>>
> >>>>> The NAs  for the distributed lag terms just reflect the fact that there
> >>>>> is no obvious way to order the residuals w.r.t. the covariates for such
> >>>>> terms, so the simple check for residual pattern is not really possible.
> >>>>>
> >>>>> One simple approach is to fit using bam(...,discrete=TRUE) which will
> >>>>> let you specify an AR1 parameter to mop up some of the residual
> >>>>> auto-correlation without resorting to a high rank smooth that then does
> >>>>> all the work of the covariates as well. The AR1 parameter can be set by
> >>>>> looking at the ACF of the residuals of the model without this. You need
> >>>>> to look at the ACF of suitably standardized residuals to check how well
> >>>>> this has worked.
> >>>>>
> >>>>> best,
> >>>>>
> >>>>> Simon
> >>>>>
> >>>>> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> >>>>>> Hello everyone,
> >>>>>>
> >>>>>> A few days ago I asked a question about concurvity in a GAM (the
> >>>>>> anologue of collinearity in a GLM) implemented in mgcv. I think my
> >>>>>> question was a bit unfocussed, so I am retrying again, but with
> >>>>>> additional information included about the autocorrelation function. I
> >>>>>> have also posted about this on Cross Validated. Given all the model
> >>>>>> output, it might make for easier
> >>>>>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> >>>>>>
> >>>>>> As mentioned previously, I have problems with concurvity in my thesis
> >>>>>> research, and don't have access to a statistician who works with time
> >>>>>> series, GAMs or R. I'd be very grateful for any (partial) answer,
> >>>>>> however short. I'll gladly return the favour where I can! For really
> >>>>>> helpful input I'd be more than happy to offer co-authorship on
> >>>>>> publication. Deadlines are very close, and I'm heading towards having
> >>>>>> no results at all if I can't solve this concurvity issue :(
> >>>>>>
> >>>>>> I'm using GAMs to try to understand the relationship between deaths
> >>>>>> and heat-related variables (e.g. temperature and humidity), using
> >>>>>> daily time series over a 14-year period from a tropical, low-income
> >>>>>> country. My aim is to understand the relationship between these
> >>>>>> variables and deaths, rather than pure prediction performance.
> >>>>>>
> >>>>>> The GAMs include distributed lag models (set up as 7-column matrices,
> >>>>>> see code at bottom of post), since deaths may occur over several days
> >>>>>> following exposure.
> >>>>>>
> >>>>>> Simple GAMs with just time, lagged temperature and lagged
> >>>>>> precipitation (a potential confounder) show very high concurvity
> >>>>>> between lagged temperature and time, regardless of the many different
> >>>>>> ways I have tried to decompose time. The autocorrelation functions
> >>>>>> (ACF) however, shows values close to zero, only just about breaching
> >>>>>> the 'significance line' in a few instances. It does show patterning
> >>>>>> though, although the regularity is difficult to define.
> >>>>>>
> >>>>>> My questions are:
> >>>>>> 1) Should I be worried about the high concurvity, or can I ignore it
> >>>>>> given the mostly non-significant ACF? I've read dozens of
> >>>>>> heat-mortality modelling studies and none report on concurvity between
> >>>>>> weather variables and time (though one 2012 paper discussed
> >>>>>> autocorrelation).
> >>>>>>
> >>>>>> 2) If I cannot ignore it, what should I do to resolve it? Would
> >>>>>> including an autoregressive term be appropriate, and if so, where can
> >>>>>> I find a coded example of how to do this? I've also come across
> >>>>>> sequential regression][1]. Would this be more or less appropriate? If
> >>>>>> appropriate, a pointer to an example would be really appreciated!
> >>>>>>
> >>>>>> Some example GAMs are specified as follows:
> >>>>>> ```r
> >>>>>> conc38b <- gam(deaths~te(year, month, week, weekday,
> >>>>>> bs=c("cr","cc","cc","cc")) + heap +
> >>>>>>                          te(temp_max, lag, k=c(10, 3)) +
> >>>>>>                          te(precip_daily_total, lag, k=c(10, 3)),
> >>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE,
> >>>>>>                          knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>>>>> 52.5), weekday = c(0, 6.5)))
> >>>>>> ```
> >>>>>> Concurvity for the above model between (temp_max, lag) and (year,
> >>>>>> month, week, weekday) is 0.91:
> >>>>>>
> >>>>>> ```r
> >>>>>> $worst
> >>>>>>                                        para te(year,month,week,weekday)
> >>>>>> te(temp_max,lag) te(precip_daily_total,lag)
> >>>>>> para                        1.000000e+00                1.125625e-29
> >>>>>>         0.3150073                  0.6666348
> >>>>>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> >>>>>>         0.9060552                  0.6652313
> >>>>>> te(temp_max,lag)            3.152795e-01                8.998113e-01
> >>>>>>         1.0000000                  0.5781015
> >>>>>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> >>>>>>         0.5805159                  1.0000000
> >>>>>> ```
> >>>>>>
> >>>>>> Output from ```gam.check()```:
> >>>>>> ```r
> >>>>>> Method: REML   Optimizer: outer newton
> >>>>>> full convergence after 16 iterations.
> >>>>>> Gradient range [-0.01467332,0.003096643]
> >>>>>> (score 8915.994 & scale 1).
> >>>>>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> >>>>>> Model rank =  544 / 544
> >>>>>>
> >>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>>>>> indicate that k is too low, especially if edf is close to k'.
> >>>>>>
> >>>>>>                                      k'      edf k-index p-value
> >>>>>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> >>>>>> te(temp_max,lag)             29.0000   3.3681      NA      NA
> >>>>>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> >>>>>> ---
> >>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>>> ```
> >>>>>>
> >>>>>> Some output from ```summary(conc38b)```:
> >>>>>> ```r
> >>>>>> Approximate significance of smooth terms:
> >>>>>>                                      edf Ref.df  Chi.sq p-value
> >>>>>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> >>>>>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> >>>>>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> >>>>>> ---
> >>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>>>
> >>>>>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> >>>>>> -REML =   8916  Scale est. = 1         n = 5107
> >>>>>> ```
> >>>>>>
> >>>>>>
> >>>>>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> >>>>>> pattern). They show peaks at 5 and 15, and then there seems to be a
> >>>>>> recurring pattern at multiples of approx. 30 (suggesting month is not
> >>>>>> modelled adequately?). Not sure what would cause the spikes at 5 and
> >>>>>> 15. There is heaping of deaths on the 15th day of each month, to which
> >>>>>> deaths with unknown date were allocated. This heaping was modelled
> >>>>>> with categorical variable/ factor ```heap``` with 169 levels (0 for
> >>>>>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> >>>>>> heaping day over the 14-year period):
> >>>>>>
> >>>>>>      [2]: https://i.stack.imgur.com/FzKyM.png
> >>>>>>      [3]: https://i.stack.imgur.com/fE3aL.png
> >>>>>>
> >>>>>>
> >>>>>> I get an identical looking ACF when I decompose time into (year,
> >>>>>> month, monthday) as in model conc39 below, although concurvity between
> >>>>>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> >>>>>>
> >>>>>> ```r
> >>>>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >>>>>>                         te(temp_max, lag, k=c(10, 4)) +
> >>>>>>                         te(precip_daily_total, lag, k=c(10, 4)),
> >>>>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> >>>>>>                         knots = list(month = c(0.5, 12.5)))
> >>>>>> ```
> >>>>>> ```r
> >>>>>>
> >>>>>> Method: REML   Optimizer: outer newton
> >>>>>> full convergence after 14 iterations.
> >>>>>> Gradient range [-0.001578187,6.155096e-05]
> >>>>>> (score 8915.763 & scale 1).
> >>>>>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> >>>>>> Model rank =  323 / 323
> >>>>>>
> >>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>>>>> indicate that k is too low, especially if edf is close to k'.
> >>>>>>
> >>>>>>                                    k'     edf k-index p-value
> >>>>>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> >>>>>> te(temp_max,lag)           39.0000  4.0875      NA      NA
> >>>>>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> >>>>>> ---
> >>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>>> ```
> >>>>>> Some output from ```summary(conc39)```:
> >>>>>> ```r
> >>>>>> Approximate significance of smooth terms:
> >>>>>>                                    edf Ref.df  Chi.sq  p-value
> >>>>>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> >>>>>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> >>>>>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> >>>>>> ---
> >>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>>>
> >>>>>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> >>>>>> -REML =   8915  Scale est. = 1         n = 5107
> >>>>>> ```
> >>>>>>
> >>>>>>
> >>>>>> ```r
> >>>>>> $worst
> >>>>>>                                       para te(year,month,monthday)
> >>>>>> te(temp_max,lag) te(precip_daily_total,lag)
> >>>>>> para                       1.000000e+00            3.261007e-31
> >>>>>> 0.3313549                  0.6666532
> >>>>>> te(year,month,monthday)    3.060763e-31            1.000000e+00
> >>>>>> 0.8266086                  0.5670777
> >>>>>> te(temp_max,lag)           3.331014e-01            8.225942e-01
> >>>>>> 1.0000000                  0.5840875
> >>>>>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> >>>>>> 0.5939380                  1.0000000
> >>>>>> ```
> >>>>>>
> >>>>>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> >>>>>> various choices for k or as ```s(time)``` with various k does not
> >>>>>> reduce concurvity either.
> >>>>>>
> >>>>>>
> >>>>>> The default approach in time series studies of heat-mortality is to
> >>>>>> model time with fixed df, generally between 7-10 df per year of data.
> >>>>>> I am, however, apprehensive about this approach because a) mortality
> >>>>>> profiles vary with locality due to sociodemographic and environmental
> >>>>>> characteristics and b) the choice of df is based on higher income
> >>>>>> countries (where nearly all these studies have been done) with
> >>>>>> different mortality profiles and so may not be appropriate for
> >>>>>> tropical, low-income countries.
> >>>>>>
> >>>>>> Although the approach of fixing (high) df does remove more temporal
> >>>>>> patterns from the ACF (see model and output below), concurvity between
> >>>>>> time and lagged temperature has now risen to 0.99! Moreover,
> >>>>>> temperature (which has been a consistent, highly significant predictor
> >>>>>> in every model of the tens (hundreds?) I have run, has now turned
> >>>>>> non-significant. I am guessing this is because time is now a very
> >>>>>> wiggly function that not only models/ removes seasonal variation, but
> >>>>>> also some of the day-to-day variation that is needed for the
> >>>>>> temperature smooth  :
> >>>>>>
> >>>>>> ```r
> >>>>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >>>>>>                          te(temp_max, lag, k=c(10,3)) +
> >>>>>>                          te(precip_daily_total, lag, k=c(10,3)),
> >>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE)
> >>>>>> ```
> >>>>>> Output from ```gam.check(conc20a, rep = 1000)```:
> >>>>>>
> >>>>>> ```r
> >>>>>> Method: REML   Optimizer: outer newton
> >>>>>> full convergence after 9 iterations.
> >>>>>> Gradient range [-0.0008983099,9.546022e-05]
> >>>>>> (score 8750.13 & scale 1).
> >>>>>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> >>>>>> Model rank =  336 / 336
> >>>>>>
> >>>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>>>>> indicate that k is too low, especially if edf is close to k'.
> >>>>>>
> >>>>>>                                     k'      edf k-index p-value
> >>>>>> s(time)                    111.0000 111.0000    0.98    0.56
> >>>>>> te(temp_max,lag)            29.0000   0.6548      NA      NA
> >>>>>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> >>>>>> ```
> >>>>>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
> >>>>>>
> >>>>>> ```r
> >>>>>>                                       para      s(time) te(temp_max,lag)
> >>>>>> te(precip_daily_total,lag)
> >>>>>> para                       1.000000e+00 2.462064e-19        0.3165236
> >>>>>>                    0.6666348
> >>>>>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
> >>>>>>                    0.6879284
> >>>>>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> >>>>>>                    0.5788711
> >>>>>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> >>>>>>                    1.0000000
> >>>>>>
> >>>>>> ```
> >>>>>>
> >>>>>> Some output from ```summary(conc20a)```:
> >>>>>> ```r
> >>>>>> Approximate significance of smooth terms:
> >>>>>>                                     edf Ref.df  Chi.sq p-value
> >>>>>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> >>>>>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> >>>>>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> >>>>>> ---
> >>>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>>>
> >>>>>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> >>>>>> -REML = 8750.1  Scale est. = 1         n = 5107
> >>>>>> ```
> >>>>>>
> >>>>>> ACF functions:
> >>>>>>
> >>>>>> [4]: https://i.stack.imgur.com/7nbXS.png
> >>>>>> [5]: https://i.stack.imgur.com/pNnZU.png
> >>>>>>
> >>>>>> Data can be found on my [GitHub][6] site in the file
> >>>>>> [data_cross_validated_post2.rds][7]. A csv version is also available.
> >>>>>> This is my code:
> >>>>>>
> >>>>>> ```r
> >>>>>> library(readr)
> >>>>>> library(mgcv)
> >>>>>>
> >>>>>> df <- read_rds("data_crossvalidated_post2.rds")
> >>>>>>
> >>>>>> # Create matrices for lagged weather variables (6 day lags) based on
> >>>>>> example by Simon Wood
> >>>>>> # in his 2017 book ("Generalized additive models: an introduction with
> >>>>>> R", p. 349) and
> >>>>>> # gamair package documentation
> >>>>>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> >>>>>>
> >>>>>> lagard <- function(x,n.lag=7) {
> >>>>>> n <- length(x); X <- matrix(NA,n,n.lag)
> >>>>>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> >>>>>> X
> >>>>>> }
> >>>>>>
> >>>>>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> >>>>>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> >>>>>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> >>>>>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> >>>>>> = df$precip_hourly_dailysum)
> >>>>>> dat$temp_max <- lagard(df$temp_max)
> >>>>>> dat$temp_min <- lagard(df$temp_min)
> >>>>>> dat$temp_mean <- lagard(df$temp_mean)
> >>>>>> dat$wbgt_max <- lagard(df$wbgt_max)
> >>>>>> dat$wbgt_mean <- lagard(df$wbgt_mean)
> >>>>>> dat$wbgt_min <- lagard(df$wbgt_min)
> >>>>>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> >>>>>> dat$sh_mean <- lagard(df$sh_mean)
> >>>>>> dat$solar_mean <- lagard(df$solar_mean)
> >>>>>> dat$wind2m_mean <- lagard(df$wind2m_mean)
> >>>>>> dat$sh_max <- lagard(df$sh_max)
> >>>>>> dat$solar_max <- lagard(df$solar_max)
> >>>>>> dat$wind2m_max <- lagard(df$wind2m_max)
> >>>>>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> >>>>>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> >>>>>> dat$precip_hourly <- lagard(df$precip_hourly)
> >>>>>> dat$precip_daily_total <- lagard( df$precip_daily_total)
> >>>>>> dat$temp <- lagard(df$temp)
> >>>>>> dat$sh <- lagard(df$sh)
> >>>>>> dat$rh <- lagard(df$rh)
> >>>>>> dat$solar <- lagard(df$solar)
> >>>>>> dat$wind2m <- lagard(df$wind2m)
> >>>>>>
> >>>>>>
> >>>>>> conc38b <- gam(deaths~te(year, month, week, weekday,
> >>>>>> bs=c("cr","cc","cc","cc")) + heap +
> >>>>>>                          te(temp_max, lag, k=c(10, 3)) +
> >>>>>>                          te(precip_daily_total, lag, k=c(10, 3)),
> >>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE,
> >>>>>>                          knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>>>>> 52.5), weekday = c(0, 6.5)))
> >>>>>>
> >>>>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >>>>>>                         te(temp_max, lag, k=c(10, 4)) +
> >>>>>>                         te(precip_daily_total, lag, k=c(10, 4)),
> >>>>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> >>>>>>                         knots = list(month = c(0.5, 12.5)))
> >>>>>>
> >>>>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >>>>>>                          te(temp_max, lag, k=c(10,3)) +
> >>>>>>                          te(precip_daily_total, lag, k=c(10,3)),
> >>>>>>                          data = dat, family = nb, method = 'REML', select = TRUE)
> >>>>>>
> >>>>>> ```
> >>>>>> Thank you if you've read this far!! :-))
> >>>>>>
> >>>>>>      [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> >>>>>>      [2]: https://i.stack.imgur.com/FzKyM.png
> >>>>>>      [3]: https://i.stack.imgur.com/fE3aL.png
> >>>>>>      [4]: https://i.stack.imgur.com/7nbXS.png
> >>>>>>      [5]: https://i.stack.imgur.com/pNnZU.png
> >>>>>>      [6]: https://github.com/JadeShodan/heat-mortality
> >>>>>>      [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>
> >>>>> --
> >>>>> Simon Wood, School of Mathematics, University of Edinburgh,
> >>>>> https://www.maths.ed.ac.uk/~swood34/
> >>>>>
> >>>>
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>
> >>> --
> >>> Michael
> >>> http://www.dewey.myzen.co.uk/home.html
> >
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html


From j@de@shod@@ m@iii@g oii googiem@ii@com  Fri Jun 10 21:36:18 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Fri, 10 Jun 2022 20:36:18 +0100
Subject: [R] 
 High concurvity/ collinearity between time and temperature in
 GAM predicting deaths but low ACF. Does this matter?
In-Reply-To: <80abf9e4-d864-43d3-e8ef-488239d22931@bath.edu>
References: <CANg3_k8JbJ9AXzuazj9m+OUScJSLBdBcE=TS-mqirLUtig2GMA@mail.gmail.com>
 <9c0aded5-7488-02a1-8aef-318c95ba8d03@bath.edu>
 <CANg3_k-g=M41cxom6DVdTunFjgZ6VfKobqW__Mq351pDpY22PQ@mail.gmail.com>
 <f371005e-6f57-2007-f859-d66595a19c0f@bath.edu>
 <CANg3_k_qMX24_a-Hppk6knrH-nt1QMLj2FFSJbkMJc0TBxqzQA@mail.gmail.com>
 <80abf9e4-d864-43d3-e8ef-488239d22931@bath.edu>
Message-ID: <CANg3_k_OZqM1H2iHn6exCpkcoG95scfQiejTPZwQjpqfJHQYAg@mail.gmail.com>

Thank you Simon!!

As you may have seen in the reply to Michael, I'm going to run with
interpolation of deaths on the 15th of each month, and then run bam()
with AR1 term.

Thanks again!

On Fri, 10 Jun 2022 at 19:50, Simon Wood <simon.wood at bath.edu> wrote:
>
> Apologies, I meant...
>
> aheap <- heap!="0"
> aheap + s(heap,bs="re",by=aheap)
>
> - fixed effect for heap day or not, and then an extra random effect if it is a heap day
>
> ... bit difficult to give a generic answer to the usefulness question -
> I guess you probably have to decide on that yourself.
>
> Simon
>
> On 09/06/2022 16:57, jade.shodan at googlemail.com wrote:
> > Hi Simon,
> >
> > (Sorry, replies and answers are out of sync due to my problems posting
> > to the list/ messages being held for moderation)
> >
> >> Did it actually fail, or simply generate warnings?
> > The model was computed (you're right, not a model failure), but
> > resulted in warnings (as per previous post) which, frankly, I didn't
> > understand.
> >
> >> if I understand your model right there is one free parameter for each observation falling on the 15th
> > That's right, one observation on each 15th day of the month.
> >
> > Thank you for the suggestion about the random effects! I had been
> > wondering about how I could model this heaping with a smooth!
> >
> > Quick question:
> >
> > You proposed:
> >
> > aheap <- heap!="0"
> > heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap
> > day + a r.e. for each heap day - no effect on non-heap days
> >
> > Is there a typo in the code above? I don't see the newly created
> > variable aheap in the model?
> > Should it read "by = aheap" as follows:
> >
> > heap + s(heap,bs="re",by=aheap)   ?
> >
> > So a full model might then look like the one below?
> >
> >   bam0 <- bam(deaths~te(year, month, week, weekday,
> > bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> > s(heap,bs="re",by=aheap)
> >                                       te(temp_max, lag, k=c(8, 3)) +
> >                                       te(precip_daily_total, lag, k=c(8, 3)),
> >                                       data = dat, family = nb, method =
> > 'fREML', select = TRUE, discrete = TRUE,
> >                                     knots = list(month = c(0.5, 12.5),
> > week = c(0.5, 52.5), weekday = c(0, 6.5)))
> >
> > One, hopefully final (!) question:
> >
> > Is it actually useful at all to keep these observations on the 15th
> > day of each month (which are huge errors), or am I better off removing
> > them from the data set (or replacing them with e.g. median values)?
> > For temperature-mortality modelling it is the day-to-day variation in
> > deaths and temperature that is of interest. So is modelling heaping
> > actually useful at all, given that this variable changes on a monthly
> > basis? (I think you are alluding to this, but I just want to make
> > sure).
> >
> > If I take them out altogether, would I be best off removing all data
> > for these dates, so that the time series jumps from day 14 to day 16?
> > Or would this create problems with e.g. the distributed lag model?
> >
> > Sorry for all these questions! Have been struggling with this for
> > months (posted on Cross Validated about the heaping issue too), and
> > feel I am finally getting somewhere with your help!
> >
> > Jade
> >
> > On Thu, 9 Jun 2022 at 15:24, Simon Wood <simon.wood at bath.edu> wrote:
> >>
> >> On 09/06/2022 12:30, jade.shodan at googlemail.com wrote:
> >>> Hi Simon,
> >>>
> >>> Thanks so much for this!! (Apologies if this is a double posting. I
> >>> seem to have a problem getting messages through to the list).
> >>>
> >>> I have two follow up questions, if you don't mind.
> >>>
> >>> 1. Does including an autoregressive term not adjust away part of the
> >>> effect of the response in a distributed lag model (where the outcome
> >>> accumulates over time)?
> >> - the hope is that it approximately deals with short timescale stuff
> >> without interfering with the longer timescales to the same extent as a
> >> high rank smooth would.
> >>
> >>> 2. I've tried to fit a model using bam (just a first attempt without
> >>> AR term), but including the factor variable heap creates errors:
> >>>
> >>> bam0 <- bam(deaths~te(year, month, week, weekday,
> >>> bs=c("cr","cc","cc","cc"), k = c(4,5,5,5)) + heap +
> >>>                                        te(temp_max, lag, k=c(8, 3)) +
> >>>                                        te(precip_daily_total, lag, k=c(8, 3)),
> >>>                                       data = dat, family = nb, method =
> >>> 'fREML', select = TRUE, discrete = TRUE,
> >>>                                       knots = list(month = c(0.5, 12.5),
> >>> week = c(0.5, 52.5), weekday = c(0, 6.5)))
> >>>
> >>> This model results in errors:
> >>>
> >>> Warning in estimate.theta(theta, family, y, mu, scale = scale1, wt = G$w,  :
> >>>     step failure in theta estimation
> >>> Warning in sqrt(family$dev.resids(object$y, object$fitted.values,
> >>> object$prior.weights)) :
> >>>     NaNs produced
> >>>
> >> Did it actually fail, or simply generate warnings?
> >>
> >> 'bam' handles factors, but if I understand your model right there is one
> >> free parameter for each observation falling on the 15th, so that you
> >> will fit data for those days exactly (and might just as well have
> >> dropped them, for all the information they contribute to the rest of the
> >> model). If you want a structure like this, I'd be inclined to make the
> >> heap variable random, something like...
> >>
> >> aheap <- heap!="0"
> >>
> >> heap + s(heap,bs="re",by=heap) ## fixed mean effect of being a heap day
> >> + a r.e. for each heap day - no effect on non-heap days
> >>
> >> best,
> >>
> >> Simon
> >>
> >>> Including heap as as.numeric(heap) runs the model without error
> >>> messages or warnings, but model diagnostics look terrible, and it also
> >>> doesn't make sense (to me) to make heap a numeric. The factor variable
> >>> heap (with 169 levels) codes the fact that all deaths for which no
> >>> date was known, were registered on the 15th day of each month. I've
> >>> coded all non-heaping days as 0. All heaping days were coded as a
> >>> value between 1-168. The time series spans 14 years, so a heaping day
> >>> in each month results in 14*12 levels = 168, plus one level for
> >>> non-heaping days.
> >>>
> >>> So my second question is: Does bam allow factor variables? And if not,
> >>> how should I model this heaping on the 15th day of the month instead?
> >>>
> >>> With thanks,
> >>>
> >>> Jade
> >>>
> >>> On Wed, 8 Jun 2022 at 12:05, Simon Wood <simon.wood at bath.edu> wrote:
> >>>> I would not worry too much about high concurvity between variables like
> >>>> temperature and time. This just reflects the fact that temperature has a
> >>>> strong temporal pattern.
> >>>>
> >>>> I would also not be too worried about the low p-values on the k check.
> >>>> The check only looks for pattern in the residuals when they are ordered
> >>>> with respect to the variables of a smooth. When you have time series
> >>>> data and some smooths involve time then it's hard not to pick up some
> >>>> degree of residual auto-correlation, which you often would not want to
> >>>> model with a higher rank smoother.
> >>>>
> >>>> The NAs  for the distributed lag terms just reflect the fact that there
> >>>> is no obvious way to order the residuals w.r.t. the covariates for such
> >>>> terms, so the simple check for residual pattern is not really possible.
> >>>>
> >>>> One simple approach is to fit using bam(...,discrete=TRUE) which will
> >>>> let you specify an AR1 parameter to mop up some of the residual
> >>>> auto-correlation without resorting to a high rank smooth that then does
> >>>> all the work of the covariates as well. The AR1 parameter can be set by
> >>>> looking at the ACF of the residuals of the model without this. You need
> >>>> to look at the ACF of suitably standardized residuals to check how well
> >>>> this has worked.
> >>>>
> >>>> best,
> >>>>
> >>>> Simon
> >>>>
> >>>> On 05/06/2022 20:01, jade.shodan--- via R-help wrote:
> >>>>> Hello everyone,
> >>>>>
> >>>>> A few days ago I asked a question about concurvity in a GAM (the
> >>>>> anologue of collinearity in a GLM) implemented in mgcv. I think my
> >>>>> question was a bit unfocussed, so I am retrying again, but with
> >>>>> additional information included about the autocorrelation function. I
> >>>>> have also posted about this on Cross Validated. Given all the model
> >>>>> output, it might make for easier
> >>>>> reading:https://stats.stackexchange.com/questions/577790/high-concurvity-collinearity-between-time-and-temperature-in-gam-predicting-dea
> >>>>>
> >>>>> As mentioned previously, I have problems with concurvity in my thesis
> >>>>> research, and don't have access to a statistician who works with time
> >>>>> series, GAMs or R. I'd be very grateful for any (partial) answer,
> >>>>> however short. I'll gladly return the favour where I can! For really
> >>>>> helpful input I'd be more than happy to offer co-authorship on
> >>>>> publication. Deadlines are very close, and I'm heading towards having
> >>>>> no results at all if I can't solve this concurvity issue :(
> >>>>>
> >>>>> I'm using GAMs to try to understand the relationship between deaths
> >>>>> and heat-related variables (e.g. temperature and humidity), using
> >>>>> daily time series over a 14-year period from a tropical, low-income
> >>>>> country. My aim is to understand the relationship between these
> >>>>> variables and deaths, rather than pure prediction performance.
> >>>>>
> >>>>> The GAMs include distributed lag models (set up as 7-column matrices,
> >>>>> see code at bottom of post), since deaths may occur over several days
> >>>>> following exposure.
> >>>>>
> >>>>> Simple GAMs with just time, lagged temperature and lagged
> >>>>> precipitation (a potential confounder) show very high concurvity
> >>>>> between lagged temperature and time, regardless of the many different
> >>>>> ways I have tried to decompose time. The autocorrelation functions
> >>>>> (ACF) however, shows values close to zero, only just about breaching
> >>>>> the 'significance line' in a few instances. It does show patterning
> >>>>> though, although the regularity is difficult to define.
> >>>>>
> >>>>> My questions are:
> >>>>> 1) Should I be worried about the high concurvity, or can I ignore it
> >>>>> given the mostly non-significant ACF? I've read dozens of
> >>>>> heat-mortality modelling studies and none report on concurvity between
> >>>>> weather variables and time (though one 2012 paper discussed
> >>>>> autocorrelation).
> >>>>>
> >>>>> 2) If I cannot ignore it, what should I do to resolve it? Would
> >>>>> including an autoregressive term be appropriate, and if so, where can
> >>>>> I find a coded example of how to do this? I've also come across
> >>>>> sequential regression][1]. Would this be more or less appropriate? If
> >>>>> appropriate, a pointer to an example would be really appreciated!
> >>>>>
> >>>>> Some example GAMs are specified as follows:
> >>>>> ```r
> >>>>> conc38b <- gam(deaths~te(year, month, week, weekday,
> >>>>> bs=c("cr","cc","cc","cc")) + heap +
> >>>>>                          te(temp_max, lag, k=c(10, 3)) +
> >>>>>                          te(precip_daily_total, lag, k=c(10, 3)),
> >>>>>                          data = dat, family = nb, method = 'REML', select = TRUE,
> >>>>>                          knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>>>> 52.5), weekday = c(0, 6.5)))
> >>>>> ```
> >>>>> Concurvity for the above model between (temp_max, lag) and (year,
> >>>>> month, week, weekday) is 0.91:
> >>>>>
> >>>>> ```r
> >>>>> $worst
> >>>>>                                        para te(year,month,week,weekday)
> >>>>> te(temp_max,lag) te(precip_daily_total,lag)
> >>>>> para                        1.000000e+00                1.125625e-29
> >>>>>         0.3150073                  0.6666348
> >>>>> te(year,month,week,weekday) 1.400648e-29                1.000000e+00
> >>>>>         0.9060552                  0.6652313
> >>>>> te(temp_max,lag)            3.152795e-01                8.998113e-01
> >>>>>         1.0000000                  0.5781015
> >>>>> te(precip_daily_total,lag)  6.666348e-01                6.652313e-01
> >>>>>         0.5805159                  1.0000000
> >>>>> ```
> >>>>>
> >>>>> Output from ```gam.check()```:
> >>>>> ```r
> >>>>> Method: REML   Optimizer: outer newton
> >>>>> full convergence after 16 iterations.
> >>>>> Gradient range [-0.01467332,0.003096643]
> >>>>> (score 8915.994 & scale 1).
> >>>>> Hessian positive definite, eigenvalue range [5.048053e-05,26.50175].
> >>>>> Model rank =  544 / 544
> >>>>>
> >>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>>>> indicate that k is too low, especially if edf is close to k'.
> >>>>>
> >>>>>                                      k'      edf k-index p-value
> >>>>> te(year,month,week,weekday) 319.0000  26.6531    0.96    0.06 .
> >>>>> te(temp_max,lag)             29.0000   3.3681      NA      NA
> >>>>> te(precip_daily_total,lag)   27.0000   0.0051      NA      NA
> >>>>> ---
> >>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>> ```
> >>>>>
> >>>>> Some output from ```summary(conc38b)```:
> >>>>> ```r
> >>>>> Approximate significance of smooth terms:
> >>>>>                                      edf Ref.df  Chi.sq p-value
> >>>>> te(year,month,week,weekday) 26.653127    319 166.803 < 2e-16 ***
> >>>>> te(temp_max,lag)             3.368129     27  11.130 0.00145 **
> >>>>> te(precip_daily_total,lag)   0.005104     27   0.002 0.69636
> >>>>> ---
> >>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>>
> >>>>> R-sq.(adj) =  0.839   Deviance explained = 53.3%
> >>>>> -REML =   8916  Scale est. = 1         n = 5107
> >>>>> ```
> >>>>>
> >>>>>
> >>>>> Below are the ACF plots (note limit y-axis = 0.1 for clarity of
> >>>>> pattern). They show peaks at 5 and 15, and then there seems to be a
> >>>>> recurring pattern at multiples of approx. 30 (suggesting month is not
> >>>>> modelled adequately?). Not sure what would cause the spikes at 5 and
> >>>>> 15. There is heaping of deaths on the 15th day of each month, to which
> >>>>> deaths with unknown date were allocated. This heaping was modelled
> >>>>> with categorical variable/ factor ```heap``` with 169 levels (0 for
> >>>>> all non-heaping days and 1-168 (i.e. 14 * 12 for each subsequent
> >>>>> heaping day over the 14-year period):
> >>>>>
> >>>>>      [2]: https://i.stack.imgur.com/FzKyM.png
> >>>>>      [3]: https://i.stack.imgur.com/fE3aL.png
> >>>>>
> >>>>>
> >>>>> I get an identical looking ACF when I decompose time into (year,
> >>>>> month, monthday) as in model conc39 below, although concurvity between
> >>>>> (temp_max, lag) and the time term has now dropped somewhat to 0.83:
> >>>>>
> >>>>> ```r
> >>>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >>>>>                         te(temp_max, lag, k=c(10, 4)) +
> >>>>>                         te(precip_daily_total, lag, k=c(10, 4)),
> >>>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> >>>>>                         knots = list(month = c(0.5, 12.5)))
> >>>>> ```
> >>>>> ```r
> >>>>>
> >>>>> Method: REML   Optimizer: outer newton
> >>>>> full convergence after 14 iterations.
> >>>>> Gradient range [-0.001578187,6.155096e-05]
> >>>>> (score 8915.763 & scale 1).
> >>>>> Hessian positive definite, eigenvalue range [1.894391e-05,26.99215].
> >>>>> Model rank =  323 / 323
> >>>>>
> >>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>>>> indicate that k is too low, especially if edf is close to k'.
> >>>>>
> >>>>>                                    k'     edf k-index p-value
> >>>>> te(year,month,monthday)    79.0000 25.0437    0.93  <2e-16 ***
> >>>>> te(temp_max,lag)           39.0000  4.0875      NA      NA
> >>>>> te(precip_daily_total,lag) 36.0000  0.0107      NA      NA
> >>>>> ---
> >>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>> ```
> >>>>> Some output from ```summary(conc39)```:
> >>>>> ```r
> >>>>> Approximate significance of smooth terms:
> >>>>>                                    edf Ref.df  Chi.sq  p-value
> >>>>> te(year,month,monthday)    38.75573     99 187.231  < 2e-16 ***
> >>>>> te(temp_max,lag)            4.06437     37  25.927 1.66e-06 ***
> >>>>> te(precip_daily_total,lag)  0.01173     36   0.008    0.557
> >>>>> ---
> >>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>>
> >>>>> R-sq.(adj) =  0.839   Deviance explained = 53.8%
> >>>>> -REML =   8915  Scale est. = 1         n = 5107
> >>>>> ```
> >>>>>
> >>>>>
> >>>>> ```r
> >>>>> $worst
> >>>>>                                       para te(year,month,monthday)
> >>>>> te(temp_max,lag) te(precip_daily_total,lag)
> >>>>> para                       1.000000e+00            3.261007e-31
> >>>>> 0.3313549                  0.6666532
> >>>>> te(year,month,monthday)    3.060763e-31            1.000000e+00
> >>>>> 0.8266086                  0.5670777
> >>>>> te(temp_max,lag)           3.331014e-01            8.225942e-01
> >>>>> 1.0000000                  0.5840875
> >>>>> te(precip_daily_total,lag) 6.666532e-01            5.670777e-01
> >>>>> 0.5939380                  1.0000000
> >>>>> ```
> >>>>>
> >>>>> Modelling time as ```te(year, doy)``` with a cyclic spline for doy and
> >>>>> various choices for k or as ```s(time)``` with various k does not
> >>>>> reduce concurvity either.
> >>>>>
> >>>>>
> >>>>> The default approach in time series studies of heat-mortality is to
> >>>>> model time with fixed df, generally between 7-10 df per year of data.
> >>>>> I am, however, apprehensive about this approach because a) mortality
> >>>>> profiles vary with locality due to sociodemographic and environmental
> >>>>> characteristics and b) the choice of df is based on higher income
> >>>>> countries (where nearly all these studies have been done) with
> >>>>> different mortality profiles and so may not be appropriate for
> >>>>> tropical, low-income countries.
> >>>>>
> >>>>> Although the approach of fixing (high) df does remove more temporal
> >>>>> patterns from the ACF (see model and output below), concurvity between
> >>>>> time and lagged temperature has now risen to 0.99! Moreover,
> >>>>> temperature (which has been a consistent, highly significant predictor
> >>>>> in every model of the tens (hundreds?) I have run, has now turned
> >>>>> non-significant. I am guessing this is because time is now a very
> >>>>> wiggly function that not only models/ removes seasonal variation, but
> >>>>> also some of the day-to-day variation that is needed for the
> >>>>> temperature smooth  :
> >>>>>
> >>>>> ```r
> >>>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >>>>>                          te(temp_max, lag, k=c(10,3)) +
> >>>>>                          te(precip_daily_total, lag, k=c(10,3)),
> >>>>>                          data = dat, family = nb, method = 'REML', select = TRUE)
> >>>>> ```
> >>>>> Output from ```gam.check(conc20a, rep = 1000)```:
> >>>>>
> >>>>> ```r
> >>>>> Method: REML   Optimizer: outer newton
> >>>>> full convergence after 9 iterations.
> >>>>> Gradient range [-0.0008983099,9.546022e-05]
> >>>>> (score 8750.13 & scale 1).
> >>>>> Hessian positive definite, eigenvalue range [0.0001420112,15.40832].
> >>>>> Model rank =  336 / 336
> >>>>>
> >>>>> Basis dimension (k) checking results. Low p-value (k-index<1) may
> >>>>> indicate that k is too low, especially if edf is close to k'.
> >>>>>
> >>>>>                                     k'      edf k-index p-value
> >>>>> s(time)                    111.0000 111.0000    0.98    0.56
> >>>>> te(temp_max,lag)            29.0000   0.6548      NA      NA
> >>>>> te(precip_daily_total,lag)  27.0000   0.0046      NA      NA
> >>>>> ```
> >>>>> Output from ```concurvity(conc20a, full=FALSE)$worst```:
> >>>>>
> >>>>> ```r
> >>>>>                                       para      s(time) te(temp_max,lag)
> >>>>> te(precip_daily_total,lag)
> >>>>> para                       1.000000e+00 2.462064e-19        0.3165236
> >>>>>                    0.6666348
> >>>>> s(time)                    2.462398e-19 1.000000e+00        0.9930674
> >>>>>                    0.6879284
> >>>>> te(temp_max,lag)           3.170844e-01 9.356384e-01        1.0000000
> >>>>>                    0.5788711
> >>>>> te(precip_daily_total,lag) 6.666348e-01 6.879284e-01        0.5788381
> >>>>>                    1.0000000
> >>>>>
> >>>>> ```
> >>>>>
> >>>>> Some output from ```summary(conc20a)```:
> >>>>> ```r
> >>>>> Approximate significance of smooth terms:
> >>>>>                                     edf Ref.df  Chi.sq p-value
> >>>>> s(time)                    1.110e+02    111 419.375  <2e-16 ***
> >>>>> te(temp_max,lag)           6.548e-01     27   0.895   0.249
> >>>>> te(precip_daily_total,lag) 4.598e-03     27   0.002   0.868
> >>>>> ---
> >>>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>>>>
> >>>>> R-sq.(adj) =  0.843   Deviance explained = 56.1%
> >>>>> -REML = 8750.1  Scale est. = 1         n = 5107
> >>>>> ```
> >>>>>
> >>>>> ACF functions:
> >>>>>
> >>>>> [4]: https://i.stack.imgur.com/7nbXS.png
> >>>>> [5]: https://i.stack.imgur.com/pNnZU.png
> >>>>>
> >>>>> Data can be found on my [GitHub][6] site in the file
> >>>>> [data_cross_validated_post2.rds][7]. A csv version is also available.
> >>>>> This is my code:
> >>>>>
> >>>>> ```r
> >>>>> library(readr)
> >>>>> library(mgcv)
> >>>>>
> >>>>> df <- read_rds("data_crossvalidated_post2.rds")
> >>>>>
> >>>>> # Create matrices for lagged weather variables (6 day lags) based on
> >>>>> example by Simon Wood
> >>>>> # in his 2017 book ("Generalized additive models: an introduction with
> >>>>> R", p. 349) and
> >>>>> # gamair package documentation
> >>>>> (https://cran.r-project.org/web/packages/gamair/gamair.pdf, p. 54)
> >>>>>
> >>>>> lagard <- function(x,n.lag=7) {
> >>>>> n <- length(x); X <- matrix(NA,n,n.lag)
> >>>>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> >>>>> X
> >>>>> }
> >>>>>
> >>>>> dat <- list(lag=matrix(0:6,nrow(df),7,byrow=TRUE),
> >>>>> deaths=df$deaths_total,doy=df$doy, year = df$year, month = df$month,
> >>>>> weekday = df$weekday, week = df$week, monthday = df$monthday, time =
> >>>>> df$time, heap=df$heap, heap_bin = df$heap_bin, precip_hourly_dailysum
> >>>>> = df$precip_hourly_dailysum)
> >>>>> dat$temp_max <- lagard(df$temp_max)
> >>>>> dat$temp_min <- lagard(df$temp_min)
> >>>>> dat$temp_mean <- lagard(df$temp_mean)
> >>>>> dat$wbgt_max <- lagard(df$wbgt_max)
> >>>>> dat$wbgt_mean <- lagard(df$wbgt_mean)
> >>>>> dat$wbgt_min <- lagard(df$wbgt_min)
> >>>>> dat$temp_wb_nasa_max <- lagard(df$temp_wb_nasa_max)
> >>>>> dat$sh_mean <- lagard(df$sh_mean)
> >>>>> dat$solar_mean <- lagard(df$solar_mean)
> >>>>> dat$wind2m_mean <- lagard(df$wind2m_mean)
> >>>>> dat$sh_max <- lagard(df$sh_max)
> >>>>> dat$solar_max <- lagard(df$solar_max)
> >>>>> dat$wind2m_max <- lagard(df$wind2m_max)
> >>>>> dat$temp_wb_nasa_mean <- lagard(df$temp_wb_nasa_mean)
> >>>>> dat$precip_hourly_dailysum <- lagard(df$precip_hourly_dailysum)
> >>>>> dat$precip_hourly <- lagard(df$precip_hourly)
> >>>>> dat$precip_daily_total <- lagard( df$precip_daily_total)
> >>>>> dat$temp <- lagard(df$temp)
> >>>>> dat$sh <- lagard(df$sh)
> >>>>> dat$rh <- lagard(df$rh)
> >>>>> dat$solar <- lagard(df$solar)
> >>>>> dat$wind2m <- lagard(df$wind2m)
> >>>>>
> >>>>>
> >>>>> conc38b <- gam(deaths~te(year, month, week, weekday,
> >>>>> bs=c("cr","cc","cc","cc")) + heap +
> >>>>>                          te(temp_max, lag, k=c(10, 3)) +
> >>>>>                          te(precip_daily_total, lag, k=c(10, 3)),
> >>>>>                          data = dat, family = nb, method = 'REML', select = TRUE,
> >>>>>                          knots = list(month = c(0.5, 12.5), week = c(0.5,
> >>>>> 52.5), weekday = c(0, 6.5)))
> >>>>>
> >>>>> conc39 <- gam(deaths~te(year, month, monthday, bs=c("cr","cc","cr")) + heap +
> >>>>>                         te(temp_max, lag, k=c(10, 4)) +
> >>>>>                         te(precip_daily_total, lag, k=c(10, 4)),
> >>>>>                         data = dat, family = nb, method = 'REML', select = TRUE,
> >>>>>                         knots = list(month = c(0.5, 12.5)))
> >>>>>
> >>>>> conc20a <- gam(deaths~s(time, k=112, fx=TRUE) + heap +
> >>>>>                          te(temp_max, lag, k=c(10,3)) +
> >>>>>                          te(precip_daily_total, lag, k=c(10,3)),
> >>>>>                          data = dat, family = nb, method = 'REML', select = TRUE)
> >>>>>
> >>>>> ```
> >>>>> Thank you if you've read this far!! :-))
> >>>>>
> >>>>>      [1]: https://scholar.google.co.uk/scholar?output=instlink&q=info:PKdjq7ZwozEJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=17865929886710916120&oi=lle
> >>>>>      [2]: https://i.stack.imgur.com/FzKyM.png
> >>>>>      [3]: https://i.stack.imgur.com/fE3aL.png
> >>>>>      [4]: https://i.stack.imgur.com/7nbXS.png
> >>>>>      [5]: https://i.stack.imgur.com/pNnZU.png
> >>>>>      [6]: https://github.com/JadeShodan/heat-mortality
> >>>>>      [7]: https://github.com/JadeShodan/heat-mortality/blob/main/data_cross_validated_post2.rds
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>> --
> >>>> Simon Wood, School of Mathematics, University of Edinburgh,
> >>>> https://www.maths.ed.ac.uk/~swood34/
> >>>>
> >> --
> >> Simon Wood, School of Mathematics, University of Edinburgh,
> >> https://www.maths.ed.ac.uk/~swood34/
> >>
> --
> Simon Wood, School of Mathematics, University of Edinburgh,
> https://www.maths.ed.ac.uk/~swood34/
>


From b@m|th030465 @end|ng |rom gm@||@com  Fri Jun 10 20:20:42 2022
From: b@m|th030465 @end|ng |rom gm@||@com (Brian Smith)
Date: Fri, 10 Jun 2022 14:20:42 -0400
Subject: [R] igraph: layout help
Message-ID: <CAEQKoCEsjz565HPLaPG6CS17B-TwxaiX9mFP5wTP5XnHb0QxFw@mail.gmail.com>

Hi,

I was trying to make a network plot of this data:

============
library(igraph)
library(network)

df1 <- data.frame(from="A",to=c("B","C","D","E","F","G"),value=1)
df2 <- data.frame(from="K",to=c("L","M","N"),value=1)
df3 <- data.frame(from="A",to="K",value=3)
my.df <- rbind(df1,df2,df3)

my.graph <- graph_from_data_frame(my.df,directed = F)
plot(my.graph)
============

What I wanted was for nodes A to G to be very close together (touching each
other). Similarly, nodes K to N should be very close together. The
connecting edge (A to K) between these sets of points/vertices should be
the only edge visible. How should I go about doing this?

Also, how can I change the parameters (node label, node color, etc.) of the
graph?

Any help or link to documentation would be helpful. Or would

thanks!

	[[alternative HTML version deleted]]


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Sat Jun 11 12:45:24 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Sat, 11 Jun 2022 15:45:24 +0500
Subject: [R] A humble request regarding QAMG method in R
In-Reply-To: <CAMfKi3Kc9Effi+SCwGJ-vU1QnLqkVYLkD1HXHNVYNO4YKdBq6g@mail.gmail.com>
References: <CAMfKi3KtMG2cE6dNc6xYn--34XVNS-9tR7wr_h3KCuCeoP8VmA@mail.gmail.com>
 <F8FF0C1D-4ACF-4843-AB19-68D58103CEE5@comcast.net>
 <CAMfKi3Kc9Effi+SCwGJ-vU1QnLqkVYLkD1HXHNVYNO4YKdBq6g@mail.gmail.com>
Message-ID: <CAMfKi3JHi-1bczRhrHqN+T9-o-fPYFFJ4YTnzJbnsyaH4S=Nqg@mail.gmail.com>

Hi, Dear Respected Professors! I hope that you are doing well. Now all
files are in .txt files. Now kindly help me with the following:

I have the R-codes for the "Quantile Augmented Mean Group" method. The
relevant codes and data are attached herewith.
Note: The link to the reference paper is:
https://www.econ.cam.ac.uk/people-files/emeritus/mhp1/fp20/qmg40-rev21.pdf
My Issue:
When I run the given codes to estimate the results for my own data. I have
to face several errors. I humbly request the experts to help me to estimate
the results for my data.
Thank you so much for your precious time.

Regards

Muhammad Zubair Chishti
Ph.D. Student
School of Business,
Zhengzhou University, Henan, China.
My Google scholar link:
https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
My ResearchGate link: https://www.researchgate.net/profile/Muhammad-Chishti




On Sat, Jun 11, 2022 at 9:28 AM Muhammad Zubair Chishti <
mzchishti at eco.qau.edu.pk> wrote:

> Hi, Dear Respected Professors! I hope that you are doing well. Now all
> files are in .txt files. Now kindly help me with the following:
>
> I have the R-codes for the "Quantile Augmented Mean Group" method. The
> relevant codes and papers are attached herewith.
> My Issue:
> When I run the given codes to estimate the results for my own data. I have
> to face several errors. I humbly request the experts to help me to estimate
> the results for my data.
> Thank you so much for your precious time.
>
> Regards
>
> Muhammad Zubair Chishti
> Ph.D. Student
> School of Business,
> Zhengzhou University, Henan, China.
> My Google scholar link:
> https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
> My ResearchGate link:
> https://www.researchgate.net/profile/Muhammad-Chishti
>
>
> On Sat, Jun 11, 2022 at 7:06 AM David Winsemius <dwinsemius at comcast.net>
> wrote:
>
>> Only the pdf attachment came through. If you want your mail-client to
>> label your code or data attachments they need to have a .txt file extension
>> or type.
>>
>> --
>> David
>>
>> > On Jun 7, 2022, at 9:44 PM, Muhammad Zubair Chishti <
>> mzchishti at eco.qau.edu.pk> wrote:
>> >
>> > Hi, Dear Respected Professors! I hope that you are doing well.
>> > Kindly help me with the following:
>> > I have the R-codes for the "Quantile Augmented Mean Group" method. The
>> > relevant codes and papers are attached herewith.
>> > My Issue:
>> > When I run the given codes to estimate the results for my own data. I
>> have
>> > to face several errors. I humbly request the experts to help me to
>> estimate
>> > the results for my data.
>> > Thank you so much for your precious time.
>> >
>> > Regards
>> >
>> > Muhammad Zubair Chishti
>> > Ph.D. Student
>> > School of Business,
>> > Zhengzhou University, Henan, China.
>> > My Google scholar link:
>> > https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
>> > My ResearchGate link:
>> https://www.researchgate.net/profile/Muhammad-Chishti
>> > <Journal of Applied Econometrics Volume issue 2020 [doi
>> 10.1002_jae.2753] Harding, Matthew; Lamarche, Carlos; Pesaran, M. Hashem --
>> Common Correlated Effects Estimation of Heterogeneous Dynamic
>> Panel_2.pdf>______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>>

From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Sat Jun 11 12:54:08 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Sat, 11 Jun 2022 15:54:08 +0500
Subject: [R] A humble request regarding QAMG method in R
Message-ID: <CAMfKi3Lqzk36et=32fKK36dfjZAC_y5oyTZqBKbUgxXnTuZDQA@mail.gmail.com>

Hi, Dear Respected Professors! I hope that you are doing well. Now all
files are in .txt files. Now kindly help me with the following:

I have the R-codes for the "Quantile Augmented Mean Group" method. The
relevant codes and data are attached herewith.
Note: The link to the reference paper is:
https://www.econ.cam.ac.uk/people-files/emeritus/mhp1/fp20/qmg40-rev21.pdf
My Issue:
When I run the given codes to estimate the results for my own data. I have
to face several errors. I humbly request the experts to help me to estimate
the results for my data.
Thank you so much for your precious time.

Regards

Muhammad Zubair Chishti
Ph.D. Student
School of Business,
Zhengzhou University, Henan, China.
My Google scholar link:
https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
My ResearchGate link: https://www.researchgate.net/profile/Muhammad-Chishti

From ||@t@ @end|ng |rom dewey@myzen@co@uk  Sat Jun 11 14:36:15 2022
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Sat, 11 Jun 2022 13:36:15 +0100
Subject: [R] A humble request regarding QAMG method in R
In-Reply-To: <CAMfKi3Lqzk36et=32fKK36dfjZAC_y5oyTZqBKbUgxXnTuZDQA@mail.gmail.com>
References: <CAMfKi3Lqzk36et=32fKK36dfjZAC_y5oyTZqBKbUgxXnTuZDQA@mail.gmail.com>
Message-ID: <7cd79bc1-a77e-c869-3c60-9e4429489fec@dewey.myzen.co.uk>

Did you forget to attach them? At any rate they did not arrive.

Michael

On 11/06/2022 11:54, Muhammad Zubair Chishti wrote:
> Hi, Dear Respected Professors! I hope that you are doing well. Now all
> files are in .txt files. Now kindly help me with the following:
> 
> I have the R-codes for the "Quantile Augmented Mean Group" method. The
> relevant codes and data are attached herewith.
> Note: The link to the reference paper is:
> https://www.econ.cam.ac.uk/people-files/emeritus/mhp1/fp20/qmg40-rev21.pdf
> My Issue:
> When I run the given codes to estimate the results for my own data. I have
> to face several errors. I humbly request the experts to help me to estimate
> the results for my data.
> Thank you so much for your precious time.
> 
> Regards
> 
> Muhammad Zubair Chishti
> Ph.D. Student
> School of Business,
> Zhengzhou University, Henan, China.
> My Google scholar link:
> https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
> My ResearchGate link: https://www.researchgate.net/profile/Muhammad-Chishti
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat Jun 11 15:56:54 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 11 Jun 2022 14:56:54 +0100
Subject: [R] igraph: layout help
In-Reply-To: <CAEQKoCEsjz565HPLaPG6CS17B-TwxaiX9mFP5wTP5XnHb0QxFw@mail.gmail.com>
References: <CAEQKoCEsjz565HPLaPG6CS17B-TwxaiX9mFP5wTP5XnHb0QxFw@mail.gmail.com>
Message-ID: <b0ed93e6-f6c0-fb0c-aa3a-cfd79c4ef314@sapo.pt>

Hello,

The code below is a hack.
First I create a layout ll, then see that vertex K coordinates are in 
the layout matrix 2nd row.

Now, to multiply K's coordinates by a number d>1 will move the point 
away from A. Add that value to the K group and plot.


ll <- layout_with_kk(my.graph)
d <- ll[2, ]*5
ll[c(2, 9:11), ] <- t(t(ll[c(2, 9:11), ]) + d)

plot(my.graph, layout = ll)


Hope this helps,

Rui Barradas

?s 19:20 de 10/06/2022, Brian Smith escreveu:
> Hi,
> 
> I was trying to make a network plot of this data:
> 
> ============
> library(igraph)
> library(network)
> 
> df1 <- data.frame(from="A",to=c("B","C","D","E","F","G"),value=1)
> df2 <- data.frame(from="K",to=c("L","M","N"),value=1)
> df3 <- data.frame(from="A",to="K",value=3)
> my.df <- rbind(df1,df2,df3)
> 
> my.graph <- graph_from_data_frame(my.df,directed = F)
> plot(my.graph)
> ============
> 
> What I wanted was for nodes A to G to be very close together (touching each
> other). Similarly, nodes K to N should be very close together. The
> connecting edge (A to K) between these sets of points/vertices should be
> the only edge visible. How should I go about doing this?
> 
> Also, how can I change the parameters (node label, node color, etc.) of the
> graph?
> 
> Any help or link to documentation would be helpful. Or would
> 
> thanks!
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From |_j_rod @end|ng |rom hotm@||@com  Sat Jun 11 17:14:11 2022
From: |_j_rod @end|ng |rom hotm@||@com (Frank S.)
Date: Sat, 11 Jun 2022 15:14:11 +0000
Subject: [R] Interpreting the result of a model with random effects
Message-ID: <AM7P191MB11223ACA72D5A4A90D4CD25FBAA99@AM7P191MB1122.EURP191.PROD.OUTLOOK.COM>

Dear R users,

 I'm analyzing a particular score "y" among several individuals, each of which belongs to a center, a factor with three
different levels (3 possible centers). I have treated the "center" as a fixed effect, and as a random term (package lme4):

1) model.fix <- glm(y ~ var.1 + var.2 + var.3 + var.4 + var.5 + center, family = "binomial", data = dat)
2) model.rand <- glmer(y ~ var.1 + var.2 + var.3 + var.4 + var.5 + (1 | center), family = "binomial", data = dat)

The issue is that both models provide exactly the same coefficients and p-values for the 5 baseline variables, so I assumed
that it was due to the small number of levels (in fact,  too few ). However, when computing anova(model.rand, model.fix),
the output indicates a p-value < 0.001 in favour of the "model.rand". What's happening? Should I take the random terms?

Thanks for any help!

Frank S.

	[[alternative HTML version deleted]]


From mm@|ten @end|ng |rom gm@||@com  Sat Jun 11 17:21:38 2022
From: mm@|ten @end|ng |rom gm@||@com (Mitchell Maltenfort)
Date: Sat, 11 Jun 2022 11:21:38 -0400
Subject: [R] Interpreting the result of a model with random effects
In-Reply-To: <AM7P191MB11223ACA72D5A4A90D4CD25FBAA99@AM7P191MB1122.EURP191.PROD.OUTLOOK.COM>
References: <AM7P191MB11223ACA72D5A4A90D4CD25FBAA99@AM7P191MB1122.EURP191.PROD.OUTLOOK.COM>
Message-ID: <CANOgrHY0Z4PQ7JTDFJGdpE8d6rcmqV=GYmw8sMiwQPxFGEiaeA@mail.gmail.com>

Looks like the center effect improves overall accuracy while being
independent of the other terms.

A few things to try

Compare coef(model.fix) to fixef(model.rand).

Add center as a fixed effect to model .fix

Try a conditional logit (clogit from survival)

See how consistent the coefficients are





On Sat, Jun 11, 2022 at 11:14 AM Frank S. <f_j_rod at hotmail.com> wrote:

> Dear R users,
>
>  I'm analyzing a particular score "y" among several individuals, each of
> which belongs to a center, a factor with three
> different levels (3 possible centers). I have treated the "center" as a
> fixed effect, and as a random term (package lme4):
>
> 1) model.fix <- glm(y ~ var.1 + var.2 + var.3 + var.4 + var.5 + center,
> family = "binomial", data = dat)
> 2) model.rand <- glmer(y ~ var.1 + var.2 + var.3 + var.4 + var.5 + (1 |
> center), family = "binomial", data = dat)
>
> The issue is that both models provide exactly the same coefficients and
> p-values for the 5 baseline variables, so I assumed
> that it was due to the small number of levels (in fact,  too few ).
> However, when computing anova(model.rand, model.fix),
> the output indicates a p-value < 0.001 in favour of the "model.rand".
> What's happening? Should I take the random terms?
>
> Thanks for any help!
>
> Frank S.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
-- 
Sent from Gmail Mobile

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sat Jun 11 18:57:45 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 11 Jun 2022 09:57:45 -0700
Subject: [R] Interpreting the result of a model with random effects
In-Reply-To: <AM7P191MB11223ACA72D5A4A90D4CD25FBAA99@AM7P191MB1122.EURP191.PROD.OUTLOOK.COM>
References: <AM7P191MB11223ACA72D5A4A90D4CD25FBAA99@AM7P191MB1122.EURP191.PROD.OUTLOOK.COM>
Message-ID: <CAGxFJbSvky6aHzrZ1FbVdwqiGWSAAfQNOijKQTKopvPd4Veuwg@mail.gmail.com>

Please note: I am not an expert.

1. If there are only 3 centers (you didn't say) , then they are not a
random selection from a larger collection of centers and a random
effects model is inappropriate anyway;

2. Otherwise, you want to estimate a centers variance component from a
sample of size 3 ??

3. You cannot -- or at least should not -- compare nested fixed
effects models (with and without 'center') with different random
effects structures. The number of df associated with random effects is
unknown, and standard (asymptotic) likelihood ratio tests are wrong.
There's a big literature on this.

So my answer is no -- the anova p-value comparison is nonsense.

Again, note my initial caveat -- perhaps it will serve as an
invitation for an expert to respond.


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Sat, Jun 11, 2022 at 8:14 AM Frank S. <f_j_rod at hotmail.com> wrote:
>
> Dear R users,
>
>  I'm analyzing a particular score "y" among several individuals, each of which belongs to a center, a factor with three
> different levels (3 possible centers). I have treated the "center" as a fixed effect, and as a random term (package lme4):
>
> 1) model.fix <- glm(y ~ var.1 + var.2 + var.3 + var.4 + var.5 + center, family = "binomial", data = dat)
> 2) model.rand <- glmer(y ~ var.1 + var.2 + var.3 + var.4 + var.5 + (1 | center), family = "binomial", data = dat)
>
> The issue is that both models provide exactly the same coefficients and p-values for the 5 baseline variables, so I assumed
> that it was due to the small number of levels (in fact,  too few ). However, when computing anova(model.rand, model.fix),
> the output indicates a p-value < 0.001 in favour of the "model.rand". What's happening? Should I take the random terms?
>
> Thanks for any help!
>
> Frank S.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @tt|tude@h@nt@nu1977 @end|ng |rom gm@||@com  Sat Jun 11 16:24:25 2022
From: @tt|tude@h@nt@nu1977 @end|ng |rom gm@||@com (Shantanu Shimpi)
Date: Sat, 11 Jun 2022 19:54:25 +0530
Subject: [R] How to do non-parametric calculations in R
Message-ID: <CAN5VMj4mGAdy7Bg513oDmM-9G+Jk_opm4LZtxPQmWmBEQ_9YfQ@mail.gmail.com>

Dear R community,

Please help me in knowing how to do following non-parametric tests:

   1.  kruskal-Wallis test
   2.  Wilcoxson rank sum test
   3.  Lee Cronbac Alpha test
   4.  Spearman's Rank correlation test
   5.  Henry Garrett method formula calculations
   6.  Factor Analysis
   7.  Chi square test

Kindly guide me on the above queries in the easiest way.

A R lover,
Col Shantanu.
India.
attitudeshantanu1977 at gmail.com

7722030088

	[[alternative HTML version deleted]]


From pro|jcn@@h @end|ng |rom gm@||@com  Sat Jun 11 22:53:37 2022
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Sat, 11 Jun 2022 16:53:37 -0400
Subject: [R] How to do non-parametric calculations in R
In-Reply-To: <CAN5VMj4mGAdy7Bg513oDmM-9G+Jk_opm4LZtxPQmWmBEQ_9YfQ@mail.gmail.com>
References: <CAN5VMj4mGAdy7Bg513oDmM-9G+Jk_opm4LZtxPQmWmBEQ_9YfQ@mail.gmail.com>
Message-ID: <a35b3702-f529-b001-6cc4-cea115a14cc6@gmail.com>

Homework!

On 2022-06-11 10:24, Shantanu Shimpi wrote:
> Dear R community,
> 
> Please help me in knowing how to do following non-parametric tests:
> 
>     1.  kruskal-Wallis test
>     2.  Wilcoxson rank sum test
>     3.  Lee Cronbac Alpha test
>     4.  Spearman's Rank correlation test
>     5.  Henry Garrett method formula calculations
>     6.  Factor Analysis
>     7.  Chi square test
> 
> Kindly guide me on the above queries in the easiest way.
> 
> A R lover,
> Col Shantanu.
> India.
> attitudeshantanu1977 at gmail.com
> 
> 7722030088
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat Jun 11 23:26:31 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 11 Jun 2022 14:26:31 -0700
Subject: [R] How to do non-parametric calculations in R
In-Reply-To: <a35b3702-f529-b001-6cc4-cea115a14cc6@gmail.com>
References: <CAN5VMj4mGAdy7Bg513oDmM-9G+Jk_opm4LZtxPQmWmBEQ_9YfQ@mail.gmail.com>
 <a35b3702-f529-b001-6cc4-cea115a14cc6@gmail.com>
Message-ID: <1D74B65E-079B-41E1-B0BC-3597D62FC98B@dcn.davis.ca.us>

Really? But it is such a random list that I thought it was a test of our ability to resist providing impromptu lectures on off-list-topics, since we all like to expound on "stuff" even when R isn't needed to understand them. Or perhaps "A R Lover" just didn't read the Posting Guide warnings about HTML email, homework, and statistics, and will soon have done do and be sharing some R code that is giving them an error.

On June 11, 2022 1:53:37 PM PDT, J C Nash <profjcnash at gmail.com> wrote:
>Homework!
>
>On 2022-06-11 10:24, Shantanu Shimpi wrote:
>> Dear R community,
>> 
>> Please help me in knowing how to do following non-parametric tests:
>> 
>>     1.  kruskal-Wallis test
>>     2.  Wilcoxson rank sum test
>>     3.  Lee Cronbac Alpha test
>>     4.  Spearman's Rank correlation test
>>     5.  Henry Garrett method formula calculations
>>     6.  Factor Analysis
>>     7.  Chi square test
>> 
>> Kindly guide me on the above queries in the easiest way.
>> 
>> A R lover,
>> Col Shantanu.
>> India.
>> attitudeshantanu1977 at gmail.com
>> 
>> 7722030088
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Sat Jun 11 14:42:33 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Sat, 11 Jun 2022 17:42:33 +0500
Subject: [R] A humble request regarding QAMG method in R
Message-ID: <CAMfKi3Kd9oP2nWe6boV4rNjVHAZ4eV=vwpnCAzh=6Gw-ZWrdbg@mail.gmail.com>

Hi, Dear Respected Professors! I hope that you are doing well. Now all
files are in .txt files. Now kindly help me with the following:

I have the R-codes for the "Quantile Augmented Mean Group" method. The
relevant codes and data are attached herewith.
Note: The link to the reference paper is:
https://www.econ.cam.ac.uk/people-files/emeritus/mhp1/fp20/qmg40-rev21.pdf
My Issue:
When I run the given codes to estimate the results for my own data. I have
to face several errors. I humbly request the experts to help me to estimate
the results for my data.
Thank you so much for your precious time.

Regards

Muhammad Zubair Chishti
Ph.D. Student
School of Business,
Zhengzhou University, Henan, China.
My Google scholar link:
https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
My ResearchGate link: https://www.researchgate.net/profile/Muhammad-Chishti

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: rq.fit.panel.all.revised.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220611/3e3486d0/attachment.txt>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: data.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220611/3e3486d0/attachment-0001.txt>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Table.3.1.and.Table.3.2.design1.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220611/3e3486d0/attachment-0002.txt>

From h@n@tezer@ @end|ng |rom gm@||@com  Sat Jun 11 23:36:21 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (hanatezera)
Date: Sun, 12 Jun 2022 00:36:21 +0300
Subject: [R] Changing sign of columns and values
Message-ID: <62a50adb.1c69fb81.f6535.329d@mx.google.com>

I have the following data set in data frameIDnumber? OA? EA? beta1? ? ? ? ? ? ? ? ? ? ? ? ?C? ? ? ?A? ? ? ?-0.052? ? ? ? ? ? ? ? ? ? ? ? ?G? ? ? ? A? ? ? ? 0.0983? ? ? ? ? ? ? ? ? ? ? ? ? G? ? ? ? T? ? ? ? -0.789....I want to change the sign of negative beta. If the negative value change to postive i want to switch its EA and OA.My desire result will beIDnumber? OA? EA? beta1? ? ? ? ? ? ? ? ? ? ? ? ?A? ? ? C? ? ?0.052? ? ? ? ? ? ? ? ? ? ? ? ?G? ? ? ? A? ? ? ? 0.0983? ? ? ? ? ? ? ? ? ? ? ? ? T? ? ? ? ?G? ? ? ? 0.789....Any one can help me with r codes??kind regards,Hana
	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun Jun 12 00:37:26 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 11 Jun 2022 15:37:26 -0700
Subject: [R] Changing sign of columns and values
In-Reply-To: <62a50adb.1c69fb81.f6535.329d@mx.google.com>
References: <62a50adb.1c69fb81.f6535.329d@mx.google.com>
Message-ID: <CAGxFJbTdmPcBx0tUntijFjQS-8D-srA73eMbG0GOr0EcPwVh7Q@mail.gmail.com>

This is a plain text list. Your html post got mangled (see below). You
are more likely to get a useful response if you follow the posting
guide (linked below) and post in plain text.

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Sat, Jun 11, 2022 at 3:27 PM hanatezera <hanatezera at gmail.com> wrote:
>
> I have the following data set in data frameIDnumber  OA  EA  beta1                         C       A       -0.052                         G        A        0.0983                          G        T        -0.789....I want to change the sign of negative beta. If the negative value change to postive i want to switch its EA and OA.My desire result will beIDnumber  OA  EA  beta1                         A      C     0.052                         G        A        0.0983                          T         G        0.789....Any one can help me with r codes? kind regards,Hana
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Sun Jun 12 00:59:27 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sun, 12 Jun 2022 08:59:27 +1000
Subject: [R] Changing sign of columns and values
In-Reply-To: <62a50adb.1c69fb81.f6535.329d@mx.google.com>
References: <62a50adb.1c69fb81.f6535.329d@mx.google.com>
Message-ID: <CA+8X3fX6_0ZD1-ictjoH2SSWg30B3MC3c8OKyNsPAA6+Ui+hyA@mail.gmail.com>

Hi Hana,
I think this is what you want:

# first read in your example
mydf<-read.table(text=
"IDnumber  OA  EA  beta
1   C       A       -0.05
2   G        A        0.098
3   G        T        -0.789",
header=TRUE,stringsAsFactors=FALSE)
# check it
mydf
 IDnumber OA EA   beta
1        1  C  A -0.050
2        2  G  A  0.098
3        3  G  T -0.789
# change values of mydf$beta to absolute values
mydf$beta<-abs(mydf$beta)
mydf
 IDnumber OA EA  beta
1        1  C  A 0.050
2        2  G  A 0.098
3        3  G  T 0.789

Jim

On Sun, Jun 12, 2022 at 8:27 AM hanatezera <hanatezera at gmail.com> wrote:
>
> I have the following data set in data frameIDnumber  OA  EA  beta1                         C       A       -0.052                         G        A        0.0983                          G        T        -0.789....I want to change the sign of negative beta. If the negative value change to postive i want to switch its EA and OA.My desire result will beIDnumber  OA  EA  beta1                         A      C     0.052                         G        A        0.0983                          T         G        0.789....Any one can help me with r codes? kind regards,Hana
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From h@n@tezer@ @end|ng |rom gm@||@com  Sun Jun 12 01:11:15 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (hanatezera)
Date: Sun, 12 Jun 2022 02:11:15 +0300
Subject: [R] Changing sign of columns and values
In-Reply-To: <CA+8X3fX6_0ZD1-ictjoH2SSWg30B3MC3c8OKyNsPAA6+Ui+hyA@mail.gmail.com>
Message-ID: <62a52119.1c69fb81.e10ce.36e6@mx.google.com>

Dear jim thanks for your help! I want to change also the value of OA and EF simultaneously.For instance i am looking the datamydf IDnumber OA EA? beta1??????? 1? A? C? 0.0502??????? 2? G? A 0.0983??????? 3? T? ?G 0.789Best,?Hana? ? ? ? ? ? ? ? ?
-------- Original message --------From: Jim Lemon <drjimlemon at gmail.com> Date: 6/12/22  1:59 AM  (GMT+03:00) To: hanatezera <hanatezera at gmail.com>, r-help mailing list <r-help at r-project.org> Subject: Re: [R] Changing sign of columns and values Hi Hana,I think this is what you want:# first read in your examplemydf<-read.table(text="IDnumber? OA? EA? beta1?? C?????? A?????? -0.052?? G??????? A??????? 0.0983?? G??????? T??????? -0.789",header=TRUE,stringsAsFactors=FALSE)# check itmydf IDnumber OA EA?? beta1??????? 1? C? A -0.0502??????? 2? G? A? 0.0983??????? 3? G? T -0.789# change values of mydf$beta to absolute valuesmydf$beta<-abs(mydf$beta)mydf IDnumber OA EA? beta1??????? 1? C? A 0.0502??????? 2? G? A 0.0983??????? 3? G? T 0.789JimOn Sun, Jun 12, 2022 at 8:27 AM hanatezera <hanatezera at gmail.com> wrote:>> I have the following data set in data frameIDnumber? OA? EA? beta1???????????????????????? C?????? A?????? -0.052???????????????????????? G??????? A??????? 0.0983????????????????????????? G??????? T??????? -0.789....I want to change the sign of negative beta. If the negative value change to postive i want to switch its EA and OA.My desire result will beIDnumber? OA? EA? beta1???????????????????????? A????? C???? 0.052???????????????????????? G??????? A??????? 0.0983????????????????????????? T???????? G??????? 0.789....Any one can help me with r codes? kind regards,Hana>???????? [[alternative HTML version deleted]]>> ______________________________________________> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see> https://stat.ethz.ch/mailman/listinfo/r-help> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html> and provide commented, minimal, self-contained, reproducible code.
	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Sun Jun 12 01:19:43 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Sat, 11 Jun 2022 23:19:43 +0000
Subject: [R] How to do non-parametric calculations in R
In-Reply-To: <1D74B65E-079B-41E1-B0BC-3597D62FC98B@dcn.davis.ca.us>
References: <CAN5VMj4mGAdy7Bg513oDmM-9G+Jk_opm4LZtxPQmWmBEQ_9YfQ@mail.gmail.com>
 <a35b3702-f529-b001-6cc4-cea115a14cc6@gmail.com>
 <1D74B65E-079B-41E1-B0BC-3597D62FC98B@dcn.davis.ca.us>
Message-ID: <BN6PR2201MB1553B210DFA6242A423899D8CFA99@BN6PR2201MB1553.namprd22.prod.outlook.com>

LOL. Thank goodness I successfully rolled my saving throw and resisted, at least for this round.
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Jeff Newmiller
Sent: Saturday, June 11, 2022 5:27 PM
To: r-help at r-project.org; J C Nash <profjcnash at gmail.com>
Subject: Re: [R] How to do non-parametric calculations in R

[External Email]

Really? But it is such a random list that I thought it was a test of our ability to resist providing impromptu lectures on off-list-topics, since we all like to expound on "stuff" even when R isn't needed to understand them. Or perhaps "A R Lover" just didn't read the Posting Guide warnings about HTML email, homework, and statistics, and will soon have done do and be sharing some R code that is giving them an error.

On June 11, 2022 1:53:37 PM PDT, J C Nash <profjcnash at gmail.com> wrote:
>Homework!
>
>On 2022-06-11 10:24, Shantanu Shimpi wrote:
>> Dear R community,
>>
>> Please help me in knowing how to do following non-parametric tests:
>>
>>     1.  kruskal-Wallis test
>>     2.  Wilcoxson rank sum test
>>     3.  Lee Cronbac Alpha test
>>     4.  Spearman's Rank correlation test
>>     5.  Henry Garrett method formula calculations
>>     6.  Factor Analysis
>>     7.  Chi square test
>>
>> Kindly guide me on the above queries in the easiest way.
>>
>> A R lover,
>> Col Shantanu.
>> India.
>> attitudeshantanu1977 at gmail.com
>>
>> 7722030088
>>
>>      [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
>> lman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVe
>> AsRzsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5Yb
>> F8c2Lixwy8&s=zhlVVKEML3MeOEdjlR2Z1gqYLVcrE0gpEiFPdo0MxNg&e=
>> PLEASE do read the posting guide 
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
>> rg_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kV
>> eAsRzsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5Y
>> bF8c2Lixwy8&s=IRGFPuDAXZu6xEr36GrRy5jeXkI0D62fDLt-FxbIqBs&e=
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailm
>an_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRz
>sn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5YbF8c2Li
>xwy8&s=zhlVVKEML3MeOEdjlR2Z1gqYLVcrE0gpEiFPdo0MxNg&e=
>PLEASE do read the posting guide 
>https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org
>_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsR
>zsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5YbF8c2L
>ixwy8&s=IRGFPuDAXZu6xEr36GrRy5jeXkI0D62fDLt-FxbIqBs&e=
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5YbF8c2Lixwy8&s=zhlVVKEML3MeOEdjlR2Z1gqYLVcrE0gpEiFPdo0MxNg&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5YbF8c2Lixwy8&s=IRGFPuDAXZu6xEr36GrRy5jeXkI0D62fDLt-FxbIqBs&e=
and provide commented, minimal, self-contained, reproducible code.


From twoo|m@n @end|ng |rom ont@rgettek@com  Sun Jun 12 01:48:57 2022
From: twoo|m@n @end|ng |rom ont@rgettek@com (Tom Woolman)
Date: Sat, 11 Jun 2022 19:48:57 -0400
Subject: [R] How to do non-parametric calculations in R
In-Reply-To: <BN6PR2201MB1553B210DFA6242A423899D8CFA99@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAN5VMj4mGAdy7Bg513oDmM-9G+Jk_opm4LZtxPQmWmBEQ_9YfQ@mail.gmail.com>
 <a35b3702-f529-b001-6cc4-cea115a14cc6@gmail.com>
 <1D74B65E-079B-41E1-B0BC-3597D62FC98B@dcn.davis.ca.us>
 <BN6PR2201MB1553B210DFA6242A423899D8CFA99@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <25f7067356d5b2f33675edd0ffa741a9@ontargettek.com>

Imagine that it's the year 2022 and you don't know how to look up 
information about performing a Kruskal-Wallis H test.

  It would take you longer to join the listserv and then write such a 
cokamemie email than to open the stats textbook you are supposed to have 
for the course, much less doing a simple search query.


On 2022-06-11 19:19, Ebert,Timothy Aaron wrote:
> LOL. Thank goodness I successfully rolled my saving throw and
> resisted, at least for this round.
> Tim
> 
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Jeff Newmiller
> Sent: Saturday, June 11, 2022 5:27 PM
> To: r-help at r-project.org; J C Nash <profjcnash at gmail.com>
> Subject: Re: [R] How to do non-parametric calculations in R
> 
> [External Email]
> 
> Really? But it is such a random list that I thought it was a test of
> our ability to resist providing impromptu lectures on off-list-topics,
> since we all like to expound on "stuff" even when R isn't needed to
> understand them. Or perhaps "A R Lover" just didn't read the Posting
> Guide warnings about HTML email, homework, and statistics, and will
> soon have done do and be sharing some R code that is giving them an
> error.
> 
> On June 11, 2022 1:53:37 PM PDT, J C Nash <profjcnash at gmail.com> wrote:
>> Homework!
>> 
>> On 2022-06-11 10:24, Shantanu Shimpi wrote:
>>> Dear R community,
>>> 
>>> Please help me in knowing how to do following non-parametric tests:
>>> 
>>>     1.  kruskal-Wallis test
>>>     2.  Wilcoxson rank sum test
>>>     3.  Lee Cronbac Alpha test
>>>     4.  Spearman's Rank correlation test
>>>     5.  Henry Garrett method formula calculations
>>>     6.  Factor Analysis
>>>     7.  Chi square test
>>> 
>>> Kindly guide me on the above queries in the easiest way.
>>> 
>>> A R lover,
>>> Col Shantanu.
>>> India.
>>> attitudeshantanu1977 at gmail.com
>>> 
>>> 7722030088
>>> 
>>>      [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
>>> lman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVe
>>> AsRzsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5Yb
>>> F8c2Lixwy8&s=zhlVVKEML3MeOEdjlR2Z1gqYLVcrE0gpEiFPdo0MxNg&e=
>>> PLEASE do read the posting guide
>>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
>>> rg_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kV
>>> eAsRzsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5Y
>>> bF8c2Lixwy8&s=IRGFPuDAXZu6xEr36GrRy5jeXkI0D62fDLt-FxbIqBs&e=
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailm
>> an_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRz
>> sn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5YbF8c2Li
>> xwy8&s=zhlVVKEML3MeOEdjlR2Z1gqYLVcrE0gpEiFPdo0MxNg&e=
>> PLEASE do read the posting guide
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org
>> _posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsR
>> zsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5YbF8c2L
>> ixwy8&s=IRGFPuDAXZu6xEr36GrRy5jeXkI0D62fDLt-FxbIqBs&e=
>> and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Sent from my phone. Please excuse my brevity.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5YbF8c2Lixwy8&s=zhlVVKEML3MeOEdjlR2Z1gqYLVcrE0gpEiFPdo0MxNg&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=4tM4fJqtbe_uTsyPRnMDpR560AlVh83t9wmvJuUPvfsciEQhPSY5YbF8c2Lixwy8&s=IRGFPuDAXZu6xEr36GrRy5jeXkI0D62fDLt-FxbIqBs&e=
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Sun Jun 12 05:42:52 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sun, 12 Jun 2022 13:42:52 +1000
Subject: [R] Changing sign of columns and values
In-Reply-To: <62a52119.1c69fb81.e10ce.36e6@mx.google.com>
References: <CA+8X3fX6_0ZD1-ictjoH2SSWg30B3MC3c8OKyNsPAA6+Ui+hyA@mail.gmail.com>
 <62a52119.1c69fb81.e10ce.36e6@mx.google.com>
Message-ID: <CA+8X3fUB-vzBi0Jfr66tDUXQT4tctENg4=4sBvWxsiKBaEBUvg@mail.gmail.com>

Hi Hana,
I didn't look closely. The simplest rule that I can see is that the
letters (nucleotides?) should be swapped if there has been a sign
change in the beta value. Therefore:

mydf<-read.table(text=
"IDnumber  OA  EA  beta
1   C       A       -0.05
2   G        A        0.098
3   G        T        -0.789",
header=TRUE,stringsAsFactors=FALSE)
# logical vector marking the rows to be swapped
swap_letters<-mydf$beta < 0
# save the OA values to be swapped
newEA<-mydf$OA[swap_letters]
# change the OAs to EAs
mydf$OA[swap_letters]<-mydf$EA[swap_letters]
# set the relevant EA values to the old OA values
mydf$EA[swap_letters]<-newEA
# change the beta values
mydf$beta<-abs(mydf$beta)
mydf

Jim



On Sun, Jun 12, 2022 at 9:11 AM hanatezera <hanatezera at gmail.com> wrote:
>
> Dear jim thanks for your help! I want to change also the value of OA and EF simultaneously.
> For instance i am looking the data
> mydf
> IDnumber OA EA  beta
> 1        1  A  C  0.050
> 2        2  G  A 0.098
> 3        3  T   G 0.789
>
> Best,
> Hana
>
>
>
> -------- Original message --------
> From: Jim Lemon <drjimlemon at gmail.com>
> Date: 6/12/22 1:59 AM (GMT+03:00)
> To: hanatezera <hanatezera at gmail.com>, r-help mailing list <r-help at r-project.org>
> Subject: Re: [R] Changing sign of columns and values
>
> Hi Hana,
> I think this is what you want:
>
> # first read in your example
> mydf<-read.table(text=
> "IDnumber  OA  EA  beta
> 1   C       A       -0.05
> 2   G        A        0.098
> 3   G        T        -0.789",
> header=TRUE,stringsAsFactors=FALSE)
> # check it
> mydf
> IDnumber OA EA   beta
> 1        1  C  A -0.050
> 2        2  G  A  0.098
> 3        3  G  T -0.789
> # change values of mydf$beta to absolute values
> mydf$beta<-abs(mydf$beta)
> mydf
> IDnumber OA EA  beta
> 1        1  C  A 0.050
> 2        2  G  A 0.098
> 3        3  G  T 0.789
>
> Jim
>
> On Sun, Jun 12, 2022 at 8:27 AM hanatezera <hanatezera at gmail.com> wrote:
> >
> > I have the following data set in data frameIDnumber  OA  EA  beta1                         C       A       -0.052                         G        A        0.0983                          G        T        -0.789....I want to change the sign of negative beta. If the negative value change to postive i want to switch its EA and OA.My desire result will beIDnumber  OA  EA  beta1                         A      C     0.052                         G        A        0.0983                          T         G        0.789....Any one can help me with r codes? kind regards,Hana
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From h@n@tezer@ @end|ng |rom gm@||@com  Sun Jun 12 13:14:34 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (hanatezera)
Date: Sun, 12 Jun 2022 14:14:34 +0300
Subject: [R] Changing sign of columns and values
In-Reply-To: <CA+8X3fUB-vzBi0Jfr66tDUXQT4tctENg4=4sBvWxsiKBaEBUvg@mail.gmail.com>
Message-ID: <62a5ca9c.1c69fb81.e10ce.4c1d@mx.google.com>

Dear Jim, Thanks a lot this is exactly i am looking for.Stay safe and blessed !Best,Hana
-------- Original message --------From: Jim Lemon <drjimlemon at gmail.com> Date: 6/12/22  6:43 AM  (GMT+03:00) To: hanatezera <hanatezera at gmail.com> Cc: r-help mailing list <r-help at r-project.org> Subject: Re: [R] Changing sign of columns and values Hi Hana,I didn't look closely. The simplest rule that I can see is that theletters (nucleotides?) should be swapped if there has been a signchange in the beta value. Therefore:mydf<-read.table(text="IDnumber? OA? EA? beta1?? C?????? A?????? -0.052?? G??????? A??????? 0.0983?? G??????? T??????? -0.789",header=TRUE,stringsAsFactors=FALSE)# logical vector marking the rows to be swappedswap_letters<-mydf$beta < 0# save the OA values to be swappednewEA<-mydf$OA[swap_letters]# change the OAs to EAsmydf$OA[swap_letters]<-mydf$EA[swap_letters]# set the relevant EA values to the old OA valuesmydf$EA[swap_letters]<-newEA# change the beta valuesmydf$beta<-abs(mydf$beta)mydfJimOn Sun, Jun 12, 2022 at 9:11 AM hanatezera <hanatezera at gmail.com> wrote:>> Dear jim thanks for your help! I want to change also the value of OA and EF simultaneously.> For instance i am looking the data> mydf> IDnumber OA EA? beta> 1??????? 1? A? C? 0.050> 2??????? 2? G? A 0.098> 3??????? 3? T?? G 0.789>> Best,> Hana>>>> -------- Original message --------> From: Jim Lemon <drjimlemon at gmail.com>> Date: 6/12/22 1:59 AM (GMT+03:00)> To: hanatezera <hanatezera at gmail.com>, r-help mailing list <r-help at r-project.org>> Subject: Re: [R] Changing sign of columns and values>> Hi Hana,> I think this is what you want:>> # first read in your example> mydf<-read.table(text=> "IDnumber? OA? EA? beta> 1?? C?????? A?????? -0.05> 2?? G??????? A??????? 0.098> 3?? G??????? T??????? -0.789",> header=TRUE,stringsAsFactors=FALSE)> # check it> mydf> IDnumber OA EA?? beta> 1??????? 1? C? A -0.050> 2??????? 2? G? A? 0.098> 3??????? 3? G? T -0.789> # change values of mydf$beta to absolute values> mydf$beta<-abs(mydf$beta)> mydf> IDnumber OA EA? beta> 1??????? 1? C? A 0.050> 2??????? 2? G? A 0.098> 3??????? 3? G? T 0.789>> Jim>> On Sun, Jun 12, 2022 at 8:27 AM hanatezera <hanatezera at gmail.com> wrote:> >> > I have the following data set in data frameIDnumber? OA? EA? beta1???????????????????????? C?????? A?????? -0.052???????????????????????? G??????? A??????? 0.0983????????????????????????? G??????? T??????? -0.789....I want to change the sign of negative beta. If the negative value change to postive i want to switch its EA and OA.My desire result will beIDnumber? OA? EA? beta1???????????????????????? A????? C???? 0.052???????????????????????? G??????? A??????? 0.0983????????????????????????? T???????? G??????? 0.789....Any one can help me with r codes? kind regards,Hana> >???????? [[alternative HTML version deleted]]> >> > ______________________________________________> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see> > https://stat.ethz.ch/mailman/listinfo/r-help> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html> > and provide commented, minimal, self-contained, reproducible code.
	[[alternative HTML version deleted]]


From h@n@tezer@ @end|ng |rom gm@||@com  Sun Jun 12 16:07:37 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Sun, 12 Jun 2022 17:07:37 +0300
Subject: [R] comparing and changing(swaping) the value of columns from two
 diffrent data farmes
Message-ID: <CAEuJ0VAtwkiAOd05hCGCdUP5_LMq2N-ZMbx1_Hyr5yjwCqJ6pg@mail.gmail.com>

I have the following two data frames
Data frame 1
"SNPID"	"OA"	"EA"	
"snp001"	"C"	"A"	
"snp002"	"G"	"A"	
"snp003"	"C"	"A"	
"snp004"	"G"	"A"	
"snp005"	"C"	"T"	

Data frame 2
SNPID	OA	EA	id00001	id00002	id00003	id00004	id00005
snp001	   A	C	 1.01	                   2	                  0.97	
          1.97	                  1.99
snp002	  A	G	 1.02	                  2	                   1	
         2	                      2
snp003	  C	A	  1	                 1.03	                    2	
          0	                       1
snp004	 A	G	   1.02	                   1.99	                     2	
             1.02	                    1.98
snp005	C	T	     1	                      0	                     1.01	
                    1	                        1

I want to  if  OA s and EAs in data frame 2 is the same as OAs and EAs
 data frame 1 in such case I want to keep all information in data
fram2 as it is . However if   OA s and EAs in data frame 2 is
different from OAs and EAs  data frame 1, I want to change OA s and
EAs in data frame 2 to  OAs and EAs  data frame 1 and   I want to
redefine the values of the dosages (ids) of the variant (the dosage d
for the i-th individual and the j-th variant would become d_ij
new=2-dij.
Dosage [j,]=2-dosage[j,]
My desire data frame looks like
Dataframe new
SNPID"	"OA"	   "EA"	       id00001	id00002	id00003	id00004	id00005
"snp001"	"C"	"A"	2-1.01	          2- 2	  2-    0.97	       2-1.97	
    2-1.99
"snp002"	"G"	"A"	2- 1.02	             2- 2	            2-1	
  2-2	                  2- 2
"snp003"	"C"	"A"	  1	               1.03	                    2	
            0	                       1
"snp004"	"G"	"A"	2-1.02	                 2-1.99	
2-2	               2-1.02	                 2-1.98
"snp005"	"C"	"T"	     1	                      0	
1.01	                       1	                        1
Dose any one can help me the r code for the above.
Kind regards,
Hana


From bgunter@4567 @end|ng |rom gm@||@com  Sun Jun 12 17:41:15 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 12 Jun 2022 08:41:15 -0700
Subject: [R] 
 comparing and changing(swaping) the value of columns from two
 diffrent data farmes
In-Reply-To: <CAEuJ0VAtwkiAOd05hCGCdUP5_LMq2N-ZMbx1_Hyr5yjwCqJ6pg@mail.gmail.com>
References: <CAEuJ0VAtwkiAOd05hCGCdUP5_LMq2N-ZMbx1_Hyr5yjwCqJ6pg@mail.gmail.com>
Message-ID: <CAGxFJbQqRZ261pXbWaQHxAou-bR7H9pnb5Ptc-uXN_Pb7hk4mg@mail.gmail.com>

Is this some sort of homework? This list has a no homework policy.

If not, please show us your attempt to solve the problem. You seem to
be asking us to do your work for you, which is not a good way to learn
R. Show us your attempt and the error messages and/or incorrect
results you got. You will improve your R skills faster this way. IMO
only, of course. The specifications you provided seem to require
fairly basic data manipulations that you should really learn how to do
yourself.

Also...
Thanks for the reproducible example -- that is a good way to get a
helpful response. But please give us the code that creates the example
rather than just providing the text, which requires us to read and
convert it ourselves, an extra step. In general, if we can just rerun
your code it makes it easier to help ... which makes it more likely
that someone will give you a useful response.

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Sun, Jun 12, 2022 at 7:07 AM anteneh asmare <hanatezera at gmail.com> wrote:
>
> I have the following two data frames
> Data frame 1
> "SNPID" "OA"    "EA"
> "snp001"        "C"     "A"
> "snp002"        "G"     "A"
> "snp003"        "C"     "A"
> "snp004"        "G"     "A"
> "snp005"        "C"     "T"
>
> Data frame 2
> SNPID   OA      EA      id00001 id00002 id00003 id00004 id00005
> snp001     A    C        1.01                      2                      0.97
>           1.97                    1.99
> snp002    A     G        1.02                     2                        1
>          2                            2
> snp003    C     A         1                      1.03                       2
>           0                            1
> snp004   A      G          1.02                    1.99                      2
>              1.02                           1.98
> snp005  C       T            1                        0                      1.01
>                     1                           1
>
> I want to  if  OA s and EAs in data frame 2 is the same as OAs and EAs
>  data frame 1 in such case I want to keep all information in data
> fram2 as it is . However if   OA s and EAs in data frame 2 is
> different from OAs and EAs  data frame 1, I want to change OA s and
> EAs in data frame 2 to  OAs and EAs  data frame 1 and   I want to
> redefine the values of the dosages (ids) of the variant (the dosage d
> for the i-th individual and the j-th variant would become d_ij
> new=2-dij.
> Dosage [j,]=2-dosage[j,]
> My desire data frame looks like
> Dataframe new
> SNPID"  "OA"       "EA"        id00001  id00002 id00003 id00004 id00005
> "snp001"        "C"     "A"     2-1.01            2- 2    2-    0.97           2-1.97
>     2-1.99
> "snp002"        "G"     "A"     2- 1.02              2- 2                   2-1
>   2-2                     2- 2
> "snp003"        "C"     "A"       1                    1.03                         2
>             0                          1
> "snp004"        "G"     "A"     2-1.02                   2-1.99
> 2-2                    2-1.02                    2-1.98
> "snp005"        "C"     "T"          1                        0
> 1.01                           1                                1
> Dose any one can help me the r code for the above.
> Kind regards,
> Hana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Sun Jun 12 19:11:46 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 12 Jun 2022 10:11:46 -0700
Subject: [R] 
 comparing and changing(swaping) the value of columns from two
 diffrent data farmes
In-Reply-To: <CAEuJ0VAnSXxicUAcukGUrGyy0ZF4yk5mmTx5GUXNnE7m12XJrQ@mail.gmail.com>
References: <CAEuJ0VAtwkiAOd05hCGCdUP5_LMq2N-ZMbx1_Hyr5yjwCqJ6pg@mail.gmail.com>
 <CAGxFJbQqRZ261pXbWaQHxAou-bR7H9pnb5Ptc-uXN_Pb7hk4mg@mail.gmail.com>
 <CAEuJ0VC-x6wOzE=Fgcr26Zq=A7mYbaonip0mPDb3Gz46w0KJ9A@mail.gmail.com>
 <CAGxFJbSuxDuGWw9b7C3ZFmtR3y1Qj7R7dchnH-J_drxo0vwhbg@mail.gmail.com>
 <CAEuJ0VAnSXxicUAcukGUrGyy0ZF4yk5mmTx5GUXNnE7m12XJrQ@mail.gmail.com>
Message-ID: <CAGxFJbRFAYuCCgJn-ZTjCf_zh143xAoDqm201u4SEG=BF+zoVw@mail.gmail.com>

Excellent!

I am sharing the below with the list in case it might be useful for others.

Here is a similar approach that uses logical indexing directly, rather
than through ifelse(), via a simple function to pick out the rows to
change. Figuring out the details of how it works may be helpful to you
in case it's not immediately obvious.

foo <- function(dat1, dat2, testcols){
      apply(dat1[, testcols] != dat2[,testcols],1,any) ## ?apply  ?any
for details
}

Yielding...

> wh <- foo(df1, df2, 2:3)
> wh ## logical index of rows that require changes
[1]  TRUE  TRUE FALSE  TRUE FALSE

> df2[wh, 4:8] <- 2 - df2[wh, 4:8]  ## per your specification
> ## and since df2's first 3 columns must be the same as df1's:
> df2[,2:3] <- df1[, 2:3]

> df2
   SNPID OA EA id00001 id00002 id00003 id00004 id00005
1 snp001  C  A    0.99    0.00    1.03    0.03    0.01
2 snp002  G  A    0.98    0.00    1.00    0.00    0.00
3 snp003  C  A    1.00    1.03    2.00    0.00    1.00
4 snp004  G  A    0.98    0.01    0.00    0.98    0.02
5 snp005  C  T    1.00    0.00    1.01    1.00    1.00



On Sun, Jun 12, 2022 at 9:32 AM anteneh asmare <hanatezera at gmail.com> wrote:
>
> Dear, Bert, Thanks i try to extract  dataframe1[ , c(2, 3)] and
> combine to dataframe2 , then i apply if else
> it works, Thanks
> best,
> Hana
>
> On 6/12/22, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> > No you don't. You need to learn R's data manipulation procedures so
> > that you can program such (in this case fairly simple) procedures
> > yourself.
> >
> > The ifelse() function might be a good place to start here.
> > ?ifelse
> >
> > Bert Gunter
> >
> >
> >
> > On Sun, Jun 12, 2022 at 8:58 AM anteneh asmare <hanatezera at gmail.com>
> > wrote:
> >>
> >> Dear Bert , Thanks for reply! I need the  the function or package that
> >> used to compare columns from two different data frames.
> >> Best,
> >> Hana
> >> On 6/12/22, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> >> > Is this some sort of homework? This list has a no homework policy.
> >> >
> >> > If not, please show us your attempt to solve the problem. You seem to
> >> > be asking us to do your work for you, which is not a good way to learn
> >> > R. Show us your attempt and the error messages and/or incorrect
> >> > results you got. You will improve your R skills faster this way. IMO
> >> > only, of course. The specifications you provided seem to require
> >> > fairly basic data manipulations that you should really learn how to do
> >> > yourself.
> >> >
> >> > Also...
> >> > Thanks for the reproducible example -- that is a good way to get a
> >> > helpful response. But please give us the code that creates the example
> >> > rather than just providing the text, which requires us to read and
> >> > convert it ourselves, an extra step. In general, if we can just rerun
> >> > your code it makes it easier to help ... which makes it more likely
> >> > that someone will give you a useful response.
> >> >
> >> > Bert Gunter
> >> >
> >> > "The trouble with having an open mind is that people keep coming along
> >> > and sticking things into it."
> >> > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >> >
> >> > On Sun, Jun 12, 2022 at 7:07 AM anteneh asmare <hanatezera at gmail.com>
> >> > wrote:
> >> >>
> >> >> I have the following two data frames
> >> >> Data frame 1
> >> >> "SNPID" "OA"    "EA"
> >> >> "snp001"        "C"     "A"
> >> >> "snp002"        "G"     "A"
> >> >> "snp003"        "C"     "A"
> >> >> "snp004"        "G"     "A"
> >> >> "snp005"        "C"     "T"
> >> >>
> >> >> Data frame 2
> >> >> SNPID   OA      EA      id00001 id00002 id00003 id00004 id00005
> >> >> snp001     A    C        1.01                      2
> >> >> 0.97
> >> >>           1.97                    1.99
> >> >> snp002    A     G        1.02                     2
> >> >> 1
> >> >>          2                            2
> >> >> snp003    C     A         1                      1.03
> >> >>  2
> >> >>           0                            1
> >> >> snp004   A      G          1.02                    1.99
> >> >>   2
> >> >>              1.02                           1.98
> >> >> snp005  C       T            1                        0
> >> >>   1.01
> >> >>                     1                           1
> >> >>
> >> >> I want to  if  OA s and EAs in data frame 2 is the same as OAs and EAs
> >> >>  data frame 1 in such case I want to keep all information in data
> >> >> fram2 as it is . However if   OA s and EAs in data frame 2 is
> >> >> different from OAs and EAs  data frame 1, I want to change OA s and
> >> >> EAs in data frame 2 to  OAs and EAs  data frame 1 and   I want to
> >> >> redefine the values of the dosages (ids) of the variant (the dosage d
> >> >> for the i-th individual and the j-th variant would become d_ij
> >> >> new=2-dij.
> >> >> Dosage [j,]=2-dosage[j,]
> >> >> My desire data frame looks like
> >> >> Dataframe new
> >> >> SNPID"  "OA"       "EA"        id00001  id00002 id00003 id00004
> >> >> id00005
> >> >> "snp001"        "C"     "A"     2-1.01            2- 2    2-    0.97
> >> >>     2-1.97
> >> >>     2-1.99
> >> >> "snp002"        "G"     "A"     2- 1.02              2- 2
> >> >>  2-1
> >> >>   2-2                     2- 2
> >> >> "snp003"        "C"     "A"       1                    1.03
> >> >>          2
> >> >>             0                          1
> >> >> "snp004"        "G"     "A"     2-1.02                   2-1.99
> >> >> 2-2                    2-1.02                    2-1.98
> >> >> "snp005"        "C"     "T"          1                        0
> >> >> 1.01                           1                                1
> >> >> Dose any one can help me the r code for the above.
> >> >> Kind regards,
> >> >> Hana
> >> >>
> >> >> ______________________________________________
> >> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> PLEASE do read the posting guide
> >> >> http://www.R-project.org/posting-guide.html
> >> >> and provide commented, minimal, self-contained, reproducible code.
> >> >
> >


From drj|m|emon @end|ng |rom gm@||@com  Mon Jun 13 01:41:48 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Mon, 13 Jun 2022 09:41:48 +1000
Subject: [R] Changing sign of columns and values
In-Reply-To: <CAEuJ0VAqKdBpQJ=Bih=9V3=bDQBL05d=okpfRxSa0fqTK4EvBg@mail.gmail.com>
References: <CA+8X3fUB-vzBi0Jfr66tDUXQT4tctENg4=4sBvWxsiKBaEBUvg@mail.gmail.com>
 <62a5ca9c.1c69fb81.e10ce.4c1d@mx.google.com>
 <CAEuJ0VAqKdBpQJ=Bih=9V3=bDQBL05d=okpfRxSa0fqTK4EvBg@mail.gmail.com>
Message-ID: <CA+8X3fWFE8zShP+depx-TeeFWZaYy=ySZpqDmcJkJwg95BTYcw@mail.gmail.com>

Hi Hana,
This is a bit more difficult, but the same basic steps apply. See the
comments for an explanation.
When you submit a problem like this, it is a lot easier if you send
the output from "dput" (e.g. dput(df1)) or set your data frames up
with "read.table" like I have done in the example below. It make it
lots easier for anyone who wants to reply to see what your dataset
looks like and input that dataset to devise a solution.
I have made some assumptions about marking the rows to be altered and
the arithmetic to do on those rows. What follows may not be as general
a solution as you want:

df1<-read.table(text=
 "SNPID OA    EA
 snp001        C     A
 snp002        G     A
 snp003        C     A
 snp004        G     A
 snp005        C     T",
 header=TRUE,stringsAsFactors=FALSE)

df2<-read.table(text=
 "SNPID   OA    EA   id00001 id00002 id00003 id00004 id00005
 snp001    A    C    1.01    2       0.97    1.97    1.99
 snp002    A    G    1.02    2       1       2       2
 snp003    C    A    1       1.03    2       0       1
 snp004    A    G    1.02    1.99    2       1.02    1.98
 snp005    C    T    1       0       1.01    1       1",
 header=TRUE,stringsAsFactors=FALSE)

# get a logical vector of the different XA values
reversals<-df1$EA != df2$EA | df1$OA != df2$OA
reversals

# set a variable to the maximum value of idxxxxx
# just to make the code easier to understand
maxid<-2

# create a copy of df2
dfnew<-df2

# now swap the XA columns and reflect the
# idxxxxxx values of the rows in which
# EA and OA are different between df1 and df2
# here I have looped through the rows
nrows<-dim(dfnew)[1]
idcols<-4:8
XAcols<-2:3
for(i in 1:nrows) {
 if(reversals[i]) {
  dfnew[i,XAcols]<-rev(dfnew[i,XAcols])
  dfnew[i,idcols]<-maxid-dfnew[i,idcols]
 }
}

dfnew

Jim

On Mon, Jun 13, 2022 at 12:09 AM anteneh asmare <hanatezera at gmail.com> wrote:
>
> Dear Jim, Morning I have the same issue regarding comparing and
> swapping the value of columns fro two different data frames.
> I have the following two data frames
> Data frame 1
> "SNPID" "OA"    "EA"
> "snp001"        "C"     "A"
> "snp002"        "G"     "A"
> "snp003"        "C"     "A"
> "snp004"        "G"     "A"
> "snp005"        "C"     "T"
>
> Data frame 2
> SNPID   OA      EA      id00001 id00002 id00003 id00004 id00005
> snp001     A    C        1.01                      2                      0.97
>           1.97                    1.99
> snp002    A     G        1.02                     2                        1
>          2                            2
> snp003    C     A         1                      1.03                       2
>           0                            1
> snp004   A      G          1.02                    1.99                      2
>              1.02                           1.98
> snp005  C       T            1                        0                      1.01
>                     1                           1
>
> I want to  if  OA s and EAs in data frame 2 is the same as OAs and EAs
>  data frame 1 in such case I want to keep all information in data
> fram2 as it is . However if   OA s and EAs in data frame 2 is
> different from OAs and EAs  data frame 1, I want to change OA s and
> EAs in data frame 2 to  OAs and EAs  data frame 1 and   I want to
> redefine the values of the dosages (ids) of the variant (the dosage d
> for the i-th individual and the j-th variant would become d_ij
> new=2-dij.
> Dosage [j,]=2-dosage[j,]
> My desire data frame looks like
> Dataframe new
> SNPID"  "OA"       "EA"        id00001  id00002 id00003 id00004 id00005
> "snp001"        "C"     "A"     2-1.01            2- 2    2-    0.97           2-1.97
>     2-1.99
> "snp002"        "G"     "A"     2- 1.02              2- 2                   2-1
>   2-2                     2- 2
> "snp003"        "C"     "A"       1                    1.03                         2
>             0                          1
> "snp004"        "G"     "A"     2-1.02                   2-1.99
> 2-2                    2-1.02                    2-1.98
> "snp005"        "C"     "T"          1                        0
> 1.01                           1                                1
>  can you help me the r code for the above.
> Kind regards,
> Hana
>
>
> On 6/12/22, hanatezera <hanatezera at gmail.com> wrote:
> > Dear Jim, Thanks a lot this is exactly i am looking for.Stay safe and
> > blessed !Best,Hana
> > -------- Original message --------From: Jim Lemon <drjimlemon at gmail.com>
> > Date: 6/12/22  6:43 AM  (GMT+03:00) To: hanatezera <hanatezera at gmail.com>
> > Cc: r-help mailing list <r-help at r-project.org> Subject: Re: [R] Changing
> > sign of columns and values Hi Hana,I didn't look closely. The simplest rule
> > that I can see is that theletters (nucleotides?) should be swapped if there
> > has been a signchange in the beta value.
> > Therefore:mydf<-read.table(text="IDnumber  OA  EA  beta1   C       A
> > -0.052   G        A        0.0983   G        T
> > -0.789",header=TRUE,stringsAsFactors=FALSE)# logical vector marking the rows
> > to be swappedswap_letters<-mydf$beta < 0# save the OA values to be
> > swappednewEA<-mydf$OA[swap_letters]# change the OAs to
> > EAsmydf$OA[swap_letters]<-mydf$EA[swap_letters]# set the relevant EA values
> > to the old OA valuesmydf$EA[swap_letters]<-newEA# change the beta
> > valuesmydf$beta<-abs(mydf$beta)mydfJimOn Sun, Jun 12, 2022 at 9:11 AM
> > hanatezera <hanatezera at gmail.com> wrote:>> Dear jim thanks for your help! I
> > want to change also the value of OA and EF simultaneously.> For instance i
> > am looking the data> mydf> IDnumber OA EA  beta> 1        1  A  C  0.050>
> > 2        2  G  A 0.098> 3        3  T   G 0.789>> Best,> Hana>>>> --------
> > Original message --------> From: Jim Lemon <drjimlemon at gmail.com>> Date:
> > 6/12/22 1:59 AM (GMT+03:00)> To: hanatezera <hanatezera at gmail.com>, r-help
> > mailing list <r-help at r-project.org>> Subject: Re: [R] Changing sign of
> > columns and values>> Hi Hana,> I think this is what you want:>> # first read
> > in your example> mydf<-read.table(text=> "IDnumber  OA  EA  beta> 1
> > C       A       -0.05> 2   G        A        0.098> 3   G        T
> > -0.789",> header=TRUE,stringsAsFactors=FALSE)> # check it> mydf> IDnumber OA
> > EA   beta> 1        1  C  A -0.050> 2        2  G  A  0.098> 3        3  G
> > T -0.789> # change values of mydf$beta to absolute values>
> > mydf$beta<-abs(mydf$beta)> mydf> IDnumber OA EA  beta> 1        1  C  A
> > 0.050> 2        2  G  A 0.098> 3        3  G  T 0.789>> Jim>> On Sun, Jun
> > 12, 2022 at 8:27 AM hanatezera <hanatezera at gmail.com> wrote:> >> > I have
> > the following data set in data frameIDnumber  OA  EA
> > beta1                         C       A       -0.052
> > G        A        0.0983                          G        T
> > -0.789....I want to change the sign of negative beta. If the negative value
> > change to postive i want to switch its EA and OA.My desire result will
> > beIDnumber  OA  EA  beta1                         A      C
> > 0.052                         G        A
> > 0.0983                          T         G        0.789....Any one can help
> > me with r codes? kind regards,Hana> >         [[alternative HTML version
> > deleted]]> >> > ______________________________________________> >
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see> >
> > https://stat.ethz.ch/mailman/listinfo/r-help> > PLEASE do read the posting
> > guide http://www.R-project.org/posting-guide.html> > and provide commented,
> > minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon Jun 13 05:13:12 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 13 Jun 2022 04:13:12 +0100
Subject: [R] Changing sign of columns and values
In-Reply-To: <CA+8X3fWFE8zShP+depx-TeeFWZaYy=ySZpqDmcJkJwg95BTYcw@mail.gmail.com>
References: <CA+8X3fUB-vzBi0Jfr66tDUXQT4tctENg4=4sBvWxsiKBaEBUvg@mail.gmail.com>
 <62a5ca9c.1c69fb81.e10ce.4c1d@mx.google.com>
 <CAEuJ0VAqKdBpQJ=Bih=9V3=bDQBL05d=okpfRxSa0fqTK4EvBg@mail.gmail.com>
 <CA+8X3fWFE8zShP+depx-TeeFWZaYy=ySZpqDmcJkJwg95BTYcw@mail.gmail.com>
Message-ID: <20eea35c-6ed9-8450-ab2f-40d151fb1249@sapo.pt>

Hello,

Using Jim's code, the following is vectorized and the result is 
identical to the for loop result.


# create a copy of df2
dfnew2 <- df2
dfnew2[reversals, XAcols] <- rev(dfnew2[reversals, XAcols])
dfnew2[reversals, idcols] <- maxid - dfnew2[reversals, idcols]

identical(dfnew, dfnew2)
#[1] TRUE


Hope this helps,

Rui Barradas

?s 00:41 de 13/06/2022, Jim Lemon escreveu:
> Hi Hana,
> This is a bit more difficult, but the same basic steps apply. See the
> comments for an explanation.
> When you submit a problem like this, it is a lot easier if you send
> the output from "dput" (e.g. dput(df1)) or set your data frames up
> with "read.table" like I have done in the example below. It make it
> lots easier for anyone who wants to reply to see what your dataset
> looks like and input that dataset to devise a solution.
> I have made some assumptions about marking the rows to be altered and
> the arithmetic to do on those rows. What follows may not be as general
> a solution as you want:
> 
> df1<-read.table(text=
>   "SNPID OA    EA
>   snp001        C     A
>   snp002        G     A
>   snp003        C     A
>   snp004        G     A
>   snp005        C     T",
>   header=TRUE,stringsAsFactors=FALSE)
> 
> df2<-read.table(text=
>   "SNPID   OA    EA   id00001 id00002 id00003 id00004 id00005
>   snp001    A    C    1.01    2       0.97    1.97    1.99
>   snp002    A    G    1.02    2       1       2       2
>   snp003    C    A    1       1.03    2       0       1
>   snp004    A    G    1.02    1.99    2       1.02    1.98
>   snp005    C    T    1       0       1.01    1       1",
>   header=TRUE,stringsAsFactors=FALSE)
> 
> # get a logical vector of the different XA values
> reversals<-df1$EA != df2$EA | df1$OA != df2$OA
> reversals
> 
> # set a variable to the maximum value of idxxxxx
> # just to make the code easier to understand
> maxid<-2
> 
> # create a copy of df2
> dfnew<-df2
> 
> # now swap the XA columns and reflect the
> # idxxxxxx values of the rows in which
> # EA and OA are different between df1 and df2
> # here I have looped through the rows
> nrows<-dim(dfnew)[1]
> idcols<-4:8
> XAcols<-2:3
> for(i in 1:nrows) {
>   if(reversals[i]) {
>    dfnew[i,XAcols]<-rev(dfnew[i,XAcols])
>    dfnew[i,idcols]<-maxid-dfnew[i,idcols]
>   }
> }
> 
> dfnew
> 
> Jim
> 
> On Mon, Jun 13, 2022 at 12:09 AM anteneh asmare <hanatezera at gmail.com> wrote:
>>
>> Dear Jim, Morning I have the same issue regarding comparing and
>> swapping the value of columns fro two different data frames.
>> I have the following two data frames
>> Data frame 1
>> "SNPID" "OA"    "EA"
>> "snp001"        "C"     "A"
>> "snp002"        "G"     "A"
>> "snp003"        "C"     "A"
>> "snp004"        "G"     "A"
>> "snp005"        "C"     "T"
>>
>> Data frame 2
>> SNPID   OA      EA      id00001 id00002 id00003 id00004 id00005
>> snp001     A    C        1.01                      2                      0.97
>>            1.97                    1.99
>> snp002    A     G        1.02                     2                        1
>>           2                            2
>> snp003    C     A         1                      1.03                       2
>>            0                            1
>> snp004   A      G          1.02                    1.99                      2
>>               1.02                           1.98
>> snp005  C       T            1                        0                      1.01
>>                      1                           1
>>
>> I want to  if  OA s and EAs in data frame 2 is the same as OAs and EAs
>>   data frame 1 in such case I want to keep all information in data
>> fram2 as it is . However if   OA s and EAs in data frame 2 is
>> different from OAs and EAs  data frame 1, I want to change OA s and
>> EAs in data frame 2 to  OAs and EAs  data frame 1 and   I want to
>> redefine the values of the dosages (ids) of the variant (the dosage d
>> for the i-th individual and the j-th variant would become d_ij
>> new=2-dij.
>> Dosage [j,]=2-dosage[j,]
>> My desire data frame looks like
>> Dataframe new
>> SNPID"  "OA"       "EA"        id00001  id00002 id00003 id00004 id00005
>> "snp001"        "C"     "A"     2-1.01            2- 2    2-    0.97           2-1.97
>>      2-1.99
>> "snp002"        "G"     "A"     2- 1.02              2- 2                   2-1
>>    2-2                     2- 2
>> "snp003"        "C"     "A"       1                    1.03                         2
>>              0                          1
>> "snp004"        "G"     "A"     2-1.02                   2-1.99
>> 2-2                    2-1.02                    2-1.98
>> "snp005"        "C"     "T"          1                        0
>> 1.01                           1                                1
>>   can you help me the r code for the above.
>> Kind regards,
>> Hana
>>
>>
>> On 6/12/22, hanatezera <hanatezera at gmail.com> wrote:
>>> Dear Jim, Thanks a lot this is exactly i am looking for.Stay safe and
>>> blessed !Best,Hana
>>> -------- Original message --------From: Jim Lemon <drjimlemon at gmail.com>
>>> Date: 6/12/22  6:43 AM  (GMT+03:00) To: hanatezera <hanatezera at gmail.com>
>>> Cc: r-help mailing list <r-help at r-project.org> Subject: Re: [R] Changing
>>> sign of columns and values Hi Hana,I didn't look closely. The simplest rule
>>> that I can see is that theletters (nucleotides?) should be swapped if there
>>> has been a signchange in the beta value.
>>> Therefore:mydf<-read.table(text="IDnumber  OA  EA  beta1   C       A
>>> -0.052   G        A        0.0983   G        T
>>> -0.789",header=TRUE,stringsAsFactors=FALSE)# logical vector marking the rows
>>> to be swappedswap_letters<-mydf$beta < 0# save the OA values to be
>>> swappednewEA<-mydf$OA[swap_letters]# change the OAs to
>>> EAsmydf$OA[swap_letters]<-mydf$EA[swap_letters]# set the relevant EA values
>>> to the old OA valuesmydf$EA[swap_letters]<-newEA# change the beta
>>> valuesmydf$beta<-abs(mydf$beta)mydfJimOn Sun, Jun 12, 2022 at 9:11 AM
>>> hanatezera <hanatezera at gmail.com> wrote:>> Dear jim thanks for your help! I
>>> want to change also the value of OA and EF simultaneously.> For instance i
>>> am looking the data> mydf> IDnumber OA EA  beta> 1        1  A  C  0.050>
>>> 2        2  G  A 0.098> 3        3  T   G 0.789>> Best,> Hana>>>> --------
>>> Original message --------> From: Jim Lemon <drjimlemon at gmail.com>> Date:
>>> 6/12/22 1:59 AM (GMT+03:00)> To: hanatezera <hanatezera at gmail.com>, r-help
>>> mailing list <r-help at r-project.org>> Subject: Re: [R] Changing sign of
>>> columns and values>> Hi Hana,> I think this is what you want:>> # first read
>>> in your example> mydf<-read.table(text=> "IDnumber  OA  EA  beta> 1
>>> C       A       -0.05> 2   G        A        0.098> 3   G        T
>>> -0.789",> header=TRUE,stringsAsFactors=FALSE)> # check it> mydf> IDnumber OA
>>> EA   beta> 1        1  C  A -0.050> 2        2  G  A  0.098> 3        3  G
>>> T -0.789> # change values of mydf$beta to absolute values>
>>> mydf$beta<-abs(mydf$beta)> mydf> IDnumber OA EA  beta> 1        1  C  A
>>> 0.050> 2        2  G  A 0.098> 3        3  G  T 0.789>> Jim>> On Sun, Jun
>>> 12, 2022 at 8:27 AM hanatezera <hanatezera at gmail.com> wrote:> >> > I have
>>> the following data set in data frameIDnumber  OA  EA
>>> beta1                         C       A       -0.052
>>> G        A        0.0983                          G        T
>>> -0.789....I want to change the sign of negative beta. If the negative value
>>> change to postive i want to switch its EA and OA.My desire result will
>>> beIDnumber  OA  EA  beta1                         A      C
>>> 0.052                         G        A
>>> 0.0983                          T         G        0.789....Any one can help
>>> me with r codes? kind regards,Hana> >         [[alternative HTML version
>>> deleted]]> >> > ______________________________________________> >
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see> >
>>> https://stat.ethz.ch/mailman/listinfo/r-help> > PLEASE do read the posting
>>> guide http://www.R-project.org/posting-guide.html> > and provide commented,
>>> minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From h@n@tezer@ @end|ng |rom gm@||@com  Mon Jun 13 09:22:19 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Mon, 13 Jun 2022 10:22:19 +0300
Subject: [R] Changing sign of columns and values
In-Reply-To: <CA+8X3fWFE8zShP+depx-TeeFWZaYy=ySZpqDmcJkJwg95BTYcw@mail.gmail.com>
References: <CA+8X3fUB-vzBi0Jfr66tDUXQT4tctENg4=4sBvWxsiKBaEBUvg@mail.gmail.com>
 <62a5ca9c.1c69fb81.e10ce.4c1d@mx.google.com>
 <CAEuJ0VAqKdBpQJ=Bih=9V3=bDQBL05d=okpfRxSa0fqTK4EvBg@mail.gmail.com>
 <CA+8X3fWFE8zShP+depx-TeeFWZaYy=ySZpqDmcJkJwg95BTYcw@mail.gmail.com>
Message-ID: <CAEuJ0VA6B+LJJRbZGp6D+HP33nmM9L+wdnPakVBg=-CxGkfEig@mail.gmail.com>

Dear Jim,  Good morning, hope you are doing very well, Here I want to
calculate the specific parameters based on the previous data. I have
attached below.
can you help me  with r functions or code to solve the above equations
Kind regards,
Hana


On 6/13/22, Jim Lemon <drjimlemon at gmail.com> wrote:
> Hi Hana,
> This is a bit more difficult, but the same basic steps apply. See the
> comments for an explanation.
> When you submit a problem like this, it is a lot easier if you send
> the output from "dput" (e.g. dput(df1)) or set your data frames up
> with "read.table" like I have done in the example below. It make it
> lots easier for anyone who wants to reply to see what your dataset
> looks like and input that dataset to devise a solution.
> I have made some assumptions about marking the rows to be altered and
> the arithmetic to do on those rows. What follows may not be as general
> a solution as you want:
>
> df1<-read.table(text=
>  "SNPID OA    EA
>  snp001        C     A
>  snp002        G     A
>  snp003        C     A
>  snp004        G     A
>  snp005        C     T",
>  header=TRUE,stringsAsFactors=FALSE)
>
> df2<-read.table(text=
>  "SNPID   OA    EA   id00001 id00002 id00003 id00004 id00005
>  snp001    A    C    1.01    2       0.97    1.97    1.99
>  snp002    A    G    1.02    2       1       2       2
>  snp003    C    A    1       1.03    2       0       1
>  snp004    A    G    1.02    1.99    2       1.02    1.98
>  snp005    C    T    1       0       1.01    1       1",
>  header=TRUE,stringsAsFactors=FALSE)
>
> # get a logical vector of the different XA values
> reversals<-df1$EA != df2$EA | df1$OA != df2$OA
> reversals
>
> # set a variable to the maximum value of idxxxxx
> # just to make the code easier to understand
> maxid<-2
>
> # create a copy of df2
> dfnew<-df2
>
> # now swap the XA columns and reflect the
> # idxxxxxx values of the rows in which
> # EA and OA are different between df1 and df2
> # here I have looped through the rows
> nrows<-dim(dfnew)[1]
> idcols<-4:8
> XAcols<-2:3
> for(i in 1:nrows) {
>  if(reversals[i]) {
>   dfnew[i,XAcols]<-rev(dfnew[i,XAcols])
>   dfnew[i,idcols]<-maxid-dfnew[i,idcols]
>  }
> }
>
> dfnew
>
> Jim
>
> On Mon, Jun 13, 2022 at 12:09 AM anteneh asmare <hanatezera at gmail.com>
> wrote:
>>
>> Dear Jim, Morning I have the same issue regarding comparing and
>> swapping the value of columns fro two different data frames.
>> I have the following two data frames
>> Data frame 1
>> "SNPID" "OA"    "EA"
>> "snp001"        "C"     "A"
>> "snp002"        "G"     "A"
>> "snp003"        "C"     "A"
>> "snp004"        "G"     "A"
>> "snp005"        "C"     "T"
>>
>> Data frame 2
>> SNPID   OA      EA      id00001 id00002 id00003 id00004 id00005
>> snp001     A    C        1.01                      2
>> 0.97
>>           1.97                    1.99
>> snp002    A     G        1.02                     2
>> 1
>>          2                            2
>> snp003    C     A         1                      1.03
>>  2
>>           0                            1
>> snp004   A      G          1.02                    1.99
>>   2
>>              1.02                           1.98
>> snp005  C       T            1                        0
>>   1.01
>>                     1                           1
>>
>> I want to  if  OA s and EAs in data frame 2 is the same as OAs and EAs
>>  data frame 1 in such case I want to keep all information in data
>> fram2 as it is . However if   OA s and EAs in data frame 2 is
>> different from OAs and EAs  data frame 1, I want to change OA s and
>> EAs in data frame 2 to  OAs and EAs  data frame 1 and   I want to
>> redefine the values of the dosages (ids) of the variant (the dosage d
>> for the i-th individual and the j-th variant would become d_ij
>> new=2-dij.
>> Dosage [j,]=2-dosage[j,]
>> My desire data frame looks like
>> Dataframe new
>> SNPID"  "OA"       "EA"        id00001  id00002 id00003 id00004 id00005
>> "snp001"        "C"     "A"     2-1.01            2- 2    2-    0.97
>>     2-1.97
>>     2-1.99
>> "snp002"        "G"     "A"     2- 1.02              2- 2
>>  2-1
>>   2-2                     2- 2
>> "snp003"        "C"     "A"       1                    1.03
>>          2
>>             0                          1
>> "snp004"        "G"     "A"     2-1.02                   2-1.99
>> 2-2                    2-1.02                    2-1.98
>> "snp005"        "C"     "T"          1                        0
>> 1.01                           1                                1
>>  can you help me the r code for the above.
>> Kind regards,
>> Hana
>>
>>
>> On 6/12/22, hanatezera <hanatezera at gmail.com> wrote:
>> > Dear Jim, Thanks a lot this is exactly i am looking for.Stay safe and
>> > blessed !Best,Hana
>> > -------- Original message --------From: Jim Lemon
>> > <drjimlemon at gmail.com>
>> > Date: 6/12/22  6:43 AM  (GMT+03:00) To: hanatezera
>> > <hanatezera at gmail.com>
>> > Cc: r-help mailing list <r-help at r-project.org> Subject: Re: [R]
>> > Changing
>> > sign of columns and values Hi Hana,I didn't look closely. The simplest
>> > rule
>> > that I can see is that theletters (nucleotides?) should be swapped if
>> > there
>> > has been a signchange in the beta value.
>> > Therefore:mydf<-read.table(text="IDnumber  OA  EA  beta1   C       A
>> > -0.052   G        A        0.0983   G        T
>> > -0.789",header=TRUE,stringsAsFactors=FALSE)# logical vector marking the
>> > rows
>> > to be swappedswap_letters<-mydf$beta < 0# save the OA values to be
>> > swappednewEA<-mydf$OA[swap_letters]# change the OAs to
>> > EAsmydf$OA[swap_letters]<-mydf$EA[swap_letters]# set the relevant EA
>> > values
>> > to the old OA valuesmydf$EA[swap_letters]<-newEA# change the beta
>> > valuesmydf$beta<-abs(mydf$beta)mydfJimOn Sun, Jun 12, 2022 at 9:11 AM
>> > hanatezera <hanatezera at gmail.com> wrote:>> Dear jim thanks for your
>> > help! I
>> > want to change also the value of OA and EF simultaneously.> For instance
>> > i
>> > am looking the data> mydf> IDnumber OA EA  beta> 1        1  A  C
>> > 0.050>
>> > 2        2  G  A 0.098> 3        3  T   G 0.789>> Best,> Hana>>>>
>> > --------
>> > Original message --------> From: Jim Lemon <drjimlemon at gmail.com>>
>> > Date:
>> > 6/12/22 1:59 AM (GMT+03:00)> To: hanatezera <hanatezera at gmail.com>,
>> > r-help
>> > mailing list <r-help at r-project.org>> Subject: Re: [R] Changing sign of
>> > columns and values>> Hi Hana,> I think this is what you want:>> # first
>> > read
>> > in your example> mydf<-read.table(text=> "IDnumber  OA  EA  beta> 1
>> > C       A       -0.05> 2   G        A        0.098> 3   G        T
>> > -0.789",> header=TRUE,stringsAsFactors=FALSE)> # check it> mydf>
>> > IDnumber OA
>> > EA   beta> 1        1  C  A -0.050> 2        2  G  A  0.098> 3        3
>> > G
>> > T -0.789> # change values of mydf$beta to absolute values>
>> > mydf$beta<-abs(mydf$beta)> mydf> IDnumber OA EA  beta> 1        1  C  A
>> > 0.050> 2        2  G  A 0.098> 3        3  G  T 0.789>> Jim>> On Sun,
>> > Jun
>> > 12, 2022 at 8:27 AM hanatezera <hanatezera at gmail.com> wrote:> >> > I
>> > have
>> > the following data set in data frameIDnumber  OA  EA
>> > beta1                         C       A       -0.052
>> > G        A        0.0983                          G        T
>> > -0.789....I want to change the sign of negative beta. If the negative
>> > value
>> > change to postive i want to switch its EA and OA.My desire result will
>> > beIDnumber  OA  EA  beta1                         A      C
>> > 0.052                         G        A
>> > 0.0983                          T         G        0.789....Any one can
>> > help
>> > me with r codes? kind regards,Hana> >         [[alternative HTML
>> > version
>> > deleted]]> >> > ______________________________________________> >
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see> >
>> > https://stat.ethz.ch/mailman/listinfo/r-help> > PLEASE do read the
>> > posting
>> > guide http://www.R-project.org/posting-guide.html> > and provide
>> > commented,
>> > minimal, self-contained, reproducible code.
>

From bgreen @end|ng |rom dy@on@br|@net@org@@u  Mon Jun 13 12:21:38 2022
From: bgreen @end|ng |rom dy@on@br|@net@org@@u (Bob Green)
Date: Mon, 13 Jun 2022 20:21:38 +1000
Subject: [R] Error creating a map with packages sf and tidyverse.
In-Reply-To: <mailman.366961.1.1655114402.34330.r-help@r-project.org>
References: <mailman.366961.1.1655114402.34330.r-help@r-project.org>
Message-ID: <mailman.366962.1.1655116326.1223.r-help@r-project.org>

I was hoping for advice about this code that I have run several 
times. When I recently ran the code, I received the error 
below:"Error in FUN(X[[i]], ...) : object 'fac' not found"

In R Studio the code runs, but instead of non-red countries being 
white, they are darker grey.

Is 'fill=fac', no longer valid, or is there some other issue?

Any advice is appreciated,

Regards

Bob

 > library(tidyverse)
-- Attaching packages 
----------------------------------------------------------------------------- 
tidyverse 1.3.1 --
v ggplot2 3.3.5     v purrr   0.3.4
v tibble  3.1.3     v dplyr   1.0.7
v tidyr   1.1.4     v stringr 1.4.0
v readr   2.1.1     v forcats 0.5.1
-- Conflicts 
-------------------------------------------------------------------------------- 
tidyverse_conflicts() --
x dplyr::filter() masks stats::filter()
x dplyr::lag()    masks stats::lag()
Warning messages:
1: In (function (kind = NULL, normal.kind = NULL, sample.kind = NULL)  :
   non-uniform 'Rounding' sampler used
2: In (function (kind = NULL, normal.kind = NULL, sample.kind = NULL)  :
   non-uniform 'Rounding' sampler used
 > library(sf)
Linking to GEOS 3.9.1, GDAL 3.2.1, PROJ 7.2.1; sf_use_s2() is TRUE
Warning message:
package 'sf' was built under R version 4.1.3
 >
 >
 >
 > world_sf = map_data("world") %>%
+    filter(region != "Antarctica") %>%
+    mutate(region=replace(region, subregion=="Alaska", "USA_Alaska")) %>%
+    st_as_sf(coords=c("long","lat"), crs=4326) %>%
+    group_by(region, group) %>% summarise(geometry=st_combine(geometry)) %>%
+    st_cast("POLYGON") %>% summarise(geometry=st_combine(geometry))
`summarise()` has grouped output by 'region'. You can override using 
the `.groups` argument.
 >
 > labs = tibble(region = c("Argentina", "Australia", 
"Barbados","Brazil", "England","Fiji", "France", "French Polynesia", 
"Ghana", "Italy","Japan", "New Zealand", "Papua New Guinea", "Peru", 
"Portugal", "Samoa", "Sao Tome and Principe", "Senegal", "Solomon 
Islands", "South Africa", "Spain", "Trinidad", "Tobago", "Turkey", 
"UK", "Uruguay", "USA"))
 > ggplot()+geom_sf(aes(fill=fac))+coord_sf(expand=FALSE)
Error in FUN(X[[i]], ...) : object 'fac' not found
 >
 > labels_sf =  st_point_on_surface(world_sf) %>% right_join(labs, by="region")
Warning messages:
1: In st_point_on_surface.sf(world_sf) :
   st_point_on_surface assumes attributes are constant over geometries of x
2: In st_point_on_surface.sfc(st_geometry(x)) :
   st_point_on_surface may not give correct results for longitude/latitude data
 >
 >
 > world_sf %>% mutate(fac = region %in% labs$region)  %>%
+    ggplot()+geom_sf(aes(fill=fac))+coord_sf( expand=FALSE)+
+    theme(axis.title = element_blank(), legend.position = "none")+
+    scale_fill_manual(values=c("FALSE"=NA, "TRUE"="red"))
 >


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Mon Jun 13 13:24:21 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Mon, 13 Jun 2022 16:24:21 +0500
Subject: [R] Kindly please help
Message-ID: <CAMfKi3JL2W8VVUTwvDJb8obWPXXMZi-OM6HZS8b2M1HJ_r1rTA@mail.gmail.com>

Hi, Dear Respected Professor! I hope that you are doing well. Kindly help
me with the following:

I have the R-codes for the "Quantile Augmented Mean Group" method. The
relevant codes and data are attached herewith.
Note: The link to the reference paper is:
https://www.econ.cam.ac.uk/people-files/emeritus/mhp1/fp20/qmg40-rev21.pdf
My Issue:
When I run the given codes to estimate the results for my own data. I have
to face several errors. I humbly request the experts to help me to estimate
the results for my data.
Thank you so much for your precious time.

Regards

Muhammad Zubair Chishti
Ph.D. Student
School of Business,
Zhengzhou University, Henan, China.
My Google scholar link:
https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
My ResearchGate link: https://www.researchgate.net/profile/Muhammad-Chishti

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Table.3.1.and.Table.3.2.design1.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220613/49e4b4c7/attachment.txt>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: rq.fit.panel.all.revised.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220613/49e4b4c7/attachment-0001.txt>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: data2.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220613/49e4b4c7/attachment-0002.txt>

From petr@p|k@| @end|ng |rom prechez@@cz  Mon Jun 13 14:12:26 2022
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Mon, 13 Jun 2022 12:12:26 +0000
Subject: [R] A humble request regarding QAMG method in R
In-Reply-To: <CAMfKi3Kd9oP2nWe6boV4rNjVHAZ4eV=vwpnCAzh=6Gw-ZWrdbg@mail.gmail.com>
References: <CAMfKi3Kd9oP2nWe6boV4rNjVHAZ4eV=vwpnCAzh=6Gw-ZWrdbg@mail.gmail.com>
Message-ID: <ff88294596d84a978a510888ec9401b6@SRVEXCHCM1302.precheza.cz>

Hallo

I tried to use your code but after about 20 minutes it still worked so I gave 
up. After I kill it, there were some warnings

9: In rq.fit.sfn(D, y, rhs = a) :
  tiny diagonals replaced with Inf when calling blkfct

but I cannot decipher what it is about.

Your code is quite complicated and you hardly get any answer if you do not 
specify better where are the errors and what they tell, especially if others 
observe the same (long) calculation.

Use traceback() and/or debug() functions to see what is going on.
You also maybe could try to test your functions on some smaller data example.

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Muhammad
> Zubair Chishti
> Sent: Saturday, June 11, 2022 2:43 PM
> To: r-help at r-project.org; David Winsemius <dwinsemius at comcast.net>;
> lists at dewey.myzen.co.uk
> Subject: [R] A humble request regarding QAMG method in R
>
> Hi, Dear Respected Professors! I hope that you are doing well. Now all files
> are in .txt files. Now kindly help me with the following:
>
> I have the R-codes for the "Quantile Augmented Mean Group" method. The
> relevant codes and data are attached herewith.
> Note: The link to the reference paper is:
> https://www.econ.cam.ac.uk/people-files/emeritus/mhp1/fp20/qmg40-
> rev21.pdf
> My Issue:
> When I run the given codes to estimate the results for my own data. I have 
> to
> face several errors. I humbly request the experts to help me to estimate the
> results for my data.
> Thank you so much for your precious time.
>
> Regards
>
> Muhammad Zubair Chishti
> Ph.D. Student
> School of Business,
> Zhengzhou University, Henan, China.
> My Google scholar link:
> https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
> My ResearchGate link: https://www.researchgate.net/profile/Muhammad-
> Chishti

From h@n@tezer@ @end|ng |rom gm@||@com  Mon Jun 13 16:38:58 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Mon, 13 Jun 2022 17:38:58 +0300
Subject: [R] Changing sign of columns and values
In-Reply-To: <CAEuJ0VA6B+LJJRbZGp6D+HP33nmM9L+wdnPakVBg=-CxGkfEig@mail.gmail.com>
References: <CA+8X3fUB-vzBi0Jfr66tDUXQT4tctENg4=4sBvWxsiKBaEBUvg@mail.gmail.com>
 <62a5ca9c.1c69fb81.e10ce.4c1d@mx.google.com>
 <CAEuJ0VAqKdBpQJ=Bih=9V3=bDQBL05d=okpfRxSa0fqTK4EvBg@mail.gmail.com>
 <CA+8X3fWFE8zShP+depx-TeeFWZaYy=ySZpqDmcJkJwg95BTYcw@mail.gmail.com>
 <CAEuJ0VA6B+LJJRbZGp6D+HP33nmM9L+wdnPakVBg=-CxGkfEig@mail.gmail.com>
Message-ID: <CAEuJ0VB3zNoXae8Y4Dyi9o5Ou+crGn0XYg3ExD8ako144LDtQA@mail.gmail.com>

Dear, Jim I try the following codes but I got an error
Dfm1<-data.table(Dfm1)
# the names of the columns to be multiplied by BETA
ID_names <- paste0("ID",4:8)
# the names of the new columns that have been multiplied by beta
new_ID_names <- paste0("new_",ID_names)
##### Now with a simple command we get the desired multiplication
datfm[, (new_ID_names) := .SD *BETA, .SDcols = ID_names]
datfm
str(datfm)
##### The PRS for each individual can be calculated using the colSums
function in base R:
PRS<-Dfm1[, colSums(.SD), .SDcols = new_ID_names]
PRS

On 6/13/22, anteneh asmare <hanatezera at gmail.com> wrote:
> Dear Jim,  Good morning, hope you are doing very well, Here I want to
> calculate the specific parameters based on the previous data. I have
> attached below.
> can you help me  with r functions or code to solve the above equations
> Kind regards,
> Hana
>
>
> On 6/13/22, Jim Lemon <drjimlemon at gmail.com> wrote:
>> Hi Hana,
>> This is a bit more difficult, but the same basic steps apply. See the
>> comments for an explanation.
>> When you submit a problem like this, it is a lot easier if you send
>> the output from "dput" (e.g. dput(df1)) or set your data frames up
>> with "read.table" like I have done in the example below. It make it
>> lots easier for anyone who wants to reply to see what your dataset
>> looks like and input that dataset to devise a solution.
>> I have made some assumptions about marking the rows to be altered and
>> the arithmetic to do on those rows. What follows may not be as general
>> a solution as you want:
>>
>> df1<-read.table(text=
>>  "SNPID OA    EA
>>  snp001        C     A
>>  snp002        G     A
>>  snp003        C     A
>>  snp004        G     A
>>  snp005        C     T",
>>  header=TRUE,stringsAsFactors=FALSE)
>>
>> df2<-read.table(text=
>>  "SNPID   OA    EA   id00001 id00002 id00003 id00004 id00005
>>  snp001    A    C    1.01    2       0.97    1.97    1.99
>>  snp002    A    G    1.02    2       1       2       2
>>  snp003    C    A    1       1.03    2       0       1
>>  snp004    A    G    1.02    1.99    2       1.02    1.98
>>  snp005    C    T    1       0       1.01    1       1",
>>  header=TRUE,stringsAsFactors=FALSE)
>>
>> # get a logical vector of the different XA values
>> reversals<-df1$EA != df2$EA | df1$OA != df2$OA
>> reversals
>>
>> # set a variable to the maximum value of idxxxxx
>> # just to make the code easier to understand
>> maxid<-2
>>
>> # create a copy of df2
>> dfnew<-df2
>>
>> # now swap the XA columns and reflect the
>> # idxxxxxx values of the rows in which
>> # EA and OA are different between df1 and df2
>> # here I have looped through the rows
>> nrows<-dim(dfnew)[1]
>> idcols<-4:8
>> XAcols<-2:3
>> for(i in 1:nrows) {
>>  if(reversals[i]) {
>>   dfnew[i,XAcols]<-rev(dfnew[i,XAcols])
>>   dfnew[i,idcols]<-maxid-dfnew[i,idcols]
>>  }
>> }
>>
>> dfnew
>>
>> Jim
>>
>> On Mon, Jun 13, 2022 at 12:09 AM anteneh asmare <hanatezera at gmail.com>
>> wrote:
>>>
>>> Dear Jim, Morning I have the same issue regarding comparing and
>>> swapping the value of columns fro two different data frames.
>>> I have the following two data frames
>>> Data frame 1
>>> "SNPID" "OA"    "EA"
>>> "snp001"        "C"     "A"
>>> "snp002"        "G"     "A"
>>> "snp003"        "C"     "A"
>>> "snp004"        "G"     "A"
>>> "snp005"        "C"     "T"
>>>
>>> Data frame 2
>>> SNPID   OA      EA      id00001 id00002 id00003 id00004 id00005
>>> snp001     A    C        1.01                      2
>>> 0.97
>>>           1.97                    1.99
>>> snp002    A     G        1.02                     2
>>> 1
>>>          2                            2
>>> snp003    C     A         1                      1.03
>>>  2
>>>           0                            1
>>> snp004   A      G          1.02                    1.99
>>>   2
>>>              1.02                           1.98
>>> snp005  C       T            1                        0
>>>   1.01
>>>                     1                           1
>>>
>>> I want to  if  OA s and EAs in data frame 2 is the same as OAs and EAs
>>>  data frame 1 in such case I want to keep all information in data
>>> fram2 as it is . However if   OA s and EAs in data frame 2 is
>>> different from OAs and EAs  data frame 1, I want to change OA s and
>>> EAs in data frame 2 to  OAs and EAs  data frame 1 and   I want to
>>> redefine the values of the dosages (ids) of the variant (the dosage d
>>> for the i-th individual and the j-th variant would become d_ij
>>> new=2-dij.
>>> Dosage [j,]=2-dosage[j,]
>>> My desire data frame looks like
>>> Dataframe new
>>> SNPID"  "OA"       "EA"        id00001  id00002 id00003 id00004 id00005
>>> "snp001"        "C"     "A"     2-1.01            2- 2    2-    0.97
>>>     2-1.97
>>>     2-1.99
>>> "snp002"        "G"     "A"     2- 1.02              2- 2
>>>  2-1
>>>   2-2                     2- 2
>>> "snp003"        "C"     "A"       1                    1.03
>>>          2
>>>             0                          1
>>> "snp004"        "G"     "A"     2-1.02                   2-1.99
>>> 2-2                    2-1.02                    2-1.98
>>> "snp005"        "C"     "T"          1                        0
>>> 1.01                           1                                1
>>>  can you help me the r code for the above.
>>> Kind regards,
>>> Hana
>>>
>>>
>>> On 6/12/22, hanatezera <hanatezera at gmail.com> wrote:
>>> > Dear Jim, Thanks a lot this is exactly i am looking for.Stay safe and
>>> > blessed !Best,Hana
>>> > -------- Original message --------From: Jim Lemon
>>> > <drjimlemon at gmail.com>
>>> > Date: 6/12/22  6:43 AM  (GMT+03:00) To: hanatezera
>>> > <hanatezera at gmail.com>
>>> > Cc: r-help mailing list <r-help at r-project.org> Subject: Re: [R]
>>> > Changing
>>> > sign of columns and values Hi Hana,I didn't look closely. The simplest
>>> > rule
>>> > that I can see is that theletters (nucleotides?) should be swapped if
>>> > there
>>> > has been a signchange in the beta value.
>>> > Therefore:mydf<-read.table(text="IDnumber  OA  EA  beta1   C       A
>>> > -0.052   G        A        0.0983   G        T
>>> > -0.789",header=TRUE,stringsAsFactors=FALSE)# logical vector marking
>>> > the
>>> > rows
>>> > to be swappedswap_letters<-mydf$beta < 0# save the OA values to be
>>> > swappednewEA<-mydf$OA[swap_letters]# change the OAs to
>>> > EAsmydf$OA[swap_letters]<-mydf$EA[swap_letters]# set the relevant EA
>>> > values
>>> > to the old OA valuesmydf$EA[swap_letters]<-newEA# change the beta
>>> > valuesmydf$beta<-abs(mydf$beta)mydfJimOn Sun, Jun 12, 2022 at 9:11 AM
>>> > hanatezera <hanatezera at gmail.com> wrote:>> Dear jim thanks for your
>>> > help! I
>>> > want to change also the value of OA and EF simultaneously.> For
>>> > instance
>>> > i
>>> > am looking the data> mydf> IDnumber OA EA  beta> 1        1  A  C
>>> > 0.050>
>>> > 2        2  G  A 0.098> 3        3  T   G 0.789>> Best,> Hana>>>>
>>> > --------
>>> > Original message --------> From: Jim Lemon <drjimlemon at gmail.com>>
>>> > Date:
>>> > 6/12/22 1:59 AM (GMT+03:00)> To: hanatezera <hanatezera at gmail.com>,
>>> > r-help
>>> > mailing list <r-help at r-project.org>> Subject: Re: [R] Changing sign of
>>> > columns and values>> Hi Hana,> I think this is what you want:>> #
>>> > first
>>> > read
>>> > in your example> mydf<-read.table(text=> "IDnumber  OA  EA  beta> 1
>>> > C       A       -0.05> 2   G        A        0.098> 3   G        T
>>> > -0.789",> header=TRUE,stringsAsFactors=FALSE)> # check it> mydf>
>>> > IDnumber OA
>>> > EA   beta> 1        1  C  A -0.050> 2        2  G  A  0.098> 3
>>> > 3
>>> > G
>>> > T -0.789> # change values of mydf$beta to absolute values>
>>> > mydf$beta<-abs(mydf$beta)> mydf> IDnumber OA EA  beta> 1        1  C
>>> > A
>>> > 0.050> 2        2  G  A 0.098> 3        3  G  T 0.789>> Jim>> On Sun,
>>> > Jun
>>> > 12, 2022 at 8:27 AM hanatezera <hanatezera at gmail.com> wrote:> >> > I
>>> > have
>>> > the following data set in data frameIDnumber  OA  EA
>>> > beta1                         C       A       -0.052
>>> > G        A        0.0983                          G        T
>>> > -0.789....I want to change the sign of negative beta. If the negative
>>> > value
>>> > change to postive i want to switch its EA and OA.My desire result will
>>> > beIDnumber  OA  EA  beta1                         A      C
>>> > 0.052                         G        A
>>> > 0.0983                          T         G        0.789....Any one
>>> > can
>>> > help
>>> > me with r codes? kind regards,Hana> >         [[alternative HTML
>>> > version
>>> > deleted]]> >> > ______________________________________________> >
>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see> >
>>> > https://stat.ethz.ch/mailman/listinfo/r-help> > PLEASE do read the
>>> > posting
>>> > guide http://www.R-project.org/posting-guide.html> > and provide
>>> > commented,
>>> > minimal, self-contained, reproducible code.
>>
>


From h@n@tezer@ @end|ng |rom gm@||@com  Mon Jun 13 21:48:02 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Mon, 13 Jun 2022 22:48:02 +0300
Subject: [R] Combining Differnt columns in one single column
Message-ID: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>

Dear All, I have the following column name in different data frame and
different size
ID_names1 <- paste0("id0000",1:9)
ID_names2 <- paste0("id000",10:99)
ID_names3 <- paste0("id00",100:999)
ID_names4 <- paste0("id0",1000:9999)
ID_names5 <- paste0("id",10000:50000)
Dose it possible to combine in to a single column in r?
Best,
Hana


From tebert @end|ng |rom u||@edu  Mon Jun 13 22:21:26 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Mon, 13 Jun 2022 20:21:26 +0000
Subject: [R] Combining Differnt columns in one single column
In-Reply-To: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>
References: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>
Message-ID: <BN6PR2201MB1553CED7003E70B0A65F2E42CFAB9@BN6PR2201MB1553.namprd22.prod.outlook.com>

ID5 <- as.data.frame(ID_names1)
colnames(ID5)<-"val" #rename the columns so they have the same name.
ID6 <- as.data.frame(ID_names2)
colnames(ID6)<-"val"
rbind(ID5, ID6)

This works for the first two, just keep doing the same thing for the others.
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of anteneh asmare
Sent: Monday, June 13, 2022 3:48 PM
To: r-help at r-project.org
Subject: [R] Combining Differnt columns in one single column

[External Email]

Dear All, I have the following column name in different data frame and different size
ID_names1 <- paste0("id0000",1:9)
ID_names2 <- paste0("id000",10:99)
ID_names3 <- paste0("id00",100:999)
ID_names4 <- paste0("id0",1000:9999)
ID_names5 <- paste0("id",10000:50000)
Dose it possible to combine in to a single column in r?
Best,
Hana

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=cjCY0UmRewiKkMVVxNu2riO8k4tZHD7s7R3R6SC-bwEM5u77LyExCSIzwI-q7Xu_&s=xrZArAFWnJf7ja13EEgxwBj-pMBEXBYSQgc6BhtNPAA&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=cjCY0UmRewiKkMVVxNu2riO8k4tZHD7s7R3R6SC-bwEM5u77LyExCSIzwI-q7Xu_&s=oUlYLeH_EF5rwea10resAheSjjvJ6nhUl8wT84OVXZM&e=
and provide commented, minimal, self-contained, reproducible code.


From h@n@tezer@ @end|ng |rom gm@||@com  Mon Jun 13 22:48:32 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Mon, 13 Jun 2022 23:48:32 +0300
Subject: [R] Combining Differnt columns in one single column
In-Reply-To: <BN6PR2201MB1553CED7003E70B0A65F2E42CFAB9@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>
 <BN6PR2201MB1553CED7003E70B0A65F2E42CFAB9@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <CAEuJ0VB2qa+gM3w=RrW9SrS7eN+Njf6GB-Yz6s++zwUo7Obu7w@mail.gmail.com>

Dear Tim it works, But i want to treat them as a column name. Is it
possible to treat as a column name ?

Best,
Hana
On 6/13/22, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> ID5 <- as.data.frame(ID_names1)
> colnames(ID5)<-"val" #rename the columns so they have the same name.
> ID6 <- as.data.frame(ID_names2)
> colnames(ID6)<-"val"
> rbind(ID5, ID6)
>
> This works for the first two, just keep doing the same thing for the
> others.
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of anteneh asmare
> Sent: Monday, June 13, 2022 3:48 PM
> To: r-help at r-project.org
> Subject: [R] Combining Differnt columns in one single column
>
> [External Email]
>
> Dear All, I have the following column name in different data frame and
> different size
> ID_names1 <- paste0("id0000",1:9)
> ID_names2 <- paste0("id000",10:99)
> ID_names3 <- paste0("id00",100:999)
> ID_names4 <- paste0("id0",1000:9999)
> ID_names5 <- paste0("id",10000:50000)
> Dose it possible to combine in to a single column in r?
> Best,
> Hana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=cjCY0UmRewiKkMVVxNu2riO8k4tZHD7s7R3R6SC-bwEM5u77LyExCSIzwI-q7Xu_&s=xrZArAFWnJf7ja13EEgxwBj-pMBEXBYSQgc6BhtNPAA&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=cjCY0UmRewiKkMVVxNu2riO8k4tZHD7s7R3R6SC-bwEM5u77LyExCSIzwI-q7Xu_&s=oUlYLeH_EF5rwea10resAheSjjvJ6nhUl8wT84OVXZM&e=
> and provide commented, minimal, self-contained, reproducible code.
>


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon Jun 13 23:12:53 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 13 Jun 2022 22:12:53 +0100
Subject: [R] Combining Differnt columns in one single column
In-Reply-To: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>
References: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>
Message-ID: <c9f6cef7-b932-b5bf-6396-6a73f9dbd7fe@sapo.pt>

Hello,

Are you looking for this?


ID_names <- sprintf("id%05d", 1:50000)
head(ID_names)
#> [1] "id00001" "id00002" "id00003" "id00004" "id00005" "id00006"
tail(ID_names)
#> [1] "id49995" "id49996" "id49997" "id49998" "id49999" "id50000"


sprintf with the format %05d pads the integers with zeros to length 5.
And you won't need to combine the 5 vectors, it already is only one.

Hope this helps,

Rui Barradas

?s 20:48 de 13/06/2022, anteneh asmare escreveu:
> Dear All, I have the following column name in different data frame and
> different size
> ID_names1 <- paste0("id0000",1:9)
> ID_names2 <- paste0("id000",10:99)
> ID_names3 <- paste0("id00",100:999)
> ID_names4 <- paste0("id0",1000:9999)
> ID_names5 <- paste0("id",10000:50000)
> Dose it possible to combine in to a single column in r?
> Best,
> Hana
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From h@n@tezer@ @end|ng |rom gm@||@com  Mon Jun 13 23:38:08 2022
From: h@n@tezer@ @end|ng |rom gm@||@com (anteneh asmare)
Date: Tue, 14 Jun 2022 00:38:08 +0300
Subject: [R] Combining Differnt columns in one single column
In-Reply-To: <c9f6cef7-b932-b5bf-6396-6a73f9dbd7fe@sapo.pt>
References: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>
 <c9f6cef7-b932-b5bf-6396-6a73f9dbd7fe@sapo.pt>
Message-ID: <CAEuJ0VBft360793LRG_G6hw_U6qTOP9M_CF4c=rADNZcwsEDYA@mail.gmail.com>

Dear Rui,  Thanks a lot! This exactly what I was looking for it.
Kind regads,
Hana

On 6/14/22, Rui Barradas <ruipbarradas at sapo.pt> wrote:
> Hello,
>
> Are you looking for this?
>
>
> ID_names <- sprintf("id%05d", 1:50000)
> head(ID_names)
> #> [1] "id00001" "id00002" "id00003" "id00004" "id00005" "id00006"
> tail(ID_names)
> #> [1] "id49995" "id49996" "id49997" "id49998" "id49999" "id50000"
>
>
> sprintf with the format %05d pads the integers with zeros to length 5.
> And you won't need to combine the 5 vectors, it already is only one.
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 20:48 de 13/06/2022, anteneh asmare escreveu:
>> Dear All, I have the following column name in different data frame and
>> different size
>> ID_names1 <- paste0("id0000",1:9)
>> ID_names2 <- paste0("id000",10:99)
>> ID_names3 <- paste0("id00",100:999)
>> ID_names4 <- paste0("id0",1000:9999)
>> ID_names5 <- paste0("id",10000:50000)
>> Dose it possible to combine in to a single column in r?
>> Best,
>> Hana
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From drj|m|emon @end|ng |rom gm@||@com  Tue Jun 14 00:26:11 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Tue, 14 Jun 2022 08:26:11 +1000
Subject: [R] Changing sign of columns and values
In-Reply-To: <CAEuJ0VB3zNoXae8Y4Dyi9o5Ou+crGn0XYg3ExD8ako144LDtQA@mail.gmail.com>
References: <CA+8X3fUB-vzBi0Jfr66tDUXQT4tctENg4=4sBvWxsiKBaEBUvg@mail.gmail.com>
 <62a5ca9c.1c69fb81.e10ce.4c1d@mx.google.com>
 <CAEuJ0VAqKdBpQJ=Bih=9V3=bDQBL05d=okpfRxSa0fqTK4EvBg@mail.gmail.com>
 <CA+8X3fWFE8zShP+depx-TeeFWZaYy=ySZpqDmcJkJwg95BTYcw@mail.gmail.com>
 <CAEuJ0VA6B+LJJRbZGp6D+HP33nmM9L+wdnPakVBg=-CxGkfEig@mail.gmail.com>
 <CAEuJ0VB3zNoXae8Y4Dyi9o5Ou+crGn0XYg3ExD8ako144LDtQA@mail.gmail.com>
Message-ID: <CA+8X3fWzwF-U0xYX_wP4_AzhbOc6RCaX9Rw3BabL6z7BXz5z4g@mail.gmail.com>

Hi Hana,
The reason that I didn't answer your last request was that I couldn't
understand it. You attached a spreadsheet that had been pasted
together from other documents. Point 1 promised a formula for
calculating PRS, but didn't show it. Point 2 was similarly lacking a
symbolic formula that could be translated into R code. It was clear to
me that the original documents that had been cut and pasted into the
spreadsheet looked very much like homework assignments. As others have
told you, we avoid doing homework for posters.

Jim

On Tue, Jun 14, 2022 at 12:38 AM anteneh asmare <hanatezera at gmail.com> wrote:
>
> Dear, Jim I try the following codes but I got an error
> Dfm1<-data.table(Dfm1)
> # the names of the columns to be multiplied by BETA
> ID_names <- paste0("ID",4:8)
> # the names of the new columns that have been multiplied by beta
> new_ID_names <- paste0("new_",ID_names)
> ##### Now with a simple command we get the desired multiplication
> datfm[, (new_ID_names) := .SD *BETA, .SDcols = ID_names]
> datfm
> str(datfm)
> ##### The PRS for each individual can be calculated using the colSums
> function in base R:
> PRS<-Dfm1[, colSums(.SD), .SDcols = new_ID_names]
> PRS
>


From tebert @end|ng |rom u||@edu  Tue Jun 14 02:31:54 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 14 Jun 2022 00:31:54 +0000
Subject: [R] Combining Differnt columns in one single column
In-Reply-To: <CAEuJ0VB2qa+gM3w=RrW9SrS7eN+Njf6GB-Yz6s++zwUo7Obu7w@mail.gmail.com>
References: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>
 <BN6PR2201MB1553CED7003E70B0A65F2E42CFAB9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAEuJ0VB2qa+gM3w=RrW9SrS7eN+Njf6GB-Yz6s++zwUo7Obu7w@mail.gmail.com>
Message-ID: <BN6PR2201MB1553A5FB7E83717DE7AACE2ECFAA9@BN6PR2201MB1553.namprd22.prod.outlook.com>

Yes, It just takes another step. Here is the new pattern:
ID_names1 <- paste0("id0000",1:9)
ID_names2 <- paste0("id000",10:99)
ID5 <-as.data.frame(ID_names1) #convert to a dataframe.
ID5$name<-colnames(ID5) #make a new variable from column name.
colnames(ID5)<-c("name","val") #rename columns to be consistent for all dataframes.
ID6 <-as.data.frame(ID_names2)
ID6$name<-colnames(ID6)
colnames(ID6)<-c("name","val")
IDall<-rbind(ID5,ID6) 


Tim

-----Original Message-----
From: anteneh asmare <hanatezera at gmail.com> 
Sent: Monday, June 13, 2022 4:49 PM
To: Ebert,Timothy Aaron <tebert at ufl.edu>
Cc: r-help at r-project.org
Subject: Re: [R] Combining Differnt columns in one single column

[External Email]

Dear Tim it works, But i want to treat them as a column name. Is it possible to treat as a column name ?

Best,
Hana
On 6/13/22, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> ID5 <- as.data.frame(ID_names1)
> colnames(ID5)<-"val" #rename the columns so they have the same name.
> ID6 <- as.data.frame(ID_names2)
> colnames(ID6)<-"val"
> rbind(ID5, ID6)
>
> This works for the first two, just keep doing the same thing for the 
> others.
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of anteneh 
> asmare
> Sent: Monday, June 13, 2022 3:48 PM
> To: r-help at r-project.org
> Subject: [R] Combining Differnt columns in one single column
>
> [External Email]
>
> Dear All, I have the following column name in different data frame and 
> different size
> ID_names1 <- paste0("id0000",1:9)
> ID_names2 <- paste0("id000",10:99)
> ID_names3 <- paste0("id00",100:999)
> ID_names4 <- paste0("id0",1000:9999)
> ID_names5 <- paste0("id",10000:50000)
> Dose it possible to combine in to a single column in r?
> Best,
> Hana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=cjCY0UmRewiKkMVVxNu2riO8k4tZHD7s7R3R6SC-bwEM5u77LyExCSIzw
> I-q7Xu_&s=xrZArAFWnJf7ja13EEgxwBj-pMBEXBYSQgc6BhtNPAA&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=cjCY0UmRewiKkMVVxNu2riO8k4tZHD7s7R3R6SC-bwEM5u77LyExCSIz
> wI-q7Xu_&s=oUlYLeH_EF5rwea10resAheSjjvJ6nhUl8wT84OVXZM&e=
> and provide commented, minimal, self-contained, reproducible code.
>

From tebert @end|ng |rom u||@edu  Tue Jun 14 02:43:26 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 14 Jun 2022 00:43:26 +0000
Subject: [R] Combining Differnt columns in one single column
In-Reply-To: <CAEuJ0VB2qa+gM3w=RrW9SrS7eN+Njf6GB-Yz6s++zwUo7Obu7w@mail.gmail.com>
References: <CAEuJ0VAH-gb4mWPOCPwL3hwZrmFGSJRE-3dGJyuL3u07vPWFjg@mail.gmail.com>
 <BN6PR2201MB1553CED7003E70B0A65F2E42CFAB9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAEuJ0VB2qa+gM3w=RrW9SrS7eN+Njf6GB-Yz6s++zwUo7Obu7w@mail.gmail.com>
Message-ID: <BN6PR2201MB1553CBBF7B2C5BB4B31CF6C6CFAA9@BN6PR2201MB1553.namprd22.prod.outlook.com>

Sorry, the last post had a small mistake. I got name and val switched. Here they are in the correct order.

ID_names1 <- paste0("id0000",1:9)
ID_names2 <- paste0("id000",10:99)
ID5 <-as.data.frame(ID_names1)
ID5$name<-colnames(ID5)
colnames(ID5)<-c("val","name")
ID6 <-as.data.frame(ID_names2)
ID6$name<-colnames(ID6)
colnames(ID6)<-c("val","name")
IDall<-rbind(ID5,ID6)


Tim
-----Original Message-----
From: anteneh asmare <hanatezera at gmail.com> 
Sent: Monday, June 13, 2022 4:49 PM
To: Ebert,Timothy Aaron <tebert at ufl.edu>
Cc: r-help at r-project.org
Subject: Re: [R] Combining Differnt columns in one single column

[External Email]

Dear Tim it works, But i want to treat them as a column name. Is it possible to treat as a column name ?

Best,
Hana
On 6/13/22, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> ID5 <- as.data.frame(ID_names1)
> colnames(ID5)<-"val" #rename the columns so they have the same name.
> ID6 <- as.data.frame(ID_names2)
> colnames(ID6)<-"val"
> rbind(ID5, ID6)
>
> This works for the first two, just keep doing the same thing for the 
> others.
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of anteneh 
> asmare
> Sent: Monday, June 13, 2022 3:48 PM
> To: r-help at r-project.org
> Subject: [R] Combining Differnt columns in one single column
>
> [External Email]
>
> Dear All, I have the following column name in different data frame and 
> different size
> ID_names1 <- paste0("id0000",1:9)
> ID_names2 <- paste0("id000",10:99)
> ID_names3 <- paste0("id00",100:999)
> ID_names4 <- paste0("id0",1000:9999)
> ID_names5 <- paste0("id",10000:50000)
> Dose it possible to combine in to a single column in r?
> Best,
> Hana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=cjCY0UmRewiKkMVVxNu2riO8k4tZHD7s7R3R6SC-bwEM5u77LyExCSIzw
> I-q7Xu_&s=xrZArAFWnJf7ja13EEgxwBj-pMBEXBYSQgc6BhtNPAA&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=cjCY0UmRewiKkMVVxNu2riO8k4tZHD7s7R3R6SC-bwEM5u77LyExCSIz
> wI-q7Xu_&s=oUlYLeH_EF5rwea10resAheSjjvJ6nhUl8wT84OVXZM&e=
> and provide commented, minimal, self-contained, reproducible code.
>

From @tephen@berm@n @end|ng |rom gmx@net  Mon Jun 13 23:47:29 2022
From: @tephen@berm@n @end|ng |rom gmx@net (Stephen Berman)
Date: Mon, 13 Jun 2022 23:47:29 +0200
Subject: [R] Why does qt() return Inf with certain negative ncp values?
Message-ID: <874k0oxmku.fsf@rub.de>

Can anyone explain why Inf appears in the following results?

> sapply(-1:-10, \(ncp) qt(1-1*(10^(-4+ncp)), 35, ncp))
 [1]  3.6527153  3.0627759  2.4158355  1.7380812  1.0506904  0.3700821
 [7]        Inf -0.9279783 -1.5341759 -2.1085213

> sapply(seq(-6.9, -7.9, -0.1), \(ncp) qt(1-1*(10^(-4+ncp)), 35, ncp))
 [1] -0.2268386        Inf        Inf        Inf -0.4857400 -0.5497784
 [7] -0.6135402 -0.6770143 -0.7401974 -0.8030853 -0.8656810

In case it matters:

> sessionInfo()
R Under development (unstable) (2022-06-05 r82452)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Linux From Scratch r11.0-165

Matrix products: default
BLAS:   /usr/lib/R/lib/libRblas.so
LAPACK: /usr/lib/R/lib/libRlapack.so

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_4.3.0 tools_4.3.0

Thanks.
Steve Berman


