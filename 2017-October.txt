From jrkrideau at yahoo.ca  Sun Oct  1 01:22:22 2017
From: jrkrideau at yahoo.ca (John Kane)
Date: Sat, 30 Sep 2017 23:22:22 +0000 (UTC)
Subject: [R] Converting SAS Code
In-Reply-To: <6ae9e890-dfe0-5028-a956-0e1d8ea0eab9@auckland.ac.nz>
References: <CAB4=Ug=V22G+J2bgEwf0Cd0LnKnb2DKCa5K47=sdk5eE8jtZgw@mail.gmail.com>
 <872874c5-21e4-47f0-5627-96f1a3c129cc@dewey.myzen.co.uk>
 <e529f831-92b1-5cf4-8d33-ba41da80f173@utoronto.ca>
 <CAGxFJbTa_4jCdhNVZ1SP3W_N_5Pgx_e6VUc7bWxiND_zGeXp2Q@mail.gmail.com>
 <OF3C6E5C21.F89996C3-ON852581AA.0066237F-852581AA.006711B9@ria.buffalo.edu>
 <e63c816a-d27a-6ecd-29e6-ea79d7fa34b3@auckland.ac.nz>
 <598ed67a-d3c0-5cb3-e242-bc04948919eb@atsu.edu>
 <6ae9e890-dfe0-5028-a956-0e1d8ea0eab9@auckland.ac.nz>
Message-ID: <1035925962.448097.1506813742569@mail.yahoo.com>

And appropriatesly
> library(fortunes) > fortune()

SAS seems to be to statistical computing what Microsoft is to personal
computing.
?? -- Bill Venables
????? 'Exegeses on Linear Models' paper (May 2000) 

    On Saturday, September 30, 2017, 4:57:23 PM EDT, Rolf Turner <r.turner at auckland.ac.nz> wrote:  
 
 On 01/10/17 01:22, Robert Baer wrote:
> 
> 
> On 9/29/2017 3:37 PM, Rolf Turner wrote:
>> On 30/09/17 07:45, JLucke at ria.buffalo.edu wrote:
>>
>> <SNIP>
>>
>>>
>>> The conceptual paradigm for R is only marginally commensurate with 
>>> that of
>>> standard statistical software.
>>> You must immerse yourself in R to become proficient.
>>
>> Fortune nomination.
> For newer list members wondering what Rolf is talking about try:
> 
> library(fortunes) fortune() to get a flavor! There are many pearls of 
> wisdom.

For the *really* new members, note that you may have to *install* the 
fortunes package first:

 > install.packages("fortunes",lib=<wherever you keep added-on packages>)

Note that the package is "fortunes" (with an "s") but the function is
fortune() (no "s").

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jrkrideau at yahoo.ca  Sun Oct  1 12:44:03 2017
From: jrkrideau at yahoo.ca (John Kane)
Date: Sun, 1 Oct 2017 10:44:03 +0000 (UTC)
Subject: [R] Adding non-data line to legend ggplot2 Maximum Contaminant
 Level
In-Reply-To: <CACftpvrcJea_P-3VqBS5c7s5Mxk=BfSREOia3TFk6ZtAN92sDA@mail.gmail.com>
References: <CACftpvrcJea_P-3VqBS5c7s5Mxk=BfSREOia3TFk6ZtAN92sDA@mail.gmail.com>
Message-ID: <1288671895.571249.1506854643259@mail.yahoo.com>

I just glanced at the problem but I think you would have to create a new variable to replace the hline. What about adding some text annotation in the graph instead?
 

    On Tuesday, September 26, 2017, 3:51:46 PM EDT, David Doyle <kydaviddoyle at gmail.com> wrote:  
 
 Hello everyone,

I have a plot showing chloride concentrations for various point over time.
I also have a dotted line that show the Secondary Maximum Contaminant Level
(my screening limit) on the graphs at 250 mg/L.? But I can not figure out
how to include the dotted line / Secondary Maximum Contaminant Level in
the legend.? Any thoughts?? My code is as following and is linked to my
data on the net.

Thank you in advance
David


#Loads the ggplot2 package.
library(ggplot2)

##This loads your data from your worksheet
MyData <-read.csv("http://doylesdartden.com/Stats/TimeSeriesExample.csv",
sep=",")


#Sets which are detections and nondetects

MyData$Detections <- ifelse(MyData$D_Chloride ==1, "Detected", "NonDetect")

#does the plot
p <- ggplot(data = MyData, aes(x=Year, y=Chloride , col=Detections)) +
? geom_point(aes(shape=Detections)) +

? #sets the detect vs. non-detect colors
? scale_colour_manual(values=c("black","red")) +

? #sets the y scale and log base 10
? scale_y_log10() +

? ##adds line
? geom_hline(aes(yintercept=250),linetype="dashed")+

? #location of the legend
? theme(legend.position=c("right")) +

? #sets the line color, type and size
? geom_line(colour="black", linetype="dotted", size=0.5) +
? ylab("Chloride (mg/L)")

## does the graph using the Location IDs as the different Locations.
p + facet_grid(Location ~ .)

??? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
  
	[[alternative HTML version deleted]]


From nyinyi at iuj.ac.jp  Mon Oct  2 04:13:41 2017
From: nyinyi at iuj.ac.jp (NYI NYI HTWE)
Date: Mon, 2 Oct 2017 11:13:41 +0900
Subject: [R] Fwd: R errors
In-Reply-To: <CAGkGp6QOB77YcjRdbPaGgVHj5AkNCA5EROyA0aSwa+C-q6VAPA@mail.gmail.com>
References: <CAGkGp6QOB77YcjRdbPaGgVHj5AkNCA5EROyA0aSwa+C-q6VAPA@mail.gmail.com>
Message-ID: <CAGkGp6QZ1ceOEGbw-08ndi13FXMWBjnTo-eC-tGFLKJ1anc2GA@mail.gmail.com>

---------- Forwarded message ----------
From: NYI NYI HTWE <nyinyi at iuj.ac.jp>
Date: Sun, Oct 1, 2017 at 1:49 PM
Subject: R errors
To: R-windows at r-project.org


Dear

I'm very new R-user.
I run a model successfully and my supervisor ask me to confirm the result.
Then I run the same model again. However, it returns the error message in
printing report and model creating as below which I haven't seen in my
first attempt. I tried several times, but the same error returned.
I removed R from my computer and re-install 2 times, but the same thing
happened.

Please let me know how to proceed.


Error in print01Report(TAbehdata, TAeff, modelname =
"TA_coev_Second_Attempt") :
  wrong parameters; note: do not include an effects object as parameter!

Error in sienaModelCreate(projname = "TA_coev_Second_Attempt", modelType =
3,  :
  modelType must have names of dependent networks


Regards

Nyi Nyi
Graduate School for International Development and Cooperation
Hiroshima University

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Mon Oct  2 11:24:47 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Mon, 2 Oct 2017 09:24:47 +0000
Subject: [R] Fwd: R errors
In-Reply-To: <CAGkGp6QZ1ceOEGbw-08ndi13FXMWBjnTo-eC-tGFLKJ1anc2GA@mail.gmail.com>
References: <CAGkGp6QOB77YcjRdbPaGgVHj5AkNCA5EROyA0aSwa+C-q6VAPA@mail.gmail.com>
 <CAGkGp6QZ1ceOEGbw-08ndi13FXMWBjnTo-eC-tGFLKJ1anc2GA@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB1677@SRVEXCHCM301.precheza.cz>

Hi

I wonder if you will get any sensible answer. You did not specify used package or function, you did not show your data nor revealed used R version and OS.

All this is probably necessary unless the smartest members of this help list discovered efficient crystal ball.

Wild guess is that something is different with your data from your first attempt.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of NYI NYI
> HTWE
> Sent: Monday, October 2, 2017 4:14 AM
> To: r-help at r-project.org
> Subject: [R] Fwd: R errors
>
> ---------- Forwarded message ----------
> From: NYI NYI HTWE <nyinyi at iuj.ac.jp>
> Date: Sun, Oct 1, 2017 at 1:49 PM
> Subject: R errors
> To: R-windows at r-project.org
>
>
> Dear
>
> I'm very new R-user.
> I run a model successfully and my supervisor ask me to confirm the result.
> Then I run the same model again. However, it returns the error message in
> printing report and model creating as below which I haven't seen in my first
> attempt. I tried several times, but the same error returned.
> I removed R from my computer and re-install 2 times, but the same thing
> happened.
>
> Please let me know how to proceed.
>
>
> Error in print01Report(TAbehdata, TAeff, modelname =
> "TA_coev_Second_Attempt") :
>   wrong parameters; note: do not include an effects object as parameter!
>
> Error in sienaModelCreate(projname = "TA_coev_Second_Attempt", modelType
> = 3,  :
>   modelType must have names of dependent networks
>
>
> Regards
>
> Nyi Nyi
> Graduate School for International Development and Cooperation Hiroshima
> University
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From lists at dewey.myzen.co.uk  Mon Oct  2 11:27:31 2017
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Mon, 2 Oct 2017 10:27:31 +0100
Subject: [R] Fwd: R errors
In-Reply-To: <CAGkGp6QZ1ceOEGbw-08ndi13FXMWBjnTo-eC-tGFLKJ1anc2GA@mail.gmail.com>
References: <CAGkGp6QOB77YcjRdbPaGgVHj5AkNCA5EROyA0aSwa+C-q6VAPA@mail.gmail.com>
 <CAGkGp6QZ1ceOEGbw-08ndi13FXMWBjnTo-eC-tGFLKJ1anc2GA@mail.gmail.com>
Message-ID: <0f74b964-4000-9905-7dda-97564833873b@dewey.myzen.co.uk>

You need to give us some more information. What package(s) are you 
using? What are TAbehdata, TAeff, TA_coev_Second_Attempt? It might help 
to show us the output of sessionInfo too.

It is better to post in plain text not HTML as HTML can end up mangled 
so we do not see what you thought you sent.

Michael

On 02/10/2017 03:13, NYI NYI HTWE wrote:
> ---------- Forwarded message ----------
> From: NYI NYI HTWE <nyinyi at iuj.ac.jp>
> Date: Sun, Oct 1, 2017 at 1:49 PM
> Subject: R errors
> To: R-windows at r-project.org
> 
> 
> Dear
> 
> I'm very new R-user.
> I run a model successfully and my supervisor ask me to confirm the result.
> Then I run the same model again. However, it returns the error message in
> printing report and model creating as below which I haven't seen in my
> first attempt. I tried several times, but the same error returned.
> I removed R from my computer and re-install 2 times, but the same thing
> happened.
> 
> Please let me know how to proceed.
> 
> 
> Error in print01Report(TAbehdata, TAeff, modelname =
> "TA_coev_Second_Attempt") :
>    wrong parameters; note: do not include an effects object as parameter!
> 
> Error in sienaModelCreate(projname = "TA_coev_Second_Attempt", modelType =
> 3,  :
>    modelType must have names of dependent networks
> 
> 
> Regards
> 
> Nyi Nyi
> Graduate School for International Development and Cooperation
> Hiroshima University
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ---
> This email has been checked for viruses by AVG.
> http://www.avg.com
> 
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From dasolexa at hotmail.com  Mon Oct  2 11:05:42 2017
From: dasolexa at hotmail.com (David)
Date: Mon, 2 Oct 2017 09:05:42 +0000
Subject: [R] Help on adding a negative binomial density plot
Message-ID: <HE1PR07MB077814B968386868751EE71BC67D0@HE1PR07MB0778.eurprd07.prod.outlook.com>

Dear list,


I am just starting on analysis of count data in R 3.4.0. My dataset was obtained from counting particles on a surface before andd after a cleaning process. The sampling positions on the surface are pre-defined and are the same before and after cleaning.  I have ~20% of 0's. I want to know if the cleaning process was useful at reducing the number of particles.


I first fit a negative binomial model using


> nbFit<-glmer.nb(Count ~ Cleaning + (1|Sampling_point) , data = myCountDB)



I now would like to add a curve to the histogram representing the negative binomial density function fitted to my data using


> curve(dnbinom(x=, size=, prob=, mu=), add=TRUE)


But I am struggling defining the arguments to dnbinom.


Using the str() function on the nbFit object I see there are many fields returned. And I get lost reading the ?glmer.nb help, greatly because of my lack of knowledge. Which ones should I use?


Thanks ever so much for your valuable help


Dave

	[[alternative HTML version deleted]]


From ashimkapoor at gmail.com  Mon Oct  2 11:48:26 2017
From: ashimkapoor at gmail.com (Ashim Kapoor)
Date: Mon, 2 Oct 2017 15:18:26 +0530
Subject: [R] Default value of the option initial in the ses function in the
 forecast package.
Message-ID: <CAC8=1err3ceBdMjKXwWaeRUogZnnR2_yXfeQNw9EKkRGvzU7_Q@mail.gmail.com>

Dear All,

I am trying to use the function ses from the forecast package.

>From its help I have :

Usage:

     ses(y, h = 10, level = c(80, 95), fan = FALSE, initial = c("optimal",
       "simple"), alpha = NULL, lambda = NULL, biasadj = FALSE, x = y, ...)

My query is that if I do not mention the initial value will its default
value be "optimal".

A MWE would be  :

library(fpp)
oildata <- window(oil,start=1996,end=2007)
fit3 <- ses(oildata, h=3)

In the above is the default value of initial "optimal" or "simple" ?

Note I have taken this example from https://www.otexts.org/fpp/7/1

Best Regards,
Ashim

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Mon Oct  2 12:00:42 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 2 Oct 2017 12:00:42 +0200
Subject: [R] Default value of the option initial in the ses function in
 the forecast package.
In-Reply-To: <CAC8=1err3ceBdMjKXwWaeRUogZnnR2_yXfeQNw9EKkRGvzU7_Q@mail.gmail.com>
References: <CAC8=1err3ceBdMjKXwWaeRUogZnnR2_yXfeQNw9EKkRGvzU7_Q@mail.gmail.com>
Message-ID: <755867D7-46E1-476D-B734-3B097FD83632@gmail.com>

The first one, i.e. "optimal"; check help for match.arg() for the idiom.

-pd


> On 2 Oct 2017, at 11:48 , Ashim Kapoor <ashimkapoor at gmail.com> wrote:
> 
> Dear All,
> 
> I am trying to use the function ses from the forecast package.
> 
> From its help I have :
> 
> Usage:
> 
>     ses(y, h = 10, level = c(80, 95), fan = FALSE, initial = c("optimal",
>       "simple"), alpha = NULL, lambda = NULL, biasadj = FALSE, x = y, ...)
> 
> My query is that if I do not mention the initial value will its default
> value be "optimal".
> 
> A MWE would be  :
> 
> library(fpp)
> oildata <- window(oil,start=1996,end=2007)
> fit3 <- ses(oildata, h=3)
> 
> In the above is the default value of initial "optimal" or "simple" ?
> 
> Note I have taken this example from https://www.otexts.org/fpp/7/1
> 
> Best Regards,
> Ashim
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From lucam1968 at gmail.com  Mon Oct  2 13:37:04 2017
From: lucam1968 at gmail.com (Luca Meyer)
Date: Mon, 2 Oct 2017 13:37:04 +0200
Subject: [R] R and Supervised learning
Message-ID: <CABQyo85knseVLNUJeHKE4BZTkJxdeKyj_wHtYvRfs-84JqpaRQ@mail.gmail.com>

Hi,

I am currently find myself selecting manually amoungts several hundreds
Google Alerts (GA) texts those that are indeed relevant for my research vs
those which are not (despite they are triggered by some relevant seach
keywords).

Basically each week I get several hundreds GA email such as:

https://www.dropbox.com/s/u7rp0ez1tamq001/Alerte%20Google%C2%A0-%20laitier%20-%20lucam1968%40gmail.com%20-%20Gmail.pdf?dl=0

and

https://www.dropbox.com/s/1ubx5enw6tc90hj/Google%20Alert%20-%20latte%20-%20lucam1968%40gmail.com%20-%20Gmail.pdf?dl=0

>From such emails I create a file such as:

https://www.dropbox.com/s/y5yqcsxp1zcmnhc/test_sample.xlsx?dl=0

And this is really becoming a time consuming procedure, hence my decision
to try appling artificial intelligence solutions to such a case.

What I would really need are 2 separate steps:

(1) A procedure that reads the GA email and creates a file such as the
excel I have shared here (only first 3 columns)

(2) Some sort of supervised learning algorithm that can learn by example
from my choices and decide on my behalf (see column 4 in the attached
file). That is: taking the output from step (1) above I can classify a few
hundreds cases and then let the algorithm learn and classify
future/additional data. I plan to regularly review such a classification,
correct missclassifications and train the algorithm again with the
objective to improve its ability to correctly classify the GA texts.

Is my explanation clear enought? Can all the above be done within R? If so,
is there any package/procedure I should be using?

Thank you in advance for any suggestion you might have.

Luca

	[[alternative HTML version deleted]]


From h.a.johnson at newcastle.ac.uk  Mon Oct  2 14:16:25 2017
From: h.a.johnson at newcastle.ac.uk (Hollie Johnson (PGR))
Date: Mon, 2 Oct 2017 12:16:25 +0000
Subject: [R] Issues with 'Miwa' algorithm in mvtnorm package
References: <DB6PR0701MB2245CF41FB255EB0FDB7A454EE7D0@DB6PR0701MB2245.eurprd07.prod.outlook.com>
Message-ID: <WM!997a3997c1a6173f92e97be9c81aa7ea5353a047ec37382b20ac366ead8132f12986cf2ab90a4830a9fcd6e847d2f45a!@mailhub-mx2.ncl.ac.uk>

Currently doing some work on local maxima on a random field and have encountered an issue with the Miwa algorithm used with the pmvnorm function in the mvtnorm R package.

Based on recommendations by Mi et al., we ran the mvtnorm package using the Miwa algorithm, since we have a maximum of 4 dimensions with non-singular matrices. However, running the estimation procedure in this way, we obtained inconsistent results. For example, using matrices A and B, it is clear to see that matrix B is the results of exchanging position 1 with position 3 in matrix A.

A =
 9.358*10^-3 -8.165*10^-3 -1.689*10^-8
-8.165*10^-3   9.358*10^-3   1.854*10^-8
-1.689*10^-8   1.854*10^-8   9.358*10^-3

B =
 9.358*10^-3 1.854*10^-8 -1.689*10^-8
 1.854*10^-8 9.358*10^-3 -8.165*10^-3
-1.689*10^-8 -8.165*10^-3 9.358*10^-3

The determinants of both matrices are small but equal, so we would expect any issues arising from the matrix being 'close' to singular to be apparent in both cases. The table below shows the output from pmvnorm calculated using the two matrices A and B, and the two different algorithms, Miwa and GenzBretz using the code below:

pmvnorm(
      mean = rep(0, 3),
      lower = rep(-Inf, 3),
      upper = rep(0, 3),
      sigma =  A,
      algorithm = 'Miwa'
    )[1]

The results are as expected, except when using matrix A with the Miwa algorithm.

Matrix Miwa GenzBrentz
--------------------------------------
A -10.766   0.041
B   0.041   0.041
--------------------------------------

Further investigation indicates that it is the values in locations (1,3) and (3,1) which are causing the issues; any values in the range (5*10^-7, 5*10^-9) and (-5*10^-9, -5*10^-7) give unusual results. Can anyone help?



	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Mon Oct  2 14:51:58 2017
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Mon, 2 Oct 2017 13:51:58 +0100
Subject: [R] Fwd: Re:  Fwd: R errors
In-Reply-To: <CAGkGp6SM_V=fGzozk46as3ghvHA3f9=KygG3jQq=YzqcezVbLA@mail.gmail.com>
References: <CAGkGp6SM_V=fGzozk46as3ghvHA3f9=KygG3jQq=YzqcezVbLA@mail.gmail.com>
Message-ID: <b14ba852-3913-cff1-e4ca-4ba1b49a5f9a@dewey.myzen.co.uk>

Please do not just reply to me but also to the list as other people 
there more expert than I may be able to help you better.

Michael


-------- Forwarded Message --------
Subject: 	Re: [R] Fwd: R errors
Date: 	Mon, 2 Oct 2017 18:39:39 +0900
From: 	NYI NYI HTWE <nyinyi at iuj.ac.jp>
To: 	Michael Dewey <lists at dewey.myzen.co.uk>



Dear Michael

Thank you for your response.
It is my fault not been describe detail condition.
I here attached the codes I used.
I'm using R 3.4.1 for windows.

I tried the same codes and got results. But my supervisor wants to 
confirm and asked me to run again. Then I got error as I stated although 
I'm using the same computer and the same codes.
I'm trying to analysis network for trade data between 208 countries for 
21 years.

Please, understand me since I'm very new R user and I do appreciate your 
kind help.

Regards

Nyi Nyi
Hiroshima University

On Mon, Oct 2, 2017 at 6:27 PM, Michael Dewey <lists at dewey.myzen.co.uk 
<mailto:lists at dewey.myzen.co.uk>> wrote:

     You need to give us some more information. What package(s) are you
     using? What are TAbehdata, TAeff, TA_coev_Second_Attempt? It might
     help to show us the output of sessionInfo too.

     It is better to post in plain text not HTML as HTML can end up
     mangled so we do not see what you thought you sent.

     Michael


     On 02/10/2017 03:13, NYI NYI HTWE wrote:

         ---------- Forwarded message ----------
         From: NYI NYI HTWE <nyinyi at iuj.ac.jp <mailto:nyinyi at iuj.ac.jp>>
         Date: Sun, Oct 1, 2017 at 1:49 PM
         Subject: R errors
         To: R-windows at r-project.org <mailto:R-windows at r-project.org>


         Dear

         I'm very new R-user.
         I run a model successfully and my supervisor ask me to confirm
         the result.
         Then I run the same model again. However, it returns the error
         message in
         printing report and model creating as below which I haven't seen
         in my
         first attempt. I tried several times, but the same error returned.
         I removed R from my computer and re-install 2 times, but the
         same thing
         happened.

         Please let me know how to proceed.


         Error in print01Report(TAbehdata, TAeff, modelname =
         "TA_coev_Second_Attempt") :
          ? ?wrong parameters; note: do not include an effects object as
         parameter!

         Error in sienaModelCreate(projname = "TA_coev_Second_Attempt",
         modelType =
         3,? :
          ? ?modelType must have names of dependent networks


         Regards

         Nyi Nyi
         Graduate School for International Development and Cooperation
         Hiroshima University

          ? ? ? ? [[alternative HTML version deleted]]

         ______________________________________________
         R-help at r-project.org <mailto:R-help at r-project.org> mailing list
         -- To UNSUBSCRIBE and more, see
         https://stat.ethz.ch/mailman/listinfo/r-help
         <https://stat.ethz.ch/mailman/listinfo/r-help>
         PLEASE do read the posting guide
         http://www.R-project.org/posting-guide.html
         <http://www.R-project.org/posting-guide.html>
         and provide commented, minimal, self-contained, reproducible code.

         ---
         This email has been checked for viruses by AVG.
         http://www.avg.com



     --     Michael
     http://www.dewey.myzen.co.uk/home.html
     <http://www.dewey.myzen.co.uk/home.html>



<http://www.avg.com/email-signature?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=emailclient> 
	Virus-free. www.avg.com 
<http://www.avg.com/email-signature?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=emailclient> 


<#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

From bgunter.4567 at gmail.com  Mon Oct  2 16:57:12 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 2 Oct 2017 07:57:12 -0700
Subject: [R] R and Supervised learning
In-Reply-To: <CABQyo85knseVLNUJeHKE4BZTkJxdeKyj_wHtYvRfs-84JqpaRQ@mail.gmail.com>
References: <CABQyo85knseVLNUJeHKE4BZTkJxdeKyj_wHtYvRfs-84JqpaRQ@mail.gmail.com>
Message-ID: <CAGxFJbSS+m2ryQQPBOp2R7=ehkvDfG2zo7zv8GGWTG5CUpuWAA@mail.gmail.com>

Luca:

1. We are not a consulting service. We *help* with R pogramming issues.
Users are typically expected to make an effort by providing R code and, if
appropriate, small data sets that illustrate their difficulties.

2. SEARCH! e.g. on "text processing R" or some such; or try Rseek.org with
such searches. R has extensive text processing capabilities, e.g. via
regex's.

3. "Supervised Learning algorithm" is far too vague to be useful.

4. See this CRAN task view:
https://cran.r-project.org/web/views/MachineLearning.html

4. The answer to your query is almost certainly yes, but you may have to do
some reading to clarify your thinking. As this involves primarily
statistical issues, you may wish to post on a statistical site like
http://stats.stackexchange.com/  to get advice. R-help site helps with R
programming primarily, not statistical methodology (although they do
sometimes intersect).

Cheers,
Bert

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Mon Oct  2 17:03:23 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 2 Oct 2017 08:03:23 -0700
Subject: [R] Issues with 'Miwa' algorithm in mvtnorm package
In-Reply-To: <WM!997a3997c1a6173f92e97be9c81aa7ea5353a047ec37382b20ac366ead8132f12986cf2ab90a4830a9fcd6e847d2f45a!@mailhub-mx2.ncl.ac.uk>
References: <DB6PR0701MB2245CF41FB255EB0FDB7A454EE7D0@DB6PR0701MB2245.eurprd07.prod.outlook.com>
 <WM!997a3997c1a6173f92e97be9c81aa7ea5353a047ec37382b20ac366ead8132f12986cf2ab90a4830a9fcd6e847d2f45a!@mailhub-mx2.ncl.ac.uk>
Message-ID: <CAGxFJbSEVaNH4v=rH2kq74CdhvXxfnARcJmVbQFeXhikg+eG2Q@mail.gmail.com>

Rather specialized.

As this appears to be primarily a statistical, not an R programming
question, you may do better posting on a statistical site like
http://stats.stackexchange.com/ if you don't get a satisfactory reply here
. Alternativey, if you think this is a package bug, perhaps contact the
package maintainer directly, as (s)he may not monitor this list.

-- Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Mon, Oct 2, 2017 at 5:16 AM, Hollie Johnson (PGR) <
h.a.johnson at newcastle.ac.uk> wrote:

> Currently doing some work on local maxima on a random field and have
> encountered an issue with the Miwa algorithm used with the pmvnorm function
> in the mvtnorm R package.
>
> Based on recommendations by Mi et al., we ran the mvtnorm package using
> the Miwa algorithm, since we have a maximum of 4 dimensions with
> non-singular matrices. However, running the estimation procedure in this
> way, we obtained inconsistent results. For example, using matrices A and B,
> it is clear to see that matrix B is the results of exchanging position 1
> with position 3 in matrix A.
>
> A =
>  9.358*10^-3 -8.165*10^-3 -1.689*10^-8
> -8.165*10^-3   9.358*10^-3   1.854*10^-8
> -1.689*10^-8   1.854*10^-8   9.358*10^-3
>
> B =
>  9.358*10^-3 1.854*10^-8 -1.689*10^-8
>  1.854*10^-8 9.358*10^-3 -8.165*10^-3
> -1.689*10^-8 -8.165*10^-3 9.358*10^-3
>
> The determinants of both matrices are small but equal, so we would expect
> any issues arising from the matrix being 'close' to singular to be apparent
> in both cases. The table below shows the output from pmvnorm calculated
> using the two matrices A and B, and the two different algorithms, Miwa and
> GenzBretz using the code below:
>
> pmvnorm(
>       mean = rep(0, 3),
>       lower = rep(-Inf, 3),
>       upper = rep(0, 3),
>       sigma =  A,
>       algorithm = 'Miwa'
>     )[1]
>
> The results are as expected, except when using matrix A with the Miwa
> algorithm.
>
> Matrix Miwa GenzBrentz
> --------------------------------------
> A -10.766   0.041
> B   0.041   0.041
> --------------------------------------
>
> Further investigation indicates that it is the values in locations (1,3)
> and (3,1) which are causing the issues; any values in the range (5*10^-7,
> 5*10^-9) and (-5*10^-9, -5*10^-7) give unusual results. Can anyone help?
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Mon Oct  2 17:16:13 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Mon, 2 Oct 2017 18:16:13 +0300
Subject: [R] Issues with 'Miwa' algorithm in mvtnorm package
In-Reply-To: <CAGxFJbSEVaNH4v=rH2kq74CdhvXxfnARcJmVbQFeXhikg+eG2Q@mail.gmail.com>
References: <DB6PR0701MB2245CF41FB255EB0FDB7A454EE7D0@DB6PR0701MB2245.eurprd07.prod.outlook.com>
 <WM!997a3997c1a6173f92e97be9c81aa7ea5353a047ec37382b20ac366ead8132f12986cf2ab90a4830a9fcd6e847d2f45a!@mailhub-mx2.ncl.ac.uk>
 <CAGxFJbSEVaNH4v=rH2kq74CdhvXxfnARcJmVbQFeXhikg+eG2Q@mail.gmail.com>
Message-ID: <CAGgJW775pQ7JOetV2qm3X5dRWKXDPDogNHsaFJfOJO-BwWebpg@mail.gmail.com>

Hi Hollie,
I tried to reproduce your example but could not. Here is what I did. Please
explain if you did something different.

x <- c(9.358*10^-3, -8.165*10^-3, -1.689*10^-8,
       -8.165*10^-3,   9.358*10^-3,   1.854*10^-8,
       -1.689*10^-8,   1.854*10^-8,   9.358*10^-3)
A <- matrix(x, nrow=3, byrow = TRUE)

y <- c(9.358*10^-3, 1.854*10^-8, -1.689*10^-8,
      1.854*10^-8, 9.358*10^-3, -8.165*10^-3,
      -1.689*10^-8, -8.165*10^-3, 9.358*10^-3)

B <- matrix(x, nrow=3, byrow = TRUE)

pmvnorm(
  mean = rep(0, 3),
  lower = rep(-Inf, 3),
  upper = rep(0, 3),
  sigma =  A,
  algorithm = 'Miwa'
) [1]

# -10.76096       <--  this is the output from the above command

pmvnorm(
  mean = rep(0, 3),
  lower = rep(-Inf, 3),
  upper = rep(0, 3),
  sigma =  B,
  algorithm = 'Miwa'
) [1]

# -10.76096       <--  this is the output from the above command

i.e. the outputs agree in the two cases.

Regards,
Eric




On Mon, Oct 2, 2017 at 6:03 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Rather specialized.
>
> As this appears to be primarily a statistical, not an R programming
> question, you may do better posting on a statistical site like
> http://stats.stackexchange.com/ if you don't get a satisfactory reply here
> . Alternativey, if you think this is a package bug, perhaps contact the
> package maintainer directly, as (s)he may not monitor this list.
>
> -- Bert
>
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
> On Mon, Oct 2, 2017 at 5:16 AM, Hollie Johnson (PGR) <
> h.a.johnson at newcastle.ac.uk> wrote:
>
> > Currently doing some work on local maxima on a random field and have
> > encountered an issue with the Miwa algorithm used with the pmvnorm
> function
> > in the mvtnorm R package.
> >
> > Based on recommendations by Mi et al., we ran the mvtnorm package using
> > the Miwa algorithm, since we have a maximum of 4 dimensions with
> > non-singular matrices. However, running the estimation procedure in this
> > way, we obtained inconsistent results. For example, using matrices A and
> B,
> > it is clear to see that matrix B is the results of exchanging position 1
> > with position 3 in matrix A.
> >
> > A =
> >  9.358*10^-3 -8.165*10^-3 -1.689*10^-8
> > -8.165*10^-3   9.358*10^-3   1.854*10^-8
> > -1.689*10^-8   1.854*10^-8   9.358*10^-3
> >
> > B =
> >  9.358*10^-3 1.854*10^-8 -1.689*10^-8
> >  1.854*10^-8 9.358*10^-3 -8.165*10^-3
> > -1.689*10^-8 -8.165*10^-3 9.358*10^-3
> >
> > The determinants of both matrices are small but equal, so we would expect
> > any issues arising from the matrix being 'close' to singular to be
> apparent
> > in both cases. The table below shows the output from pmvnorm calculated
> > using the two matrices A and B, and the two different algorithms, Miwa
> and
> > GenzBretz using the code below:
> >
> > pmvnorm(
> >       mean = rep(0, 3),
> >       lower = rep(-Inf, 3),
> >       upper = rep(0, 3),
> >       sigma =  A,
> >       algorithm = 'Miwa'
> >     )[1]
> >
> > The results are as expected, except when using matrix A with the Miwa
> > algorithm.
> >
> > Matrix Miwa GenzBrentz
> > --------------------------------------
> > A -10.766   0.041
> > B   0.041   0.041
> > --------------------------------------
> >
> > Further investigation indicates that it is the values in locations (1,3)
> > and (3,1) which are causing the issues; any values in the range (5*10^-7,
> > 5*10^-9) and (-5*10^-9, -5*10^-7) give unusual results. Can anyone help?
> >
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> > posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Mon Oct  2 17:42:08 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Mon, 2 Oct 2017 18:42:08 +0300
Subject: [R] Issues with 'Miwa' algorithm in mvtnorm package
In-Reply-To: <WM!ca0f3869d3b72d2acf53f57e0d8496366406a11bd0bc3966c40ee90e3a8e4594f43f23850188183ccab6f106cf9e2a0f!@mailhub-mx4.ncl.ac.uk>
References: <DB6PR0701MB2245CF41FB255EB0FDB7A454EE7D0@DB6PR0701MB2245.eurprd07.prod.outlook.com>
 <WM!997a3997c1a6173f92e97be9c81aa7ea5353a047ec37382b20ac366ead8132f12986cf2ab90a4830a9fcd6e847d2f45a!@mailhub-mx2.ncl.ac.uk>
 <CAGxFJbSEVaNH4v=rH2kq74CdhvXxfnARcJmVbQFeXhikg+eG2Q@mail.gmail.com>
 <CAGgJW775pQ7JOetV2qm3X5dRWKXDPDogNHsaFJfOJO-BwWebpg@mail.gmail.com>
 <DB6PR0701MB2245C24FA1CDF85D0729C429EE7D0@DB6PR0701MB2245.eurprd07.prod.outlook.com>
 <WM!5d94d98e8333bbbe63ca5f0f265c5804fac42d66061e7aff7a56fb889b1032132d1f174cce62f38a93829dc5eeb6f782!@mailhub-mx5.ncl.ac.uk>
 <WM!ca0f3869d3b72d2acf53f57e0d8496366406a11bd0bc3966c40ee90e3a8e4594f43f23850188183ccab6f106cf9e2a0f!@mailhub-mx4.ncl.ac.uk>
Message-ID: <CAGgJW77z9n7o_j+knQV15cDuiQsxrCu_hhOyYO=+d0=qwtu2qA@mail.gmail.com>

Good point. Now this returns 0.04062184. Hmmm.....

On Mon, Oct 2, 2017 at 6:30 PM, Hollie Johnson (PGR) <
h.a.johnson at newcastle.ac.uk> wrote:

> Hi Eric,
>
>
> Thanks for having a look into this. I think you have a small typo...
>
> B <- matrix(x, nrow=3, byrow = TRUE) should read B <- matrix(y, nrow=3,
> byrow = TRUE)
>
>
> Regards, Hollie
> ------------------------------
> *From:* R-help <r-help-bounces at r-project.org> on behalf of Eric Berger <
> ericjberger at gmail.com>
> *Sent:* 02 October 2017 16:16:13
> *To:* Bert Gunter
> *Cc:* r-help at r-project.org; Hollie Johnson (PGR)
> *Subject:* Re: [R] Issues with 'Miwa' algorithm in mvtnorm package
>
> Hi Hollie,
> I tried to reproduce your example but could not. Here is what I did. Please
> explain if you did something different.
>
> x <- c(9.358*10^-3, -8.165*10^-3, -1.689*10^-8,
>        -8.165*10^-3,   9.358*10^-3,   1.854*10^-8,
>        -1.689*10^-8,   1.854*10^-8,   9.358*10^-3)
> A <- matrix(x, nrow=3, byrow = TRUE)
>
> y <- c(9.358*10^-3, 1.854*10^-8, -1.689*10^-8,
>       1.854*10^-8, 9.358*10^-3, -8.165*10^-3,
>       -1.689*10^-8, -8.165*10^-3, 9.358*10^-3)
>
> B <- matrix(x, nrow=3, byrow = TRUE)
>
> pmvnorm(
>   mean = rep(0, 3),
>   lower = rep(-Inf, 3),
>   upper = rep(0, 3),
>   sigma =  A,
>   algorithm = 'Miwa'
> ) [1]
>
> # -10.76096       <--  this is the output from the above command
>
> pmvnorm(
>   mean = rep(0, 3),
>   lower = rep(-Inf, 3),
>   upper = rep(0, 3),
>   sigma =  B,
>   algorithm = 'Miwa'
> ) [1]
>
> # -10.76096       <--  this is the output from the above command
>
> i.e. the outputs agree in the two cases.
>
> Regards,
> Eric
>
>
>
>
> On Mon, Oct 2, 2017 at 6:03 PM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
>
> > Rather specialized.
> >
> > As this appears to be primarily a statistical, not an R programming
> > question, you may do better posting on a statistical site like
> > http://stats.stackexchange.com/ if you don't get a satisfactory reply
> here
> > . Alternativey, if you think this is a package bug, perhaps contact the
> > package maintainer directly, as (s)he may not monitor this list.
> >
> > -- Bert
> >
> >
> >
> > Bert Gunter
> >
> > "The trouble with having an open mind is that people keep coming along
> and
> > sticking things into it."
> > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >
> > On Mon, Oct 2, 2017 at 5:16 AM, Hollie Johnson (PGR) <
> > h.a.johnson at newcastle.ac.uk> wrote:
> >
> > > Currently doing some work on local maxima on a random field and have
> > > encountered an issue with the Miwa algorithm used with the pmvnorm
> > function
> > > in the mvtnorm R package.
> > >
> > > Based on recommendations by Mi et al., we ran the mvtnorm package using
> > > the Miwa algorithm, since we have a maximum of 4 dimensions with
> > > non-singular matrices. However, running the estimation procedure in
> this
> > > way, we obtained inconsistent results. For example, using matrices A
> and
> > B,
> > > it is clear to see that matrix B is the results of exchanging position
> 1
> > > with position 3 in matrix A.
> > >
> > > A =
> > >  9.358*10^-3 -8.165*10^-3 -1.689*10^-8
> > > -8.165*10^-3   9.358*10^-3   1.854*10^-8
> > > -1.689*10^-8   1.854*10^-8   9.358*10^-3
> > >
> > > B =
> > >  9.358*10^-3 1.854*10^-8 -1.689*10^-8
> > >  1.854*10^-8 9.358*10^-3 -8.165*10^-3
> > > -1.689*10^-8 -8.165*10^-3 9.358*10^-3
> > >
> > > The determinants of both matrices are small but equal, so we would
> expect
> > > any issues arising from the matrix being 'close' to singular to be
> > apparent
> > > in both cases. The table below shows the output from pmvnorm calculated
> > > using the two matrices A and B, and the two different algorithms, Miwa
> > and
> > > GenzBretz using the code below:
> > >
> > > pmvnorm(
> > >       mean = rep(0, 3),
> > >       lower = rep(-Inf, 3),
> > >       upper = rep(0, 3),
> > >       sigma =  A,
> > >       algorithm = 'Miwa'
> > >     )[1]
> > >
> > > The results are as expected, except when using matrix A with the Miwa
> > > algorithm.
> > >
> > > Matrix Miwa GenzBrentz
> > > --------------------------------------
> > > A -10.766   0.041
> > > B   0.041   0.041
> > > --------------------------------------
> > >
> > > Further investigation indicates that it is the values in locations
> (1,3)
> > > and (3,1) which are causing the issues; any values in the range
> (5*10^-7,
> > > 5*10^-9) and (-5*10^-9, -5*10^-7) give unusual results. Can anyone
> help?
> > >
> > >
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/
> > > posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> > posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From mckellercran at gmail.com  Mon Oct  2 17:56:46 2017
From: mckellercran at gmail.com (Matthew Keller)
Date: Mon, 2 Oct 2017 09:56:46 -0600
Subject: [R] fwrite() not found in data.table package
Message-ID: <CAB7vCMQkCO_f8nwPzQO9VAPQsTremQsJ7OjwQJutzLRKW12zGg@mail.gmail.com>

Hi all,

I used to use fwrite() function in data.table but I cannot get it to work
now. The function is not in the data.table package, even though a help page
exists for it. My session info is below. Any ideas on how to get fwrite()
to work would be much appreciated. Thanks!

> sessionInfo()
R version 3.2.0 (2015-04-16)
Platform: x86_64-unknown-linux-gnu (64-bit)
Running under: Red Hat Enterprise Linux Server release 6.3 (Santiago)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8    LC_PAPER=en_US.UTF-8
 [8] LC_NAME=C                  LC_ADDRESS=C
LC_TELEPHONE=C             LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] data.table_1.10.5

loaded via a namespace (and not attached):
[1] tools_3.2.0  chron_2.3-47 tcltk_3.2.0

-- 
Matthew C Keller
Asst. Professor of Psychology
University of Colorado at Boulder
www.matthewckeller.com

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Mon Oct  2 18:18:36 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 2 Oct 2017 09:18:36 -0700
Subject: [R] Help on adding a negative binomial density plot
In-Reply-To: <HE1PR07MB077814B968386868751EE71BC67D0@HE1PR07MB0778.eurprd07.prod.outlook.com>
References: <HE1PR07MB077814B968386868751EE71BC67D0@HE1PR07MB0778.eurprd07.prod.outlook.com>
Message-ID: <2CB9AF1C-3C0C-4E83-818A-7DE09C0A40F5@comcast.net>


> On Oct 2, 2017, at 2:05 AM, David <dasolexa at hotmail.com> wrote:
> 
> Dear list,
> 
> 
> I am just starting on analysis of count data in R 3.4.0. My dataset was obtained from counting particles on a surface before andd after a cleaning process. The sampling positions on the surface are pre-defined and are the same before and after cleaning.  I have ~20% of 0's. I want to know if the cleaning process was useful at reducing the number of particles.
> 
> 
> I first fit a negative binomial model using
> 
> 
>> nbFit<-glmer.nb(Count ~ Cleaning + (1|Sampling_point) , data = myCountDB)
> 
> 
> 
> I now would like to add a curve to the histogram representing the negative binomial density function fitted to my data using
> 
> 
>> curve(dnbinom(x=, size=, prob=, mu=), add=TRUE)

Why not use the predict function in that package?

See ?merMod

-- 
David.
> 
> 
> But I am struggling defining the arguments to dnbinom.
> 
> 
> Using the str() function on the nbFit object I see there are many fields returned. And I get lost reading the ?glmer.nb help, greatly because of my lack of knowledge. Which ones should I use?
> 
> 
> Thanks ever so much for your valuable help
> 
> 
> Dave
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From ericjberger at gmail.com  Mon Oct  2 18:32:37 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Mon, 2 Oct 2017 19:32:37 +0300
Subject: [R] Issues with 'Miwa' algorithm in mvtnorm package
In-Reply-To: <CAGgJW77z9n7o_j+knQV15cDuiQsxrCu_hhOyYO=+d0=qwtu2qA@mail.gmail.com>
References: <DB6PR0701MB2245CF41FB255EB0FDB7A454EE7D0@DB6PR0701MB2245.eurprd07.prod.outlook.com>
 <WM!997a3997c1a6173f92e97be9c81aa7ea5353a047ec37382b20ac366ead8132f12986cf2ab90a4830a9fcd6e847d2f45a!@mailhub-mx2.ncl.ac.uk>
 <CAGxFJbSEVaNH4v=rH2kq74CdhvXxfnARcJmVbQFeXhikg+eG2Q@mail.gmail.com>
 <CAGgJW775pQ7JOetV2qm3X5dRWKXDPDogNHsaFJfOJO-BwWebpg@mail.gmail.com>
 <DB6PR0701MB2245C24FA1CDF85D0729C429EE7D0@DB6PR0701MB2245.eurprd07.prod.outlook.com>
 <WM!5d94d98e8333bbbe63ca5f0f265c5804fac42d66061e7aff7a56fb889b1032132d1f174cce62f38a93829dc5eeb6f782!@mailhub-mx5.ncl.ac.uk>
 <WM!ca0f3869d3b72d2acf53f57e0d8496366406a11bd0bc3966c40ee90e3a8e4594f43f23850188183ccab6f106cf9e2a0f!@mailhub-mx4.ncl.ac.uk>
 <CAGgJW77z9n7o_j+knQV15cDuiQsxrCu_hhOyYO=+d0=qwtu2qA@mail.gmail.com>
Message-ID: <CAGgJW77SPHLrv7HcbeTfhusYdwphx0q6Cwq3zmHSX94Hh8wTGQ@mail.gmail.com>

Hi Hollie,
Clearly a negative number for the result of pmvnorm() makes no sense.
On the plus side, the routine seems to know it failed since the "error"
attribute returns an NA.
You can check for this error condition programmatically, as follows:

myprob <- pmvnorm(mean=rep(0, 3),lower=rep(-Inf, 3), upper=rep(0, 3),
sigma=A, algorithm=Miwa)
if ( is.na(attr(myprob,"error"))) stop("calculation failed")

By comparison, if you call pmvnorm with the GenzBretz method
pmvnorm( ..., algorithm=GenzBretz)
the "error" attribute returns a sensible value, e.g. 3.855762e-6

You can get a positive answer with Miwa by cranking up the number of steps
it uses (to its maximum of 4098, for example)

pmvnorm( ..., algorithm=Miwa(steps=4098))

But this is still not a "correct" answer and the "error" attribute is set
to NA to flag it.

HTH,

Eric



On Mon, Oct 2, 2017 at 6:42 PM, Eric Berger <ericjberger at gmail.com> wrote:

> Good point. Now this returns 0.04062184. Hmmm.....
>
> On Mon, Oct 2, 2017 at 6:30 PM, Hollie Johnson (PGR) <
> h.a.johnson at newcastle.ac.uk> wrote:
>
>> Hi Eric,
>>
>>
>> Thanks for having a look into this. I think you have a small typo...
>>
>> B <- matrix(x, nrow=3, byrow = TRUE) should read B <- matrix(y, nrow=3,
>> byrow = TRUE)
>>
>>
>> Regards, Hollie
>> ------------------------------
>> *From:* R-help <r-help-bounces at r-project.org> on behalf of Eric Berger <
>> ericjberger at gmail.com>
>> *Sent:* 02 October 2017 16:16:13
>> *To:* Bert Gunter
>> *Cc:* r-help at r-project.org; Hollie Johnson (PGR)
>> *Subject:* Re: [R] Issues with 'Miwa' algorithm in mvtnorm package
>>
>> Hi Hollie,
>> I tried to reproduce your example but could not. Here is what I did.
>> Please
>> explain if you did something different.
>>
>> x <- c(9.358*10^-3, -8.165*10^-3, -1.689*10^-8,
>>        -8.165*10^-3,   9.358*10^-3,   1.854*10^-8,
>>        -1.689*10^-8,   1.854*10^-8,   9.358*10^-3)
>> A <- matrix(x, nrow=3, byrow = TRUE)
>>
>> y <- c(9.358*10^-3, 1.854*10^-8, -1.689*10^-8,
>>       1.854*10^-8, 9.358*10^-3, -8.165*10^-3,
>>       -1.689*10^-8, -8.165*10^-3, 9.358*10^-3)
>>
>> B <- matrix(x, nrow=3, byrow = TRUE)
>>
>> pmvnorm(
>>   mean = rep(0, 3),
>>   lower = rep(-Inf, 3),
>>   upper = rep(0, 3),
>>   sigma =  A,
>>   algorithm = 'Miwa'
>> ) [1]
>>
>> # -10.76096       <--  this is the output from the above command
>>
>> pmvnorm(
>>   mean = rep(0, 3),
>>   lower = rep(-Inf, 3),
>>   upper = rep(0, 3),
>>   sigma =  B,
>>   algorithm = 'Miwa'
>> ) [1]
>>
>> # -10.76096       <--  this is the output from the above command
>>
>> i.e. the outputs agree in the two cases.
>>
>> Regards,
>> Eric
>>
>>
>>
>>
>> On Mon, Oct 2, 2017 at 6:03 PM, Bert Gunter <bgunter.4567 at gmail.com>
>> wrote:
>>
>> > Rather specialized.
>> >
>> > As this appears to be primarily a statistical, not an R programming
>> > question, you may do better posting on a statistical site like
>> > http://stats.stackexchange.com/ if you don't get a satisfactory reply
>> here
>> > . Alternativey, if you think this is a package bug, perhaps contact the
>> > package maintainer directly, as (s)he may not monitor this list.
>> >
>> > -- Bert
>> >
>> >
>> >
>> > Bert Gunter
>> >
>> > "The trouble with having an open mind is that people keep coming along
>> and
>> > sticking things into it."
>> > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>> >
>> > On Mon, Oct 2, 2017 at 5:16 AM, Hollie Johnson (PGR) <
>> > h.a.johnson at newcastle.ac.uk> wrote:
>> >
>> > > Currently doing some work on local maxima on a random field and have
>> > > encountered an issue with the Miwa algorithm used with the pmvnorm
>> > function
>> > > in the mvtnorm R package.
>> > >
>> > > Based on recommendations by Mi et al., we ran the mvtnorm package
>> using
>> > > the Miwa algorithm, since we have a maximum of 4 dimensions with
>> > > non-singular matrices. However, running the estimation procedure in
>> this
>> > > way, we obtained inconsistent results. For example, using matrices A
>> and
>> > B,
>> > > it is clear to see that matrix B is the results of exchanging
>> position 1
>> > > with position 3 in matrix A.
>> > >
>> > > A =
>> > >  9.358*10^-3 -8.165*10^-3 -1.689*10^-8
>> > > -8.165*10^-3   9.358*10^-3   1.854*10^-8
>> > > -1.689*10^-8   1.854*10^-8   9.358*10^-3
>> > >
>> > > B =
>> > >  9.358*10^-3 1.854*10^-8 -1.689*10^-8
>> > >  1.854*10^-8 9.358*10^-3 -8.165*10^-3
>> > > -1.689*10^-8 -8.165*10^-3 9.358*10^-3
>> > >
>> > > The determinants of both matrices are small but equal, so we would
>> expect
>> > > any issues arising from the matrix being 'close' to singular to be
>> > apparent
>> > > in both cases. The table below shows the output from pmvnorm
>> calculated
>> > > using the two matrices A and B, and the two different algorithms, Miwa
>> > and
>> > > GenzBretz using the code below:
>> > >
>> > > pmvnorm(
>> > >       mean = rep(0, 3),
>> > >       lower = rep(-Inf, 3),
>> > >       upper = rep(0, 3),
>> > >       sigma =  A,
>> > >       algorithm = 'Miwa'
>> > >     )[1]
>> > >
>> > > The results are as expected, except when using matrix A with the Miwa
>> > > algorithm.
>> > >
>> > > Matrix Miwa GenzBrentz
>> > > --------------------------------------
>> > > A -10.766   0.041
>> > > B   0.041   0.041
>> > > --------------------------------------
>> > >
>> > > Further investigation indicates that it is the values in locations
>> (1,3)
>> > > and (3,1) which are causing the issues; any values in the range
>> (5*10^-7,
>> > > 5*10^-9) and (-5*10^-9, -5*10^-7) give unusual results. Can anyone
>> help?
>> > >
>> > >
>> > >
>> > >         [[alternative HTML version deleted]]
>> > >
>> > > ______________________________________________
>> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > > https://stat.ethz.ch/mailman/listinfo/r-help
>> > > PLEASE do read the posting guide http://www.R-project.org/
>> > > posting-guide.html
>> > > and provide commented, minimal, self-contained, reproducible code.
>> > >
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/
>> > posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From federico.calboli at kuleuven.be  Mon Oct  2 16:47:35 2017
From: federico.calboli at kuleuven.be (Federico Calboli)
Date: Mon, 2 Oct 2017 14:47:35 +0000
Subject: [R] valid package repositories
Message-ID: <A37D1DB3-4B7A-4D10-B2B5-779E1CE72D15@kuleuven.be>

Hi All,

I noticed that it is quite common to find in papers mentions to ?R libraries? developed for the algorithms/models/code/whatever that is being described by the paper, so that third parties will be able to use said method for themselves.  On further enquiries these libraries are not actually available on CRAN, but need to be requested from the devs.  

That is in itself does not seem a big issue, were it not for the fact most of the time I am in such situation the code is very specific for the environment of the developer, and does not actually work on any machine I try to run it on (something that is painfully true for code calling C/C++/Fortran).  A second pattern I seem to have noticed is that, despite said libraries being advertised for general use in a *published* paper, when I raise the issue the library is not actually formally published and it does not actually work like a CRAN published library would, I get a vague ?the person who actually did the work left and nobody can maintain the code/fix stuff/finish the job?.

As a referee I am trying to weed out what I see as malpractice: the promise that third parties outside the developers might actually use the code because it has been packaged as a R library, a claim that seems to boost publishing chances.

Thus my question: when can I consider a library to be properly published and really publicly available?  CRAN and BioConductor are clearly gold standards.  What about Github?  I am currently using the rule ?not on CRAN == outright rejection?.  If Github is as good as CRAN I will include it on my list of ?the code is available in a functional state as claimed?.

Finally, please note the scope of my query:  I am not looking at those cases where a colleague gives me half finished code that might be useful but I need to sort out.  I am looking at formal claims ?we have developed a method to do X and said method is available to the public as a R library?.  If that is the claim I expect it to be true.

Best

F




--
Federico Calboli
LBEG - Laboratory of Biodiversity and Evolutionary Genomics
Charles Deberiotstraat 32 box 2439
3000 Leuven
+32 16 32 87 67






From h.a.johnson at newcastle.ac.uk  Mon Oct  2 17:30:42 2017
From: h.a.johnson at newcastle.ac.uk (Hollie Johnson (PGR))
Date: Mon, 2 Oct 2017 15:30:42 +0000
Subject: [R] Issues with 'Miwa' algorithm in mvtnorm package
In-Reply-To: <WM!5d94d98e8333bbbe63ca5f0f265c5804fac42d66061e7aff7a56fb889b1032132d1f174cce62f38a93829dc5eeb6f782!@mailhub-mx5.ncl.ac.uk>
References: <DB6PR0701MB2245CF41FB255EB0FDB7A454EE7D0@DB6PR0701MB2245.eurprd07.prod.outlook.com>
 <WM!997a3997c1a6173f92e97be9c81aa7ea5353a047ec37382b20ac366ead8132f12986cf2ab90a4830a9fcd6e847d2f45a!@mailhub-mx2.ncl.ac.uk>
 <CAGxFJbSEVaNH4v=rH2kq74CdhvXxfnARcJmVbQFeXhikg+eG2Q@mail.gmail.com>
 <CAGgJW775pQ7JOetV2qm3X5dRWKXDPDogNHsaFJfOJO-BwWebpg@mail.gmail.com>,
 <WM!5d94d98e8333bbbe63ca5f0f265c5804fac42d66061e7aff7a56fb889b1032132d1f174cce62f38a93829dc5eeb6f782!@mailhub-mx5.ncl.ac.uk>
 <DB6PR0701MB2245C24FA1CDF85D0729C429EE7D0@DB6PR0701MB2245.eurprd07.prod.outlook.com>
Message-ID: <WM!061e3915034c86b81648793660b5a54a13a313168ee87fbb4c1a72e68c3cd2c592dd71c743e24122a8a77fe78ea67d01!@mailhub-mx3.ncl.ac.uk>

Hi Eric,


Thanks for having a look into this. I think you have a small typo...

B <- matrix(x, nrow=3, byrow = TRUE) should read B <- matrix(y, nrow=3, byrow = TRUE)


Regards, Hollie

________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of Eric Berger <ericjberger at gmail.com>
Sent: 02 October 2017 16:16:13
To: Bert Gunter
Cc: r-help at r-project.org; Hollie Johnson (PGR)
Subject: Re: [R] Issues with 'Miwa' algorithm in mvtnorm package

Hi Hollie,
I tried to reproduce your example but could not. Here is what I did. Please
explain if you did something different.

x <- c(9.358*10^-3, -8.165*10^-3, -1.689*10^-8,
       -8.165*10^-3,   9.358*10^-3,   1.854*10^-8,
       -1.689*10^-8,   1.854*10^-8,   9.358*10^-3)
A <- matrix(x, nrow=3, byrow = TRUE)

y <- c(9.358*10^-3, 1.854*10^-8, -1.689*10^-8,
      1.854*10^-8, 9.358*10^-3, -8.165*10^-3,
      -1.689*10^-8, -8.165*10^-3, 9.358*10^-3)

B <- matrix(x, nrow=3, byrow = TRUE)

pmvnorm(
  mean = rep(0, 3),
  lower = rep(-Inf, 3),
  upper = rep(0, 3),
  sigma =  A,
  algorithm = 'Miwa'
) [1]

# -10.76096       <--  this is the output from the above command

pmvnorm(
  mean = rep(0, 3),
  lower = rep(-Inf, 3),
  upper = rep(0, 3),
  sigma =  B,
  algorithm = 'Miwa'
) [1]

# -10.76096       <--  this is the output from the above command

i.e. the outputs agree in the two cases.

Regards,
Eric




On Mon, Oct 2, 2017 at 6:03 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Rather specialized.
>
> As this appears to be primarily a statistical, not an R programming
> question, you may do better posting on a statistical site like
> http://stats.stackexchange.com/ if you don't get a satisfactory reply here
> . Alternativey, if you think this is a package bug, perhaps contact the
> package maintainer directly, as (s)he may not monitor this list.
>
> -- Bert
>
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
> On Mon, Oct 2, 2017 at 5:16 AM, Hollie Johnson (PGR) <
> h.a.johnson at newcastle.ac.uk> wrote:
>
> > Currently doing some work on local maxima on a random field and have
> > encountered an issue with the Miwa algorithm used with the pmvnorm
> function
> > in the mvtnorm R package.
> >
> > Based on recommendations by Mi et al., we ran the mvtnorm package using
> > the Miwa algorithm, since we have a maximum of 4 dimensions with
> > non-singular matrices. However, running the estimation procedure in this
> > way, we obtained inconsistent results. For example, using matrices A and
> B,
> > it is clear to see that matrix B is the results of exchanging position 1
> > with position 3 in matrix A.
> >
> > A =
> >  9.358*10^-3 -8.165*10^-3 -1.689*10^-8
> > -8.165*10^-3   9.358*10^-3   1.854*10^-8
> > -1.689*10^-8   1.854*10^-8   9.358*10^-3
> >
> > B =
> >  9.358*10^-3 1.854*10^-8 -1.689*10^-8
> >  1.854*10^-8 9.358*10^-3 -8.165*10^-3
> > -1.689*10^-8 -8.165*10^-3 9.358*10^-3
> >
> > The determinants of both matrices are small but equal, so we would expect
> > any issues arising from the matrix being 'close' to singular to be
> apparent
> > in both cases. The table below shows the output from pmvnorm calculated
> > using the two matrices A and B, and the two different algorithms, Miwa
> and
> > GenzBretz using the code below:
> >
> > pmvnorm(
> >       mean = rep(0, 3),
> >       lower = rep(-Inf, 3),
> >       upper = rep(0, 3),
> >       sigma =  A,
> >       algorithm = 'Miwa'
> >     )[1]
> >
> > The results are as expected, except when using matrix A with the Miwa
> > algorithm.
> >
> > Matrix Miwa GenzBrentz
> > --------------------------------------
> > A -10.766   0.041
> > B   0.041   0.041
> > --------------------------------------
> >
> > Further investigation indicates that it is the values in locations (1,3)
> > and (3,1) which are causing the issues; any values in the range (5*10^-7,
> > 5*10^-9) and (-5*10^-9, -5*10^-7) give unusual results. Can anyone help?
> >
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> > posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Mon Oct  2 18:55:28 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 02 Oct 2017 09:55:28 -0700
Subject: [R] fwrite() not found in data.table package
In-Reply-To: <CAB7vCMQkCO_f8nwPzQO9VAPQsTremQsJ7OjwQJutzLRKW12zGg@mail.gmail.com>
References: <CAB7vCMQkCO_f8nwPzQO9VAPQsTremQsJ7OjwQJutzLRKW12zGg@mail.gmail.com>
Message-ID: <82E72752-81CC-4304-A9EE-3F32B7E3DF88@dcn.davis.ca.us>

You are asking about (a) a contributed package (b) for a package version that is not in CRAN and (c) an R version that is outdated, which stretches the definition of "on topic" here. Since that function does not appear to have been removed from that package (I am not installing a development version to test if it is broken for your benefit), I will throw out a guess that if you update R to 3.4.1 or 3.4.2 then things might start working. If not, I suggest you use the CRAN version of the package and create a reproducible example (check it with package reprex) and try again here, or ask one of the maintainers of that package. 
-- 
Sent from my phone. Please excuse my brevity.

On October 2, 2017 8:56:46 AM PDT, Matthew Keller <mckellercran at gmail.com> wrote:
>Hi all,
>
>I used to use fwrite() function in data.table but I cannot get it to
>work
>now. The function is not in the data.table package, even though a help
>page
>exists for it. My session info is below. Any ideas on how to get
>fwrite()
>to work would be much appreciated. Thanks!
>
>> sessionInfo()
>R version 3.2.0 (2015-04-16)
>Platform: x86_64-unknown-linux-gnu (64-bit)
>Running under: Red Hat Enterprise Linux Server release 6.3 (Santiago)
>
>locale:
> [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
>LC_PAPER=en_US.UTF-8
> [8] LC_NAME=C                  LC_ADDRESS=C
>LC_TELEPHONE=C             LC_MEASUREMENT=en_US.UTF-8
>LC_IDENTIFICATION=C
>
>attached base packages:
>[1] stats     graphics  grDevices utils     datasets  methods   base
>
>other attached packages:
>[1] data.table_1.10.5
>
>loaded via a namespace (and not attached):
>[1] tools_3.2.0  chron_2.3-47 tcltk_3.2.0


From mckellercran at gmail.com  Mon Oct  2 19:10:30 2017
From: mckellercran at gmail.com (Matthew Keller)
Date: Mon, 2 Oct 2017 11:10:30 -0600
Subject: [R] fwrite() not found in data.table package
In-Reply-To: <82E72752-81CC-4304-A9EE-3F32B7E3DF88@dcn.davis.ca.us>
References: <CAB7vCMQkCO_f8nwPzQO9VAPQsTremQsJ7OjwQJutzLRKW12zGg@mail.gmail.com>
 <82E72752-81CC-4304-A9EE-3F32B7E3DF88@dcn.davis.ca.us>
Message-ID: <CAB7vCMTcG4jPLtkCH-wz7OySLuf7H9c2k5Ef=aTHxA_QdpR9=w@mail.gmail.com>

Thanks Jeff!

It turns out that my problem was that I tried to install the newest
data.table package while the old data.table package was loaded in R. Full
instructions for installing data.table are here:
https://github.com/Rdatatable/data.table/wiki/Installation

On Mon, Oct 2, 2017 at 10:55 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> You are asking about (a) a contributed package (b) for a package version
> that is not in CRAN and (c) an R version that is outdated, which stretches
> the definition of "on topic" here. Since that function does not appear to
> have been removed from that package (I am not installing a development
> version to test if it is broken for your benefit), I will throw out a guess
> that if you update R to 3.4.1 or 3.4.2 then things might start working. If
> not, I suggest you use the CRAN version of the package and create a
> reproducible example (check it with package reprex) and try again here, or
> ask one of the maintainers of that package.
> --
> Sent from my phone. Please excuse my brevity.
>
> On October 2, 2017 8:56:46 AM PDT, Matthew Keller <mckellercran at gmail.com>
> wrote:
> >Hi all,
> >
> >I used to use fwrite() function in data.table but I cannot get it to
> >work
> >now. The function is not in the data.table package, even though a help
> >page
> >exists for it. My session info is below. Any ideas on how to get
> >fwrite()
> >to work would be much appreciated. Thanks!
> >
> >> sessionInfo()
> >R version 3.2.0 (2015-04-16)
> >Platform: x86_64-unknown-linux-gnu (64-bit)
> >Running under: Red Hat Enterprise Linux Server release 6.3 (Santiago)
> >
> >locale:
> > [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
> >LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
> >LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
> >LC_PAPER=en_US.UTF-8
> > [8] LC_NAME=C                  LC_ADDRESS=C
> >LC_TELEPHONE=C             LC_MEASUREMENT=en_US.UTF-8
> >LC_IDENTIFICATION=C
> >
> >attached base packages:
> >[1] stats     graphics  grDevices utils     datasets  methods   base
> >
> >other attached packages:
> >[1] data.table_1.10.5
> >
> >loaded via a namespace (and not attached):
> >[1] tools_3.2.0  chron_2.3-47 tcltk_3.2.0
>



-- 
Matthew C Keller
Asst. Professor of Psychology
University of Colorado at Boulder
www.matthewckeller.com

	[[alternative HTML version deleted]]


From peter.langfelder at gmail.com  Mon Oct  2 19:17:38 2017
From: peter.langfelder at gmail.com (Peter Langfelder)
Date: Mon, 2 Oct 2017 10:17:38 -0700
Subject: [R] valid package repositories
In-Reply-To: <A37D1DB3-4B7A-4D10-B2B5-779E1CE72D15@kuleuven.be>
References: <A37D1DB3-4B7A-4D10-B2B5-779E1CE72D15@kuleuven.be>
Message-ID: <CA+hbrhXWoqijHvrZwCr7t_6_G=yBCJAAsPBoDT1WcSRrSn5pNQ@mail.gmail.com>

On Mon, Oct 2, 2017 at 7:47 AM, Federico Calboli
<federico.calboli at kuleuven.be> wrote:

>
> Thus my question: when can I consider a library to be properly published and really publicly available?  CRAN and BioConductor are clearly gold standards.  What about Github?  I am currently using the rule ?not on CRAN == outright rejection?.  If Github is as good as CRAN I will include it on my list of ?the code is available in a functional state as claimed?.

CRAN has certain rules that are necessary for CRAN to function but may
not be necessary for a package to be useful (e.g. size of data in a
non-data package, licensing, run time of examples etc). I would ask
two things from developers of a new package: 1. package is available
for download from somewhere public; 2. package passes R CMD check
without errors or warnings. Possibly also an explanation why they
cannot upload the package to CRAN or Bioconductor, but I would not
make the acceptance by CRAN or Bioconductor a condition for
publishing.

Just my humble opinion.

Peter


From lucam1968 at gmail.com  Mon Oct  2 19:21:16 2017
From: lucam1968 at gmail.com (Luca Meyer)
Date: Mon, 2 Oct 2017 19:21:16 +0200
Subject: [R] R and Supervised learning
In-Reply-To: <CAGxFJbSS+m2ryQQPBOp2R7=ehkvDfG2zo7zv8GGWTG5CUpuWAA@mail.gmail.com>
References: <CABQyo85knseVLNUJeHKE4BZTkJxdeKyj_wHtYvRfs-84JqpaRQ@mail.gmail.com>
 <CAGxFJbSS+m2ryQQPBOp2R7=ehkvDfG2zo7zv8GGWTG5CUpuWAA@mail.gmail.com>
Message-ID: <CABQyo86XEeAC-ik5jXpY6XvqLtvywHCbeC4_fSa=c7tNyLgP3A@mail.gmail.com>

Hi Bert,

Thank you for your useful suggestions I will follow them and come back to
this list with any specific R code issue I might have.

Kind regards,

Luca

2017-10-02 16:57 GMT+02:00 Bert Gunter <bgunter.4567 at gmail.com>:

> Luca:
>
> 1. We are not a consulting service. We *help* with R pogramming issues.
> Users are typically expected to make an effort by providing R code and, if
> appropriate, small data sets that illustrate their difficulties.
>
> 2. SEARCH! e.g. on "text processing R" or some such; or try Rseek.org with
> such searches. R has extensive text processing capabilities, e.g. via
> regex's.
>
> 3. "Supervised Learning algorithm" is far too vague to be useful.
>
> 4. See this CRAN task view:
> https://cran.r-project.org/web/views/MachineLearning.html
>
> 4. The answer to your query is almost certainly yes, but you may have to
> do some reading to clarify your thinking. As this involves primarily
> statistical issues, you may wish to post on a statistical site like
> http://stats.stackexchange.com/  to get advice. R-help site helps with R
> programming primarily, not statistical methodology (although they do
> sometimes intersect).
>
> Cheers,
> Bert
>
>
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Mon Oct  2 19:25:10 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 02 Oct 2017 10:25:10 -0700
Subject: [R] valid package repositories
In-Reply-To: <A37D1DB3-4B7A-4D10-B2B5-779E1CE72D15@kuleuven.be>
References: <A37D1DB3-4B7A-4D10-B2B5-779E1CE72D15@kuleuven.be>
Message-ID: <C21439A1-AF72-4AB3-8A7D-C883F0C07F65@dcn.davis.ca.us>

I tend to regard GitHub as a bit of wild west... anyone can upload anything there, working or not. CRAN packages at least have to compile so there is some additional verification in being there.

GitHub does have the advantage that you can easily download it and run an example if the authors have set up such scaffolding... which is better than "it ran once on that laptop that died". However, there is a distinct extra level of sophistication involved in getting researchers to make those examples or test cases beyond their mainline code, and nothing about GitHub requires that such features be present in uploaded code.
-- 
Sent from my phone. Please excuse my brevity.

On October 2, 2017 7:47:35 AM PDT, Federico Calboli <federico.calboli at kuleuven.be> wrote:
>Hi All,
>
>I noticed that it is quite common to find in papers mentions to ?R
>libraries? developed for the algorithms/models/code/whatever that is
>being described by the paper, so that third parties will be able to use
>said method for themselves.  On further enquiries these libraries are
>not actually available on CRAN, but need to be requested from the devs.
> 
>
>That is in itself does not seem a big issue, were it not for the fact
>most of the time I am in such situation the code is very specific for
>the environment of the developer, and does not actually work on any
>machine I try to run it on (something that is painfully true for code
>calling C/C++/Fortran).  A second pattern I seem to have noticed is
>that, despite said libraries being advertised for general use in a
>*published* paper, when I raise the issue the library is not actually
>formally published and it does not actually work like a CRAN published
>library would, I get a vague ?the person who actually did the work left
>and nobody can maintain the code/fix stuff/finish the job?.
>
>As a referee I am trying to weed out what I see as malpractice: the
>promise that third parties outside the developers might actually use
>the code because it has been packaged as a R library, a claim that
>seems to boost publishing chances.
>
>Thus my question: when can I consider a library to be properly
>published and really publicly available?  CRAN and BioConductor are
>clearly gold standards.  What about Github?  I am currently using the
>rule ?not on CRAN == outright rejection?.  If Github is as good as CRAN
>I will include it on my list of ?the code is available in a functional
>state as claimed?.
>
>Finally, please note the scope of my query:  I am not looking at those
>cases where a colleague gives me half finished code that might be
>useful but I need to sort out.  I am looking at formal claims ?we have
>developed a method to do X and said method is available to the public
>as a R library?.  If that is the claim I expect it to be true.
>
>Best
>
>F
>
>
>
>
>--
>Federico Calboli
>LBEG - Laboratory of Biodiversity and Evolutionary Genomics
>Charles Deberiotstraat 32 box 2439
>3000 Leuven
>+32 16 32 87 67
>
>
>
>
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From bhh at xs4all.nl  Mon Oct  2 20:49:26 2017
From: bhh at xs4all.nl (Berend Hasselman)
Date: Mon, 2 Oct 2017 20:49:26 +0200
Subject: [R] valid package repositories
In-Reply-To: <A37D1DB3-4B7A-4D10-B2B5-779E1CE72D15@kuleuven.be>
References: <A37D1DB3-4B7A-4D10-B2B5-779E1CE72D15@kuleuven.be>
Message-ID: <BCDA3437-1240-4777-9ED8-198D70446143@xs4all.nl>


> On 2 Oct 2017, at 16:47, Federico Calboli <federico.calboli at kuleuven.be> wrote:
> .....

> As a referee I am trying to weed out what I see as malpractice: the promise that third parties outside the developers might actually use the code because it has been packaged as a R library, a claim that seems to boost publishing chances.
> 
> Thus my question: when can I consider a library to be properly published and really publicly available?  CRAN and BioConductor are clearly gold standards.  What about Github?  I am currently using the rule ?not on CRAN == outright rejection?.  If Github is as good as CRAN I will include it on my list of ?the code is available in a functional state as claimed?.
> 

As others have suggested:
I would insist that code is presented as valid R package which the maker has at least checked with R CMD check with no errors (preferably with the --as-cran option).

In addition I would also insist that  packages have been sent to the winbuilder and passed all checks without error or warning.

Berend Hasselman


From henrik.bengtsson at gmail.com  Mon Oct  2 21:00:37 2017
From: henrik.bengtsson at gmail.com (Henrik Bengtsson)
Date: Mon, 2 Oct 2017 12:00:37 -0700
Subject: [R] valid package repositories
In-Reply-To: <C21439A1-AF72-4AB3-8A7D-C883F0C07F65@dcn.davis.ca.us>
References: <A37D1DB3-4B7A-4D10-B2B5-779E1CE72D15@kuleuven.be>
 <C21439A1-AF72-4AB3-8A7D-C883F0C07F65@dcn.davis.ca.us>
Message-ID: <CAFDcVCReZ90Yow6oH8RKE+wUt9ieq4VNAU42FgVGYyAgJZbJYA@mail.gmail.com>

Here's my view on this:

CRAN = Comprehensive R Archive Network.  The "Archive" part is very
important - it "promises" the research community that R packages that
have ever been published on CRAN, and all the versions of each
package, will be available also in the future.  It requires quite a
bit for a package/code to disappear from CRAN, e.g. a package contains
code/data that is not allowed to be shared (due to licenses and
copyrights).  Not even the original developer/maintainer can remove a
package that has already been released on CRAN.  What we see at times,
a package is "archived" on CRAN (i.e. no longer available via
install.packages()), but the old package versions are still
distributed.  That CRAN protects us this way is extremely valuable to
the research community, open science, and reproducible research.  The
Bioconductor has a similar philosophy.

However convenient GitHub / GitLab / ... is for development etc, it
certainly does not provide scientific archiving - in that sense it is
no different than sharing packages on Dropbox, Google Drive, etc.

/Henrik


On Mon, Oct 2, 2017 at 10:25 AM, Jeff Newmiller
<jdnewmil at dcn.davis.ca.us> wrote:
> I tend to regard GitHub as a bit of wild west... anyone can upload anything there, working or not. CRAN packages at least have to compile so there is some additional verification in being there.
>
> GitHub does have the advantage that you can easily download it and run an example if the authors have set up such scaffolding... which is better than "it ran once on that laptop that died". However, there is a distinct extra level of sophistication involved in getting researchers to make those examples or test cases beyond their mainline code, and nothing about GitHub requires that such features be present in uploaded code.
> --
> Sent from my phone. Please excuse my brevity.
>
> On October 2, 2017 7:47:35 AM PDT, Federico Calboli <federico.calboli at kuleuven.be> wrote:
>>Hi All,
>>
>>I noticed that it is quite common to find in papers mentions to ?R
>>libraries? developed for the algorithms/models/code/whatever that is
>>being described by the paper, so that third parties will be able to use
>>said method for themselves.  On further enquiries these libraries are
>>not actually available on CRAN, but need to be requested from the devs.
>>
>>
>>That is in itself does not seem a big issue, were it not for the fact
>>most of the time I am in such situation the code is very specific for
>>the environment of the developer, and does not actually work on any
>>machine I try to run it on (something that is painfully true for code
>>calling C/C++/Fortran).  A second pattern I seem to have noticed is
>>that, despite said libraries being advertised for general use in a
>>*published* paper, when I raise the issue the library is not actually
>>formally published and it does not actually work like a CRAN published
>>library would, I get a vague ?the person who actually did the work left
>>and nobody can maintain the code/fix stuff/finish the job?.
>>
>>As a referee I am trying to weed out what I see as malpractice: the
>>promise that third parties outside the developers might actually use
>>the code because it has been packaged as a R library, a claim that
>>seems to boost publishing chances.
>>
>>Thus my question: when can I consider a library to be properly
>>published and really publicly available?  CRAN and BioConductor are
>>clearly gold standards.  What about Github?  I am currently using the
>>rule ?not on CRAN == outright rejection?.  If Github is as good as CRAN
>>I will include it on my list of ?the code is available in a functional
>>state as claimed?.
>>
>>Finally, please note the scope of my query:  I am not looking at those
>>cases where a colleague gives me half finished code that might be
>>useful but I need to sort out.  I am looking at formal claims ?we have
>>developed a method to do X and said method is available to the public
>>as a R library?.  If that is the claim I expect it to be true.
>>
>>Best
>>
>>F
>>
>>
>>
>>
>>--
>>Federico Calboli
>>LBEG - Laboratory of Biodiversity and Evolutionary Genomics
>>Charles Deberiotstraat 32 box 2439
>>3000 Leuven
>>+32 16 32 87 67
>>
>>
>>
>>
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jordandev678 at gmail.com  Mon Oct  2 22:01:46 2017
From: jordandev678 at gmail.com (Jordan J)
Date: Mon, 2 Oct 2017 21:01:46 +0100
Subject: [R] valid package repositories
In-Reply-To: <CAFDcVCReZ90Yow6oH8RKE+wUt9ieq4VNAU42FgVGYyAgJZbJYA@mail.gmail.com>
References: <A37D1DB3-4B7A-4D10-B2B5-779E1CE72D15@kuleuven.be>
 <C21439A1-AF72-4AB3-8A7D-C883F0C07F65@dcn.davis.ca.us>
 <CAFDcVCReZ90Yow6oH8RKE+wUt9ieq4VNAU42FgVGYyAgJZbJYA@mail.gmail.com>
Message-ID: <CAAnfzJOddjmxJwyZds-98+Ak6RGi0QKBLdMi8uq2cSqKdacyNA@mail.gmail.com>

I would be on a similar wavelength to Peter, I believe it should be
more about the state of the package rather than the location.

Yes, the location matters to a degree but I think GitHub is more than
well enough established at this point to consider their hosting
sufficiently reliable.
The most important thing in my opinion is that it is a valid package
that passes a build/check.

There a number of things that may make CRAN unsuitable compared to
GitHub, the most obvious of which is license issues.
The CRAN Repository Policy has a specific list of licenses and notes
licenses outside that list are generally not accepted.
There is also cross platform requirements in CRAN (at least two major
platforms), which make sense for the default central repository but
are by no means required for a package to have utility and be worthy
of publishing.
Finally, GitHub when used properly also provides a full history of
changes, rationales, etc that CRAN doesn't provide for as it is not
that type of hosting service.

None of that is to say I think CRAN shouldn't be the default. Having
the package in the primary central repository should always be the
preferred option, but should not be the only option.
Personally, I would accept a GitHub library that passes a build/check
without much hesitation.


From drjimlemon at gmail.com  Mon Oct  2 23:27:30 2017
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 3 Oct 2017 08:27:30 +1100
Subject: [R] Fwd: R errors
In-Reply-To: <CAGkGp6QZ1ceOEGbw-08ndi13FXMWBjnTo-eC-tGFLKJ1anc2GA@mail.gmail.com>
References: <CAGkGp6QOB77YcjRdbPaGgVHj5AkNCA5EROyA0aSwa+C-q6VAPA@mail.gmail.com>
 <CAGkGp6QZ1ceOEGbw-08ndi13FXMWBjnTo-eC-tGFLKJ1anc2GA@mail.gmail.com>
Message-ID: <CA+8X3fVVm3AMZOKuUvoMVb5JTW11w62O1oNokEZKmxN8BnYpEg@mail.gmail.com>

Hi Nyi,
It is very unlikely that you ran exactly the same code twice and got a
different result. Since we do not know what that code was, we can't
debug it (and probably don't want to) so the best thing to do is to
take the R script that you ran the first time (you do have the R
commands in a file?) and compare it line by line with the second
attempt. On a wild guess, you may have changed one of the operators in
the model formula.

Jim


On Mon, Oct 2, 2017 at 1:13 PM, NYI NYI HTWE <nyinyi at iuj.ac.jp> wrote:
> ---------- Forwarded message ----------
> From: NYI NYI HTWE <nyinyi at iuj.ac.jp>
> Date: Sun, Oct 1, 2017 at 1:49 PM
> Subject: R errors
> To: R-windows at r-project.org
>
>
> Dear
>
> I'm very new R-user.
> I run a model successfully and my supervisor ask me to confirm the result.
> Then I run the same model again. However, it returns the error message in
> printing report and model creating as below which I haven't seen in my
> first attempt. I tried several times, but the same error returned.
> I removed R from my computer and re-install 2 times, but the same thing
> happened.
>
> Please let me know how to proceed.
>
>
> Error in print01Report(TAbehdata, TAeff, modelname =
> "TA_coev_Second_Attempt") :
>   wrong parameters; note: do not include an effects object as parameter!
>
> Error in sienaModelCreate(projname = "TA_coev_Second_Attempt", modelType =
> 3,  :
>   modelType must have names of dependent networks
>
>
> Regards
>
> Nyi Nyi
> Graduate School for International Development and Cooperation
> Hiroshima University
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ashimkapoor at gmail.com  Tue Oct  3 06:48:38 2017
From: ashimkapoor at gmail.com (Ashim Kapoor)
Date: Tue, 3 Oct 2017 10:18:38 +0530
Subject: [R] Default value of the option initial in the ses function in
 the forecast package.
In-Reply-To: <755867D7-46E1-476D-B734-3B097FD83632@gmail.com>
References: <CAC8=1err3ceBdMjKXwWaeRUogZnnR2_yXfeQNw9EKkRGvzU7_Q@mail.gmail.com>
 <755867D7-46E1-476D-B734-3B097FD83632@gmail.com>
Message-ID: <CAC8=1eqa0BUXDF=M6sP-R5uxwK2vJ8DVC6NvVFYkhZ7MN7G+OA@mail.gmail.com>

Dear Peter,

Many thanks,
Ashim

On Mon, Oct 2, 2017 at 3:30 PM, peter dalgaard <pdalgd at gmail.com> wrote:

> The first one, i.e. "optimal"; check help for match.arg() for the idiom.
>
> -pd
>
>
> > On 2 Oct 2017, at 11:48 , Ashim Kapoor <ashimkapoor at gmail.com> wrote:
> >
> > Dear All,
> >
> > I am trying to use the function ses from the forecast package.
> >
> > From its help I have :
> >
> > Usage:
> >
> >     ses(y, h = 10, level = c(80, 95), fan = FALSE, initial = c("optimal",
> >       "simple"), alpha = NULL, lambda = NULL, biasadj = FALSE, x = y,
> ...)
> >
> > My query is that if I do not mention the initial value will its default
> > value be "optimal".
> >
> > A MWE would be  :
> >
> > library(fpp)
> > oildata <- window(oil,start=1996,end=2007)
> > fit3 <- ses(oildata, h=3)
> >
> > In the above is the default value of initial "optimal" or "simple" ?
> >
> > Note I have taken this example from https://www.otexts.org/fpp/7/1
> >
> > Best Regards,
> > Ashim
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From dasolexa at hotmail.com  Tue Oct  3 13:20:02 2017
From: dasolexa at hotmail.com (David)
Date: Tue, 3 Oct 2017 11:20:02 +0000
Subject: [R] Help on adding a negative binomial density plot
In-Reply-To: <2CB9AF1C-3C0C-4E83-818A-7DE09C0A40F5@comcast.net>
References: <HE1PR07MB077814B968386868751EE71BC67D0@HE1PR07MB0778.eurprd07.prod.outlook.com>,
 <2CB9AF1C-3C0C-4E83-818A-7DE09C0A40F5@comcast.net>
Message-ID: <HE1PR07MB0778DFA66A9A7F9EF2A31F83C6720@HE1PR07MB0778.eurprd07.prod.outlook.com>

Dear David,


thanks ever so much for your answer.


Do you mean predicting the original values based on the fitted model and then comparing observed vs. predicted by, for example, a scatterplot?


Thanks,


David

________________________________
De: David Winsemius <dwinsemius at comcast.net>
Enviado: lunes, 2 de octubre de 2017 18:18:36
Para: David
Cc: R-help
Asunto: Re: [R] Help on adding a negative binomial density plot


> On Oct 2, 2017, at 2:05 AM, David <dasolexa at hotmail.com> wrote:
>
> Dear list,
>
>
> I am just starting on analysis of count data in R 3.4.0. My dataset was obtained from counting particles on a surface before andd after a cleaning process. The sampling positions on the surface are pre-defined and are the same before and after cleaning.  I have ~20% of 0's. I want to know if the cleaning process was useful at reducing the number of particles.
>
>
> I first fit a negative binomial model using
>
>
>> nbFit<-glmer.nb(Count ~ Cleaning + (1|Sampling_point) , data = myCountDB)
>
>
>
> I now would like to add a curve to the histogram representing the negative binomial density function fitted to my data using
>
>
>> curve(dnbinom(x=, size=, prob=, mu=), add=TRUE)

Why not use the predict function in that package?

See ?merMod

--
David.
>
>
> But I am struggling defining the arguments to dnbinom.
>
>
> Using the str() function on the nbFit object I see there are many fields returned. And I get lost reading the ?glmer.nb help, greatly because of my lack of knowledge. Which ones should I use?
>
>
> Thanks ever so much for your valuable help
>
>
> Dave
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law






	[[alternative HTML version deleted]]


From karthik.100 at gmail.com  Tue Oct  3 16:28:22 2017
From: karthik.100 at gmail.com (Karthiko S)
Date: Tue, 3 Oct 2017 19:58:22 +0530
Subject: [R] Deletion in Binary Search Tree
Message-ID: <CAD-yskpsNW=ez2+TT4zqHZs_--Q=_=bqBhrvZ_X4WL+C=QYDVQ@mail.gmail.com>

Dear All,

I am unable to complete the R code for deleting nodes in a binary search
tree.

Have provided the code snippet below for an usecase where the node to be
deleted would have a leftnode and no rightnode. I am stuck up at how to
release the node pertaining to the key value given by the function
call and then
assigning the leftnode in place of the deleted note.

Any help would be appreciated .

    deletenode<-function(node,key)

    {

    if(identical(node,NULL)==TRUE)

    {

    return(node)

    }

    else

    {

    if(key<node$key)

    {

    node$leftnode<-deletenode(node$leftnode,key)

    }

    if(key>node$key)

    {

    node$leftnode<- deletenode(node$rightnode,key)

    }

    if(key==node$key)

    {

      if((identical(node$leftnode,NULL)==TRUE)&&

                      (identical(node$rightnode,NULL)==FALSE))

    {

    temp<-node

    node<-node$rightnode

    remove(temp)

    return(node)

    }



    }

    deletenode(root,key)


Thanks and Regards

karthik

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Oct  3 18:13:04 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 3 Oct 2017 09:13:04 -0700
Subject: [R] Deletion in Binary Search Tree
In-Reply-To: <CAD-yskpsNW=ez2+TT4zqHZs_--Q=_=bqBhrvZ_X4WL+C=QYDVQ@mail.gmail.com>
References: <CAD-yskpsNW=ez2+TT4zqHZs_--Q=_=bqBhrvZ_X4WL+C=QYDVQ@mail.gmail.com>
Message-ID: <CAGxFJbSGL8RQsZ9HvSi28BMgfPepHyri4z7ZZFZoQHG1NsQOeg@mail.gmail.com>

"I am unable to complete the R code for deleting nodes in a binary search
tree."

What does this mean? What errors did you receive or what did or did not
happen with your current code? With what code was the tree built?

Also, this smells like homework. We do not do homework (or try not to) on
this list.

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Tue, Oct 3, 2017 at 7:28 AM, Karthiko S <karthik.100 at gmail.com> wrote:

> Dear All,
>
> I am unable to complete the R code for deleting nodes in a binary search
> tree.
>
> Have provided the code snippet below for an usecase where the node to be
> deleted would have a leftnode and no rightnode. I am stuck up at how to
> release the node pertaining to the key value given by the function
> call and then
> assigning the leftnode in place of the deleted note.
>
> Any help would be appreciated .
>
>     deletenode<-function(node,key)
>
>     {
>
>     if(identical(node,NULL)==TRUE)
>
>     {
>
>     return(node)
>
>     }
>
>     else
>
>     {
>
>     if(key<node$key)
>
>     {
>
>     node$leftnode<-deletenode(node$leftnode,key)
>
>     }
>
>     if(key>node$key)
>
>     {
>
>     node$leftnode<- deletenode(node$rightnode,key)
>
>     }
>
>     if(key==node$key)
>
>     {
>
>       if((identical(node$leftnode,NULL)==TRUE)&&
>
>                       (identical(node$rightnode,NULL)==FALSE))
>
>     {
>
>     temp<-node
>
>     node<-node$rightnode
>
>     remove(temp)
>
>     return(node)
>
>     }
>
>
>
>     }
>
>     deletenode(root,key)
>
>
> Thanks and Regards
>
> karthik
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Tue Oct  3 18:15:05 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 3 Oct 2017 09:15:05 -0700
Subject: [R] Help on adding a negative binomial density plot
In-Reply-To: <HE1PR07MB0778DFA66A9A7F9EF2A31F83C6720@HE1PR07MB0778.eurprd07.prod.outlook.com>
References: <HE1PR07MB077814B968386868751EE71BC67D0@HE1PR07MB0778.eurprd07.prod.outlook.com>
 <2CB9AF1C-3C0C-4E83-818A-7DE09C0A40F5@comcast.net>
 <HE1PR07MB0778DFA66A9A7F9EF2A31F83C6720@HE1PR07MB0778.eurprd07.prod.outlook.com>
Message-ID: <EB743E8D-108C-4771-A35F-DD78C4AD6C3A@comcast.net>


> On Oct 3, 2017, at 4:20 AM, David <dasolexa at hotmail.com> wrote:
> 
> Dear David,
> 
> thanks ever so much for your answer.
> 
> Do you mean predicting the original values based on the fitted model

Yes.

> and then comparing observed vs. predicted by, for example, a scatterplot?

Not really sure what plot type would be most illuminating. You indicated your original plan was for a histogram, which made me think that either there were a large number of replicates at each point or you planned on aggregating the counts without regard for their locations, i.e. nature of the "Sampling_point" variable. There might be issues related to distances from edges or adjacency that might require a "spatial" treatment (for which packages are available). Also unclear was the number of different cleaning methods or cleaning intensities or cleaning durations, i.e. the structure of the "Cleaning" variable.

As always ... and as pointed out in the Posting Guide ... a complete description of the data structure (and size) of the problem, and an example dataset is most helpful in providing illustrations of code. This might also be more on-topic on the mixed models mailing list if it involves any tricky issues. If it just involves running the code, then this mailing list is probably adequate to the task.

And it's a plain text list so be warned that an html formatted version of a dataset will generally arrive in a mangled form to the list readership.

> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html

-- 
David.


> 
> Thanks,
> 
> David
> De: David Winsemius <dwinsemius at comcast.net>
> Enviado: lunes, 2 de octubre de 2017 18:18:36
> Para: David
> Cc: R-help
> Asunto: Re: [R] Help on adding a negative binomial density plot
>  
> 
> > On Oct 2, 2017, at 2:05 AM, David <dasolexa at hotmail.com> wrote:
> > 
> > Dear list,
> > 
> > 
> > I am just starting on analysis of count data in R 3.4.0. My dataset was obtained from counting particles on a surface before andd after a cleaning process. The sampling positions on the surface are pre-defined and are the same before and after cleaning.  I have ~20% of 0's. I want to know if the cleaning process was useful at reducing the number of particles.
> > 
> > 
> > I first fit a negative binomial model using
> > 
> > 
> >> nbFit<-glmer.nb(Count ~ Cleaning + (1|Sampling_point) , data = myCountDB)
> > 
> > 
> > 
> > I now would like to add a curve to the histogram representing the negative binomial density function fitted to my data using
> > 
> > 
> >> curve(dnbinom(x=, size=, prob=, mu=), add=TRUE)
> 
> Why not use the predict function in that package?
> 
> See ?merMod
> 
> -- 
> David.
> > 
> > 
> > But I am struggling defining the arguments to dnbinom.
> > 
> > 
> > Using the str() function on the nbFit object I see there are many fields returned. And I get lost reading the ?glmer.nb help, greatly because of my lack of knowledge. Which ones should I use?
> > 
> > 
> > Thanks ever so much for your valuable help
> > 
> > 
> > Dave
> > 
> >        [[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > and provide commented, minimal, self-contained, reproducible code.
> 

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From davidecortellino at gmail.com  Tue Oct  3 21:08:31 2017
From: davidecortellino at gmail.com (davide cortellino)
Date: Tue, 3 Oct 2017 21:08:31 +0200
Subject: [R] Multiple test of proportion in r
Message-ID: <4C48D1DE-BDEA-4752-940B-5AECB83B149A@gmail.com>

Dear all



I'm currently facing the following situation. We have run a marketing campaign providing to some members one of two type of coupones. In addition to this, some of these members were already touched in the short past by another campaign.

So I have the following dataset, where:

-mixpre3M= is a flag variable telling us if the customer had already purchased somethign in the past

bono recibido: is the kind of coupon recived by the customer. The type "3euros" is the coupon identifying the customers already touched in the past by a campaign. the type "benchmark" identify the customer who haven't received any coupon (control group)
tran_during: is the N of redeemers or purchasers
enviados: is the number of people included in each group
mixpre3M bono_recibido TRAN_DURING_CAMP_FLG enviados
0 benchmark 5948 33336 
1 benchmark 557 2102 
0 BONO3EUROS 96 1233 
1 BONO3EUROS 17 83 
0 BONO6EUROS 4823 25434 
1 BONO6EUROS 626 1793
What I want achive is if there is a redemption or purchasing rate significatively different between each group, and see between which group there is difference

Now, I have the following doubts:

a. I understand I should run a multiple comparison test, like maybe a GLM with binominal distribution, but I'm not sure it is a correct procedure, considering that some of the groups (for instance the fourth one, with n=83) are quite smaller than the main other groups. Is the model I choose the correct one for this kind of analysis, and I should exclude the smaller groups?

b. I understand this is a kinda similar to a multiple A/B test. Does anyone know any tutorial or material which could help with the topic? Never managed this kind of test before

Many thanks for the help Bests


Inviato da iPad
	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Oct  3 21:57:54 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 3 Oct 2017 12:57:54 -0700
Subject: [R] Multiple test of proportion in r
In-Reply-To: <4C48D1DE-BDEA-4752-940B-5AECB83B149A@gmail.com>
References: <4C48D1DE-BDEA-4752-940B-5AECB83B149A@gmail.com>
Message-ID: <CAGxFJbQ2MZiBSURVWzQiZ8VQS62YCDBAMSUH2jv22Lv=BYq7mQ@mail.gmail.com>

Davide:

1. I think you should seek out a local statistical resource for help, as
you are clearly beyond your statistical capabilities, as you note.

2. Failing that, try a statistical help list like stats.stackexchange.com .
This is an R programming help list, and your questions are not (yet) about
R programming. I would be careful about internet advice, however, as I
think you may need help deciding what sorts of questions to ask even more
than getting answers to those you have asked. Internet help is not very
good at dealing with this imho.

Cheers,
Bert




Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Tue, Oct 3, 2017 at 12:08 PM, davide cortellino <
davidecortellino at gmail.com> wrote:

> Dear all
>
>
>
> I'm currently facing the following situation. We have run a marketing
> campaign providing to some members one of two type of coupones. In addition
> to this, some of these members were already touched in the short past by
> another campaign.
>
> So I have the following dataset, where:
>
> -mixpre3M= is a flag variable telling us if the customer had already
> purchased somethign in the past
>
> bono recibido: is the kind of coupon recived by the customer. The type
> "3euros" is the coupon identifying the customers already touched in the
> past by a campaign. the type "benchmark" identify the customer who haven't
> received any coupon (control group)
> tran_during: is the N of redeemers or purchasers
> enviados: is the number of people included in each group
> mixpre3M bono_recibido TRAN_DURING_CAMP_FLG enviados
> 0 benchmark 5948 33336
> 1 benchmark 557 2102
> 0 BONO3EUROS 96 1233
> 1 BONO3EUROS 17 83
> 0 BONO6EUROS 4823 25434
> 1 BONO6EUROS 626 1793
> What I want achive is if there is a redemption or purchasing rate
> significatively different between each group, and see between which group
> there is difference
>
> Now, I have the following doubts:
>
> a. I understand I should run a multiple comparison test, like maybe a GLM
> with binominal distribution, but I'm not sure it is a correct procedure,
> considering that some of the groups (for instance the fourth one, with
> n=83) are quite smaller than the main other groups. Is the model I choose
> the correct one for this kind of analysis, and I should exclude the smaller
> groups?
>
> b. I understand this is a kinda similar to a multiple A/B test. Does
> anyone know any tutorial or material which could help with the topic? Never
> managed this kind of test before
>
> Many thanks for the help Bests
>
>
> Inviato da iPad
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From hemantsain55 at gmail.com  Wed Oct  4 10:11:08 2017
From: hemantsain55 at gmail.com (Hemant Sain)
Date: Wed, 4 Oct 2017 13:41:08 +0530
Subject: [R] RFM Analysis Help
Message-ID: <CAJL6Qs_NQMot6W-BnnT9hD-UVVrd=M2JWKFN-ia=865TXqXz4Q@mail.gmail.com>

I'm trying to perform a RFM analysis on attached dataset,
i'm able to get the results using the auto_rfm function but i want to
define my own breaks for RFM,
when i tried to define my own breaks i got the identical result i.e 111 for
every ID.
please help me with this with working R script.
Thanks
hemantsain.com

From ericjberger at gmail.com  Wed Oct  4 11:57:07 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Wed, 4 Oct 2017 12:57:07 +0300
Subject: [R] RFM Analysis Help
In-Reply-To: <CAJL6Qs_NQMot6W-BnnT9hD-UVVrd=M2JWKFN-ia=865TXqXz4Q@mail.gmail.com>
References: <CAJL6Qs_NQMot6W-BnnT9hD-UVVrd=M2JWKFN-ia=865TXqXz4Q@mail.gmail.com>
Message-ID: <CAGgJW75xogW8JWCk93q2yYxSMxdHSAhdMXT06zAKBY0bQ88=hw@mail.gmail.com>

I don't see any attached dataset.

On Wed, Oct 4, 2017 at 11:11 AM, Hemant Sain <hemantsain55 at gmail.com> wrote:

> I'm trying to perform a RFM analysis on attached dataset,
> i'm able to get the results using the auto_rfm function but i want to
> define my own breaks for RFM,
> when i tried to define my own breaks i got the identical result i.e 111 for
> every ID.
> please help me with this with working R script.
> Thanks
> hemantsain.com
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Wed Oct  4 15:49:32 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 04 Oct 2017 14:49:32 +0100
Subject: [R] RFM Analysis Help
In-Reply-To: <CAJL6Qs_NQMot6W-BnnT9hD-UVVrd=M2JWKFN-ia=865TXqXz4Q@mail.gmail.com>
References: <CAJL6Qs_NQMot6W-BnnT9hD-UVVrd=M2JWKFN-ia=865TXqXz4Q@mail.gmail.com>
Message-ID: <1B29E677-88E5-4A14-A02C-57205166083F@dcn.davis.ca.us>

You won't deliver your question successfully to the mailing list if you don't follow the Posting Guide, particularly with regard to attachments. The most reliable way is to include your question in the text of your PLAIN TEXT format email with no attachments. Yes, there is an option in Gmail to turn off html format. 
-- 
Sent from my phone. Please excuse my brevity.

On October 4, 2017 9:11:08 AM GMT+01:00, Hemant Sain <hemantsain55 at gmail.com> wrote:
>I'm trying to perform a RFM analysis on attached dataset,
>i'm able to get the results using the auto_rfm function but i want to
>define my own breaks for RFM,
>when i tried to define my own breaks i got the identical result i.e 111
>for
>every ID.
>please help me with this with working R script.
>Thanks
>hemantsain.com
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From olehost at gmail.com  Wed Oct  4 14:47:47 2017
From: olehost at gmail.com (=?UTF-8?B?T2xlIEjDuHN0?=)
Date: Wed, 4 Oct 2017 14:47:47 +0200
Subject: [R] Issue calling MICE package
Message-ID: <CANxTjoxr8Sb4hB4hgwXBwtzAVnAh7exGjmfBZvRuBrLhCmJd_g@mail.gmail.com>

I want to call the mice function from the MICE package from my own package.
However I run into this issue, which can be reproduced on the command line:

mice::mice(airquality)#> Error in check.method(setup, data): The
following functions were not found: mice.impute.pmm, mice.impute.pmm

I have no problems when doing

library(mice)
mice(airquality)

Is this a bug or am I missing something?

Thanks,
Ole H?st

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Wed Oct  4 17:00:48 2017
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Wed, 4 Oct 2017 16:00:48 +0100
Subject: [R] Issue calling MICE package
In-Reply-To: <CANxTjoxr8Sb4hB4hgwXBwtzAVnAh7exGjmfBZvRuBrLhCmJd_g@mail.gmail.com>
References: <CANxTjoxr8Sb4hB4hgwXBwtzAVnAh7exGjmfBZvRuBrLhCmJd_g@mail.gmail.com>
Message-ID: <dafe0645-f6bf-3a5a-12d9-3ce326c6ba58@dewey.myzen.co.uk>

Dear Ole

One of the experts may be able to diagnose this without extra 
information but I suspect you have not got the right magic in your 
NAMESPACE file in your package. You may need to re-read section 1.5.1 of 
the Writing R extensions manual.

Michael

On 04/10/2017 13:47, Ole H?st wrote:
> I want to call the mice function from the MICE package from my own package.
> However I run into this issue, which can be reproduced on the command line:
> 
> mice::mice(airquality)#> Error in check.method(setup, data): The
> following functions were not found: mice.impute.pmm, mice.impute.pmm
> 
> I have no problems when doing
> 
> library(mice)
> mice(airquality)
> 
> Is this a bug or am I missing something?
> 
> Thanks,
> Ole H?st
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ---
> This email has been checked for viruses by AVG.
> http://www.avg.com
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From pdalgd at gmail.com  Wed Oct  4 19:16:41 2017
From: pdalgd at gmail.com (Peter Dalgaard)
Date: Wed, 4 Oct 2017 19:16:41 +0200
Subject: [R] Issue calling MICE package
In-Reply-To: <dafe0645-f6bf-3a5a-12d9-3ce326c6ba58@dewey.myzen.co.uk>
References: <CANxTjoxr8Sb4hB4hgwXBwtzAVnAh7exGjmfBZvRuBrLhCmJd_g@mail.gmail.com>
 <dafe0645-f6bf-3a5a-12d9-3ce326c6ba58@dewey.myzen.co.uk>
Message-ID: <54DB4B8E-A7F0-45FE-938D-AE02D9F18863@gmail.com>

IIUC, this would be an isssue with MICE (or rather "mice"), which isn't Ole's. It could be a namespace issue, but it could also be that some start-up code is not executed if library() is bypasses (see .onAttach et al.).

-pd

> On 4 Oct 2017, at 17:00 , Michael Dewey <lists at dewey.myzen.co.uk> wrote:
> 
> Dear Ole
> 
> One of the experts may be able to diagnose this without extra information but I suspect you have not got the right magic in your NAMESPACE file in your package. You may need to re-read section 1.5.1 of the Writing R extensions manual.
> 
> Michael
> 
> On 04/10/2017 13:47, Ole H?st wrote:
>> I want to call the mice function from the MICE package from my own package.
>> However I run into this issue, which can be reproduced on the command line:
>> mice::mice(airquality)#> Error in check.method(setup, data): The
>> following functions were not found: mice.impute.pmm, mice.impute.pmm
>> I have no problems when doing
>> library(mice)
>> mice(airquality)
>> Is this a bug or am I missing something?
>> Thanks,
>> Ole H?st
>> 	[[alternative HTML version deleted]]
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> ---
>> This email has been checked for viruses by AVG.
>> http://www.avg.com
> 
> -- 
> Michael
> http://www.dewey.myzen.co.uk/home.html
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From r.turner at auckland.ac.nz  Wed Oct  4 21:58:53 2017
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 5 Oct 2017 08:58:53 +1300
Subject: [R] [FORGED]  RFM Analysis Help
In-Reply-To: <CAJL6Qs_NQMot6W-BnnT9hD-UVVrd=M2JWKFN-ia=865TXqXz4Q@mail.gmail.com>
References: <CAJL6Qs_NQMot6W-BnnT9hD-UVVrd=M2JWKFN-ia=865TXqXz4Q@mail.gmail.com>
Message-ID: <0a9a6e15-69db-3592-be75-4aeeb48123ef@auckland.ac.nz>


On 04/10/17 21:11, Hemant Sain wrote:

> I'm trying to perform a RFM analysis on attached dataset,

<SNIP>

Is "RFM" the Russian [1] version of "RTFM"?

(Sorry, just couldn't resist!)

cheers,

Rolf Turner

[1] The Russian language had no definite articles.  Or indefinite
     ones either, for that matter.


-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From davidsmi at microsoft.com  Wed Oct  4 22:08:48 2017
From: davidsmi at microsoft.com (David Smith)
Date: Wed, 4 Oct 2017 20:08:48 +0000
Subject: [R] Revolutions blog: September 2017 roundup
Message-ID: <BN6PR21MB0497AD7EB7E8DB947BB987E3C8730@BN6PR21MB0497.namprd21.prod.outlook.com>

Since 2008, Microsoft (formerly Revolution Analytics) staff and guests
have written about R every weekday at the Revolutions blog
(http://blog.revolutionanalytics.com) and every month I post a summary
of articles from the previous month of particular interest to readers
of r-help.

In case you missed them, here are some articles related to R from the
month of September:

The mathpix package converts images of hand-drawn equations to their LaTeX
equivalent:
http://blog.revolutionanalytics.com/2017/09/convert-hand-drawn-equations-to-latex-with-the-mathpix-package.html

R 3.4.2 is released:
http://blog.revolutionanalytics.com/2017/09/r-342-is-released.html

Applying image featurization to the problem of classifying wood knots in lumber:
http://blog.revolutionanalytics.com/2017/09/wood-knots.html

Microsoft ML Server 9.2, which provides operationalization for R, is released:
http://blog.revolutionanalytics.com/2017/09/microsoft-ml-server-92.html 

A roundup of news stories related to R from the Ignite conference:
http://blog.revolutionanalytics.com/2017/09/news-from-ignite.html

A tutorial on launching a Spark cluster with R on HDInsight:
http://blog.revolutionanalytics.com/2017/09/hdinsight-tutorial.html

A map of pirate attacks, and an ethics lesson on scraping data:
http://blog.revolutionanalytics.com/2017/09/pirating-pirate-data-for-pirate-day.html

A preview of ALTREP, the new vector implementation that promises to bring
performance improvements to the R engine:
http://blog.revolutionanalytics.com/2017/09/altrep-preview.html

The USGS used R to visualize the rainfall from hurricanes Irma
http://blog.revolutionanalytics.com/2017/09/hurricane-irmas-rains-visualized-with-r.html
and Harvey
http://blog.revolutionanalytics.com/2017/09/hurricane-harvey-usgs.html

Highlights of some R applications presented at the EARL London conference:
http://blog.revolutionanalytics.com/2017/09/recap-earl-london-2017.html

Microsoft R Open 3.4.1, featuring R 3.4.1, is now available:
http://blog.revolutionanalytics.com/2017/09/mro-341-now-available.html

A introduction to R data frames for SQL Server users:
http://blog.revolutionanalytics.com/2017/09/r-services-working-with-data-frames.html

The "newsflash" package extracts trending topics from cable news closed
captioning streams: http://blog.revolutionanalytics.com/2017/09/newsflash.html

"Data Visualization for Social Science", a new online textbook with a focus on
ggplot2:
http://blog.revolutionanalytics.com/2017/09/data-visualization-for-social-science.html

Knime 3.4 now includes connections to Microsoft R and Azure data services:
http://blog.revolutionanalytics.com/2017/09/knime-azure.html

A preview of the EARL London 2017 conference:
http://blog.revolutionanalytics.com/2017/09/preview-of-earl-london-2017.html

"Practical Data Science for Stats", a PeerJ Preprints collections of R-related
papers:
http://blog.revolutionanalytics.com/2017/09/practical-data-science-for-stats.html

And some general interest stories (not necessarily related to R):

* Music by Michael Kiwanuka
http://blog.revolutionanalytics.com/2017/09/because-its-friday-big-little-songs.html

* A talk on artificial intelligence and runaway reward functions:
http://blog.revolutionanalytics.com/2017/09/because-its-friday-blue-skies-or-skynet.html

* SpaceX's blooper reel:
http://blog.revolutionanalytics.com/2017/09/because-its-friday-spacex.html

* A new edX course on Artifical Intelligence development:
http://blog.revolutionanalytics.com/2017/09/a-new-edx-course-on-building-artificial-intelligence-applications.html

* A search for syncopation in Radiohead's "Videotape":
http://blog.revolutionanalytics.com/2017/09/because-its-friday-radiohead-videotape.html

* A system for lifelike synthesis of a speaker's video and audio:
http://blog.revolutionanalytics.com/2017/09/friday-fake-views.html

As always, thanks for the comments and please keep sending suggestions to
me at davidsmi at microsoft.com or via Twitter (I'm @revodavid).

Cheers,
# David

-- 
David M Smith <davidsmi at microsoft.com>
R Community Lead, Microsoft AI & Research? 
Tel: +1 (312) 9205766 (Chicago IL, USA)
Twitter: @revodavid | Blog: ?http://blog.revolutionanalytics.com


From olehost at gmail.com  Thu Oct  5 09:28:28 2017
From: olehost at gmail.com (=?UTF-8?B?T2xlIEjDuHN0?=)
Date: Thu, 5 Oct 2017 09:28:28 +0200
Subject: [R] Issue calling MICE package
In-Reply-To: <54DB4B8E-A7F0-45FE-938D-AE02D9F18863@gmail.com>
References: <CANxTjoxr8Sb4hB4hgwXBwtzAVnAh7exGjmfBZvRuBrLhCmJd_g@mail.gmail.com>
 <dafe0645-f6bf-3a5a-12d9-3ce326c6ba58@dewey.myzen.co.uk>
 <54DB4B8E-A7F0-45FE-938D-AE02D9F18863@gmail.com>
Message-ID: <CANxTjoyvUP=wDVr_7bnQQudqb8dMH0XzAmpyLG64zcLJMaR6nw@mail.gmail.com>

Sorry, I was not clear enough. The reason I want to use mice::mice() rather
than library(mice); mice() is that I want to call it from my own package.
But the reprex works from the command line as well, straight after
launching R:

  mice::mice(airquality)
  #> Error in check.method(setup, data): The following functions were not
found: mice.impute.pmm, mice.impute.pmm

The mice.impute functions are exported from the mice package but not
found.I cannot figure out why but I was hoping someone else had come across
the issue.

Interestingly, I can circumvent the problem by doing
  foreach(i = ...) %dopar% {mice::mice()}

Thanks for your help,
Ole



On Wed, Oct 4, 2017 at 7:16 PM, Peter Dalgaard <pdalgd at gmail.com> wrote:

> IIUC, this would be an isssue with MICE (or rather "mice"), which isn't
> Ole's. It could be a namespace issue, but it could also be that some
> start-up code is not executed if library() is bypasses (see .onAttach et
> al.).
>
> -pd
>
> > On 4 Oct 2017, at 17:00 , Michael Dewey <lists at dewey.myzen.co.uk> wrote:
> >
> > Dear Ole
> >
> > One of the experts may be able to diagnose this without extra
> information but I suspect you have not got the right magic in your
> NAMESPACE file in your package. You may need to re-read section 1.5.1 of
> the Writing R extensions manual.
> >
> > Michael
> >
> > On 04/10/2017 13:47, Ole H?st wrote:
> >> I want to call the mice function from the MICE package from my own
> package.
> >> However I run into this issue, which can be reproduced on the command
> line:
> >> mice::mice(airquality)#> Error in check.method(setup, data): The
> >> following functions were not found: mice.impute.pmm, mice.impute.pmm
> >> I have no problems when doing
> >> library(mice)
> >> mice(airquality)
> >> Is this a bug or am I missing something?
> >> Thanks,
> >> Ole H?st
> >>      [[alternative HTML version deleted]]
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >> ---
> >> This email has been checked for viruses by AVG.
> >> http://www.avg.com
> >
> > --
> > Michael
> > http://www.dewey.myzen.co.uk/home.html
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Thu Oct  5 12:41:58 2017
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 5 Oct 2017 21:41:58 +1100
Subject: [R] RFM Analysis Help
In-Reply-To: <CAJL6Qs9O5TiT_LKHaROp4JesVtPTQLGajbEb-Gy6X5mzaPQGzA@mail.gmail.com>
References: <CAJL6Qs_NQMot6W-BnnT9hD-UVVrd=M2JWKFN-ia=865TXqXz4Q@mail.gmail.com>
 <CA+8X3fUHHmq4TWjFBFW9PxmccFYRghtobX=OfejsRwc=aqYr-Q@mail.gmail.com>
 <CAJL6Qs9O5TiT_LKHaROp4JesVtPTQLGajbEb-Gy6X5mzaPQGzA@mail.gmail.com>
Message-ID: <CA+8X3fWOc6-WQnx1-EizfFFmXsKzjROSFRjvisZs_dgAwK+WbA@mail.gmail.com>

Hi Hemant,
As I suspected, the code broke when I got to the line:

result <- rfm_auto(df, id="user_id", payment ="subtotal_amount",
date="created_at")

Error in rfm_auto(df, id = "user_id", payment = "subtotal_amount", date = "cr
eated_at") :
 could not find function "rfm_auto"

It looks like you are using the hoxo-m/easyRFM function hosted on
GitHub. I may get a chance to look at this, but I can't promise
anything.

Jim



On Thu, Oct 5, 2017 at 5:28 PM, Hemant Sain <hemantsain55 at gmail.com> wrote:
> Hey Jim,
> I'm trying to perform a RFM analysis on a dataset,
> I'm able to get the results using the auto_rfm function but i want to define
> my own breaks values for RFM,
> when i tried to define my own breaks i got the identical result i.e 111 for
> every ID.
> please help me with this with working R script.
>
> Thanks
>
>
>
> Dataset:
>
> user_id subtotal_amount created_at
>
> 3451945 19.32 6/11/2017 17:40
> 5404261 20 6/16/2017 22:45
>
> 3572177 9.78 7/6/2017 0:41
> 1197515 11.97 5/20/2017 17:48
> 7288355 14.76 6/5/2017 17:48
> 3071276 7.99 6/11/2017 0:13
> 8568400 15.98 6/22/2017 0:59
> 429475 7.99 6/8/2017 18:14
> 6805938 13.97 7/1/2017 23:30
> 561442 11.67 6/22/2017 18:13
> 1127373 11.27 6/11/2017 16:43
> 5973764 12.07 6/19/2017 22:43
> 683302 12.37 6/19/2017 17:18
> 391019 26.64 6/23/2017 23:57
> 580790 22.85 5/26/2017 23:34
> 1315314 6.29 6/18/2017 23:10
> 6980574 8.67 5/13/2017 20:21
> 8279240 17.26 6/2/2017 2:46
> 8700821 9.48 6/26/2017 1:47
> 778933 13.05 5/11/2017 17:50
> 1028301 9.47 5/31/2017 20:56
> 8305179 8.49 6/16/2017 0:17
> 8294420 12.65 6/11/2017 18:04
> 5775051 11.28 6/26/2017 17:13
> 3527917 7.99 6/4/2017 21:31
> 7434689 9.78 5/5/2017 16:43
> 6299124 20.65 5/25/2017 22:55
> 407736 6.88 6/5/2017 23:40
> 6207916 11.48 6/17/2017 16:31
> 2284913 10.08 7/21/2017 16:22
> 5833389 15.67 7/31/2017 17:19
> 1537907 22.63 7/4/2017 17:23
> 8791577 12.97 7/27/2017 0:09
> 7390743 18.36 5/7/2017 16:22
> 8562057 17.64 5/27/2017 19:07
> 393153 7.98 7/19/2017 16:42
> 6764358 15.37 7/14/2017 20:20
> 6444042 9.28 5/3/2017 19:54
> 442647 15.96 7/22/2017 0:01
> 6665810 7.99 7/3/2017 16:20
> 3318928 7.99 6/11/2017 22:36
> 565493 24.56 7/11/2017 16:06
> 3337179 16.86 5/31/2017 0:20
> 394651 21.67 5/22/2017 23:30
> 421849 23.57 5/24/2017 17:12
> 404111 22.06 5/22/2017 23:05
> 3967182 8.29 7/23/2017 23:00
> 8380345 10.38 6/22/2017 15:11
> 6843512 6.49 6/16/2017 2:11
> 3562940 8.18 7/18/2017 17:09
> 678953 6.99 7/7/2017 16:37
> 477935 8.47 7/19/2017 16:07
> 8069635 6.79 7/27/2017 17:29
> 435287 15.98 7/25/2017 21:39
> 7210916 11.47 7/12/2017 17:30
> 320190 16.86 6/10/2017 23:16
> 7101677 9.28 6/6/2017 20:50
> 1358520 16.65 7/10/2017 17:52
> 485601 5.99 7/18/2017 17:46
> 7288355 24.32 7/28/2017 0:01
> 8657204 6.46 6/26/2017 22:32
> 368087 15.26 6/17/2017 17:20
> 5532715 11.78 7/24/2017 16:54
> 318181 11.16 5/14/2017 23:01
> 457094 10.92 6/15/2017 18:43
> 8733533 9.67 6/16/2017 17:06
> 2229405 15.37 6/16/2017 16:59
> 654301 11.77 6/24/2017 0:16
> 440110 26.85 5/3/2017 0:58
> 478324 8.79 7/10/2017 17:02
> 927885 17.05 5/20/2017 15:41
> 1489397 8.47 7/26/2017 17:55
> 454200 13.94 7/26/2017 1:11
> 7235501 13.81 5/19/2017 1:13
> 527673 9.97 5/20/2017 0:37
> 2438553 7.99 6/18/2017 23:09
> 2592988 39.8 6/24/2017 17:37
> 538511 15.15 6/11/2017 16:58
> 481081 14.87 5/19/2017 17:57
> 1999017 18.06 7/21/2017 16:28
> 3925889 8.08 6/26/2017 16:17
> 1046802 10.67 7/19/2017 23:07
> 434850 7.49 7/2/2017 23:00
> 370681 8.47 7/14/2017 18:19
> 3554336 21.54 7/28/2017 16:50
> 5731193 9.67 5/22/2017 16:41
> 1062134 15.05 6/5/2017 21:13
> 408175 17.94 5/23/2017 20:18
> 8733533 8.38 7/15/2017 18:16
> 588771 13.87 6/25/2017 23:49
> 6338225 6.79 7/1/2017 18:59
> 6340638 17.95 6/15/2017 23:25
> 6926244 9.07 7/17/2017 19:02
> 8880079 10.58 6/14/2017 20:37
> 7333070 20.76 5/6/2017 17:22
> 6409065 13.76 7/30/2017 18:00
> 946735 7.57 7/21/2017 19:33
> 386328 15.98 7/21/2017 1:50
> 377431 9.16 5/30/2017 22:53
> 1870101 10.28 6/1/2017 17:00
> 318128 10.74 7/15/2017 22:16
> 3122611 7.68 6/27/2017 18:01
> 7528015 17.16 6/26/2017 21:10
> 6993335 8.57 6/13/2017 17:14
> 3424400 9.27 5/11/2017 18:51
> 7515441 15.77 6/26/2017 22:25
> 3962258 10.26 6/21/2017 1:33
> 6470596 12.16 7/31/2017 17:57
> 3415331 8.08 7/17/2017 19:19
> 3301515 10 7/2/2017 20:09
> 5359396 9.47 6/26/2017 19:44
> 7975103 13.87 6/29/2017 17:08
> 440393 6.18 5/3/2017 16:48
> 2170350 17.26 7/29/2017 23:11
> 370726 19.25 6/2/2017 16:03
> 761305 21.82 7/14/2017 17:25
> 7849625 12.03 7/24/2017 16:50
> 321713 7.68 7/12/2017 16:49
> 7300770 9.57 6/27/2017 16:06
> 1095644 8.47 5/9/2017 14:40
> 321315 17.94 5/4/2017 16:07
> 7579307 11.78 7/6/2017 19:42
> 7981910 7.99 6/11/2017 20:30
> 2731720 56.86 7/21/2017 16:35
> 628140 17.45 7/20/2017 17:31
> 321926 9.48 7/25/2017 17:25
> 7000119 9.37 5/28/2017 23:37
> 9101259 14.86 6/26/2017 16:31
> 8682720 17.73 7/15/2017 17:29
> 445002 30.25 5/4/2017 23:05
> 7319495 13.48 6/26/2017 17:09
> 7900556 7.99 5/28/2017 20:17
> 927932 11.37 5/7/2017 22:35
> 674966 6.89 6/28/2017 21:27
> 323736 33.25 5/17/2017 18:02
> 1096148 12.46 7/28/2017 17:36
> 3195598 6.98 5/16/2017 19:30
> 685341 7.83 6/10/2017 17:43
> 7006511 11.15 7/10/2017 20:34
> 320245 20.47 5/20/2017 16:03
> 2387580 7.36 6/26/2017 16:54
> 492746 7.99 6/25/2017 0:59
> 974050 8.49 7/16/2017 19:01
> 1706322 14.99 5/24/2017 20:22
> 7288355 12.47 5/19/2017 1:56
> 50496250 16.58 7/26/2017 0:24
> 447509 6.29 5/5/2017 16:32
> 1330217 8.27 5/19/2017 17:34
> 2154446 7.99 7/6/2017 16:41
> 1038646 8.69 5/13/2017 22:35
> 314670 8.49 5/29/2017 23:09
> 563231 26.32 7/31/2017 18:16
> 699366 12 7/4/2017 17:46
> 8306831 6.99 5/22/2017 21:58
> 4378079 10.17 7/10/2017 18:07
> 8307283 15.87 5/31/2017 19:22
> 6493978 17.32 7/18/2017 16:13
> 1299335 17.32 7/19/2017 23:24
> 1041199 11.07 5/25/2017 17:32
> 956047 7.27 5/24/2017 16:35
> 377134 19.25 7/31/2017 18:03
> 3395660 20.56 7/24/2017 18:25
> 482106 51.29 7/14/2017 17:02
> 521363 16.27 7/18/2017 22:03
> 537518 10.98 6/13/2017 21:48
> 1943828 11.77 6/29/2017 18:30
> 606395 10.98 7/11/2017 1:15
> 1228153 15.34 5/1/2017 18:48
> 6437041 10.27 5/12/2017 15:39
> 3109401 10.28 6/26/2017 16:43
> 530302 8.57 7/10/2017 18:10
> 3109401 28.94 6/22/2017 15:43
> 6461282 12.58 6/9/2017 22:52
> 8296976 16.15 5/29/2017 23:11
> 2018954 9.98 7/14/2017 23:33
> 6241196 19.56 7/14/2017 22:54
> 8217936 11.47 6/16/2017 17:10
> 463122 8.29 7/15/2017 18:28
> 532110 10.46 7/26/2017 16:53
> 9100252 15.07 6/26/2017 15:43
> 439030 9.37 7/19/2017 17:23
> 326157 11.73 6/13/2017 0:27
> 1146325 21.85 5/16/2017 0:30
> 1502399 8.29 7/6/2017 18:57
> 2623130 7.99 5/22/2017 15:57
> 2747554 11.28 5/7/2017 17:15
> 1645383 7.77 5/31/2017 18:16
> 2739083 6.66 7/3/2017 17:53
> 6004810 10.08 6/4/2017 19:39
> 3759866 14.06 7/1/2017 18:19
> 770582 10.59 5/11/2017 22:01
> 1186104 19.92 6/12/2017 17:19
> 636778 7.99 5/5/2017 17:42
> 6147540 12.82 7/23/2017 22:26
> 5813054 14.88 6/26/2017 17:00
> 4115178 12 7/3/2017 21:24
> 8964829 7.99 6/27/2017 21:28
> 3944025 9.96 5/16/2017 16:33
> 8227862 10.48 7/23/2017 19:14
> 7104071 14.46 6/14/2017 18:53
> 4255115 7.38 5/21/2017 21:36
> 2550433 8.49 6/8/2017 17:30
> 6824172 13.17 6/14/2017 20:36
> 455032 6.97 5/17/2017 17:38
> 1206605 10.67 6/24/2017 21:08
> 337571 8.67 7/7/2017 18:52
> 633665 11.98 7/9/2017 17:16
> 2202055 8.47 6/1/2017 17:54
> 3581705 15.15 6/4/2017 17:48
> 50484076 8.28 7/26/2017 18:44
> 1145712 14.85 7/12/2017 17:48
> 599316 13.47 5/5/2017 17:35
> 321097 17.85 5/10/2017 16:17
> 316951 6.88 7/11/2017 16:59
> 566170 8.49 6/11/2017 17:19
> 4183643 9.67 6/20/2017 16:54
> 3312216 27.02 6/15/2017 20:22
> 507997 8.78 5/31/2017 20:20
> 1564977 10.77 5/15/2017 18:26
> 1063879 10.67 5/11/2017 16:56
> 6768611 10.08 6/18/2017 16:02
> 466723 9.52 5/12/2017 0:21
> 461771 9.78 6/9/2017 19:00
> 1160016 15.05 6/27/2017 16:31
> 7808075 7.99 5/10/2017 18:05
> 3388025 7.28 5/26/2017 21:04
> 367393 16.94 7/31/2017 16:09
> 748047 8.37 7/12/2017 20:35
> 5312887 7.18 7/9/2017 23:32
> 9503792 44.46 7/16/2017 23:59
> 794037 16.66 7/28/2017 23:19
> 7742605 13.28 5/19/2017 17:45
> 424303 8.47 7/3/2017 16:56
> 423285 10.07 5/16/2017 16:30
> 744532 15.45 5/10/2017 21:47
> 1315758 12.45 5/4/2017 17:17
> 5768484 9.28 6/1/2017 16:47
> 1749414 7.88 7/23/2017 18:19
> 1943828 11.77 5/24/2017 18:45
> 6554452 9.58 7/14/2017 20:16
> 7333070 6.18 5/17/2017 22:34
> 6335238 9.27 5/23/2017 0:12
> 7898943 14.96 7/27/2017 18:50
> 439121 8.78 6/28/2017 16:56
> 321315 9.47 6/2/2017 16:43
> 970063 9.67 5/15/2017 13:20
> 744532 17.54 6/11/2017 19:27
> 380324 9.47 6/24/2017 1:00
> 50512368 9.47 7/27/2017 19:13
> 475428 9.48 6/17/2017 18:44
> 378535 17.36 7/6/2017 17:08
> 483547 11.29 7/31/2017 17:53
> 7825238 9.87 5/15/2017 17:55
> 3881856 8.08 6/26/2017 17:39
> 1188351 6.77 7/18/2017 17:29
> 3955854 9.37 5/1/2017 17:18
> 431483 7.88 7/2/2017 21:45
> 6906319 7.97 5/31/2017 17:05
> 2414336 17.26 5/10/2017 18:20
> 838785 9.47 6/28/2017 17:59
> 317459 9.97 7/10/2017 16:50
> 439914 9.67 7/14/2017 18:20
> 760177 17.94 6/27/2017 16:09
> 318091 14.77 5/14/2017 17:26
> 1274436 10.28 7/26/2017 17:29
> 2626118 8.49 7/7/2017 17:29
> 3224795 13.28 7/27/2017 23:22
> 7210916 10.78 6/30/2017 17:09
> 374293 13.58 7/28/2017 23:46
> 466723 8.07 5/16/2017 16:34
> 1591942 7.99 6/22/2017 17:04
> 1967943 6.49 5/5/2017 16:42
> 477759 6.38 5/12/2017 16:50
> 331106 10.67 7/5/2017 17:05
> 1170093 7.99 5/8/2017 18:37
> 3993066 22.26 6/4/2017 23:34
> 385427 10.78 5/10/2017 21:07
> 3563453 8.29 6/6/2017 16:43
> 992545 18.35 5/3/2017 15:35
> 430623 15.17 5/5/2017 16:10
> 7288355 14.76 6/3/2017 18:38
> 9124832 8.49 7/26/2017 21:53
> 7737402 14.54 5/17/2017 23:30
> 3913032 8.18 6/19/2017 17:37
> 335627 17.35 5/31/2017 16:34
> 1340737 11.87 7/12/2017 21:47
> 468557 7.98 5/10/2017 16:47
> 458337 7.49 5/23/2017 16:36
> 606287 10.48 5/1/2017 16:33
> 2321336 8.38 5/22/2017 16:25
>
>
>
>
>
> R code
>
>
> library(data.table)
> df = fread("rfm_90.csv", header = T , stringsAsFactors = FALSE, check.names
> = FALSE)[-1,]
>
> sum(is.na(df))
>
> df$user_id = as.integer(df$user_id)
> df$subtotal_amount = as.numeric(df$subtotal_amount)
> df$created_at = as.POSIXct(df$created_at)
> df$created_at = as.Date(df$created_at)
>
>
> occurences = table(unlist(df$user_id))
> occurences = as.data.frame(occurences)
> occurences = occurences[order(-occurences$Freq),]
>
>  r <-c(30,60,90)
>  f <-c(2,5,8)
>  m <-c(10,20,30)
>
>  getScoreWithBreaks <- function(df,r,f,m) {
>
>    ## scoring the Recency
>    len = length(r)
>    R_Score <- c(rep(1,length(df[,1])))
>    df <- cbind(df,R_Score)
>    for(i in 1:len){
>      if(i == 1){
>        p1=0
>      }else{
>        p1=r[i-1]
>      }
>      p2=r[i]
>
>      if(dim(df[p1<df$Recency & df$Recency<=p2,])[1]>0) df[p1<df$Recency &
> df$Recency<=p2,]$R_Score = len - i+ 2
>    }
>
>    ## scoring the Frequency
>    len = length(f)
>    F_Score <- c(rep(1,length(df[,1])))
>    df <- cbind(df,F_Score)
>    for(i in 1:len){
>      if(i == 1){
>        p1=0
>      }else{
>        p1=f[i-1]
>      }
>      p2=f[i]
>
>      if(dim(df[p1<df$Frequency & df$Frequency<=p2,])[1]>0)
> df[p1<df$Frequency & df$Frequency<=p2,]$F_Score = i
>    }
>    if(dim(df[f[len]<df$Frequency,])[1]>0) df[f[len]<df$Frequency,]$F_Score =
> len+1
>
>    ## scoring the Monetary
>    len = length(m)
>    M_Score <- c(rep(1,length(df[,1])))
>    df <- cbind(df,M_Score)
>    for(i in 1:len){
>      if(i == 1){
>        p1=0
>      }else{
>        p1=m[i-1]
>      }
>      p2=m[i]
>
>      if(dim(df[p1<df$Monetary & df$Monetary<=p2,])[1]>0) df[p1<df$Monetary &
> df$Monetary<=p2,]$M_Score = i
>    }
>    if(dim(df[m[len]<df$Monetary,])[1]>0) df[m[len]<df$Monetary,]$M_Score =
> len+1
>
>    #order the dfframe by R_Score, F_Score, and M_Score desc
>    df <- df[order(-df$R_Score,-df$F_Score,-df$M_Score),]
>
>    # caculate the total score
>    Total_Score <- c(100*df$R_Score + 10*df$F_Score+df$M_Score)
>
>    df <- cbind(df,Total_Score)
>
>    return(df)
>
>  }
>
>
>
>  df2<-getScoreWithBreaks(df,r,f,m)
>
> # result <- rfm_auto(df, id="user_id", payment ="subtotal_amount",
> date="created_at")
>
> breaks = result$breaks
> classes = result$classes
> summary = result$get_table
>
> classes = as.data.frame(classes)
> breaks = as.data.frame(breaks)
> summary = as.data.frame(summary)
>
>
>
> On 5 October 2017 at 02:52, Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>> Hi Hemant,
>> No data or source code came through with your message. Try including
>> the source code in the body of the message and if your data is small,
>> the output of the "dput" function. Otherwise make up a small dataset
>> that does the same thing you describe. Also let us know what R package
>> you are using.
>>
>> Jim
>>
>>
>> On Wed, Oct 4, 2017 at 7:11 PM, Hemant Sain <hemantsain55 at gmail.com>
>> wrote:
>> > I'm trying to perform a RFM analysis on attached dataset,
>> > i'm able to get the results using the auto_rfm function but i want to
>> > define my own breaks for RFM,
>> > when i tried to define my own breaks i got the identical result i.e 111
>> > for
>> > every ID.
>> > please help me with this with working R script.
>> > Thanks
>> > hemantsain.com
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> --
> hemantsain.com


From jrkrideau at yahoo.ca  Thu Oct  5 15:49:20 2017
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 5 Oct 2017 13:49:20 +0000 (UTC)
Subject: [R] Adding non-data line to legend ggplot2 Maximum Contaminant
 Level
In-Reply-To: <CACftpvqBvvStdwRLV3zMjVCg=7Q4YC3JNRJz-n06fYGvyVuJqA@mail.gmail.com>
References: <CACftpvrcJea_P-3VqBS5c7s5Mxk=BfSREOia3TFk6ZtAN92sDA@mail.gmail.com>
 <1288671895.571249.1506854643259@mail.yahoo.com>
 <CACftpvpUuBd6wKEPcQhdtPgUYgFpunm1DUuEhF4T7t9RPn+qtw@mail.gmail.com>
 <1405576678.1024110.1506938994002@mail.yahoo.com>
 <CACftpvqBvvStdwRLV3zMjVCg=7Q4YC3JNRJz-n06fYGvyVuJqA@mail.gmail.com>
Message-ID: <645649565.2020671.1507211360156@mail.yahoo.com>

Well, here is one way but it seems a bit clumsy.
In words, I created a new data.frame with "250" in the Chloride vector and "SMCL" in the Detections vector and supplessed one legend. 
Warning: For my convenience I am using different data.frame names . 

library(ggplot2)

MyData <-read.csv("http://doylesdartden.com/Stats/TimeSeriesExample.csv", sep=",")

MyData$Detections <- ifelse(MyData$D_Chloride ==1, "Detected", "NonDetect")


dat1 <-? MyData[, c(1, 3, 4, 6)]

dat2 <-? dat1[, c(1,2)]

dat2$Detections <-? rep("SMCL", nrow(dat2))
dat$Chloride <-? rep(250, nrow(dat2))

dat3 <-? rbind(dat1, dat2)

p <-? ggplot(dat3, aes(Year, Chloride, colour = Detections)) + 
?????????? geom_point(aes(shape=Detections), show.legend = F) +
?????????? geom_line( aes(Year, Chloride))

    On Tuesday, October 3, 2017, 3:17:00 PM EDT, David Doyle <kydaviddoyle at gmail.com> wrote:  
 
 Hi John,
I used your recommendation of adding a new variable (see below) and it is plotting the line at the 250 mg/L mark but I still can not figure out how to get it added to the legend.
Thanks again for your help.
David


#Loads the ggplot2 package.? NOTE you have to install it before hand.library(ggplot2)
##This loads your data from your worksheetMyData <-read.csv("http://doylesdartden.com/Stats/TimeSeriesExample.csv", sep=",")

#Sets which are detections and nondetects
MyData$Detections <- ifelse(MyData$D_Chloride ==1, "Detected", "NonDetect")
# adds my screening LevelMyData$SMCL <- rep(250, nrow(MyData))
#does the plotp <- ggplot(data = MyData, aes(x=Year, y=Chloride, col=Detections)) +? geom_point(aes(shape=Detections)) +? geom_line(data = MyData,aes(y=SMCL))+?? ?#sets the detect vs. non-detect colors? scale_colour_manual(values=c("black","red")) +???? #sets the y scale and log base 10? scale_y_log10() +??? #location of the legend??? theme(legend.position=c("right")) +??? #sets the line color, type and size? geom_line(colour="black", linetype="dotted", size=0.5) +?
? ylab("Chloride (mg/L)")
## does the graph using the Location IDs as the different Locations.p + facet_grid(Location ~ .)

On Mon, Oct 2, 2017 at 5:09 AM, John Kane <jrkrideau at yahoo.ca> wrote:

MyData$newvar <- rep(250, nrow(MyData))
Again, I have not tried graphing this. 
 

    On Sunday, October 1, 2017, 10:38:09 PM EDT, David Doyle <kydaviddoyle at gmail.com> wrote:  
 
 Hi John,
How do I add a second y variable??ThanksDavid
On Sun, Oct 1, 2017 at 5:44 AM, John Kane <jrkrideau at yahoo.ca> wrote:

I just glanced at the problem but I think you would have to create a new variable to replace the hline. What about adding some text annotation in the graph instead?
 

    On Tuesday, September 26, 2017, 3:51:46 PM EDT, David Doyle <kydaviddoyle at gmail.com> wrote:  
 
 Hello everyone,

I have a plot showing chloride concentrations for various point over time.
I also have a dotted line that show the Secondary Maximum Contaminant Level
(my screening limit) on the graphs at 250 mg/L.? But I can not figure out
how to include the dotted line / Secondary Maximum Contaminant Level in
the legend.? Any thoughts?? My code is as following and is linked to my
data on the net.

Thank you in advance
David


#Loads the ggplot2 package.
library(ggplot2)

##This loads your data from your worksheet
MyData <-read.csv("http:// doylesdartden.com/Stats/ TimeSeriesExample.csv",
sep=",")


#Sets which are detections and nondetects

MyData$Detections <- ifelse(MyData$D_Chloride ==1, "Detected", "NonDetect")

#does the plot
p <- ggplot(data = MyData, aes(x=Year, y=Chloride , col=Detections)) +
? geom_point(aes(shape= Detections)) +

? #sets the detect vs. non-detect colors
? scale_colour_manual(values=c(" black","red")) +

? #sets the y scale and log base 10
? scale_y_log10() +

? ##adds line
? geom_hline(aes(yintercept=250) ,linetype="dashed")+

? #location of the legend
? theme(legend.position=c(" right")) +

? #sets the line color, type and size
? geom_line(colour="black", linetype="dotted", size=0.5) +
? ylab("Chloride (mg/L)")

## does the graph using the Location IDs as the different Locations.
p + facet_grid(Location ~ .)

??? [[alternative HTML version deleted]]

______________________________ ________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/ listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/ posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
  

  


	[[alternative HTML version deleted]]


From jeanphilippe.fontaine at gssi.infn.it  Thu Oct  5 16:12:20 2017
From: jeanphilippe.fontaine at gssi.infn.it (jean-philippe)
Date: Thu, 5 Oct 2017 16:12:20 +0200
Subject: [R] dealing with a messy dataset
Message-ID: <59D63DC4.1060408@gssi.infn.it>

dear R-users,


I am facing a quite regular and basic problem when it comes to dealing 
with datasets, but I cannot find any satisfying answer so far.
I have a messy dataset of galaxies like that :

And XVIII          000214.5+450520  0.69 17   9 0.00  -8.7 26.8 6.44  
6.78 < 6.65  -44  0.5 MESSIER031               0.6  1.54
PAndAS-03          000356.4+405319  0.10 17     0.00  -3.6 27.8 
4.38                    2.8 MESSIER031               2.8  1.75
PAndAS-04          000442.9+472142  0.05 22     0.00  -6.6 23.1 
5.59              -108  2.5 MESSIER031               2.5  1.75
PAndAS-05          000524.1+435535  0.06 31     0.00  -4.5 25.6 
4.75               103  2.8 MESSIER031               2.8  1.75
ESO409-015         000531.8-280553  3.00 78  23 0.00 -14.6 24.1 8.10  
8.25   8.10  769 -2.0 NGC0024                 -1.5 -2.05
AGC748778          000634.4+153039  0.61 70   3 0.00 -10.4 24.9 6.39  
5.70   6.64  486 -1.9 NGC0253                 -1.5 -2.72
And XX             000730.7+350756  0.20 33   5 0.00  -5.8 27.1 5.26  
5.70        -182  2.4 MESSIER031               2.4  1.75

What I would like to do is to read this dataset, but I would like that 
the space between And and XVIII is not interpreted as 2 different 
columns but as the name of the galaxy in one column.
How is it possible to do so?

For instance I did this 
data1<-read.table("lvg_table2.txt",skip=70,fill=T) where I used fill=T 
because the rows don't have the same number of features since R splits 
the name of the galaxies into 2 columns because of the space.


Best Regards, thanks in advance


Jean-Philippe Fontaine

-- 
Jean-Philippe Fontaine
PhD Student in Astroparticle Physics,
Gran Sasso Science Institute (GSSI),
Viale Francesco Crispi 7,
67100 L'Aquila, Italy
Mobile: +393487128593, +33615653774


From boris.steipe at utoronto.ca  Thu Oct  5 16:19:55 2017
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Thu, 5 Oct 2017 10:19:55 -0400
Subject: [R] dealing with a messy dataset
In-Reply-To: <59D63DC4.1060408@gssi.infn.it>
References: <59D63DC4.1060408@gssi.infn.it>
Message-ID: <3C4667D6-3682-4799-BFB2-5EE59823B7E4@utoronto.ca>

Is this a fixed width format?
If so, read.fwf() in base, or read_fwf() in the readr package will solve the problem. You may need to trim trailing spaces though.


B.




> On Oct 5, 2017, at 10:12 AM, jean-philippe <jeanphilippe.fontaine at gssi.infn.it> wrote:
> 
> dear R-users,
> 
> 
> I am facing a quite regular and basic problem when it comes to dealing with datasets, but I cannot find any satisfying answer so far.
> I have a messy dataset of galaxies like that :
> 
> And XVIII          000214.5+450520  0.69 17   9 0.00  -8.7 26.8 6.44  6.78 < 6.65  -44  0.5 MESSIER031               0.6  1.54
> PAndAS-03          000356.4+405319  0.10 17     0.00  -3.6 27.8 4.38                    2.8 MESSIER031               2.8  1.75
> PAndAS-04          000442.9+472142  0.05 22     0.00  -6.6 23.1 5.59              -108  2.5 MESSIER031               2.5  1.75
> PAndAS-05          000524.1+435535  0.06 31     0.00  -4.5 25.6 4.75               103  2.8 MESSIER031               2.8  1.75
> ESO409-015         000531.8-280553  3.00 78  23 0.00 -14.6 24.1 8.10  8.25   8.10  769 -2.0 NGC0024                 -1.5 -2.05
> AGC748778          000634.4+153039  0.61 70   3 0.00 -10.4 24.9 6.39  5.70   6.64  486 -1.9 NGC0253                 -1.5 -2.72
> And XX             000730.7+350756  0.20 33   5 0.00  -5.8 27.1 5.26  5.70        -182  2.4 MESSIER031               2.4  1.75
> 
> What I would like to do is to read this dataset, but I would like that the space between And and XVIII is not interpreted as 2 different columns but as the name of the galaxy in one column.
> How is it possible to do so?
> 
> For instance I did this data1<-read.table("lvg_table2.txt",skip=70,fill=T) where I used fill=T because the rows don't have the same number of features since R splits the name of the galaxies into 2 columns because of the space.
> 
> 
> Best Regards, thanks in advance
> 
> 
> Jean-Philippe Fontaine
> 
> -- 
> Jean-Philippe Fontaine
> PhD Student in Astroparticle Physics,
> Gran Sasso Science Institute (GSSI),
> Viale Francesco Crispi 7,
> 67100 L'Aquila, Italy
> Mobile: +393487128593, +33615653774
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jeanphilippe.fontaine at gssi.infn.it  Thu Oct  5 16:33:38 2017
From: jeanphilippe.fontaine at gssi.infn.it (jean-philippe)
Date: Thu, 5 Oct 2017 16:33:38 +0200
Subject: [R] dealing with a messy dataset
In-Reply-To: <3C4667D6-3682-4799-BFB2-5EE59823B7E4@utoronto.ca>
References: <59D63DC4.1060408@gssi.infn.it>
 <3C4667D6-3682-4799-BFB2-5EE59823B7E4@utoronto.ca>
Message-ID: <59D642C2.5020306@gssi.infn.it>

dear Boris,

Thanks for your answer!

Yes it seems to  be a fixed-width format. I didn't remember this type of 
datasets since I am not used to analyze and process them.

Thanks anyway, it seems to fix the problem (I just need to reflect a bit 
more on the width of each feature)!


Cheers

Jean-Philippe

On 05/10/2017 16:19, Boris Steipe wrote:
> Is this a fixed width format?
> If so, read.fwf() in base, or read_fwf() in the readr package will solve the problem. You may need to trim trailing spaces though.
>
>
> B.
>
>
>
>
>> On Oct 5, 2017, at 10:12 AM, jean-philippe <jeanphilippe.fontaine at gssi.infn.it> wrote:
>>
>> dear R-users,
>>
>>
>> I am facing a quite regular and basic problem when it comes to dealing with datasets, but I cannot find any satisfying answer so far.
>> I have a messy dataset of galaxies like that :
>>
>> And XVIII          000214.5+450520  0.69 17   9 0.00  -8.7 26.8 6.44  6.78 < 6.65  -44  0.5 MESSIER031               0.6  1.54
>> PAndAS-03          000356.4+405319  0.10 17     0.00  -3.6 27.8 4.38                    2.8 MESSIER031               2.8  1.75
>> PAndAS-04          000442.9+472142  0.05 22     0.00  -6.6 23.1 5.59              -108  2.5 MESSIER031               2.5  1.75
>> PAndAS-05          000524.1+435535  0.06 31     0.00  -4.5 25.6 4.75               103  2.8 MESSIER031               2.8  1.75
>> ESO409-015         000531.8-280553  3.00 78  23 0.00 -14.6 24.1 8.10  8.25   8.10  769 -2.0 NGC0024                 -1.5 -2.05
>> AGC748778          000634.4+153039  0.61 70   3 0.00 -10.4 24.9 6.39  5.70   6.64  486 -1.9 NGC0253                 -1.5 -2.72
>> And XX             000730.7+350756  0.20 33   5 0.00  -5.8 27.1 5.26  5.70        -182  2.4 MESSIER031               2.4  1.75
>>
>> What I would like to do is to read this dataset, but I would like that the space between And and XVIII is not interpreted as 2 different columns but as the name of the galaxy in one column.
>> How is it possible to do so?
>>
>> For instance I did this data1<-read.table("lvg_table2.txt",skip=70,fill=T) where I used fill=T because the rows don't have the same number of features since R splits the name of the galaxies into 2 columns because of the space.
>>
>>
>> Best Regards, thanks in advance
>>
>>
>> Jean-Philippe Fontaine
>>
>> -- 
>> Jean-Philippe Fontaine
>> PhD Student in Astroparticle Physics,
>> Gran Sasso Science Institute (GSSI),
>> Viale Francesco Crispi 7,
>> 67100 L'Aquila, Italy
>> Mobile: +393487128593, +33615653774
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

-- 
Jean-Philippe Fontaine
PhD Student in Astroparticle Physics,
Gran Sasso Science Institute (GSSI),
Viale Francesco Crispi 7,
67100 L'Aquila, Italy
Mobile: +393487128593, +33615653774


From jholtman at gmail.com  Thu Oct  5 16:49:35 2017
From: jholtman at gmail.com (jim holtman)
Date: Thu, 5 Oct 2017 10:49:35 -0400
Subject: [R] dealing with a messy dataset
In-Reply-To: <59D63DC4.1060408@gssi.infn.it>
References: <59D63DC4.1060408@gssi.infn.it>
Message-ID: <CAAxdm-5u7o_=RnSDaWn963=vt8MYgTWduLk2+Zx=90X4CvU5dw@mail.gmail.com>

It looks like fixed width.  I just used the last position of each
field to get the size and used the 'readr' package;

    > input <- "And XVIII          000214.5+450520  0.69 17   9 0.00
-8.7 26.8 6.44  6.78 < 6.65  -44  0.5 MESSIER031               0.6
1.54
    + PAndAS-03          000356.4+405319  0.10 17     0.00  -3.6 27.8
4.38                    2.8 MESSIER031               2.8  1.75
    + PAndAS-04          000442.9+472142  0.05 22     0.00  -6.6 23.1
5.59              -108  2.5 MESSIER031               2.5  1.75
    + PAndAS-05          000524.1+435535  0.06 31     0.00  -4.5 25.6
4.75               103  2.8 MESSIER031               2.8  1.75
    + ESO409-015         000531.8-280553  3.00 78  23 0.00 -14.6 24.1
8.10  8.25   8.10  769 -2.0 NGC0024                 -1.5 -2.05
    + AGC748778          000634.4+153039  0.61 70   3 0.00 -10.4 24.9
6.39  5.70   6.64  486 -1.9 NGC0253                 -1.5 -2.72
    + And XX             000730.7+350756  0.20 33   5 0.00  -5.8 27.1
5.26  5.70        -182  2.4 MESSIER031               2.4  1.75"
    >
    > start <- c(1, 20, 35, 41, 44, 48, 53, 59, 64, 69, 75, 77, 82, 87,
    +            92, 114, 121, 127)
    > read_fwf(input, fwf_widths(diff(start)))
    # A tibble: 7 x 17
              X1              X2    X3    X4    X5    X6    X7    X8
 X9   X10   X11   X12   X13   X14
           <chr>           <chr> <dbl> <int> <int> <dbl> <dbl> <dbl>
<dbl> <dbl> <chr> <dbl> <int> <dbl>
    1  And XVIII 000214.5+450520  0.69    17     9     0  -8.7  26.8
6.44  6.78     <  6.65   -44   0.5
    2  PAndAS-03 000356.4+405319  0.10    17    NA     0  -3.6  27.8
4.38    NA  <NA>    NA    NA   2.8
    3  PAndAS-04 000442.9+472142  0.05    22    NA     0  -6.6  23.1
5.59    NA  <NA>    NA  -108   2.5
    4  PAndAS-05 000524.1+435535  0.06    31    NA     0  -4.5  25.6
4.75    NA  <NA>    NA   103   2.8
    5 ESO409-015 000531.8-280553  3.00    78    23     0 -14.6  24.1
8.10  8.25  <NA>  8.10   769  -2.0
    6  AGC748778 000634.4+153039  0.61    70     3     0 -10.4  24.9
6.39  5.70  <NA>  6.64   486  -1.9
    7     And XX 000730.7+350756  0.20    33     5     0  -5.8  27.1
5.26  5.70  <NA>    NA  -182   2.4
    # ... with 3 more variables: X15 <chr>, X16 <dbl>, X17 <dbl>
    >


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.


On Thu, Oct 5, 2017 at 10:12 AM, jean-philippe
<jeanphilippe.fontaine at gssi.infn.it> wrote:
> dear R-users,
>
>
> I am facing a quite regular and basic problem when it comes to dealing with
> datasets, but I cannot find any satisfying answer so far.
> I have a messy dataset of galaxies like that :
>
> And XVIII          000214.5+450520  0.69 17   9 0.00  -8.7 26.8 6.44  6.78 <
> 6.65  -44  0.5 MESSIER031               0.6  1.54
> PAndAS-03          000356.4+405319  0.10 17     0.00  -3.6 27.8 4.38
> 2.8 MESSIER031               2.8  1.75
> PAndAS-04          000442.9+472142  0.05 22     0.00  -6.6 23.1 5.59
> -108  2.5 MESSIER031               2.5  1.75
> PAndAS-05          000524.1+435535  0.06 31     0.00  -4.5 25.6 4.75
> 103  2.8 MESSIER031               2.8  1.75
> ESO409-015         000531.8-280553  3.00 78  23 0.00 -14.6 24.1 8.10  8.25
> 8.10  769 -2.0 NGC0024                 -1.5 -2.05
> AGC748778          000634.4+153039  0.61 70   3 0.00 -10.4 24.9 6.39  5.70
> 6.64  486 -1.9 NGC0253                 -1.5 -2.72
> And XX             000730.7+350756  0.20 33   5 0.00  -5.8 27.1 5.26  5.70
> -182  2.4 MESSIER031               2.4  1.75
>
> What I would like to do is to read this dataset, but I would like that the
> space between And and XVIII is not interpreted as 2 different columns but as
> the name of the galaxy in one column.
> How is it possible to do so?
>
> For instance I did this data1<-read.table("lvg_table2.txt",skip=70,fill=T)
> where I used fill=T because the rows don't have the same number of features
> since R splits the name of the galaxies into 2 columns because of the space.
>
>
> Best Regards, thanks in advance
>
>
> Jean-Philippe Fontaine
>
> --
> Jean-Philippe Fontaine
> PhD Student in Astroparticle Physics,
> Gran Sasso Science Institute (GSSI),
> Viale Francesco Crispi 7,
> 67100 L'Aquila, Italy
> Mobile: +393487128593, +33615653774
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jeanphilippe.fontaine at gssi.infn.it  Thu Oct  5 17:02:07 2017
From: jeanphilippe.fontaine at gssi.infn.it (jean-philippe)
Date: Thu, 5 Oct 2017 17:02:07 +0200
Subject: [R] dealing with a messy dataset
In-Reply-To: <CAAxdm-5u7o_=RnSDaWn963=vt8MYgTWduLk2+Zx=90X4CvU5dw@mail.gmail.com>
References: <59D63DC4.1060408@gssi.infn.it>
 <CAAxdm-5u7o_=RnSDaWn963=vt8MYgTWduLk2+Zx=90X4CvU5dw@mail.gmail.com>
Message-ID: <59D6496F.1080405@gssi.infn.it>

dear Jim,

Thanks for your reply and your proposition.

I forgot to provide the header of the dataframe, here it is:
================================================================================
Byte-by-byte Description of file: lvg_table2.dat
--------------------------------------------------------------------------------
    Bytes Format Units       Label   Explanations
--------------------------------------------------------------------------------
    1- 18 A18    ---         Name    Galaxy name in well-known catalogs
   20- 21 I2     h           RAh     Hour of Right Ascension (J2000)
   22- 23 I2     min         RAm     Minute of Right Ascension (J2000)
   24- 27 F4.1   s           RAs     Second of Right Ascension (J2000)
       28 A1     ---         DE-     Sign of the Declination (J2000)
   29- 30 I2     deg         DEd     Degree of Declination (J2000)
   31- 32 I2     arcmin      DEm     Arcminute of Declination (J2000)
   33- 34 I2     arcsec      DEs     Arcsecond of Declination (J2000)
   36- 40 F5.2   kpc         a26     ? Major linear diameter (1)
   42- 43 I2     deg         inc     ? Inclination
   45- 47 I3     km/s        Vm      ? Amplitude of rotational velocity (2)
   49- 52 F4.2   mag         AB      ? Internal B band extinction (3)
   54- 58 F5.1   mag         BMag    ? Absolute B band magnitude (4)
   60- 63 F4.1   mag/arcsec2 SBB     ? Average B band surface brightness (5)
   65- 69 F5.2   [solLum]    logKLum ? Log K_S_ band luminosity (6)
   71- 75 F5.2   [solMass]   logM26  ? Log mass within Holmberg radius (7)
       77 A1     ---       l_logMHI  Limit flag on logMHI
   78- 82 F5.2   [solMass]   logMHI  ? Log hydrogen mass (8)
   84- 87 I4     km/s        VLG     ? Radial velocity (9)
   89- 92 F4.1   ---         Theta1  ? Tidal index (10)
   94-116 A23    ---         MD      Main disturber name (11)
  118-121 F4.1   ---         Theta5  ? Another tidal index (12)
  123-127 F5.2   [-]         Thetaj  ? Log K band luminosity density (13)
--------------------------------------------------------------------------------

The idea for me is to select only the galaxy name and the logMHI values 
for these galaxies, so quite a simple job when the dataset is tidy 
enough. I was thinking as usual to use select from dplyr.
That is why I was just asking how to read this kind of files which, for 
me so far, are uncommon.

Doing what you propose, it formats most of the columns correctly except 
few ones, I will see how I can change some width to get it correctly:

           X1              X2    X3    X4    X5    X6    X7    X8 X9    
X10   X11   X12   X13   X14          X15   X16     X17
        (chr)           (chr) (dbl) (int) (dbl) (dbl) (chr) (dbl) (chr)  
(chr) (int) (chr) (chr) (chr)        (chr) (dbl)   (chr)
1   UGC12894 000022.5+392944  2.78    33    21     0 -13.3  25.2 7.5 8  
8.1     7   7.9 2  61 9 -1.    3 NGC7640    -1 0  0.12
2        WLM 000158.1-152740  3.25    90    22     0 -14.1  24.8 7.7 0  
8.2     7   7.8 4  -1 6  0. 0 MESSIER031     0 2  1.75
3  And XVIII 000214.5+450520  0.69    17     9     0  -8.7  26.8 6.4 4  
6.7     8 < 6.6 5  -4 4  0. 5 MESSIER031     0 6  1.54
4  PAndAS-03 000356.4+405319  0.10    17    NA     0  -3.6  27.8 
4.3      8    NA    NA    NA    2. 8 MESSIER031     2 8  1.75
5  PAndAS-04 000442.9+472142  0.05    22    NA     0  -6.6  23.1 
5.5      9    NA    NA   -10 8  2. 5 MESSIER031     2 5  1.75
6  PAndAS-05 000524.1+435535  0.06    31    NA     0  -4.5  25.6 
4.7      5    NA    NA    10 3  2. 8 MESSIER031     2 8  1.75
7 ESO409-015 000531.8-280553  3.00    78    23     0 -14.6  24.1 8.1 0  
8.2     5   8.1 0  76 9 -2.    0 NGC0024    -1 5 -2.05
8  AGC748778 000634.4+153039  0.61    70     3     0 -10.4  24.9 6.3 9  
5.7     0   6.6 4  48 6 -1.    9 NGC0253    -1 5 -2.72
9     And XX 000730.7+350756  0.20    33     5     0  -5.8  27.1 5.2 6  
5.7     0    NA   -18 2  2. 4 MESSIER031     2 4  1.75


Cheers, thanks again


Jean-Philippe
On 05/10/2017 16:49, jim holtman wrote:
> start <- c(1, 20, 35, 41, 44, 48, 53, 59, 64, 69, 75, 77, 82, 87,
>      +            92, 114, 121, 127)
>      > read_fwf(input, fwf_widths(diff(start)))

-- 
Jean-Philippe Fontaine
PhD Student in Astroparticle Physics,
Gran Sasso Science Institute (GSSI),
Viale Francesco Crispi 7,
67100 L'Aquila, Italy
Mobile: +393487128593, +33615653774


From boris.steipe at utoronto.ca  Thu Oct  5 17:10:38 2017
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Thu, 5 Oct 2017 11:10:38 -0400
Subject: [R] dealing with a messy dataset
In-Reply-To: <59D6496F.1080405@gssi.infn.it>
References: <59D63DC4.1060408@gssi.infn.it>
 <CAAxdm-5u7o_=RnSDaWn963=vt8MYgTWduLk2+Zx=90X4CvU5dw@mail.gmail.com>
 <59D6496F.1080405@gssi.infn.it>
Message-ID: <28DE0894-80D3-4725-9FD1-F566FF5613ED@utoronto.ca>

Since you have an authoritative description of the format, by all means use that - not a guess based on a visual inspection of where data appears in a sample row.

B. 




> On Oct 5, 2017, at 11:02 AM, jean-philippe <jeanphilippe.fontaine at gssi.infn.it> wrote:
> 
> dear Jim,
> 
> Thanks for your reply and your proposition.
> 
> I forgot to provide the header of the dataframe, here it is:
> ================================================================================
> Byte-by-byte Description of file: lvg_table2.dat
> --------------------------------------------------------------------------------
>   Bytes Format Units       Label   Explanations
> --------------------------------------------------------------------------------
>   1- 18 A18    ---         Name    Galaxy name in well-known catalogs
>  20- 21 I2     h           RAh     Hour of Right Ascension (J2000)
>  22- 23 I2     min         RAm     Minute of Right Ascension (J2000)
>  24- 27 F4.1   s           RAs     Second of Right Ascension (J2000)
>      28 A1     ---         DE-     Sign of the Declination (J2000)
>  29- 30 I2     deg         DEd     Degree of Declination (J2000)
>  31- 32 I2     arcmin      DEm     Arcminute of Declination (J2000)
>  33- 34 I2     arcsec      DEs     Arcsecond of Declination (J2000)
>  36- 40 F5.2   kpc         a26     ? Major linear diameter (1)
>  42- 43 I2     deg         inc     ? Inclination
>  45- 47 I3     km/s        Vm      ? Amplitude of rotational velocity (2)
>  49- 52 F4.2   mag         AB      ? Internal B band extinction (3)
>  54- 58 F5.1   mag         BMag    ? Absolute B band magnitude (4)
>  60- 63 F4.1   mag/arcsec2 SBB     ? Average B band surface brightness (5)
>  65- 69 F5.2   [solLum]    logKLum ? Log K_S_ band luminosity (6)
>  71- 75 F5.2   [solMass]   logM26  ? Log mass within Holmberg radius (7)
>      77 A1     ---       l_logMHI  Limit flag on logMHI
>  78- 82 F5.2   [solMass]   logMHI  ? Log hydrogen mass (8)
>  84- 87 I4     km/s        VLG     ? Radial velocity (9)
>  89- 92 F4.1   ---         Theta1  ? Tidal index (10)
>  94-116 A23    ---         MD      Main disturber name (11)
> 118-121 F4.1   ---         Theta5  ? Another tidal index (12)
> 123-127 F5.2   [-]         Thetaj  ? Log K band luminosity density (13)
> --------------------------------------------------------------------------------
> 
> The idea for me is to select only the galaxy name and the logMHI values for these galaxies, so quite a simple job when the dataset is tidy enough. I was thinking as usual to use select from dplyr.
> That is why I was just asking how to read this kind of files which, for me so far, are uncommon.
> 
> Doing what you propose, it formats most of the columns correctly except few ones, I will see how I can change some width to get it correctly:
> 
>          X1              X2    X3    X4    X5    X6    X7    X8 X9    X10   X11   X12   X13   X14          X15   X16     X17
>       (chr)           (chr) (dbl) (int) (dbl) (dbl) (chr) (dbl) (chr)  (chr) (int) (chr) (chr) (chr)        (chr) (dbl)   (chr)
> 1   UGC12894 000022.5+392944  2.78    33    21     0 -13.3  25.2 7.5 8  8.1     7   7.9 2  61 9 -1.    3 NGC7640    -1 0  0.12
> 2        WLM 000158.1-152740  3.25    90    22     0 -14.1  24.8 7.7 0  8.2     7   7.8 4  -1 6  0. 0 MESSIER031     0 2  1.75
> 3  And XVIII 000214.5+450520  0.69    17     9     0  -8.7  26.8 6.4 4  6.7     8 < 6.6 5  -4 4  0. 5 MESSIER031     0 6  1.54
> 4  PAndAS-03 000356.4+405319  0.10    17    NA     0  -3.6  27.8 4.3      8    NA    NA    NA    2. 8 MESSIER031     2 8  1.75
> 5  PAndAS-04 000442.9+472142  0.05    22    NA     0  -6.6  23.1 5.5      9    NA    NA   -10 8  2. 5 MESSIER031     2 5  1.75
> 6  PAndAS-05 000524.1+435535  0.06    31    NA     0  -4.5  25.6 4.7      5    NA    NA    10 3  2. 8 MESSIER031     2 8  1.75
> 7 ESO409-015 000531.8-280553  3.00    78    23     0 -14.6  24.1 8.1 0  8.2     5   8.1 0  76 9 -2.    0 NGC0024    -1 5 -2.05
> 8  AGC748778 000634.4+153039  0.61    70     3     0 -10.4  24.9 6.3 9  5.7     0   6.6 4  48 6 -1.    9 NGC0253    -1 5 -2.72
> 9     And XX 000730.7+350756  0.20    33     5     0  -5.8  27.1 5.2 6  5.7     0    NA   -18 2  2. 4 MESSIER031     2 4  1.75
> 
> 
> Cheers, thanks again
> 
> 
> Jean-Philippe
> On 05/10/2017 16:49, jim holtman wrote:
>> start <- c(1, 20, 35, 41, 44, 48, 53, 59, 64, 69, 75, 77, 82, 87,
>>     +            92, 114, 121, 127)
>>     > read_fwf(input, fwf_widths(diff(start)))
> 
> -- 
> Jean-Philippe Fontaine
> PhD Student in Astroparticle Physics,
> Gran Sasso Science Institute (GSSI),
> Viale Francesco Crispi 7,
> 67100 L'Aquila, Italy
> Mobile: +393487128593, +33615653774
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Thu Oct  5 18:39:05 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 5 Oct 2017 09:39:05 -0700
Subject: [R] Issue calling MICE package
In-Reply-To: <CANxTjoyvUP=wDVr_7bnQQudqb8dMH0XzAmpyLG64zcLJMaR6nw@mail.gmail.com>
References: <CANxTjoxr8Sb4hB4hgwXBwtzAVnAh7exGjmfBZvRuBrLhCmJd_g@mail.gmail.com>
 <dafe0645-f6bf-3a5a-12d9-3ce326c6ba58@dewey.myzen.co.uk>
 <54DB4B8E-A7F0-45FE-938D-AE02D9F18863@gmail.com>
 <CANxTjoyvUP=wDVr_7bnQQudqb8dMH0XzAmpyLG64zcLJMaR6nw@mail.gmail.com>
Message-ID: <8EB4D539-E52B-4D3F-A64F-B08FF1589FD7@comcast.net>


> On Oct 5, 2017, at 12:28 AM, Ole H?st <olehost at gmail.com> wrote:
> 
> Sorry, I was not clear enough. The reason I want to use mice::mice() rather
> than library(mice); mice() is that I want to call it from my own package.
> But the reprex works from the command line as well, straight after
> launching R:
> 
>  mice::mice(airquality)
>  #> Error in check.method(setup, data): The following functions were not
> found: mice.impute.pmm, mice.impute.pmm
> 

You were (twice) advised to review what was in your NAMESPACE file. If you want further informed advice you may need to shear that document.

-- 
David.


> The mice.impute functions are exported from the mice package but not
> found.I cannot figure out why but I was hoping someone else had come across
> the issue.
> 
> Interestingly, I can circumvent the problem by doing
>  foreach(i = ...) %dopar% {mice::mice()}
> 
> Thanks for your help,
> Ole
> 
> 
> 
> On Wed, Oct 4, 2017 at 7:16 PM, Peter Dalgaard <pdalgd at gmail.com> wrote:
> 
>> IIUC, this would be an isssue with MICE (or rather "mice"), which isn't
>> Ole's. It could be a namespace issue, but it could also be that some
>> start-up code is not executed if library() is bypasses (see .onAttach et
>> al.).
>> 
>> -pd
>> 
>>> On 4 Oct 2017, at 17:00 , Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>>> 
>>> Dear Ole
>>> 
>>> One of the experts may be able to diagnose this without extra
>> information but I suspect you have not got the right magic in your
>> NAMESPACE file in your package. You may need to re-read section 1.5.1 of
>> the Writing R extensions manual.
>>> 
>>> Michael
>>> 
>>> On 04/10/2017 13:47, Ole H?st wrote:
>>>> I want to call the mice function from the MICE package from my own
>> package.
>>>> However I run into this issue, which can be reproduced on the command
>> line:
>>>> mice::mice(airquality)#> Error in check.method(setup, data): The
>>>> following functions were not found: mice.impute.pmm, mice.impute.pmm
>>>> I have no problems when doing
>>>> library(mice)
>>>> mice(airquality)
>>>> Is this a bug or am I missing something?
>>>> Thanks,
>>>> Ole H?st
>>>>     [[alternative HTML version deleted]]
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> ---
>>>> This email has been checked for viruses by AVG.
>>>> http://www.avg.com
>>> 
>>> --
>>> Michael
>>> http://www.dewey.myzen.co.uk/home.html
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From jholtman at gmail.com  Thu Oct  5 18:42:53 2017
From: jholtman at gmail.com (jim holtman)
Date: Thu, 5 Oct 2017 12:42:53 -0400
Subject: [R] dealing with a messy dataset
In-Reply-To: <59D6496F.1080405@gssi.infn.it>
References: <59D63DC4.1060408@gssi.infn.it>
 <CAAxdm-5u7o_=RnSDaWn963=vt8MYgTWduLk2+Zx=90X4CvU5dw@mail.gmail.com>
 <59D6496F.1080405@gssi.infn.it>
Message-ID: <CAAxdm-6hJG9EOoDTCBmsOFDiJcC14Hf6r4zQ_hFrkA+M6nUdbQ@mail.gmail.com>

You should be able to use that header information to create the
correct parameters to the read_fwf function to read in the data.

Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.


On Thu, Oct 5, 2017 at 11:02 AM, jean-philippe
<jeanphilippe.fontaine at gssi.infn.it> wrote:
> dear Jim,
>
> Thanks for your reply and your proposition.
>
> I forgot to provide the header of the dataframe, here it is:
> ================================================================================
> Byte-by-byte Description of file: lvg_table2.dat
> --------------------------------------------------------------------------------
>    Bytes Format Units       Label   Explanations
> --------------------------------------------------------------------------------
>    1- 18 A18    ---         Name    Galaxy name in well-known catalogs
>   20- 21 I2     h           RAh     Hour of Right Ascension (J2000)
>   22- 23 I2     min         RAm     Minute of Right Ascension (J2000)
>   24- 27 F4.1   s           RAs     Second of Right Ascension (J2000)
>       28 A1     ---         DE-     Sign of the Declination (J2000)
>   29- 30 I2     deg         DEd     Degree of Declination (J2000)
>   31- 32 I2     arcmin      DEm     Arcminute of Declination (J2000)
>   33- 34 I2     arcsec      DEs     Arcsecond of Declination (J2000)
>   36- 40 F5.2   kpc         a26     ? Major linear diameter (1)
>   42- 43 I2     deg         inc     ? Inclination
>   45- 47 I3     km/s        Vm      ? Amplitude of rotational velocity (2)
>   49- 52 F4.2   mag         AB      ? Internal B band extinction (3)
>   54- 58 F5.1   mag         BMag    ? Absolute B band magnitude (4)
>   60- 63 F4.1   mag/arcsec2 SBB     ? Average B band surface brightness (5)
>   65- 69 F5.2   [solLum]    logKLum ? Log K_S_ band luminosity (6)
>   71- 75 F5.2   [solMass]   logM26  ? Log mass within Holmberg radius (7)
>       77 A1     ---       l_logMHI  Limit flag on logMHI
>   78- 82 F5.2   [solMass]   logMHI  ? Log hydrogen mass (8)
>   84- 87 I4     km/s        VLG     ? Radial velocity (9)
>   89- 92 F4.1   ---         Theta1  ? Tidal index (10)
>   94-116 A23    ---         MD      Main disturber name (11)
>  118-121 F4.1   ---         Theta5  ? Another tidal index (12)
>  123-127 F5.2   [-]         Thetaj  ? Log K band luminosity density (13)
> --------------------------------------------------------------------------------
>
> The idea for me is to select only the galaxy name and the logMHI values for
> these galaxies, so quite a simple job when the dataset is tidy enough. I was
> thinking as usual to use select from dplyr.
> That is why I was just asking how to read this kind of files which, for me
> so far, are uncommon.
>
> Doing what you propose, it formats most of the columns correctly except few
> ones, I will see how I can change some width to get it correctly:
>
>           X1              X2    X3    X4    X5    X6    X7    X8 X9    X10
> X11   X12   X13   X14          X15   X16     X17
>        (chr)           (chr) (dbl) (int) (dbl) (dbl) (chr) (dbl) (chr)
> (chr) (int) (chr) (chr) (chr)        (chr) (dbl)   (chr)
> 1   UGC12894 000022.5+392944  2.78    33    21     0 -13.3  25.2 7.5 8  8.1
> 7   7.9 2  61 9 -1.    3 NGC7640    -1 0  0.12
> 2        WLM 000158.1-152740  3.25    90    22     0 -14.1 24.8 7.7 0 8.2
> 7   7.8 4  -1 6  0. 0 MESSIER031     0 2  1.75
> 3  And XVIII 000214.5+450520  0.69    17     9     0  -8.7  26.8 6.4 4  6.7
> 8 < 6.6 5  -4 4  0. 5 MESSIER031     0 6  1.54
> 4  PAndAS-03 000356.4+405319  0.10    17    NA     0  -3.6  27.8 4.3      8
> NA    NA    NA    2. 8 MESSIER031     2 8  1.75
> 5  PAndAS-04 000442.9+472142  0.05    22    NA     0  -6.6  23.1 5.5      9
> NA    NA   -10 8  2. 5 MESSIER031     2 5  1.75
> 6  PAndAS-05 000524.1+435535  0.06    31    NA     0  -4.5  25.6 4.7      5
> NA    NA    10 3  2. 8 MESSIER031     2 8  1.75
> 7 ESO409-015 000531.8-280553  3.00    78    23     0 -14.6  24.1 8.1 0  8.2
> 5   8.1 0  76 9 -2.    0 NGC0024    -1 5 -2.05
> 8  AGC748778 000634.4+153039 0.61 70     3     0 -10.4  24.9 6.3 9  5.7
> 0   6.6 4  48 6 -1.    9 NGC0253    -1 5 -2.72
> 9     And XX 000730.7+350756  0.20    33     5     0  -5.8  27.1 5.2 6  5.7
> 0    NA   -18 2  2. 4 MESSIER031     2 4  1.75
>
>
> Cheers, thanks again
>
>
> Jean-Philippe
> On 05/10/2017 16:49, jim holtman wrote:
>>
>> start <- c(1, 20, 35, 41, 44, 48, 53, 59, 64, 69, 75, 77, 82, 87,
>>      +            92, 114, 121, 127)
>>      > read_fwf(input, fwf_widths(diff(start)))
>
>
> --
> Jean-Philippe Fontaine
> PhD Student in Astroparticle Physics,
> Gran Sasso Science Institute (GSSI),
> Viale Francesco Crispi 7,
> 67100 L'Aquila, Italy
> Mobile: +393487128593, +33615653774
>


From jeanphilippe.fontaine at gssi.infn.it  Thu Oct  5 18:46:18 2017
From: jeanphilippe.fontaine at gssi.infn.it (jean-philippe)
Date: Thu, 5 Oct 2017 18:46:18 +0200
Subject: [R] dealing with a messy dataset
In-Reply-To: <CAAxdm-6hJG9EOoDTCBmsOFDiJcC14Hf6r4zQ_hFrkA+M6nUdbQ@mail.gmail.com>
References: <59D63DC4.1060408@gssi.infn.it>
 <CAAxdm-5u7o_=RnSDaWn963=vt8MYgTWduLk2+Zx=90X4CvU5dw@mail.gmail.com>
 <59D6496F.1080405@gssi.infn.it>
 <CAAxdm-6hJG9EOoDTCBmsOFDiJcC14Hf6r4zQ_hFrkA+M6nUdbQ@mail.gmail.com>
Message-ID: <59D661DA.6080901@gssi.infn.it>

dear Jim,


Yes I fixed the problem. Thanks again all of you for your contribution!
This worked :

start <- c(1, 20, 35, 41, 44, 48, 53, 59, 64, 70, 76, 78, 83, 88,
            +            93, 114, 122, 127)
data1<-read_fwf("lvg_table2.txt",skip=70, fwf_widths(diff(start)))

Well now I know how to deal with fixed-width files :)


Cheers


Jean-Philippe

On 05/10/2017 18:42, jim holtman wrote:
> You should be able to use that header information to create the
> correct parameters to the read_fwf function to read in the data.
>
> Jim Holtman
> Data Munger Guru
>
> What is the problem that you are trying to solve?
> Tell me what you want to do, not how you want to do it.
>
>
> On Thu, Oct 5, 2017 at 11:02 AM, jean-philippe
> <jeanphilippe.fontaine at gssi.infn.it> wrote:
>> dear Jim,
>>
>> Thanks for your reply and your proposition.
>>
>> I forgot to provide the header of the dataframe, here it is:
>> ================================================================================
>> Byte-by-byte Description of file: lvg_table2.dat
>> --------------------------------------------------------------------------------
>>     Bytes Format Units       Label   Explanations
>> --------------------------------------------------------------------------------
>>     1- 18 A18    ---         Name    Galaxy name in well-known catalogs
>>    20- 21 I2     h           RAh     Hour of Right Ascension (J2000)
>>    22- 23 I2     min         RAm     Minute of Right Ascension (J2000)
>>    24- 27 F4.1   s           RAs     Second of Right Ascension (J2000)
>>        28 A1     ---         DE-     Sign of the Declination (J2000)
>>    29- 30 I2     deg         DEd     Degree of Declination (J2000)
>>    31- 32 I2     arcmin      DEm     Arcminute of Declination (J2000)
>>    33- 34 I2     arcsec      DEs     Arcsecond of Declination (J2000)
>>    36- 40 F5.2   kpc         a26     ? Major linear diameter (1)
>>    42- 43 I2     deg         inc     ? Inclination
>>    45- 47 I3     km/s        Vm      ? Amplitude of rotational velocity (2)
>>    49- 52 F4.2   mag         AB      ? Internal B band extinction (3)
>>    54- 58 F5.1   mag         BMag    ? Absolute B band magnitude (4)
>>    60- 63 F4.1   mag/arcsec2 SBB     ? Average B band surface brightness (5)
>>    65- 69 F5.2   [solLum]    logKLum ? Log K_S_ band luminosity (6)
>>    71- 75 F5.2   [solMass]   logM26  ? Log mass within Holmberg radius (7)
>>        77 A1     ---       l_logMHI  Limit flag on logMHI
>>    78- 82 F5.2   [solMass]   logMHI  ? Log hydrogen mass (8)
>>    84- 87 I4     km/s        VLG     ? Radial velocity (9)
>>    89- 92 F4.1   ---         Theta1  ? Tidal index (10)
>>    94-116 A23    ---         MD      Main disturber name (11)
>>   118-121 F4.1   ---         Theta5  ? Another tidal index (12)
>>   123-127 F5.2   [-]         Thetaj  ? Log K band luminosity density (13)
>> --------------------------------------------------------------------------------
>>
>> The idea for me is to select only the galaxy name and the logMHI values for
>> these galaxies, so quite a simple job when the dataset is tidy enough. I was
>> thinking as usual to use select from dplyr.
>> That is why I was just asking how to read this kind of files which, for me
>> so far, are uncommon.
>>
>> Doing what you propose, it formats most of the columns correctly except few
>> ones, I will see how I can change some width to get it correctly:
>>
>>            X1              X2    X3    X4    X5    X6    X7    X8 X9    X10
>> X11   X12   X13   X14          X15   X16     X17
>>         (chr)           (chr) (dbl) (int) (dbl) (dbl) (chr) (dbl) (chr)
>> (chr) (int) (chr) (chr) (chr)        (chr) (dbl)   (chr)
>> 1   UGC12894 000022.5+392944  2.78    33    21     0 -13.3  25.2 7.5 8  8.1
>> 7   7.9 2  61 9 -1.    3 NGC7640    -1 0  0.12
>> 2        WLM 000158.1-152740  3.25    90    22     0 -14.1 24.8 7.7 0 8.2
>> 7   7.8 4  -1 6  0. 0 MESSIER031     0 2  1.75
>> 3  And XVIII 000214.5+450520  0.69    17     9     0  -8.7  26.8 6.4 4  6.7
>> 8 < 6.6 5  -4 4  0. 5 MESSIER031     0 6  1.54
>> 4  PAndAS-03 000356.4+405319  0.10    17    NA     0  -3.6  27.8 4.3      8
>> NA    NA    NA    2. 8 MESSIER031     2 8  1.75
>> 5  PAndAS-04 000442.9+472142  0.05    22    NA     0  -6.6  23.1 5.5      9
>> NA    NA   -10 8  2. 5 MESSIER031     2 5  1.75
>> 6  PAndAS-05 000524.1+435535  0.06    31    NA     0  -4.5  25.6 4.7      5
>> NA    NA    10 3  2. 8 MESSIER031     2 8  1.75
>> 7 ESO409-015 000531.8-280553  3.00    78    23     0 -14.6  24.1 8.1 0  8.2
>> 5   8.1 0  76 9 -2.    0 NGC0024    -1 5 -2.05
>> 8  AGC748778 000634.4+153039 0.61 70     3     0 -10.4  24.9 6.3 9  5.7
>> 0   6.6 4  48 6 -1.    9 NGC0253    -1 5 -2.72
>> 9     And XX 000730.7+350756  0.20    33     5     0  -5.8  27.1 5.2 6  5.7
>> 0    NA   -18 2  2. 4 MESSIER031     2 4  1.75
>>
>>
>> Cheers, thanks again
>>
>>
>> Jean-Philippe
>> On 05/10/2017 16:49, jim holtman wrote:
>>> start <- c(1, 20, 35, 41, 44, 48, 53, 59, 64, 69, 75, 77, 82, 87,
>>>       +            92, 114, 121, 127)
>>>       > read_fwf(input, fwf_widths(diff(start)))
>>
>> --
>> Jean-Philippe Fontaine
>> PhD Student in Astroparticle Physics,
>> Gran Sasso Science Institute (GSSI),
>> Viale Francesco Crispi 7,
>> 67100 L'Aquila, Italy
>> Mobile: +393487128593, +33615653774
>>

-- 
Jean-Philippe Fontaine
PhD Student in Astroparticle Physics,
Gran Sasso Science Institute (GSSI),
Viale Francesco Crispi 7,
67100 L'Aquila, Italy
Mobile: +393487128593, +33615653774


From boris.steipe at utoronto.ca  Thu Oct  5 19:54:56 2017
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Thu, 5 Oct 2017 13:54:56 -0400
Subject: [R] dealing with a messy dataset
In-Reply-To: <59D661DA.6080901@gssi.infn.it>
References: <59D63DC4.1060408@gssi.infn.it>
 <CAAxdm-5u7o_=RnSDaWn963=vt8MYgTWduLk2+Zx=90X4CvU5dw@mail.gmail.com>
 <59D6496F.1080405@gssi.infn.it>
 <CAAxdm-6hJG9EOoDTCBmsOFDiJcC14Hf6r4zQ_hFrkA+M6nUdbQ@mail.gmail.com>
 <59D661DA.6080901@gssi.infn.it>
Message-ID: <7D5227FB-3603-41BB-AFEC-017240D3CC13@utoronto.ca>

Just for the record - and posterity: this is the Wrong way to go about defining a fixed width format and the strategy has a significant probability of corrupting data in ways that are hard to spot and hard to debug. If you _have_ the specification, then _use_ the specification.

Consider what you are actually doing here:

start <- c(1, 20, 35, 41, 44, 48, 53, 59, 64, 70, 76, 78, 83, 88,
           93, 114, 122, 127)
diff(start)  # produces a vector of assumed column widths
# [1] 19 15  6  3  4  5  6  5  6  6  2  5  5  5 21  8  5

First:
Jim's solution uses fwf_widths(), which is not correct in the first place since not all positions of the record are actually specified: position 19, 35, 41, 44, 48, 53, 59 etc. etc. are columns that are intentionally left blank to visually separate numbers in the file. fwf_widths() assumes there are no gaps in the data. Can you assume these blank columns are always " "? Maybe in your case you can get away with this, but in the general case that's a risky assumption to make. As you said, you only care about two columns anyway, and those happen to be correct - but that's all the more reason not to even touch the others.

Second:
Jim's solution is a visual approximation, based on a small number of sample rows you have provided. As the spec shows, this gets it wrong because your sample does not exhaust all possibilities and he has no way of knowing the semantics of the data. The spec has 23 columns of data, you are reading only 17. For example the second column in that approach reads 15 characters, and now contains RAh, RAm, RAs, DE-, DEd, DEm and DEs all rolled into one string.

Third:
When writing code like this, you end up with a whole line of "magic numbers". This is unmaintainable. I would require my students to write something like the following, which comments what the columns _mean_:

cNames <- character()
cStarts <- integer()
cEnds <- integer()

cNames[1]  <- "Name" # Galaxy name in well-known catalogs
cStarts[1] <- 1L
cEnds[1]   <- 18L

cNames[2]  <- "logMHI" # Log hydrogen mass
cStarts[2] <- 78L
cEnds[2]   <- 82L


and then read

myCatalogue <- read_fwf(input,
                        col_positions = fwf_positions(cStart, cEnd),
                        col_names = cNames)

It may take you longer to type this up, but the chance of getting it right is much higher, and most importantly, it's crystal clear, explicit, extensible, and your summer student will see exactly what's going on here.


I hope this is helpful, and it comes closer to "how to deal with fixed-width files".
Cheers,
 
Boris



> On Oct 5, 2017, at 12:46 PM, jean-philippe <jeanphilippe.fontaine at gssi.infn.it> wrote:
> 
> dear Jim,
> 
> 
> Yes I fixed the problem. Thanks again all of you for your contribution!
> This worked :
> 
> start <- c(1, 20, 35, 41, 44, 48, 53, 59, 64, 70, 76, 78, 83, 88,
>           +            93, 114, 122, 127)
> data1<-read_fwf("lvg_table2.txt",skip=70, fwf_widths(diff(start)))
> 
> Well now I know how to deal with fixed-width files :)
> 
> 
> Cheers
> 
> 
> Jean-Philippe
> 
> On 05/10/2017 18:42, jim holtman wrote:
>> You should be able to use that header information to create the
>> correct parameters to the read_fwf function to read in the data.
>> 
>> Jim Holtman
>> Data Munger Guru
>> 
>> What is the problem that you are trying to solve?
>> Tell me what you want to do, not how you want to do it.
>> 
>> 
>> On Thu, Oct 5, 2017 at 11:02 AM, jean-philippe
>> <jeanphilippe.fontaine at gssi.infn.it> wrote:
>>> dear Jim,
>>> 
>>> Thanks for your reply and your proposition.
>>> 
>>> I forgot to provide the header of the dataframe, here it is:
>>> ================================================================================
>>> Byte-by-byte Description of file: lvg_table2.dat
>>> --------------------------------------------------------------------------------
>>>    Bytes Format Units       Label   Explanations
>>> --------------------------------------------------------------------------------
>>>    1- 18 A18    ---         Name    Galaxy name in well-known catalogs
>>>   20- 21 I2     h           RAh     Hour of Right Ascension (J2000)
>>>   22- 23 I2     min         RAm     Minute of Right Ascension (J2000)
>>>   24- 27 F4.1   s           RAs     Second of Right Ascension (J2000)
>>>       28 A1     ---         DE-     Sign of the Declination (J2000)
>>>   29- 30 I2     deg         DEd     Degree of Declination (J2000)
>>>   31- 32 I2     arcmin      DEm     Arcminute of Declination (J2000)
>>>   33- 34 I2     arcsec      DEs     Arcsecond of Declination (J2000)
>>>   36- 40 F5.2   kpc         a26     ? Major linear diameter (1)
>>>   42- 43 I2     deg         inc     ? Inclination
>>>   45- 47 I3     km/s        Vm      ? Amplitude of rotational velocity (2)
>>>   49- 52 F4.2   mag         AB      ? Internal B band extinction (3)
>>>   54- 58 F5.1   mag         BMag    ? Absolute B band magnitude (4)
>>>   60- 63 F4.1   mag/arcsec2 SBB     ? Average B band surface brightness (5)
>>>   65- 69 F5.2   [solLum]    logKLum ? Log K_S_ band luminosity (6)
>>>   71- 75 F5.2   [solMass]   logM26  ? Log mass within Holmberg radius (7)
>>>       77 A1     ---       l_logMHI  Limit flag on logMHI
>>>   78- 82 F5.2   [solMass]   logMHI  ? Log hydrogen mass (8)
>>>   84- 87 I4     km/s        VLG     ? Radial velocity (9)
>>>   89- 92 F4.1   ---         Theta1  ? Tidal index (10)
>>>   94-116 A23    ---         MD      Main disturber name (11)
>>>  118-121 F4.1   ---         Theta5  ? Another tidal index (12)
>>>  123-127 F5.2   [-]         Thetaj  ? Log K band luminosity density (13)
>>> --------------------------------------------------------------------------------
>>> 
>>> The idea for me is to select only the galaxy name and the logMHI values for
>>> these galaxies, so quite a simple job when the dataset is tidy enough. I was
>>> thinking as usual to use select from dplyr.
>>> That is why I was just asking how to read this kind of files which, for me
>>> so far, are uncommon.
>>> 
>>> Doing what you propose, it formats most of the columns correctly except few
>>> ones, I will see how I can change some width to get it correctly:
>>> 
>>>           X1              X2    X3    X4    X5    X6    X7    X8 X9    X10
>>> X11   X12   X13   X14          X15   X16     X17
>>>        (chr)           (chr) (dbl) (int) (dbl) (dbl) (chr) (dbl) (chr)
>>> (chr) (int) (chr) (chr) (chr)        (chr) (dbl)   (chr)
>>> 1   UGC12894 000022.5+392944  2.78    33    21     0 -13.3  25.2 7.5 8  8.1
>>> 7   7.9 2  61 9 -1.    3 NGC7640    -1 0  0.12
>>> 2        WLM 000158.1-152740  3.25    90    22     0 -14.1 24.8 7.7 0 8.2
>>> 7   7.8 4  -1 6  0. 0 MESSIER031     0 2  1.75
>>> 3  And XVIII 000214.5+450520  0.69    17     9     0  -8.7  26.8 6.4 4  6.7
>>> 8 < 6.6 5  -4 4  0. 5 MESSIER031     0 6  1.54
>>> 4  PAndAS-03 000356.4+405319  0.10    17    NA     0  -3.6  27.8 4.3      8
>>> NA    NA    NA    2. 8 MESSIER031     2 8  1.75
>>> 5  PAndAS-04 000442.9+472142  0.05    22    NA     0  -6.6  23.1 5.5      9
>>> NA    NA   -10 8  2. 5 MESSIER031     2 5  1.75
>>> 6  PAndAS-05 000524.1+435535  0.06    31    NA     0  -4.5  25.6 4.7      5
>>> NA    NA    10 3  2. 8 MESSIER031     2 8  1.75
>>> 7 ESO409-015 000531.8-280553  3.00    78    23     0 -14.6  24.1 8.1 0  8.2
>>> 5   8.1 0  76 9 -2.    0 NGC0024    -1 5 -2.05
>>> 8  AGC748778 000634.4+153039 0.61 70     3     0 -10.4  24.9 6.3 9  5.7
>>> 0   6.6 4  48 6 -1.    9 NGC0253    -1 5 -2.72
>>> 9     And XX 000730.7+350756  0.20    33     5     0  -5.8  27.1 5.2 6  5.7
>>> 0    NA   -18 2  2. 4 MESSIER031     2 4  1.75
>>> 
>>> 
>>> Cheers, thanks again
>>> 
>>> 
>>> Jean-Philippe
>>> On 05/10/2017 16:49, jim holtman wrote:
>>>> start <- c(1, 20, 35, 41, 44, 48, 53, 59, 64, 69, 75, 77, 82, 87,
>>>>      +            92, 114, 121, 127)
>>>>      > read_fwf(input, fwf_widths(diff(start)))
>>> 
>>> --
>>> Jean-Philippe Fontaine
>>> PhD Student in Astroparticle Physics,
>>> Gran Sasso Science Institute (GSSI),
>>> Viale Francesco Crispi 7,
>>> 67100 L'Aquila, Italy
>>> Mobile: +393487128593, +33615653774
>>> 
> 
> -- 
> Jean-Philippe Fontaine
> PhD Student in Astroparticle Physics,
> Gran Sasso Science Institute (GSSI),
> Viale Francesco Crispi 7,
> 67100 L'Aquila, Italy
> Mobile: +393487128593, +33615653774
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From alexandra.thorn at gmail.com  Thu Oct  5 19:54:06 2017
From: alexandra.thorn at gmail.com (Alexandra Thorn)
Date: Thu, 5 Oct 2017 13:54:06 -0400
Subject: [R] working with ordinal predictor variables?
Message-ID: <20171005135406.2be24d00@athorn-Lemur-Ultra>

I'm trying to develop a linear model for crop productivity based on
variables published as part of the SSURGO database released by the
USDA.  My default is to just run lm() with continuous predictor
variables as numeric, and discrete predictor variables as factors, but
some of the discrete variables are ordinal (e.g. drainage class, which
ranges from excessively drained to excessively poorly drained), but
this doesn't make use of the fact that the predictor variables have a
known order.

How do I correctly set up a regression model (with lm or similar) to
detect the influence of ordinal variables?

How will the output differ compared to the dummy variable outputs for
unordered categorical variables.

Thanks,
Alex


From kpmainali at gmail.com  Thu Oct  5 19:02:02 2017
From: kpmainali at gmail.com (Kumar Mainali)
Date: Thu, 5 Oct 2017 13:02:02 -0400
Subject: [R] fraction of null deviance explained by each node/variable in
 regression trees
Message-ID: <CABK368gupQ43GGzOw-cS8ai43UKdBPfZ-gfdLy3838us5Y6Z_Q@mail.gmail.com>

I have used packages rpart, mvpart and tree for classification and
regression trees. I want to calculate fraction of null deviance explained
by each node and variable in the tree. For instance, at the first split,
this would be (1 - (sum of residual deviance in each of the two
leaves)/deviance at the root). In the subsequent splits, this formula is
slightly different.

There probably is a function to get this done within R. Any help is
appreciated.

Thank you,
Kumar Mainali

-- 
Postdoctoral Associate
Department of Biology
University of Maryland, College Park
?

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Oct  5 21:35:15 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 5 Oct 2017 12:35:15 -0700
Subject: [R] working with ordinal predictor variables?
In-Reply-To: <20171005135406.2be24d00@athorn-Lemur-Ultra>
References: <20171005135406.2be24d00@athorn-Lemur-Ultra>
Message-ID: <CAGxFJbRmVfq4ZRTNbSeEqHkj4g5rZ4omWVncaH0qoGssgA-6iA@mail.gmail.com>

I would consider this is a question for a statistics forum such as
stats.stackexchange.com, not R-help, which is about R programming. They do
sometimes intersect, as here, but I think you need to *understand what
you're doing* before you write the R code to do it.

Obviously, IMO.

Cheers,
Bert




Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Thu, Oct 5, 2017 at 10:54 AM, Alexandra Thorn <alexandra.thorn at gmail.com>
wrote:

> I'm trying to develop a linear model for crop productivity based on
> variables published as part of the SSURGO database released by the
> USDA.  My default is to just run lm() with continuous predictor
> variables as numeric, and discrete predictor variables as factors, but
> some of the discrete variables are ordinal (e.g. drainage class, which
> ranges from excessively drained to excessively poorly drained), but
> this doesn't make use of the fact that the predictor variables have a
> known order.
>
> How do I correctly set up a regression model (with lm or similar) to
> detect the influence of ordinal variables?
>
> How will the output differ compared to the dummy variable outputs for
> unordered categorical variables.
>
> Thanks,
> Alex
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From boris.steipe at utoronto.ca  Thu Oct  5 21:45:17 2017
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Thu, 5 Oct 2017 15:45:17 -0400
Subject: [R] working with ordinal predictor variables?
In-Reply-To: <CAGxFJbRmVfq4ZRTNbSeEqHkj4g5rZ4omWVncaH0qoGssgA-6iA@mail.gmail.com>
References: <20171005135406.2be24d00@athorn-Lemur-Ultra>
 <CAGxFJbRmVfq4ZRTNbSeEqHkj4g5rZ4omWVncaH0qoGssgA-6iA@mail.gmail.com>
Message-ID: <691FEE6A-94EB-45B2-8A8B-D95843FE45DE@utoronto.ca>

This article may be helpful, at least to get you started:

https://www.r-bloggers.com/ordinal-data/

Cheers,
Boris




> On Oct 5, 2017, at 3:35 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> I would consider this is a question for a statistics forum such as
> stats.stackexchange.com, not R-help, which is about R programming. They do
> sometimes intersect, as here, but I think you need to *understand what
> you're doing* before you write the R code to do it.
> 
> Obviously, IMO.
> 
> Cheers,
> Bert
> 
> 
> 
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> On Thu, Oct 5, 2017 at 10:54 AM, Alexandra Thorn <alexandra.thorn at gmail.com>
> wrote:
> 
>> I'm trying to develop a linear model for crop productivity based on
>> variables published as part of the SSURGO database released by the
>> USDA.  My default is to just run lm() with continuous predictor
>> variables as numeric, and discrete predictor variables as factors, but
>> some of the discrete variables are ordinal (e.g. drainage class, which
>> ranges from excessively drained to excessively poorly drained), but
>> this doesn't make use of the fact that the predictor variables have a
>> known order.
>> 
>> How do I correctly set up a regression model (with lm or similar) to
>> detect the influence of ordinal variables?
>> 
>> How will the output differ compared to the dummy variable outputs for
>> unordered categorical variables.
>> 
>> Thanks,
>> Alex
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From macqueen1 at llnl.gov  Fri Oct  6 01:43:39 2017
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Thu, 5 Oct 2017 23:43:39 +0000
Subject: [R] working with ordinal predictor variables?
In-Reply-To: <20171005135406.2be24d00@athorn-Lemur-Ultra>
References: <20171005135406.2be24d00@athorn-Lemur-Ultra>
Message-ID: <E4C081BA-939C-426F-B81A-B6438394C9FD@llnl.gov>

Try looking at the help page for factor
  ?factor
for something to start with.

--
Don MacQueen
Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062
Lab cell 925-724-7509
 
 

On 10/5/17, 10:54 AM, "R-help on behalf of Alexandra Thorn" <r-help-bounces at r-project.org on behalf of alexandra.thorn at gmail.com> wrote:

    I'm trying to develop a linear model for crop productivity based on
    variables published as part of the SSURGO database released by the
    USDA.  My default is to just run lm() with continuous predictor
    variables as numeric, and discrete predictor variables as factors, but
    some of the discrete variables are ordinal (e.g. drainage class, which
    ranges from excessively drained to excessively poorly drained), but
    this doesn't make use of the fact that the predictor variables have a
    known order.
    
    How do I correctly set up a regression model (with lm or similar) to
    detect the influence of ordinal variables?
    
    How will the output differ compared to the dummy variable outputs for
    unordered categorical variables.
    
    Thanks,
    Alex
    
    ______________________________________________
    R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    https://stat.ethz.ch/mailman/listinfo/r-help
    PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    and provide commented, minimal, self-contained, reproducible code.
    


From akram2004hatem at gmail.com  Fri Oct  6 04:19:19 2017
From: akram2004hatem at gmail.com (Akram Alhadainy)
Date: Thu, 5 Oct 2017 22:19:19 -0400
Subject: [R] Help in R package
Message-ID: <CAEc2e4xqWo9Y1cGBBAQ-s=h+8=EH9qKXRYDFjSU59SbMpGZKNA@mail.gmail.com>

Hello,

I watched your YouTube videos for explanation for R package. They are
really helpful for new beginner for using R. Please I need your help for
solving inequalities that contain beta functions using R studio. I attached
the file that contains this inequalities.

I appreciate your help and looking forward to hear from you.

Akram
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Inequalities.pdf
Type: application/pdf
Size: 289288 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20171005/26179912/attachment.pdf>

From miaojpm at gmail.com  Fri Oct  6 08:56:27 2017
From: miaojpm at gmail.com (John)
Date: Thu, 5 Oct 2017 23:56:27 -0700
Subject: [R] Time series: xts/zoo object at annual (yearly) frequency
Message-ID: <CABcx46AnOb0QYmyDJAv+xxZHxG7F=tWdjeQo6BWc5tw6XUtyCQ@mail.gmail.com>

Hi,

   I'd like to make a time series at an annual frequency.

> a<-xts(x=c(2,4,5), order.by=c("1991","1992","1993"))
Error in xts(x = c(2, 4, 5), order.by = c("1991", "1992", "1993")) :
  order.by requires an appropriate time-based object
> a<-xts(x=c(2,4,5), order.by=1991:1993)
Error in xts(x = c(2, 4, 5), order.by = 1991:1993) :
  order.by requires an appropriate time-based object

  How should I do it? I know that to do for quarterly or monthly time
series, we use as.yearqtr or as.yearmon. What about annual?

   Thanks,

John

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Fri Oct  6 09:26:59 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Fri, 6 Oct 2017 10:26:59 +0300
Subject: [R] Time series: xts/zoo object at annual (yearly) frequency
In-Reply-To: <CABcx46AnOb0QYmyDJAv+xxZHxG7F=tWdjeQo6BWc5tw6XUtyCQ@mail.gmail.com>
References: <CABcx46AnOb0QYmyDJAv+xxZHxG7F=tWdjeQo6BWc5tw6XUtyCQ@mail.gmail.com>
Message-ID: <CAGgJW75_-Xjt5YwCUsKzzVVQLbW6jLfbkH=TjS586GyUOzy9Wg@mail.gmail.com>

Hi John,
Here's one way to do it:
vec <- c(2,4,5)
yrs <- seq(from=as.Date("1991-01-01"),by="1 year",length=length(vec))
a <-  xts(x=vec, order.by=yrs)

HTH,
Eric


On Fri, Oct 6, 2017 at 9:56 AM, John <miaojpm at gmail.com> wrote:

> Hi,
>
>    I'd like to make a time series at an annual frequency.
>
> > a<-xts(x=c(2,4,5), order.by=c("1991","1992","1993"))
> Error in xts(x = c(2, 4, 5), order.by = c("1991", "1992", "1993")) :
>   order.by requires an appropriate time-based object
> > a<-xts(x=c(2,4,5), order.by=1991:1993)
> Error in xts(x = c(2, 4, 5), order.by = 1991:1993) :
>   order.by requires an appropriate time-based object
>
>   How should I do it? I know that to do for quarterly or monthly time
> series, we use as.yearqtr or as.yearmon. What about annual?
>
>    Thanks,
>
> John
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From hemantsain55 at gmail.com  Fri Oct  6 08:58:27 2017
From: hemantsain55 at gmail.com (Hemant Sain)
Date: Fri, 6 Oct 2017 12:28:27 +0530
Subject: [R] Help RFM analysis in R (i want a code where i can define my own
 breaks instead of system defined breaks used in auto_RFM package)
Message-ID: <CAJL6Qs_56mMkT9DBrb5w7V9cop+h-EHpmWWqd3k1MiD7PbpE9g@mail.gmail.com>

I'm trying to perform an RFM analysis on the attached dataset,
I'm able to get the results using the auto_rfm function but i want to
define my own breaks for RFM.
as follow
r <-c(30,60,90)
 f <-c(2,5,8)
 m <-c(10,20,30)
but when i tried to define my own breaks i got the identical result for RFM
i.e 111 for every ID.
please help me with this with working R script so that i can get
output according to my own defined values of RFM instead of system defined
breaks.
Thanks


*Dataset:*

*user_id subtotal_amount created_at*

*3451945 19.32 6/11/2017 17:40*
*5404261 20 6/16/2017 22:45*

*3572177 9.78 7/6/2017 0:41*
*1197515 11.97 5/20/2017 17:48*
*7288355 14.76 6/5/2017 17:48*
*3071276 7.99 6/11/2017 0:13*
*8568400 15.98 6/22/2017 0:59*
*429475 7.99 6/8/2017 18:14*
*6805938 13.97 7/1/2017 23:30*
*561442 11.67 6/22/2017 18:13*
*1127373 11.27 6/11/2017 16:43*
*5973764 12.07 6/19/2017 22:43*
*683302 12.37 6/19/2017 17:18*
*391019 26.64 6/23/2017 23:57*
*580790 22.85 5/26/2017 23:34*
*1315314 6.29 6/18/2017 23:10*
*6980574 8.67 5/13/2017 20:21*
*8279240 17.26 6/2/2017 2:46*
*8700821 9.48 6/26/2017 1:47*
*778933 13.05 5/11/2017 17:50*
*1028301 9.47 5/31/2017 20:56*
*8305179 8.49 6/16/2017 0:17*
*8294420 12.65 6/11/2017 18:04*
*5775051 11.28 6/26/2017 17:13*
*3527917 7.99 6/4/2017 21:31*
*7434689 9.78 5/5/2017 16:43*
*6299124 20.65 5/25/2017 22:55*
*407736 6.88 6/5/2017 23:40*
*6207916 11.48 6/17/2017 16:31*
*2284913 10.08 7/21/2017 16:22*
*5833389 15.67 7/31/2017 17:19*
*1537907 22.63 7/4/2017 17:23*
*8791577 12.97 7/27/2017 0:09*
*7390743 18.36 5/7/2017 16:22*
*8562057 17.64 5/27/2017 19:07*
*393153 7.98 7/19/2017 16:42*
*6764358 15.37 7/14/2017 20:20*
*6444042 9.28 5/3/2017 19:54*
*442647 15.96 7/22/2017 0:01*
*6665810 7.99 7/3/2017 16:20*
*3318928 7.99 6/11/2017 22:36*
*565493 24.56 7/11/2017 16:06*
*3337179 16.86 5/31/2017 0:20*
*394651 21.67 5/22/2017 23:30*
*421849 23.57 5/24/2017 17:12*
*404111 22.06 5/22/2017 23:05*
*3967182 8.29 7/23/2017 23:00*
*8380345 10.38 6/22/2017 15:11*
*6843512 6.49 6/16/2017 2:11*
*3562940 8.18 7/18/2017 17:09*
*678953 6.99 7/7/2017 16:37*
*477935 8.47 7/19/2017 16:07*
*8069635 6.79 7/27/2017 17:29*
*435287 15.98 7/25/2017 21:39*
*7210916 11.47 7/12/2017 17:30*
*320190 16.86 6/10/2017 23:16*
*7101677 9.28 6/6/2017 20:50*
*1358520 16.65 7/10/2017 17:52*
*485601 5.99 7/18/2017 17:46*
*7288355 24.32 7/28/2017 0:01*
*8657204 6.46 6/26/2017 22:32*
*368087 15.26 6/17/2017 17:20*
*5532715 11.78 7/24/2017 16:54*
*318181 11.16 5/14/2017 23:01*
*457094 10.92 6/15/2017 18:43*
*8733533 9.67 6/16/2017 17:06*
*2229405 15.37 6/16/2017 16:59*
*654301 11.77 6/24/2017 0:16*
*440110 26.85 5/3/2017 0:58*
*478324 8.79 7/10/2017 17:02*
*927885 17.05 5/20/2017 15:41*
*1489397 8.47 7/26/2017 17:55*
*454200 13.94 7/26/2017 1:11*
*7235501 13.81 5/19/2017 1:13*
*527673 9.97 5/20/2017 0:37*
*2438553 7.99 6/18/2017 23:09*
*2592988 39.8 6/24/2017 17:37*
*538511 15.15 6/11/2017 16:58*
*481081 14.87 5/19/2017 17:57*
*1999017 18.06 7/21/2017 16:28*
*3925889 8.08 6/26/2017 16:17*
*1046802 10.67 7/19/2017 23:07*
*434850 7.49 7/2/2017 23:00*
*370681 8.47 7/14/2017 18:19*
*3554336 21.54 7/28/2017 16:50*
*5731193 9.67 5/22/2017 16:41*
*1062134 15.05 6/5/2017 21:13*
*408175 17.94 5/23/2017 20:18*
*8733533 8.38 7/15/2017 18:16*
*588771 13.87 6/25/2017 23:49*
*6338225 6.79 7/1/2017 18:59*
*6340638 17.95 6/15/2017 23:25*
*6926244 9.07 7/17/2017 19:02*
*8880079 10.58 6/14/2017 20:37*
*7333070 20.76 5/6/2017 17:22*
*6409065 13.76 7/30/2017 18:00*
*946735 7.57 7/21/2017 19:33*
*386328 15.98 7/21/2017 1:50*
*377431 9.16 5/30/2017 22:53*
*1870101 10.28 6/1/2017 17:00*
*318128 10.74 7/15/2017 22:16*
*3122611 7.68 6/27/2017 18:01*
*7528015 17.16 6/26/2017 21:10*
*6993335 8.57 6/13/2017 17:14*
*3424400 9.27 5/11/2017 18:51*
*7515441 15.77 6/26/2017 22:25*
*3962258 10.26 6/21/2017 1:33*
*6470596 12.16 7/31/2017 17:57*
*3415331 8.08 7/17/2017 19:19*
*3301515 10 7/2/2017 20:09*
*5359396 9.47 6/26/2017 19:44*
*7975103 13.87 6/29/2017 17:08*
*440393 6.18 5/3/2017 16:48*
*2170350 17.26 7/29/2017 23:11*
*370726 19.25 6/2/2017 16:03*
*761305 21.82 7/14/2017 17:25*
*7849625 12.03 7/24/2017 16:50*
*321713 7.68 7/12/2017 16:49*
*7300770 9.57 6/27/2017 16:06*
*1095644 8.47 5/9/2017 14:40*
*321315 17.94 5/4/2017 16:07*
*7579307 11.78 7/6/2017 19:42*
*7981910 7.99 6/11/2017 20:30*
*2731720 56.86 7/21/2017 16:35*
*628140 17.45 7/20/2017 17:31*
*321926 9.48 7/25/2017 17:25*
*7000119 9.37 5/28/2017 23:37*
*9101259 14.86 6/26/2017 16:31*
*8682720 17.73 7/15/2017 17:29*
*445002 30.25 5/4/2017 23:05*
*7319495 13.48 6/26/2017 17:09*
*7900556 7.99 5/28/2017 20:17*
*927932 11.37 5/7/2017 22:35*
*674966 6.89 6/28/2017 21:27*
*323736 33.25 5/17/2017 18:02*
*1096148 12.46 7/28/2017 17:36*
*3195598 6.98 5/16/2017 19:30*
*685341 7.83 6/10/2017 17:43*
*7006511 11.15 7/10/2017 20:34*
*320245 20.47 5/20/2017 16:03*
*2387580 7.36 6/26/2017 16:54*
*492746 7.99 6/25/2017 0:59*
*974050 8.49 7/16/2017 19:01*
*1706322 14.99 5/24/2017 20:22*
*7288355 12.47 5/19/2017 1:56*
*50496250 16.58 7/26/2017 0:24*
*447509 6.29 5/5/2017 16:32*
*1330217 8.27 5/19/2017 17:34*
*2154446 7.99 7/6/2017 16:41*
*1038646 8.69 5/13/2017 22:35*
*314670 8.49 5/29/2017 23:09*
*563231 26.32 7/31/2017 18:16*
*699366 12 7/4/2017 17:46*
*8306831 6.99 5/22/2017 21:58*
*4378079 10.17 7/10/2017 18:07*
*8307283 15.87 5/31/2017 19:22*
*6493978 17.32 7/18/2017 16:13*
*1299335 17.32 7/19/2017 23:24*
*1041199 11.07 5/25/2017 17:32*
*956047 7.27 5/24/2017 16:35*
*377134 19.25 7/31/2017 18:03*
*3395660 20.56 7/24/2017 18:25*
*482106 51.29 7/14/2017 17:02*
*521363 16.27 7/18/2017 22:03*
*537518 10.98 6/13/2017 21:48*
*1943828 11.77 6/29/2017 18:30*
*606395 10.98 7/11/2017 1:15*
*1228153 15.34 5/1/2017 18:48*
*6437041 10.27 5/12/2017 15:39*
*3109401 10.28 6/26/2017 16:43*
*530302 8.57 7/10/2017 18:10*
*3109401 28.94 6/22/2017 15:43*
*6461282 12.58 6/9/2017 22:52*
*8296976 16.15 5/29/2017 23:11*
*2018954 9.98 7/14/2017 23:33*
*6241196 19.56 7/14/2017 22:54*
*8217936 11.47 6/16/2017 17:10*
*463122 8.29 7/15/2017 18:28*
*532110 10.46 7/26/2017 16:53*
*9100252 15.07 6/26/2017 15:43*
*439030 9.37 7/19/2017 17:23*
*326157 11.73 6/13/2017 0:27*
*1146325 21.85 5/16/2017 0:30*
*1502399 8.29 7/6/2017 18:57*
*2623130 7.99 5/22/2017 15:57*
*2747554 11.28 5/7/2017 17:15*
*1645383 7.77 5/31/2017 18:16*
*2739083 6.66 7/3/2017 17:53*
*6004810 10.08 6/4/2017 19:39*
*3759866 14.06 7/1/2017 18:19*
*770582 10.59 5/11/2017 22:01*
*1186104 19.92 6/12/2017 17:19*
*636778 7.99 5/5/2017 17:42*
*6147540 12.82 7/23/2017 22:26*
*5813054 14.88 6/26/2017 17:00*
*4115178 12 7/3/2017 21:24*
*8964829 7.99 6/27/2017 21:28*
*3944025 9.96 5/16/2017 16:33*
*8227862 10.48 7/23/2017 19:14*
*7104071 14.46 6/14/2017 18:53*
*4255115 7.38 5/21/2017 21:36*
*2550433 8.49 6/8/2017 17:30*
*6824172 13.17 6/14/2017 20:36*
*455032 6.97 5/17/2017 17:38*
*1206605 10.67 6/24/2017 21:08*
*337571 8.67 7/7/2017 18:52*
*633665 11.98 7/9/2017 17:16*
*2202055 8.47 6/1/2017 17:54*
*3581705 15.15 6/4/2017 17:48*
*50484076 8.28 7/26/2017 18:44*
*1145712 14.85 7/12/2017 17:48*
*599316 13.47 5/5/2017 17:35*
*321097 17.85 5/10/2017 16:17*
*316951 6.88 7/11/2017 16:59*
*566170 8.49 6/11/2017 17:19*
*4183643 9.67 6/20/2017 16:54*
*3312216 27.02 6/15/2017 20:22*
*507997 8.78 5/31/2017 20:20*
*1564977 10.77 5/15/2017 18:26*
*1063879 10.67 5/11/2017 16:56*
*6768611 10.08 6/18/2017 16:02*
*466723 9.52 5/12/2017 0:21*
*461771 9.78 6/9/2017 19:00*
*1160016 15.05 6/27/2017 16:31*
*7808075 7.99 5/10/2017 18:05*
*3388025 7.28 5/26/2017 21:04*
*367393 16.94 7/31/2017 16:09*
*748047 8.37 7/12/2017 20:35*
*5312887 7.18 7/9/2017 23:32*
*9503792 44.46 7/16/2017 23:59*
*794037 16.66 7/28/2017 23:19*
*7742605 13.28 5/19/2017 17:45*
*424303 8.47 7/3/2017 16:56*
*423285 10.07 5/16/2017 16:30*
*744532 15.45 5/10/2017 21:47*
*1315758 12.45 5/4/2017 17:17*
*5768484 9.28 6/1/2017 16:47*
*1749414 7.88 7/23/2017 18:19*
*1943828 11.77 5/24/2017 18:45*
*6554452 9.58 7/14/2017 20:16*
*7333070 6.18 5/17/2017 22:34*
*6335238 9.27 5/23/2017 0:12*
*7898943 14.96 7/27/2017 18:50*
*439121 8.78 6/28/2017 16:56*
*321315 9.47 6/2/2017 16:43*
*970063 9.67 5/15/2017 13:20*
*744532 17.54 6/11/2017 19:27*
*380324 9.47 6/24/2017 1:00*
*50512368 9.47 7/27/2017 19:13*
*475428 9.48 6/17/2017 18:44*
*378535 17.36 7/6/2017 17:08*
*483547 11.29 7/31/2017 17:53*
*7825238 9.87 5/15/2017 17:55*
*3881856 8.08 6/26/2017 17:39*
*1188351 6.77 7/18/2017 17:29*
*3955854 9.37 5/1/2017 17:18*
*431483 7.88 7/2/2017 21:45*
*6906319 7.97 5/31/2017 17:05*
*2414336 17.26 5/10/2017 18:20*
*838785 9.47 6/28/2017 17:59*
*317459 9.97 7/10/2017 16:50*
*439914 9.67 7/14/2017 18:20*
*760177 17.94 6/27/2017 16:09*
*318091 14.77 5/14/2017 17:26*
*1274436 10.28 7/26/2017 17:29*
*2626118 8.49 7/7/2017 17:29*
*3224795 13.28 7/27/2017 23:22*
*7210916 10.78 6/30/2017 17:09*
*374293 13.58 7/28/2017 23:46*
*466723 8.07 5/16/2017 16:34*
*1591942 7.99 6/22/2017 17:04*
*1967943 6.49 5/5/2017 16:42*
*477759 6.38 5/12/2017 16:50*
*331106 10.67 7/5/2017 17:05*
*1170093 7.99 5/8/2017 18:37*
*3993066 22.26 6/4/2017 23:34*
*385427 10.78 5/10/2017 21:07*
*3563453 8.29 6/6/2017 16:43*
*992545 18.35 5/3/2017 15:35*
*430623 15.17 5/5/2017 16:10*
*7288355 14.76 6/3/2017 18:38*
*9124832 8.49 7/26/2017 21:53*
*7737402 14.54 5/17/2017 23:30*
*3913032 8.18 6/19/2017 17:37*
*335627 17.35 5/31/2017 16:34*
*1340737 11.87 7/12/2017 21:47*
*468557 7.98 5/10/2017 16:47*
*458337 7.49 5/23/2017 16:36*
*606287 10.48 5/1/2017 16:33*
*2321336 8.38 5/22/2017 16:25*





*R code*


*library(data.table)*
*df = fread("rfm_90.csv", header = T , stringsAsFactors = FALSE,
check.names = FALSE)[-1,]*

*sum(is.na <http://is.na/>(df))*

*df$user_id = as.integer(df$user_id)*
*df$subtotal_amount = as.numeric(df$subtotal_amount)*
*df$created_at = as.POSIXct(df$created_at)*
*df$created_at = as.Date(df$created_at)*


*occurences = table(unlist(df$user_id))*
*occurences = as.data.frame(occurences)*
*occurences = occurences[order(-occurences$Freq),]*

* r <-c(30,60,90)*
* f <-c(2,5,8)*
* m <-c(10,20,30)*

* getScoreWithBreaks <- function(df,r,f,m) {*

*   ## scoring the Recency*
*   len = length(r)*
*   R_Score <- c(rep(1,length(df[,1])))*
*   df <- cbind(df,R_Score)*
*   for(i in 1:len){*
*     if(i == 1){*
*       p1=0*
*     }else{*
*       p1=r[i-1]*
*     }*
*     p2=r[i]*

*     if(dim(df[p1<df$Recency & df$Recency<=p2,])[1]>0) df[p1<df$Recency &
df$Recency<=p2,]$R_Score = len - i+ 2*
*   }*

*   ## scoring the Frequency*
*   len = length(f)*
*   F_Score <- c(rep(1,length(df[,1])))*
*   df <- cbind(df,F_Score)*
*   for(i in 1:len){*
*     if(i == 1){*
*       p1=0*
*     }else{*
*       p1=f[i-1]*
*     }*
*     p2=f[i]*

*     if(dim(df[p1<df$Frequency & df$Frequency<=p2,])[1]>0)
df[p1<df$Frequency & df$Frequency<=p2,]$F_Score = i*
*   }*
*   if(dim(df[f[len]<df$Frequency,])[1]>0) df[f[len]<df$Frequency,]$F_Score
= len+1*

*   ## scoring the Monetary*
*   len = length(m)*
*   M_Score <- c(rep(1,length(df[,1])))*
*   df <- cbind(df,M_Score)*
*   for(i in 1:len){*
*     if(i == 1){*
*       p1=0*
*     }else{*
*       p1=m[i-1]*
*     }*
*     p2=m[i]*

*     if(dim(df[p1<df$Monetary & df$Monetary<=p2,])[1]>0) df[p1<df$Monetary
& df$Monetary<=p2,]$M_Score = i*
*   }*
*   if(dim(df[m[len]<df$Monetary,])[1]>0) df[m[len]<df$Monetary,]$M_Score =
len+1*

*   #order the dfframe by R_Score, F_Score, and M_Score desc*
*   df <- df[order(-df$R_Score,-df$F_Score,-df$M_Score),]*

*   # caculate the total score*
*   Total_Score <- c(100*df$R_Score + 10*df$F_Score+df$M_Score)*

*   df <- cbind(df,Total_Score)*

*   return(df)*

* }*



* df2<-getScoreWithBreaks(df,r,f,m)*

*# *
*library(easyRFM)*
*result <- rfm_auto(df, id="user_id", payment ="subtotal_amount",
date="created_at")*

*breaks = result$breaks*
*classes = result$classes*
*summary = result$get_table*

*classes = as.data.frame(classes)*
*breaks = as.data.frame(breaks)*
*summary = as.data.frame(summary)*
-- 
hemantsain.com

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Fri Oct  6 12:22:48 2017
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 6 Oct 2017 21:22:48 +1100
Subject: [R] Help RFM analysis in R (i want a code where i can define my
 own breaks instead of system defined breaks used in auto_RFM package)
In-Reply-To: <CAJL6Qs_56mMkT9DBrb5w7V9cop+h-EHpmWWqd3k1MiD7PbpE9g@mail.gmail.com>
References: <CAJL6Qs_56mMkT9DBrb5w7V9cop+h-EHpmWWqd3k1MiD7PbpE9g@mail.gmail.com>
Message-ID: <CA+8X3fXS+U02KxGTG1Y3BeaFR2puHb-S-v60UK8b30KSb1rcrg@mail.gmail.com>

Hi Hemant,
I had a chance to look at this. Here is a function that allows you to
rank customers on the raw values of recency, frequency and monetary. I
added the code for cutting the raw values into intervals, but haven't
had a chance to test it and I can't test it right now. It may be
helpful.

rfm_df<-read.table("rfm.csv",header=TRUE,sep="\t",stringsAsFactors = FALSE)

# expects a three (or more) column data frame where
# column 1 is customer ID, column 2 is amount of purchase
# and column 3 is date of purchase
qdrfm<-function(x,rbreaks=NULL,fbreaks=NULL,mbreaks=NULL,
 date.format="%Y-%m-%d") {

 today<-as.Date(date(), "%a %b %d %H:%M:%S %Y")
 x$rscore<-today-as.Date(x[,3],date.format)
 if(!is.null(rbreaks))
  x$rscore<-cut(x$rscore,breaks=rbreaks,labels=FALSE)
  custIDs<-unique(x[,1])
 ncust<-length(custIDs)
 rfmout<-data.frame(custID=custIDs,rscore=rep(0,ncust),
  fscore=rep(0,ncust),mscore=rep(0,ncust))
 rfmout$rscore=rank(by(x$rscore,x[,1],min))
 if(!is.null(fbreaks)) rfmout$fscore<-rank(by(x[,3],x[,1],length))
 else
  rfmout$fscore<-cut(by(x[,3],x[,1],length),breaks=fbreaks,labels=FALSE)
 if(!is.null(mbreaks)) rfmout$mscore<-rank(by(x[,2],x[,1],sum))
 else
  rfmout$mscore<-cut(by(x[,2],x[,1],sum),breaks=mbreaks,labels=FALSE)
 rfmout$cscore<-rank((order(rfmout$rscore)+
  order(rfmout$fscore,decreasing=TRUE)+
  order(rfmout$mscore,decreasing=TRUE))/3)
 rfmout$cscore<-rfmout$cscore-min(rfmout$cscore)+1
 return(rfmout[order(rfmout$cscore),])
}

qdrfm(rfm_df,date.format="%m/%d/%Y")

Jim


From ggrothendieck at gmail.com  Fri Oct  6 14:50:18 2017
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 6 Oct 2017 08:50:18 -0400
Subject: [R] Time series: xts/zoo object at annual (yearly) frequency
In-Reply-To: <CABcx46AnOb0QYmyDJAv+xxZHxG7F=tWdjeQo6BWc5tw6XUtyCQ@mail.gmail.com>
References: <CABcx46AnOb0QYmyDJAv+xxZHxG7F=tWdjeQo6BWc5tw6XUtyCQ@mail.gmail.com>
Message-ID: <CAP01uRnKfcukLZJPqnGMAWaS-r9iSM_GNg6GHC2GBmH3wWgQeg@mail.gmail.com>

Maybe one of these are close enough:

  xts(c(2, 4, 5), yearqtr(1991:1993))

  as.xts(ts(c(2, 4, 5), 1991))

of if you want only a plain year as the index then then use zoo,
zooreg or ts class:

  library(zoo)
  zoo(c(2, 4, 5), 1991:1993)

  zooreg(c(2, 4, 5), 1991)

  ts(c(2, 4, 5), 1991)

On Fri, Oct 6, 2017 at 2:56 AM, John <miaojpm at gmail.com> wrote:
> Hi,
>
>    I'd like to make a time series at an annual frequency.
>
>> a<-xts(x=c(2,4,5), order.by=c("1991","1992","1993"))
> Error in xts(x = c(2, 4, 5), order.by = c("1991", "1992", "1993")) :
>   order.by requires an appropriate time-based object
>> a<-xts(x=c(2,4,5), order.by=1991:1993)
> Error in xts(x = c(2, 4, 5), order.by = 1991:1993) :
>   order.by requires an appropriate time-based object
>
>   How should I do it? I know that to do for quarterly or monthly time
> series, we use as.yearqtr or as.yearmon. What about annual?
>
>    Thanks,
>
> John
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From dwinsemius at comcast.net  Fri Oct  6 17:44:30 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 6 Oct 2017 08:44:30 -0700
Subject: [R] Help in R package
In-Reply-To: <CAEc2e4xqWo9Y1cGBBAQ-s=h+8=EH9qKXRYDFjSU59SbMpGZKNA@mail.gmail.com>
References: <CAEc2e4xqWo9Y1cGBBAQ-s=h+8=EH9qKXRYDFjSU59SbMpGZKNA@mail.gmail.com>
Message-ID: <EC8044AF-F8E9-45A1-BFCC-3DA53DCDDB0A@comcast.net>


> On Oct 5, 2017, at 7:19 PM, Akram Alhadainy <akram2004hatem at gmail.com> wrote:
> 
> Hello,
> 
> I watched your YouTube videos for explanation for R package.

Rhelp is a mailing list. It does not release Youtube videos.

> They are
> really helpful for new beginner for using R. Please I need your help for
> solving inequalities that contain beta functions using R studio. I attached
> the file that contains this inequalities.

We are also not a code writing service and we don't do homework. Please read the Posting Guide.


> 
> I appreciate your help and looking forward to hear from you.
> 
> Akram
> <Inequalities.pdf>______________________________________________


David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From bedanajoshuasy at gmail.com  Fri Oct  6 15:05:17 2017
From: bedanajoshuasy at gmail.com (Joshua Bedana)
Date: Fri, 6 Oct 2017 06:05:17 -0700
Subject: [R] How to resolve this error
Message-ID: <CAG-ZcD5EpvX5+jJGVVvOViMcCps5P1UNCog6PDe16Ccc90S-YQ@mail.gmail.com>

> library(SpatioTemporal)
> library(plotrix)
> library(maps)
> palay.cov<-read.csv("C:/Users/BEDANA-PC/Desktop/STThesisWD/Thesis
Data/palay.covar.csv")
> palay.o<-read.table("C:/Users/BEDANA-PC/Desktop/STThesisWD/Thesis
Data/palay.obs.txt")
> palay.obs<-as.matrix(palay.o)
> palay.stc<-read.table("C:/Users/BEDANA-PC/Desktop/STThesisWD/Thesis
Data/palay.stcovars.txt")
> palay.stcov<-as.matrix(palay.stc)
> palay.data.raw<-list(X=palay.cov,obs=palay.obs,Y=palay.stcov)
> ##matrix of observations
> obs<-palay.data.raw$obs
> ##data.frame of geographical covariates
> geocov<-palay.data.raw$X
> ##matrix of spatio-temporal covariate
> stcov<-list(Y=palay.data.raw$Y)
> ##creating ST Data
> palay.data<-createSTdata(obs,geocov,SpatioTemporal=stcov)
Error in stCheckObs(obs) :
  Duplicated observations, i.e. multiple elements with same obs$date and
obs$ID.
>

please help me.

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Fri Oct  6 19:53:50 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 6 Oct 2017 10:53:50 -0700
Subject: [R] How to resolve this error
In-Reply-To: <CAG-ZcD5EpvX5+jJGVVvOViMcCps5P1UNCog6PDe16Ccc90S-YQ@mail.gmail.com>
References: <CAG-ZcD5EpvX5+jJGVVvOViMcCps5P1UNCog6PDe16Ccc90S-YQ@mail.gmail.com>
Message-ID: <CAGxFJbQf4=ODmBuEwjVEYmJ7uXayvEoyiv_LVA_5E++Wptb4AQ@mail.gmail.com>

?duplicated  to find the duplicated elements.

e.g.

duplicated(obs[,c("date","ID")]) ## data frame of relevant columns

Note that a web search on "Find duplicates in R" would have brought this up
immediately. ** Do** avail yourself of standard search capabilities before
posting.

Cheers,
Bert




Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Fri, Oct 6, 2017 at 6:05 AM, Joshua Bedana <bedanajoshuasy at gmail.com>
wrote:

> > library(SpatioTemporal)
> > library(plotrix)
> > library(maps)
> > palay.cov<-read.csv("C:/Users/BEDANA-PC/Desktop/STThesisWD/Thesis
> Data/palay.covar.csv")
> > palay.o<-read.table("C:/Users/BEDANA-PC/Desktop/STThesisWD/Thesis
> Data/palay.obs.txt")
> > palay.obs<-as.matrix(palay.o)
> > palay.stc<-read.table("C:/Users/BEDANA-PC/Desktop/STThesisWD/Thesis
> Data/palay.stcovars.txt")
> > palay.stcov<-as.matrix(palay.stc)
> > palay.data.raw<-list(X=palay.cov,obs=palay.obs,Y=palay.stcov)
> > ##matrix of observations
> > obs<-palay.data.raw$obs
> > ##data.frame of geographical covariates
> > geocov<-palay.data.raw$X
> > ##matrix of spatio-temporal covariate
> > stcov<-list(Y=palay.data.raw$Y)
> > ##creating ST Data
> > palay.data<-createSTdata(obs,geocov,SpatioTemporal=stcov)
> Error in stCheckObs(obs) :
>   Duplicated observations, i.e. multiple elements with same obs$date and
> obs$ID.
> >
>
> please help me.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From paulbernal07 at gmail.com  Fri Oct  6 20:30:55 2017
From: paulbernal07 at gmail.com (Paul Bernal)
Date: Fri, 6 Oct 2017 13:30:55 -0500
Subject: [R] Formatting the dates generated by the forecast function
Message-ID: <CAMOcQfPh=oBN8or31-8OCiAQP-B_87kZ3uZRaq1++DKKq-h7NQ@mail.gmail.com>

Dear friend, hope you are doing great,

I have the following code:

> myTseriesData <- ts(data[,2], start=c(2000,01), end=c(2017,9),
frequency=12)
> myTseriesModel <- auto.arima(myTseriesData, d=1, D=1)
> myTseriesForecast <- forecast(myTseriesModel, h=12)
> # I want to be able to format the dates generated by the forecast function


is there a way to change the format of the dates generated by the forecast
function into a YYYY-mm-dd format?

Any hint or guidance will be greatly appreciated,

Best of regards,

Paul

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Fri Oct  6 23:54:05 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 6 Oct 2017 14:54:05 -0700
Subject: [R] Formatting the dates generated by the forecast function
In-Reply-To: <CAMOcQfPh=oBN8or31-8OCiAQP-B_87kZ3uZRaq1++DKKq-h7NQ@mail.gmail.com>
References: <CAMOcQfPh=oBN8or31-8OCiAQP-B_87kZ3uZRaq1++DKKq-h7NQ@mail.gmail.com>
Message-ID: <CAGxFJbR4WSxNNZDjgw-yp8FmkZFnq7ZDVx6bTK=fBu3m0ot2Dw@mail.gmail.com>

?format.Date

-- Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Fri, Oct 6, 2017 at 11:30 AM, Paul Bernal <paulbernal07 at gmail.com> wrote:

> Dear friend, hope you are doing great,
>
> I have the following code:
>
> > myTseriesData <- ts(data[,2], start=c(2000,01), end=c(2017,9),
> frequency=12)
> > myTseriesModel <- auto.arima(myTseriesData, d=1, D=1)
> > myTseriesForecast <- forecast(myTseriesModel, h=12)
> > # I want to be able to format the dates generated by the forecast
> function
>
>
> is there a way to change the format of the dates generated by the forecast
> function into a YYYY-mm-dd format?
>
> Any hint or guidance will be greatly appreciated,
>
> Best of regards,
>
> Paul
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r-subscribe at mail.ru  Sat Oct  7 04:58:43 2017
From: r-subscribe at mail.ru (=?UTF-8?B?VGVkIChCZWdpbm5lcik=?=)
Date: Sat, 07 Oct 2017 05:58:43 +0300
Subject: [R] =?utf-8?q?Adjusted_survival_curves?=
In-Reply-To: <mailman.0.1507284001.42721.r-help@r-project.org>
References: <mailman.0.1507284001.42721.r-help@r-project.org>
Message-ID: <1507345123.11352231@f487.i.mail.ru>


For adjusted survival curves I took the sample code from here: 
https://rpubs.com/daspringate/survival
and adapted for my date, but got error.
I would like to understand what is my mistake. Thanks!

#ADAPTATION FOR MY DATA
library(survival)
library(survminer)
df<-read.csv("F:/R/data/base.csv", header = TRUE, sep = ";")
head(df)
ID start stop censor sex age stage treatment
1 1 0 66 0 2 1 3 1
2 2 0 18 0 1 2 4 2
3 3 0 43 1 2 3 3 1
4 4 0 47 1 2 3 NA 2
5 5 0 26 0 1 4 3 NA

S <- Surv(
time = df$start, 
time2 = df$stop, 
event = df$censor)
head(S)
[1] (0,66+] (0,18+] (0,43] (0,47] (0,26+] (0,29+]

model <- coxph(S ~ df$treatment + df$age + df$sex + df$stage, data = df)

plot(survfit(model), 
las=1,
xscale = 1.00,
xlab = "Months after diagnosis",
ylab = "Proportion survived",
main = "Baseline Hazard Curve")

# BEFORE NOW everything works, but then ERROR
treat <- with(colon,
data.frame(
treatment = levels(df$treatment),
age = rep(levels(df$age)[1], 2),
sex = rep(levels(df$sex)[1], 2),
stage = rep(levels(df$stage)[1], 2)))

str(treat)
'data.frame': 0 obs. of 0 variables
	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sat Oct  7 05:39:22 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 6 Oct 2017 20:39:22 -0700
Subject: [R] Adjusted survival curves
In-Reply-To: <1507345123.11352231@f487.i.mail.ru>
References: <mailman.0.1507284001.42721.r-help@r-project.org>
 <1507345123.11352231@f487.i.mail.ru>
Message-ID: <FB895A6C-A760-4A52-BB97-577477D7B65A@comcast.net>


On Oct 6, 2017, at 7:58 PM, Ted (Beginner) via R-help <r-help at r-project.org> wrote:


> For adjusted survival curves I took the sample code from here: 
> https://rpubs.com/daspringate/survival
> and adapted for my date, but got error.
> I would like to understand what is my mistake. Thanks!
> 
> #ADAPTATION FOR MY DATA
> library(survival)
> library(survminer)
> df<-read.csv("F:/R/data/base.csv", header = TRUE, sep = ";")
> head(df)
> ID start stop censor sex age stage treatment
> 1 1 0 66 0 2 1 3 1
> 2 2 0 18 0 1 2 4 2
> 3 3 0 43 1 2 3 3 1
> 4 4 0 47 1 2 3 NA 2
> 5 5 0 26 0 1 4 3 NA
> 
> S <- Surv(
> time = df$start, 
> time2 = df$stop, 
> event = df$censor)
> head(S)
> [1] (0,66+] (0,18+] (0,43] (0,47] (0,26+] (0,29+]
> 
> model <- coxph(S ~ df$treatment + df$age + df$sex + df$stage, data = df)


---------
Se if this works:

model <- coxph( Surv(time = start, 
                     time2 = stop, 
                     event = censor)~ treatment + age + sex + stage, data = df)


R regression functions allow you to use column names. The tokens in the formula get handled with evaluation inside the `data` argument's environment.
------------

> plot(survfit(model), 
> las=1,
> xscale = 1.00,
> xlab = "Months after diagnosis",
> ylab = "Proportion survived",
> main = "Baseline Hazard Curve")
> 
> # BEFORE NOW everything works, but then ERROR
> treat <- with(colon,
> data.frame(
> treatment = levels(df$treatment),
> age = rep(levels(df$age)[1], 2),
> sex = rep(levels(df$sex)[1], 2),
> stage = rep(levels(df$stage)[1], 2)))
> 
> str(treat)
> 'data.frame': 0 obs. of 0 variables
> 	[[alternative HTML version deleted]]

In this instance you got away with posting in html, but it might not always be effective.
> 


David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From peter.wagley09 at gmail.com  Sat Oct  7 01:16:05 2017
From: peter.wagley09 at gmail.com (Peter Wagey)
Date: Fri, 6 Oct 2017 16:16:05 -0700
Subject: [R] bootstrapping results in table format
Message-ID: <CAA7XnHe1iHc-KBP0UpERSDrfWg_98cikDm+Owo+Z8f_Ye8OwSg@mail.gmail.com>

Hi R users,
I was struggling to put the results into table format. Would you mind to
show using following data and code how we can put the results into table? I
further would like to have a  confidence interval for each group.


set.seed(1000)
data <- as.data.table(list(x1 = runif(200), x2 = runif(200), group =
runif(200)>0.5))
data.frame(data)
head(data)
stat <- function(x, i) {x[i, c(m1 = mean(x1))]}
A<-data[, list(list(boot(.SD, stat, R = 10))), by = group]$V1

Thanks

	[[alternative HTML version deleted]]


From akram2004hatem at gmail.com  Sat Oct  7 03:17:33 2017
From: akram2004hatem at gmail.com (Akram Alhadainy)
Date: Fri, 6 Oct 2017 21:17:33 -0400
Subject: [R] beta binomial distribution
Message-ID: <CAEc2e4w8mBhbBDM+qRWYBknpsy9LSh7WEBnmSQ6=p0zD0TwYBw@mail.gmail.com>

Hi,
I need to write two inequalities depend on cumulative distribution (CDF) of
beta binomial distribution where alpha and beta are unknown and need to
find them.
 CDF of betabinomial(2,10,alpha,beta) <0.3<=CDF of
betabinomial(3,10,alpha,beta)
and
 CDF of betabinomial(5,10,alpha,beta) <0.8<=CDF of
betabinomial(6,10,alpha,beta)
How I can do that using r studio package?
I tried to do that using pbetabinom but it gives error and could not
discover this function? How I can define this distribution in r studio?
Thank you,
Akram

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Sat Oct  7 12:01:11 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Sat, 7 Oct 2017 12:01:11 +0200
Subject: [R] Adjusted survival curves
In-Reply-To: <1507345123.11352231@f487.i.mail.ru>
References: <mailman.0.1507284001.42721.r-help@r-project.org>
 <1507345123.11352231@f487.i.mail.ru>
Message-ID: <D263C351-2B10-4630-B839-97A7AFF97A3A@gmail.com>


> On 7 Oct 2017, at 04:58 , Ted (Beginner) via R-help <r-help at r-project.org> wrote:
> 
> 
> For adjusted survival curves I took the sample code from here: 
> https://rpubs.com/daspringate/survival
> and adapted for my date, but got error.
> I would like to understand what is my mistake. Thanks!
> 
> #ADAPTATION FOR MY DATA
> library(survival)
> library(survminer)
> df<-read.csv("F:/R/data/base.csv", header = TRUE, sep = ";")
> head(df)
> ID start stop censor sex age stage treatment
> 1 1 0 66 0 2 1 3 1
> 2 2 0 18 0 1 2 4 2
> 3 3 0 43 1 2 3 3 1
> 4 4 0 47 1 2 3 NA 2
> 5 5 0 26 0 1 4 3 NA
> 
> S <- Surv(
> time = df$start, 
> time2 = df$stop, 
> event = df$censor)
> head(S)
> [1] (0,66+] (0,18+] (0,43] (0,47] (0,26+] (0,29+]
> 
> model <- coxph(S ~ df$treatment + df$age + df$sex + df$stage, data = df)
> 
> plot(survfit(model), 
> las=1,
> xscale = 1.00,
> xlab = "Months after diagnosis",
> ylab = "Proportion survived",
> main = "Baseline Hazard Curve")
> 
> # BEFORE NOW everything works, but then ERROR
> treat <- with(colon,
> data.frame(
> treatment = levels(df$treatment),
> age = rep(levels(df$age)[1], 2),
> sex = rep(levels(df$sex)[1], 2),
> stage = rep(levels(df$stage)[1], 2)))
> 
> str(treat)
> 'data.frame': 0 obs. of 0 variables
> 	[[alternative HTML version deleted]]

None of your variables are factors (and age likely cannot be), so levels returns() NULL and you get the effect of:

> data.frame(a=NULL, b=NULL, c=NULL)
data frame with 0 columns and 0 rows

I think this is due to insufficient understanding of the example you are trying to copy. Perhaps try studying it in more detail, view intermediate results, etc. (It's unclear where the "colon" data in the rpubs example come from, though. The data set in survival differs. Ask RStudio, or the author...)

-pd


> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From r-subscribe at mail.ru  Sat Oct  7 18:01:19 2017
From: r-subscribe at mail.ru (=?UTF-8?B?VGVkIEJlZ2lubmVyIChSU3R1ZGlvKQ==?=)
Date: Sat, 07 Oct 2017 19:01:19 +0300
Subject: [R] =?utf-8?q?Adjusted_survival_curves?=
In-Reply-To: <mailman.0.1507370401.63416.r-help@r-project.org>
References: <mailman.0.1507370401.63416.r-help@r-project.org>
Message-ID: <1507392079.400038600@f456.i.mail.ru>


For adjusted survival curves I took the sample code from here: 
https://rpubs.com/daspringate/survival
and adapted for my date, but ... have a QUESTION.

library(survival)
library(survminer)
df<-read.csv("base.csv", header = TRUE, sep = ";")
head(df)
ID start stop censor sex age stage treatment
1 1 0 66 0 2 1 3 1
2 2 0 18 0 1 2 4 2
3 3 0 43 1 2 3 3 1
4 4 0 47 1 2 3 NA 2
5 5 0 26 0 1 4 3 NA

# THANKS, DAVID WINSEMIUS for remark!!! ("R regression functions allow you to use column names.")
model <- coxph( Surv(time = start,
time2 = stop,
event = censor) ~ treatment + age + sex + stage, data = df)

plot(survfit(model), 
las=1,
xscale = 1.00,
xlab = "Months after diagnosis",
ylab = "Proportion survived",
main = "Baseline Hazard Curve")

#HOW TO EXPLAIN TO R, THAT treatment, age, sex and stage ARE FACTORS, AND NOT CONTINUOUS VAR?
treat <- with (df,
data.frame (
treatment = levels(treatment),
age = rep(levels(age)[1], 2),
sex = rep(levels(sex)[1], 2),
stage = rep(levels(stage)[1], 2)))

str(treat)
	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sat Oct  7 18:08:20 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 7 Oct 2017 09:08:20 -0700
Subject: [R] beta binomial distribution
In-Reply-To: <CAEc2e4w8mBhbBDM+qRWYBknpsy9LSh7WEBnmSQ6=p0zD0TwYBw@mail.gmail.com>
References: <CAEc2e4w8mBhbBDM+qRWYBknpsy9LSh7WEBnmSQ6=p0zD0TwYBw@mail.gmail.com>
Message-ID: <06C42112-958F-460F-820F-05D954BC2046@comcast.net>


> On Oct 6, 2017, at 6:17 PM, Akram Alhadainy <akram2004hatem at gmail.com> wrote:
> 
> Hi,
> I need to write two inequalities depend on cumulative distribution (CDF) of
> beta binomial distribution where alpha and beta are unknown and need to
> find them.
> CDF of betabinomial(2,10,alpha,beta) <0.3<=CDF of
> betabinomial(3,10,alpha,beta)
> and
> CDF of betabinomial(5,10,alpha,beta) <0.8<=CDF of
> betabinomial(6,10,alpha,beta)

It remains unclear what this apparent homework problem us supposed to be teaching you to do. I see no constraints on alpha and beta, and you have shown no effort to submit complete code.

> How I can do that using r studio package?
> I tried to do that using pbetabinom but it gives error and could not
> discover this function? How I can define this distribution in r studio?

I'm not a regular user of the rstudio IDE but maybe it incorporates a search function? In a non-rstudio environment I would suggest:

install.packages("sos")
library(sos)
findFn("betabinomial")

# should also succeed in rstudio.

> Thank you,
> Akram
> 
> 	[[alternative HTML version deleted]]

And learn to post in plain text. Also read these documents.

> 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From jdnewmil at dcn.davis.ca.us  Sat Oct  7 18:18:13 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 07 Oct 2017 17:18:13 +0100
Subject: [R] Adjusted survival curves
In-Reply-To: <1507392079.400038600@f456.i.mail.ru>
References: <mailman.0.1507370401.63416.r-help@r-project.org>
 <1507392079.400038600@f456.i.mail.ru>
Message-ID: <29323ECA-36E1-44C5-BE46-D251E1D936DF@dcn.davis.ca.us>

Change the columns into factors before you give them to the coxph function, e.g.

df$treatment <- factor( df$treatment )

-- 
Sent from my phone. Please excuse my brevity.

On October 7, 2017 5:01:19 PM GMT+01:00, "Ted Beginner (RStudio) via R-help" <r-help at r-project.org> wrote:
>
>For adjusted survival curves I took the sample code from here: 
>https://rpubs.com/daspringate/survival
>and adapted for my date, but ... have a QUESTION.
>
>library(survival)
>library(survminer)
>df<-read.csv("base.csv", header = TRUE, sep = ";")
>head(df)
>ID start stop censor sex age stage treatment
>1 1 0 66 0 2 1 3 1
>2 2 0 18 0 1 2 4 2
>3 3 0 43 1 2 3 3 1
>4 4 0 47 1 2 3 NA 2
>5 5 0 26 0 1 4 3 NA
>
># THANKS, DAVID WINSEMIUS for remark!!! ("R regression functions allow
>you to use column names.")
>model <- coxph( Surv(time = start,
>time2 = stop,
>event = censor) ~ treatment + age + sex + stage, data = df)
>
>plot(survfit(model), 
>las=1,
>xscale = 1.00,
>xlab = "Months after diagnosis",
>ylab = "Proportion survived",
>main = "Baseline Hazard Curve")
>
>#HOW TO EXPLAIN TO R, THAT treatment, age, sex and stage ARE FACTORS,
>AND NOT CONTINUOUS VAR?
>treat <- with (df,
>data.frame (
>treatment = levels(treatment),
>age = rep(levels(age)[1], 2),
>sex = rep(levels(sex)[1], 2),
>stage = rep(levels(stage)[1], 2)))
>
>str(treat)
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Sat Oct  7 18:19:00 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 7 Oct 2017 09:19:00 -0700
Subject: [R] Adjusted survival curves
In-Reply-To: <1507392079.400038600@f456.i.mail.ru>
References: <mailman.0.1507370401.63416.r-help@r-project.org>
 <1507392079.400038600@f456.i.mail.ru>
Message-ID: <13C9CBC5-99D6-4A68-980B-B1A729CD36AC@comcast.net>


> On Oct 7, 2017, at 9:01 AM, Ted Beginner (RStudio) via R-help <r-help at r-project.org> wrote:
> 
> 
> For adjusted survival curves I took the sample code from here: 
> https://rpubs.com/daspringate/survival
> and adapted for my date, but ... have a QUESTION.
> 
> library(survival)
> library(survminer)
> df<-read.csv("base.csv", header = TRUE, sep = ";")
> head(df)
> ID start stop censor sex age stage treatment
> 1 1 0 66 0 2 1 3 1
> 2 2 0 18 0 1 2 4 2
> 3 3 0 43 1 2 3 3 1
> 4 4 0 47 1 2 3 NA 2
> 5 5 0 26 0 1 4 3 NA
> 
> # THANKS, DAVID WINSEMIUS for remark!!! ("R regression functions allow you to use column names.")
> model <- coxph( Surv(time = start,
> time2 = stop,
> event = censor) ~ treatment + age + sex + stage, data = df)
> 
> plot(survfit(model), 
> las=1,
> xscale = 1.00,
> xlab = "Months after diagnosis",
> ylab = "Proportion survived",
> main = "Baseline Hazard Curve")
> 
> #HOW TO EXPLAIN TO R, THAT treatment, age, sex and stage ARE FACTORS, AND NOT CONTINUOUS VAR?
> treat <- with (df,
> data.frame (
> treatment = levels(treatment),
> age = rep(levels(age)[1], 2),
> sex = rep(levels(sex)[1], 2),
> stage = rep(levels(stage)[1], 2)))

Perhaps:

treat2 <- treat
treat2[,c('treatment', 'age', 'sex', 'stage')] <- lapply( treat2[,c('treatment', 'age', 'sex', 'stage')], factor) 


> treat2
  ID start stop censor sex age stage treatment
1  1     0   66      0   2   1     3         1
2  2     0   18      0   1   2     4         2
3  3     0   43      1   2   3     3         1
4  4     0   47      1   2   3  <NA>         2
5  5     0   26      0   1   4     3      <NA>
> str(treat2)
'data.frame':	5 obs. of  8 variables:
 $ ID       : int  1 2 3 4 5
 $ start    : int  0 0 0 0 0
 $ stop     : int  66 18 43 47 26
 $ censor   : int  0 0 1 1 0
 $ sex      : Factor w/ 2 levels "1","2": 2 1 2 2 1
 $ age      : Factor w/ 4 levels "1","2","3","4": 1 2 3 3 4
 $ stage    : Factor w/ 2 levels "3","4": 1 2 1 NA 1

I saw Peter Dalgaard's reply and I'm guessing that your 'age' was grouped into some categorical  variable, perhaps decades?

> 
> str(treat)
> 	[[alternative HTML version deleted]]

You should learn to post in plain text and use dput to present your data structures. At your console do this

dput(treat)

# and this will appear. Copy it to your plain-text message:

structure(list(ID = 1:5, start = c(0L, 0L, 0L, 0L, 0L), stop = c(66L, 
18L, 43L, 47L, 26L), censor = c(0L, 0L, 1L, 1L, 0L), sex = c(2L, 
1L, 2L, 2L, 1L), age = c(1L, 2L, 3L, 3L, 4L), stage = c(3L, 4L, 
3L, NA, 3L), treatment = c(1L, 2L, 1L, 2L, NA)), .Names = c("ID", 
"start", "stop", "censor", "sex", "age", "stage", "treatment"
), class = "data.frame", row.names = c("1", "2", "3", "4", "5"
)):



Then we can just do this:

 treat <- structure(list(ID = 1:5, start = c(0L, 0L, 0L, 0L, 0L), stop = c(66L, 
18L, 43L, 47L, 26L), censor = c(0L, 0L, 1L, 1L, 0L), sex = c(2L, 
1L, 2L, 2L, 1L), age = c(1L, 2L, 3L, 3L, 4L), stage = c(3L, 4L, 
3L, NA, 3L), treatment = c(1L, 2L, 1L, 2L, NA)), .Names = c("ID", 
"start", "stop", "censor", "sex", "age", "stage", "treatment"
), class = "data.frame", row.names = c("1", "2", "3", "4", "5"
))

-- 
David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From bogaso.christofer at gmail.com  Sat Oct  7 19:54:12 2017
From: bogaso.christofer at gmail.com (Christofer Bogaso)
Date: Sat, 7 Oct 2017 23:24:12 +0530
Subject: [R] load() failed to load .Rdata file in AWS-Ububtu
Message-ID: <CA+dpOJkVVAZniVewvD8TofE8qTiO1AbRJB=FFsWsCAHfFa9Zbg@mail.gmail.com>

Hi again,

I hope this is the right place to post my question on running R within
Ubuntu, however if not, any pointer on right distribution list will be
helpful.

I am currently using R in Ubuntu which is hosted in Amazon - AWS.

I have a .Rdata file in AWS which I am trying to load in R. Used
following code, however, fails to load showing some permission issue.
However that same .Rdata file is getting loaded perfectly when I try
in my regular iOS.

Below is my code and corresponding result :

ubuntu at ip-172-31-23-148:~$ R


R version 3.4.2 (2017-09-28) -- "Short Summer"

Copyright (C) 2017 The R Foundation for Statistical Computing

Platform: x86_64-pc-linux-gnu (64-bit)


R is free software and comes with ABSOLUTELY NO WARRANTY.

You are welcome to redistribute it under certain conditions.

Type 'license()' or 'licence()' for distribution details.


  Natural language support but running in an English locale


R is a collaborative project with many contributors.

Type 'contributors()' for more information and

'citation()' on how to cite R or R packages in publications.


Type 'demo()' for some demos, 'help()' for on-line help, or

'help.start()' for an HTML browser interface to help.

Type 'q()' to quit R.


During startup - Warning message:

Setting LC_CTYPE failed, using "C"

> file.exists('/srv/shiny-server/Dat.Rdata')

[1] TRUE

> load('/srv/shiny-server/Dat.Rdata')

Error in readChar(con, 5L, useBytes = TRUE) : cannot open the connection

In addition: Warning message:

In readChar(con, 5L, useBytes = TRUE) :

  cannot open compressed file '/srv/shiny-server/Dat.Rdata', probable
reason 'Permission denied'

> readRDS('/srv/shiny-server/Dat.Rdata')

Error in gzfile(file, "rb") : cannot open the connection

In addition: Warning message:

In gzfile(file, "rb") :

  cannot open compressed file '/srv/shiny-server/Dat.Rdata', probable
reason 'Permission denied'


Can someone help me to understand where it went wrong with Ubuntu? I
also tried with changing the extension from .Rdata to .RData, however
observing the same error.

Any pointer will be highly appreciated.

Thanks for your time.


From ericjberger at gmail.com  Sat Oct  7 21:09:39 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Sat, 7 Oct 2017 22:09:39 +0300
Subject: [R] load() failed to load .Rdata file in AWS-Ububtu
In-Reply-To: <CA+dpOJkVVAZniVewvD8TofE8qTiO1AbRJB=FFsWsCAHfFa9Zbg@mail.gmail.com>
References: <CA+dpOJkVVAZniVewvD8TofE8qTiO1AbRJB=FFsWsCAHfFa9Zbg@mail.gmail.com>
Message-ID: <CAGgJW77i4q0ALtXfKOeeXjOS2q-BXLuYj+tav6D7dia6ft6ZSg@mail.gmail.com>

Hi Christofer,
The directory /srv/shiny-server would normally be owned by the root user.
Your options would seem to be to either (1) bring up the R session as root
(dangerous)
or (2) try copying the file to your local directory and read it from there
(if allowed).
e.g. from the Unix shell:
> cd ~   (i.e. cd to your home directory)
> cp /srv/shiny-server/Dat.Rdata .   (Note the '.' at the end. This may not
work - if not then you can try the following
> sudo cp /srv/shiny-server/Dat.Rdata . (if you have sudo privileges - only
do this if the former command did not work)
> chmod 777 Dat.Rdata  (a bit of overkill - again preface by sudo if it
does not work without it)

Then in your R session you can do the load from the file in this location.
R>  load("~/Dat.Rdata")

HTH,
Eric


On Sat, Oct 7, 2017 at 8:54 PM, Christofer Bogaso <
bogaso.christofer at gmail.com> wrote:

> Hi again,
>
> I hope this is the right place to post my question on running R within
> Ubuntu, however if not, any pointer on right distribution list will be
> helpful.
>
> I am currently using R in Ubuntu which is hosted in Amazon - AWS.
>
> I have a .Rdata file in AWS which I am trying to load in R. Used
> following code, however, fails to load showing some permission issue.
> However that same .Rdata file is getting loaded perfectly when I try
> in my regular iOS.
>
> Below is my code and corresponding result :
>
> ubuntu at ip-172-31-23-148:~$ R
>
>
> R version 3.4.2 (2017-09-28) -- "Short Summer"
>
> Copyright (C) 2017 The R Foundation for Statistical Computing
>
> Platform: x86_64-pc-linux-gnu (64-bit)
>
>
> R is free software and comes with ABSOLUTELY NO WARRANTY.
>
> You are welcome to redistribute it under certain conditions.
>
> Type 'license()' or 'licence()' for distribution details.
>
>
>   Natural language support but running in an English locale
>
>
> R is a collaborative project with many contributors.
>
> Type 'contributors()' for more information and
>
> 'citation()' on how to cite R or R packages in publications.
>
>
> Type 'demo()' for some demos, 'help()' for on-line help, or
>
> 'help.start()' for an HTML browser interface to help.
>
> Type 'q()' to quit R.
>
>
> During startup - Warning message:
>
> Setting LC_CTYPE failed, using "C"
>
> > file.exists('/srv/shiny-server/Dat.Rdata')
>
> [1] TRUE
>
> > load('/srv/shiny-server/Dat.Rdata')
>
> Error in readChar(con, 5L, useBytes = TRUE) : cannot open the connection
>
> In addition: Warning message:
>
> In readChar(con, 5L, useBytes = TRUE) :
>
>   cannot open compressed file '/srv/shiny-server/Dat.Rdata', probable
> reason 'Permission denied'
>
> > readRDS('/srv/shiny-server/Dat.Rdata')
>
> Error in gzfile(file, "rb") : cannot open the connection
>
> In addition: Warning message:
>
> In gzfile(file, "rb") :
>
>   cannot open compressed file '/srv/shiny-server/Dat.Rdata', probable
> reason 'Permission denied'
>
>
> Can someone help me to understand where it went wrong with Ubuntu? I
> also tried with changing the extension from .Rdata to .RData, however
> observing the same error.
>
> Any pointer will be highly appreciated.
>
> Thanks for your time.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Sat Oct  7 22:32:36 2017
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sun, 8 Oct 2017 09:32:36 +1300
Subject: [R] [FORGED]  bootstrapping results in table format
In-Reply-To: <CAA7XnHe1iHc-KBP0UpERSDrfWg_98cikDm+Owo+Z8f_Ye8OwSg@mail.gmail.com>
References: <CAA7XnHe1iHc-KBP0UpERSDrfWg_98cikDm+Owo+Z8f_Ye8OwSg@mail.gmail.com>
Message-ID: <545003ab-8dd5-85db-e770-4735fbee43e9@auckland.ac.nz>

On 07/10/17 12:16, Peter Wagey wrote:
> Hi R users,
> I was struggling to put the results into table format. Would you mind to
> show using following data and code how we can put the results into table? I
> further would like to have a  confidence interval for each group.
> 
> 
> set.seed(1000)
> data <- as.data.table(list(x1 = runif(200), x2 = runif(200), group =
> runif(200)>0.5))
> data.frame(data)
> head(data)
> stat <- function(x, i) {x[i, c(m1 = mean(x1))]}
> A<-data[, list(list(boot(.SD, stat, R = 10))), by = group]$V1

(1) Don't post in html.

(2) Don't send code that makes use of packages that you don't mention
(in this case "data.table").

(3) Don't use "data" as the name of a data object.  There is a base 
function called "data" whence you run the risk of getting toadally 
incomprehensible error messages as the result of certain syntax errors.
Also see fortune("dog").

cheers,

Rolf Turner


-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From dwinsemius at comcast.net  Sat Oct  7 22:49:12 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 7 Oct 2017 13:49:12 -0700
Subject: [R] [FORGED]  bootstrapping results in table format
In-Reply-To: <545003ab-8dd5-85db-e770-4735fbee43e9@auckland.ac.nz>
References: <CAA7XnHe1iHc-KBP0UpERSDrfWg_98cikDm+Owo+Z8f_Ye8OwSg@mail.gmail.com>
 <545003ab-8dd5-85db-e770-4735fbee43e9@auckland.ac.nz>
Message-ID: <A8BE55F0-775E-46F0-AA7C-DF7D26D86C13@comcast.net>


> On Oct 7, 2017, at 1:32 PM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> 
> On 07/10/17 12:16, Peter Wagey wrote:
>> Hi R users,
>> I was struggling to put the results into table format. Would you mind to
>> show using following data and code how we can put the results into table? I
>> further would like to have a  confidence interval for each group.
>> set.seed(1000)
>> data <- as.data.table(list(x1 = runif(200), x2 = runif(200), group =
>> runif(200)>0.5))
>> data.frame(data)
>> head(data)
>> stat <- function(x, i) {x[i, c(m1 = mean(x1))]}
>> A<-data[, list(list(boot(.SD, stat, R = 10))), by = group]$V1
> 
> (1) Don't post in html.
> 
> (2) Don't send code that makes use of packages that you don't mention
> (in this case "data.table").

Not to mention the boot package.

> 
> (3) Don't use "data" as the name of a data object.  There is a base function called "data" whence you run the risk of getting toadally incomprehensible error messages as the result of certain syntax errors.
> Also see fortune("dog").

Dear Peter;

Rolf makes good points.

What sort of table do you want? Have you looked at the structure of `A`? 


str(A)  # output omitted in the interest of brevity

# It's an unnamed list of two 'boot' objects (one for each group), each of which is a named list.

> names(A[[1]])
 [1] "t0"        "t"         "R"         "data"      "seed"      "statistic" "sim"      
 [8] "call"      "stype"     "strata"    "weights"  

> A[[1]]$t0
       m1 
0.5158232 


--- 
David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From bogaso.christofer at gmail.com  Sun Oct  8 19:05:37 2017
From: bogaso.christofer at gmail.com (Christofer Bogaso)
Date: Sun, 8 Oct 2017 22:35:37 +0530
Subject: [R] load() failed to load .Rdata file in AWS-Ububtu
In-Reply-To: <CAGgJW77i4q0ALtXfKOeeXjOS2q-BXLuYj+tav6D7dia6ft6ZSg@mail.gmail.com>
References: <CA+dpOJkVVAZniVewvD8TofE8qTiO1AbRJB=FFsWsCAHfFa9Zbg@mail.gmail.com>
 <CAGgJW77i4q0ALtXfKOeeXjOS2q-BXLuYj+tav6D7dia6ft6ZSg@mail.gmail.com>
Message-ID: <CA+dpOJ=a3ZaWqedYFTiDD2FbtbSBkTJtrWJqUX7RoEYm_DpvxA@mail.gmail.com>

Thanks Eric for your pointer. However I just altered the argument of
load() function a little bit to get that loaded. Below is the line
what I tried.

ubuntu at ip-172-31-23-148:~$ R


R version 3.4.2 (2017-09-28) -- "Short Summer"

Copyright (C) 2017 The R Foundation for Statistical Computing

Platform: x86_64-pc-linux-gnu (64-bit)


R is free software and comes with ABSOLUTELY NO WARRANTY.

You are welcome to redistribute it under certain conditions.

Type 'license()' or 'licence()' for distribution details.


  Natural language support but running in an English locale


R is a collaborative project with many contributors.

Type 'contributors()' for more information and

'citation()' on how to cite R or R packages in publications.


Type 'demo()' for some demos, 'help()' for on-line help, or

'help.start()' for an HTML browser interface to help.

Type 'q()' to quit R.


During startup - Warning message:

Setting LC_CTYPE failed, using "C"

> load("/home/ubuntu/Dat.RData")

>

However it still failing when I try that within my shiny app in AWS.

Below are my ui.R and server.R files respectively.

library(shiny)

# Define UI for miles per gallon application
fluidPage(
      fluidRow(
        column(12,
          tableOutput('table')
        )
      )
    )

server.R :

library(shiny)

load("/home/ubuntu/Dat.RData")

shinyServer(function(input, output) {
output$table = renderTable(head(data.frame(1:20, 1:20), 20))
})

with above setup when I deploy my shiny app I get below error :

18.221.184.94:3838 says
The application unexpectedly exited
Diagnostic information has been dumped to the JavaScript error console.

appreciate any help to get rid of this error.

Thanks for your time.

On Sun, Oct 8, 2017 at 12:39 AM, Eric Berger <ericjberger at gmail.com> wrote:
> Hi Christofer,
> The directory /srv/shiny-server would normally be owned by the root user.
> Your options would seem to be to either (1) bring up the R session as root
> (dangerous)
> or (2) try copying the file to your local directory and read it from there
> (if allowed).
> e.g. from the Unix shell:
>> cd ~   (i.e. cd to your home directory)
>> cp /srv/shiny-server/Dat.Rdata .   (Note the '.' at the end. This may not
>> work - if not then you can try the following
>> sudo cp /srv/shiny-server/Dat.Rdata . (if you have sudo privileges - only
>> do this if the former command did not work)
>> chmod 777 Dat.Rdata  (a bit of overkill - again preface by sudo if it does
>> not work without it)
>
> Then in your R session you can do the load from the file in this location.
> R>  load("~/Dat.Rdata")
>
> HTH,
> Eric
>
>
> On Sat, Oct 7, 2017 at 8:54 PM, Christofer Bogaso
> <bogaso.christofer at gmail.com> wrote:
>>
>> Hi again,
>>
>> I hope this is the right place to post my question on running R within
>> Ubuntu, however if not, any pointer on right distribution list will be
>> helpful.
>>
>> I am currently using R in Ubuntu which is hosted in Amazon - AWS.
>>
>> I have a .Rdata file in AWS which I am trying to load in R. Used
>> following code, however, fails to load showing some permission issue.
>> However that same .Rdata file is getting loaded perfectly when I try
>> in my regular iOS.
>>
>> Below is my code and corresponding result :
>>
>> ubuntu at ip-172-31-23-148:~$ R
>>
>>
>> R version 3.4.2 (2017-09-28) -- "Short Summer"
>>
>> Copyright (C) 2017 The R Foundation for Statistical Computing
>>
>> Platform: x86_64-pc-linux-gnu (64-bit)
>>
>>
>> R is free software and comes with ABSOLUTELY NO WARRANTY.
>>
>> You are welcome to redistribute it under certain conditions.
>>
>> Type 'license()' or 'licence()' for distribution details.
>>
>>
>>   Natural language support but running in an English locale
>>
>>
>> R is a collaborative project with many contributors.
>>
>> Type 'contributors()' for more information and
>>
>> 'citation()' on how to cite R or R packages in publications.
>>
>>
>> Type 'demo()' for some demos, 'help()' for on-line help, or
>>
>> 'help.start()' for an HTML browser interface to help.
>>
>> Type 'q()' to quit R.
>>
>>
>> During startup - Warning message:
>>
>> Setting LC_CTYPE failed, using "C"
>>
>> > file.exists('/srv/shiny-server/Dat.Rdata')
>>
>> [1] TRUE
>>
>> > load('/srv/shiny-server/Dat.Rdata')
>>
>> Error in readChar(con, 5L, useBytes = TRUE) : cannot open the connection
>>
>> In addition: Warning message:
>>
>> In readChar(con, 5L, useBytes = TRUE) :
>>
>>   cannot open compressed file '/srv/shiny-server/Dat.Rdata', probable
>> reason 'Permission denied'
>>
>> > readRDS('/srv/shiny-server/Dat.Rdata')
>>
>> Error in gzfile(file, "rb") : cannot open the connection
>>
>> In addition: Warning message:
>>
>> In gzfile(file, "rb") :
>>
>>   cannot open compressed file '/srv/shiny-server/Dat.Rdata', probable
>> reason 'Permission denied'
>>
>>
>> Can someone help me to understand where it went wrong with Ubuntu? I
>> also tried with changing the extension from .Rdata to .RData, however
>> observing the same error.
>>
>> Any pointer will be highly appreciated.
>>
>> Thanks for your time.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>


From arunkumar413 at gmail.com  Sun Oct  8 16:27:11 2017
From: arunkumar413 at gmail.com (Arun Kumar)
Date: Sun, 8 Oct 2017 19:57:11 +0530
Subject: [R] multi variable analysis
Message-ID: <CAN+mmKa36wNVaoXn9HNWyN02yVeoKyumNnnN3SWG1atOnL7XOg@mail.gmail.com>

Hi All,

I have been given three raw data csv files related to an advertising
campaign. Impressions, clicks and conversion. They asked me to suggest
optimisation suggestions using uni variable and multi variable analysis.
The clicks or conversions are spread across various parameters like device,
country, city, ad type, browser, postal code etc. How do I approach the
analysis and modelling in this regard.

Thanks,
Arun

	[[alternative HTML version deleted]]


From bigfloppydog at gmail.com  Sun Oct  8 16:30:30 2017
From: bigfloppydog at gmail.com (Big Floppy Dog)
Date: Sun, 8 Oct 2017 09:30:30 -0500
Subject: [R] how to overlay 2d pdf atop scatter plot using ggplot2
Message-ID: <CAJ_H8e_4+BgOQep5+dfCU5waMRu9Q5HQRnoR6MuG_zkH6XLPKg@mail.gmail.com>

Note: I have posted this on SO also but while the question has been
upvoted, there has been no answer yet.

https://stackoverflow.com/questions/46622243/ggplot-plot-2d-probability-density-function-on-top-of-points-on-ggplot

Apologies for those who have seen it there also but I thought that this
list of experts may have someone who knows the answer.

I have the following example code:



require(mvtnorm)
require(ggplot2)
set.seed(1234)
xx <- data.frame(rmvt(100, df = c(13, 13)))
ggplot(data = xx,  aes(x = X1, y= X2)) + geom_point() + geom_density2d()



It yields a scatterplot of X2 against X1 and a KDE contour plot of the
density (as it should).

My question is: is it possible to change the contour plot to display
the contours

of a two-dimensional density function (say dmvt), using ggplot2?

The remaining figures in my document are in ggplot2 and therefore I
am looking for a ggplot2 solution.

Thanks in advance!

BFD

	[[alternative HTML version deleted]]


From ludovico92 at hotmail.it  Sun Oct  8 16:39:55 2017
From: ludovico92 at hotmail.it (Ludovico Piccolo)
Date: Sun, 8 Oct 2017 14:39:55 +0000
Subject: [R] Manipulations with CO2 dataset on R
Message-ID: <DB5PR02MB1333C046CD992AFA24DC3C84B9770@DB5PR02MB1333.eurprd02.prod.outlook.com>

Hi,



I just started a new course this semester on R, I never used it in my life and i'm stuck on these questions from 3 days, it would be really nice if someone could explain me the answers with the relative commands.

thanks a lot in advance

The following 7 questions are based on the CO2 dataset of R.

1) How many of the plants in CO2 are Mc2 for Plant?

2) How many are either Mc2 or Mn2?

3) How many are Quebec for Type and nonchilled for Treatment?

4) How many have a concentration (conc) of 350 or bigger?

5) How many have a concentration between 350 and 435 (inclusive)?

6) How many have a concentration between 300 and 450 (inclusive) and are nonchilled?

7)How many have an uptake that is less than 1/10 of the concentration (in the units reported)?



	[[alternative HTML version deleted]]


From maitra at email.com  Sun Oct  8 20:32:33 2017
From: maitra at email.com (Ranjan Maitra)
Date: Sun, 8 Oct 2017 13:32:33 -0500
Subject: [R] Manipulations with CO2 dataset on R
In-Reply-To: <DB5PR02MB1333C046CD992AFA24DC3C84B9770@DB5PR02MB1333.eurprd02.prod.outlook.com>
References: <DB5PR02MB1333C046CD992AFA24DC3C84B9770@DB5PR02MB1333.eurprd02.prod.outlook.com>
Message-ID: <20171008133233.0a13123d41d2fccd79b44d06@email.com>

Hello Ludovico,

What is your goal in taking this class? If you are expecting someone else to do your homework for you, you should really consider whether you should waste your time and your  funding source's money to take this class.

In any case, HW questions are off-topic for this list.

So, Ride the Bus, Go to Class, Call your Mom/Dad.

HTH!

On Sun, 8 Oct 2017 14:39:55 +0000 Ludovico Piccolo <ludovico92 at hotmail.it> wrote:

> Hi,
> 
> 
> 
> I just started a new course this semester on R, I never used it in my life and i'm stuck on these questions from 3 days, it would be really nice if someone could explain me the answers with the relative commands.
> 
> thanks a lot in advance
> 
> The following 7 questions are based on the CO2 dataset of R.
> 
> 1) How many of the plants in CO2 are Mc2 for Plant?
> 
> 2) How many are either Mc2 or Mn2?
> 
> 3) How many are Quebec for Type and nonchilled for Treatment?
> 
> 4) How many have a concentration (conc) of 350 or bigger?
> 
> 5) How many have a concentration between 350 and 435 (inclusive)?
> 
> 6) How many have a concentration between 300 and 450 (inclusive) and are nonchilled?
> 
> 7)How many have an uptake that is less than 1/10 of the concentration (in the units reported)?
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Important Notice: This mailbox is ignored: e-mails are set to be deleted on receipt. Please respond to the mailing list if appropriate. For those needing to send personal or professional e-mail, please use appropriate addresses.


From maitra at email.com  Sun Oct  8 20:37:50 2017
From: maitra at email.com (Ranjan Maitra)
Date: Sun, 8 Oct 2017 13:37:50 -0500
Subject: [R] multi variable analysis
In-Reply-To: <CAN+mmKa36wNVaoXn9HNWyN02yVeoKyumNnnN3SWG1atOnL7XOg@mail.gmail.com>
References: <CAN+mmKa36wNVaoXn9HNWyN02yVeoKyumNnnN3SWG1atOnL7XOg@mail.gmail.com>
Message-ID: <20171008133750.ac5bf5be1a1fd6ca0f9e16a8@email.com>

Arun,

I would suggest either getting enrolled in a multivariate statistics class and the pre-requisites for this or going back to your boss and asking him or her to hire a MS-level statistician since this is clearly beyond your current state of knowledge. 

Don't expect R-help to be a free consulting service. And of course, your query has nothing to do with R.

Thanks,
Ranjan

On Sun, 8 Oct 2017 19:57:11 +0530 Arun Kumar <arunkumar413 at gmail.com> wrote:

> Hi All,
> 
> I have been given three raw data csv files related to an advertising
> campaign. Impressions, clicks and conversion. They asked me to suggest
> optimisation suggestions using uni variable and multi variable analysis.
> The clicks or conversions are spread across various parameters like device,
> country, city, ad type, browser, postal code etc. How do I approach the
> analysis and modelling in this regard.
> 
> Thanks,
> Arun
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Important Notice: This mailbox is ignored: e-mails are set to be deleted on receipt. Please respond to the mailing list if appropriate. For those needing to send personal or professional e-mail, please use appropriate addresses.


From maitra at email.com  Sun Oct  8 20:51:43 2017
From: maitra at email.com (Ranjan Maitra)
Date: Sun, 8 Oct 2017 13:51:43 -0500
Subject: [R] how to overlay 2d pdf atop scatter plot using ggplot2
In-Reply-To: <CAJ_H8e_4+BgOQep5+dfCU5waMRu9Q5HQRnoR6MuG_zkH6XLPKg@mail.gmail.com>
References: <CAJ_H8e_4+BgOQep5+dfCU5waMRu9Q5HQRnoR6MuG_zkH6XLPKg@mail.gmail.com>
Message-ID: <20171008135143.83c52101c4cb861b49421fd1@email.com>

Hi,

I am no expert on ggplot2 and I do not know the answer to your question. I looked around a bit but could not find an answer right away. But one possibility could be, if a direct approach is not possible, to draw ellipses corresponding to the confidence regions of the multivariate t density and use geom_polygon to draw this successively? 

I will wait for a couple of days to see if there is a better answer posted and then write some code, unless you get to it first.

Thanks,
Ranjan


On Sun, 8 Oct 2017 09:30:30 -0500 Big Floppy Dog <bigfloppydog at gmail.com> wrote:

> Note: I have posted this on SO also but while the question has been
> upvoted, there has been no answer yet.
> 
> https://stackoverflow.com/questions/46622243/ggplot-plot-2d-probability-density-function-on-top-of-points-on-ggplot
> 
> Apologies for those who have seen it there also but I thought that this
> list of experts may have someone who knows the answer.
> 
> I have the following example code:
> 
> 
> 
> require(mvtnorm)
> require(ggplot2)
> set.seed(1234)
> xx <- data.frame(rmvt(100, df = c(13, 13)))
> ggplot(data = xx,  aes(x = X1, y= X2)) + geom_point() + geom_density2d()
> 
> 
> 
> It yields a scatterplot of X2 against X1 and a KDE contour plot of the
> density (as it should).
> 
> My question is: is it possible to change the contour plot to display
> the contours
> 
> of a two-dimensional density function (say dmvt), using ggplot2?
> 
> The remaining figures in my document are in ggplot2 and therefore I
> am looking for a ggplot2 solution.
> 
> Thanks in advance!
> 
> BFD
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Important Notice: This mailbox is ignored: e-mails are set to be deleted on receipt. Please respond to the mailing list if appropriate. For those needing to send personal or professional e-mail, please use appropriate addresses.


From ulrik.stervbo at gmail.com  Sun Oct  8 21:32:20 2017
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Sun, 8 Oct 2017 19:32:20 +0000
Subject: [R] how to overlay 2d pdf atop scatter plot using ggplot2
In-Reply-To: <20171008135143.83c52101c4cb861b49421fd1@email.com>
References: <CAJ_H8e_4+BgOQep5+dfCU5waMRu9Q5HQRnoR6MuG_zkH6XLPKg@mail.gmail.com>
 <20171008135143.83c52101c4cb861b49421fd1@email.com>
Message-ID: <CAKVAULNHa0pi+Do5cZb1oWCcRGGQXvD6p5udmvgkjic4eZeuSQ@mail.gmail.com>

How about geom_contour()?

Am So., 8. Okt. 2017, 20:52 schrieb Ranjan Maitra <maitra at email.com>:

> Hi,
>
> I am no expert on ggplot2 and I do not know the answer to your question. I
> looked around a bit but could not find an answer right away. But one
> possibility could be, if a direct approach is not possible, to draw
> ellipses corresponding to the confidence regions of the multivariate t
> density and use geom_polygon to draw this successively?
>
> I will wait for a couple of days to see if there is a better answer posted
> and then write some code, unless you get to it first.
>
> Thanks,
> Ranjan
>
>
> On Sun, 8 Oct 2017 09:30:30 -0500 Big Floppy Dog <bigfloppydog at gmail.com>
> wrote:
>
> > Note: I have posted this on SO also but while the question has been
> > upvoted, there has been no answer yet.
> >
> >
> https://stackoverflow.com/questions/46622243/ggplot-plot-2d-probability-density-function-on-top-of-points-on-ggplot
> >
> > Apologies for those who have seen it there also but I thought that this
> > list of experts may have someone who knows the answer.
> >
> > I have the following example code:
> >
> >
> >
> > require(mvtnorm)
> > require(ggplot2)
> > set.seed(1234)
> > xx <- data.frame(rmvt(100, df = c(13, 13)))
> > ggplot(data = xx,  aes(x = X1, y= X2)) + geom_point() + geom_density2d()
> >
> >
> >
> > It yields a scatterplot of X2 against X1 and a KDE contour plot of the
> > density (as it should).
> >
> > My question is: is it possible to change the contour plot to display
> > the contours
> >
> > of a two-dimensional density function (say dmvt), using ggplot2?
> >
> > The remaining figures in my document are in ggplot2 and therefore I
> > am looking for a ggplot2 solution.
> >
> > Thanks in advance!
> >
> > BFD
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> Important Notice: This mailbox is ignored: e-mails are set to be deleted
> on receipt. Please respond to the mailing list if appropriate. For those
> needing to send personal or professional e-mail, please use appropriate
> addresses.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bigfloppydog at gmail.com  Mon Oct  9 06:02:10 2017
From: bigfloppydog at gmail.com (Big Floppy Dog)
Date: Sun, 8 Oct 2017 23:02:10 -0500
Subject: [R] example of geom_contour() with function argument
Message-ID: <CAJ_H8e81OuVNxon89PwAW3v=xJuQCBRJriALVv1ejsGiAjBGrw@mail.gmail.com>

Can someone please point me to an example with geom_contour() that uses a
function? The help does not have an example of a function, and also  I did
not find anything from online searches.

TIA,
BFD


-----------------------------------------------------------------------------------------------

How about geom_contour()?

Am So., 8. Okt. 2017, 20:52 schrieb Ranjan Maitra <maitra at email.com>:

> Hi,
>
> I am no expert on ggplot2 and I do not know the answer to your question. I
> looked around a bit but could not find an answer right away. But one
> possibility could be, if a direct approach is not possible, to draw
> ellipses corresponding to the confidence regions of the multivariate t
> density and use geom_polygon to draw this successively?
>
> I will wait for a couple of days to see if there is a better answer posted
> and then write some code, unless you get to it first.
>
> Thanks,
> Ranjan
>
>
> On Sun, 8 Oct 2017 09:30:30 -0500 Big Floppy Dog <bigfloppydog at gmail.com>
> wrote:
>
> > Note: I have posted this on SO also but while the question has been
> > upvoted, there has been no answer yet.
> >
> >
>
https://stackoverflow.com/questions/46622243/ggplot-plot-2d-probability-density-function-on-top-of-points-on-ggplot
> >
> > Apologies for those who have seen it there also but I thought that this
> > list of experts may have someone who knows the answer.
> >
> > I have the following example code:
> >
> >
> >
> > require(mvtnorm)
> > require(ggplot2)
> > set.seed(1234)
> > xx <- data.frame(rmvt(100, df = c(13, 13)))
> > ggplot(data = xx,  aes(x = X1, y= X2)) + geom_point() + geom_density2d()
> >
> >
> >
> > It yields a scatterplot of X2 against X1 and a KDE contour plot of the
> > density (as it should).
> >
> > My question is: is it possible to change the contour plot to display
> > the contours
> >
> > of a two-dimensional density function (say dmvt), using ggplot2?
> >
> > The remaining figures in my document are in ggplot2 and therefore I
> > am looking for a ggplot2 solution.
> >
> > Thanks in advance!
> >
> > BFD
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Mon Oct  9 09:07:25 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Mon, 9 Oct 2017 10:07:25 +0300
Subject: [R] load() failed to load .Rdata file in AWS-Ububtu
In-Reply-To: <CA+dpOJ=a3ZaWqedYFTiDD2FbtbSBkTJtrWJqUX7RoEYm_DpvxA@mail.gmail.com>
References: <CA+dpOJkVVAZniVewvD8TofE8qTiO1AbRJB=FFsWsCAHfFa9Zbg@mail.gmail.com>
 <CAGgJW77i4q0ALtXfKOeeXjOS2q-BXLuYj+tav6D7dia6ft6ZSg@mail.gmail.com>
 <CA+dpOJ=a3ZaWqedYFTiDD2FbtbSBkTJtrWJqUX7RoEYm_DpvxA@mail.gmail.com>
Message-ID: <CAGgJW77H-YEGDPdg1R5FduXJ2YLRAz7tFZGOGPn4QsWfv-am-Q@mail.gmail.com>

Hi Christofer,
The shiny code you have written does not depend on loading the Dat.RData
file.
I commented out that line and ran your shiny app on my machine and it works
fine.
What happens if you comment out  (or remove) the line
     load("/home/ubuntu/Dat.RData)
Does your shiny app still fail? If so, then your problem is quite basic
with your  AWS setup.
Try to work through a simple tutorial/documentation example that shows how
to get shiny running on AWS and see if following the steps helps you
discover the problem.

Good luck,
Eric




On Sun, Oct 8, 2017 at 8:05 PM, Christofer Bogaso <
bogaso.christofer at gmail.com> wrote:

> Thanks Eric for your pointer. However I just altered the argument of
> load() function a little bit to get that loaded. Below is the line
> what I tried.
>
> ubuntu at ip-172-31-23-148:~$ R
>
>
> R version 3.4.2 (2017-09-28) -- "Short Summer"
>
> Copyright (C) 2017 The R Foundation for Statistical Computing
>
> Platform: x86_64-pc-linux-gnu (64-bit)
>
>
> R is free software and comes with ABSOLUTELY NO WARRANTY.
>
> You are welcome to redistribute it under certain conditions.
>
> Type 'license()' or 'licence()' for distribution details.
>
>
>   Natural language support but running in an English locale
>
>
> R is a collaborative project with many contributors.
>
> Type 'contributors()' for more information and
>
> 'citation()' on how to cite R or R packages in publications.
>
>
> Type 'demo()' for some demos, 'help()' for on-line help, or
>
> 'help.start()' for an HTML browser interface to help.
>
> Type 'q()' to quit R.
>
>
> During startup - Warning message:
>
> Setting LC_CTYPE failed, using "C"
>
> > load("/home/ubuntu/Dat.RData")
>
> >
>
> However it still failing when I try that within my shiny app in AWS.
>
> Below are my ui.R and server.R files respectively.
>
> library(shiny)
>
> # Define UI for miles per gallon application
> fluidPage(
>       fluidRow(
>         column(12,
>           tableOutput('table')
>         )
>       )
>     )
>
> server.R :
>
> library(shiny)
>
> load("/home/ubuntu/Dat.RData")
>
> shinyServer(function(input, output) {
> output$table = renderTable(head(data.frame(1:20, 1:20), 20))
> })
>
> with above setup when I deploy my shiny app I get below error :
>
> 18.221.184.94:3838 says
> The application unexpectedly exited
> Diagnostic information has been dumped to the JavaScript error console.
>
> appreciate any help to get rid of this error.
>
> Thanks for your time.
>
> On Sun, Oct 8, 2017 at 12:39 AM, Eric Berger <ericjberger at gmail.com>
> wrote:
> > Hi Christofer,
> > The directory /srv/shiny-server would normally be owned by the root user.
> > Your options would seem to be to either (1) bring up the R session as
> root
> > (dangerous)
> > or (2) try copying the file to your local directory and read it from
> there
> > (if allowed).
> > e.g. from the Unix shell:
> >> cd ~   (i.e. cd to your home directory)
> >> cp /srv/shiny-server/Dat.Rdata .   (Note the '.' at the end. This may
> not
> >> work - if not then you can try the following
> >> sudo cp /srv/shiny-server/Dat.Rdata . (if you have sudo privileges -
> only
> >> do this if the former command did not work)
> >> chmod 777 Dat.Rdata  (a bit of overkill - again preface by sudo if it
> does
> >> not work without it)
> >
> > Then in your R session you can do the load from the file in this
> location.
> > R>  load("~/Dat.Rdata")
> >
> > HTH,
> > Eric
> >
> >
> > On Sat, Oct 7, 2017 at 8:54 PM, Christofer Bogaso
> > <bogaso.christofer at gmail.com> wrote:
> >>
> >> Hi again,
> >>
> >> I hope this is the right place to post my question on running R within
> >> Ubuntu, however if not, any pointer on right distribution list will be
> >> helpful.
> >>
> >> I am currently using R in Ubuntu which is hosted in Amazon - AWS.
> >>
> >> I have a .Rdata file in AWS which I am trying to load in R. Used
> >> following code, however, fails to load showing some permission issue.
> >> However that same .Rdata file is getting loaded perfectly when I try
> >> in my regular iOS.
> >>
> >> Below is my code and corresponding result :
> >>
> >> ubuntu at ip-172-31-23-148:~$ R
> >>
> >>
> >> R version 3.4.2 (2017-09-28) -- "Short Summer"
> >>
> >> Copyright (C) 2017 The R Foundation for Statistical Computing
> >>
> >> Platform: x86_64-pc-linux-gnu (64-bit)
> >>
> >>
> >> R is free software and comes with ABSOLUTELY NO WARRANTY.
> >>
> >> You are welcome to redistribute it under certain conditions.
> >>
> >> Type 'license()' or 'licence()' for distribution details.
> >>
> >>
> >>   Natural language support but running in an English locale
> >>
> >>
> >> R is a collaborative project with many contributors.
> >>
> >> Type 'contributors()' for more information and
> >>
> >> 'citation()' on how to cite R or R packages in publications.
> >>
> >>
> >> Type 'demo()' for some demos, 'help()' for on-line help, or
> >>
> >> 'help.start()' for an HTML browser interface to help.
> >>
> >> Type 'q()' to quit R.
> >>
> >>
> >> During startup - Warning message:
> >>
> >> Setting LC_CTYPE failed, using "C"
> >>
> >> > file.exists('/srv/shiny-server/Dat.Rdata')
> >>
> >> [1] TRUE
> >>
> >> > load('/srv/shiny-server/Dat.Rdata')
> >>
> >> Error in readChar(con, 5L, useBytes = TRUE) : cannot open the connection
> >>
> >> In addition: Warning message:
> >>
> >> In readChar(con, 5L, useBytes = TRUE) :
> >>
> >>   cannot open compressed file '/srv/shiny-server/Dat.Rdata', probable
> >> reason 'Permission denied'
> >>
> >> > readRDS('/srv/shiny-server/Dat.Rdata')
> >>
> >> Error in gzfile(file, "rb") : cannot open the connection
> >>
> >> In addition: Warning message:
> >>
> >> In gzfile(file, "rb") :
> >>
> >>   cannot open compressed file '/srv/shiny-server/Dat.Rdata', probable
> >> reason 'Permission denied'
> >>
> >>
> >> Can someone help me to understand where it went wrong with Ubuntu? I
> >> also tried with changing the extension from .Rdata to .RData, however
> >> observing the same error.
> >>
> >> Any pointer will be highly appreciated.
> >>
> >> Thanks for your time.
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> >
>

	[[alternative HTML version deleted]]


From ulrik.stervbo at gmail.com  Mon Oct  9 09:22:50 2017
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Mon, 09 Oct 2017 07:22:50 +0000
Subject: [R] example of geom_contour() with function argument
In-Reply-To: <CAJ_H8e81OuVNxon89PwAW3v=xJuQCBRJriALVv1ejsGiAjBGrw@mail.gmail.com>
References: <CAJ_H8e81OuVNxon89PwAW3v=xJuQCBRJriALVv1ejsGiAjBGrw@mail.gmail.com>
Message-ID: <CAKVAULMx0dSUErV0ehaX2aUJuzJos7hsU4i7j5Hj9DynA7C9fw@mail.gmail.com>

Hi BFD,

?geom_contour() *does* have helpful examples. Your Google-foo is weak:
Searching for geom_contour brought me:
http://ggplot2.tidyverse.org/reference/geom_contour.html as the first
result.

HTH
Ulrik

On Mon, 9 Oct 2017 at 08:04 Big Floppy Dog <bigfloppydog at gmail.com> wrote:

> Can someone please point me to an example with geom_contour() that uses a
> function? The help does not have an example of a function, and also  I did
> not find anything from online searches.
>
> TIA,
> BFD
>
>
>
> -----------------------------------------------------------------------------------------------
>
> How about geom_contour()?
>
> Am So., 8. Okt. 2017, 20:52 schrieb Ranjan Maitra <maitra at email.com>:
>
> > Hi,
> >
> > I am no expert on ggplot2 and I do not know the answer to your question.
> I
> > looked around a bit but could not find an answer right away. But one
> > possibility could be, if a direct approach is not possible, to draw
> > ellipses corresponding to the confidence regions of the multivariate t
> > density and use geom_polygon to draw this successively?
> >
> > I will wait for a couple of days to see if there is a better answer
> posted
> > and then write some code, unless you get to it first.
> >
> > Thanks,
> > Ranjan
> >
> >
> > On Sun, 8 Oct 2017 09:30:30 -0500 Big Floppy Dog <bigfloppydog at gmail.com
> >
> > wrote:
> >
> > > Note: I have posted this on SO also but while the question has been
> > > upvoted, there has been no answer yet.
> > >
> > >
> >
>
> https://stackoverflow.com/questions/46622243/ggplot-plot-2d-probability-density-function-on-top-of-points-on-ggplot
> > >
> > > Apologies for those who have seen it there also but I thought that this
> > > list of experts may have someone who knows the answer.
> > >
> > > I have the following example code:
> > >
> > >
> > >
> > > require(mvtnorm)
> > > require(ggplot2)
> > > set.seed(1234)
> > > xx <- data.frame(rmvt(100, df = c(13, 13)))
> > > ggplot(data = xx,  aes(x = X1, y= X2)) + geom_point() +
> geom_density2d()
> > >
> > >
> > >
> > > It yields a scatterplot of X2 against X1 and a KDE contour plot of the
> > > density (as it should).
> > >
> > > My question is: is it possible to change the contour plot to display
> > > the contours
> > >
> > > of a two-dimensional density function (say dmvt), using ggplot2?
> > >
> > > The remaining figures in my document are in ggplot2 and therefore I
> > > am looking for a ggplot2 solution.
> > >
> > > Thanks in advance!
> > >
> > > BFD
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Mon Oct  9 09:55:15 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Mon, 9 Oct 2017 07:55:15 +0000
Subject: [R] Manipulations with CO2 dataset on R
In-Reply-To: <DB5PR02MB1333C046CD992AFA24DC3C84B9770@DB5PR02MB1333.eurprd02.prod.outlook.com>
References: <DB5PR02MB1333C046CD992AFA24DC3C84B9770@DB5PR02MB1333.eurprd02.prod.outlook.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB2360@SRVEXCHCM301.precheza.cz>

Hi

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ludovico
> Piccolo
> Sent: Sunday, October 8, 2017 4:40 PM
> To: r-help at r-project.org
> Subject: [R] Manipulations with CO2 dataset on R
>
> Hi,
> I just started a new course this semester on R, I never used it in my life and i'm
> stuck on these questions from 3 days, it would be really nice if someone could
> explain me the answers with the relative commands.
>
> thanks a lot in advance
>
> The following 7 questions are based on the CO2 dataset of R.
>
> 1) How many of the plants in CO2 are Mc2 for Plant?

?table

>
> 2) How many are either Mc2 or Mn2?

?table or ?aggregate

>
> 3) How many are Quebec for Type and nonchilled for Treatment?

?aggregate

>
> 4) How many have a concentration (conc) of 350 or bigger?

?table and ?">"

>
> 5) How many have a concentration between 350 and 435 (inclusive)?

?table and ?">"

>
> 6) How many have a concentration between 300 and 450 (inclusive) and are
> nonchilled?

?table or ?aggregate and ?">"

>
> 7)How many have an uptake that is less than 1/10 of the concentration (in the
> units reported)?
>

?table or ?aggregate and ?">"

R comes with quite extensive help pages and with R-intro which you should consult first.

Cheers
Petr

> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From r-subscribe at mail.ru  Mon Oct  9 11:52:12 2017
From: r-subscribe at mail.ru (=?UTF-8?B?VGVkIEJlZ2lubmVyIChSU3R1ZGlvKQ==?=)
Date: Mon, 09 Oct 2017 12:52:12 +0300
Subject: [R] =?utf-8?q?Adjusted_survival_curves?=
In-Reply-To: <mailman.0.1507456801.11011.r-help@r-project.org>
References: <mailman.0.1507456801.11011.r-help@r-project.org>
Message-ID: <1507542732.11476634@f480.i.mail.ru>


Adjusted survival curves (Thanks to sample code:  https://rpubs.com/daspringate/survival )
Thanks to Moderator/Admin's Great Work! For a successful solution I used advice that could be understood:
1. Peter Dalgaard: The code does not work, because the covariates are not factors.
2. Jeff Newmiller: "Change the columns into factors before you give them to the coxph function, e.g. df$treatment <- as.factor(df$treatment)"
And I study David Winsemius's instructions.THANKS!!! 
Code works:

library(survival)
library(survminer)
df<-read.csv("F:/R/data/edgr-orig.csv", header = TRUE, sep = ";")

head(df)
# "age" means the age groups
ID start stop censor sex age stage treatment
1   0    66    0     2   1   3     1
2   0    18    0     1   2   4     2
3   0    43    1     2   3   3     1
4   0    47    1     2   3   NA    2
5   0    26    0     1   4   3    NA

# Change continuous var. as factors
df$sex<-as.factor(df$sex)
df$age<-as.factor(df$age)
df$stage<-as.factor(df$stage)
df$treatment<-as.factor(df$treatment)

S <- Surv(
??time = df$start, 
??time2 = df$stop, 
??event = df$censor)

model <- coxph( S ~ treatment + age + sex + stage, data = df)

plot(survfit(model), 
?????las=1,
?????xscale = 1.00,
?????xlab = "Months after diagnosis",
?????ylab = "Proportion survived",
?????main = "Baseline Hazard Curve")

treat <- with(df,
??????????????data.frame(
??????????????treatment = levels(treatment),
??????????????age = rep(levels(age)[1], 2),
??????????????sex = rep(levels(sex)[1], 2),
??????????????stage = rep(levels(stage)[1], 2)))

plot(survfit(model, newdata = treat), 
?????las=1,
?????xscale = 1.00,
?????conf.int = TRUE,
?????xlab = "Months after diagnosis",
?????ylab = "Proportion survived",
?????col = c("red", "green"))

legend(8, 0.9, 
???????legend = c("Beta blockers", 
??????????????????"No beta blockers"), 
???????lty = 1, 
???????col = c("green", "red"))
	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Mon Oct  9 12:21:56 2017
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 9 Oct 2017 21:21:56 +1100
Subject: [R] Help RFM analysis in R (i want a code where i can define my
 own breaks instead of system defined breaks used in auto_RFM package)
In-Reply-To: <CAJL6Qs_LMVaJAtGz_aK+6Qz6miPcmTD41KZA-z=H2h_yYj_e=w@mail.gmail.com>
References: <CAJL6Qs_56mMkT9DBrb5w7V9cop+h-EHpmWWqd3k1MiD7PbpE9g@mail.gmail.com>
 <CA+8X3fXS+U02KxGTG1Y3BeaFR2puHb-S-v60UK8b30KSb1rcrg@mail.gmail.com>
 <CAJL6Qs9xo_+8c7gzkNb-MQGmouXX_qCeiAZ7cFggpK8xhEJSQQ@mail.gmail.com>
 <CA+8X3fV7nt=FmF=sXvAPmLVHu02Yod4T1eRR-XpuNV3UmxdTvA@mail.gmail.com>
 <CAJL6Qs-6w1NYcS04eiaWumybEw=Xko63zQDZ2LV5qOHRiXgDXw@mail.gmail.com>
 <CA+8X3fVQwre+d5AiB1GrU9EEHXHE7+zaj=7wPmEhtRBOXqoU-Q@mail.gmail.com>
 <CAJL6Qs_D86KkFPLPfY1gKBMXeUFp=Am=3FzKQConL-E+O-76aw@mail.gmail.com>
 <CAJL6Qs_LMVaJAtGz_aK+6Qz6miPcmTD41KZA-z=H2h_yYj_e=w@mail.gmail.com>
Message-ID: <CA+8X3fUeAXA90_vaF6oSR19G7eNUO3SNUZEDHrPUWh1CDfDD=A@mail.gmail.com>

Hi Hemant,
Here is an example that might answer your questions. Please don't run
previous code as it might not work.

I define the break values as arguments to the function
(rbreaks,fbreaks,mbreaks) If you want the breaks to work, make sure that
they cover the range of the input values, otherwise you get NAs.

# expects a three (or more) column data frame where
# column 1 is customer ID, column 2 is amount of purchase
# and column 3 is date of purchase
qdrfm<-function(x,rbreaks=3,fbreaks=3,mbreaks=3,date.format="%Y-%m-%d",
 weights=c(1,1,1),finish=NA) {

 # if no finish date is specified, use current date
 if(is.na(finish)) finish<-as.Date(date(), "%a %b %d %H:%M:%S %Y")
 x$rscore<-as.numeric(finish-as.Date(x[,3],date.format))
 x$rscore<-as.numeric(cut(x$rscore,breaks=rbreaks,labels=FALSE))
 custIDs<-unique(x[,1])
 ncust<-length(custIDs)
 rfmout<-data.frame(custID=custIDs,rscore=rep(0,ncust),
  fscore=rep(0,ncust),mscore=rep(0,ncust))
 rfmout$rscore<-cut(by(x$rscore,x[,1],min),breaks=rbreaks,labels=FALSE)
 rfmout$fscore<-cut(table(x[,1]),breaks=fbreaks,labels=FALSE)
 rfmout$mscore<-cut(by(x[,2],x[,1],sum),breaks=mbreaks,labels=FALSE)
 rfmout$cscore<-(weights[1]*rfmout$rscore+
  weights[2]*rfmout$fscore+
  weights[3]*rfmout$mscore)/sum(weights)
 return(rfmout[order(rfmout$cscore),])
}

set.seed(12345)
x2<-data.frame(ID=sample(1:50,250,TRUE),
 purchase=round(runif(250,5,100),2),
 date=paste(rep(2016,250),sample(1:12,250,TRUE),
  sample(1:28,250,TRUE),sep="-"))

# example 1
qdrfm(x2)

# example 2
qdrfm(x2,rbreaks=c(0,200,400),fbreaks=c(0,5,10),mbreaks=c(0,350,700),
 finish=as.Date("2017-01-01"))

Jim

	[[alternative HTML version deleted]]


From plummerm at iarc.fr  Wed Oct  4 17:33:21 2017
From: plummerm at iarc.fr (Martyn Plummer)
Date: Wed, 4 Oct 2017 15:33:21 +0000
Subject: [R] Help wanted - R contributed documentation
Message-ID: <1507131200.4028.21.camel@iarc.fr>

The R Foundation is looking for a volunteer to organize the collection
of contributed documentation for the R project.  The collection is
currently hosted on CRAN at https://cran.r-project.org/other-docs.html
We want to move it off the CRAN web site.

We think that this task would be suitable for someone whose primary
interest is in library and information science. Ideally the collection
would be reorganized as an R-project.org epub archive, based on re-
using available open source software which features the following: 

* export to .bib
* support for multiple revisions
* support for supplementary material (such as code and data).

If you are interested then please reply privately to me. The R
Foundation board will discuss this at the next meeting.  You can, of
course, put together a team of people but we really want a single
person to act as point of reference.

Martyn Plummer
Co-president, The R Foundation






_______________________________________________
R-announce at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From murdoch.duncan at gmail.com  Mon Oct  9 17:02:40 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 9 Oct 2017 11:02:40 -0400
Subject: [R] Regular expression help
Message-ID: <b2da07f4-69c1-7a0d-eb1e-94c760b57a1a@stats.uwo.ca>

I have a file containing "words" like


a

a/b

a/b/c

where there may be multiple words on a line (separated by spaces).? The 
a, b, and c strings can contain non-space, non-slash characters. I'd 
like to use gsub() to extract the c strings (which should be empty if 
there are none).

A real example is

"f 147/1315/587 2820/1320/587 3624/1321/587 1852/1322/587"

which I'd like to transform to

" 587 587 587 587"

Another real example is

"f 1067 28680 24462"

which should transform to "?? ".

I've tried a few different regexprs, but am unable to find a way to say 
"transform words by deleting everything up to and including the 2nd 
slash" when there might be zero, one or two slashes.? Any suggestions?

Duncan Murdoch


From ulrik.stervbo at gmail.com  Mon Oct  9 17:23:44 2017
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Mon, 09 Oct 2017 15:23:44 +0000
Subject: [R] Regular expression help
In-Reply-To: <b2da07f4-69c1-7a0d-eb1e-94c760b57a1a@stats.uwo.ca>
References: <b2da07f4-69c1-7a0d-eb1e-94c760b57a1a@stats.uwo.ca>
Message-ID: <CAKVAULM9+-zWoMu9qb11UYWFSECS1zdUxFjMOxXrzpatw_yARQ@mail.gmail.com>

Hi Duncan,

why not split on / and take the correct elements? It is not as elegant as
regex but could do the trick.

Best,
Ulrik

On Mon, 9 Oct 2017 at 17:03 Duncan Murdoch <murdoch.duncan at gmail.com> wrote:

> I have a file containing "words" like
>
>
> a
>
> a/b
>
> a/b/c
>
> where there may be multiple words on a line (separated by spaces).  The
> a, b, and c strings can contain non-space, non-slash characters. I'd
> like to use gsub() to extract the c strings (which should be empty if
> there are none).
>
> A real example is
>
> "f 147/1315/587 2820/1320/587 3624/1321/587 1852/1322/587"
>
> which I'd like to transform to
>
> " 587 587 587 587"
>
> Another real example is
>
> "f 1067 28680 24462"
>
> which should transform to "   ".
>
> I've tried a few different regexprs, but am unable to find a way to say
> "transform words by deleting everything up to and including the 2nd
> slash" when there might be zero, one or two slashes.  Any suggestions?
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Mon Oct  9 17:34:43 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Mon, 9 Oct 2017 18:34:43 +0300
Subject: [R] Regular expression help
In-Reply-To: <CAKVAULM9+-zWoMu9qb11UYWFSECS1zdUxFjMOxXrzpatw_yARQ@mail.gmail.com>
References: <b2da07f4-69c1-7a0d-eb1e-94c760b57a1a@stats.uwo.ca>
 <CAKVAULM9+-zWoMu9qb11UYWFSECS1zdUxFjMOxXrzpatw_yARQ@mail.gmail.com>
Message-ID: <CAGgJW75HCDQdCZ42tseBnHq6D7HUnXev-CRHd3Vz8aqkbzhfww@mail.gmail.com>

Hi Duncan,
You can try this:

library(readr)
f <- function(s) {
  t <- unlist(readr::tokenize(paste0(gsub(" ",",",s),"\n",collapse="")))
  i <- grep("[a-zA-Z0-9]*/[a-zA-Z0-9]*/",t)
  u <- sub("[a-zA-Z0-9]*/[a-zA-Z0-9]*/","",t[i])
  paste0(u,collapse=" ")
}

f("f 147/1315/587 2820/1320/587 3624/1321/587 1852/1322/587")
# "587 587 587 587"

f("f 1067 28680 24462")
# ""

HTH,
Eric


On Mon, Oct 9, 2017 at 6:23 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
wrote:

> Hi Duncan,
>
> why not split on / and take the correct elements? It is not as elegant as
> regex but could do the trick.
>
> Best,
> Ulrik
>
> On Mon, 9 Oct 2017 at 17:03 Duncan Murdoch <murdoch.duncan at gmail.com>
> wrote:
>
> > I have a file containing "words" like
> >
> >
> > a
> >
> > a/b
> >
> > a/b/c
> >
> > where there may be multiple words on a line (separated by spaces).  The
> > a, b, and c strings can contain non-space, non-slash characters. I'd
> > like to use gsub() to extract the c strings (which should be empty if
> > there are none).
> >
> > A real example is
> >
> > "f 147/1315/587 2820/1320/587 3624/1321/587 1852/1322/587"
> >
> > which I'd like to transform to
> >
> > " 587 587 587 587"
> >
> > Another real example is
> >
> > "f 1067 28680 24462"
> >
> > which should transform to "   ".
> >
> > I've tried a few different regexprs, but am unable to find a way to say
> > "transform words by deleting everything up to and including the 2nd
> > slash" when there might be zero, one or two slashes.  Any suggestions?
> >
> > Duncan Murdoch
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Mon Oct  9 17:45:27 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 9 Oct 2017 17:45:27 +0200
Subject: [R] Regular expression help
In-Reply-To: <b2da07f4-69c1-7a0d-eb1e-94c760b57a1a@stats.uwo.ca>
References: <b2da07f4-69c1-7a0d-eb1e-94c760b57a1a@stats.uwo.ca>
Message-ID: <6491BCAC-8F7A-4838-9F49-0CCBC91ACCBC@gmail.com>


> On 9 Oct 2017, at 17:02 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> I have a file containing "words" like
> 
> 
> a
> 
> a/b
> 
> a/b/c
> 
> where there may be multiple words on a line (separated by spaces).  The a, b, and c strings can contain non-space, non-slash characters. I'd like to use gsub() to extract the c strings (which should be empty if there are none).
> 
> A real example is
> 
> "f 147/1315/587 2820/1320/587 3624/1321/587 1852/1322/587"
> 
> which I'd like to transform to
> 
> " 587 587 587 587"
> 
> Another real example is
> 
> "f 1067 28680 24462"
> 
> which should transform to "   ".
> 
> I've tried a few different regexprs, but am unable to find a way to say "transform words by deleting everything up to and including the 2nd slash" when there might be zero, one or two slashes.  Any suggestions?
> 

I think you might need something like this:

s <- "f 147/1315/587 2820/1320/587 3624/1321/587 1852/1322/587"
l <- strsplit(s, " ")[[1]]
pat <- "[[:alnum:]]*/[[:alnum:]]*/([[:alnum:]]*)"
paste(ifelse(grepl(pat,l),gsub(pat, "\\1", l), ""), collapse=" ")

-pd

> Duncan Murdoch
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From hemantsain55 at gmail.com  Mon Oct  9 12:40:21 2017
From: hemantsain55 at gmail.com (Hemant Sain)
Date: Mon, 9 Oct 2017 16:10:21 +0530
Subject: [R] Help RFM analysis in R (i want a code where i can define my
 own breaks instead of system defined breaks used in auto_RFM package)
In-Reply-To: <CA+8X3fUeAXA90_vaF6oSR19G7eNUO3SNUZEDHrPUWh1CDfDD=A@mail.gmail.com>
References: <CAJL6Qs_56mMkT9DBrb5w7V9cop+h-EHpmWWqd3k1MiD7PbpE9g@mail.gmail.com>
 <CA+8X3fXS+U02KxGTG1Y3BeaFR2puHb-S-v60UK8b30KSb1rcrg@mail.gmail.com>
 <CAJL6Qs9xo_+8c7gzkNb-MQGmouXX_qCeiAZ7cFggpK8xhEJSQQ@mail.gmail.com>
 <CA+8X3fV7nt=FmF=sXvAPmLVHu02Yod4T1eRR-XpuNV3UmxdTvA@mail.gmail.com>
 <CAJL6Qs-6w1NYcS04eiaWumybEw=Xko63zQDZ2LV5qOHRiXgDXw@mail.gmail.com>
 <CA+8X3fVQwre+d5AiB1GrU9EEHXHE7+zaj=7wPmEhtRBOXqoU-Q@mail.gmail.com>
 <CAJL6Qs_D86KkFPLPfY1gKBMXeUFp=Am=3FzKQConL-E+O-76aw@mail.gmail.com>
 <CAJL6Qs_LMVaJAtGz_aK+6Qz6miPcmTD41KZA-z=H2h_yYj_e=w@mail.gmail.com>
 <CA+8X3fUeAXA90_vaF6oSR19G7eNUO3SNUZEDHrPUWh1CDfDD=A@mail.gmail.com>
Message-ID: <CAJL6Qs_LacF9k-jT8n6Du9G+kZEQpxknJRV=muH9g3-NTu5SSA@mail.gmail.com>

I'm getting all the rows as NA in  Cscore and almost most of the
observation in R and F and M are also NA.
what can be the reason for this. also suggest me the appropriate solution.

On 9 October 2017 at 15:51, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Hemant,
> Here is an example that might answer your questions. Please don't run
> previous code as it might not work.
>
> I define the break values as arguments to the function
> (rbreaks,fbreaks,mbreaks) If you want the breaks to work, make sure that
> they cover the range of the input values, otherwise you get NAs.
>
> # expects a three (or more) column data frame where
> # column 1 is customer ID, column 2 is amount of purchase
> # and column 3 is date of purchase
> qdrfm<-function(x,rbreaks=3,fbreaks=3,mbreaks=3,date.format="%Y-%m-%d",
>  weights=c(1,1,1),finish=NA) {
>
>  # if no finish date is specified, use current date
>  if(is.na(finish)) finish<-as.Date(date(), "%a %b %d %H:%M:%S %Y")
>  x$rscore<-as.numeric(finish-as.Date(x[,3],date.format))
>  x$rscore<-as.numeric(cut(x$rscore,breaks=rbreaks,labels=FALSE))
>  custIDs<-unique(x[,1])
>  ncust<-length(custIDs)
>  rfmout<-data.frame(custID=custIDs,rscore=rep(0,ncust),
>   fscore=rep(0,ncust),mscore=rep(0,ncust))
>  rfmout$rscore<-cut(by(x$rscore,x[,1],min),breaks=rbreaks,labels=FALSE)
>  rfmout$fscore<-cut(table(x[,1]),breaks=fbreaks,labels=FALSE)
>  rfmout$mscore<-cut(by(x[,2],x[,1],sum),breaks=mbreaks,labels=FALSE)
>  rfmout$cscore<-(weights[1]*rfmout$rscore+
>   weights[2]*rfmout$fscore+
>   weights[3]*rfmout$mscore)/sum(weights)
>  return(rfmout[order(rfmout$cscore),])
> }
>
> set.seed(12345)
> x2<-data.frame(ID=sample(1:50,250,TRUE),
>  purchase=round(runif(250,5,100),2),
>  date=paste(rep(2016,250),sample(1:12,250,TRUE),
>   sample(1:28,250,TRUE),sep="-"))
>
> # example 1
> qdrfm(x2)
>
> # example 2
> qdrfm(x2,rbreaks=c(0,200,400),fbreaks=c(0,5,10),mbreaks=c(0,350,700),
>  finish=as.Date("2017-01-01"))
>
> Jim
>
>


-- 
hemantsain.com

	[[alternative HTML version deleted]]


From bigfloppydog at gmail.com  Mon Oct  9 15:03:33 2017
From: bigfloppydog at gmail.com (Big Floppy Dog)
Date: Mon, 9 Oct 2017 08:03:33 -0500
Subject: [R] example of geom_contour() with function argument
In-Reply-To: <CAKVAULMx0dSUErV0ehaX2aUJuzJos7hsU4i7j5Hj9DynA7C9fw@mail.gmail.com>
References: <CAJ_H8e81OuVNxon89PwAW3v=xJuQCBRJriALVv1ejsGiAjBGrw@mail.gmail.com>
 <CAKVAULMx0dSUErV0ehaX2aUJuzJos7hsU4i7j5Hj9DynA7C9fw@mail.gmail.com>
Message-ID: <CAJ_H8e917ACQqXh-jea=7ReAQrtS+29maTzdo4vSHr5HNUq2cg@mail.gmail.com>

Hello Ulrik,

I apologize, but I can not see how to provide a pdf in place of the density
function which calculates a KDE (that is, something from the dataset in the
example). Can you please point to the specific example that might help?

Here is what I get:

require(mvtnorm)
require(ggplot2)
set.seed(1234)
xx <- data.frame(rmvt(100, df = c(13, 13)))


v <- ggplot(faithfuld, aes(waiting, eruptions, z = drmvt, df = c(13,13)))
v + geom_contour()

Don't know how to automatically pick scale for object of type function.
Defaulting to continuous.
Error: Aesthetics must be either length 1 or the same as the data (5625):
x, y, z, df


Can you please tell me how to use this here? Or is some other example more
appropriate?

TIA,
BFD



On Mon, Oct 9, 2017 at 2:22 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
wrote:

> Hi BFD,
>
> ?geom_contour() *does* have helpful examples. Your Google-foo is weak:
> Searching for geom_contour brought me: http://ggplot2.tidyverse.
> org/reference/geom_contour.html as the first result.
>
> HTH
> Ulrik
>
> On Mon, 9 Oct 2017 at 08:04 Big Floppy Dog <bigfloppydog at gmail.com> wrote:
>
>> Can someone please point me to an example with geom_contour() that uses a
>> function? The help does not have an example of a function, and also  I did
>> not find anything from online searches.
>>
>> TIA,
>> BFD
>>
>>
>> ------------------------------------------------------------
>> -----------------------------------
>>
>> How about geom_contour()?
>>
>> Am So., 8. Okt. 2017, 20:52 schrieb Ranjan Maitra <maitra at email.com>:
>>
>> > Hi,
>> >
>> > I am no expert on ggplot2 and I do not know the answer to your
>> question. I
>> > looked around a bit but could not find an answer right away. But one
>> > possibility could be, if a direct approach is not possible, to draw
>> > ellipses corresponding to the confidence regions of the multivariate t
>> > density and use geom_polygon to draw this successively?
>> >
>> > I will wait for a couple of days to see if there is a better answer
>> posted
>> > and then write some code, unless you get to it first.
>> >
>> > Thanks,
>> > Ranjan
>> >
>> >
>> > On Sun, 8 Oct 2017 09:30:30 -0500 Big Floppy Dog <
>> bigfloppydog at gmail.com>
>> > wrote:
>> >
>> > > Note: I have posted this on SO also but while the question has been
>> > > upvoted, there has been no answer yet.
>> > >
>> > >
>> >
>> https://stackoverflow.com/questions/46622243/ggplot-
>> plot-2d-probability-density-function-on-top-of-points-on-ggplot
>> > >
>> > > Apologies for those who have seen it there also but I thought that
>> this
>> > > list of experts may have someone who knows the answer.
>> > >
>> > > I have the following example code:
>> > >
>> > >
>> > >
>> > > require(mvtnorm)
>> > > require(ggplot2)
>> > > set.seed(1234)
>> > > xx <- data.frame(rmvt(100, df = c(13, 13)))
>> > > ggplot(data = xx,  aes(x = X1, y= X2)) + geom_point() +
>> geom_density2d()
>> > >
>> > >
>> > >
>> > > It yields a scatterplot of X2 against X1 and a KDE contour plot of the
>> > > density (as it should).
>> > >
>> > > My question is: is it possible to change the contour plot to display
>> > > the contours
>> > >
>> > > of a two-dimensional density function (say dmvt), using ggplot2?
>> > >
>> > > The remaining figures in my document are in ggplot2 and therefore I
>> > > am looking for a ggplot2 solution.
>> > >
>> > > Thanks in advance!
>> > >
>> > > BFD
>> > >
>> > >       [[alternative HTML version deleted]]
>> > >
>> > > ______________________________________________
>> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > > https://stat.ethz.ch/mailman/listinfo/r-help
>> > > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > > and provide commented, minimal, self-contained, reproducible code.
>> > >
>> >
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From eha at posteo.de  Tue Oct  3 13:31:28 2017
From: eha at posteo.de (Eugene Ha)
Date: Tue, 3 Oct 2017 13:31:28 +0200
Subject: [R] [R-pkgs] nofrills: Low-Cost Anonymous Functions
Message-ID: <E8D38D92-D00D-43AA-A2FD-741949501DE8@posteo.de>

Dear R Users,

nofrills (0.2.0) has been published on CRAN:
https://cran.r-project.org/package=nofrills

This lightweight package provides `fn()`, a compact variation of the
usual syntax of function declaration, in order to support
tidyverse-style quasiquotation of a function?s arguments and body.

Aside from enabling a shorter but fully general syntax for function
declarations, the main benefit of `fn()` is its ability to create
_pure_ functions via quasiquotation, which is tricky to guarantee in
base R, due to the mutability of bindings in a function?s lexical scope.

To learn more, see the landing page of the source repository:
https://github.com/egnha/nofrills

Bug reports and other feedback are welcome at the issues page on GitHub:
https://github.com/egnha/nofrills/issues

Eugene Ha
_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages

From wdunlap at tibco.com  Mon Oct  9 17:50:20 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 9 Oct 2017 08:50:20 -0700
Subject: [R] Regular expression help
In-Reply-To: <b2da07f4-69c1-7a0d-eb1e-94c760b57a1a@stats.uwo.ca>
References: <b2da07f4-69c1-7a0d-eb1e-94c760b57a1a@stats.uwo.ca>
Message-ID: <CAF8bMcZSVN575acrxn0Ei7Pki3P7EKC+R=xUH5NvfosgAxowrA@mail.gmail.com>

> x <- "f 147/1315/587 2820/1320/587 3624/1321/587 1852/1322/587"
> gsub("(^| *)([^/ ]*/?){0,2}", "\\1", x)
[1] " 587 587 587 587"
> y <- "aa aa/ aa/bb aa/bb/ aa/bb/cc aa/bb/cc/ aa/bb/cc/dd aa/bb/cc/dd/"
> gsub("(^| *)([^/ ]*/?){0,2}", "\\1", y)
[1] "    cc cc/ cc/dd cc/dd/"


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Oct 9, 2017 at 8:02 AM, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> I have a file containing "words" like
>
>
> a
>
> a/b
>
> a/b/c
>
> where there may be multiple words on a line (separated by spaces).  The a,
> b, and c strings can contain non-space, non-slash characters. I'd like to
> use gsub() to extract the c strings (which should be empty if there are
> none).
>
> A real example is
>
> "f 147/1315/587 2820/1320/587 3624/1321/587 1852/1322/587"
>
> which I'd like to transform to
>
> " 587 587 587 587"
>
> Another real example is
>
> "f 1067 28680 24462"
>
> which should transform to "   ".
>
> I've tried a few different regexprs, but am unable to find a way to say
> "transform words by deleting everything up to and including the 2nd slash"
> when there might be zero, one or two slashes.  Any suggestions?
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Mon Oct  9 18:06:13 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 9 Oct 2017 09:06:13 -0700
Subject: [R] Regular expression help
In-Reply-To: <CAF8bMcZSVN575acrxn0Ei7Pki3P7EKC+R=xUH5NvfosgAxowrA@mail.gmail.com>
References: <b2da07f4-69c1-7a0d-eb1e-94c760b57a1a@stats.uwo.ca>
 <CAF8bMcZSVN575acrxn0Ei7Pki3P7EKC+R=xUH5NvfosgAxowrA@mail.gmail.com>
Message-ID: <CAF8bMcbfom82j95_GJO0ygZiN_UcSnorckeVaK8he_BFJa7=HQ@mail.gmail.com>

"(^| +)([^/ ]*/?){0,2}", with the first "*" replaced by "+" would be a bit
better.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Oct 9, 2017 at 8:50 AM, William Dunlap <wdunlap at tibco.com> wrote:

> > x <- "f 147/1315/587 2820/1320/587 3624/1321/587 1852/1322/587"
> > gsub("(^| *)([^/ ]*/?){0,2}", "\\1", x)
> [1] " 587 587 587 587"
> > y <- "aa aa/ aa/bb aa/bb/ aa/bb/cc aa/bb/cc/ aa/bb/cc/dd aa/bb/cc/dd/"
> > gsub("(^| *)([^/ ]*/?){0,2}", "\\1", y)
> [1] "    cc cc/ cc/dd cc/dd/"
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Mon, Oct 9, 2017 at 8:02 AM, Duncan Murdoch <murdoch.duncan at gmail.com>
> wrote:
>
>> I have a file containing "words" like
>>
>>
>> a
>>
>> a/b
>>
>> a/b/c
>>
>> where there may be multiple words on a line (separated by spaces).  The
>> a, b, and c strings can contain non-space, non-slash characters. I'd like
>> to use gsub() to extract the c strings (which should be empty if there are
>> none).
>>
>> A real example is
>>
>> "f 147/1315/587 2820/1320/587 3624/1321/587 1852/1322/587"
>>
>> which I'd like to transform to
>>
>> " 587 587 587 587"
>>
>> Another real example is
>>
>> "f 1067 28680 24462"
>>
>> which should transform to "   ".
>>
>> I've tried a few different regexprs, but am unable to find a way to say
>> "transform words by deleting everything up to and including the 2nd slash"
>> when there might be zero, one or two slashes.  Any suggestions?
>>
>> Duncan Murdoch
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Mon Oct  9 18:15:07 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 9 Oct 2017 12:15:07 -0400
Subject: [R] Regular expression help
In-Reply-To: <CAKVAULM9+-zWoMu9qb11UYWFSECS1zdUxFjMOxXrzpatw_yARQ@mail.gmail.com>
References: <b2da07f4-69c1-7a0d-eb1e-94c760b57a1a@stats.uwo.ca>
 <CAKVAULM9+-zWoMu9qb11UYWFSECS1zdUxFjMOxXrzpatw_yARQ@mail.gmail.com>
Message-ID: <2c6befd1-9871-dd7c-4079-139c75a23dc8@gmail.com>

On 09/10/2017 11:23 AM, Ulrik Stervbo wrote:
> Hi Duncan,
> 
> why not split on / and take the correct elements? It is not as elegant 
> as regex but could do the trick.

Thanks for the suggestion.  There are likely many thousands of lines of 
data like the two real examples (which had about 5000 and 60000 lines 
respectively), so I was thinking that would be too slow, as it would 
involve nested strsplit() calls.  But in fact, it's not so bad, so I 
might go with it.  Here's a stab at it:

lines <- <the lines to be split, e.g. the lines starting with "f" in 
http://sci.esa.int/science-e/www/object/doc.cfm?fobjectid=54726>

l2 <- strsplit(lines, " ")
l3 <- lapply(l2, function(x) {
         y <- strsplit(x, "/")
         sapply(y, function(z) if (length(z) == 3) z[3] else "")
       })

Duncan

> 
> Best,
> Ulrik
> 
> On Mon, 9 Oct 2017 at 17:03 Duncan Murdoch <murdoch.duncan at gmail.com 
> <mailto:murdoch.duncan at gmail.com>> wrote:
> 
>     I have a file containing "words" like
> 
> 
>     a
> 
>     a/b
> 
>     a/b/c
> 
>     where there may be multiple words on a line (separated by spaces).? The
>     a, b, and c strings can contain non-space, non-slash characters. I'd
>     like to use gsub() to extract the c strings (which should be empty if
>     there are none).
> 
>     A real example is
> 
>     "f 147/1315/587 2820/1320/587 3624/1321/587 1852/1322/587"
> 
>     which I'd like to transform to
> 
>     " 587 587 587 587"
> 
>     Another real example is
> 
>     "f 1067 28680 24462"
> 
>     which should transform to "?? ".
> 
>     I've tried a few different regexprs, but am unable to find a way to say
>     "transform words by deleting everything up to and including the 2nd
>     slash" when there might be zero, one or two slashes.? Any suggestions?
> 
>     Duncan Murdoch
> 
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     and provide commented, minimal, self-contained, reproducible code.
>


From murdoch.duncan at gmail.com  Mon Oct  9 18:23:39 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 9 Oct 2017 12:23:39 -0400
Subject: [R] Regular expression help
In-Reply-To: <CAF8bMcbfom82j95_GJO0ygZiN_UcSnorckeVaK8he_BFJa7=HQ@mail.gmail.com>
References: <b2da07f4-69c1-7a0d-eb1e-94c760b57a1a@stats.uwo.ca>
 <CAF8bMcZSVN575acrxn0Ei7Pki3P7EKC+R=xUH5NvfosgAxowrA@mail.gmail.com>
 <CAF8bMcbfom82j95_GJO0ygZiN_UcSnorckeVaK8he_BFJa7=HQ@mail.gmail.com>
Message-ID: <717aba5f-ecce-42dc-46c3-a22387cd5f2a@gmail.com>

On 09/10/2017 12:06 PM, William Dunlap wrote:
> "(^| +)([^/ ]*/?){0,2}", with the first "*" replaced by "+" would be a 
> bit better.

Thanks!  I think I actually need the *, because theoretically the b part 
of the word could be empty, i.e. "a//c" would be legal and should become 
"c".

Duncan Murdoch

> 
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com <http://tibco.com>
> 
> On Mon, Oct 9, 2017 at 8:50 AM, William Dunlap <wdunlap at tibco.com 
> <mailto:wdunlap at tibco.com>> wrote:
> 
>      > x <- "f 147/1315/587 2820/1320/587 3624/1321/587 1852/1322/587"
>      > gsub("(^| *)([^/ ]*/?){0,2}", "\\1", x)
>     [1] " 587 587 587 587"
>      > y <- "aa aa/ aa/bb aa/bb/ aa/bb/cc aa/bb/cc/ aa/bb/cc/dd
>     aa/bb/cc/dd/"
>      > gsub("(^| *)([^/ ]*/?){0,2}", "\\1", y)
>     [1] "? ? cc cc/ cc/dd cc/dd/"
> 
> 
>     Bill Dunlap
>     TIBCO Software
>     wdunlap tibco.com <http://tibco.com>
> 
>     On Mon, Oct 9, 2017 at 8:02 AM, Duncan Murdoch
>     <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>> wrote:
> 
>         I have a file containing "words" like
> 
> 
>         a
> 
>         a/b
> 
>         a/b/c
> 
>         where there may be multiple words on a line (separated by
>         spaces).? The a, b, and c strings can contain non-space,
>         non-slash characters. I'd like to use gsub() to extract the c
>         strings (which should be empty if there are none).
> 
>         A real example is
> 
>         "f 147/1315/587 2820/1320/587 3624/1321/587 1852/1322/587"
> 
>         which I'd like to transform to
> 
>         " 587 587 587 587"
> 
>         Another real example is
> 
>         "f 1067 28680 24462"
> 
>         which should transform to "?? ".
> 
>         I've tried a few different regexprs, but am unable to find a way
>         to say "transform words by deleting everything up to and
>         including the 2nd slash" when there might be zero, one or two
>         slashes.? Any suggestions?
> 
>         Duncan Murdoch
> 
>         ______________________________________________
>         R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>         -- To UNSUBSCRIBE and more, see
>         https://stat.ethz.ch/mailman/listinfo/r-help
>         <https://stat.ethz.ch/mailman/listinfo/r-help>
>         PLEASE do read the posting guide
>         http://www.R-project.org/posting-guide.html
>         <http://www.R-project.org/posting-guide.html>
>         and provide commented, minimal, self-contained, reproducible code.
> 
> 
>


From dwinsemius at comcast.net  Mon Oct  9 18:32:40 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 9 Oct 2017 09:32:40 -0700
Subject: [R] example of geom_contour() with function argument
In-Reply-To: <CAJ_H8e917ACQqXh-jea=7ReAQrtS+29maTzdo4vSHr5HNUq2cg@mail.gmail.com>
References: <CAJ_H8e81OuVNxon89PwAW3v=xJuQCBRJriALVv1ejsGiAjBGrw@mail.gmail.com>
 <CAKVAULMx0dSUErV0ehaX2aUJuzJos7hsU4i7j5Hj9DynA7C9fw@mail.gmail.com>
 <CAJ_H8e917ACQqXh-jea=7ReAQrtS+29maTzdo4vSHr5HNUq2cg@mail.gmail.com>
Message-ID: <E21F1BA0-ED17-4F6A-8124-4975EB1F089C@comcast.net>


> On Oct 9, 2017, at 6:03 AM, Big Floppy Dog <bigfloppydog at gmail.com> wrote:
> 
> Hello Ulrik,
> 
> I apologize, but I can not see how to provide a pdf in place of the density
> function which calculates a KDE (that is, something from the dataset in the
> example). Can you please point to the specific example that might help?
> 
> Here is what I get:
> 
> require(mvtnorm)
> require(ggplot2)
> set.seed(1234)
> xx <- data.frame(rmvt(100, df = c(13, 13)))
> 
> 
> v <- ggplot(faithfuld, aes(waiting, eruptions, z = drmvt, df = c(13,13)))
> v + geom_contour()
> 
> Don't know how to automatically pick scale for object of type function.
> Defaulting to continuous.
> Error: Aesthetics must be either length 1 or the same as the data (5625):
> x, y, z, df
> 

That's not what I get:

> v <- ggplot(faithfuld, aes(waiting, eruptions, z = drmvt, df = c(13,13)))
> v + geom_contour()
Error in FUN(X[[i]], ...) : object 'drmvt' not found
> 
> ? faithfuld
> str(faithfuld)
Classes ?tbl_df?, ?tbl? and 'data.frame':	5625 obs. of  3 variables:
 $ eruptions: num  1.6 1.65 1.69 1.74 1.79 ...
 $ waiting  : num  43 43 43 43 43 43 43 43 43 43 ...
 $ density  : num  0.00322 0.00384 0.00444 0.00498 0.00542 ...

So you are apparently trying to throw together code and data that you don't understand. The data you are using is already a density estimate designed to simply be plotted. It is not the original data. Furthermore you are passing drmvt that is apparently not in either the mvtnorm nor the ggplot2 packages.

You should determine where that function is and then determine how to do a 2d estimate on the original data. I'm guessing this is homework so not inclined to offer a complete solution.

-- 
David.


> 
> Can you please tell me how to use this here? Or is some other example more
> appropriate?
> 
> TIA,
> BFD
> 
> 
> 
> On Mon, Oct 9, 2017 at 2:22 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
> wrote:
> 
>> Hi BFD,
>> 
>> ?geom_contour() *does* have helpful examples. Your Google-foo is weak:
>> Searching for geom_contour brought me: http://ggplot2.tidyverse.
>> org/reference/geom_contour.html as the first result.
>> 
>> HTH
>> Ulrik
>> 
>> On Mon, 9 Oct 2017 at 08:04 Big Floppy Dog <bigfloppydog at gmail.com> wrote:
>> 
>>> Can someone please point me to an example with geom_contour() that uses a
>>> function? The help does not have an example of a function, and also  I did
>>> not find anything from online searches.
>>> 
>>> TIA,
>>> BFD
>>> 
>>> 
>>> ------------------------------------------------------------
>>> -----------------------------------
>>> 
>>> How about geom_contour()?
>>> 
>>> Am So., 8. Okt. 2017, 20:52 schrieb Ranjan Maitra <maitra at email.com>:
>>> 
>>>> Hi,
>>>> 
>>>> I am no expert on ggplot2 and I do not know the answer to your
>>> question. I
>>>> looked around a bit but could not find an answer right away. But one
>>>> possibility could be, if a direct approach is not possible, to draw
>>>> ellipses corresponding to the confidence regions of the multivariate t
>>>> density and use geom_polygon to draw this successively?
>>>> 
>>>> I will wait for a couple of days to see if there is a better answer
>>> posted
>>>> and then write some code, unless you get to it first.
>>>> 
>>>> Thanks,
>>>> Ranjan
>>>> 
>>>> 
>>>> On Sun, 8 Oct 2017 09:30:30 -0500 Big Floppy Dog <
>>> bigfloppydog at gmail.com>
>>>> wrote:
>>>> 
>>>>> Note: I have posted this on SO also but while the question has been
>>>>> upvoted, there has been no answer yet.
>>>>> 
>>>>> 
>>>> 
>>> https://stackoverflow.com/questions/46622243/ggplot-
>>> plot-2d-probability-density-function-on-top-of-points-on-ggplot
>>>>> 
>>>>> Apologies for those who have seen it there also but I thought that
>>> this
>>>>> list of experts may have someone who knows the answer.
>>>>> 
>>>>> I have the following example code:
>>>>> 
>>>>> 
>>>>> 
>>>>> require(mvtnorm)
>>>>> require(ggplot2)
>>>>> set.seed(1234)
>>>>> xx <- data.frame(rmvt(100, df = c(13, 13)))
>>>>> ggplot(data = xx,  aes(x = X1, y= X2)) + geom_point() +
>>> geom_density2d()
>>>>> 
>>>>> 
>>>>> 
>>>>> It yields a scatterplot of X2 against X1 and a KDE contour plot of the
>>>>> density (as it should).
>>>>> 
>>>>> My question is: is it possible to change the contour plot to display
>>>>> the contours
>>>>> 
>>>>> of a two-dimensional density function (say dmvt), using ggplot2?
>>>>> 
>>>>> The remaining figures in my document are in ggplot2 and therefore I
>>>>> am looking for a ggplot2 solution.
>>>>> 
>>>>> Thanks in advance!
>>>>> 
>>>>> BFD
>>>>> 
>>>>>      [[alternative HTML version deleted]]
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>> 
>>>> 
>>> 
>>>        [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/
>>> posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From bogaso.christofer at gmail.com  Mon Oct  9 19:24:44 2017
From: bogaso.christofer at gmail.com (Christofer Bogaso)
Date: Mon, 9 Oct 2017 22:54:44 +0530
Subject: [R] load() failed to load .Rdata file in AWS-Ububtu
In-Reply-To: <CAGgJW77H-YEGDPdg1R5FduXJ2YLRAz7tFZGOGPn4QsWfv-am-Q@mail.gmail.com>
References: <CA+dpOJkVVAZniVewvD8TofE8qTiO1AbRJB=FFsWsCAHfFa9Zbg@mail.gmail.com>
 <CAGgJW77i4q0ALtXfKOeeXjOS2q-BXLuYj+tav6D7dia6ft6ZSg@mail.gmail.com>
 <CA+dpOJ=a3ZaWqedYFTiDD2FbtbSBkTJtrWJqUX7RoEYm_DpvxA@mail.gmail.com>
 <CAGgJW77H-YEGDPdg1R5FduXJ2YLRAz7tFZGOGPn4QsWfv-am-Q@mail.gmail.com>
Message-ID: <CA+dpOJmpUvWZ08cA9aoMzTBkGhLPU+X6_cEe1u1iWxiKgDq6sQ@mail.gmail.com>

Hi Eric, thanks for your further pointer.

I have put a line with load() function just as an illustration of a
bigger project of mine, which appears failing due to load() function
issue.

If I comment out that line my shiny app is working correctly locally
and globally.

otherwise, locally my shiny app is working but not with AWS.

On Mon, Oct 9, 2017 at 12:37 PM, Eric Berger <ericjberger at gmail.com> wrote:
> Hi Christofer,
> The shiny code you have written does not depend on loading the Dat.RData
> file.
> I commented out that line and ran your shiny app on my machine and it works
> fine.
> What happens if you comment out  (or remove) the line
>      load("/home/ubuntu/Dat.RData)
> Does your shiny app still fail? If so, then your problem is quite basic with
> your  AWS setup.
> Try to work through a simple tutorial/documentation example that shows how
> to get shiny running on AWS and see if following the steps helps you
> discover the problem.
>
> Good luck,
> Eric
>
>
>
>
> On Sun, Oct 8, 2017 at 8:05 PM, Christofer Bogaso
> <bogaso.christofer at gmail.com> wrote:
>>
>> Thanks Eric for your pointer. However I just altered the argument of
>> load() function a little bit to get that loaded. Below is the line
>> what I tried.
>>
>> ubuntu at ip-172-31-23-148:~$ R
>>
>>
>> R version 3.4.2 (2017-09-28) -- "Short Summer"
>>
>> Copyright (C) 2017 The R Foundation for Statistical Computing
>>
>> Platform: x86_64-pc-linux-gnu (64-bit)
>>
>>
>> R is free software and comes with ABSOLUTELY NO WARRANTY.
>>
>> You are welcome to redistribute it under certain conditions.
>>
>> Type 'license()' or 'licence()' for distribution details.
>>
>>
>>   Natural language support but running in an English locale
>>
>>
>> R is a collaborative project with many contributors.
>>
>> Type 'contributors()' for more information and
>>
>> 'citation()' on how to cite R or R packages in publications.
>>
>>
>> Type 'demo()' for some demos, 'help()' for on-line help, or
>>
>> 'help.start()' for an HTML browser interface to help.
>>
>> Type 'q()' to quit R.
>>
>>
>> During startup - Warning message:
>>
>> Setting LC_CTYPE failed, using "C"
>>
>> > load("/home/ubuntu/Dat.RData")
>>
>> >
>>
>> However it still failing when I try that within my shiny app in AWS.
>>
>> Below are my ui.R and server.R files respectively.
>>
>> library(shiny)
>>
>> # Define UI for miles per gallon application
>> fluidPage(
>>       fluidRow(
>>         column(12,
>>           tableOutput('table')
>>         )
>>       )
>>     )
>>
>> server.R :
>>
>> library(shiny)
>>
>> load("/home/ubuntu/Dat.RData")
>>
>> shinyServer(function(input, output) {
>> output$table = renderTable(head(data.frame(1:20, 1:20), 20))
>> })
>>
>> with above setup when I deploy my shiny app I get below error :
>>
>> 18.221.184.94:3838 says
>> The application unexpectedly exited
>> Diagnostic information has been dumped to the JavaScript error console.
>>
>> appreciate any help to get rid of this error.
>>
>> Thanks for your time.
>>
>> On Sun, Oct 8, 2017 at 12:39 AM, Eric Berger <ericjberger at gmail.com>
>> wrote:
>> > Hi Christofer,
>> > The directory /srv/shiny-server would normally be owned by the root
>> > user.
>> > Your options would seem to be to either (1) bring up the R session as
>> > root
>> > (dangerous)
>> > or (2) try copying the file to your local directory and read it from
>> > there
>> > (if allowed).
>> > e.g. from the Unix shell:
>> >> cd ~   (i.e. cd to your home directory)
>> >> cp /srv/shiny-server/Dat.Rdata .   (Note the '.' at the end. This may
>> >> not
>> >> work - if not then you can try the following
>> >> sudo cp /srv/shiny-server/Dat.Rdata . (if you have sudo privileges -
>> >> only
>> >> do this if the former command did not work)
>> >> chmod 777 Dat.Rdata  (a bit of overkill - again preface by sudo if it
>> >> does
>> >> not work without it)
>> >
>> > Then in your R session you can do the load from the file in this
>> > location.
>> > R>  load("~/Dat.Rdata")
>> >
>> > HTH,
>> > Eric
>> >
>> >
>> > On Sat, Oct 7, 2017 at 8:54 PM, Christofer Bogaso
>> > <bogaso.christofer at gmail.com> wrote:
>> >>
>> >> Hi again,
>> >>
>> >> I hope this is the right place to post my question on running R within
>> >> Ubuntu, however if not, any pointer on right distribution list will be
>> >> helpful.
>> >>
>> >> I am currently using R in Ubuntu which is hosted in Amazon - AWS.
>> >>
>> >> I have a .Rdata file in AWS which I am trying to load in R. Used
>> >> following code, however, fails to load showing some permission issue.
>> >> However that same .Rdata file is getting loaded perfectly when I try
>> >> in my regular iOS.
>> >>
>> >> Below is my code and corresponding result :
>> >>
>> >> ubuntu at ip-172-31-23-148:~$ R
>> >>
>> >>
>> >> R version 3.4.2 (2017-09-28) -- "Short Summer"
>> >>
>> >> Copyright (C) 2017 The R Foundation for Statistical Computing
>> >>
>> >> Platform: x86_64-pc-linux-gnu (64-bit)
>> >>
>> >>
>> >> R is free software and comes with ABSOLUTELY NO WARRANTY.
>> >>
>> >> You are welcome to redistribute it under certain conditions.
>> >>
>> >> Type 'license()' or 'licence()' for distribution details.
>> >>
>> >>
>> >>   Natural language support but running in an English locale
>> >>
>> >>
>> >> R is a collaborative project with many contributors.
>> >>
>> >> Type 'contributors()' for more information and
>> >>
>> >> 'citation()' on how to cite R or R packages in publications.
>> >>
>> >>
>> >> Type 'demo()' for some demos, 'help()' for on-line help, or
>> >>
>> >> 'help.start()' for an HTML browser interface to help.
>> >>
>> >> Type 'q()' to quit R.
>> >>
>> >>
>> >> During startup - Warning message:
>> >>
>> >> Setting LC_CTYPE failed, using "C"
>> >>
>> >> > file.exists('/srv/shiny-server/Dat.Rdata')
>> >>
>> >> [1] TRUE
>> >>
>> >> > load('/srv/shiny-server/Dat.Rdata')
>> >>
>> >> Error in readChar(con, 5L, useBytes = TRUE) : cannot open the
>> >> connection
>> >>
>> >> In addition: Warning message:
>> >>
>> >> In readChar(con, 5L, useBytes = TRUE) :
>> >>
>> >>   cannot open compressed file '/srv/shiny-server/Dat.Rdata', probable
>> >> reason 'Permission denied'
>> >>
>> >> > readRDS('/srv/shiny-server/Dat.Rdata')
>> >>
>> >> Error in gzfile(file, "rb") : cannot open the connection
>> >>
>> >> In addition: Warning message:
>> >>
>> >> In gzfile(file, "rb") :
>> >>
>> >>   cannot open compressed file '/srv/shiny-server/Dat.Rdata', probable
>> >> reason 'Permission denied'
>> >>
>> >>
>> >> Can someone help me to understand where it went wrong with Ubuntu? I
>> >> also tried with changing the extension from .Rdata to .RData, however
>> >> observing the same error.
>> >>
>> >> Any pointer will be highly appreciated.
>> >>
>> >> Thanks for your time.
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >
>> >
>
>


From ericjberger at gmail.com  Mon Oct  9 21:29:21 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Mon, 9 Oct 2017 22:29:21 +0300
Subject: [R] load() failed to load .Rdata file in AWS-Ububtu
In-Reply-To: <CA+dpOJmpUvWZ08cA9aoMzTBkGhLPU+X6_cEe1u1iWxiKgDq6sQ@mail.gmail.com>
References: <CA+dpOJkVVAZniVewvD8TofE8qTiO1AbRJB=FFsWsCAHfFa9Zbg@mail.gmail.com>
 <CAGgJW77i4q0ALtXfKOeeXjOS2q-BXLuYj+tav6D7dia6ft6ZSg@mail.gmail.com>
 <CA+dpOJ=a3ZaWqedYFTiDD2FbtbSBkTJtrWJqUX7RoEYm_DpvxA@mail.gmail.com>
 <CAGgJW77H-YEGDPdg1R5FduXJ2YLRAz7tFZGOGPn4QsWfv-am-Q@mail.gmail.com>
 <CA+dpOJmpUvWZ08cA9aoMzTBkGhLPU+X6_cEe1u1iWxiKgDq6sQ@mail.gmail.com>
Message-ID: <CAGgJW74qi2iwVSfexgRxJL1MV4RO=RRaNgZNTdVyTt19uqh8jg@mail.gmail.com>

Hi Christofer,
A few comments.
1. Your experiment seems to show (I hope) that the issue may not be a shiny
issue.
     If that is the case you can try to do things in a simpler setting,
such as a (non-shiny) R session,
     say from the shell. i.e. start an interactive R session and enter the
load command within that session.
2. Perhaps your RData file contains objects that need some libraries to be
loaded? (Not sure why that would cause a problem but on your local system
you can look at the variables resulting from the load command and see if
any of them require some libraries to be loaded.)
3. I found this reference which gives some useful information about RData
files that might also be helpful:
https://www.r-bloggers.com/safe-loading-of-rdata-files-2/

Good luck,
Eric


On Mon, Oct 9, 2017 at 8:24 PM, Christofer Bogaso <
bogaso.christofer at gmail.com> wrote:

> Hi Eric, thanks for your further pointer.
>
> I have put a line with load() function just as an illustration of a
> bigger project of mine, which appears failing due to load() function
> issue.
>
> If I comment out that line my shiny app is working correctly locally
> and globally.
>
> otherwise, locally my shiny app is working but not with AWS.
>
> On Mon, Oct 9, 2017 at 12:37 PM, Eric Berger <ericjberger at gmail.com>
> wrote:
> > Hi Christofer,
> > The shiny code you have written does not depend on loading the Dat.RData
> > file.
> > I commented out that line and ran your shiny app on my machine and it
> works
> > fine.
> > What happens if you comment out  (or remove) the line
> >      load("/home/ubuntu/Dat.RData)
> > Does your shiny app still fail? If so, then your problem is quite basic
> with
> > your  AWS setup.
> > Try to work through a simple tutorial/documentation example that shows
> how
> > to get shiny running on AWS and see if following the steps helps you
> > discover the problem.
> >
> > Good luck,
> > Eric
> >
> >
> >
> >
> > On Sun, Oct 8, 2017 at 8:05 PM, Christofer Bogaso
> > <bogaso.christofer at gmail.com> wrote:
> >>
> >> Thanks Eric for your pointer. However I just altered the argument of
> >> load() function a little bit to get that loaded. Below is the line
> >> what I tried.
> >>
> >> ubuntu at ip-172-31-23-148:~$ R
> >>
> >>
> >> R version 3.4.2 (2017-09-28) -- "Short Summer"
> >>
> >> Copyright (C) 2017 The R Foundation for Statistical Computing
> >>
> >> Platform: x86_64-pc-linux-gnu (64-bit)
> >>
> >>
> >> R is free software and comes with ABSOLUTELY NO WARRANTY.
> >>
> >> You are welcome to redistribute it under certain conditions.
> >>
> >> Type 'license()' or 'licence()' for distribution details.
> >>
> >>
> >>   Natural language support but running in an English locale
> >>
> >>
> >> R is a collaborative project with many contributors.
> >>
> >> Type 'contributors()' for more information and
> >>
> >> 'citation()' on how to cite R or R packages in publications.
> >>
> >>
> >> Type 'demo()' for some demos, 'help()' for on-line help, or
> >>
> >> 'help.start()' for an HTML browser interface to help.
> >>
> >> Type 'q()' to quit R.
> >>
> >>
> >> During startup - Warning message:
> >>
> >> Setting LC_CTYPE failed, using "C"
> >>
> >> > load("/home/ubuntu/Dat.RData")
> >>
> >> >
> >>
> >> However it still failing when I try that within my shiny app in AWS.
> >>
> >> Below are my ui.R and server.R files respectively.
> >>
> >> library(shiny)
> >>
> >> # Define UI for miles per gallon application
> >> fluidPage(
> >>       fluidRow(
> >>         column(12,
> >>           tableOutput('table')
> >>         )
> >>       )
> >>     )
> >>
> >> server.R :
> >>
> >> library(shiny)
> >>
> >> load("/home/ubuntu/Dat.RData")
> >>
> >> shinyServer(function(input, output) {
> >> output$table = renderTable(head(data.frame(1:20, 1:20), 20))
> >> })
> >>
> >> with above setup when I deploy my shiny app I get below error :
> >>
> >> 18.221.184.94:3838 says
> >> The application unexpectedly exited
> >> Diagnostic information has been dumped to the JavaScript error console.
> >>
> >> appreciate any help to get rid of this error.
> >>
> >> Thanks for your time.
> >>
> >> On Sun, Oct 8, 2017 at 12:39 AM, Eric Berger <ericjberger at gmail.com>
> >> wrote:
> >> > Hi Christofer,
> >> > The directory /srv/shiny-server would normally be owned by the root
> >> > user.
> >> > Your options would seem to be to either (1) bring up the R session as
> >> > root
> >> > (dangerous)
> >> > or (2) try copying the file to your local directory and read it from
> >> > there
> >> > (if allowed).
> >> > e.g. from the Unix shell:
> >> >> cd ~   (i.e. cd to your home directory)
> >> >> cp /srv/shiny-server/Dat.Rdata .   (Note the '.' at the end. This may
> >> >> not
> >> >> work - if not then you can try the following
> >> >> sudo cp /srv/shiny-server/Dat.Rdata . (if you have sudo privileges -
> >> >> only
> >> >> do this if the former command did not work)
> >> >> chmod 777 Dat.Rdata  (a bit of overkill - again preface by sudo if it
> >> >> does
> >> >> not work without it)
> >> >
> >> > Then in your R session you can do the load from the file in this
> >> > location.
> >> > R>  load("~/Dat.Rdata")
> >> >
> >> > HTH,
> >> > Eric
> >> >
> >> >
> >> > On Sat, Oct 7, 2017 at 8:54 PM, Christofer Bogaso
> >> > <bogaso.christofer at gmail.com> wrote:
> >> >>
> >> >> Hi again,
> >> >>
> >> >> I hope this is the right place to post my question on running R
> within
> >> >> Ubuntu, however if not, any pointer on right distribution list will
> be
> >> >> helpful.
> >> >>
> >> >> I am currently using R in Ubuntu which is hosted in Amazon - AWS.
> >> >>
> >> >> I have a .Rdata file in AWS which I am trying to load in R. Used
> >> >> following code, however, fails to load showing some permission issue.
> >> >> However that same .Rdata file is getting loaded perfectly when I try
> >> >> in my regular iOS.
> >> >>
> >> >> Below is my code and corresponding result :
> >> >>
> >> >> ubuntu at ip-172-31-23-148:~$ R
> >> >>
> >> >>
> >> >> R version 3.4.2 (2017-09-28) -- "Short Summer"
> >> >>
> >> >> Copyright (C) 2017 The R Foundation for Statistical Computing
> >> >>
> >> >> Platform: x86_64-pc-linux-gnu (64-bit)
> >> >>
> >> >>
> >> >> R is free software and comes with ABSOLUTELY NO WARRANTY.
> >> >>
> >> >> You are welcome to redistribute it under certain conditions.
> >> >>
> >> >> Type 'license()' or 'licence()' for distribution details.
> >> >>
> >> >>
> >> >>   Natural language support but running in an English locale
> >> >>
> >> >>
> >> >> R is a collaborative project with many contributors.
> >> >>
> >> >> Type 'contributors()' for more information and
> >> >>
> >> >> 'citation()' on how to cite R or R packages in publications.
> >> >>
> >> >>
> >> >> Type 'demo()' for some demos, 'help()' for on-line help, or
> >> >>
> >> >> 'help.start()' for an HTML browser interface to help.
> >> >>
> >> >> Type 'q()' to quit R.
> >> >>
> >> >>
> >> >> During startup - Warning message:
> >> >>
> >> >> Setting LC_CTYPE failed, using "C"
> >> >>
> >> >> > file.exists('/srv/shiny-server/Dat.Rdata')
> >> >>
> >> >> [1] TRUE
> >> >>
> >> >> > load('/srv/shiny-server/Dat.Rdata')
> >> >>
> >> >> Error in readChar(con, 5L, useBytes = TRUE) : cannot open the
> >> >> connection
> >> >>
> >> >> In addition: Warning message:
> >> >>
> >> >> In readChar(con, 5L, useBytes = TRUE) :
> >> >>
> >> >>   cannot open compressed file '/srv/shiny-server/Dat.Rdata', probable
> >> >> reason 'Permission denied'
> >> >>
> >> >> > readRDS('/srv/shiny-server/Dat.Rdata')
> >> >>
> >> >> Error in gzfile(file, "rb") : cannot open the connection
> >> >>
> >> >> In addition: Warning message:
> >> >>
> >> >> In gzfile(file, "rb") :
> >> >>
> >> >>   cannot open compressed file '/srv/shiny-server/Dat.Rdata', probable
> >> >> reason 'Permission denied'
> >> >>
> >> >>
> >> >> Can someone help me to understand where it went wrong with Ubuntu? I
> >> >> also tried with changing the extension from .Rdata to .RData, however
> >> >> observing the same error.
> >> >>
> >> >> Any pointer will be highly appreciated.
> >> >>
> >> >> Thanks for your time.
> >> >>
> >> >> ______________________________________________
> >> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> PLEASE do read the posting guide
> >> >> http://www.R-project.org/posting-guide.html
> >> >> and provide commented, minimal, self-contained, reproducible code.
> >> >
> >> >
> >
> >
>

	[[alternative HTML version deleted]]


From bigfloppydog at gmail.com  Mon Oct  9 18:52:09 2017
From: bigfloppydog at gmail.com (Big Floppy Dog)
Date: Mon, 9 Oct 2017 11:52:09 -0500
Subject: [R] example of geom_contour() with function argument
In-Reply-To: <E21F1BA0-ED17-4F6A-8124-4975EB1F089C@comcast.net>
References: <CAJ_H8e81OuVNxon89PwAW3v=xJuQCBRJriALVv1ejsGiAjBGrw@mail.gmail.com>
 <CAKVAULMx0dSUErV0ehaX2aUJuzJos7hsU4i7j5Hj9DynA7C9fw@mail.gmail.com>
 <CAJ_H8e917ACQqXh-jea=7ReAQrtS+29maTzdo4vSHr5HNUq2cg@mail.gmail.com>
 <E21F1BA0-ED17-4F6A-8124-4975EB1F089C@comcast.net>
Message-ID: <CAJ_H8e-SMgX2BQY_eRsHQCocmqJe4sMPX3P-4Kp=9O_Yi+MvEQ@mail.gmail.com>

Hi,

This is not a HW problem, sadly: I was last in a classroom 30 years ago,
and can no longer run off to the instructor :-(

I apologize but I cut and paste the wrong snippet earlier and made a typo
in doing so, but the result is the same with the more appropriate  snippet.

require(mvtnorm)
require(ggplot2)
set.seed(1234)
xx <- data.frame(rmvt(100, df = c(13, 13)))

v <- ggplot(data = xx, aes(x = X1, y = X2, z = dmvt, df = c(13,13)))
v + geom_contour()

Don't know how to automatically pick scale for object of type function.
Defaulting to continuous.
Error: Aesthetics must be either length 1 or the same as the data (100): x,
y, z, df

I do not understand how to put in a function as an argument to
geom_contour() and the examples in the help fileor in the link that Ulrik
sent are not very helpful to me. Hence, I was asking for some examples that
might be helpful.

I guess the answer is to make a second dataset that is regular and make the
function estimate that, but how do I combine this?

TIA.
BFD


On Mon, Oct 9, 2017 at 11:32 AM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Oct 9, 2017, at 6:03 AM, Big Floppy Dog <bigfloppydog at gmail.com>
> wrote:
> >
> > Hello Ulrik,
> >
> > I apologize, but I can not see how to provide a pdf in place of the
> density
> > function which calculates a KDE (that is, something from the dataset in
> the
> > example). Can you please point to the specific example that might help?
> >
> > Here is what I get:
> >
> > require(mvtnorm)
> > require(ggplot2)
> > set.seed(1234)
> > xx <- data.frame(rmvt(100, df = c(13, 13)))
> >
> >
> > v <- ggplot(faithfuld, aes(waiting, eruptions, z = drmvt, df = c(13,13)))
> > v + geom_contour()
> >
> > Don't know how to automatically pick scale for object of type function.
> > Defaulting to continuous.
> > Error: Aesthetics must be either length 1 or the same as the data (5625):
> > x, y, z, df
> >
>
> That's not what I get:
>
> > v <- ggplot(faithfuld, aes(waiting, eruptions, z = drmvt, df = c(13,13)))
> > v + geom_contour()
> Error in FUN(X[[i]], ...) : object 'drmvt' not found
> >
> > ? faithfuld
> > str(faithfuld)
> Classes ?tbl_df?, ?tbl? and 'data.frame':       5625 obs. of  3 variables:
>  $ eruptions: num  1.6 1.65 1.69 1.74 1.79 ...
>  $ waiting  : num  43 43 43 43 43 43 43 43 43 43 ...
>  $ density  : num  0.00322 0.00384 0.00444 0.00498 0.00542 ...
>
> So you are apparently trying to throw together code and data that you
> don't understand. The data you are using is already a density estimate
> designed to simply be plotted. It is not the original data. Furthermore you
> are passing drmvt that is apparently not in either the mvtnorm nor the
> ggplot2 packages.
>
> You should determine where that function is and then determine how to do a
> 2d estimate on the original data. I'm guessing this is homework so not
> inclined to offer a complete solution.
>
> --
> David.
>
>
> >
> > Can you please tell me how to use this here? Or is some other example
> more
> > appropriate?
> >
> > TIA,
> > BFD
> >
> >
> >
> > On Mon, Oct 9, 2017 at 2:22 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
> > wrote:
> >
> >> Hi BFD,
> >>
> >> ?geom_contour() *does* have helpful examples. Your Google-foo is weak:
> >> Searching for geom_contour brought me: http://ggplot2.tidyverse.
> >> org/reference/geom_contour.html as the first result.
> >>
> >> HTH
> >> Ulrik
> >>
> >> On Mon, 9 Oct 2017 at 08:04 Big Floppy Dog <bigfloppydog at gmail.com>
> wrote:
> >>
> >>> Can someone please point me to an example with geom_contour() that
> uses a
> >>> function? The help does not have an example of a function, and also  I
> did
> >>> not find anything from online searches.
> >>>
> >>> TIA,
> >>> BFD
> >>>
> >>>
> >>> ------------------------------------------------------------
> >>> -----------------------------------
> >>>
> >>> How about geom_contour()?
> >>>
> >>> Am So., 8. Okt. 2017, 20:52 schrieb Ranjan Maitra <maitra at email.com>:
> >>>
> >>>> Hi,
> >>>>
> >>>> I am no expert on ggplot2 and I do not know the answer to your
> >>> question. I
> >>>> looked around a bit but could not find an answer right away. But one
> >>>> possibility could be, if a direct approach is not possible, to draw
> >>>> ellipses corresponding to the confidence regions of the multivariate t
> >>>> density and use geom_polygon to draw this successively?
> >>>>
> >>>> I will wait for a couple of days to see if there is a better answer
> >>> posted
> >>>> and then write some code, unless you get to it first.
> >>>>
> >>>> Thanks,
> >>>> Ranjan
> >>>>
> >>>>
> >>>> On Sun, 8 Oct 2017 09:30:30 -0500 Big Floppy Dog <
> >>> bigfloppydog at gmail.com>
> >>>> wrote:
> >>>>
> >>>>> Note: I have posted this on SO also but while the question has been
> >>>>> upvoted, there has been no answer yet.
> >>>>>
> >>>>>
> >>>>
> >>> https://stackoverflow.com/questions/46622243/ggplot-
> >>> plot-2d-probability-density-function-on-top-of-points-on-ggplot
> >>>>>
> >>>>> Apologies for those who have seen it there also but I thought that
> >>> this
> >>>>> list of experts may have someone who knows the answer.
> >>>>>
> >>>>> I have the following example code:
> >>>>>
> >>>>>
> >>>>>
> >>>>> require(mvtnorm)
> >>>>> require(ggplot2)
> >>>>> set.seed(1234)
> >>>>> xx <- data.frame(rmvt(100, df = c(13, 13)))
> >>>>> ggplot(data = xx,  aes(x = X1, y= X2)) + geom_point() +
> >>> geom_density2d()
> >>>>>
> >>>>>
> >>>>>
> >>>>> It yields a scatterplot of X2 against X1 and a KDE contour plot of
> the
> >>>>> density (as it should).
> >>>>>
> >>>>> My question is: is it possible to change the contour plot to display
> >>>>> the contours
> >>>>>
> >>>>> of a two-dimensional density function (say dmvt), using ggplot2?
> >>>>>
> >>>>> The remaining figures in my document are in ggplot2 and therefore I
> >>>>> am looking for a ggplot2 solution.
> >>>>>
> >>>>> Thanks in advance!
> >>>>>
> >>>>> BFD
> >>>>>
> >>>>>      [[alternative HTML version deleted]]
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> >>>> http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>
> >>>>
> >>>>
> >>>
> >>>        [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/
> >>> posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> 'Any technology distinguishable from magic is insufficiently advanced.'
>  -Gehm's Corollary to Clarke's Third Law
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From r-subscribe at mail.ru  Mon Oct  9 21:54:08 2017
From: r-subscribe at mail.ru (=?UTF-8?B?VGVkIEJlZ2lubmVyIChSU3R1ZGlvKQ==?=)
Date: Mon, 09 Oct 2017 22:54:08 +0300
Subject: [R] =?utf-8?q?Adjusted_survival_curves?=
In-Reply-To: <mailman.0.1507543201.21741.r-help@r-project.org>
References: <mailman.0.1507543201.21741.r-help@r-project.org>
Message-ID: <1507578848.451740191@f470.i.mail.ru>


Adjusted survival curves. (Sample code here:  https://rpubs.com/daspringate/survival )
Deep gratitude?to Moderator/Admin!
At?David Winsemius prompt, more elegant working code:Thanks, Ted :)
library(survival)
library(survminer)
df<-read.csv("F:/R/data/edgr-orig.csv", header = TRUE, sep = ";")

df2 <- df
df2[,c('treatment', 'age', 'sex', 'stage')] <- lapply(df2[,c('treatment', 'age', 'sex', 'stage')], factor)

model <- coxph (Surv(time = start, 
???????????????????????????????????time2 = stop, 
???????????????????????????????????event = censor)~ treatment + age + sex + stage, data = df2)
treat <- with(df2,
??????????????data.frame(
??????????????treatment = levels(treatment),
??????????????age = rep(levels(age)[1], 2),
??????????????sex = rep(levels(sex)[1], 2),
??????????????stage = rep(levels(stage)[1], 2)))

plot(survfit(model, newdata = treat), 
?????las=1,
?????xscale = 1.00,
?????conf.int = TRUE,
?????xlab = "Months after diagnosis",
?????ylab = "Proportion survived",
?????col = c("red", "green"))
	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Mon Oct  9 22:57:20 2017
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 10 Oct 2017 07:57:20 +1100
Subject: [R] Help RFM analysis in R (i want a code where i can define my
 own breaks instead of system defined breaks used in auto_RFM package)
In-Reply-To: <CAJL6Qs_LacF9k-jT8n6Du9G+kZEQpxknJRV=muH9g3-NTu5SSA@mail.gmail.com>
References: <CAJL6Qs_56mMkT9DBrb5w7V9cop+h-EHpmWWqd3k1MiD7PbpE9g@mail.gmail.com>
 <CA+8X3fXS+U02KxGTG1Y3BeaFR2puHb-S-v60UK8b30KSb1rcrg@mail.gmail.com>
 <CAJL6Qs9xo_+8c7gzkNb-MQGmouXX_qCeiAZ7cFggpK8xhEJSQQ@mail.gmail.com>
 <CA+8X3fV7nt=FmF=sXvAPmLVHu02Yod4T1eRR-XpuNV3UmxdTvA@mail.gmail.com>
 <CAJL6Qs-6w1NYcS04eiaWumybEw=Xko63zQDZ2LV5qOHRiXgDXw@mail.gmail.com>
 <CA+8X3fVQwre+d5AiB1GrU9EEHXHE7+zaj=7wPmEhtRBOXqoU-Q@mail.gmail.com>
 <CAJL6Qs_D86KkFPLPfY1gKBMXeUFp=Am=3FzKQConL-E+O-76aw@mail.gmail.com>
 <CAJL6Qs_LMVaJAtGz_aK+6Qz6miPcmTD41KZA-z=H2h_yYj_e=w@mail.gmail.com>
 <CA+8X3fUeAXA90_vaF6oSR19G7eNUO3SNUZEDHrPUWh1CDfDD=A@mail.gmail.com>
 <CAJL6Qs_LacF9k-jT8n6Du9G+kZEQpxknJRV=muH9g3-NTu5SSA@mail.gmail.com>
Message-ID: <CA+8X3fUyQJChXRvryO7Rc3kL1Oz-ynQf3EJfDzCbo6LYahs9yg@mail.gmail.com>

I seriously doubt that you are running the code I sent. What you have
probably done is to run your data, which has a different date format,
without changing the breaks or the date format arguments. As you
haven't provided any example that shows what you are doing, I can't
guess what the problem is.

Jim


On Mon, Oct 9, 2017 at 9:40 PM, Hemant Sain <hemantsain55 at gmail.com> wrote:
> I'm getting all the rows as NA in  Cscore and almost most of the observation
> in R and F and M are also NA.
> what can be the reason for this. also suggest me the appropriate solution.
>
> On 9 October 2017 at 15:51, Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>> Hi Hemant,
>> Here is an example that might answer your questions. Please don't run
>> previous code as it might not work.
>>
>> I define the break values as arguments to the function
>> (rbreaks,fbreaks,mbreaks) If you want the breaks to work, make sure that
>> they cover the range of the input values, otherwise you get NAs.
>>
>> # expects a three (or more) column data frame where
>> # column 1 is customer ID, column 2 is amount of purchase
>> # and column 3 is date of purchase
>> qdrfm<-function(x,rbreaks=3,fbreaks=3,mbreaks=3,date.format="%Y-%m-%d",
>>  weights=c(1,1,1),finish=NA) {
>>
>>  # if no finish date is specified, use current date
>>  if(is.na(finish)) finish<-as.Date(date(), "%a %b %d %H:%M:%S %Y")
>>  x$rscore<-as.numeric(finish-as.Date(x[,3],date.format))
>>  x$rscore<-as.numeric(cut(x$rscore,breaks=rbreaks,labels=FALSE))
>>  custIDs<-unique(x[,1])
>>  ncust<-length(custIDs)
>>  rfmout<-data.frame(custID=custIDs,rscore=rep(0,ncust),
>>   fscore=rep(0,ncust),mscore=rep(0,ncust))
>>  rfmout$rscore<-cut(by(x$rscore,x[,1],min),breaks=rbreaks,labels=FALSE)
>>  rfmout$fscore<-cut(table(x[,1]),breaks=fbreaks,labels=FALSE)
>>  rfmout$mscore<-cut(by(x[,2],x[,1],sum),breaks=mbreaks,labels=FALSE)
>>  rfmout$cscore<-(weights[1]*rfmout$rscore+
>>   weights[2]*rfmout$fscore+
>>   weights[3]*rfmout$mscore)/sum(weights)
>>  return(rfmout[order(rfmout$cscore),])
>> }
>>
>> set.seed(12345)
>> x2<-data.frame(ID=sample(1:50,250,TRUE),
>>  purchase=round(runif(250,5,100),2),
>>  date=paste(rep(2016,250),sample(1:12,250,TRUE),
>>   sample(1:28,250,TRUE),sep="-"))
>>
>> # example 1
>> qdrfm(x2)
>>
>> # example 2
>> qdrfm(x2,rbreaks=c(0,200,400),fbreaks=c(0,5,10),mbreaks=c(0,350,700),
>>  finish=as.Date("2017-01-01"))
>>
>> Jim
>>
>
>
>
> --
> hemantsain.com


From jdnewmil at dcn.davis.ca.us  Mon Oct  9 23:01:25 2017
From: jdnewmil at dcn.davis.ca.us (jdnewmil)
Date: Mon, 09 Oct 2017 14:01:25 -0700
Subject: [R] example of geom_contour() with function argument
In-Reply-To: <CAJ_H8e-SMgX2BQY_eRsHQCocmqJe4sMPX3P-4Kp=9O_Yi+MvEQ@mail.gmail.com>
References: <CAJ_H8e81OuVNxon89PwAW3v=xJuQCBRJriALVv1ejsGiAjBGrw@mail.gmail.com>
 <CAKVAULMx0dSUErV0ehaX2aUJuzJos7hsU4i7j5Hj9DynA7C9fw@mail.gmail.com>
 <CAJ_H8e917ACQqXh-jea=7ReAQrtS+29maTzdo4vSHr5HNUq2cg@mail.gmail.com>
 <E21F1BA0-ED17-4F6A-8124-4975EB1F089C@comcast.net>
 <CAJ_H8e-SMgX2BQY_eRsHQCocmqJe4sMPX3P-4Kp=9O_Yi+MvEQ@mail.gmail.com>
Message-ID: <1c438b8fc056bf3caa1b18f9f822de71@dcn.davis.ca.us>

library(mvtnorm) # you were misusing "require"... only use require if 
you plan to
library(ggplot2) # test the return value and fail gracefully when the 
package is missing
set.seed( 1234 )
xx <- data.frame( rmvt( 100, df = c( 13, 13 ) ) )
xx2 <- expand.grid( X1 = seq( -5, 5, 0.1 ) # all combinations... could 
be used to fill a matrix
                   , X2 = seq( -5, 5, 0.1 )
                   )
# compute density as a function of the grid of points
xx2$d <- dmvt( as.matrix( xx2[,1:2] ) ) # feels weird not specifying 
measures of centrality or spread
ggplot( data = xx
       ,  aes( x = X1
             , y = X2
             )
       ) +
     geom_point() + # might want this line after the geom_contour
     geom_contour( data = xx2 # may want to consider geom_tile as well
                 , mapping = aes( x = X1
                                , y = X2
                                , z = d
                                )
                 )
#' ![](https://i.imgur.com/8ExFYtI.png)
## generated/tested with the reprex package to double check that it is 
reproducible

On 2017-10-09 09:52, Big Floppy Dog wrote:
> Hi,
> 
> This is not a HW problem, sadly: I was last in a classroom 30 years 
> ago,
> and can no longer run off to the instructor :-(
> 
> I apologize but I cut and paste the wrong snippet earlier and made a 
> typo
> in doing so, but the result is the same with the more appropriate  
> snippet.
> 
> require(mvtnorm)
> require(ggplot2)
> set.seed(1234)
> xx <- data.frame(rmvt(100, df = c(13, 13)))
> 
> v <- ggplot(data = xx, aes(x = X1, y = X2, z = dmvt, df = c(13,13)))
> v + geom_contour()
> 
> Don't know how to automatically pick scale for object of type function.
> Defaulting to continuous.
> Error: Aesthetics must be either length 1 or the same as the data 
> (100): x,
> y, z, df
> 
> I do not understand how to put in a function as an argument to
> geom_contour() and the examples in the help fileor in the link that 
> Ulrik
> sent are not very helpful to me. Hence, I was asking for some examples 
> that
> might be helpful.
> 
> I guess the answer is to make a second dataset that is regular and make 
> the
> function estimate that, but how do I combine this?
> 
> TIA.
> BFD
> 
> 
> On Mon, Oct 9, 2017 at 11:32 AM, David Winsemius 
> <dwinsemius at comcast.net>
> wrote:
> 
>> 
>> > On Oct 9, 2017, at 6:03 AM, Big Floppy Dog <bigfloppydog at gmail.com>
>> wrote:
>> >
>> > Hello Ulrik,
>> >
>> > I apologize, but I can not see how to provide a pdf in place of the
>> density
>> > function which calculates a KDE (that is, something from the dataset in
>> the
>> > example). Can you please point to the specific example that might help?
>> >
>> > Here is what I get:
>> >
>> > require(mvtnorm)
>> > require(ggplot2)
>> > set.seed(1234)
>> > xx <- data.frame(rmvt(100, df = c(13, 13)))
>> >
>> >
>> > v <- ggplot(faithfuld, aes(waiting, eruptions, z = drmvt, df = c(13,13)))
>> > v + geom_contour()
>> >
>> > Don't know how to automatically pick scale for object of type function.
>> > Defaulting to continuous.
>> > Error: Aesthetics must be either length 1 or the same as the data (5625):
>> > x, y, z, df
>> >
>> 
>> That's not what I get:
>> 
>> > v <- ggplot(faithfuld, aes(waiting, eruptions, z = drmvt, df = c(13,13)))
>> > v + geom_contour()
>> Error in FUN(X[[i]], ...) : object 'drmvt' not found
>> >
>> > ? faithfuld
>> > str(faithfuld)
>> Classes ?tbl_df?, ?tbl? and 'data.frame':       5625 obs. of  3 
>> variables:
>>  $ eruptions: num  1.6 1.65 1.69 1.74 1.79 ...
>>  $ waiting  : num  43 43 43 43 43 43 43 43 43 43 ...
>>  $ density  : num  0.00322 0.00384 0.00444 0.00498 0.00542 ...
>> 
>> So you are apparently trying to throw together code and data that you
>> don't understand. The data you are using is already a density estimate
>> designed to simply be plotted. It is not the original data. 
>> Furthermore you
>> are passing drmvt that is apparently not in either the mvtnorm nor the
>> ggplot2 packages.
>> 
>> You should determine where that function is and then determine how to 
>> do a
>> 2d estimate on the original data. I'm guessing this is homework so not
>> inclined to offer a complete solution.
>> 
>> --
>> David.
>> 
>> 
>> >
>> > Can you please tell me how to use this here? Or is some other example
>> more
>> > appropriate?
>> >
>> > TIA,
>> > BFD
>> >
>> >
>> >
>> > On Mon, Oct 9, 2017 at 2:22 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
>> > wrote:
>> >
>> >> Hi BFD,
>> >>
>> >> ?geom_contour() *does* have helpful examples. Your Google-foo is weak:
>> >> Searching for geom_contour brought me: http://ggplot2.tidyverse.
>> >> org/reference/geom_contour.html as the first result.
>> >>
>> >> HTH
>> >> Ulrik
>> >>
>> >> On Mon, 9 Oct 2017 at 08:04 Big Floppy Dog <bigfloppydog at gmail.com>
>> wrote:
>> >>
>> >>> Can someone please point me to an example with geom_contour() that
>> uses a
>> >>> function? The help does not have an example of a function, and also  I
>> did
>> >>> not find anything from online searches.
>> >>>
>> >>> TIA,
>> >>> BFD
>> >>>
>> >>>
>> >>> ------------------------------------------------------------
>> >>> -----------------------------------
>> >>>
>> >>> How about geom_contour()?
>> >>>
>> >>> Am So., 8. Okt. 2017, 20:52 schrieb Ranjan Maitra <maitra at email.com>:
>> >>>
>> >>>> Hi,
>> >>>>
>> >>>> I am no expert on ggplot2 and I do not know the answer to your
>> >>> question. I
>> >>>> looked around a bit but could not find an answer right away. But one
>> >>>> possibility could be, if a direct approach is not possible, to draw
>> >>>> ellipses corresponding to the confidence regions of the multivariate t
>> >>>> density and use geom_polygon to draw this successively?
>> >>>>
>> >>>> I will wait for a couple of days to see if there is a better answer
>> >>> posted
>> >>>> and then write some code, unless you get to it first.
>> >>>>
>> >>>> Thanks,
>> >>>> Ranjan
>> >>>>
>> >>>>
>> >>>> On Sun, 8 Oct 2017 09:30:30 -0500 Big Floppy Dog <
>> >>> bigfloppydog at gmail.com>
>> >>>> wrote:
>> >>>>
>> >>>>> Note: I have posted this on SO also but while the question has been
>> >>>>> upvoted, there has been no answer yet.
>> >>>>>
>> >>>>>
>> >>>>
>> >>> https://stackoverflow.com/questions/46622243/ggplot-
>> >>> plot-2d-probability-density-function-on-top-of-points-on-ggplot
>> >>>>>
>> >>>>> Apologies for those who have seen it there also but I thought that
>> >>> this
>> >>>>> list of experts may have someone who knows the answer.
>> >>>>>
>> >>>>> I have the following example code:
>> >>>>>
>> >>>>>
>> >>>>>
>> >>>>> require(mvtnorm)
>> >>>>> require(ggplot2)
>> >>>>> set.seed(1234)
>> >>>>> xx <- data.frame(rmvt(100, df = c(13, 13)))
>> >>>>> ggplot(data = xx,  aes(x = X1, y= X2)) + geom_point() +
>> >>> geom_density2d()
>> >>>>>
>> >>>>>
>> >>>>>
>> >>>>> It yields a scatterplot of X2 against X1 and a KDE contour plot of
>> the
>> >>>>> density (as it should).
>> >>>>>
>> >>>>> My question is: is it possible to change the contour plot to display
>> >>>>> the contours
>> >>>>>
>> >>>>> of a two-dimensional density function (say dmvt), using ggplot2?
>> >>>>>
>> >>>>> The remaining figures in my document are in ggplot2 and therefore I
>> >>>>> am looking for a ggplot2 solution.
>> >>>>>
>> >>>>> Thanks in advance!
>> >>>>>
>> >>>>> BFD
>> >>>>>
>> >>>>>      [[alternative HTML version deleted]]
>> >>>>>
>> >>>>> ______________________________________________
>> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>> PLEASE do read the posting guide
>> >>>> http://www.R-project.org/posting-guide.html
>> >>>>> and provide commented, minimal, self-contained, reproducible code.
>> >>>>>
>> >>>>
>> >>>>
>> >>>
>> >>>        [[alternative HTML version deleted]]
>> >>>
>> >>> ______________________________________________
>> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>> PLEASE do read the posting guide http://www.R-project.org/
>> >>> posting-guide.html
>> >>> and provide commented, minimal, self-contained, reproducible code.
>> >>>
>> >>
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> 
>> David Winsemius
>> Alameda, CA, USA
>> 
>> 'Any technology distinguishable from magic is insufficiently 
>> advanced.'
>>  -Gehm's Corollary to Clarke's Third Law
>> 
>> 
>> 
>> 
>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From georges at yorku.ca  Tue Oct 10 03:08:20 2017
From: georges at yorku.ca (Georges Monette)
Date: Mon, 9 Oct 2017 21:08:20 -0400
Subject: [R] Regular expression help
In-Reply-To: <b2da07f4-69c1-7a0d-eb1e-94c760b57a1a@stats.uwo.ca>
References: <b2da07f4-69c1-7a0d-eb1e-94c760b57a1a@stats.uwo.ca>
Message-ID: <6dc4ae94-3845-d908-e58a-036127dd21ac@yorku.ca>

How about this (I'm showing it as a pipe because it's easier to read 
that way):

library(magrittr)
"f 147/1315/587 2820/1320/587 3624/1321/587 1852/1322/587" %>%
 ? strsplit(' ') %>%
 ? unlist %>%
 ? sub('^[^/]*/*','',.) %>%
 ? sub('^[^/]*/*','',.) %>%
 ? paste(collapse = ' ')

Georges Monette

-- 
Georges Monette, PhD P.Stat.(SSC) | Associate Professor. Faculty of Science, Department of Mathematics & Statistics | North 626 Ross Building | York University | 4700 Keele Street, Toronto, ON M3J 1P3 | Telephone: 416-736-5250 | Fax: 416-736-5757 | E-Mail: georges at yorku.ca


On 2017-10-09 11:02 AM, Duncan Murdoch wrote:
> I have a file containing "words" like
>
>
> a
>
> a/b
>
> a/b/c
>
> where there may be multiple words on a line (separated by spaces).? 
> The a, b, and c strings can contain non-space, non-slash characters. 
> I'd like to use gsub() to extract the c strings (which should be empty 
> if there are none).
>
> A real example is
>
> "f 147/1315/587 2820/1320/587 3624/1321/587 1852/1322/587"
>
> which I'd like to transform to
>
> " 587 587 587 587"
>
> Another real example is
>
> "f 1067 28680 24462"
>
> which should transform to "?? ".
>
> I've tried a few different regexprs, but am unable to find a way to 
> say "transform words by deleting everything up to and including the 
> 2nd slash" when there might be zero, one or two slashes.? Any 
> suggestions?
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bigfloppydog at gmail.com  Tue Oct 10 05:29:50 2017
From: bigfloppydog at gmail.com (Big Floppy Dog)
Date: Mon, 9 Oct 2017 22:29:50 -0500
Subject: [R] example of geom_contour() with function argument
In-Reply-To: <1c438b8fc056bf3caa1b18f9f822de71@dcn.davis.ca.us>
References: <CAJ_H8e81OuVNxon89PwAW3v=xJuQCBRJriALVv1ejsGiAjBGrw@mail.gmail.com>
 <CAKVAULMx0dSUErV0ehaX2aUJuzJos7hsU4i7j5Hj9DynA7C9fw@mail.gmail.com>
 <CAJ_H8e917ACQqXh-jea=7ReAQrtS+29maTzdo4vSHr5HNUq2cg@mail.gmail.com>
 <E21F1BA0-ED17-4F6A-8124-4975EB1F089C@comcast.net>
 <CAJ_H8e-SMgX2BQY_eRsHQCocmqJe4sMPX3P-4Kp=9O_Yi+MvEQ@mail.gmail.com>
 <1c438b8fc056bf3caa1b18f9f822de71@dcn.davis.ca.us>
Message-ID: <CAJ_H8e_Vj93aZqT5wspzNaC+x2J-HVKdvpOvsAxNYepVa_XLCg@mail.gmail.com>

Thank you very much! So, it appears that a grid has to be created for the
function to be used in stat_contour(). Thanks again for this example! It is
very helpful (and could be a worthwhile addition to geom_contour's help
example).

Btw, I was also trying to make the contour plot have shaded regions
corresponding to how much mass there is in between wach contour and I seem
to be getting something very ugly (and useless). Any suggestions?

library(mvtnorm)
## you were misusing "require"... only use require if you plan to
library(ggplot2)
## test the return value and fail gracefully when the
package is missing
set.seed( 1234 )
xx <- data.frame( rmvt( 100, df = c( 13, 13 ) ) )
xx2 <- expand.grid( X1 = seq( -5, 5, 0.1 )
                   ## all combinations... could  be used to fill a matrix
                   , X2 = seq( -5, 5, 0.1 )
                   )
## compute density as a function of the grid of points
xx2$d <- dmvt( as.matrix( xx2[,1:2] ) ) #! feels weird not specifying
measures of centrality or spread


ggplot( data = xx
       ,  aes( x = X1
             , y = X2
             )
       ) +
    geom_tile(data = xx2, aes(fill = d, alpha = 0.01)) +
    geom_contour(data = xx2, aes(x = X1
                             , y = X2
                             , z = d
                               )
                 ) +
    geom_point() + theme_light() +
    theme(legend.position="none")


Also, I had not completely appreciated the different between require() and
library(). I will look into the differences again! Thanks for pointing this
out.

TIAA.
BFD



On Mon, Oct 9, 2017 at 4:01 PM, jdnewmil <jdnewmil at dcn.davis.ca.us> wrote:

> library(mvtnorm) # you were misusing "require"... only use require if you
> plan to
> library(ggplot2) # test the return value and fail gracefully when the
> package is missing
> set.seed( 1234 )
> xx <- data.frame( rmvt( 100, df = c( 13, 13 ) ) )
> xx2 <- expand.grid( X1 = seq( -5, 5, 0.1 ) # all combinations... could be
> used to fill a matrix
>                   , X2 = seq( -5, 5, 0.1 )
>                   )
> # compute density as a function of the grid of points
> xx2$d <- dmvt( as.matrix( xx2[,1:2] ) ) # feels weird not specifying
> measures of centrality or spread
> ggplot( data = xx
>       ,  aes( x = X1
>             , y = X2
>             )
>       ) +
>     geom_point() + # might want this line after the geom_contour
>     geom_contour( data = xx2 # may want to consider geom_tile as well
>                 , mapping = aes( x = X1
>                                , y = X2
>                                , z = d
>                                )
>                 )
> #' ![](https://i.imgur.com/8ExFYtI.png)
> ## generated/tested with the reprex package to double check that it is
> reproducible
>
>
> On 2017-10-09 09:52, Big Floppy Dog wrote:
>
>> Hi,
>>
>> This is not a HW problem, sadly: I was last in a classroom 30 years ago,
>> and can no longer run off to the instructor :-(
>>
>> I apologize but I cut and paste the wrong snippet earlier and made a typo
>> in doing so, but the result is the same with the more appropriate
>> snippet.
>>
>> require(mvtnorm)
>> require(ggplot2)
>> set.seed(1234)
>> xx <- data.frame(rmvt(100, df = c(13, 13)))
>>
>> v <- ggplot(data = xx, aes(x = X1, y = X2, z = dmvt, df = c(13,13)))
>> v + geom_contour()
>>
>> Don't know how to automatically pick scale for object of type function.
>> Defaulting to continuous.
>> Error: Aesthetics must be either length 1 or the same as the data (100):
>> x,
>> y, z, df
>>
>> I do not understand how to put in a function as an argument to
>> geom_contour() and the examples in the help fileor in the link that Ulrik
>> sent are not very helpful to me. Hence, I was asking for some examples
>> that
>> might be helpful.
>>
>> I guess the answer is to make a second dataset that is regular and make
>> the
>> function estimate that, but how do I combine this?
>>
>> TIA.
>> BFD
>>
>>
>> On Mon, Oct 9, 2017 at 11:32 AM, David Winsemius <dwinsemius at comcast.net>
>> wrote:
>>
>>
>>> > On Oct 9, 2017, at 6:03 AM, Big Floppy Dog <bigfloppydog at gmail.com>
>>> wrote:
>>> >
>>> > Hello Ulrik,
>>> >
>>> > I apologize, but I can not see how to provide a pdf in place of the
>>> density
>>> > function which calculates a KDE (that is, something from the dataset in
>>> the
>>> > example). Can you please point to the specific example that might help?
>>> >
>>> > Here is what I get:
>>> >
>>> > require(mvtnorm)
>>> > require(ggplot2)
>>> > set.seed(1234)
>>> > xx <- data.frame(rmvt(100, df = c(13, 13)))
>>> >
>>> >
>>> > v <- ggplot(faithfuld, aes(waiting, eruptions, z = drmvt, df =
>>> c(13,13)))
>>> > v + geom_contour()
>>> >
>>> > Don't know how to automatically pick scale for object of type function.
>>> > Defaulting to continuous.
>>> > Error: Aesthetics must be either length 1 or the same as the data
>>> (5625):
>>> > x, y, z, df
>>> >
>>>
>>> That's not what I get:
>>>
>>> > v <- ggplot(faithfuld, aes(waiting, eruptions, z = drmvt, df =
>>> c(13,13)))
>>> > v + geom_contour()
>>> Error in FUN(X[[i]], ...) : object 'drmvt' not found
>>> >
>>> > ? faithfuld
>>> > str(faithfuld)
>>> Classes ?tbl_df?, ?tbl? and 'data.frame':       5625 obs. of  3
>>> variables:
>>>  $ eruptions: num  1.6 1.65 1.69 1.74 1.79 ...
>>>  $ waiting  : num  43 43 43 43 43 43 43 43 43 43 ...
>>>  $ density  : num  0.00322 0.00384 0.00444 0.00498 0.00542 ...
>>>
>>> So you are apparently trying to throw together code and data that you
>>> don't understand. The data you are using is already a density estimate
>>> designed to simply be plotted. It is not the original data. Furthermore
>>> you
>>> are passing drmvt that is apparently not in either the mvtnorm nor the
>>> ggplot2 packages.
>>>
>>> You should determine where that function is and then determine how to do
>>> a
>>> 2d estimate on the original data. I'm guessing this is homework so not
>>> inclined to offer a complete solution.
>>>
>>> --
>>> David.
>>>
>>>
>>> >
>>> > Can you please tell me how to use this here? Or is some other example
>>> more
>>> > appropriate?
>>> >
>>> > TIA,
>>> > BFD
>>> >
>>> >
>>> >
>>> > On Mon, Oct 9, 2017 at 2:22 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com
>>> >
>>> > wrote:
>>> >
>>> >> Hi BFD,
>>> >>
>>> >> ?geom_contour() *does* have helpful examples. Your Google-foo is weak:
>>> >> Searching for geom_contour brought me: http://ggplot2.tidyverse.
>>> >> org/reference/geom_contour.html as the first result.
>>> >>
>>> >> HTH
>>> >> Ulrik
>>> >>
>>> >> On Mon, 9 Oct 2017 at 08:04 Big Floppy Dog <bigfloppydog at gmail.com>
>>> wrote:
>>> >>
>>> >>> Can someone please point me to an example with geom_contour() that
>>> uses a
>>> >>> function? The help does not have an example of a function, and also
>>> I
>>> did
>>> >>> not find anything from online searches.
>>> >>>
>>> >>> TIA,
>>> >>> BFD
>>> >>>
>>> >>>
>>> >>> ------------------------------------------------------------
>>> >>> -----------------------------------
>>> >>>
>>> >>> How about geom_contour()?
>>> >>>
>>> >>> Am So., 8. Okt. 2017, 20:52 schrieb Ranjan Maitra <maitra at email.com
>>> >:
>>> >>>
>>> >>>> Hi,
>>> >>>>
>>> >>>> I am no expert on ggplot2 and I do not know the answer to your
>>> >>> question. I
>>> >>>> looked around a bit but could not find an answer right away. But one
>>> >>>> possibility could be, if a direct approach is not possible, to draw
>>> >>>> ellipses corresponding to the confidence regions of the
>>> multivariate t
>>> >>>> density and use geom_polygon to draw this successively?
>>> >>>>
>>> >>>> I will wait for a couple of days to see if there is a better answer
>>> >>> posted
>>> >>>> and then write some code, unless you get to it first.
>>> >>>>
>>> >>>> Thanks,
>>> >>>> Ranjan
>>> >>>>
>>> >>>>
>>> >>>> On Sun, 8 Oct 2017 09:30:30 -0500 Big Floppy Dog <
>>> >>> bigfloppydog at gmail.com>
>>> >>>> wrote:
>>> >>>>
>>> >>>>> Note: I have posted this on SO also but while the question has been
>>> >>>>> upvoted, there has been no answer yet.
>>> >>>>>
>>> >>>>>
>>> >>>>
>>> >>> https://stackoverflow.com/questions/46622243/ggplot-
>>> >>> plot-2d-probability-density-function-on-top-of-points-on-ggplot
>>> >>>>>
>>> >>>>> Apologies for those who have seen it there also but I thought that
>>> >>> this
>>> >>>>> list of experts may have someone who knows the answer.
>>> >>>>>
>>> >>>>> I have the following example code:
>>> >>>>>
>>> >>>>>
>>> >>>>>
>>> >>>>> require(mvtnorm)
>>> >>>>> require(ggplot2)
>>> >>>>> set.seed(1234)
>>> >>>>> xx <- data.frame(rmvt(100, df = c(13, 13)))
>>> >>>>> ggplot(data = xx,  aes(x = X1, y= X2)) + geom_point() +
>>> >>> geom_density2d()
>>> >>>>>
>>> >>>>>
>>> >>>>>
>>> >>>>> It yields a scatterplot of X2 against X1 and a KDE contour plot of
>>> the
>>> >>>>> density (as it should).
>>> >>>>>
>>> >>>>> My question is: is it possible to change the contour plot to
>>> display
>>> >>>>> the contours
>>> >>>>>
>>> >>>>> of a two-dimensional density function (say dmvt), using ggplot2?
>>> >>>>>
>>> >>>>> The remaining figures in my document are in ggplot2 and therefore I
>>> >>>>> am looking for a ggplot2 solution.
>>> >>>>>
>>> >>>>> Thanks in advance!
>>> >>>>>
>>> >>>>> BFD
>>> >>>>>
>>> >>>>>      [[alternative HTML version deleted]]
>>> >>>>>
>>> >>>>> ______________________________________________
>>> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >>>>> PLEASE do read the posting guide
>>> >>>> http://www.R-project.org/posting-guide.html
>>> >>>>> and provide commented, minimal, self-contained, reproducible code.
>>> >>>>>
>>> >>>>
>>> >>>>
>>> >>>
>>> >>>        [[alternative HTML version deleted]]
>>> >>>
>>> >>> ______________________________________________
>>> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >>> PLEASE do read the posting guide http://www.R-project.org/
>>> >>> posting-guide.html
>>> >>> and provide commented, minimal, self-contained, reproducible code.
>>> >>>
>>> >>
>>> >
>>> >       [[alternative HTML version deleted]]
>>> >
>>> > ______________________________________________
>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>> > PLEASE do read the posting guide http://www.R-project.org/
>>> posting-guide.html
>>> > and provide commented, minimal, self-contained, reproducible code.
>>>
>>> David Winsemius
>>> Alameda, CA, USA
>>>
>>> 'Any technology distinguishable from magic is insufficiently advanced.'
>>>  -Gehm's Corollary to Clarke's Third Law
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From r-packages at r-project.org  Tue Oct 10 10:26:38 2017
From: r-packages at r-project.org (Johannes Titz via R-packages)
Date: Tue, 10 Oct 2017 10:26:38 +0200
Subject: [R] [R-pkgs] New package: leabRa
Message-ID: <20171010082638.wm367y5nmtrvrflb@emachines>

Dear R enthusiasts,

I am happy to announce that the package Leab*R*a is now on CRAN
(https://cran.r-project.org/web/packages/leabRa/)

Leab*R*a provides the Leabra artificial neural network algorithm (O?Reilly,
1996, ftp://grey.colorado.edu/pub/oreilly/thesis/oreilly_thesis.all.pdf)
for R. Leabra stands for ?local error driven and associative biologically
realistic algorithm?. It is the Rolls Royce of artificial neural networks
because it combines error driven learning and self organized learning in an
elegant way, while focusing on a biologically plausible learning rule.

The vignette shows how to construct networks:
https://cran.r-project.org/web/packages/leabRa/vignettes/leabRa.html

Feedback welcome: https://github.com/johannes-titz/leabRa/

Best,
Johannes

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages

From hemantsain55 at gmail.com  Tue Oct 10 07:19:39 2017
From: hemantsain55 at gmail.com (Hemant Sain)
Date: Tue, 10 Oct 2017 10:49:39 +0530
Subject: [R] Help RFM analysis in R (i want a code where i can define my
 own breaks instead of system defined breaks used in auto_RFM package)
In-Reply-To: <CA+8X3fUyQJChXRvryO7Rc3kL1Oz-ynQf3EJfDzCbo6LYahs9yg@mail.gmail.com>
References: <CAJL6Qs_56mMkT9DBrb5w7V9cop+h-EHpmWWqd3k1MiD7PbpE9g@mail.gmail.com>
 <CA+8X3fXS+U02KxGTG1Y3BeaFR2puHb-S-v60UK8b30KSb1rcrg@mail.gmail.com>
 <CAJL6Qs9xo_+8c7gzkNb-MQGmouXX_qCeiAZ7cFggpK8xhEJSQQ@mail.gmail.com>
 <CA+8X3fV7nt=FmF=sXvAPmLVHu02Yod4T1eRR-XpuNV3UmxdTvA@mail.gmail.com>
 <CAJL6Qs-6w1NYcS04eiaWumybEw=Xko63zQDZ2LV5qOHRiXgDXw@mail.gmail.com>
 <CA+8X3fVQwre+d5AiB1GrU9EEHXHE7+zaj=7wPmEhtRBOXqoU-Q@mail.gmail.com>
 <CAJL6Qs_D86KkFPLPfY1gKBMXeUFp=Am=3FzKQConL-E+O-76aw@mail.gmail.com>
 <CAJL6Qs_LMVaJAtGz_aK+6Qz6miPcmTD41KZA-z=H2h_yYj_e=w@mail.gmail.com>
 <CA+8X3fUeAXA90_vaF6oSR19G7eNUO3SNUZEDHrPUWh1CDfDD=A@mail.gmail.com>
 <CAJL6Qs_LacF9k-jT8n6Du9G+kZEQpxknJRV=muH9g3-NTu5SSA@mail.gmail.com>
 <CA+8X3fUyQJChXRvryO7Rc3kL1Oz-ynQf3EJfDzCbo6LYahs9yg@mail.gmail.com>
Message-ID: <CAJL6Qs-jJkJE_R2zSKf3xZhwZf8LrXF+VMLBEns9Ns5Z4dtRqg@mail.gmail.com>

Hello Jim,
i have converted all my variable data type according to your attached
example including date, and my dataset looks like this.


ID                 purchase                 date
1234             10.2                         2017-02-18
3453             18.9                         2017-03-22
7689              8                             2017-03-24



but when I'm passing the data into the function it is giving me same values
for entire observations i. r=2, f=2, m=2

and which part of your code is responsible to calculate recency
and frequency score i mean how it will determine how many times a user made
a purchase in last 30 days so that we can put that user into our own
defined category.

one more thing it would be great if you can explain lil bit about finish
date. because i'm not able to understand what do you meant by finish date.

Thanks

On 10 October 2017 at 02:27, Jim Lemon <drjimlemon at gmail.com> wrote:

> I seriously doubt that you are running the code I sent. What you have
> probably done is to run your data, which has a different date format,
> without changing the breaks or the date format arguments. As you
> haven't provided any example that shows what you are doing, I can't
> guess what the problem is.
>
> Jim
>
>
> On Mon, Oct 9, 2017 at 9:40 PM, Hemant Sain <hemantsain55 at gmail.com>
> wrote:
> > I'm getting all the rows as NA in  Cscore and almost most of the
> observation
> > in R and F and M are also NA.
> > what can be the reason for this. also suggest me the appropriate
> solution.
> >
> > On 9 October 2017 at 15:51, Jim Lemon <drjimlemon at gmail.com> wrote:
> >>
> >> Hi Hemant,
> >> Here is an example that might answer your questions. Please don't run
> >> previous code as it might not work.
> >>
> >> I define the break values as arguments to the function
> >> (rbreaks,fbreaks,mbreaks) If you want the breaks to work, make sure that
> >> they cover the range of the input values, otherwise you get NAs.
> >>
> >> # expects a three (or more) column data frame where
> >> # column 1 is customer ID, column 2 is amount of purchase
> >> # and column 3 is date of purchase
> >> qdrfm<-function(x,rbreaks=3,fbreaks=3,mbreaks=3,date.format="%Y-%m-%d",
> >>  weights=c(1,1,1),finish=NA) {
> >>
> >>  # if no finish date is specified, use current date
> >>  if(is.na(finish)) finish<-as.Date(date(), "%a %b %d %H:%M:%S %Y")
> >>  x$rscore<-as.numeric(finish-as.Date(x[,3],date.format))
> >>  x$rscore<-as.numeric(cut(x$rscore,breaks=rbreaks,labels=FALSE))
> >>  custIDs<-unique(x[,1])
> >>  ncust<-length(custIDs)
> >>  rfmout<-data.frame(custID=custIDs,rscore=rep(0,ncust),
> >>   fscore=rep(0,ncust),mscore=rep(0,ncust))
> >>  rfmout$rscore<-cut(by(x$rscore,x[,1],min),breaks=rbreaks,labels=FALSE)
> >>  rfmout$fscore<-cut(table(x[,1]),breaks=fbreaks,labels=FALSE)
> >>  rfmout$mscore<-cut(by(x[,2],x[,1],sum),breaks=mbreaks,labels=FALSE)
> >>  rfmout$cscore<-(weights[1]*rfmout$rscore+
> >>   weights[2]*rfmout$fscore+
> >>   weights[3]*rfmout$mscore)/sum(weights)
> >>  return(rfmout[order(rfmout$cscore),])
> >> }
> >>
> >> set.seed(12345)
> >> x2<-data.frame(ID=sample(1:50,250,TRUE),
> >>  purchase=round(runif(250,5,100),2),
> >>  date=paste(rep(2016,250),sample(1:12,250,TRUE),
> >>   sample(1:28,250,TRUE),sep="-"))
> >>
> >> # example 1
> >> qdrfm(x2)
> >>
> >> # example 2
> >> qdrfm(x2,rbreaks=c(0,200,400),fbreaks=c(0,5,10),mbreaks=c(0,350,700),
> >>  finish=as.Date("2017-01-01"))
> >>
> >> Jim
> >>
> >
> >
> >
> > --
> > hemantsain.com
>



-- 
hemantsain.com

	[[alternative HTML version deleted]]


From niharikasinghal1990 at gmail.com  Tue Oct 10 14:08:50 2017
From: niharikasinghal1990 at gmail.com (niharika singhal)
Date: Tue, 10 Oct 2017 14:08:50 +0200
Subject: [R] "Time Series Plotting"
Message-ID: <CADe9EtS=ahEC=O1b4QGXxKuR2AoyZ9Y_fHXQF9yP7qNSXtJaqQ@mail.gmail.com>

Hello,


I need some help in plotting time series.


I have dataframe Data with two column and thousands of row, I want wherever
the sequence corresponding to Energy column is changed the color change
should be reflected in Time Series plot, some rows of dataframe are below



Energy  sequence

13536     3

14335     3

14638     3

25363     3

18511     2

18371     2

14555     3



I am able to plot time series by:-

Data.energy<- ts(Data$ Energy, frequency=1)

plot(Data.energy)



How should I add the color effect based on the sequence column value?



Thanks & Regards

Niharika Singhal

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Tue Oct 10 14:23:50 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 10 Oct 2017 12:23:50 +0000
Subject: [R] "Time Series Plotting"
In-Reply-To: <CADe9EtS=ahEC=O1b4QGXxKuR2AoyZ9Y_fHXQF9yP7qNSXtJaqQ@mail.gmail.com>
References: <CADe9EtS=ahEC=O1b4QGXxKuR2AoyZ9Y_fHXQF9yP7qNSXtJaqQ@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB265B@SRVEXCHCM301.precheza.cz>

Hi

I wonder why do you want to change it to ts. If I am not mistaken


plot(Data$Energy, col=Data$sequence)

or

plot(1:nrow(Data), Data$Energy, col=Data$sequence)

should do the trick.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of niharika
> singhal
> Sent: Tuesday, October 10, 2017 2:09 PM
> To: r-help at r-project.org
> Subject: [R] "Time Series Plotting"
>
> Hello,
>
>
> I need some help in plotting time series.
>
>
> I have dataframe Data with two column and thousands of row, I want wherever
> the sequence corresponding to Energy column is changed the color change
> should be reflected in Time Series plot, some rows of dataframe are below
>
>
>
> Energy  sequence
>
> 13536     3
>
> 14335     3
>
> 14638     3
>
> 25363     3
>
> 18511     2
>
> 18371     2
>
> 14555     3
>
>
>
> I am able to plot time series by:-
>
> Data.energy<- ts(Data$ Energy, frequency=1)
>
> plot(Data.energy)
>
>
>
> How should I add the color effect based on the sequence column value?
>
>
>
> Thanks & Regards
>
> Niharika Singhal
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From dwinsemius at comcast.net  Tue Oct 10 18:09:17 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 10 Oct 2017 09:09:17 -0700
Subject: [R] Regular expression help
In-Reply-To: <6dc4ae94-3845-d908-e58a-036127dd21ac@yorku.ca>
References: <b2da07f4-69c1-7a0d-eb1e-94c760b57a1a@stats.uwo.ca>
 <6dc4ae94-3845-d908-e58a-036127dd21ac@yorku.ca>
Message-ID: <5B7F3085-CBB4-46B1-81DC-A64A139C3093@comcast.net>


> On Oct 9, 2017, at 6:08 PM, Georges Monette <georges at yorku.ca> wrote:
> 
> How about this (I'm showing it as a pipe because it's easier to read that way):
> 
> library(magrittr)
> "f 147/1315/587 2820/1320/587 3624/1321/587 1852/1322/587" %>%
>   strsplit(' ') %>%
>   unlist %>%
>   sub('^[^/]*/*','',.) %>%
>   sub('^[^/]*/*','',.) %>%
>   paste(collapse = ' ')

I'm old school R, so I don't find that particularly readable. I read the later specification as saying each line began with an f, so the fourth item after an strsplit becomes the target.

This seemed more readable to me:

Lines <- readLines(url("http://sci.esa.int/science-e/www/object/doc.cfm?fobjectid=54726"))
lines <- Lines[ grepl("^f", Lines) ]

str(lines)
# chr [1:62908] "f 14327 6959 18747" "f 8258 15598 18980" "f 27662 21871 21939" ...

l2 <- strsplit(lines, " ")  # in that file the separators were spaces
l3 <- sapply(l2[1:3], function(x) { if (length(x) == 4) x[4] else ""
      })
l3
#[1] "18747" "18980" "21939"

# Remove the `[1:3]` to get the entire result.


Best;
David.

> 
> Georges Monette
> 
> -- 
> Georges Monette, PhD P.Stat.(SSC) | Associate Professor. Faculty of Science, Department of Mathematics & Statistics | North 626 Ross Building | York University | 4700 Keele Street, Toronto, ON M3J 1P3 | Telephone: 416-736-5250 | Fax: 416-736-5757 | E-Mail: georges at yorku.ca
> 
> 
> On 2017-10-09 11:02 AM, Duncan Murdoch wrote:
>> I have a file containing "words" like
>> 
>> 
>> a
>> 
>> a/b
>> 
>> a/b/c
>> 
>> where there may be multiple words on a line (separated by spaces).  The a, b, and c strings can contain non-space, non-slash characters. I'd like to use gsub() to extract the c strings (which should be empty if there are none).
>> 
>> A real example is
>> 
>> "f 147/1315/587 2820/1320/587 3624/1321/587 1852/1322/587"
>> 
>> which I'd like to transform to
>> 
>> " 587 587 587 587"
>> 
>> Another real example is
>> 
>> "f 1067 28680 24462"
>> 
>> which should transform to "   ".
>> 
>> I've tried a few different regexprs, but am unable to find a way to say "transform words by deleting everything up to and including the 2nd slash" when there might be zero, one or two slashes.  Any suggestions?
>> 
>> Duncan Murdoch
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From davidecortellino at gmail.com  Tue Oct 10 18:09:12 2017
From: davidecortellino at gmail.com (davide cortellino)
Date: Tue, 10 Oct 2017 18:09:12 +0200
Subject: [R] Power test binominal GLM model
Message-ID: <CAJCzyVcFNGY61E3aKmQcFgyt4ETxXOQX+SWKSY8voNUD5DoWaQ@mail.gmail.com>

Dear All


I have run the following GLM binominal model on a dataset composed by the
following variables:

TRAN_DURING_CAMP_FLG enviados bono_recibido
                 0        1     benchmark
                 0        1     benchmark
                 0        1     benchmark
                 0        1     benchmark
                 0        1     benchmark
                 0        1     benchmark


   - tran_during_flag= redemption yes/no (1/0)
   - enviados= counter variables, all 1's
   - bono_recibido= benchmark(control group) or test groups (two type of
   test groups)

The model used has been

glm(TRAN_DURING_CAMP_FLG~bono_recibido,exp2,family="binomial")

                          Estimate Std. Error     z value
Pr(>|z|)(Intercept)             -1.4924117 0.01372190 -108.761315
0.000000e+00
bono_recibidoBONO3EUROS -0.8727739 0.09931119   -8.788274 1.518758e-18
bono_recibidoBONO6EUROS  0.1069435 0.02043840    5.232480 1.672507e-07

The scope for this model was to test if there was significative difference
in the redemption rate between control group and test groups. Now, applying
the post hoc test:

> Treat.comp<-glht(mod.binposthoc,mcp(bono_recibido='Tukey'))> summary(Treat.comp) # el modelo se encuentra en  log odds aqui

     Simultaneous Tests for General Linear Hypotheses
Multiple Comparisons of Means: Tukey Contrasts

Fit: glm(formula = TRAN_DURING_CAMP_FLG ~ bono_recibido, family = "binomial",
    data = exp2)
Linear Hypotheses:
                             Estimate Std. Error z value Pr(>|z|)
BONO3EUROS - benchmark == 0  -0.87277    0.09931  -8.788  < 1e-09 ***
BONO6EUROS - benchmark == 0   0.10694    0.02044   5.232 3.34e-07 ***
BONO6EUROS - BONO3EUROS == 0  0.97972    0.09952   9.845  < 1e-09
***---Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ?
1(Adjusted p values reported -- single-step method)

It confirm that the differences are significatively differents, however, I
would check the power of the model in assessing these differences. I have
checked several time both on cross validates and on the web but it seems
there is no pre-made function which enable the user to compute the power of
glm models. Is it the case? Does anyone know of available packages or
methodologies to achive a power test in a glm binominal model?

Bests

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Tue Oct 10 18:47:37 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 10 Oct 2017 09:47:37 -0700
Subject: [R] Power test binominal GLM model
In-Reply-To: <CAJCzyVcFNGY61E3aKmQcFgyt4ETxXOQX+SWKSY8voNUD5DoWaQ@mail.gmail.com>
References: <CAJCzyVcFNGY61E3aKmQcFgyt4ETxXOQX+SWKSY8voNUD5DoWaQ@mail.gmail.com>
Message-ID: <1A226DE7-C9C8-47B8-A38C-241CC2BA8335@comcast.net>


> On Oct 10, 2017, at 9:09 AM, davide cortellino <davidecortellino at gmail.com> wrote:
> 
> Dear All
> 
> 
> I have run the following GLM binominal model on a dataset composed by the
> following variables:
> 
> TRAN_DURING_CAMP_FLG enviados bono_recibido
>                 0        1     benchmark
>                 0        1     benchmark
>                 0        1     benchmark
>                 0        1     benchmark
>                 0        1     benchmark
>                 0        1     benchmark
> 
> 
>   - tran_during_flag= redemption yes/no (1/0)
>   - enviados= counter variables, all 1's
>   - bono_recibido= benchmark(control group) or test groups (two type of
>   test groups)
> 
> The model used has been
> 
> glm(TRAN_DURING_CAMP_FLG~bono_recibido,exp2,family="binomial")
> 
>                          Estimate Std. Error     z value
> Pr(>|z|)(Intercept)             -1.4924117 0.01372190 -108.761315
> 0.000000e+00
> bono_recibidoBONO3EUROS -0.8727739 0.09931119   -8.788274 1.518758e-18
> bono_recibidoBONO6EUROS  0.1069435 0.02043840    5.232480 1.672507e-07
> 
> The scope for this model was to test if there was significative difference
> in the redemption rate between control group and test groups. Now, applying
> the post hoc test:
> 
>> Treat.comp<-glht(mod.binposthoc,mcp(bono_recibido='Tukey'))> summary(Treat.comp) # el modelo se encuentra en  log odds aqui
> 
>     Simultaneous Tests for General Linear Hypotheses
> Multiple Comparisons of Means: Tukey Contrasts
> 
> Fit: glm(formula = TRAN_DURING_CAMP_FLG ~ bono_recibido, family = "binomial",
>    data = exp2)
> Linear Hypotheses:
>                             Estimate Std. Error z value Pr(>|z|)
> BONO3EUROS - benchmark == 0  -0.87277    0.09931  -8.788  < 1e-09 ***
> BONO6EUROS - benchmark == 0   0.10694    0.02044   5.232 3.34e-07 ***
> BONO6EUROS - BONO3EUROS == 0  0.97972    0.09952   9.845  < 1e-09
> ***---Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ?
> 1(Adjusted p values reported -- single-step method)
> 
> It confirm that the differences are significatively differents, however, I
> would check the power of the model in assessing these differences. I have
> checked several time both on cross validates and on the web but it seems
> there is no pre-made function which enable the user to compute the power of
> glm models. Is it the case? Does anyone know of available packages or
> methodologies to achive a power test in a glm binominal model?

What's the point? The time to do power tests is before the experiment is performed. There's really no value in doing post hoc power testing, and this is especially true when you have highly significant results.

> Bests
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From 538280 at gmail.com  Tue Oct 10 18:48:20 2017
From: 538280 at gmail.com (Greg Snow)
Date: Tue, 10 Oct 2017 10:48:20 -0600
Subject: [R] Power test binominal GLM model
In-Reply-To: <CAJCzyVcFNGY61E3aKmQcFgyt4ETxXOQX+SWKSY8voNUD5DoWaQ@mail.gmail.com>
References: <CAJCzyVcFNGY61E3aKmQcFgyt4ETxXOQX+SWKSY8voNUD5DoWaQ@mail.gmail.com>
Message-ID: <CAFEqCdxiP_Jv9Lp3SNQnvfHwAspt=ci=ttdA6HC0TGyGh2s0eQ@mail.gmail.com>

You may find the answers to this question on Cross Validated (along
with the discussion) to be useful:
https://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments

On Tue, Oct 10, 2017 at 10:09 AM, davide cortellino
<davidecortellino at gmail.com> wrote:
> Dear All
>
>
> I have run the following GLM binominal model on a dataset composed by the
> following variables:
>
> TRAN_DURING_CAMP_FLG enviados bono_recibido
>                  0        1     benchmark
>                  0        1     benchmark
>                  0        1     benchmark
>                  0        1     benchmark
>                  0        1     benchmark
>                  0        1     benchmark
>
>
>    - tran_during_flag= redemption yes/no (1/0)
>    - enviados= counter variables, all 1's
>    - bono_recibido= benchmark(control group) or test groups (two type of
>    test groups)
>
> The model used has been
>
> glm(TRAN_DURING_CAMP_FLG~bono_recibido,exp2,family="binomial")
>
>                           Estimate Std. Error     z value
> Pr(>|z|)(Intercept)             -1.4924117 0.01372190 -108.761315
> 0.000000e+00
> bono_recibidoBONO3EUROS -0.8727739 0.09931119   -8.788274 1.518758e-18
> bono_recibidoBONO6EUROS  0.1069435 0.02043840    5.232480 1.672507e-07
>
> The scope for this model was to test if there was significative difference
> in the redemption rate between control group and test groups. Now, applying
> the post hoc test:
>
>> Treat.comp<-glht(mod.binposthoc,mcp(bono_recibido='Tukey'))> summary(Treat.comp) # el modelo se encuentra en  log odds aqui
>
>      Simultaneous Tests for General Linear Hypotheses
> Multiple Comparisons of Means: Tukey Contrasts
>
> Fit: glm(formula = TRAN_DURING_CAMP_FLG ~ bono_recibido, family = "binomial",
>     data = exp2)
> Linear Hypotheses:
>                              Estimate Std. Error z value Pr(>|z|)
> BONO3EUROS - benchmark == 0  -0.87277    0.09931  -8.788  < 1e-09 ***
> BONO6EUROS - benchmark == 0   0.10694    0.02044   5.232 3.34e-07 ***
> BONO6EUROS - BONO3EUROS == 0  0.97972    0.09952   9.845  < 1e-09
> ***---Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ?
> 1(Adjusted p values reported -- single-step method)
>
> It confirm that the differences are significatively differents, however, I
> would check the power of the model in assessing these differences. I have
> checked several time both on cross validates and on the web but it seems
> there is no pre-made function which enable the user to compute the power of
> glm models. Is it the case? Does anyone know of available packages or
> methodologies to achive a power test in a glm binominal model?
>
> Bests
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From chocold12 at gmail.com  Tue Oct 10 22:41:23 2017
From: chocold12 at gmail.com (lily li)
Date: Tue, 10 Oct 2017 14:41:23 -0600
Subject: [R] About multiple panels with limited space in-between
Message-ID: <CAN5afy_f2_BRs-Tt-rPwoBssqqP45w4qFEbOt3cRivF2gqQ=qQ@mail.gmail.com>

Hi R users,

I have a question about plotting. The following two datasets are an
example. What I have in mind is like the attached figure, but just have two
rows, top row is for DF1, bottom row is for DF2. The top and bottom rows,
have x-axis as var1, var2, var3, etc, while the y-axis represents A. For
each row, only the leftmost panel has y-axis ticks and label. For the two
rows, only the bottom row has x-axis ticks and label. I am not very clear
about the coding. Thanks if you could give me any suggestions.


DF1: Year 2000
A     var1   var2   var3   var4   var5
1.2   0.8     0.9     1.1     1.2     12
1.8   0.9     1.2     1.0     2.1     15
1.5   0.7     1.1     0.9     2.2     16
1.6   0.9     1.0     0.7     2.5     18
...

DF2: Year 2001
A     var1   var2   var3   var4   var5
1.1   0.85     0.9     1.1     1.2     22
1.4   0.99     1.2     1.0     2.1     25
0.8   0.74     1.1     0.9     2.2     26
0.6   0.92     1.0     0.7     2.5     28
...

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Wed Oct 11 00:00:13 2017
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 11 Oct 2017 09:00:13 +1100
Subject: [R] Help RFM analysis in R (i want a code where i can define my
 own breaks instead of system defined breaks used in auto_RFM package)
In-Reply-To: <CAJL6Qs-jJkJE_R2zSKf3xZhwZf8LrXF+VMLBEns9Ns5Z4dtRqg@mail.gmail.com>
References: <CAJL6Qs_56mMkT9DBrb5w7V9cop+h-EHpmWWqd3k1MiD7PbpE9g@mail.gmail.com>
 <CA+8X3fXS+U02KxGTG1Y3BeaFR2puHb-S-v60UK8b30KSb1rcrg@mail.gmail.com>
 <CAJL6Qs9xo_+8c7gzkNb-MQGmouXX_qCeiAZ7cFggpK8xhEJSQQ@mail.gmail.com>
 <CA+8X3fV7nt=FmF=sXvAPmLVHu02Yod4T1eRR-XpuNV3UmxdTvA@mail.gmail.com>
 <CAJL6Qs-6w1NYcS04eiaWumybEw=Xko63zQDZ2LV5qOHRiXgDXw@mail.gmail.com>
 <CA+8X3fVQwre+d5AiB1GrU9EEHXHE7+zaj=7wPmEhtRBOXqoU-Q@mail.gmail.com>
 <CAJL6Qs_D86KkFPLPfY1gKBMXeUFp=Am=3FzKQConL-E+O-76aw@mail.gmail.com>
 <CAJL6Qs_LMVaJAtGz_aK+6Qz6miPcmTD41KZA-z=H2h_yYj_e=w@mail.gmail.com>
 <CA+8X3fUeAXA90_vaF6oSR19G7eNUO3SNUZEDHrPUWh1CDfDD=A@mail.gmail.com>
 <CAJL6Qs_LacF9k-jT8n6Du9G+kZEQpxknJRV=muH9g3-NTu5SSA@mail.gmail.com>
 <CA+8X3fUyQJChXRvryO7Rc3kL1Oz-ynQf3EJfDzCbo6LYahs9yg@mail.gmail.com>
 <CAJL6Qs-jJkJE_R2zSKf3xZhwZf8LrXF+VMLBEns9Ns5Z4dtRqg@mail.gmail.com>
Message-ID: <CA+8X3fV7jiTcV9frtYeq4drzLnh+q8PaosuH9ca-6qMWU8pReQ@mail.gmail.com>

Hi Hemant,
see inline below.

On Tue, Oct 10, 2017 at 4:19 PM, Hemant Sain <hemantsain55 at gmail.com> wrote:
> Hello Jim,
> i have converted all my variable data type according to your attached
> example including date, and my dataset looks like this.
>
>
> ID                 purchase                 date
> 1234             10.2                         2017-02-18
> 3453             18.9                         2017-03-22
> 7689              8                             2017-03-24
>
As I don't have your data set, I can't tell you why you are getting
the same values. First, I'll create a data set that looks something
like your example, except with 50 customers and 250 transactions, all
in 2017:

set.seed(12345)
x3<-data.frame(ID=sample(1234:1283,250,TRUE),
 purchase=round(runif(250,5,100),2),
 date=paste(rep(2017,250),sample(1:9,250,TRUE),
  sample(1:28,250,TRUE),sep="-"))

Look at it carefully. Is there anything that you think is wrong?
>
>
> but when I'm passing the data into the function it is giving me same values
> for entire observations i. r=2, f=2, m=2
>
> and which part of your code is responsible to calculate recency and
> frequency score i mean how it will determine how many times a user made a
> purchase in last 30 days so that we can put that user into our own defined
> category.
>
Here is the function commented for easier understanding.

Recency is calculated as the most recent purchase for each customer
from the "finish" date, which defaults to the current date. If you are
examining historical data, you may want to set a different "finish"
date.

Frequency is simply the number of purchases recorded for each customer.

Monetary is the sum of the purchase amounts for each customer

The default breaks for each score are those calculated by the "cut"
function. If you want specific breaks, they _must_ cover the range of
the values or cut will generate NAs. I have added a printout of the
ranges of the raw recency, frequency and monetary scores so that you
can enter your own breaks.

qdrfm<-function(x,rbreaks=3,fbreaks=3,mbreaks=3,
 date.format="%Y-%m-%d",weights=c(1,1,1),finish=NA) {

 # if no finish date is specified, use current date
 if(is.na(finish)) finish<-as.Date(date(), "%a %b %d %H:%M:%S %Y")
 x$rscore<-as.numeric(finish-as.Date(x[,3],date.format))
 cat("Range of purchase recency",range(x$rscore),"\n")
 x$rscore<-as.numeric(cut(x$rscore,breaks=rbreaks,labels=FALSE))
 cat("Range of purchase freqency",range(table(x[,1])),"\n")
 cat("Range of purchase amount",range(by(x[,2],x[,1],sum)),"\n")
 custIDs<-unique(x[,1])
 ncust<-length(custIDs)
 # initialize a data frame to hold the output
 rfmout<-data.frame(custID=custIDs,rscore=rep(0,ncust),
  fscore=rep(0,ncust),mscore=rep(0,ncust))
 # categorize the minimum number of days
 # since last purchase for each customer
 rfmout$rscore<-cut(by(x$rscore,x[,1],min),breaks=rbreaks,labels=FALSE)
 # categorize the number of purchases
 # recorded for each customer
 rfmout$fscore<-cut(table(x[,1]),breaks=fbreaks,labels=FALSE)
 # categorize the amount purchased
 # by each customer
 rfmout$mscore<-cut(by(x[,2],x[,1],sum),breaks=mbreaks,labels=FALSE)
 # calculate the RFM score from the
 # optionally weighted average of the above
 rfmout$cscore<-round((weights[1]*rfmout$rscore+
  weights[2]*rfmout$fscore+
  weights[3]*rfmout$mscore)/sum(weights),2)
 return(rfmout[order(rfmout$cscore),])
}

# run the dataset with default breaks
qdrfm(x3)

# now specify breaks with respect to the printout of the raw scores
qdrfm(x3,rbreaks=c(0,150,300),fbreaks=c(0,5,11),mbreaks=c(0,300,600))

# now give the total amount purchased twice the weight
qdrfm(x3,rbreaks=c(0,150,300),fbreaks=c(0,5,11),
 mbreaks=c(0,300,600),weights=c(1,1,2))

I hope that this will explain the function better.

Jim


From chocold12 at gmail.com  Wed Oct 11 00:13:20 2017
From: chocold12 at gmail.com (lily li)
Date: Tue, 10 Oct 2017 16:13:20 -0600
Subject: [R] About multiple panels with limited space in-between
In-Reply-To: <CAN5afy_f2_BRs-Tt-rPwoBssqqP45w4qFEbOt3cRivF2gqQ=qQ@mail.gmail.com>
References: <CAN5afy_f2_BRs-Tt-rPwoBssqqP45w4qFEbOt3cRivF2gqQ=qQ@mail.gmail.com>
Message-ID: <CAN5afy8wAtN+z783TDjucAmjO3co+=Bm15Q1mC5qSqgkvB7M=A@mail.gmail.com>

I use the code below to plot, but have some difficulties.

par(mfrow=c(2,5))
par(mar=c(2,1,1,0), oma=c(4,4,.5,.5))
plot(DF1$var1,DF1$A)
plot(DF1$var2,DF1$A, ylab=F); plot(DF1$var3,DF1$A,ylab=F);
plot(DF1$var4,DF1$A, ylab=F); plot(DF1$var5,DF1$A,ylab=F)
plot(DF2$var1,DF2$A)
plot(DF2$var2,DF2$A, ylab=F); plot(DF2$var3,DF2$A, ylab=F);
plot(DF2$var4,DF2$A, ylab=F); plot(DF2$var5,DF2$A, ylab=F)

Changing "ylab=F" to "labels=F" got the error message: "labels" is not a
graphical prameter.
For each row, I would like to use the leftmost y-axis, so the other figures
do not need to have y-axis. Then how to just have x-labels for the bottom
row, but have ticks and numbers in the x-labels for the top row? Also, I
forgot to attach the figure and here it is. This is more complicated, but I
just need two rows, and there are spaces between the two rows. Thanks first.

On Tue, Oct 10, 2017 at 2:41 PM, lily li <chocold12 at gmail.com> wrote:

> Hi R users,
>
> I have a question about plotting. The following two datasets are an
> example. What I have in mind is like the attached figure, but just have two
> rows, top row is for DF1, bottom row is for DF2. The top and bottom rows,
> have x-axis as var1, var2, var3, etc, while the y-axis represents A. For
> each row, only the leftmost panel has y-axis ticks and label. For the two
> rows, only the bottom row has x-axis ticks and label. I am not very clear
> about the coding. Thanks if you could give me any suggestions.
>
>
> DF1: Year 2000
> A     var1   var2   var3   var4   var5
> 1.2   0.8     0.9     1.1     1.2     12
> 1.8   0.9     1.2     1.0     2.1     15
> 1.5   0.7     1.1     0.9     2.2     16
> 1.6   0.9     1.0     0.7     2.5     18
> ...
>
> DF2: Year 2001
> A     var1   var2   var3   var4   var5
> 1.1   0.85     0.9     1.1     1.2     22
> 1.4   0.99     1.2     1.0     2.1     25
> 0.8   0.74     1.1     0.9     2.2     26
> 0.6   0.92     1.0     0.7     2.5     28
> ...
>
>

From samuel.knapp at tum.de  Wed Oct 11 00:07:11 2017
From: samuel.knapp at tum.de (Samuel Knapp)
Date: Wed, 11 Oct 2017 00:07:11 +0200
Subject: [R] Unbalanced data in split-plot analysis with aov()
Message-ID: <a8cc3b15-e48f-d08a-f2fd-01d222895e8f@tum.de>

Dear all,

I'm analysing a split-plot experiment, where there are sometimes one or 
two values missing. I realized that if the data is slightly unbalanced, 
the effect of the subplot-treatment will also appear and be tested 
against the mainplot-error term.

I replicated this with the Oats dataset from Yates (1935), contained in 
the nlme package, where Variety is on mainplot, and nitro on subplot.

 > # Oats dataset (Yates 1935) from the nlme package
 > require(nlme); data <- get(data(Oats))
 > data$nitro <- factor(data$nitro);data$Block <- 
as.factor(as.character(data$Block))
 > nrow(data) # 6 Blocks * 4 N-levels * 3 Varieties = 72 obs -> 
orthogonal and balanced
[1] 72
 > # split-plot anova
 > summary(aov(yield ~ Block+Variety*nitro + Error(Block/Variety),data))

Error: Block
 ????? Df Sum Sq Mean Sq
Block? 5? 15875??? 3175

Error: Block:Variety
 ????????? Df Sum Sq Mean Sq F value Pr(>F)
Variety??? 2?? 1786?? 893.2?? 1.485? 0.272
Residuals 10?? 6013?? 601.3

Error: Within
 ????????????? Df Sum Sq Mean Sq F value?? Pr(>F)
nitro????????? 3? 20020??? 6674? 37.686 2.46e-12 ***
Variety:nitro? 6??? 322????? 54?? 0.303??? 0.932
Residuals???? 45?? 7969???? 177
---
Signif. codes:? 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
 > reduceddata <- data[-5,] # delete one observation
 > summary(aov(yield ~ Block+Variety*nitro + 
Error(Block/Variety),reduceddata))

Error: Block
 ????? Df Sum Sq Mean Sq
Block? 5? 16070??? 3214

Error: Block:Variety
 ????????? Df Sum Sq Mean Sq F value Pr(>F)
Variety??? 2?? 1820?? 910.0?? 1.373? 0.302
nitro????? 1????? 1???? 1.0?? 0.001? 0.970
Residuals? 9?? 5964?? 662.7

Error: Within
 ????????????? Df Sum Sq Mean Sq F value?? Pr(>F)
nitro????????? 3? 19766??? 6589?? 36.88 4.49e-12 ***
Variety:nitro? 6??? 333????? 55??? 0.31??? 0.928
Residuals???? 44?? 7860???? 179

Although I fully understand, that mixed-models should be preferred for 
non-orthogonal/unbalanced data, I think it's still valid to use the 
aov() approach, when only 1 or 2 datapoints are missing. Also, I don't 
really understand, why the structure of the ANOVA changes suddenly. Is 
there any argument, I could supply to change this behaviour?

When I do the same with lm() and subsequent anova(), and calculate 
F-value for Variety by hand, the estimates are still quite robust.

Best regards,

Sam


-- 
Samuel Knapp

Lehrstuhl f?r Pflanzenern?hrung
Technische Universit?t M?nchen
(Chair of Plant Nutrition
Technical University of Munich)

Emil-Ramann-Strasse 2
D-85354 Freising

Tel. +49 8161 71-3578
samuel.knapp at tum.de
www.researchgate.net/profile/Samuel_Knapp

-- 
Samuel Knapp

Lehrstuhl f?r Pflanzenern?hrung
Technische Universit?t M?nchen
(Chair of Plant Nutrition
Technical University of Munich)

Emil-Ramann-Strasse 2
D-85354 Freising

Tel. +49 8161 71-3578	
samuel.knapp at tum.de
www.researchgate.net/profile/Samuel_Knapp


	[[alternative HTML version deleted]]


From maitra at email.com  Wed Oct 11 06:06:54 2017
From: maitra at email.com (Ranjan Maitra)
Date: Tue, 10 Oct 2017 23:06:54 -0500
Subject: [R] example of geom_contour() with function argument
In-Reply-To: <1c438b8fc056bf3caa1b18f9f822de71@dcn.davis.ca.us>
References: <CAJ_H8e81OuVNxon89PwAW3v=xJuQCBRJriALVv1ejsGiAjBGrw@mail.gmail.com>
 <CAKVAULMx0dSUErV0ehaX2aUJuzJos7hsU4i7j5Hj9DynA7C9fw@mail.gmail.com>
 <CAJ_H8e917ACQqXh-jea=7ReAQrtS+29maTzdo4vSHr5HNUq2cg@mail.gmail.com>
 <E21F1BA0-ED17-4F6A-8124-4975EB1F089C@comcast.net>
 <CAJ_H8e-SMgX2BQY_eRsHQCocmqJe4sMPX3P-4Kp=9O_Yi+MvEQ@mail.gmail.com>
 <1c438b8fc056bf3caa1b18f9f822de71@dcn.davis.ca.us>
Message-ID: <20171010230654.48a056d66dd8c0e0ad6d3c60@email.com>

Here is an alternative using geom_polygon which captures the "spirit" of the fatter tails of the multivariate t (in my opinion, because the display is quantile-based). Note that I have modified the OP's question somewhat to use non-identity matrices.

I do came up with a few questions while creating this suggestions which are at the end. 

## code follows

nu <- 5  ## this is the degrees of freedom of the multivariate t. 

library(mvtnorm) 
library(ggplot2)

sig <- matrix(c(1, 0.5, 0.5, 1), ncol = 2)  ## this is the sigma parameter for the multivariate t

xx <- data.frame( rmvt(n = 100, df = c(nu, nu), sigma = sig)) ## generating the original sample

rtsq <- rowSums(x = matrix(rt(n = 2e6, df = nu)^2, ncol = 2)) ## generating the sample for the ellipse-quantiles. Note that this is a cumbersome calculation because it is the sum of two independent t-squared random variables with the same degrees of freedom so I am using simulation to get the quantiles. This is the sample from which I will create the quantiles.

g <- ggplot( data = xx
         ,  aes( x = X1
              , y = X2
                )
           ) + geom_point(colour = "red", size = 2)      ## initial setup

library(ellipse)

for (i in seq(from = 0.01, to = 0.99, length.out = 20)) {
    el.df <- data.frame(ellipse(x = sig, t = sqrt(quantile(rtsq, probs = i))))    ## create the data for the given quantile of the ellipse.
    names(el.df) <- c("x", "y")
    g <- g + geom_polygon(data=el.df, aes(x=x, y=y), fill = NA, linetype=1, colour = "blue") ## plot the ellipse
}

g + theme_bw() 

Hope this helps the OP! 

Questions and comments:

1. stat_ellipse would of course have been easier, but I could not figure out how to specify either the degrees of freedom or the correlation/variance-covariance matrix of the multivariate t. 

2. How does one reduce the line-width of the ellipses (i.e. the polygons). 

3. Also, it seems to me that the right way to draw the intensities of the contour plot in the way the OP has now asked would  be to plot the annular ellipses, each with alpha proportional to the squared difference between the two ellipse constants (t in the ellipse function above). (That would display the density correctly.) I am not sure how to draw the annular ellipse using geom_polygon or something else. 

Many thanks and best wishes,
Ranjan













On Mon, 09 Oct 2017 14:01:25 -0700 jdnewmil <jdnewmil at dcn.davis.ca.us> wrote:

> library(mvtnorm) # you were misusing "require"... only use require if 
> you plan to
> library(ggplot2) # test the return value and fail gracefully when the 
> package is missing
> set.seed( 1234 )
> xx <- data.frame( rmvt( 100, df = c( 13, 13 ) ) )
> xx2 <- expand.grid( X1 = seq( -5, 5, 0.1 ) # all combinations... could 
> be used to fill a matrix
>                    , X2 = seq( -5, 5, 0.1 )
>                    )
> # compute density as a function of the grid of points
> xx2$d <- dmvt( as.matrix( xx2[,1:2] ) ) # feels weird not specifying 
> measures of centrality or spread
> ggplot( data = xx
>        ,  aes( x = X1
>              , y = X2
>              )
>        ) +
>      geom_point() + # might want this line after the geom_contour
>      geom_contour( data = xx2 # may want to consider geom_tile as well
>                  , mapping = aes( x = X1
>                                 , y = X2
>                                 , z = d
>                                 )
>                  )
> #' ![](https://i.imgur.com/8ExFYtI.png)
> ## generated/tested with the reprex package to double check that it is 
> reproducible
> 
> On 2017-10-09 09:52, Big Floppy Dog wrote:
> > Hi,
> > 
> > This is not a HW problem, sadly: I was last in a classroom 30 years 
> > ago,
> > and can no longer run off to the instructor :-(
> > 
> > I apologize but I cut and paste the wrong snippet earlier and made a 
> > typo
> > in doing so, but the result is the same with the more appropriate  
> > snippet.
> > 
> > require(mvtnorm)
> > require(ggplot2)
> > set.seed(1234)
> > xx <- data.frame(rmvt(100, df = c(13, 13)))
> > 
> > v <- ggplot(data = xx, aes(x = X1, y = X2, z = dmvt, df = c(13,13)))
> > v + geom_contour()
> > 
> > Don't know how to automatically pick scale for object of type function.
> > Defaulting to continuous.
> > Error: Aesthetics must be either length 1 or the same as the data 
> > (100): x,
> > y, z, df
> > 
> > I do not understand how to put in a function as an argument to
> > geom_contour() and the examples in the help fileor in the link that 
> > Ulrik
> > sent are not very helpful to me. Hence, I was asking for some examples 
> > that
> > might be helpful.
> > 
> > I guess the answer is to make a second dataset that is regular and make 
> > the
> > function estimate that, but how do I combine this?
> > 
> > TIA.
> > BFD
> > 
> > 
> > On Mon, Oct 9, 2017 at 11:32 AM, David Winsemius 
> > <dwinsemius at comcast.net>
> > wrote:
> > 
> >> 
> >> > On Oct 9, 2017, at 6:03 AM, Big Floppy Dog <bigfloppydog at gmail.com>
> >> wrote:
> >> >
> >> > Hello Ulrik,
> >> >
> >> > I apologize, but I can not see how to provide a pdf in place of the
> >> density
> >> > function which calculates a KDE (that is, something from the dataset in
> >> the
> >> > example). Can you please point to the specific example that might help?
> >> >
> >> > Here is what I get:
> >> >
> >> > require(mvtnorm)
> >> > require(ggplot2)
> >> > set.seed(1234)
> >> > xx <- data.frame(rmvt(100, df = c(13, 13)))
> >> >
> >> >
> >> > v <- ggplot(faithfuld, aes(waiting, eruptions, z = drmvt, df = c(13,13)))
> >> > v + geom_contour()
> >> >
> >> > Don't know how to automatically pick scale for object of type function.
> >> > Defaulting to continuous.
> >> > Error: Aesthetics must be either length 1 or the same as the data (5625):
> >> > x, y, z, df
> >> >
> >> 
> >> That's not what I get:
> >> 
> >> > v <- ggplot(faithfuld, aes(waiting, eruptions, z = drmvt, df = c(13,13)))
> >> > v + geom_contour()
> >> Error in FUN(X[[i]], ...) : object 'drmvt' not found
> >> >
> >> > ? faithfuld
> >> > str(faithfuld)
> >> Classes ?tbl_df?, ?tbl? and 'data.frame':       5625 obs. of  3 
> >> variables:
> >>  $ eruptions: num  1.6 1.65 1.69 1.74 1.79 ...
> >>  $ waiting  : num  43 43 43 43 43 43 43 43 43 43 ...
> >>  $ density  : num  0.00322 0.00384 0.00444 0.00498 0.00542 ...
> >> 
> >> So you are apparently trying to throw together code and data that you
> >> don't understand. The data you are using is already a density estimate
> >> designed to simply be plotted. It is not the original data. 
> >> Furthermore you
> >> are passing drmvt that is apparently not in either the mvtnorm nor the
> >> ggplot2 packages.
> >> 
> >> You should determine where that function is and then determine how to 
> >> do a
> >> 2d estimate on the original data. I'm guessing this is homework so not
> >> inclined to offer a complete solution.
> >> 
> >> --
> >> David.
> >> 
> >> 
> >> >
> >> > Can you please tell me how to use this here? Or is some other example
> >> more
> >> > appropriate?
> >> >
> >> > TIA,
> >> > BFD
> >> >
> >> >
> >> >
> >> > On Mon, Oct 9, 2017 at 2:22 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
> >> > wrote:
> >> >
> >> >> Hi BFD,
> >> >>
> >> >> ?geom_contour() *does* have helpful examples. Your Google-foo is weak:
> >> >> Searching for geom_contour brought me: http://ggplot2.tidyverse.
> >> >> org/reference/geom_contour.html as the first result.
> >> >>
> >> >> HTH
> >> >> Ulrik
> >> >>
> >> >> On Mon, 9 Oct 2017 at 08:04 Big Floppy Dog <bigfloppydog at gmail.com>
> >> wrote:
> >> >>
> >> >>> Can someone please point me to an example with geom_contour() that
> >> uses a
> >> >>> function? The help does not have an example of a function, and also  I
> >> did
> >> >>> not find anything from online searches.
> >> >>>
> >> >>> TIA,
> >> >>> BFD
> >> >>>
> >> >>>
> >> >>> ------------------------------------------------------------
> >> >>> -----------------------------------
> >> >>>
> >> >>> How about geom_contour()?
> >> >>>
> >> >>> Am So., 8. Okt. 2017, 20:52 schrieb Ranjan Maitra <maitra at email.com>:
> >> >>>
> >> >>>> Hi,
> >> >>>>
> >> >>>> I am no expert on ggplot2 and I do not know the answer to your
> >> >>> question. I
> >> >>>> looked around a bit but could not find an answer right away. But one
> >> >>>> possibility could be, if a direct approach is not possible, to draw
> >> >>>> ellipses corresponding to the confidence regions of the multivariate t
> >> >>>> density and use geom_polygon to draw this successively?
> >> >>>>
> >> >>>> I will wait for a couple of days to see if there is a better answer
> >> >>> posted
> >> >>>> and then write some code, unless you get to it first.
> >> >>>>
> >> >>>> Thanks,
> >> >>>> Ranjan
> >> >>>>
> >> >>>>
> >> >>>> On Sun, 8 Oct 2017 09:30:30 -0500 Big Floppy Dog <
> >> >>> bigfloppydog at gmail.com>
> >> >>>> wrote:
> >> >>>>
> >> >>>>> Note: I have posted this on SO also but while the question has been
> >> >>>>> upvoted, there has been no answer yet.
> >> >>>>>
> >> >>>>>
> >> >>>>
> >> >>> https://stackoverflow.com/questions/46622243/ggplot-
> >> >>> plot-2d-probability-density-function-on-top-of-points-on-ggplot
> >> >>>>>
> >> >>>>> Apologies for those who have seen it there also but I thought that
> >> >>> this
> >> >>>>> list of experts may have someone who knows the answer.
> >> >>>>>
> >> >>>>> I have the following example code:
> >> >>>>>
> >> >>>>>
> >> >>>>>
> >> >>>>> require(mvtnorm)
> >> >>>>> require(ggplot2)
> >> >>>>> set.seed(1234)
> >> >>>>> xx <- data.frame(rmvt(100, df = c(13, 13)))
> >> >>>>> ggplot(data = xx,  aes(x = X1, y= X2)) + geom_point() +
> >> >>> geom_density2d()
> >> >>>>>
> >> >>>>>
> >> >>>>>
> >> >>>>> It yields a scatterplot of X2 against X1 and a KDE contour plot of
> >> the
> >> >>>>> density (as it should).
> >> >>>>>
> >> >>>>> My question is: is it possible to change the contour plot to display
> >> >>>>> the contours
> >> >>>>>
> >> >>>>> of a two-dimensional density function (say dmvt), using ggplot2?
> >> >>>>>
> >> >>>>> The remaining figures in my document are in ggplot2 and therefore I
> >> >>>>> am looking for a ggplot2 solution.
> >> >>>>>
> >> >>>>> Thanks in advance!
> >> >>>>>
> >> >>>>> BFD
> >> >>>>>
> >> >>>>>      [[alternative HTML version deleted]]
> >> >>>>>
> >> >>>>> ______________________________________________
> >> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >> >>>>> PLEASE do read the posting guide
> >> >>>> http://www.R-project.org/posting-guide.html
> >> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >> >>>>>
> >> >>>>
> >> >>>>
> >> >>>
> >> >>>        [[alternative HTML version deleted]]
> >> >>>
> >> >>> ______________________________________________
> >> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >> >>> PLEASE do read the posting guide http://www.R-project.org/
> >> >>> posting-guide.html
> >> >>> and provide commented, minimal, self-contained, reproducible code.
> >> >>>
> >> >>
> >> >
> >> >       [[alternative HTML version deleted]]
> >> >
> >> > ______________________________________________
> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide http://www.R-project.org/
> >> posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
> >> 
> >> David Winsemius
> >> Alameda, CA, USA
> >> 
> >> 'Any technology distinguishable from magic is insufficiently 
> >> advanced.'
> >>  -Gehm's Corollary to Clarke's Third Law
> >> 
> >> 
> >> 
> >> 
> >> 
> >> 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Important Notice: This mailbox is ignored: e-mails are set to be deleted on receipt. Please respond to the mailing list if appropriate. For those needing to send personal or professional e-mail, please use appropriate addresses.


From chocold12 at gmail.com  Wed Oct 11 06:22:36 2017
From: chocold12 at gmail.com (lily li)
Date: Tue, 10 Oct 2017 22:22:36 -0600
Subject: [R] about taylor.diagram
Message-ID: <CAN5afy_V2UQxkYZVQZaOSiHY8LCXvCzsXpxaMBPQxUokmati0A@mail.gmail.com>

Hi R users,

I don't know if you have used taylor.diagram function. Why my diagram is
not like 1/4th of a round shape, but more flat, like 1/4th of an oval?
Thanks.

	[[alternative HTML version deleted]]


From kevinluopumc at gmail.com  Wed Oct 11 02:58:38 2017
From: kevinluopumc at gmail.com (kevin luo)
Date: Wed, 11 Oct 2017 08:58:38 +0800
Subject: [R] Notes for new R version 3.4.2
Message-ID: <CAGjKR_93udpOvZsArEkJ5aOdYa8XFp34pmVChYfQunXoTEzW1Q@mail.gmail.com>

Dear officers,

Sorry to bother you.
Recently, I have installed the R version 3.4.2. But some cautions appear in
the console as the following:
Note: no visible global function definition for 'radixsort'
This note didn't affect the normal operation of some statistical packages.
BUT it does great affect the ggplot2 and the other graphics systems in my
PC.
So, what can I do to fix this bugs or is there any remedy to make things ok?

Thanks for your reading!

Best!

Kai Luo

Ph.D. candidate
Peking Union Medical College
Beijing,China
zip: 100730

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Wed Oct 11 09:08:27 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 11 Oct 2017 07:08:27 +0000
Subject: [R] "Time Series Plotting"
In-Reply-To: <CADe9EtRfq96+ETPYE+5EAK=hy=qHGgCUQ9pLN5RenjQ+b0kx+w@mail.gmail.com>
References: <CADe9EtS=ahEC=O1b4QGXxKuR2AoyZ9Y_fHXQF9yP7qNSXtJaqQ@mail.gmail.com>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB265B@SRVEXCHCM301.precheza.cz>
 <CADe9EtR4mo-hrqWDrjkcNo3NcTaPSHfmyBWBSCg1-dprNPjHRg@mail.gmail.com>
 <CADe9EtRfq96+ETPYE+5EAK=hy=qHGgCUQ9pLN5RenjQ+b0kx+w@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB273D@SRVEXCHCM301.precheza.cz>

Hi
keep your emails to R help, I do not offer private consultance and others could have different opinion how to solve your problem.
Did you even try my suggestion? If not, why not? If yes in what respect it does not comply with your expectations.
Cheers
Petr

From: niharika singhal [mailto:niharikasinghal1990 at gmail.com]
Sent: Tuesday, October 10, 2017 2:34 PM
To: PIKAL Petr <petr.pikal at precheza.cz>
Subject: Re: [R] "Time Series Plotting"

Hello,

My Energy column looks like in the image in the attachment, and my sequence column correspond to state.
I wanted to look at the graph and see the state correspond to the energy column in the Times series

Thanks & Regards
Niharika


On Tue, Oct 10, 2017 at 2:28 PM, niharika singhal <niharikasinghal1990 at gmail.com<mailto:niharikasinghal1990 at gmail.com>> wrote:
hello,

I just wanted the result in Time series thats why i am doing ts

On Tue, Oct 10, 2017 at 2:23 PM, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
Hi

I wonder why do you want to change it to ts. If I am not mistaken


plot(Data$Energy, col=Data$sequence)

or

plot(1:nrow(Data), Data$Energy, col=Data$sequence)

should do the trick.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of niharika
> singhal
> Sent: Tuesday, October 10, 2017 2:09 PM
> To: r-help at r-project.org<mailto:r-help at r-project.org>
> Subject: [R] "Time Series Plotting"
>
> Hello,
>
>
> I need some help in plotting time series.
>
>
> I have dataframe Data with two column and thousands of row, I want wherever
> the sequence corresponding to Energy column is changed the color change
> should be reflected in Time Series plot, some rows of dataframe are below
>
>
>
> Energy  sequence
>
> 13536     3
>
> 14335     3
>
> 14638     3
>
> 25363     3
>
> 18511     2
>
> 18371     2
>
> 14555     3
>
>
>
> I am able to plot time series by:-
>
> Data.energy<- ts(Data$ Energy, frequency=1)
>
> plot(Data.energy)
>
>
>
> How should I add the color effect based on the sequence column value?
>
>
>
> Thanks & Regards
>
> Niharika Singhal
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

	[[alternative HTML version deleted]]


From tring at gvdnet.dk  Wed Oct 11 09:45:51 2017
From: tring at gvdnet.dk (Troels Ring)
Date: Wed, 11 Oct 2017 09:45:51 +0200
Subject: [R] changing "," to "." in data.frame
Message-ID: <ac5e01b5-b5fd-e0bf-6466-f00579f59b7d@gvdnet.dk>

Dear friends - I have a data.frame with "," instead of "." and found the 
discussion

http://r.789695.n4.nabble.com/How-to-replace-all-commas-with-semicolon-in-a-string-tt4721187.html#a4721192

so copying the code of Ulrik(I hope:-)) I tried

(making some data)

AX <- 
data.frame(a=chartr(".",",",rnorm(5)),b=chartr(".",",",rnorm(5)),c=rnorm(5)) 


library(plyr)

adply(AX, 2, function(x){
if(!is.numeric(x[[1]]){
gsub(",", ".", x[[1]])
}else{
x[[1]]
}
})

and got the unwelcome error

Error: unexpected '{' in:
"adply(AX, 2, function(x){
if(!is.numeric(x[[1]]){"

Here is:

 > sessionInfo()
R version 3.4.1 (2017-06-30)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

Matrix products: default

locale:
[1] LC_COLLATE=Danish_Denmark.1252  LC_CTYPE=Danish_Denmark.1252
[3] LC_MONETARY=Danish_Denmark.1252 LC_NUMERIC=C
[5] LC_TIME=Danish_Denmark.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods base

other attached packages:
[1] plyr_1.8.4      TinnRcom_1.0.20 formatR_1.5 svSocket_0.9-57

loaded via a namespace (and not attached):
[1] compiler_3.4.1 tools_3.4.1    svMisc_0.9-70  Rcpp_0.12.13 tcltk_3.4.1

All best wishes

Troels Ring, Aalborg, Denmark


From petr.pikal at precheza.cz  Wed Oct 11 09:54:06 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 11 Oct 2017 07:54:06 +0000
Subject: [R] changing "," to "." in data.frame
In-Reply-To: <ac5e01b5-b5fd-e0bf-6466-f00579f59b7d@gvdnet.dk>
References: <ac5e01b5-b5fd-e0bf-6466-f00579f59b7d@gvdnet.dk>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB27BC@SRVEXCHCM301.precheza.cz>

Hi

missing ) after if statement.

adply(AX, 2, function(x){
if(!is.numeric(x[[1]])){
gsub(",", ".", x[[1]])
}else{
x[[1]]
}
})

However the data frame is transposed after the function which could be desired or not.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Troels Ring
> Sent: Wednesday, October 11, 2017 9:46 AM
> To: r-help at r-project.org
> Subject: [R] changing "," to "." in data.frame
>
> Dear friends - I have a data.frame with "," instead of "." and found the
> discussion
>
> http://r.789695.n4.nabble.com/How-to-replace-all-commas-with-semicolon-in-
> a-string-tt4721187.html#a4721192
>
> so copying the code of Ulrik(I hope:-)) I tried
>
> (making some data)
>
> AX <-
> data.frame(a=chartr(".",",",rnorm(5)),b=chartr(".",",",rnorm(5)),c=rnorm(5))
>
>
> library(plyr)
>
> adply(AX, 2, function(x){
> if(!is.numeric(x[[1]]){
> gsub(",", ".", x[[1]])
> }else{
> x[[1]]
> }
> })
>
> and got the unwelcome error
>
> Error: unexpected '{' in:
> "adply(AX, 2, function(x){
> if(!is.numeric(x[[1]]){"
>
> Here is:
>
>  > sessionInfo()
> R version 3.4.1 (2017-06-30)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 7 x64 (build 7601) Service Pack 1
>
> Matrix products: default
>
> locale:
> [1] LC_COLLATE=Danish_Denmark.1252  LC_CTYPE=Danish_Denmark.1252
> [3] LC_MONETARY=Danish_Denmark.1252 LC_NUMERIC=C
> [5] LC_TIME=Danish_Denmark.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods base
>
> other attached packages:
> [1] plyr_1.8.4      TinnRcom_1.0.20 formatR_1.5 svSocket_0.9-57
>
> loaded via a namespace (and not attached):
> [1] compiler_3.4.1 tools_3.4.1    svMisc_0.9-70  Rcpp_0.12.13 tcltk_3.4.1
>
> All best wishes
>
> Troels Ring, Aalborg, Denmark
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From bhh at xs4all.nl  Wed Oct 11 09:58:41 2017
From: bhh at xs4all.nl (Berend Hasselman)
Date: Wed, 11 Oct 2017 09:58:41 +0200
Subject: [R] changing "," to "." in data.frame
In-Reply-To: <ac5e01b5-b5fd-e0bf-6466-f00579f59b7d@gvdnet.dk>
References: <ac5e01b5-b5fd-e0bf-6466-f00579f59b7d@gvdnet.dk>
Message-ID: <2FA66D13-5CE7-4470-9069-5B06C6A70455@xs4all.nl>


> On 11 Oct 2017, at 09:45, Troels Ring <tring at gvdnet.dk> wrote:
> 
> Dear friends - I have a data.frame with "," instead of "." and found the discussion
> 
> http://r.789695.n4.nabble.com/How-to-replace-all-commas-with-semicolon-in-a-string-tt4721187.html#a4721192
> 
> so copying the code of Ulrik(I hope:-)) I tried
> 
> (making some data)
> 
> AX <- data.frame(a=chartr(".",",",rnorm(5)),b=chartr(".",",",rnorm(5)),c=rnorm(5)) 
> 
> library(plyr)
> 
> adply(AX, 2, function(x){
> if(!is.numeric(x[[1]]){
> gsub(",", ".", x[[1]])
> }else{
> x[[1]]
> }
> })
> 
> and got the unwelcome error
> 
> Error: unexpected '{' in:
> "adply(AX, 2, function(x){
> if(!is.numeric(x[[1]]){"
> 


Missing ) in if(!is.numeric(x[[1]]){

It should be 

if(!is.numeric(x[[1]]) ){

Berend Hasselman

> Here is:
> 
> > sessionInfo()
> R version 3.4.1 (2017-06-30)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 7 x64 (build 7601) Service Pack 1
> 
> Matrix products: default
> 
> locale:
> [1] LC_COLLATE=Danish_Denmark.1252  LC_CTYPE=Danish_Denmark.1252
> [3] LC_MONETARY=Danish_Denmark.1252 LC_NUMERIC=C
> [5] LC_TIME=Danish_Denmark.1252
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods base
> 
> other attached packages:
> [1] plyr_1.8.4      TinnRcom_1.0.20 formatR_1.5 svSocket_0.9-57
> 
> loaded via a namespace (and not attached):
> [1] compiler_3.4.1 tools_3.4.1    svMisc_0.9-70  Rcpp_0.12.13 tcltk_3.4.1
> 
> All best wishes
> 
> Troels Ring, Aalborg, Denmark
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Wed Oct 11 09:59:46 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 11 Oct 2017 07:59:46 +0000
Subject: [R] changing "," to "." in data.frame
In-Reply-To: <ac5e01b5-b5fd-e0bf-6466-f00579f59b7d@gvdnet.dk>
References: <ac5e01b5-b5fd-e0bf-6466-f00579f59b7d@gvdnet.dk>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB27D1@SRVEXCHCM301.precheza.cz>

And as follow up,

fff<-function(x) gsub(",", ".", x)

BX <- apply(apply(AX, 2, fff), 2, as.numeric)

this seems to be easier.

Cheers
Petr




S pozdravem | Best Regards
RNDr. Petr PIKAL
Vedouc? V?zkumu a v?voje | Research Manager
PRECHEZA a.s.
n?b?. Dr. Edvarda Bene?e 1170/24 | 750 02 P?erov | Czech Republic
Tel: +420 581 252 256 | GSM: +420 724 008 364
petr.pikal at precheza.cz | www.precheza.cz

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Troels Ring
> Sent: Wednesday, October 11, 2017 9:46 AM
> To: r-help at r-project.org
> Subject: [R] changing "," to "." in data.frame
>
> Dear friends - I have a data.frame with "," instead of "." and found the
> discussion
>
> http://r.789695.n4.nabble.com/How-to-replace-all-commas-with-semicolon-in-
> a-string-tt4721187.html#a4721192
>
> so copying the code of Ulrik(I hope:-)) I tried
>
> (making some data)
>
> AX <-
> data.frame(a=chartr(".",",",rnorm(5)),b=chartr(".",",",rnorm(5)),c=rnorm(5))
>
>
> library(plyr)
>
> adply(AX, 2, function(x){
> if(!is.numeric(x[[1]]){
> gsub(",", ".", x[[1]])
> }else{
> x[[1]]
> }
> })
>
> and got the unwelcome error
>
> Error: unexpected '{' in:
> "adply(AX, 2, function(x){
> if(!is.numeric(x[[1]]){"
>
> Here is:
>
>  > sessionInfo()
> R version 3.4.1 (2017-06-30)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 7 x64 (build 7601) Service Pack 1
>
> Matrix products: default
>
> locale:
> [1] LC_COLLATE=Danish_Denmark.1252  LC_CTYPE=Danish_Denmark.1252
> [3] LC_MONETARY=Danish_Denmark.1252 LC_NUMERIC=C
> [5] LC_TIME=Danish_Denmark.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods base
>
> other attached packages:
> [1] plyr_1.8.4      TinnRcom_1.0.20 formatR_1.5 svSocket_0.9-57
>
> loaded via a namespace (and not attached):
> [1] compiler_3.4.1 tools_3.4.1    svMisc_0.9-70  Rcpp_0.12.13 tcltk_3.4.1
>
> All best wishes
>
> Troels Ring, Aalborg, Denmark
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


From ericjberger at gmail.com  Wed Oct 11 10:03:03 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Wed, 11 Oct 2017 11:03:03 +0300
Subject: [R] changing "," to "." in data.frame
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB27D1@SRVEXCHCM301.precheza.cz>
References: <ac5e01b5-b5fd-e0bf-6466-f00579f59b7d@gvdnet.dk>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB27D1@SRVEXCHCM301.precheza.cz>
Message-ID: <CAGgJW77Y+cC2Krzg=B7vzRmKZTBH8jCHGxq0E0q2YNXUzypzfw@mail.gmail.com>

Along the lines of Petr's followup:

AXnew <- data.frame(lapply( AX, function(s) sub(",",".",s)))



On Wed, Oct 11, 2017 at 10:59 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:

> And as follow up,
>
> fff<-function(x) gsub(",", ".", x)
>
> BX <- apply(apply(AX, 2, fff), 2, as.numeric)
>
> this seems to be easier.
>
> Cheers
> Petr
>
>
>
>
> S pozdravem | Best Regards
> RNDr. Petr PIKAL
> Vedouc? V?zkumu a v?voje | Research Manager
> PRECHEZA a.s.
> n?b?. Dr. Edvarda Bene?e 1170/24 | 750 02 P?erov | Czech Republic
> Tel: +420 581 252 256 | GSM: +420 724 008 364
> petr.pikal at precheza.cz | www.precheza.cz
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Troels
> Ring
> > Sent: Wednesday, October 11, 2017 9:46 AM
> > To: r-help at r-project.org
> > Subject: [R] changing "," to "." in data.frame
> >
> > Dear friends - I have a data.frame with "," instead of "." and found the
> > discussion
> >
> > http://r.789695.n4.nabble.com/How-to-replace-all-commas-
> with-semicolon-in-
> > a-string-tt4721187.html#a4721192
> >
> > so copying the code of Ulrik(I hope:-)) I tried
> >
> > (making some data)
> >
> > AX <-
> > data.frame(a=chartr(".",",",rnorm(5)),b=chartr(".",",",
> rnorm(5)),c=rnorm(5))
> >
> >
> > library(plyr)
> >
> > adply(AX, 2, function(x){
> > if(!is.numeric(x[[1]]){
> > gsub(",", ".", x[[1]])
> > }else{
> > x[[1]]
> > }
> > })
> >
> > and got the unwelcome error
> >
> > Error: unexpected '{' in:
> > "adply(AX, 2, function(x){
> > if(!is.numeric(x[[1]]){"
> >
> > Here is:
> >
> >  > sessionInfo()
> > R version 3.4.1 (2017-06-30)
> > Platform: x86_64-w64-mingw32/x64 (64-bit)
> > Running under: Windows 7 x64 (build 7601) Service Pack 1
> >
> > Matrix products: default
> >
> > locale:
> > [1] LC_COLLATE=Danish_Denmark.1252  LC_CTYPE=Danish_Denmark.1252
> > [3] LC_MONETARY=Danish_Denmark.1252 LC_NUMERIC=C
> > [5] LC_TIME=Danish_Denmark.1252
> >
> > attached base packages:
> > [1] stats     graphics  grDevices utils     datasets  methods base
> >
> > other attached packages:
> > [1] plyr_1.8.4      TinnRcom_1.0.20 formatR_1.5 svSocket_0.9-57
> >
> > loaded via a namespace (and not attached):
> > [1] compiler_3.4.1 tools_3.4.1    svMisc_0.9-70  Rcpp_0.12.13 tcltk_3.4.1
> >
> > All best wishes
> >
> > Troels Ring, Aalborg, Denmark
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From tring at gvdnet.dk  Wed Oct 11 10:03:36 2017
From: tring at gvdnet.dk (Troels Ring)
Date: Wed, 11 Oct 2017 10:03:36 +0200
Subject: [R] changing "," to "." in data.frame
In-Reply-To: <ac5e01b5-b5fd-e0bf-6466-f00579f59b7d@gvdnet.dk>
References: <ac5e01b5-b5fd-e0bf-6466-f00579f59b7d@gvdnet.dk>
Message-ID: <10757fc7-35cb-2fc8-b8b3-9b8694998ede@gvdnet.dk>

Thanks a  lot  -

BW
Troels


Den 11-10-2017 kl. 09:58 skrev Berend Hasselman:
>> On 11 Oct 2017, at 09:45, Troels Ring <tring at gvdnet.dk> wrote:
>>
>> Dear friends - I have a data.frame with "," instead of "." and found the discussion
>>
>> http://r.789695.n4.nabble.com/How-to-replace-all-commas-with-semicolon-in-a-string-tt4721187.html#a4721192
>>
>> so copying the code of Ulrik(I hope:-)) I tried
>>
>> (making some data)
>>
>> AX <- data.frame(a=chartr(".",",",rnorm(5)),b=chartr(".",",",rnorm(5)),c=rnorm(5))
>>
>> library(plyr)
>>
>> adply(AX, 2, function(x){
>> if(!is.numeric(x[[1]]){
>> gsub(",", ".", x[[1]])
>> }else{
>> x[[1]]
>> }
>> })
>>
>> and got the unwelcome error
>>
>> Error: unexpected '{' in:
>> "adply(AX, 2, function(x){
>> if(!is.numeric(x[[1]]){"
>>
>
> Missing ) in if(!is.numeric(x[[1]]){
>
> It should be
>
> if(!is.numeric(x[[1]]) ){
>
> Berend Hasselman
>
>> Here is:
>>
>>> sessionInfo()
>> R version 3.4.1 (2017-06-30)
>> Platform: x86_64-w64-mingw32/x64 (64-bit)
>> Running under: Windows 7 x64 (build 7601) Service Pack 1
>>
>> Matrix products: default
>>
>> locale:
>> [1] LC_COLLATE=Danish_Denmark.1252  LC_CTYPE=Danish_Denmark.1252
>> [3] LC_MONETARY=Danish_Denmark.1252 LC_NUMERIC=C
>> [5] LC_TIME=Danish_Denmark.1252
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods base
>>
>> other attached packages:
>> [1] plyr_1.8.4      TinnRcom_1.0.20 formatR_1.5 svSocket_0.9-57
>>
>> loaded via a namespace (and not attached):
>> [1] compiler_3.4.1 tools_3.4.1    svMisc_0.9-70  Rcpp_0.12.13 tcltk_3.4.1
>>
>> All best wishes
>>
>> Troels Ring, Aalborg, Denmark
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From pdalgd at gmail.com  Wed Oct 11 11:55:49 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 11 Oct 2017 11:55:49 +0200
Subject: [R] Unbalanced data in split-plot analysis with aov()
In-Reply-To: <a8cc3b15-e48f-d08a-f2fd-01d222895e8f@tum.de>
References: <a8cc3b15-e48f-d08a-f2fd-01d222895e8f@tum.de>
Message-ID: <D86B47BA-2F12-42BC-AD57-71F6DEF5D480@gmail.com>


> On 11 Oct 2017, at 00:07 , Samuel Knapp <samuel.knapp at tum.de> wrote:
> 
> Dear all,
> 
> I'm analysing a split-plot experiment, where there are sometimes one or 
> two values missing. I realized that if the data is slightly unbalanced, 
> the effect of the subplot-treatment will also appear and be tested 
> against the mainplot-error term.
> 
> I replicated this with the Oats dataset from Yates (1935), contained in 
> the nlme package, where Variety is on mainplot, and nitro on subplot.
> 
>> # Oats dataset (Yates 1935) from the nlme package
>> require(nlme); data <- get(data(Oats))
>> data$nitro <- factor(data$nitro);data$Block <- 
> as.factor(as.character(data$Block))
>> nrow(data) # 6 Blocks * 4 N-levels * 3 Varieties = 72 obs -> 
> orthogonal and balanced
> [1] 72
>> # split-plot anova
>> summary(aov(yield ~ Block+Variety*nitro + Error(Block/Variety),data))
> 
> Error: Block
>       Df Sum Sq Mean Sq
> Block  5  15875    3175
> 
> Error: Block:Variety
>           Df Sum Sq Mean Sq F value Pr(>F)
> Variety    2   1786   893.2   1.485  0.272
> Residuals 10   6013   601.3
> 
> Error: Within
>               Df Sum Sq Mean Sq F value   Pr(>F)
> nitro          3  20020    6674  37.686 2.46e-12 ***
> Variety:nitro  6    322      54   0.303    0.932
> Residuals     45   7969     177
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>> reduceddata <- data[-5,] # delete one observation
>> summary(aov(yield ~ Block+Variety*nitro + 
> Error(Block/Variety),reduceddata))
> 
> Error: Block
>       Df Sum Sq Mean Sq
> Block  5  16070    3214
> 
> Error: Block:Variety
>           Df Sum Sq Mean Sq F value Pr(>F)
> Variety    2   1820   910.0   1.373  0.302
> nitro      1      1     1.0   0.001  0.970
> Residuals  9   5964   662.7
> 
> Error: Within
>               Df Sum Sq Mean Sq F value   Pr(>F)
> nitro          3  19766    6589   36.88 4.49e-12 ***
> Variety:nitro  6    333      55    0.31    0.928
> Residuals     44   7860     179
> 
> Although I fully understand, that mixed-models should be preferred for 
> non-orthogonal/unbalanced data, I think it's still valid to use the 
> aov() approach, when only 1 or 2 datapoints are missing. Also, I don't 
> really understand, why the structure of the ANOVA changes suddenly. Is 
> there any argument, I could supply to change this behaviour?

The answer to this is in the math, and not really possible to cover here. 

If things are non-orthogonal, you don't have the pretty separation of effects into error strata and one effect of that is that you can see the same effect occurring multiple time with different error terms. (In a plain block design, if not all treatments occur equally often in each block, the block means contain information about the treatment effects if block effects are considered random). 

Worse, if the balanced structure of the design is broken, the orthogonal decomposition of random effects does not correspond to the covariance structure that you think you are assuming. The consequences of this require considerable expertise to figure out. (I forget the details, but the nature is like having to assume variance of random effects to depend on block size.)

One known-valid way of dealing with a small number of missing values, is to retain the design, set the value of the missing values to something arbitrary, and include a dummy variable for each missing observation which is 1 for the position of that value and zero elsewhere. You then still get the effect of estimating the dummy coefficient in multiple strata corresponding to an extended model where the effect of the dummy is different for block means than for residuals. However, at least you then know fairly well what is going on. 

So, something like this (removing block from the fixed effects because you also had is as a random effect):

> i5 <- rep(0,72); i5[5] <- 1
> summary(aov(yield ~ i5 + nitro*Variety + Error(Block/Variety),data))

Error: Block
          Df Sum Sq Mean Sq F value  Pr(>F)   
i5         1  14163   14163   33.08 0.00453 **
Residuals  4   1713     428                   
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Error: Block:Variety
          Df Sum Sq Mean Sq F value Pr(>F)
i5         1     26    26.0   0.039  0.847
Variety    2   1809   904.7   1.365  0.304
Residuals  9   5964   662.7               

Error: Within
              Df Sum Sq Mean Sq F value   Pr(>F)    
i5             1    352     352   1.971    0.167    
nitro          3  19766    6589  36.885 4.49e-12 ***
nitro:Variety  6    333      55   0.310    0.928    
Residuals     44   7860     179                     
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

You will notice that in this analysis, you can modify the value for yield[5] at will, and still get the same output, except of course for the "i5" entries. Also, quite interestingly, this reproduces the analysis with the 5th obs removed, _provided_ that you put nitro before Variety in the model spec: 

> summary(aov(yield ~ nitro*Variety + Error(Block/Variety),data[-5,]))

Error: Block
          Df Sum Sq Mean Sq F value  Pr(>F)   
nitro      1  14357   14357   33.53 0.00442 **
Residuals  4   1713     428                   
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Error: Block:Variety
          Df Sum Sq Mean Sq F value Pr(>F)
nitro      1     11    11.5   0.017  0.898
Variety    2   1809   904.7   1.365  0.304
Residuals  9   5964   662.7               

Error: Within
              Df Sum Sq Mean Sq F value   Pr(>F)    
nitro          3  19766    6589   36.88 4.49e-12 ***
nitro:Variety  6    333      55    0.31    0.928    
Residuals     44   7860     179                     
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

I suppose this means that the nitro term adjusts the analyses in the lower error strata in the same way as the missing value indicator, but I really can't explain the effect in detail.

-pd

> 
> When I do the same with lm() and subsequent anova(), and calculate 
> F-value for Variety by hand, the estimates are still quite robust.
> 
> Best regards,
> 
> Sam
> 
> 
> -- 
> Samuel Knapp
> 
> Lehrstuhl f?r Pflanzenern?hrung
> Technische Universit?t M?nchen
> (Chair of Plant Nutrition
> Technical University of Munich)
> 
> Emil-Ramann-Strasse 2
> D-85354 Freising
> 
> Tel. +49 8161 71-3578
> samuel.knapp at tum.de
> www.researchgate.net/profile/Samuel_Knapp
> 
> -- 
> Samuel Knapp
> 
> Lehrstuhl f?r Pflanzenern?hrung
> Technische Universit?t M?nchen
> (Chair of Plant Nutrition
> Technical University of Munich)
> 
> Emil-Ramann-Strasse 2
> D-85354 Freising
> 
> Tel. +49 8161 71-3578	
> samuel.knapp at tum.de
> www.researchgate.net/profile/Samuel_Knapp
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From ligges at statistik.tu-dortmund.de  Wed Oct 11 12:24:39 2017
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Wed, 11 Oct 2017 12:24:39 +0200
Subject: [R] Notes for new R version 3.4.2
In-Reply-To: <CAGjKR_93udpOvZsArEkJ5aOdYa8XFp34pmVChYfQunXoTEzW1Q@mail.gmail.com>
References: <CAGjKR_93udpOvZsArEkJ5aOdYa8XFp34pmVChYfQunXoTEzW1Q@mail.gmail.com>
Message-ID: <1cc61f29-38aa-6eb0-a818-d921dc055fa4@statistik.tu-dortmund.de>

Sounds like you have some base packages from an old version of R. 
Perhaps copy in another library tree that comes firt on the search path?

Best,
Uwe Ligges





On 11.10.2017 02:58, kevin luo wrote:
> Dear officers,
> 
> Sorry to bother you.
> Recently, I have installed the R version 3.4.2. But some cautions appear in
> the console as the following:
> Note: no visible global function definition for 'radixsort'
> This note didn't affect the normal operation of some statistical packages.
> BUT it does great affect the ggplot2 and the other graphics systems in my
> PC.
> So, what can I do to fix this bugs or is there any remedy to make things ok?
> 
> Thanks for your reading!
> 
> Best!
> 
> Kai Luo
> 
> Ph.D. candidate
> Peking Union Medical College
> Beijing,China
> zip: 100730
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From r-subscribe at mail.ru  Wed Oct 11 12:59:19 2017
From: r-subscribe at mail.ru (=?UTF-8?B?VGVkIEJlZ2lubmVyIChSU3R1ZGlvKQ==?=)
Date: Wed, 11 Oct 2017 13:59:19 +0300
Subject: [R] =?utf-8?q?dput=28treat=29?=
In-Reply-To: <mailman.0.1507716001.54982.r-help@r-project.org>
References: <mailman.0.1507716001.54982.r-help@r-project.org>
Message-ID: <1507719559.961771669@f68.i.mail.ru>


I got advice here that I didn't understand! Can I ask to explain me the meaning of this procedure: first get the structure, and then assign it back. For what? Thanks!? (Great thanks to Moderator/Admin!)

You should learn to post in plain text and use dput to present your data structures. At your console do this
dput(treat)
# and this will appear. Copy it to your plain-text message:
structure(list(ID = 1:5, start = c(0L, 0L, 0L, 0L, 0L), stop = c(66L,
18L, 43L, 47L, 26L), censor = c(0L, 0L, 1L, 1L, 0L), sex = c(2L,
1L, 2L, 2L, 1L), age = c(1L, 2L, 3L, 3L, 4L), stage = c(3L, 4L,
3L, NA, 3L), treatment = c(1L, 2L, 1L, 2L, NA)), .Names = c("ID",
"start", "stop", "censor", "sex", "age", "stage", "treatment"
), class = "data.frame", row.names = c("1", "2", "3", "4", "5"
)):
Then we can just do this:
?treat <- structure(list(ID = 1:5, start = c(0L, 0L, 0L, 0L, 0L), stop = c(66L,
18L, 43L, 47L, 26L), censor = c(0L, 0L, 1L, 1L, 0L), sex = c(2L,
1L, 2L, 2L, 1L), age = c(1L, 2L, 3L, 3L, 4L), stage = c(3L, 4L,
3L, NA, 3L), treatment = c(1L, 2L, 1L, 2L, NA)), .Names = c("ID",
"start", "stop", "censor", "sex", "age", "stage", "treatment"
), class = "data.frame", row.names = c("1", "2", "3", "4", "5"
))

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Wed Oct 11 13:10:21 2017
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Wed, 11 Oct 2017 12:10:21 +0100
Subject: [R] dput(treat)
In-Reply-To: <1507719559.961771669@f68.i.mail.ru>
References: <mailman.0.1507716001.54982.r-help@r-project.org>
 <1507719559.961771669@f68.i.mail.ru>
Message-ID: <97275eb2-8b1f-6c74-4bb3-bdb903b0bf0c@dewey.myzen.co.uk>

Dear Ted

Comments in line

On 11/10/2017 11:59, Ted Beginner (RStudio) via R-help wrote:
> 
> I got advice here that I didn't understand! Can I ask to explain me the meaning of this procedure: first get the structure, and then assign it back. For what? Thanks!? (Great thanks to Moderator/Admin!)
> 
> You should learn to post in plain text and use dput to present your data structures. At your console do this

So this is what you do to display your data-frame in a plain text way.

> dput(treat)
> # and this will appear. Copy it to your plain-text message:
> structure(list(ID = 1:5, start = c(0L, 0L, 0L, 0L, 0L), stop = c(66L,
> 18L, 43L, 47L, 26L), censor = c(0L, 0L, 1L, 1L, 0L), sex = c(2L,
> 1L, 2L, 2L, 1L), age = c(1L, 2L, 3L, 3L, 4L), stage = c(3L, 4L,
> 3L, NA, 3L), treatment = c(1L, 2L, 1L, 2L, NA)), .Names = c("ID",
> "start", "stop", "censor", "sex", "age", "stage", "treatment"
> ), class = "data.frame", row.names = c("1", "2", "3", "4", "5"
> )):

Then this is what we, the readers, do to read in your data-frame so we 
are sure we have exactly the same as you had.

> Then we can just do this:
>  ?treat <- structure(list(ID = 1:5, start = c(0L, 0L, 0L, 0L, 0L), stop = c(66L,
> 18L, 43L, 47L, 26L), censor = c(0L, 0L, 1L, 1L, 0L), sex = c(2L,
> 1L, 2L, 2L, 1L), age = c(1L, 2L, 3L, 3L, 4L), stage = c(3L, 4L,
> 3L, NA, 3L), treatment = c(1L, 2L, 1L, 2L, NA)), .Names = c("ID",
> "start", "stop", "censor", "sex", "age", "stage", "treatment"
> ), class = "data.frame", row.names = c("1", "2", "3", "4", "5"
> ))
>

If you carry on posting in HTML it is possible that your e-mails will 
get scrambled so it would be best to post in plain text. It does not 
seem to have had a bad effect this time.

> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ---
> This email has been checked for viruses by AVG.
> http://www.avg.com
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From ruipbarradas at sapo.pt  Wed Oct 11 13:27:47 2017
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Wed, 11 Oct 2017 12:27:47 +0100
Subject: [R] dput(treat)
In-Reply-To: <1507719559.961771669@f68.i.mail.ru>
References: <mailman.0.1507716001.54982.r-help@r-project.org>
 <1507719559.961771669@f68.i.mail.ru>
Message-ID: <59DE0033.7030804@sapo.pt>

Hello,

The best way to post data is to use dput. The moderator/admin (?) did 
NOT say that you should "first get the structure, and then assign it 
back", they gave you an example of the use of dput and then what us, 
when reading your post would do with that output.

They wrote that "Then we can just do this".

We can simply assign that output to a variable and create an exact copy 
of what you have in your R session, that's why dput is so usefull.

When you post data, always post the output of dput. If it's a large 
dataset, such as a data.frame with many rows, you can post a small 
subset with

dput(head(df, 20))  # or 30 or 50

This will only give the structure of the first 20 (or 30 or 50) rows.


Hope this helps,

Rui Barradas

Em 11-10-2017 11:59, Ted Beginner (RStudio) via R-help escreveu:
>
> I got advice here that I didn't understand! Can I ask to explain me the meaning of this procedure: first get the structure, and then assign it back. For what? Thanks!  (Great thanks to Moderator/Admin!)
>
> You should learn to post in plain text and use dput to present your data structures. At your console do this
> dput(treat)
> # and this will appear. Copy it to your plain-text message:
> structure(list(ID = 1:5, start = c(0L, 0L, 0L, 0L, 0L), stop = c(66L,
> 18L, 43L, 47L, 26L), censor = c(0L, 0L, 1L, 1L, 0L), sex = c(2L,
> 1L, 2L, 2L, 1L), age = c(1L, 2L, 3L, 3L, 4L), stage = c(3L, 4L,
> 3L, NA, 3L), treatment = c(1L, 2L, 1L, 2L, NA)), .Names = c("ID",
> "start", "stop", "censor", "sex", "age", "stage", "treatment"
> ), class = "data.frame", row.names = c("1", "2", "3", "4", "5"
> )):
> Then we can just do this:
>   treat <- structure(list(ID = 1:5, start = c(0L, 0L, 0L, 0L, 0L), stop = c(66L,
> 18L, 43L, 47L, 26L), censor = c(0L, 0L, 1L, 1L, 0L), sex = c(2L,
> 1L, 2L, 2L, 1L), age = c(1L, 2L, 3L, 3L, 4L), stage = c(3L, 4L,
> 3L, NA, 3L), treatment = c(1L, 2L, 1L, 2L, NA)), .Names = c("ID",
> "start", "stop", "censor", "sex", "age", "stage", "treatment"
> ), class = "data.frame", row.names = c("1", "2", "3", "4", "5"
> ))
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From asafaneh at yahoo.com  Wed Oct 11 13:55:35 2017
From: asafaneh at yahoo.com (Ashraf Afana)
Date: Wed, 11 Oct 2017 11:55:35 +0000 (UTC)
Subject: [R] Iterate in a Spatial Polygon Dataframe in R?
References: <1549344805.9868903.1507722935102.ref@mail.yahoo.com>
Message-ID: <1549344805.9868903.1507722935102@mail.yahoo.com>


Hi all,

?I'm trying to iterate in a SpatialPolygonDataFrame thatcontains 110 features. I tried to use the following code?
iterate.spdf =function(x){ for (i in 1:nrow(x)){ p = x[i, ]} return(p)}
but with no success as it returns with aSpatialPolygonDataFrame that contains onlyone feature. The code seems to overwrite the polygons to end up with only onefeature.
?Any suggestions?Ashraf, cheers


	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Wed Oct 11 16:12:47 2017
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 11 Oct 2017 14:12:47 +0000
Subject: [R] changing "," to "." in data.frame
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB27D1@SRVEXCHCM301.precheza.cz>
References: <ac5e01b5-b5fd-e0bf-6466-f00579f59b7d@gvdnet.dk>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB27D1@SRVEXCHCM301.precheza.cz>
Message-ID: <33aac76591e541fcbfd2fcf2d474657a@exch-2p-mbx-w2.ads.tamu.edu>

Minor modification:

fff <- function(x) as.numeric(chartr(",", ".", x))
BX <- sapply(AX, fff)

Or this keeps the original data frame:

AX[, 1:2] <- sapply(AX[, 1:2], fff)

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of PIKAL Petr
Sent: Wednesday, October 11, 2017 3:00 AM
To: Troels Ring <tring at gvdnet.dk>; r-help at r-project.org
Subject: Re: [R] changing "," to "." in data.frame

And as follow up,

fff<-function(x) gsub(",", ".", x)

BX <- apply(apply(AX, 2, fff), 2, as.numeric)

this seems to be easier.

Cheers
Petr




S pozdravem | Best Regards
RNDr. Petr PIKAL
Vedouc? V?zkumu a v?voje | Research Manager PRECHEZA a.s.
n?b?. Dr. Edvarda Bene?e 1170/24 | 750 02 P?erov | Czech Republic
Tel: +420 581 252 256 | GSM: +420 724 008 364 petr.pikal at precheza.cz | www.precheza.cz

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Troels 
> Ring
> Sent: Wednesday, October 11, 2017 9:46 AM
> To: r-help at r-project.org
> Subject: [R] changing "," to "." in data.frame
>
> Dear friends - I have a data.frame with "," instead of "." and found 
> the discussion
>
> http://r.789695.n4.nabble.com/How-to-replace-all-commas-with-semicolon
> -in-
> a-string-tt4721187.html#a4721192
>
> so copying the code of Ulrik(I hope:-)) I tried
>
> (making some data)
>
> AX <-
> data.frame(a=chartr(".",",",rnorm(5)),b=chartr(".",",",rnorm(5)),c=rno
> rm(5))
>
>
> library(plyr)
>
> adply(AX, 2, function(x){
> if(!is.numeric(x[[1]]){
> gsub(",", ".", x[[1]])
> }else{
> x[[1]]
> }
> })
>
> and got the unwelcome error
>
> Error: unexpected '{' in:
> "adply(AX, 2, function(x){
> if(!is.numeric(x[[1]]){"
>
> Here is:
>
>  > sessionInfo()
> R version 3.4.1 (2017-06-30)
> Platform: x86_64-w64-mingw32/x64 (64-bit) Running under: Windows 7 x64 
> (build 7601) Service Pack 1
>
> Matrix products: default
>
> locale:
> [1] LC_COLLATE=Danish_Denmark.1252  LC_CTYPE=Danish_Denmark.1252 [3]
> LC_MONETARY=Danish_Denmark.1252 LC_NUMERIC=C [5] 
> LC_TIME=Danish_Denmark.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods base
>
> other attached packages:
> [1] plyr_1.8.4      TinnRcom_1.0.20 formatR_1.5 svSocket_0.9-57
>
> loaded via a namespace (and not attached):
> [1] compiler_3.4.1 tools_3.4.1    svMisc_0.9-70  Rcpp_0.12.13 tcltk_3.4.1
>
> All best wishes
>
> Troels Ring, Aalborg, Denmark
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Jon.SKOIEN at ec.europa.eu  Wed Oct 11 17:37:52 2017
From: Jon.SKOIEN at ec.europa.eu (Jon.SKOIEN at ec.europa.eu)
Date: Wed, 11 Oct 2017 15:37:52 +0000
Subject: [R] Iterate in a Spatial Polygon Dataframe in R?
In-Reply-To: <1549344805.9868903.1507722935102@mail.yahoo.com>
References: <1549344805.9868903.1507722935102.ref@mail.yahoo.com>,
 <1549344805.9868903.1507722935102@mail.yahoo.com>
Message-ID: <548D52560D5BDE45AEC4EF876BF2A1940113B1B9@S-DC-ESTB01-J.net1.cec.eu.int>


Ashraf,

What do you want your function to do?
Currently you are overwriting p each time you go through your for-loop, and you finally return p = x[nrow(x), ]

It is similar to

y = 101:200
for (i in 1:100) {
  x = y[i]
}
x


Best,
Jon


--
Jon Olav Sk?ien
European Commission
Joint Research Centre ? JRC.E.1
Disaster Risk Management Unit
Building 26b 1/144 | Via E.Fermi 2749, I-21027 Ispra (VA) Italy, TP 267
Disclaimer: Views expressed in this email are those of the individual and do not necessarily represent official views of the European Commission.


________________________________________
From: R-help [r-help-bounces at r-project.org] on behalf of Ashraf Afana via R-help [r-help at r-project.org]
Sent: 11 October 2017 13:55
To: r-help at r-project.org
Subject: [R] Iterate in a Spatial Polygon Dataframe in R?

Hi all,

 I'm trying to iterate in a SpatialPolygonDataFrame thatcontains 110 features. I tried to use the following code
iterate.spdf =function(x){ for (i in 1:nrow(x)){ p = x[i, ]} return(p)}
but with no success as it returns with aSpatialPolygonDataFrame that contains onlyone feature. The code seems to overwrite the polygons to end up with only onefeature.
 Any suggestions?Ashraf, cheers


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Wed Oct 11 17:45:54 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 11 Oct 2017 08:45:54 -0700
Subject: [R] about taylor.diagram
In-Reply-To: <CAN5afy_V2UQxkYZVQZaOSiHY8LCXvCzsXpxaMBPQxUokmati0A@mail.gmail.com>
References: <CAN5afy_V2UQxkYZVQZaOSiHY8LCXvCzsXpxaMBPQxUokmati0A@mail.gmail.com>
Message-ID: <7107CDAF-56C9-4D5F-9F15-41AFC45A683C@comcast.net>


> On Oct 10, 2017, at 9:22 PM, lily li <chocold12 at gmail.com> wrote:
> 
> Hi R users,
> 
> I don't know if you have used taylor.diagram function. Why my diagram is
> not like 1/4th of a round shape, but more flat, like 1/4th of an oval?
> Thanks.

Posters to rhelp are asked in the posting guide to include a call to the library function at the start of an example involving a function that will not be in the base packages. And they are asked to include code that creates a dataset suitable for passing to that function. My personal opinion is htat the audience shoudl ignore postings that fail to do this.


> 
> 	[[alternative HTML version deleted]]

And HTML mail is not the proper mode for sending postings.

> PLEASE,PLEASE, 
> PLEASE,PLEASE
>  ....do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From peter.wight17 at gmail.com  Wed Oct 11 16:48:23 2017
From: peter.wight17 at gmail.com (Peter Wight)
Date: Wed, 11 Oct 2017 10:48:23 -0400
Subject: [R] issues with the WD
Message-ID: <CAEG5CHvqdXLazrNWoHDPcB=jERh99Fuoc-t18ErC4GiWOZOr7A@mail.gmail.com>

ood morning,

I am reaching out to discus an issue I am having with setting a working
directing.

I have tried github, stack overload, setwd, getwd, all to no avail. I am
used to setting a working directory in 3.1.1 and have yet to get 3.4.1 to
work properly.

Let me know if there is anyone who I can talk to about this.

Thank you so much,

My best,

Peter Wight, MPA

	[[alternative HTML version deleted]]


From andrewharmon42 at gmail.com  Wed Oct 11 20:33:24 2017
From: andrewharmon42 at gmail.com (Andrew Harmon)
Date: Wed, 11 Oct 2017 13:33:24 -0500
Subject: [R] Converting SAS Code
In-Reply-To: <89F13DAC-95BB-4D81-B1B1-B511B25A529D@gmail.com>
References: <CAB4=Ug=V22G+J2bgEwf0Cd0LnKnb2DKCa5K47=sdk5eE8jtZgw@mail.gmail.com>
 <872874c5-21e4-47f0-5627-96f1a3c129cc@dewey.myzen.co.uk>
 <e529f831-92b1-5cf4-8d33-ba41da80f173@utoronto.ca>
 <CAGxFJbTa_4jCdhNVZ1SP3W_N_5Pgx_e6VUc7bWxiND_zGeXp2Q@mail.gmail.com>
 <OF3C6E5C21.F89996C3-ON852581AA.0066237F-852581AA.006711B9@ria.buffalo.edu>
 <e63c816a-d27a-6ecd-29e6-ea79d7fa34b3@auckland.ac.nz>
 <598ed67a-d3c0-5cb3-e242-bc04948919eb@atsu.edu>
 <89F13DAC-95BB-4D81-B1B1-B511B25A529D@gmail.com>
Message-ID: <CAB4=Ugknb1Qz=SiwPniT=dD2RgnN+vmJ3C7K9Bs9iTTwj7LMYw@mail.gmail.com>

I have no problem setting up my mixed model, or performing anova or lsmeans
on my model?s outputs. However, performing lsd mean separation is giving me
fits.



So I do not have a problem when using two-way anova model. When using the
code:

fit.yield.add <- lm(data = ryzup, Yield ~ Rep + Nitrogen + Treatment)

LSD.test(fit.yield.add, trt = "Nitrogen", alpha = 0.1, console = TRUE)



It works beautifully. However when I run a mixed model:



yield.lmer <- lmer(data = ryzup, Yield ~ Nitrogen + Treatment +
(1|Rep/Nitrogen), REML = FALSE)

LSD.test(yield.lmer, trt = "Nitrogen", alpha = 0.1, console = TRUE)



It gives me fits. It produces errors like:



Error in as.data.frame.default(x[[i]], optional = TRUE) :

  cannot coerce class "structure("merModLmerTest", package = "lmerTest")"
to a data.frame


Do you have any suggestions for that?

Thanks

	[[alternative HTML version deleted]]


From mmalten at gmail.com  Wed Oct 11 21:32:40 2017
From: mmalten at gmail.com (Mitchell Maltenfort)
Date: Wed, 11 Oct 2017 15:32:40 -0400
Subject: [R] Converting SAS Code
In-Reply-To: <CAB4=Ugknb1Qz=SiwPniT=dD2RgnN+vmJ3C7K9Bs9iTTwj7LMYw@mail.gmail.com>
References: <CAB4=Ug=V22G+J2bgEwf0Cd0LnKnb2DKCa5K47=sdk5eE8jtZgw@mail.gmail.com>
 <872874c5-21e4-47f0-5627-96f1a3c129cc@dewey.myzen.co.uk>
 <e529f831-92b1-5cf4-8d33-ba41da80f173@utoronto.ca>
 <CAGxFJbTa_4jCdhNVZ1SP3W_N_5Pgx_e6VUc7bWxiND_zGeXp2Q@mail.gmail.com>
 <OF3C6E5C21.F89996C3-ON852581AA.0066237F-852581AA.006711B9@ria.buffalo.edu>
 <e63c816a-d27a-6ecd-29e6-ea79d7fa34b3@auckland.ac.nz>
 <598ed67a-d3c0-5cb3-e242-bc04948919eb@atsu.edu>
 <89F13DAC-95BB-4D81-B1B1-B511B25A529D@gmail.com>
 <CAB4=Ugknb1Qz=SiwPniT=dD2RgnN+vmJ3C7K9Bs9iTTwj7LMYw@mail.gmail.com>
Message-ID: <CANOgrHZ1bMDgb_-qLm2ppy3J5cQz8T82=phkA4rybEWP8xW2rg@mail.gmail.com>

I believe the lmerTest package's "difflsmeans" is what you need.

On Wed, Oct 11, 2017 at 2:33 PM, Andrew Harmon <andrewharmon42 at gmail.com>
wrote:

> I have no problem setting up my mixed model, or performing anova or lsmeans
> on my model?s outputs. However, performing lsd mean separation is giving me
> fits.
>
>
>
> So I do not have a problem when using two-way anova model. When using the
> code:
>
> fit.yield.add <- lm(data = ryzup, Yield ~ Rep + Nitrogen + Treatment)
>
> LSD.test(fit.yield.add, trt = "Nitrogen", alpha = 0.1, console = TRUE)
>
>
>
> It works beautifully. However when I run a mixed model:
>
>
>
> yield.lmer <- lmer(data = ryzup, Yield ~ Nitrogen + Treatment +
> (1|Rep/Nitrogen), REML = FALSE)
>
> LSD.test(yield.lmer, trt = "Nitrogen", alpha = 0.1, console = TRUE)
>
>
>
> It gives me fits. It produces errors like:
>
>
>
> Error in as.data.frame.default(x[[i]], optional = TRUE) :
>
>   cannot coerce class "structure("merModLmerTest", package = "lmerTest")"
> to a data.frame
>
>
> Do you have any suggestions for that?
>
> Thanks
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Wed Oct 11 22:47:03 2017
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 12 Oct 2017 07:47:03 +1100
Subject: [R] about taylor.diagram
In-Reply-To: <CAN5afy_V2UQxkYZVQZaOSiHY8LCXvCzsXpxaMBPQxUokmati0A@mail.gmail.com>
References: <CAN5afy_V2UQxkYZVQZaOSiHY8LCXvCzsXpxaMBPQxUokmati0A@mail.gmail.com>
Message-ID: <CA+8X3fXXQOpoD8vTFXdFjqax-S3mF2es8WBSon0tcxm_xzQCLw@mail.gmail.com>

Hi lily,
I assume that you are using the taylor.diagram function from the
plotrix package. You have most likely started a plot window that is
not square or specified margins that distort the plot. As your image
did not come through, we can't see what has gone wrong.

Jim

On Wed, Oct 11, 2017 at 3:22 PM, lily li <chocold12 at gmail.com> wrote:
> Hi R users,
>
> I don't know if you have used taylor.diagram function. Why my diagram is
> not like 1/4th of a round shape, but more flat, like 1/4th of an oval?
> Thanks.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Wed Oct 11 23:54:49 2017
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 12 Oct 2017 08:54:49 +1100
Subject: [R] RFM analysis
In-Reply-To: <CAJL6Qs_t1Bs=+tmwk=Lrby_4tC8c7kxysymDmsGQ-cT7eisbdw@mail.gmail.com>
References: <CAJL6Qs_UG_YuukhBq+QCKPvaNpHaKZR9dNMvSOMKKH8VzFmdjQ@mail.gmail.com>
 <CAJL6Qs_t1Bs=+tmwk=Lrby_4tC8c7kxysymDmsGQ-cT7eisbdw@mail.gmail.com>
Message-ID: <CA+8X3fW8fXv_p4y-7vkWmtqWwFBm3jd7kRN1vKCv5wF-oeveJA@mail.gmail.com>

Hi Hemant,
Let's take it one step at a time. Save this code as "qdrfm.R" in your
R working directory: It includes the comments I added last time and
fixes a bug in the recency scoring.

qdrfm<-function(x,rbreaks=3,fbreaks=3,mbreaks=3,
 date.format="%Y-%m-%d",weights=c(1,1,1),finish=NA) {

 # if no finish date is specified, use current date
 if(is.na(finish)) finish<-as.Date(date(), "%a %b %d %H:%M:%S %Y")
 x$rscore<-as.numeric(finish-as.Date(x[,3],date.format))
 cat("Range of purchase recency",range(x$rscore),"\n")
 cat("Range of purchase freqency",range(table(x[,1])),"\n")
 cat("Range of purchase amount",range(by(x[,2],x[,1],sum)),"\n")
 custIDs<-unique(x[,1])
 ncust<-length(custIDs)
 # initialize a data frame to hold the output
 rfmout<-data.frame(custID=custIDs,rscore=rep(0,ncust),
  fscore=rep(0,ncust),mscore=rep(0,ncust))
 # categorize the minimum number of days
 # since last purchase for each customer
 rfmout$rscore<-cut(by(x$rscore,x[,1],min),breaks=rbreaks,labels=FALSE)
 # categorize the number of purchases
 # recorded for each customer
 rfmout$fscore<-cut(table(x[,1]),breaks=fbreaks,labels=FALSE)
 # categorize the amount purchased
 # by each customer
 rfmout$mscore<-cut(by(x[,2],x[,1],sum),breaks=mbreaks,labels=FALSE)
 # calculate the RFM score from the
 # optionally weighted average of the above
 rfmout$cscore<-round((weights[1]*rfmout$rscore+
  weights[2]*rfmout$fscore+
  weights[3]*rfmout$mscore)/sum(weights),2)
 return(rfmout[order(rfmout$cscore),])
}

Now you can load the function into your workspace like this:

source("qdrfm.R")

Load your data:

df<-read.csv("df.csv")

Run the function with the defaults except for the finish date:

df.rfm<-qdrfm(df,finish=as.Date("2017-08-31"))
Range of purchase recency 31 122
Range of purchase freqency 1 4
Range of purchase amount 5.97 127.65

Your problem is now apparent. If I use the following breaks, I will
generate NA values in all three scores:

df.rfm2<-qdrfm(df,rbreaks=c(10,30,50),fbreaks=c(1,2,3),
 mbreaks=c(8,14,400),finish=as.Date("2017-08-31"))
head(df.rfm2)

As I wrote before, the breaks _must_ cover the range of values if you
want a sensible analysis:

df.rfm3<-qdrfm(df,rbreaks=c(0,75,150),fbreaks=c(0,2,5),
 mbreaks=c(0,75,150),finish=as.Date("2017-08-31"))
head(df.rfm3)

Looking at df.rfm3, it seems that the recency score is the only one
discriminating users. This suggests to me that the data distributions
are causing a problem.  First, you have 946 users in a dataset of 1000
rows, meaning that almost all made only one transaction. Second, your
purchase amounts are concentrated in the 0-20 range. Therefore if I
change the breaks to reflect this, I get a much better separation of
customers:

df.rfm4<-qdrfm(df,rbreaks=c(0,75,150),fbreaks=c(0,1,5),
 mbreaks=c(0,10,150),finish=as.Date("2017-08-31"))

Maybe this will get you going.

Jim

On Wed, Oct 11, 2017 at 4:43 PM, Hemant Sain <hemantsain55 at gmail.com> wrote:
> Also try to put finish date as 2017-08-31.
> and help me with the complete running r code.
>
> On 11 October 2017 at 10:36, Hemant Sain <hemantsain55 at gmail.com> wrote:
>>
>> Hey Jim,
>> i'm attaching you the actual dataset i'm working on and i want RFM breaks
>> as
>> r=(10,30,50), f=(1,2,3),m=(8,14,400).
>>


From drjimlemon at gmail.com  Thu Oct 12 00:03:32 2017
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 12 Oct 2017 09:03:32 +1100
Subject: [R] issues with the WD
In-Reply-To: <CAEG5CHvqdXLazrNWoHDPcB=jERh99Fuoc-t18ErC4GiWOZOr7A@mail.gmail.com>
References: <CAEG5CHvqdXLazrNWoHDPcB=jERh99Fuoc-t18ErC4GiWOZOr7A@mail.gmail.com>
Message-ID: <CA+8X3fXwdWOQ3C8dormNACyfqV0EQvbJNthFi4k=xwygu-6A=g@mail.gmail.com>

Hi Peter,
What is the command that you used in R-3.1.1? If this is embedded in
code, do you still have the same directory structure that you had
then?

Jim


On Thu, Oct 12, 2017 at 1:48 AM, Peter Wight <peter.wight17 at gmail.com> wrote:
> ood morning,
>
> I am reaching out to discus an issue I am having with setting a working
> directing.
>
> I have tried github, stack overload, setwd, getwd, all to no avail. I am
> used to setting a working directory in 3.1.1 and have yet to get 3.4.1 to
> work properly.
>
> Let me know if there is anyone who I can talk to about this.
>
> Thank you so much,
>
> My best,
>
> Peter Wight, MPA
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From roslinaump at gmail.com  Thu Oct 12 06:26:37 2017
From: roslinaump at gmail.com (roslinazairimah zakaria)
Date: Thu, 12 Oct 2017 12:26:37 +0800
Subject: [R] dput(treat)
In-Reply-To: <1507719559.961771669@f68.i.mail.ru>
References: <mailman.0.1507716001.54982.r-help@r-project.org>
 <1507719559.961771669@f68.i.mail.ru>
Message-ID: <CANTvJZJOpw=-vj7aih+RKqTz1t3j_rYXnPNP112TNYXqOXLwMw@mail.gmail.com>

Hi,

If you have a data, run dput(data), then in the console you will see the
output.  Copy the output and place it in your email.

HTH.

On Wed, Oct 11, 2017 at 6:59 PM, Ted Beginner (RStudio) via R-help <
r-help at r-project.org> wrote:

>
> I got advice here that I didn't understand! Can I ask to explain me the
> meaning of this procedure: first get the structure, and then assign it
> back. For what? Thanks!  (Great thanks to Moderator/Admin!)
>
> You should learn to post in plain text and use dput to present your data
> structures. At your console do this
> dput(treat)
> # and this will appear. Copy it to your plain-text message:
> structure(list(ID = 1:5, start = c(0L, 0L, 0L, 0L, 0L), stop = c(66L,
> 18L, 43L, 47L, 26L), censor = c(0L, 0L, 1L, 1L, 0L), sex = c(2L,
> 1L, 2L, 2L, 1L), age = c(1L, 2L, 3L, 3L, 4L), stage = c(3L, 4L,
> 3L, NA, 3L), treatment = c(1L, 2L, 1L, 2L, NA)), .Names = c("ID",
> "start", "stop", "censor", "sex", "age", "stage", "treatment"
> ), class = "data.frame", row.names = c("1", "2", "3", "4", "5"
> )):
> Then we can just do this:
>  treat <- structure(list(ID = 1:5, start = c(0L, 0L, 0L, 0L, 0L), stop =
> c(66L,
> 18L, 43L, 47L, 26L), censor = c(0L, 0L, 1L, 1L, 0L), sex = c(2L,
> 1L, 2L, 2L, 1L), age = c(1L, 2L, 3L, 3L, 4L), stage = c(3L, 4L,
> 3L, NA, 3L), treatment = c(1L, 2L, 1L, 2L, NA)), .Names = c("ID",
> "start", "stop", "censor", "sex", "age", "stage", "treatment"
> ), class = "data.frame", row.names = c("1", "2", "3", "4", "5"
> ))
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.




-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From miaojpm at gmail.com  Thu Oct 12 09:06:40 2017
From: miaojpm at gmail.com (John)
Date: Thu, 12 Oct 2017 00:06:40 -0700
Subject: [R] dual y-axis for ggplot
Message-ID: <CABcx46CJ+_sQKVKh09Q6cLtHZS=Zbd81WmeuVYJ7jum1jKfA8g@mail.gmail.com>

Hi,

   To my knowledge, an excellent of ggplot with a second y-axis is

https://rpubs.com/MarkusLoew/226759

   In this example, the author uses two colors for the two lines, but the
line shapes are the same -- both are solid. Could each line have its own
color as well as its own shape? For example, can I make the red line with
the linetype "twodash", while the blue line with the linetype "solid"?
   For convenience, I copied the codes as follows.

########
p <- ggplot(obs, aes(x = Timestamp))
  p <- p + geom_line(aes(y = air_temp, colour = "Temperature"))

  # adding the relative humidity data, transformed to match roughly the
range of the temperature
  p <- p + geom_line(aes(y = rel_hum/5, colour = "Humidity"))

  # now adding the secondary axis, following the example in the help file
?scale_y_continuous
  # and, very important, reverting the above transformation
  p <- p + scale_y_continuous(sec.axis = sec_axis(~.*5, name = "Relative
humidity [%]"))

  # modifying colours and theme options
  p <- p + scale_colour_manual(values = c("blue", "red"))
  p <- p + labs(y = "Air temperature [?C]",
                x = "Date and time",
                colour = "Parameter")
  p <- p + theme(legend.position = c(0.8, 0.9))
p
########

   Thanks,

John

	[[alternative HTML version deleted]]


From miaojpm at gmail.com  Thu Oct 12 09:14:00 2017
From: miaojpm at gmail.com (John)
Date: Thu, 12 Oct 2017 00:14:00 -0700
Subject: [R] dual y-axis for ggplot
In-Reply-To: <CABcx46CJ+_sQKVKh09Q6cLtHZS=Zbd81WmeuVYJ7jum1jKfA8g@mail.gmail.com>
References: <CABcx46CJ+_sQKVKh09Q6cLtHZS=Zbd81WmeuVYJ7jum1jKfA8g@mail.gmail.com>
Message-ID: <CABcx46DUBxoMYOLzwcTeh7CR6DtKT513cp2_84EoM3kX33BNnA@mail.gmail.com>

Sorry let me clarify.
If I modify the line
 p <- p + geom_line(aes(y = air_temp, colour = "Temperature"))
by
p <- p + geom_line(aes(y = air_temp, colour = "Temperature", linetype
="Temperature"))
and
p <- p + geom_line(aes(y = rel_hum/5, colour = "Humidity"))
by
p <- p + geom_line(aes(y = rel_hum/5, colour = "Humidity",
linetype="Humidity"))
and
p <- p + scale_linetype_manual(values = c("twodash", "solid")),

I will have two lines with different color and different line type,

but I will have two legends  (one with blue/red,solid, the other with two
dash/solid, black ).

How can I have only one legend with blue/two dash and red/solid?




2017-10-12 0:06 GMT-07:00 John <miaojpm at gmail.com>:

> Hi,
>
>    To my knowledge, an excellent of ggplot with a second y-axis is
>
> https://rpubs.com/MarkusLoew/226759
>
>    In this example, the author uses two colors for the two lines, but the
> line shapes are the same -- both are solid. Could each line have its own
> color as well as its own shape? For example, can I make the red line with
> the linetype "twodash", while the blue line with the linetype "solid"?
>    For convenience, I copied the codes as follows.
>
> ########
> p <- ggplot(obs, aes(x = Timestamp))
>   p <- p + geom_line(aes(y = air_temp, colour = "Temperature"))
>
>   # adding the relative humidity data, transformed to match roughly the
> range of the temperature
>   p <- p + geom_line(aes(y = rel_hum/5, colour = "Humidity"))
>
>   # now adding the secondary axis, following the example in the help file
> ?scale_y_continuous
>   # and, very important, reverting the above transformation
>   p <- p + scale_y_continuous(sec.axis = sec_axis(~.*5, name = "Relative
> humidity [%]"))
>
>   # modifying colours and theme options
>   p <- p + scale_colour_manual(values = c("blue", "red"))
>   p <- p + labs(y = "Air temperature [?C]",
>                 x = "Date and time",
>                 colour = "Parameter")
>   p <- p + theme(legend.position = c(0.8, 0.9))
> p
> ########
>
>    Thanks,
>
> John
>

	[[alternative HTML version deleted]]


From hemantsain55 at gmail.com  Thu Oct 12 10:17:52 2017
From: hemantsain55 at gmail.com (Hemant Sain)
Date: Thu, 12 Oct 2017 13:47:52 +0530
Subject: [R] How to define proper breaks in RFM analysis
Message-ID: <CAJL6Qs8tPRTVFi7h4Pwz9i55zPJx+K+1geF4E=S=SJ1Ke_LUjQ@mail.gmail.com>

Hello,
I'm working on RFM analysis and i wanted to define my own breaks but my
frequency distribution is not normally distributed so when I'm using
quartile its not giving the optimal results.
so I'm looking for a better approach where i can define breaks dynamically
because after visualization i can do it easily but i want to apply this
model so that it can automatically define the breaks according to data set.
I'm attaching sample data for reference.

Thanks

                           *Freq*
5
15
1
8
2
2
2
1
1
2
2
1
1
1
1
4
10
22
24
1
1
2
2
7
1
1
4
6
1
1
2
2
17
2
7
1
5
2
7
7
1
4
13
4
1
9
1
1
1
3
2
1
1
2
4
5
8
4
14
18
4
1
1
1
7
1
1
1
1
10
6
4
6
1
3
13
2
4
8
11
2
4
6
1
1
3
1
2
3
2
2
5
2
1
1
1
3
2
3
1
2
9
34
8
1
1
1
1
2
1
5
4
9
1
1
4
2
2
1
1
2
25
5
2
1
1
1
6
1
36
3
4
5
3
3
13
2
1
4
1
1
9
7
1
1
15
1
1
2
5
7
3
3
1
8
7
5
1
1
5
14
3
2
2
1
5
3
4
3
1
2
9
2
15
2
1
3
56
3
2
4
2
26
3
1
4
1
1
12
1
18
7
6
7
2
3
2
1
4
15
10
7
5
3
6
4
9
1
2
2
2
3
1
1
8
3
1
2
3
1
10
3
1
3
5
1
8
3
2
3
7
1
5
2
1
1
6
2
1
9
1
20
2
1
4
21
5
4
4
1
1
1
11
7
4
6
1
3
3
12
4
7
4
3
3
1
2
10
5
11
3
2
1
3
30
1
4
5
1
7
3
1
3
9
2
2
14
10
1
1
1
1
5
1
1
18
32
1
4
5
4
3
2
9
2
6
3
2
2
2
2
2
2
4
4
3
1
1
2
4
6
1
1
1
2
2
1
1
3
1
1
3
1
2
3
2
9
4
1
2
4
3
3
3
1
2
1
3
4
3
1
3
1
5
14
7
1
1
1
1
4
3
4
5
8
1
10
3
2
7
5
4
5
7
4
4
10
3
7
12
1
1
3
1
6
1
5
9
2
1
3
4
3
2
2
5
1
4
6
5
3
1
1
6
3
1
1
2
1
5
5
2
1
2
5
1
2
1
6
5
5
3
2
4
8
1
2
5
1
1
1
4
1
4
1
2
6
3
4
4
2
2
2
2
1
1
2
5
2
2
1
5
2
2
1
2
4
1
3
3
1
3
4
2
2
3
1
4
1
1
6
6
2
4
5
2
1
8
1
2
1
1
3
4
3
3
2
2
1
4
1
5
1
4
1
1
2
1
1
1
8
1
1
1
1
1
3
3
5
2
3
1
1
5
2
3
6
3
3
14
2
1
1
2
1
2
4
2
1
6
1
7
2
3
3
2
2
2
2
1
2
4
1
6
2
5
2
1
2
2
5
8
4
1
1
1
1
4
1
3
2
1
2
2
3
3
3
6
1
1
1
5
7
1
5
2
1
1
1
3
20
2
3
3
1
2
1
15
4
4
1
1
2
1
1
3
2
6
5
1
5
1
7
4
3
2
5
2
1
1
3
2
6
2
4
2
1
24
4
17
1
3
2
2
2
2
8
1
3
1
9
2
4
1
1
6
3
4
1
9
2
1
3
2
6
2
1
3
1
20
3
4
7
3
7
1
7
1
5
2
1
1
1
2
1
2
4
1
1
2
3
4
1
4
1
1
1
1
1
2
1
6
2
3
2
1
9
8
11
1
1
2
1
3
1
2
15
5
2
2
9
1
4
2
1
1
14
1
1
2
1
3
10
1
1
3
1
1
1
4
2
8
1
2
2
1
11
1
3
1
7
1
3
10
3
3
1
1
1
11
1
2
1
1
2
2
5
5
4
1
2
5
2
4
3
1
1
2
2
4
2
1
1
4
1
4
1
5
1
1
2
1
4
7
1
2
1
2
9
3
1
7
2
2
1
2
3
5
2
7
1
5
1
1
2
2
2
4
2
8
4
6
1
1
1
1
1
16
2
1
6
4
4
1
1
1
1
4
3
2
6
11
10
21
2
1
1
3
2
2
2
7
1
6
4
1
7
4
11
1
2
8
1
1
1
1
2
17
1
2
3
4
2
1
2
2
4
3
2
3
1
3
3
1
3
37
4
3
1
2
1
1
3
1
2
3
1
5
1
2
1
3
2
3
3
4
4
2
4
1
1
3
3
2
1
2
1
1
3
1
1
3
3
4
2
4
1
1
1
10
3
2
2
2
2
2
2
1
19
2
2
4
1
3
1
13
5
2
1
2
2
4
2
1
3
5
1
1
1
6
3
9
4
1
1
1
3
1
17
4
1
4
6
2
1
2
4
4
2
2
2
4
4
1
1
1
2
7
2
1
2
8
1
1
7
4
1
2
1
1
3
1
3
1
1
3
5
8
1
1
4
1
1
15
1
1
6
2
3
3
8
4
1
4
2
4
2
5
4
4
1
1
7
1
10
1
10
2
15
7
3
1
3
2
19
2
9
1
10
2
1
2
2
4
6
1
4
3
1
2
3
2
1
3
7
1
4
2
13
2
5
3
6
2
1
2
1
5
1


-- 
hemantsain.com

	[[alternative HTML version deleted]]


From miaojpm at gmail.com  Thu Oct 12 15:12:49 2017
From: miaojpm at gmail.com (John)
Date: Thu, 12 Oct 2017 06:12:49 -0700
Subject: [R] can't print ggplot with Chinese characters to pdf files
Message-ID: <CABcx46A4OL5+Ja-e0eAr=7iF2_obaCOZh4a7vHMLcqkZNpD3MQ@mail.gmail.com>

I install the Chinese font "Kaiti TC" on my mac, but I can't print the
figures to pdf file by "marrangeGrob" command, which is in the package
"gridExtra". Error message after I type "ggsave(......)" (last line of the
program):

"Saving 7.47 x 5.15 in image
Error in grid.Call.graphics(L_text, as.graphicsAnnot(x$label), x$x, x$y,  :
  invalid font type
In addition: There were 50 or more warnings (use warnings() to see the
first 50)"

How can I install the font so that I can see the figures on the pdf files?
Some information that may be useful:
(1) If I use the English ggtitle, there is no problem at all. The pdf file
present the figures perfectly.
(2) If I use Chinese title, I can still see the figures p1 and p2 on
Studio, but I can't see it at the pdf file produced by ggsave. It gives an
error message



rm(list=ls())
library(ggplot2)
library(gridExtra)
df1<-data.frame(x=1:2, y=3:4, z=5:6)
#p1<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("test1")
#p2<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("test2")
p1<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("??")  #Chinese title
p2<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("??")  #Chinese title
p1<-p1+theme(text = element_text(family = "Kaiti TC"))
p2<-p2+theme(text = element_text(family = "Kaiti TC"))

p<-array(list(NA), dim=2)
p[[1]]<-p1
p[[2]]<-p2
p_series <- lapply(1:(length(p)), function(.x) p[.x][[1]])
m2 <- marrangeGrob(p_series, nrow=2, ncol=1)
ggsave("test_plot_chinese.pdf", m2)

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Thu Oct 12 18:13:04 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Thu, 12 Oct 2017 19:13:04 +0300
Subject: [R] dual y-axis for ggplot
In-Reply-To: <CABcx46DUBxoMYOLzwcTeh7CR6DtKT513cp2_84EoM3kX33BNnA@mail.gmail.com>
References: <CABcx46CJ+_sQKVKh09Q6cLtHZS=Zbd81WmeuVYJ7jum1jKfA8g@mail.gmail.com>
 <CABcx46DUBxoMYOLzwcTeh7CR6DtKT513cp2_84EoM3kX33BNnA@mail.gmail.com>
Message-ID: <CAGgJW75-0Q7sVwt+X9YLDwNdbUMwWipQB59bRKR3mAeBumWS8w@mail.gmail.com>

Hi John,
You can try the following:

override.linetype=c("twodash","solid")
p <- ggplot(obs, aes(x = Timestamp))
p <- p + geom_line(aes(y = air_temp, colour = "Temperature", linetype
="Temperature"))
p <- p + geom_line(aes(y = rel_hum/5, colour = "Humidity",
linetype="Humidity"))
p <- p +
guides(colour=guide_legend(override.aes=list(linetype=override.linetype)))
p <- p + scale_colour_manual(values = c("blue", "red"))
p <- p + scale_linetype_manual(values = c("twodash", "solid"),guide=FALSE)
p

I searched for help and used the info I found at this link:
https://stackoverflow.com/questions/37140266/how-to-merge-color-line-style-and-shape-legends-in-ggplot

HTH,
Eric



On Thu, Oct 12, 2017 at 10:14 AM, John <miaojpm at gmail.com> wrote:

> Sorry let me clarify.
> If I modify the line
>  p <- p + geom_line(aes(y = air_temp, colour = "Temperature"))
> by
> p <- p + geom_line(aes(y = air_temp, colour = "Temperature", linetype
> ="Temperature"))
> and
> p <- p + geom_line(aes(y = rel_hum/5, colour = "Humidity"))
> by
> p <- p + geom_line(aes(y = rel_hum/5, colour = "Humidity",
> linetype="Humidity"))
> and
> p <- p + scale_linetype_manual(values = c("twodash", "solid")),
>
> I will have two lines with different color and different line type,
>
> but I will have two legends  (one with blue/red,solid, the other with two
> dash/solid, black ).
>
> How can I have only one legend with blue/two dash and red/solid?
>
>
>
>
> 2017-10-12 0:06 GMT-07:00 John <miaojpm at gmail.com>:
>
> > Hi,
> >
> >    To my knowledge, an excellent of ggplot with a second y-axis is
> >
> > https://rpubs.com/MarkusLoew/226759
> >
> >    In this example, the author uses two colors for the two lines, but the
> > line shapes are the same -- both are solid. Could each line have its own
> > color as well as its own shape? For example, can I make the red line with
> > the linetype "twodash", while the blue line with the linetype "solid"?
> >    For convenience, I copied the codes as follows.
> >
> > ########
> > p <- ggplot(obs, aes(x = Timestamp))
> >   p <- p + geom_line(aes(y = air_temp, colour = "Temperature"))
> >
> >   # adding the relative humidity data, transformed to match roughly the
> > range of the temperature
> >   p <- p + geom_line(aes(y = rel_hum/5, colour = "Humidity"))
> >
> >   # now adding the secondary axis, following the example in the help file
> > ?scale_y_continuous
> >   # and, very important, reverting the above transformation
> >   p <- p + scale_y_continuous(sec.axis = sec_axis(~.*5, name = "Relative
> > humidity [%]"))
> >
> >   # modifying colours and theme options
> >   p <- p + scale_colour_manual(values = c("blue", "red"))
> >   p <- p + labs(y = "Air temperature [?C]",
> >                 x = "Date and time",
> >                 colour = "Parameter")
> >   p <- p + theme(legend.position = c(0.8, 0.9))
> > p
> > ########
> >
> >    Thanks,
> >
> > John
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From l.handke at tu-braunschweig.de  Thu Oct 12 15:18:17 2017
From: l.handke at tu-braunschweig.de (l.handke at tu-braunschweig.de)
Date: Thu, 12 Oct 2017 15:18:17 +0200
Subject: [R] Recurrence plots in R using different colours
Message-ID: <00d901d3435c$92a53870$b7efa950$@tu-braunschweig.de>

Hello,

I am an absolutely beginner with regards to R, so forgive me for my
potentially very stupid questions.

I have been attempting to create recurrence plots using R. The data I am
using is based on a mutually exclusive and exhaustive coding scheme with
over 40 individual codes which can be assigned to 6 higher order categories.
When I carry out the usual rqa command, my plot is pretty much black - no
surprises there really considering the exhaustive nature of my coding scheme
and the fact that there are only 6 different categories. 

What i would like to do is colour-code my categories in order to arrive at a
plot that shows the colour of the reccurence of the specific category.
Eventually that would mean a plot with six different colours (next to white,
which means no recurrence), rather than the plain black and white plot which
does not reflect the nature of the recurrence. I hope that my explanation is
clear enough.

My procedure up until now has been:

1) perform a recurrence quantification analysis:

testplot <- rqa(time.series = test$Code, embedding.dim = 1, time.lag = 1,

               radius = 0.1,lmin =2, vmin =2, distanceToBorder = 2, save.RM
= TRUE, do.plot = TRUE)

 

This plot is based on a TRUE FALSE sparse matrix.

2) Accordingly the next step was to transform this TRUE FALSE matrix to one
that contains the values for the codes for TRUE and 0 for false:

RP1 <- testplot$recurrence.matrix
RP2 <- as.matrix(RP1)*1
TS <- test$Code
 
for (i in seq(1,dim(RP2)[1])) {
  for (j in seq(1,dim(RP2)[1])) {
    if (RP2[i,j] == 1) {
      RP2[i,j] <- TS[i]
    }
  }
}

This results in a matrix of this type:

1    0    3    1    
0    4    0    0   
1    0    5    6    

 

Now I am missing step 3 --> how can plot this matrix by previously assigning
different colours to the numbers?

Thank you in advance for your help!

Best, Lisa

 

 

--

M.Sc. Lisa Handke

Technische Universit?t Braunschweig

Institut f?r Psychologie

Lehrstuhl f?r Arbeits-, Organisations- und Sozialpsychologie 

Spielmannstrasse 19, Raum 103

D-38106 Braunschweig

 

Phone: +49 (0)531-391-2539

Fax:   +49 (0)531-391-8173

Mail:   <mailto:l.handke at tu-braunschweig.de> l.handke at tu-braunschweig.de

 <http://www.tu-braunschweig.de/psychologie/abt/aos>
www.tu-braunschweig.de/psychologie/abt/aos

 


	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Thu Oct 12 19:17:04 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 12 Oct 2017 10:17:04 -0700
Subject: [R] How to define proper breaks in RFM analysis
In-Reply-To: <CAJL6Qs8tPRTVFi7h4Pwz9i55zPJx+K+1geF4E=S=SJ1Ke_LUjQ@mail.gmail.com>
References: <CAJL6Qs8tPRTVFi7h4Pwz9i55zPJx+K+1geF4E=S=SJ1Ke_LUjQ@mail.gmail.com>
Message-ID: <2F9336C3-A145-4491-A0AF-438C4516D435@comcast.net>


> On Oct 12, 2017, at 1:17 AM, Hemant Sain <hemantsain55 at gmail.com> wrote:
> 
> Hello,
> I'm working on RFM analysis and i wanted to define my own breaks but my
> frequency distribution is not normally distributed so when I'm using
> quartile its not giving the optimal results.
> so I'm looking for a better approach where i can define breaks dynamically
> because after visualization i can do it easily but i want to apply this
> model so that it can automatically define the breaks according to data set.
> I'm attaching sample data for reference.
> 
> Thanks
> 
>                           *Freq*
> 5
> 15
> 1
> 8
> 2

Have you read the Posting Guide?
And don't skip:  https://stat.ethz.ch/mailman/listinfo/r-help

-- 
David.
> 2
> 
snipped reams of useless "data".
> 
> -- 
> hemantsain.com
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From drjimlemon at gmail.com  Thu Oct 12 22:56:10 2017
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 13 Oct 2017 07:56:10 +1100
Subject: [R] Recurrence plots in R using different colours
In-Reply-To: <00d901d3435c$92a53870$b7efa950$@tu-braunschweig.de>
References: <00d901d3435c$92a53870$b7efa950$@tu-braunschweig.de>
Message-ID: <CA+8X3fVPCKWzPoEkvcXijh-+5fCf6_xCyOgC0rDxQOP49UBP6A@mail.gmail.com>

Hi Lisa,
I'm not sure exactly what sort of plot you want, but this may help:

rcol<-matrix(c(1,0,3,1,0,4,0,0,1,0,5,6),ncol=4,byrow=TRUE)
# replace zeros with NAs as there is no numeric color for zero
rcol[rcol==0]<-NA
library(plotrix)
color2D.matplot(rcol,cellcolors=rcol,yat=1:3)

Jim

On Fri, Oct 13, 2017 at 12:18 AM,  <l.handke at tu-braunschweig.de> wrote:
> Hello,
>
> I am an absolutely beginner with regards to R, so forgive me for my
> potentially very stupid questions.
>
> I have been attempting to create recurrence plots using R. The data I am
> using is based on a mutually exclusive and exhaustive coding scheme with
> over 40 individual codes which can be assigned to 6 higher order categories.
> When I carry out the usual rqa command, my plot is pretty much black - no
> surprises there really considering the exhaustive nature of my coding scheme
> and the fact that there are only 6 different categories.
>
> What i would like to do is colour-code my categories in order to arrive at a
> plot that shows the colour of the reccurence of the specific category.
> Eventually that would mean a plot with six different colours (next to white,
> which means no recurrence), rather than the plain black and white plot which
> does not reflect the nature of the recurrence. I hope that my explanation is
> clear enough.
>
> My procedure up until now has been:
>
> 1) perform a recurrence quantification analysis:
>
> testplot <- rqa(time.series = test$Code, embedding.dim = 1, time.lag = 1,
>
>                radius = 0.1,lmin =2, vmin =2, distanceToBorder = 2, save.RM
> = TRUE, do.plot = TRUE)
>
>
>
> This plot is based on a TRUE FALSE sparse matrix.
>
> 2) Accordingly the next step was to transform this TRUE FALSE matrix to one
> that contains the values for the codes for TRUE and 0 for false:
>
> RP1 <- testplot$recurrence.matrix
> RP2 <- as.matrix(RP1)*1
> TS <- test$Code
>
> for (i in seq(1,dim(RP2)[1])) {
>   for (j in seq(1,dim(RP2)[1])) {
>     if (RP2[i,j] == 1) {
>       RP2[i,j] <- TS[i]
>     }
>   }
> }
>
> This results in a matrix of this type:
>
> 1    0    3    1
> 0    4    0    0
> 1    0    5    6
>
>
>
> Now I am missing step 3 --> how can plot this matrix by previously assigning
> different colours to the numbers?
>
> Thank you in advance for your help!
>
> Best, Lisa
>
>
>
>
>
> --
>
> M.Sc. Lisa Handke
>
> Technische Universit?t Braunschweig
>
> Institut f?r Psychologie
>
> Lehrstuhl f?r Arbeits-, Organisations- und Sozialpsychologie
>
> Spielmannstrasse 19, Raum 103
>
> D-38106 Braunschweig
>
>
>
> Phone: +49 (0)531-391-2539
>
> Fax:   +49 (0)531-391-8173
>
> Mail:   <mailto:l.handke at tu-braunschweig.de> l.handke at tu-braunschweig.de
>
>  <http://www.tu-braunschweig.de/psychologie/abt/aos>
> www.tu-braunschweig.de/psychologie/abt/aos
>
>
>
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From yasing053 at gmail.com  Thu Oct 12 22:25:35 2017
From: yasing053 at gmail.com (Yasin Gocgun)
Date: Thu, 12 Oct 2017 23:25:35 +0300
Subject: [R] comparing two strings from data
Message-ID: <CAJJuoESdqP70W4qHSSdCWem59R5qkaeaEf64qAt0UvLKe2Eh0w@mail.gmail.com>

Hi,

I have two columns that contain numbers along with letters (as shown below)
and have different lengths. Each entry in the first column is likely to be
 found in the second column at most once.

For each entry of the first column, if that entry is found in the second
column, I would like to get the corresponding index. For instance, if the
first entry of the first column is 5th entry in the second column, I would
like to keep this index 5.

AST2017000005534   TUR2017000001428
CTS2017000079930    CTS2017000071989
CTS2017000079931     CTS2017000072015

In a loop, when I use the following code to get those indices,


data_2 = read.csv("excel_data.csv")
column_1 = data_2$data1
column_2 = data_2$data2

match_list <- array(0,dim=c(310,1));  # 310 is the length of the first
column

for (indx in 1: 310){
    for(indx2 in 1:713){ # 713 is the length of the second column
        if(column_1[indx] == column_2[indx2] ){
            match_list[indx,1] = indx2;
            break;
        }
    }
}


R provides the following error:

Error in Ops.factor(column_1[indx], column_2[indx2]) :
  level sets of factors are different

So can someone explain me how I can resolve this issue?

Thnak you,

Yasin

	[[alternative HTML version deleted]]


From 538280 at gmail.com  Thu Oct 12 23:16:06 2017
From: 538280 at gmail.com (Greg Snow)
Date: Thu, 12 Oct 2017 15:16:06 -0600
Subject: [R] comparing two strings from data
In-Reply-To: <CAJJuoESdqP70W4qHSSdCWem59R5qkaeaEf64qAt0UvLKe2Eh0w@mail.gmail.com>
References: <CAJJuoESdqP70W4qHSSdCWem59R5qkaeaEf64qAt0UvLKe2Eh0w@mail.gmail.com>
Message-ID: <CAFEqCdznDwQUeazuKuXLgKR0mDt0kKz91XZYdM1vMjJ3Zyq4hA@mail.gmail.com>

The error is because the read.csv function converted both columns to
factors.  The simplest thing to do is to set stringsAsFactors=FALSE is
the call to read.csv so that they are compared as strings.  You could
also call as.character on each of the columns if you don't want to
read the data in again.

Also, look at the match function, I think it will give you what you
want without the explicit looping.


On Thu, Oct 12, 2017 at 2:25 PM, Yasin Gocgun <yasing053 at gmail.com> wrote:
> Hi,
>
> I have two columns that contain numbers along with letters (as shown below)
> and have different lengths. Each entry in the first column is likely to be
>  found in the second column at most once.
>
> For each entry of the first column, if that entry is found in the second
> column, I would like to get the corresponding index. For instance, if the
> first entry of the first column is 5th entry in the second column, I would
> like to keep this index 5.
>
> AST2017000005534   TUR2017000001428
> CTS2017000079930    CTS2017000071989
> CTS2017000079931     CTS2017000072015
>
> In a loop, when I use the following code to get those indices,
>
>
> data_2 = read.csv("excel_data.csv")
> column_1 = data_2$data1
> column_2 = data_2$data2
>
> match_list <- array(0,dim=c(310,1));  # 310 is the length of the first
> column
>
> for (indx in 1: 310){
>     for(indx2 in 1:713){ # 713 is the length of the second column
>         if(column_1[indx] == column_2[indx2] ){
>             match_list[indx,1] = indx2;
>             break;
>         }
>     }
> }
>
>
> R provides the following error:
>
> Error in Ops.factor(column_1[indx], column_2[indx2]) :
>   level sets of factors are different
>
> So can someone explain me how I can resolve this issue?
>
> Thnak you,
>
> Yasin
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From boris.steipe at utoronto.ca  Thu Oct 12 23:16:42 2017
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Thu, 12 Oct 2017 17:16:42 -0400
Subject: [R] comparing two strings from data
In-Reply-To: <CAJJuoESdqP70W4qHSSdCWem59R5qkaeaEf64qAt0UvLKe2Eh0w@mail.gmail.com>
References: <CAJJuoESdqP70W4qHSSdCWem59R5qkaeaEf64qAt0UvLKe2Eh0w@mail.gmail.com>
Message-ID: <9FBDA218-44B6-427E-9C12-AEBB81182495@utoronto.ca>

It's generally a very good idea to examine the structure of data after you have read it in. str(data2) would have shown you that read.csv() turned your strings into factors, and that's why the == operator no longer does what you think it does.

use ...

data_2 <- read.csv("excel_data.csv", stringsAsFactors = FALSE)

... to turn this off. Also, the %in% operator will achieve more directly what you are trying to do. No need for loops.

B.




> On Oct 12, 2017, at 4:25 PM, Yasin Gocgun <yasing053 at gmail.com> wrote:
> 
> Hi,
> 
> I have two columns that contain numbers along with letters (as shown below)
> and have different lengths. Each entry in the first column is likely to be
> found in the second column at most once.
> 
> For each entry of the first column, if that entry is found in the second
> column, I would like to get the corresponding index. For instance, if the
> first entry of the first column is 5th entry in the second column, I would
> like to keep this index 5.
> 
> AST2017000005534   TUR2017000001428
> CTS2017000079930    CTS2017000071989
> CTS2017000079931     CTS2017000072015
> 
> In a loop, when I use the following code to get those indices,
> 
> 
> data_2 = read.csv("excel_data.csv")
> column_1 = data_2$data1
> column_2 = data_2$data2
> 
> match_list <- array(0,dim=c(310,1));  # 310 is the length of the first
> column
> 
> for (indx in 1: 310){
>    for(indx2 in 1:713){ # 713 is the length of the second column
>        if(column_1[indx] == column_2[indx2] ){
>            match_list[indx,1] = indx2;
>            break;
>        }
>    }
> }
> 
> 
> R provides the following error:
> 
> Error in Ops.factor(column_1[indx], column_2[indx2]) :
>  level sets of factors are different
> 
> So can someone explain me how I can resolve this issue?
> 
> Thnak you,
> 
> Yasin
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From paul at stat.auckland.ac.nz  Fri Oct 13 04:24:10 2017
From: paul at stat.auckland.ac.nz (Paul Murrell)
Date: Fri, 13 Oct 2017 15:24:10 +1300
Subject: [R] [FORGED] can't print ggplot with Chinese characters to pdf
 files
In-Reply-To: <CABcx46A4OL5+Ja-e0eAr=7iF2_obaCOZh4a7vHMLcqkZNpD3MQ@mail.gmail.com>
References: <CABcx46A4OL5+Ja-e0eAr=7iF2_obaCOZh4a7vHMLcqkZNpD3MQ@mail.gmail.com>
Message-ID: <1daf3c68-a4ee-3755-911d-7f997528499e@stat.auckland.ac.nz>

Hi

Instead of ...

ggsave("test_plot_chinese.pdf", m2)

... try ...

cairo_pdf("test_plot_chinese.pdf")
print(m2)
dev.off()

Paul

On 13/10/17 02:12, John wrote:
> I install the Chinese font "Kaiti TC" on my mac, but I can't print the
> figures to pdf file by "marrangeGrob" command, which is in the package
> "gridExtra". Error message after I type "ggsave(......)" (last line of the
> program):
> 
> "Saving 7.47 x 5.15 in image
> Error in grid.Call.graphics(L_text, as.graphicsAnnot(x$label), x$x, x$y,  :
>    invalid font type
> In addition: There were 50 or more warnings (use warnings() to see the
> first 50)"
> 
> How can I install the font so that I can see the figures on the pdf files?
> Some information that may be useful:
> (1) If I use the English ggtitle, there is no problem at all. The pdf file
> present the figures perfectly.
> (2) If I use Chinese title, I can still see the figures p1 and p2 on
> Studio, but I can't see it at the pdf file produced by ggsave. It gives an
> error message
> 
> 
> 
> rm(list=ls())
> library(ggplot2)
> library(gridExtra)
> df1<-data.frame(x=1:2, y=3:4, z=5:6)
> #p1<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("test1")
> #p2<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("test2")
> p1<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("??")  #Chinese title
> p2<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("??")  #Chinese title
> p1<-p1+theme(text = element_text(family = "Kaiti TC"))
> p2<-p2+theme(text = element_text(family = "Kaiti TC"))
> 
> p<-array(list(NA), dim=2)
> p[[1]]<-p1
> p[[2]]<-p2
> p_series <- lapply(1:(length(p)), function(.x) p[.x][[1]])
> m2 <- marrangeGrob(p_series, nrow=2, ncol=1)
> ggsave("test_plot_chinese.pdf", m2)
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From miaojpm at gmail.com  Fri Oct 13 04:48:21 2017
From: miaojpm at gmail.com (John)
Date: Thu, 12 Oct 2017 19:48:21 -0700
Subject: [R] [FORGED] can't print ggplot with Chinese characters to pdf
	files
In-Reply-To: <1daf3c68-a4ee-3755-911d-7f997528499e@stat.auckland.ac.nz>
References: <CABcx46A4OL5+Ja-e0eAr=7iF2_obaCOZh4a7vHMLcqkZNpD3MQ@mail.gmail.com>
 <1daf3c68-a4ee-3755-911d-7f997528499e@stat.auckland.ac.nz>
Message-ID: <CABcx46DFdnwUjdO4ciaD5aeoGSDR2UgUJwMvjWG6WkjBPJ=wrg@mail.gmail.com>

Thanks, Paul. Following your solution,  I got this error message:

Warning message:
In cairo_pdf("test_plot_chinese.pdf") : failed to load cairo DLL

Is there anything else I need to install?

Thanks,

John

2017-10-12 19:24 GMT-07:00 Paul Murrell <paul at stat.auckland.ac.nz>:

> Hi
>
> Instead of ...
>
> ggsave("test_plot_chinese.pdf", m2)
>
> ... try ...
>
> cairo_pdf("test_plot_chinese.pdf")
> print(m2)
> dev.off()
>
> Paul
>
>
> On 13/10/17 02:12, John wrote:
>
>> I install the Chinese font "Kaiti TC" on my mac, but I can't print the
>> figures to pdf file by "marrangeGrob" command, which is in the package
>> "gridExtra". Error message after I type "ggsave(......)" (last line of the
>> program):
>>
>> "Saving 7.47 x 5.15 in image
>> Error in grid.Call.graphics(L_text, as.graphicsAnnot(x$label), x$x, x$y,
>> :
>>    invalid font type
>> In addition: There were 50 or more warnings (use warnings() to see the
>> first 50)"
>>
>> How can I install the font so that I can see the figures on the pdf files?
>> Some information that may be useful:
>> (1) If I use the English ggtitle, there is no problem at all. The pdf file
>> present the figures perfectly.
>> (2) If I use Chinese title, I can still see the figures p1 and p2 on
>> Studio, but I can't see it at the pdf file produced by ggsave. It gives an
>> error message
>>
>>
>>
>> rm(list=ls())
>> library(ggplot2)
>> library(gridExtra)
>> df1<-data.frame(x=1:2, y=3:4, z=5:6)
>> #p1<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("test1")
>> #p2<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("test2")
>> p1<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("??")  #Chinese title
>> p2<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("??")  #Chinese title
>> p1<-p1+theme(text = element_text(family = "Kaiti TC"))
>> p2<-p2+theme(text = element_text(family = "Kaiti TC"))
>>
>> p<-array(list(NA), dim=2)
>> p[[1]]<-p1
>> p[[2]]<-p2
>> p_series <- lapply(1:(length(p)), function(.x) p[.x][[1]])
>> m2 <- marrangeGrob(p_series, nrow=2, ncol=1)
>> ggsave("test_plot_chinese.pdf", m2)
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> --
> Dr Paul Murrell
> Department of Statistics
> The University of Auckland
> Private Bag 92019
> Auckland
> New Zealand
> 64 9 3737599 x85392
> paul at stat.auckland.ac.nz
> http://www.stat.auckland.ac.nz/~paul/
>

	[[alternative HTML version deleted]]


From paul at stat.auckland.ac.nz  Fri Oct 13 04:55:11 2017
From: paul at stat.auckland.ac.nz (Paul Murrell)
Date: Fri, 13 Oct 2017 15:55:11 +1300
Subject: [R] [FORGED] can't print ggplot with Chinese characters to pdf
 files
In-Reply-To: <CABcx46DFdnwUjdO4ciaD5aeoGSDR2UgUJwMvjWG6WkjBPJ=wrg@mail.gmail.com>
References: <CABcx46A4OL5+Ja-e0eAr=7iF2_obaCOZh4a7vHMLcqkZNpD3MQ@mail.gmail.com>
 <1daf3c68-a4ee-3755-911d-7f997528499e@stat.auckland.ac.nz>
 <CABcx46DFdnwUjdO4ciaD5aeoGSDR2UgUJwMvjWG6WkjBPJ=wrg@mail.gmail.com>
Message-ID: <7f9f2636-c3d3-40b1-8ce3-9d4b2bde3336@stat.auckland.ac.nz>

Hi

By the looks of it, you need to install Cairo graphics ...

https://www.cairographics.org/download/

Paul

On 13/10/17 15:48, John wrote:
> Thanks, Paul. Following your solution, ?I got this error message:
> 
> Warning message:
> In cairo_pdf("test_plot_chinese.pdf") : failed to load cairo DLL
> 
> Is there anything else I need to install?
> 
> Thanks,
> 
> John
> 
> 2017-10-12 19:24 GMT-07:00 Paul Murrell <paul at stat.auckland.ac.nz 
> <mailto:paul at stat.auckland.ac.nz>>:
> 
>     Hi
> 
>     Instead of ...
> 
>     ggsave("test_plot_chinese.pdf", m2)
> 
>     ... try ...
> 
>     cairo_pdf("test_plot_chinese.pdf")
>     print(m2)
>     dev.off()
> 
>     Paul
> 
> 
>     On 13/10/17 02:12, John wrote:
> 
>         I install the Chinese font "Kaiti TC" on my mac, but I can't
>         print the
>         figures to pdf file by "marrangeGrob" command, which is in the
>         package
>         "gridExtra". Error message after I type "ggsave(......)" (last
>         line of the
>         program):
> 
>         "Saving 7.47 x 5.15 in image
>         Error in grid.Call.graphics(L_text, as.graphicsAnnot(x$label),
>         x$x, x$y,? :
>          ? ?invalid font type
>         In addition: There were 50 or more warnings (use warnings() to
>         see the
>         first 50)"
> 
>         How can I install the font so that I can see the figures on the
>         pdf files?
>         Some information that may be useful:
>         (1) If I use the English ggtitle, there is no problem at all.
>         The pdf file
>         present the figures perfectly.
>         (2) If I use Chinese title, I can still see the figures p1 and p2 on
>         Studio, but I can't see it at the pdf file produced by ggsave.
>         It gives an
>         error message
> 
> 
> 
>         rm(list=ls())
>         library(ggplot2)
>         library(gridExtra)
>         df1<-data.frame(x=1:2, y=3:4, z=5:6)
>         #p1<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("test1")
>         #p2<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("test2")
>         p1<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("??") 
>         #Chinese title
>         p2<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("??") 
>         #Chinese title
>         p1<-p1+theme(text = element_text(family = "Kaiti TC"))
>         p2<-p2+theme(text = element_text(family = "Kaiti TC"))
> 
>         p<-array(list(NA), dim=2)
>         p[[1]]<-p1
>         p[[2]]<-p2
>         p_series <- lapply(1:(length(p)), function(.x) p[.x][[1]])
>         m2 <- marrangeGrob(p_series, nrow=2, ncol=1)
>         ggsave("test_plot_chinese.pdf", m2)
> 
>          ? ? ? ? [[alternative HTML version deleted]]
> 
>         ______________________________________________
>         R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>         -- To UNSUBSCRIBE and more, see
>         https://stat.ethz.ch/mailman/listinfo/r-help
>         <https://stat.ethz.ch/mailman/listinfo/r-help>
>         PLEASE do read the posting guide
>         http://www.R-project.org/posting-guide.html
>         <http://www.R-project.org/posting-guide.html>
>         and provide commented, minimal, self-contained, reproducible code.
> 
> 
>     -- 
>     Dr Paul Murrell
>     Department of Statistics
>     The University of Auckland
>     Private Bag 92019
>     Auckland
>     New Zealand
>     64 9 3737599 x85392
>     paul at stat.auckland.ac.nz <mailto:paul at stat.auckland.ac.nz>
>     http://www.stat.auckland.ac.nz/~paul/
>     <http://www.stat.auckland.ac.nz/~paul/>
> 
> 

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From ericjberger at gmail.com  Fri Oct 13 06:39:05 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Fri, 13 Oct 2017 07:39:05 +0300
Subject: [R] comparing two strings from data
In-Reply-To: <9FBDA218-44B6-427E-9C12-AEBB81182495@utoronto.ca>
References: <CAJJuoESdqP70W4qHSSdCWem59R5qkaeaEf64qAt0UvLKe2Eh0w@mail.gmail.com>
 <9FBDA218-44B6-427E-9C12-AEBB81182495@utoronto.ca>
Message-ID: <CAGgJW74r=NYKwmcGs6T4V6VYbj4WA27iarBCGGq6u+5+idKy-Q@mail.gmail.com>

Combining and completing the advice from Greg and Boris the complete
solution is two lines:

data_2 <- read.csv("excel_data.csv", stringsAsFactors = FALSE)
match_list <- match( data_2$data1, data_2$data2 )

The vector match_list will have the matching position when it exists and
NA's otherwise. Its length will be the same as the length of data_2$data1.

You should get experience in reading the help information for R functions.
In this case, type ?match to get information about the 'match' function.

HTH,
Eric


On Fri, Oct 13, 2017 at 12:16 AM, Boris Steipe <boris.steipe at utoronto.ca>
wrote:

> It's generally a very good idea to examine the structure of data after you
> have read it in. str(data2) would have shown you that read.csv() turned
> your strings into factors, and that's why the == operator no longer does
> what you think it does.
>
> use ...
>
> data_2 <- read.csv("excel_data.csv", stringsAsFactors = FALSE)
>
> ... to turn this off. Also, the %in% operator will achieve more directly
> what you are trying to do. No need for loops.
>
> B.
>
>
>
>
> > On Oct 12, 2017, at 4:25 PM, Yasin Gocgun <yasing053 at gmail.com> wrote:
> >
> > Hi,
> >
> > I have two columns that contain numbers along with letters (as shown
> below)
> > and have different lengths. Each entry in the first column is likely to
> be
> > found in the second column at most once.
> >
> > For each entry of the first column, if that entry is found in the second
> > column, I would like to get the corresponding index. For instance, if the
> > first entry of the first column is 5th entry in the second column, I
> would
> > like to keep this index 5.
> >
> > AST2017000005534   TUR2017000001428
> > CTS2017000079930    CTS2017000071989
> > CTS2017000079931     CTS2017000072015
> >
> > In a loop, when I use the following code to get those indices,
> >
> >
> > data_2 = read.csv("excel_data.csv")
> > column_1 = data_2$data1
> > column_2 = data_2$data2
> >
> > match_list <- array(0,dim=c(310,1));  # 310 is the length of the first
> > column
> >
> > for (indx in 1: 310){
> >    for(indx2 in 1:713){ # 713 is the length of the second column
> >        if(column_1[indx] == column_2[indx2] ){
> >            match_list[indx,1] = indx2;
> >            break;
> >        }
> >    }
> > }
> >
> >
> > R provides the following error:
> >
> > Error in Ops.factor(column_1[indx], column_2[indx2]) :
> >  level sets of factors are different
> >
> > So can someone explain me how I can resolve this issue?
> >
> > Thnak you,
> >
> > Yasin
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From burtonpp at me.com  Fri Oct 13 02:08:48 2017
From: burtonpp at me.com (Paul Burton)
Date: Thu, 12 Oct 2017 20:08:48 -0400
Subject: [R] error in installing igraph
Message-ID: <436AFE55-8D9C-4063-8319-1E866A2807D0@me.com>

Jeff,

Did you ever get this resolved?

I m having the same problem with igraph.

Paul


From miaojpm at gmail.com  Fri Oct 13 04:48:42 2017
From: miaojpm at gmail.com (John)
Date: Thu, 12 Oct 2017 19:48:42 -0700
Subject: [R] dual y-axis for ggplot
In-Reply-To: <CAGgJW75-0Q7sVwt+X9YLDwNdbUMwWipQB59bRKR3mAeBumWS8w@mail.gmail.com>
References: <CABcx46CJ+_sQKVKh09Q6cLtHZS=Zbd81WmeuVYJ7jum1jKfA8g@mail.gmail.com>
 <CABcx46DUBxoMYOLzwcTeh7CR6DtKT513cp2_84EoM3kX33BNnA@mail.gmail.com>
 <CAGgJW75-0Q7sVwt+X9YLDwNdbUMwWipQB59bRKR3mAeBumWS8w@mail.gmail.com>
Message-ID: <CABcx46ARPh7BsO+2dWpvPWXSVKUqhC__fXNcGLU8VhUgz6xrMQ@mail.gmail.com>

Thanks, Eric! It works well.

2017-10-12 9:13 GMT-07:00 Eric Berger <ericjberger at gmail.com>:

> Hi John,
> You can try the following:
>
> override.linetype=c("twodash","solid")
> p <- ggplot(obs, aes(x = Timestamp))
> p <- p + geom_line(aes(y = air_temp, colour = "Temperature", linetype
> ="Temperature"))
> p <- p + geom_line(aes(y = rel_hum/5, colour = "Humidity",
> linetype="Humidity"))
> p <- p + guides(colour=guide_legend(override.aes=list(linetype=
> override.linetype)))
> p <- p + scale_colour_manual(values = c("blue", "red"))
> p <- p + scale_linetype_manual(values = c("twodash", "solid"),guide=FALSE)
> p
>
> I searched for help and used the info I found at this link:
> https://stackoverflow.com/questions/37140266/how-to-
> merge-color-line-style-and-shape-legends-in-ggplot
>
> HTH,
> Eric
>
>
>
> On Thu, Oct 12, 2017 at 10:14 AM, John <miaojpm at gmail.com> wrote:
>
>> Sorry let me clarify.
>> If I modify the line
>>  p <- p + geom_line(aes(y = air_temp, colour = "Temperature"))
>> by
>> p <- p + geom_line(aes(y = air_temp, colour = "Temperature", linetype
>> ="Temperature"))
>> and
>> p <- p + geom_line(aes(y = rel_hum/5, colour = "Humidity"))
>> by
>> p <- p + geom_line(aes(y = rel_hum/5, colour = "Humidity",
>> linetype="Humidity"))
>> and
>> p <- p + scale_linetype_manual(values = c("twodash", "solid")),
>>
>> I will have two lines with different color and different line type,
>>
>> but I will have two legends  (one with blue/red,solid, the other with two
>> dash/solid, black ).
>>
>> How can I have only one legend with blue/two dash and red/solid?
>>
>>
>>
>>
>> 2017-10-12 0:06 GMT-07:00 John <miaojpm at gmail.com>:
>>
>> > Hi,
>> >
>> >    To my knowledge, an excellent of ggplot with a second y-axis is
>> >
>> > https://rpubs.com/MarkusLoew/226759
>> >
>> >    In this example, the author uses two colors for the two lines, but
>> the
>> > line shapes are the same -- both are solid. Could each line have its own
>> > color as well as its own shape? For example, can I make the red line
>> with
>> > the linetype "twodash", while the blue line with the linetype "solid"?
>> >    For convenience, I copied the codes as follows.
>> >
>> > ########
>> > p <- ggplot(obs, aes(x = Timestamp))
>> >   p <- p + geom_line(aes(y = air_temp, colour = "Temperature"))
>> >
>> >   # adding the relative humidity data, transformed to match roughly the
>> > range of the temperature
>> >   p <- p + geom_line(aes(y = rel_hum/5, colour = "Humidity"))
>> >
>> >   # now adding the secondary axis, following the example in the help
>> file
>> > ?scale_y_continuous
>> >   # and, very important, reverting the above transformation
>> >   p <- p + scale_y_continuous(sec.axis = sec_axis(~.*5, name = "Relative
>> > humidity [%]"))
>> >
>> >   # modifying colours and theme options
>> >   p <- p + scale_colour_manual(values = c("blue", "red"))
>> >   p <- p + labs(y = "Air temperature [?C]",
>> >                 x = "Date and time",
>> >                 colour = "Parameter")
>> >   p <- p + theme(legend.position = c(0.8, 0.9))
>> > p
>> > ########
>> >
>> >    Thanks,
>> >
>> > John
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Fri Oct 13 06:54:20 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Fri, 13 Oct 2017 07:54:20 +0300
Subject: [R] comparing two strings from data
In-Reply-To: <CAGgJW74r=NYKwmcGs6T4V6VYbj4WA27iarBCGGq6u+5+idKy-Q@mail.gmail.com>
References: <CAJJuoESdqP70W4qHSSdCWem59R5qkaeaEf64qAt0UvLKe2Eh0w@mail.gmail.com>
 <9FBDA218-44B6-427E-9C12-AEBB81182495@utoronto.ca>
 <CAGgJW74r=NYKwmcGs6T4V6VYbj4WA27iarBCGGq6u+5+idKy-Q@mail.gmail.com>
Message-ID: <CAGgJW76UHVR_mj-772=UjSKSNukuh3DeEG5BohFUMDVzaX30YA@mail.gmail.com>

One additional comment. If you want 0 instead of NA when there is no match
then the match statement should read:

match_list <- match( data_2$data1, data_2$data2, nomatch=0)



On Fri, Oct 13, 2017 at 7:39 AM, Eric Berger <ericjberger at gmail.com> wrote:

> Combining and completing the advice from Greg and Boris the complete
> solution is two lines:
>
> data_2 <- read.csv("excel_data.csv", stringsAsFactors = FALSE)
> match_list <- match( data_2$data1, data_2$data2 )
>
> The vector match_list will have the matching position when it exists and
> NA's otherwise. Its length will be the same as the length of data_2$data1.
>
> You should get experience in reading the help information for R functions.
> In this case, type ?match to get information about the 'match' function.
>
> HTH,
> Eric
>
>
> On Fri, Oct 13, 2017 at 12:16 AM, Boris Steipe <boris.steipe at utoronto.ca>
> wrote:
>
>> It's generally a very good idea to examine the structure of data after
>> you have read it in. str(data2) would have shown you that read.csv() turned
>> your strings into factors, and that's why the == operator no longer does
>> what you think it does.
>>
>> use ...
>>
>> data_2 <- read.csv("excel_data.csv", stringsAsFactors = FALSE)
>>
>> ... to turn this off. Also, the %in% operator will achieve more directly
>> what you are trying to do. No need for loops.
>>
>> B.
>>
>>
>>
>>
>> > On Oct 12, 2017, at 4:25 PM, Yasin Gocgun <yasing053 at gmail.com> wrote:
>> >
>> > Hi,
>> >
>> > I have two columns that contain numbers along with letters (as shown
>> below)
>> > and have different lengths. Each entry in the first column is likely to
>> be
>> > found in the second column at most once.
>> >
>> > For each entry of the first column, if that entry is found in the second
>> > column, I would like to get the corresponding index. For instance, if
>> the
>> > first entry of the first column is 5th entry in the second column, I
>> would
>> > like to keep this index 5.
>> >
>> > AST2017000005534   TUR2017000001428
>> > CTS2017000079930    CTS2017000071989
>> > CTS2017000079931     CTS2017000072015
>> >
>> > In a loop, when I use the following code to get those indices,
>> >
>> >
>> > data_2 = read.csv("excel_data.csv")
>> > column_1 = data_2$data1
>> > column_2 = data_2$data2
>> >
>> > match_list <- array(0,dim=c(310,1));  # 310 is the length of the first
>> > column
>> >
>> > for (indx in 1: 310){
>> >    for(indx2 in 1:713){ # 713 is the length of the second column
>> >        if(column_1[indx] == column_2[indx2] ){
>> >            match_list[indx,1] = indx2;
>> >            break;
>> >        }
>> >    }
>> > }
>> >
>> >
>> > R provides the following error:
>> >
>> > Error in Ops.factor(column_1[indx], column_2[indx2]) :
>> >  level sets of factors are different
>> >
>> > So can someone explain me how I can resolve this issue?
>> >
>> > Thnak you,
>> >
>> > Yasin
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Fri Oct 13 07:05:18 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 13 Oct 2017 05:05:18 +0000
Subject: [R] How to define proper breaks in RFM analysis
In-Reply-To: <CAJL6Qs8tPRTVFi7h4Pwz9i55zPJx+K+1geF4E=S=SJ1Ke_LUjQ@mail.gmail.com>
References: <CAJL6Qs8tPRTVFi7h4Pwz9i55zPJx+K+1geF4E=S=SJ1Ke_LUjQ@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB2CC0@SRVEXCHCM301.precheza.cz>

Hi

Your statement about attaching data is problematic. We cannot do much with it. Instead use output from dput(yourdata) to show us what exactly your data look like.

We also do not know how do you want to split your data. It would be nice if you can show also what should be the bins with respective data. Unless you provide this information you probably would not get any sensible answer.

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Hemant Sain
> Sent: Thursday, October 12, 2017 10:18 AM
> To: r-help mailing list <r-help at r-project.org>
> Subject: [R] How to define proper breaks in RFM analysis
>
> Hello,
> I'm working on RFM analysis and i wanted to define my own breaks but my
> frequency distribution is not normally distributed so when I'm using quartile its
> not giving the optimal results.
> so I'm looking for a better approach where i can define breaks dynamically
> because after visualization i can do it easily but i want to apply this model so
> that it can automatically define the breaks according to data set.
> I'm attaching sample data for reference.
>
> Thanks
>
>                            *Freq*
> 5
> 15
> 1
> 8
> 2
> 2
> 2
> 1
> 1
> 2
> 2
> 1
> 1
> 1
> 1
> 4
> 10
> 22
> 24
> 1
> 1
> 2
> 2
> 7
> 1
> 1
> 4
> 6
> 1
> 1
> 2
> 2
> 17
> 2
> 7
> 1
> 5
> 2
> 7
> 7
> 1
> 4
> 13
> 4
> 1
> 9
> 1
> 1
> 1
> 3
> 2
> 1
> 1
> 2
> 4
> 5
> 8
> 4
> 14
> 18
> 4
> 1
> 1
> 1
> 7
> 1
> 1
> 1
> 1
> 10
> 6
> 4
> 6
> 1
> 3
> 13
> 2
> 4
> 8
> 11
> 2
> 4
> 6
> 1
> 1
> 3
> 1
> 2
> 3
> 2
> 2
> 5
> 2
> 1
> 1
> 1
> 3
> 2
> 3
> 1
> 2
> 9
> 34
> 8
> 1
> 1
> 1
> 1
> 2
> 1
> 5
> 4
> 9
> 1
> 1
> 4
> 2
> 2
> 1
> 1
> 2
> 25
> 5
> 2
> 1
> 1
> 1
> 6
> 1
> 36
> 3
> 4
> 5
> 3
> 3
> 13
> 2
> 1
> 4
> 1
> 1
> 9
> 7
> 1
> 1
> 15
> 1
> 1
> 2
> 5
> 7
> 3
> 3
> 1
> 8
> 7
> 5
> 1
> 1
> 5
> 14
> 3
> 2
> 2
> 1
> 5
> 3
> 4
> 3
> 1
> 2
> 9
> 2
> 15
> 2
> 1
> 3
> 56
> 3
> 2
> 4
> 2
> 26
> 3
> 1
> 4
> 1
> 1
> 12
> 1
> 18
> 7
> 6
> 7
> 2
> 3
> 2
> 1
> 4
> 15
> 10
> 7
> 5
> 3
> 6
> 4
> 9
> 1
> 2
> 2
> 2
> 3
> 1
> 1
> 8
> 3
> 1
> 2
> 3
> 1
> 10
> 3
> 1
> 3
> 5
> 1
> 8
> 3
> 2
> 3
> 7
> 1
> 5
> 2
> 1
> 1
> 6
> 2
> 1
> 9
> 1
> 20
> 2
> 1
> 4
> 21
> 5
> 4
> 4
> 1
> 1
> 1
> 11
> 7
> 4
> 6
> 1
> 3
> 3
> 12
> 4
> 7
> 4
> 3
> 3
> 1
> 2
> 10
> 5
> 11
> 3
> 2
> 1
> 3
> 30
> 1
> 4
> 5
> 1
> 7
> 3
> 1
> 3
> 9
> 2
> 2
> 14
> 10
> 1
> 1
> 1
> 1
> 5
> 1
> 1
> 18
> 32
> 1
> 4
> 5
> 4
> 3
> 2
> 9
> 2
> 6
> 3
> 2
> 2
> 2
> 2
> 2
> 2
> 4
> 4
> 3
> 1
> 1
> 2
> 4
> 6
> 1
> 1
> 1
> 2
> 2
> 1
> 1
> 3
> 1
> 1
> 3
> 1
> 2
> 3
> 2
> 9
> 4
> 1
> 2
> 4
> 3
> 3
> 3
> 1
> 2
> 1
> 3
> 4
> 3
> 1
> 3
> 1
> 5
> 14
> 7
> 1
> 1
> 1
> 1
> 4
> 3
> 4
> 5
> 8
> 1
> 10
> 3
> 2
> 7
> 5
> 4
> 5
> 7
> 4
> 4
> 10
> 3
> 7
> 12
> 1
> 1
> 3
> 1
> 6
> 1
> 5
> 9
> 2
> 1
> 3
> 4
> 3
> 2
> 2
> 5
> 1
> 4
> 6
> 5
> 3
> 1
> 1
> 6
> 3
> 1
> 1
> 2
> 1
> 5
> 5
> 2
> 1
> 2
> 5
> 1
> 2
> 1
> 6
> 5
> 5
> 3
> 2
> 4
> 8
> 1
> 2
> 5
> 1
> 1
> 1
> 4
> 1
> 4
> 1
> 2
> 6
> 3
> 4
> 4
> 2
> 2
> 2
> 2
> 1
> 1
> 2
> 5
> 2
> 2
> 1
> 5
> 2
> 2
> 1
> 2
> 4
> 1
> 3
> 3
> 1
> 3
> 4
> 2
> 2
> 3
> 1
> 4
> 1
> 1
> 6
> 6
> 2
> 4
> 5
> 2
> 1
> 8
> 1
> 2
> 1
> 1
> 3
> 4
> 3
> 3
> 2
> 2
> 1
> 4
> 1
> 5
> 1
> 4
> 1
> 1
> 2
> 1
> 1
> 1
> 8
> 1
> 1
> 1
> 1
> 1
> 3
> 3
> 5
> 2
> 3
> 1
> 1
> 5
> 2
> 3
> 6
> 3
> 3
> 14
> 2
> 1
> 1
> 2
> 1
> 2
> 4
> 2
> 1
> 6
> 1
> 7
> 2
> 3
> 3
> 2
> 2
> 2
> 2
> 1
> 2
> 4
> 1
> 6
> 2
> 5
> 2
> 1
> 2
> 2
> 5
> 8
> 4
> 1
> 1
> 1
> 1
> 4
> 1
> 3
> 2
> 1
> 2
> 2
> 3
> 3
> 3
> 6
> 1
> 1
> 1
> 5
> 7
> 1
> 5
> 2
> 1
> 1
> 1
> 3
> 20
> 2
> 3
> 3
> 1
> 2
> 1
> 15
> 4
> 4
> 1
> 1
> 2
> 1
> 1
> 3
> 2
> 6
> 5
> 1
> 5
> 1
> 7
> 4
> 3
> 2
> 5
> 2
> 1
> 1
> 3
> 2
> 6
> 2
> 4
> 2
> 1
> 24
> 4
> 17
> 1
> 3
> 2
> 2
> 2
> 2
> 8
> 1
> 3
> 1
> 9
> 2
> 4
> 1
> 1
> 6
> 3
> 4
> 1
> 9
> 2
> 1
> 3
> 2
> 6
> 2
> 1
> 3
> 1
> 20
> 3
> 4
> 7
> 3
> 7
> 1
> 7
> 1
> 5
> 2
> 1
> 1
> 1
> 2
> 1
> 2
> 4
> 1
> 1
> 2
> 3
> 4
> 1
> 4
> 1
> 1
> 1
> 1
> 1
> 2
> 1
> 6
> 2
> 3
> 2
> 1
> 9
> 8
> 11
> 1
> 1
> 2
> 1
> 3
> 1
> 2
> 15
> 5
> 2
> 2
> 9
> 1
> 4
> 2
> 1
> 1
> 14
> 1
> 1
> 2
> 1
> 3
> 10
> 1
> 1
> 3
> 1
> 1
> 1
> 4
> 2
> 8
> 1
> 2
> 2
> 1
> 11
> 1
> 3
> 1
> 7
> 1
> 3
> 10
> 3
> 3
> 1
> 1
> 1
> 11
> 1
> 2
> 1
> 1
> 2
> 2
> 5
> 5
> 4
> 1
> 2
> 5
> 2
> 4
> 3
> 1
> 1
> 2
> 2
> 4
> 2
> 1
> 1
> 4
> 1
> 4
> 1
> 5
> 1
> 1
> 2
> 1
> 4
> 7
> 1
> 2
> 1
> 2
> 9
> 3
> 1
> 7
> 2
> 2
> 1
> 2
> 3
> 5
> 2
> 7
> 1
> 5
> 1
> 1
> 2
> 2
> 2
> 4
> 2
> 8
> 4
> 6
> 1
> 1
> 1
> 1
> 1
> 16
> 2
> 1
> 6
> 4
> 4
> 1
> 1
> 1
> 1
> 4
> 3
> 2
> 6
> 11
> 10
> 21
> 2
> 1
> 1
> 3
> 2
> 2
> 2
> 7
> 1
> 6
> 4
> 1
> 7
> 4
> 11
> 1
> 2
> 8
> 1
> 1
> 1
> 1
> 2
> 17
> 1
> 2
> 3
> 4
> 2
> 1
> 2
> 2
> 4
> 3
> 2
> 3
> 1
> 3
> 3
> 1
> 3
> 37
> 4
> 3
> 1
> 2
> 1
> 1
> 3
> 1
> 2
> 3
> 1
> 5
> 1
> 2
> 1
> 3
> 2
> 3
> 3
> 4
> 4
> 2
> 4
> 1
> 1
> 3
> 3
> 2
> 1
> 2
> 1
> 1
> 3
> 1
> 1
> 3
> 3
> 4
> 2
> 4
> 1
> 1
> 1
> 10
> 3
> 2
> 2
> 2
> 2
> 2
> 2
> 1
> 19
> 2
> 2
> 4
> 1
> 3
> 1
> 13
> 5
> 2
> 1
> 2
> 2
> 4
> 2
> 1
> 3
> 5
> 1
> 1
> 1
> 6
> 3
> 9
> 4
> 1
> 1
> 1
> 3
> 1
> 17
> 4
> 1
> 4
> 6
> 2
> 1
> 2
> 4
> 4
> 2
> 2
> 2
> 4
> 4
> 1
> 1
> 1
> 2
> 7
> 2
> 1
> 2
> 8
> 1
> 1
> 7
> 4
> 1
> 2
> 1
> 1
> 3
> 1
> 3
> 1
> 1
> 3
> 5
> 8
> 1
> 1
> 4
> 1
> 1
> 15
> 1
> 1
> 6
> 2
> 3
> 3
> 8
> 4
> 1
> 4
> 2
> 4
> 2
> 5
> 4
> 4
> 1
> 1
> 7
> 1
> 10
> 1
> 10
> 2
> 15
> 7
> 3
> 1
> 3
> 2
> 19
> 2
> 9
> 1
> 10
> 2
> 1
> 2
> 2
> 4
> 6
> 1
> 4
> 3
> 1
> 2
> 3
> 2
> 1
> 3
> 7
> 1
> 4
> 2
> 13
> 2
> 5
> 3
> 6
> 2
> 1
> 2
> 1
> 5
> 1
>
>
> --
> hemantsain.com
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From jdnewmil at dcn.davis.ca.us  Fri Oct 13 09:17:48 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 13 Oct 2017 08:17:48 +0100
Subject: [R] comparing two strings from data
In-Reply-To: <CAJJuoESdqP70W4qHSSdCWem59R5qkaeaEf64qAt0UvLKe2Eh0w@mail.gmail.com>
References: <CAJJuoESdqP70W4qHSSdCWem59R5qkaeaEf64qAt0UvLKe2Eh0w@mail.gmail.com>
Message-ID: <98562C09-4543-4A14-AAC9-E0DA17816713@dcn.davis.ca.us>

data_2 <- read.csv("excel_data.csv",stringsAsFactors=FALSE)
column_1 <- data_2$data1
column_2 <- data_2$data2
result <- match( column_1, column_2 )

Please read the Posting Guide mentioned at the bottom of this and every posting, in particular about posting plain text so that what we see will be what you saw when you sent the message. You should also read about how to create reproducible examples, e.g. about using dput as mentioned in [1] and [2], and verifying the example before sending it [3].

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

[2] http://adv-r.had.co.nz/Reproducibility.html

[3] https://cran.r-project.org/web/packages/reprex/index.html (read the vignette)
-- 
Sent from my phone. Please excuse my brevity.

On October 12, 2017 9:25:35 PM GMT+01:00, Yasin Gocgun <yasing053 at gmail.com> wrote:
>Hi,
>
>I have two columns that contain numbers along with letters (as shown
>below)
>and have different lengths. Each entry in the first column is likely to
>be
> found in the second column at most once.
>
>For each entry of the first column, if that entry is found in the
>second
>column, I would like to get the corresponding index. For instance, if
>the
>first entry of the first column is 5th entry in the second column, I
>would
>like to keep this index 5.
>
>AST2017000005534   TUR2017000001428
>CTS2017000079930    CTS2017000071989
>CTS2017000079931     CTS2017000072015
>
>In a loop, when I use the following code to get those indices,
>
>
>data_2 = read.csv("excel_data.csv")
>column_1 = data_2$data1
>column_2 = data_2$data2
>
>match_list <- array(0,dim=c(310,1));  # 310 is the length of the first
>column
>
>for (indx in 1: 310){
>    for(indx2 in 1:713){ # 713 is the length of the second column
>        if(column_1[indx] == column_2[indx2] ){
>            match_list[indx,1] = indx2;
>            break;
>        }
>    }
>}
>
>
>R provides the following error:
>
>Error in Ops.factor(column_1[indx], column_2[indx2]) :
>  level sets of factors are different
>
>So can someone explain me how I can resolve this issue?
>
>Thnak you,
>
>Yasin
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From hemantsain55 at gmail.com  Fri Oct 13 08:51:05 2017
From: hemantsain55 at gmail.com (Hemant Sain)
Date: Fri, 13 Oct 2017 12:21:05 +0530
Subject: [R] How to define proper breaks in RFM analysis
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB2CC0@SRVEXCHCM301.precheza.cz>
References: <CAJL6Qs8tPRTVFi7h4Pwz9i55zPJx+K+1geF4E=S=SJ1Ke_LUjQ@mail.gmail.com>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB2CC0@SRVEXCHCM301.precheza.cz>
Message-ID: <CAJL6Qs_eykK=Wa1XPkH37KoZrdM=4DYqtAdn7=P6-0ZiufVMWQ@mail.gmail.com>

Hey,
i want to define 3 ideal breaks (bin) for each variable one of those
variables is attached in the previous email,
i don't want to consider quartile method because quartile is not working
ideally for that data set because data distribution is non normal.
so i want you to suggest another method so that i can define 3 breaks with
the ideal interval for Recency, frequency and monetary to calculate RFM
score.
i'm again attaching you some of the data set.
please look into it and help me with the R code.
Thanks



*Data*

user_id subtotal_amount created_at Recency Frequency Monetary
194849 6.99 8/22/2017 9 5 9.996
194978 14.78 8/28/2017 3 15 16.308
198614 18.44 7/31/2017 31 1 18.44
234569 34.99 8/20/2017 11 8 13.5075
252686 7.99 7/31/2017 31 2 7.99
291719 21.26 8/25/2017 6 2 15.67
291787 46.1 8/31/2017 0 2 32.57
292630 24.34 7/31/2017 31 1 24.34
295204 21.86 7/18/2017 44 1 21.86
295989 8.98 8/20/2017 11 2 14.095
298883 14.38 8/24/2017 7 2 11.185
308824 10.77 7/31/2017 31 1 10.77
308874 8.29 6/11/2017 81 1 8.29
309088 17.16 8/3/2017 28 1 17.16
309126 20.54 7/30/2017 32 1 20.54
309127 15.24 8/2/2017 29 4 13.3925
309159 10.78 8/28/2017 3 10 13.694
309170 8.66 8/29/2017 2 22 9.383636364
309190 7.19 8/31/2017 0 24 10.33791667
309218 8.49 6/22/2017 70 1 8.49
309250 18.27 7/30/2017 32 1 18.27
309358 8 8/31/2017 0 2 11.99
309418 43.21 8/13/2017 18 2 26.35
309421 6.49 8/26/2017 5 7 10.72428571
309440 20.37 6/24/2017 68 1 20.37
309468 11.37 6/10/2017 82 1 11.37
309538 9.08 7/30/2017 32 4 10.075
309548 7.06 8/30/2017 1 6 7.83
309564 9.57 6/10/2017 82 1 9.57
309616 7.37 6/27/2017 65 1 7.37
309751 8.87 8/5/2017 26 2 8.925
309788 11.21 8/4/2017 27 2 10.81
309842 10.68 8/31/2017 0 17 10.49647059
309938 17.77 8/20/2017 11 2 12.38
310017 8.06 8/31/2017 0 7 12.12
310125 8.47 8/4/2017 27 1 8.47
310126 23.66 8/5/2017 26 5 21.908
310294 12.57 8/13/2017 18 2 9.675
312589 18.34 8/29/2017 2 7 12.93
312591 11.96 8/16/2017 15 7 15.12571429
312593 8.98 7/2/2017 60 1 8.98
312595 19.37 8/18/2017 13 4 11.8025
312633 8.77 8/27/2017 4 13 7.446923077
312634 8.49 6/29/2017 63 4 8.49
312659 10.08 6/23/2017 69 1 10.08
313602 7.49 8/26/2017 5 9 8.704444444
313615 10.47 6/6/2017 86 1 10.47
313618 10.49 7/23/2017 39 1 10.49
313625 7.28 7/26/2017 36 1 7.28
313630 8.37 8/23/2017 8 3 23.95
313635 9.07 8/3/2017 28 2 8.025
313651 7.78 8/30/2017 1 1 7.78
313668 15.77 6/3/2017 89 1 15.77
313679 10.17 8/14/2017 17 2 10.17
313691 10.56 8/8/2017 23 4 12.03
313693 93.86 8/24/2017 7 5 38.108
313695 7.99 8/23/2017 8 8 7.615
313706 17.05 8/27/2017 4 4 16.355
313708 20.87 8/5/2017 26 14 12.20857143
313715 7.99 8/28/2017 3 18 10.63333333
313741 32.5 8/12/2017 19 4 17.245
313744 16.96 8/8/2017 23 1 16.96
313765 7.49 8/19/2017 12 1 7.49
313778 8.38 7/24/2017 38 1 8.38
313785 11.97 8/29/2017 2 7 10.25571429
313818 6.49 7/31/2017 31 1 6.49
313822 20.35 7/18/2017 44 1 20.35
313828 10.28 7/20/2017 42 1 10.28
313843 11.87 6/19/2017 73 1 11.87
313847 19.36 8/25/2017 6 10 9.525
313858 8.08 8/25/2017 6 6 9.076666667
313862 6 7/28/2017 34 4 11.3575
313866 11.16 8/31/2017 0 6 15.6
313868 9.27 8/26/2017 5 1 9.27
313879 10.08 7/3/2017 59 3 12.01
313889 7.97 8/18/2017 13 13 8.322307692
313890 19.86 8/6/2017 25 2 17.51
313891 17.94 7/26/2017 36 4 15.0475
313892 9.88 8/30/2017 1 8 10.39875
313899 9.27 8/31/2017 0 11 12.88909091
313904 19.94 8/9/2017 22 2 14.705
313905 19.12 8/12/2017 19 4 22.3525
313914 9.08 8/18/2017 13 6 13.64333333
313917 10.17 8/28/2017 3 1 10.17
313922 7.99 6/30/2017 62 1 7.99
313923 9.57 8/14/2017 17 3 10.07333333
313927 6.99 7/25/2017 37 1 6.99
313928 8.79 8/31/2017 0 2 7.78
313934 7.19 8/29/2017 2 3 11.01666667
313936 9.38 6/27/2017 65 2 9.83
313937 7.56 8/15/2017 16 2 9.86
313938 22.34 8/30/2017 1 5 18.678
313948 21.16 8/5/2017 26 2 19.81
313951 9.27 8/29/2017 2 1 9.27
313958 8.49 8/30/2017 1 1 8.49
313972 10.77 6/12/2017 80 1 10.77
313975 11.74 7/25/2017 37 3 13.19666667
313989 6.48 8/22/2017 9 2 14.415
313992 8.49 8/22/2017 9 3 8.323333333
313997 9.38 6/12/2017 80 1 9.38
314000 8.27 7/10/2017 52 2 8.27
314003 20.35 8/20/2017 11 9 9.475555556
314005 9.88 8/28/2017 3 34 10.44970588
314006 8.47 8/28/2017 3 8 24.32625
314017 6.88 8/3/2017 28 1 6.88
314018 17.24 7/18/2017 44 1 17.24
314020 21.36 8/29/2017 2 1 21.36
314022 10.28 8/5/2017 26 1 10.28
314023 21.64 7/4/2017 58 2 17.895
314035 12.77 7/10/2017 52 1 12.77
314037 21.74 8/12/2017 19 5 13.4
314048 10.47 8/25/2017 6 4 9.8975
314054 12.78 8/30/2017 1 9 13.40333333
314059 22.94 8/5/2017 26 1 22.94
314082 23.04 8/23/2017 8 1 23.04
314086 13.26 8/21/2017 10 4 12.39
314090 7.08 8/6/2017 25 2 8.08
314091 10.28 6/26/2017 66 2 10.28
314092 13.94 8/7/2017 24 1 13.94
314099 6.19 7/30/2017 32 1 6.19
314107 24.35 8/18/2017 13 2 21.155
314108 8.17 8/31/2017 0 25 9.0932
314111 10.58 8/28/2017 3 5 10.816
314114 7.23 8/16/2017 15 2 7.23
314120 27.24 7/22/2017 40 1 27.24
314121 14.37 8/7/2017 24 1 14.37
314122 17.66 6/21/2017 71 1 17.66
314127 21.16 8/28/2017 3 6 19.955
314134 24.62 6/30/2017 62 1 24.62
314140 27.72 8/25/2017 6 36 9.754166667
314143 14.48 8/17/2017 14 3 12.10666667
314145 21.56 7/14/2017 48 4 18.0125
314146 8.26 8/15/2017 16 5 9.788
314153 13.17 8/1/2017 30 3 15.4
314160 24.56 8/9/2017 22 3 12.84
314161 16.15 8/29/2017 2 13 18.17
314163 7.88 8/21/2017 10 2 8.175
314164 9.97 7/14/2017 48 1 9.97
314167 13.46 8/28/2017 3 4 10.96
314173 19.75 6/19/2017 73 1 19.75
314175 50.55 6/12/2017 80 1 50.55
314178 34.04 8/28/2017 3 9 18.92666667
314179 11.47 8/22/2017 9 7 15.49857143
314181 17.97 7/13/2017 49 1 17.97
314186 9.74 7/28/2017 34 1 9.74
314189 6.97 8/29/2017 2 15 9.236666667
314190 10.06 8/6/2017 25 1 10.06
314192 26.76 7/31/2017 31 1 26.76
314198 8.07 8/21/2017 10 2 7.78
314202 21.82 8/12/2017 19 5 16.184
314207 9.67 8/29/2017 2 7 11.39571429
314208 9.27 8/28/2017 3 3 9.27
314214 9.36 8/6/2017 25 3 12.54
314221 10.67 6/30/2017 62 1 10.67
314222 18.39 8/24/2017 7 8 16.1175
314223 62.42 8/24/2017 7 7 33.48285714
314226 16.71 8/16/2017 15 5 12.082
314229 18.56 8/26/2017 5 1 18.56
314231 32.21 7/9/2017 53 1 32.21
314238 16.86 8/13/2017 18 5 13.928
314239 13.66 8/25/2017 6 14 9.75
314246 22.72 8/28/2017 3 3 17.17666667
314255 8.18 8/30/2017 1 2 7.485
314256 10 7/3/2017 59 2 11.68
314258 9.47 8/11/2017 20 1 9.47
314260 18.66 8/4/2017 27 5 16.464
314263 14.16 7/25/2017 37 3 22.13333333
314274 32.82 8/6/2017 25 4 19.73
314276 13.26 8/4/2017 27 3 12.43
314283 20.25 6/16/2017 76 1 20.25
314288 8.07 7/9/2017 53 2 8.67
314289 20.14 8/30/2017 1 9 16.61555556
314296 7.99 6/30/2017 62 2 7.99
314298 7.49 8/28/2017 3 15 8.435333333
314299 30.15 7/11/2017 51 2 21.4
314301 8.69 7/19/2017 43 1 8.69
314306 13.07 7/23/2017 39 3 13.64
314314 7.74 8/31/2017 0 56 7.876071429
314315 18.94 8/17/2017 14 3 16.41333333
314325 6.79 7/29/2017 33 2 7.39
314331 7.57 8/17/2017 14 4 11.9975
314338 10.07 8/24/2017 7 2 10.07
314340 8.07 8/31/2017 0 26 11.98923077
314343 19.34 8/17/2017 14 3 19.74
314344 26.07 8/7/2017 24 1 26.07
314348 19.44 7/31/2017 31 4 16.9
314353 27.14 6/19/2017 73 1 27.14
314355 13.98 7/24/2017 38 1 13.98
314356 9.98 8/29/2017 2 12 10.505
314359 15.54 8/15/2017 16 1 15.54
314371 6.97 8/27/2017 4 18 9.247222222
314375 10.48 7/12/2017 50 7 9.217142857
314376 8.58 7/4/2017 58 6 7.795
314377 9.77 8/15/2017 16 7 13.2
314384 13.66 8/4/2017 27 2 17.995
314387 17.15 7/23/2017 39 3 16.84666667
314389 11.77 8/25/2017 6 2 11.77
314390 19.74 8/23/2017 8 1 19.74
314395 9.67 8/24/2017 7 4 9.1375
314396 7.18 8/25/2017 6 15 7.585333333
314398 12.02 8/22/2017 9 10 11.365
314401 16.54 8/31/2017 0 7 19.61571429
314408 16.27 8/25/2017 6 5 10.136
314410 12.17 7/27/2017 35 3 11.84
314413 8.28 8/29/2017 2 6 7.73
314416 20.65 8/14/2017 17 4 12.075
314420 11.47 8/26/2017 5 9 9.922222222
314424 39.88 6/14/2017 78 1 39.88
314425 8.98 8/3/2017 28 2 8.98
314431 9.87 7/23/2017 39 2 9.12
314434 25.57 8/25/2017 6 2 17.545
314439 7.39 8/29/2017 2 3 7.39
314445 7.67 8/4/2017 27 1 7.67
314446 18.14 8/12/2017 19 1 18.14
314460 7.97 8/31/2017 0 8 11.92875
314466 6.06 8/22/2017 9 3 10.51
314472 20.26 8/30/2017 1 1 20.26
314473 16.95 8/9/2017 22 2 15.025
314474 22.53 8/5/2017 26 3 20.16666667
314475 11.97 6/11/2017 81 1 11.97
314484 8.8 8/27/2017 4 10 9.492
314486 7.19 7/17/2017 45 3 7.186666667
314504 28.33 6/10/2017 82 1 28.33
314509 6.08 7/17/2017 45 3 6.846666667
314512 12.45 8/12/2017 19 5 13.516
314519 14.08 7/31/2017 31 1 14.08
314527 8.08 8/21/2017 10 8 8.51625
314531 8.27 8/31/2017 0 3 9.096666667
314532 6.38 7/10/2017 52 2 7.23
314535 29.81 7/8/2017 54 3 17.15333333
314538 8.27 8/14/2017 17 7 8.647142857
314541 9.27 8/28/2017 3 1 9.27
314544 18.16 7/30/2017 32 5 13.646
314549 8.27 8/24/2017 7 2 11.62
314556 8.07 6/15/2017 77 1 8.07
314566 7.99 8/11/2017 20 1 7.99
314571 10.27 8/29/2017 2 6 10.28666667
314581 49.94 7/25/2017 37 2 41.975
314587 7.97 8/15/2017 16 1 7.97
314595 11.18 8/23/2017 8 9 11.93333333
314597 11.95 7/4/2017 58 1 11.95
314598 10.08 8/28/2017 3 20 10.2225
314600 8.98 8/24/2017 7 2 8.03
314601 24.34 7/16/2017 46 1 24.34
314616 10.08 8/18/2017 13 4 14.52
314619 17.66 8/27/2017 4 21 15.1752381
314623 10.17 8/10/2017 21 5 11.036
314628 18.76 7/19/2017 43 4 14.9125
314632 6.68 8/25/2017 6 4 7.935
314639 17.44 7/12/2017 50 1 17.44
314640 9.67 8/4/2017 27 1 9.67
314646 29.3 6/24/2017 68 1 29.3
314650 9.47 8/31/2017 0 11 11.36727273
314670 8.49 8/30/2017 1 7 8.49
314672 7.18 7/10/2017 52 4 7.585
314678 8.17 8/30/2017 1 6 11.43666667
314688 9.47 8/1/2017 30 1 9.47
314689 29.42 8/6/2017 25 3 28.91666667
314708 20.83 8/30/2017 1 3 12.76
314717 15.36 8/31/2017 0 12 10.28833333
314721 17.26 7/24/2017 38 4 11.6425
314723 6.79 8/26/2017 5 7 8.287142857
314726 8.37 8/18/2017 13 4 9.0675
314727 10.27 8/29/2017 2 3 10.33666667
314728 10.48 8/27/2017 4 3 9.91
314731 10.67 8/31/2017 0 1 10.67
314733 7.18 6/13/2017 79 2 7.68
314738 9.06 8/12/2017 19 10 13.196
314744 18.06 8/31/2017 0 5 19.202
314745 7.78 8/29/2017 2 11 9.722727273
314747 9.76 8/28/2017 3 3 9.693333333
314756 14.27 8/20/2017 11 2 11.625
314762 8.47 8/24/2017 7 1 8.47
314763 9.67 8/4/2017 27 3 9.206666667
314767 11.95 8/29/2017 2 30 11.36366667
314775 8.67 8/22/2017 9 1 8.67
314776 13.47 8/15/2017 16 4 10.7325
314782 8.48 8/27/2017 4 5 9.754
314783 8.57 8/18/2017 13 1 8.57
314785 7.63 8/31/2017 0 7 7.832857143
314787 23.72 8/30/2017 1 3 13.33
314793 6.99 6/10/2017 82 1 6.99
314797 10.78 8/23/2017 8 3 10.78
314803 7.28 8/28/2017 3 9 9.412222222
314807 7.32 7/18/2017 44 2 7.32
314811 11.67 8/31/2017 0 2 9.83
314814 8.27 8/31/2017 0 14 7.998571429
314828 9.85 8/19/2017 12 10 16.641
314829 22.96 7/6/2017 56 1 22.96
314832 9.38 6/8/2017 84 1 9.38
314843 8.28 6/5/2017 87 1 8.28
314863 16.14 6/14/2017 78 1 16.14
314868 7.37 8/21/2017 10 5 14.546
314871 6.98 8/28/2017 3 1 6.98
314882 13.38 7/30/2017 32 1 13.38
314883 7.77 8/25/2017 6 18 8.441666667
314898 9.67 8/31/2017 0 32 7.9753125
314900 6.47 8/15/2017 16 1 6.47
314902 7.44 8/19/2017 12 4 12.2425
314904 16.56 8/16/2017 15 5 15.222
314909 16.27 8/19/2017 12 4 14.9175
314912 7.77 8/1/2017 30 3 8.71
314915 8.16 7/11/2017 51 2 10.18
314933 11.67 8/21/2017 10 9 11.67
314940 9.06 8/8/2017 23 2 12.9
314957 8.57 8/31/2017 0 6 12.78833333
314972 11.47 6/29/2017 63 3 11.14
314975 9.66 8/9/2017 22 2 9.615
314985 9.38 7/7/2017 55 2 8.54
314996 13.54 7/13/2017 49 2 12.295
315002 11.43 7/8/2017 54 2 16.525
315032 7.19 6/23/2017 69 2 8.09
315048 17.98 8/31/2017 0 2 17.98
315051 6.79 7/7/2017 55 4 6.8125
315054 11.97 8/22/2017 9 4 10.025
315056 8.78 6/27/2017 65 3 8.766666667
315059 25.14 8/9/2017 22 1 25.14
315061 30.44 6/24/2017 68 1 30.44
315063 9.67 8/30/2017 1 2 9.72
315070 6.67 8/15/2017 16 4 8.94
315072 16.96 8/15/2017 16 6 17.21833333
315073 16.66 6/19/2017 73 1 16.66
315082 7.67 8/7/2017 24 1 7.67
315083 30.89 6/8/2017 84 1 30.89
315089 9.37 7/19/2017 43 2 9.67
315097 8.44 7/18/2017 44 2 12.13
315098 11.37 6/30/2017 62 1 11.37
315110 9.78 8/16/2017 15 1 9.78
315111 40.17 8/11/2017 20 3 20.54
315116 11.68 7/19/2017 43 1 11.68
315122 8.27 6/30/2017 62 1 8.27
315126 9.59 7/2/2017 60 3 10.34
315128 17.83 8/21/2017 10 1 17.83
315132 7.99 7/25/2017 37 2 12.665
315147 8 8/26/2017 5 3 10.71333333
315155 10 7/3/2017 59 2 9.785
315156 8.16 8/23/2017 8 9 9.218888889
315160 16.77 8/27/2017 4 4 12.85
315161 11.28 8/1/2017 30 1 11.28
315166 7.98 8/28/2017 3 2 10.175
315177 14.05 8/15/2017 16 4 10.45
315184 5.99 6/27/2017 65 3 7.413333333
315187 9.52 8/3/2017 28 3 10.01333333
315191 7.98 8/18/2017 13 3 12.70666667
315195 18.85 7/29/2017 33 1 18.85
315198 10.98 7/27/2017 35 2 18.06
315203 6.99 7/7/2017 55 1 6.99
315204 16.26 8/25/2017 6 3 13.83333333
315205 31.63 8/22/2017 9 4 25.605
315230 20.55 8/12/2017 19 3 21.18666667
315233 20.95 8/5/2017 26 1 20.95
315235 8.47 8/6/2017 25 3 7.71
315242 11.16 6/9/2017 83 1 11.16
315246 8.98 8/30/2017 1 5 8.86
315252 8.99 8/20/2017 11 14 9.035
315262 11.87 8/29/2017 2 7 23.83
315264 13.75 6/3/2017 89 1 13.75
315266 10.59 6/11/2017 81 1 10.59
315270 11.98 8/26/2017 5 1 11.98
315273 15.16 8/24/2017 7 1 15.16
315278 9.28 8/31/2017 0 4 11.775
315287 27.03 8/24/2017 7 3 15.45333333
315293 8.34 8/31/2017 0 4 8.1175
315294 8.47 8/24/2017 7 5 9.28
315295 24.54 8/26/2017 5 8 18.445
315296 8.47 6/1/2017 91 1 8.47
315323 21.94 8/27/2017 4 10 14.309
315329 12.37 7/31/2017 31 3 12.87
315333 6.88 6/18/2017 74 2 6.935
315337 9.28 8/28/2017 3 7 8.272857143
315347 6.78 8/10/2017 21 5 7.678
315348 5.99 8/11/2017 20 4 13.7975
315355 15.74 8/15/2017 16 5 16.822
315364 6.89 8/26/2017 5 7 11.82428571
315372 20.92 8/3/2017 28 4 15.1725
315375 7.55 8/6/2017 25 4 11.4875
315377 11.37 8/25/2017 6 10 10.366
315384 9.47 8/30/2017 1 3 7.546666667
315385 6.47 8/8/2017 23 7 6.727142857
315388 7.89 8/31/2017 0 12 11.265
315391 12 8/21/2017 10 1 12
315396 7.36 6/28/2017 64 1 7.36
315398 12.37 8/27/2017 4 3 10.07666667
315400 17.34 8/25/2017 6 1 17.34
315401 8.98 8/12/2017 19 6 9.126666667
315415 12.36 6/30/2017 62 1 12.36
315417 10.58 8/28/2017 3 5 9.052
315424 8.27 8/23/2017 8 9 10.50222222
315427 9.47 8/5/2017 26 2 10.57
315437 11.87 7/13/2017 49 1 11.87
315440 10.56 7/31/2017 31 3 11.42
315446 6.17 8/3/2017 28 4 17.525
315447 9.08 8/10/2017 21 3 9.806666667
315448 7.99 7/29/2017 33 2 9.28
315449 18.94 8/30/2017 1 2 12.865
315453 13.26 8/21/2017 10 5 8.512
315461 7.18 7/26/2017 36 1 7.18
315466 19.75 8/30/2017 1 4 22.1525
315468 6.99 7/29/2017 33 6 10.36166667
315473 12.94 8/29/2017 2 5 13.476
315474 8.37 8/17/2017 14 3 9.466666667
315477 6.49 8/31/2017 0 1 6.49
315480 18.94 6/25/2017 67 1 18.94
315483 12.07 8/6/2017 25 6 12.48833333
315489 8.17 8/8/2017 23 3 13.06
315492 6.67 8/8/2017 23 1 6.67
315497 9.65 8/21/2017 10 1 9.65
315498 12.36 8/5/2017 26 2 10.265
315499 13.17 7/30/2017 32 1 13.17
315503 8.71 6/29/2017 63 5 12.854
315511 9.67 8/15/2017 16 5 9.992
315513 9.58 8/24/2017 7 2 9.125
315522 8.47 7/12/2017 50 1 8.47
315523 10.47 8/1/2017 30 2 8.63
315532 8.47 8/16/2017 15 5 11.362
315533 10.29 6/7/2017 85 1 10.29
315538 6.39 7/8/2017 54 2 16.51
315542 18.66 7/6/2017 56 1 18.66
315549 21.54 8/22/2017 9 6 20.71333333
315550 59.33 8/1/2017 30 5 19.566
315551 17.56 8/24/2017 7 5 12.908
315552 10.75 7/22/2017 40 3 8.796666667
315556 6.06 7/26/2017 36 2 7.66
315559 14.98 8/15/2017 16 4 21.93
315562 13.15 8/6/2017 25 8 9.9725
315563 9.47 8/30/2017 1 1 9.47
315567 18.77 8/28/2017 3 2 25.955
315575 10.86 8/22/2017 9 5 10.626
315579 7.38 7/31/2017 31 1 7.38
315581 8.78 8/17/2017 14 1 8.78
315582 6.99 8/19/2017 12 1 6.99
315591 22.86 8/11/2017 20 4 22.4925
315599 7.77 8/9/2017 22 1 7.77
315602 6.18 8/20/2017 11 4 6.18
315608 12.36 8/21/2017 10 1 12.36
315609 8.98 7/10/2017 52 2 11.21
315610 7.99 8/25/2017 6 6 14.73833333
315611 8.49 8/31/2017 0 3 8.323333333
315618 0 7/25/2017 37 4 17.85
315629 8.67 8/6/2017 25 4 8.17
315632 14.66 8/15/2017 16 2 10.475
315634 8.47 7/25/2017 37 2 8.82
315638 13.25 7/25/2017 37 2 13.055
315642 17.47 7/22/2017 40 2 12.13
315645 6.99 7/6/2017 56 1 6.99
315649 22.03 8/6/2017 25 1 22.03
315650 8.43 8/25/2017 6 2 9.15
315651 12.94 8/15/2017 16 5 14.666
315654 7.49 8/8/2017 23 2 9.98
315655 13.95 7/28/2017 34 2 11.21
315660 8.27 7/27/2017 35 1 8.27
315663 6.99 8/29/2017 2 5 6.664
315665 9.48 6/30/2017 62 2 8.885
315670 10.07 8/17/2017 14 2 8.47
315672 10.78 8/5/2017 26 1 10.78
315673 12.48 8/3/2017 28 2 17.265
315680 14.26 8/21/2017 10 4 14.13
315684 8.07 6/2/2017 90 1 8.07
315685 11.97 7/20/2017 42 3 10.64666667
315688 11.9 8/27/2017 4 3 10.49
315689 39.9 7/2/2017 60 1 39.9
315697 30.23 8/4/2017 27 3 18.56
315700 11.05 8/6/2017 25 4 10.335
315702 12.06 8/4/2017 27 2 10.765
315703 8.47 8/20/2017 11 2 9.915
315705 8.07 8/14/2017 17 3 8.043333333
315707 23.34 7/29/2017 33 1 23.34
315711 10.57 7/6/2017 56 4 11.3325
315712 22.36 8/7/2017 24 1 22.36
315717 8.88 7/22/2017 40 1 8.88
315723 10.47 8/21/2017 10 6 10.95166667
315725 6.79 8/22/2017 9 6 7.338333333
315726 10.97 6/23/2017 69 2 9.48
315730 12.01 8/30/2017 1 4 11.875
315731 28.73 8/15/2017 16 5 14.042
315740 7.28 8/9/2017 22 2 7.28
315754 8.18 6/11/2017 81 1 8.18
315755 9.24 8/27/2017 4 8 8.22375
315760 22 7/3/2017 59 1 22
315768 18.76 8/4/2017 27 2 19.405
315779 21.55 6/10/2017 82 1 21.55
315785 6.79 6/5/2017 87 1 6.79
315788 10.58 8/15/2017 16 3 10.05333333
315793 6.79 7/25/2017 37 4 9.23
315799 12.59 8/23/2017 8 3 10.35333333
315802 11.86 8/31/2017 0 3 17.02
315809 8.76 8/1/2017 30 2 8.76
315817 11.26 7/30/2017 32 2 9.765
315818 9.67 6/20/2017 72 1 9.67
315826 8.48 8/6/2017 25 4 8.8525
315845 11.07 8/5/2017 26 1 11.07
315853 8.47 7/29/2017 33 5 16.268
315854 27.93 7/9/2017 53 1 27.93
315855 12.76 7/5/2017 57 4 10.57
315856 10.78 7/28/2017 34 1 10.78
315860 17.46 8/24/2017 7 1 17.46
315861 8.49 8/8/2017 23 2 7.39
315873 32.84 7/30/2017 32 1 32.84
315875 20.75 6/12/2017 80 1 20.75
315883 19.64 6/13/2017 79 1 19.64

On 13 October 2017 at 10:35, PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hi
>
> Your statement about attaching data is problematic. We cannot do much with
> it. Instead use output from dput(yourdata) to show us what exactly your
> data look like.
>
> We also do not know how do you want to split your data. It would be nice
> if you can show also what should be the bins with respective data. Unless
> you provide this information you probably would not get any sensible answer.
>
> Cheers
> Petr
>
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Hemant
> Sain
> > Sent: Thursday, October 12, 2017 10:18 AM
> > To: r-help mailing list <r-help at r-project.org>
> > Subject: [R] How to define proper breaks in RFM analysis
> >
> > Hello,
> > I'm working on RFM analysis and i wanted to define my own breaks but my
> > frequency distribution is not normally distributed so when I'm using
> quartile its
> > not giving the optimal results.
> > so I'm looking for a better approach where i can define breaks
> dynamically
> > because after visualization i can do it easily but i want to apply this
> model so
> > that it can automatically define the breaks according to data set.
> > I'm attaching sample data for reference.
> >
> > Thanks
> >
> >                            *Freq*
> > 5
> > 15
> > 1
> > 8
> > 2
> > 2
> > 2
> > 1
> > 1
> > 2
> > 2
> > 1
> > 1
> > 1
> > 1
> > 4
> > 10
> > 22
> > 24
> > 1
> > 1
> > 2
> > 2
> > 7
> > 1
> > 1
> > 4
> > 6
> > 1
> > 1
> > 2
> > 2
> > 17
> > 2
> > 7
> > 1
> > 5
> > 2
> > 7
> > 7
> > 1
> > 4
> > 13
> > 4
> > 1
> > 9
> > 1
> > 1
> > 1
> > 3
> > 2
> > 1
> > 1
> > 2
> > 4
> > 5
> > 8
> > 4
> > 14
> > 18
> > 4
> > 1
> > 1
> > 1
> > 7
> > 1
> > 1
> > 1
> > 1
> > 10
> > 6
> > 4
> > 6
> > 1
> > 3
> > 13
> > 2
> > 4
> > 8
> > 11
> > 2
> > 4
> > 6
> > 1
> > 1
> > 3
> > 1
> > 2
> > 3
> > 2
> > 2
> > 5
> > 2
> > 1
> > 1
> > 1
> > 3
> > 2
> > 3
> > 1
> > 2
> > 9
> > 34
> > 8
> > 1
> > 1
> > 1
> > 1
> > 2
> > 1
> > 5
> > 4
> > 9
> > 1
> > 1
> > 4
> > 2
> > 2
> > 1
> > 1
> > 2
> > 25
> > 5
> > 2
> > 1
> > 1
> > 1
> > 6
> > 1
> > 36
> > 3
> > 4
> > 5
> > 3
> > 3
> > 13
> > 2
> > 1
> > 4
> > 1
> > 1
> > 9
> > 7
> > 1
> > 1
> > 15
> > 1
> > 1
> > 2
> > 5
> > 7
> > 3
> > 3
> > 1
> > 8
> > 7
> > 5
> > 1
> > 1
> > 5
> > 14
> > 3
> > 2
> > 2
> > 1
> > 5
> > 3
> > 4
> > 3
> > 1
> > 2
> > 9
> > 2
> > 15
> > 2
> > 1
> > 3
> > 56
> > 3
> > 2
> > 4
> > 2
> > 26
> > 3
> > 1
> > 4
> > 1
> > 1
> > 12
> > 1
> > 18
> > 7
> > 6
> > 7
> > 2
> > 3
> > 2
> > 1
> > 4
> > 15
> > 10
> > 7
> > 5
> > 3
> > 6
> > 4
> > 9
> > 1
> > 2
> > 2
> > 2
> > 3
> > 1
> > 1
> > 8
> > 3
> > 1
> > 2
> > 3
> > 1
> > 10
> > 3
> > 1
> > 3
> > 5
> > 1
> > 8
> > 3
> > 2
> > 3
> > 7
> > 1
> > 5
> > 2
> > 1
> > 1
> > 6
> > 2
> > 1
> > 9
> > 1
> > 20
> > 2
> > 1
> > 4
> > 21
> > 5
> > 4
> > 4
> > 1
> > 1
> > 1
> > 11
> > 7
> > 4
> > 6
> > 1
> > 3
> > 3
> > 12
> > 4
> > 7
> > 4
> > 3
> > 3
> > 1
> > 2
> > 10
> > 5
> > 11
> > 3
> > 2
> > 1
> > 3
> > 30
> > 1
> > 4
> > 5
> > 1
> > 7
> > 3
> > 1
> > 3
> > 9
> > 2
> > 2
> > 14
> > 10
> > 1
> > 1
> > 1
> > 1
> > 5
> > 1
> > 1
> > 18
> > 32
> > 1
> > 4
> > 5
> > 4
> > 3
> > 2
> > 9
> > 2
> > 6
> > 3
> > 2
> > 2
> > 2
> > 2
> > 2
> > 2
> > 4
> > 4
> > 3
> > 1
> > 1
> > 2
> > 4
> > 6
> > 1
> > 1
> > 1
> > 2
> > 2
> > 1
> > 1
> > 3
> > 1
> > 1
> > 3
> > 1
> > 2
> > 3
> > 2
> > 9
> > 4
> > 1
> > 2
> > 4
> > 3
> > 3
> > 3
> > 1
> > 2
> > 1
> > 3
> > 4
> > 3
> > 1
> > 3
> > 1
> > 5
> > 14
> > 7
> > 1
> > 1
> > 1
> > 1
> > 4
> > 3
> > 4
> > 5
> > 8
> > 1
> > 10
> > 3
> > 2
> > 7
> > 5
> > 4
> > 5
> > 7
> > 4
> > 4
> > 10
> > 3
> > 7
> > 12
> > 1
> > 1
> > 3
> > 1
> > 6
> > 1
> > 5
> > 9
> > 2
> > 1
> > 3
> > 4
> > 3
> > 2
> > 2
> > 5
> > 1
> > 4
> > 6
> > 5
> > 3
> > 1
> > 1
> > 6
> > 3
> > 1
> > 1
> > 2
> > 1
> > 5
> > 5
> > 2
> > 1
> > 2
> > 5
> > 1
> > 2
> > 1
> > 6
> > 5
> > 5
> > 3
> > 2
> > 4
> > 8
> > 1
> > 2
> > 5
> > 1
> > 1
> > 1
> > 4
> > 1
> > 4
> > 1
> > 2
> > 6
> > 3
> > 4
> > 4
> > 2
> > 2
> > 2
> > 2
> > 1
> > 1
> > 2
> > 5
> > 2
> > 2
> > 1
> > 5
> > 2
> > 2
> > 1
> > 2
> > 4
> > 1
> > 3
> > 3
> > 1
> > 3
> > 4
> > 2
> > 2
> > 3
> > 1
> > 4
> > 1
> > 1
> > 6
> > 6
> > 2
> > 4
> > 5
> > 2
> > 1
> > 8
> > 1
> > 2
> > 1
> > 1
> > 3
> > 4
> > 3
> > 3
> > 2
> > 2
> > 1
> > 4
> > 1
> > 5
> > 1
> > 4
> > 1
> > 1
> > 2
> > 1
> > 1
> > 1
> > 8
> > 1
> > 1
> > 1
> > 1
> > 1
> > 3
> > 3
> > 5
> > 2
> > 3
> > 1
> > 1
> > 5
> > 2
> > 3
> > 6
> > 3
> > 3
> > 14
> > 2
> > 1
> > 1
> > 2
> > 1
> > 2
> > 4
> > 2
> > 1
> > 6
> > 1
> > 7
> > 2
> > 3
> > 3
> > 2
> > 2
> > 2
> > 2
> > 1
> > 2
> > 4
> > 1
> > 6
> > 2
> > 5
> > 2
> > 1
> > 2
> > 2
> > 5
> > 8
> > 4
> > 1
> > 1
> > 1
> > 1
> > 4
> > 1
> > 3
> > 2
> > 1
> > 2
> > 2
> > 3
> > 3
> > 3
> > 6
> > 1
> > 1
> > 1
> > 5
> > 7
> > 1
> > 5
> > 2
> > 1
> > 1
> > 1
> > 3
> > 20
> > 2
> > 3
> > 3
> > 1
> > 2
> > 1
> > 15
> > 4
> > 4
> > 1
> > 1
> > 2
> > 1
> > 1
> > 3
> > 2
> > 6
> > 5
> > 1
> > 5
> > 1
> > 7
> > 4
> > 3
> > 2
> > 5
> > 2
> > 1
> > 1
> > 3
> > 2
> > 6
> > 2
> > 4
> > 2
> > 1
> > 24
> > 4
> > 17
> > 1
> > 3
> > 2
> > 2
> > 2
> > 2
> > 8
> > 1
> > 3
> > 1
> > 9
> > 2
> > 4
> > 1
> > 1
> > 6
> > 3
> > 4
> > 1
> > 9
> > 2
> > 1
> > 3
> > 2
> > 6
> > 2
> > 1
> > 3
> > 1
> > 20
> > 3
> > 4
> > 7
> > 3
> > 7
> > 1
> > 7
> > 1
> > 5
> > 2
> > 1
> > 1
> > 1
> > 2
> > 1
> > 2
> > 4
> > 1
> > 1
> > 2
> > 3
> > 4
> > 1
> > 4
> > 1
> > 1
> > 1
> > 1
> > 1
> > 2
> > 1
> > 6
> > 2
> > 3
> > 2
> > 1
> > 9
> > 8
> > 11
> > 1
> > 1
> > 2
> > 1
> > 3
> > 1
> > 2
> > 15
> > 5
> > 2
> > 2
> > 9
> > 1
> > 4
> > 2
> > 1
> > 1
> > 14
> > 1
> > 1
> > 2
> > 1
> > 3
> > 10
> > 1
> > 1
> > 3
> > 1
> > 1
> > 1
> > 4
> > 2
> > 8
> > 1
> > 2
> > 2
> > 1
> > 11
> > 1
> > 3
> > 1
> > 7
> > 1
> > 3
> > 10
> > 3
> > 3
> > 1
> > 1
> > 1
> > 11
> > 1
> > 2
> > 1
> > 1
> > 2
> > 2
> > 5
> > 5
> > 4
> > 1
> > 2
> > 5
> > 2
> > 4
> > 3
> > 1
> > 1
> > 2
> > 2
> > 4
> > 2
> > 1
> > 1
> > 4
> > 1
> > 4
> > 1
> > 5
> > 1
> > 1
> > 2
> > 1
> > 4
> > 7
> > 1
> > 2
> > 1
> > 2
> > 9
> > 3
> > 1
> > 7
> > 2
> > 2
> > 1
> > 2
> > 3
> > 5
> > 2
> > 7
> > 1
> > 5
> > 1
> > 1
> > 2
> > 2
> > 2
> > 4
> > 2
> > 8
> > 4
> > 6
> > 1
> > 1
> > 1
> > 1
> > 1
> > 16
> > 2
> > 1
> > 6
> > 4
> > 4
> > 1
> > 1
> > 1
> > 1
> > 4
> > 3
> > 2
> > 6
> > 11
> > 10
> > 21
> > 2
> > 1
> > 1
> > 3
> > 2
> > 2
> > 2
> > 7
> > 1
> > 6
> > 4
> > 1
> > 7
> > 4
> > 11
> > 1
> > 2
> > 8
> > 1
> > 1
> > 1
> > 1
> > 2
> > 17
> > 1
> > 2
> > 3
> > 4
> > 2
> > 1
> > 2
> > 2
> > 4
> > 3
> > 2
> > 3
> > 1
> > 3
> > 3
> > 1
> > 3
> > 37
> > 4
> > 3
> > 1
> > 2
> > 1
> > 1
> > 3
> > 1
> > 2
> > 3
> > 1
> > 5
> > 1
> > 2
> > 1
> > 3
> > 2
> > 3
> > 3
> > 4
> > 4
> > 2
> > 4
> > 1
> > 1
> > 3
> > 3
> > 2
> > 1
> > 2
> > 1
> > 1
> > 3
> > 1
> > 1
> > 3
> > 3
> > 4
> > 2
> > 4
> > 1
> > 1
> > 1
> > 10
> > 3
> > 2
> > 2
> > 2
> > 2
> > 2
> > 2
> > 1
> > 19
> > 2
> > 2
> > 4
> > 1
> > 3
> > 1
> > 13
> > 5
> > 2
> > 1
> > 2
> > 2
> > 4
> > 2
> > 1
> > 3
> > 5
> > 1
> > 1
> > 1
> > 6
> > 3
> > 9
> > 4
> > 1
> > 1
> > 1
> > 3
> > 1
> > 17
> > 4
> > 1
> > 4
> > 6
> > 2
> > 1
> > 2
> > 4
> > 4
> > 2
> > 2
> > 2
> > 4
> > 4
> > 1
> > 1
> > 1
> > 2
> > 7
> > 2
> > 1
> > 2
> > 8
> > 1
> > 1
> > 7
> > 4
> > 1
> > 2
> > 1
> > 1
> > 3
> > 1
> > 3
> > 1
> > 1
> > 3
> > 5
> > 8
> > 1
> > 1
> > 4
> > 1
> > 1
> > 15
> > 1
> > 1
> > 6
> > 2
> > 3
> > 3
> > 8
> > 4
> > 1
> > 4
> > 2
> > 4
> > 2
> > 5
> > 4
> > 4
> > 1
> > 1
> > 7
> > 1
> > 10
> > 1
> > 10
> > 2
> > 15
> > 7
> > 3
> > 1
> > 3
> > 2
> > 19
> > 2
> > 9
> > 1
> > 10
> > 2
> > 1
> > 2
> > 2
> > 4
> > 6
> > 1
> > 4
> > 3
> > 1
> > 2
> > 3
> > 2
> > 1
> > 3
> > 7
> > 1
> > 4
> > 2
> > 13
> > 2
> > 5
> > 3
> > 6
> > 2
> > 1
> > 2
> > 1
> > 5
> > 1
> >
> >
> > --
> > hemantsain.com
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>



-- 
hemantsain.com

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Fri Oct 13 11:51:35 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 13 Oct 2017 09:51:35 +0000
Subject: [R] How to define proper breaks in RFM analysis
In-Reply-To: <CAJL6Qs_eykK=Wa1XPkH37KoZrdM=4DYqtAdn7=P6-0ZiufVMWQ@mail.gmail.com>
References: <CAJL6Qs8tPRTVFi7h4Pwz9i55zPJx+K+1geF4E=S=SJ1Ke_LUjQ@mail.gmail.com>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB2CC0@SRVEXCHCM301.precheza.cz>
 <CAJL6Qs_eykK=Wa1XPkH37KoZrdM=4DYqtAdn7=P6-0ZiufVMWQ@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB2EAE@SRVEXCHCM301.precheza.cz>

Hi

You expect us to solve your problem but you ignore advice already recieved.

Your data are unreadable, use dput(yourdata) instead. see ?dput

> test<-read.table("clipboard", heade=T)
Error in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  :
  line 115 did not have 6 elements

What is ?ideal interval? can you define it? Should it be such to provide eqal number of observations?

Or maybe you could normalise your values and use quartile method.

Cheers
Petr

From: Hemant Sain [mailto:hemantsain55 at gmail.com]
Sent: Friday, October 13, 2017 8:51 AM
To: PIKAL Petr <petr.pikal at precheza.cz>
Cc: r-help mailing list <r-help at r-project.org>
Subject: Re: [R] How to define proper breaks in RFM analysis

Hey,
i want to define 3 ideal breaks (bin) for each variable one of those variables is attached in the previous email,
i don't want to consider quartile method because quartile is not working ideally for that data set because data distribution is non normal.
so i want you to suggest another method so that i can define 3 breaks with the ideal interval for Recency, frequency and monetary to calculate RFM score.
i'm again attaching you some of the data set.
please look into it and help me with the R code.
Thanks



Data

user_id

subtotal_amount

created_at

Recency

Frequency

Monetary

194849

6.99

8/22/2017

9

5

9.996

194978

14.78

8/28/2017

3

15

16.308

198614

18.44

7/31/2017

31

1

18.44

234569

34.99

8/20/2017

11

8

13.5075

252686

7.99

7/31/2017

31

2

7.99

291719

21.26

8/25/2017

6

2

15.67

291787

46.1

8/31/2017

0

2

32.57

292630

24.34

7/31/2017

31

1

24.34

295204

21.86

7/18/2017

44

1

21.86

295989

8.98

8/20/2017

11

2

14.095

298883

14.38

8/24/2017

7

2

11.185

308824

10.77

7/31/2017

31

1

10.77

308874

8.29

6/11/2017

81

1

8.29

309088

17.16

8/3/2017

28

1

17.16

309126

20.54

7/30/2017

32

1

20.54

309127

15.24

8/2/2017

29

4

13.3925

309159

10.78

8/28/2017

3

10

13.694

309170

8.66

8/29/2017

2

22

9.383636364

309190

7.19

8/31/2017

0

24

10.33791667

309218

8.49

6/22/2017

70

1

8.49

309250

18.27

7/30/2017

32

1

18.27

309358

8

8/31/2017

0

2

11.99

309418

43.21

8/13/2017

18

2

26.35

309421

6.49

8/26/2017

5

7

10.72428571

309440

20.37

6/24/2017

68

1

20.37

309468

11.37

6/10/2017

82

1

11.37

309538

9.08

7/30/2017

32

4

10.075

309548

7.06

8/30/2017

1

6

7.83

309564

9.57

6/10/2017

82

1

9.57

309616

7.37

6/27/2017

65

1

7.37

309751

8.87

8/5/2017

26

2

8.925

309788

11.21

8/4/2017

27

2

10.81

309842

10.68

8/31/2017

0

17

10.49647059

309938

17.77

8/20/2017

11

2

12.38

310017

8.06

8/31/2017

0

7

12.12

310125

8.47

8/4/2017

27

1

8.47

310126

23.66

8/5/2017

26

5

21.908

310294

12.57

8/13/2017

18

2

9.675

312589

18.34

8/29/2017

2

7

12.93

312591

11.96

8/16/2017

15

7

15.12571429

312593

8.98

7/2/2017

60

1

8.98

312595

19.37

8/18/2017

13

4

11.8025

312633

8.77

8/27/2017

4

13

7.446923077

312634

8.49

6/29/2017

63

4

8.49

312659

10.08

6/23/2017

69

1

10.08

313602

7.49

8/26/2017

5

9

8.704444444

313615

10.47

6/6/2017

86

1

10.47

313618

10.49

7/23/2017

39

1

10.49

313625

7.28

7/26/2017

36

1

7.28

313630

8.37

8/23/2017

8

3

23.95

313635

9.07

8/3/2017

28

2

8.025

313651

7.78

8/30/2017

1

1

7.78

313668

15.77

6/3/2017

89

1

15.77

313679

10.17

8/14/2017

17

2

10.17

313691

10.56

8/8/2017

23

4

12.03

313693

93.86

8/24/2017

7

5

38.108

313695

7.99

8/23/2017

8

8

7.615

313706

17.05

8/27/2017

4

4

16.355

313708

20.87

8/5/2017

26

14

12.20857143

313715

7.99

8/28/2017

3

18

10.63333333

313741

32.5

8/12/2017

19

4

17.245

313744

16.96

8/8/2017

23

1

16.96

313765

7.49

8/19/2017

12

1

7.49

313778

8.38

7/24/2017

38

1

8.38

313785

11.97

8/29/2017

2

7

10.25571429

313818

6.49

7/31/2017

31

1

6.49

313822

20.35

7/18/2017

44

1

20.35

313828

10.28

7/20/2017

42

1

10.28

313843

11.87

6/19/2017

73

1

11.87

313847

19.36

8/25/2017

6

10

9.525

313858

8.08

8/25/2017

6

6

9.076666667

313862

6

7/28/2017

34

4

11.3575

313866

11.16

8/31/2017

0

6

15.6

313868

9.27

8/26/2017

5

1

9.27

313879

10.08

7/3/2017

59

3

12.01

313889

7.97

8/18/2017

13

13

8.322307692

313890

19.86

8/6/2017

25

2

17.51

313891

17.94

7/26/2017

36

4

15.0475

313892

9.88

8/30/2017

1

8

10.39875

313899

9.27

8/31/2017

0

11

12.88909091

313904

19.94

8/9/2017

22

2

14.705

313905

19.12

8/12/2017

19

4

22.3525

313914

9.08

8/18/2017

13

6

13.64333333

313917

10.17

8/28/2017

3

1

10.17

313922

7.99

6/30/2017

62

1

7.99

313923

9.57

8/14/2017

17

3

10.07333333

313927

6.99

7/25/2017

37

1

6.99

313928

8.79

8/31/2017

0

2

7.78

313934

7.19

8/29/2017

2

3

11.01666667

313936

9.38

6/27/2017

65

2

9.83

313937

7.56

8/15/2017

16

2

9.86

313938

22.34

8/30/2017

1

5

18.678

313948

21.16

8/5/2017

26

2

19.81

313951

9.27

8/29/2017

2

1

9.27

313958

8.49

8/30/2017

1

1

8.49

313972

10.77

6/12/2017

80

1

10.77

313975

11.74

7/25/2017

37

3

13.19666667

313989

6.48

8/22/2017

9

2

14.415

313992

8.49

8/22/2017

9

3

8.323333333

313997

9.38

6/12/2017

80

1

9.38

314000

8.27

7/10/2017

52

2

8.27

314003

20.35

8/20/2017

11

9

9.475555556

314005

9.88

8/28/2017

3

34

10.44970588

314006

8.47

8/28/2017

3

8

24.32625

314017

6.88

8/3/2017

28

1

6.88

314018

17.24

7/18/2017

44

1

17.24

314020

21.36

8/29/2017

2

1

21.36

314022

10.28

8/5/2017

26

1

10.28

314023

21.64

7/4/2017

58

2

17.895

314035

12.77

7/10/2017

52

1

12.77

314037

21.74

8/12/2017

19

5

13.4

314048

10.47

8/25/2017

6

4

9.8975

314054

12.78

8/30/2017

1

9

13.40333333

314059

22.94

8/5/2017

26

1

22.94

314082

23.04

8/23/2017

8

1

23.04

314086

13.26

8/21/2017

10

4

12.39

314090

7.08

8/6/2017

25

2

8.08

314091

10.28

6/26/2017

66

2

10.28

314092

13.94

8/7/2017

24

1

13.94

314099

6.19

7/30/2017

32

1

6.19

314107

24.35

8/18/2017

13

2

21.155

314108

8.17

8/31/2017

0

25

9.0932

314111

10.58

8/28/2017

3

5

10.816

314114

7.23

8/16/2017

15

2

7.23

314120

27.24

7/22/2017

40

1

27.24

314121

14.37

8/7/2017

24

1

14.37

314122

17.66

6/21/2017

71

1

17.66

314127

21.16

8/28/2017

3

6

19.955

314134

24.62

6/30/2017

62

1

24.62

314140

27.72

8/25/2017

6

36

9.754166667

314143

14.48

8/17/2017

14

3

12.10666667

314145

21.56

7/14/2017

48

4

18.0125

314146

8.26

8/15/2017

16

5

9.788

314153

13.17

8/1/2017

30

3

15.4

314160

24.56

8/9/2017

22

3

12.84

314161

16.15

8/29/2017

2

13

18.17

314163

7.88

8/21/2017

10

2

8.175

314164

9.97

7/14/2017

48

1

9.97

314167

13.46

8/28/2017

3

4

10.96

314173

19.75

6/19/2017

73

1

19.75

314175

50.55

6/12/2017

80

1

50.55

314178

34.04

8/28/2017

3

9

18.92666667

314179

11.47

8/22/2017

9

7

15.49857143

314181

17.97

7/13/2017

49

1

17.97

314186

9.74

7/28/2017

34

1

9.74

314189

6.97

8/29/2017

2

15

9.236666667

314190

10.06

8/6/2017

25

1

10.06

314192

26.76

7/31/2017

31

1

26.76

314198

8.07

8/21/2017

10

2

7.78

314202

21.82

8/12/2017

19

5

16.184

314207

9.67

8/29/2017

2

7

11.39571429

314208

9.27

8/28/2017

3

3

9.27

314214

9.36

8/6/2017

25

3

12.54

314221

10.67

6/30/2017

62

1

10.67

314222

18.39

8/24/2017

7

8

16.1175

314223

62.42

8/24/2017

7

7

33.48285714

314226

16.71

8/16/2017

15

5

12.082

314229

18.56

8/26/2017

5

1

18.56

314231

32.21

7/9/2017

53

1

32.21

314238

16.86

8/13/2017

18

5

13.928

314239

13.66

8/25/2017

6

14

9.75

314246

22.72

8/28/2017

3

3

17.17666667

314255

8.18

8/30/2017

1

2

7.485

314256

10

7/3/2017

59

2

11.68

314258

9.47

8/11/2017

20

1

9.47

314260

18.66

8/4/2017

27

5

16.464

314263

14.16

7/25/2017

37

3

22.13333333

314274

32.82

8/6/2017

25

4

19.73

314276

13.26

8/4/2017

27

3

12.43

314283

20.25

6/16/2017

76

1

20.25

314288

8.07

7/9/2017

53

2

8.67

314289

20.14

8/30/2017

1

9

16.61555556

314296

7.99

6/30/2017

62

2

7.99

314298

7.49

8/28/2017

3

15

8.435333333

314299

30.15

7/11/2017

51

2

21.4

314301

8.69

7/19/2017

43

1

8.69

314306

13.07

7/23/2017

39

3

13.64

314314

7.74

8/31/2017

0

56

7.876071429

314315

18.94

8/17/2017

14

3

16.41333333

314325

6.79

7/29/2017

33

2

7.39

314331

7.57

8/17/2017

14

4

11.9975

314338

10.07

8/24/2017

7

2

10.07

314340

8.07

8/31/2017

0

26

11.98923077

314343

19.34

8/17/2017

14

3

19.74

314344

26.07

8/7/2017

24

1

26.07

314348

19.44

7/31/2017

31

4

16.9

314353

27.14

6/19/2017

73

1

27.14

314355

13.98

7/24/2017

38

1

13.98

314356

9.98

8/29/2017

2

12

10.505

314359

15.54

8/15/2017

16

1

15.54

314371

6.97

8/27/2017

4

18

9.247222222

314375

10.48

7/12/2017

50

7

9.217142857

314376

8.58

7/4/2017

58

6

7.795

314377

9.77

8/15/2017

16

7

13.2

314384

13.66

8/4/2017

27

2

17.995

314387

17.15

7/23/2017

39

3

16.84666667

314389

11.77

8/25/2017

6

2

11.77

314390

19.74

8/23/2017

8

1

19.74

314395

9.67

8/24/2017

7

4

9.1375

314396

7.18

8/25/2017

6

15

7.585333333

314398

12.02

8/22/2017

9

10

11.365

314401

16.54

8/31/2017

0

7

19.61571429

314408

16.27

8/25/2017

6

5

10.136

314410

12.17

7/27/2017

35

3

11.84

314413

8.28

8/29/2017

2

6

7.73

314416

20.65

8/14/2017

17

4

12.075

314420

11.47

8/26/2017

5

9

9.922222222

314424

39.88

6/14/2017

78

1

39.88

314425

8.98

8/3/2017

28

2

8.98

314431

9.87

7/23/2017

39

2

9.12

314434

25.57

8/25/2017

6

2

17.545

314439

7.39

8/29/2017

2

3

7.39

314445

7.67

8/4/2017

27

1

7.67

314446

18.14

8/12/2017

19

1

18.14

314460

7.97

8/31/2017

0

8

11.92875

314466

6.06

8/22/2017

9

3

10.51

314472

20.26

8/30/2017

1

1

20.26

314473

16.95

8/9/2017

22

2

15.025

314474

22.53

8/5/2017

26

3

20.16666667

314475

11.97

6/11/2017

81

1

11.97

314484

8.8

8/27/2017

4

10

9.492

314486

7.19

7/17/2017

45

3

7.186666667

314504

28.33

6/10/2017

82

1

28.33

314509

6.08

7/17/2017

45

3

6.846666667

314512

12.45

8/12/2017

19

5

13.516

314519

14.08

7/31/2017

31

1

14.08

314527

8.08

8/21/2017

10

8

8.51625

314531

8.27

8/31/2017

0

3

9.096666667

314532

6.38

7/10/2017

52

2

7.23

314535

29.81

7/8/2017

54

3

17.15333333

314538

8.27

8/14/2017

17

7

8.647142857

314541

9.27

8/28/2017

3

1

9.27

314544

18.16

7/30/2017

32

5

13.646

314549

8.27

8/24/2017

7

2

11.62

314556

8.07

6/15/2017

77

1

8.07

314566

7.99

8/11/2017

20

1

7.99

314571

10.27

8/29/2017

2

6

10.28666667

314581

49.94

7/25/2017

37

2

41.975

314587

7.97

8/15/2017

16

1

7.97

314595

11.18

8/23/2017

8

9

11.93333333

314597

11.95

7/4/2017

58

1

11.95

314598

10.08

8/28/2017

3

20

10.2225

314600

8.98

8/24/2017

7

2

8.03

314601

24.34

7/16/2017

46

1

24.34

314616

10.08

8/18/2017

13

4

14.52

314619

17.66

8/27/2017

4

21

15.1752381

314623

10.17

8/10/2017

21

5

11.036

314628

18.76

7/19/2017

43

4

14.9125

314632

6.68

8/25/2017

6

4

7.935

314639

17.44

7/12/2017

50

1

17.44

314640

9.67

8/4/2017

27

1

9.67

314646

29.3

6/24/2017

68

1

29.3

314650

9.47

8/31/2017

0

11

11.36727273

314670

8.49

8/30/2017

1

7

8.49

314672

7.18

7/10/2017

52

4

7.585

314678

8.17

8/30/2017

1

6

11.43666667

314688

9.47

8/1/2017

30

1

9.47

314689

29.42

8/6/2017

25

3

28.91666667

314708

20.83

8/30/2017

1

3

12.76

314717

15.36

8/31/2017

0

12

10.28833333

314721

17.26

7/24/2017

38

4

11.6425

314723

6.79

8/26/2017

5

7

8.287142857

314726

8.37

8/18/2017

13

4

9.0675

314727

10.27

8/29/2017

2

3

10.33666667

314728

10.48

8/27/2017

4

3

9.91

314731

10.67

8/31/2017

0

1

10.67

314733

7.18

6/13/2017

79

2

7.68

314738

9.06

8/12/2017

19

10

13.196

314744

18.06

8/31/2017

0

5

19.202

314745

7.78

8/29/2017

2

11

9.722727273

314747

9.76

8/28/2017

3

3

9.693333333

314756

14.27

8/20/2017

11

2

11.625

314762

8.47

8/24/2017

7

1

8.47

314763

9.67

8/4/2017

27

3

9.206666667

314767

11.95

8/29/2017

2

30

11.36366667

314775

8.67

8/22/2017

9

1

8.67

314776

13.47

8/15/2017

16

4

10.7325

314782

8.48

8/27/2017

4

5

9.754

314783

8.57

8/18/2017

13

1

8.57

314785

7.63

8/31/2017

0

7

7.832857143

314787

23.72

8/30/2017

1

3

13.33

314793

6.99

6/10/2017

82

1

6.99

314797

10.78

8/23/2017

8

3

10.78

314803

7.28

8/28/2017

3

9

9.412222222

314807

7.32

7/18/2017

44

2

7.32

314811

11.67

8/31/2017

0

2

9.83

314814

8.27

8/31/2017

0

14

7.998571429

314828

9.85

8/19/2017

12

10

16.641

314829

22.96

7/6/2017

56

1

22.96

314832

9.38

6/8/2017

84

1

9.38

314843

8.28

6/5/2017

87

1

8.28

314863

16.14

6/14/2017

78

1

16.14

314868

7.37

8/21/2017

10

5

14.546

314871

6.98

8/28/2017

3

1

6.98

314882

13.38

7/30/2017

32

1

13.38

314883

7.77

8/25/2017

6

18

8.441666667

314898

9.67

8/31/2017

0

32

7.9753125

314900

6.47

8/15/2017

16

1

6.47

314902

7.44

8/19/2017

12

4

12.2425

314904

16.56

8/16/2017

15

5

15.222

314909

16.27

8/19/2017

12

4

14.9175

314912

7.77

8/1/2017

30

3

8.71

314915

8.16

7/11/2017

51

2

10.18

314933

11.67

8/21/2017

10

9

11.67

314940

9.06

8/8/2017

23

2

12.9

314957

8.57

8/31/2017

0

6

12.78833333

314972

11.47

6/29/2017

63

3

11.14

314975

9.66

8/9/2017

22

2

9.615

314985

9.38

7/7/2017

55

2

8.54

314996

13.54

7/13/2017

49

2

12.295

315002

11.43

7/8/2017

54

2

16.525

315032

7.19

6/23/2017

69

2

8.09

315048

17.98

8/31/2017

0

2

17.98

315051

6.79

7/7/2017

55

4

6.8125

315054

11.97

8/22/2017

9

4

10.025

315056

8.78

6/27/2017

65

3

8.766666667

315059

25.14

8/9/2017

22

1

25.14

315061

30.44

6/24/2017

68

1

30.44

315063

9.67

8/30/2017

1

2

9.72

315070

6.67

8/15/2017

16

4

8.94

315072

16.96

8/15/2017

16

6

17.21833333

315073

16.66

6/19/2017

73

1

16.66

315082

7.67

8/7/2017

24

1

7.67

315083

30.89

6/8/2017

84

1

30.89

315089

9.37

7/19/2017

43

2

9.67

315097

8.44

7/18/2017

44

2

12.13

315098

11.37

6/30/2017

62

1

11.37

315110

9.78

8/16/2017

15

1

9.78

315111

40.17

8/11/2017

20

3

20.54

315116

11.68

7/19/2017

43

1

11.68

315122

8.27

6/30/2017

62

1

8.27

315126

9.59

7/2/2017

60

3

10.34

315128

17.83

8/21/2017

10

1

17.83

315132

7.99

7/25/2017

37

2

12.665

315147

8

8/26/2017

5

3

10.71333333

315155

10

7/3/2017

59

2

9.785

315156

8.16

8/23/2017

8

9

9.218888889

315160

16.77

8/27/2017

4

4

12.85

315161

11.28

8/1/2017

30

1

11.28

315166

7.98

8/28/2017

3

2

10.175

315177

14.05

8/15/2017

16

4

10.45

315184

5.99

6/27/2017

65

3

7.413333333

315187

9.52

8/3/2017

28

3

10.01333333

315191

7.98

8/18/2017

13

3

12.70666667

315195

18.85

7/29/2017

33

1

18.85

315198

10.98

7/27/2017

35

2

18.06

315203

6.99

7/7/2017

55

1

6.99

315204

16.26

8/25/2017

6

3

13.83333333

315205

31.63

8/22/2017

9

4

25.605

315230

20.55

8/12/2017

19

3

21.18666667

315233

20.95

8/5/2017

26

1

20.95

315235

8.47

8/6/2017

25

3

7.71

315242

11.16

6/9/2017

83

1

11.16

315246

8.98

8/30/2017

1

5

8.86

315252

8.99

8/20/2017

11

14

9.035

315262

11.87

8/29/2017

2

7

23.83

315264

13.75

6/3/2017

89

1

13.75

315266

10.59

6/11/2017

81

1

10.59

315270

11.98

8/26/2017

5

1

11.98

315273

15.16

8/24/2017

7

1

15.16

315278

9.28

8/31/2017

0

4

11.775

315287

27.03

8/24/2017

7

3

15.45333333

315293

8.34

8/31/2017

0

4

8.1175

315294

8.47

8/24/2017

7

5

9.28

315295

24.54

8/26/2017

5

8

18.445

315296

8.47

6/1/2017

91

1

8.47

315323

21.94

8/27/2017

4

10

14.309

315329

12.37

7/31/2017

31

3

12.87

315333

6.88

6/18/2017

74

2

6.935

315337

9.28

8/28/2017

3

7

8.272857143

315347

6.78

8/10/2017

21

5

7.678

315348

5.99

8/11/2017

20

4

13.7975

315355

15.74

8/15/2017

16

5

16.822

315364

6.89

8/26/2017

5

7

11.82428571

315372

20.92

8/3/2017

28

4

15.1725

315375

7.55

8/6/2017

25

4

11.4875

315377

11.37

8/25/2017

6

10

10.366

315384

9.47

8/30/2017

1

3

7.546666667

315385

6.47

8/8/2017

23

7

6.727142857

315388

7.89

8/31/2017

0

12

11.265

315391

12

8/21/2017

10

1

12

315396

7.36

6/28/2017

64

1

7.36

315398

12.37

8/27/2017

4

3

10.07666667

315400

17.34

8/25/2017

6

1

17.34

315401

8.98

8/12/2017

19

6

9.126666667

315415

12.36

6/30/2017

62

1

12.36

315417

10.58

8/28/2017

3

5

9.052

315424

8.27

8/23/2017

8

9

10.50222222

315427

9.47

8/5/2017

26

2

10.57

315437

11.87

7/13/2017

49

1

11.87

315440

10.56

7/31/2017

31

3

11.42

315446

6.17

8/3/2017

28

4

17.525

315447

9.08

8/10/2017

21

3

9.806666667

315448

7.99

7/29/2017

33

2

9.28

315449

18.94

8/30/2017

1

2

12.865

315453

13.26

8/21/2017

10

5

8.512

315461

7.18

7/26/2017

36

1

7.18

315466

19.75

8/30/2017

1

4

22.1525

315468

6.99

7/29/2017

33

6

10.36166667

315473

12.94

8/29/2017

2

5

13.476

315474

8.37

8/17/2017

14

3

9.466666667

315477

6.49

8/31/2017

0

1

6.49

315480

18.94

6/25/2017

67

1

18.94

315483

12.07

8/6/2017

25

6

12.48833333

315489

8.17

8/8/2017

23

3

13.06

315492

6.67

8/8/2017

23

1

6.67

315497

9.65

8/21/2017

10

1

9.65

315498

12.36

8/5/2017

26

2

10.265

315499

13.17

7/30/2017

32

1

13.17

315503

8.71

6/29/2017

63

5

12.854

315511

9.67

8/15/2017

16

5

9.992

315513

9.58

8/24/2017

7

2

9.125

315522

8.47

7/12/2017

50

1

8.47

315523

10.47

8/1/2017

30

2

8.63

315532

8.47

8/16/2017

15

5

11.362

315533

10.29

6/7/2017

85

1

10.29

315538

6.39

7/8/2017

54

2

16.51

315542

18.66

7/6/2017

56

1

18.66

315549

21.54

8/22/2017

9

6

20.71333333

315550

59.33

8/1/2017

30

5

19.566

315551

17.56

8/24/2017

7

5

12.908

315552

10.75

7/22/2017

40

3

8.796666667

315556

6.06

7/26/2017

36

2

7.66

315559

14.98

8/15/2017

16

4

21.93

315562

13.15

8/6/2017

25

8

9.9725

315563

9.47

8/30/2017

1

1

9.47

315567

18.77

8/28/2017

3

2

25.955

315575

10.86

8/22/2017

9

5

10.626

315579

7.38

7/31/2017

31

1

7.38

315581

8.78

8/17/2017

14

1

8.78

315582

6.99

8/19/2017

12

1

6.99

315591

22.86

8/11/2017

20

4

22.4925

315599

7.77

8/9/2017

22

1

7.77

315602

6.18

8/20/2017

11

4

6.18

315608

12.36

8/21/2017

10

1

12.36

315609

8.98

7/10/2017

52

2

11.21

315610

7.99

8/25/2017

6

6

14.73833333

315611

8.49

8/31/2017

0

3

8.323333333

315618

0

7/25/2017

37

4

17.85

315629

8.67

8/6/2017

25

4

8.17

315632

14.66

8/15/2017

16

2

10.475

315634

8.47

7/25/2017

37

2

8.82

315638

13.25

7/25/2017

37

2

13.055

315642

17.47

7/22/2017

40

2

12.13

315645

6.99

7/6/2017

56

1

6.99

315649

22.03

8/6/2017

25

1

22.03

315650

8.43

8/25/2017

6

2

9.15

315651

12.94

8/15/2017

16

5

14.666

315654

7.49

8/8/2017

23

2

9.98

315655

13.95

7/28/2017

34

2

11.21

315660

8.27

7/27/2017

35

1

8.27

315663

6.99

8/29/2017

2

5

6.664

315665

9.48

6/30/2017

62

2

8.885

315670

10.07

8/17/2017

14

2

8.47

315672

10.78

8/5/2017

26

1

10.78

315673

12.48

8/3/2017

28

2

17.265

315680

14.26

8/21/2017

10

4

14.13

315684

8.07

6/2/2017

90

1

8.07

315685

11.97

7/20/2017

42

3

10.64666667

315688

11.9

8/27/2017

4

3

10.49

315689

39.9

7/2/2017

60

1

39.9

315697

30.23

8/4/2017

27

3

18.56

315700

11.05

8/6/2017

25

4

10.335

315702

12.06

8/4/2017

27

2

10.765

315703

8.47

8/20/2017

11

2

9.915

315705

8.07

8/14/2017

17

3

8.043333333

315707

23.34

7/29/2017

33

1

23.34

315711

10.57

7/6/2017

56

4

11.3325

315712

22.36

8/7/2017

24

1

22.36

315717

8.88

7/22/2017

40

1

8.88

315723

10.47

8/21/2017

10

6

10.95166667

315725

6.79

8/22/2017

9

6

7.338333333

315726

10.97

6/23/2017

69

2

9.48

315730

12.01

8/30/2017

1

4

11.875

315731

28.73

8/15/2017

16

5

14.042

315740

7.28

8/9/2017

22

2

7.28

315754

8.18

6/11/2017

81

1

8.18

315755

9.24

8/27/2017

4

8

8.22375

315760

22

7/3/2017

59

1

22

315768

18.76

8/4/2017

27

2

19.405

315779

21.55

6/10/2017

82

1

21.55

315785

6.79

6/5/2017

87

1

6.79

315788

10.58

8/15/2017

16

3

10.05333333

315793

6.79

7/25/2017

37

4

9.23

315799

12.59

8/23/2017

8

3

10.35333333

315802

11.86

8/31/2017

0

3

17.02

315809

8.76

8/1/2017

30

2

8.76

315817

11.26

7/30/2017

32

2

9.765

315818

9.67

6/20/2017

72

1

9.67

315826

8.48

8/6/2017

25

4

8.8525

315845

11.07

8/5/2017

26

1

11.07

315853

8.47

7/29/2017

33

5

16.268

315854

27.93

7/9/2017

53

1

27.93

315855

12.76

7/5/2017

57

4

10.57

315856

10.78

7/28/2017

34

1

10.78

315860

17.46

8/24/2017

7

1

17.46

315861

8.49

8/8/2017

23

2

7.39

315873

32.84

7/30/2017

32

1

32.84

315875

20.75

6/12/2017

80

1

20.75

315883

19.64

6/13/2017

79

1

19.64


On 13 October 2017 at 10:35, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
Hi

Your statement about attaching data is problematic. We cannot do much with it. Instead use output from dput(yourdata) to show us what exactly your data look like.

We also do not know how do you want to split your data. It would be nice if you can show also what should be the bins with respective data. Unless you provide this information you probably would not get any sensible answer.

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of Hemant Sain
> Sent: Thursday, October 12, 2017 10:18 AM
> To: r-help mailing list <r-help at r-project.org<mailto:r-help at r-project.org>>
> Subject: [R] How to define proper breaks in RFM analysis
>
> Hello,
> I'm working on RFM analysis and i wanted to define my own breaks but my
> frequency distribution is not normally distributed so when I'm using quartile its
> not giving the optimal results.
> so I'm looking for a better approach where i can define breaks dynamically
> because after visualization i can do it easily but i want to apply this model so
> that it can automatically define the breaks according to data set.
> I'm attaching sample data for reference.
>
> Thanks
>
>                            *Freq*
> 5
> 15
> 1
> 8
> 2
> 2
> 2
> 1
> 1
> 2
> 2
> 1
> 1
> 1
> 1
> 4
> 10
> 22
> 24
> 1
> 1
> 2
> 2
> 7
> 1
> 1
> 4
> 6
> 1
> 1
> 2
> 2
> 17
> 2
> 7
> 1
> 5
> 2
> 7
> 7
> 1
> 4
> 13
> 4
> 1
> 9
> 1
> 1
> 1
> 3
> 2
> 1
> 1
> 2
> 4
> 5
> 8
> 4
> 14
> 18
> 4
> 1
> 1
> 1
> 7
> 1
> 1
> 1
> 1
> 10
> 6
> 4
> 6
> 1
> 3
> 13
> 2
> 4
> 8
> 11
> 2
> 4
> 6
> 1
> 1
> 3
> 1
> 2
> 3
> 2
> 2
> 5
> 2
> 1
> 1
> 1
> 3
> 2
> 3
> 1
> 2
> 9
> 34
> 8
> 1
> 1
> 1
> 1
> 2
> 1
> 5
> 4
> 9
> 1
> 1
> 4
> 2
> 2
> 1
> 1
> 2
> 25
> 5
> 2
> 1
> 1
> 1
> 6
> 1
> 36
> 3
> 4
> 5
> 3
> 3
> 13
> 2
> 1
> 4
> 1
> 1
> 9
> 7
> 1
> 1
> 15
> 1
> 1
> 2
> 5
> 7
> 3
> 3
> 1
> 8
> 7
> 5
> 1
> 1
> 5
> 14
> 3
> 2
> 2
> 1
> 5
> 3
> 4
> 3
> 1
> 2
> 9
> 2
> 15
> 2
> 1
> 3
> 56
> 3
> 2
> 4
> 2
> 26
> 3
> 1
> 4
> 1
> 1
> 12
> 1
> 18
> 7
> 6
> 7
> 2
> 3
> 2
> 1
> 4
> 15
> 10
> 7
> 5
> 3
> 6
> 4
> 9
> 1
> 2
> 2
> 2
> 3
> 1
> 1
> 8
> 3
> 1
> 2
> 3
> 1
> 10
> 3
> 1
> 3
> 5
> 1
> 8
> 3
> 2
> 3
> 7
> 1
> 5
> 2
> 1
> 1
> 6
> 2
> 1
> 9
> 1
> 20
> 2
> 1
> 4
> 21
> 5
> 4
> 4
> 1
> 1
> 1
> 11
> 7
> 4
> 6
> 1
> 3
> 3
> 12
> 4
> 7
> 4
> 3
> 3
> 1
> 2
> 10
> 5
> 11
> 3
> 2
> 1
> 3
> 30
> 1
> 4
> 5
> 1
> 7
> 3
> 1
> 3
> 9
> 2
> 2
> 14
> 10
> 1
> 1
> 1
> 1
> 5
> 1
> 1
> 18
> 32
> 1
> 4
> 5
> 4
> 3
> 2
> 9
> 2
> 6
> 3
> 2
> 2
> 2
> 2
> 2
> 2
> 4
> 4
> 3
> 1
> 1
> 2
> 4
> 6
> 1
> 1
> 1
> 2
> 2
> 1
> 1
> 3
> 1
> 1
> 3
> 1
> 2
> 3
> 2
> 9
> 4
> 1
> 2
> 4
> 3
> 3
> 3
> 1
> 2
> 1
> 3
> 4
> 3
> 1
> 3
> 1
> 5
> 14
> 7
> 1
> 1
> 1
> 1
> 4
> 3
> 4
> 5
> 8
> 1
> 10
> 3
> 2
> 7
> 5
> 4
> 5
> 7
> 4
> 4
> 10
> 3
> 7
> 12
> 1
> 1
> 3
> 1
> 6
> 1
> 5
> 9
> 2
> 1
> 3
> 4
> 3
> 2
> 2
> 5
> 1
> 4
> 6
> 5
> 3
> 1
> 1
> 6
> 3
> 1
> 1
> 2
> 1
> 5
> 5
> 2
> 1
> 2
> 5
> 1
> 2
> 1
> 6
> 5
> 5
> 3
> 2
> 4
> 8
> 1
> 2
> 5
> 1
> 1
> 1
> 4
> 1
> 4
> 1
> 2
> 6
> 3
> 4
> 4
> 2
> 2
> 2
> 2
> 1
> 1
> 2
> 5
> 2
> 2
> 1
> 5
> 2
> 2
> 1
> 2
> 4
> 1
> 3
> 3
> 1
> 3
> 4
> 2
> 2
> 3
> 1
> 4
> 1
> 1
> 6
> 6
> 2
> 4
> 5
> 2
> 1
> 8
> 1
> 2
> 1
> 1
> 3
> 4
> 3
> 3
> 2
> 2
> 1
> 4
> 1
> 5
> 1
> 4
> 1
> 1
> 2
> 1
> 1
> 1
> 8
> 1
> 1
> 1
> 1
> 1
> 3
> 3
> 5
> 2
> 3
> 1
> 1
> 5
> 2
> 3
> 6
> 3
> 3
> 14
> 2
> 1
> 1
> 2
> 1
> 2
> 4
> 2
> 1
> 6
> 1
> 7
> 2
> 3
> 3
> 2
> 2
> 2
> 2
> 1
> 2
> 4
> 1
> 6
> 2
> 5
> 2
> 1
> 2
> 2
> 5
> 8
> 4
> 1
> 1
> 1
> 1
> 4
> 1
> 3
> 2
> 1
> 2
> 2
> 3
> 3
> 3
> 6
> 1
> 1
> 1
> 5
> 7
> 1
> 5
> 2
> 1
> 1
> 1
> 3
> 20
> 2
> 3
> 3
> 1
> 2
> 1
> 15
> 4
> 4
> 1
> 1
> 2
> 1
> 1
> 3
> 2
> 6
> 5
> 1
> 5
> 1
> 7
> 4
> 3
> 2
> 5
> 2
> 1
> 1
> 3
> 2
> 6
> 2
> 4
> 2
> 1
> 24
> 4
> 17
> 1
> 3
> 2
> 2
> 2
> 2
> 8
> 1
> 3
> 1
> 9
> 2
> 4
> 1
> 1
> 6
> 3
> 4
> 1
> 9
> 2
> 1
> 3
> 2
> 6
> 2
> 1
> 3
> 1
> 20
> 3
> 4
> 7
> 3
> 7
> 1
> 7
> 1
> 5
> 2
> 1
> 1
> 1
> 2
> 1
> 2
> 4
> 1
> 1
> 2
> 3
> 4
> 1
> 4
> 1
> 1
> 1
> 1
> 1
> 2
> 1
> 6
> 2
> 3
> 2
> 1
> 9
> 8
> 11
> 1
> 1
> 2
> 1
> 3
> 1
> 2
> 15
> 5
> 2
> 2
> 9
> 1
> 4
> 2
> 1
> 1
> 14
> 1
> 1
> 2
> 1
> 3
> 10
> 1
> 1
> 3
> 1
> 1
> 1
> 4
> 2
> 8
> 1
> 2
> 2
> 1
> 11
> 1
> 3
> 1
> 7
> 1
> 3
> 10
> 3
> 3
> 1
> 1
> 1
> 11
> 1
> 2
> 1
> 1
> 2
> 2
> 5
> 5
> 4
> 1
> 2
> 5
> 2
> 4
> 3
> 1
> 1
> 2
> 2
> 4
> 2
> 1
> 1
> 4
> 1
> 4
> 1
> 5
> 1
> 1
> 2
> 1
> 4
> 7
> 1
> 2
> 1
> 2
> 9
> 3
> 1
> 7
> 2
> 2
> 1
> 2
> 3
> 5
> 2
> 7
> 1
> 5
> 1
> 1
> 2
> 2
> 2
> 4
> 2
> 8
> 4
> 6
> 1
> 1
> 1
> 1
> 1
> 16
> 2
> 1
> 6
> 4
> 4
> 1
> 1
> 1
> 1
> 4
> 3
> 2
> 6
> 11
> 10
> 21
> 2
> 1
> 1
> 3
> 2
> 2
> 2
> 7
> 1
> 6
> 4
> 1
> 7
> 4
> 11
> 1
> 2
> 8
> 1
> 1
> 1
> 1
> 2
> 17
> 1
> 2
> 3
> 4
> 2
> 1
> 2
> 2
> 4
> 3
> 2
> 3
> 1
> 3
> 3
> 1
> 3
> 37
> 4
> 3
> 1
> 2
> 1
> 1
> 3
> 1
> 2
> 3
> 1
> 5
> 1
> 2
> 1
> 3
> 2
> 3
> 3
> 4
> 4
> 2
> 4
> 1
> 1
> 3
> 3
> 2
> 1
> 2
> 1
> 1
> 3
> 1
> 1
> 3
> 3
> 4
> 2
> 4
> 1
> 1
> 1
> 10
> 3
> 2
> 2
> 2
> 2
> 2
> 2
> 1
> 19
> 2
> 2
> 4
> 1
> 3
> 1
> 13
> 5
> 2
> 1
> 2
> 2
> 4
> 2
> 1
> 3
> 5
> 1
> 1
> 1
> 6
> 3
> 9
> 4
> 1
> 1
> 1
> 3
> 1
> 17
> 4
> 1
> 4
> 6
> 2
> 1
> 2
> 4
> 4
> 2
> 2
> 2
> 4
> 4
> 1
> 1
> 1
> 2
> 7
> 2
> 1
> 2
> 8
> 1
> 1
> 7
> 4
> 1
> 2
> 1
> 1
> 3
> 1
> 3
> 1
> 1
> 3
> 5
> 8
> 1
> 1
> 4
> 1
> 1
> 15
> 1
> 1
> 6
> 2
> 3
> 3
> 8
> 4
> 1
> 4
> 2
> 4
> 2
> 5
> 4
> 4
> 1
> 1
> 7
> 1
> 10
> 1
> 10
> 2
> 15
> 7
> 3
> 1
> 3
> 2
> 19
> 2
> 9
> 1
> 10
> 2
> 1
> 2
> 2
> 4
> 6
> 1
> 4
> 3
> 1
> 2
> 3
> 2
> 1
> 3
> 7
> 1
> 4
> 2
> 13
> 2
> 5
> 3
> 6
> 2
> 1
> 2
> 1
> 5
> 1
>
>
> --
> hemantsain.com<http://hemantsain.com>
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.



--
hemantsain.com<http://hemantsain.com>

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

	[[alternative HTML version deleted]]


From marineband2007 at gmail.com  Fri Oct 13 15:50:35 2017
From: marineband2007 at gmail.com (y tanaka)
Date: Fri, 13 Oct 2017 22:50:35 +0900
Subject: [R] Quotation marks hinder for loop
Message-ID: <CAFRsubtajVC3_k+qo7-LZs3WgLPGDQ9TCEmAcef9PcQ7N2oULg@mail.gmail.com>

Dear mailing list members,

My question is maybe very basic, but I could not find the solution.

I would like to do the following things

1)
colnames(V1)[2] <- par$V2[1]
colnames(V2)[2] <- par$V2[2]
colnames(V3)[2] <- par$V2[3]
...
colnames(V37)[2] <- par$V2[37]

2)
V1 <- V1[,-1]
V2 <- V2[,-1]
V3 <- V3[,-1]
...
V37 <- V37[,-1]

3)
ms <- merge(V1,V2)
ms <- merge(ms,V3)
ms <- merge(ms,V4)
...
ms <- merge(ms,V37)

Since these codes take a lot of space in my R console and I am also afraid
of making any mistakes while typing the codes, I would like to make this
code simple by using for loop.

I know that the assign function is useful and another code as follows
worked well with it:
for(i in 1:N_var){
  assign(paste("V", i, sep=""), post_df[Nrange$start[i]:Nrange$end[i],])
}

However, I could not find how to apply for loop to 1) - 3) above.

1)
>for (i in 1:N_var){
+   assign(colnames(paste("V", i, sep=""))[2], par$V2[i])
+ }
Error in assign(colnames(paste("V", i, sep = ""))[2], par$V2[i]) :
  invalid first argument

2)
>for(i in 1:N_var){
  assign(paste("V", i, sep=""), paste("V", i, sep="")[,-1])
}
Error in paste("V", i, sep = "")[, -1] : incorrect number of dimensions


I suppose that it might be important to call object names without quotation
mark, so
I tried the noquote function, but it did not change the situation.

1)
> for (i in 1:N_var){
+   assign(colnames(noquote(paste("V", i, sep=""))[2], par$V2[i]))
+ }
Error in if (do.NULL) NULL else if (nc > 0L) paste0(prefix, seq_len(nc))
else character() :
  argument is not interpretable as logical

2)
> for(i in 1:N_var){
+   assign(paste("V", i, sep=""), noquote(paste("V", i, sep=""))[,-1])
+ }
Error in unclass(x)[...] : incorrect number of dimensions

I would appreciate it if someone would help me.

Best regards,
Yohei Tanaka
==========================
Yohei Tanaka
Tohoku University
Graduate school of Economics
Doctoral student
email: marineband2007 at gmail.com

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Oct 13 18:33:07 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 13 Oct 2017 09:33:07 -0700
Subject: [R] Quotation marks hinder for loop
In-Reply-To: <CAFRsubtajVC3_k+qo7-LZs3WgLPGDQ9TCEmAcef9PcQ7N2oULg@mail.gmail.com>
References: <CAFRsubtajVC3_k+qo7-LZs3WgLPGDQ9TCEmAcef9PcQ7N2oULg@mail.gmail.com>
Message-ID: <06559115-4582-4970-BDB6-0C9C8F229B39@comcast.net>


> On Oct 13, 2017, at 6:50 AM, y tanaka <marineband2007 at gmail.com> wrote:
> 
> Dear mailing list members,
> 
> My question is maybe very basic, but I could not find the solution.
> 
> I would like to do the following things
> 
> 1)
> colnames(V1)[2] <- par$V2[1]
> colnames(V2)[2] <- par$V2[2]
> colnames(V3)[2] <- par$V2[3]
> ...
> colnames(V37)[2] <- par$V2[37]

You are making a basic error in planning your analysis. The items V1-V37 should be in a list. Then you just do something like this,

bigVlist <- mapply( function(x,y) {colnames(x)[2] <- y}, bigVlist, par$V2)

> 
> 2)
> V1 <- V1[,-1]
> V2 <- V2[,-1]
> V3 <- V3[,-1]
> ...
> V37 <- V37[,-1]

bigVlist <- lapply( bigVlist , function(x) x[ , -1])

> 
> 3)
> ms <- merge(V1,V2)
> ms <- merge(ms,V3)
> ms <- merge(ms,V4)
> ...
> ms <- merge(ms,V37)

ms <- do.call(merge, bigVlist)

If you had provided a sample of items with equivalent structure to your actual use case we might have been motivated to provide tested code.



> 
> Since these codes take a lot of space in my R console and I am also afraid
> of making any mistakes while typing the codes, I would like to make this
> code simple by using for loop.
> 
> I know that the assign function is useful and another code as follows
> worked well with it:
> for(i in 1:N_var){
>  assign(paste("V", i, sep=""), post_df[Nrange$start[i]:Nrange$end[i],])
> }
> 
> However, I could not find how to apply for loop to 1) - 3) above.
> 
> 1)
>> for (i in 1:N_var){
> +   assign(colnames(paste("V", i, sep=""))[2], par$V2[i])

You cannot embed extraction/indexing functions on the "lefthand side", i.e. the first argument, of an `assign(` operation.

-- 
David.


> + }
> Error in assign(colnames(paste("V", i, sep = ""))[2], par$V2[i]) :
>  invalid first argument
> 
> 2)
>> for(i in 1:N_var){
>  assign(paste("V", i, sep=""), paste("V", i, sep="")[,-1])
> }
> Error in paste("V", i, sep = "")[, -1] : incorrect number of dimensions
> 
> 
> I suppose that it might be important to call object names without quotation
> mark, so
> I tried the noquote function, but it did not change the situation.
> 
> 1)
>> for (i in 1:N_var){
> +   assign(colnames(noquote(paste("V", i, sep=""))[2], par$V2[i]))
> + }
> Error in if (do.NULL) NULL else if (nc > 0L) paste0(prefix, seq_len(nc))
> else character() :
>  argument is not interpretable as logical
> 
> 2)
>> for(i in 1:N_var){
> +   assign(paste("V", i, sep=""), noquote(paste("V", i, sep=""))[,-1])
> + }
> Error in unclass(x)[...] : incorrect number of dimensions
> 
> I would appreciate it if someone would help me.
> 
> Best regards,
> Yohei Tanaka
> ==========================
> Yohei Tanaka
> Tohoku University
> Graduate school of Economics
> Doctoral student
> email: marineband2007 at gmail.com
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From dwinsemius at comcast.net  Fri Oct 13 19:27:15 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 13 Oct 2017 10:27:15 -0700
Subject: [R] How to define proper breaks in RFM analysis
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB2EAE@SRVEXCHCM301.precheza.cz>
References: <CAJL6Qs8tPRTVFi7h4Pwz9i55zPJx+K+1geF4E=S=SJ1Ke_LUjQ@mail.gmail.com>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB2CC0@SRVEXCHCM301.precheza.cz>
 <CAJL6Qs_eykK=Wa1XPkH37KoZrdM=4DYqtAdn7=P6-0ZiufVMWQ@mail.gmail.com>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB2EAE@SRVEXCHCM301.precheza.cz>
Message-ID: <51049A06-D7B1-4F5D-93DC-CF40D9DFC726@comcast.net>


> On Oct 13, 2017, at 2:51 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:
> 
> Hi
> 
> You expect us to solve your problem but you ignore advice already recieved.
> 
> Your data are unreadable, use dput(yourdata) instead. see ?dput
> 
>> test<-read.table("clipboard", heade=T)
> Error in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  :
>  line 115 did not have 6 elements

I didn't have such a problem: (illustrated with a more minimal example)

dat <-  scan( what=list("",1,"",1L,1L,1), 
             text="194849 6.99 8/22/2017 9 5 9.996
194978 14.78 8/28/2017 3 15 16.308
198614 18.44 7/31/2017 31 1 18.44
234569 34.99 8/20/2017 11 8 13.5075
252686 7.99 7/31/2017 31 2 7.99
291719 21.26 8/25/2017 6 2 15.67
291787 46.1 8/31/2017 0 2 32.57
292630 24.34 7/31/2017 31 1 24.34
295204 21.86 7/18/2017 44 1 21.86
295989 8.98 8/20/2017 11 2 14.095
298883 14.38 8/24/2017 7 2 11.185
308824 10.77 7/31/2017 31 1 10.77")

names(dat) <- c("user_id", "subtotal_amount", "created_at", "Recency", "Frequency", "Monetary")
dat <- data.frame(dat,stringsAsFactors=FALSE)

I suspect read.table would also have worked for me, but I was expecting difficulties based on Petr's posting.


#And ended up with this result (on the original copied data):
> str(dat)
'data.frame':	500 obs. of  6 variables:
 $ user_id        : chr  "194849" "194978" "198614" "234569" ...
 $ subtotal_amount: num  6.99 14.78 18.44 34.99 7.99 ...
 $ created_at     : chr  "8/22/2017" "8/28/2017" "7/31/2017" "8/20/2017" ...
 $ Recency        : int  9 3 31 11 31 6 0 31 44 11 ...
 $ Frequency      : int  5 15 1 8 2 2 2 1 1 2 ...
 $ Monetary       : num  10 16.31 18.44 13.51 7.99 ...

...  but the following criticism seems, well, _critical_ (as in essential for one to address if a reasonable proposal is to be offered.)


> What is ?ideal interval? can you define it? Should it be such to provide eqal number of observations?

That is the crucial question for you to answer, Hemant. Read the ?quartile help page if your answer is "yes" or even "maybe".
> 
> Or maybe you could normalise your values and use quartile method.

Well, maybe not so much on that last one, Petr. Normalization should not affect the classification based on quartiles. It doesn't change the ordering of variables.

-- 
David.

> 
> Cheers
> Petr
> 
> From: Hemant Sain [mailto:hemantsain55 at gmail.com]
> Sent: Friday, October 13, 2017 8:51 AM
> To: PIKAL Petr <petr.pikal at precheza.cz>
> Cc: r-help mailing list <r-help at r-project.org>
> Subject: Re: [R] How to define proper breaks in RFM analysis
> 
> Hey,
> i want to define 3 ideal breaks (bin) for each variable one of those variables is attached in the previous email,
> i don't want to consider quartile method because quartile is not working ideally for that data set because data distribution is non normal.
> so i want you to suggest another method so that i can define 3 breaks with the ideal interval for Recency, frequency and monetary to calculate RFM score.
> i'm again attaching you some of the data set.
> please look into it and help me with the R code.
> Thanks
> 
> 
> 
> Data
> 
> user_id
> 
> subtotal_amount
> 
> created_at
> 
> Recency
> 
> Frequency
> 
> Monetary
> 
> 194849
> 
> 6.99
> 
> 8/22/2017
> 
snipped

> 
> 
> On 13 October 2017 at 10:35, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
> Hi
> 
> Your statement about attaching data is problematic. We cannot do much with it. Instead use output from dput(yourdata) to show us what exactly your data look like.
> 
> We also do not know how do you want to split your data. It would be nice if you can show also what should be the bins with respective data. Unless you provide this information you probably would not get any sensible answer.
> 
> Cheers
> Petr
> 
> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of Hemant Sain
>> Sent: Thursday, October 12, 2017 10:18 AM
>> To: r-help mailing list <r-help at r-project.org<mailto:r-help at r-project.org>>
>> Subject: [R] How to define proper breaks in RFM analysis
>> 
>> Hello,
>> I'm working on RFM analysis and i wanted to define my own breaks but my
>> frequency distribution is not normally distributed so when I'm using quartile its
>> not giving the optimal results.
>> so I'm looking for a better approach where i can define breaks dynamically
>> because after visualization i can do it easily but i want to apply this model so
>> that it can automatically define the breaks according to data set.
>> I'm attaching sample data for reference.
>> 
>> Thanks
>> 
>>                           *Freq*
>> 5
>> 15
>> 1
snipped
> .
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From lauranynkevanderlaan at hotmail.com  Fri Oct 13 18:02:01 2017
From: lauranynkevanderlaan at hotmail.com (Nynke l)
Date: Fri, 13 Oct 2017 16:02:01 +0000
Subject: [R] Model within subjects treatment variable and multiple
 measurements per treatment: is this the correct model?
Message-ID: <SN1PR10MB10073B7F68B7F9CF3E74241FDB480@SN1PR10MB1007.namprd10.prod.outlook.com>

Hello all,


I have a question regarding my analysis and how to correctly model this in r syntax.

I have a dataset from an experiment in which each subject received 3 treatments (HealthPrime, HedPrime and NonfPrime; within subjects factor). In each of these treatments, 40 measurements have been done of my DV (choice_topbottom, contineous variable) and two other predictor variables (tastiness_dif_topminbottom and healthiness_dif_topminbottom). In addition, I have a subject level variable (DEBQ_restraint) for which I would like to know if there is an interaction with treatment or the predictor variables.


What I basically want to know is if there is a difference between the treatments in how tastiness_dif_topminbottom and healthiness_dif_topminbottom relate to choice_topbottom. And I would like to know whether this is influenced by DEBQ_restraint.


Is the model below correct to take into account the fact that the treatment is a within subjects factor and that there are multiple measurements per treatment?


fit <- lmer(choice_topbottom ~ tastiness_dif_topminbottom + healthiness_dif_topminbottom + HealthPrime*tastiness_dif_topminbottom + HedPrime*healthiness_dif_topminbottom + HedPrime*tastiness_dif_topminbottom + HealthPrime*healthiness_dif_topminbottom + (1 | pp_code) + DEBQ_restraint + tastiness_dif_topminbottom*DEBQ_restraint + healthiness_dif_topminbottom*DEBQ_restraint + HealthPrime*tastiness_dif_topminbottom*DEBQ_restraint + HedPrime*healthiness_dif_topminbottom*DEBQ_restraint + HedPrime*tastiness_dif_topminbottom*DEBQ_restraint + HealthPrime*healthiness_dif_topminbottom*DEBQ_restraint, data = data)

I hope someone with more experience in these kind of models can let me know whether this is correct.

Many thanks in advance!


Best,

Nynke

	[[alternative HTML version deleted]]


From abdelghani.sabrine at gmail.com  Fri Oct 13 17:38:23 2017
From: abdelghani.sabrine at gmail.com (Sabrina Abdelghani)
Date: Fri, 13 Oct 2017 16:38:23 +0100
Subject: [R] Information
Message-ID: <CAKGs1WUedLtWB3k-y-qqBUOdJ7fYvrX7kf=wQX15m9rm4Uw_Sg@mail.gmail.com>

Hello,
Can you help me about the R function to estimate Vector Autoregressive
(VAR) model allowing fot the GARCH effet : VAR-DCC-GARCH model please.


<http://www.avg.com/email-signature?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
Garanti
sans virus. www.avg.com
<http://www.avg.com/email-signature?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
<#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

	[[alternative HTML version deleted]]


From ebs15242 at gmail.com  Fri Oct 13 20:07:59 2017
From: ebs15242 at gmail.com (Ed Siefker)
Date: Fri, 13 Oct 2017 13:07:59 -0500
Subject: [R] data.matrix output is not numeric
Message-ID: <CALRb-odTA=M8-_DxDL-Y_7eLm0p5XisyHWDkoHC5+EWme-WO0w@mail.gmail.com>

I have a data frame full of integer values.  I need a matrix full of
numeric values.

?data.matrix reads:

     Return the matrix obtained by converting all the variables in a
     data frame to numeric mode and then binding them together as the
     columns of a matrix.

This does not work.

test.df <- data.frame(a=as.integer(c(1,2,3)), b=as.integer(c(4,5,6)))
> class(test.df[[1,1]])
[1] "integer"
> class(data.matrix(test.df)[[1]])
[1] "integer"

What's going on here?


From rmh at temple.edu  Fri Oct 13 20:15:28 2017
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri, 13 Oct 2017 14:15:28 -0400
Subject: [R] data.matrix output is not numeric
In-Reply-To: <CALRb-odTA=M8-_DxDL-Y_7eLm0p5XisyHWDkoHC5+EWme-WO0w@mail.gmail.com>
References: <CALRb-odTA=M8-_DxDL-Y_7eLm0p5XisyHWDkoHC5+EWme-WO0w@mail.gmail.com>
Message-ID: <CAGx1TMAp83iJr6u6nwrpeFSO0KtfHs_PH126E4waX5KELrnjiA@mail.gmail.com>

integer is a subclass of numeric.

> is.numeric(data.matrix(test.df)[[1]])
[1] TRUE

See  ?integer, ?numeric, ?storage.mode

On Fri, Oct 13, 2017 at 2:07 PM, Ed Siefker <ebs15242 at gmail.com> wrote:
> I have a data frame full of integer values.  I need a matrix full of
> numeric values.
>
> ?data.matrix reads:
>
>      Return the matrix obtained by converting all the variables in a
>      data frame to numeric mode and then binding them together as the
>      columns of a matrix.
>
> This does not work.
>
> test.df <- data.frame(a=as.integer(c(1,2,3)), b=as.integer(c(4,5,6)))
>> class(test.df[[1,1]])
> [1] "integer"
>> class(data.matrix(test.df)[[1]])
> [1] "integer"
>
> What's going on here?
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From KWamae at kemri-wellcome.org  Fri Oct 13 21:09:50 2017
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Fri, 13 Oct 2017 19:09:50 +0000
Subject: [R] Populate one data frame with values from another dataframe for
 rows that match
Message-ID: <13D7E727-E10A-468E-BA7D-EA6EE6D3EDF4@contoso.com>

I'm trying to populate the column ?pf_mcl? in myDF1 with values from myDF2, where rows match based on column "studyno" but the solutions I have found so far don't seem to be giving me the desired output.

Below is a snapshot of the data.frames.

myDF1 <- structure(list(studyno = c("J1000/9", "J1000/9", "J1000/9", "J1000/9",
"J1000/9", "J1000/9"), date = structure(c(17123, 17127, 17135,
17144, 17148, 17155), class = "Date"), pf_mcl = c(NA_integer_,
NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_
), year = c(2016, 2016, 2016, 2016, 2016, 2016)), .Names = c("studyno",
"date", "pf_mcl", "year"), row.names = c(NA, 6L), class = "data.frame")

myDF2 <- structure(list(studyno = c("J740/4", "J1000/9", "J895/7", "J931/6",
"J609/1", "J941/3"), pf_mcl = c(0L, 0L, 0L, 0L, 0L, 0L)), .Names = c("studyno",
"pf_mcl"), row.names = c(NA, 6L), class = "data.frame")

myDF2 is a well curated subset of myDF1. Some rows in the two datasets match based on "studyno", one may find that values are missing in myDF1$pf_mcl or the values are wrong.

All I want to do is identify a matching row in myDF2 and populate myDF1$pf_mcl with the value in myDF2$pf_mcl. If a row does not match based on ?studyno?, the value should remain the same.

It's probably worth mentioning, the two data frames have other columns...I have selected a few for example purposes.



______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Fri Oct 13 21:39:57 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 13 Oct 2017 12:39:57 -0700
Subject: [R] Model within subjects treatment variable and multiple
 measurements per treatment: is this the correct model?
In-Reply-To: <SN1PR10MB10073B7F68B7F9CF3E74241FDB480@SN1PR10MB1007.namprd10.prod.outlook.com>
References: <SN1PR10MB10073B7F68B7F9CF3E74241FDB480@SN1PR10MB1007.namprd10.prod.outlook.com>
Message-ID: <CAGxFJbSYvp7sfzQ1uR3nxoda2c=xMT_fM-LuvnYkd9Xn4NgPEA@mail.gmail.com>

Post on r-sig-mixed-models, not here. In PLAIN TEXT NOT HTML.

-- Bert


On Oct 13, 2017 10:55 AM, "Nynke l" <lauranynkevanderlaan at hotmail.com>
wrote:

> Hello all,
>
>
> I have a question regarding my analysis and how to correctly model this in
> r syntax.
>
> I have a dataset from an experiment in which each subject received 3
> treatments (HealthPrime, HedPrime and NonfPrime; within subjects factor).
> In each of these treatments, 40 measurements have been done of my DV
> (choice_topbottom, contineous variable) and two other predictor variables
> (tastiness_dif_topminbottom and healthiness_dif_topminbottom). In addition,
> I have a subject level variable (DEBQ_restraint) for which I would like to
> know if there is an interaction with treatment or the predictor variables.
>
>
> What I basically want to know is if there is a difference between the
> treatments in how tastiness_dif_topminbottom and
> healthiness_dif_topminbottom relate to choice_topbottom. And I would like
> to know whether this is influenced by DEBQ_restraint.
>
>
> Is the model below correct to take into account the fact that the
> treatment is a within subjects factor and that there are multiple
> measurements per treatment?
>
>
> fit <- lmer(choice_topbottom ~ tastiness_dif_topminbottom +
> healthiness_dif_topminbottom + HealthPrime*tastiness_dif_topminbottom +
> HedPrime*healthiness_dif_topminbottom + HedPrime*tastiness_dif_topminbottom
> + HealthPrime*healthiness_dif_topminbottom + (1 | pp_code) +
> DEBQ_restraint + tastiness_dif_topminbottom*DEBQ_restraint +
> healthiness_dif_topminbottom*DEBQ_restraint + HealthPrime*tastiness_dif_topminbottom*DEBQ_restraint
> + HedPrime*healthiness_dif_topminbottom*DEBQ_restraint +
> HedPrime*tastiness_dif_topminbottom*DEBQ_restraint +
> HealthPrime*healthiness_dif_topminbottom*DEBQ_restraint, data = data)
>
> I hope someone with more experience in these kind of models can let me
> know whether this is correct.
>
> Many thanks in advance!
>
>
> Best,
>
> Nynke
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Fri Oct 13 21:41:40 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 13 Oct 2017 12:41:40 -0700
Subject: [R] Information
In-Reply-To: <CAKGs1WUedLtWB3k-y-qqBUOdJ7fYvrX7kf=wQX15m9rm4Uw_Sg@mail.gmail.com>
References: <CAKGs1WUedLtWB3k-y-qqBUOdJ7fYvrX7kf=wQX15m9rm4Uw_Sg@mail.gmail.com>
Message-ID: <CAGxFJbTJ+dmsKsF6CLO6-7UnZSvKTCZWaNhm74_W5haZuqWBOQ@mail.gmail.com>

No.

This is not a statistical consulting service.

-- Bert


On Oct 13, 2017 10:56 AM, "Sabrina Abdelghani" <abdelghani.sabrine at gmail.com>
wrote:

Hello,
Can you help me about the R function to estimate Vector Autoregressive
(VAR) model allowing fot the GARCH effet : VAR-DCC-GARCH model please.


<http://www.avg.com/email-signature?utm_medium=email&
utm_source=link&utm_campaign=sig-email&utm_content=webmail>
Garanti
sans virus. www.avg.com
<http://www.avg.com/email-signature?utm_medium=email&
utm_source=link&utm_campaign=sig-email&utm_content=webmail>
<#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Fri Oct 13 21:43:34 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 13 Oct 2017 12:43:34 -0700
Subject: [R] Populate one data frame with values from another dataframe
 for rows that match
In-Reply-To: <13D7E727-E10A-468E-BA7D-EA6EE6D3EDF4@contoso.com>
References: <13D7E727-E10A-468E-BA7D-EA6EE6D3EDF4@contoso.com>
Message-ID: <CAGxFJbQHosDoKu-XjAJmnW+Eyf=rKMyoM3QOp9FD3yg=vRp5VQ@mail.gmail.com>

?merge

Bert

On Oct 13, 2017 12:09 PM, "Kevin Wamae" <KWamae at kemri-wellcome.org> wrote:

> I'm trying to populate the column ?pf_mcl? in myDF1 with values from
> myDF2, where rows match based on column "studyno" but the solutions I have
> found so far don't seem to be giving me the desired output.
>
> Below is a snapshot of the data.frames.
>
> myDF1 <- structure(list(studyno = c("J1000/9", "J1000/9", "J1000/9",
> "J1000/9",
> "J1000/9", "J1000/9"), date = structure(c(17123, 17127, 17135,
> 17144, 17148, 17155), class = "Date"), pf_mcl = c(NA_integer_,
> NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_
> ), year = c(2016, 2016, 2016, 2016, 2016, 2016)), .Names = c("studyno",
> "date", "pf_mcl", "year"), row.names = c(NA, 6L), class = "data.frame")
>
> myDF2 <- structure(list(studyno = c("J740/4", "J1000/9", "J895/7",
> "J931/6",
> "J609/1", "J941/3"), pf_mcl = c(0L, 0L, 0L, 0L, 0L, 0L)), .Names =
> c("studyno",
> "pf_mcl"), row.names = c(NA, 6L), class = "data.frame")
>
> myDF2 is a well curated subset of myDF1. Some rows in the two datasets
> match based on "studyno", one may find that values are missing in
> myDF1$pf_mcl or the values are wrong.
>
> All I want to do is identify a matching row in myDF2 and populate
> myDF1$pf_mcl with the value in myDF2$pf_mcl. If a row does not match based
> on ?studyno?, the value should remain the same.
>
> It's probably worth mentioning, the two data frames have other columns...I
> have selected a few for example purposes.
>
>
>
> ______________________________________________________________________
>
> This e-mail contains information which is confidential. It is intended
> only for the use of the named recipient. If you have received this e-mail
> in error, please let us know by replying to the sender, and immediately
> delete it from your system.  Please note, that in these circumstances, the
> use, disclosure, distribution or copying of this information is strictly
> prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility
> for the  accuracy or completeness of this message as it has been
> transmitted over a public network. Although the Programme has taken
> reasonable precautions to ensure no viruses are present in emails, it
> cannot accept responsibility for any loss or damage arising from the use of
> the email or attachments. Any views expressed in this message are those of
> the individual sender, except where the sender specifically states them to
> be the views of KEMRI-Wellcome Trust Programme.
> ______________________________________________________________________
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Fri Oct 13 22:34:25 2017
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Fri, 13 Oct 2017 21:34:25 +0100
Subject: [R] Populate one data frame with values from another dataframe
 for rows that match
In-Reply-To: <13D7E727-E10A-468E-BA7D-EA6EE6D3EDF4@contoso.com>
References: <13D7E727-E10A-468E-BA7D-EA6EE6D3EDF4@contoso.com>
Message-ID: <59E12351.9070503@sapo.pt>

Hello,

Try the following.


myDF1$studyno <- as.character(myDF1$studyno)
myDF2$studyno <- as.character(myDF2$studyno)
i1 <- which(names(myDF1) == "pf_mcl")

merge(myDF1[-i1], myDF2, by = "studyno")


Hope this helps,

Rui Barradas

Em 13-10-2017 20:09, Kevin Wamae escreveu:
> I'm trying to populate the column ?pf_mcl? in myDF1 with values from myDF2, where rows match based on column "studyno" but the solutions I have found so far don't seem to be giving me the desired output.
>
> Below is a snapshot of the data.frames.
>
> myDF1 <- structure(list(studyno = c("J1000/9", "J1000/9", "J1000/9", "J1000/9",
> "J1000/9", "J1000/9"), date = structure(c(17123, 17127, 17135,
> 17144, 17148, 17155), class = "Date"), pf_mcl = c(NA_integer_,
> NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_
> ), year = c(2016, 2016, 2016, 2016, 2016, 2016)), .Names = c("studyno",
> "date", "pf_mcl", "year"), row.names = c(NA, 6L), class = "data.frame")
>
> myDF2 <- structure(list(studyno = c("J740/4", "J1000/9", "J895/7", "J931/6",
> "J609/1", "J941/3"), pf_mcl = c(0L, 0L, 0L, 0L, 0L, 0L)), .Names = c("studyno",
> "pf_mcl"), row.names = c(NA, 6L), class = "data.frame")
>
> myDF2 is a well curated subset of myDF1. Some rows in the two datasets match based on "studyno", one may find that values are missing in myDF1$pf_mcl or the values are wrong.
>
> All I want to do is identify a matching row in myDF2 and populate myDF1$pf_mcl with the value in myDF2$pf_mcl. If a row does not match based on ?studyno?, the value should remain the same.
>
> It's probably worth mentioning, the two data frames have other columns...I have selected a few for example purposes.
>
>
>
> ______________________________________________________________________
>
> This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
> ______________________________________________________________________
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From drjimlemon at gmail.com  Sat Oct 14 00:54:35 2017
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sat, 14 Oct 2017 09:54:35 +1100
Subject: [R] How to define proper breaks in RFM analysis
In-Reply-To: <51049A06-D7B1-4F5D-93DC-CF40D9DFC726@comcast.net>
References: <CAJL6Qs8tPRTVFi7h4Pwz9i55zPJx+K+1geF4E=S=SJ1Ke_LUjQ@mail.gmail.com>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB2CC0@SRVEXCHCM301.precheza.cz>
 <CAJL6Qs_eykK=Wa1XPkH37KoZrdM=4DYqtAdn7=P6-0ZiufVMWQ@mail.gmail.com>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB2EAE@SRVEXCHCM301.precheza.cz>
 <51049A06-D7B1-4F5D-93DC-CF40D9DFC726@comcast.net>
Message-ID: <CA+8X3fXiBJJuwnBnTWuQZZDUQhkJWH_h8o3eXwVLx1yT8FP9eg@mail.gmail.com>

Hemant's problem is that the indicators are not distributed uniformly.
With a uniform distribution, categorization gives a reasonably optimal
separation of cases. One approach would be to drop categorization and
calculate the overall score as the mean of the standardized indicator
scores. Whether this is an option I do not know. I did offer an
"eyeball" set of breaks in a previous email, but apparently this was
not sufficient.

Jim

On Sat, Oct 14, 2017 at 4:27 AM, David Winsemius <dwinsemius at comcast.net> wrote:
>
>> On Oct 13, 2017, at 2:51 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:
>>
>> Hi
>>
>> You expect us to solve your problem but you ignore advice already recieved.
>>
>> Your data are unreadable, use dput(yourdata) instead. see ?dput
>>
>>> test<-read.table("clipboard", heade=T)
>> Error in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  :
>>  line 115 did not have 6 elements
>
> I didn't have such a problem: (illustrated with a more minimal example)
>
> dat <-  scan( what=list("",1,"",1L,1L,1),
>              text="194849 6.99 8/22/2017 9 5 9.996
> 194978 14.78 8/28/2017 3 15 16.308
> 198614 18.44 7/31/2017 31 1 18.44
> 234569 34.99 8/20/2017 11 8 13.5075
> 252686 7.99 7/31/2017 31 2 7.99
> 291719 21.26 8/25/2017 6 2 15.67
> 291787 46.1 8/31/2017 0 2 32.57
> 292630 24.34 7/31/2017 31 1 24.34
> 295204 21.86 7/18/2017 44 1 21.86
> 295989 8.98 8/20/2017 11 2 14.095
> 298883 14.38 8/24/2017 7 2 11.185
> 308824 10.77 7/31/2017 31 1 10.77")
>
> names(dat) <- c("user_id", "subtotal_amount", "created_at", "Recency", "Frequency", "Monetary")
> dat <- data.frame(dat,stringsAsFactors=FALSE)
>
> I suspect read.table would also have worked for me, but I was expecting difficulties based on Petr's posting.
>
>
> #And ended up with this result (on the original copied data):
>> str(dat)
> 'data.frame':   500 obs. of  6 variables:
>  $ user_id        : chr  "194849" "194978" "198614" "234569" ...
>  $ subtotal_amount: num  6.99 14.78 18.44 34.99 7.99 ...
>  $ created_at     : chr  "8/22/2017" "8/28/2017" "7/31/2017" "8/20/2017" ...
>  $ Recency        : int  9 3 31 11 31 6 0 31 44 11 ...
>  $ Frequency      : int  5 15 1 8 2 2 2 1 1 2 ...
>  $ Monetary       : num  10 16.31 18.44 13.51 7.99 ...
>
> ...  but the following criticism seems, well, _critical_ (as in essential for one to address if a reasonable proposal is to be offered.)
>
>
>> What is ?ideal interval? can you define it? Should it be such to provide eqal number of observations?
>
> That is the crucial question for you to answer, Hemant. Read the ?quartile help page if your answer is "yes" or even "maybe".
>>
>> Or maybe you could normalise your values and use quartile method.
>
> Well, maybe not so much on that last one, Petr. Normalization should not affect the classification based on quartiles. It doesn't change the ordering of variables.
>
> --
> David.
>
>>
>> Cheers
>> Petr
>>
>> From: Hemant Sain [mailto:hemantsain55 at gmail.com]
>> Sent: Friday, October 13, 2017 8:51 AM
>> To: PIKAL Petr <petr.pikal at precheza.cz>
>> Cc: r-help mailing list <r-help at r-project.org>
>> Subject: Re: [R] How to define proper breaks in RFM analysis
>>
>> Hey,
>> i want to define 3 ideal breaks (bin) for each variable one of those variables is attached in the previous email,
>> i don't want to consider quartile method because quartile is not working ideally for that data set because data distribution is non normal.
>> so i want you to suggest another method so that i can define 3 breaks with the ideal interval for Recency, frequency and monetary to calculate RFM score.
>> i'm again attaching you some of the data set.
>> please look into it and help me with the R code.
>> Thanks
>>
>>
>>
>> Data
>>
>> user_id
>>
>> subtotal_amount
>>
>> created_at
>>
>> Recency
>>
>> Frequency
>>
>> Monetary
>>
>> 194849
>>
>> 6.99
>>
>> 8/22/2017
>>
> snipped
>
>>
>>
>> On 13 October 2017 at 10:35, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
>> Hi
>>
>> Your statement about attaching data is problematic. We cannot do much with it. Instead use output from dput(yourdata) to show us what exactly your data look like.
>>
>> We also do not know how do you want to split your data. It would be nice if you can show also what should be the bins with respective data. Unless you provide this information you probably would not get any sensible answer.
>>
>> Cheers
>> Petr
>>
>>
>>> -----Original Message-----
>>> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of Hemant Sain
>>> Sent: Thursday, October 12, 2017 10:18 AM
>>> To: r-help mailing list <r-help at r-project.org<mailto:r-help at r-project.org>>
>>> Subject: [R] How to define proper breaks in RFM analysis
>>>
>>> Hello,
>>> I'm working on RFM analysis and i wanted to define my own breaks but my
>>> frequency distribution is not normally distributed so when I'm using quartile its
>>> not giving the optimal results.
>>> so I'm looking for a better approach where i can define breaks dynamically
>>> because after visualization i can do it easily but i want to apply this model so
>>> that it can automatically define the breaks according to data set.
>>> I'm attaching sample data for reference.
>>>
>>> Thanks
>>>
>>>                           *Freq*
>>> 5
>>> 15
>>> 1
> snipped
>> .
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From stephen.berman at gmx.net  Fri Oct 13 22:36:48 2017
From: stephen.berman at gmx.net (Stephen Berman)
Date: Fri, 13 Oct 2017 22:36:48 +0200
Subject: [R] make check Error: could not get timezone
Message-ID: <874lr2pyxr.fsf@gmx.net>

I just built the latest R-patched from source (SVN-Revision: 73548, Last
Changed Date: 2017-10-12) and the build completed without a problem but
`make check' errored out:

  running code in 'reg-tests-1d.R' ...make[3]: *** [Makefile.common:100: reg-tests-1d.Rout] Error 1
  make[3]: Leaving directory '/sources/R-patched/tests'
  make[2]: *** [Makefile.common:275: test-Reg] Error 2
  make[2]: Leaving directory '/sources/R-patched/tests'
  make[1]: *** [Makefile.common:165: test-all-basics] Error 1
  make[1]: Leaving directory '/sources/R-patched/tests'
  make: *** [Makefile:239: check] Error 2

The test log reg-tests-1d.Rout.fail ends like this:

  > ## PR#17186 - Sys.timezone() on some Debian-derived platforms
  > (S.t <- Sys.timezone())
  [1] NA
  > if(is.na(S.t) || !nzchar(S.t)) stop("could not get timezone")
  Error: could not get timezone
  Execution halted

Is this a cause for concern and if so, what should I do?  My system is
Linux From Scratch 8.1 (x86_64, linux 4.12.7, glibc 2.26, gcc 7.2.0).

Thanks,
Steve Berman


From KWamae at kemri-wellcome.org  Sat Oct 14 07:48:12 2017
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Sat, 14 Oct 2017 05:48:12 +0000
Subject: [R] Populate one data frame with values from another dataframe
 for rows that match
In-Reply-To: <CAGxFJbQHosDoKu-XjAJmnW+Eyf=rKMyoM3QOp9FD3yg=vRp5VQ@mail.gmail.com>
References: <13D7E727-E10A-468E-BA7D-EA6EE6D3EDF4@contoso.com>
 <CAGxFJbQHosDoKu-XjAJmnW+Eyf=rKMyoM3QOp9FD3yg=vRp5VQ@mail.gmail.com>
Message-ID: <69104DFC-EC38-44A6-BBA4-CB6D097C11BF@kemri-wellcome.org>

Dear @Bert Gunter<mailto:bgunter.4567 at gmail.com>, I tried merge and I faced many challenges. @Rui Barradas<mailto:ruipbarradas at sapo.pt> solution is working.

From: Bert Gunter <bgunter.4567 at gmail.com>
Date: Friday, 13 October 2017 at 22:44
To: Kevin Wamae <KWamae at kemri-wellcome.org>
Cc: R-help <R-help at r-project.org>
Subject: Re: [R] Populate one data frame with values from another dataframe for rows that match

?merge

Bert

On Oct 13, 2017 12:09 PM, "Kevin Wamae" <KWamae at kemri-wellcome.org<mailto:KWamae at kemri-wellcome.org>> wrote:
I'm trying to populate the column ?pf_mcl? in myDF1 with values from myDF2, where rows match based on column "studyno" but the solutions I have found so far don't seem to be giving me the desired output.

Below is a snapshot of the data.frames.

myDF1 <- structure(list(studyno = c("J1000/9", "J1000/9", "J1000/9", "J1000/9",
"J1000/9", "J1000/9"), date = structure(c(17123, 17127, 17135,
17144, 17148, 17155), class = "Date"), pf_mcl = c(NA_integer_,
NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_
), year = c(2016, 2016, 2016, 2016, 2016, 2016)), .Names = c("studyno",
"date", "pf_mcl", "year"), row.names = c(NA, 6L), class = "data.frame")

myDF2 <- structure(list(studyno = c("J740/4", "J1000/9", "J895/7", "J931/6",
"J609/1", "J941/3"), pf_mcl = c(0L, 0L, 0L, 0L, 0L, 0L)), .Names = c("studyno",
"pf_mcl"), row.names = c(NA, 6L), class = "data.frame")

myDF2 is a well curated subset of myDF1. Some rows in the two datasets match based on "studyno", one may find that values are missing in myDF1$pf_mcl or the values are wrong.

All I want to do is identify a matching row in myDF2 and populate myDF1$pf_mcl with the value in myDF2$pf_mcl. If a row does not match based on ?studyno?, the value should remain the same.

It's probably worth mentioning, the two data frames have other columns...I have selected a few for example purposes.



______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

	[[alternative HTML version deleted]]


From KWamae at kemri-wellcome.org  Sat Oct 14 07:49:43 2017
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Sat, 14 Oct 2017 05:49:43 +0000
Subject: [R] Populate one data frame with values from another dataframe
 for rows that match
In-Reply-To: <59E12351.9070503@sapo.pt>
References: <13D7E727-E10A-468E-BA7D-EA6EE6D3EDF4@contoso.com>
 <59E12351.9070503@sapo.pt>
Message-ID: <7899CF5A-481F-4788-B2ED-7D6605AC646C@kemri-wellcome.org>

Dear @Rui Barradas, thank you for the solution. It works perfectly.


On 13/10/2017, 23:35, "Rui Barradas" <ruipbarradas at sapo.pt> wrote:

    Hello,
    
    Try the following.
    
    
    myDF1$studyno <- as.character(myDF1$studyno)
    myDF2$studyno <- as.character(myDF2$studyno)
    i1 <- which(names(myDF1) == "pf_mcl")
    
    merge(myDF1[-i1], myDF2, by = "studyno")
    
    
    Hope this helps,
    
    Rui Barradas
    
    Em 13-10-2017 20:09, Kevin Wamae escreveu:
    > I'm trying to populate the column ?pf_mcl? in myDF1 with values from myDF2, where rows match based on column "studyno" but the solutions I have found so far don't seem to be giving me the desired output.
    >
    > Below is a snapshot of the data.frames.
    >
    > myDF1 <- structure(list(studyno = c("J1000/9", "J1000/9", "J1000/9", "J1000/9",
    > "J1000/9", "J1000/9"), date = structure(c(17123, 17127, 17135,
    > 17144, 17148, 17155), class = "Date"), pf_mcl = c(NA_integer_,
    > NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_
    > ), year = c(2016, 2016, 2016, 2016, 2016, 2016)), .Names = c("studyno",
    > "date", "pf_mcl", "year"), row.names = c(NA, 6L), class = "data.frame")
    >
    > myDF2 <- structure(list(studyno = c("J740/4", "J1000/9", "J895/7", "J931/6",
    > "J609/1", "J941/3"), pf_mcl = c(0L, 0L, 0L, 0L, 0L, 0L)), .Names = c("studyno",
    > "pf_mcl"), row.names = c(NA, 6L), class = "data.frame")
    >
    > myDF2 is a well curated subset of myDF1. Some rows in the two datasets match based on "studyno", one may find that values are missing in myDF1$pf_mcl or the values are wrong.
    >
    > All I want to do is identify a matching row in myDF2 and populate myDF1$pf_mcl with the value in myDF2$pf_mcl. If a row does not match based on ?studyno?, the value should remain the same.
    >
    > It's probably worth mentioning, the two data frames have other columns...I have selected a few for example purposes.
    >
    >
    >
    > ______________________________________________________________________
    >
    > This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
    > ______________________________________________________________________
    >
    > 	[[alternative HTML version deleted]]
    >
    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.
    >
    


______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

From ericjberger at gmail.com  Sat Oct 14 11:42:34 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Sat, 14 Oct 2017 12:42:34 +0300
Subject: [R] Populate one data frame with values from another dataframe
 for rows that match
In-Reply-To: <7899CF5A-481F-4788-B2ED-7D6605AC646C@kemri-wellcome.org>
References: <13D7E727-E10A-468E-BA7D-EA6EE6D3EDF4@contoso.com>
 <59E12351.9070503@sapo.pt>
 <7899CF5A-481F-4788-B2ED-7D6605AC646C@kemri-wellcome.org>
Message-ID: <CAGgJW76HtnfNpXNr9gEQ_+D3QV0BrGJsymL7rWaWYHo4nu7LaA@mail.gmail.com>

Hi Kevin,
I think there are issues with Rui's proposed solution. For example, if
there are rows in myDF1 which have a studyno
which does not match any row in myDF2, then you will lose those rows. In
your original request you said that you wanted to keep those rows.

To demonstrate my point I need to modify your sample data. Specifically, I
changed some studyno settings in myDF1, and also the entries of pf_mcl in
myDF1.

myDF1 <- structure(list(studyno = c("J1000/8", "J1000/9", "J1000/9",
"J1000/9",
"J1000/5", "J1000/6"), date = structure(c(17123, 17127, 17135,
17144, 17148, 17155), class = "Date"), pf_mcl = c(1:6
), year = c(2016, 2016, 2016, 2016, 2016, 2016)), .Names = c("studyno",
"date", "pf_mcl", "year"), row.names = c(NA, 6L), class = "data.frame")

myDF2 <- structure(list(studyno = c("J740/4", "J1000/9", "J895/7", "J931/6",
"J609/1", "J941/3"), pf_mcl = c(0L, 0L, 0L, 0L, 0L, 0L)), .Names =
c("studyno",
"pf_mcl"), row.names = c(NA, 6L), class = "data.frame")

#Rui's proposal gives the following result
# studyno       date year pf_mcl
# 1 J1000/9 2016-11-22 2016      0
# 2 J1000/9 2016-11-30 2016      0
# 3 J1000/9 2016-12-09 2016      0

My proposal

library(dplyr)

myDF1$studyno <- as.character(myDF1$studyno)
myDF2$studyno <- as.character(myDF2$studyno)
myDF3 <- merge(myDF1, myDF2, by="studyno", all.x=TRUE ) %>%
                dplyr::mutate( pf_mcl = ifelse( is.na(pf_mcl.y), pf_mcl.x,
pf_mcl.y ) ) %>%
                dplyr::select( studyno, date, pf_mcl )

# The results of this approach
#  studyno       date pf_mcl
# 1 J1000/5 2016-12-13      5
# 2 J1000/6 2016-12-20      6
# 3 J1000/8 2016-11-18      1
# 4 J1000/9 2016-11-22      0
# 5 J1000/9 2016-11-30      0
# 6 J1000/9 2016-12-09      0

Comparing the two results you see that no rows have been dropped in my
approach.

HTH,

Eric





On Sat, Oct 14, 2017 at 8:49 AM, Kevin Wamae <KWamae at kemri-wellcome.org>
wrote:

> Dear @Rui Barradas, thank you for the solution. It works perfectly.
>
>
> On 13/10/2017, 23:35, "Rui Barradas" <ruipbarradas at sapo.pt> wrote:
>
>     Hello,
>
>     Try the following.
>
>
>     myDF1$studyno <- as.character(myDF1$studyno)
>     myDF2$studyno <- as.character(myDF2$studyno)
>     i1 <- which(names(myDF1) == "pf_mcl")
>
>     merge(myDF1[-i1], myDF2, by = "studyno")
>
>
>     Hope this helps,
>
>     Rui Barradas
>
>     Em 13-10-2017 20:09, Kevin Wamae escreveu:
>     > I'm trying to populate the column ?pf_mcl? in myDF1 with values from
> myDF2, where rows match based on column "studyno" but the solutions I have
> found so far don't seem to be giving me the desired output.
>     >
>     > Below is a snapshot of the data.frames.
>     >
>     > myDF1 <- structure(list(studyno = c("J1000/9", "J1000/9", "J1000/9",
> "J1000/9",
>     > "J1000/9", "J1000/9"), date = structure(c(17123, 17127, 17135,
>     > 17144, 17148, 17155), class = "Date"), pf_mcl = c(NA_integer_,
>     > NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_
>     > ), year = c(2016, 2016, 2016, 2016, 2016, 2016)), .Names =
> c("studyno",
>     > "date", "pf_mcl", "year"), row.names = c(NA, 6L), class =
> "data.frame")
>     >
>     > myDF2 <- structure(list(studyno = c("J740/4", "J1000/9", "J895/7",
> "J931/6",
>     > "J609/1", "J941/3"), pf_mcl = c(0L, 0L, 0L, 0L, 0L, 0L)), .Names =
> c("studyno",
>     > "pf_mcl"), row.names = c(NA, 6L), class = "data.frame")
>     >
>     > myDF2 is a well curated subset of myDF1. Some rows in the two
> datasets match based on "studyno", one may find that values are missing in
> myDF1$pf_mcl or the values are wrong.
>     >
>     > All I want to do is identify a matching row in myDF2 and populate
> myDF1$pf_mcl with the value in myDF2$pf_mcl. If a row does not match based
> on ?studyno?, the value should remain the same.
>     >
>     > It's probably worth mentioning, the two data frames have other
> columns...I have selected a few for example purposes.
>     >
>     >
>     >
>     > ____________________________________________________________
> __________
>     >
>     > This e-mail contains information which is confidential. It is
> intended only for the use of the named recipient. If you have received this
> e-mail in error, please let us know by replying to the sender, and
> immediately delete it from your system.  Please note, that in these
> circumstances, the use, disclosure, distribution or copying of this
> information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot
> accept any responsibility for the  accuracy or completeness of this message
> as it has been transmitted over a public network. Although the Programme
> has taken reasonable precautions to ensure no viruses are present in
> emails, it cannot accept responsibility for any loss or damage arising from
> the use of the email or attachments. Any views expressed in this message
> are those of the individual sender, except where the sender specifically
> states them to be the views of KEMRI-Wellcome Trust Programme.
>     > ____________________________________________________________
> __________
>     >
>     >   [[alternative HTML version deleted]]
>     >
>     > ______________________________________________
>     > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>     > https://stat.ethz.ch/mailman/listinfo/r-help
>     > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
>     > and provide commented, minimal, self-contained, reproducible code.
>     >
>
>
>
> ______________________________________________________________________
>
> This e-mail contains information which is confidential. It is intended
> only for the use of the named recipient. If you have received this e-mail
> in error, please let us know by replying to the sender, and immediately
> delete it from your system.  Please note, that in these circumstances, the
> use, disclosure, distribution or copying of this information is strictly
> prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility
> for the  accuracy or completeness of this message as it has been
> transmitted over a public network. Although the Programme has taken
> reasonable precautions to ensure no viruses are present in emails, it
> cannot accept responsibility for any loss or damage arising from the use of
> the email or attachments. Any views expressed in this message are those of
> the individual sender, except where the sender specifically states them to
> be the views of KEMRI-Wellcome Trust Programme.
> ______________________________________________________________________
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From stephen.berman at gmx.net  Sat Oct 14 12:02:50 2017
From: stephen.berman at gmx.net (Stephen Berman)
Date: Sat, 14 Oct 2017 12:02:50 +0200
Subject: [R] make check Error: could not get timezone
In-Reply-To: <874lr2pyxr.fsf@gmx.net> (Stephen Berman's message of "Fri, 13
 Oct 2017 22:36:48 +0200")
References: <874lr2pyxr.fsf@gmx.net>
Message-ID: <87vaji6o8l.fsf@gmx.net>

On Fri, 13 Oct 2017 22:36:48 +0200 Stephen Berman <stephen.berman at gmx.net> wrote:

> I just built the latest R-patched from source (SVN-Revision: 73548, Last
> Changed Date: 2017-10-12) and the build completed without a problem but
> `make check' errored out:
>
>   running code in 'reg-tests-1d.R' ...make[3]: *** [Makefile.common:100:
> reg-tests-1d.Rout] Error 1
>   make[3]: Leaving directory '/sources/R-patched/tests'
>   make[2]: *** [Makefile.common:275: test-Reg] Error 2
>   make[2]: Leaving directory '/sources/R-patched/tests'
>   make[1]: *** [Makefile.common:165: test-all-basics] Error 1
>   make[1]: Leaving directory '/sources/R-patched/tests'
>   make: *** [Makefile:239: check] Error 2
>
> The test log reg-tests-1d.Rout.fail ends like this:
>
>   > ## PR#17186 - Sys.timezone() on some Debian-derived platforms
>   > (S.t <- Sys.timezone())
>   [1] NA
>   > if(is.na(S.t) || !nzchar(S.t)) stop("could not get timezone")
>   Error: could not get timezone
>   Execution halted
>
> Is this a cause for concern and if so, what should I do?  My system is
> Linux From Scratch 8.1 (x86_64, linux 4.12.7, glibc 2.26, gcc 7.2.0).

In the meantime I installed R and it seems to be functioning correctly.

I reran the tests with `make -k check' and the above test failed again,
but it was the only one.  I looked at the source of Sys.timezone and on
my system as currently configured (TZ is not set, there is no
/etc/timezone file) this function always returns NA, so the test cannot
succeed.  I built and configured my system following the Linux From
Scratch instructions, so (unless I made a mistake somewhere) it seems
this test is irrelevant for that system.  If this conclusion is correct,
maybe the test can be skipped if /etc/timezone does not exist, or at
least not error out.  Or does that mean my system is somehow
misconfigured?

Steve Berman


From KWamae at kemri-wellcome.org  Sat Oct 14 12:26:57 2017
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Sat, 14 Oct 2017 10:26:57 +0000
Subject: [R] Populate one data frame with values from another dataframe
 for rows that match
In-Reply-To: <CAGgJW76HtnfNpXNr9gEQ_+D3QV0BrGJsymL7rWaWYHo4nu7LaA@mail.gmail.com>
References: <13D7E727-E10A-468E-BA7D-EA6EE6D3EDF4@contoso.com>
 <59E12351.9070503@sapo.pt>
 <7899CF5A-481F-4788-B2ED-7D6605AC646C@kemri-wellcome.org>
 <CAGgJW76HtnfNpXNr9gEQ_+D3QV0BrGJsymL7rWaWYHo4nu7LaA@mail.gmail.com>
Message-ID: <8693F382-2F45-4B32-8ED5-5A1B20826C18@kemri-wellcome.org>

Dear @Eric<mailto:ericjberger at gmail.com>, thank you so very much for noticing that. When I tested @Rui<mailto:ruipbarradas at sapo.pt>?s solution, it was on a smaller dataset that had purely matching rows. I had considered including non-matching rows to evaluate what the alternative would be.

Also, I hadn?t even tested it on the larger dataset. I have now and noticed that it went further to omit rows that did not match, just like you said.

Your proposed solution works well.

Much appreciated?I?ll get in touch in case I encounter any problems


From: Eric Berger <ericjberger at gmail.com>
Date: Saturday, 14 October 2017 at 12:43
To: Kevin Wamae <KWamae at kemri-wellcome.org>
Cc: Rui Barradas <ruipbarradas at sapo.pt>, "r-help at r-project.org" <r-help at r-project.org>
Subject: Re: [R] Populate one data frame with values from another dataframe for rows that match

Hi Kevin,
I think there are issues with Rui's proposed solution. For example, if there are rows in myDF1 which have a studyno
which does not match any row in myDF2, then you will lose those rows. In your original request you said that you wanted to keep those rows.

To demonstrate my point I need to modify your sample data. Specifically, I changed some studyno settings in myDF1, and also the entries of pf_mcl in myDF1.

myDF1 <- structure(list(studyno = c("J1000/8", "J1000/9", "J1000/9", "J1000/9",
"J1000/5", "J1000/6"), date = structure(c(17123, 17127, 17135,
17144, 17148, 17155), class = "Date"), pf_mcl = c(1:6
), year = c(2016, 2016, 2016, 2016, 2016, 2016)), .Names = c("studyno",
"date", "pf_mcl", "year"), row.names = c(NA, 6L), class = "data.frame")

myDF2 <- structure(list(studyno = c("J740/4", "J1000/9", "J895/7", "J931/6",
"J609/1", "J941/3"), pf_mcl = c(0L, 0L, 0L, 0L, 0L, 0L)), .Names = c("studyno",
"pf_mcl"), row.names = c(NA, 6L), class = "data.frame")

#Rui's proposal gives the following result
# studyno       date year pf_mcl
# 1 J1000/9 2016-11-22 2016      0
# 2 J1000/9 2016-11-30 2016      0
# 3 J1000/9 2016-12-09 2016      0

My proposal

library(dplyr)

myDF1$studyno <- as.character(myDF1$studyno)
myDF2$studyno <- as.character(myDF2$studyno)
myDF3 <- merge(myDF1, myDF2, by="studyno", all.x=TRUE ) %>%
                dplyr::mutate( pf_mcl = ifelse( is.na<http://is.na>(pf_mcl.y), pf_mcl.x, pf_mcl.y ) ) %>%
                dplyr::select( studyno, date, pf_mcl )

# The results of this approach
#  studyno       date pf_mcl
# 1 J1000/5 2016-12-13      5
# 2 J1000/6 2016-12-20      6
# 3 J1000/8 2016-11-18      1
# 4 J1000/9 2016-11-22      0
# 5 J1000/9 2016-11-30      0
# 6 J1000/9 2016-12-09      0

Comparing the two results you see that no rows have been dropped in my approach.

HTH,

Eric





On Sat, Oct 14, 2017 at 8:49 AM, Kevin Wamae <KWamae at kemri-wellcome.org<mailto:KWamae at kemri-wellcome.org>> wrote:
Dear @Rui Barradas, thank you for the solution. It works perfectly.


On 13/10/2017, 23:35, "Rui Barradas" <ruipbarradas at sapo.pt<mailto:ruipbarradas at sapo.pt>> wrote:

    Hello,

    Try the following.


    myDF1$studyno <- as.character(myDF1$studyno)
    myDF2$studyno <- as.character(myDF2$studyno)
    i1 <- which(names(myDF1) == "pf_mcl")

    merge(myDF1[-i1], myDF2, by = "studyno")


    Hope this helps,

    Rui Barradas

    Em 13-10-2017 20:09, Kevin Wamae escreveu:
    > I'm trying to populate the column ?pf_mcl? in myDF1 with values from myDF2, where rows match based on column "studyno" but the solutions I have found so far don't seem to be giving me the desired output.
    >
    > Below is a snapshot of the data.frames.
    >
    > myDF1 <- structure(list(studyno = c("J1000/9", "J1000/9", "J1000/9", "J1000/9",
    > "J1000/9", "J1000/9"), date = structure(c(17123, 17127, 17135,
    > 17144, 17148, 17155), class = "Date"), pf_mcl = c(NA_integer_,
    > NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_
    > ), year = c(2016, 2016, 2016, 2016, 2016, 2016)), .Names = c("studyno",
    > "date", "pf_mcl", "year"), row.names = c(NA, 6L), class = "data.frame")
    >
    > myDF2 <- structure(list(studyno = c("J740/4", "J1000/9", "J895/7", "J931/6",
    > "J609/1", "J941/3"), pf_mcl = c(0L, 0L, 0L, 0L, 0L, 0L)), .Names = c("studyno",
    > "pf_mcl"), row.names = c(NA, 6L), class = "data.frame")
    >
    > myDF2 is a well curated subset of myDF1. Some rows in the two datasets match based on "studyno", one may find that values are missing in myDF1$pf_mcl or the values are wrong.
    >
    > All I want to do is identify a matching row in myDF2 and populate myDF1$pf_mcl with the value in myDF2$pf_mcl. If a row does not match based on ?studyno?, the value should remain the same.
    >
    > It's probably worth mentioning, the two data frames have other columns...I have selected a few for example purposes.
    >
    >
    >
    > ______________________________________________________________________
    >
    > This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
    > ______________________________________________________________________
    >
    >   [[alternative HTML version deleted]]
    >
    > ______________________________________________
    > R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.
    >



______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________
______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Sat Oct 14 12:49:16 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 14 Oct 2017 11:49:16 +0100
Subject: [R] make check Error: could not get timezone
In-Reply-To: <87vaji6o8l.fsf@gmx.net>
References: <874lr2pyxr.fsf@gmx.net> <87vaji6o8l.fsf@gmx.net>
Message-ID: <A186F055-E1CC-4C30-9C8E-E7F997AD9E55@dcn.davis.ca.us>

Off topic. Read the Posting Guide.
-- 
Sent from my phone. Please excuse my brevity.

On October 14, 2017 11:02:50 AM GMT+01:00, Stephen Berman <stephen.berman at gmx.net> wrote:
>On Fri, 13 Oct 2017 22:36:48 +0200 Stephen Berman
><stephen.berman at gmx.net> wrote:
>
>> I just built the latest R-patched from source (SVN-Revision: 73548,
>Last
>> Changed Date: 2017-10-12) and the build completed without a problem
>but
>> `make check' errored out:
>>
>>   running code in 'reg-tests-1d.R' ...make[3]: ***
>[Makefile.common:100:
>> reg-tests-1d.Rout] Error 1
>>   make[3]: Leaving directory '/sources/R-patched/tests'
>>   make[2]: *** [Makefile.common:275: test-Reg] Error 2
>>   make[2]: Leaving directory '/sources/R-patched/tests'
>>   make[1]: *** [Makefile.common:165: test-all-basics] Error 1
>>   make[1]: Leaving directory '/sources/R-patched/tests'
>>   make: *** [Makefile:239: check] Error 2
>>
>> The test log reg-tests-1d.Rout.fail ends like this:
>>
>>   > ## PR#17186 - Sys.timezone() on some Debian-derived platforms
>>   > (S.t <- Sys.timezone())
>>   [1] NA
>>   > if(is.na(S.t) || !nzchar(S.t)) stop("could not get timezone")
>>   Error: could not get timezone
>>   Execution halted
>>
>> Is this a cause for concern and if so, what should I do?  My system
>is
>> Linux From Scratch 8.1 (x86_64, linux 4.12.7, glibc 2.26, gcc 7.2.0).
>
>In the meantime I installed R and it seems to be functioning correctly.
>
>I reran the tests with `make -k check' and the above test failed again,
>but it was the only one.  I looked at the source of Sys.timezone and on
>my system as currently configured (TZ is not set, there is no
>/etc/timezone file) this function always returns NA, so the test cannot
>succeed.  I built and configured my system following the Linux From
>Scratch instructions, so (unless I made a mistake somewhere) it seems
>this test is irrelevant for that system.  If this conclusion is
>correct,
>maybe the test can be skipped if /etc/timezone does not exist, or at
>least not error out.  Or does that mean my system is somehow
>misconfigured?
>
>Steve Berman
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From stephen.berman at gmx.net  Sat Oct 14 13:07:37 2017
From: stephen.berman at gmx.net (Stephen Berman)
Date: Sat, 14 Oct 2017 13:07:37 +0200
Subject: [R] make check Error: could not get timezone
In-Reply-To: <A186F055-E1CC-4C30-9C8E-E7F997AD9E55@dcn.davis.ca.us> (Jeff
 Newmiller's message of "Sat, 14 Oct 2017 11:49:16 +0100")
References: <874lr2pyxr.fsf@gmx.net> <87vaji6o8l.fsf@gmx.net>
 <A186F055-E1CC-4C30-9C8E-E7F997AD9E55@dcn.davis.ca.us>
Message-ID: <87o9pa6l8m.fsf@gmx.net>

On Sat, 14 Oct 2017 11:49:16 +0100 Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:

> Off topic. Read the Posting Guide.

I did do that before posting; the choice seemed to be between R-help
("intended to be comprehensible to people who want to use R to solve
problems but who are not necessarily interested in or knowledgeable
about programming") and R-devel ("for questions and discussion about R
development and programming").  Obviously I need to install R before I
can use it, and in my case I had to build it before I could install it,
and running the tests is recommended, and that's where the issue arose;
I couldn't tell if this was due to a bug in R or something on my end,
i.e., I needed help, so that's why I asked here.  If you think R-devel
is the right address, I'll ask there.

Steve Berman


From annijanh at gmail.com  Sat Oct 14 16:58:22 2017
From: annijanh at gmail.com (Janh Anni)
Date: Sat, 14 Oct 2017 10:58:22 -0400
Subject: [R] Bootstrapped Regression
Message-ID: <CAFCoDdCjZpO26afEPRGqR0zEn8UDUgkz88N2wWr-7Ca_sq6mVA@mail.gmail.com>

Greetings!

We are trying to obtain confidence and prediction intervals for a predicted
Y value from bootstrapped linear regression using the boot function. Does
anyone know how to code it?  Greatly appreciated.

Janh

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sat Oct 14 17:12:43 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 14 Oct 2017 08:12:43 -0700
Subject: [R] Bootstrapped Regression
In-Reply-To: <CAFCoDdCjZpO26afEPRGqR0zEn8UDUgkz88N2wWr-7Ca_sq6mVA@mail.gmail.com>
References: <CAFCoDdCjZpO26afEPRGqR0zEn8UDUgkz88N2wWr-7Ca_sq6mVA@mail.gmail.com>
Message-ID: <CAGxFJbQMpYvNPNgCwH-jwvvaPLTcLs9QkrSQoeiMWbTEhsBL=A@mail.gmail.com>

R-help is not a free coding service. We expect users to make the effort to
learn R and *may* provide help when they get stuck. Pay a local R
programmer if you do not wish to make such an effort.

Cheers,
Bert


On Oct 14, 2017 7:58 AM, "Janh Anni" <annijanh at gmail.com> wrote:

Greetings!

We are trying to obtain confidence and prediction intervals for a predicted
Y value from bootstrapped linear regression using the boot function. Does
anyone know how to code it?  Greatly appreciated.

Janh

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Sat Oct 14 19:20:34 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Sat, 14 Oct 2017 10:20:34 -0700
Subject: [R] Populate one data frame with values from another dataframe
 for rows that match
In-Reply-To: <69104DFC-EC38-44A6-BBA4-CB6D097C11BF@kemri-wellcome.org>
References: <13D7E727-E10A-468E-BA7D-EA6EE6D3EDF4@contoso.com>
 <CAGxFJbQHosDoKu-XjAJmnW+Eyf=rKMyoM3QOp9FD3yg=vRp5VQ@mail.gmail.com>
 <69104DFC-EC38-44A6-BBA4-CB6D097C11BF@kemri-wellcome.org>
Message-ID: <CAF8bMcaBfrs0n2qj5O3EZePkkPu-gCdF+LqbksATBc1HahYhTQ@mail.gmail.com>

Your example used one distinct studyno in DF1 and one distinct pf_mcl in
DF2.  I think that makes it hard to see what is going on, but maybe I
completely misunderstand the problem.  In any case, let's redefine myDF1
and myDF2.  Note that myDF1 contains a studyno not in myDF2 and vice versa.

myDF1 <- structure(list(studyno = c("J1000/9", "J895/7", "J931/6", "J666/6",
"J1000/9", "J1000/9"), date = structure(c(17123, 17127, 17135,
17144, 17148, 17155), class = "Date"), pf_mcl = c(NA_integer_,
2L, 3L, 4L, 5L, NA_integer_
), year = c(2016, 2016, 2016, 2016, 2016, 2016)), .Names = c("studyno",
"date", "pf_mcl", "year"), row.names = c(NA, 6L), class = "data.frame")

myDF2 <- structure(list(studyno = c("J740/4", "J1000/9", "J895/7", "J931/6",
"J609/1", "J941/3"), pf_mcl = c(101L, 102L, 103L, 104L, 105L, 106L)),
.Names = c("studyno", "pf_mcl"), row.names = c(NA, 6L), class =
"data.frame")

m <- merge(myDF1, myDF2, by="studyno", all.x=TRUE, all.y=FALSE,
suffixes=c(".raw", ".curated"))


The results are:

> myDF1
  studyno       date pf_mcl year
1 J1000/9 2016-11-18     NA 2016
2  J895/7 2016-11-22      2 2016
3  J931/6 2016-11-30      3 2016
4  J666/6 2016-12-09      4 2016
5 J1000/9 2016-12-13      5 2016
6 J1000/9 2016-12-20     NA 2016
> myDF2
  studyno pf_mcl
1  J740/4    101
2 J1000/9    102
3  J895/7    103
4  J931/6    104
5  J609/1    105
6  J941/3    106
> m
  studyno       date pf_mcl.raw year pf_mcl.curated
1 J1000/9 2016-11-18         NA 2016            102
2 J1000/9 2016-12-13          5 2016            102
3 J1000/9 2016-12-20         NA 2016            102
4  J666/6 2016-12-09          4 2016             NA
5  J895/7 2016-11-22          2 2016            103
6  J931/6 2016-11-30          3 2016            104


Now your problem is to combine the columns pf_mcl.raw and pf_mcl.curated in
the way you want.  ifelse() may be useful for that.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Fri, Oct 13, 2017 at 10:48 PM, Kevin Wamae <KWamae at kemri-wellcome.org>
wrote:

> Dear @Bert Gunter<mailto:bgunter.4567 at gmail.com>, I tried merge and I
> faced many challenges. @Rui Barradas<mailto:ruipbarradas at sapo.pt>
> solution is working.
>
> From: Bert Gunter <bgunter.4567 at gmail.com>
> Date: Friday, 13 October 2017 at 22:44
> To: Kevin Wamae <KWamae at kemri-wellcome.org>
> Cc: R-help <R-help at r-project.org>
> Subject: Re: [R] Populate one data frame with values from another
> dataframe for rows that match
>
> ?merge
>
> Bert
>
> On Oct 13, 2017 12:09 PM, "Kevin Wamae" <KWamae at kemri-wellcome.org<mailto:
> KWamae at kemri-wellcome.org>> wrote:
> I'm trying to populate the column ?pf_mcl? in myDF1 with values from
> myDF2, where rows match based on column "studyno" but the solutions I have
> found so far don't seem to be giving me the desired output.
>
> Below is a snapshot of the data.frames.
>
> myDF1 <- structure(list(studyno = c("J1000/9", "J1000/9", "J1000/9",
> "J1000/9",
> "J1000/9", "J1000/9"), date = structure(c(17123, 17127, 17135,
> 17144, 17148, 17155), class = "Date"), pf_mcl = c(NA_integer_,
> NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_
> ), year = c(2016, 2016, 2016, 2016, 2016, 2016)), .Names = c("studyno",
> "date", "pf_mcl", "year"), row.names = c(NA, 6L), class = "data.frame")
>
> myDF2 <- structure(list(studyno = c("J740/4", "J1000/9", "J895/7",
> "J931/6",
> "J609/1", "J941/3"), pf_mcl = c(0L, 0L, 0L, 0L, 0L, 0L)), .Names =
> c("studyno",
> "pf_mcl"), row.names = c(NA, 6L), class = "data.frame")
>
> myDF2 is a well curated subset of myDF1. Some rows in the two datasets
> match based on "studyno", one may find that values are missing in
> myDF1$pf_mcl or the values are wrong.
>
> All I want to do is identify a matching row in myDF2 and populate
> myDF1$pf_mcl with the value in myDF2$pf_mcl. If a row does not match based
> on ?studyno?, the value should remain the same.
>
> It's probably worth mentioning, the two data frames have other columns...I
> have selected a few for example purposes.
>
>
>
> ______________________________________________________________________
>
> This e-mail contains information which is confidential. It is intended
> only for the use of the named recipient. If you have received this e-mail
> in error, please let us know by replying to the sender, and immediately
> delete it from your system.  Please note, that in these circumstances, the
> use, disclosure, distribution or copying of this information is strictly
> prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility
> for the  accuracy or completeness of this message as it has been
> transmitted over a public network. Although the Programme has taken
> reasonable precautions to ensure no viruses are present in emails, it
> cannot accept responsibility for any loss or damage arising from the use of
> the email or attachments. Any views expressed in this message are those of
> the individual sender, except where the sender specifically states them to
> be the views of KEMRI-Wellcome Trust Programme.
> ______________________________________________________________________
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
> UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________________________________
>
> This e-mail contains information which is confidential. It is intended
> only for the use of the named recipient. If you have received this e-mail
> in error, please let us know by replying to the sender, and immediately
> delete it from your system.  Please note, that in these circumstances, the
> use, disclosure, distribution or copying of this information is strictly
> prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility
> for the  accuracy or completeness of this message as it has been
> transmitted over a public network. Although the Programme has taken
> reasonable precautions to ensure no viruses are present in emails, it
> cannot accept responsibility for any loss or damage arising from the use of
> the email or attachments. Any views expressed in this message are those of
> the individual sender, except where the sender specifically states them to
> be the views of KEMRI-Wellcome Trust Programme.
> ______________________________________________________________________
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Sat Oct 14 19:58:47 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 14 Oct 2017 18:58:47 +0100
Subject: [R] make check Error: could not get timezone
In-Reply-To: <87o9pa6l8m.fsf@gmx.net>
References: <874lr2pyxr.fsf@gmx.net> <87vaji6o8l.fsf@gmx.net>
 <A186F055-E1CC-4C30-9C8E-E7F997AD9E55@dcn.davis.ca.us>
 <87o9pa6l8m.fsf@gmx.net>
Message-ID: <5C80EE4E-D368-4342-816F-3CE501B92873@dcn.davis.ca.us>

Your confusion would be alleviated if you read the whole thing. E.g.

Questions likely to prompt discussion unintelligible to non-programmers should rather go to R-devel than R-help. [...] For example, questions involving C, C++, etc. code should go to R-devel.

It is unfortunate that the errors involved in installing R on some platforms involve understanding C, but it is a reality that goes with such broad platform support and parsing through C compiler errors is not what this list is about.

In addition, anything you do with a patched or development version of R bears the risk of incomplete support, discussion of which belongs on R-devel. Patched versions are intended to fix specific bugs, so if you are not immediately inconvenienced by such a bug or are not prepared to help fix or at least troubleshoot problems then you probably should not be installing those versions. 
-- 
Sent from my phone. Please excuse my brevity.

On October 14, 2017 12:07:37 PM GMT+01:00, Stephen Berman <stephen.berman at gmx.net> wrote:
>On Sat, 14 Oct 2017 11:49:16 +0100 Jeff Newmiller
><jdnewmil at dcn.davis.ca.us> wrote:
>
>> Off topic. Read the Posting Guide.
>
>I did do that before posting; the choice seemed to be between R-help
>("intended to be comprehensible to people who want to use R to solve
>problems but who are not necessarily interested in or knowledgeable
>about programming") and R-devel ("for questions and discussion about R
>development and programming").  Obviously I need to install R before I
>can use it, and in my case I had to build it before I could install it,
>and running the tests is recommended, and that's where the issue arose;
>I couldn't tell if this was due to a bug in R or something on my end,
>i.e., I needed help, so that's why I asked here.  If you think R-devel
>is the right address, I'll ask there.
>
>Steve Berman


From ruipbarradas at sapo.pt  Sat Oct 14 21:29:17 2017
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Sat, 14 Oct 2017 20:29:17 +0100
Subject: [R] Bootstrapped Regression
In-Reply-To: <CAFCoDdCjZpO26afEPRGqR0zEn8UDUgkz88N2wWr-7Ca_sq6mVA@mail.gmail.com>
References: <CAFCoDdCjZpO26afEPRGqR0zEn8UDUgkz88N2wWr-7Ca_sq6mVA@mail.gmail.com>
Message-ID: <59E2658D.8040809@sapo.pt>

Hello,

Give us a dataset example. If your dataset is named x post the output of

dput(head(x, 20))

And post the code you are running for the regression.
What have you tried? The help page for function boot has an example of 
generelized linear regression (nuke.boot) that you might adapt to your 
needs.

Hope this helps,

Rui Barradas

Em 14-10-2017 15:58, Janh Anni escreveu:
> Greetings!
>
> We are trying to obtain confidence and prediction intervals for a predicted
> Y value from bootstrapped linear regression using the boot function. Does
> anyone know how to code it?  Greatly appreciated.
>
> Janh
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From annijanh at gmail.com  Sun Oct 15 03:22:25 2017
From: annijanh at gmail.com (Janh Anni)
Date: Sat, 14 Oct 2017 21:22:25 -0400
Subject: [R] Bootstrapped Regression
In-Reply-To: <CAGxFJbQMpYvNPNgCwH-jwvvaPLTcLs9QkrSQoeiMWbTEhsBL=A@mail.gmail.com>
References: <CAFCoDdCjZpO26afEPRGqR0zEn8UDUgkz88N2wWr-7Ca_sq6mVA@mail.gmail.com>
 <CAGxFJbQMpYvNPNgCwH-jwvvaPLTcLs9QkrSQoeiMWbTEhsBL=A@mail.gmail.com>
Message-ID: <CAFCoDdAjydbqADkzHfQLW7OSnHC=JXvoWe0jt324p+q=w+KgOQ@mail.gmail.com>

Hello Rui,

Thanks for your helpful suggestions.  Just for illustration, let's use the
well known Duncan dataset of prestige vs education + income that is
contained in the "car" package.  Suppose I wish to use boot function to
bootstrap a linear regression of prestige ~ education + income and use the
following script:

duncan.function <- function(data, indices) {data = data[indices,]

mod <- lm(prestige ~ education + income, data=data,)

coefficients(mod)}

Results <- boot(Duncan, duncan.function , 1000)
Results

So the 1000 bootstrapped coefficients are contained in Results and I can
use the boot.ci function in the same boot package to obtain the confidence
intervals for the, say, education coefficient with something like:

boot.ci(Results, index=2, conf = 0.95, type=c("basic", "norm", "perc",
"bca"))

Then, suppose I am interested in getting a confidence interval for the
predicted  prestige at, say, education = 50 and income = 75.  The question
is how do I get boot to compute 1000 values of the predicted prestige at
education = 50 and income = 75, so that I can subsequently (hopefully) have
boot.ci compute the confidence intervals as it did for the bootstrapped
coefficients? As for prediction intervals, it wouldn't seem conceptually
feasible in this context?  Thanks again for all your help.

Janh

On Sat, Oct 14, 2017 at 11:12 AM, Bert Gunter <bgunter.4567 at gmail.com>
wrote:

> R-help is not a free coding service. We expect users to make the effort to
> learn R and *may* provide help when they get stuck. Pay a local R
> programmer if you do not wish to make such an effort.
>
> Cheers,
> Bert
>
>
> On Oct 14, 2017 7:58 AM, "Janh Anni" <annijanh at gmail.com> wrote:
>
> Greetings!
>
> We are trying to obtain confidence and prediction intervals for a predicted
> Y value from bootstrapped linear regression using the boot function. Does
> anyone know how to code it?  Greatly appreciated.
>
> Janh
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>

	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Sun Oct 15 09:25:06 2017
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Sun, 15 Oct 2017 08:25:06 +0100
Subject: [R] Bootstrapped Regression
In-Reply-To: <CAFCoDdAjydbqADkzHfQLW7OSnHC=JXvoWe0jt324p+q=w+KgOQ@mail.gmail.com>
References: <CAFCoDdCjZpO26afEPRGqR0zEn8UDUgkz88N2wWr-7Ca_sq6mVA@mail.gmail.com>
 <CAGxFJbQMpYvNPNgCwH-jwvvaPLTcLs9QkrSQoeiMWbTEhsBL=A@mail.gmail.com>
 <CAFCoDdAjydbqADkzHfQLW7OSnHC=JXvoWe0jt324p+q=w+KgOQ@mail.gmail.com>
Message-ID: <59E30D52.6070708@sapo.pt>

Hello,

Much clearer now, thanks.
It's a matter of changing the function boot calls to return the 
predicted values at the point of interess, education = 50, income = 75.

I have changed the way the function uses the indices a bit, the result 
is the same, it's just the way I usually do it.

pred.duncan.function <- function(data, indices) {
     mod <- lm(prestige ~ education + income, data = data[indices, ])
     new <- data.frame(education = 50, income = 75)
     predict(mod, newdata = new)
}

set.seed(94)    # make the results reproducible

Predicted <- boot(Duncan, pred.duncan.function , 1000)
head(Predicted)
Predicted$t0
boot.ci(Predicted, index = 1, conf = 0.95, type=c("basic", "norm", 
"perc", "bca"))


Hope this helps,

Rui Barradas

Em 15-10-2017 02:22, Janh Anni escreveu:
> Hello Rui,
>
> Thanks for your helpful suggestions.  Just for illustration, let's use the
> well known Duncan dataset of prestige vs education + income that is
> contained in the "car" package.  Suppose I wish to use boot function to
> bootstrap a linear regression of prestige ~ education + income and use the
> following script:
>
> duncan.function <- function(data, indices) {data = data[indices,]
>
> mod <- lm(prestige ~ education + income, data=data,)
>
> coefficients(mod)}
>
> Results <- boot(Duncan, duncan.function , 1000)
> Results
>
> So the 1000 bootstrapped coefficients are contained in Results and I can
> use the boot.ci function in the same boot package to obtain the confidence
> intervals for the, say, education coefficient with something like:
>
> boot.ci(Results, index=2, conf = 0.95, type=c("basic", "norm", "perc",
> "bca"))
>
> Then, suppose I am interested in getting a confidence interval for the
> predicted  prestige at, say, education = 50 and income = 75.  The question
> is how do I get boot to compute 1000 values of the predicted prestige at
> education = 50 and income = 75, so that I can subsequently (hopefully) have
> boot.ci compute the confidence intervals as it did for the bootstrapped
> coefficients? As for prediction intervals, it wouldn't seem conceptually
> feasible in this context?  Thanks again for all your help.
>
> Janh
>
> On Sat, Oct 14, 2017 at 11:12 AM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
>
>> R-help is not a free coding service. We expect users to make the effort to
>> learn R and *may* provide help when they get stuck. Pay a local R
>> programmer if you do not wish to make such an effort.
>>
>> Cheers,
>> Bert
>>
>>
>> On Oct 14, 2017 7:58 AM, "Janh Anni" <annijanh at gmail.com> wrote:
>>
>> Greetings!
>>
>> We are trying to obtain confidence and prediction intervals for a predicted
>> Y value from bootstrapped linear regression using the boot function. Does
>> anyone know how to code it?  Greatly appreciated.
>>
>> Janh
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From KWamae at kemri-wellcome.org  Sun Oct 15 13:03:45 2017
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Sun, 15 Oct 2017 11:03:45 +0000
Subject: [R] Populate one data frame with values from another dataframe
 for rows that match
In-Reply-To: <CAF8bMcaBfrs0n2qj5O3EZePkkPu-gCdF+LqbksATBc1HahYhTQ@mail.gmail.com>
References: <13D7E727-E10A-468E-BA7D-EA6EE6D3EDF4@contoso.com>
 <CAGxFJbQHosDoKu-XjAJmnW+Eyf=rKMyoM3QOp9FD3yg=vRp5VQ@mail.gmail.com>
 <69104DFC-EC38-44A6-BBA4-CB6D097C11BF@kemri-wellcome.org>
 <CAF8bMcaBfrs0n2qj5O3EZePkkPu-gCdF+LqbksATBc1HahYhTQ@mail.gmail.com>
Message-ID: <10E68023-1D3A-4012-B3A0-0704FEEDFC40@kemri-wellcome.org>

Dear @William<mailto:wdunlap at tibco.com>, thanks for the feedback. I have tested it on the larger dataset and noticed that it created two variables, pf_raw and pf_curated.

The output we were looking for, was one that takes the variable pf_mcl in curated dataset and replaces pf_mcl in matching rows within the raw dataset.

@Eric<mailto:ericjberger at gmail.com>?s solution was able to achieve that. Nonetheless, we do appreciate your solution.

Regards
------------------
Kevin Wamae

From: William Dunlap <wdunlap at tibco.com>
Date: Saturday, 14 October 2017 at 20:21
To: Kevin Wamae <KWamae at kemri-wellcome.org>
Cc: Bert Gunter <bgunter.4567 at gmail.com>, Rui Barradas <ruipbarradas at sapo.pt>, R-help <R-help at r-project.org>
Subject: Re: [R] Populate one data frame with values from another dataframe for rows that match

Your example used one distinct studyno in DF1 and one distinct pf_mcl in DF2.  I think that makes it hard to see what is going on, but maybe I completely misunderstand the problem.  In any case, let's redefine myDF1 and myDF2.  Note that myDF1 contains a studyno not in myDF2 and vice versa.

myDF1 <- structure(list(studyno = c("J1000/9", "J895/7", "J931/6", "J666/6",
"J1000/9", "J1000/9"), date = structure(c(17123, 17127, 17135,
17144, 17148, 17155), class = "Date"), pf_mcl = c(NA_integer_,
2L, 3L, 4L, 5L, NA_integer_
), year = c(2016, 2016, 2016, 2016, 2016, 2016)), .Names = c("studyno",
"date", "pf_mcl", "year"), row.names = c(NA, 6L), class = "data.frame")

myDF2 <- structure(list(studyno = c("J740/4", "J1000/9", "J895/7", "J931/6",
"J609/1", "J941/3"), pf_mcl = c(101L, 102L, 103L, 104L, 105L, 106L)),
.Names = c("studyno", "pf_mcl"), row.names = c(NA, 6L), class = "data.frame")

m <- merge(myDF1, myDF2, by="studyno", all.x=TRUE, all.y=FALSE, suffixes=c(".raw", ".curated"))


The results are:

> myDF1
  studyno       date pf_mcl year
1 J1000/9 2016-11-18     NA 2016
2  J895/7 2016-11-22      2 2016
3  J931/6 2016-11-30      3 2016
4  J666/6 2016-12-09      4 2016
5 J1000/9 2016-12-13      5 2016
6 J1000/9 2016-12-20     NA 2016
> myDF2
  studyno pf_mcl
1  J740/4    101
2 J1000/9    102
3  J895/7    103
4  J931/6    104
5  J609/1    105
6  J941/3    106
> m
  studyno       date pf_mcl.raw year pf_mcl.curated
1 J1000/9 2016-11-18         NA 2016            102
2 J1000/9 2016-12-13          5 2016            102
3 J1000/9 2016-12-20         NA 2016            102
4  J666/6 2016-12-09          4 2016             NA
5  J895/7 2016-11-22          2 2016            103
6  J931/6 2016-11-30          3 2016            104


Now your problem is to combine the columns pf_mcl.raw and pf_mcl.curated in the way you want.  ifelse() may be useful for that.


Bill Dunlap
TIBCO Software
wdunlap tibco.com<http://tibco.com>

On Fri, Oct 13, 2017 at 10:48 PM, Kevin Wamae <KWamae at kemri-wellcome.org<mailto:KWamae at kemri-wellcome.org>> wrote:
Dear @Bert Gunter<mailto:bgunter.4567 at gmail.com<mailto:bgunter.4567 at gmail.com>>, I tried merge and I faced many challenges. @Rui Barradas<mailto:ruipbarradas at sapo.pt<mailto:ruipbarradas at sapo.pt>> solution is working.

From: Bert Gunter <bgunter.4567 at gmail.com<mailto:bgunter.4567 at gmail.com>>
Date: Friday, 13 October 2017 at 22:44
To: Kevin Wamae <KWamae at kemri-wellcome.org<mailto:KWamae at kemri-wellcome.org>>
Cc: R-help <R-help at r-project.org<mailto:R-help at r-project.org>>
Subject: Re: [R] Populate one data frame with values from another dataframe for rows that match

?merge

Bert

On Oct 13, 2017 12:09 PM, "Kevin Wamae" <KWamae at kemri-wellcome.org<mailto:KWamae at kemri-wellcome.org><mailto:KWamae at kemri-wellcome.org<mailto:KWamae at kemri-wellcome.org>>> wrote:
I'm trying to populate the column ?pf_mcl? in myDF1 with values from myDF2, where rows match based on column "studyno" but the solutions I have found so far don't seem to be giving me the desired output.

Below is a snapshot of the data.frames.

myDF1 <- structure(list(studyno = c("J1000/9", "J1000/9", "J1000/9", "J1000/9",
"J1000/9", "J1000/9"), date = structure(c(17123, 17127, 17135,
17144, 17148, 17155), class = "Date"), pf_mcl = c(NA_integer_,
NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_
), year = c(2016, 2016, 2016, 2016, 2016, 2016)), .Names = c("studyno",
"date", "pf_mcl", "year"), row.names = c(NA, 6L), class = "data.frame")

myDF2 <- structure(list(studyno = c("J740/4", "J1000/9", "J895/7", "J931/6",
"J609/1", "J941/3"), pf_mcl = c(0L, 0L, 0L, 0L, 0L, 0L)), .Names = c("studyno",
"pf_mcl"), row.names = c(NA, 6L), class = "data.frame")

myDF2 is a well curated subset of myDF1. Some rows in the two datasets match based on "studyno", one may find that values are missing in myDF1$pf_mcl or the values are wrong.

All I want to do is identify a matching row in myDF2 and populate myDF1$pf_mcl with the value in myDF2$pf_mcl. If a row does not match based on ?studyno?, the value should remain the same.

It's probably worth mentioning, the two data frames have other columns...I have selected a few for example purposes.



______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org><mailto:R-help at r-project.org<mailto:R-help at r-project.org>> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

	[[alternative HTML version deleted]]


From KWamae at kemri-wellcome.org  Sun Oct 15 13:07:11 2017
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Sun, 15 Oct 2017 11:07:11 +0000
Subject: [R] Populate one data frame with values from another dataframe
 for rows that match
In-Reply-To: <10E68023-1D3A-4012-B3A0-0704FEEDFC40@kemri-wellcome.org>
References: <13D7E727-E10A-468E-BA7D-EA6EE6D3EDF4@contoso.com>
 <CAGxFJbQHosDoKu-XjAJmnW+Eyf=rKMyoM3QOp9FD3yg=vRp5VQ@mail.gmail.com>
 <69104DFC-EC38-44A6-BBA4-CB6D097C11BF@kemri-wellcome.org>
 <CAF8bMcaBfrs0n2qj5O3EZePkkPu-gCdF+LqbksATBc1HahYhTQ@mail.gmail.com>
 <10E68023-1D3A-4012-B3A0-0704FEEDFC40@kemri-wellcome.org>
Message-ID: <CAA0D502-4A66-496B-AEF5-1634D0A855B5@kemri-wellcome.org>

Pardon me, here?s @Eric<mailto:ericjberger at gmail.com>?s solution?

myDF1$studyno <- as.character(myDF1$studyno)
myDF2$studyno <- as.character(myDF2$studyno)
myDF3 <- merge(myDF1, myDF2, by="studyno", all.x=TRUE ) %>%
                dplyr::mutate( pf_mcl = ifelse( is.na<http://is.na/>(pf_mcl.y), pf_mcl.x, pf_mcl.y ) ) %>%
                dplyr::select( studyno, date, pf_mcl )

Regards
------------------
Kevin Wamae

From: Kevin Wamae <KWamae at kemri-wellcome.org>
Date: Sunday, 15 October 2017 at 14:03
To: William Dunlap <wdunlap at tibco.com>
Cc: Bert Gunter <bgunter.4567 at gmail.com>, Rui Barradas <ruipbarradas at sapo.pt>, Eric Berger <ericjberger at gmail.com>, R-help <R-help at r-project.org>
Subject: Re: [R] Populate one data frame with values from another dataframe for rows that match

Dear @William<mailto:wdunlap at tibco.com>, thanks for the feedback. I have tested it on the larger dataset and noticed that it created two variables, pf_raw and pf_curated.

The output we were looking for, was one that takes the variable pf_mcl in curated dataset and replaces pf_mcl in matching rows within the raw dataset.

@Eric<mailto:ericjberger at gmail.com>?s solution was able to achieve that. Nonetheless, we do appreciate your solution.

Regards
------------------
Kevin Wamae

From: William Dunlap <wdunlap at tibco.com>
Date: Saturday, 14 October 2017 at 20:21
To: Kevin Wamae <KWamae at kemri-wellcome.org>
Cc: Bert Gunter <bgunter.4567 at gmail.com>, Rui Barradas <ruipbarradas at sapo.pt>, R-help <R-help at r-project.org>
Subject: Re: [R] Populate one data frame with values from another dataframe for rows that match

Your example used one distinct studyno in DF1 and one distinct pf_mcl in DF2.  I think that makes it hard to see what is going on, but maybe I completely misunderstand the problem.  In any case, let's redefine myDF1 and myDF2.  Note that myDF1 contains a studyno not in myDF2 and vice versa.

myDF1 <- structure(list(studyno = c("J1000/9", "J895/7", "J931/6", "J666/6",
"J1000/9", "J1000/9"), date = structure(c(17123, 17127, 17135,
17144, 17148, 17155), class = "Date"), pf_mcl = c(NA_integer_,
2L, 3L, 4L, 5L, NA_integer_
), year = c(2016, 2016, 2016, 2016, 2016, 2016)), .Names = c("studyno",
"date", "pf_mcl", "year"), row.names = c(NA, 6L), class = "data.frame")

myDF2 <- structure(list(studyno = c("J740/4", "J1000/9", "J895/7", "J931/6",
"J609/1", "J941/3"), pf_mcl = c(101L, 102L, 103L, 104L, 105L, 106L)),
.Names = c("studyno", "pf_mcl"), row.names = c(NA, 6L), class = "data.frame")

m <- merge(myDF1, myDF2, by="studyno", all.x=TRUE, all.y=FALSE, suffixes=c(".raw", ".curated"))


The results are:

> myDF1
  studyno       date pf_mcl year
1 J1000/9 2016-11-18     NA 2016
2  J895/7 2016-11-22      2 2016
3  J931/6 2016-11-30      3 2016
4  J666/6 2016-12-09      4 2016
5 J1000/9 2016-12-13      5 2016
6 J1000/9 2016-12-20     NA 2016
> myDF2
  studyno pf_mcl
1  J740/4    101
2 J1000/9    102
3  J895/7    103
4  J931/6    104
5  J609/1    105
6  J941/3    106
> m
  studyno       date pf_mcl.raw year pf_mcl.curated
1 J1000/9 2016-11-18         NA 2016            102
2 J1000/9 2016-12-13          5 2016            102
3 J1000/9 2016-12-20         NA 2016            102
4  J666/6 2016-12-09          4 2016             NA
5  J895/7 2016-11-22          2 2016            103
6  J931/6 2016-11-30          3 2016            104


Now your problem is to combine the columns pf_mcl.raw and pf_mcl.curated in the way you want.  ifelse() may be useful for that.


Bill Dunlap
TIBCO Software
wdunlap tibco.com<http://tibco.com>

On Fri, Oct 13, 2017 at 10:48 PM, Kevin Wamae <KWamae at kemri-wellcome.org<mailto:KWamae at kemri-wellcome.org>> wrote:
Dear @Bert Gunter<mailto:bgunter.4567 at gmail.com<mailto:bgunter.4567 at gmail.com>>, I tried merge and I faced many challenges. @Rui Barradas<mailto:ruipbarradas at sapo.pt<mailto:ruipbarradas at sapo.pt>> solution is working.

From: Bert Gunter <bgunter.4567 at gmail.com<mailto:bgunter.4567 at gmail.com>>
Date: Friday, 13 October 2017 at 22:44
To: Kevin Wamae <KWamae at kemri-wellcome.org<mailto:KWamae at kemri-wellcome.org>>
Cc: R-help <R-help at r-project.org<mailto:R-help at r-project.org>>
Subject: Re: [R] Populate one data frame with values from another dataframe for rows that match

?merge

Bert

On Oct 13, 2017 12:09 PM, "Kevin Wamae" <KWamae at kemri-wellcome.org<mailto:KWamae at kemri-wellcome.org><mailto:KWamae at kemri-wellcome.org<mailto:KWamae at kemri-wellcome.org>>> wrote:
I'm trying to populate the column ?pf_mcl? in myDF1 with values from myDF2, where rows match based on column "studyno" but the solutions I have found so far don't seem to be giving me the desired output.

Below is a snapshot of the data.frames.

myDF1 <- structure(list(studyno = c("J1000/9", "J1000/9", "J1000/9", "J1000/9",
"J1000/9", "J1000/9"), date = structure(c(17123, 17127, 17135,
17144, 17148, 17155), class = "Date"), pf_mcl = c(NA_integer_,
NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_
), year = c(2016, 2016, 2016, 2016, 2016, 2016)), .Names = c("studyno",
"date", "pf_mcl", "year"), row.names = c(NA, 6L), class = "data.frame")

myDF2 <- structure(list(studyno = c("J740/4", "J1000/9", "J895/7", "J931/6",
"J609/1", "J941/3"), pf_mcl = c(0L, 0L, 0L, 0L, 0L, 0L)), .Names = c("studyno",
"pf_mcl"), row.names = c(NA, 6L), class = "data.frame")

myDF2 is a well curated subset of myDF1. Some rows in the two datasets match based on "studyno", one may find that values are missing in myDF1$pf_mcl or the values are wrong.

All I want to do is identify a matching row in myDF2 and populate myDF1$pf_mcl with the value in myDF2$pf_mcl. If a row does not match based on ?studyno?, the value should remain the same.

It's probably worth mentioning, the two data frames have other columns...I have selected a few for example purposes.



______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org><mailto:R-help at r-project.org<mailto:R-help at r-project.org>> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

	[[alternative HTML version deleted]]


From milujisb at gmail.com  Sun Oct 15 23:02:07 2017
From: milujisb at gmail.com (Miluji Sb)
Date: Sun, 15 Oct 2017 23:02:07 +0200
Subject: [R] Download data from NASA for multiple locations - RCurl
Message-ID: <CAMLwc7MLcExXDYGyetJfmFEFJMyo=1-1EiRUKmgd72xjPN_kCg@mail.gmail.com>

Dear all,

i am trying to download time-series climatic data from GES DISC (NASA)
Hydrology Data Rods web-service. Unfortunately, no wget method is
available.

Five parameters are needed for data retrieval: variable, location,
startDate, endDate, and type. For example:

###
https://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/access/timeseries.cgi?variable=GLDAS2:GLDAS_NOAH025_3H_v2.0:Tair_f_inst&startDate=1970-01-01T00&endDate=1979-12-31T00&location=GEOM:POINT(-71.06,%2042.36)&type=asc2
###

In this case, variable: Tair_f_inst (temperature), location: (-71.06,
42.36), startDate: 01 January 1970; endDate: 31 December 1979; type:  asc2
(output 2-column ASCII).

I am trying to download data for 100 US cities, data for which I have in
the following data.frame:

###
cities <-  dput(droplevels(head(cities, 5)))
structure(list(city = structure(1:5, .Label = c("Boston", "Bridgeport",
"Cambridge", "Fall River", "Hartford"), class = "factor"), state =
structure(c(2L,
1L, 2L, 2L, 1L), .Label = c(" CT ", " MA "), class = "factor"),
    lon = c(-71.06, -73.19, -71.11, -71.16, -72.67), lat = c(42.36,
    41.18, 42.37, 41.7, 41.77)), .Names = c("city", "state",
"lon", "lat"), row.names = c(NA, 5L), class = "data.frame")
###

Is it possible to download the data for the multiple locations
automatically (e.g. RCurl) and save them as csv? Essentially, reading
coordinates from the data.frame and entering it in the URL.

I would also like to add identifying information to each of the data files
from the cities data.frame. I have been doing the following for a single
file:

###
x <- readLines(con=url("
https://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/access/timeseries.cgi?variable=GLDAS2:GLDAS_NOAH025_3H_v2.0:Tair_f_inst&startDate=1970-01-01T00&endDate=1979-12-31T00&location=GEOM:POINT(-71.06,%2042.36)&type=asc2
"))
x <- x[-(1:13)]

mydata <- data.frame(year = substr(x,1,4),
                     month = substr(x, 6,7),
                     day = substr(x, 9, 10),
                     hour = substr(x, 12, 13),
                     temp = substr(x, 21, 27))

mydata$city <- rep(cities[1,1], nrow(mydata))
mydata$state <- rep(cities[1,2], nrow(mydata))
mydata$lon <- rep(cities[1,3], nrow(mydata))
mydata$lat <- rep(cities[1,4], nrow(mydata))
###

Help and advice would be greatly appreciated. Thank you!

Sincerely,

Milu

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sun Oct 15 23:45:44 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 15 Oct 2017 14:45:44 -0700
Subject: [R] Download data from NASA for multiple locations - RCurl
In-Reply-To: <CAMLwc7MLcExXDYGyetJfmFEFJMyo=1-1EiRUKmgd72xjPN_kCg@mail.gmail.com>
References: <CAMLwc7MLcExXDYGyetJfmFEFJMyo=1-1EiRUKmgd72xjPN_kCg@mail.gmail.com>
Message-ID: <74C4AC4B-9AB3-47A5-A54C-C36C5E1338D0@comcast.net>


> On Oct 15, 2017, at 2:02 PM, Miluji Sb <milujisb at gmail.com> wrote:
> 
> Dear all,
> 
> i am trying to download time-series climatic data from GES DISC (NASA)
> Hydrology Data Rods web-service. Unfortunately, no wget method is
> available.
> 
> Five parameters are needed for data retrieval: variable, location,
> startDate, endDate, and type. For example:
> 
> ###
> https://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/access/timeseries.cgi?variable=GLDAS2:GLDAS_NOAH025_3H_v2.0:Tair_f_inst&startDate=1970-01-01T00&endDate=1979-12-31T00&location=GEOM:POINT(-71.06,%2042.36)&type=asc2
> ###
> 
> In this case, variable: Tair_f_inst (temperature), location: (-71.06,
> 42.36), startDate: 01 January 1970; endDate: 31 December 1979; type:  asc2
> (output 2-column ASCII).
> 
> I am trying to download data for 100 US cities, data for which I have in
> the following data.frame:
> 
> ###
> cities <-  dput(droplevels(head(cities, 5)))
> structure(list(city = structure(1:5, .Label = c("Boston", "Bridgeport",
> "Cambridge", "Fall River", "Hartford"), class = "factor"), state =
> structure(c(2L,
> 1L, 2L, 2L, 1L), .Label = c(" CT ", " MA "), class = "factor"),
>    lon = c(-71.06, -73.19, -71.11, -71.16, -72.67), lat = c(42.36,
>    41.18, 42.37, 41.7, 41.77)), .Names = c("city", "state",
> "lon", "lat"), row.names = c(NA, 5L), class = "data.frame")
> ###
> 
> Is it possible to download the data for the multiple locations
> automatically (e.g. RCurl) and save them as csv? Essentially, reading
> coordinates from the data.frame and entering it in the URL.
> 
> I would also like to add identifying information to each of the data files
> from the cities data.frame. I have been doing the following for a single
> file:

Didn't seem that difficult:

library(downloader)  # makes things easier for Macs, perhaps not needed 
# if not used will need to use download.file

for( i in 1:5) { 
  target1 <- paste0("https://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/access/timeseries.cgi?variable=GLDAS2:GLDAS_NOAH025_3H_v2.0:Tair_f_inst&startDate=1970-01-01T00&endDate=1979-12-31T00&location=GEOM:POINT(", 
                     cities[i, "lon"],
                     ",%20", cities[i,"lat"],
                     ")&type=asc2")
  target2 <- paste0("~/",    # change for whatever destination directory you may prefer.
                    cities[i,"city"], 
                    cities[i,"state"], ".asc")
  download(url=target1, destfile=target2)
                }

Now I have 5 named files with extensions ".asc" in my user directory (since I'm on a Mac). It is a slow website so patience is needed.

-- 
David


> 
> ###
> x <- readLines(con=url("
> https://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/access/timeseries.cgi?variable=GLDAS2:GLDAS_NOAH025_3H_v2.0:Tair_f_inst&startDate=1970-01-01T00&endDate=1979-12-31T00&location=GEOM:POINT(-71.06,%2042.36)&type=asc2
> "))
> x <- x[-(1:13)]
> 
> mydata <- data.frame(year = substr(x,1,4),
>                     month = substr(x, 6,7),
>                     day = substr(x, 9, 10),
>                     hour = substr(x, 12, 13),
>                     temp = substr(x, 21, 27))
> 
> mydata$city <- rep(cities[1,1], nrow(mydata))
> mydata$state <- rep(cities[1,2], nrow(mydata))
> mydata$lon <- rep(cities[1,3], nrow(mydata))
> mydata$lat <- rep(cities[1,4], nrow(mydata))
> ###
> 
> Help and advice would be greatly appreciated. Thank you!
> 
> Sincerely,
> 
> Milu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From milujisb at gmail.com  Mon Oct 16 00:35:36 2017
From: milujisb at gmail.com (Miluji Sb)
Date: Mon, 16 Oct 2017 00:35:36 +0200
Subject: [R] Download data from NASA for multiple locations - RCurl
In-Reply-To: <74C4AC4B-9AB3-47A5-A54C-C36C5E1338D0@comcast.net>
References: <CAMLwc7MLcExXDYGyetJfmFEFJMyo=1-1EiRUKmgd72xjPN_kCg@mail.gmail.com>
 <74C4AC4B-9AB3-47A5-A54C-C36C5E1338D0@comcast.net>
Message-ID: <CAMLwc7OxFgqtTayGn3jq_bcCY3XmaFax7XpdWOYuQgjYJGQZPA@mail.gmail.com>

Dear David,

This is amazing, thank you so much. If I may ask another question:

The output looks like the following:

###
dput(head(x,15))
c("Metadata for Requested Time Series:", "",
"prod_name=GLDAS_NOAH025_3H_v2.0",
"param_short_name=Tair_f_inst", "param_name=Near surface air temperature",
"unit=K", "begin_time=1970-01-01T00", "end_time=1979-12-31T21",
"lat= 42.36", "lon=-71.06", "Request_time=2017-10-15 22:20:03 GMT",
"", "Date&Time               Data", "1970-01-01T00:00:00\t267.769",
"1970-01-01T03:00:00\t264.595")
###

Thus I need to drop the first 13 rows and do the following to add
identifying information:

###
mydata <- data.frame(year = substr(x,1,4),
                     month = substr(x, 6,7),
                     day = substr(x, 9, 10),
                     hour = substr(x, 12, 13),
                     temp = substr(x, 21, 27))

mydata$city <- rep(cities[1,1], nrow(mydata))
mydata$state <- rep(cities[1,2], nrow(mydata))
mydata$lon <- rep(cities[1,3], nrow(mydata))
mydata$lat <- rep(cities[1,4], nrow(mydata))
###

Is it possible to incorporate these into your code so the data looks like
this:

dput(droplevels(head(mydata)))
structure(list(year = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = "1970",
class = "factor"),
    month = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = "01", class =
"factor"),
    day = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = "01", class =
"factor"),
    hour = structure(1:6, .Label = c("00", "03", "06", "09",
    "12", "15"), class = "factor"), temp = structure(c(6L, 4L,
    2L, 1L, 3L, 5L), .Label = c("261.559", "262.525", "262.648",
    "264.595", "265.812", "267.769"), class = "factor"), city =
structure(c(1L,
    1L, 1L, 1L, 1L, 1L), .Label = "Boston", class = "factor"),
    state = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = " MA ", class =
"factor"),
    lon = c(-71.06, -71.06, -71.06, -71.06, -71.06, -71.06),
    lat = c(42.36, 42.36, 42.36, 42.36, 42.36, 42.36)), .Names = c("year",
"month", "day", "hour", "temp", "city", "state", "lon", "lat"
), row.names = c(NA, 6L), class = "data.frame")

Apologies for asking repeated questions and thank you again!

Sincerely,

Milu

On Sun, Oct 15, 2017 at 11:45 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Oct 15, 2017, at 2:02 PM, Miluji Sb <milujisb at gmail.com> wrote:
> >
> > Dear all,
> >
> > i am trying to download time-series climatic data from GES DISC (NASA)
> > Hydrology Data Rods web-service. Unfortunately, no wget method is
> > available.
> >
> > Five parameters are needed for data retrieval: variable, location,
> > startDate, endDate, and type. For example:
> >
> > ###
> > https://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/access/
> timeseries.cgi?variable=GLDAS2:GLDAS_NOAH025_3H_v2.0:
> Tair_f_inst&startDate=1970-01-01T00&endDate=1979-12-31T00&
> location=GEOM:POINT(-71.06,%2042.36)&type=asc2
> > ###
> >
> > In this case, variable: Tair_f_inst (temperature), location: (-71.06,
> > 42.36), startDate: 01 January 1970; endDate: 31 December 1979; type:
> asc2
> > (output 2-column ASCII).
> >
> > I am trying to download data for 100 US cities, data for which I have in
> > the following data.frame:
> >
> > ###
> > cities <-  dput(droplevels(head(cities, 5)))
> > structure(list(city = structure(1:5, .Label = c("Boston", "Bridgeport",
> > "Cambridge", "Fall River", "Hartford"), class = "factor"), state =
> > structure(c(2L,
> > 1L, 2L, 2L, 1L), .Label = c(" CT ", " MA "), class = "factor"),
> >    lon = c(-71.06, -73.19, -71.11, -71.16, -72.67), lat = c(42.36,
> >    41.18, 42.37, 41.7, 41.77)), .Names = c("city", "state",
> > "lon", "lat"), row.names = c(NA, 5L), class = "data.frame")
> > ###
> >
> > Is it possible to download the data for the multiple locations
> > automatically (e.g. RCurl) and save them as csv? Essentially, reading
> > coordinates from the data.frame and entering it in the URL.
> >
> > I would also like to add identifying information to each of the data
> files
> > from the cities data.frame. I have been doing the following for a single
> > file:
>
> Didn't seem that difficult:
>
> library(downloader)  # makes things easier for Macs, perhaps not needed
> # if not used will need to use download.file
>
> for( i in 1:5) {
>   target1 <- paste0("https://hydro1.gesdisc.eosdis.nasa.gov/daac-
> bin/access/timeseries.cgi?variable=GLDAS2:GLDAS_NOAH025_
> 3H_v2.0:Tair_f_inst&startDate=1970-01-01T00&endDate=1979-12-
> 31T00&location=GEOM:POINT(",
>                      cities[i, "lon"],
>                      ",%20", cities[i,"lat"],
>                      ")&type=asc2")
>   target2 <- paste0("~/",    # change for whatever destination directory
> you may prefer.
>                     cities[i,"city"],
>                     cities[i,"state"], ".asc")
>   download(url=target1, destfile=target2)
>                 }
>
> Now I have 5 named files with extensions ".asc" in my user directory
> (since I'm on a Mac). It is a slow website so patience is needed.
>
> --
> David
>
>
> >
> > ###
> > x <- readLines(con=url("
> > https://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/access/
> timeseries.cgi?variable=GLDAS2:GLDAS_NOAH025_3H_v2.0:
> Tair_f_inst&startDate=1970-01-01T00&endDate=1979-12-31T00&
> location=GEOM:POINT(-71.06,%2042.36)&type=asc2
> > "))
> > x <- x[-(1:13)]
> >
> > mydata <- data.frame(year = substr(x,1,4),
> >                     month = substr(x, 6,7),
> >                     day = substr(x, 9, 10),
> >                     hour = substr(x, 12, 13),
> >                     temp = substr(x, 21, 27))
> >
> > mydata$city <- rep(cities[1,1], nrow(mydata))
> > mydata$state <- rep(cities[1,2], nrow(mydata))
> > mydata$lon <- rep(cities[1,3], nrow(mydata))
> > mydata$lat <- rep(cities[1,4], nrow(mydata))
> > ###
> >
> > Help and advice would be greatly appreciated. Thank you!
> >
> > Sincerely,
> >
> > Milu
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> 'Any technology distinguishable from magic is insufficiently advanced.'
>  -Gehm's Corollary to Clarke's Third Law
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From annijanh at gmail.com  Mon Oct 16 03:01:47 2017
From: annijanh at gmail.com (Janh Anni)
Date: Sun, 15 Oct 2017 21:01:47 -0400
Subject: [R] Bootstrapped Regression
In-Reply-To: <59E30D52.6070708@sapo.pt>
References: <CAFCoDdCjZpO26afEPRGqR0zEn8UDUgkz88N2wWr-7Ca_sq6mVA@mail.gmail.com>
 <CAGxFJbQMpYvNPNgCwH-jwvvaPLTcLs9QkrSQoeiMWbTEhsBL=A@mail.gmail.com>
 <CAFCoDdAjydbqADkzHfQLW7OSnHC=JXvoWe0jt324p+q=w+KgOQ@mail.gmail.com>
 <59E30D52.6070708@sapo.pt>
Message-ID: <CAFCoDdA=W3nE3Tz84EdfeOk4REmsdtVVFzJ3hf5ndp4DxoZ1oA@mail.gmail.com>

Hello Rui,

It was perfect!  Thank you so much for your kindness.  It is greatly
appreciated.

All the best,
Janh

On Sun, Oct 15, 2017 at 3:25 AM, Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> Much clearer now, thanks.
> It's a matter of changing the function boot calls to return the predicted
> values at the point of interess, education = 50, income = 75.
>
> I have changed the way the function uses the indices a bit, the result is
> the same, it's just the way I usually do it.
>
> pred.duncan.function <- function(data, indices) {
>     mod <- lm(prestige ~ education + income, data = data[indices, ])
>     new <- data.frame(education = 50, income = 75)
>     predict(mod, newdata = new)
> }
>
> set.seed(94)    # make the results reproducible
>
> Predicted <- boot(Duncan, pred.duncan.function , 1000)
> head(Predicted)
> Predicted$t0
> boot.ci(Predicted, index = 1, conf = 0.95, type=c("basic", "norm",
> "perc", "bca"))
>
>
> Hope this helps,
>
> Rui Barradas
>
> Em 15-10-2017 02:22, Janh Anni escreveu:
>
>> Hello Rui,
>>
>> Thanks for your helpful suggestions.  Just for illustration, let's use the
>> well known Duncan dataset of prestige vs education + income that is
>> contained in the "car" package.  Suppose I wish to use boot function to
>> bootstrap a linear regression of prestige ~ education + income and use the
>> following script:
>>
>> duncan.function <- function(data, indices) {data = data[indices,]
>>
>> mod <- lm(prestige ~ education + income, data=data,)
>>
>> coefficients(mod)}
>>
>> Results <- boot(Duncan, duncan.function , 1000)
>> Results
>>
>> So the 1000 bootstrapped coefficients are contained in Results and I can
>> use the boot.ci function in the same boot package to obtain the
>> confidence
>> intervals for the, say, education coefficient with something like:
>>
>> boot.ci(Results, index=2, conf = 0.95, type=c("basic", "norm", "perc",
>> "bca"))
>>
>> Then, suppose I am interested in getting a confidence interval for the
>> predicted  prestige at, say, education = 50 and income = 75.  The question
>> is how do I get boot to compute 1000 values of the predicted prestige at
>> education = 50 and income = 75, so that I can subsequently (hopefully)
>> have
>> boot.ci compute the confidence intervals as it did for the bootstrapped
>> coefficients? As for prediction intervals, it wouldn't seem conceptually
>> feasible in this context?  Thanks again for all your help.
>>
>> Janh
>>
>> On Sat, Oct 14, 2017 at 11:12 AM, Bert Gunter <bgunter.4567 at gmail.com>
>> wrote:
>>
>> R-help is not a free coding service. We expect users to make the effort to
>>> learn R and *may* provide help when they get stuck. Pay a local R
>>> programmer if you do not wish to make such an effort.
>>>
>>> Cheers,
>>> Bert
>>>
>>>
>>> On Oct 14, 2017 7:58 AM, "Janh Anni" <annijanh at gmail.com> wrote:
>>>
>>> Greetings!
>>>
>>> We are trying to obtain confidence and prediction intervals for a
>>> predicted
>>> Y value from bootstrapped linear regression using the boot function. Does
>>> anyone know how to code it?  Greatly appreciated.
>>>
>>> Janh
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>> ng-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>
>>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>

	[[alternative HTML version deleted]]


From miaojpm at gmail.com  Mon Oct 16 03:46:59 2017
From: miaojpm at gmail.com (John)
Date: Sun, 15 Oct 2017 18:46:59 -0700
Subject: [R] [FORGED] can't print ggplot with Chinese characters to pdf
	files
In-Reply-To: <7f9f2636-c3d3-40b1-8ce3-9d4b2bde3336@stat.auckland.ac.nz>
References: <CABcx46A4OL5+Ja-e0eAr=7iF2_obaCOZh4a7vHMLcqkZNpD3MQ@mail.gmail.com>
 <1daf3c68-a4ee-3755-911d-7f997528499e@stat.auckland.ac.nz>
 <CABcx46DFdnwUjdO4ciaD5aeoGSDR2UgUJwMvjWG6WkjBPJ=wrg@mail.gmail.com>
 <7f9f2636-c3d3-40b1-8ce3-9d4b2bde3336@stat.auckland.ac.nz>
Message-ID: <CABcx46AzBfR9c_CyBXXr6OiPkK=YHBU3KHYZZ4N8y53s5VsZqQ@mail.gmail.com>

Hi,


   Sorry to bother you with this question here.

   I tried to install Macports on my Mac OS Sierra, and type "sudo port
install cairo", but it did not respond. I haven't seen any file name or app
called Macports, but one file called "port" which is located at


loca/bin/port"


   How should I do it?


   Thanks,


John

*******


Last login: Sun Oct 15 02:52:49 on console

Johns-MacBook-Pro:~ john$ /opt/local/bin/port ; exit;

MacPorts 2.4.2

Entering shell mode... ("help" for help, "quit" to quit)

[Users/john] > sudo port install cairo

Error: Unrecognized action "port sudo"




2017-10-12 19:55 GMT-07:00 Paul Murrell <paul at stat.auckland.ac.nz>:

> Hi
>
> By the looks of it, you need to install Cairo graphics ...
>
> https://www.cairographics.org/download/
>
> Paul
>
> On 13/10/17 15:48, John wrote:
>
>> Thanks, Paul. Following your solution,  I got this error message:
>>
>> Warning message:
>> In cairo_pdf("test_plot_chinese.pdf") : failed to load cairo DLL
>>
>> Is there anything else I need to install?
>>
>> Thanks,
>>
>> John
>>
>> 2017-10-12 19:24 GMT-07:00 Paul Murrell <paul at stat.auckland.ac.nz
>> <mailto:paul at stat.auckland.ac.nz>>:
>>
>>
>>     Hi
>>
>>     Instead of ...
>>
>>     ggsave("test_plot_chinese.pdf", m2)
>>
>>     ... try ...
>>
>>     cairo_pdf("test_plot_chinese.pdf")
>>     print(m2)
>>     dev.off()
>>
>>     Paul
>>
>>
>>     On 13/10/17 02:12, John wrote:
>>
>>         I install the Chinese font "Kaiti TC" on my mac, but I can't
>>         print the
>>         figures to pdf file by "marrangeGrob" command, which is in the
>>         package
>>         "gridExtra". Error message after I type "ggsave(......)" (last
>>         line of the
>>         program):
>>
>>         "Saving 7.47 x 5.15 in image
>>         Error in grid.Call.graphics(L_text, as.graphicsAnnot(x$label),
>>         x$x, x$y,  :
>>             invalid font type
>>         In addition: There were 50 or more warnings (use warnings() to
>>         see the
>>         first 50)"
>>
>>         How can I install the font so that I can see the figures on the
>>         pdf files?
>>         Some information that may be useful:
>>         (1) If I use the English ggtitle, there is no problem at all.
>>         The pdf file
>>         present the figures perfectly.
>>         (2) If I use Chinese title, I can still see the figures p1 and p2
>> on
>>         Studio, but I can't see it at the pdf file produced by ggsave.
>>         It gives an
>>         error message
>>
>>
>>
>>         rm(list=ls())
>>         library(ggplot2)
>>         library(gridExtra)
>>         df1<-data.frame(x=1:2, y=3:4, z=5:6)
>>         #p1<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("test1")
>>         #p2<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("test2")
>>         p1<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("??")
>>  #Chinese title
>>         p2<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("??")
>>  #Chinese title
>>         p1<-p1+theme(text = element_text(family = "Kaiti TC"))
>>         p2<-p2+theme(text = element_text(family = "Kaiti TC"))
>>
>>         p<-array(list(NA), dim=2)
>>         p[[1]]<-p1
>>         p[[2]]<-p2
>>         p_series <- lapply(1:(length(p)), function(.x) p[.x][[1]])
>>         m2 <- marrangeGrob(p_series, nrow=2, ncol=1)
>>         ggsave("test_plot_chinese.pdf", m2)
>>
>>                  [[alternative HTML version deleted]]
>>
>>         ______________________________________________
>>         R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>>         -- To UNSUBSCRIBE and more, see
>>         https://stat.ethz.ch/mailman/listinfo/r-help
>>         <https://stat.ethz.ch/mailman/listinfo/r-help>
>>         PLEASE do read the posting guide
>>         http://www.R-project.org/posting-guide.html
>>         <http://www.R-project.org/posting-guide.html>
>>         and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>     --     Dr Paul Murrell
>>     Department of Statistics
>>     The University of Auckland
>>     Private Bag 92019
>>     Auckland
>>     New Zealand
>>     64 9 3737599 x85392
>>     paul at stat.auckland.ac.nz <mailto:paul at stat.auckland.ac.nz>
>>     http://www.stat.auckland.ac.nz/~paul/
>>     <http://www.stat.auckland.ac.nz/~paul/>
>>
>>
>>
> --
> Dr Paul Murrell
> Department of Statistics
> The University of Auckland
> Private Bag 92019
> Auckland
> New Zealand
> 64 9 3737599 x85392
> paul at stat.auckland.ac.nz
> http://www.stat.auckland.ac.nz/~paul/
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Mon Oct 16 07:29:54 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sun, 15 Oct 2017 22:29:54 -0700
Subject: [R] [FORGED] can't print ggplot with Chinese characters to pdf
	files
In-Reply-To: <CABcx46AzBfR9c_CyBXXr6OiPkK=YHBU3KHYZZ4N8y53s5VsZqQ@mail.gmail.com>
References: <CABcx46A4OL5+Ja-e0eAr=7iF2_obaCOZh4a7vHMLcqkZNpD3MQ@mail.gmail.com>
 <1daf3c68-a4ee-3755-911d-7f997528499e@stat.auckland.ac.nz>
 <CABcx46DFdnwUjdO4ciaD5aeoGSDR2UgUJwMvjWG6WkjBPJ=wrg@mail.gmail.com>
 <7f9f2636-c3d3-40b1-8ce3-9d4b2bde3336@stat.auckland.ac.nz>
 <CABcx46AzBfR9c_CyBXXr6OiPkK=YHBU3KHYZZ4N8y53s5VsZqQ@mail.gmail.com>
Message-ID: <CAGxFJbTeEWHVY1CwapWJkRh6MpeQiBk+G_Ozc9AW+L=A2HxGng@mail.gmail.com>

Wrong list. Post to r-sig-mac instead.

Bert



On Oct 15, 2017 6:47 PM, "John" <miaojpm at gmail.com> wrote:

> Hi,
>
>
>    Sorry to bother you with this question here.
>
>    I tried to install Macports on my Mac OS Sierra, and type "sudo port
> install cairo", but it did not respond. I haven't seen any file name or app
> called Macports, but one file called "port" which is located at
>
>
> loca/bin/port"
>
>
>    How should I do it?
>
>
>    Thanks,
>
>
> John
>
> *******
>
>
> Last login: Sun Oct 15 02:52:49 on console
>
> Johns-MacBook-Pro:~ john$ /opt/local/bin/port ; exit;
>
> MacPorts 2.4.2
>
> Entering shell mode... ("help" for help, "quit" to quit)
>
> [Users/john] > sudo port install cairo
>
> Error: Unrecognized action "port sudo"
>
>
>
>
> 2017-10-12 19:55 GMT-07:00 Paul Murrell <paul at stat.auckland.ac.nz>:
>
> > Hi
> >
> > By the looks of it, you need to install Cairo graphics ...
> >
> > https://www.cairographics.org/download/
> >
> > Paul
> >
> > On 13/10/17 15:48, John wrote:
> >
> >> Thanks, Paul. Following your solution,  I got this error message:
> >>
> >> Warning message:
> >> In cairo_pdf("test_plot_chinese.pdf") : failed to load cairo DLL
> >>
> >> Is there anything else I need to install?
> >>
> >> Thanks,
> >>
> >> John
> >>
> >> 2017-10-12 19:24 GMT-07:00 Paul Murrell <paul at stat.auckland.ac.nz
> >> <mailto:paul at stat.auckland.ac.nz>>:
> >>
> >>
> >>     Hi
> >>
> >>     Instead of ...
> >>
> >>     ggsave("test_plot_chinese.pdf", m2)
> >>
> >>     ... try ...
> >>
> >>     cairo_pdf("test_plot_chinese.pdf")
> >>     print(m2)
> >>     dev.off()
> >>
> >>     Paul
> >>
> >>
> >>     On 13/10/17 02:12, John wrote:
> >>
> >>         I install the Chinese font "Kaiti TC" on my mac, but I can't
> >>         print the
> >>         figures to pdf file by "marrangeGrob" command, which is in the
> >>         package
> >>         "gridExtra". Error message after I type "ggsave(......)" (last
> >>         line of the
> >>         program):
> >>
> >>         "Saving 7.47 x 5.15 in image
> >>         Error in grid.Call.graphics(L_text, as.graphicsAnnot(x$label),
> >>         x$x, x$y,  :
> >>             invalid font type
> >>         In addition: There were 50 or more warnings (use warnings() to
> >>         see the
> >>         first 50)"
> >>
> >>         How can I install the font so that I can see the figures on the
> >>         pdf files?
> >>         Some information that may be useful:
> >>         (1) If I use the English ggtitle, there is no problem at all.
> >>         The pdf file
> >>         present the figures perfectly.
> >>         (2) If I use Chinese title, I can still see the figures p1 and
> p2
> >> on
> >>         Studio, but I can't see it at the pdf file produced by ggsave.
> >>         It gives an
> >>         error message
> >>
> >>
> >>
> >>         rm(list=ls())
> >>         library(ggplot2)
> >>         library(gridExtra)
> >>         df1<-data.frame(x=1:2, y=3:4, z=5:6)
> >>         #p1<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("test1")
> >>         #p2<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("test2")
> >>         p1<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("??")
> >>  #Chinese title
> >>         p2<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("??")
> >>  #Chinese title
> >>         p1<-p1+theme(text = element_text(family = "Kaiti TC"))
> >>         p2<-p2+theme(text = element_text(family = "Kaiti TC"))
> >>
> >>         p<-array(list(NA), dim=2)
> >>         p[[1]]<-p1
> >>         p[[2]]<-p2
> >>         p_series <- lapply(1:(length(p)), function(.x) p[.x][[1]])
> >>         m2 <- marrangeGrob(p_series, nrow=2, ncol=1)
> >>         ggsave("test_plot_chinese.pdf", m2)
> >>
> >>                  [[alternative HTML version deleted]]
> >>
> >>         ______________________________________________
> >>         R-help at r-project.org <mailto:R-help at r-project.org> mailing list
> >>         -- To UNSUBSCRIBE and more, see
> >>         https://stat.ethz.ch/mailman/listinfo/r-help
> >>         <https://stat.ethz.ch/mailman/listinfo/r-help>
> >>         PLEASE do read the posting guide
> >>         http://www.R-project.org/posting-guide.html
> >>         <http://www.R-project.org/posting-guide.html>
> >>         and provide commented, minimal, self-contained, reproducible
> code.
> >>
> >>
> >>     --     Dr Paul Murrell
> >>     Department of Statistics
> >>     The University of Auckland
> >>     Private Bag 92019
> >>     Auckland
> >>     New Zealand
> >>     64 9 3737599 x85392
> >>     paul at stat.auckland.ac.nz <mailto:paul at stat.auckland.ac.nz>
> >>     http://www.stat.auckland.ac.nz/~paul/
> >>     <http://www.stat.auckland.ac.nz/~paul/>
> >>
> >>
> >>
> > --
> > Dr Paul Murrell
> > Department of Statistics
> > The University of Auckland
> > Private Bag 92019
> > Auckland
> > New Zealand
> > 64 9 3737599 x85392
> > paul at stat.auckland.ac.nz
> > http://www.stat.auckland.ac.nz/~paul/
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From chalabi.elahe at yahoo.de  Mon Oct 16 14:25:34 2017
From: chalabi.elahe at yahoo.de (Elahe chalabi)
Date: Mon, 16 Oct 2017 12:25:34 +0000 (UTC)
Subject: [R] ROC curve for each fold in one plot
References: <1534918912.1359998.1508156734033.ref@mail.yahoo.com>
Message-ID: <1534918912.1359998.1508156734033@mail.yahoo.com>

Hi all,

I have tried a 5 fold cross validation using caret package with random forest method on iris dataset as example. Then I need ROC curve for each fold:


  > set.seed(1)
  > train_control <- trainControl(method="cv", number=5,savePredictions = TRUE,classProbs = TRUE) 
  > output <- train(Species~., data=iris, trControl=train_control, method="rf")
  > library(pROC)  
  > selectedIndices <- output$pred$Resample == "Fold1"
  > plot.roc(output$pred$obs[selectedIndices],output$pred$setosa[selectedIndices])  > selectedIndices <- output$pred$Resample == "Fold2"  
  > plot.roc(output$pred$obs[selectedIndices],output$pred$setosa[selectedIndices])
  > selectedIndices <- output$pred$Resample == "Fold3"
  > plot.roc(output$pred$obs[selectedIndices],output$pred$setosa[selectedIndices])

and the same for Fold4 and Fold5,now how can I bring all the plots in one plot with labels for each fold?

Thanks for any help!
Elahe


From meghna.govil at gmail.com  Mon Oct 16 17:28:53 2017
From: meghna.govil at gmail.com (Meghna Govil)
Date: Mon, 16 Oct 2017 10:28:53 -0500
Subject: [R] survival analysis - predict function
Message-ID: <CAHc67oTagcWxX=cUNOHrf_jCRruFh-1CKURKSofD80dE76JELQ@mail.gmail.com>

Hi

I'm trying to predict the values for a survreg object called
loglogistic_na. Here is the definition of loglogistic_na and following that
the syntax used for the predict function. But upon execution I don't get
any output. Not sure what I'm doing wrong:

loglogistic_na <- survreg(Surv(time_na,event_na) ~ t_na, dist="loglogistic")
summary(loglogistic_na)
extractAIC(weibull_na)
extractAIC(loglogistic_na)

# first way
l.pred <- predict(loglogistic_na, train1_model)
l.pred.q <- predict(loglogistic_na, train1_model,
type="quantile"p=pct,se=TRUE)
result <- cbind(data.frame(train1_model, l.pred), l.pred.q)
names(result) <- c("Actual", "Predicted", "Lower", "Upper")
head(result)


Thanks

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Mon Oct 16 19:13:48 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 16 Oct 2017 10:13:48 -0700
Subject: [R] Download data from NASA for multiple locations - RCurl
In-Reply-To: <CAMLwc7OxFgqtTayGn3jq_bcCY3XmaFax7XpdWOYuQgjYJGQZPA@mail.gmail.com>
References: <CAMLwc7MLcExXDYGyetJfmFEFJMyo=1-1EiRUKmgd72xjPN_kCg@mail.gmail.com>
 <74C4AC4B-9AB3-47A5-A54C-C36C5E1338D0@comcast.net>
 <CAMLwc7OxFgqtTayGn3jq_bcCY3XmaFax7XpdWOYuQgjYJGQZPA@mail.gmail.com>
Message-ID: <2431DE9A-BE0B-4D8A-B3FF-9465D95CA742@comcast.net>


> On Oct 15, 2017, at 3:35 PM, Miluji Sb <milujisb at gmail.com> wrote:
> 
> Dear David,
> 
> This is amazing, thank you so much. If I may ask another question:
> 
> The output looks like the following:
> 
> ###
> dput(head(x,15))
> c("Metadata for Requested Time Series:", "", "prod_name=GLDAS_NOAH025_3H_v2.0", 
> "param_short_name=Tair_f_inst", "param_name=Near surface air temperature", 
> "unit=K", "begin_time=1970-01-01T00", "end_time=1979-12-31T21", 
> "lat= 42.36", "lon=-71.06", "Request_time=2017-10-15 22:20:03 GMT", 
> "", "Date&Time               Data", "1970-01-01T00:00:00\t267.769", 
> "1970-01-01T03:00:00\t264.595")
> ###
> 
> Thus I need to drop the first 13 rows and do the following to add identifying information:

Are you having difficulty reading in the data from disk? The `read.table` function has a "skip" parameter.
> 
> ###
> mydata <- data.frame(year = substr(x,1,4),

That would not appear to do anything useful with x. The `x` object is not a long string. The items you want are in separate elements of x. 

substr(x,1,4)   # now returns
 [1] "Meta" ""     "prod" "para" "para" "unit" "begi" "end_" "lat=" "lon=" "Requ" ""     "Date"
[14] "1970" "1970"

You need to learn basic R indexing. The year might be extracted from the 7th element of x x via code like this:

    year <- substr( x[7], 1,4)

>                      month = substr(x, 6,7),
>                      day = substr(x, 9, 10),
>                      hour = substr(x, 12, 13),
>                      temp = substr(x, 21, 27))

The time and temp items would naturally be read in with read.table (or in the case of tab-delimited data with read.delim) after skipping the first 14 lines.


> 
> mydata$city <- rep(cities[1,1], nrow(mydata))

There's no need to use `rep` with data.frame. If one argument to data.frame is length n then all single elelment arguments will be "recycled" to fill in the needed number of rows. Please take the time to work through all the pages of "Introduction to R" (shipped with all distributions of R) or pick another introductory text. We cannot provide tutoring to all students. You need to put in the needed self-study first.

-- 
David.


> mydata$state <- rep(cities[1,2], nrow(mydata))
> mydata$lon <- rep(cities[1,3], nrow(mydata))
> mydata$lat <- rep(cities[1,4], nrow(mydata))
> ###
> 
> Is it possible to incorporate these into your code so the data looks like this:
> 
> dput(droplevels(head(mydata)))
> structure(list(year = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = "1970", class = "factor"), 
>     month = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = "01", class = "factor"), 
>     day = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = "01", class = "factor"), 
>     hour = structure(1:6, .Label = c("00", "03", "06", "09", 
>     "12", "15"), class = "factor"), temp = structure(c(6L, 4L, 
>     2L, 1L, 3L, 5L), .Label = c("261.559", "262.525", "262.648", 
>     "264.595", "265.812", "267.769"), class = "factor"), city = structure(c(1L, 
>     1L, 1L, 1L, 1L, 1L), .Label = "Boston", class = "factor"), 
>     state = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = " MA ", class = "factor"), 
>     lon = c(-71.06, -71.06, -71.06, -71.06, -71.06, -71.06), 
>     lat = c(42.36, 42.36, 42.36, 42.36, 42.36, 42.36)), .Names = c("year", 
> "month", "day", "hour", "temp", "city", "state", "lon", "lat"
> ), row.names = c(NA, 6L), class = "data.frame")
> 
> Apologies for asking repeated questions and thank you again!

Of course it's possible. I don't understand where the difficulty lies. 
> 
> Sincerely,
> 
> Milu
> 
> On Sun, Oct 15, 2017 at 11:45 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> > On Oct 15, 2017, at 2:02 PM, Miluji Sb <milujisb at gmail.com> wrote:
> >
> > Dear all,
> >
> > i am trying to download time-series climatic data from GES DISC (NASA)
> > Hydrology Data Rods web-service. Unfortunately, no wget method is
> > available.
> >
> > Five parameters are needed for data retrieval: variable, location,
> > startDate, endDate, and type. For example:
> >
> > ###
> > https://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/access/timeseries.cgi?variable=GLDAS2:GLDAS_NOAH025_3H_v2.0:Tair_f_inst&startDate=1970-01-01T00&endDate=1979-12-31T00&location=GEOM:POINT(-71.06,%2042.36)&type=asc2
> > ###
> >
> > In this case, variable: Tair_f_inst (temperature), location: (-71.06,
> > 42.36), startDate: 01 January 1970; endDate: 31 December 1979; type:  asc2
> > (output 2-column ASCII).
> >
> > I am trying to download data for 100 US cities, data for which I have in
> > the following data.frame:
> >
> > ###
> > cities <-  dput(droplevels(head(cities, 5)))
> > structure(list(city = structure(1:5, .Label = c("Boston", "Bridgeport",
> > "Cambridge", "Fall River", "Hartford"), class = "factor"), state =
> > structure(c(2L,
> > 1L, 2L, 2L, 1L), .Label = c(" CT ", " MA "), class = "factor"),
> >    lon = c(-71.06, -73.19, -71.11, -71.16, -72.67), lat = c(42.36,
> >    41.18, 42.37, 41.7, 41.77)), .Names = c("city", "state",
> > "lon", "lat"), row.names = c(NA, 5L), class = "data.frame")
> > ###
> >
> > Is it possible to download the data for the multiple locations
> > automatically (e.g. RCurl) and save them as csv? Essentially, reading
> > coordinates from the data.frame and entering it in the URL.
> >
> > I would also like to add identifying information to each of the data files
> > from the cities data.frame. I have been doing the following for a single
> > file:
> 
> Didn't seem that difficult:
> 
> library(downloader)  # makes things easier for Macs, perhaps not needed
> # if not used will need to use download.file
> 
> for( i in 1:5) {
>   target1 <- paste0("https://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/access/timeseries.cgi?variable=GLDAS2:GLDAS_NOAH025_3H_v2.0:Tair_f_inst&startDate=1970-01-01T00&endDate=1979-12-31T00&location=GEOM:POINT(",
>                      cities[i, "lon"],
>                      ",%20", cities[i,"lat"],
>                      ")&type=asc2")
>   target2 <- paste0("~/",    # change for whatever destination directory you may prefer.
>                     cities[i,"city"],
>                     cities[i,"state"], ".asc")
>   download(url=target1, destfile=target2)
>                 }
> 
> Now I have 5 named files with extensions ".asc" in my user directory (since I'm on a Mac). It is a slow website so patience is needed.
> 
> --
> David
> 
> 
> >
> > ###
> > x <- readLines(con=url("
> > https://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/access/timeseries.cgi?variable=GLDAS2:GLDAS_NOAH025_3H_v2.0:Tair_f_inst&startDate=1970-01-01T00&endDate=1979-12-31T00&location=GEOM:POINT(-71.06,%2042.36)&type=asc2
> > "))
> > x <- x[-(1:13)]
> >
> > mydata <- data.frame(year = substr(x,1,4),
> >                     month = substr(x, 6,7),
> >                     day = substr(x, 9, 10),
> >                     hour = substr(x, 12, 13),
> >                     temp = substr(x, 21, 27))
> >
> > mydata$city <- rep(cities[1,1], nrow(mydata))
> > mydata$state <- rep(cities[1,2], nrow(mydata))
> > mydata$lon <- rep(cities[1,3], nrow(mydata))
> > mydata$lat <- rep(cities[1,4], nrow(mydata))
> > ###
> >
> > Help and advice would be greatly appreciated. Thank you!
> >
> > Sincerely,
> >
> > Milu
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
> 
> 
> 
> 
> 
> 

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From megov31 at yahoo.com  Mon Oct 16 18:28:17 2017
From: megov31 at yahoo.com (Meghna Govil)
Date: Mon, 16 Oct 2017 11:28:17 -0500
Subject: [R] survival analysis - predict function
References: <CAHc67oTt4ZhA4BBRvfJu9mCZ+QmhC+emEqJdRJxWVykAwHiXEQ@mail.gmail.com>
Message-ID: <8633AFB6-59D6-4DFB-98D3-E7BDC46EAB99@yahoo.com>


> Hi
> 
> I'm trying to predict the values for a survreg object called loglogistic_na. Here is the definition of loglogistic_na and following that the syntax used for the predict function. But upon execution I don't get any output. Not sure what I'm doing wrong:
> 
> loglogistic_na <- survreg(Surv(time_na,event_na) ~ t_na, dist="loglogistic")
> summary(loglogistic_na)
> extractAIC(weibull_na)
> extractAIC(loglogistic_na)
> 
> # first way
> l.pred <- predict(loglogistic_na, train1_model)
> l.pred.q <- predict(loglogistic_na, train1_model, type="quantile"p=pct,se=TRUE)
> result <- cbind(data.frame(train1_model, l.pred), l.pred.q)
> names(result) <- c("Actual", "Predicted", "Lower", "Upper")
> head(result)
> 
> 
> Thanks
> 
> 


From milujisb at gmail.com  Mon Oct 16 22:43:16 2017
From: milujisb at gmail.com (Miluji Sb)
Date: Mon, 16 Oct 2017 22:43:16 +0200
Subject: [R] Download data from NASA for multiple locations - RCurl
In-Reply-To: <2431DE9A-BE0B-4D8A-B3FF-9465D95CA742@comcast.net>
References: <CAMLwc7MLcExXDYGyetJfmFEFJMyo=1-1EiRUKmgd72xjPN_kCg@mail.gmail.com>
 <74C4AC4B-9AB3-47A5-A54C-C36C5E1338D0@comcast.net>
 <CAMLwc7OxFgqtTayGn3jq_bcCY3XmaFax7XpdWOYuQgjYJGQZPA@mail.gmail.com>
 <2431DE9A-BE0B-4D8A-B3FF-9465D95CA742@comcast.net>
Message-ID: <CAMLwc7NHFQnBmkgChvi+=yEE+fRWVtAgzhuOvWv9CQG4ni_2iA@mail.gmail.com>

I have done the following using readLines

directory <- "~/"
files <- list.files(directory)
data_frames <- vector("list", length(files))
for (i in seq_along(files)) {
  df <- readLines(file.path(directory, files[i]))
  df <- df[-(1:13)]
  df <- data.frame(year = substr(df,1,4),
                       month = substr(df, 6,7),
                       day = substr(df, 9, 10),
                       hour = substr(df, 12, 13),
                       temp = substr(df, 21, 27))
  data_frames[[i]] <- df
}

What I have been have been having trouble is adding the following
information from the cities file (100 cities) for each of the downloaded
data files. I would like to do the following but automatically:

###
mydata$city <- rep(cities[1,1], nrow(mydata))
mydata$state <- rep(cities[1,2], nrow(mydata))
mydata$lon <- rep(cities[1,3], nrow(mydata))
mydata$lat <- rep(cities[1,4], nrow(mydata))
###

The information for cities look like this:

###
cities <-  dput(droplevels(head(cities, 5)))
structure(list(city = structure(1:5, .Label = c("Boston", "Bridgeport",
"Cambridge", "Fall River", "Hartford"), class = "factor"), state =
structure(c(2L,
1L, 2L, 2L, 1L), .Label = c(" CT ", " MA "), class = "factor"),
    lon = c(-71.06, -73.19, -71.11, -71.16, -72.67), lat = c(42.36,
    41.18, 42.37, 41.7, 41.77)), .Names = c("city", "state",
"lon", "lat"), row.names = c(NA, 5L), class = "data.frame")
###

Apologies if this seems trivial but I have been having a hard time. Thank
you again.

Sincerely,

Milu

On Mon, Oct 16, 2017 at 7:13 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Oct 15, 2017, at 3:35 PM, Miluji Sb <milujisb at gmail.com> wrote:
> >
> > Dear David,
> >
> > This is amazing, thank you so much. If I may ask another question:
> >
> > The output looks like the following:
> >
> > ###
> > dput(head(x,15))
> > c("Metadata for Requested Time Series:", "", "prod_name=GLDAS_NOAH025_3H_
> v2.0",
> > "param_short_name=Tair_f_inst", "param_name=Near surface air
> temperature",
> > "unit=K", "begin_time=1970-01-01T00", "end_time=1979-12-31T21",
> > "lat= 42.36", "lon=-71.06", "Request_time=2017-10-15 22:20:03 GMT",
> > "", "Date&Time               Data", "1970-01-01T00:00:00\t267.769",
> > "1970-01-01T03:00:00\t264.595")
> > ###
> >
> > Thus I need to drop the first 13 rows and do the following to add
> identifying information:
>
> Are you having difficulty reading in the data from disk? The `read.table`
> function has a "skip" parameter.
> >
> > ###
> > mydata <- data.frame(year = substr(x,1,4),
>
> That would not appear to do anything useful with x. The `x` object is not
> a long string. The items you want are in separate elements of x.
>
> substr(x,1,4)   # now returns
>  [1] "Meta" ""     "prod" "para" "para" "unit" "begi" "end_" "lat=" "lon="
> "Requ" ""     "Date"
> [14] "1970" "1970"
>
> You need to learn basic R indexing. The year might be extracted from the
> 7th element of x x via code like this:
>
>     year <- substr( x[7], 1,4)
>
> >                      month = substr(x, 6,7),
> >                      day = substr(x, 9, 10),
> >                      hour = substr(x, 12, 13),
> >                      temp = substr(x, 21, 27))
>
> The time and temp items would naturally be read in with read.table (or in
> the case of tab-delimited data with read.delim) after skipping the first 14
> lines.
>
>
> >
> > mydata$city <- rep(cities[1,1], nrow(mydata))
>
> There's no need to use `rep` with data.frame. If one argument to
> data.frame is length n then all single elelment arguments will be
> "recycled" to fill in the needed number of rows. Please take the time to
> work through all the pages of "Introduction to R" (shipped with all
> distributions of R) or pick another introductory text. We cannot provide
> tutoring to all students. You need to put in the needed self-study first.
>
> --
> David.
>
>
> > mydata$state <- rep(cities[1,2], nrow(mydata))
> > mydata$lon <- rep(cities[1,3], nrow(mydata))
> > mydata$lat <- rep(cities[1,4], nrow(mydata))
> > ###
> >
> > Is it possible to incorporate these into your code so the data looks
> like this:
> >
> > dput(droplevels(head(mydata)))
> > structure(list(year = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label =
> "1970", class = "factor"),
> >     month = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = "01", class =
> "factor"),
> >     day = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = "01", class =
> "factor"),
> >     hour = structure(1:6, .Label = c("00", "03", "06", "09",
> >     "12", "15"), class = "factor"), temp = structure(c(6L, 4L,
> >     2L, 1L, 3L, 5L), .Label = c("261.559", "262.525", "262.648",
> >     "264.595", "265.812", "267.769"), class = "factor"), city =
> structure(c(1L,
> >     1L, 1L, 1L, 1L, 1L), .Label = "Boston", class = "factor"),
> >     state = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = " MA ", class
> = "factor"),
> >     lon = c(-71.06, -71.06, -71.06, -71.06, -71.06, -71.06),
> >     lat = c(42.36, 42.36, 42.36, 42.36, 42.36, 42.36)), .Names =
> c("year",
> > "month", "day", "hour", "temp", "city", "state", "lon", "lat"
> > ), row.names = c(NA, 6L), class = "data.frame")
> >
> > Apologies for asking repeated questions and thank you again!
>
> Of course it's possible. I don't understand where the difficulty lies.
> >
> > Sincerely,
> >
> > Milu
> >
> > On Sun, Oct 15, 2017 at 11:45 PM, David Winsemius <
> dwinsemius at comcast.net> wrote:
> >
> > > On Oct 15, 2017, at 2:02 PM, Miluji Sb <milujisb at gmail.com> wrote:
> > >
> > > Dear all,
> > >
> > > i am trying to download time-series climatic data from GES DISC (NASA)
> > > Hydrology Data Rods web-service. Unfortunately, no wget method is
> > > available.
> > >
> > > Five parameters are needed for data retrieval: variable, location,
> > > startDate, endDate, and type. For example:
> > >
> > > ###
> > > https://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/access/
> timeseries.cgi?variable=GLDAS2:GLDAS_NOAH025_3H_v2.0:
> Tair_f_inst&startDate=1970-01-01T00&endDate=1979-12-31T00&
> location=GEOM:POINT(-71.06,%2042.36)&type=asc2
> > > ###
> > >
> > > In this case, variable: Tair_f_inst (temperature), location: (-71.06,
> > > 42.36), startDate: 01 January 1970; endDate: 31 December 1979; type:
> asc2
> > > (output 2-column ASCII).
> > >
> > > I am trying to download data for 100 US cities, data for which I have
> in
> > > the following data.frame:
> > >
> > > ###
> > > cities <-  dput(droplevels(head(cities, 5)))
> > > structure(list(city = structure(1:5, .Label = c("Boston", "Bridgeport",
> > > "Cambridge", "Fall River", "Hartford"), class = "factor"), state =
> > > structure(c(2L,
> > > 1L, 2L, 2L, 1L), .Label = c(" CT ", " MA "), class = "factor"),
> > >    lon = c(-71.06, -73.19, -71.11, -71.16, -72.67), lat = c(42.36,
> > >    41.18, 42.37, 41.7, 41.77)), .Names = c("city", "state",
> > > "lon", "lat"), row.names = c(NA, 5L), class = "data.frame")
> > > ###
> > >
> > > Is it possible to download the data for the multiple locations
> > > automatically (e.g. RCurl) and save them as csv? Essentially, reading
> > > coordinates from the data.frame and entering it in the URL.
> > >
> > > I would also like to add identifying information to each of the data
> files
> > > from the cities data.frame. I have been doing the following for a
> single
> > > file:
> >
> > Didn't seem that difficult:
> >
> > library(downloader)  # makes things easier for Macs, perhaps not needed
> > # if not used will need to use download.file
> >
> > for( i in 1:5) {
> >   target1 <- paste0("https://hydro1.gesdisc.eosdis.nasa.gov/daac-
> bin/access/timeseries.cgi?variable=GLDAS2:GLDAS_NOAH025_
> 3H_v2.0:Tair_f_inst&startDate=1970-01-01T00&endDate=1979-12-
> 31T00&location=GEOM:POINT(",
> >                      cities[i, "lon"],
> >                      ",%20", cities[i,"lat"],
> >                      ")&type=asc2")
> >   target2 <- paste0("~/",    # change for whatever destination directory
> you may prefer.
> >                     cities[i,"city"],
> >                     cities[i,"state"], ".asc")
> >   download(url=target1, destfile=target2)
> >                 }
> >
> > Now I have 5 named files with extensions ".asc" in my user directory
> (since I'm on a Mac). It is a slow website so patience is needed.
> >
> > --
> > David
> >
> >
> > >
> > > ###
> > > x <- readLines(con=url("
> > > https://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/access/
> timeseries.cgi?variable=GLDAS2:GLDAS_NOAH025_3H_v2.0:
> Tair_f_inst&startDate=1970-01-01T00&endDate=1979-12-31T00&
> location=GEOM:POINT(-71.06,%2042.36)&type=asc2
> > > "))
> > > x <- x[-(1:13)]
> > >
> > > mydata <- data.frame(year = substr(x,1,4),
> > >                     month = substr(x, 6,7),
> > >                     day = substr(x, 9, 10),
> > >                     hour = substr(x, 12, 13),
> > >                     temp = substr(x, 21, 27))
> > >
> > > mydata$city <- rep(cities[1,1], nrow(mydata))
> > > mydata$state <- rep(cities[1,2], nrow(mydata))
> > > mydata$lon <- rep(cities[1,3], nrow(mydata))
> > > mydata$lat <- rep(cities[1,4], nrow(mydata))
> > > ###
> > >
> > > Help and advice would be greatly appreciated. Thank you!
> > >
> > > Sincerely,
> > >
> > > Milu
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> > 'Any technology distinguishable from magic is insufficiently advanced.'
>  -Gehm's Corollary to Clarke's Third Law
> >
> >
> >
> >
> >
> >
>
> David Winsemius
> Alameda, CA, USA
>
> 'Any technology distinguishable from magic is insufficiently advanced.'
>  -Gehm's Corollary to Clarke's Third Law
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Mon Oct 16 23:13:31 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 16 Oct 2017 14:13:31 -0700
Subject: [R] survival analysis - predict function
In-Reply-To: <CAHc67oTagcWxX=cUNOHrf_jCRruFh-1CKURKSofD80dE76JELQ@mail.gmail.com>
References: <CAHc67oTagcWxX=cUNOHrf_jCRruFh-1CKURKSofD80dE76JELQ@mail.gmail.com>
Message-ID: <1A0B7EED-8A61-46AF-8204-FB3072E5C065@comcast.net>


> On Oct 16, 2017, at 8:28 AM, Meghna Govil <meghna.govil at gmail.com> wrote:
> 
> Hi
> 
> I'm trying to predict the values for a survreg object called
> loglogistic_na. Here is the definition of loglogistic_na and following that
> the syntax used for the predict function. But upon execution I don't get
> any output. Not sure what I'm doing wrong:
> 
> loglogistic_na <- survreg(Surv(time_na,event_na) ~ t_na, dist="loglogistic")
> summary(loglogistic_na)
> extractAIC(weibull_na)
> extractAIC(loglogistic_na)
> 
> # first way
> l.pred <- predict(loglogistic_na, train1_model)
> l.pred.q <- predict(loglogistic_na, train1_model,
> type="quantile"p=pct,se=TRUE)
> result <- cbind(data.frame(train1_model, l.pred), l.pred.q)
> names(result) <- c("Actual", "Predicted", "Lower", "Upper")
> head(result)
> 
> 
> Thanks
> 
> 	[[alternative HTML version deleted]]

I'm wondering if your failure to provide a data argument to survreg leads to a situation where attempting to use predictions on a newdata argument fails.

-- 
David
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From dwinsemius at comcast.net  Mon Oct 16 23:30:23 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 16 Oct 2017 14:30:23 -0700
Subject: [R] Download data from NASA for multiple locations - RCurl
In-Reply-To: <CAMLwc7NHFQnBmkgChvi+=yEE+fRWVtAgzhuOvWv9CQG4ni_2iA@mail.gmail.com>
References: <CAMLwc7MLcExXDYGyetJfmFEFJMyo=1-1EiRUKmgd72xjPN_kCg@mail.gmail.com>
 <74C4AC4B-9AB3-47A5-A54C-C36C5E1338D0@comcast.net>
 <CAMLwc7OxFgqtTayGn3jq_bcCY3XmaFax7XpdWOYuQgjYJGQZPA@mail.gmail.com>
 <2431DE9A-BE0B-4D8A-B3FF-9465D95CA742@comcast.net>
 <CAMLwc7NHFQnBmkgChvi+=yEE+fRWVtAgzhuOvWv9CQG4ni_2iA@mail.gmail.com>
Message-ID: <D700FB59-2B02-4775-8DF5-46E8D70BF607@comcast.net>


> On Oct 16, 2017, at 1:43 PM, Miluji Sb <milujisb at gmail.com> wrote:
> 
> I have done the following using readLines
> 
> directory <- "~/"
> files <- list.files(directory)
> data_frames <- vector("list", length(files))
> for (i in seq_along(files)) {
>   df <- readLines(file.path(directory, files[i]))
>   df <- df[-(1:13)]
>   df <- data.frame(year = substr(df,1,4),
>                        month = substr(df, 6,7),
>                        day = substr(df, 9, 10),
>                        hour = substr(df, 12, 13),
>                        temp = substr(df, 21, 27))




>   data_frames[[i]] <- df
> }
> 
> What I have been have been having trouble is adding the following information from the cities file (100 cities) for each of the downloaded data files. I would like to do the following but automatically:
> 
> ###
> mydata$city <- rep(cities[1,1], nrow(mydata))  
> mydata$state <- rep(cities[1,2], nrow(mydata))
> mydata$lon <- rep(cities[1,3], nrow(mydata))
> mydata$lat <- rep(cities[1,4], nrow(mydata))
> ###
> 

Why not store  the lat/lon data in the file name and then extract all 4 items from the file name within the loop?

-- 
David.


> The information for cities look like this:
> 
> ###
> cities <-  dput(droplevels(head(cities, 5)))
> structure(list(city = structure(1:5, .Label = c("Boston", "Bridgeport", 
> "Cambridge", "Fall River", "Hartford"), class = "factor"), state = structure(c(2L, 
> 1L, 2L, 2L, 1L), .Label = c(" CT ", " MA "), class = "factor"), 
>     lon = c(-71.06, -73.19, -71.11, -71.16, -72.67), lat = c(42.36, 
>     41.18, 42.37, 41.7, 41.77)), .Names = c("city", "state", 
> "lon", "lat"), row.names = c(NA, 5L), class = "data.frame")
> ###
> 
> Apologies if this seems trivial but I have been having a hard time. Thank you again.
> 
> Sincerely,
> 
> Milu
> 
> On Mon, Oct 16, 2017 at 7:13 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> > On Oct 15, 2017, at 3:35 PM, Miluji Sb <milujisb at gmail.com> wrote:
> >
> > Dear David,
> >
> > This is amazing, thank you so much. If I may ask another question:
> >
> > The output looks like the following:
> >
> > ###
> > dput(head(x,15))
> > c("Metadata for Requested Time Series:", "", "prod_name=GLDAS_NOAH025_3H_v2.0",
> > "param_short_name=Tair_f_inst", "param_name=Near surface air temperature",
> > "unit=K", "begin_time=1970-01-01T00", "end_time=1979-12-31T21",
> > "lat= 42.36", "lon=-71.06", "Request_time=2017-10-15 22:20:03 GMT",
> > "", "Date&Time               Data", "1970-01-01T00:00:00\t267.769",
> > "1970-01-01T03:00:00\t264.595")
> > ###
> >
> > Thus I need to drop the first 13 rows and do the following to add identifying information:
> 
> Are you having difficulty reading in the data from disk? The `read.table` function has a "skip" parameter.
> >
> > ###
> > mydata <- data.frame(year = substr(x,1,4),
> 
> That would not appear to do anything useful with x. The `x` object is not a long string. The items you want are in separate elements of x.
> 
> substr(x,1,4)   # now returns
>  [1] "Meta" ""     "prod" "para" "para" "unit" "begi" "end_" "lat=" "lon=" "Requ" ""     "Date"
> [14] "1970" "1970"
> 
> You need to learn basic R indexing. The year might be extracted from the 7th element of x x via code like this:
> 
>     year <- substr( x[7], 1,4)
> 
> >                      month = substr(x, 6,7),
> >                      day = substr(x, 9, 10),
> >                      hour = substr(x, 12, 13),
> >                      temp = substr(x, 21, 27))
> 
> The time and temp items would naturally be read in with read.table (or in the case of tab-delimited data with read.delim) after skipping the first 14 lines.
> 
> 
> >
> > mydata$city <- rep(cities[1,1], nrow(mydata))
> 
> There's no need to use `rep` with data.frame. If one argument to data.frame is length n then all single elelment arguments will be "recycled" to fill in the needed number of rows. Please take the time to work through all the pages of "Introduction to R" (shipped with all distributions of R) or pick another introductory text. We cannot provide tutoring to all students. You need to put in the needed self-study first.
> 
> --
> David.
> 
> 
> > mydata$state <- rep(cities[1,2], nrow(mydata))
> > mydata$lon <- rep(cities[1,3], nrow(mydata))
> > mydata$lat <- rep(cities[1,4], nrow(mydata))
> > ###
> >
> > Is it possible to incorporate these into your code so the data looks like this:
> >
> > dput(droplevels(head(mydata)))
> > structure(list(year = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = "1970", class = "factor"),
> >     month = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = "01", class = "factor"),
> >     day = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = "01", class = "factor"),
> >     hour = structure(1:6, .Label = c("00", "03", "06", "09",
> >     "12", "15"), class = "factor"), temp = structure(c(6L, 4L,
> >     2L, 1L, 3L, 5L), .Label = c("261.559", "262.525", "262.648",
> >     "264.595", "265.812", "267.769"), class = "factor"), city = structure(c(1L,
> >     1L, 1L, 1L, 1L, 1L), .Label = "Boston", class = "factor"),
> >     state = structure(c(1L, 1L, 1L, 1L, 1L, 1L), .Label = " MA ", class = "factor"),
> >     lon = c(-71.06, -71.06, -71.06, -71.06, -71.06, -71.06),
> >     lat = c(42.36, 42.36, 42.36, 42.36, 42.36, 42.36)), .Names = c("year",
> > "month", "day", "hour", "temp", "city", "state", "lon", "lat"
> > ), row.names = c(NA, 6L), class = "data.frame")
> >
> > Apologies for asking repeated questions and thank you again!
> 
> Of course it's possible. I don't understand where the difficulty lies.
> >
> > Sincerely,
> >
> > Milu
> >
> > On Sun, Oct 15, 2017 at 11:45 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> >
> > > On Oct 15, 2017, at 2:02 PM, Miluji Sb <milujisb at gmail.com> wrote:
> > >
> > > Dear all,
> > >
> > > i am trying to download time-series climatic data from GES DISC (NASA)
> > > Hydrology Data Rods web-service. Unfortunately, no wget method is
> > > available.
> > >
> > > Five parameters are needed for data retrieval: variable, location,
> > > startDate, endDate, and type. For example:
> > >
> > > ###
> > > https://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/access/timeseries.cgi?variable=GLDAS2:GLDAS_NOAH025_3H_v2.0:Tair_f_inst&startDate=1970-01-01T00&endDate=1979-12-31T00&location=GEOM:POINT(-71.06,%2042.36)&type=asc2
> > > ###
> > >
> > > In this case, variable: Tair_f_inst (temperature), location: (-71.06,
> > > 42.36), startDate: 01 January 1970; endDate: 31 December 1979; type:  asc2
> > > (output 2-column ASCII).
> > >
> > > I am trying to download data for 100 US cities, data for which I have in
> > > the following data.frame:
> > >
> > > ###
> > > cities <-  dput(droplevels(head(cities, 5)))
> > > structure(list(city = structure(1:5, .Label = c("Boston", "Bridgeport",
> > > "Cambridge", "Fall River", "Hartford"), class = "factor"), state =
> > > structure(c(2L,
> > > 1L, 2L, 2L, 1L), .Label = c(" CT ", " MA "), class = "factor"),
> > >    lon = c(-71.06, -73.19, -71.11, -71.16, -72.67), lat = c(42.36,
> > >    41.18, 42.37, 41.7, 41.77)), .Names = c("city", "state",
> > > "lon", "lat"), row.names = c(NA, 5L), class = "data.frame")
> > > ###
> > >
> > > Is it possible to download the data for the multiple locations
> > > automatically (e.g. RCurl) and save them as csv? Essentially, reading
> > > coordinates from the data.frame and entering it in the URL.
> > >
> > > I would also like to add identifying information to each of the data files
> > > from the cities data.frame. I have been doing the following for a single
> > > file:
> >
> > Didn't seem that difficult:
> >
> > library(downloader)  # makes things easier for Macs, perhaps not needed
> > # if not used will need to use download.file
> >
> > for( i in 1:5) {
> >   target1 <- paste0("https://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/access/timeseries.cgi?variable=GLDAS2:GLDAS_NOAH025_3H_v2.0:Tair_f_inst&startDate=1970-01-01T00&endDate=1979-12-31T00&location=GEOM:POINT(",
> >                      cities[i, "lon"],
> >                      ",%20", cities[i,"lat"],
> >                      ")&type=asc2")
> >   target2 <- paste0("~/",    # change for whatever destination directory you may prefer.
> >                     cities[i,"city"],
> >                     cities[i,"state"], ".asc")
> >   download(url=target1, destfile=target2)
> >                 }
> >
> > Now I have 5 named files with extensions ".asc" in my user directory (since I'm on a Mac). It is a slow website so patience is needed.
> >
> > --
> > David
> >
> >
> > >
> > > ###
> > > x <- readLines(con=url("
> > > https://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/access/timeseries.cgi?variable=GLDAS2:GLDAS_NOAH025_3H_v2.0:Tair_f_inst&startDate=1970-01-01T00&endDate=1979-12-31T00&location=GEOM:POINT(-71.06,%2042.36)&type=asc2
> > > "))
> > > x <- x[-(1:13)]
> > >
> > > mydata <- data.frame(year = substr(x,1,4),
> > >                     month = substr(x, 6,7),
> > >                     day = substr(x, 9, 10),
> > >                     hour = substr(x, 12, 13),
> > >                     temp = substr(x, 21, 27))
> > >
> > > mydata$city <- rep(cities[1,1], nrow(mydata))
> > > mydata$state <- rep(cities[1,2], nrow(mydata))
> > > mydata$lon <- rep(cities[1,3], nrow(mydata))
> > > mydata$lat <- rep(cities[1,4], nrow(mydata))
> > > ###
> > >
> > > Help and advice would be greatly appreciated. Thank you!
> > >
> > > Sincerely,
> > >
> > > Milu
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> > 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
> >
> >
> >
> >
> >
> >
> 
> David Winsemius
> Alameda, CA, USA
> 
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
> 
> 
> 
> 
> 
> 

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From miaojpm at gmail.com  Tue Oct 17 11:24:55 2017
From: miaojpm at gmail.com (John)
Date: Tue, 17 Oct 2017 02:24:55 -0700
Subject: [R] ggplot / second axis / just a notation problem
Message-ID: <CABcx46ATqvapBdkm_njjLN0srurdkNvXLL8MN7cJUhfPDXpoFA@mail.gmail.com>

Hi,

   I have a question on ggplot2 with the second axis, but I don't think one
needs to know ggplot2 package in order to answer this question.

   In this example,
https://rpubs.com/MarkusLoew/226759
   since the transformation of the second axis is given by y1=y2*5,
#####
   p <- p + scale_y_continuous(sec.axis = sec_axis(~.*5, name = "Relative
humidity [%]"))
#####
   Note  "~.*5"
   What can I do if the transformation is given by y1=y2-3 or y1=y2/5?
   Should I write "~.-3" or "~./5"?

   Thanks,

John

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Tue Oct 17 11:47:33 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Tue, 17 Oct 2017 12:47:33 +0300
Subject: [R] ggplot / second axis / just a notation problem
In-Reply-To: <CABcx46ATqvapBdkm_njjLN0srurdkNvXLL8MN7cJUhfPDXpoFA@mail.gmail.com>
References: <CABcx46ATqvapBdkm_njjLN0srurdkNvXLL8MN7cJUhfPDXpoFA@mail.gmail.com>
Message-ID: <CAGgJW75fAFbDA_Uig9TMvohKoXoQK6v+V2FuCxeOAG80NW5Qhg@mail.gmail.com>

Hi John,
Why not just try both and see which one makes sense?


On Tue, Oct 17, 2017 at 12:24 PM, John <miaojpm at gmail.com> wrote:

> Hi,
>
>    I have a question on ggplot2 with the second axis, but I don't think one
> needs to know ggplot2 package in order to answer this question.
>
>    In this example,
> https://rpubs.com/MarkusLoew/226759
>    since the transformation of the second axis is given by y1=y2*5,
> #####
>    p <- p + scale_y_continuous(sec.axis = sec_axis(~.*5, name = "Relative
> humidity [%]"))
> #####
>    Note  "~.*5"
>    What can I do if the transformation is given by y1=y2-3 or y1=y2/5?
>    Should I write "~.-3" or "~./5"?
>
>    Thanks,
>
> John
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ashimkapoor at gmail.com  Tue Oct 17 14:20:34 2017
From: ashimkapoor at gmail.com (Ashim Kapoor)
Date: Tue, 17 Oct 2017 17:50:34 +0530
Subject: [R] regex in maps package
Message-ID: <CAC8=1erROwzbBabAxamtD7FYwQVhZT37LWRBYhQM1oQ7nXr-+g@mail.gmail.com>

Dear All,

>From :

?maps::map

we have :

     map(database = "world", regions = ".", exact = FALSE, boundary = TRUE,
       interior = TRUE, projection = "", parameters = NULL, orientation =
NULL,
       fill = FALSE, col = 1, plot = TRUE, add = FALSE, namesonly = FALSE,
       xlim = NULL, ylim = NULL, wrap = FALSE, resolution = if (plot) 1
else 0,
       type = "l", bg = par("bg"), mar = c(4.1, 4.1, par("mar")[3], 0.1),
       myborder = 0.01, namefield="name", lforce="n", ...)

My query is the regex regions="." will match any single character. Should
it not be regions =".*" to be precise? Although I do realize that "." will
also match any string except "" (the empty string)? To be precise it should
be ".*" which can match an empty string as well ANY arbitrary string. I was
confused for a short while when I first read this. Look forward to
clarification on this.

Many thanks,
Ashim

	[[alternative HTML version deleted]]


From abetz at Portola.com  Tue Oct 17 19:32:36 2017
From: abetz at Portola.com (Andreas Betz)
Date: Tue, 17 Oct 2017 17:32:36 +0000
Subject: [R] greport
Message-ID: <D9F32A57D4946B4EA48C3D4248FF721739DF1CCA@Cruz-MB2.Portola.com>

Hello,

I am trying to setup grereport by Frank Harell on my Fedora system. Unfortunately some links for latex on the homepage are disabled. Thus I cannot find ocgtools for Fedora. Further Acrobat reader has not been updated after 2013. Can somebody give me some pointers how to set up the package.

Thank you

Andreas


This email message is for the sole use of the intended r...{{dropped:11}}


From roy.mendelssohn at noaa.gov  Tue Oct 17 21:22:38 2017
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Tue, 17 Oct 2017 12:22:38 -0700
Subject: [R] ggridges help
Message-ID: <E88F02FC-A417-4B04-8DD7-BA3226C41E3F@noaa.gov>

Hi All:

I am just not understanding ggridges.  The data I have are time series at different depths in the ocean.  I want to make a joy plot of the time series by depth.

If I was just doing a ggplot2 line plot I would be doing:

ggplot(plotFrame, aes(x = time, y = cycle, group = depth)) + geom_line()

but translating that to ggridges has not worked right.  Below is the result from dput() of a simplified data frame that has 3 depths and 2 years of monthly data.  (In fact I have  20 depths and 30  years of monthly data).   The command above will work with this data frame.

Thanks for any help.

-Roy

>dput(plotFrame)
structure(list(time = structure(c(719236800, 721915200, 724507200, 
727185600, 729777600, 732283200, 734961600, 737553600, 740232000, 
742824000, 745502400, 748180800, 750772800, 753451200, 756043200, 
758721600, 761313600, 763819200, 766497600, 769089600, 771768000, 
774360000, 777038400, 779716800, 719236800, 721915200, 724507200, 
727185600, 729777600, 732283200, 734961600, 737553600, 740232000, 
742824000, 745502400, 748180800, 750772800, 753451200, 756043200, 
758721600, 761313600, 763819200, 766497600, 769089600, 771768000, 
774360000, 777038400, 779716800, 719236800, 721915200, 724507200, 
727185600, 729777600, 732283200, 734961600, 737553600, 740232000, 
742824000, 745502400, 748180800, 750772800, 753451200, 756043200, 
758721600, 761313600, 763819200, 766497600, 769089600, 771768000, 
774360000, 777038400, 779716800), class = c("POSIXct", "POSIXt"
), tzone = "UTC"), depth = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L), .Label = c("20", "40", "60"), class = "factor"), cycle = structure(c(-0.164397110685046, 
-0.0639063592004652, -0.124275650584243, 0.232340199700421, 0.23265929828899, 
0.452800479990173, 0.631515844862171, 0.708775442811806, 0.402797787246307, 
0.540279471159411, 0.625583653374072, 0.607609707505232, 0.789645459141814, 
0.57943178332249, 0.395406041578379, 0.278792362706845, 0.000158405680203533, 
0.0618374997078718, 0.0942838757427736, 0.046510899158667, 0.136800366664298, 
0.165118554160811, 0.228812312665972, -0.225383761996565, -0.204115732057999, 
-0.0883683879679482, -0.135111844938738, 0.147562070872115, 0.198086355354394, 
0.386803300687593, 0.684023051288189, 0.69669009829253, 0.381213154479, 
0.550118050327324, 0.641294267691433, 0.614909878956221, 0.772779409518665, 
0.581967160929841, 0.381662488154885, 0.293380662335543, 0.0391733417449068, 
-0.0844674860904995, 0.163695040677223, 0.0444585016269223, 0.130561029192561, 
0.180784990884611, 0.242929491090375, -0.126543843540014, -0.112781525045155, 
-0.1803388763034, -0.0939120153437669, 0.150968491445835, 0.0992298571202001, 
0.289294645512006, 0.540517483378127, 0.625091194385051, 0.432224338078479, 
0.504654513110009, 0.62584673393424, 0.56834612321311, 0.789331138620147, 
0.6389908671341, 0.45156693996368, 0.412578785088203, 0.212440848202924, 
0.146392303930216, 0.654494252844301, 0.470248736982212, 0.239891529116349, 
0.200137949677769, 0.2858429346658, -0.121094155739595), .Dim = 72L)), .Names = c("time", 
"depth", "cycle"), row.names = c(NA, -72L), class = "data.frame")

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new street address***
110 McAllister Way
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From bgunter.4567 at gmail.com  Tue Oct 17 21:39:04 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 17 Oct 2017 12:39:04 -0700
Subject: [R] ggridges help
In-Reply-To: <E88F02FC-A417-4B04-8DD7-BA3226C41E3F@noaa.gov>
References: <E88F02FC-A417-4B04-8DD7-BA3226C41E3F@noaa.gov>
Message-ID: <CAGxFJbRk+i6C3TECgYg+32zSnDRZ8XfkPaDUTTOvqkUE8LwrVQ@mail.gmail.com>

...and your question is...?
... and the code you tried that didn't work was?

Bert


On Oct 17, 2017 12:22 PM, "Roy Mendelssohn - NOAA Federal" <
roy.mendelssohn at noaa.gov> wrote:

> Hi All:
>
> I am just not understanding ggridges.  The data I have are time series at
> different depths in the ocean.  I want to make a joy plot of the time
> series by depth.
>
> If I was just doing a ggplot2 line plot I would be doing:
>
> ggplot(plotFrame, aes(x = time, y = cycle, group = depth)) + geom_line()
>
> but translating that to ggridges has not worked right.  Below is the
> result from dput() of a simplified data frame that has 3 depths and 2 years
> of monthly data.  (In fact I have  20 depths and 30  years of monthly
> data).   The command above will work with this data frame.
>
> Thanks for any help.
>
> -Roy
>
> >dput(plotFrame)
> structure(list(time = structure(c(719236800, 721915200, 724507200,
> 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> 774360000, 777038400, 779716800, 719236800, 721915200, 724507200,
> 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> 774360000, 777038400, 779716800, 719236800, 721915200, 724507200,
> 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> 774360000, 777038400, 779716800), class = c("POSIXct", "POSIXt"
> ), tzone = "UTC"), depth = structure(c(1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 3L), .Label = c("20", "40", "60"), class = "factor"), cycle =
> structure(c(-0.164397110685046,
> -0.0639063592004652, -0.124275650584243, 0.232340199700421,
> 0.23265929828899,
> 0.452800479990173, 0.631515844862171, 0.708775442811806, 0.402797787246307,
> 0.540279471159411, 0.625583653374072, 0.607609707505232, 0.789645459141814,
> 0.57943178332249, 0.395406041578379, 0.278792362706845,
> 0.000158405680203533,
> 0.0618374997078718, 0.0942838757427736, 0.046510899158667,
> 0.136800366664298,
> 0.165118554160811, 0.228812312665972, -0.225383761996565,
> -0.204115732057999,
> -0.0883683879679482, -0.135111844938738, 0.147562070872115,
> 0.198086355354394,
> 0.386803300687593, 0.684023051288189, 0.69669009829253, 0.381213154479,
> 0.550118050327324, 0.641294267691433, 0.614909878956221, 0.772779409518665,
> 0.581967160929841, 0.381662488154885, 0.293380662335543,
> 0.0391733417449068,
> -0.0844674860904995, 0.163695040677223, 0.0444585016269223,
> 0.130561029192561,
> 0.180784990884611, 0.242929491090375, -0.126543843540014,
> -0.112781525045155,
> -0.1803388763034, -0.0939120153437669, 0.150968491445835,
> 0.0992298571202001,
> 0.289294645512006, 0.540517483378127, 0.625091194385051, 0.432224338078479,
> 0.504654513110009, 0.62584673393424, 0.56834612321311, 0.789331138620147,
> 0.6389908671341, 0.45156693996368, 0.412578785088203, 0.212440848202924,
> 0.146392303930216, 0.654494252844301, 0.470248736982212, 0.239891529116349,
> 0.200137949677769, 0.2858429346658, -0.121094155739595), .Dim = 72L)),
> .Names = c("time",
> "depth", "cycle"), row.names = c(NA, -72L), class = "data.frame")
>
> **********************
> "The contents of this message do not reflect any position of the U.S.
> Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division
> Southwest Fisheries Science Center
> ***Note new street address***
> 110 McAllister Way
> Santa Cruz, CA 95060
> Phone: (831)-420-3666
> Fax: (831) 420-3980
> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
>
> "Old age and treachery will overcome youth and skill."
> "From those who have been given much, much will be expected"
> "the arc of the moral universe is long, but it bends toward justice" -MLK
> Jr.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From roy.mendelssohn at noaa.gov  Tue Oct 17 21:43:03 2017
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Tue, 17 Oct 2017 12:43:03 -0700
Subject: [R] ggridges help
In-Reply-To: <CAGxFJbRk+i6C3TECgYg+32zSnDRZ8XfkPaDUTTOvqkUE8LwrVQ@mail.gmail.com>
References: <E88F02FC-A417-4B04-8DD7-BA3226C41E3F@noaa.gov>
 <CAGxFJbRk+i6C3TECgYg+32zSnDRZ8XfkPaDUTTOvqkUE8LwrVQ@mail.gmail.com>
Message-ID: <89B56317-70F8-42B8-9521-945329EE4904@noaa.gov>

I have tried:

ggplot(plotFrame, aes(x = time, y = cycle, height = cycle, group = depth)) + geom_ridgeline()
ggplot(plotFrame, aes(x = time, y = depth, height = cycle, group = depth)) + geom_ridgeline()
ggplot(plotFrame, aes(x = time, y = depth, group = depth)) + geom_density_ridges()

none are producing a plot that was a ridgeline for each depth showing the time series at that depth.  The plot should be like the geom_line plot,  but as a ridgeline for each depth.

-Roy

> On Oct 17, 2017, at 12:39 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> ...and your question is...?
> ... and the code you tried that didn't work was?
> 
> Bert
> 
> 
> On Oct 17, 2017 12:22 PM, "Roy Mendelssohn - NOAA Federal" <roy.mendelssohn at noaa.gov> wrote:
> Hi All:
> 
> I am just not understanding ggridges.  The data I have are time series at different depths in the ocean.  I want to make a joy plot of the time series by depth.
> 
> If I was just doing a ggplot2 line plot I would be doing:
> 
> ggplot(plotFrame, aes(x = time, y = cycle, group = depth)) + geom_line()
> 
> but translating that to ggridges has not worked right.  Below is the result from dput() of a simplified data frame that has 3 depths and 2 years of monthly data.  (In fact I have  20 depths and 30  years of monthly data).   The command above will work with this data frame.
> 
> Thanks for any help.
> 
> -Roy
> 
> >dput(plotFrame)
> structure(list(time = structure(c(719236800, 721915200, 724507200,
> 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> 774360000, 777038400, 779716800, 719236800, 721915200, 724507200,
> 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> 774360000, 777038400, 779716800, 719236800, 721915200, 724507200,
> 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> 774360000, 777038400, 779716800), class = c("POSIXct", "POSIXt"
> ), tzone = "UTC"), depth = structure(c(1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 3L), .Label = c("20", "40", "60"), class = "factor"), cycle = structure(c(-0.164397110685046,
> -0.0639063592004652, -0.124275650584243, 0.232340199700421, 0.23265929828899,
> 0.452800479990173, 0.631515844862171, 0.708775442811806, 0.402797787246307,
> 0.540279471159411, 0.625583653374072, 0.607609707505232, 0.789645459141814,
> 0.57943178332249, 0.395406041578379, 0.278792362706845, 0.000158405680203533,
> 0.0618374997078718, 0.0942838757427736, 0.046510899158667, 0.136800366664298,
> 0.165118554160811, 0.228812312665972, -0.225383761996565, -0.204115732057999,
> -0.0883683879679482, -0.135111844938738, 0.147562070872115, 0.198086355354394,
> 0.386803300687593, 0.684023051288189, 0.69669009829253, 0.381213154479,
> 0.550118050327324, 0.641294267691433, 0.614909878956221, 0.772779409518665,
> 0.581967160929841, 0.381662488154885, 0.293380662335543, 0.0391733417449068,
> -0.0844674860904995, 0.163695040677223, 0.0444585016269223, 0.130561029192561,
> 0.180784990884611, 0.242929491090375, -0.126543843540014, -0.112781525045155,
> -0.1803388763034, -0.0939120153437669, 0.150968491445835, 0.0992298571202001,
> 0.289294645512006, 0.540517483378127, 0.625091194385051, 0.432224338078479,
> 0.504654513110009, 0.62584673393424, 0.56834612321311, 0.789331138620147,
> 0.6389908671341, 0.45156693996368, 0.412578785088203, 0.212440848202924,
> 0.146392303930216, 0.654494252844301, 0.470248736982212, 0.239891529116349,
> 0.200137949677769, 0.2858429346658, -0.121094155739595), .Dim = 72L)), .Names = c("time",
> "depth", "cycle"), row.names = c(NA, -72L), class = "data.frame")
> 
> **********************
> "The contents of this message do not reflect any position of the U.S. Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division
> Southwest Fisheries Science Center
> ***Note new street address***
> 110 McAllister Way
> Santa Cruz, CA 95060
> Phone: (831)-420-3666
> Fax: (831) 420-3980
> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
> 
> "Old age and treachery will overcome youth and skill."
> "From those who have been given much, much will be expected"
> "the arc of the moral universe is long, but it bends toward justice" -MLK Jr.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new street address***
110 McAllister Way
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From dwinsemius at comcast.net  Tue Oct 17 21:47:12 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 17 Oct 2017 12:47:12 -0700
Subject: [R] greport
In-Reply-To: <D9F32A57D4946B4EA48C3D4248FF721739DF1CCA@Cruz-MB2.Portola.com>
References: <D9F32A57D4946B4EA48C3D4248FF721739DF1CCA@Cruz-MB2.Portola.com>
Message-ID: <6D866256-04E2-4E15-B2C2-6274162E1F55@comcast.net>


> On Oct 17, 2017, at 10:32 AM, Andreas Betz <abetz at Portola.com> wrote:
> 
> Hello,
> 
> I am trying to setup grereport by Frank Harell on my Fedora system. Unfortunately some links for latex on the homepage are disabled. Thus I cannot find ocgtools for Fedora.

Have you tried searching on google? The first hit for me was:

https://fedora.pkgs.org/25/fedora-x86_64/texlive-lib-2016-17.20160520.fc25.x86_64.rpm.html

... which appears to promise delivery of your request.

> Further Acrobat reader has not been updated after 2013. Can somebody give me some pointers how to set up the package.

This doesn't appear to be a question about R.

> 
> Thank you
> 
> Andreas
> 
> 
> This email message is for the sole use of the intended...{{dropped:20}}


From wdunlap at tibco.com  Tue Oct 17 22:09:56 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 17 Oct 2017 13:09:56 -0700
Subject: [R] ggridges help
In-Reply-To: <89B56317-70F8-42B8-9521-945329EE4904@noaa.gov>
References: <E88F02FC-A417-4B04-8DD7-BA3226C41E3F@noaa.gov>
 <CAGxFJbRk+i6C3TECgYg+32zSnDRZ8XfkPaDUTTOvqkUE8LwrVQ@mail.gmail.com>
 <89B56317-70F8-42B8-9521-945329EE4904@noaa.gov>
Message-ID: <CAF8bMcZPu4d29ZGQx+rBLjkJeicMiNNPyY32udikTqZkWu7H4g@mail.gmail.com>

Does the following work for you?

   ggplot2::ggplot(plotFrame, aes(x = time, y = depth, height = cycle,
group = depth)) + ggridges::geom_ridgeline(fill="red", min_height=-0.25)


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Oct 17, 2017 at 12:43 PM, Roy Mendelssohn - NOAA Federal <
roy.mendelssohn at noaa.gov> wrote:

> I have tried:
>
> ggplot(plotFrame, aes(x = time, y = cycle, height = cycle, group = depth))
> + geom_ridgeline()
> ggplot(plotFrame, aes(x = time, y = depth, height = cycle, group = depth))
> + geom_ridgeline()
> ggplot(plotFrame, aes(x = time, y = depth, group = depth)) +
> geom_density_ridges()
>
> none are producing a plot that was a ridgeline for each depth showing the
> time series at that depth.  The plot should be like the geom_line plot,
> but as a ridgeline for each depth.
>
> -Roy
>
> > On Oct 17, 2017, at 12:39 PM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
> >
> > ...and your question is...?
> > ... and the code you tried that didn't work was?
> >
> > Bert
> >
> >
> > On Oct 17, 2017 12:22 PM, "Roy Mendelssohn - NOAA Federal" <
> roy.mendelssohn at noaa.gov> wrote:
> > Hi All:
> >
> > I am just not understanding ggridges.  The data I have are time series
> at different depths in the ocean.  I want to make a joy plot of the time
> series by depth.
> >
> > If I was just doing a ggplot2 line plot I would be doing:
> >
> > ggplot(plotFrame, aes(x = time, y = cycle, group = depth)) + geom_line()
> >
> > but translating that to ggridges has not worked right.  Below is the
> result from dput() of a simplified data frame that has 3 depths and 2 years
> of monthly data.  (In fact I have  20 depths and 30  years of monthly
> data).   The command above will work with this data frame.
> >
> > Thanks for any help.
> >
> > -Roy
> >
> > >dput(plotFrame)
> > structure(list(time = structure(c(719236800, 721915200, 724507200,
> > 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> > 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> > 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> > 774360000, 777038400, 779716800, 719236800, 721915200, 724507200,
> > 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> > 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> > 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> > 774360000, 777038400, 779716800, 719236800, 721915200, 724507200,
> > 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> > 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> > 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> > 774360000, 777038400, 779716800), class = c("POSIXct", "POSIXt"
> > ), tzone = "UTC"), depth = structure(c(1L, 1L, 1L, 1L, 1L, 1L,
> > 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> > 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> > 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L,
> > 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> > 3L, 3L), .Label = c("20", "40", "60"), class = "factor"), cycle =
> structure(c(-0.164397110685046,
> > -0.0639063592004652, -0.124275650584243, 0.232340199700421,
> 0.23265929828899,
> > 0.452800479990173, 0.631515844862171, 0.708775442811806,
> 0.402797787246307,
> > 0.540279471159411, 0.625583653374072, 0.607609707505232,
> 0.789645459141814,
> > 0.57943178332249, 0.395406041578379, 0.278792362706845,
> 0.000158405680203533,
> > 0.0618374997078718, 0.0942838757427736, 0.046510899158667,
> 0.136800366664298,
> > 0.165118554160811, 0.228812312665972, -0.225383761996565,
> -0.204115732057999,
> > -0.0883683879679482, -0.135111844938738, 0.147562070872115,
> 0.198086355354394,
> > 0.386803300687593, 0.684023051288189, 0.69669009829253, 0.381213154479,
> > 0.550118050327324, 0.641294267691433, 0.614909878956221,
> 0.772779409518665,
> > 0.581967160929841, 0.381662488154885, 0.293380662335543,
> 0.0391733417449068,
> > -0.0844674860904995, 0.163695040677223, 0.0444585016269223,
> 0.130561029192561,
> > 0.180784990884611, 0.242929491090375, -0.126543843540014,
> -0.112781525045155,
> > -0.1803388763034, -0.0939120153437669, 0.150968491445835,
> 0.0992298571202001,
> > 0.289294645512006, 0.540517483378127, 0.625091194385051,
> 0.432224338078479,
> > 0.504654513110009, 0.62584673393424, 0.56834612321311, 0.789331138620147,
> > 0.6389908671341, 0.45156693996368, 0.412578785088203, 0.212440848202924,
> > 0.146392303930216, 0.654494252844301, 0.470248736982212,
> 0.239891529116349,
> > 0.200137949677769, 0.2858429346658, -0.121094155739595), .Dim = 72L)),
> .Names = c("time",
> > "depth", "cycle"), row.names = c(NA, -72L), class = "data.frame")
> >
> > **********************
> > "The contents of this message do not reflect any position of the U.S.
> Government or NOAA."
> > **********************
> > Roy Mendelssohn
> > Supervisory Operations Research Analyst
> > NOAA/NMFS
> > Environmental Research Division
> > Southwest Fisheries Science Center
> > ***Note new street address***
> > 110 McAllister Way
> > Santa Cruz, CA 95060
> > Phone: (831)-420-3666
> > Fax: (831) 420-3980
> > e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
> >
> > "Old age and treachery will overcome youth and skill."
> > "From those who have been given much, much will be expected"
> > "the arc of the moral universe is long, but it bends toward justice"
> -MLK Jr.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> **********************
> "The contents of this message do not reflect any position of the U.S.
> Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division
> Southwest Fisheries Science Center
> ***Note new street address***
> 110 McAllister Way
> Santa Cruz, CA 95060
> Phone: (831)-420-3666
> Fax: (831) 420-3980
> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
>
> "Old age and treachery will overcome youth and skill."
> "From those who have been given much, much will be expected"
> "the arc of the moral universe is long, but it bends toward justice" -MLK
> Jr.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From roy.mendelssohn at noaa.gov  Tue Oct 17 22:26:14 2017
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Tue, 17 Oct 2017 13:26:14 -0700
Subject: [R] ggridges help
In-Reply-To: <CAF8bMcZPu4d29ZGQx+rBLjkJeicMiNNPyY32udikTqZkWu7H4g@mail.gmail.com>
References: <E88F02FC-A417-4B04-8DD7-BA3226C41E3F@noaa.gov>
 <CAGxFJbRk+i6C3TECgYg+32zSnDRZ8XfkPaDUTTOvqkUE8LwrVQ@mail.gmail.com>
 <89B56317-70F8-42B8-9521-945329EE4904@noaa.gov>
 <CAF8bMcZPu4d29ZGQx+rBLjkJeicMiNNPyY32udikTqZkWu7H4g@mail.gmail.com>
Message-ID: <DDCE2946-C53C-44DF-B4E0-43F652F77464@noaa.gov>

yes, thanks,  and I was getting close to that.  One thing I found is the manual says the height is the distance above the y-line,  which should be, but doesn't have to be positive.  In fact,  the time series are  estimates of a cycle,  and has negative values,  which unfortunately are not included in my sub-sample.  And the negative values are not handled properly (the series disappears for the negative values)

Also,  maybe it is my bad eyesight,  but in examples of the older, deprecated ggjoy package,  there seem to be a slight offset added to the depth effect,  which doesn't appear to be the case now, or am I missing something.  There is this in the manual:


> position	
> Position adjustment, either as a string, or the result of a call to a position adjustment function.

I assume this refers to the ggplot2 position adjustments.  Would one of those calls have that effect?

Thanks,

-Roy




> On Oct 17, 2017, at 1:09 PM, William Dunlap <wdunlap at tibco.com> wrote:
> 
> Does the following work for you?
>  
>    ggplot2::ggplot(plotFrame, aes(x = time, y = depth, height = cycle, group = depth)) + ggridges::geom_ridgeline(fill="red", min_height=-0.25)
> 
> 
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
> 
> On Tue, Oct 17, 2017 at 12:43 PM, Roy Mendelssohn - NOAA Federal <roy.mendelssohn at noaa.gov> wrote:
> I have tried:
> 
> ggplot(plotFrame, aes(x = time, y = cycle, height = cycle, group = depth)) + geom_ridgeline()
> ggplot(plotFrame, aes(x = time, y = depth, height = cycle, group = depth)) + geom_ridgeline()
> ggplot(plotFrame, aes(x = time, y = depth, group = depth)) + geom_density_ridges()
> 
> none are producing a plot that was a ridgeline for each depth showing the time series at that depth.  The plot should be like the geom_line plot,  but as a ridgeline for each depth.
> 
> -Roy
> 
> > On Oct 17, 2017, at 12:39 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> >
> > ...and your question is...?
> > ... and the code you tried that didn't work was?
> >
> > Bert
> >
> >
> > On Oct 17, 2017 12:22 PM, "Roy Mendelssohn - NOAA Federal" <roy.mendelssohn at noaa.gov> wrote:
> > Hi All:
> >
> > I am just not understanding ggridges.  The data I have are time series at different depths in the ocean.  I want to make a joy plot of the time series by depth.
> >
> > If I was just doing a ggplot2 line plot I would be doing:
> >
> > ggplot(plotFrame, aes(x = time, y = cycle, group = depth)) + geom_line()
> >
> > but translating that to ggridges has not worked right.  Below is the result from dput() of a simplified data frame that has 3 depths and 2 years of monthly data.  (In fact I have  20 depths and 30  years of monthly data).   The command above will work with this data frame.
> >
> > Thanks for any help.
> >
> > -Roy
> >
> > >dput(plotFrame)
> > structure(list(time = structure(c(719236800, 721915200, 724507200,
> > 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> > 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> > 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> > 774360000, 777038400, 779716800, 719236800, 721915200, 724507200,
> > 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> > 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> > 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> > 774360000, 777038400, 779716800, 719236800, 721915200, 724507200,
> > 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> > 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> > 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> > 774360000, 777038400, 779716800), class = c("POSIXct", "POSIXt"
> > ), tzone = "UTC"), depth = structure(c(1L, 1L, 1L, 1L, 1L, 1L,
> > 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> > 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> > 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L,
> > 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> > 3L, 3L), .Label = c("20", "40", "60"), class = "factor"), cycle = structure(c(-0.164397110685046,
> > -0.0639063592004652, -0.124275650584243, 0.232340199700421, 0.23265929828899,
> > 0.452800479990173, 0.631515844862171, 0.708775442811806, 0.402797787246307,
> > 0.540279471159411, 0.625583653374072, 0.607609707505232, 0.789645459141814,
> > 0.57943178332249, 0.395406041578379, 0.278792362706845, 0.000158405680203533,
> > 0.0618374997078718, 0.0942838757427736, 0.046510899158667, 0.136800366664298,
> > 0.165118554160811, 0.228812312665972, -0.225383761996565, -0.204115732057999,
> > -0.0883683879679482, -0.135111844938738, 0.147562070872115, 0.198086355354394,
> > 0.386803300687593, 0.684023051288189, 0.69669009829253, 0.381213154479,
> > 0.550118050327324, 0.641294267691433, 0.614909878956221, 0.772779409518665,
> > 0.581967160929841, 0.381662488154885, 0.293380662335543, 0.0391733417449068,
> > -0.0844674860904995, 0.163695040677223, 0.0444585016269223, 0.130561029192561,
> > 0.180784990884611, 0.242929491090375, -0.126543843540014, -0.112781525045155,
> > -0.1803388763034, -0.0939120153437669, 0.150968491445835, 0.0992298571202001,
> > 0.289294645512006, 0.540517483378127, 0.625091194385051, 0.432224338078479,
> > 0.504654513110009, 0.62584673393424, 0.56834612321311, 0.789331138620147,
> > 0.6389908671341, 0.45156693996368, 0.412578785088203, 0.212440848202924,
> > 0.146392303930216, 0.654494252844301, 0.470248736982212, 0.239891529116349,
> > 0.200137949677769, 0.2858429346658, -0.121094155739595), .Dim = 72L)), .Names = c("time",
> > "depth", "cycle"), row.names = c(NA, -72L), class = "data.frame")
> >
> > **********************
> > "The contents of this message do not reflect any position of the U.S. Government or NOAA."
> > **********************
> > Roy Mendelssohn
> > Supervisory Operations Research Analyst
> > NOAA/NMFS
> > Environmental Research Division
> > Southwest Fisheries Science Center
> > ***Note new street address***
> > 110 McAllister Way
> > Santa Cruz, CA 95060
> > Phone: (831)-420-3666
> > Fax: (831) 420-3980
> > e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
> >
> > "Old age and treachery will overcome youth and skill."
> > "From those who have been given much, much will be expected"
> > "the arc of the moral universe is long, but it bends toward justice" -MLK Jr.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> **********************
> "The contents of this message do not reflect any position of the U.S. Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division
> Southwest Fisheries Science Center
> ***Note new street address***
> 110 McAllister Way
> Santa Cruz, CA 95060
> Phone: (831)-420-3666
> Fax: (831) 420-3980
> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
> 
> "Old age and treachery will overcome youth and skill."
> "From those who have been given much, much will be expected"
> "the arc of the moral universe is long, but it bends toward justice" -MLK Jr.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new street address***
110 McAllister Way
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From wdunlap at tibco.com  Tue Oct 17 23:01:04 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 17 Oct 2017 14:01:04 -0700
Subject: [R] ggridges help
In-Reply-To: <DDCE2946-C53C-44DF-B4E0-43F652F77464@noaa.gov>
References: <E88F02FC-A417-4B04-8DD7-BA3226C41E3F@noaa.gov>
 <CAGxFJbRk+i6C3TECgYg+32zSnDRZ8XfkPaDUTTOvqkUE8LwrVQ@mail.gmail.com>
 <89B56317-70F8-42B8-9521-945329EE4904@noaa.gov>
 <CAF8bMcZPu4d29ZGQx+rBLjkJeicMiNNPyY32udikTqZkWu7H4g@mail.gmail.com>
 <DDCE2946-C53C-44DF-B4E0-43F652F77464@noaa.gov>
Message-ID: <CAF8bMcaDmS5fuuHuh+xSH5BubB_n1pz9R2D3BXAwW5jteVH-9Q@mail.gmail.com>

The min_height = -0.25 is there to make it show cycle values down to -1/4.
You may want to change it to -1 so it shows more of the cycle values.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Oct 17, 2017 at 1:26 PM, Roy Mendelssohn - NOAA Federal <
roy.mendelssohn at noaa.gov> wrote:

> yes, thanks,  and I was getting close to that.  One thing I found is the
> manual says the height is the distance above the y-line,  which should be,
> but doesn't have to be positive.  In fact,  the time series are  estimates
> of a cycle,  and has negative values,  which unfortunately are not included
> in my sub-sample.  And the negative values are not handled properly (the
> series disappears for the negative values)
>
> Also,  maybe it is my bad eyesight,  but in examples of the older,
> deprecated ggjoy package,  there seem to be a slight offset added to the
> depth effect,  which doesn't appear to be the case now, or am I missing
> something.  There is this in the manual:
>
>
> > position
> > Position adjustment, either as a string, or the result of a call to a
> position adjustment function.
>
> I assume this refers to the ggplot2 position adjustments.  Would one of
> those calls have that effect?
>
> Thanks,
>
> -Roy
>
>
>
>
> > On Oct 17, 2017, at 1:09 PM, William Dunlap <wdunlap at tibco.com> wrote:
> >
> > Does the following work for you?
> >
> >    ggplot2::ggplot(plotFrame, aes(x = time, y = depth, height = cycle,
> group = depth)) + ggridges::geom_ridgeline(fill="red", min_height=-0.25)
> >
> >
> > Bill Dunlap
> > TIBCO Software
> > wdunlap tibco.com
> >
> > On Tue, Oct 17, 2017 at 12:43 PM, Roy Mendelssohn - NOAA Federal <
> roy.mendelssohn at noaa.gov> wrote:
> > I have tried:
> >
> > ggplot(plotFrame, aes(x = time, y = cycle, height = cycle, group =
> depth)) + geom_ridgeline()
> > ggplot(plotFrame, aes(x = time, y = depth, height = cycle, group =
> depth)) + geom_ridgeline()
> > ggplot(plotFrame, aes(x = time, y = depth, group = depth)) +
> geom_density_ridges()
> >
> > none are producing a plot that was a ridgeline for each depth showing
> the time series at that depth.  The plot should be like the geom_line
> plot,  but as a ridgeline for each depth.
> >
> > -Roy
> >
> > > On Oct 17, 2017, at 12:39 PM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
> > >
> > > ...and your question is...?
> > > ... and the code you tried that didn't work was?
> > >
> > > Bert
> > >
> > >
> > > On Oct 17, 2017 12:22 PM, "Roy Mendelssohn - NOAA Federal" <
> roy.mendelssohn at noaa.gov> wrote:
> > > Hi All:
> > >
> > > I am just not understanding ggridges.  The data I have are time series
> at different depths in the ocean.  I want to make a joy plot of the time
> series by depth.
> > >
> > > If I was just doing a ggplot2 line plot I would be doing:
> > >
> > > ggplot(plotFrame, aes(x = time, y = cycle, group = depth)) +
> geom_line()
> > >
> > > but translating that to ggridges has not worked right.  Below is the
> result from dput() of a simplified data frame that has 3 depths and 2 years
> of monthly data.  (In fact I have  20 depths and 30  years of monthly
> data).   The command above will work with this data frame.
> > >
> > > Thanks for any help.
> > >
> > > -Roy
> > >
> > > >dput(plotFrame)
> > > structure(list(time = structure(c(719236800, 721915200, 724507200,
> > > 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> > > 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> > > 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> > > 774360000, 777038400, 779716800, 719236800, 721915200, 724507200,
> > > 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> > > 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> > > 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> > > 774360000, 777038400, 779716800, 719236800, 721915200, 724507200,
> > > 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> > > 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> > > 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> > > 774360000, 777038400, 779716800), class = c("POSIXct", "POSIXt"
> > > ), tzone = "UTC"), depth = structure(c(1L, 1L, 1L, 1L, 1L, 1L,
> > > 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> > > 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> > > 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L,
> > > 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> > > 3L, 3L), .Label = c("20", "40", "60"), class = "factor"), cycle =
> structure(c(-0.164397110685046,
> > > -0.0639063592004652, -0.124275650584243, 0.232340199700421,
> 0.23265929828899,
> > > 0.452800479990173, 0.631515844862171, 0.708775442811806,
> 0.402797787246307,
> > > 0.540279471159411, 0.625583653374072, 0.607609707505232,
> 0.789645459141814,
> > > 0.57943178332249, 0.395406041578379, 0.278792362706845,
> 0.000158405680203533,
> > > 0.0618374997078718, 0.0942838757427736, 0.046510899158667,
> 0.136800366664298,
> > > 0.165118554160811, 0.228812312665972, -0.225383761996565,
> -0.204115732057999,
> > > -0.0883683879679482, -0.135111844938738, 0.147562070872115,
> 0.198086355354394,
> > > 0.386803300687593, 0.684023051288189, 0.69669009829253, 0.381213154479,
> > > 0.550118050327324, 0.641294267691433, 0.614909878956221,
> 0.772779409518665,
> > > 0.581967160929841, 0.381662488154885, 0.293380662335543,
> 0.0391733417449068,
> > > -0.0844674860904995, 0.163695040677223, 0.0444585016269223,
> 0.130561029192561,
> > > 0.180784990884611, 0.242929491090375, -0.126543843540014,
> -0.112781525045155,
> > > -0.1803388763034, -0.0939120153437669, 0.150968491445835,
> 0.0992298571202001,
> > > 0.289294645512006, 0.540517483378127, 0.625091194385051,
> 0.432224338078479,
> > > 0.504654513110009, 0.62584673393424, 0.56834612321311,
> 0.789331138620147,
> > > 0.6389908671341, 0.45156693996368, 0.412578785088203,
> 0.212440848202924,
> > > 0.146392303930216, 0.654494252844301, 0.470248736982212,
> 0.239891529116349,
> > > 0.200137949677769, 0.2858429346658, -0.121094155739595), .Dim = 72L)),
> .Names = c("time",
> > > "depth", "cycle"), row.names = c(NA, -72L), class = "data.frame")
> > >
> > > **********************
> > > "The contents of this message do not reflect any position of the U.S.
> Government or NOAA."
> > > **********************
> > > Roy Mendelssohn
> > > Supervisory Operations Research Analyst
> > > NOAA/NMFS
> > > Environmental Research Division
> > > Southwest Fisheries Science Center
> > > ***Note new street address***
> > > 110 McAllister Way
> > > Santa Cruz, CA 95060
> > > Phone: (831)-420-3666
> > > Fax: (831) 420-3980
> > > e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
> > >
> > > "Old age and treachery will overcome youth and skill."
> > > "From those who have been given much, much will be expected"
> > > "the arc of the moral universe is long, but it bends toward justice"
> -MLK Jr.
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > **********************
> > "The contents of this message do not reflect any position of the U.S.
> Government or NOAA."
> > **********************
> > Roy Mendelssohn
> > Supervisory Operations Research Analyst
> > NOAA/NMFS
> > Environmental Research Division
> > Southwest Fisheries Science Center
> > ***Note new street address***
> > 110 McAllister Way
> > Santa Cruz, CA 95060
> > Phone: (831)-420-3666
> > Fax: (831) 420-3980
> > e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
> >
> > "Old age and treachery will overcome youth and skill."
> > "From those who have been given much, much will be expected"
> > "the arc of the moral universe is long, but it bends toward justice"
> -MLK Jr.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> **********************
> "The contents of this message do not reflect any position of the U.S.
> Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division
> Southwest Fisheries Science Center
> ***Note new street address***
> 110 McAllister Way
> Santa Cruz, CA 95060
> Phone: (831)-420-3666
> Fax: (831) 420-3980
> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
>
> "Old age and treachery will overcome youth and skill."
> "From those who have been given much, much will be expected"
> "the arc of the moral universe is long, but it bends toward justice" -MLK
> Jr.
>
>

	[[alternative HTML version deleted]]


From roy.mendelssohn at noaa.gov  Wed Oct 18 01:29:55 2017
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Tue, 17 Oct 2017 16:29:55 -0700
Subject: [R] ggridges help
In-Reply-To: <CAF8bMcaDmS5fuuHuh+xSH5BubB_n1pz9R2D3BXAwW5jteVH-9Q@mail.gmail.com>
References: <E88F02FC-A417-4B04-8DD7-BA3226C41E3F@noaa.gov>
 <CAGxFJbRk+i6C3TECgYg+32zSnDRZ8XfkPaDUTTOvqkUE8LwrVQ@mail.gmail.com>
 <89B56317-70F8-42B8-9521-945329EE4904@noaa.gov>
 <CAF8bMcZPu4d29ZGQx+rBLjkJeicMiNNPyY32udikTqZkWu7H4g@mail.gmail.com>
 <DDCE2946-C53C-44DF-B4E0-43F652F77464@noaa.gov>
 <CAF8bMcaDmS5fuuHuh+xSH5BubB_n1pz9R2D3BXAwW5jteVH-9Q@mail.gmail.com>
Message-ID: <9F07760F-E542-4A47-B04B-85EB58D70335@noaa.gov>

Perfect,  thank you.  

I find the Unix style help usual in R is really only helpful once you know what everything is doing.  That makes a good vignette, that shows what all of the options do in a careful way,  really important. 

Thanks again.

-Roy

> On Oct 17, 2017, at 2:01 PM, William Dunlap <wdunlap at tibco.com> wrote:
> 
> The min_height = -0.25 is there to make it show cycle values down to -1/4.  You may want to change it to -1 so it shows more of the cycle values.
> 
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
> 
> On Tue, Oct 17, 2017 at 1:26 PM, Roy Mendelssohn - NOAA Federal <roy.mendelssohn at noaa.gov> wrote:
> yes, thanks,  and I was getting close to that.  One thing I found is the manual says the height is the distance above the y-line,  which should be, but doesn't have to be positive.  In fact,  the time series are  estimates of a cycle,  and has negative values,  which unfortunately are not included in my sub-sample.  And the negative values are not handled properly (the series disappears for the negative values)
> 
> Also,  maybe it is my bad eyesight,  but in examples of the older, deprecated ggjoy package,  there seem to be a slight offset added to the depth effect,  which doesn't appear to be the case now, or am I missing something.  There is this in the manual:
> 
> 
> > position
> > Position adjustment, either as a string, or the result of a call to a position adjustment function.
> 
> I assume this refers to the ggplot2 position adjustments.  Would one of those calls have that effect?
> 
> Thanks,
> 
> -Roy
> 
> 
> 
> 
> > On Oct 17, 2017, at 1:09 PM, William Dunlap <wdunlap at tibco.com> wrote:
> >
> > Does the following work for you?
> >
> >    ggplot2::ggplot(plotFrame, aes(x = time, y = depth, height = cycle, group = depth)) + ggridges::geom_ridgeline(fill="red", min_height=-0.25)
> >
> >
> > Bill Dunlap
> > TIBCO Software
> > wdunlap tibco.com
> >
> > On Tue, Oct 17, 2017 at 12:43 PM, Roy Mendelssohn - NOAA Federal <roy.mendelssohn at noaa.gov> wrote:
> > I have tried:
> >
> > ggplot(plotFrame, aes(x = time, y = cycle, height = cycle, group = depth)) + geom_ridgeline()
> > ggplot(plotFrame, aes(x = time, y = depth, height = cycle, group = depth)) + geom_ridgeline()
> > ggplot(plotFrame, aes(x = time, y = depth, group = depth)) + geom_density_ridges()
> >
> > none are producing a plot that was a ridgeline for each depth showing the time series at that depth.  The plot should be like the geom_line plot,  but as a ridgeline for each depth.
> >
> > -Roy
> >
> > > On Oct 17, 2017, at 12:39 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> > >
> > > ...and your question is...?
> > > ... and the code you tried that didn't work was?
> > >
> > > Bert
> > >
> > >
> > > On Oct 17, 2017 12:22 PM, "Roy Mendelssohn - NOAA Federal" <roy.mendelssohn at noaa.gov> wrote:
> > > Hi All:
> > >
> > > I am just not understanding ggridges.  The data I have are time series at different depths in the ocean.  I want to make a joy plot of the time series by depth.
> > >
> > > If I was just doing a ggplot2 line plot I would be doing:
> > >
> > > ggplot(plotFrame, aes(x = time, y = cycle, group = depth)) + geom_line()
> > >
> > > but translating that to ggridges has not worked right.  Below is the result from dput() of a simplified data frame that has 3 depths and 2 years of monthly data.  (In fact I have  20 depths and 30  years of monthly data).   The command above will work with this data frame.
> > >
> > > Thanks for any help.
> > >
> > > -Roy
> > >
> > > >dput(plotFrame)
> > > structure(list(time = structure(c(719236800, 721915200, 724507200,
> > > 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> > > 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> > > 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> > > 774360000, 777038400, 779716800, 719236800, 721915200, 724507200,
> > > 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> > > 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> > > 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> > > 774360000, 777038400, 779716800, 719236800, 721915200, 724507200,
> > > 727185600, 729777600, 732283200, 734961600, 737553600, 740232000,
> > > 742824000, 745502400, 748180800, 750772800, 753451200, 756043200,
> > > 758721600, 761313600, 763819200, 766497600, 769089600, 771768000,
> > > 774360000, 777038400, 779716800), class = c("POSIXct", "POSIXt"
> > > ), tzone = "UTC"), depth = structure(c(1L, 1L, 1L, 1L, 1L, 1L,
> > > 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> > > 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> > > 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L,
> > > 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> > > 3L, 3L), .Label = c("20", "40", "60"), class = "factor"), cycle = structure(c(-0.164397110685046,
> > > -0.0639063592004652, -0.124275650584243, 0.232340199700421, 0.23265929828899,
> > > 0.452800479990173, 0.631515844862171, 0.708775442811806, 0.402797787246307,
> > > 0.540279471159411, 0.625583653374072, 0.607609707505232, 0.789645459141814,
> > > 0.57943178332249, 0.395406041578379, 0.278792362706845, 0.000158405680203533,
> > > 0.0618374997078718, 0.0942838757427736, 0.046510899158667, 0.136800366664298,
> > > 0.165118554160811, 0.228812312665972, -0.225383761996565, -0.204115732057999,
> > > -0.0883683879679482, -0.135111844938738, 0.147562070872115, 0.198086355354394,
> > > 0.386803300687593, 0.684023051288189, 0.69669009829253, 0.381213154479,
> > > 0.550118050327324, 0.641294267691433, 0.614909878956221, 0.772779409518665,
> > > 0.581967160929841, 0.381662488154885, 0.293380662335543, 0.0391733417449068,
> > > -0.0844674860904995, 0.163695040677223, 0.0444585016269223, 0.130561029192561,
> > > 0.180784990884611, 0.242929491090375, -0.126543843540014, -0.112781525045155,
> > > -0.1803388763034, -0.0939120153437669, 0.150968491445835, 0.0992298571202001,
> > > 0.289294645512006, 0.540517483378127, 0.625091194385051, 0.432224338078479,
> > > 0.504654513110009, 0.62584673393424, 0.56834612321311, 0.789331138620147,
> > > 0.6389908671341, 0.45156693996368, 0.412578785088203, 0.212440848202924,
> > > 0.146392303930216, 0.654494252844301, 0.470248736982212, 0.239891529116349,
> > > 0.200137949677769, 0.2858429346658, -0.121094155739595), .Dim = 72L)), .Names = c("time",
> > > "depth", "cycle"), row.names = c(NA, -72L), class = "data.frame")
> > >
> > > **********************
> > > "The contents of this message do not reflect any position of the U.S. Government or NOAA."
> > > **********************
> > > Roy Mendelssohn
> > > Supervisory Operations Research Analyst
> > > NOAA/NMFS
> > > Environmental Research Division
> > > Southwest Fisheries Science Center
> > > ***Note new street address***
> > > 110 McAllister Way
> > > Santa Cruz, CA 95060
> > > Phone: (831)-420-3666
> > > Fax: (831) 420-3980
> > > e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
> > >
> > > "Old age and treachery will overcome youth and skill."
> > > "From those who have been given much, much will be expected"
> > > "the arc of the moral universe is long, but it bends toward justice" -MLK Jr.
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > **********************
> > "The contents of this message do not reflect any position of the U.S. Government or NOAA."
> > **********************
> > Roy Mendelssohn
> > Supervisory Operations Research Analyst
> > NOAA/NMFS
> > Environmental Research Division
> > Southwest Fisheries Science Center
> > ***Note new street address***
> > 110 McAllister Way
> > Santa Cruz, CA 95060
> > Phone: (831)-420-3666
> > Fax: (831) 420-3980
> > e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
> >
> > "Old age and treachery will overcome youth and skill."
> > "From those who have been given much, much will be expected"
> > "the arc of the moral universe is long, but it bends toward justice" -MLK Jr.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> **********************
> "The contents of this message do not reflect any position of the U.S. Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division
> Southwest Fisheries Science Center
> ***Note new street address***
> 110 McAllister Way
> Santa Cruz, CA 95060
> Phone: (831)-420-3666
> Fax: (831) 420-3980
> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
> 
> "Old age and treachery will overcome youth and skill."
> "From those who have been given much, much will be expected"
> "the arc of the moral universe is long, but it bends toward justice" -MLK Jr.
> 
> 

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new street address***
110 McAllister Way
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From miaojpm at gmail.com  Wed Oct 18 02:35:04 2017
From: miaojpm at gmail.com (John)
Date: Tue, 17 Oct 2017 17:35:04 -0700
Subject: [R] ggplot / second axis / just a notation problem
In-Reply-To: <CAGgJW75fAFbDA_Uig9TMvohKoXoQK6v+V2FuCxeOAG80NW5Qhg@mail.gmail.com>
References: <CABcx46ATqvapBdkm_njjLN0srurdkNvXLL8MN7cJUhfPDXpoFA@mail.gmail.com>
 <CAGgJW75fAFbDA_Uig9TMvohKoXoQK6v+V2FuCxeOAG80NW5Qhg@mail.gmail.com>
Message-ID: <CABcx46BXz9bDW+_NH-OgWfCtwHHThcVSfGwTrjv5MFrGRtZ+3A@mail.gmail.com>

Yes, I did. I should have posted the results here.
"~./5" works, but  "~.-3" does not. Multiplication and division should
work; I am wondering how I should present the plus and the minus in this
context.

Thanks,

John

2017-10-17 2:47 GMT-07:00 Eric Berger <ericjberger at gmail.com>:

> Hi John,
> Why not just try both and see which one makes sense?
>
>
> On Tue, Oct 17, 2017 at 12:24 PM, John <miaojpm at gmail.com> wrote:
>
>> Hi,
>>
>>    I have a question on ggplot2 with the second axis, but I don't think
>> one
>> needs to know ggplot2 package in order to answer this question.
>>
>>    In this example,
>> https://rpubs.com/MarkusLoew/226759
>>    since the transformation of the second axis is given by y1=y2*5,
>> #####
>>    p <- p + scale_y_continuous(sec.axis = sec_axis(~.*5, name = "Relative
>> humidity [%]"))
>> #####
>>    Note  "~.*5"
>>    What can I do if the transformation is given by y1=y2-3 or y1=y2/5?
>>    Should I write "~.-3" or "~./5"?
>>
>>    Thanks,
>>
>> John
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From marna.wagley at gmail.com  Wed Oct 18 09:45:35 2017
From: marna.wagley at gmail.com (Marna Wagley)
Date: Wed, 18 Oct 2017 00:45:35 -0700
Subject: [R] creating tables with replacement
Message-ID: <CAMwU6B2_gCeNTcbxhUTmZNPpH0DWV3MRCcXU0YTdnrUMiRpomQ@mail.gmail.com>

Hi R User,
I am new in R and trying to create tables with selecting rows randomly (but
with replacement) for each group but each group should have same number as
original. Is it possible to create it using the following example data set?

Your help is highly appreciated.

dat1<-structure(list(RegionA = structure(c(1L, 1L, 2L, 3L, 3L, 4L,

5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L), .Label = c("Ra",

"Rb", "Rc", "Rd", "Re", "Rf"), class = "factor"), site = structure(c(1L,

12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 2L, 3L, 4L, 5L, 6L, 7L,

8L, 9L, 10L, 11L), .Label = c("s1", "s10", "s11", "s12", "s13",

"s14", "s15", "s16", "s17", "s18", "s19", "s2", "s3", "s4", "s5",

"s6", "s7", "s8", "s9"), class = "factor"), temp = c(23L, 21L,

10L, 15L, 16L, 8L, 13L, 1L, 23L, 19L, 25L, 19L, 12L, 16L, 19L,

21L, 12L, 5L, 7L), group = structure(c(1L, 1L, 1L, 2L, 2L, 2L,

2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), .Label = c("A",

"B", "C"), class = "factor")), .Names = c("RegionA", "site",

"temp", "group"), class = "data.frame", row.names = c(NA, -19L

))


Here group A has 3 rows,

B had 7 rows, and

C has 9 rows


I want to select rows randomly (with replacement) for each group but each
group should have same number of rows as above (group A=3 rows, B=7 rows, and
c=9 rows). This way I want to create at least 10 tables (10 random tables)
and save them separately in Excel.


Thanks

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Wed Oct 18 10:37:42 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 18 Oct 2017 08:37:42 +0000
Subject: [R] creating tables with replacement
In-Reply-To: <CAMwU6B2_gCeNTcbxhUTmZNPpH0DWV3MRCcXU0YTdnrUMiRpomQ@mail.gmail.com>
References: <CAMwU6B2_gCeNTcbxhUTmZNPpH0DWV3MRCcXU0YTdnrUMiRpomQ@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3950@SRVEXCHCM301.precheza.cz>

Hi

maybe there is another more elegant solution but something like this

> idx <- 1:nrow(dat1)
> lll <- split(idx, dat1$group)
> dat1[unlist(lapply(lll, sample, rep=TRUE)),]

gives you selected rows.

You could use for cycle or save those data frames manually

Cheers

Petr
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Marna
> Wagley
> Sent: Wednesday, October 18, 2017 9:46 AM
> To: r-help mailing list <r-help at r-project.org>
> Subject: [R] creating tables with replacement
>
> Hi R User,
> I am new in R and trying to create tables with selecting rows randomly (but
> with replacement) for each group but each group should have same number as
> original. Is it possible to create it using the following example data set?
>
> Your help is highly appreciated.
>
> dat1<-structure(list(RegionA = structure(c(1L, 1L, 2L, 3L, 3L, 4L,
>
> 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L), .Label = c("Ra",
>
> "Rb", "Rc", "Rd", "Re", "Rf"), class = "factor"), site = structure(c(1L,
>
> 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 2L, 3L, 4L, 5L, 6L, 7L,
>
> 8L, 9L, 10L, 11L), .Label = c("s1", "s10", "s11", "s12", "s13",
>
> "s14", "s15", "s16", "s17", "s18", "s19", "s2", "s3", "s4", "s5",
>
> "s6", "s7", "s8", "s9"), class = "factor"), temp = c(23L, 21L,
>
> 10L, 15L, 16L, 8L, 13L, 1L, 23L, 19L, 25L, 19L, 12L, 16L, 19L,
>
> 21L, 12L, 5L, 7L), group = structure(c(1L, 1L, 1L, 2L, 2L, 2L,
>
> 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), .Label = c("A",
>
> "B", "C"), class = "factor")), .Names = c("RegionA", "site",
>
> "temp", "group"), class = "data.frame", row.names = c(NA, -19L
>
> ))
>
>
> Here group A has 3 rows,
>
> B had 7 rows, and
>
> C has 9 rows
>
>
> I want to select rows randomly (with replacement) for each group but each
> group should have same number of rows as above (group A=3 rows, B=7 rows,
> and
> c=9 rows). This way I want to create at least 10 tables (10 random tables) and
> save them separately in Excel.
>
>
> Thanks
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From crosspide at hotmail.com  Wed Oct 18 11:32:15 2017
From: crosspide at hotmail.com (Cristina Pascual)
Date: Wed, 18 Oct 2017 09:32:15 +0000
Subject: [R] conformal predictions
Message-ID: <HE1PR0201MB2348E3EFC69EC470DB0F6DE0A94D0@HE1PR0201MB2348.eurprd02.prod.outlook.com>

Dear community,


I want to use conformal prediction to measure how good are my svm-regression predictions  (having used library e1071).


I have gone through the conformal package documentation. But I'm pretty newer and need some more and detailed examples.


Can anybody help me?

Thanks in advance,


	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Wed Oct 18 16:15:11 2017
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 18 Oct 2017 14:15:11 +0000
Subject: [R] creating tables with replacement
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3950@SRVEXCHCM301.precheza.cz>
References: <CAMwU6B2_gCeNTcbxhUTmZNPpH0DWV3MRCcXU0YTdnrUMiRpomQ@mail.gmail.com>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3950@SRVEXCHCM301.precheza.cz>
Message-ID: <fa7c255c8b9d42bb9cc074061bdc9f38@exch-2p-mbx-w2.ads.tamu.edu>

Building on Petr's suggestion, you could modify his code to get all 10 samples at once in a compact format:

> Samples <- lapply(lll, function(x) replicate(10, sample(x, rep=TRUE)))
# Samples is a list containing 3 matrices, one for each group
# Each column gives the index (row) numbers for a particular sample
> str(Samples)
List of 3
 $ A: int [1:3, 1:10] 2 1 3 1 1 3 2 3 1 2 ...
 $ B: int [1:7, 1:10] 8 5 8 6 5 8 10 6 4 7 ...
 $ C: int [1:9, 1:10] 14 15 19 13 18 19 19 12 13 16 ...
# Row numbers for all 10 samples of 3 for group A
> Samples$A
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    2    1    2    2    3    3    2    3    2     3
[2,]    1    1    3    1    3    3    2    3    3     3
[3,]    3    3    1    2    2    1    2    3    3     1
# The first sample from group A
> dat1[Samples$A[ , 1], ]
  RegionA site temp group
2      Ra   s2   21     A
1      Ra   s1   23     A
3      Rb   s3   10     A

# Save the first sample from group A as a .csv file
> write.csv(dat1[Samples$A[ , 1], ], row.names=FALSE, file="Test.csv")

# Save all ten group A samples as a single .csv file
# Faster but you have to split them into separate tables by hand
> write.csv(dat1[Samples$A[ , 1:10], ], row.names=FALSE, file="Test.csv")

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of PIKAL Petr
Sent: Wednesday, October 18, 2017 3:38 AM
To: Marna Wagley <marna.wagley at gmail.com>; r-help mailing list <r-help at r-project.org>
Subject: Re: [R] creating tables with replacement

Hi

maybe there is another more elegant solution but something like this

> idx <- 1:nrow(dat1)
> lll <- split(idx, dat1$group)
> dat1[unlist(lapply(lll, sample, rep=TRUE)),]

gives you selected rows.

You could use for cycle or save those data frames manually

Cheers

Petr
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Marna 
> Wagley
> Sent: Wednesday, October 18, 2017 9:46 AM
> To: r-help mailing list <r-help at r-project.org>
> Subject: [R] creating tables with replacement
>
> Hi R User,
> I am new in R and trying to create tables with selecting rows randomly 
> (but with replacement) for each group but each group should have same 
> number as original. Is it possible to create it using the following example data set?
>
> Your help is highly appreciated.
>
> dat1<-structure(list(RegionA = structure(c(1L, 1L, 2L, 3L, 3L, 4L,
>
> 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L), .Label = c("Ra",
>
> "Rb", "Rc", "Rd", "Re", "Rf"), class = "factor"), site = 
> structure(c(1L,
>
> 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 2L, 3L, 4L, 5L, 6L, 7L,
>
> 8L, 9L, 10L, 11L), .Label = c("s1", "s10", "s11", "s12", "s13",
>
> "s14", "s15", "s16", "s17", "s18", "s19", "s2", "s3", "s4", "s5",
>
> "s6", "s7", "s8", "s9"), class = "factor"), temp = c(23L, 21L,
>
> 10L, 15L, 16L, 8L, 13L, 1L, 23L, 19L, 25L, 19L, 12L, 16L, 19L,
>
> 21L, 12L, 5L, 7L), group = structure(c(1L, 1L, 1L, 2L, 2L, 2L,
>
> 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), .Label = c("A",
>
> "B", "C"), class = "factor")), .Names = c("RegionA", "site",
>
> "temp", "group"), class = "data.frame", row.names = c(NA, -19L
>
> ))
>
>
> Here group A has 3 rows,
>
> B had 7 rows, and
>
> C has 9 rows
>
>
> I want to select rows randomly (with replacement) for each group but 
> each group should have same number of rows as above (group A=3 rows, 
> B=7 rows, and
> c=9 rows). This way I want to create at least 10 tables (10 random 
> tables) and save them separately in Excel.
>
>
> Thanks
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From ericjberger at gmail.com  Wed Oct 18 16:54:49 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Wed, 18 Oct 2017 17:54:49 +0300
Subject: [R] Problem with tq_mutate_xy() from the tidyquant package
Message-ID: <CAGgJW76dta1OjUUXzSZerG_43rHbV9pJ6_1=b=pGKB+zaK+7Sg@mail.gmail.com>

I was able to reproduce the problem with this self-contained example. Maybe
it could be reproduced with an even smaller one ...


library(tidyquant)  # Loads tidyverse, tidyquant, financial pkgs, xts/zoo
library(xts)

dtV <- as.Date("2017-01-01") + 1:100
locL <- list( foo=xts(rnorm(100), order.by=dtV), bar=xts(rnorm(100),
order.by=dtV) )
fullXts <- do.call(merge,locL)
smallXts <- fullXts["2017-02-01::"]

rolling_corrOnePair <- function( aXts, col1, col2, window ) {  #window =
window size in days
  tbl  <- timetk::tk_tbl( aXts[,c(col1,col2)], rename_index="date" )
  colnames(tbl) <- c("date","x","y")
  naV <- sapply(1:nrow(tbl), function(i) any(is.na(tbl[i,])) )
  tbl <- tbl[!naV,]# remove any rows with NAs
   tq_mutate_xy( data=tbl, x = x, y = y, mutate_fun=runCor,
                n = window, use="all.obs",  # runCor args
                col_rename = "rolling_corr" ) # tq_mutate args
}

foo <- rolling_corrOnePair( aXts=smallXts, col1="foo", col2="bar",
window=30 )

## This produces the error
# Error in runCov(x, y, n, use = use, sample = sample, cumulative ) :
#   n = 30 is outside valid range: [1, 1]

Thanks for any help,

Eric

	[[alternative HTML version deleted]]


From marna.wagley at gmail.com  Wed Oct 18 18:54:39 2017
From: marna.wagley at gmail.com (Marna Wagley)
Date: Wed, 18 Oct 2017 09:54:39 -0700
Subject: [R] creating tables with replacement
In-Reply-To: <fa7c255c8b9d42bb9cc074061bdc9f38@exch-2p-mbx-w2.ads.tamu.edu>
References: <CAMwU6B2_gCeNTcbxhUTmZNPpH0DWV3MRCcXU0YTdnrUMiRpomQ@mail.gmail.com>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3950@SRVEXCHCM301.precheza.cz>
 <fa7c255c8b9d42bb9cc074061bdc9f38@exch-2p-mbx-w2.ads.tamu.edu>
Message-ID: <CAMwU6B1VbdzU3gbGFZaP4R1R6eFeetV5RdPmkvbc4B6E8uODig@mail.gmail.com>

Dear David and Petr,
It worked and I think now I can modify the code little bit to fit my
requirement. Thank you so much for your help.

Thanks,

MW

On Wed, Oct 18, 2017 at 7:15 AM, David L Carlson <dcarlson at tamu.edu> wrote:

> Building on Petr's suggestion, you could modify his code to get all 10
> samples at once in a compact format:
>
> > Samples <- lapply(lll, function(x) replicate(10, sample(x, rep=TRUE)))
> # Samples is a list containing 3 matrices, one for each group
> # Each column gives the index (row) numbers for a particular sample
> > str(Samples)
> List of 3
>  $ A: int [1:3, 1:10] 2 1 3 1 1 3 2 3 1 2 ...
>  $ B: int [1:7, 1:10] 8 5 8 6 5 8 10 6 4 7 ...
>  $ C: int [1:9, 1:10] 14 15 19 13 18 19 19 12 13 16 ...
> # Row numbers for all 10 samples of 3 for group A
> > Samples$A
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
> [1,]    2    1    2    2    3    3    2    3    2     3
> [2,]    1    1    3    1    3    3    2    3    3     3
> [3,]    3    3    1    2    2    1    2    3    3     1
> # The first sample from group A
> > dat1[Samples$A[ , 1], ]
>   RegionA site temp group
> 2      Ra   s2   21     A
> 1      Ra   s1   23     A
> 3      Rb   s3   10     A
>
> # Save the first sample from group A as a .csv file
> > write.csv(dat1[Samples$A[ , 1], ], row.names=FALSE, file="Test.csv")
>
> # Save all ten group A samples as a single .csv file
> # Faster but you have to split them into separate tables by hand
> > write.csv(dat1[Samples$A[ , 1:10], ], row.names=FALSE, file="Test.csv")
>
> ----------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77843-4352
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of PIKAL Petr
> Sent: Wednesday, October 18, 2017 3:38 AM
> To: Marna Wagley <marna.wagley at gmail.com>; r-help mailing list <
> r-help at r-project.org>
> Subject: Re: [R] creating tables with replacement
>
> Hi
>
> maybe there is another more elegant solution but something like this
>
> > idx <- 1:nrow(dat1)
> > lll <- split(idx, dat1$group)
> > dat1[unlist(lapply(lll, sample, rep=TRUE)),]
>
> gives you selected rows.
>
> You could use for cycle or save those data frames manually
>
> Cheers
>
> Petr
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Marna
> > Wagley
> > Sent: Wednesday, October 18, 2017 9:46 AM
> > To: r-help mailing list <r-help at r-project.org>
> > Subject: [R] creating tables with replacement
> >
> > Hi R User,
> > I am new in R and trying to create tables with selecting rows randomly
> > (but with replacement) for each group but each group should have same
> > number as original. Is it possible to create it using the following
> example data set?
> >
> > Your help is highly appreciated.
> >
> > dat1<-structure(list(RegionA = structure(c(1L, 1L, 2L, 3L, 3L, 4L,
> >
> > 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L), .Label = c("Ra",
> >
> > "Rb", "Rc", "Rd", "Re", "Rf"), class = "factor"), site =
> > structure(c(1L,
> >
> > 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 2L, 3L, 4L, 5L, 6L, 7L,
> >
> > 8L, 9L, 10L, 11L), .Label = c("s1", "s10", "s11", "s12", "s13",
> >
> > "s14", "s15", "s16", "s17", "s18", "s19", "s2", "s3", "s4", "s5",
> >
> > "s6", "s7", "s8", "s9"), class = "factor"), temp = c(23L, 21L,
> >
> > 10L, 15L, 16L, 8L, 13L, 1L, 23L, 19L, 25L, 19L, 12L, 16L, 19L,
> >
> > 21L, 12L, 5L, 7L), group = structure(c(1L, 1L, 1L, 2L, 2L, 2L,
> >
> > 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), .Label = c("A",
> >
> > "B", "C"), class = "factor")), .Names = c("RegionA", "site",
> >
> > "temp", "group"), class = "data.frame", row.names = c(NA, -19L
> >
> > ))
> >
> >
> > Here group A has 3 rows,
> >
> > B had 7 rows, and
> >
> > C has 9 rows
> >
> >
> > I want to select rows randomly (with replacement) for each group but
> > each group should have same number of rows as above (group A=3 rows,
> > B=7 rows, and
> > c=9 rows). This way I want to create at least 10 tables (10 random
> > tables) and save them separately in Excel.
> >
> >
> > Thanks
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jun.yan at uconn.edu  Wed Oct 18 12:37:19 2017
From: jun.yan at uconn.edu (Yan, Jun)
Date: Wed, 18 Oct 2017 10:37:19 +0000
Subject: [R] 2018 ASA Computing/Graphics: Chambers Software Award and
 Student Paper Competition
Message-ID: <BN6PR05MB3267B377502548C00B3BF21BF04D0@BN6PR05MB3267.namprd05.prod.outlook.com>

Dear R-help Listers,


The following two student competitions are of interests to the now many student R package developers. I'd appreciate your help in spreading them.


#1. John M. Chambers Statistical Software Award 2018

The Statistical Computing Section of the American Statistical
Association announces the competition for the John M. Chambers
Statistical Software Award. In 1998 the Association for Computing
Machinery (ACM) presented the ACM Software System Award to John
Chambers for the design and development of S. Dr. Chambers generously
donated his award to the Statistical Computing Section to endow an
annual prize for statistical software written by, or in collaboration
with, an undergraduate or graduate student. The prize carries with it
a cash award, which has been increased from $1,000 to $2,000 starting
from 2018. See http://stat-computing.org/awards/jmc/history.html for

the history of the award.

Both individuals and teams are eligible to participate in the
competition. To be eligible, at least one individual within the team
must have begun the development while a student and must either
currently be a student, or have completed all requirements for her/his
last degree after January 1, 2017. The award will be given to the
student, or split between student team members if the team consists of
multiple students, up to a maximum of three students. If the software
was created by a team, the contribution of the student(s) must be
substantial.

To apply for the award, teams must provide the following materials:

Current CVs of all team members.

A letter from a faculty mentor at the academic institution of one of
the students. The letter should confirm that the student had
substantial participation in the development of the software, certify
her/his student status when the software began to be developed,
confirm that he/she is still a student (or provide a date of degree
completion), and briefly discuss the importance of the software to
statistical practice.

A brief, one to two page description of the software, summarizing what
it does, how it does it, and why it is an important contribution. If
any student team member has continued developing the software after
finishing her/his studies, the description should indicate what was
developed when the individual was a student and what has been added
since.

An installable software package with its source code for use by the
award committee. It should be accompanied by enough information to
allow the judges to effectively use and evaluate the software
(including its design considerations). This information can be
provided in a variety of ways, including but not limited to: a user
manual, a manuscript, a URL, and online help to the system.

All materials must be in English. We prefer that electronic text be
submitted as PDF files. The entries will be judged on a variety of
dimensions, including the importance and relevance for statistical
practice of the tasks performed by the software, ease of use, clarity
of description, elegance and availability for use by the statistical
community. Preference will be given to those entries that are grounded
in software design rather than calculation. The decision of the award
committee is final.

All application materials MUST BE RECEIVED by 5:00pm EST, Friday,
December 15, 2017. The submission window will be open at
http://asa.stat.uconn.edu on December 1, 2017. Questions are to be


emailed to Professor Jun Yan.

#2. Student Paper Competition 2018

The Statistical Computing and Statistical Graphics Sections of the ASA
are co-sponsoring a student paper competition on the topics of
Statistical Computing and Statistical Graphics. Students are
encouraged to submit a paper in one of these areas, which might be
original methodological research, some novel computing or graphical
application in statistics, or any other suitable contribution (for
example, a software-related project). The selected winners will
present their papers in a topic-contributed session at the 2017 Joint
Statistical Meetings. The prize carries with it a cash award of
$1,000.

Anyone who is a student (graduate or undergraduate) on or after
September 1, 2017 is eligible to participate. An entry must include an
abstract, a six page manuscript (including figures, tables and
references; a two-column format is acceptable), blinded versions of
the abstract and manuscript (with no author names or other information
that easily identifies the authors), a CV, and a letter from a faculty
member familiar with the student's work. The applicant must be the
first author of the paper. The faculty letter must include a
verification of the applicant's student status and, in the case of
joint authorship, should indicate what fraction of the contribution is
attributable to the applicant. We prefer that electronic submissions
of papers consist of PDF files. All materials must be in English.

Students may submit papers to no more than two sections and may accept
only one section's award. Students must inform both sections applied
to when he or she wins and accepts an award, thereby removing the
student from the award competition for the second section.

All application materials MUST BE RECEIVED by 5:00 PM EST, Friday,
December 15, 2017. The submission window will be open at
http://asa.stat.uconn.edu on December 1, 2017. They will be reviewed

by the Student Paper Competition Award committee of the Statistical
Computing and Graphics Sections. The selection criteria used by the
committee will include innovation and significance of the contribution
as well as the professional quality of the manuscript. Award
announcements will be made by January 15th, 2018.

Additional important information on the competition may be found at
http://stat-computing.org/awards/student/faq.html and, for all ASA
sponsored student paper competitions Additional important information
on the competition may be found at
http://stat-computing.org/awards/student/faq.html and, for all ASA

sponsored student paper competitions, at
http://amstat.org/ASA/Your-Career/Student-Paper-Competitions.aspx?hkey=6481db83-6316-44b1-be71-d4796b76f583.

Jun Yan

Department of Statistics

University of Connecticut

jun.yan at uconn.edu


	[[alternative HTML version deleted]]


From Wade.A.Wall at erdc.dren.mil  Wed Oct 18 17:58:11 2017
From: Wade.A.Wall at erdc.dren.mil (Wall, Wade A ERDC-RDE-CERL-IL CIV)
Date: Wed, 18 Oct 2017 15:58:11 +0000
Subject: [R] Error messages using nonlinear regression function (nls)
Message-ID: <C1524BE4BF45454293B8DF38FB9B5ECAE9E91129@MS-EX1VKS.erdc.dren.mil>

Hi all,

I am trying to use nonlinear regression (nls) to analyze some seed germination data, but am having problems with error codes.

The data that I have closely matches the germination dataset included in the drc package.

Here is the head of the data

  temp species start end germinated TotSeeds TotGerminated Prop
1   10   wheat     0   1          0       20             0  0.0
2   10   wheat     1   2          0       20             0  0.0
3   10   wheat     2   3          0       20             0  0.0
4   10   wheat     3   4          0       20             0  0.0
5   10   wheat     4   5          0       20             0  0.0
6   10   wheat     5   6          4       20             4  0.2

temp is the temperature under which the seeds were germinated, species denotes the species (wheat, mungbean, or rice)
Start and end denote the beginning and end of a period of time and germinated denotes how many seeds germinated during
that period of time. Prop represents the proportion of seeds that have germinated.

I have attempted to mimic Fox and Weisberg's appendix to Nonlinear Regression found here.
https://socserv.socsci.mcmaster.ca/jfox/Books/Companion/appendix/Appendix-Nonlinear-Regression.pdf

My first step is to look at a single species, wheat, and use nls on the individual temperatures.

I have tried to use both the nlsList function and to attempt to estimate the parameters using lm(), but I
receive error messages on both. Here is the code.

library(drc) ### for germination dataset
data(germination)

wheat = germination[germination$species == "wheat",] ### subset by wheat
scatterplot(Prop ~ end|temp,data=wheat,box=FALSE,reg=FALSE) ### view the data
wheat$temp = as.factor(wheat$temp) ### convert to factor

### First, try to use nlsList
wheat.list <- nlsList(Prop ~ SSlogis(end,phi1,phi2,phi3)| temp,pool=FALSE,data=wheat) ### 

### next, try to use lm to estimate starting parameters.
wheat.list = list()

for (i in 1:length(levels(wheat$temp))){
  tmpDat = wheat[wheat$temp == levels(wheat$temp)[i],]
  tmp.lm <- lm(Prop ~ end,data = tmpDat)
  tmp.nls <- nls(Prop ~ theta1 / (1 + exp(-(theta2 + theta3*end))),
      start = list(theta1 = .5,theta2=coef(tmp.lm)[1],theta3 = coef(tmp.lm)[2]),
      data = tmpDat,trace=TRUE)
  tmp2.nls <- nls(Prop ~ SSlogis(end,phi1,phi2,phi3),data=tmpDat)
  wheat.list[i] <- tmp.nls
}
#### End code
nlsList just returns an empty list.

When I try to loop through the individual temperatures, for the first temperature, nls converges when I provide 
starting values, but when I try to use SSlogis(), I get the error messsage

Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : 
  NA/NaN/Inf in 'x'

For the second temperature (16), I get the following error message when provided initial values using lm:

Error in nls(Prop ~ theta1/(1 + exp(-(theta2 + theta3 * end))), start = list(theta1 = 0.5,  : 
  singular gradient
  
I have tried to read through posts, but none of them seem to apply to this case. The data seem relatively simply and 
I am not sure what I am doing wrong. Any help would be appreciated.

Wade


From roy.mendelssohn at noaa.gov  Wed Oct 18 19:43:06 2017
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Wed, 18 Oct 2017 10:43:06 -0700
Subject: [R] dygraphs, multiple graphs and shiny
Message-ID: <1D879106-8771-42F0-9195-14554E595947@noaa.gov>

Hi All:

This is really getting into the weeds,  but I am hoping someone will have a solution.  I am trying to use dygrahs for R, within Shiny.

The situation arises when I am combining a number of dygraphs into one plot.  If I am just in an RNotebook, if you look at:

https://stackoverflow.com/questions/30509866/for-loop-over-dygraph-does-not-work-in-r

the solution to have the plot shown from a RNotebook is code like this:

> library(dygraphs)
> lungDeaths <- cbind(mdeaths, fdeaths)
> res <- lapply(1:2, function(i) dygraph(lungDeaths[, i]))
> htmltools::tagList(res)

and if you put that into an RNotebook and knit it, it works.  Okay in order to have a reproducible example,  I now try to create a Shiny App using that example,  and the template generated by RStudio.  To use dygraphs in Shiny,  you replace the normal render and plot routines,  such that the following "works"  in the sense when run the graph and slider are shown - this is showing just a single digraph:

> library(shiny)
> 
> # Define UI for application that draws a histogram
> ui <- fluidPage(
>    
>    # Application title
>    titlePanel("Test"),
>    
>    # Sidebar with a slider input for number of bins 
>    sidebarLayout(
>       sidebarPanel(
>          sliderInput("bins",
>                      "Number of bins:",
>                      min = 1,
>                      max = 50,
>                      value = 30)
>       ),
>       
>       # Show a plot of the generated distribution
>       mainPanel(
>         dygraphs::dygraphOutput("distPlot")
>       )
>    )
> )
> 
> # Define server logic required to draw a histogram
> server <- function(input, output) {
>    
>    output$distPlot <- dygraphs::renderDygraph({
>      lungDeaths <- cbind(mdeaths, fdeaths)
>      res <- lapply(1:2, function(i) dygraph(lungDeaths[, i]))
>      dygraph(lungDeaths[, 1])
>    })
> }
> 
> # Run the application 
> shinyApp(ui = ui, server = server)
> 


Now make the single change to try and render the combined plot:

> library(shiny)
> 
> # Define UI for application that draws a histogram
> ui <- fluidPage(
>    
>    # Application title
>    titlePanel("Test"),
>    
>    # Sidebar with a slider input for number of bins 
>    sidebarLayout(
>       sidebarPanel(
>          sliderInput("bins",
>                      "Number of bins:",
>                      min = 1,
>                      max = 50,
>                      value = 30)
>       ),
>       
>       # Show a plot of the generated distribution
>       mainPanel(
>         dygraphs::dygraphOutput("distPlot")
>       )
>    )
> )
> 
> # Define server logic required to draw a histogram
> server <- function(input, output) {
>    
>    output$distPlot <- dygraphs::renderDygraph({
>      lungDeaths <- cbind(mdeaths, fdeaths)
>      res <- lapply(1:2, function(i) dygraph(lungDeaths[, i]))
>      htmltools::tagList(res)
>    })
> }
> 
> # Run the application 
> shinyApp(ui = ui, server = server)
> 



If you run the second example,  the plot does not appear.

Thanks for any help.

-Roy

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new street address***
110 McAllister Way
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From roy.mendelssohn at noaa.gov  Wed Oct 18 23:10:14 2017
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Wed, 18 Oct 2017 14:10:14 -0700
Subject: [R] dygraphs, multiple graphs and shiny
In-Reply-To: <1D879106-8771-42F0-9195-14554E595947@noaa.gov>
References: <1D879106-8771-42F0-9195-14554E595947@noaa.gov>
Message-ID: <72C8687C-1F18-4FA4-AC41-1173BE07EBE0@noaa.gov>

Answering my own question.  It took a lot of trial and error,  but the code below will work.  The trick is to do form the lis to plots, create the html tag, and use renderUI() for that, and then in the UI.R part use  htmlOutput() to output the result.

-Roy


> library(shiny)
> 
> # Define UI for application that draws a histogram
> ui <- fluidPage(
>    
>    # Application title
>    titlePanel("Test"),
>    
>    # Sidebar with a slider input for number of bins 
>    sidebarLayout(
>       sidebarPanel(
>          sliderInput("bins",
>                      "Number of bins:",
>                      min = 1,
>                      max = 50,
>                      value = 30)
>       ),
>       
>       # Show a plot of the generated distribution
>       mainPanel(
>         #dygraphs::dygraphOutput("distPlot")
>         htmlOutput("distPlot")
>       )
>    )
> )
> 
> # Define server logic required to draw a histogram
> server <- function(input, output) {
>   lungDeaths <- cbind(mdeaths, fdeaths)
>  # output$distPlot <- dygraphs::renderDygraph({
>         res = list()
>         res[[1]] <-  dygraph(lungDeaths[, 1], group = 'lungs') %>% dyRangeSelector()
>         res[[2]] <-  dygraph(lungDeaths[, 1], group = 'lungs') %>% dyRangeSelector()
>        res <- htmltools::tagList(res)
>        output$distPlot <- renderUI({
>         res
>        })
> }
> 
> # Run the application 
> shinyApp(ui = ui, server = server)
> 




> On Oct 18, 2017, at 10:43 AM, Roy Mendelssohn - NOAA Federal <roy.mendelssohn at noaa.gov> wrote:
> 
> Hi All:
> 
> This is really getting into the weeds,  but I am hoping someone will have a solution.  I am trying to use dygrahs for R, within Shiny.
> 
> The situation arises when I am combining a number of dygraphs into one plot.  If I am just in an RNotebook, if you look at:
> 
> https://stackoverflow.com/questions/30509866/for-loop-over-dygraph-does-not-work-in-r
> 
> the solution to have the plot shown from a RNotebook is code like this:
> 
>> library(dygraphs)
>> lungDeaths <- cbind(mdeaths, fdeaths)
>> res <- lapply(1:2, function(i) dygraph(lungDeaths[, i]))
>> htmltools::tagList(res)
> 
> and if you put that into an RNotebook and knit it, it works.  Okay in order to have a reproducible example,  I now try to create a Shiny App using that example,  and the template generated by RStudio.  To use dygraphs in Shiny,  you replace the normal render and plot routines,  such that the following "works"  in the sense when run the graph and slider are shown - this is showing just a single digraph:
> 
>> library(shiny)
>> 
>> # Define UI for application that draws a histogram
>> ui <- fluidPage(
>> 
>>   # Application title
>>   titlePanel("Test"),
>> 
>>   # Sidebar with a slider input for number of bins 
>>   sidebarLayout(
>>      sidebarPanel(
>>         sliderInput("bins",
>>                     "Number of bins:",
>>                     min = 1,
>>                     max = 50,
>>                     value = 30)
>>      ),
>> 
>>      # Show a plot of the generated distribution
>>      mainPanel(
>>        dygraphs::dygraphOutput("distPlot")
>>      )
>>   )
>> )
>> 
>> # Define server logic required to draw a histogram
>> server <- function(input, output) {
>> 
>>   output$distPlot <- dygraphs::renderDygraph({
>>     lungDeaths <- cbind(mdeaths, fdeaths)
>>     res <- lapply(1:2, function(i) dygraph(lungDeaths[, i]))
>>     dygraph(lungDeaths[, 1])
>>   })
>> }
>> 
>> # Run the application 
>> shinyApp(ui = ui, server = server)
>> 
> 
> 
> Now make the single change to try and render the combined plot:
> 
>> library(shiny)
>> 
>> # Define UI for application that draws a histogram
>> ui <- fluidPage(
>> 
>>   # Application title
>>   titlePanel("Test"),
>> 
>>   # Sidebar with a slider input for number of bins 
>>   sidebarLayout(
>>      sidebarPanel(
>>         sliderInput("bins",
>>                     "Number of bins:",
>>                     min = 1,
>>                     max = 50,
>>                     value = 30)
>>      ),
>> 
>>      # Show a plot of the generated distribution
>>      mainPanel(
>>        dygraphs::dygraphOutput("distPlot")
>>      )
>>   )
>> )
>> 
>> # Define server logic required to draw a histogram
>> server <- function(input, output) {
>> 
>>   output$distPlot <- dygraphs::renderDygraph({
>>     lungDeaths <- cbind(mdeaths, fdeaths)
>>     res <- lapply(1:2, function(i) dygraph(lungDeaths[, i]))
>>     htmltools::tagList(res)
>>   })
>> }
>> 
>> # Run the application 
>> shinyApp(ui = ui, server = server)
>> 
> 
> 
> 
> If you run the second example,  the plot does not appear.
> 
> Thanks for any help.
> 
> -Roy
> 
> **********************
> "The contents of this message do not reflect any position of the U.S. Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division
> Southwest Fisheries Science Center
> ***Note new street address***
> 110 McAllister Way
> Santa Cruz, CA 95060
> Phone: (831)-420-3666
> Fax: (831) 420-3980
> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
> 
> "Old age and treachery will overcome youth and skill."
> "From those who have been given much, much will be expected" 
> "the arc of the moral universe is long, but it bends toward justice" -MLK Jr.
> 

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new street address***
110 McAllister Way
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From petr.pikal at precheza.cz  Thu Oct 19 07:55:51 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 19 Oct 2017 05:55:51 +0000
Subject: [R] ROC curve for each fold in one plot
In-Reply-To: <1534918912.1359998.1508156734033@mail.yahoo.com>
References: <1534918912.1359998.1508156734033.ref@mail.yahoo.com>
 <1534918912.1359998.1508156734033@mail.yahoo.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3C81@SRVEXCHCM301.precheza.cz>

Hi

I did not see any response but what about

par(mfrow=c(2,2))

before plotting?

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Elahe chalabi
> via R-help
> Sent: Monday, October 16, 2017 2:26 PM
> To: R-help Mailing List <r-help at r-project.org>
> Subject: [R] ROC curve for each fold in one plot
>
> Hi all,
>
> I have tried a 5 fold cross validation using caret package with random forest
> method on iris dataset as example. Then I need ROC curve for each fold:
>
>
>   > set.seed(1)
>   > train_control <- trainControl(method="cv", number=5,savePredictions =
> TRUE,classProbs = TRUE)
>   > output <- train(Species~., data=iris, trControl=train_control, method="rf")
>   > library(pROC)
>   > selectedIndices <- output$pred$Resample == "Fold1"
>   >
> plot.roc(output$pred$obs[selectedIndices],output$pred$setosa[selectedIndices
> ])  > selectedIndices <- output$pred$Resample == "Fold2"
>   >
> plot.roc(output$pred$obs[selectedIndices],output$pred$setosa[selectedIndices
> ])
>   > selectedIndices <- output$pred$Resample == "Fold3"
>   >
> plot.roc(output$pred$obs[selectedIndices],output$pred$setosa[selectedIndices
> ])
>
> and the same for Fold4 and Fold5,now how can I bring all the plots in one plot
> with labels for each fold?
>
> Thanks for any help!
> Elahe
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From petr.pikal at precheza.cz  Thu Oct 19 08:21:11 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 19 Oct 2017 06:21:11 +0000
Subject: [R] Error messages using nonlinear regression function (nls)
In-Reply-To: <C1524BE4BF45454293B8DF38FB9B5ECAE9E91129@MS-EX1VKS.erdc.dren.mil>
References: <C1524BE4BF45454293B8DF38FB9B5ECAE9E91129@MS-EX1VKS.erdc.dren.mil>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3C98@SRVEXCHCM301.precheza.cz>

Hi

Thanks for the code but where is Prop?

It is not a variable in germination data set so we do not know how you did the computation.

My wild guess is, that your Prop do not follow logistic curve and therefore no results from nlsList

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Wall, Wade A
> ERDC-RDE-CERL-IL CIV
> Sent: Wednesday, October 18, 2017 5:58 PM
> To: r-help at r-project.org
> Subject: [R] Error messages using nonlinear regression function (nls)
>
> Hi all,
>
> I am trying to use nonlinear regression (nls) to analyze some seed germination
> data, but am having problems with error codes.
>
> The data that I have closely matches the germination dataset included in the
> drc package.
>
> Here is the head of the data
>
>   temp species start end germinated TotSeeds TotGerminated Prop
> 1   10   wheat     0   1          0       20             0  0.0
> 2   10   wheat     1   2          0       20             0  0.0
> 3   10   wheat     2   3          0       20             0  0.0
> 4   10   wheat     3   4          0       20             0  0.0
> 5   10   wheat     4   5          0       20             0  0.0
> 6   10   wheat     5   6          4       20             4  0.2
>
> temp is the temperature under which the seeds were germinated, species
> denotes the species (wheat, mungbean, or rice)
> Start and end denote the beginning and end of a period of time and
> germinated denotes how many seeds germinated during
> that period of time. Prop represents the proportion of seeds that have
> germinated.
>
> I have attempted to mimic Fox and Weisberg's appendix to Nonlinear
> Regression found here.
> https://socserv.socsci.mcmaster.ca/jfox/Books/Companion/appendix/Appendix
> -Nonlinear-Regression.pdf
>
> My first step is to look at a single species, wheat, and use nls on the individual
> temperatures.
>
> I have tried to use both the nlsList function and to attempt to estimate the
> parameters using lm(), but I
> receive error messages on both. Here is the code.
>
> library(drc) ### for germination dataset
> data(germination)
>
> wheat = germination[germination$species == "wheat",] ### subset by wheat
> scatterplot(Prop ~ end|temp,data=wheat,box=FALSE,reg=FALSE) ### view the
> data
> wheat$temp = as.factor(wheat$temp) ### convert to factor
>
> ### First, try to use nlsList
> wheat.list <- nlsList(Prop ~ SSlogis(end,phi1,phi2,phi3)|
> temp,pool=FALSE,data=wheat) ###
>
> ### next, try to use lm to estimate starting parameters.
> wheat.list = list()
>
> for (i in 1:length(levels(wheat$temp))){
>   tmpDat = wheat[wheat$temp == levels(wheat$temp)[i],]
>   tmp.lm <- lm(Prop ~ end,data = tmpDat)
>   tmp.nls <- nls(Prop ~ theta1 / (1 + exp(-(theta2 + theta3*end))),
>       start = list(theta1 = .5,theta2=coef(tmp.lm)[1],theta3 = coef(tmp.lm)[2]),
>       data = tmpDat,trace=TRUE)
>   tmp2.nls <- nls(Prop ~ SSlogis(end,phi1,phi2,phi3),data=tmpDat)
>   wheat.list[i] <- tmp.nls
> }
> #### End code
> nlsList just returns an empty list.
>
> When I try to loop through the individual temperatures, for the first
> temperature, nls converges when I provide
> starting values, but when I try to use SSlogis(), I get the error messsage
>
> Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :
>   NA/NaN/Inf in 'x'
>
> For the second temperature (16), I get the following error message when
> provided initial values using lm:
>
> Error in nls(Prop ~ theta1/(1 + exp(-(theta2 + theta3 * end))), start = list(theta1
> = 0.5,  :
>   singular gradient
>
> I have tried to read through posts, but none of them seem to apply to this case.
> The data seem relatively simply and
> I am not sure what I am doing wrong. Any help would be appreciated.
>
> Wade
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From lihelenjiang at gmail.com  Thu Oct 19 11:08:10 2017
From: lihelenjiang at gmail.com (Li Jiang)
Date: Thu, 19 Oct 2017 10:08:10 +0100
Subject: [R] looping using 'diverse' package measures
Message-ID: <CAPLk+frjdpNk_B+xmT0=C6dQ3Bw0fMGMLwv_7hHYGYy6-L35jQ@mail.gmail.com>

Hi everyone,

I'm new at R (although I'm a Stata user for some time and somehow
proficient in it) and I'm trying to use the 'diverse' R package to compute
a few diversity measures on a sample of firms for a period of about 10
years. I was wondering if you can give me some hints on how to best proceed
on using the 'diverse' package.

My sample has the following setup. It's comprised of a annual variable
number of firms which are identified by the companyid variable and the year
variable (unbalanced panel). In addition I also have a variable identifying
the worker, workerid. I then have a set of variables which i want to use as
the basis for calculating some of the measures in the 'diverse' package. An
example of the sample is as follows, using the gender variable (0 for male
and 1 for female) as the variable of interest:

companyid   year    workerid    gender
85390   1999    46446384    0
85390   1999    126800000   1
85390   1999    163300000   0
85390   1999    60225451    0
85390   1999    60195422    0
85390   2000    60225451    0
85390   2000    3571000000  1
85390   2000    163300000   0
85390   2000    163300000   0
85390   2000    126800000   0
85390   2001    60195422    0
85390   2001    60225451    1
85390   2001    46446384    0
85390   2001    60195422    0
85390   2001    60225451    0
4391076 2005    13753759    0
4391076 2005    49988911    0
4391076 2005    112400000   0
4391076 2005    185500000   0
4391076 2005    35649643    0
4391076 2005    65809705    0
4391076 2005    114200000   0
4391076 2005    192100000   0
4391076 2005    64258701    0
4391076 2005    1212000000  1

Based on the 'diverse' need to calculate for each firm, for each year, for
instance the diversity(gender) measure.  in Stata this would be obtained
just a issuing a by firm year command, but have no idea how to tackle this
is issue in R. Any ideas?

Best wishes,

Li

	[[alternative HTML version deleted]]


From f.pancotto at unimore.it  Thu Oct 19 11:30:24 2017
From: f.pancotto at unimore.it (Francesca PANCOTTO)
Date: Thu, 19 Oct 2017 11:30:24 +0200
Subject: [R] Select part of character row name in a data frame
Message-ID: <A4BF8626-7033-41A4-A4E9-5CFBE6582BE3@unimore.it>

Dear R contributors,

I have a problem in selecting in an efficient way, rows of a data frame according to a condition,
which is a part of a row name of the table.

The data frame is made of 64 rows and 2 columns, but the row names are very long but I need to select them according to a small part of it and perform calculations on the subsets.

This is the example:
											X			Y
"Unique to strat          "                                                    0.0482        28.39
"Unique to crt.dummy        "                                                  0.0441        25.92
"Unique to gender                   "                                          0.0159         9.36
"Unique to age                       "                                         0.0839        49.37
"Unique to gg_right1              "                                            0.0019         1.10
"Unique to strat:crt.dummy                 "                                   0.0689        40.54
"Common to strat, and crt.dummy         "                                     -0.0392       -23.09
"Common to strat, and gender         "                                        -0.0031        -1.84
"Common to crt.dummy, and gender     "                                         0.0038         2.21
"Common to strat, and age                 "                                    0.0072         4.21

X and Y are the two columns of variables, while ?Unique to strat?, are the row names. I am interested to select for example those rows 
whose name contains ?strat? only. It would be very easy if these names were simple, but they are not and involve also spaces.
I tried with select matches from dplyr but works for column names but I did not find how to use it on row names, which are of course character values.

Thanks for any help you can provide.

----------------------------------
Francesca Pancotto, PhD


From es at enricoschumann.net  Thu Oct 19 12:12:29 2017
From: es at enricoschumann.net (Enrico Schumann)
Date: Thu, 19 Oct 2017 12:12:29 +0200
Subject: [R] Select part of character row name in a data frame
In-Reply-To: <A4BF8626-7033-41A4-A4E9-5CFBE6582BE3@unimore.it>
Message-ID: <20171019121229.Horde.0jy3AKIEL74ZKUfe4atWSWW@webmail.your-server.de>


Quoting Francesca PANCOTTO <f.pancotto at unimore.it>:

> Dear R contributors,
>
> I have a problem in selecting in an efficient way, rows of a data  
> frame according to a condition,
> which is a part of a row name of the table.
>
> The data frame is made of 64 rows and 2 columns, but the row names  
> are very long but I need to select them according to a small part of  
> it and perform calculations on the subsets.
>
> This is the example:
> 											X			Y
> "Unique to strat          "                                           
>           0.0482        28.39
> "Unique to crt.dummy        "                                         
>           0.0441        25.92
> "Unique to gender                   "                                 
>           0.0159         9.36
> "Unique to age                       "                                
>           0.0839        49.37
> "Unique to gg_right1              "                                   
>           0.0019         1.10
> "Unique to strat:crt.dummy                 "                          
>           0.0689        40.54
> "Common to strat, and crt.dummy         "                             
>          -0.0392       -23.09
> "Common to strat, and gender         "                                
>          -0.0031        -1.84
> "Common to crt.dummy, and gender     "                                
>           0.0038         2.21
> "Common to strat, and age                 "                           
>           0.0072         4.21
>
> X and Y are the two columns of variables, while ?Unique to strat?,  
> are the row names. I am interested to select for example those rows
> whose name contains ?strat? only. It would be very easy if these  
> names were simple, but they are not and involve also spaces.
> I tried with select matches from dplyr but works for column names  
> but I did not find how to use it on row names, which are of course  
> character values.
>
> Thanks for any help you can provide.
>
> ----------------------------------
> Francesca Pancotto, PhD
>

Use ?grep or ?grepl:

     df[grep("strat", row.names(df)), ]

(in which 'df' is your data frame)


-- 
Enrico Schumann
Lucerne, Switzerland
http://enricoschumann.net


From f.pancotto at unimore.it  Thu Oct 19 12:14:53 2017
From: f.pancotto at unimore.it (Francesca PANCOTTO)
Date: Thu, 19 Oct 2017 12:14:53 +0200
Subject: [R] Select part of character row name in a data frame
In-Reply-To: <20171019121229.Horde.0jy3AKIEL74ZKUfe4atWSWW@webmail.your-server.de>
References: <20171019121229.Horde.0jy3AKIEL74ZKUfe4atWSWW@webmail.your-server.de>
Message-ID: <581F475A-1289-4F14-BB94-F190F492303A@unimore.it>

Thanks a lot, so simple so efficient!

I will study more the grep command I did not know.

Thanks!


Francesca Pancotto

> Il giorno 19 ott 2017, alle ore 12:12, Enrico Schumann <es at enricoschumann.net> ha scritto:
> 
>  df[grep("strat", row.names(df)), ]


	[[alternative HTML version deleted]]


From lihelenjiang at gmail.com  Thu Oct 19 12:29:24 2017
From: lihelenjiang at gmail.com (Li Jiang)
Date: Thu, 19 Oct 2017 11:29:24 +0100
Subject: [R] looping using 'diverse' package measures
Message-ID: <CAPLk+fo=4=hbvTe2Ocn+3z-kOx8aHzuB8gOTzSXVD=odRtXdhQ@mail.gmail.com>

Hi everyone,

I'm new at R (although I'm a Stata user for some time and somehow
proficient in it) and I'm trying to use the 'diverse' R package to compute
a few diversity measures on a sample of firms for a period of about 10
years. I was wondering if you can give me some hints on how to best proceed
on using the 'diverse' package.

My sample has the following setup. It's comprised of a annual variable
number of firms which are identified by the companyid variable and the year
variable (unbalanced panel). In addition I also have a variable identifying
the worker, workerid. I then have a set of variables which i want to use as
the basis for calculating some of the measures in the 'diverse' package. An
example of the sample is as follows, using the gender variable (0 for male
and 1 for female) as the variable of interest:

companyid   year    workerid    gender
85390   1999    46446384    0
85390   1999    126800000   1
85390   1999    163300000   0
85390   1999    60225451    0
85390   1999    60195422    0
85390   2000    60225451    0
85390   2000    3571000000  1
85390   2000    163300000   0
85390   2000    163300000   0
85390   2000    126800000   0
85390   2001    60195422    0
85390   2001    60225451    1
85390   2001    46446384    0
85390   2001    60195422    0
85390   2001    60225451    0
4391076 2005    13753759    0
4391076 2005    49988911    0
4391076 2005    112400000   0
4391076 2005    185500000   0
4391076 2005    35649643    0
4391076 2005    65809705    0
4391076 2005    114200000   0
4391076 2005    192100000   0
4391076 2005    64258701    0
4391076 2005    1212000000  1

Based on the 'diverse' need to calculate for each firm, for each year, for
instance the diversity(gender) measure.  in Stata this would be obtained
just a issuing a by firm year command, but have no idea how to tackle this
is issue in R. Any ideas?

Best wishes,

Li

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Thu Oct 19 12:50:38 2017
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Thu, 19 Oct 2017 11:50:38 +0100
Subject: [R] looping using 'diverse' package measures
In-Reply-To: <CAPLk+fo=4=hbvTe2Ocn+3z-kOx8aHzuB8gOTzSXVD=odRtXdhQ@mail.gmail.com>
References: <CAPLk+fo=4=hbvTe2Ocn+3z-kOx8aHzuB8gOTzSXVD=odRtXdhQ@mail.gmail.com>
Message-ID: <bf87c054-1fb7-6001-4951-06b32602d0c3@dewey.myzen.co.uk>

Dear Li

Not absolutely sure what you want to do but try
?aggregate
?by
?apply (or one of the other apply functions, possibly tapply


On 19/10/2017 11:29, Li Jiang wrote:
> Hi everyone,
> 
> I'm new at R (although I'm a Stata user for some time and somehow
> proficient in it) and I'm trying to use the 'diverse' R package to compute
> a few diversity measures on a sample of firms for a period of about 10
> years. I was wondering if you can give me some hints on how to best proceed
> on using the 'diverse' package.
> 
> My sample has the following setup. It's comprised of a annual variable
> number of firms which are identified by the companyid variable and the year
> variable (unbalanced panel). In addition I also have a variable identifying
> the worker, workerid. I then have a set of variables which i want to use as
> the basis for calculating some of the measures in the 'diverse' package. An
> example of the sample is as follows, using the gender variable (0 for male
> and 1 for female) as the variable of interest:
> 
> companyid   year    workerid    gender
> 85390   1999    46446384    0
> 85390   1999    126800000   1
> 85390   1999    163300000   0
> 85390   1999    60225451    0
> 85390   1999    60195422    0
> 85390   2000    60225451    0
> 85390   2000    3571000000  1
> 85390   2000    163300000   0
> 85390   2000    163300000   0
> 85390   2000    126800000   0
> 85390   2001    60195422    0
> 85390   2001    60225451    1
> 85390   2001    46446384    0
> 85390   2001    60195422    0
> 85390   2001    60225451    0
> 4391076 2005    13753759    0
> 4391076 2005    49988911    0
> 4391076 2005    112400000   0
> 4391076 2005    185500000   0
> 4391076 2005    35649643    0
> 4391076 2005    65809705    0
> 4391076 2005    114200000   0
> 4391076 2005    192100000   0
> 4391076 2005    64258701    0
> 4391076 2005    1212000000  1
> 
> Based on the 'diverse' need to calculate for each firm, for each year, for
> instance the diversity(gender) measure.  in Stata this would be obtained
> just a issuing a by firm year command, but have no idea how to tackle this
> is issue in R. Any ideas?
> 
> Best wishes,
> 
> Li
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ---
> This email has been checked for viruses by AVG.
> http://www.avg.com
> 
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From dcarlson at tamu.edu  Thu Oct 19 15:35:26 2017
From: dcarlson at tamu.edu (David L Carlson)
Date: Thu, 19 Oct 2017 13:35:26 +0000
Subject: [R] looping using 'diverse' package measures
In-Reply-To: <CAPLk+frjdpNk_B+xmT0=C6dQ3Bw0fMGMLwv_7hHYGYy6-L35jQ@mail.gmail.com>
References: <CAPLk+frjdpNk_B+xmT0=C6dQ3Bw0fMGMLwv_7hHYGYy6-L35jQ@mail.gmail.com>
Message-ID: <1c30b3d31bb04a819646feb1931c0896@exch-2p-mbx-w2.ads.tamu.edu>

You really need to spend some time learning the basics of R. There are thousands of R packages, so you also need to spend time reading the documentation for the package so that you can show us what the data format should be like. Here are some simple ways to transform the data. You should also use dput() to include your data in your email, not just a listing which can remove important information about the structure of the original data:

> Example <- structure(list(companyid = c(85390L, 85390L, 85390L, 85390L, 
85390L, 85390L, 85390L, 85390L, 85390L, 85390L, 85390L, 85390L, 
85390L, 85390L, 85390L, 4391076L, 4391076L, 4391076L, 4391076L, 
4391076L, 4391076L, 4391076L, 4391076L, 4391076L, 4391076L), 
    year = c(1999L, 1999L, 1999L, 1999L, 1999L, 2000L, 2000L, 
    2000L, 2000L, 2000L, 2001L, 2001L, 2001L, 2001L, 2001L, 2005L, 
    2005L, 2005L, 2005L, 2005L, 2005L, 2005L, 2005L, 2005L, 2005L
    ), workerid = c(46446384, 126800000, 163300000, 60225451, 
    60195422, 60225451, 3.571e+09, 163300000, 163300000, 126800000, 
    60195422, 60225451, 46446384, 60195422, 60225451, 13753759, 
    49988911, 112400000, 185500000, 35649643, 65809705, 114200000, 
    192100000, 64258701, 1.212e+09), gender = c(0L, 1L, 0L, 0L, 
    0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
    0L, 0L, 0L, 0L, 0L, 1L)), .Names = c("companyid", "year", 
"workerid", "gender"), class = "data.frame", row.names = c(NA, 
-25L))
> aggregate(gender~companyid+year, Example, mean)
  companyid year gender
1     85390 1999    0.2
2     85390 2000    0.2
3     85390 2001    0.2
4   4391076 2005    0.1

> aggregate(gender~companyid+year, Example, table)
  companyid year gender.0 gender.1
1     85390 1999        4        1
2     85390 2000        4        1
3     85390 2001        4        1
4   4391076 2005        9        1

> x <- xtabs(~gender+companyid+year, Example)
> ftable(x, row.vars=2:3, col.vars=1)
               gender 0 1
companyid year           
85390     1999        4 1
          2000        4 1
          2001        4 1
          2005        0 0
4391076   1999        0 0
          2000        0 0
          2001        0 0
          2005        9 1

You should read these manual pages:
?dput
?aggregate
?xtabs
?ftable

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352




-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Li Jiang
Sent: Thursday, October 19, 2017 4:08 AM
To: r-help at r-project.org
Subject: [R] looping using 'diverse' package measures

Hi everyone,

I'm new at R (although I'm a Stata user for some time and somehow proficient in it) and I'm trying to use the 'diverse' R package to compute a few diversity measures on a sample of firms for a period of about 10 years. I was wondering if you can give me some hints on how to best proceed on using the 'diverse' package.

My sample has the following setup. It's comprised of a annual variable number of firms which are identified by the companyid variable and the year variable (unbalanced panel). In addition I also have a variable identifying the worker, workerid. I then have a set of variables which i want to use as the basis for calculating some of the measures in the 'diverse' package. An example of the sample is as follows, using the gender variable (0 for male and 1 for female) as the variable of interest:

companyid   year    workerid    gender
85390   1999    46446384    0
85390   1999    126800000   1
85390   1999    163300000   0
85390   1999    60225451    0
85390   1999    60195422    0
85390   2000    60225451    0
85390   2000    3571000000  1
85390   2000    163300000   0
85390   2000    163300000   0
85390   2000    126800000   0
85390   2001    60195422    0
85390   2001    60225451    1
85390   2001    46446384    0
85390   2001    60195422    0
85390   2001    60225451    0
4391076 2005    13753759    0
4391076 2005    49988911    0
4391076 2005    112400000   0
4391076 2005    185500000   0
4391076 2005    35649643    0
4391076 2005    65809705    0
4391076 2005    114200000   0
4391076 2005    192100000   0
4391076 2005    64258701    0
4391076 2005    1212000000  1

Based on the 'diverse' need to calculate for each firm, for each year, for instance the diversity(gender) measure.  in Stata this would be obtained just a issuing a by firm year command, but have no idea how to tackle this is issue in R. Any ideas?

Best wishes,

Li

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From viveksutra at gmail.com  Thu Oct 19 16:47:02 2017
From: viveksutra at gmail.com (Vivek Sutradhara)
Date: Thu, 19 Oct 2017 16:47:02 +0200
Subject: [R] overlaying points and lines on a surface3d rgl plot with axes
Message-ID: <CAHLp6SDHmy1cnKMt-JwJWuZEqnhkt2Pp0xYqEBGr+b=kCrCkuQ@mail.gmail.com>

Hi R users and experts,
I am interested in learning more about the use of 3D plots. Specifically, I
want to add points and lines to a surface plot. And get the axes and labels
plotted also. Here is what I have tried with an example data set :

library(rgl)
vol2 <- 2*volcano # Exaggerate the relief
library(reshape)
mvol2 <- melt(vol2)
str(mvol2)
# First, persp and persp3d plots do not succeed
#persp(mvol2$X1,mvol2$X2,vol2,xlab="X2",ylab="X1",zlab="value",axes=TRUE,ticktype="detailed",cex.lab=1)
persp3d(mvol2$X2,mvol2$X1,vol2,axes=TRUE)
# Error - Increasing 'x' and 'y' values expected ??
# Next, tried to get the axes with the plot3d command
plot3d(mvol2$X1,mvol2$X2,mvol2$value,type="n",xlab="x1",ylab="x2",zlab="value")
# then get the surface - why the x,z, y  sequence?
surface3d(mvol2$X1,vol2,mvol2$X2)

I get the following error message for the surface3d command :
Error in rgl.surface(x = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,  :
  Bad dimension for cols

Additionally, I want points and lines plotted. For example, the points
(1,1,200) and (51,2,232).
I don't seem to have succeeded at any single step. I would appreciate any
help that I can get.
Vivek

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Thu Oct 19 18:01:08 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 19 Oct 2017 09:01:08 -0700
Subject: [R] Select part of character row name in a data frame
In-Reply-To: <581F475A-1289-4F14-BB94-F190F492303A@unimore.it>
References: <20171019121229.Horde.0jy3AKIEL74ZKUfe4atWSWW@webmail.your-server.de>
 <581F475A-1289-4F14-BB94-F190F492303A@unimore.it>
Message-ID: <BC27AD8B-7F9B-4D77-A055-56FECA387FC7@dcn.davis.ca.us>

(Re-)read the discussion of indexing (both `[` and `[[`) and be sure to get clear on the difference between matrices and data frames in the Introduction to R document that comes with R. There are many ways to create numeric vectors, character vectors, and logical vectors that can then be used as indexes, including the straightforward way:

df[ c(
"Unique to strat          ",
"Unique to strat:crt.dummy                 ",
"Common to strat, and crt.dummy         ",
"Common to strat, and gender         ",
"Common to strat, and age                 ") ,]
-- 
Sent from my phone. Please excuse my brevity.

On October 19, 2017 3:14:53 AM PDT, Francesca PANCOTTO <f.pancotto at unimore.it> wrote:
>Thanks a lot, so simple so efficient!
>
>I will study more the grep command I did not know.
>
>Thanks!
>
>
>Francesca Pancotto
>
>> Il giorno 19 ott 2017, alle ore 12:12, Enrico Schumann
><es at enricoschumann.net> ha scritto:
>> 
>>  df[grep("strat", row.names(df)), ]
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From viveksutra at gmail.com  Thu Oct 19 20:52:07 2017
From: viveksutra at gmail.com (Vivek Sutradhara)
Date: Thu, 19 Oct 2017 20:52:07 +0200
Subject: [R] overlaying points and lines on a surface3d rgl plot with
	axes
In-Reply-To: <CAHLp6SDHmy1cnKMt-JwJWuZEqnhkt2Pp0xYqEBGr+b=kCrCkuQ@mail.gmail.com>
References: <CAHLp6SDHmy1cnKMt-JwJWuZEqnhkt2Pp0xYqEBGr+b=kCrCkuQ@mail.gmail.com>
Message-ID: <CAHLp6SB9+4FhS6xiLqJVqfURxVLJWuw-H+3SukiXAw6cgow7mA@mail.gmail.com>

Hi R-users,
I think that I have figured out what I should do. But I would welcome any
comments clarifying any of the questions that I have raised.  (I am showing
my revised code below). The questions which are still unresolved for me are
the following :
1. With the points3d or the plot3D function, the points that are plotted
land on the backside of the surface. How do I bring them to the front. I
have partially solved the problem for points and lines by making the
surface transparent with alpha=0.5. Is there a better way?
2. For a different data set, I have to rotate the surface to arrive at the
optimum theta and phi angles. Is there a way of interactively finding out
what these angles are (i.e. from the initial position). After arriving at
this optimum angle, is there then a method of plotting the points on top of
the surface?
Thanks, Vivek

The code which worked for me :
library(rgl)
vol2 <- 2*volcano # Exaggerate the relief
library(reshape)
mvol2 <- melt(vol2)
str(mvol2)
x <- 1:nrow(vol2)
y <- 1:ncol(vol2)
zlim <- range(vol2)
zlen <- zlim[2] - zlim[1] + 1
colorlut <- terrain.colors(zlen)
open3d()
persp3d(x, y, vol2, col = colorlut)
grid3d(c("x", "y+", "z"))
surface3d(x, y, vol2, color = col, back = "lines")
grid3d(c("x", "y+", "z"))
surface3d(x, y, vol2, color = col, back = "lines")

2017-10-19 16:47 GMT+02:00 Vivek Sutradhara <viveksutra at gmail.com>:

> Hi R users and experts,
> I am interested in learning more about the use of 3D plots. Specifically,
> I want to add points and lines to a surface plot. And get the axes and
> labels plotted also. Here is what I have tried with an example data set :
>
> library(rgl)
> vol2 <- 2*volcano # Exaggerate the relief
> library(reshape)
> mvol2 <- melt(vol2)
> str(mvol2)
> # First, persp and persp3d plots do not succeed
> #persp(mvol2$X1,mvol2$X2,vol2,xlab="X2",ylab="X1",zlab="
> value",axes=TRUE,ticktype="detailed",cex.lab=1)
> persp3d(mvol2$X2,mvol2$X1,vol2,axes=TRUE)
> # Error - Increasing 'x' and 'y' values expected ??
> # Next, tried to get the axes with the plot3d command
> plot3d(mvol2$X1,mvol2$X2,mvol2$value,type="n",xlab="x1"
> ,ylab="x2",zlab="value")
> # then get the surface - why the x,z, y  sequence?
> surface3d(mvol2$X1,vol2,mvol2$X2)
>
> I get the following error message for the surface3d command :
> Error in rgl.surface(x = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
> :
>   Bad dimension for cols
>
> Additionally, I want points and lines plotted. For example, the points
> (1,1,200) and (51,2,232).
> I don't seem to have succeeded at any single step. I would appreciate any
> help that I can get.
> Vivek
>

	[[alternative HTML version deleted]]


From miaojpm at gmail.com  Fri Oct 20 04:21:52 2017
From: miaojpm at gmail.com (John)
Date: Thu, 19 Oct 2017 19:21:52 -0700
Subject: [R] [FORGED] can't print ggplot with Chinese characters to pdf
	files
In-Reply-To: <7f9f2636-c3d3-40b1-8ce3-9d4b2bde3336@stat.auckland.ac.nz>
References: <CABcx46A4OL5+Ja-e0eAr=7iF2_obaCOZh4a7vHMLcqkZNpD3MQ@mail.gmail.com>
 <1daf3c68-a4ee-3755-911d-7f997528499e@stat.auckland.ac.nz>
 <CABcx46DFdnwUjdO4ciaD5aeoGSDR2UgUJwMvjWG6WkjBPJ=wrg@mail.gmail.com>
 <7f9f2636-c3d3-40b1-8ce3-9d4b2bde3336@stat.auckland.ac.nz>
Message-ID: <CABcx46BHMFzKriQP+JHZdyTG0rkL8EycYUfKkE1-iitfCtPgoQ@mail.gmail.com>

Hi,

   Following Paul's instruction, I have installed the Cairo. I tried to run
the program, and there is no error message at all. I did see the Chinese
title in the plot if I ask my RStudio to show the plot (if I type "p1"),
but the pdf file shows the plots without the Chinese titles.

library(ggplot2)
library(gridExtra)
df1<-data.frame(x=1:2, y=3:4, z=5:6)
#p1<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("test1")
#p2<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("test2")
p1<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("??")
p2<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("??")
p1<-p1+theme(text = element_text(family = "Kaiti TC"))
p2<-p2+theme(text = element_text(family = "Kaiti TC"))

p<-array(list(NA), dim=2)
p[[1]]<-p1
p[[2]]<-p2
p_series <- lapply(1:(length(p)), function(.x) p[.x][[1]])
m2 <- marrangeGrob(p_series, nrow=2, ncol=1)
#ggsave("test_plot_chinese.pdf", m2)
cairo_pdf("test_plot_chinese.pdf")
print(m2)
dev.off()




2017-10-12 19:55 GMT-07:00 Paul Murrell <paul at stat.auckland.ac.nz>:

> Hi
>
> By the looks of it, you need to install Cairo graphics ...
>
> https://www.cairographics.org/download/
>
> Paul
>
> On 13/10/17 15:48, John wrote:
>
>> Thanks, Paul. Following your solution,  I got this error message:
>>
>> Warning message:
>> In cairo_pdf("test_plot_chinese.pdf") : failed to load cairo DLL
>>
>> Is there anything else I need to install?
>>
>> Thanks,
>>
>> John
>>
>> 2017-10-12 19:24 GMT-07:00 Paul Murrell <paul at stat.auckland.ac.nz
>> <mailto:paul at stat.auckland.ac.nz>>:
>>
>>
>>     Hi
>>
>>     Instead of ...
>>
>>     ggsave("test_plot_chinese.pdf", m2)
>>
>>     ... try ...
>>
>>     cairo_pdf("test_plot_chinese.pdf")
>>     print(m2)
>>     dev.off()
>>
>>     Paul
>>
>>
>>     On 13/10/17 02:12, John wrote:
>>
>>         I install the Chinese font "Kaiti TC" on my mac, but I can't
>>         print the
>>         figures to pdf file by "marrangeGrob" command, which is in the
>>         package
>>         "gridExtra". Error message after I type "ggsave(......)" (last
>>         line of the
>>         program):
>>
>>         "Saving 7.47 x 5.15 in image
>>         Error in grid.Call.graphics(L_text, as.graphicsAnnot(x$label),
>>         x$x, x$y,  :
>>             invalid font type
>>         In addition: There were 50 or more warnings (use warnings() to
>>         see the
>>         first 50)"
>>
>>         How can I install the font so that I can see the figures on the
>>         pdf files?
>>         Some information that may be useful:
>>         (1) If I use the English ggtitle, there is no problem at all.
>>         The pdf file
>>         present the figures perfectly.
>>         (2) If I use Chinese title, I can still see the figures p1 and p2
>> on
>>         Studio, but I can't see it at the pdf file produced by ggsave.
>>         It gives an
>>         error message
>>
>>
>>
>>         rm(list=ls())
>>         library(ggplot2)
>>         library(gridExtra)
>>         df1<-data.frame(x=1:2, y=3:4, z=5:6)
>>         #p1<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("test1")
>>         #p2<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("test2")
>>         p1<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("??")
>>  #Chinese title
>>         p2<-ggplot(df1, aes(x=x, y=y))+geom_line()+ggtitle("??")
>>  #Chinese title
>>         p1<-p1+theme(text = element_text(family = "Kaiti TC"))
>>         p2<-p2+theme(text = element_text(family = "Kaiti TC"))
>>
>>         p<-array(list(NA), dim=2)
>>         p[[1]]<-p1
>>         p[[2]]<-p2
>>         p_series <- lapply(1:(length(p)), function(.x) p[.x][[1]])
>>         m2 <- marrangeGrob(p_series, nrow=2, ncol=1)
>>         ggsave("test_plot_chinese.pdf", m2)
>>
>>                  [[alternative HTML version deleted]]
>>
>>         ______________________________________________
>>         R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>>         -- To UNSUBSCRIBE and more, see
>>         https://stat.ethz.ch/mailman/listinfo/r-help
>>         <https://stat.ethz.ch/mailman/listinfo/r-help>
>>         PLEASE do read the posting guide
>>         http://www.R-project.org/posting-guide.html
>>         <http://www.R-project.org/posting-guide.html>
>>         and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>     --     Dr Paul Murrell
>>     Department of Statistics
>>     The University of Auckland
>>     Private Bag 92019
>>     Auckland
>>     New Zealand
>>     64 9 3737599 x85392
>>     paul at stat.auckland.ac.nz <mailto:paul at stat.auckland.ac.nz>
>>     http://www.stat.auckland.ac.nz/~paul/
>>     <http://www.stat.auckland.ac.nz/~paul/>
>>
>>
>>
> --
> Dr Paul Murrell
> Department of Statistics
> The University of Auckland
> Private Bag 92019
> Auckland
> New Zealand
> 64 9 3737599 x85392
> paul at stat.auckland.ac.nz
> http://www.stat.auckland.ac.nz/~paul/
>

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Fri Oct 20 08:33:36 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 20 Oct 2017 06:33:36 +0000
Subject: [R] Error messages using nonlinear regression function (nls)
In-Reply-To: <C1524BE4BF45454293B8DF38FB9B5ECAE9E967D6@MS-EX1VKS.erdc.dren.mil>
References: <C1524BE4BF45454293B8DF38FB9B5ECAE9E91129@MS-EX1VKS.erdc.dren.mil>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3C98@SRVEXCHCM301.precheza.cz>
 <C1524BE4BF45454293B8DF38FB9B5ECAE9E967D6@MS-EX1VKS.erdc.dren.mil>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3EAC@SRVEXCHCM301.precheza.cz>

Hi

Keep your messages in the list, you increase your chance to get some answer.

I changed your data to groupedData object (see below), but I did not find any problem in it.

plot(wlg)
gives reasonable picture and I am not such expert to see any problem with data. Seems to me, that something has to be wrong with nlsList function.

> wheat.list <- nlsList(Prop ~ SSlogis(end,Asym, xmid, scal), data=wlg)
Warning message:
6 times caught the same error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...): NA/NaN/Inf in 'x'

produces empty list. So maybe others could find some problem.

Cheers
Petr

> dput(wlg)
structure(list(temp = structure(c(2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L), .Label = c("40", "10", "16", "22", "28",
"34"), class = c("ordered", "factor")), species = structure(c(3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), .Label = c("mungbean",
"rice", "wheat"), class = "factor"), start = c(0, 1, 2, 3, 4,
5, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17, 0, 2, 3, 4, 5, 6, 7,
9, 10, 11, 12, 13, 14, 16, 17, 0, 2, 3, 4, 5, 6, 7, 9, 10, 11,
12, 13, 14, 16, 17, 0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13,
14, 16, 17, 0, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17,
0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17), end = c(1,
2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17, 18, 2, 3, 4,
5, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17, 18, 2, 3, 4, 5, 6, 7,
9, 10, 11, 12, 13, 14, 16, 17, 18, 1, 2, 3, 4, 5, 6, 7, 9, 10,
11, 12, 13, 14, 16, 17, 18, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12,
13, 14, 16, 17, 18, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14,
16, 17, 18), germinated = c(0L, 0L, 0L, 0L, 0L, 4L, 6L, 8L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 11L, 7L, 0L, 1L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 12L, 3L, 0L, 3L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 8L, 0L, 8L, 3L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 5L, 11L, 1L, 1L, 0L, 0L, 0L,
0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L,
2L, 0L, 0L, 0L, 0L, 0L, 0L), TotSeeds = c(20, 20, 20, 20, 20,
20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
20, 20, 20, 20, 20, 20, 20, 20), TotGerminated = c(0, 0, 0, 0,
0, 4, 10, 18, 18, 18, 18, 18, 18, 18, 18, 18, 0, 0, 11, 18, 18,
19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 0, 1, 13, 16, 16, 19,
19, 19, 19, 19, 19, 19, 19, 19, 19, 0, 0, 0, 8, 8, 16, 19, 19,
19, 19, 19, 19, 19, 19, 19, 19, 0, 5, 16, 17, 18, 18, 18, 18,
18, 19, 19, 19, 19, 19, 19, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 3,
3, 3, 3, 3, 3), Prop = c(0, 0, 0, 0, 0, 0.2, 0.5, 0.9, 0.9, 0.9,
0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0, 0, 0.55, 0.9, 0.9, 0.95, 0.95,
0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0, 0.05, 0.65,
0.8, 0.8, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
0.95, 0, 0, 0, 0.4, 0.4, 0.8, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
0.95, 0.95, 0.95, 0.95, 0, 0.25, 0.8, 0.85, 0.9, 0.9, 0.9, 0.9,
0.9, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0, 0, 0, 0, 0, 0, 0,
0, 0.05, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15)), .Names = c("temp",
"species", "start", "end", "germinated", "TotSeeds", "TotGerminated",
"Prop"), row.names = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
11L, 12L, 13L, 14L, 15L, 16L, 25L, 26L, 27L, 28L, 29L, 30L, 31L,
32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 63L, 64L, 65L, 66L, 67L,
68L, 69L, 70L, 71L, 72L, 73L, 74L, 75L, 76L, 77L, 95L, 96L, 97L,
98L, 99L, 100L, 101L, 102L, 103L, 104L, 105L, 106L, 107L, 108L,
109L, 110L, 127L, 128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L,
136L, 137L, 138L, 139L, 140L, 141L, 159L, 160L, 161L, 162L, 163L,
164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L, 172L, 173L, 174L
), class = c("nfnGroupedData", "nfGroupedData", "groupedData",
"data.frame"), formula = Prop ~ end | temp, FUN = function (x)
max(x, na.rm = TRUE), order.groups = TRUE)
>

> sessionInfo()
R Under development (unstable) (2017-07-31 r73003)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 14393)

Matrix products: default

locale:
[1] LC_COLLATE=Czech_Czech Republic.1250  LC_CTYPE=Czech_Czech Republic.1250
[3] LC_MONETARY=Czech_Czech Republic.1250 LC_NUMERIC=C
[5] LC_TIME=Czech_Czech Republic.1250

attached base packages:
[1] stats     datasets  utils     grDevices graphics  methods   base

other attached packages:
[1] nlme_3.1-131    lattice_0.20-35 fun_0.1

loaded via a namespace (and not attached):
 [1] compiler_3.5.0   colorspace_1.3-2 scales_0.4.1     lazyeval_0.2.0
 [5] plyr_1.8.4       tools_3.5.0      gtable_0.2.0     tibble_1.3.3
 [9] Rcpp_0.12.12     ggplot2_2.2.1    grid_3.5.0       rlang_0.1.1
[13] munsell_0.4.3
>

> -----Original Message-----
> From: Wall, Wade A ERDC-RDE-CERL-IL CIV
> [mailto:Wade.A.Wall at erdc.dren.mil]
> Sent: Thursday, October 19, 2017 3:18 PM
> To: PIKAL Petr <petr.pikal at precheza.cz>
> Subject: RE: Error messages using nonlinear regression function (nls)
>
> "Prop" is the proportion of seeds that have germinated from the total. Here is
> the code I used to transform the germination data set. When graphed, the
> logistic curve provides a reasonable fit.
>
> data(germination)
> germination$TotSeeds = 20
> germination$TotGerminated = 0
> tmpCount = 0
>
> for(i in 2:nrow(germination)){
>   if((germination$temp[i]!=germination$temp[i-1]) | (germination$species[i] !=
> germination$species[i-1])) {tmpCount = 0}
>   if((germination$temp[i]==germination$temp[i-1]) & (germination$species[i]
> == germination$species[i-1]))
>   {tmpCount = tmpCount + germination$germinated[i];
> germination$TotGerminated[i]=tmpCount}
> }
> germination$Prop = germination$TotGerminated/germination$TotSeeds
> germination = germination[germination$end != Inf,]
>
> Wade
>
> -----Original Message-----
> From: PIKAL Petr [mailto:petr.pikal at precheza.cz]
> Sent: Thursday, October 19, 2017 1:21 AM
> To: Wall, Wade A ERDC-RDE-CERL-IL CIV <Wade.A.Wall at erdc.dren.mil>; r-
> help at r-project.org
> Subject: RE: Error messages using nonlinear regression function (nls)
>
> Hi
>
> Thanks for the code but where is Prop?
>
> It is not a variable in germination data set so we do not know how you did the
> computation.
>
> My wild guess is, that your Prop do not follow logistic curve and therefore no
> results from nlsList
>
> Cheers
> Petr
>
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Wall,
> > Wade A ERDC-RDE-CERL-IL CIV
> > Sent: Wednesday, October 18, 2017 5:58 PM
> > To: r-help at r-project.org
> > Subject: [R] Error messages using nonlinear regression function (nls)
> >
> > Hi all,
> >
> > I am trying to use nonlinear regression (nls) to analyze some seed
> > germination data, but am having problems with error codes.
> >
> > The data that I have closely matches the germination dataset included
> > in the drc package.
> >
> > Here is the head of the data
> >
> >   temp species start end germinated TotSeeds TotGerminated Prop
> > 1   10   wheat     0   1          0       20             0  0.0
> > 2   10   wheat     1   2          0       20             0  0.0
> > 3   10   wheat     2   3          0       20             0  0.0
> > 4   10   wheat     3   4          0       20             0  0.0
> > 5   10   wheat     4   5          0       20             0  0.0
> > 6   10   wheat     5   6          4       20             4  0.2
> >
> > temp is the temperature under which the seeds were germinated, species
> > denotes the species (wheat, mungbean, or rice) Start and end denote
> > the beginning and end of a period of time and germinated denotes how
> > many seeds germinated during that period of time. Prop represents the
> > proportion of seeds that have germinated.
> >
> > I have attempted to mimic Fox and Weisberg's appendix to Nonlinear
> > Regression found here.
> > Blockedhttps://socserv.socsci.mcmaster.ca/jfox/Books/Companion/appendi
> > x/AppendixBlocked
> > -Nonlinear-Regression.pdf
> >
> > My first step is to look at a single species, wheat, and use nls on
> > the individual temperatures.
> >
> > I have tried to use both the nlsList function and to attempt to
> > estimate the parameters using lm(), but I receive error messages on
> > both. Here is the code.
> >
> > library(drc) ### for germination dataset
> > data(germination)
> >
> > wheat = germination[germination$species == "wheat",] ### subset by
> > wheat scatterplot(Prop ~ end|temp,data=wheat,box=FALSE,reg=FALSE) ###
> > view the data wheat$temp = as.factor(wheat$temp) ### convert to factor
> >
> > ### First, try to use nlsList
> > wheat.list <- nlsList(Prop ~ SSlogis(end,phi1,phi2,phi3)|
> > temp,pool=FALSE,data=wheat) ###
> >
> > ### next, try to use lm to estimate starting parameters.
> > wheat.list = list()
> >
> > for (i in 1:length(levels(wheat$temp))){
> >   tmpDat = wheat[wheat$temp == levels(wheat$temp)[i],]
> >   tmp.lm <- lm(Prop ~ end,data = tmpDat)
> >   tmp.nls <- nls(Prop ~ theta1 / (1 + exp(-(theta2 + theta3*end))),
> >       start = list(theta1 = .5,theta2=coef(tmp.lm)[1],theta3 = coef(tmp.lm)[2]),
> >       data = tmpDat,trace=TRUE)
> >   tmp2.nls <- nls(Prop ~ SSlogis(end,phi1,phi2,phi3),data=tmpDat)
> >   wheat.list[i] <- tmp.nls
> > }
> > #### End code
> > nlsList just returns an empty list.
> >
> > When I try to loop through the individual temperatures, for the first
> > temperature, nls converges when I provide starting values, but when I
> > try to use SSlogis(), I get the error messsage
> >
> > Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :
> >   NA/NaN/Inf in 'x'
> >
> > For the second temperature (16), I get the following error message
> > when provided initial values using lm:
> >
> > Error in nls(Prop ~ theta1/(1 + exp(-(theta2 + theta3 * end))), start
> > = list(theta1 = 0.5,  :
> >   singular gradient
> >
> > I have tried to read through posts, but none of them seem to apply to this
> case.
> > The data seem relatively simply and
> > I am not sure what I am doing wrong. Any help would be appreciated.
> >
> > Wade
> >

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From cdetermanjr at gmail.com  Thu Oct 19 15:53:26 2017
From: cdetermanjr at gmail.com (Charles Determan)
Date: Thu, 19 Oct 2017 08:53:26 -0500
Subject: [R] [R-pkgs] gpuR 2.0.0 released
Message-ID: <CAKxd1KMfxonkJi7kLFaaLanTKs1gWN=6+xYuNhR1xapPX3CjJg@mail.gmail.com>

Dear R users,

I am happy to announce the most recent version of gpuR has been
released. There are several new enhancements to the package including
the ability to use user written OpenCL kernels.  A full list of
changes from the NEWS are shown below.

API Changes:

1. deviceType, gpuInfo, cpuInfo not longer accepts 'platform_idx'
parameter as OpenCL contexts cannot contain more than one platform.

New Features:

1. Added functionality to create custom OpenCL functions from user
provided kernels

2. Added 'synchronize' function to assure completion of device calls
(necessary for benchmarking)

3. Added determinant function (det)

4. Allow for gpuR object - base object interaction (e.g. vclMatrix * matrix)

5. Added ?inplace' function for ?inplace' operations. These operations
include '+', '-', '*', '/', 'sin', 'asin', 'sinh', 'cos', 'acos',
'cosh', 'tan', 'atan', 'tanh'.

6. Added 'sqrt', 'sum', 'sign','pmin', and 'pmax' functions

7. Methods to pass two gpuR matrix objects to 'cov'

8. Added 'norm' method

9. Added gpuRmatrix/gpuRvector Arith '+','-' methods

Bug Fixes:

1. Fixed incorrect device info when using different contexts

2. Fixed Integer Matrix Multiplication

3. All OpenCL devices will be initialized on startup (previous version
occasionally would omit some devices)


There are many more features in the works.  Suggestions and
contributions continue to be welcomed.  Please submit all through my
github issues https://github.com/cdeterman/gpuR/issues
<https://github.com/cdeterman/gpuR.git>


Also, thanks to all those as well for testing this package on various
GPU devices and operating systems.  A lot of the stability of this
package is made possible by your efforts.


Kind regards,

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages

From f.pancotto at unimore.it  Fri Oct 20 10:17:03 2017
From: f.pancotto at unimore.it (Francesca PANCOTTO)
Date: Fri, 20 Oct 2017 10:17:03 +0200
Subject: [R] Select part of character row name in a data frame
In-Reply-To: <BC27AD8B-7F9B-4D77-A055-56FECA387FC7@dcn.davis.ca.us>
References: <20171019121229.Horde.0jy3AKIEL74ZKUfe4atWSWW@webmail.your-server.de>
 <581F475A-1289-4F14-BB94-F190F492303A@unimore.it>
 <BC27AD8B-7F9B-4D77-A055-56FECA387FC7@dcn.davis.ca.us>
Message-ID: <D823E9D6-D82D-49EE-8F00-2219E94F0B14@unimore.it>

I did not need to select the whole character sentence, otherwise I would know how to do it.. from basic introduction to R as you suggest.
Grep works perfectly.

f.

----------------------------------
Francesca Pancotto, PhD
> Il giorno 19 ott 2017, alle ore 18:01, Jeff Newmiller <jdnewmil at dcn.davis.CA.us> ha scritto:
> 
> (Re-)read the discussion of indexing (both `[` and `[[`) and be sure to get clear on the difference between matrices and data frames in the Introduction to R document that comes with R. There are many ways to create numeric vectors, character vectors, and logical vectors that can then be used as indexes, including the straightforward way:
> 
> df[ c(
> "Unique to strat          ",
> "Unique to strat:crt.dummy                 ",
> "Common to strat, and crt.dummy         ",
> "Common to strat, and gender         ",
> "Common to strat, and age                 ") ,]
> -- 
> Sent from my phone. Please excuse my brevity.
> 
> On October 19, 2017 3:14:53 AM PDT, Francesca PANCOTTO <f.pancotto at unimore.it> wrote:
>> Thanks a lot, so simple so efficient!
>> 
>> I will study more the grep command I did not know.
>> 
>> Thanks!
>> 
>> 
>> Francesca Pancotto
>> 
>>> Il giorno 19 ott 2017, alle ore 12:12, Enrico Schumann
>> <es at enricoschumann.net> ha scritto:
>>> 
>>> df[grep("strat", row.names(df)), ]
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From maechler at stat.math.ethz.ch  Fri Oct 20 13:03:32 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 20 Oct 2017 13:03:32 +0200
Subject: [R] Error messages using nonlinear regression function (nls)
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3EAC@SRVEXCHCM301.precheza.cz>
References: <C1524BE4BF45454293B8DF38FB9B5ECAE9E91129@MS-EX1VKS.erdc.dren.mil>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3C98@SRVEXCHCM301.precheza.cz>
 <C1524BE4BF45454293B8DF38FB9B5ECAE9E967D6@MS-EX1VKS.erdc.dren.mil>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3EAC@SRVEXCHCM301.precheza.cz>
Message-ID: <23017.55300.22402.6027@stat.math.ethz.ch>

>>>>> PIKAL Petr <petr.pikal at precheza.cz>
>>>>>     on Fri, 20 Oct 2017 06:33:36 +0000 writes:

    > Hi
    > Keep your messages in the list, you increase your chance to get some answer.

    > I changed your data to groupedData object (see below), but I did not find any problem in it.

    > plot(wlg)
    > gives reasonable picture and I am not such expert to see any problem with data. Seems to me, that something has to be wrong with nlsList function.

    >> wheat.list <- nlsList(Prop ~ SSlogis(end,Asym, xmid, scal), data=wlg)
    > Warning message:
    > 6 times caught the same error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...): NA/NaN/Inf in 'x'

    > produces empty list. So maybe others could find some problem.

    > Cheers
    > Petr

Thank you, Petr,  for the reproducible example.

This indeed can be traced back to a bug in SSlogis() that has
been there since Doug Bates added the 'nls' package on Martin's
day 1999 (==> before R version 1.0.0 !) to R (svn rev 6455).

It's this part which does contain a thinko (by whomever, only
Doug may be able to remember/recall, but I guess it would have
been Jos? Pinheiro, then the grad student doing the nitty gritty),
I have added 2 '----------' below to make the 2 important lines stand out :

   z <- xy[["y"]]
 ---------------------------------------------------------------------
   if (min(z) <= 0) { z <- z - 1.05 * min(z) } # avoid zeroes
   z <- z/(1.05 * max(z))		# scale to within unit height
 ---------------------------------------------------------------------
   xy[["z"]] <- log(z/(1 - z))		# logit transformation
   aux <- coef(lm(x ~ z, xy))

the first of the 2 lines,  commented   "avoid zeroes"
is obviously *not* avoiding zeroes in the case min(z) == 0 , 
and even though the 2 lines  should rescale an interval  [0, M]
to [eps, 1 - eps] they don't in this case.

Consequently, the next line  log(z/(1 - z))  transforms
the 0s into -Inf  and these lead to the warning (or error in nls())

  " NA/NaN/Inf in 'x' "

One could fix this up by  replacing  min(z)  by  min(z, -1e-7)
which may be best for pure back compatibility, but I really
don't like it either :

The famous  logit - transform   log( z / (1-z))
is really anti-symmetric around z = 1/2 , in particular should
treat 0 and 1 "symmetrically" and I find it ugly that
the previous fixup (our two ominous lines) is not at all
symmetric wrt 1/2, notably the 2nd transformation is made
unconditionally but the first one not.

Fortunately, the same source file, <R>/src/library/stats/R/zzzModels.R 
also defines  the SSfpl()  == 4-parameter logistic model

and there, the 'init' function needs to do the same scaling to
(0, 1)  and does it much nicer, indeed (anti)symmetrically.

I'm looking into using that in SSlogis() as well,
fixing this bug.

Martin Maechler
ETH Zurich and R Core Team


From traxplayer at gmail.com  Fri Oct 20 14:27:30 2017
From: traxplayer at gmail.com (=?UTF-8?Q?Martin_M=C3=B8ller_Skarbiniks_Pedersen?=)
Date: Fri, 20 Oct 2017 14:27:30 +0200
Subject: [R] [FORGED] can't print ggplot with Chinese characters to pdf
	files
In-Reply-To: <CABcx46BHMFzKriQP+JHZdyTG0rkL8EycYUfKkE1-iitfCtPgoQ@mail.gmail.com>
References: <CABcx46A4OL5+Ja-e0eAr=7iF2_obaCOZh4a7vHMLcqkZNpD3MQ@mail.gmail.com>
 <1daf3c68-a4ee-3755-911d-7f997528499e@stat.auckland.ac.nz>
 <CABcx46DFdnwUjdO4ciaD5aeoGSDR2UgUJwMvjWG6WkjBPJ=wrg@mail.gmail.com>
 <7f9f2636-c3d3-40b1-8ce3-9d4b2bde3336@stat.auckland.ac.nz>
 <CABcx46BHMFzKriQP+JHZdyTG0rkL8EycYUfKkE1-iitfCtPgoQ@mail.gmail.com>
Message-ID: <CAGAA5bdnb9nf35HJ520J55Jr38tBjsvhUNM-+FiQhewAQFu9YA@mail.gmail.com>

On 20 October 2017 at 04:21, John <miaojpm at gmail.com> wrote:
>
> Hi,
>
>    Following Paul's instruction, I have installed the Cairo. I tried to
run
> the program, and there is no error message at all. I did see the Chinese
> title in the plot if I ask my RStudio to show the plot (if I type "p1"),
> but the pdf file shows the plots without the Chinese titles.


[...]

>
> cairo_pdf("test_plot_chinese.pdf")
> print(m2)
> dev.off()
>

The help page for cairo_pdf says: "
?cairo_pdf? and ?cairo_ps? sometimes record _bitmaps_ and not vector
graphics.  On the other hand, they can (on suitable platforms) include a
much
     wider range of UTF-8 glyphs, and embed the fonts used.
"
I am not sure what sometimes means but maybe the fonts are not embedded in
the pdf-file?

try:
embed_fonts('test_plot_chinese.pdf')

note that it might fails if you haven't installed gs (ghostscript)


Regards
Martin

	[[alternative HTML version deleted]]


From celine_jouanin at yahoo.fr  Fri Oct 20 14:25:56 2017
From: celine_jouanin at yahoo.fr (Jouanin Celine)
Date: Fri, 20 Oct 2017 12:25:56 +0000 (UTC)
Subject: [R] Permission denied with tk_choose.dir
References: <1790987080.1349577.1508502356400.ref@mail.yahoo.com>
Message-ID: <1790987080.1349577.1508502356400@mail.yahoo.com>

Dear R Help list,
I'm facing a problem with "tk_choose.dir", I try to find an explanation and a solution.

I use :rm(list=ls(all=TRUE))
datawd<-"/mnt/3-Biot/data_input/"
setwd(datawd) files<-tk_choose.files(filters = matrix(c("/*.csv",".csv"),1, 2, byrow = TRUE),caption ="Select file with csv format") 
If I change the name of the folder like this ("data_input" becomes "data") and delete the folder "data_input" and then run :
rm(list=ls(all=TRUE))
datawd<-"/mnt/3-Biot/data/"
setwd(datawd) files<-tk_choose.files(filters = matrix(c("/*.csv",".csv"),1, 2, byrow = TRUE),caption ="Select file with csv format") 
I received this error :"Impossible to acess to the directory "/mnt/3-BioT/data_input" permission denied"
The old path and folder is kept in memory in tk_choose.file.
What can I do ?
Thanks for your help,
C?line






I use R version 3.2.3 on linux-gnu

C?line Jouanin

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Fri Oct 20 15:35:01 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 20 Oct 2017 13:35:01 +0000
Subject: [R] Error messages using nonlinear regression function (nls)
In-Reply-To: <23017.55300.22402.6027@stat.math.ethz.ch>
References: <C1524BE4BF45454293B8DF38FB9B5ECAE9E91129@MS-EX1VKS.erdc.dren.mil>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3C98@SRVEXCHCM301.precheza.cz>
 <C1524BE4BF45454293B8DF38FB9B5ECAE9E967D6@MS-EX1VKS.erdc.dren.mil>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3EAC@SRVEXCHCM301.precheza.cz>
 <23017.55300.22402.6027@stat.math.ethz.ch>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB4127@SRVEXCHCM301.precheza.cz>

Thank you Martin.

If I understand correctly, OP could do

wheat.list <- nlsList(Prop ~ SSfpl(end, A, B, xmid, scal), data=wlg)

or add some small value to all zeroes

wlg$prop < -wlg$Prop+1e-7
wheat.list <- nlsList(prop ~ SSlogis(end,Asym, xmid, scal), data=wlg)

which gives fairly reasonable results.
plot(augPred(wheat.list))

Am I correct?

Cheers
Petr

> -----Original Message-----
> From: Martin Maechler [mailto:maechler at stat.math.ethz.ch]
> Sent: Friday, October 20, 2017 1:04 PM
> To: PIKAL Petr <petr.pikal at precheza.cz>
> Cc: Wall, Wade A ERDC-RDE-CERL-IL CIV <Wade.A.Wall at erdc.dren.mil>; r-
> help at r-project.org
> Subject: Re: [R] Error messages using nonlinear regression function (nls)
>
> >>>>> PIKAL Petr <petr.pikal at precheza.cz>
> >>>>>     on Fri, 20 Oct 2017 06:33:36 +0000 writes:
>
>     > Hi
>     > Keep your messages in the list, you increase your chance to get some
> answer.
>
>     > I changed your data to groupedData object (see below), but I did not find
> any problem in it.
>
>     > plot(wlg)
>     > gives reasonable picture and I am not such expert to see any problem with
> data. Seems to me, that something has to be wrong with nlsList function.
>
>     >> wheat.list <- nlsList(Prop ~ SSlogis(end,Asym, xmid, scal), data=wlg)
>     > Warning message:
>     > 6 times caught the same error in lm.fit(x, y, offset = offset, singular.ok =
> singular.ok, ...): NA/NaN/Inf in 'x'
>
>     > produces empty list. So maybe others could find some problem.
>
>     > Cheers
>     > Petr
>
> Thank you, Petr,  for the reproducible example.
>
> This indeed can be traced back to a bug in SSlogis() that has been there since
> Doug Bates added the 'nls' package on Martin's day 1999 (==> before R version
> 1.0.0 !) to R (svn rev 6455).
>
> It's this part which does contain a thinko (by whomever, only Doug may be able
> to remember/recall, but I guess it would have been Jos? Pinheiro, then the grad
> student doing the nitty gritty), I have added 2 '----------' below to make the 2
> important lines stand out :
>
>    z <- xy[["y"]]
>  ---------------------------------------------------------------------
>    if (min(z) <= 0) { z <- z - 1.05 * min(z) } # avoid zeroes
>    z <- z/(1.05 * max(z))             # scale to within unit height
>  ---------------------------------------------------------------------
>    xy[["z"]] <- log(z/(1 - z))                # logit transformation
>    aux <- coef(lm(x ~ z, xy))
>
> the first of the 2 lines,  commented   "avoid zeroes"
> is obviously *not* avoiding zeroes in the case min(z) == 0 , and even though the
> 2 lines  should rescale an interval  [0, M] to [eps, 1 - eps] they don't in this case.
>
> Consequently, the next line  log(z/(1 - z))  transforms the 0s into -Inf  and these
> lead to the warning (or error in nls())
>
>   " NA/NaN/Inf in 'x' "
>
> One could fix this up by  replacing  min(z)  by  min(z, -1e-7) which may be best
> for pure back compatibility, but I really don't like it either :
>
> The famous  logit - transform   log( z / (1-z))
> is really anti-symmetric around z = 1/2 , in particular should treat 0 and 1
> "symmetrically" and I find it ugly that the previous fixup (our two ominous
> lines) is not at all symmetric wrt 1/2, notably the 2nd transformation is made
> unconditionally but the first one not.
>
> Fortunately, the same source file, <R>/src/library/stats/R/zzzModels.R
> also defines  the SSfpl()  == 4-parameter logistic model
>
> and there, the 'init' function needs to do the same scaling to (0, 1)  and does it
> much nicer, indeed (anti)symmetrically.
>
> I'm looking into using that in SSlogis() as well, fixing this bug.
>
> Martin Maechler
> ETH Zurich and R Core Team

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


From evangelinaviotto at gmail.com  Fri Oct 20 16:37:12 2017
From: evangelinaviotto at gmail.com (Evangelina Viotto)
Date: Fri, 20 Oct 2017 11:37:12 -0300
Subject: [R] nls() and loop
Message-ID: <CAEMjoTkNRyuRA8trq46wo_CrzWg3mZ3ZbUsuyYp=qvsN2N7sLw@mail.gmail.com>

Hello I?m need fitt growth curve with data length-age. I want to evaluate
which is the function that best predicts my data, to do so I compare the
Akaikes of different models. I'm now need to evaluate if changing the
initial values changes the parameters and which do not allow to estimate
the model.
To do this I use the function nls(); and I randomize the initial values
(real positive number).  To that I put it inside a function that every time
q is executed it changes the initial parameters and affter and then do a
loop  y and  save a list of the results that interest me in the function.
this problem is does not converge by the initial values, the loop stops and
throws error.
I need to continue and  save initial values with the error that generates
those values


Cheers

Vangi



ANO<- c( 1.65, 1.69, 1.72, 1.72, 1.72, 1.72, 1.73, 2.66 ,2.66, 2.66, 2.66,
2.76, 2.76, 2.76 ,2.76, 2.78, 2.8, 3.65, 3.65 ,3.65, 3.78, 3.78, 5.07, 7.02,
7.1, 7.81, 8.72, 8.74, 8.78, 8.8, 8.8, 8.83, 8.98, 9.1, 9.11, 9.75, 9.82,
9.84, 9.87, 9.87, 10.99, 11.67, 11.8, 11.81, 13.93, 14.83, 15.82, 15.99,
16.87, 16.88, 16.9, 17.68, 17.79, 17.8, 17.8)


SVL<-c(26.11,29.02,41.13,30.96,37.74,29.02,33.38,24.18,34.35,35.8,29.99,42.59,27.57,47.43,46.95,30.47,29.75,35.8,40.65,36.29,34.83,29.02,43.5,75,68,70,67.5,80,77.5,68,68,73.84,72.14,68,64.5,58.5,72.63,78.44,71.17,70.69,77,79,78,68.5,69.72,71.66,77,77,79,76.5,78.5,79,73,80,69.72)

data<-data.frame (SVL, ANO)# creo data frame
data
> Logiscorri<-function(){
+   a<-runif(1, min=0, max=150)#devuelve 1 al azar dentro de un max y un
min
+   b<-runif(1, min=0, max=100)
+   g<-runif (1, min=0, max=1)
+   d<-runif (1,min=0, max=100)
+
+   ## estimo la curva de distribucion de mis datos
+   caiman<-nls(SVL~DE+(alfa/(1+exp(-gamma*ANO))),
+               data=data,
+               start=list(alfa= a  ,gamma= g, DE= d),
+               control=nls.control(maxiter = 100, warnOnly=TRUE),
+               trace=FALSE)
+   caimansum<-summary(caiman)#ME DA LOS PARAMETROS ESTIMADO, EL NUM DE
ITERACIONES
+   ## analizamos akaike
+   akaike<-AIC(caiman)
+   Bayesiano<-BIC(caiman)
+   alfa<-coef(caiman)[1]
+   beta<-coef(caiman)[2]
+   gamma<- coef(caiman)[3]
+   DE<- coef(caiman)[4]
+   formu<-formula(caiman)
+
+   ValoresIniciales<-c(a, g, d)
+   resultados<-list(formu, caimansum, ValoresIniciales, akaike, Bayesiano)
+   return(resultados)
+ }
> Logiscorri()
[[1]]
SVL ~ DE + (alfa/(1 + exp(-gamma * ANO)))
<environment: 0x16a6b89c>

[[2]]

Formula: SVL ~ DE + (alfa/(1 + exp(-gamma * ANO)))

Parameters:
      Estimate Std. Error t value Pr(>|t|)
alfa  133.0765     6.9537  19.138  < 2e-16 ***
gamma   0.2746     0.0371   7.401 1.13e-09 ***
DE    -54.0467     7.1047  -7.607 5.34e-10 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 6.821 on 52 degrees of freedom

Number of iterations to convergence: 30
Achieved convergence tolerance: 4.995e-06


[[3]]
[1] 112.2528283   0.4831461  38.5151401

[[4]]
[1] 372.2001

[[5]]
[1] 380.2294

> resultados<-list()
> resultados
list()
> for(i in 1:10){
+   resultados[i]<- list(Logiscorri())
+ }
Error in chol2inv(object$m$Rmat()) :
  element (2, 2) is zero, so the inverse cannot be computed
In addition: Warning message:
In nls(SVL ~ DE + (alfa/(1 + exp(-gamma * ANO))), data = data, start =
list(alfa = a,  :
  singular gradient
> names(resultados)<- 1:10
Error in names(resultados) <- 1:10 :
  'names' attribute [10] must be the same length as the vector [4]
> parametros<- t(sapply(LogisticoConCorri, "[[", "par?metros")) #estp lo
que hace es ir item por item de la lista y sacar los par?metros
Error in FUN(X[[i]], ...) : subscript out of bounds
> colnames(parametros)<- c("alfa", "beta", "gamma", "DE")
Error in dimnames(x) <- dn :
  length of 'dimnames' [2] not equal to array extent
> akaikefinal<- sapply(LogisticoConCorri, "[[", "akaike")#esto va item por
item de la lista y saca el akaike
Error in FUN(X[[i]], ...) : subscript out of bounds
> bayesfinal<- sapply(LogisticoConCorri, "[[", "Bayesiano")
Error in FUN(X[[i]], ...) : subscript out of bounds
>
> --
Bi?l. Evangelina V. Viotto
Laboratorio Ecolog?a Animal
Centro de investigaciones Cient?ficas y de Transferencias de
Tecnolog?a Aplicada a la Producci?n
(CICyTTP-CONICET-UADER)
Diamante, Entre R?os
Argentina

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Fri Oct 20 16:54:43 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 20 Oct 2017 07:54:43 -0700
Subject: [R] Error messages using nonlinear regression function (nls)
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3EAC@SRVEXCHCM301.precheza.cz>
References: <C1524BE4BF45454293B8DF38FB9B5ECAE9E91129@MS-EX1VKS.erdc.dren.mil>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3C98@SRVEXCHCM301.precheza.cz>
 <C1524BE4BF45454293B8DF38FB9B5ECAE9E967D6@MS-EX1VKS.erdc.dren.mil>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3EAC@SRVEXCHCM301.precheza.cz>
Message-ID: <CAF8bMcarQprF_OSC1hpwHPZbpdCTGg4+--+U2GxNWuB=KiqeJQ@mail.gmail.com>

The Logistic link does not work well with zero or unit proportions.  Try
pulling it away from the edges with
   wlg$Prop <- pmin( 1-1e-10, pmax( 1e-10, w1g$Prop ))




Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Oct 19, 2017 at 11:33 PM, PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hi
>
> Keep your messages in the list, you increase your chance to get some
> answer.
>
> I changed your data to groupedData object (see below), but I did not find
> any problem in it.
>
> plot(wlg)
> gives reasonable picture and I am not such expert to see any problem with
> data. Seems to me, that something has to be wrong with nlsList function.
>
> > wheat.list <- nlsList(Prop ~ SSlogis(end,Asym, xmid, scal), data=wlg)
> Warning message:
> 6 times caught the same error in lm.fit(x, y, offset = offset, singular.ok
> = singular.ok, ...): NA/NaN/Inf in 'x'
>
> produces empty list. So maybe others could find some problem.
>
> Cheers
> Petr
>
> > dput(wlg)
> structure(list(temp = structure(c(2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
> 4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
> 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
> 6L, 6L, 6L, 6L, 6L, 6L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L), .Label = c("40", "10", "16", "22", "28",
> "34"), class = c("ordered", "factor")), species = structure(c(3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), .Label = c("mungbean",
> "rice", "wheat"), class = "factor"), start = c(0, 1, 2, 3, 4,
> 5, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17, 0, 2, 3, 4, 5, 6, 7,
> 9, 10, 11, 12, 13, 14, 16, 17, 0, 2, 3, 4, 5, 6, 7, 9, 10, 11,
> 12, 13, 14, 16, 17, 0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13,
> 14, 16, 17, 0, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17,
> 0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17), end = c(1,
> 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17, 18, 2, 3, 4,
> 5, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17, 18, 2, 3, 4, 5, 6, 7,
> 9, 10, 11, 12, 13, 14, 16, 17, 18, 1, 2, 3, 4, 5, 6, 7, 9, 10,
> 11, 12, 13, 14, 16, 17, 18, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12,
> 13, 14, 16, 17, 18, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14,
> 16, 17, 18), germinated = c(0L, 0L, 0L, 0L, 0L, 4L, 6L, 8L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 11L, 7L, 0L, 1L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 12L, 3L, 0L, 3L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 8L, 0L, 8L, 3L, 0L, 0L,
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 5L, 11L, 1L, 1L, 0L, 0L, 0L,
> 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L,
> 2L, 0L, 0L, 0L, 0L, 0L, 0L), TotSeeds = c(20, 20, 20, 20, 20,
> 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
> 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
> 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
> 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
> 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
> 20, 20, 20, 20, 20, 20, 20, 20), TotGerminated = c(0, 0, 0, 0,
> 0, 4, 10, 18, 18, 18, 18, 18, 18, 18, 18, 18, 0, 0, 11, 18, 18,
> 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 0, 1, 13, 16, 16, 19,
> 19, 19, 19, 19, 19, 19, 19, 19, 19, 0, 0, 0, 8, 8, 16, 19, 19,
> 19, 19, 19, 19, 19, 19, 19, 19, 0, 5, 16, 17, 18, 18, 18, 18,
> 18, 19, 19, 19, 19, 19, 19, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 3,
> 3, 3, 3, 3, 3), Prop = c(0, 0, 0, 0, 0, 0.2, 0.5, 0.9, 0.9, 0.9,
> 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0, 0, 0.55, 0.9, 0.9, 0.95, 0.95,
> 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0, 0.05, 0.65,
> 0.8, 0.8, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
> 0.95, 0, 0, 0, 0.4, 0.4, 0.8, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
> 0.95, 0.95, 0.95, 0.95, 0, 0.25, 0.8, 0.85, 0.9, 0.9, 0.9, 0.9,
> 0.9, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0, 0, 0, 0, 0, 0, 0,
> 0, 0.05, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15)), .Names = c("temp",
> "species", "start", "end", "germinated", "TotSeeds", "TotGerminated",
> "Prop"), row.names = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
> 11L, 12L, 13L, 14L, 15L, 16L, 25L, 26L, 27L, 28L, 29L, 30L, 31L,
> 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 63L, 64L, 65L, 66L, 67L,
> 68L, 69L, 70L, 71L, 72L, 73L, 74L, 75L, 76L, 77L, 95L, 96L, 97L,
> 98L, 99L, 100L, 101L, 102L, 103L, 104L, 105L, 106L, 107L, 108L,
> 109L, 110L, 127L, 128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L,
> 136L, 137L, 138L, 139L, 140L, 141L, 159L, 160L, 161L, 162L, 163L,
> 164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L, 172L, 173L, 174L
> ), class = c("nfnGroupedData", "nfGroupedData", "groupedData",
> "data.frame"), formula = Prop ~ end | temp, FUN = function (x)
> max(x, na.rm = TRUE), order.groups = TRUE)
> >
>
> > sessionInfo()
> R Under development (unstable) (2017-07-31 r73003)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 10 x64 (build 14393)
>
> Matrix products: default
>
> locale:
> [1] LC_COLLATE=Czech_Czech Republic.1250  LC_CTYPE=Czech_Czech
> Republic.1250
> [3] LC_MONETARY=Czech_Czech Republic.1250 LC_NUMERIC=C
> [5] LC_TIME=Czech_Czech Republic.1250
>
> attached base packages:
> [1] stats     datasets  utils     grDevices graphics  methods   base
>
> other attached packages:
> [1] nlme_3.1-131    lattice_0.20-35 fun_0.1
>
> loaded via a namespace (and not attached):
>  [1] compiler_3.5.0   colorspace_1.3-2 scales_0.4.1     lazyeval_0.2.0
>  [5] plyr_1.8.4       tools_3.5.0      gtable_0.2.0     tibble_1.3.3
>  [9] Rcpp_0.12.12     ggplot2_2.2.1    grid_3.5.0       rlang_0.1.1
> [13] munsell_0.4.3
> >
>
> > -----Original Message-----
> > From: Wall, Wade A ERDC-RDE-CERL-IL CIV
> > [mailto:Wade.A.Wall at erdc.dren.mil]
> > Sent: Thursday, October 19, 2017 3:18 PM
> > To: PIKAL Petr <petr.pikal at precheza.cz>
> > Subject: RE: Error messages using nonlinear regression function (nls)
> >
> > "Prop" is the proportion of seeds that have germinated from the total.
> Here is
> > the code I used to transform the germination data set. When graphed, the
> > logistic curve provides a reasonable fit.
> >
> > data(germination)
> > germination$TotSeeds = 20
> > germination$TotGerminated = 0
> > tmpCount = 0
> >
> > for(i in 2:nrow(germination)){
> >   if((germination$temp[i]!=germination$temp[i-1]) |
> (germination$species[i] !=
> > germination$species[i-1])) {tmpCount = 0}
> >   if((germination$temp[i]==germination$temp[i-1]) &
> (germination$species[i]
> > == germination$species[i-1]))
> >   {tmpCount = tmpCount + germination$germinated[i];
> > germination$TotGerminated[i]=tmpCount}
> > }
> > germination$Prop = germination$TotGerminated/germination$TotSeeds
> > germination = germination[germination$end != Inf,]
> >
> > Wade
> >
> > -----Original Message-----
> > From: PIKAL Petr [mailto:petr.pikal at precheza.cz]
> > Sent: Thursday, October 19, 2017 1:21 AM
> > To: Wall, Wade A ERDC-RDE-CERL-IL CIV <Wade.A.Wall at erdc.dren.mil>; r-
> > help at r-project.org
> > Subject: RE: Error messages using nonlinear regression function (nls)
> >
> > Hi
> >
> > Thanks for the code but where is Prop?
> >
> > It is not a variable in germination data set so we do not know how you
> did the
> > computation.
> >
> > My wild guess is, that your Prop do not follow logistic curve and
> therefore no
> > results from nlsList
> >
> > Cheers
> > Petr
> >
> >
> > > -----Original Message-----
> > > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Wall,
> > > Wade A ERDC-RDE-CERL-IL CIV
> > > Sent: Wednesday, October 18, 2017 5:58 PM
> > > To: r-help at r-project.org
> > > Subject: [R] Error messages using nonlinear regression function (nls)
> > >
> > > Hi all,
> > >
> > > I am trying to use nonlinear regression (nls) to analyze some seed
> > > germination data, but am having problems with error codes.
> > >
> > > The data that I have closely matches the germination dataset included
> > > in the drc package.
> > >
> > > Here is the head of the data
> > >
> > >   temp species start end germinated TotSeeds TotGerminated Prop
> > > 1   10   wheat     0   1          0       20             0  0.0
> > > 2   10   wheat     1   2          0       20             0  0.0
> > > 3   10   wheat     2   3          0       20             0  0.0
> > > 4   10   wheat     3   4          0       20             0  0.0
> > > 5   10   wheat     4   5          0       20             0  0.0
> > > 6   10   wheat     5   6          4       20             4  0.2
> > >
> > > temp is the temperature under which the seeds were germinated, species
> > > denotes the species (wheat, mungbean, or rice) Start and end denote
> > > the beginning and end of a period of time and germinated denotes how
> > > many seeds germinated during that period of time. Prop represents the
> > > proportion of seeds that have germinated.
> > >
> > > I have attempted to mimic Fox and Weisberg's appendix to Nonlinear
> > > Regression found here.
> > > Blockedhttps://socserv.socsci.mcmaster.ca/jfox/Books/Companion/appendi
> > > x/AppendixBlocked
> > > -Nonlinear-Regression.pdf
> > >
> > > My first step is to look at a single species, wheat, and use nls on
> > > the individual temperatures.
> > >
> > > I have tried to use both the nlsList function and to attempt to
> > > estimate the parameters using lm(), but I receive error messages on
> > > both. Here is the code.
> > >
> > > library(drc) ### for germination dataset
> > > data(germination)
> > >
> > > wheat = germination[germination$species == "wheat",] ### subset by
> > > wheat scatterplot(Prop ~ end|temp,data=wheat,box=FALSE,reg=FALSE) ###
> > > view the data wheat$temp = as.factor(wheat$temp) ### convert to factor
> > >
> > > ### First, try to use nlsList
> > > wheat.list <- nlsList(Prop ~ SSlogis(end,phi1,phi2,phi3)|
> > > temp,pool=FALSE,data=wheat) ###
> > >
> > > ### next, try to use lm to estimate starting parameters.
> > > wheat.list = list()
> > >
> > > for (i in 1:length(levels(wheat$temp))){
> > >   tmpDat = wheat[wheat$temp == levels(wheat$temp)[i],]
> > >   tmp.lm <- lm(Prop ~ end,data = tmpDat)
> > >   tmp.nls <- nls(Prop ~ theta1 / (1 + exp(-(theta2 + theta3*end))),
> > >       start = list(theta1 = .5,theta2=coef(tmp.lm)[1],theta3 =
> coef(tmp.lm)[2]),
> > >       data = tmpDat,trace=TRUE)
> > >   tmp2.nls <- nls(Prop ~ SSlogis(end,phi1,phi2,phi3),data=tmpDat)
> > >   wheat.list[i] <- tmp.nls
> > > }
> > > #### End code
> > > nlsList just returns an empty list.
> > >
> > > When I try to loop through the individual temperatures, for the first
> > > temperature, nls converges when I provide starting values, but when I
> > > try to use SSlogis(), I get the error messsage
> > >
> > > Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...)
> :
> > >   NA/NaN/Inf in 'x'
> > >
> > > For the second temperature (16), I get the following error message
> > > when provided initial values using lm:
> > >
> > > Error in nls(Prop ~ theta1/(1 + exp(-(theta2 + theta3 * end))), start
> > > = list(theta1 = 0.5,  :
> > >   singular gradient
> > >
> > > I have tried to read through posts, but none of them seem to apply to
> this
> > case.
> > > The data seem relatively simply and
> > > I am not sure what I am doing wrong. Any help would be appreciated.
> > >
> > > Wade
> > >
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Fri Oct 20 17:02:59 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 20 Oct 2017 17:02:59 +0200
Subject: [R] Error messages using nonlinear regression function (nls)
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB4127@SRVEXCHCM301.precheza.cz>
References: <C1524BE4BF45454293B8DF38FB9B5ECAE9E91129@MS-EX1VKS.erdc.dren.mil>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3C98@SRVEXCHCM301.precheza.cz>
 <C1524BE4BF45454293B8DF38FB9B5ECAE9E967D6@MS-EX1VKS.erdc.dren.mil>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB3EAC@SRVEXCHCM301.precheza.cz>
 <23017.55300.22402.6027@stat.math.ethz.ch>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB4127@SRVEXCHCM301.precheza.cz>
Message-ID: <23018.4131.660946.905137@stat.math.ethz.ch>

>>>>> PIKAL Petr <petr.pikal at precheza.cz>
>>>>>     on Fri, 20 Oct 2017 13:35:01 +0000 writes:

    > Thank you Martin.
    > If I understand correctly, OP could do

    > wheat.list <- nlsList(Prop ~ SSfpl(end, A, B, xmid, scal), data=wlg)

(which is really a different model with one parameter more, and
 given the relatively small sample size per group, it may not be
 the best idea)

    > or add some small value to all zeroes

    > wlg$prop <- wlg$Prop+1e-7
    > wheat.list <- nlsList(prop ~ SSlogis(end,Asym, xmid, scal), data=wlg)

    > which gives fairly reasonable results.
    > plot(augPred(wheat.list))

    > Am I correct?

Yes, indeed, Petr,  I forgot to mention that obvious workaround.

(Interestingly, he could also _subtract_ a small positive value
 which is less intuitive, but work too given the code I showed.)

Martin



    > Cheers
    > Petr

    >> -----Original Message-----
    >> From: Martin Maechler [mailto:maechler at stat.math.ethz.ch]
    >> Sent: Friday, October 20, 2017 1:04 PM
    >> To: PIKAL Petr <petr.pikal at precheza.cz>
    >> Cc: Wall, Wade A ERDC-RDE-CERL-IL CIV <Wade.A.Wall at erdc.dren.mil>; r-
    >> help at r-project.org
    >> Subject: Re: [R] Error messages using nonlinear regression function (nls)
    >> 
    >> >>>>> PIKAL Petr <petr.pikal at precheza.cz>
    >> >>>>>     on Fri, 20 Oct 2017 06:33:36 +0000 writes:
    >> 
    >> > Hi
    >> > Keep your messages in the list, you increase your chance to get some
    >> answer.
    >> 
    >> > I changed your data to groupedData object (see below), but I did not find
    >> any problem in it.
    >> 
    >> > plot(wlg)
    >> > gives reasonable picture and I am not such expert to see any problem with
    >> data. Seems to me, that something has to be wrong with nlsList function.
    >> 
    >> >> wheat.list <- nlsList(Prop ~ SSlogis(end,Asym, xmid, scal), data=wlg)
    >> > Warning message:
    >> > 6 times caught the same error in lm.fit(x, y, offset = offset, singular.ok =
    >> singular.ok, ...): NA/NaN/Inf in 'x'
    >> 
    >> > produces empty list. So maybe others could find some problem.
    >> 
    >> > Cheers
    >> > Petr
    >> 
    >> Thank you, Petr,  for the reproducible example.
    >> 
    >> This indeed can be traced back to a bug in SSlogis() that has been there since
    >> Doug Bates added the 'nls' package on Martin's day 1999 (==> before R version
    >> 1.0.0 !) to R (svn rev 6455).
    >> 
    >> It's this part which does contain a thinko (by whomever, only Doug may be able
    >> to remember/recall, but I guess it would have been Jos? Pinheiro, then the grad
    >> student doing the nitty gritty), I have added 2 '----------' below to make the 2
    >> important lines stand out :
    >> 
    >> z <- xy[["y"]]
    >> ---------------------------------------------------------------------
    >> if (min(z) <= 0) { z <- z - 1.05 * min(z) } # avoid zeroes
    >> z <- z/(1.05 * max(z))             # scale to within unit height
    >> ---------------------------------------------------------------------
    >> xy[["z"]] <- log(z/(1 - z))                # logit transformation
    >> aux <- coef(lm(x ~ z, xy))
    >> 
    >> the first of the 2 lines,  commented   "avoid zeroes"
    >> is obviously *not* avoiding zeroes in the case min(z) == 0 , and even though the
    >> 2 lines  should rescale an interval  [0, M] to [eps, 1 - eps] they don't in this case.
    >> 
    >> Consequently, the next line  log(z/(1 - z))  transforms the 0s into -Inf  and these
    >> lead to the warning (or error in nls())
    >> 
    >> " NA/NaN/Inf in 'x' "
    >> 
    >> One could fix this up by  replacing  min(z)  by  min(z, -1e-7) which may be best
    >> for pure back compatibility, but I really don't like it either :
    >> 
    >> The famous  logit - transform   log( z / (1-z))
    >> is really anti-symmetric around z = 1/2 , in particular should treat 0 and 1
    >> "symmetrically" and I find it ugly that the previous fixup (our two ominous
    >> lines) is not at all symmetric wrt 1/2, notably the 2nd transformation is made
    >> unconditionally but the first one not.
    >> 
    >> Fortunately, the same source file, <R>/src/library/stats/R/zzzModels.R
    >> also defines  the SSfpl()  == 4-parameter logistic model
    >> 
    >> and there, the 'init' function needs to do the same scaling to (0, 1)  and does it
    >> much nicer, indeed (anti)symmetrically.
    >> 
    >> I'm looking into using that in SSlogis() as well, fixing this bug.
    >> 
    >> Martin Maechler
    >> ETH Zurich and R Core Team

    > ________________________________
    > Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
    > Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
    > Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
    > Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

    > V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
    > - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
    > - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
    > - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
    > - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

    > This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
    > If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
    > If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
    > The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

    > In case that this e-mail forms part of business dealings:
    > - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
    > - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
    > - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
    > - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


From jdnewmil at dcn.davis.ca.us  Fri Oct 20 17:20:55 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 20 Oct 2017 08:20:55 -0700
Subject: [R] nls() and loop
In-Reply-To: <CAEMjoTkNRyuRA8trq46wo_CrzWg3mZ3ZbUsuyYp=qvsN2N7sLw@mail.gmail.com>
References: <CAEMjoTkNRyuRA8trq46wo_CrzWg3mZ3ZbUsuyYp=qvsN2N7sLw@mail.gmail.com>
Message-ID: <4FE6917D-480D-45EB-9E63-17758D4B0DD0@dcn.davis.ca.us>

?tryCatch
-- 
Sent from my phone. Please excuse my brevity.

On October 20, 2017 7:37:12 AM PDT, Evangelina Viotto <evangelinaviotto at gmail.com> wrote:
>Hello I?m need fitt growth curve with data length-age. I want to
>evaluate
>which is the function that best predicts my data, to do so I compare
>the
>Akaikes of different models. I'm now need to evaluate if changing the
>initial values changes the parameters and which do not allow to
>estimate
>the model.
>To do this I use the function nls(); and I randomize the initial values
>(real positive number).  To that I put it inside a function that every
>time
>q is executed it changes the initial parameters and affter and then do
>a
>loop  y and  save a list of the results that interest me in the
>function.
>this problem is does not converge by the initial values, the loop stops
>and
>throws error.
>I need to continue and  save initial values with the error that
>generates
>those values
>
>
>Cheers
>
>Vangi
>
>
>
>ANO<- c( 1.65, 1.69, 1.72, 1.72, 1.72, 1.72, 1.73, 2.66 ,2.66, 2.66,
>2.66,
>2.76, 2.76, 2.76 ,2.76, 2.78, 2.8, 3.65, 3.65 ,3.65, 3.78, 3.78, 5.07,
>7.02,
>7.1, 7.81, 8.72, 8.74, 8.78, 8.8, 8.8, 8.83, 8.98, 9.1, 9.11, 9.75,
>9.82,
>9.84, 9.87, 9.87, 10.99, 11.67, 11.8, 11.81, 13.93, 14.83, 15.82,
>15.99,
>16.87, 16.88, 16.9, 17.68, 17.79, 17.8, 17.8)
>
>
>SVL<-c(26.11,29.02,41.13,30.96,37.74,29.02,33.38,24.18,34.35,35.8,29.99,42.59,27.57,47.43,46.95,30.47,29.75,35.8,40.65,36.29,34.83,29.02,43.5,75,68,70,67.5,80,77.5,68,68,73.84,72.14,68,64.5,58.5,72.63,78.44,71.17,70.69,77,79,78,68.5,69.72,71.66,77,77,79,76.5,78.5,79,73,80,69.72)
>
>data<-data.frame (SVL, ANO)# creo data frame
>data
>> Logiscorri<-function(){
>+   a<-runif(1, min=0, max=150)#devuelve 1 al azar dentro de un max y
>un
>min
>+   b<-runif(1, min=0, max=100)
>+   g<-runif (1, min=0, max=1)
>+   d<-runif (1,min=0, max=100)
>+
>+   ## estimo la curva de distribucion de mis datos
>+   caiman<-nls(SVL~DE+(alfa/(1+exp(-gamma*ANO))),
>+               data=data,
>+               start=list(alfa= a  ,gamma= g, DE= d),
>+               control=nls.control(maxiter = 100, warnOnly=TRUE),
>+               trace=FALSE)
>+   caimansum<-summary(caiman)#ME DA LOS PARAMETROS ESTIMADO, EL NUM DE
>ITERACIONES
>+   ## analizamos akaike
>+   akaike<-AIC(caiman)
>+   Bayesiano<-BIC(caiman)
>+   alfa<-coef(caiman)[1]
>+   beta<-coef(caiman)[2]
>+   gamma<- coef(caiman)[3]
>+   DE<- coef(caiman)[4]
>+   formu<-formula(caiman)
>+
>+   ValoresIniciales<-c(a, g, d)
>+   resultados<-list(formu, caimansum, ValoresIniciales, akaike,
>Bayesiano)
>+   return(resultados)
>+ }
>> Logiscorri()
>[[1]]
>SVL ~ DE + (alfa/(1 + exp(-gamma * ANO)))
><environment: 0x16a6b89c>
>
>[[2]]
>
>Formula: SVL ~ DE + (alfa/(1 + exp(-gamma * ANO)))
>
>Parameters:
>      Estimate Std. Error t value Pr(>|t|)
>alfa  133.0765     6.9537  19.138  < 2e-16 ***
>gamma   0.2746     0.0371   7.401 1.13e-09 ***
>DE    -54.0467     7.1047  -7.607 5.34e-10 ***
>---
>Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
>Residual standard error: 6.821 on 52 degrees of freedom
>
>Number of iterations to convergence: 30
>Achieved convergence tolerance: 4.995e-06
>
>
>[[3]]
>[1] 112.2528283   0.4831461  38.5151401
>
>[[4]]
>[1] 372.2001
>
>[[5]]
>[1] 380.2294
>
>> resultados<-list()
>> resultados
>list()
>> for(i in 1:10){
>+   resultados[i]<- list(Logiscorri())
>+ }
>Error in chol2inv(object$m$Rmat()) :
>  element (2, 2) is zero, so the inverse cannot be computed
>In addition: Warning message:
>In nls(SVL ~ DE + (alfa/(1 + exp(-gamma * ANO))), data = data, start =
>list(alfa = a,  :
>  singular gradient
>> names(resultados)<- 1:10
>Error in names(resultados) <- 1:10 :
>  'names' attribute [10] must be the same length as the vector [4]
>> parametros<- t(sapply(LogisticoConCorri, "[[", "par?metros")) #estp
>lo
>que hace es ir item por item de la lista y sacar los par?metros
>Error in FUN(X[[i]], ...) : subscript out of bounds
>> colnames(parametros)<- c("alfa", "beta", "gamma", "DE")
>Error in dimnames(x) <- dn :
>  length of 'dimnames' [2] not equal to array extent
>> akaikefinal<- sapply(LogisticoConCorri, "[[", "akaike")#esto va item
>por
>item de la lista y saca el akaike
>Error in FUN(X[[i]], ...) : subscript out of bounds
>> bayesfinal<- sapply(LogisticoConCorri, "[[", "Bayesiano")
>Error in FUN(X[[i]], ...) : subscript out of bounds
>>
>> --
>Bi?l. Evangelina V. Viotto
>Laboratorio Ecolog?a Animal
>Centro de investigaciones Cient?ficas y de Transferencias de
>Tecnolog?a Aplicada a la Producci?n
>(CICyTTP-CONICET-UADER)
>Diamante, Entre R?os
>Argentina
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From profjcnash at gmail.com  Fri Oct 20 18:53:22 2017
From: profjcnash at gmail.com (J C Nash)
Date: Fri, 20 Oct 2017 12:53:22 -0400
Subject: [R] nls() and loop
In-Reply-To: <4FE6917D-480D-45EB-9E63-17758D4B0DD0@dcn.davis.ca.us>
References: <CAEMjoTkNRyuRA8trq46wo_CrzWg3mZ3ZbUsuyYp=qvsN2N7sLw@mail.gmail.com>
 <4FE6917D-480D-45EB-9E63-17758D4B0DD0@dcn.davis.ca.us>
Message-ID: <411e40fa-6d38-8057-8a5c-7097f8081d82@gmail.com>

Yes, some form of try() is often needed with nls() to avoid scripts stopping.

You might also find nlxb() from package nlsr more reliable in finding solutions. It uses analytic derivatives if
available if the model is given as an expression, and a Marquardt stabilized solver. But do expect it to take more
iterations. The syntax is close, but not perfectly equivalent, to that of nls().

JN

On 2017-10-20 11:20 AM, Jeff Newmiller wrote:
> ?tryCatch
>


From tmrsg11 at gmail.com  Fri Oct 20 20:11:22 2017
From: tmrsg11 at gmail.com (C W)
Date: Fri, 20 Oct 2017 14:11:22 -0400
Subject: [R] What exactly is an dgCMatrix-class. There are so many
	attributes.
Message-ID: <CAE2FW2kqZ=1EYb3Yx21Mzk9FHyHBVUAqrCjF0DFkk6WWbGO0tw@mail.gmail.com>

Dear R list,

I came across dgCMatrix. I believe this class is associated with sparse
matrix.

I see there are 8 attributes to train$data, I am confused why are there so
many, some are vectors, what do they do?

Here's the R code:

library(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
test <- agaricus.test
attributes(train$data)

Where is the data, is it in $p, $i, or $x?

Thank you very much!

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Fri Oct 20 20:42:36 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 20 Oct 2017 11:42:36 -0700
Subject: [R] What exactly is an dgCMatrix-class. There are so many
	attributes.
In-Reply-To: <CAE2FW2kqZ=1EYb3Yx21Mzk9FHyHBVUAqrCjF0DFkk6WWbGO0tw@mail.gmail.com>
References: <CAE2FW2kqZ=1EYb3Yx21Mzk9FHyHBVUAqrCjF0DFkk6WWbGO0tw@mail.gmail.com>
Message-ID: <CAF8bMcYc07sAy5qR+jjvZH0ADhjtJoyQ6GUfKhuuQABsNiWOPA@mail.gmail.com>

You should not really have worry about the internal structure of such a
thing - just treat it like a matrix.  E.g.,

> train$data[1:3,1:3] # dots mean 0's
3 x 3 sparse Matrix of class "dgCMatrix"
     cap-shape=bell cap-shape=conical cap-shape=convex
[1,]              .                 .                1
[2,]              .                 .                1
[3,]              1                 .                .
> dim(train$data)
[1] 6513  126
> p <- train$data %*% matrix(1:126, ncol=1)
> dim(p)
[1] 6513    1


If that doesn't work in some situation, convert it to a matrix with
as.matrix.

To see the details, in R, type
   class?dgCMatrix
or
   help("dgCMatrix-class")
In a browser seach window type
   R dgCmatrix
You should not have to make use of those details in your code.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Fri, Oct 20, 2017 at 11:11 AM, C W <tmrsg11 at gmail.com> wrote:

> Dear R list,
>
> I came across dgCMatrix. I believe this class is associated with sparse
> matrix.
>
> I see there are 8 attributes to train$data, I am confused why are there so
> many, some are vectors, what do they do?
>
> Here's the R code:
>
> library(xgboost)
> data(agaricus.train, package='xgboost')
> data(agaricus.test, package='xgboost')
> train <- agaricus.train
> test <- agaricus.test
> attributes(train$data)
>
> Where is the data, is it in $p, $i, or $x?
>
> Thank you very much!
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Oct 20 21:22:26 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 20 Oct 2017 12:22:26 -0700
Subject: [R] What exactly is an dgCMatrix-class. There are so many
	attributes.
In-Reply-To: <CAE2FW2kqZ=1EYb3Yx21Mzk9FHyHBVUAqrCjF0DFkk6WWbGO0tw@mail.gmail.com>
References: <CAE2FW2kqZ=1EYb3Yx21Mzk9FHyHBVUAqrCjF0DFkk6WWbGO0tw@mail.gmail.com>
Message-ID: <DD993CA5-4057-469D-9B1A-E98241354563@comcast.net>


> On Oct 20, 2017, at 11:11 AM, C W <tmrsg11 at gmail.com> wrote:
> 
> Dear R list,
> 
> I came across dgCMatrix. I believe this class is associated with sparse
> matrix.

Yes. See:

 help('dgCMatrix-class', pack=Matrix)

If Martin Maechler happens to respond to this you should listen to him rather than anything I write. Much of what the Matrix package does appears to be magical to one such as I.

> 
> I see there are 8 attributes to train$data, I am confused why are there so
> many, some are vectors, what do they do?
> 
> Here's the R code:
> 
> library(xgboost)
> data(agaricus.train, package='xgboost')
> data(agaricus.test, package='xgboost')
> train <- agaricus.train
> test <- agaricus.test
> attributes(train$data)
> 

I got a bit of an annoying surprise when I did something similar. It appearred to me that I did not need to load the xgboost library since all that was being asked was "where is the data" in an object that should be loaded from that library using the `data` function. The last command asking for the attributes filled up my console with a 100K length vector (actually 2 of such vectors). The `str` function returns a more useful result.

> data(agaricus.train, package='xgboost')
> train <- agaricus.train
> names( attributes(train$data) )
[1] "i"        "p"        "Dim"      "Dimnames" "x"        "factors"  "class"   
> str(train$data)
Formal class 'dgCMatrix' [package "Matrix"] with 6 slots
  ..@ i       : int [1:143286] 2 6 8 11 18 20 21 24 28 32 ...
  ..@ p       : int [1:127] 0 369 372 3306 5845 6489 6513 8380 8384 10991 ...
  ..@ Dim     : int [1:2] 6513 126
  ..@ Dimnames:List of 2
  .. ..$ : NULL
  .. ..$ : chr [1:126] "cap-shape=bell" "cap-shape=conical" "cap-shape=convex" "cap-shape=flat" ...
  ..@ x       : num [1:143286] 1 1 1 1 1 1 1 1 1 1 ...
  ..@ factors : list()

> Where is the data, is it in $p, $i, or $x?

So the "data" (meaning the values of the sparse matrix) are in the @x leaf. The values all appear to be the number 1. The @i leaf is the sequence of row locations for the values entries while the @p items are somehow connected with the columns (I think, since 127 and 126=number of columns from the @Dim leaf are only off by 1). 

Doing this > colSums(as.matrix(train$data))
                  cap-shape=bell                cap-shape=conical 
                             369                                3 
                cap-shape=convex                   cap-shape=flat 
                            2934                             2539 
               cap-shape=knobbed                 cap-shape=sunken 
                             644                               24 
             cap-surface=fibrous              cap-surface=grooves 
                            1867                                4 
               cap-surface=scaly               cap-surface=smooth 
                            2607                             2035 
                 cap-color=brown                   cap-color=buff 
                            1816  
# now snipping the rest of that output.



Now this makes me think that the @p vector gives you the cumulative sum of number of items per column:

> all( cumsum( colSums(as.matrix(train$data)) ) == train$data at p[-1] )
[1] TRUE

> 
> Thank you very much!
> 
> 	[[alternative HTML version deleted]]

Please read the Posting Guide. Your code was not mangled in this instance, but HTML code often arrives in an unreadable mess.

> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From tmrsg11 at gmail.com  Fri Oct 20 21:51:16 2017
From: tmrsg11 at gmail.com (C W)
Date: Fri, 20 Oct 2017 15:51:16 -0400
Subject: [R] What exactly is an dgCMatrix-class. There are so many
	attributes.
In-Reply-To: <DD993CA5-4057-469D-9B1A-E98241354563@comcast.net>
References: <CAE2FW2kqZ=1EYb3Yx21Mzk9FHyHBVUAqrCjF0DFkk6WWbGO0tw@mail.gmail.com>
 <DD993CA5-4057-469D-9B1A-E98241354563@comcast.net>
Message-ID: <CAE2FW2nkH0ypjvPHjzPS632_Px=hBvSEWO_pcH9e3y3bhbcQFw@mail.gmail.com>

Thank you for your responses.

I guess I don't feel alone. I don't find the documentation go into any
detail.

I also find it surprising that,

> object.size(train$data)
1730904 bytes

> object.size(as.matrix(train$data))
6575016 bytes

the dgCMatrix actually takes less memory, though it *looks* like the
opposite.

Cheers!

On Fri, Oct 20, 2017 at 3:22 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Oct 20, 2017, at 11:11 AM, C W <tmrsg11 at gmail.com> wrote:
> >
> > Dear R list,
> >
> > I came across dgCMatrix. I believe this class is associated with sparse
> > matrix.
>
> Yes. See:
>
>  help('dgCMatrix-class', pack=Matrix)
>
> If Martin Maechler happens to respond to this you should listen to him
> rather than anything I write. Much of what the Matrix package does appears
> to be magical to one such as I.
>
> >
> > I see there are 8 attributes to train$data, I am confused why are there
> so
> > many, some are vectors, what do they do?
> >
> > Here's the R code:
> >
> > library(xgboost)
> > data(agaricus.train, package='xgboost')
> > data(agaricus.test, package='xgboost')
> > train <- agaricus.train
> > test <- agaricus.test
> > attributes(train$data)
> >
>
> I got a bit of an annoying surprise when I did something similar. It
> appearred to me that I did not need to load the xgboost library since all
> that was being asked was "where is the data" in an object that should be
> loaded from that library using the `data` function. The last command asking
> for the attributes filled up my console with a 100K length vector (actually
> 2 of such vectors). The `str` function returns a more useful result.
>
> > data(agaricus.train, package='xgboost')
> > train <- agaricus.train
> > names( attributes(train$data) )
> [1] "i"        "p"        "Dim"      "Dimnames" "x"        "factors"
> "class"
> > str(train$data)
> Formal class 'dgCMatrix' [package "Matrix"] with 6 slots
>   ..@ i       : int [1:143286] 2 6 8 11 18 20 21 24 28 32 ...
>   ..@ p       : int [1:127] 0 369 372 3306 5845 6489 6513 8380 8384 10991
> ...
>   ..@ Dim     : int [1:2] 6513 126
>   ..@ Dimnames:List of 2
>   .. ..$ : NULL
>   .. ..$ : chr [1:126] "cap-shape=bell" "cap-shape=conical"
> "cap-shape=convex" "cap-shape=flat" ...
>   ..@ x       : num [1:143286] 1 1 1 1 1 1 1 1 1 1 ...
>   ..@ factors : list()
>
> > Where is the data, is it in $p, $i, or $x?
>
> So the "data" (meaning the values of the sparse matrix) are in the @x
> leaf. The values all appear to be the number 1. The @i leaf is the sequence
> of row locations for the values entries while the @p items are somehow
> connected with the columns (I think, since 127 and 126=number of columns
> from the @Dim leaf are only off by 1).
>
> Doing this > colSums(as.matrix(train$data))
>                   cap-shape=bell                cap-shape=conical
>                              369                                3
>                 cap-shape=convex                   cap-shape=flat
>                             2934                             2539
>                cap-shape=knobbed                 cap-shape=sunken
>                              644                               24
>              cap-surface=fibrous              cap-surface=grooves
>                             1867                                4
>                cap-surface=scaly               cap-surface=smooth
>                             2607                             2035
>                  cap-color=brown                   cap-color=buff
>                             1816
> # now snipping the rest of that output.
>
>
>
> Now this makes me think that the @p vector gives you the cumulative sum of
> number of items per column:
>
> > all( cumsum( colSums(as.matrix(train$data)) ) == train$data at p[-1] )
> [1] TRUE
>
> >
> > Thank you very much!
> >
> >       [[alternative HTML version deleted]]
>
> Please read the Posting Guide. Your code was not mangled in this instance,
> but HTML code often arrives in an unreadable mess.
>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> 'Any technology distinguishable from magic is insufficiently advanced.'
>  -Gehm's Corollary to Clarke's Third Law
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From tmrsg11 at gmail.com  Fri Oct 20 22:01:06 2017
From: tmrsg11 at gmail.com (C W)
Date: Fri, 20 Oct 2017 16:01:06 -0400
Subject: [R] What exactly is an dgCMatrix-class. There are so many
	attributes.
In-Reply-To: <CAE2FW2nkH0ypjvPHjzPS632_Px=hBvSEWO_pcH9e3y3bhbcQFw@mail.gmail.com>
References: <CAE2FW2kqZ=1EYb3Yx21Mzk9FHyHBVUAqrCjF0DFkk6WWbGO0tw@mail.gmail.com>
 <DD993CA5-4057-469D-9B1A-E98241354563@comcast.net>
 <CAE2FW2nkH0ypjvPHjzPS632_Px=hBvSEWO_pcH9e3y3bhbcQFw@mail.gmail.com>
Message-ID: <CAE2FW2nuXrFgoo0ZCmgq4W0YEZtiSUjuFDkaOKbkUcni2=gw1A@mail.gmail.com>

Subsetting using [] vs. head(), gives different results.

R code:

> head(train$data, 5)
[1] 0 0 1 0 0

> train$data[1:5, 1:5]
5 x 5 sparse Matrix of class "dgCMatrix"
     cap-shape=bell cap-shape=conical cap-shape=convex
[1,]              .                 .                1
[2,]              .                 .                1
[3,]              1                 .                .
[4,]              .                 .                1
[5,]              .                 .                1
     cap-shape=flat cap-shape=knobbed
[1,]              .                 .
[2,]              .                 .
[3,]              .                 .
[4,]              .                 .
[5,]              .                 .

On Fri, Oct 20, 2017 at 3:51 PM, C W <tmrsg11 at gmail.com> wrote:

> Thank you for your responses.
>
> I guess I don't feel alone. I don't find the documentation go into any
> detail.
>
> I also find it surprising that,
>
> > object.size(train$data)
> 1730904 bytes
>
> > object.size(as.matrix(train$data))
> 6575016 bytes
>
> the dgCMatrix actually takes less memory, though it *looks* like the
> opposite.
>
> Cheers!
>
> On Fri, Oct 20, 2017 at 3:22 PM, David Winsemius <dwinsemius at comcast.net>
> wrote:
>
>>
>> > On Oct 20, 2017, at 11:11 AM, C W <tmrsg11 at gmail.com> wrote:
>> >
>> > Dear R list,
>> >
>> > I came across dgCMatrix. I believe this class is associated with sparse
>> > matrix.
>>
>> Yes. See:
>>
>>  help('dgCMatrix-class', pack=Matrix)
>>
>> If Martin Maechler happens to respond to this you should listen to him
>> rather than anything I write. Much of what the Matrix package does appears
>> to be magical to one such as I.
>>
>> >
>> > I see there are 8 attributes to train$data, I am confused why are there
>> so
>> > many, some are vectors, what do they do?
>> >
>> > Here's the R code:
>> >
>> > library(xgboost)
>> > data(agaricus.train, package='xgboost')
>> > data(agaricus.test, package='xgboost')
>> > train <- agaricus.train
>> > test <- agaricus.test
>> > attributes(train$data)
>> >
>>
>> I got a bit of an annoying surprise when I did something similar. It
>> appearred to me that I did not need to load the xgboost library since all
>> that was being asked was "where is the data" in an object that should be
>> loaded from that library using the `data` function. The last command asking
>> for the attributes filled up my console with a 100K length vector (actually
>> 2 of such vectors). The `str` function returns a more useful result.
>>
>> > data(agaricus.train, package='xgboost')
>> > train <- agaricus.train
>> > names( attributes(train$data) )
>> [1] "i"        "p"        "Dim"      "Dimnames" "x"        "factors"
>> "class"
>> > str(train$data)
>> Formal class 'dgCMatrix' [package "Matrix"] with 6 slots
>>   ..@ i       : int [1:143286] 2 6 8 11 18 20 21 24 28 32 ...
>>   ..@ p       : int [1:127] 0 369 372 3306 5845 6489 6513 8380 8384 10991
>> ...
>>   ..@ Dim     : int [1:2] 6513 126
>>   ..@ Dimnames:List of 2
>>   .. ..$ : NULL
>>   .. ..$ : chr [1:126] "cap-shape=bell" "cap-shape=conical"
>> "cap-shape=convex" "cap-shape=flat" ...
>>   ..@ x       : num [1:143286] 1 1 1 1 1 1 1 1 1 1 ...
>>   ..@ factors : list()
>>
>> > Where is the data, is it in $p, $i, or $x?
>>
>> So the "data" (meaning the values of the sparse matrix) are in the @x
>> leaf. The values all appear to be the number 1. The @i leaf is the sequence
>> of row locations for the values entries while the @p items are somehow
>> connected with the columns (I think, since 127 and 126=number of columns
>> from the @Dim leaf are only off by 1).
>>
>> Doing this > colSums(as.matrix(train$data))
>>                   cap-shape=bell                cap-shape=conical
>>                              369                                3
>>                 cap-shape=convex                   cap-shape=flat
>>                             2934                             2539
>>                cap-shape=knobbed                 cap-shape=sunken
>>                              644                               24
>>              cap-surface=fibrous              cap-surface=grooves
>>                             1867                                4
>>                cap-surface=scaly               cap-surface=smooth
>>                             2607                             2035
>>                  cap-color=brown                   cap-color=buff
>>                             1816
>> # now snipping the rest of that output.
>>
>>
>>
>> Now this makes me think that the @p vector gives you the cumulative sum
>> of number of items per column:
>>
>> > all( cumsum( colSums(as.matrix(train$data)) ) == train$data at p[-1] )
>> [1] TRUE
>>
>> >
>> > Thank you very much!
>> >
>> >       [[alternative HTML version deleted]]
>>
>> Please read the Posting Guide. Your code was not mangled in this
>> instance, but HTML code often arrives in an unreadable mess.
>>
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> David Winsemius
>> Alameda, CA, USA
>>
>> 'Any technology distinguishable from magic is insufficiently advanced.'
>>  -Gehm's Corollary to Clarke's Third Law
>>
>>
>>
>>
>>
>>
>

	[[alternative HTML version deleted]]


From marna.wagley at gmail.com  Fri Oct 20 22:12:07 2017
From: marna.wagley at gmail.com (Marna Wagley)
Date: Fri, 20 Oct 2017 13:12:07 -0700
Subject: [R] create a loop
Message-ID: <CAMwU6B1dPxpN9MkWYKWtkew=P5tGOXk92xZvC6p2Wyp+coMWAw@mail.gmail.com>

Hi R Users,
I do have very big data sets and wanted to run some of the analyses many
times with randomization (1000 times).
I have done the analysis using an example data but it need to be done with
randomized data (1000 times). I am doing manually for 10000 times but
taking so much time, I wonder whether it is possible to perform the
analysis with creating a loop for many replicated datasets?  The code and
the example data sets are attached.

I will be very grateful if someone help me to create the loop for the
following example data and the analyses.

I appreciate  your help.


MW

#####

dat1<-structure(list(RegionA = structure(c(1L, 1L, 2L, 3L, 3L, 4L, 5L, 5L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L), .Label = c("Ra", "Rb", "Rc",
"Rd", "Re", "Rf"), class = "factor"), site = structure(c(1L, 12L, 13L, 14L,
15L, 16L, 17L, 18L, 19L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L), .Label
= c("s1", "s10", "s11", "s12", "s13", "s14", "s15", "s16", "s17", "s18",
"s19", "s2", "s3", "s4", "s5", "s6", "s7", "s8", "s9"), class = "factor"),
temp = c(23L, 21L, 10L, 15L, 16L, 8L, 13L, 1L, 23L, 19L, 25L, 19L, 12L, 16L,
19L, 21L, 12L, 5L, 7L), group = structure(c(1L, 1L, 1L, 2L, 2L, 2L,

2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), .Label = c("A", "B",
"C"), class = "factor")), .Names = c("RegionA", "site", "temp", "group"),
class = "data.frame", row.names = c(NA, -19L))

head(dat1)


dat2<-structure(list(group = structure(1:3, .Label = c("A", "B", "C"

), class = "factor"), totalP = c(250L, 375L, 180L), sampled = c(25L,

37L, 27L)), .Names = c("group", "total.pop", "sampled.pop"), class =
"data.frame", row.names = c(NA,

-3L))


##

idx <- 1:nrow(dat1)

lll <- split(idx, dat1$group)


##########################

#Replication 1 create a resampled data

############################

Replication1<-dat1[unlist(lapply(lll, sample, rep=TRUE)),]


Summary.Rep1<-ddply(Replication1, c("group"), summarise,

N    = length(group),

       mean = mean(temp, na.rm=TRUE),

               sd   = sd(temp),

               se   = sd / sqrt(N),

               variance=sd^2

)

#merge two datasets (dat1 and dat2)

Rep1<-merge(Summary.Rep1, dat2, by="group")


#calclate adjusted mean. variance

Rep1$adj.mean<-(Rep1$total.pop*Rep1$mean)/sum(Rep1$total.pop)

Rep1$adj.var<-(Rep1$variance)/(Rep1$sampled.pop/(1-(Rep1$sampled.pop/Rep1$
total.pop)))

Rep1$over.adj.var<-(Rep1$total.pop/sum(Rep1$total.pop))^2*Rep1$adj.var


Rep1$total<-Rep1$adj.mean*(Rep1$total.pop)

##

Estimated.TotalTemp<-sum(Rep1$adj.mean)*sum(Rep1$total.pop)

Estimated.totalvar<-sum(Rep1$adj.var)

Estimated.SE<-sqrt(Estimated.totalvar)*sum(Rep1$total.pop)

RESULTS.R1<-data.frame(Estimated.TotalTemp, SE=Estimated.SE)

RESULTS.R1




##########################

#Replication 2 create a resampled data

############################

Replication2<-dat1[unlist(lapply(lll, sample, rep=TRUE)),]


Summary.Rep2<-ddply(Replication2, c("group"), summarise,

N    = length(group),

       mean = mean(temp, na.rm=TRUE),

               sd   = sd(temp),

               se   = sd / sqrt(N),

               variance=sd^2

)

#merge two datasets

Rep1<-merge(Summary.Rep2, dat2, by="group")


#calclate adjusted mean. variance

Rep2$adj.mean<-(Rep2$total.pop*Rep2$mean)/sum(Rep2$total.pop)

Rep2$adj.var<-(Rep2$variance)/(Rep2$sampled.pop/(1-(Rep2$sampled.pop/Rep2$
total.pop)))

Rep2$over.adj.var<-(Rep2$total.pop/sum(Rep2$total.pop))^2*Rep2$adj.var


Rep2$total<-Rep2$adj.mean*(Rep2$total.pop)


##

Estimated.TotalTemp<-sum(Rep2$adj.mean)*sum(Rep2$total.pop)

Estimated.totalvar<-sum(Rep2$adj.var)

Estimated.SE<-sqrt(Estimated.totalvar)*sum(Rep2$total.pop)

RESULTS.R2<-data.frame(Estimated.TotalTemp, SE=Estimated.SE)


##############################

#combined all results from 1000 runs

ALL.Results(Restult.R1, Result.R2....)

	[[alternative HTML version deleted]]


From laradutrasilva at gmail.com  Sat Oct 21 02:11:26 2017
From: laradutrasilva at gmail.com (Lara Dutra Silva)
Date: Sat, 21 Oct 2017 00:11:26 +0000
Subject: [R] Help_urgent_how to calculate mean and sd in biomod 2
Message-ID: <CAJRXdJCS9GRCXVJNhqDcCQTki4c53Cy077v0mjYjVS8oWwRGzg@mail.gmail.com>

Hello

I am new in R. I am trying to implement Biomod 2 package.

However, I have a doubt. I want to calculate the mean and sd of
"Testing.data"
(ROC and TSS)


> # let's print the ROC scores of all selected models

> myBiomodModelEval_55["ROC","Testing.data",,,]

 RUN1  RUN2  RUN3  RUN4  RUN5  RUN6  RUN7  RUN8  RUN9 RUN10

0.938 0.938 0.926 0.931 0.939 0.918 0.920 0.914 0.935 0.919

>

> # let's print the TSS scores

> myBiomodModelEval_55["TSS","Testing.data",,,]

 RUN1  RUN2  RUN3  RUN4  RUN5  RUN6  RUN7  RUN8  RUN9 RUN10

0.746 0.763 0.717 0.758 0.754 0.704 0.700 0.725 0.742 0.721

>



I try to use "apply"

apply(myBiomodModelEval_55["TSS","Testing.data",,,], 1, mean)

apply(myBiomodModelEval_55["ROC","Testing.data",,,],1, mean)

apply(myBiomodModelEval_55["ROC","Testing.data",,,], 1, sd)

apply(myBiomodModelEval_55["TSS","Testing.data",,,], 1, sd)



I can not figure it out because it runs error.
The problem is in the dimension?

Error in apply(myBiomodModelEval_55["TSS", "Testing.data", , , ], 1, mean) :

  dim(X) must have a positive length

>


How can I solve this?

This is the structure of object

> dimnames(myBiomodModelEval_55)
[[1]]
[1] "ROC" "TSS"

[[2]]
[1] "Testing.data" "Cutoff"       "Sensitivity"  "Specificity"

[[3]]
[1] "GAM"

[[4]]
 [1] "RUN1"  "RUN2"  "RUN3"  "RUN4"  "RUN5"  "RUN6"  "RUN7"  "RUN8"  "RUN9"
[10] "RUN10"

[[5]]
Acacia_AllData
     "AllData"


Regards,
Silva

	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Sat Oct 21 06:40:45 2017
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Sat, 21 Oct 2017 05:40:45 +0100
Subject: [R] Help_urgent_how to calculate mean and sd in biomod 2
In-Reply-To: <CAJRXdJCS9GRCXVJNhqDcCQTki4c53Cy077v0mjYjVS8oWwRGzg@mail.gmail.com>
References: <CAJRXdJCS9GRCXVJNhqDcCQTki4c53Cy077v0mjYjVS8oWwRGzg@mail.gmail.com>
Message-ID: <59EACFCD.1000103@sapo.pt>

Hello,

This is because myBiomodModelEval_55["ROC","Testing.data",,,] is a 
vector not an array or matrix. What the error message is saying is that 
dim() is not returning a value (it's length is not positive, since it 
cannot be negative length(dim(.)) must be zero). Try it.
Or see the class of those objects.

class(myBiomodModelEval_55["ROC","Testing.data",,,])
class(myBiomodModelEval_55["TSS","Testing.data",,,])


You do not apply(), simply do

mean(myBiomodModelEval_55["ROC","Testing.data",,,])

And the same for sd().

Hope this helps,

Rui Barradas

Em 21-10-2017 01:11, Lara Dutra Silva escreveu:
> Hello
>
> I am new in R. I am trying to implement Biomod 2 package.
>
> However, I have a doubt. I want to calculate the mean and sd of
> "Testing.data"
> (ROC and TSS)
>
>
>> # let's print the ROC scores of all selected models
>
>> myBiomodModelEval_55["ROC","Testing.data",,,]
>
>   RUN1  RUN2  RUN3  RUN4  RUN5  RUN6  RUN7  RUN8  RUN9 RUN10
>
> 0.938 0.938 0.926 0.931 0.939 0.918 0.920 0.914 0.935 0.919
>
>>
>
>> # let's print the TSS scores
>
>> myBiomodModelEval_55["TSS","Testing.data",,,]
>
>   RUN1  RUN2  RUN3  RUN4  RUN5  RUN6  RUN7  RUN8  RUN9 RUN10
>
> 0.746 0.763 0.717 0.758 0.754 0.704 0.700 0.725 0.742 0.721
>
>>
>
>
>
> I try to use "apply"
>
> apply(myBiomodModelEval_55["TSS","Testing.data",,,], 1, mean)
>
> apply(myBiomodModelEval_55["ROC","Testing.data",,,],1, mean)
>
> apply(myBiomodModelEval_55["ROC","Testing.data",,,], 1, sd)
>
> apply(myBiomodModelEval_55["TSS","Testing.data",,,], 1, sd)
>
>
>
> I can not figure it out because it runs error.
> The problem is in the dimension?
>
> Error in apply(myBiomodModelEval_55["TSS", "Testing.data", , , ], 1, mean) :
>
>    dim(X) must have a positive length
>
>>
>
>
> How can I solve this?
>
> This is the structure of object
>
>> dimnames(myBiomodModelEval_55)
> [[1]]
> [1] "ROC" "TSS"
>
> [[2]]
> [1] "Testing.data" "Cutoff"       "Sensitivity"  "Specificity"
>
> [[3]]
> [1] "GAM"
>
> [[4]]
>   [1] "RUN1"  "RUN2"  "RUN3"  "RUN4"  "RUN5"  "RUN6"  "RUN7"  "RUN8"  "RUN9"
> [10] "RUN10"
>
> [[5]]
> Acacia_AllData
>       "AllData"
>
>
> Regards,
> Silva
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From morkus at protonmail.com  Sat Oct 21 13:35:19 2017
From: morkus at protonmail.com (Morkus)
Date: Sat, 21 Oct 2017 07:35:19 -0400
Subject: [R] Problem when trying to run Java in R:
Message-ID: <wXugqhHDrV6BFbzd1ja-qa0Od__Q1rBn29Q525GZbz2UhFxMYVwgD7d6M6UCJDH8QBVtCY2-1XXfi-C45hEvIidJ-UfFn985ILMw9cAb4nY=@protonmail.com>

Hello All,

Although running Java from R used to work, for some mysterious reason, it's stopped.

Today when I tried to load a basic JDBC driver (or the sample .jinit()) code, I got:

- JavaVM: requested Java version ((null)) not available. Using Java at "" instead.

- JavaVM: Failed to load JVM: /bundle/Libraries/libserver.dylib

- JavaVM FATAL: Failed to load the jvm library.

I saw postings on StackOverflow about this issue, but none of the suggested fixes helped.

I'm on Mac OS 10.13.

My JAVA_HOME is: /Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home

java -version
java version "1.8.0_144"
Java(TM) SE Runtime Environment (build 1.8.0_144-b01)
Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)

I'm using R: 3.3.3

Here is sample code that also throws this same error:

    library(rJava)
    .jinit() # this starts the JVM
    s <- .jnew("java/lang/String", "Hello World!")

(this is the "hello world" equivalent from the rJava site)

----

I also tried to use Java 7 (rev. 51), which used to work, but that still fails.

Also tried a fresh install of R/RStudio on another machine with the same results (same Java version, etc., however).

I suspect Java itself is working fine since JDBC code from Java programs has no issues.

Not sure why loading Java in R stopped working, but would appreciate any suggestions.

Thanks very much,

Sent from [ProtonMail](https://protonmail.com), Swiss-based encrypted email.
	[[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Sat Oct 21 16:50:47 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 21 Oct 2017 16:50:47 +0200
Subject: [R] What exactly is an dgCMatrix-class. There are so
	many	attributes.
In-Reply-To: <CAE2FW2nkH0ypjvPHjzPS632_Px=hBvSEWO_pcH9e3y3bhbcQFw@mail.gmail.com>
References: <CAE2FW2kqZ=1EYb3Yx21Mzk9FHyHBVUAqrCjF0DFkk6WWbGO0tw@mail.gmail.com>
 <DD993CA5-4057-469D-9B1A-E98241354563@comcast.net>
 <CAE2FW2nkH0ypjvPHjzPS632_Px=hBvSEWO_pcH9e3y3bhbcQFw@mail.gmail.com>
Message-ID: <23019.24263.905696.240862@stat.math.ethz.ch>

>>>>> C W <tmrsg11 at gmail.com>
>>>>>     on Fri, 20 Oct 2017 15:51:16 -0400 writes:

    > Thank you for your responses.  I guess I don't feel
    > alone. I don't find the documentation go into any detail.

    > I also find it surprising that,

    >> object.size(train$data)
    > 1730904 bytes

    >> object.size(as.matrix(train$data))
    > 6575016 bytes

    > the dgCMatrix actually takes less memory, though it
    > *looks* like the opposite.

to whom?

The whole idea of these sparse matrix classes in the 'Matrix'
package (and everywhere else in applied math, CS, ...) is that
1. they need  much less memory   and
2. matrix arithmetic with them can be much faster because it is based on
   sophisticated sparse matrix linear algebra, notably the
   sparse Cholesky decomposition for solve() etc.

Of course the efficency only applies if most of the
matrix entries _are_ 0.
You can measure the  "sparsity" or rather the  "density", of a
matrix by

  nnzero(A) / length(A)

where length(A) == nrow(A) * ncol(A)  as for regular matrices
(but it does *not* integer overflow)
and nnzero(.) is a simple utility from Matrix
which -- very efficiently for sparseMatrix objects -- gives the
number of nonzero entries of the matrix.
   
All of these classes are formally defined classes and have
therefore help pages.  Here  ?dgCMatrix-class  which then points
to  ?CsparseMatrix-class  (and I forget if Rstudio really helps
you find these ..; in emacs ESS they are found nicely via the usual key)

To get started, you may further look at  ?Matrix _and_  ?sparseMatrix
(and possibly the Matrix package vignettes --- though they need
 work -- I'm happy for collaborators there !)

Bill Dunlap's comment applies indeed:
In principle all these matrices should work like regular numeric
matrices, just faster with less memory foot print if they are
really sparse (and not just formally of a sparseMatrix class)
  ((and there are quite a few more niceties in the package))

Martin Maechler
(here, maintainer of 'Matrix')


    > On Fri, Oct 20, 2017 at 3:22 PM, David Winsemius <dwinsemius at comcast.net>
    > wrote:

    >> > On Oct 20, 2017, at 11:11 AM, C W <tmrsg11 at gmail.com> wrote:
    >> >
    >> > Dear R list,
    >> >
    >> > I came across dgCMatrix. I believe this class is associated with sparse
    >> > matrix.
    >> 
    >> Yes. See:
    >> 
    >> help('dgCMatrix-class', pack=Matrix)
    >> 
    >> If Martin Maechler happens to respond to this you should listen to him
    >> rather than anything I write. Much of what the Matrix package does appears
    >> to be magical to one such as I.
    >> 
    >> >
    >> > I see there are 8 attributes to train$data, I am confused why are there
    >> so
    >> > many, some are vectors, what do they do?
    >> >
    >> > Here's the R code:
    >> >
    >> > library(xgboost)
    >> > data(agaricus.train, package='xgboost')
    >> > data(agaricus.test, package='xgboost')
    >> > train <- agaricus.train
    >> > test <- agaricus.test
    >> > attributes(train$data)
    >> >
    >> 
    >> I got a bit of an annoying surprise when I did something similar. It
    >> appearred to me that I did not need to load the xgboost library since all
    >> that was being asked was "where is the data" in an object that should be
    >> loaded from that library using the `data` function. The last command asking
    >> for the attributes filled up my console with a 100K length vector (actually
    >> 2 of such vectors). The `str` function returns a more useful result.
    >> 
    >> > data(agaricus.train, package='xgboost')
    >> > train <- agaricus.train
    >> > names( attributes(train$data) )
    >> [1] "i"        "p"        "Dim"      "Dimnames" "x"        "factors"
    >> "class"
    >> > str(train$data)
    >> Formal class 'dgCMatrix' [package "Matrix"] with 6 slots
    >> ..@ i       : int [1:143286] 2 6 8 11 18 20 21 24 28 32 ...
    >> ..@ p       : int [1:127] 0 369 372 3306 5845 6489 6513 8380 8384 10991
    >> ...
    >> ..@ Dim     : int [1:2] 6513 126
    >> ..@ Dimnames:List of 2
    >> .. ..$ : NULL
    >> .. ..$ : chr [1:126] "cap-shape=bell" "cap-shape=conical"
    >> "cap-shape=convex" "cap-shape=flat" ...
    >> ..@ x       : num [1:143286] 1 1 1 1 1 1 1 1 1 1 ...
    >> ..@ factors : list()
    >> 
    >> > Where is the data, is it in $p, $i, or $x?
    >> 
    >> So the "data" (meaning the values of the sparse matrix) are in the @x
    >> leaf. The values all appear to be the number 1. The @i leaf is the sequence
    >> of row locations for the values entries while the @p items are somehow
    >> connected with the columns (I think, since 127 and 126=number of columns
    >> from the @Dim leaf are only off by 1).

You are right David.

well, they follow sparse matrix standards which (like C) start
counting at 0.

    >> 
    >> Doing this > colSums(as.matrix(train$data))

The above colSums() again is "very" inefficient:
All such R functions  have smartly defined  Matrix methods that
directly work on sparse matrices.

Note that  as.matrix(M)  can "blow up" your R, when the matrix M
is really large and sparse such that its dense version does not
even fit in your computer's RAM.

    >> cap-shape=bell                cap-shape=conical
    >> 369                                3
    >> cap-shape=convex                   cap-shape=flat
    >> 2934                             2539
    >> cap-shape=knobbed                 cap-shape=sunken
    >> 644                               24
    >> cap-surface=fibrous              cap-surface=grooves
    >> 1867                                4
    >> cap-surface=scaly               cap-surface=smooth
    >> 2607                             2035
    >> cap-color=brown                   cap-color=buff
    >> 1816
    >> # now snipping the rest of that output.
    >> 
    >> 
    >> 
    >> Now this makes me think that the @p vector gives you the cumulative sum of
    >> number of items per column:
    >> 
    >> > all( cumsum( colSums(as.matrix(train$data)) ) == train$data at p[-1] )
    >> [1] TRUE
    >> 
    >> >
    >> > Thank you very much!
    >> >
    >> >       [[alternative HTML version deleted]]
    >> 
    >> Please read the Posting Guide. Your code was not mangled in this instance,
    >> but HTML code often arrives in an unreadable mess.
    >> 
    >> >
    >> > ______________________________________________
    >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    >> > https://stat.ethz.ch/mailman/listinfo/r-help
    >> > PLEASE do read the posting guide http://www.R-project.org/posti
    >> ng-guide.html
    >> > and provide commented, minimal, self-contained, reproducible code.
    >> 
    >> David Winsemius
    >> Alameda, CA, USA
    >> 
    >> 'Any technology distinguishable from magic is insufficiently advanced.'
    >> -Gehm's Corollary to Clarke's Third Law
    >> 
    >> 
    >> 
    >> 
    >> 
    >> 

    > [[alternative HTML version deleted]]

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From milujisb at gmail.com  Sat Oct 21 17:01:02 2017
From: milujisb at gmail.com (Miluji Sb)
Date: Sat, 21 Oct 2017 17:01:02 +0200
Subject: [R] Skip error in downloading file in loop
Message-ID: <CAMLwc7NVC-M6t+M2HrRDFAZ=M7hTfP9KqSuuL_1FaTDNc8wjug@mail.gmail.com>

I am trying to download data from NASA web-service.

I am using the following code;

for( i in 1:8) {
  target1 <- paste0("
https://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/access/timeseries.cgi?variable=NLDAS:NLDAS_FORA0125_H.002:TMP2m&location=GEOM:POINT(
",
                    cities[i, "lon_nldas"],
                    ",%20", cities[i,"lat_nldas"],

")&startDate=1980-01-01T00&endDate=2016-12-31T23&type=asc2")
  target2 <- paste0("/Users/dasgupta/Dropbox (FEEM)/Flu Paper/climate
data/temperature/nldas/",    # change for whatever destination directory
you may prefer.
                    cities[i,"city"], "_",
                    cities[i,"state"], ".csv")
  download.file(url=target1, destfile=target2,  method = "libcurl")
}

Any time the coordinates provided is out data coverage, the loop fails and
stops. Is there any way to force R to skip the error and continue with the
rest of the download?

My data looks like this:

dput(droplevels(head(cities, 8)))
structure(list(city = structure(1:8, .Label = c("Boston", "Bridgeport",
"Cambridge", "Fall River", "Hartford", "Lowell", "Lynn", "New Bedford"
), class = "factor"), state = structure(c(2L, 1L, 2L, 2L, 1L,
2L, 2L, 2L), .Label = c(" CT ", " MA "), class = "factor"), lon_nldas =
c(-71.05673836,
-73.19126922, -71.1060061, -71.14364822, -72.67401574, -71.31283992,
-70.82521832, -70.80586599), lat_nldas = c(42.35866236, 41.18188268,
42.36679363, 41.69735342, 41.76349276, 42.64588413, 42.46370197,
41.63767375)), .Names = c("city", "state", "lon_nldas", "lat_nldas"
), row.names = c(NA, 8L), class = "data.frame")

Any help will be appreciated. Thank you very much!

Sincerely,

Milu

	[[alternative HTML version deleted]]


From sezenismail at gmail.com  Sat Oct 21 17:31:21 2017
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Sat, 21 Oct 2017 18:31:21 +0300
Subject: [R] create a loop
In-Reply-To: <CAMwU6B1dPxpN9MkWYKWtkew=P5tGOXk92xZvC6p2Wyp+coMWAw@mail.gmail.com>
References: <CAMwU6B1dPxpN9MkWYKWtkew=P5tGOXk92xZvC6p2Wyp+coMWAw@mail.gmail.com>
Message-ID: <CABOzBw3OoR=1F9BYpBFzMSJ9vJsHGZYKq_uAbx9H66UgaNEH3Q@mail.gmail.com>

you sould look at "boot" package. also search "bootstrap R" keywords in
google.

20 Eki 2017 23:12 tarihinde "Marna Wagley" <marna.wagley at gmail.com> yazd?:

> Hi R Users,
> I do have very big data sets and wanted to run some of the analyses many
> times with randomization (1000 times).
> I have done the analysis using an example data but it need to be done with
> randomized data (1000 times). I am doing manually for 10000 times but
> taking so much time, I wonder whether it is possible to perform the
> analysis with creating a loop for many replicated datasets?  The code and
> the example data sets are attached.
>
> I will be very grateful if someone help me to create the loop for the
> following example data and the analyses.
>
> I appreciate  your help.
>
>
> MW
>
> #####
>
> dat1<-structure(list(RegionA = structure(c(1L, 1L, 2L, 3L, 3L, 4L, 5L, 5L,
> 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L), .Label = c("Ra", "Rb", "Rc",
> "Rd", "Re", "Rf"), class = "factor"), site = structure(c(1L, 12L, 13L, 14L,
> 15L, 16L, 17L, 18L, 19L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L), .Label
> = c("s1", "s10", "s11", "s12", "s13", "s14", "s15", "s16", "s17", "s18",
> "s19", "s2", "s3", "s4", "s5", "s6", "s7", "s8", "s9"), class = "factor"),
> temp = c(23L, 21L, 10L, 15L, 16L, 8L, 13L, 1L, 23L, 19L, 25L, 19L, 12L,
> 16L,
> 19L, 21L, 12L, 5L, 7L), group = structure(c(1L, 1L, 1L, 2L, 2L, 2L,
>
> 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), .Label = c("A", "B",
> "C"), class = "factor")), .Names = c("RegionA", "site", "temp", "group"),
> class = "data.frame", row.names = c(NA, -19L))
>
> head(dat1)
>
>
> dat2<-structure(list(group = structure(1:3, .Label = c("A", "B", "C"
>
> ), class = "factor"), totalP = c(250L, 375L, 180L), sampled = c(25L,
>
> 37L, 27L)), .Names = c("group", "total.pop", "sampled.pop"), class =
> "data.frame", row.names = c(NA,
>
> -3L))
>
>
> ##
>
> idx <- 1:nrow(dat1)
>
> lll <- split(idx, dat1$group)
>
>
> ##########################
>
> #Replication 1 create a resampled data
>
> ############################
>
> Replication1<-dat1[unlist(lapply(lll, sample, rep=TRUE)),]
>
>
> Summary.Rep1<-ddply(Replication1, c("group"), summarise,
>
> N    = length(group),
>
>        mean = mean(temp, na.rm=TRUE),
>
>                sd   = sd(temp),
>
>                se   = sd / sqrt(N),
>
>                variance=sd^2
>
> )
>
> #merge two datasets (dat1 and dat2)
>
> Rep1<-merge(Summary.Rep1, dat2, by="group")
>
>
> #calclate adjusted mean. variance
>
> Rep1$adj.mean<-(Rep1$total.pop*Rep1$mean)/sum(Rep1$total.pop)
>
> Rep1$adj.var<-(Rep1$variance)/(Rep1$sampled.pop/(1-(Rep1$sampled.pop/Rep1$
> total.pop)))
>
> Rep1$over.adj.var<-(Rep1$total.pop/sum(Rep1$total.pop))^2*Rep1$adj.var
>
>
> Rep1$total<-Rep1$adj.mean*(Rep1$total.pop)
>
> ##
>
> Estimated.TotalTemp<-sum(Rep1$adj.mean)*sum(Rep1$total.pop)
>
> Estimated.totalvar<-sum(Rep1$adj.var)
>
> Estimated.SE<-sqrt(Estimated.totalvar)*sum(Rep1$total.pop)
>
> RESULTS.R1<-data.frame(Estimated.TotalTemp, SE=Estimated.SE)
>
> RESULTS.R1
>
>
>
>
> ##########################
>
> #Replication 2 create a resampled data
>
> ############################
>
> Replication2<-dat1[unlist(lapply(lll, sample, rep=TRUE)),]
>
>
> Summary.Rep2<-ddply(Replication2, c("group"), summarise,
>
> N    = length(group),
>
>        mean = mean(temp, na.rm=TRUE),
>
>                sd   = sd(temp),
>
>                se   = sd / sqrt(N),
>
>                variance=sd^2
>
> )
>
> #merge two datasets
>
> Rep1<-merge(Summary.Rep2, dat2, by="group")
>
>
> #calclate adjusted mean. variance
>
> Rep2$adj.mean<-(Rep2$total.pop*Rep2$mean)/sum(Rep2$total.pop)
>
> Rep2$adj.var<-(Rep2$variance)/(Rep2$sampled.pop/(1-(Rep2$sampled.pop/Rep2$
> total.pop)))
>
> Rep2$over.adj.var<-(Rep2$total.pop/sum(Rep2$total.pop))^2*Rep2$adj.var
>
>
> Rep2$total<-Rep2$adj.mean*(Rep2$total.pop)
>
>
> ##
>
> Estimated.TotalTemp<-sum(Rep2$adj.mean)*sum(Rep2$total.pop)
>
> Estimated.totalvar<-sum(Rep2$adj.var)
>
> Estimated.SE<-sqrt(Estimated.totalvar)*sum(Rep2$total.pop)
>
> RESULTS.R2<-data.frame(Estimated.TotalTemp, SE=Estimated.SE)
>
>
> ##############################
>
> #combined all results from 1000 runs
>
> ALL.Results(Restult.R1, Result.R2....)
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Sat Oct 21 17:58:16 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 21 Oct 2017 17:58:16 +0200
Subject: [R] nls() and loop
In-Reply-To: <CAEMjoTkNRyuRA8trq46wo_CrzWg3mZ3ZbUsuyYp=qvsN2N7sLw@mail.gmail.com>
References: <CAEMjoTkNRyuRA8trq46wo_CrzWg3mZ3ZbUsuyYp=qvsN2N7sLw@mail.gmail.com>
Message-ID: <23019.28312.32030.340213@stat.math.ethz.ch>

Dear Vangi,

>>>>> Evangelina Viotto <evangelinaviotto at gmail.com>
>>>>>     on Fri, 20 Oct 2017 11:37:12 -0300 writes:

    > Hello I?m need fitt growth curve with data length-age. I want to evaluate
    > which is the function that best predicts my data, to do so I compare the
    > Akaikes of different models. I'm now need to evaluate if changing the
    > initial values changes the parameters and which do not allow to estimate
    > the model.

    > To do this I use the function nls(); 

good!

    > and I randomize the initial values (real positive number).

Not a very good idea, I'm sorry to say:
You have enough observations to fit such a simple 3-parameter model:

I'm using your data showing you two models, both provided with R
as "self starting models" already.
Self starting means the use the data (and math and linear least
squares) to get good initial (aka "starting") values for the parameters.

The first model, SSasymp() is equivalent yours but more smartly
parametrized: it uses exp(lrc) (!) -- see   help(SSasymp)   in R, 	      
the 2nd model assumes the true curve goes through the origin ( = (0,0)
and hence uses one parameter less.
As we will see, both models fit ok, but the more simple models
may be preferable.

Here is the (self contained) R code, including your data at the beginning:



ANO <- c( 1.65, 1.69, 1.72, 1.72, 1.72, 1.72, 1.73, 2.66 ,2.66, 2.66, 2.66, 2.76, 2.76, 2.76 ,2.76, 2.78, 2.8, 3.65, 3.65 ,3.65, 3.78, 3.78, 5.07, 7.02, 7.1, 7.81, 8.72, 8.74, 8.78, 8.8, 8.8, 8.83, 8.98, 9.1, 9.11, 9.75, 9.82, 9.84, 9.87, 9.87, 10.99, 11.67, 11.8, 11.81, 13.93, 14.83, 15.82, 15.99, 16.87, 16.88, 16.9, 17.68, 17.79, 17.8, 17.8)

SVL <- c(26.11,29.02,41.13,30.96,37.74,29.02,33.38,24.18,34.35,35.8,29.99,42.59,27.57,47.43,46.95,30.47,29.75,35.8,40.65,36.29,34.83,29.02,43.5,75,68,70,67.5,80,77.5,68,68,73.84,72.14,68,64.5,58.5,72.63,78.44,71.17,70.69,77,79,78,68.5,69.72,71.66,77,77,79,76.5,78.5,79,73,80,69.72)

d.SA <- data.frame(SVL, ANO) # creo data frame {but do _not_ call it 'data'}
str(d.SA) ## 55 x 2
summary(d.SA) # to get an idea;  the plot below is more useful

## MM: just do this: it's equivalent to your model (but better parametrized!)
fm1 <- nls(SVL ~ SSasymp(ANO, Asym, lrc, c0), data = d.SA)
summary(fm1)

## Compute nicely spaced predicted values for plotting:
ANO. <- seq(-1/2, 30, by = 1/8)
SVL. <- predict(fm1, newdata = list(ANO = ANO.))

plot(SVL ~ ANO, d.SA, xlim = range(ANO, ANO.), ylim = range(SVL, SVL.))
lines(SVL. ~ ANO., col="red", lwd=2)
abline(h = coef(fm1)[["Asym"]], col = "tomato", lty=2, lwd = 1.5)
abline(h=0, v=0, lty=3, lwd = 1/2)

## Both from summary  (where 'lrc' has large variance) and because of the fit,
## Trying the Asymptotic through the origin ==> 1 parameter less instead :
fmO <- nls(SVL ~ SSasympOrig(ANO, Asym, lrc), data = d.SA)
summary(fmO)## (much better determined)
SVL.O <- predict(fmO, newdata = list(ANO = ANO.))
lines(SVL.O ~ ANO., col="blue", lwd=2)# does go through origin (0,0)
abline(h = coef(fmO)[["Asym"]], col = "skyblue", lty=2, lwd = 1.5)

## look close, I'd probably take the simpler one:
## and classical statistical inference also does not see a significant
## difference between the two fitted models :
anova(fm1, fmO)
## Analysis of Variance Table

## Model 1: SVL ~ SSasymp(ANO, Asym, lrc, c0)
## Model 2: SVL ~ SSasympOrig(ANO, Asym, lrc)
##   Res.Df Res.Sum Sq Df Sum Sq F value Pr(>F)
## 1     52     2635.1
## 2     53     2702.6 -1 -67.55   1.333 0.2535

---------------

I see that the 10 nice self-starting models that came with nls
already in the  1990's   are not known and probably not
understood by many.

I'm working at making their help pages nicer, notably by
slightly enhancing  the nice model-visualizing plot, you already now
get in R when you run

    example(SSasymp)
or
    example(SSasympOrig)

(but unfortunately, they currently use 'lwd = 0' to draw the asymptote
 which shows fine on a PDF but not on a typical my screen graphics device.)


Martin Maechler
ETH Zurich and R Core team


From dwinsemius at comcast.net  Sat Oct 21 18:05:38 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 21 Oct 2017 09:05:38 -0700
Subject: [R] What exactly is an dgCMatrix-class. There are so many
	attributes.
In-Reply-To: <23019.24263.905696.240862@stat.math.ethz.ch>
References: <CAE2FW2kqZ=1EYb3Yx21Mzk9FHyHBVUAqrCjF0DFkk6WWbGO0tw@mail.gmail.com>
 <DD993CA5-4057-469D-9B1A-E98241354563@comcast.net>
 <CAE2FW2nkH0ypjvPHjzPS632_Px=hBvSEWO_pcH9e3y3bhbcQFw@mail.gmail.com>
 <23019.24263.905696.240862@stat.math.ethz.ch>
Message-ID: <EE580E3F-4326-40E3-8F67-869B7C8BE197@comcast.net>


> On Oct 21, 2017, at 7:50 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> 
>>>>>> C W <tmrsg11 at gmail.com>
>>>>>>    on Fri, 20 Oct 2017 15:51:16 -0400 writes:
> 
>> Thank you for your responses.  I guess I don't feel
>> alone. I don't find the documentation go into any detail.
> 
>> I also find it surprising that,
> 
>>> object.size(train$data)
>> 1730904 bytes
> 
>>> object.size(as.matrix(train$data))
>> 6575016 bytes
> 
>> the dgCMatrix actually takes less memory, though it
>> *looks* like the opposite.
> 
> to whom?
> 
> The whole idea of these sparse matrix classes in the 'Matrix'
> package (and everywhere else in applied math, CS, ...) is that
> 1. they need  much less memory   and
> 2. matrix arithmetic with them can be much faster because it is based on
>   sophisticated sparse matrix linear algebra, notably the
>   sparse Cholesky decomposition for solve() etc.
> 
> Of course the efficency only applies if most of the
> matrix entries _are_ 0.
> You can measure the  "sparsity" or rather the  "density", of a
> matrix by
> 
>  nnzero(A) / length(A)
> 
> where length(A) == nrow(A) * ncol(A)  as for regular matrices
> (but it does *not* integer overflow)
> and nnzero(.) is a simple utility from Matrix
> which -- very efficiently for sparseMatrix objects -- gives the
> number of nonzero entries of the matrix.
> 
> All of these classes are formally defined classes and have
> therefore help pages.  Here  ?dgCMatrix-class  which then points
> to  ?CsparseMatrix-class  (and I forget if Rstudio really helps
> you find these ..; in emacs ESS they are found nicely via the usual key)
> 
> To get started, you may further look at  ?Matrix _and_  ?sparseMatrix
> (and possibly the Matrix package vignettes --- though they need
> work -- I'm happy for collaborators there !)
> 
> Bill Dunlap's comment applies indeed:
> In principle all these matrices should work like regular numeric
> matrices, just faster with less memory foot print if they are
> really sparse (and not just formally of a sparseMatrix class)
>  ((and there are quite a few more niceties in the package))
> 
> Martin Maechler
> (here, maintainer of 'Matrix')
> 
> 
>> On Fri, Oct 20, 2017 at 3:22 PM, David Winsemius <dwinsemius at comcast.net>
>> wrote:
> 
>>>> On Oct 20, 2017, at 11:11 AM, C W <tmrsg11 at gmail.com> wrote:
>>>> 
>>>> Dear R list,
>>>> 
>>>> I came across dgCMatrix. I believe this class is associated with sparse
>>>> matrix.
>>> 
>>> Yes. See:
>>> 
>>> help('dgCMatrix-class', pack=Matrix)
>>> 
>>> If Martin Maechler happens to respond to this you should listen to him
>>> rather than anything I write. Much of what the Matrix package does appears
>>> to be magical to one such as I.
>>> 
>>>> 
>>>> I see there are 8 attributes to train$data, I am confused why are there
>>> so
>>>> many, some are vectors, what do they do?
>>>> 
>>>> Here's the R code:
>>>> 
>>>> library(xgboost)
>>>> data(agaricus.train, package='xgboost')
>>>> data(agaricus.test, package='xgboost')
>>>> train <- agaricus.train
>>>> test <- agaricus.test
>>>> attributes(train$data)
>>>> 
>>> 
>>> I got a bit of an annoying surprise when I did something similar. It
>>> appearred to me that I did not need to load the xgboost library since all
>>> that was being asked was "where is the data" in an object that should be
>>> loaded from that library using the `data` function. The last command asking
>>> for the attributes filled up my console with a 100K length vector (actually
>>> 2 of such vectors). The `str` function returns a more useful result.
>>> 
>>>> data(agaricus.train, package='xgboost')
>>>> train <- agaricus.train
>>>> names( attributes(train$data) )
>>> [1] "i"        "p"        "Dim"      "Dimnames" "x"        "factors"
>>> "class"
>>>> str(train$data)
>>> Formal class 'dgCMatrix' [package "Matrix"] with 6 slots
>>> ..@ i       : int [1:143286] 2 6 8 11 18 20 21 24 28 32 ...
>>> ..@ p       : int [1:127] 0 369 372 3306 5845 6489 6513 8380 8384 10991
>>> ...
>>> ..@ Dim     : int [1:2] 6513 126
>>> ..@ Dimnames:List of 2
>>> .. ..$ : NULL
>>> .. ..$ : chr [1:126] "cap-shape=bell" "cap-shape=conical"
>>> "cap-shape=convex" "cap-shape=flat" ...
>>> ..@ x       : num [1:143286] 1 1 1 1 1 1 1 1 1 1 ...
>>> ..@ factors : list()
>>> 
>>>> Where is the data, is it in $p, $i, or $x?
>>> 
>>> So the "data" (meaning the values of the sparse matrix) are in the @x
>>> leaf. The values all appear to be the number 1. The @i leaf is the sequence
>>> of row locations for the values entries while the @p items are somehow
>>> connected with the columns (I think, since 127 and 126=number of columns
>>> from the @Dim leaf are only off by 1).
> 
> You are right David.
> 
> well, they follow sparse matrix standards which (like C) start
> counting at 0.
> 
>>> 
>>> Doing this > colSums(as.matrix(train$data))
> 
> The above colSums() again is "very" inefficient:
> All such R functions  have smartly defined  Matrix methods that
> directly work on sparse matrices.

I did get an error with colSums(train$data)

> colSums(train$data)
Error in colSums(train$data) : 
  'x' must be an array of at least two dimensions

Which as it turned out was due to my having not yet loaded pkg:Matrix. Perhaps the xgboost package only imports certain functions from pkg:Matrix and that colSums is not one of them. This resembles the errors I get when I try to use grip package functions on ggplot2 objects. Since ggplot2 is built on top of grid I always am surprised when this happens and after a headslap and explicitly loading pfk:grid I continue on my stumbling way.


library(Matrix)
colSums(train$data)   # no error


> Note that  as.matrix(M)  can "blow up" your R, when the matrix M
> is really large and sparse such that its dense version does not
> even fit in your computer's RAM.

I did know that, so I first calculated whether the dense matrix version of that object would fit in my RAM space and it fit easily so I proceeded. 

I find the TsparseMatrix indexing easier for my more naive notion of sparsity, although thinking about it now,  I think I can see that the CsparseMatrix more closely resembles the "folded vector" design of dense R matrices. I will sometimes coerce CMatrix objeccts to TMatrix objects if I am working on the "inner" indices. I should probably stop doing that.

I sincerely hope my stumbling efforts have not caused any delays.

-- 
David.


> 
>>> cap-shape=bell                cap-shape=conical
>>> 369                                3
>>> cap-shape=convex                   cap-shape=flat
>>> 2934                             2539
>>> cap-shape=knobbed                 cap-shape=sunken
>>> 644                               24
>>> cap-surface=fibrous              cap-surface=grooves
>>> 1867                                4
>>> cap-surface=scaly               cap-surface=smooth
>>> 2607                             2035
>>> cap-color=brown                   cap-color=buff
>>> 1816
>>> # now snipping the rest of that output.
>>> 
>>> 
>>> 
>>> Now this makes me think that the @p vector gives you the cumulative sum of
>>> number of items per column:
>>> 
>>>> all( cumsum( colSums(as.matrix(train$data)) ) == train$data at p[-1] )
>>> [1] TRUE
>>> 
>>>> 
>>>> Thank you very much!
>>>> 
>>>>      [[alternative HTML version deleted]]
>>> 
>>> Please read the Posting Guide. Your code was not mangled in this instance,
>>> but HTML code often arrives in an unreadable mess.
>>> 
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>> ng-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> David Winsemius
>>> Alameda, CA, USA
>>> 
>>> 'Any technology distinguishable from magic is insufficiently advanced.'
>>> -Gehm's Corollary to Clarke's Third Law
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
> 
>> [[alternative HTML version deleted]]
> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From maechler at stat.math.ethz.ch  Sat Oct 21 18:27:37 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 21 Oct 2017 18:27:37 +0200
Subject: [R] What exactly is an dgCMatrix-class. There are so
	many	attributes.
In-Reply-To: <CAE2FW2nuXrFgoo0ZCmgq4W0YEZtiSUjuFDkaOKbkUcni2=gw1A@mail.gmail.com>
References: <CAE2FW2kqZ=1EYb3Yx21Mzk9FHyHBVUAqrCjF0DFkk6WWbGO0tw@mail.gmail.com>
 <DD993CA5-4057-469D-9B1A-E98241354563@comcast.net>
 <CAE2FW2nkH0ypjvPHjzPS632_Px=hBvSEWO_pcH9e3y3bhbcQFw@mail.gmail.com>
 <CAE2FW2nuXrFgoo0ZCmgq4W0YEZtiSUjuFDkaOKbkUcni2=gw1A@mail.gmail.com>
Message-ID: <23019.30073.107041.649241@stat.math.ethz.ch>

>>>>> C W <tmrsg11 at gmail.com>
>>>>>     on Fri, 20 Oct 2017 16:01:06 -0400 writes:

    > Subsetting using [] vs. head(), gives different results.
    > R code:

    >> head(train$data, 5)
    > [1] 0 0 1 0 0

The above is surprising ... and points to a bug somewhere.
It is different (and correct) after you do

   require(Matrix)

but I think something like that should happen
semi-automatically.

As I just see, it is even worse if you get the data from xgboost
without loading the xgboost package, which you can do (and is
also more efficient !):

If you start R, and then do

   data(agaricus.train, package='xgboost')

   loadedNamespaces() # does not contain "xgboost" nor "Matrix"

so, no wonder

   head(agaricus.train $ data)

does not find head()s "Matrix" method [which _is_ exported by Matrix
via  exportMethods(.)].
But even more curiously, even after I do

    loadNamespace("Matrix")
    methods(head)

now does show the "Matrix" method,
but then head() *still* does not call it.  There's a bug
somewhere and I suspect it's in R's data() or methods package or
?? rather than in 'Matrix'.
But that will be another thread on R-devel or R's bugzilla.

Martin


    >> train$data[1:5, 1:5]
    > 5 x 5 sparse Matrix of class "dgCMatrix"
    >      cap-shape=bell cap-shape=conical cap-shape=convex
    > [1,]              .                 .                1
    > [2,]              .                 .                1
    > [3,]              1                 .                .
    > [4,]              .                 .                1
    > [5,]              .                 .                1
    >      cap-shape=flat cap-shape=knobbed
    > [1,]              .                 .
    > [2,]              .                 .
    > [3,]              .                 .
    > [4,]              .                 .
    > [5,]              .                 .

    > On Fri, Oct 20, 2017 at 3:51 PM, C W <tmrsg11 at gmail.com> wrote:

    >> Thank you for your responses.
    >> 
    >> I guess I don't feel alone. I don't find the documentation go into any
    >> detail.
    >> 
    >> I also find it surprising that,
    >> 
    >> > object.size(train$data)
    >> 1730904 bytes
    >> 
    >> > object.size(as.matrix(train$data))
    >> 6575016 bytes
    >> 
    >> the dgCMatrix actually takes less memory, though it *looks* like the
    >> opposite.
    >> 
    >> Cheers!
    >> 
    >> On Fri, Oct 20, 2017 at 3:22 PM, David Winsemius <dwinsemius at comcast.net>
    >> wrote:
    >> 
    >>> 
    >>> > On Oct 20, 2017, at 11:11 AM, C W <tmrsg11 at gmail.com> wrote:
    >>> >
    >>> > Dear R list,
    >>> >
    >>> > I came across dgCMatrix. I believe this class is associated with sparse
    >>> > matrix.
    >>> 
    >>> Yes. See:
    >>> 
    >>> help('dgCMatrix-class', pack=Matrix)
    >>> 
    >>> If Martin Maechler happens to respond to this you should listen to him
    >>> rather than anything I write. Much of what the Matrix package does appears
    >>> to be magical to one such as I.
    >>> 
    >>> >
    >>> > I see there are 8 attributes to train$data, I am confused why are there
    >>> so
    >>> > many, some are vectors, what do they do?
    >>> >
    >>> > Here's the R code:
    >>> >
    >>> > library(xgboost)
    >>> > data(agaricus.train, package='xgboost')
    >>> > data(agaricus.test, package='xgboost')
    >>> > train <- agaricus.train
    >>> > test <- agaricus.test
    >>> > attributes(train$data)
    >>> >
    >>> 
    >>> I got a bit of an annoying surprise when I did something similar. It
    >>> appearred to me that I did not need to load the xgboost library since all
    >>> that was being asked was "where is the data" in an object that should be
    >>> loaded from that library using the `data` function. The last command asking
    >>> for the attributes filled up my console with a 100K length vector (actually
    >>> 2 of such vectors). The `str` function returns a more useful result.
    >>> 
    >>> > data(agaricus.train, package='xgboost')
    >>> > train <- agaricus.train
    >>> > names( attributes(train$data) )
    >>> [1] "i"        "p"        "Dim"      "Dimnames" "x"        "factors"
    >>> "class"
    >>> > str(train$data)
    >>> Formal class 'dgCMatrix' [package "Matrix"] with 6 slots
    >>> ..@ i       : int [1:143286] 2 6 8 11 18 20 21 24 28 32 ...
    >>> ..@ p       : int [1:127] 0 369 372 3306 5845 6489 6513 8380 8384 10991
    >>> ...
    >>> ..@ Dim     : int [1:2] 6513 126
    >>> ..@ Dimnames:List of 2
    >>> .. ..$ : NULL
    >>> .. ..$ : chr [1:126] "cap-shape=bell" "cap-shape=conical"
    >>> "cap-shape=convex" "cap-shape=flat" ...
    >>> ..@ x       : num [1:143286] 1 1 1 1 1 1 1 1 1 1 ...
    >>> ..@ factors : list()
    >>> 
    >>> > Where is the data, is it in $p, $i, or $x?
    >>> 
    >>> So the "data" (meaning the values of the sparse matrix) are in the @x
    >>> leaf. The values all appear to be the number 1. The @i leaf is the sequence
    >>> of row locations for the values entries while the @p items are somehow
    >>> connected with the columns (I think, since 127 and 126=number of columns
    >>> from the @Dim leaf are only off by 1).
    >>> 
    >>> Doing this > colSums(as.matrix(train$data))
    >>> cap-shape=bell                cap-shape=conical
    >>> 369                                3
    >>> cap-shape=convex                   cap-shape=flat
    >>> 2934                             2539
    >>> cap-shape=knobbed                 cap-shape=sunken
    >>> 644                               24
    >>> cap-surface=fibrous              cap-surface=grooves
    >>> 1867                                4
    >>> cap-surface=scaly               cap-surface=smooth
    >>> 2607                             2035
    >>> cap-color=brown                   cap-color=buff
    >>> 1816
    >>> # now snipping the rest of that output.
    >>> 
    >>> 
    >>> 
    >>> Now this makes me think that the @p vector gives you the cumulative sum
    >>> of number of items per column:
    >>> 
    >>> > all( cumsum( colSums(as.matrix(train$data)) ) == train$data at p[-1] )
    >>> [1] TRUE
    >>> 
    >>> >
    >>> > Thank you very much!
    >>> >
    >>> >       [[alternative HTML version deleted]]
    >>> 
    >>> Please read the Posting Guide. Your code was not mangled in this
    >>> instance, but HTML code often arrives in an unreadable mess.
    >>> 
    >>> >
    >>> > ______________________________________________
    >>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    >>> > https://stat.ethz.ch/mailman/listinfo/r-help
    >>> > PLEASE do read the posting guide http://www.R-project.org/posti
    >>> ng-guide.html
    >>> > and provide commented, minimal, self-contained, reproducible code.
    >>> 
    >>> David Winsemius
    >>> Alameda, CA, USA
    >>> 
    >>> 'Any technology distinguishable from magic is insufficiently advanced.'
    >>> -Gehm's Corollary to Clarke's Third Law
    >>> 
    >>> 
    >>> 
    >>> 
    >>> 
    >>> 
    >> 

    > [[alternative HTML version deleted]]

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Sat Oct 21 18:50:16 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 21 Oct 2017 09:50:16 -0700
Subject: [R] Skip error in downloading file in loop
In-Reply-To: <CAMLwc7NVC-M6t+M2HrRDFAZ=M7hTfP9KqSuuL_1FaTDNc8wjug@mail.gmail.com>
References: <CAMLwc7NVC-M6t+M2HrRDFAZ=M7hTfP9KqSuuL_1FaTDNc8wjug@mail.gmail.com>
Message-ID: <AA6E0FA2-872D-445E-B8BD-B19E3918805F@dcn.davis.ca.us>

One of the recommendations in the Posting Guide is to read the archives before posting... even following along with recent posts would have pointed out the existence of the tryCatch function. 

?tryCatch
-- 
Sent from my phone. Please excuse my brevity.

On October 21, 2017 8:01:02 AM PDT, Miluji Sb <milujisb at gmail.com> wrote:
>I am trying to download data from NASA web-service.
>
>I am using the following code;
>
>for( i in 1:8) {
>  target1 <- paste0("
>https://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/access/timeseries.cgi?variable=NLDAS:NLDAS_FORA0125_H.002:TMP2m&location=GEOM:POINT(
>",
>                    cities[i, "lon_nldas"],
>                    ",%20", cities[i,"lat_nldas"],
>
>")&startDate=1980-01-01T00&endDate=2016-12-31T23&type=asc2")
>  target2 <- paste0("/Users/dasgupta/Dropbox (FEEM)/Flu Paper/climate
>data/temperature/nldas/",    # change for whatever destination
>directory
>you may prefer.
>                    cities[i,"city"], "_",
>                    cities[i,"state"], ".csv")
>  download.file(url=target1, destfile=target2,  method = "libcurl")
>}
>
>Any time the coordinates provided is out data coverage, the loop fails
>and
>stops. Is there any way to force R to skip the error and continue with
>the
>rest of the download?
>
>My data looks like this:
>
>dput(droplevels(head(cities, 8)))
>structure(list(city = structure(1:8, .Label = c("Boston", "Bridgeport",
>"Cambridge", "Fall River", "Hartford", "Lowell", "Lynn", "New Bedford"
>), class = "factor"), state = structure(c(2L, 1L, 2L, 2L, 1L,
>2L, 2L, 2L), .Label = c(" CT ", " MA "), class = "factor"), lon_nldas =
>c(-71.05673836,
>-73.19126922, -71.1060061, -71.14364822, -72.67401574, -71.31283992,
>-70.82521832, -70.80586599), lat_nldas = c(42.35866236, 41.18188268,
>42.36679363, 41.69735342, 41.76349276, 42.64588413, 42.46370197,
>41.63767375)), .Names = c("city", "state", "lon_nldas", "lat_nldas"
>), row.names = c(NA, 8L), class = "data.frame")
>
>Any help will be appreciated. Thank you very much!
>
>Sincerely,
>
>Milu
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From maechler at stat.math.ethz.ch  Sat Oct 21 19:13:45 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 21 Oct 2017 19:13:45 +0200
Subject: [R] What exactly is an dgCMatrix-class. There are so many
	attributes.
In-Reply-To: <EE580E3F-4326-40E3-8F67-869B7C8BE197@comcast.net>
References: <CAE2FW2kqZ=1EYb3Yx21Mzk9FHyHBVUAqrCjF0DFkk6WWbGO0tw@mail.gmail.com>
 <DD993CA5-4057-469D-9B1A-E98241354563@comcast.net>
 <CAE2FW2nkH0ypjvPHjzPS632_Px=hBvSEWO_pcH9e3y3bhbcQFw@mail.gmail.com>
 <23019.24263.905696.240862@stat.math.ethz.ch>
 <EE580E3F-4326-40E3-8F67-869B7C8BE197@comcast.net>
Message-ID: <23019.32841.638436.747859@stat.math.ethz.ch>

>>>>> David Winsemius <dwinsemius at comcast.net>
>>>>>     on Sat, 21 Oct 2017 09:05:38 -0700 writes:

    >> On Oct 21, 2017, at 7:50 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
    >> 
    >>>>>>> C W <tmrsg11 at gmail.com>
    >>>>>>> on Fri, 20 Oct 2017 15:51:16 -0400 writes:
    >> 
    >>> Thank you for your responses.  I guess I don't feel
    >>> alone. I don't find the documentation go into any detail.
    >> 
    >>> I also find it surprising that,
    >> 
    >>>> object.size(train$data)
    >>> 1730904 bytes
    >> 
    >>>> object.size(as.matrix(train$data))
    >>> 6575016 bytes
    >> 
    >>> the dgCMatrix actually takes less memory, though it
    >>> *looks* like the opposite.
    >> 
    >> to whom?
    >> 
    >> The whole idea of these sparse matrix classes in the 'Matrix'
    >> package (and everywhere else in applied math, CS, ...) is that
    >> 1. they need  much less memory   and
    >> 2. matrix arithmetic with them can be much faster because it is based on
    >> sophisticated sparse matrix linear algebra, notably the
    >> sparse Cholesky decomposition for solve() etc.
    >> 
    >> Of course the efficency only applies if most of the
    >> matrix entries _are_ 0.
    >> You can measure the  "sparsity" or rather the  "density", of a
    >> matrix by
    >> 
    >> nnzero(A) / length(A)
    >> 
    >> where length(A) == nrow(A) * ncol(A)  as for regular matrices
    >> (but it does *not* integer overflow)
    >> and nnzero(.) is a simple utility from Matrix
    >> which -- very efficiently for sparseMatrix objects -- gives the
    >> number of nonzero entries of the matrix.
    >> 
    >> All of these classes are formally defined classes and have
    >> therefore help pages.  Here  ?dgCMatrix-class  which then points
    >> to  ?CsparseMatrix-class  (and I forget if Rstudio really helps
    >> you find these ..; in emacs ESS they are found nicely via the usual key)
    >> 
    >> To get started, you may further look at  ?Matrix _and_  ?sparseMatrix
    >> (and possibly the Matrix package vignettes --- though they need
    >> work -- I'm happy for collaborators there !)
    >> 
    >> Bill Dunlap's comment applies indeed:
    >> In principle all these matrices should work like regular numeric
    >> matrices, just faster with less memory foot print if they are
    >> really sparse (and not just formally of a sparseMatrix class)
    >> ((and there are quite a few more niceties in the package))
    >> 
    >> Martin Maechler
    >> (here, maintainer of 'Matrix')
    >> 
    >> 
    >>> On Fri, Oct 20, 2017 at 3:22 PM, David Winsemius <dwinsemius at comcast.net>
    >>> wrote:
    >> 
    >>>>> On Oct 20, 2017, at 11:11 AM, C W <tmrsg11 at gmail.com> wrote:
    >>>>> 
    >>>>> Dear R list,
    >>>>> 
    >>>>> I came across dgCMatrix. I believe this class is associated with sparse
    >>>>> matrix.
    >>>> 
    >>>> Yes. See:
    >>>> 
    >>>> help('dgCMatrix-class', pack=Matrix)
    >>>> 
    >>>> If Martin Maechler happens to respond to this you should listen to him
    >>>> rather than anything I write. Much of what the Matrix package does appears
    >>>> to be magical to one such as I.
    >>>> 
[............]    

    >>>>> data(agaricus.train, package='xgboost')
    >>>>> train <- agaricus.train
    >>>>> names( attributes(train$data) )
    >>>> [1] "i"        "p"        "Dim"      "Dimnames" "x"        "factors"
    >>>> "class"
    >>>>> str(train$data)
    >>>> Formal class 'dgCMatrix' [package "Matrix"] with 6 slots
    >>>> ..@ i       : int [1:143286] 2 6 8 11 18 20 21 24 28 32 ...
    >>>> ..@ p       : int [1:127] 0 369 372 3306 5845 6489 6513 8380 8384 10991
    >>>> ...
    >>>> ..@ Dim     : int [1:2] 6513 126
    >>>> ..@ Dimnames:List of 2
    >>>> .. ..$ : NULL
    >>>> .. ..$ : chr [1:126] "cap-shape=bell" "cap-shape=conical"
    >>>> "cap-shape=convex" "cap-shape=flat" ...
    >>>> ..@ x       : num [1:143286] 1 1 1 1 1 1 1 1 1 1 ...
    >>>> ..@ factors : list()
    >>>> 
    >>>>> Where is the data, is it in $p, $i, or $x?
    >>>> 
    >>>> So the "data" (meaning the values of the sparse matrix) are in the @x
    >>>> leaf. The values all appear to be the number 1. The @i leaf is the sequence
    >>>> of row locations for the values entries while the @p items are somehow
    >>>> connected with the columns (I think, since 127 and 126=number of columns
    >>>> from the @Dim leaf are only off by 1).
    >> 
    >> You are right David.
    >> 
    >> well, they follow sparse matrix standards which (like C) start
    >> counting at 0.
    >> 
    >>>> 
    >>>> Doing this > colSums(as.matrix(train$data))
    >> 
    >> The above colSums() again is "very" inefficient:
    >> All such R functions  have smartly defined  Matrix methods that
    >> directly work on sparse matrices.

    > I did get an error with colSums(train$data)

    >> colSums(train$data)
    > Error in colSums(train$data) : 
    > 'x' must be an array of at least two dimensions

The same problem  C.W. saw with head()

It, e.g., all works after calling  str() on train$data.

But I am still puzzled, because head() is similar to str():
both are S3 generics (in "utils")  but str()'s useMethod() I
think see that the class belongs to package "Matrix" and hence
attaches it {not just *load* it -- hence, import etc does not matter}.
but  head() does not.

Even more curiously,  colSums()  *also*  attaches Matrix but
still fails, but it works on a 2nd call 

Example 1, in a fresh R session:

--------------------------------------------------------------------------------

> data(agaricus.train, package="xgboost")
> M <- agaricus.train$data
> methods(str)

[1] str.data.frame* str.Date*       str.default*    str.dendrogram* str.logLik*     str.POSIXt*    
# see '?methods' for accessing help and source code

> str(M)
Loading required package: Matrix  <<<<<<<<< SEE ! <<<<<<<<<<<<<<<<<
Formal class 'dgCMatrix' [package "Matrix"] with 6 slots
  ..@ i       : int [1:143286] 2 6 8 11 18 20 21 24 28 32 ...
  ..@ p       : int [1:127] 0 369 372 3306 5845 6489 6513 8380 8384 10991 ...
  ..@ Dim     : int [1:2] 6513 126
  ..@ Dimnames:List of 2
  .. ..$ : NULL
R  .. ..$ : chr [1:126] "cap-shape=bell" "cap-shape=conical" "cap-shape=convex" "cap-shape=flat" ...
  ..@ x       : num [1:143286] 1 1 1 1 1 1 1 1 1 1 ...
  ..@ factors : list()
> 
> head(M)
6 x 126 sparse Matrix of class "dgCMatrix"
   [[ suppressing 126 column names ?cap-shape=bell?, ?cap-shape=conical?, ?cap-shape=convex? ... ]]
[1,] . . 1 . . . . . . 1 1 . . . . . . . . . 1 . . . . . . . . 1 . . . 1 . 1 . . . 1 1 . . . . . . . . . . . 1 . .
................
................
................

---------------------------------------------------------------------------------
-

See, str()  is a nice one generic function ==> attaches Matrix (see
the message where I have added '<<<<<<<<< SEE ! <<<<.........'),
but as we know  head() does not strangely.

Now, the curious  colSums() behavior:

Example 2, in a fresh R session:
-----------------------------------------------------------------------------
> data(agaricus.train, package='xgboost')
> M <- agaricus.train$data
> cm <- colSums(M) ## first time, loads Matrix but then fails !!
Loading required package: Matrix
Error in colSums(M) : 'x' must be an array of at least two dimensions
> cm <- colSums(M) ## 2nd time, works because Matrix methods are all there
> str(cm)
 Named num [1:126] 369 3 2934 2539 644 ...
 - attr(*, "names")= chr [1:126] "cap-shape=bell" "cap-shape=conical" "cap-shape=convex" "cap-shape=flat" ...
> 
-----------------------------------------------------------------------------

    > Which as it turned out was due to my having not yet loaded pkg:Matrix. Perhaps the xgboost package only imports certain functions from pkg:Matrix and that colSums is not one of them. This resembles the errors I get when I try to use grip package functions on ggplot2 objects. Since ggplot2 is built on top of grid I always am surprised when this happens and after a headslap and explicitly loading pfk:grid I continue on my stumbling way.


    > library(Matrix)
    > colSums(train$data)   # no error


    >> Note that  as.matrix(M)  can "blow up" your R, when the matrix M
    >> is really large and sparse such that its dense version does not
    >> even fit in your computer's RAM.

    > I did know that, so I first calculated whether the dense matrix version of that object would fit in my RAM space and it fit easily so I proceeded. 

    > I find the TsparseMatrix indexing easier for my more naive notion of sparsity, although thinking about it now,  I think I can see that the CsparseMatrix more closely resembles the "folded vector" design of dense R matrices. I will sometimes coerce CMatrix objeccts to TMatrix objects if I am working on the "inner" indices. I should probably stop doing that.

Well, it depends if speed and efficiency are the only important
issues.
The triplet representation (<==> TsparseMatrix)  is of course
much easier to understand and explain than the column-compressed
one (CsparseMatrix) -- but the latter is the one that is
efficiently used in the C-level libraries for matrix
multiplication, Cholesky etc.


    > I sincerely hope my stumbling efforts have not caused any delays.

Not at all,  thank you David for all your helping on R-help !!!
Martin

    > -- 
    > David.

    [..................]


    > David Winsemius
    > Alameda, CA, USA

    > 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law

ok.... given your other statement,  it may be that  Matrix  *is*
sufficiently adanced  ;-) :-)


From jropers at gmail.com  Sun Oct 22 17:56:12 2017
From: jropers at gmail.com (Jacques Ropers)
Date: Sun, 22 Oct 2017 17:56:12 +0200
Subject: [R] ggplot2 and tikzDevice : problems with accents
Message-ID: <6d3c4ef6-f5e6-3848-fd01-1037676f9d5b@gmail.com>

Hi all,

I can't fathom why the accented "?" in the following ggplot2 graph makes 
R hangs when using tikzdevice,? whereas it works using simple pdf device.

######
library(tikzDevice)
library(ggplot2)

options(tikzDefaultEngine = "luatex")


tikzLualatexPackages =c(
 ? "\\usepackage{tikz}\n",
 ? "\\usepackage[active,

tightpage,psfixbb]{preview}\n",
 ? "\\usepackage{fontspec,xunicode}\n",
 ? "\\PreviewEnvironment{pgfpicture}\n",
 ? "\\setlength\\PreviewBorder{0pt}\n"
)


x<- rnorm(10)
y<- rnorm(10)
df <- data.frame(x=x, y=y)

#### This example works
tikz(standAlone = TRUE)
ggplot(df, aes(x=x, y=y))+geom_point() +xlab("e")
dev.off()
####

#### This example hangs and never completes
tikz(standAlone = TRUE)
ggplot(df, aes(x=x, y=y))+geom_point() +xlab("?")
dev.off()
####

#### Working OK when using pdf device
pdf()
ggplot(df, aes(x=x, y=y))+geom_point() +xlab("?")
dev.off()
####

Thanks for your help

Jacques


	[[alternative HTML version deleted]]


From jsorkin at som.umaryland.edu  Sun Oct 22 15:04:19 2017
From: jsorkin at som.umaryland.edu (Sorkin, John)
Date: Sun, 22 Oct 2017 13:04:19 +0000
Subject: [R] Syntax for fit.contrast
Message-ID: <BY2PR0301MB061685A793413A3EC741F5EFE2410@BY2PR0301MB0616.namprd03.prod.outlook.com>

I have a model (run with glm) that has a factor, type. Type has two levels, "general" and "regional". I am trying to get estimates (and SEs) for the model with type="general" and type ="regional" using fit.contrast but I can't get the syntax of the coefficients to use in fit.contrast correct. I hope someone can show me how to use fit.contrast, or some other method to get estimate with SEs. (I know I can use the variance co-variance matrix, but I would rather not have to code the linear contrast my self from the coefficients of the matrix)

Thank  you,

John


My model:

model=glm(events~type,family=poisson(link=log),offset=log(SS),data=data)


Model details:

> summary(data$type)

 general regional
      16       16

> levels(data$type)
[1] "general"  "regional"

> contrasts(data$type)
         regional
general         0
regional        1


I have tried the following syntax for fit.contrast

fit.contrast(model,type,c(1,0))
and get an error:
Error in `[[<-`(`*tmp*`, varname, value = cmat) :
  no such index at level 1


> fit.contrast(model,type,c(0,1),showall=TRUE)
and get an error:
Error in `[[<-`(`*tmp*`, varname, value = cmat) :
  no such index at level 1



> fit.contrast(model,type,c(1,-1),showall=TRUE)
and get an error:
Error in `[[<-`(`*tmp*`, varname, value = cmat) :
  no such index at level 1


> fit.contrast(model,type,c(0))
and get an error:
Error in make.contrasts(coeff, ncol(coeff)) :
  Too many contrasts specified. Must be less than the number of factor levels (columns).

> fit.contrast(model,type,c(1))
Error in make.contrasts(coeff, ncol(coeff)) :
and get an error
  Too many contrasts specified. Must be less than the number of factor levels (columns).








John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)


	[[alternative HTML version deleted]]


From chalabi.elahe at yahoo.de  Sun Oct 22 16:13:43 2017
From: chalabi.elahe at yahoo.de (Elahe chalabi)
Date: Sun, 22 Oct 2017 14:13:43 +0000 (UTC)
Subject: [R] Test set and Train set in Caret package train function
References: <535779036.2485914.1508681623507.ref@mail.yahoo.com>
Message-ID: <535779036.2485914.1508681623507@mail.yahoo.com>

Hey all,

Does anyone know how we can get train set and test set for each fold of 5 fold cross validation in Caret package? Imagine if I want to do cross validation by random forest method, I do the following in Caret:

set.seed(12)
train_control <- trainControl(method="cv", number=5,savePredictions = TRUE)
rfmodel <- train(Species~., data=iris, trControl=train_control, method="rf")
first_holdout <- subset(rfmodel$pred, Resample == "Fold1")
str(first_holdout)
'data.frame':	90 obs. of  5 variables:
$ pred    : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 
$ obs     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 
$ rowIndex: int  2 3 9 11 25 29 35 36 41 50 ...
$ mtry    : num  2 2 2 2 2 2 2 2 2 2 ...
$ Resample: chr  "Fold1" "Fold1" "Fold1" "Fold1" ...

Are these 90 observations in Fold1 used as training set? If yes then where is the test set for this fold?

thanks for any help! 

Elahe


From varinsacha at yahoo.fr  Sun Oct 22 17:33:20 2017
From: varinsacha at yahoo.fr (varin sacha)
Date: Sun, 22 Oct 2017 15:33:20 +0000 (UTC)
Subject: [R] Add a vertical line and some values on a plot
References: <894779362.2554993.1508686400784.ref@mail.yahoo.com>
Message-ID: <894779362.2554993.1508686400784@mail.yahoo.com>

Dear R-experts,

Here below is my code, 
I would like to add a vertical line on my plot, showing the median and I would like to place some values on this graph as well, i.e. 4.3 and -8.4. How can I do ?
Many thanks for your reply.



A=c(1,2.3,4,3.5,4.3,2.5,6.3,-0.1,-1.5,3.7,-2.3,-3.5,5.4,3.2, -10.5,-8.4,-9.4) 
d <- density(A)
plot(d)
median(A)


From ruipbarradas at sapo.pt  Sun Oct 22 18:26:56 2017
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Sun, 22 Oct 2017 17:26:56 +0100
Subject: [R] Add a vertical line and some values on a plot
In-Reply-To: <894779362.2554993.1508686400784@mail.yahoo.com>
References: <894779362.2554993.1508686400784.ref@mail.yahoo.com>
 <894779362.2554993.1508686400784@mail.yahoo.com>
Message-ID: <59ECC6D0.3010601@sapo.pt>

Hello,

After the plot just do

abline(v = median(A))

As for how to plot points, see, well, ?points().

Hope this helps,

Rui Barradas

Em 22-10-2017 16:33, varin sacha via R-help escreveu:
> Dear R-experts,
>
> Here below is my code,
> I would like to add a vertical line on my plot, showing the median and I would like to place some values on this graph as well, i.e. 4.3 and -8.4. How can I do ?
> Many thanks for your reply.
>
>
>
> A=c(1,2.3,4,3.5,4.3,2.5,6.3,-0.1,-1.5,3.7,-2.3,-3.5,5.4,3.2, -10.5,-8.4,-9.4)
> d <- density(A)
> plot(d)
> median(A)
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dwinsemius at comcast.net  Sun Oct 22 19:20:48 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 22 Oct 2017 10:20:48 -0700
Subject: [R] Syntax for fit.contrast
In-Reply-To: <BY2PR0301MB061685A793413A3EC741F5EFE2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
References: <BY2PR0301MB061685A793413A3EC741F5EFE2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
Message-ID: <EDE44E15-E20D-466E-B821-0B4F36E602F7@comcast.net>


> On Oct 22, 2017, at 6:04 AM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> 
> I have a model (run with glm) that has a factor, type. Type has two levels, "general" and "regional". I am trying to get estimates (and SEs) for the model with type="general" and type ="regional" using fit.contrast

 ?fit.contrast
No documentation for ?fit.contrast? in specified packages and libraries:
you could try ???fit.contrast?

Perhaps the gmodels function of that name?

> but I can't get the syntax of the coefficients to use in fit.contrast correct. I hope someone can show me how to use fit.contrast, or some other method to get estimate with SEs. (I know I can use the variance co-variance matrix, but I would rather not have to code the linear contrast my self from the coefficients of the matrix)
> 

I'm having trouble understanding what you are trying to extract. There are only 2 levels so there is really only one interesting contrast ("general" vs "regional") , and it's magnitude would be reported by just typing `model`, and it's SE would show up in output of `summary(model)`.

I'm thinking you should pick one of the examples in gmodels::fit.contrast that most resembles your real problem,  post it,  and  and then explain what difficulties you are having with interpretation.

-- 
David.


> Thank  you,
> 
> John
> 
> 
> My model:
> 
> model=glm(events~type,family=poisson(link=log),offset=log(SS),data=data)
> 
> 
> Model details:
> 
>> summary(data$type)
> 
> general regional
>      16       16
> 
>> levels(data$type)
> [1] "general"  "regional"
> 
>> contrasts(data$type)
>         regional
> general         0
> regional        1
> 
> 
> I have tried the following syntax for fit.contrast
> 
> fit.contrast(model,type,c(1,0))
> and get an error:
> Error in `[[<-`(`*tmp*`, varname, value = cmat) :
>  no such index at level 1
> 
> 
>> fit.contrast(model,type,c(0,1),showall=TRUE)
> and get an error:
> Error in `[[<-`(`*tmp*`, varname, value = cmat) :
>  no such index at level 1
> 
> 
> 
>> fit.contrast(model,type,c(1,-1),showall=TRUE)
> and get an error:
> Error in `[[<-`(`*tmp*`, varname, value = cmat) :
>  no such index at level 1
> 
> 
>> fit.contrast(model,type,c(0))
> and get an error:
> Error in make.contrasts(coeff, ncol(coeff)) :
>  Too many contrasts specified. Must be less than the number of factor levels (columns).
> 
>> fit.contrast(model,type,c(1))
> Error in make.contrasts(coeff, ncol(coeff)) :
> and get an error
>  Too many contrasts specified. Must be less than the number of factor levels (columns).
> 
> 
> 
> 
> 
> 
> 
> 
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From traxplayer at gmail.com  Sun Oct 22 22:03:58 2017
From: traxplayer at gmail.com (=?UTF-8?Q?Martin_M=C3=B8ller_Skarbiniks_Pedersen?=)
Date: Sun, 22 Oct 2017 22:03:58 +0200
Subject: [R] ggplot2 and tikzDevice : problems with accents
In-Reply-To: <6d3c4ef6-f5e6-3848-fd01-1037676f9d5b@gmail.com>
References: <6d3c4ef6-f5e6-3848-fd01-1037676f9d5b@gmail.com>
Message-ID: <CAGAA5bfOOaYGFCzQdRkXTzj=b_UxbCswBd++cFMS-drpnaJPwg@mail.gmail.com>

On 22 October 2017 at 17:56, Jacques Ropers <jropers at gmail.com> wrote:
>
> Hi all,
>
> I can't fathom why the accented "?" in the following ggplot2 graph makes
> R hangs when using tikzdevice,  whereas it works using simple pdf device.
>
[...]

I just tried your code and my R doesn't hang but it does give an TeX-error:

(/usr/share/texlive/texmf-dError in getMetricsFromLatex(TeXMetrics, verbose
= verbose) :
TeX was unable to calculate metrics for the following string
or character:

77

Common reasons for failure include:
  * The string contains a character which is special to LaTeX unless
    escaped properly, such as % or $.
  * The string makes use of LaTeX commands provided by a package and
    the tikzDevice was not told to load the package.

Regards
Martin

	[[alternative HTML version deleted]]


From jsorkin at som.umaryland.edu  Mon Oct 23 00:56:16 2017
From: jsorkin at som.umaryland.edu (Sorkin, John)
Date: Sun, 22 Oct 2017 22:56:16 +0000
Subject: [R] Syntax for fit.contrast (from package gmodels)
In-Reply-To: <EDE44E15-E20D-466E-B821-0B4F36E602F7@comcast.net>
References: <BY2PR0301MB061685A793413A3EC741F5EFE2410@BY2PR0301MB0616.namprd03.prod.outlook.com>,
 <EDE44E15-E20D-466E-B821-0B4F36E602F7@comcast.net>
Message-ID: <BY2PR0301MB0616B317C0123DA2E6E61DA0E2410@BY2PR0301MB0616.namprd03.prod.outlook.com>

David,

Thank you for responding to my post.


Please consider the following output (typeregional is a factor having two levels, "regional" vs. "general"):

Call:
glm(formula = events ~ type, family = poisson(link = log), data = data,
    offset = log(SS))

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-43.606  -17.295   -4.651    4.204   38.421

Coefficients:
             Estimate Std. Error z value Pr(>|z|)
(Intercept)  -2.52830    0.01085 -233.13   <2e-16 ***
typeregional  0.33788    0.01641   20.59   <2e-16 ***


Let's forget for a moment that the model is a Poisson regression and pretend that the output is from a simple linear regression, e.g. from lm.


To get the estimate for "general" one simply needs to use the value of the intercept i.e. -2.5830. Similarly to get the 95% CI of general one simply needs to compute -2.52830-(1.96*0.01085) and -2.52830+(1.96*0.01085).


To get the estimate for "regional" one needs to compute intercept + typeregional, i.e. -2.52830 + 0.33788. To get the 95% CI is somewhat more difficult as one needs to use results from the variance-covariance matix, specifically the variance of intercept, the variance of "regional", and the covariance of (intercept,"regional") which involves:

var =  var(intercept) + var(regional) +2*(covar(intercept,regional)),

and then get the SE of the variance

SE=sqrt(var)

95% CI = intercept + regional - 1.95*SE and intercept + regional + 1.95*SE.


I was hoping that a contrast statement could be written that would give me the point estimate and SE for "general" and its SE and another contrast statement could be written that would give me the point estimate and SE for "general" and it SE without my having to work directly with the variance-covariance matrix. I tried doing this using the fit.contrast statements (from the gmodels package):


fit.contrast(model,type,c(1,0),showall=TRUE)

fit.contrast(model,type,c(0,1),showall=TRUE)


and received the error message,

Error in `[[<-`(`*tmp*`, varname, value = c(0, 1)) :
  no such index at level 1


Perhaps fit.contrast is not the way to accomplish my goal. Perhaps my goal can be accomplished without a contrast statement, but I don't know how.


Thank you,

John





John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)



________________________________
From: David Winsemius <dwinsemius at comcast.net>
Sent: Sunday, October 22, 2017 1:20 PM
To: Sorkin, John
Cc: r-help at r-project.org
Subject: Re: [R] Syntax for fit.contrast


> On Oct 22, 2017, at 6:04 AM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
>
> I have a model (run with glm) that has a factor, type. Type has two levels, "general" and "regional". I am trying to get estimates (and SEs) for the model with type="general" and type ="regional" using fit.contrast

 ?fit.contrast
No documentation for ?fit.contrast? in specified packages and libraries:
you could try ???fit.contrast?

Perhaps the gmodels function of that name?

> but I can't get the syntax of the coefficients to use in fit.contrast correct. I hope someone can show me how to use fit.contrast, or some other method to get estimate with SEs. (I know I can use the variance co-variance matrix, but I would rather not have to code the linear contrast my self from the coefficients of the matrix)
>

I'm having trouble understanding what you are trying to extract. There are only 2 levels so there is really only one interesting contrast ("general" vs "regional") , and it's magnitude would be reported by just typing `model`, and it's SE would show up in output of `summary(model)`.

I'm thinking you should pick one of the examples in gmodels::fit.contrast that most resembles your real problem,  post it,  and  and then explain what difficulties you are having with interpretation.

--
David.


> Thank  you,
>
> John
>
>
> My model:
>
> model=glm(events~type,family=poisson(link=log),offset=log(SS),data=data)
>
>
> Model details:
>
>> summary(data$type)
>
> general regional
>      16       16
>
>> levels(data$type)
> [1] "general"  "regional"
>
>> contrasts(data$type)
>         regional
> general         0
> regional        1
>
>
> I have tried the following syntax for fit.contrast
>
> fit.contrast(model,type,c(1,0))
> and get an error:
> Error in `[[<-`(`*tmp*`, varname, value = cmat) :
>  no such index at level 1
>
>
>> fit.contrast(model,type,c(0,1),showall=TRUE)
> and get an error:
> Error in `[[<-`(`*tmp*`, varname, value = cmat) :
>  no such index at level 1
>
>
>
>> fit.contrast(model,type,c(1,-1),showall=TRUE)
> and get an error:
> Error in `[[<-`(`*tmp*`, varname, value = cmat) :
>  no such index at level 1
>
>
>> fit.contrast(model,type,c(0))
> and get an error:
> Error in make.contrasts(coeff, ncol(coeff)) :
>  Too many contrasts specified. Must be less than the number of factor levels (columns).
>
>> fit.contrast(model,type,c(1))
> Error in make.contrasts(coeff, ncol(coeff)) :
> and get an error
>  Too many contrasts specified. Must be less than the number of factor levels (columns).
>
>
>
>
>
>
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law






	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Mon Oct 23 01:56:16 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 22 Oct 2017 16:56:16 -0700
Subject: [R] Syntax for fit.contrast (from package gmodels)
In-Reply-To: <BY2PR0301MB0616B317C0123DA2E6E61DA0E2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
References: <BY2PR0301MB061685A793413A3EC741F5EFE2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
 <EDE44E15-E20D-466E-B821-0B4F36E602F7@comcast.net>
 <BY2PR0301MB0616B317C0123DA2E6E61DA0E2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
Message-ID: <D54F490B-9F22-461E-A592-D0887D92EFA1@comcast.net>


> On Oct 22, 2017, at 3:56 PM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> 
> David,
> Thank you for responding to my post.
> 
> Please consider the following output (typeregional is a factor having two levels, "regional" vs. "general"):
> Call:
> glm(formula = events ~ type, family = poisson(link = log), data = data, 
>     offset = log(SS))
> 
> Deviance Residuals: 
>     Min       1Q   Median       3Q      Max  
> -43.606  -17.295   -4.651    4.204   38.421  
> 
> Coefficients:
>              Estimate Std. Error z value Pr(>|z|)    
> (Intercept)  -2.52830    0.01085 -233.13   <2e-16 ***
> typeregional  0.33788    0.01641   20.59   <2e-16 ***
> 
> Let's forget for a moment that the model is a Poisson regression and pretend that the output is from a simple linear regression, e.g. from lm.
> 
> To get the estimate for "general" one simply needs to use the value of the intercept i.e. -2.5830. Similarly to get the 95% CI of general one simply needs to compute -2.52830-(1.96*0.01085) and -2.52830+(1.96*0.01085).
> 
> To get the estimate for "regional" one needs to compute intercept + typeregional, i.e. -2.52830 + 0.33788. To get the 95% CI is somewhat more difficult as one needs to use results from the variance-covariance matix, specifically the variance of intercept, the variance of "regional", and the covariance of (intercept,"regional") which involves:
> var =  var(intercept) + var(regional) +2*(covar(intercept,regional)),
> and then get the SE of the variance
> SE=sqrt(var)
> 95% CI = intercept + regional - 1.95*SE and intercept + regional + 1.95*SE.
> 
> I was hoping that a contrast statement could be written that would give me the point estimate and SE for "general" and its SE and another contrast statement could be written that would give me the point estimate and SE for "general" and it SE without my having to work directly with the variance-covariance matrix. I tried doing this using the fit.contrast statements (from the gmodels package):

I'm guessing that the second contrast you were hoping for was actually for "regional".

Contrasts, hence the name, are for differences between two levels (or more accurately between the means on the scale specified by the link parameter. In the absence of another level the only other reference point would be a value of zero or perhaps the value you specified by your offset term.

-- 
David


> 
> fit.contrast(model,type,c(1,0),showall=TRUE)
> fit.contrast(model,type,c(0,1),showall=TRUE)
> 
> and received the error message, 
> Error in `[[<-`(`*tmp*`, varname, value = c(0, 1)) : 
>   no such index at level 1
> 
> Perhaps fit.contrast is not the way to accomplish my goal. Perhaps my goal can be accomplished without a contrast statement, but I don't know how.
> 
> Thank you,
> John
>  
> 
> 
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing) 
> 
> 
> 
> From: David Winsemius <dwinsemius at comcast.net>
> Sent: Sunday, October 22, 2017 1:20 PM
> To: Sorkin, John
> Cc: r-help at r-project.org
> Subject: Re: [R] Syntax for fit.contrast
>  
> 
> > On Oct 22, 2017, at 6:04 AM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> > 
> > I have a model (run with glm) that has a factor, type. Type has two levels, "general" and "regional". I am trying to get estimates (and SEs) for the model with type="general" and type ="regional" using fit.contrast
> 
>  ?fit.contrast
> No documentation for ?fit.contrast? in specified packages and libraries:
> you could try ???fit.contrast?
> 
> Perhaps the gmodels function of that name?
> 
> > but I can't get the syntax of the coefficients to use in fit.contrast correct. I hope someone can show me how to use fit.contrast, or some other method to get estimate with SEs. (I know I can use the variance co-variance matrix, but I would rather not have to code the linear contrast my self from the coefficients of the matrix)
> > 
> 
> I'm having trouble understanding what you are trying to extract. There are only 2 levels so there is really only one interesting contrast ("general" vs "regional") , and it's magnitude would be reported by just typing `model`, and it's SE would show up in output of `summary(model)`.
> 
> I'm thinking you should pick one of the examples in gmodels::fit.contrast that most resembles your real problem,  post it,  and  and then explain what difficulties you are having with interpretation.
> 
> -- 
> David.
> 
> 
> > Thank  you,
> > 
> > John
> > 
> > 
> > My model:
> > 
> > model=glm(events~type,family=poisson(link=log),offset=log(SS),data=data)
> > 
> > 
> > Model details:
> > 
> >> summary(data$type)
> > 
> > general regional
> >      16       16
> > 
> >> levels(data$type)
> > [1] "general"  "regional"
> > 
> >> contrasts(data$type)
> >         regional
> > general         0
> > regional        1
> > 
> > 
> > I have tried the following syntax for fit.contrast
> > 
> > fit.contrast(model,type,c(1,0))
> > and get an error:
> > Error in `[[<-`(`*tmp*`, varname, value = cmat) :
> >  no such index at level 1
> > 
> > 
> >> fit.contrast(model,type,c(0,1),showall=TRUE)
> > and get an error:
> > Error in `[[<-`(`*tmp*`, varname, value = cmat) :
> >  no such index at level 1
> > 
> > 
> > 
> >> fit.contrast(model,type,c(1,-1),showall=TRUE)
> > and get an error:
> > Error in `[[<-`(`*tmp*`, varname, value = cmat) :
> >  no such index at level 1
> > 
> > 
> >> fit.contrast(model,type,c(0))
> > and get an error:
> > Error in make.contrasts(coeff, ncol(coeff)) :
> >  Too many contrasts specified. Must be less than the number of factor levels (columns).
> > 
> >> fit.contrast(model,type,c(1))
> > Error in make.contrasts(coeff, ncol(coeff)) :
> > and get an error
> >  Too many contrasts specified. Must be less than the number of factor levels (columns).
> > 
> > 
> > 
> > 
> > 
> > 
> > 
> > 
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine
> > Chief, Biostatistics and Informatics
> > University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> > Baltimore VA Medical Center
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > (Phone) 410-605-7119
> > (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> > 
> > 
> >        [[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From jsorkin at som.umaryland.edu  Mon Oct 23 02:01:26 2017
From: jsorkin at som.umaryland.edu (Sorkin, John)
Date: Mon, 23 Oct 2017 00:01:26 +0000
Subject: [R] Syntax for fit.contrast (from package gmodels)
In-Reply-To: <D54F490B-9F22-461E-A592-D0887D92EFA1@comcast.net>
References: <BY2PR0301MB061685A793413A3EC741F5EFE2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
 <EDE44E15-E20D-466E-B821-0B4F36E602F7@comcast.net>
 <BY2PR0301MB0616B317C0123DA2E6E61DA0E2410@BY2PR0301MB0616.namprd03.prod.outlook.com>,
 <D54F490B-9F22-461E-A592-D0887D92EFA1@comcast.net>
Message-ID: <BY2PR0301MB06168B8166AAE969A06E5FBFE2410@BY2PR0301MB0616.namprd03.prod.outlook.com>

David,

Again  you have my thanks!.

You are correct. What I want is not technically a contrast. What I want is the estimate for "regional" and its SE. I don't mind if I get these on the log scale; I can get the anti-log. Can you suggest  how I can get the point estimate and its SE for "regional"? The predict function will give the point estimate, but not (to my knowledge) the SE.

Thank you,

John


John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)



________________________________
From: David Winsemius <dwinsemius at comcast.net>
Sent: Sunday, October 22, 2017 7:56 PM
To: Sorkin, John
Cc: r-help at r-project.org
Subject: Re: [R] Syntax for fit.contrast (from package gmodels)


> On Oct 22, 2017, at 3:56 PM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
>
> David,
> Thank you for responding to my post.
>
> Please consider the following output (typeregional is a factor having two levels, "regional" vs. "general"):
> Call:
> glm(formula = events ~ type, family = poisson(link = log), data = data,
>     offset = log(SS))
>
> Deviance Residuals:
>     Min       1Q   Median       3Q      Max
> -43.606  -17.295   -4.651    4.204   38.421
>
> Coefficients:
>              Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -2.52830    0.01085 -233.13   <2e-16 ***
> typeregional  0.33788    0.01641   20.59   <2e-16 ***
>
> Let's forget for a moment that the model is a Poisson regression and pretend that the output is from a simple linear regression, e.g. from lm.
>
> To get the estimate for "general" one simply needs to use the value of the intercept i.e. -2.5830. Similarly to get the 95% CI of general one simply needs to compute -2.52830-(1.96*0.01085) and -2.52830+(1.96*0.01085).
>
> To get the estimate for "regional" one needs to compute intercept + typeregional, i.e. -2.52830 + 0.33788. To get the 95% CI is somewhat more difficult as one needs to use results from the variance-covariance matix, specifically the variance of intercept, the variance of "regional", and the covariance of (intercept,"regional") which involves:
> var =  var(intercept) + var(regional) +2*(covar(intercept,regional)),
> and then get the SE of the variance
> SE=sqrt(var)
> 95% CI = intercept + regional - 1.95*SE and intercept + regional + 1.95*SE.
>
> I was hoping that a contrast statement could be written that would give me the point estimate and SE for "general" and its SE and another contrast statement could be written that would give me the point estimate and SE for "general" and it SE without my having to work directly with the variance-covariance matrix. I tried doing this using the fit.contrast statements (from the gmodels package):

I'm guessing that the second contrast you were hoping for was actually for "regional".

Contrasts, hence the name, are for differences between two levels (or more accurately between the means on the scale specified by the link parameter. In the absence of another level the only other reference point would be a value of zero or perhaps the value you specified by your offset term.

--
David


>
> fit.contrast(model,type,c(1,0),showall=TRUE)
> fit.contrast(model,type,c(0,1),showall=TRUE)
>
> and received the error message,
> Error in `[[<-`(`*tmp*`, varname, value = c(0, 1)) :
>   no such index at level 1
>
> Perhaps fit.contrast is not the way to accomplish my goal. Perhaps my goal can be accomplished without a contrast statement, but I don't know how.
>
> Thank you,
> John
>
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
>
> From: David Winsemius <dwinsemius at comcast.net>
> Sent: Sunday, October 22, 2017 1:20 PM
> To: Sorkin, John
> Cc: r-help at r-project.org
> Subject: Re: [R] Syntax for fit.contrast
>
>
> > On Oct 22, 2017, at 6:04 AM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> >
> > I have a model (run with glm) that has a factor, type. Type has two levels, "general" and "regional". I am trying to get estimates (and SEs) for the model with type="general" and type ="regional" using fit.contrast
>
>  ?fit.contrast
> No documentation for ?fit.contrast? in specified packages and libraries:
> you could try ???fit.contrast?
>
> Perhaps the gmodels function of that name?
>
> > but I can't get the syntax of the coefficients to use in fit.contrast correct. I hope someone can show me how to use fit.contrast, or some other method to get estimate with SEs. (I know I can use the variance co-variance matrix, but I would rather not have to code the linear contrast my self from the coefficients of the matrix)
> >
>
> I'm having trouble understanding what you are trying to extract. There are only 2 levels so there is really only one interesting contrast ("general" vs "regional") , and it's magnitude would be reported by just typing `model`, and it's SE would show up in output of `summary(model)`.
>
> I'm thinking you should pick one of the examples in gmodels::fit.contrast that most resembles your real problem,  post it,  and  and then explain what difficulties you are having with interpretation.
>
> --
> David.
>
>
> > Thank  you,
> >
> > John
> >
> >
> > My model:
> >
> > model=glm(events~type,family=poisson(link=log),offset=log(SS),data=data)
> >
> >
> > Model details:
> >
> >> summary(data$type)
> >
> > general regional
> >      16       16
> >
> >> levels(data$type)
> > [1] "general"  "regional"
> >
> >> contrasts(data$type)
> >         regional
> > general         0
> > regional        1
> >
> >
> > I have tried the following syntax for fit.contrast
> >
> > fit.contrast(model,type,c(1,0))
> > and get an error:
> > Error in `[[<-`(`*tmp*`, varname, value = cmat) :
> >  no such index at level 1
> >
> >
> >> fit.contrast(model,type,c(0,1),showall=TRUE)
> > and get an error:
> > Error in `[[<-`(`*tmp*`, varname, value = cmat) :
> >  no such index at level 1
> >
> >
> >
> >> fit.contrast(model,type,c(1,-1),showall=TRUE)
> > and get an error:
> > Error in `[[<-`(`*tmp*`, varname, value = cmat) :
> >  no such index at level 1
> >
> >
> >> fit.contrast(model,type,c(0))
> > and get an error:
> > Error in make.contrasts(coeff, ncol(coeff)) :
> >  Too many contrasts specified. Must be less than the number of factor levels (columns).
> >
> >> fit.contrast(model,type,c(1))
> > Error in make.contrasts(coeff, ncol(coeff)) :
> > and get an error
> >  Too many contrasts specified. Must be less than the number of factor levels (columns).
> >
> >
> >
> >
> >
> >
> >
> >
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine
> > Chief, Biostatistics and Informatics
> > University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> > Baltimore VA Medical Center
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > (Phone) 410-605-7119
> > (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> >
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law






	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Mon Oct 23 02:15:29 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 22 Oct 2017 17:15:29 -0700
Subject: [R] Syntax for fit.contrast (from package gmodels)
In-Reply-To: <BY2PR0301MB06168B8166AAE969A06E5FBFE2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
References: <BY2PR0301MB061685A793413A3EC741F5EFE2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
 <EDE44E15-E20D-466E-B821-0B4F36E602F7@comcast.net>
 <BY2PR0301MB0616B317C0123DA2E6E61DA0E2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
 <D54F490B-9F22-461E-A592-D0887D92EFA1@comcast.net>
 <BY2PR0301MB06168B8166AAE969A06E5FBFE2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
Message-ID: <908B23E5-0841-4E23-8807-7DC17769FDDF@comcast.net>


> On Oct 22, 2017, at 5:01 PM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> 
> David,
> Again  you have my thanks!.
> You are correct. What I want is not technically a contrast. What I want is the estimate for "regional" and its SE.

There needs to be a reference value for the contrast. Contrasts are differences. I gave you the choice of two references (treatment constrast or the offset value you specified). Pick one or suggest an alternate value. Typical alternate values are the global mean or zero.

Read ?predict.glm

"se.fit	    logical switch indicating if standard errors are required."


> I don't mind if I get these on the log scale; I can get the anti-log. Can you suggest  how I can get the point estimate and its SE for "regional"? The predict function will give the point estimate, but not (to my knowledge) the SE.


> Thank you,
> John
> 
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing) 
> 
> 
> 
> From: David Winsemius <dwinsemius at comcast.net>
> Sent: Sunday, October 22, 2017 7:56 PM
> To: Sorkin, John
> Cc: r-help at r-project.org
> Subject: Re: [R] Syntax for fit.contrast (from package gmodels)
>  
> 
> > On Oct 22, 2017, at 3:56 PM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> > 
> > David,
> > Thank you for responding to my post.
> > 
> > Please consider the following output (typeregional is a factor having two levels, "regional" vs. "general"):
> > Call:
> > glm(formula = events ~ type, family = poisson(link = log), data = data, 
> >     offset = log(SS))
> > 
> > Deviance Residuals: 
> >     Min       1Q   Median       3Q      Max  
> > -43.606  -17.295   -4.651    4.204   38.421  
> > 
> > Coefficients:
> >              Estimate Std. Error z value Pr(>|z|)    
> > (Intercept)  -2.52830    0.01085 -233.13   <2e-16 ***
> > typeregional  0.33788    0.01641   20.59   <2e-16 ***
> > 
> > Let's forget for a moment that the model is a Poisson regression and pretend that the output is from a simple linear regression, e.g. from lm.
> > 
> > To get the estimate for "general" one simply needs to use the value of the intercept i.e. -2.5830. Similarly to get the 95% CI of general one simply needs to compute -2.52830-(1.96*0.01085) and -2.52830+(1.96*0.01085).
> > 
> > To get the estimate for "regional" one needs to compute intercept + typeregional, i.e. -2.52830 + 0.33788. To get the 95% CI is somewhat more difficult as one needs to use results from the variance-covariance matix, specifically the variance of intercept, the variance of "regional", and the covariance of (intercept,"regional") which involves:
> > var =  var(intercept) + var(regional) +2*(covar(intercept,regional)),
> > and then get the SE of the variance
> > SE=sqrt(var)
> > 95% CI = intercept + regional - 1.95*SE and intercept + regional + 1.95*SE.
> > 
> > I was hoping that a contrast statement could be written that would give me the point estimate and SE for "general" and its SE and another contrast statement could be written that would give me the point estimate and SE for "general" and it SE without my having to work directly with the variance-covariance matrix. I tried doing this using the fit.contrast statements (from the gmodels package):
> 
> I'm guessing that the second contrast you were hoping for was actually for "regional".
> 
> Contrasts, hence the name, are for differences between two levels (or more accurately between the means on the scale specified by the link parameter. In the absence of another level the only other reference point would be a value of zero or perhaps the value you specified by your offset term.
> 
> -- 
> David
> 
> 
> > 
> > fit.contrast(model,type,c(1,0),showall=TRUE)
> > fit.contrast(model,type,c(0,1),showall=TRUE)
> > 
> > and received the error message, 
> > Error in `[[<-`(`*tmp*`, varname, value = c(0, 1)) : 
> >   no such index at level 1
> > 
> > Perhaps fit.contrast is not the way to accomplish my goal. Perhaps my goal can be accomplished without a contrast statement, but I don't know how.
> > 
> > Thank you,
> > John
> >  
> > 
> > 
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine
> > Chief, Biostatistics and Informatics
> > University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> > Baltimore VA Medical Center
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > (Phone) 410-605-7119
> > (Fax) 410-605-7913 (Please call phone number above prior to faxing) 
> > 
> > 
> > 
> > From: David Winsemius <dwinsemius at comcast.net>
> > Sent: Sunday, October 22, 2017 1:20 PM
> > To: Sorkin, John
> > Cc: r-help at r-project.org
> > Subject: Re: [R] Syntax for fit.contrast
> >  
> > 
> > > On Oct 22, 2017, at 6:04 AM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> > > 
> > > I have a model (run with glm) that has a factor, type. Type has two levels, "general" and "regional". I am trying to get estimates (and SEs) for the model with type="general" and type ="regional" using fit.contrast
> > 
> >  ?fit.contrast
> > No documentation for ?fit.contrast? in specified packages and libraries:
> > you could try ???fit.contrast?
> > 
> > Perhaps the gmodels function of that name?
> > 
> > > but I can't get the syntax of the coefficients to use in fit.contrast correct. I hope someone can show me how to use fit.contrast, or some other method to get estimate with SEs. (I know I can use the variance co-variance matrix, but I would rather not have to code the linear contrast my self from the coefficients of the matrix)
> > > 
> > 
> > I'm having trouble understanding what you are trying to extract. There are only 2 levels so there is really only one interesting contrast ("general" vs "regional") , and it's magnitude would be reported by just typing `model`, and it's SE would show up in output of `summary(model)`.
> > 
> > I'm thinking you should pick one of the examples in gmodels::fit.contrast that most resembles your real problem,  post it,  and  and then explain what difficulties you are having with interpretation.
> > 
> > -- 
> > David.
> > 
> > 
> > > Thank  you,
> > > 
> > > John
> > > 
> > > 
> > > My model:
> > > 
> > > model=glm(events~type,family=poisson(link=log),offset=log(SS),data=data)
> > > 
> > > 
> > > Model details:
> > > 
> > >> summary(data$type)
> > > 
> > > general regional
> > >      16       16
> > > 
> > >> levels(data$type)
> > > [1] "general"  "regional"
> > > 
> > >> contrasts(data$type)
> > >         regional
> > > general         0
> > > regional        1
> > > 
> > > 
> > > I have tried the following syntax for fit.contrast
> > > 
> > > fit.contrast(model,type,c(1,0))
> > > and get an error:
> > > Error in `[[<-`(`*tmp*`, varname, value = cmat) :
> > >  no such index at level 1
> > > 
> > > 
> > >> fit.contrast(model,type,c(0,1),showall=TRUE)
> > > and get an error:
> > > Error in `[[<-`(`*tmp*`, varname, value = cmat) :
> > >  no such index at level 1
> > > 
> > > 
> > > 
> > >> fit.contrast(model,type,c(1,-1),showall=TRUE)
> > > and get an error:
> > > Error in `[[<-`(`*tmp*`, varname, value = cmat) :
> > >  no such index at level 1
> > > 
> > > 
> > >> fit.contrast(model,type,c(0))
> > > and get an error:
> > > Error in make.contrasts(coeff, ncol(coeff)) :
> > >  Too many contrasts specified. Must be less than the number of factor levels (columns).
> > > 
> > >> fit.contrast(model,type,c(1))
> > > Error in make.contrasts(coeff, ncol(coeff)) :
> > > and get an error
> > >  Too many contrasts specified. Must be less than the number of factor levels (columns).
> > > 
> > > 
> > > 
> > > 
> > > 
> > > 
> > > 
> > > 
> > > John David Sorkin M.D., Ph.D.
> > > Professor of Medicine
> > > Chief, Biostatistics and Informatics
> > > University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> > > Baltimore VA Medical Center
> > > 10 North Greene Street
> > > GRECC (BT/18/GR)
> > > Baltimore, MD 21201-1524
> > > (Phone) 410-605-7119
> > > (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> > > 
> > > 
> > >        [[alternative HTML version deleted]]
> > > 
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > 
> > David Winsemius
> > Alameda, CA, USA
> > 
> > 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
> 
> David Winsemius
> Alameda, CA, USA
> 
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From jsorkin at som.umaryland.edu  Mon Oct 23 02:26:28 2017
From: jsorkin at som.umaryland.edu (Sorkin, John)
Date: Mon, 23 Oct 2017 00:26:28 +0000
Subject: [R] Syntax for fit.contrast (from package gmodels)
In-Reply-To: <908B23E5-0841-4E23-8807-7DC17769FDDF@comcast.net>
References: <BY2PR0301MB061685A793413A3EC741F5EFE2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
 <EDE44E15-E20D-466E-B821-0B4F36E602F7@comcast.net>
 <BY2PR0301MB0616B317C0123DA2E6E61DA0E2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
 <D54F490B-9F22-461E-A592-D0887D92EFA1@comcast.net>
 <BY2PR0301MB06168B8166AAE969A06E5FBFE2410@BY2PR0301MB0616.namprd03.prod.outlook.com>,
 <908B23E5-0841-4E23-8807-7DC17769FDDF@comcast.net>
Message-ID: <BY2PR0301MB0616304354EDD5F70B9625D4E2460@BY2PR0301MB0616.namprd03.prod.outlook.com>

David,

predict.glm and se.fit were exactly what I was looking for.

Many  thanks!

John


John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)



________________________________
From: David Winsemius <dwinsemius at comcast.net>
Sent: Sunday, October 22, 2017 8:15 PM
To: Sorkin, John
Cc: r-help at r-project.org
Subject: Re: [R] Syntax for fit.contrast (from package gmodels)


> On Oct 22, 2017, at 5:01 PM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
>
> David,
> Again  you have my thanks!.
> You are correct. What I want is not technically a contrast. What I want is the estimate for "regional" and its SE.

There needs to be a reference value for the contrast. Contrasts are differences. I gave you the choice of two references (treatment constrast or the offset value you specified). Pick one or suggest an alternate value. Typical alternate values are the global mean or zero.

Read ?predict.glm

"se.fit     logical switch indicating if standard errors are required."


> I don't mind if I get these on the log scale; I can get the anti-log. Can you suggest  how I can get the point estimate and its SE for "regional"? The predict function will give the point estimate, but not (to my knowledge) the SE.


> Thank you,
> John
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
>
> From: David Winsemius <dwinsemius at comcast.net>
> Sent: Sunday, October 22, 2017 7:56 PM
> To: Sorkin, John
> Cc: r-help at r-project.org
> Subject: Re: [R] Syntax for fit.contrast (from package gmodels)
>
>
> > On Oct 22, 2017, at 3:56 PM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> >
> > David,
> > Thank you for responding to my post.
> >
> > Please consider the following output (typeregional is a factor having two levels, "regional" vs. "general"):
> > Call:
> > glm(formula = events ~ type, family = poisson(link = log), data = data,
> >     offset = log(SS))
> >
> > Deviance Residuals:
> >     Min       1Q   Median       3Q      Max
> > -43.606  -17.295   -4.651    4.204   38.421
> >
> > Coefficients:
> >              Estimate Std. Error z value Pr(>|z|)
> > (Intercept)  -2.52830    0.01085 -233.13   <2e-16 ***
> > typeregional  0.33788    0.01641   20.59   <2e-16 ***
> >
> > Let's forget for a moment that the model is a Poisson regression and pretend that the output is from a simple linear regression, e.g. from lm.
> >
> > To get the estimate for "general" one simply needs to use the value of the intercept i.e. -2.5830. Similarly to get the 95% CI of general one simply needs to compute -2.52830-(1.96*0.01085) and -2.52830+(1.96*0.01085).
> >
> > To get the estimate for "regional" one needs to compute intercept + typeregional, i.e. -2.52830 + 0.33788. To get the 95% CI is somewhat more difficult as one needs to use results from the variance-covariance matix, specifically the variance of intercept, the variance of "regional", and the covariance of (intercept,"regional") which involves:
> > var =  var(intercept) + var(regional) +2*(covar(intercept,regional)),
> > and then get the SE of the variance
> > SE=sqrt(var)
> > 95% CI = intercept + regional - 1.95*SE and intercept + regional + 1.95*SE.
> >
> > I was hoping that a contrast statement could be written that would give me the point estimate and SE for "general" and its SE and another contrast statement could be written that would give me the point estimate and SE for "general" and it SE without my having to work directly with the variance-covariance matrix. I tried doing this using the fit.contrast statements (from the gmodels package):
>
> I'm guessing that the second contrast you were hoping for was actually for "regional".
>
> Contrasts, hence the name, are for differences between two levels (or more accurately between the means on the scale specified by the link parameter. In the absence of another level the only other reference point would be a value of zero or perhaps the value you specified by your offset term.
>
> --
> David
>
>
> >
> > fit.contrast(model,type,c(1,0),showall=TRUE)
> > fit.contrast(model,type,c(0,1),showall=TRUE)
> >
> > and received the error message,
> > Error in `[[<-`(`*tmp*`, varname, value = c(0, 1)) :
> >   no such index at level 1
> >
> > Perhaps fit.contrast is not the way to accomplish my goal. Perhaps my goal can be accomplished without a contrast statement, but I don't know how.
> >
> > Thank you,
> > John
> >
> >
> >
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine
> > Chief, Biostatistics and Informatics
> > University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> > Baltimore VA Medical Center
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > (Phone) 410-605-7119
> > (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> >
> >
> >
> > From: David Winsemius <dwinsemius at comcast.net>
> > Sent: Sunday, October 22, 2017 1:20 PM
> > To: Sorkin, John
> > Cc: r-help at r-project.org
> > Subject: Re: [R] Syntax for fit.contrast
> >
> >
> > > On Oct 22, 2017, at 6:04 AM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> > >
> > > I have a model (run with glm) that has a factor, type. Type has two levels, "general" and "regional". I am trying to get estimates (and SEs) for the model with type="general" and type ="regional" using fit.contrast
> >
> >  ?fit.contrast
> > No documentation for ?fit.contrast? in specified packages and libraries:
> > you could try ???fit.contrast?
> >
> > Perhaps the gmodels function of that name?
> >
> > > but I can't get the syntax of the coefficients to use in fit.contrast correct. I hope someone can show me how to use fit.contrast, or some other method to get estimate with SEs. (I know I can use the variance co-variance matrix, but I would rather not have to code the linear contrast my self from the coefficients of the matrix)
> > >
> >
> > I'm having trouble understanding what you are trying to extract. There are only 2 levels so there is really only one interesting contrast ("general" vs "regional") , and it's magnitude would be reported by just typing `model`, and it's SE would show up in output of `summary(model)`.
> >
> > I'm thinking you should pick one of the examples in gmodels::fit.contrast that most resembles your real problem,  post it,  and  and then explain what difficulties you are having with interpretation.
> >
> > --
> > David.
> >
> >
> > > Thank  you,
> > >
> > > John
> > >
> > >
> > > My model:
> > >
> > > model=glm(events~type,family=poisson(link=log),offset=log(SS),data=data)
> > >
> > >
> > > Model details:
> > >
> > >> summary(data$type)
> > >
> > > general regional
> > >      16       16
> > >
> > >> levels(data$type)
> > > [1] "general"  "regional"
> > >
> > >> contrasts(data$type)
> > >         regional
> > > general         0
> > > regional        1
> > >
> > >
> > > I have tried the following syntax for fit.contrast
> > >
> > > fit.contrast(model,type,c(1,0))
> > > and get an error:
> > > Error in `[[<-`(`*tmp*`, varname, value = cmat) :
> > >  no such index at level 1
> > >
> > >
> > >> fit.contrast(model,type,c(0,1),showall=TRUE)
> > > and get an error:
> > > Error in `[[<-`(`*tmp*`, varname, value = cmat) :
> > >  no such index at level 1
> > >
> > >
> > >
> > >> fit.contrast(model,type,c(1,-1),showall=TRUE)
> > > and get an error:
> > > Error in `[[<-`(`*tmp*`, varname, value = cmat) :
> > >  no such index at level 1
> > >
> > >
> > >> fit.contrast(model,type,c(0))
> > > and get an error:
> > > Error in make.contrasts(coeff, ncol(coeff)) :
> > >  Too many contrasts specified. Must be less than the number of factor levels (columns).
> > >
> > >> fit.contrast(model,type,c(1))
> > > Error in make.contrasts(coeff, ncol(coeff)) :
> > > and get an error
> > >  Too many contrasts specified. Must be less than the number of factor levels (columns).
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > > John David Sorkin M.D., Ph.D.
> > > Professor of Medicine
> > > Chief, Biostatistics and Informatics
> > > University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> > > Baltimore VA Medical Center
> > > 10 North Greene Street
> > > GRECC (BT/18/GR)
> > > Baltimore, MD 21201-1524
> > > (Phone) 410-605-7119
> > > (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> > >
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> > 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
>
> David Winsemius
> Alameda, CA, USA
>
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law






	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Mon Oct 23 03:10:44 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sun, 22 Oct 2017 18:10:44 -0700
Subject: [R] Problem when trying to run Java in R:
In-Reply-To: <wXugqhHDrV6BFbzd1ja-qa0Od__Q1rBn29Q525GZbz2UhFxMYVwgD7d6M6UCJDH8QBVtCY2-1XXfi-C45hEvIidJ-UfFn985ILMw9cAb4nY=@protonmail.com>
References: <wXugqhHDrV6BFbzd1ja-qa0Od__Q1rBn29Q525GZbz2UhFxMYVwgD7d6M6UCJDH8QBVtCY2-1XXfi-C45hEvIidJ-UfFn985ILMw9cAb4nY=@protonmail.com>
Message-ID: <2ED50BEA-D53E-49F9-9431-C4E2F299EFD0@dcn.davis.ca.us>

I suggest asking a Mac-OS-specific question like this on the r-sig-mac mailing list. A common problem is attempting to access a 64-bit implementation of Java from a 32-bit version of R, or vice versa, but you may want to include the output of sessionInfo() and a reproducible example (both input and output) when you ask your question again to help them help you. 
-- 
Sent from my phone. Please excuse my brevity.

On October 21, 2017 4:35:19 AM PDT, Morkus via R-help <r-help at r-project.org> wrote:
>Hello All,
>
>Although running Java from R used to work, for some mysterious reason,
>it's stopped.
>
>Today when I tried to load a basic JDBC driver (or the sample .jinit())
>code, I got:
>
>- JavaVM: requested Java version ((null)) not available. Using Java at
>"" instead.
>
>- JavaVM: Failed to load JVM: /bundle/Libraries/libserver.dylib
>
>- JavaVM FATAL: Failed to load the jvm library.
>
>I saw postings on StackOverflow about this issue, but none of the
>suggested fixes helped.
>
>I'm on Mac OS 10.13.
>
>My JAVA_HOME is:
>/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home
>
>java -version
>java version "1.8.0_144"
>Java(TM) SE Runtime Environment (build 1.8.0_144-b01)
>Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)
>
>I'm using R: 3.3.3
>
>Here is sample code that also throws this same error:
>
>    library(rJava)
>    .jinit() # this starts the JVM
>    s <- .jnew("java/lang/String", "Hello World!")
>
>(this is the "hello world" equivalent from the rJava site)
>
>----
>
>I also tried to use Java 7 (rev. 51), which used to work, but that
>still fails.
>
>Also tried a fresh install of R/RStudio on another machine with the
>same results (same Java version, etc., however).
>
>I suspect Java itself is working fine since JDBC code from Java
>programs has no issues.
>
>Not sure why loading Java in R stopped working, but would appreciate
>any suggestions.
>
>Thanks very much,
>
>Sent from [ProtonMail](https://protonmail.com), Swiss-based encrypted
>email.
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Mon Oct 23 03:13:48 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 22 Oct 2017 18:13:48 -0700
Subject: [R] Syntax for fit.contrast (from package gmodels)
In-Reply-To: <BY2PR0301MB0616304354EDD5F70B9625D4E2460@BY2PR0301MB0616.namprd03.prod.outlook.com>
References: <BY2PR0301MB061685A793413A3EC741F5EFE2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
 <EDE44E15-E20D-466E-B821-0B4F36E602F7@comcast.net>
 <BY2PR0301MB0616B317C0123DA2E6E61DA0E2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
 <D54F490B-9F22-461E-A592-D0887D92EFA1@comcast.net>
 <BY2PR0301MB06168B8166AAE969A06E5FBFE2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
 <908B23E5-0841-4E23-8807-7DC17769FDDF@comcast.net>
 <BY2PR0301MB0616304354EDD5F70B9625D4E2460@BY2PR0301MB0616.namprd03.prod.outlook.com>
Message-ID: <75B80339-380B-4928-9B5B-3BE68AAF4A1D@comcast.net>


> On Oct 22, 2017, at 5:26 PM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> 
> David,
> predict.glm and se.fit were exactly what I was looking for.

The default 'se' delivered for a listed contrast is not for that particular level per se, but rather for the difference between that level and the non-listed factor level (its mean) which is "embedded" as it were in the Intercept. The se for the Intercept is for the mean of all the reference levels jointly being zero.

Apologies for any mis-steps in this effort. I'm enjoying my fourth IPA. I usually do not drink and derive.

-- 
David.


> Many  thanks!
> John
> 
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing) 
> 
> 
> 
> From: David Winsemius <dwinsemius at comcast.net>
> Sent: Sunday, October 22, 2017 8:15 PM
> To: Sorkin, John
> Cc: r-help at r-project.org
> Subject: Re: [R] Syntax for fit.contrast (from package gmodels)
>  
> 
> > On Oct 22, 2017, at 5:01 PM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> > 
> > David,
> > Again  you have my thanks!.
> > You are correct. What I want is not technically a contrast. What I want is the estimate for "regional" and its SE.
> 
> There needs to be a reference value for the contrast. Contrasts are differences. I gave you the choice of two references (treatment constrast or the offset value you specified). Pick one or suggest an alternate value. Typical alternate values are the global mean or zero.
> 
> Read ?predict.glm
> 
> "se.fit     logical switch indicating if standard errors are required."
> 
> 
> > I don't mind if I get these on the log scale; I can get the anti-log. Can you suggest  how I can get the point estimate and its SE for "regional"? The predict function will give the point estimate, but not (to my knowledge) the SE.
> 
> 
> > Thank you,
> > John
> > 
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine
> > Chief, Biostatistics and Informatics
> > University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> > Baltimore VA Medical Center
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > (Phone) 410-605-7119
> > (Fax) 410-605-7913 (Please call phone number above prior to faxing) 
> > 
> > 
> > 
> > From: David Winsemius <dwinsemius at comcast.net>
> > Sent: Sunday, October 22, 2017 7:56 PM
> > To: Sorkin, John
> > Cc: r-help at r-project.org
> > Subject: Re: [R] Syntax for fit.contrast (from package gmodels)
> >  
> > 
> > > On Oct 22, 2017, at 3:56 PM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> > > 
> > > David,
> > > Thank you for responding to my post.
> > > 
> > > Please consider the following output (typeregional is a factor having two levels, "regional" vs. "general"):
> > > Call:
> > > glm(formula = events ~ type, family = poisson(link = log), data = data, 
> > >     offset = log(SS))
> > > 
> > > Deviance Residuals: 
> > >     Min       1Q   Median       3Q      Max  
> > > -43.606  -17.295   -4.651    4.204   38.421  
> > > 
> > > Coefficients:
> > >              Estimate Std. Error z value Pr(>|z|)    
> > > (Intercept)  -2.52830    0.01085 -233.13   <2e-16 ***
> > > typeregional  0.33788    0.01641   20.59   <2e-16 ***
> > > 
> > > Let's forget for a moment that the model is a Poisson regression and pretend that the output is from a simple linear regression, e.g. from lm.
> > > 
> > > To get the estimate for "general" one simply needs to use the value of the intercept i.e. -2.5830. Similarly to get the 95% CI of general one simply needs to compute -2.52830-(1.96*0.01085) and -2.52830+(1.96*0.01085).
> > > 
> > > To get the estimate for "regional" one needs to compute intercept + typeregional, i.e. -2.52830 + 0.33788. To get the 95% CI is somewhat more difficult as one needs to use results from the variance-covariance matix, specifically the variance of intercept, the variance of "regional", and the covariance of (intercept,"regional") which involves:
> > > var =  var(intercept) + var(regional) +2*(covar(intercept,regional)),
> > > and then get the SE of the variance
> > > SE=sqrt(var)
> > > 95% CI = intercept + regional - 1.95*SE and intercept + regional + 1.95*SE.
> > > 
> > > I was hoping that a contrast statement could be written that would give me the point estimate and SE for "general" and its SE and another contrast statement could be written that would give me the point estimate and SE for "general" and it SE without my having to work directly with the variance-covariance matrix. I tried doing this using the fit.contrast statements (from the gmodels package):
> > 
> > I'm guessing that the second contrast you were hoping for was actually for "regional".
> > 
> > Contrasts, hence the name, are for differences between two levels (or more accurately between the means on the scale specified by the link parameter. In the absence of another level the only other reference point would be a value of zero or perhaps the value you specified by your offset term.
> > 
> > -- 
> > David
> > 
> > 
> > > 
> > > fit.contrast(model,type,c(1,0),showall=TRUE)
> > > fit.contrast(model,type,c(0,1),showall=TRUE)
> > > 
> > > and received the error message, 
> > > Error in `[[<-`(`*tmp*`, varname, value = c(0, 1)) : 
> > >   no such index at level 1
> > > 
> > > Perhaps fit.contrast is not the way to accomplish my goal. Perhaps my goal can be accomplished without a contrast statement, but I don't know how.
> > > 
> > > Thank you,
> > > John
> > >  
> > > 
> > > 
> > > John David Sorkin M.D., Ph.D.
> > > Professor of Medicine
> > > Chief, Biostatistics and Informatics
> > > University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> > > Baltimore VA Medical Center
> > > 10 North Greene Street
> > > GRECC (BT/18/GR)
> > > Baltimore, MD 21201-1524
> > > (Phone) 410-605-7119
> > > (Fax) 410-605-7913 (Please call phone number above prior to faxing) 
> > > 
> > > 
> > > 
> > > From: David Winsemius <dwinsemius at comcast.net>
> > > Sent: Sunday, October 22, 2017 1:20 PM
> > > To: Sorkin, John
> > > Cc: r-help at r-project.org
> > > Subject: Re: [R] Syntax for fit.contrast
> > >  
> > > 
> > > > On Oct 22, 2017, at 6:04 AM, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> > > > 
> > > > I have a model (run with glm) that has a factor, type. Type has two levels, "general" and "regional". I am trying to get estimates (and SEs) for the model with type="general" and type ="regional" using fit.contrast
> > > 
> > >  ?fit.contrast
> > > No documentation for ?fit.contrast? in specified packages and libraries:
> > > you could try ???fit.contrast?
> > > 
> > > Perhaps the gmodels function of that name?
> > > 
> > > > but I can't get the syntax of the coefficients to use in fit.contrast correct. I hope someone can show me how to use fit.contrast, or some other method to get estimate with SEs. (I know I can use the variance co-variance matrix, but I would rather not have to code the linear contrast my self from the coefficients of the matrix)
> > > > 
> > > 
> > > I'm having trouble understanding what you are trying to extract. There are only 2 levels so there is really only one interesting contrast ("general" vs "regional") , and it's magnitude would be reported by just typing `model`, and it's SE would show up in output of `summary(model)`.
> > > 
> > > I'm thinking you should pick one of the examples in gmodels::fit.contrast that most resembles your real problem,  post it,  and  and then explain what difficulties you are having with interpretation.
> > > 
> > > -- 
> > > David.
> > > 
> > > 
> > > > Thank  you,
> > > > 
> > > > John
> > > > 
> > > > 
> > > > My model:
> > > > 
> > > > model=glm(events~type,family=poisson(link=log),offset=log(SS),data=data)
> > > > 
> > > > 
> > > > Model details:
> > > > 
> > > >> summary(data$type)
> > > > 
> > > > general regional
> > > >      16       16
> > > > 
> > > >> levels(data$type)
> > > > [1] "general"  "regional"
> > > > 
> > > >> contrasts(data$type)
> > > >         regional
> > > > general         0
> > > > regional        1
> > > > 
> > > > 
> > > > I have tried the following syntax for fit.contrast
> > > > 
> > > > fit.contrast(model,type,c(1,0))
> > > > and get an error:
> > > > Error in `[[<-`(`*tmp*`, varname, value = cmat) :
> > > >  no such index at level 1
> > > > 
> > > > 
> > > >> fit.contrast(model,type,c(0,1),showall=TRUE)
> > > > and get an error:
> > > > Error in `[[<-`(`*tmp*`, varname, value = cmat) :
> > > >  no such index at level 1
> > > > 
> > > > 
> > > > 
> > > >> fit.contrast(model,type,c(1,-1),showall=TRUE)
> > > > and get an error:
> > > > Error in `[[<-`(`*tmp*`, varname, value = cmat) :
> > > >  no such index at level 1
> > > > 
> > > > 
> > > >> fit.contrast(model,type,c(0))
> > > > and get an error:
> > > > Error in make.contrasts(coeff, ncol(coeff)) :
> > > >  Too many contrasts specified. Must be less than the number of factor levels (columns).
> > > > 
> > > >> fit.contrast(model,type,c(1))
> > > > Error in make.contrasts(coeff, ncol(coeff)) :
> > > > and get an error
> > > >  Too many contrasts specified. Must be less than the number of factor levels (columns).
> > > > 
> > > > 
> > > > 
> > > > 
> > > > 
> > > > 
> > > > 
> > > > 
> > > > John David Sorkin M.D., Ph.D.
> > > > Professor of Medicine
> > > > Chief, Biostatistics and Informatics
> > > > University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> > > > Baltimore VA Medical Center
> > > > 10 North Greene Street
> > > > GRECC (BT/18/GR)
> > > > Baltimore, MD 21201-1524
> > > > (Phone) 410-605-7119
> > > > (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> > > > 
> > > > 
> > > >        [[alternative HTML version deleted]]
> > > > 
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > 
> > > David Winsemius
> > > Alameda, CA, USA
> > > 
> > > 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
> > 
> > David Winsemius
> > Alameda, CA, USA
> > 
> > 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
> 
> David Winsemius
> Alameda, CA, USA
> 
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From dwinsemius at comcast.net  Mon Oct 23 04:17:03 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 22 Oct 2017 19:17:03 -0700
Subject: [R] Problem when trying to run Java in R:
In-Reply-To: <wXugqhHDrV6BFbzd1ja-qa0Od__Q1rBn29Q525GZbz2UhFxMYVwgD7d6M6UCJDH8QBVtCY2-1XXfi-C45hEvIidJ-UfFn985ILMw9cAb4nY=@protonmail.com>
References: <wXugqhHDrV6BFbzd1ja-qa0Od__Q1rBn29Q525GZbz2UhFxMYVwgD7d6M6UCJDH8QBVtCY2-1XXfi-C45hEvIidJ-UfFn985ILMw9cAb4nY=@protonmail.com>
Message-ID: <31BAC733-5C06-467C-AA89-CEA54C61A8C8@comcast.net>


> On Oct 21, 2017, at 4:35 AM, Morkus via R-help <r-help at r-project.org> wrote:
> 
> Hello All,
> 
> Although running Java from R used to work, for some mysterious reason, it's stopped.
> 
> Today when I tried to load a basic JDBC driver (or the sample .jinit()) code, I got:
> 
> - JavaVM: requested Java version ((null)) not available. Using Java at "" instead.
> 
> - JavaVM: Failed to load JVM: /bundle/Libraries/libserver.dylib
> 
> - JavaVM FATAL: Failed to load the jvm library.
> 
> I saw postings on StackOverflow about this issue, but none of the suggested fixes helped.
> 
> I'm on Mac OS 10.13.
> 
> My JAVA_HOME is: /Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home
> 
> java -version
> java version "1.8.0_144"

I am not sure why but I do notice that currently:  Java Recommended Version 8 Update 151 

AND this quoted warning is on the current download page .. 

macOS Sierra 10.12 users: A few issues have been reported on Sierra. See FAQ for more information.

Others have reported success with this at the Terminal command line:

sudo R CMD javareconf

export JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0.jdk/Contents/Home"
export LD_LIBRARY_PATH=/Library/Java/JavaVirtualMachines/jdk1.8.0_05.jdk/Contents/Home/jre/lib/server 

#AND then ....  at the R command line:

install.packages('rJava', type='source')

> Java(TM) SE Runtime Environment (build 1.8.0_144-b01)
> Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)
> 
> I'm using R: 3.3.3
> 
> Here is sample code that also throws this same error:
> 
>    library(rJava)
>    .jinit() # this starts the JVM
>    s <- .jnew("java/lang/String", "Hello World!")

I don't get any error running:

 sessionInfo()

R version 3.4.2 Patched (2017-10-04 r73465)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: OS X El Capitan 10.11.6

Matrix products: default
BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] grDevices utils     datasets  graphics  stats     methods   base     

other attached packages:
 [1] rJava_0.9-8     dplyr_0.7.2     gmodels_2.16.2  Matrix_1.2-11   xgboost_0.6-4  
 [6] rms_5.1-1       SparseM_1.77    Hmisc_4.0-3     ggplot2_2.2.1   Formula_1.2-1  
[11] survival_2.41-3 sos_2.0-0       brew_1.0-6      lattice_0.20-35

loaded via a namespace (and not attached):
 [1] Rcpp_0.12.11        mvtnorm_1.0-6       gtools_3.5.0        visNetwork_1.0.3   
 [5] zoo_1.8-0           assertthat_0.2.0    digest_0.6.12       R6_2.2.2           
 [9] plyr_1.8.4          backports_1.1.0     acepack_1.4.1       MatrixModels_0.4-1 
[13] rlang_0.1.2         lazyeval_0.2.0      mxnet_0.10.1        multcomp_1.4-6     
[17] gdata_2.17.0        rstudioapi_0.6      data.table_1.10.4   rpart_4.1-11       
[21] gamlss.data_5.0-0   checkmate_1.8.3     DiagrammeR_0.9.0    splines_3.4.2      
[25] stringr_1.2.0       foreign_0.8-69      htmlwidgets_0.8     igraph_1.0.1       
[29] munsell_0.4.3       compiler_3.4.2      influenceR_0.1.0    rgexf_0.15.3       
[33] pkgconfig_2.0.1     gamlss_5.0-2        base64enc_0.1-3     gamlss.dist_5.0-0  
[37] htmltools_0.3.6     nnet_7.3-12         tibble_1.3.1        gridExtra_2.2.1    
[41] htmlTable_1.9       codetools_0.2-15    XML_3.98-1.7        viridisLite_0.2.0  
[45] MASS_7.3-47         grid_3.4.2          nlme_3.1-131        polspline_1.1.12   
[49] jsonlite_1.5        gtable_0.2.0        magrittr_1.5        scales_0.4.1       
[53] stringi_1.1.5       viridis_0.4.0       bindrcpp_0.2        latticeExtra_0.6-28
[57] sandwich_2.3-4      TH.data_1.0-8       RColorBrewer_1.1-2  tools_3.4.2        
[61] glue_1.1.1          Rook_1.1-1          parallel_3.4.2      colorspace_1.3-2   
[65] cluster_2.0.6       knitr_1.15.1        bindr_0.1           quantreg_5.33     

Running this produces:

Sys.getenv()[grep("jre", Sys.getenv())]
#DYLD_LIBRARY_PATH           /Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server


--
David. 
> 
> (this is the "hello world" equivalent from the rJava site)
> 
> ----
> 
> I also tried to use Java 7 (rev. 51), which used to work, but that still fails.
> 
> Also tried a fresh install of R/RStudio on another machine with the same results (same Java version, etc., however).
> 
> I suspect Java itself is working fine since JDBC code from Java programs has no issues.
> 
> Not sure why loading Java in R stopped working, but would appreciate any suggestions.
> 
> Thanks very much,
> 
> Sent from [ProtonMail](https://protonmail.com), Swiss-based encrypted email.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From hemantsain55 at gmail.com  Mon Oct 23 07:02:57 2017
From: hemantsain55 at gmail.com (Hemant Sain)
Date: Mon, 23 Oct 2017 10:32:57 +0530
Subject: [R] How to define proper breaks in RFM analysis
In-Reply-To: <CA+8X3fXiBJJuwnBnTWuQZZDUQhkJWH_h8o3eXwVLx1yT8FP9eg@mail.gmail.com>
References: <CAJL6Qs8tPRTVFi7h4Pwz9i55zPJx+K+1geF4E=S=SJ1Ke_LUjQ@mail.gmail.com>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB2CC0@SRVEXCHCM301.precheza.cz>
 <CAJL6Qs_eykK=Wa1XPkH37KoZrdM=4DYqtAdn7=P6-0ZiufVMWQ@mail.gmail.com>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB2EAE@SRVEXCHCM301.precheza.cz>
 <51049A06-D7B1-4F5D-93DC-CF40D9DFC726@comcast.net>
 <CA+8X3fXiBJJuwnBnTWuQZZDUQhkJWH_h8o3eXwVLx1yT8FP9eg@mail.gmail.com>
Message-ID: <CAJL6Qs9sv2+VFN-oO9tKfM72j_Ft=5ErO_Lb1mqOLBX576mdBA@mail.gmail.com>

hello,
I'm confused what you guys are talking about.
i just want to set ideal threshold values for my RFM scores which can be
done using Quantiles but i don't want to use quantiles because my data is
not normally distributed so it will lead to wrong ranges of breaks. to fix
this problem I'm looking for an approach which can define the ideal range
to breaks to categorize RFM scores into 3 segments.
that's all i want.
THanks


On 14 October 2017 at 04:24, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hemant's problem is that the indicators are not distributed uniformly.
> With a uniform distribution, categorization gives a reasonably optimal
> separation of cases. One approach would be to drop categorization and
> calculate the overall score as the mean of the standardized indicator
> scores. Whether this is an option I do not know. I did offer an
> "eyeball" set of breaks in a previous email, but apparently this was
> not sufficient.
>
> Jim
>
> On Sat, Oct 14, 2017 at 4:27 AM, David Winsemius <dwinsemius at comcast.net>
> wrote:
> >
> >> On Oct 13, 2017, at 2:51 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:
> >>
> >> Hi
> >>
> >> You expect us to solve your problem but you ignore advice already
> recieved.
> >>
> >> Your data are unreadable, use dput(yourdata) instead. see ?dput
> >>
> >>> test<-read.table("clipboard", heade=T)
> >> Error in scan(file = file, what = what, sep = sep, quote = quote, dec =
> dec,  :
> >>  line 115 did not have 6 elements
> >
> > I didn't have such a problem: (illustrated with a more minimal example)
> >
> > dat <-  scan( what=list("",1,"",1L,1L,1),
> >              text="194849 6.99 8/22/2017 9 5 9.996
> > 194978 14.78 8/28/2017 3 15 16.308
> > 198614 18.44 7/31/2017 31 1 18.44
> > 234569 34.99 8/20/2017 11 8 13.5075
> > 252686 7.99 7/31/2017 31 2 7.99
> > 291719 21.26 8/25/2017 6 2 15.67
> > 291787 46.1 8/31/2017 0 2 32.57
> > 292630 24.34 7/31/2017 31 1 24.34
> > 295204 21.86 7/18/2017 44 1 21.86
> > 295989 8.98 8/20/2017 11 2 14.095
> > 298883 14.38 8/24/2017 7 2 11.185
> > 308824 10.77 7/31/2017 31 1 10.77")
> >
> > names(dat) <- c("user_id", "subtotal_amount", "created_at", "Recency",
> "Frequency", "Monetary")
> > dat <- data.frame(dat,stringsAsFactors=FALSE)
> >
> > I suspect read.table would also have worked for me, but I was expecting
> difficulties based on Petr's posting.
> >
> >
> > #And ended up with this result (on the original copied data):
> >> str(dat)
> > 'data.frame':   500 obs. of  6 variables:
> >  $ user_id        : chr  "194849" "194978" "198614" "234569" ...
> >  $ subtotal_amount: num  6.99 14.78 18.44 34.99 7.99 ...
> >  $ created_at     : chr  "8/22/2017" "8/28/2017" "7/31/2017" "8/20/2017"
> ...
> >  $ Recency        : int  9 3 31 11 31 6 0 31 44 11 ...
> >  $ Frequency      : int  5 15 1 8 2 2 2 1 1 2 ...
> >  $ Monetary       : num  10 16.31 18.44 13.51 7.99 ...
> >
> > ...  but the following criticism seems, well, _critical_ (as in
> essential for one to address if a reasonable proposal is to be offered.)
> >
> >
> >> What is ?ideal interval? can you define it? Should it be such to
> provide eqal number of observations?
> >
> > That is the crucial question for you to answer, Hemant. Read the
> ?quartile help page if your answer is "yes" or even "maybe".
> >>
> >> Or maybe you could normalise your values and use quartile method.
> >
> > Well, maybe not so much on that last one, Petr. Normalization should not
> affect the classification based on quartiles. It doesn't change the
> ordering of variables.
> >
> > --
> > David.
> >
> >>
> >> Cheers
> >> Petr
> >>
> >> From: Hemant Sain [mailto:hemantsain55 at gmail.com]
> >> Sent: Friday, October 13, 2017 8:51 AM
> >> To: PIKAL Petr <petr.pikal at precheza.cz>
> >> Cc: r-help mailing list <r-help at r-project.org>
> >> Subject: Re: [R] How to define proper breaks in RFM analysis
> >>
> >> Hey,
> >> i want to define 3 ideal breaks (bin) for each variable one of those
> variables is attached in the previous email,
> >> i don't want to consider quartile method because quartile is not
> working ideally for that data set because data distribution is non normal.
> >> so i want you to suggest another method so that i can define 3 breaks
> with the ideal interval for Recency, frequency and monetary to calculate
> RFM score.
> >> i'm again attaching you some of the data set.
> >> please look into it and help me with the R code.
> >> Thanks
> >>
> >>
> >>
> >> Data
> >>
> >> user_id
> >>
> >> subtotal_amount
> >>
> >> created_at
> >>
> >> Recency
> >>
> >> Frequency
> >>
> >> Monetary
> >>
> >> 194849
> >>
> >> 6.99
> >>
> >> 8/22/2017
> >>
> > snipped
> >
> >>
> >>
> >> On 13 October 2017 at 10:35, PIKAL Petr <petr.pikal at precheza.cz<mailto:
> petr.pikal at precheza.cz>> wrote:
> >> Hi
> >>
> >> Your statement about attaching data is problematic. We cannot do much
> with it. Instead use output from dput(yourdata) to show us what exactly
> your data look like.
> >>
> >> We also do not know how do you want to split your data. It would be
> nice if you can show also what should be the bins with respective data.
> Unless you provide this information you probably would not get any sensible
> answer.
> >>
> >> Cheers
> >> Petr
> >>
> >>
> >>> -----Original Message-----
> >>> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-
> bounces at r-project.org>] On Behalf Of Hemant Sain
> >>> Sent: Thursday, October 12, 2017 10:18 AM
> >>> To: r-help mailing list <r-help at r-project.org<mailto:r
> -help at r-project.org>>
> >>> Subject: [R] How to define proper breaks in RFM analysis
> >>>
> >>> Hello,
> >>> I'm working on RFM analysis and i wanted to define my own breaks but my
> >>> frequency distribution is not normally distributed so when I'm using
> quartile its
> >>> not giving the optimal results.
> >>> so I'm looking for a better approach where i can define breaks
> dynamically
> >>> because after visualization i can do it easily but i want to apply
> this model so
> >>> that it can automatically define the breaks according to data set.
> >>> I'm attaching sample data for reference.
> >>>
> >>> Thanks
> >>>
> >>>                           *Freq*
> >>> 5
> >>> 15
> >>> 1
> > snipped
> >> .
> >>
> >>       [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> > 'Any technology distinguishable from magic is insufficiently advanced.'
>  -Gehm's Corollary to Clarke's Third Law
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>



-- 
hemantsain.com

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Mon Oct 23 09:40:34 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 23 Oct 2017 00:40:34 -0700
Subject: [R] How to define proper breaks in RFM analysis
In-Reply-To: <CAJL6Qs9sv2+VFN-oO9tKfM72j_Ft=5ErO_Lb1mqOLBX576mdBA@mail.gmail.com>
References: <CAJL6Qs8tPRTVFi7h4Pwz9i55zPJx+K+1geF4E=S=SJ1Ke_LUjQ@mail.gmail.com>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB2CC0@SRVEXCHCM301.precheza.cz>
 <CAJL6Qs_eykK=Wa1XPkH37KoZrdM=4DYqtAdn7=P6-0ZiufVMWQ@mail.gmail.com>
 <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB2EAE@SRVEXCHCM301.precheza.cz>
 <51049A06-D7B1-4F5D-93DC-CF40D9DFC726@comcast.net>
 <CA+8X3fXiBJJuwnBnTWuQZZDUQhkJWH_h8o3eXwVLx1yT8FP9eg@mail.gmail.com>
 <CAJL6Qs9sv2+VFN-oO9tKfM72j_Ft=5ErO_Lb1mqOLBX576mdBA@mail.gmail.com>
Message-ID: <BB7BE35E-9C57-4D12-8924-19DC6CB51778@dcn.davis.ca.us>

Using quantiles does not imply assumption of normality, unless you drag that assumption in separately. Please go review statistics again, offlist, and come back when you need help with R.
-- 
Sent from my phone. Please excuse my brevity.

On October 22, 2017 10:02:57 PM PDT, Hemant Sain <hemantsain55 at gmail.com> wrote:
>hello,
>I'm confused what you guys are talking about.
>i just want to set ideal threshold values for my RFM scores which can
>be
>done using Quantiles but i don't want to use quantiles because my data
>is
>not normally distributed so it will lead to wrong ranges of breaks. to
>fix
>this problem I'm looking for an approach which can define the ideal
>range
>to breaks to categorize RFM scores into 3 segments.
>that's all i want.
>THanks
>
>
>On 14 October 2017 at 04:24, Jim Lemon <drjimlemon at gmail.com> wrote:
>
>> Hemant's problem is that the indicators are not distributed
>uniformly.
>> With a uniform distribution, categorization gives a reasonably
>optimal
>> separation of cases. One approach would be to drop categorization and
>> calculate the overall score as the mean of the standardized indicator
>> scores. Whether this is an option I do not know. I did offer an
>> "eyeball" set of breaks in a previous email, but apparently this was
>> not sufficient.
>>
>> Jim
>>
>> On Sat, Oct 14, 2017 at 4:27 AM, David Winsemius
><dwinsemius at comcast.net>
>> wrote:
>> >
>> >> On Oct 13, 2017, at 2:51 AM, PIKAL Petr <petr.pikal at precheza.cz>
>wrote:
>> >>
>> >> Hi
>> >>
>> >> You expect us to solve your problem but you ignore advice already
>> recieved.
>> >>
>> >> Your data are unreadable, use dput(yourdata) instead. see ?dput
>> >>
>> >>> test<-read.table("clipboard", heade=T)
>> >> Error in scan(file = file, what = what, sep = sep, quote = quote,
>dec =
>> dec,  :
>> >>  line 115 did not have 6 elements
>> >
>> > I didn't have such a problem: (illustrated with a more minimal
>example)
>> >
>> > dat <-  scan( what=list("",1,"",1L,1L,1),
>> >              text="194849 6.99 8/22/2017 9 5 9.996
>> > 194978 14.78 8/28/2017 3 15 16.308
>> > 198614 18.44 7/31/2017 31 1 18.44
>> > 234569 34.99 8/20/2017 11 8 13.5075
>> > 252686 7.99 7/31/2017 31 2 7.99
>> > 291719 21.26 8/25/2017 6 2 15.67
>> > 291787 46.1 8/31/2017 0 2 32.57
>> > 292630 24.34 7/31/2017 31 1 24.34
>> > 295204 21.86 7/18/2017 44 1 21.86
>> > 295989 8.98 8/20/2017 11 2 14.095
>> > 298883 14.38 8/24/2017 7 2 11.185
>> > 308824 10.77 7/31/2017 31 1 10.77")
>> >
>> > names(dat) <- c("user_id", "subtotal_amount", "created_at",
>"Recency",
>> "Frequency", "Monetary")
>> > dat <- data.frame(dat,stringsAsFactors=FALSE)
>> >
>> > I suspect read.table would also have worked for me, but I was
>expecting
>> difficulties based on Petr's posting.
>> >
>> >
>> > #And ended up with this result (on the original copied data):
>> >> str(dat)
>> > 'data.frame':   500 obs. of  6 variables:
>> >  $ user_id        : chr  "194849" "194978" "198614" "234569" ...
>> >  $ subtotal_amount: num  6.99 14.78 18.44 34.99 7.99 ...
>> >  $ created_at     : chr  "8/22/2017" "8/28/2017" "7/31/2017"
>"8/20/2017"
>> ...
>> >  $ Recency        : int  9 3 31 11 31 6 0 31 44 11 ...
>> >  $ Frequency      : int  5 15 1 8 2 2 2 1 1 2 ...
>> >  $ Monetary       : num  10 16.31 18.44 13.51 7.99 ...
>> >
>> > ...  but the following criticism seems, well, _critical_ (as in
>> essential for one to address if a reasonable proposal is to be
>offered.)
>> >
>> >
>> >> What is ?ideal interval? can you define it? Should it be such to
>> provide eqal number of observations?
>> >
>> > That is the crucial question for you to answer, Hemant. Read the
>> ?quartile help page if your answer is "yes" or even "maybe".
>> >>
>> >> Or maybe you could normalise your values and use quartile method.
>> >
>> > Well, maybe not so much on that last one, Petr. Normalization
>should not
>> affect the classification based on quartiles. It doesn't change the
>> ordering of variables.
>> >
>> > --
>> > David.
>> >
>> >>
>> >> Cheers
>> >> Petr
>> >>
>> >> From: Hemant Sain [mailto:hemantsain55 at gmail.com]
>> >> Sent: Friday, October 13, 2017 8:51 AM
>> >> To: PIKAL Petr <petr.pikal at precheza.cz>
>> >> Cc: r-help mailing list <r-help at r-project.org>
>> >> Subject: Re: [R] How to define proper breaks in RFM analysis
>> >>
>> >> Hey,
>> >> i want to define 3 ideal breaks (bin) for each variable one of
>those
>> variables is attached in the previous email,
>> >> i don't want to consider quartile method because quartile is not
>> working ideally for that data set because data distribution is non
>normal.
>> >> so i want you to suggest another method so that i can define 3
>breaks
>> with the ideal interval for Recency, frequency and monetary to
>calculate
>> RFM score.
>> >> i'm again attaching you some of the data set.
>> >> please look into it and help me with the R code.
>> >> Thanks
>> >>
>> >>
>> >>
>> >> Data
>> >>
>> >> user_id
>> >>
>> >> subtotal_amount
>> >>
>> >> created_at
>> >>
>> >> Recency
>> >>
>> >> Frequency
>> >>
>> >> Monetary
>> >>
>> >> 194849
>> >>
>> >> 6.99
>> >>
>> >> 8/22/2017
>> >>
>> > snipped
>> >
>> >>
>> >>
>> >> On 13 October 2017 at 10:35, PIKAL Petr
><petr.pikal at precheza.cz<mailto:
>> petr.pikal at precheza.cz>> wrote:
>> >> Hi
>> >>
>> >> Your statement about attaching data is problematic. We cannot do
>much
>> with it. Instead use output from dput(yourdata) to show us what
>exactly
>> your data look like.
>> >>
>> >> We also do not know how do you want to split your data. It would
>be
>> nice if you can show also what should be the bins with respective
>data.
>> Unless you provide this information you probably would not get any
>sensible
>> answer.
>> >>
>> >> Cheers
>> >> Petr
>> >>
>> >>
>> >>> -----Original Message-----
>> >>> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-
>> bounces at r-project.org>] On Behalf Of Hemant Sain
>> >>> Sent: Thursday, October 12, 2017 10:18 AM
>> >>> To: r-help mailing list <r-help at r-project.org<mailto:r
>> -help at r-project.org>>
>> >>> Subject: [R] How to define proper breaks in RFM analysis
>> >>>
>> >>> Hello,
>> >>> I'm working on RFM analysis and i wanted to define my own breaks
>but my
>> >>> frequency distribution is not normally distributed so when I'm
>using
>> quartile its
>> >>> not giving the optimal results.
>> >>> so I'm looking for a better approach where i can define breaks
>> dynamically
>> >>> because after visualization i can do it easily but i want to
>apply
>> this model so
>> >>> that it can automatically define the breaks according to data
>set.
>> >>> I'm attaching sample data for reference.
>> >>>
>> >>> Thanks
>> >>>
>> >>>                           *Freq*
>> >>> 5
>> >>> 15
>> >>> 1
>> > snipped
>> >> .
>> >>
>> >>       [[alternative HTML version deleted]]
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >
>> > David Winsemius
>> > Alameda, CA, USA
>> >
>> > 'Any technology distinguishable from magic is insufficiently
>advanced.'
>>  -Gehm's Corollary to Clarke's Third Law
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>
>
>
>-- 
>hemantsain.com
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From maechler at stat.math.ethz.ch  Mon Oct 23 10:10:17 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 23 Oct 2017 10:10:17 +0200
Subject: [R] Syntax for fit.contrast (from package gmodels)
In-Reply-To: <BY2PR0301MB0616B317C0123DA2E6E61DA0E2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
References: <BY2PR0301MB061685A793413A3EC741F5EFE2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
 <EDE44E15-E20D-466E-B821-0B4F36E602F7@comcast.net>
 <BY2PR0301MB0616B317C0123DA2E6E61DA0E2410@BY2PR0301MB0616.namprd03.prod.outlook.com>
Message-ID: <23021.41961.530675.108187@stat.math.ethz.ch>

>>>>> Sorkin, John <jsorkin at som.umaryland.edu>
>>>>>     on Sun, 22 Oct 2017 22:56:16 +0000 writes:

    > David,
    > Thank you for responding to my post.


    > Please consider the following output (typeregional is a factor having two levels, "regional" vs. "general"):

    > Call:
    > glm(formula = events ~ type, family = poisson(link = log), data = data,
    > offset = log(SS))

    > Deviance Residuals:
    > Min       1Q   Median       3Q      Max
    > -43.606  -17.295   -4.651    4.204   38.421

    > Coefficients:
    > Estimate Std. Error z value Pr(>|z|)
    > (Intercept)  -2.52830    0.01085 -233.13   <2e-16 ***
    > typeregional  0.33788    0.01641   20.59   <2e-16 ***


    > Let's forget for a moment that the model is a Poisson regression and pretend that the output is from a simple linear regression, e.g. from lm.


    > To get the estimate for "general" one simply needs to use the value of the intercept i.e. -2.5830. Similarly to get the 95% CI of general one simply needs to compute -2.52830-(1.96*0.01085) and -2.52830+(1.96*0.01085).

I'm pretty sure you can just use  (the base R) functions

  dummy.coef()

or
  model.tables()

possibly with SE=TRUE to get coefficients for all levels of a factor..
I'd like to have tried to show this here, but for that we'd have
wanted to see a "MRE" or "ReprEx" (minimal reproducible example) ..

    > To get the estimate for "regional" one needs to compute intercept + typeregional, i.e. -2.52830 + 0.33788. To get the 95% CI is somewhat more difficult as one needs to use results from the variance-covariance matix, specifically the variance of intercept, the variance of "regional", and the covariance of (intercept,"regional") which involves:

    > var =  var(intercept) + var(regional) +2*(covar(intercept,regional)),

    > and then get the SE of the variance

    > SE=sqrt(var)

    > 95% CI = intercept + regional - 1.95*SE and intercept + regional + 1.95*SE.


    > I was hoping that a contrast statement could be written that would give me the point estimate and SE for "general" and its SE and another contrast statement could be written that would give me the point estimate and SE for "general" and it SE without my having to work directly with the variance-covariance matrix. I tried doing this using the fit.contrast statements (from the gmodels package):


    > fit.contrast(model,type,c(1,0),showall=TRUE)

    > fit.contrast(model,type,c(0,1),showall=TRUE)


    > and received the error message,

    > Error in `[[<-`(`*tmp*`, varname, value = c(0, 1)) :
    > no such index at level 1


    > Perhaps fit.contrast is not the way to accomplish my goal. Perhaps my goal can be accomplished without a contrast statement, but I don't know how.

My guess is that "standard R" aka "base R" would be
sufficient to get what you'd want, notably if you'd consider
using  se.contrast() additionally.

Martin


    > Thank you,
    > John


From rene.j.suarez at gmail.com  Mon Oct 23 20:36:26 2017
From: rene.j.suarez at gmail.com (Rene J Suarez-Soto)
Date: Mon, 23 Oct 2017 14:36:26 -0400
Subject: [R] R base packages
Message-ID: <CAKeefvuKi-HzOrdkTnoZMRw5ieQkOk2S5+cbNCzAomsyFFCb8A@mail.gmail.com>

I installed R 3.4.2 (Windows) and noticed that 30 folders are under my
R_HOME/library folder. I assume all of these are R base packages. Is this
correct? Where can I see a list of current R base packages? Also; are R
base packages also in CRAN? and are these packages updated only when a new
version of R is released or can the be updated at a different time? Thanks

	[[alternative HTML version deleted]]


From kendejan at yahoo.fr  Mon Oct 23 19:54:57 2017
From: kendejan at yahoo.fr (kende jan)
Date: Mon, 23 Oct 2017 17:54:57 +0000 (UTC)
Subject: [R] Linear regression with tranformed dependant variable
References: <221504244.3614450.1508781297607.ref@mail.yahoo.com>
Message-ID: <221504244.3614450.1508781297607@mail.yahoo.com>

Dear all,?I am trying to fit a multiple linear regression model with a transformed dependant variable (the normality assumption was not verified...).?I have realised a sqrt(variable) transformation...?The results are great, but I don't know how to interprete the beta coefficients... Is it possible to do another transformation to get interpretable beta coefficients to express the variations in the original untransformed dependant variable ??Thank you very much for your help!No?mie?
	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Mon Oct 23 21:11:16 2017
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Mon, 23 Oct 2017 20:11:16 +0100
Subject: [R] Linear regression with tranformed dependant variable
In-Reply-To: <221504244.3614450.1508781297607@mail.yahoo.com>
References: <221504244.3614450.1508781297607.ref@mail.yahoo.com>
 <221504244.3614450.1508781297607@mail.yahoo.com>
Message-ID: <59EE3ED4.9090501@sapo.pt>

Hello,

R-Help answers questions on R code, your question is about statistics. 
You should try posting the question to

https://stats.stackexchange.com/

Hope this helps,

Rui Barradas

Em 23-10-2017 18:54, kende jan via R-help escreveu:
> Dear all, I am trying to fit a multiple linear regression model with a transformed dependant variable (the normality assumption was not verified...). I have realised a sqrt(variable) transformation... The results are great, but I don't know how to interprete the beta coefficients... Is it possible to do another transformation to get interpretable beta coefficients to express the variations in the original untransformed dependant variable ? Thank you very much for your help!No?mie
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bgunter.4567 at gmail.com  Mon Oct 23 21:50:10 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 23 Oct 2017 12:50:10 -0700
Subject: [R] R base packages
In-Reply-To: <CAKeefvuKi-HzOrdkTnoZMRw5ieQkOk2S5+cbNCzAomsyFFCb8A@mail.gmail.com>
References: <CAKeefvuKi-HzOrdkTnoZMRw5ieQkOk2S5+cbNCzAomsyFFCb8A@mail.gmail.com>
Message-ID: <CAGxFJbSJwvvG=hDCV6yKEvSbBeNyzCNuD-64QEjDi+nHC7HhWQ@mail.gmail.com>

?library

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Mon, Oct 23, 2017 at 11:36 AM, Rene J Suarez-Soto <
rene.j.suarez at gmail.com> wrote:

> I installed R 3.4.2 (Windows) and noticed that 30 folders are under my
> R_HOME/library folder. I assume all of these are R base packages. Is this
> correct? Where can I see a list of current R base packages? Also; are R
> base packages also in CRAN? and are these packages updated only when a new
> version of R is released or can the be updated at a different time? Thanks
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ebs15242 at gmail.com  Mon Oct 23 23:09:47 2017
From: ebs15242 at gmail.com (Ed Siefker)
Date: Mon, 23 Oct 2017 16:09:47 -0500
Subject: [R] A list of data frames and a list of colnames.
Message-ID: <CALRb-ocy-WLbErbOTa4yDhgDxRZpf2Xkj9FRAe67Lk6r5ba9Tg@mail.gmail.com>

I have a list of file names, and a list of data frames contained in those files.

mynames <- list.files()
mydata <- lapply(mynames, read.delim)

Every file contains two columns.

> colnames(mydata[[1]])
[1] "Name"     "NumReads"
> colnames(mydata[[2]])
[1] "Name"     "NumReads"

I can set the colnames easily enough with a for loop.

for (i in seq_along(mynames)) {
    colnames(mydata[[i]])[2] <- mynames[i]
}

Is there a nicer way to do this?


From richcala at microsoft.com  Mon Oct 23 23:19:51 2017
From: richcala at microsoft.com (Rich Calaway)
Date: Mon, 23 Oct 2017 21:19:51 +0000
Subject: [R] R base packages
In-Reply-To: <CAGxFJbSJwvvG=hDCV6yKEvSbBeNyzCNuD-64QEjDi+nHC7HhWQ@mail.gmail.com>
References: <CAKeefvuKi-HzOrdkTnoZMRw5ieQkOk2S5+cbNCzAomsyFFCb8A@mail.gmail.com>
 <CAGxFJbSJwvvG=hDCV6yKEvSbBeNyzCNuD-64QEjDi+nHC7HhWQ@mail.gmail.com>
Message-ID: <CY4PR21MB07607E645AFCECDEE4DFD896DE460@CY4PR21MB0760.namprd21.prod.outlook.com>

This will give you a list of the current base packages:

priority <- installed.packages()[,"Priority"]
names(priority)[priority %in% "base"]

Change "base" to "recommended" to see the current recommended packages.

The base packages are part of R, they are not available separately. If you try to install one, you'll see a warning such as the following:

2: package 'compiler' is a base package, and should not be updated

Cheers,

Rich Calaway
Microsoft R Team

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bert Gunter
Sent: Monday, October 23, 2017 12:50 PM
To: Rene J Suarez-Soto <rene.j.suarez at gmail.com>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] R base packages

?library

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Mon, Oct 23, 2017 at 11:36 AM, Rene J Suarez-Soto < rene.j.suarez at gmail.com> wrote:

> I installed R 3.4.2 (Windows) and noticed that 30 folders are under my 
> R_HOME/library folder. I assume all of these are R base packages. Is 
> this correct? Where can I see a list of current R base packages? Also; 
> are R base packages also in CRAN? and are these packages updated only 
> when a new version of R is released or can the be updated at a 
> different time? Thanks
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.
> ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=02%7C01%7Crichcala%40micros
> oft.com%7Ca2db429d766c4519841008d51a4f4ed1%7C72f988bf86f141af91ab2d7cd
> 011db47%7C1%7C0%7C636443850296347312&sdata=3BPg6KXz7hpnY1P69pQ6O6h2%2B
> 3A1W5dfT4Rwpi9UCgE%3D&reserved=0 PLEASE do read the posting guide 
> https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.R-
> project.org%2F&data=02%7C01%7Crichcala%40microsoft.com%7Ca2db429d766c4
> 519841008d51a4f4ed1%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C63644
> 3850296347312&sdata=znCtjImoj3%2FytzeIyJ6x1qVJ1nCYzPptoyz2n7M%2FhMg%3D
> &reserved=0
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=02%7C01%7Crichcala%40microsoft.com%7Ca2db429d766c4519841008d51a4f4ed1%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636443850296347312&sdata=3BPg6KXz7hpnY1P69pQ6O6h2%2B3A1W5dfT4Rwpi9UCgE%3D&reserved=0
PLEASE do read the posting guide https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.R-project.org%2Fposting-guide.html&data=02%7C01%7Crichcala%40microsoft.com%7Ca2db429d766c4519841008d51a4f4ed1%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636443850296347312&sdata=Jch5hvNbcJDIwJ4OFI9snayCkSuptZr7CfLz1DuLIy0%3D&reserved=0
and provide commented, minimal, self-contained, reproducible code.


From ruipbarradas at sapo.pt  Mon Oct 23 23:22:09 2017
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Mon, 23 Oct 2017 22:22:09 +0100
Subject: [R] A list of data frames and a list of colnames.
In-Reply-To: <CALRb-ocy-WLbErbOTa4yDhgDxRZpf2Xkj9FRAe67Lk6r5ba9Tg@mail.gmail.com>
References: <CALRb-ocy-WLbErbOTa4yDhgDxRZpf2Xkj9FRAe67Lk6r5ba9Tg@mail.gmail.com>
Message-ID: <59EE5D81.3060703@sapo.pt>

Hello,

I think that your code is simple enough to be considered "nice". If you 
are worried about the for loop, don't, were loops worrying they wouldn't 
exist.

Hope this helps,

Rui Barradas

Em 23-10-2017 22:09, Ed Siefker escreveu:
> I have a list of file names, and a list of data frames contained in those files.
>
> mynames <- list.files()
> mydata <- lapply(mynames, read.delim)
>
> Every file contains two columns.
>
>> colnames(mydata[[1]])
> [1] "Name"     "NumReads"
>> colnames(mydata[[2]])
> [1] "Name"     "NumReads"
>
> I can set the colnames easily enough with a for loop.
>
> for (i in seq_along(mynames)) {
>      colnames(mydata[[i]])[2] <- mynames[i]
> }
>
> Is there a nicer way to do this?
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From frainj at gmail.com  Mon Oct 23 23:42:26 2017
From: frainj at gmail.com (John C Frain)
Date: Mon, 23 Oct 2017 22:42:26 +0100
Subject: [R] Linear regression with tranformed dependant variable
In-Reply-To: <59EE3ED4.9090501@sapo.pt>
References: <221504244.3614450.1508781297607.ref@mail.yahoo.com>
 <221504244.3614450.1508781297607@mail.yahoo.com> <59EE3ED4.9090501@sapo.pt>
Message-ID: <CAHrK517aP+SnqU0TC8P4TEMQ56-b7NdfTY-fgBuEcHLSivd1wg@mail.gmail.com>

Before going to stackexchange you should consider if a square root
transformation is appropriate for the model that you are trying to
estimate. If you do so, you may be able to interpret the coefficients
yourself. If no explanation is obvious you probably should not be using a
square root transformation.

Also you might  google "square root transformation regression" and you will
find several useful links.

John C Frain
3 Aranleigh Park
Rathfarnham
Dublin 14
Ireland
www.tcd.ie/Economics/staff/frainj/home.html
mailto:frainj at tcd.ie
mailto:frainj at gmail.com

On 23 October 2017 at 20:11, Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> R-Help answers questions on R code, your question is about statistics. You
> should try posting the question to
>
> https://stats.stackexchange.com/
>
> Hope this helps,
>
> Rui Barradas
>
> Em 23-10-2017 18:54, kende jan via R-help escreveu:
>
>> Dear all, I am trying to fit a multiple linear regression model with a
>> transformed dependant variable (the normality assumption was not
>> verified...). I have realised a sqrt(variable) transformation... The
>> results are great, but I don't know how to interprete the beta
>> coefficients... Is it possible to do another transformation to get
>> interpretable beta coefficients to express the variations in the original
>> untransformed dependant variable ? Thank you very much for your help!No?mie
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From btyner at gmail.com  Mon Oct 23 23:53:21 2017
From: btyner at gmail.com (Benjamin Tyner)
Date: Mon, 23 Oct 2017 17:53:21 -0400
Subject: [R] read.table(..., header == FALSE,
 colClasses = <vector with names attribute>)
Message-ID: <3a4099b3-4627-b7fe-612f-d9abeca4f65d@gmail.com>

Hello

I noticed that starting with R version 3.3.0 onward, this generates a 
warning:

 ?? > txt <- c("a", "3.14")
 ?? > read.table(file = textConnection(txt), header = FALSE, colClasses 
= c(x = "character", y = "numeric"))

the warning is "not all columns named in 'colClasses' exist" and I guess 
the change was made in response to this?

 ?? https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=16478

Regardless, I am wondering whether this is desirable, that as a result 
of the change, the code has become stricter about the presence of a 
(formerly) harmless names attribute. Or am I missing something?

Regards

Ben


From jonnvan at gmail.com  Tue Oct 24 00:12:57 2017
From: jonnvan at gmail.com (jonnvan at gmail.com)
Date: Mon, 23 Oct 2017 17:12:57 -0500
Subject: [R] How to save and restore a workspace
Message-ID: <C12851BD-8049-409A-B025-DA6222E955B5@gmail.com>

Hello,
I recently downloaded R in hopes of learning to use it for statistics. 
I have promptly run into a problem, as I am unable to save, and later recover, a workspace so I can resume work where I left off. 
I am using Windows.
I indicate "yes" to the pop up after q().  Then when I later reopen R Console and click on File, I cannot get my prior workspace to appear in the R Console frame so I can resume work. 
In the File drop down menu I have tried Load Workspace, Load History, Display file(s)..., opened R Type: R Workspace with no luck.
I have read about this in two different books, the R Manual, and R FAQs, used the RGui help function, and still cannot do it. 
I have used Windows for years, but I am ignorant about programming.
Would appreciate any help you might offer.
I live in the Denver area, so if there are any local resources you could direct me to, I would be grateful for that as well.

Thank you,
Jon VanDeventer

Sent from my iPad

From drjimlemon at gmail.com  Tue Oct 24 00:45:29 2017
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 24 Oct 2017 09:45:29 +1100
Subject: [R] How to save and restore a workspace
In-Reply-To: <C12851BD-8049-409A-B025-DA6222E955B5@gmail.com>
References: <C12851BD-8049-409A-B025-DA6222E955B5@gmail.com>
Message-ID: <CA+8X3fUdDHpW3-Kip94X8Zz85zMSnQ54b-H7MP5hQWpbUg18fw@mail.gmail.com>

Hi Jon,
Saving your workspace doesn't mean that everything will be rerun when
you start a new R session. I just means that persistent objects like
data frames will be there. If you type:

objects()

you will see all of those things that were there when you ended in the
last session. Things like commands will be in the "history", so you
can retrieve them just as you did at the end of the last session (up
arrow). It may be a better solution if you save sequences of commands
as R script files (e.g. "something.R") as you can run them:

source("something.R")

and edit and save them again.

Jim

On Tue, Oct 24, 2017 at 9:12 AM,  <jonnvan at gmail.com> wrote:
> Hello,
> I recently downloaded R in hopes of learning to use it for statistics.
> I have promptly run into a problem, as I am unable to save, and later recover, a workspace so I can resume work where I left off.
> I am using Windows.
> I indicate "yes" to the pop up after q().  Then when I later reopen R Console and click on File, I cannot get my prior workspace to appear in the R Console frame so I can resume work.
> In the File drop down menu I have tried Load Workspace, Load History, Display file(s)..., opened R Type: R Workspace with no luck.
> I have read about this in two different books, the R Manual, and R FAQs, used the RGui help function, and still cannot do it.
> I have used Windows for years, but I am ignorant about programming.
> Would appreciate any help you might offer.
> I live in the Denver area, so if there are any local resources you could direct me to, I would be grateful for that as well.
>
> Thank you,
> Jon VanDeventer
>
> Sent from my iPad
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Tue Oct 24 01:04:10 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 23 Oct 2017 16:04:10 -0700
Subject: [R] How to save and restore a workspace
In-Reply-To: <CA+8X3fUdDHpW3-Kip94X8Zz85zMSnQ54b-H7MP5hQWpbUg18fw@mail.gmail.com>
References: <C12851BD-8049-409A-B025-DA6222E955B5@gmail.com>
 <CA+8X3fUdDHpW3-Kip94X8Zz85zMSnQ54b-H7MP5hQWpbUg18fw@mail.gmail.com>
Message-ID: <CAGxFJbS26BvQ-ZnDYcNqg93YNJuVNz+Enh=GWcPuN5DPSiMeEA@mail.gmail.com>

Suggestion:

Read the "Intro to R tutorial" that ships with R. Alternatively, there are
also many good online tutorials, some free, some not. You'll need to spend
some time and effort to learn R's basic data structures and paradigms even
if you don't get involved in any serious programming.

If this is not something you are willing or able to do, you might check
here for a point and click GUI interface that give you access to some of
R's basic functionality: http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/  . You
will have to determine whether this serves your needs and priorities of
course.


Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Mon, Oct 23, 2017 at 3:45 PM, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Jon,
> Saving your workspace doesn't mean that everything will be rerun when
> you start a new R session. I just means that persistent objects like
> data frames will be there. If you type:
>
> objects()
>
> you will see all of those things that were there when you ended in the
> last session. Things like commands will be in the "history", so you
> can retrieve them just as you did at the end of the last session (up
> arrow). It may be a better solution if you save sequences of commands
> as R script files (e.g. "something.R") as you can run them:
>
> source("something.R")
>
> and edit and save them again.
>
> Jim
>
> On Tue, Oct 24, 2017 at 9:12 AM,  <jonnvan at gmail.com> wrote:
> > Hello,
> > I recently downloaded R in hopes of learning to use it for statistics.
> > I have promptly run into a problem, as I am unable to save, and later
> recover, a workspace so I can resume work where I left off.
> > I am using Windows.
> > I indicate "yes" to the pop up after q().  Then when I later reopen R
> Console and click on File, I cannot get my prior workspace to appear in the
> R Console frame so I can resume work.
> > In the File drop down menu I have tried Load Workspace, Load History,
> Display file(s)..., opened R Type: R Workspace with no luck.
> > I have read about this in two different books, the R Manual, and R FAQs,
> used the RGui help function, and still cannot do it.
> > I have used Windows for years, but I am ignorant about programming.
> > Would appreciate any help you might offer.
> > I live in the Denver area, so if there are any local resources you could
> direct me to, I would be grateful for that as well.
> >
> > Thank you,
> > Jon VanDeventer
> >
> > Sent from my iPad
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Oct 24 02:27:52 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 23 Oct 2017 17:27:52 -0700
Subject: [R] A list of data frames and a list of colnames.
In-Reply-To: <CALRb-ocy-WLbErbOTa4yDhgDxRZpf2Xkj9FRAe67Lk6r5ba9Tg@mail.gmail.com>
References: <CALRb-ocy-WLbErbOTa4yDhgDxRZpf2Xkj9FRAe67Lk6r5ba9Tg@mail.gmail.com>
Message-ID: <CAGxFJbSJ3tFxNFVEh2dzi2dN-WML-iSQrB7tG0kcUgdXoax8BA@mail.gmail.com>

This doesn't make sense to me:



On Mon, Oct 23, 2017 at 2:09 PM, Ed Siefker <ebs15242 at gmail.com> wrote:

> I have a list of file names, and a list of data frames contained in those
> files.
>
> mynames <- list.files()
>
## a character vector of file names

mydata <- lapply(mynames, read.delim)
>
# A list of data frames

>
> Every file contains two columns.
>
> > colnames(mydata[[1]])
> [1] "Name"     "NumReads"
>
# Note that names() can be used instead of colnames()

> > colnames(mydata[[2]])
> [1] "Name"     "NumReads"
>
# Ditto

>
> I can set the colnames easily enough with a for loop.
>
> for (i in seq_along(mynames)) {
>     colnames(mydata[[i]])[2] <- mynames[i] ## again, names() can be used
> instead of colnames
> }
>
> You are naming the  the 2nd column of the ith data frame with the file
name of the file from which the data frame was read. Is this really what
you want to do? Or have I misunderstood or erred?

Cheers,
Bert


Is there a nicer way to do this?
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Tue Oct 24 02:46:45 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 23 Oct 2017 17:46:45 -0700
Subject: [R] How to save and restore a workspace
In-Reply-To: <CA+8X3fUdDHpW3-Kip94X8Zz85zMSnQ54b-H7MP5hQWpbUg18fw@mail.gmail.com>
References: <C12851BD-8049-409A-B025-DA6222E955B5@gmail.com>
 <CA+8X3fUdDHpW3-Kip94X8Zz85zMSnQ54b-H7MP5hQWpbUg18fw@mail.gmail.com>
Message-ID: <560188EC-FE31-482D-A28C-3D2C523C4574@dcn.davis.ca.us>

To be specific, the effect of loading packages with the "library" function is NOT saved when you save the workspace... and if you come back much later after you have updated some packages the old saved objects may not work well with those new packages. At the very least you have to re-load all necessary packages yourself before you load a saved workspace. Many users avoid ever using the "save workspace" option because it can cause old mistakes to come back and haunt you later. 

This kind of issue has lead me to become quite systematic about writing R files (as Jim suggested below) that re-do all of my work on a given task (omitting the false starts needed to learn things) on each of my projects starting from a blank workspace. It may seem like a tedious approach to the beginner, but it is actually very comforting to know that I can look through every step and verify that it did what I wanted it to, without wondering how some particular intermediate result was computed six months ago and saved for later use. If one step is particularly slow, I may comment out (our put an "if" around) the statements that originally created and saved that object with saveRDS, and load it instead of recomputing it once I trust it. But the key is to always focus on building sequences of R statements, and later on building functions rather than poking and prodding a workspace like it was a spreadsheet. 
-- 
Sent from my phone. Please excuse my brevity.

On October 23, 2017 3:45:29 PM PDT, Jim Lemon <drjimlemon at gmail.com> wrote:
>Hi Jon,
>Saving your workspace doesn't mean that everything will be rerun when
>you start a new R session. I just means that persistent objects like
>data frames will be there. If you type:
>
>objects()
>
>you will see all of those things that were there when you ended in the
>last session. Things like commands will be in the "history", so you
>can retrieve them just as you did at the end of the last session (up
>arrow). It may be a better solution if you save sequences of commands
>as R script files (e.g. "something.R") as you can run them:
>
>source("something.R")
>
>and edit and save them again.
>
>Jim
>
>On Tue, Oct 24, 2017 at 9:12 AM,  <jonnvan at gmail.com> wrote:
>> Hello,
>> I recently downloaded R in hopes of learning to use it for
>statistics.
>> I have promptly run into a problem, as I am unable to save, and later
>recover, a workspace so I can resume work where I left off.
>> I am using Windows.
>> I indicate "yes" to the pop up after q().  Then when I later reopen R
>Console and click on File, I cannot get my prior workspace to appear in
>the R Console frame so I can resume work.
>> In the File drop down menu I have tried Load Workspace, Load History,
>Display file(s)..., opened R Type: R Workspace with no luck.
>> I have read about this in two different books, the R Manual, and R
>FAQs, used the RGui help function, and still cannot do it.
>> I have used Windows for years, but I am ignorant about programming.
>> Would appreciate any help you might offer.
>> I live in the Denver area, so if there are any local resources you
>could direct me to, I would be grateful for that as well.
>>
>> Thank you,
>> Jon VanDeventer
>>
>> Sent from my iPad
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Tue Oct 24 02:51:59 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 23 Oct 2017 17:51:59 -0700
Subject: [R] read.table(..., header == FALSE,
	colClasses = <vector with names attribute>)
In-Reply-To: <3a4099b3-4627-b7fe-612f-d9abeca4f65d@gmail.com>
References: <3a4099b3-4627-b7fe-612f-d9abeca4f65d@gmail.com>
Message-ID: <A310F93F-00B3-48E2-9149-E3CE3219C390@dcn.davis.ca.us>

You are constructing the equivalent of a two-line data file, and complaining that it is not treating it like it was one line. If it did used to accept this silently [skeptical] then I for one am glad it produces a warning now. 
-- 
Sent from my phone. Please excuse my brevity.

On October 23, 2017 2:53:21 PM PDT, Benjamin Tyner <btyner at gmail.com> wrote:
>Hello
>
>I noticed that starting with R version 3.3.0 onward, this generates a 
>warning:
>
> ?? > txt <- c("a", "3.14")
>?? > read.table(file = textConnection(txt), header = FALSE, colClasses 
>= c(x = "character", y = "numeric"))
>
>the warning is "not all columns named in 'colClasses' exist" and I
>guess 
>the change was made in response to this?
>
> ?? https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=16478
>
>Regardless, I am wondering whether this is desirable, that as a result 
>of the change, the code has become stricter about the presence of a 
>(formerly) harmless names attribute. Or am I missing something?
>
>Regards
>
>Ben
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From sebastien.bihorel at cognigencorp.com  Tue Oct 24 03:57:52 2017
From: sebastien.bihorel at cognigencorp.com (Sebastien Bihorel)
Date: Mon, 23 Oct 2017 21:57:52 -0400 (EDT)
Subject: [R] Issue of reproducibility with gam and lm.wfit in different
 versions of R
Message-ID: <667522419.1487592.1508810272582.JavaMail.zimbra@cognigencorp.com>

Dear R users, 

I recently stumbled upon problems of reproducibility while running GAM analyses in different R and gam package versions. In the example below, a small dataset is created in which the y and x1 variables are 100% correlated. The intents of this example were primarily for regression testing and, secondarily, to evaluate how the gam algorithm behaves under extreme/limit conditions. 

I ran this little snippet in 5 different environments and got 100% consistent results until I switched to R 3.3.2. 
* Comparing results from environments 1, 2, and 3 shows that changing the version of the gam package did not change the results under R 3.3.0. 
* Comparing results from environments 3 and 4 shows that changing the version of R altered the values of the AIC and the output of the step.gam call (changed to a NULL object) 
* Comparing results from environments 4 and 5 shows that reverting to an older version of the gam package in R 3.3.2 still produced altered AIC values and the NULL output from step.gam call 

Further investigations into these differences seem to show that the lm.wfit call in the gam.fit function (called from within gam and step.gam) may result in different values in R 3.3.0 and 3.3.2. 

So my questions are 2-fold: 
1- Would you have any information about why lm.wfit would produce different outcomes in R 3.3.0 and 3.3.2? The source code does not appear significantly different.
2- Is it expected that the step.gam function returns a NULL object in R 3.3.2 while a valid model has been identified? Looking at the source of step.gam, the line 157 (if(is.null(form.list)) break) seems to be the reason the function breaks out and returns a NULL value. 

I thank you in advance for your time. 

Sebastien Bihorel 






library(gam) 

dat <- data.frame( 
y = c(57,57,98,83,122,69,108,86,80,87,75,76,97,101,121,111,105,84,65,54,61,71,125,60,50,112,102,110,77,45,93,62,120,115,70,113,117,85,46,123,89,95,116,55,110,92,109,100,72,88,105,119,94,45,67,58,60,45,107,73,100,79,47,99,51,53,68,125,90,48,82,85,65,52,70,59,125,49,118,103,91,124,78,81,63,63), 
x1 = c(52,52,93,78,117,64,103,81,75,82,70,71,92,96,116,106,100,79,60,49,56,66,120,55,45,107,97,105,72,40,88,57,115,110,65,108,112,80,41,118,84,90,111,50,105,87,104,95,67,83,100,114,89,40,62,53,55,40,102,68,95,74,42,94,46,48,63,120,85,43,77,80,60,47,65,54,120,44,113,98,86,119,73,76,58,58), 
x2 = c(0.0001,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0001,0) 
) 

summary(dat) 

scope <- list( 
x1=c('1','x1','ns(x1, df=2)'), 
x2=c('1','x2') 
) 

gam.object <- gam(y~1, data=dat) 
step.object <- step.gam(gam.object, scope=scope, trace=2) 
is.null(step.object) 

# Run this if you want to evaluate one lm.wfit example call made from within gam functions
x <- cbind(rep(1,nrow(dat)), dat$x1)
names(x) <- c('Intercept', 'x1')
z <- dat$y
w <- rep(1,nrow(dat))
str(eval(expression(lm.wfit(x, z, w, method = "qr", singular.ok = TRUE))))





###################################################################### 
#1 R 3.1.2 (x86_64-redhat-linux-gnu (64-bit)) / gam 1.09.1 
###################################################################### 
> step.object <- step.gam(gam.object, scope=scope, trace=2) 
Start: y ~ 1; AIC= 797.0034 
Trial: y ~ x1 + 1 ; AIC= -5121.796 
Trial: y ~ 1 + x2 ; AIC= 796.915 
Step:1 y ~ x1 ; AIC= -5121.796 
Trial: y ~ ns(x1, df=2) + 1 ; AIC= -5123.816 
Trial: y ~ x1 + x2 ; AIC= -5174.408 
Step:2 y ~ x1 + x2 ; AIC= -5174.408 
Trial: y ~ ns(x1, df=2) + x2 ; AIC= -5164.829 
> is.null(step.object) 
[1] FALSE 

###################################################################### 
#2: R 3.3.0 (x86_64-redhat-linux-gnu (64-bit)) / gam 1.12 
###################################################################### 
> step.object <- step.gam(gam.object, scope=scope, trace=2) 
Start: y ~ 1; AIC= 797.0034 
Trial: y ~ x1 + 1 ; AIC= -5121.796 
Trial: y ~ 1 + x2 ; AIC= 796.915 
Step:1 y ~ x1 ; AIC= -5121.796 
Trial: y ~ ns(x1, df=2) + 1 ; AIC= -5123.816 
Trial: y ~ x1 + x2 ; AIC= -5174.408 
Step:2 y ~ x1 + x2 ; AIC= -5174.408 
Trial: y ~ ns(x1, df=2) + x2 ; AIC= -5164.829 
> is.null(step.object) 
[1] FALSE 

###################################################################### 
#3: R 3.3.0 (x86_64-redhat-linux-gnu (64-bit)) / gam 1.14-4 
###################################################################### 
> step.object <- step.gam(gam.object, scope=scope, trace=2) 
Start: y ~ 1; AIC= 797.0034 
Trial: y ~ x1 + 1 ; AIC= -5121.796 
Trial: y ~ 1 + x2 ; AIC= 796.915 
Step:1 y ~ x1 ; AIC= -5121.796 
Trial: y ~ ns(x1, df=2) + 1 ; AIC= -5123.816 
Trial: y ~ x1 + x2 ; AIC= -5174.408 
Step:2 y ~ x1 + x2 ; AIC= -5174.408 
Trial: y ~ ns(x1, df=2) + x2 ; AIC= -5164.829 
> is.null(step.object) 
[1] FALSE 

###################################################################### 
#4: R 3.3.2 (x86_64-redhat-linux-gnu (64-bit)) / gam 1.14-4 
###################################################################### 
> step.object <- step.gam(gam.object, scope=scope, trace=2) 
Start: y ~ 1; AIC= 797.0034 
Trial: y ~ x1 + 1 ; AIC= -5122.886 
Trial: y ~ 1 + x2 ; AIC= 796.915 
Step:1 y ~ x1 ; AIC= -5122.886 
Trial: y ~ ns(x1, df=2) + 1 ; AIC= -5177.531 
Trial: y ~ x1 + x2 ; AIC= -5177.531 
Step:2 y ~ ns(x1, df = 2) ; AIC= -5177.531 
Trial: y ~ ns(x1, df=2) + x2 ; AIC= -5222.703 
Step:3 y ~ ns(x1, df = 2) + x2 ; AIC= -5222.703 
> is.null(step.object) 
[1] TRUE 

###################################################################### 
#5: R 3.3.2 (x86_64-redhat-linux-gnu (64-bit)) / gam 1.09.1 
###################################################################### 
> step.object <- step.gam(gam.object, scope=scope, trace=2) 
Start: y ~ 1; AIC= 797.0034 
Trial: y ~ x1 + 1 ; AIC= -5122.886 
Trial: y ~ 1 + x2 ; AIC= 796.915 
Step:1 y ~ x1 ; AIC= -5122.886 
Trial: y ~ ns(x1, df=2) + 1 ; AIC= -5177.531 
Trial: y ~ x1 + x2 ; AIC= -5177.531 
Step:2 y ~ ns(x1, df = 2) ; AIC= -5177.531 
Trial: y ~ ns(x1, df=2) + x2 ; AIC= -5222.703 
Step:3 y ~ ns(x1, df = 2) + x2 ; AIC= -5222.703 
> is.null(step.object) 
[1] TRUE


From alaios at yahoo.com  Tue Oct 24 11:56:44 2017
From: alaios at yahoo.com (Alaios)
Date: Tue, 24 Oct 2017 09:56:44 +0000 (UTC)
Subject: [R] draw a circle with a gradient fill
References: <283095417.3220432.1508839004607.ref@mail.yahoo.com>
Message-ID: <283095417.3220432.1508839004607@mail.yahoo.com>

Hi all,I would like to draw a simple circle where the color gradient follows the rule color = 1/(r^2) where r is the distance from the circle. I would also like to add a color bar with values going from -40 to -110 (and associate those with the color gradient that fills the circle).
So far I experiemented with draw circle install.packages('plotrix')require(plotrix)colors<-c('#fef0d9','#fdcc8a','#fc8d59','#e34a33','#b30000')plot(c(-15,15),c(-15,15),type='n')draw.circle(0, 0, 10, nv = 1000, border = NULL, col = colors[1], lty = 1, lwd = 1)draw.circle(0, 0, 8, nv = 1000, border = NULL, col = colors[2], lty = 1, lwd = 1)draw.circle(0, 0, 6, nv = 1000, border = NULL, col = colors[3], lty = 1, lwd = 1)draw.circle(0, 0, 4, nv = 1000, border = NULL, col = colors[4], lty = 1, lwd = 1)draw.circle(0, 0, 2, nv = 1000, border = NULL, col = colors[5], lty = 1, lwd = 1)

but this package does not give easily color gradients and so my solutions contains 5 same colors filled rings.
Will there be any suggested improvements on my code above?
Thanks a lotAlex
	[[alternative HTML version deleted]]


From btyner at gmail.com  Tue Oct 24 13:21:33 2017
From: btyner at gmail.com (Benjamin Tyner)
Date: Tue, 24 Oct 2017 07:21:33 -0400
Subject: [R] read.table(..., header == FALSE,
 colClasses = <vector with names attribute>)
In-Reply-To: <A310F93F-00B3-48E2-9149-E3CE3219C390@dcn.davis.ca.us>
References: <3a4099b3-4627-b7fe-612f-d9abeca4f65d@gmail.com>
 <A310F93F-00B3-48E2-9149-E3CE3219C390@dcn.davis.ca.us>
Message-ID: <bfd60816-0715-40cf-4388-77fbe44e2d31@gmail.com>

Jeff,

Thank you for your reply. The intent was to construct a minimum 
reproducible example. The same warning occurs when the 'file' argument 
points to a file on disk with a million lines. But you are correct, my 
example was slightly malformed and in fact gives an error under R 
version 3.2.2. Please allow me to try again; in older versions of R,

 ?? > read.table(file = textConnection("a\t3.14"), header = FALSE, 
colClasses = c(x = "character", y = "numeric"), sep="\t")
 ? ?? V1?? V2
 ?? 1? a 3.14

(with no warning). As of version 3.3.0,

 ?? > read.table(file = textConnection("a\t3.14"), header = FALSE, 
colClasses = c(x = "character", y = "numeric"), sep="\t")
 ? ?? V1?? V2
 ?? 1? a 3.14
 ?? Warning message:
 ?? In read.table(file = textConnection("a\t3.14"), header = FALSE,? :
 ? ?? not all columns named in 'colClasses' exist

My intent was not to complain but rather to learn more about best 
practices regarding the names attribute.

Regards

Ben



On 10/23/2017 08:51 PM, Jeff Newmiller wrote:
> You are constructing the equivalent of a two-line data file, and complaining that it is not treating it like it was one line. If it did used to accept this silently [skeptical] then I for one am glad it produces a warning now.


From friendly at yorku.ca  Tue Oct 24 14:10:30 2017
From: friendly at yorku.ca (Michael Friendly)
Date: Tue, 24 Oct 2017 08:10:30 -0400
Subject: [R] Linear regression with tranformed dependant variable
In-Reply-To: <59EE3ED4.9090501@sapo.pt>
References: <221504244.3614450.1508781297607.ref@mail.yahoo.com>
 <221504244.3614450.1508781297607@mail.yahoo.com> <59EE3ED4.9090501@sapo.pt>
Message-ID: <355f15a3-5d0d-f1e7-8262-9e2d320e4e87@yorku.ca>

Step back a minute:  normality is NOT required for predictors in a 
multiple regression model, though the sqrt(x) transformation may
also make the relationship more nearly linear, and linearity IS
assumed when you fit a simple model such as y ~ x + w + z.
(Normality is only required for the residuals/errors)

To see what's going on, you can make make partial regression /
added-variable plots using car::avplots. The loess smooth will
show you if the relationship is non-linear.

HTH
-Michael

> Em 23-10-2017 18:54, kende jan via R-help escreveu:
>> Dear all, I am trying to fit a multiple linear regression model with a 
>> transformed dependant variable (the normality assumption was not 
>> verified...). I have realised a sqrt(variable) transformation... The 
>> results are great, but I don't know how to interprete the beta 
>> coefficients... Is it possible to do another transformation to get 
>> interpretable beta coefficients to express the variations in the 
>> original untransformed dependant variable ? Thank you very much for 
>> your help!No?mie
>> ????[[alternative HTML version deleted]]
>>
>> ______________________________________________


From friendly at yorku.ca  Tue Oct 24 14:10:30 2017
From: friendly at yorku.ca (Michael Friendly)
Date: Tue, 24 Oct 2017 08:10:30 -0400
Subject: [R] Linear regression with tranformed dependant variable
In-Reply-To: <59EE3ED4.9090501@sapo.pt>
References: <221504244.3614450.1508781297607.ref@mail.yahoo.com>
 <221504244.3614450.1508781297607@mail.yahoo.com> <59EE3ED4.9090501@sapo.pt>
Message-ID: <355f15a3-5d0d-f1e7-8262-9e2d320e4e87@yorku.ca>

Step back a minute:  normality is NOT required for predictors in a 
multiple regression model, though the sqrt(x) transformation may
also make the relationship more nearly linear, and linearity IS
assumed when you fit a simple model such as y ~ x + w + z.
(Normality is only required for the residuals/errors)

To see what's going on, you can make make partial regression /
added-variable plots using car::avplots. The loess smooth will
show you if the relationship is non-linear.

HTH
-Michael

> Em 23-10-2017 18:54, kende jan via R-help escreveu:
>> Dear all, I am trying to fit a multiple linear regression model with a 
>> transformed dependant variable (the normality assumption was not 
>> verified...). I have realised a sqrt(variable) transformation... The 
>> results are great, but I don't know how to interprete the beta 
>> coefficients... Is it possible to do another transformation to get 
>> interpretable beta coefficients to express the variations in the 
>> original untransformed dependant variable ? Thank you very much for 
>> your help!No?mie
>> ????[[alternative HTML version deleted]]
>>
>> ______________________________________________


From es at enricoschumann.net  Fri Oct 20 14:38:50 2017
From: es at enricoschumann.net (Enrico Schumann)
Date: Fri, 20 Oct 2017 14:38:50 +0200
Subject: [R] [R-pkgs] NMOF 1.2-2 (Numerical Methods and Optimization in
	Finance)
Message-ID: <20171020143850.Horde.f2cnVm9t67LTsmwHkJscUG7@webmail.your-server.de>

Dear all,

version 1.2-2 of package NMOF is on CRAN now.

NMOF stands for 'Numerical Methods and Optimization
in Finance'. The package provides R code and datasets
for the book with the same name, written by Manfred
Gilli, Dietmar Maringer and Enrico Schumann, published
by Elsevier/Academic Press in 2011.

The package has finally crossed the 1.0 line: It is
10 years since the development of NMOF began, and many
of the functions -- notably those for optimization --
have been in continuous use since then. That implies
a certain maturity, and so it was time to upgrade
the version to 1.0 (and beyond already).

Since my last announcement on this list [1], a number
of functions have been added to the package:
'SAopt' (Simulated Annealing), 'CPPIgap' (portfolio
insurance), 'minvar' (computation of minimum-variance
portfolios), and more. See the NEWS file [2] and the
ChangeLog [3] for all details.

Many of the new functions are described, with
examples, in the Manual [4].


Kind regards
     Enrico


[1] https://stat.ethz.ch/pipermail/r-packages/2016/001510.html
[2] https://github.com/enricoschumann/NMOF/blob/master/NEWS
[3] https://github.com/enricoschumann/NMOF/blob/master/ChangeLog
[4] http://enricoschumann.net/NMOF.htm#NMOFmanual


-- 
Enrico Schumann
Lucerne, Switzerland
http://enricoschumann.net

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From kogalurshear at gmail.com  Fri Oct 20 19:25:02 2017
From: kogalurshear at gmail.com (Udaya B. Kogalur)
Date: Fri, 20 Oct 2017 13:25:02 -0400
Subject: [R] [R-pkgs] randomForestSRC 2.5.1 and OpenMP
Message-ID: <CACsjJ_cqZy-GBCJ3ryLoxoat=To=bRJScepA5nTxgxZ2zbc2Yw@mail.gmail.com>

Dear useRs:

The latest build of randomForestSRC (2.5.1) includes a configuration
file that switches on the -fopenmp tag during compilation on systems
that support OpenMP parallel processing.  Previously, to achieve
parallel processing, we required the additional step of executing an
autoconf command before compiling and installing the package from
source.

We also believe the binaries provided by CRAN for Windows, and MacOS
El Capitan support OpenMP out-of-the-box on many systems.  If you are
running a serial version of our package, we strongly recommend an
upgrade path that will allow you to run the parallel version.  The
performance gains will be significant especially in large n and large
p scenarios.  This upgrade path may be as simple as installing the
2.5.1 CRAN binaries.  But, on some systems, it may also involve a
scenario where you will need to install an OpenMP capable C-compiler
such as GCC, and then compile and install 2.5.1 from source.

Documentation and more can be found at:
https://kogalur.github.io/randomForestSRC/

Thank you.

Udaya Kogalur

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From r-packages at r-project.org  Mon Oct 23 12:55:19 2017
From: r-packages at r-project.org (Lukas Lehnert via R-packages)
Date: Mon, 23 Oct 2017 12:55:19 +0200
Subject: [R] [R-pkgs] hsdar is back on CRAN
Message-ID: <3529837.SCD6RYzQl2@pc19329>

hsdar is back on CRAN

We are very happy to announce that a new version of the hsdar-package 
(hyperspectral data analysis in R) is available on CRAN. 

The introductory vignette is available here: 
https://cran.r-project.org/web/packages/hsdar/vignettes/Hsdar-intro.pdf

The main class "Speclib" has been changed in that the additional information 
is now stored in the SI slot (function "SI" replaced "attribute" for 
update/access). This change was used to restructure how the supplementary data 
is internally stored and allows the user to use e.g. raster images as source 
of additional information (which can be of advantage if e.g. terrain is 
suspected to play a role for hyperspectral classification/regression).  
Additionally, the Speclib-class has now methods compatible with the 
iteratively reading/writing functions of the raster-package (writeStart-, 
writeValues-, writeStop-methods for Speclib class are implemented).

Besides these technical changes, the new version of PROSPECT (version D) is 
implemented.

Cheers

Lukas Lehnert
Hanna Meyer
J?rg Bendix

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From maechler at stat.math.ethz.ch  Tue Oct 24 14:55:19 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 24 Oct 2017 14:55:19 +0200
Subject: [R] read.table(..., header == FALSE,
 colClasses = <vector with names attribute>)
In-Reply-To: <bfd60816-0715-40cf-4388-77fbe44e2d31@gmail.com>
References: <3a4099b3-4627-b7fe-612f-d9abeca4f65d@gmail.com>
 <A310F93F-00B3-48E2-9149-E3CE3219C390@dcn.davis.ca.us>
 <bfd60816-0715-40cf-4388-77fbe44e2d31@gmail.com>
Message-ID: <23023.14391.496804.485394@stat.math.ethz.ch>

>>>>> Benjamin Tyner <btyner at gmail.com>
>>>>>     on Tue, 24 Oct 2017 07:21:33 -0400 writes:

    > Jeff,
    > Thank you for your reply. The intent was to construct a minimum 
    > reproducible example. The same warning occurs when the 'file' argument 
    > points to a file on disk with a million lines. But you are correct, my 
    > example was slightly malformed and in fact gives an error under R 
    > version 3.2.2. Please allow me to try again; in older versions of R,

    > ?? > read.table(file = textConnection("a\t3.14"), header = FALSE, 
    > colClasses = c(x = "character", y = "numeric"), sep="\t")
    > ? ?? V1?? V2
    > ?? 1? a 3.14

    > (with no warning). As of version 3.3.0,

    > ?? > read.table(file = textConnection("a\t3.14"), header = FALSE, 
    > colClasses = c(x = "character", y = "numeric"), sep="\t")
    > ? ?? V1?? V2
    > ?? 1? a 3.14
    > ?? Warning message:
    > ?? In read.table(file = textConnection("a\t3.14"), header = FALSE,? :
    > ? ?? not all columns named in 'colClasses' exist

    > My intent was not to complain but rather to learn more about best 
    > practices regarding the names attribute.

which is a nice attitude, thank you.

An even shorter MRE (as header=FALSE is default, and the default
sep="" works, too):

> tt <- read.table(textConnection("a 3.14"), colClasses = c(x="character", y="numeric"))
Warning message:
In read.table(file = textConnection("a 3.14"), colClasses = c(x = "character",  :
  not all columns named in 'colClasses' exist
> 

If you read in the help page -- you did read that before posting, did you?---
how 'colClasses' should be specified ,

    colClasses: character.  A vector of classes to be assumed for the
	      columns.  If unnamed, recycled as necessary.  If named, names
	      are matched with unspecified values being taken to be ?NA?.

	      Possible values are ..................
	      .........

and the 'x' and 'y' names you used, are matched with the
colnames ... which on the other hand are "V1" and "V2"  for
you, and so you provoke a warning.

Once you have read (and understood) the above part of the help
page, it becomes, easy, no?

> tt <- read.table(textConnection("a 3.14"), colClasses = c("character","numeric"))
> t2 <- read.table(textConnection("a 3.14"), colClasses=c(x="character",y="numeric"), col.names=c("x","y"))
> t2
  x    y
1 a 3.14
> 

i.e., no warning in both of these two cases.  

So please, please, PLEASE: at least non-beginners like you *should*
take the effort to read the help page (and report if these seem
incomplete or otherwise improvable)... 

Best,
Martin Maechler
ETH Zurich


From morkus at protonmail.com  Tue Oct 24 16:01:36 2017
From: morkus at protonmail.com (Morkus)
Date: Tue, 24 Oct 2017 10:01:36 -0400
Subject: [R] Creating a data table (frame?) from a SQL Statement?
Message-ID: <lz72EYKXdg1ZLStPTsYy2dg8eq0CnYli_3VtUb7oz866D7ktMyOfUUjcRObMlMwTUoEJUBuunQWoXHJCjgx68kg9m00cfTCIQ7-iC5j3TYc=@protonmail.com>

Hello,

I'm new to R so this is probably a simple question for somebody.

I have an RScript that reads a CSV on the disk using read.table(...). It then does a boxM test using that data.

However, I'm now trying to load the same data via an SQL command, but I can't seem to get the data structure defined so R will like it -- using the included "iris" dataset.

I've tried these ways of loading the SQL statement into a compatible R Structure:

irisQuery <- data(dbGetQuery(conn, "select * from iris"))

irisQuery <- data.frame(dbGetQuery(conn, "select * from iris"))

irisQuery <- table(dbGetQuery(conn, "select * from iris"))

.
.
.
Followed by:

boxM(irisQuery[,-5], irisQuery[,5])

Nothing works work.

For example, if i use ...

irisQuery <- data.frame(dbGetQuery(conn, "select * from iris"))

I get this error from R on the boxM test:  Error: is.numeric(x) || is.logical(x) is not TRUE

The SQL Statement is returning results, but their not CSV. Not sure if that matters.
-------------------

So, how do I read a SQL statement into an R structure like I read the CSV using "read.table" so the boxM test will work?

Thanks very much in advance.

- M

Sent from [ProtonMail](https://protonmail.com), Swiss-based encrypted email.
	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Oct 24 17:37:51 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 24 Oct 2017 08:37:51 -0700
Subject: [R] draw a circle with a gradient fill
In-Reply-To: <283095417.3220432.1508839004607@mail.yahoo.com>
References: <283095417.3220432.1508839004607.ref@mail.yahoo.com>
 <283095417.3220432.1508839004607@mail.yahoo.com>
Message-ID: <CAGxFJbT7Du_k=jXXostBHdZ0B6GxymCmoLzqWW7LR2s-YKomFQ@mail.gmail.com>

There are lots of different ways to do this. I don't use plotrix, which
uses base R graphics functionality, so I can't help you with your current
approach. However, it is easy to do this "by hand" in the newer,
object-based grid package (on which ggpot and lattice are built) . The
idea, which probably works for plotrix, too, is to simply draw concentric
circles of *decreasing* radiii with a gradient of fill colors; each
successive fill will overwrite the previous one.

Here's some code using grid that you can customize that gives you the idea.
Obviously, you'd also have to add your color bar using grid graphics
functionaity, which is again straightforward.

grid.newpage()
mycols<- colorRampPalette(c("yellow","orange","red"))
radii = .3*seq(1,.05,length.out = 25)
grid.circle(x=.5,y=.5, r = radii,
         gp= gpar(fill=mycols(25), lwd = 0))

However, probably plotrix and ggplot would allow you to do this more easily
with higher level graphics funtions. Searching (e.g. on rseek.org) might
bring up what you want if you don't get a more satisfactory reply here.

Finally, **Please** post in plain text, not HTML. As you can see from the
quoted message below, your text got mangled. This, of course, will
discourage others to give you the response you need.

Cheers,
Bert




Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Tue, Oct 24, 2017 at 2:56 AM, Alaios via R-help <r-help at r-project.org>
wrote:

> Hi all,I would like to draw a simple circle where the color gradient
> follows the rule color = 1/(r^2) where r is the distance from the circle. I
> would also like to add a color bar with values going from -40 to -110 (and
> associate those with the color gradient that fills the circle).
> So far I experiemented with draw circle install.packages('plotrix')
> require(plotrix)colors<-c('#fef0d9','#fdcc8a','#fc8d59','#
> e34a33','#b30000')plot(c(-15,15),c(-15,15),type='n')draw.circle(0, 0, 10,
> nv = 1000, border = NULL, col = colors[1], lty = 1, lwd = 1)draw.circle(0,
> 0, 8, nv = 1000, border = NULL, col = colors[2], lty = 1, lwd =
> 1)draw.circle(0, 0, 6, nv = 1000, border = NULL, col = colors[3], lty = 1,
> lwd = 1)draw.circle(0, 0, 4, nv = 1000, border = NULL, col = colors[4], lty
> = 1, lwd = 1)draw.circle(0, 0, 2, nv = 1000, border = NULL, col =
> colors[5], lty = 1, lwd = 1)
>
> but this package does not give easily color gradients and so my solutions
> contains 5 same colors filled rings.
> Will there be any suggested improvements on my code above?
> Thanks a lotAlex
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Tue Oct 24 18:40:45 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 24 Oct 2017 09:40:45 -0700
Subject: [R] Creating a data table (frame?) from a SQL Statement?
In-Reply-To: <lz72EYKXdg1ZLStPTsYy2dg8eq0CnYli_3VtUb7oz866D7ktMyOfUUjcRObMlMwTUoEJUBuunQWoXHJCjgx68kg9m00cfTCIQ7-iC5j3TYc=@protonmail.com>
References: <lz72EYKXdg1ZLStPTsYy2dg8eq0CnYli_3VtUb7oz866D7ktMyOfUUjcRObMlMwTUoEJUBuunQWoXHJCjgx68kg9m00cfTCIQ7-iC5j3TYc=@protonmail.com>
Message-ID: <0538EC9D-4E9B-455C-A48D-6423D742E4DD@dcn.davis.ca.us>

Your question desperately needs a reproducible example (a.k.a. "reprex"), because you have to be using contributed packages to do any of this. You also need to clarify whether you are intending to access data already in an external database, or are planning to load it using R and manipulate it with SQL.

I suggest that you install the "reprex" and "sqldf" packages and read their documentation and try out their examples. Then when you ask a question use reprex to be sure we see all the steps needed to get to where you are stuck. In particular we need the library calls that preceeded your sample code, but the reprex package will verify that you have it all there before you send the question. 

FWIW, note that the data function has a specific purpose of pulling data sets by name out of packages into your workspace, and giving it the output of randomly selected functions is not likely to work. Start reading help files soon... they may seem dense at first but the only way to get past that is to get started soon. Type

?data

at the console to start learning about that function.

Note that csv files are outside the world of standard SQL, so some of what you are doing may be very tightly linked with the particular SQL engine you are using.
-- 
Sent from my phone. Please excuse my brevity.

On October 24, 2017 7:01:36 AM PDT, Morkus via R-help <r-help at r-project.org> wrote:
>Hello,
>
>I'm new to R so this is probably a simple question for somebody.
>
>I have an RScript that reads a CSV on the disk using read.table(...).
>It then does a boxM test using that data.
>
>However, I'm now trying to load the same data via an SQL command, but I
>can't seem to get the data structure defined so R will like it -- using
>the included "iris" dataset.
>
>I've tried these ways of loading the SQL statement into a compatible R
>Structure:
>
>irisQuery <- data(dbGetQuery(conn, "select * from iris"))
>
>irisQuery <- data.frame(dbGetQuery(conn, "select * from iris"))
>
>irisQuery <- table(dbGetQuery(conn, "select * from iris"))
>
>.
>.
>.
>Followed by:
>
>boxM(irisQuery[,-5], irisQuery[,5])
>
>Nothing works work.
>
>For example, if i use ...
>
>irisQuery <- data.frame(dbGetQuery(conn, "select * from iris"))
>
>I get this error from R on the boxM test:  Error: is.numeric(x) ||
>is.logical(x) is not TRUE
>
>The SQL Statement is returning results, but their not CSV. Not sure if
>that matters.
>-------------------
>
>So, how do I read a SQL statement into an R structure like I read the
>CSV using "read.table" so the boxM test will work?
>
>Thanks very much in advance.
>
>- M
>
>Sent from [ProtonMail](https://protonmail.com), Swiss-based encrypted
>email.
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From ebs15242 at gmail.com  Tue Oct 24 19:58:02 2017
From: ebs15242 at gmail.com (Ed Siefker)
Date: Tue, 24 Oct 2017 12:58:02 -0500
Subject: [R] as.data.frame doesn't set col.names
Message-ID: <CALRb-ocavFTRrOgh1uPtn7q0e_M70f-_WNncKAueNAoxH7O4xA@mail.gmail.com>

Why doesn't this work?

> samples$geno <- as.data.frame(sapply(yo, toupper), col.names="geno")
> samples
                          quant_samples   age sapply(yo, toupper)
E11.5 F20het BA40     E11.5 F20het BA40 E11.5              F20HET
E11.5 F20het BA45     E11.5 F20het BA45 E11.5              F20HET


From jdnewmil at dcn.davis.ca.us  Tue Oct 24 19:57:58 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 24 Oct 2017 10:57:58 -0700
Subject: [R] Creating a data table (frame?) from a SQL Statement?
In-Reply-To: <EtX_CBMAM76ul1hYtiU-AGZdvO7-G97JlwD3VxXG0TkEC2RtZZQbdUnur6PiWdgJ46sDguEDtgvGn1-kusP0UDIQLX8gHta_R6fid3Jr4lQ=@protonmail.com>
References: <lz72EYKXdg1ZLStPTsYy2dg8eq0CnYli_3VtUb7oz866D7ktMyOfUUjcRObMlMwTUoEJUBuunQWoXHJCjgx68kg9m00cfTCIQ7-iC5j3TYc=@protonmail.com>
 <0538EC9D-4E9B-455C-A48D-6423D742E4DD@dcn.davis.ca.us>
 <EtX_CBMAM76ul1hYtiU-AGZdvO7-G97JlwD3VxXG0TkEC2RtZZQbdUnur6PiWdgJ46sDguEDtgvGn1-kusP0UDIQLX8gHta_R6fid3Jr4lQ=@protonmail.com>
Message-ID: <034FDAEC-6786-4A1F-AF6C-3E75D7DA4C58@dcn.davis.ca.us>

Please always cc the list... the group usually has better answers than any one person, and I don't do private consulting on the net. For future reference, there is also an r-sig-db mailing list where this question really belongs. 

Since I almost never use jdbc, I would need a reprex (hint you are ignoring my advice... a good way to get ignored), which this still isn't. There is probably enough here for someone more familiar than I to help you with. I for one find it hard to understand why read.table is successfully figuring out how to make sense of the presumably-valid data frame that dbGetQuery is returning. That is, you really don't need to send everything through read.table in order to make use of it. Maybe try

irisDF <- dbGetQuery(conn, "select * from iris")

str( irisDF )

-- 
Sent from my phone. Please excuse my brevity.

On October 24, 2017 10:11:28 AM PDT, Morkus <morkus at protonmail.com> wrote:
>Jeff,
>
>Excellent points, thank you!
>
>Yes, I have all the required packages installed.
>
>Below's the actual full script.
>
>require(Rserve)
>require(biotools)
>library(rJava)
>library(RJDBC)
>drv <- JDBC("com.mysql.jdbc.Driver","/Users/.../mysql.jar")
>conn <- dbConnect(drv, "jdbc:mysql://localhost:3306/morkus","root",
>"password")
>dbListTables(conn)                                                     
>           // works!
>count <- dbGetQuery(conn,"select count(*) from iris")            //
>works!
>irisQuery <- read.table(dbGetQuery(conn, "select * from iris")) works!
>irisQuery                                                              
>              // displays table from iris dataset I imported into MySQL
>boxM(irisQuery[,-5], irisQuery[,5])                                    
> / />>>  FAILS! <<< "Error: is.numeric(x) || is.logical(x) is not TRUE"
>dbDisconnect(conn)                                                     
>            // displays TRUE.
>
>----
>
>Few rows of displayed data in R Console from line above: "irisQuery"
>
>   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
>1            5.1         3.5          1.4         0.2     setosa
>2            4.9           3          1.4         0.2     setosa
>3            4.7         3.2          1.3         0.2     setosa
>4            4.6         3.1          1.5         0.2     setosa
>5              5         3.6          1.4         0.2     setosa
>6            5.4         3.9          1.7         0.4     setosa
>7            4.6         3.4          1.4         0.3     setosa
>.
>.
>.
>---
>
>Now, if I do the boxM statistic reading a CSV file from disk, like
>this:
>
>irisQuery = read.table('/Users/.../iris.csv', sep=',', header=FALSE,
>stringsAsFactor=FALSE)
>
>Then everything works!  (But I'm not reading files from a CSV on the
>disk; rather, from SQL as above.)
>
>---
>
>So my problem is that I can't seem to package the SQL Result (shown
>above) so that it works like the read.table. I'm missing some R
>function to transform the SQL to a "table" R can work with.
>
>Does that help fill in the missing pieces?
>
>Sorry my first posting wasn't this complete.
>
>Thanks again in advance,
>
>- M
>
>Sent from [ProtonMail](https://protonmail.com), Swiss-based encrypted
>email.
>
>> -------- Original Message --------
>> Subject: Re: [R] Creating a data table (frame?) from a SQL Statement?
>> Local Time: October 24, 2017 12:40 PM
>> UTC Time: October 24, 2017 4:40 PM
>> From: jdnewmil at dcn.davis.ca.us
>> To: Morkus <morkus at protonmail.com>, r-help at r-project.org
><r-help at r-project.org>
>>
>> Your question desperately needs a reproducible example (a.k.a.
>"reprex"), because you have to be using contributed packages to do any
>of this. You also need to clarify whether you are intending to access
>data already in an external database, or are planning to load it using
>R and manipulate it with SQL.
>>
>> I suggest that you install the "reprex" and "sqldf" packages and read
>their documentation and try out their examples. Then when you ask a
>question use reprex to be sure we see all the steps needed to get to
>where you are stuck. In particular we need the library calls that
>preceeded your sample code, but the reprex package will verify that you
>have it all there before you send the question.
>>
>> FWIW, note that the data function has a specific purpose of pulling
>data sets by name out of packages into your workspace, and giving it
>the output of randomly selected functions is not likely to work. Start
>reading help files soon... they may seem dense at first but the only
>way to get past that is to get started soon. Type
>>
>> ?data
>>
>> at the console to start learning about that function.
>>
>> Note that csv files are outside the world of standard SQL, so some of
>what you are doing may be very tightly linked with the particular SQL
>engine you are using.
>>
>> Sent from my phone. Please excuse my brevity.
>>
>> On October 24, 2017 7:01:36 AM PDT, Morkus via R-help
>r-help at r-project.org wrote:
>>
>>> Hello,
>>> I'm new to R so this is probably a simple question for somebody.
>>> I have an RScript that reads a CSV on the disk using
>read.table(...).
>>> It then does a boxM test using that data.
>>> However, I'm now trying to load the same data via an SQL command,
>but I
>>> can't seem to get the data structure defined so R will like it --
>using
>>> the included "iris" dataset.
>>> I've tried these ways of loading the SQL statement into a compatible
>R
>>> Structure:
>>> irisQuery <- data(dbGetQuery(conn, "select * from iris"))
>>> irisQuery <- data.frame(dbGetQuery(conn, "select * from iris"))
>>> irisQuery <- table(dbGetQuery(conn, "select * from iris"))
>>> .
>>> .
>>> .
>>> Followed by:
>>> boxM(irisQuery[,-5], irisQuery[,5])
>>> Nothing works work.
>>> For example, if i use ...
>>> irisQuery <- data.frame(dbGetQuery(conn, "select * from iris"))
>>> I get this error from R on the boxM test: Error: is.numeric(x) ||
>>> is.logical(x) is not TRUE
>>>
>>> The SQL Statement is returning results, but their not CSV. Not sure
>if
>>> that matters.
>>>
>>> So, how do I read a SQL statement into an R structure like I read
>the
>>> CSV using "read.table" so the boxM test will work?
>>> Thanks very much in advance.
>>>
>>> - M
>>>
>>> Sent from [ProtonMail](https://protonmail.com), Swiss-based
>encrypted
>>> email.
>>> [[alternative HTML version deleted]]
>>> ---------------------------------------------------------------
>>>
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.


From ebs15242 at gmail.com  Tue Oct 24 20:00:01 2017
From: ebs15242 at gmail.com (Ed Siefker)
Date: Tue, 24 Oct 2017 13:00:01 -0500
Subject: [R] as.data.frame doesn't set col.names
In-Reply-To: <CALRb-ocavFTRrOgh1uPtn7q0e_M70f-_WNncKAueNAoxH7O4xA@mail.gmail.com>
References: <CALRb-ocavFTRrOgh1uPtn7q0e_M70f-_WNncKAueNAoxH7O4xA@mail.gmail.com>
Message-ID: <CALRb-od+pq2-_=KSOL9RJf_mB2y=YwX7ru1VJg9_W_NaZQgxMg@mail.gmail.com>

Wait.  Now I'm really confused.

>
> head(samples)
                          quant_samples   age sapply(yo, toupper)
E11.5 F20het BA40     E11.5 F20het BA40 E11.5              F20HET
E11.5 F20het BA45     E11.5 F20het BA45 E11.5              F20HET
E11.5 F20het BB84     E11.5 F20het BB84 E11.5              F20HET
E11.5 F9.20DKO KTr3 E11.5 F9.20DKO KTr3 E11.5            F9.20DKO
E11.5 F9.20DKO PEd2 E11.5 F9.20DKO PEd2 E11.5            F9.20DKO
E11.5 F9.20DKO j0J1 E11.5 F9.20DKO j0J1 E11.5            F9.20DKO
> colnames(samples)
[1] "quant_samples" "age"           "geno"

Really, really confused.

On Tue, Oct 24, 2017 at 12:58 PM, Ed Siefker <ebs15242 at gmail.com> wrote:
> Why doesn't this work?
>
>> samples$geno <- as.data.frame(sapply(yo, toupper), col.names="geno")
>> samples
>                           quant_samples   age sapply(yo, toupper)
> E11.5 F20het BA40     E11.5 F20het BA40 E11.5              F20HET
> E11.5 F20het BA45     E11.5 F20het BA45 E11.5              F20HET


From booboo at gforcecable.com  Tue Oct 24 21:05:01 2017
From: booboo at gforcecable.com (BooBoo)
Date: Tue, 24 Oct 2017 15:05:01 -0400
Subject: [R] Problem Subsetting Rows that Have NA's
Message-ID: <79a2cf04-dd3e-4508-2150-e496fd7bc386@gforcecable.com>

This has every appearance of being a bug. If it is not a bug, can 
someone tell me what I am asking for when I ask for "x[x[,2]==0,]". Thanks.

 > #here is the toy dataset
 > x <- rbind(c(1,1),c(2,2),c(3,3),c(4,0),c(5,0),c(6,NA),
+   c(7,NA),c(8,NA),c(9,NA),c(10,NA)
+ )
 > x
       [,1] [,2]
  [1,]    1    1
  [2,]    2    2
  [3,]    3    3
  [4,]    4    0
  [5,]    5    0
  [6,]    6   NA
  [7,]    7   NA
  [8,]    8   NA
  [9,]    9   NA
[10,]   10   NA
 >
 > #it contains rows that have NA's
 > x[is.na(x[,2]),]
      [,1] [,2]
[1,]    6   NA
[2,]    7   NA
[3,]    8   NA
[4,]    9   NA
[5,]   10   NA
 >
 > #seems like an unreasonable answer to a reasonable question
 > x[x[,2]==0,]
      [,1] [,2]
[1,]    4    0
[2,]    5    0
[3,]   NA   NA
[4,]   NA   NA
[5,]   NA   NA
[6,]   NA   NA
[7,]   NA   NA
 >
 > #this is more what I was expecting
 > x[which(x[,2]==0),]
      [,1] [,2]
[1,]    4    0
[2,]    5    0
 >


From btupper at bigelow.org  Tue Oct 24 22:12:47 2017
From: btupper at bigelow.org (Ben Tupper)
Date: Tue, 24 Oct 2017 16:12:47 -0400
Subject: [R] Problem Subsetting Rows that Have NA's
In-Reply-To: <79a2cf04-dd3e-4508-2150-e496fd7bc386@gforcecable.com>
References: <79a2cf04-dd3e-4508-2150-e496fd7bc386@gforcecable.com>
Message-ID: <C3B99EBD-DF45-4B69-B4A4-ABCB106E04CA@bigelow.org>

Hi,

It's related to how NAs are treated in comparison operations.  See the Details section of https://www.rdocumentation.org/packages/base/versions/3.4.1/topics/Comparison <https://www.rdocumentation.org/packages/base/versions/3.4.1/topics/Comparison>

You can try something like this...

x[which(x[,2] %in% 0),]
#      [,1] [,2]
# [1,]    4    0
# [2,]    5    0


... but I'm not sure if it is bullet proof.  Others may have more insight.

Cheers,
Ben



> On Oct 24, 2017, at 3:05 PM, BooBoo <booboo at gforcecable.com> wrote:
> 
> This has every appearance of being a bug. If it is not a bug, can someone tell me what I am asking for when I ask for "x[x[,2]==0,]". Thanks.
> 
> > #here is the toy dataset
> > x <- rbind(c(1,1),c(2,2),c(3,3),c(4,0),c(5,0),c(6,NA),
> +   c(7,NA),c(8,NA),c(9,NA),c(10,NA)
> + )
> > x
>      [,1] [,2]
> [1,]    1    1
> [2,]    2    2
> [3,]    3    3
> [4,]    4    0
> [5,]    5    0
> [6,]    6   NA
> [7,]    7   NA
> [8,]    8   NA
> [9,]    9   NA
> [10,]   10   NA
> >
> > #it contains rows that have NA's
> > x[is.na(x[,2]),]
>     [,1] [,2]
> [1,]    6   NA
> [2,]    7   NA
> [3,]    8   NA
> [4,]    9   NA
> [5,]   10   NA
> >
> > #seems like an unreasonable answer to a reasonable question
> > x[x[,2]==0,]
>     [,1] [,2]
> [1,]    4    0
> [2,]    5    0
> [3,]   NA   NA
> [4,]   NA   NA
> [5,]   NA   NA
> [6,]   NA   NA
> [7,]   NA   NA
> >
> > #this is more what I was expecting
> > x[which(x[,2]==0),]
>     [,1] [,2]
> [1,]    4    0
> [2,]    5    0
> >
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Ecocast Reports: http://seascapemodeling.org/ecocast.html
Tick Reports: https://report.bigelow.org/tick/
Jellyfish Reports: https://jellyfish.bigelow.org/jellyfish/




	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Tue Oct 24 22:45:23 2017
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 24 Oct 2017 20:45:23 +0000
Subject: [R] as.data.frame doesn't set col.names
In-Reply-To: <CALRb-od+pq2-_=KSOL9RJf_mB2y=YwX7ru1VJg9_W_NaZQgxMg@mail.gmail.com>
References: <CALRb-ocavFTRrOgh1uPtn7q0e_M70f-_WNncKAueNAoxH7O4xA@mail.gmail.com>
 <CALRb-od+pq2-_=KSOL9RJf_mB2y=YwX7ru1VJg9_W_NaZQgxMg@mail.gmail.com>
Message-ID: <96f7237c48b641fbbad902b245daff68@exch-2p-mbx-w2.ads.tamu.edu>

You left out all the most important bits of information. What is yo? Are you trying to assign a data frame to a single column in another data frame? Printing head(samples) tells us nothing about what data types you have, especially if the things that look like text are really factors that were created when you used one of the read.*() functions. Use str(samples) to see what you are dealing with. 

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ed Siefker
Sent: Tuesday, October 24, 2017 1:00 PM
To: r-help <r-help at r-project.org>
Subject: Re: [R] as.data.frame doesn't set col.names

Wait.  Now I'm really confused.

>
> head(samples)
                          quant_samples   age sapply(yo, toupper)
E11.5 F20het BA40     E11.5 F20het BA40 E11.5              F20HET
E11.5 F20het BA45     E11.5 F20het BA45 E11.5              F20HET
E11.5 F20het BB84     E11.5 F20het BB84 E11.5              F20HET
E11.5 F9.20DKO KTr3 E11.5 F9.20DKO KTr3 E11.5            F9.20DKO
E11.5 F9.20DKO PEd2 E11.5 F9.20DKO PEd2 E11.5            F9.20DKO
E11.5 F9.20DKO j0J1 E11.5 F9.20DKO j0J1 E11.5            F9.20DKO
> colnames(samples)
[1] "quant_samples" "age"           "geno"

Really, really confused.

On Tue, Oct 24, 2017 at 12:58 PM, Ed Siefker <ebs15242 at gmail.com> wrote:
> Why doesn't this work?
>
>> samples$geno <- as.data.frame(sapply(yo, toupper), col.names="geno") 
>> samples
>                           quant_samples   age sapply(yo, toupper)
> E11.5 F20het BA40     E11.5 F20het BA40 E11.5              F20HET
> E11.5 F20het BA45     E11.5 F20het BA45 E11.5              F20HET

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Tue Oct 24 23:45:11 2017
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 25 Oct 2017 08:45:11 +1100
Subject: [R] draw a circle with a gradient fill
In-Reply-To: <283095417.3220432.1508839004607@mail.yahoo.com>
References: <283095417.3220432.1508839004607.ref@mail.yahoo.com>
 <283095417.3220432.1508839004607@mail.yahoo.com>
Message-ID: <CA+8X3fUd6JOWqtXH_YP_gh+J3XtK38yyr7MFBtkK17e+fzJiHQ@mail.gmail.com>

Hi Alex,
This is harder than it looks for a number of reasons. First you want
to desaturate the colors as you move out, which requires something
more sophisticated than the approximation I have done in the code
below. RGB may not be the best colorspace to use for this. Second is
that when you draw a circle, it isn't a circle, it's a polygon.
Therefore you have to fiddle with the line width and spacing to get it
to look smooth. I hope that this example is helpful, but I am not sure
that it illustrates what you want:

library(plotrix)
plot(c(-15,15),c(-15,15),type="n")
ccolors<-rep(NA,60)
cindex<-1
for(rad in seq(10,0.1,length.out=60)) {
 r<-floor(179+76*rad*rad/10000)
 g<-b<-floor(2*rad*rad)
 ccolors[cindex]<-rgb(r/255,g/255,b/255)
 draw.circle(0,0,rad,border=ccolors[cindex],col=NA,
  lwd=3,nv=100+rad*10)
 cindex<-cindex+1
}
ccolors
color.legend(2,-15,15,-13,legend=seq(-40,-110,length.out=5),
 rect.col=ccolors[c(1,22,35,47,60)])

Jim

On Tue, Oct 24, 2017 at 8:56 PM, Alaios via R-help <r-help at r-project.org> wrote:
> Hi all,I would like to draw a simple circle where the color gradient follows the rule color = 1/(r^2) where r is the distance from the circle. I would also like to add a color bar with values going from -40 to -110 (and associate those with the color gradient that fills the circle).
> So far I experiemented with draw circle install.packages('plotrix')require(plotrix)colors<-c('#fef0d9','#fdcc8a','#fc8d59','#e34a33','#b30000')plot(c(-15,15),c(-15,15),type='n')draw.circle(0, 0, 10, nv = 1000, border = NULL, col = colors[1], lty = 1, lwd = 1)draw.circle(0, 0, 8, nv = 1000, border = NULL, col = colors[2], lty = 1, lwd = 1)draw.circle(0, 0, 6, nv = 1000, border = NULL, col = colors[3], lty = 1, lwd = 1)draw.circle(0, 0, 4, nv = 1000, border = NULL, col = colors[4], lty = 1, lwd = 1)draw.circle(0, 0, 2, nv = 1000, border = NULL, col = colors[5], lty = 1, lwd = 1)
>
> but this package does not give easily color gradients and so my solutions contains 5 same colors filled rings.
> Will there be any suggested improvements on my code above?
> Thanks a lotAlex
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From btyner at gmail.com  Wed Oct 25 00:29:50 2017
From: btyner at gmail.com (Benjamin Tyner)
Date: Tue, 24 Oct 2017 18:29:50 -0400
Subject: [R] read.table(..., header == FALSE,
 colClasses = <vector with names attribute>)
In-Reply-To: <23023.14391.496804.485394@stat.math.ethz.ch>
References: <3a4099b3-4627-b7fe-612f-d9abeca4f65d@gmail.com>
 <A310F93F-00B3-48E2-9149-E3CE3219C390@dcn.davis.ca.us>
 <bfd60816-0715-40cf-4388-77fbe44e2d31@gmail.com>
 <23023.14391.496804.485394@stat.math.ethz.ch>
Message-ID: <ec9162cf-ffd0-b4e2-bb8c-5db577c58cfa@gmail.com>

Yes, it makes sense now; lesson learned. Thank you both! Sometimes it 
seems that no matter how good the documentation, some useR will 
inevitably (ab)use the code in ways that were never intended by the 
authors. Then when the code and/or documentation changes, it is not 
always obvious to the useR whether the intent of the authors has 
changed, or whether the useR had just been "getting the right answer for 
the wrong reason" all along. In this particular case, the change was 
documented as stemming from a "new feature" (as opposed to a bugfix or 
more stringent argument checking) that might appear to be a non fully 
backwards compatible change. For example one might have the (apparently) 
bad habit of using col.names as a shortcut to rename headers on-the-fly ...

 ?? > getRversion()
 ?? [1] ?3.2.2?

 ?? > read.table(textConnection("x y\na 3.14"), header = TRUE, 
colClasses = c(x = "character", y = "numeric"), col.names = c("foo", "bar"))
 ? ?? foo? bar
 ?? 1?? a 3.14

but indeed, the names attribute has zero effect on the result:

 ?? > read.table(textConnection("x y\na 3.14"), header = TRUE, 
colClasses = c(y = "character", x = "numeric"), col.names = c("foo", "bar"))
 ? ?? foo? bar
 ?? 1?? a 3.14

so I agree it is good that we are checking for that now.

Regards
Ben

On 10/24/2017 08:55 AM, Martin Maechler wrote:
>>>>>> Benjamin Tyner <btyner at gmail.com>
>>>>>>      on Tue, 24 Oct 2017 07:21:33 -0400 writes:
>      > Jeff,
>      > Thank you for your reply. The intent was to construct a minimum
>      > reproducible example. The same warning occurs when the 'file' argument
>      > points to a file on disk with a million lines. But you are correct, my
>      > example was slightly malformed and in fact gives an error under R
>      > version 3.2.2. Please allow me to try again; in older versions of R,
>
>      > ?? > read.table(file = textConnection("a\t3.14"), header = FALSE,
>      > colClasses = c(x = "character", y = "numeric"), sep="\t")
>      > ? ?? V1?? V2
>      > ?? 1? a 3.14
>
>      > (with no warning). As of version 3.3.0,
>
>      > ?? > read.table(file = textConnection("a\t3.14"), header = FALSE,
>      > colClasses = c(x = "character", y = "numeric"), sep="\t")
>      > ? ?? V1?? V2
>      > ?? 1? a 3.14
>      > ?? Warning message:
>      > ?? In read.table(file = textConnection("a\t3.14"), header = FALSE,? :
>      > ? ?? not all columns named in 'colClasses' exist
>
>      > My intent was not to complain but rather to learn more about best
>      > practices regarding the names attribute.
>
> which is a nice attitude, thank you.
>
> An even shorter MRE (as header=FALSE is default, and the default
> sep="" works, too):
>
>> tt <- read.table(textConnection("a 3.14"), colClasses = c(x="character", y="numeric"))
> Warning message:
> In read.table(file = textConnection("a 3.14"), colClasses = c(x = "character",  :
>    not all columns named in 'colClasses' exist
> If you read in the help page -- you did read that before posting, did you?---
> how 'colClasses' should be specified ,
>
>      colClasses: character.  A vector of classes to be assumed for the
> 	      columns.  If unnamed, recycled as necessary.  If named, names
> 	      are matched with unspecified values being taken to be ?NA?.
>
> 	      Possible values are ..................
> 	      .........
>
> and the 'x' and 'y' names you used, are matched with the
> colnames ... which on the other hand are "V1" and "V2"  for
> you, and so you provoke a warning.
>
> Once you have read (and understood) the above part of the help
> page, it becomes, easy, no?
>
>> tt <- read.table(textConnection("a 3.14"), colClasses = c("character","numeric"))
>> t2 <- read.table(textConnection("a 3.14"), colClasses=c(x="character",y="numeric"), col.names=c("x","y"))
>> t2
>    x    y
> 1 a 3.14
> i.e., no warning in both of these two cases.
>
> So please, please, PLEASE: at least non-beginners like you *should*
> take the effort to read the help page (and report if these seem
> incomplete or otherwise improvable)...
>
> Best,
> Martin Maechler
> ETH Zurich


From esawiek at gmail.com  Wed Oct 25 01:38:04 2017
From: esawiek at gmail.com (Ek Esawi)
Date: Tue, 24 Oct 2017 19:38:04 -0400
Subject: [R] Problem Subsetting Rows that Have NA's
In-Reply-To: <79a2cf04-dd3e-4508-2150-e496fd7bc386@gforcecable.com>
References: <79a2cf04-dd3e-4508-2150-e496fd7bc386@gforcecable.com>
Message-ID: <CA+ZkTxsR_nr+jdHK0_kRyNMhex=kX7oaTHwdqL9smzAX9m=FSQ@mail.gmail.com>

z <- x[x[,2]==0&!is.na(x[,2]),]  seems to work and get you what you want,
but doesn't answer your question,
z <- x[x[,2]==0&!is.na(x[,2]),]

Best of luck,
EK

On Tue, Oct 24, 2017 at 3:05 PM, BooBoo <booboo at gforcecable.com> wrote:

> This has every appearance of being a bug. If it is not a bug, can someone
> tell me what I am asking for when I ask for "x[x[,2]==0,]". Thanks.
>
> > #here is the toy dataset
> > x <- rbind(c(1,1),c(2,2),c(3,3),c(4,0),c(5,0),c(6,NA),
> +   c(7,NA),c(8,NA),c(9,NA),c(10,NA)
> + )
> > x
>       [,1] [,2]
>  [1,]    1    1
>  [2,]    2    2
>  [3,]    3    3
>  [4,]    4    0
>  [5,]    5    0
>  [6,]    6   NA
>  [7,]    7   NA
>  [8,]    8   NA
>  [9,]    9   NA
> [10,]   10   NA
> >
> > #it contains rows that have NA's
> > x[is.na(x[,2]),]
>      [,1] [,2]
> [1,]    6   NA
> [2,]    7   NA
> [3,]    8   NA
> [4,]    9   NA
> [5,]   10   NA
> >
> > #seems like an unreasonable answer to a reasonable question
> > x[x[,2]==0,]
>      [,1] [,2]
> [1,]    4    0
> [2,]    5    0
> [3,]   NA   NA
> [4,]   NA   NA
> [5,]   NA   NA
> [6,]   NA   NA
> [7,]   NA   NA
> >
> > #this is more what I was expecting
> > x[which(x[,2]==0),]
>      [,1] [,2]
> [1,]    4    0
> [2,]    5    0
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ccberry at ucsd.edu  Wed Oct 25 01:26:47 2017
From: ccberry at ucsd.edu (Berry, Charles)
Date: Tue, 24 Oct 2017 23:26:47 +0000
Subject: [R] draw a circle with a gradient fill
In-Reply-To: <283095417.3220432.1508839004607@mail.yahoo.com>
References: <283095417.3220432.1508839004607.ref@mail.yahoo.com>
 <283095417.3220432.1508839004607@mail.yahoo.com>
Message-ID: <1706B7A4-1368-494F-A697-7DE2E3C25A12@ucsd.edu>


> On Oct 24, 2017, at 2:56 AM, Alaios via R-help <r-help at r-project.org> wrote:
> 
> Hi all,I would like to draw a simple circle where the color gradient follows the rule color = 1/(r^2) where r is the distance from the circle.

This is called a radial gradient fill in SVG speak.

[snip]

> but this package does not give easily color gradients and so my solutions contains 5 same colors filled rings.


I would look at the gridSVG package. Examples of various gradient fills are here: 

https://www.stat.auckland.ac.nz/~paul/Reports/leaf/leaf.html#idp625472

HTH,

Chuck

From istazahn at gmail.com  Wed Oct 25 10:38:43 2017
From: istazahn at gmail.com (Ista Zahn)
Date: Wed, 25 Oct 2017 04:38:43 -0400
Subject: [R] Problem Subsetting Rows that Have NA's
In-Reply-To: <79a2cf04-dd3e-4508-2150-e496fd7bc386@gforcecable.com>
References: <79a2cf04-dd3e-4508-2150-e496fd7bc386@gforcecable.com>
Message-ID: <CA+vqiLHpxdwpMmsiX1VmSKUDRzraJtnwQoCTt52=c8uY=TbG0A@mail.gmail.com>

On Tue, Oct 24, 2017 at 3:05 PM, BooBoo <booboo at gforcecable.com> wrote:
> This has every appearance of being a bug. If it is not a bug, can someone
> tell me what I am asking for when I ask for "x[x[,2]==0,]". Thanks.

You are asking for elements of x where the second column is equal to zero.

help("==")

and

help("[")

explain what happens when missing values are involved. I agree that
the behavior is surprising, but your first instinct when you discover
something surprising should be to read the documentation, not to post
to this list. After having read the documentation you may post back
here if anything remains unclear.

Best,
Ista

>
>> #here is the toy dataset
>> x <- rbind(c(1,1),c(2,2),c(3,3),c(4,0),c(5,0),c(6,NA),
> +   c(7,NA),c(8,NA),c(9,NA),c(10,NA)
> + )
>> x
>       [,1] [,2]
>  [1,]    1    1
>  [2,]    2    2
>  [3,]    3    3
>  [4,]    4    0
>  [5,]    5    0
>  [6,]    6   NA
>  [7,]    7   NA
>  [8,]    8   NA
>  [9,]    9   NA
> [10,]   10   NA
>>
>> #it contains rows that have NA's
>> x[is.na(x[,2]),]
>      [,1] [,2]
> [1,]    6   NA
> [2,]    7   NA
> [3,]    8   NA
> [4,]    9   NA
> [5,]   10   NA
>>
>> #seems like an unreasonable answer to a reasonable question
>> x[x[,2]==0,]
>      [,1] [,2]
> [1,]    4    0
> [2,]    5    0
> [3,]   NA   NA
> [4,]   NA   NA
> [5,]   NA   NA
> [6,]   NA   NA
> [7,]   NA   NA
>>
>> #this is more what I was expecting
>> x[which(x[,2]==0),]
>      [,1] [,2]
> [1,]    4    0
> [2,]    5    0
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Wed Oct 25 13:27:02 2017
From: pdalgd at gmail.com (Peter Dalgaard)
Date: Wed, 25 Oct 2017 13:27:02 +0200
Subject: [R] as.data.frame doesn't set col.names
In-Reply-To: <96f7237c48b641fbbad902b245daff68@exch-2p-mbx-w2.ads.tamu.edu>
References: <CALRb-ocavFTRrOgh1uPtn7q0e_M70f-_WNncKAueNAoxH7O4xA@mail.gmail.com>
 <CALRb-od+pq2-_=KSOL9RJf_mB2y=YwX7ru1VJg9_W_NaZQgxMg@mail.gmail.com>
 <96f7237c48b641fbbad902b245daff68@exch-2p-mbx-w2.ads.tamu.edu>
Message-ID: <1ED27488-3B40-4F08-B892-4B013B7276C9@gmail.com>


> On 24 Oct 2017, at 22:45 , David L Carlson <dcarlson at tamu.edu> wrote:
> 
> You left out all the most important bits of information. What is yo? Are you trying to assign a data frame to a single column in another data frame? Printing head(samples) tells us nothing about what data types you have, especially if the things that look like text are really factors that were created when you used one of the read.*() functions. Use str(samples) to see what you are dealing with. 

Actually, I think there is enough information to diagnose this. The main issue is as you point out, assignment of an entire data frame to a column of another data frame:

> l <- letters[1:5]
> s <- as.data.frame(sapply(l,toupper))
> dput(s)
structure(list(`sapply(l, toupper)` = structure(1:5, .Label = c("A", 
"B", "C", "D", "E"), class = "factor")), .Names = "sapply(l, toupper)", row.names = c("a", 
"b", "c", "d", "e"), class = "data.frame")

(incidentally, setting col.names has no effect on this; notice that it is only documented as an argument to "list" and "matrix" methods, and sapply() returns a vector) 

Now, if we do this:

> dd <- data.frame(A=l)
> dd$B <- s

we end up with a data frame whose B "column" is another data frame

> dput(dd)
structure(list(A = structure(1:5, .Label = c("a", "b", "c", "d", 
"e"), class = "factor"), B = structure(list(`sapply(l, toupper)` = structure(1:5, .Label = c("A", 
"B", "C", "D", "E"), class = "factor")), .Names = "sapply(l, toupper)", row.names = c("a", 
"b", "c", "d", "e"), class = "data.frame")), .Names = c("A", 
"B"), row.names = c(NA, -5L), class = "data.frame")

in printing such data frames, the inner frame "wins" the column names, which is sensible if you consider what would happen if it had more than one column:

> dd
  A sapply(l, toupper)
1 a                  A
2 b                  B
3 c                  C
4 d                  D
5 e                  E

To get the effect that Ed probably expected, do

> dd <- data.frame(A=l)
> dd["B"] <- s
> dd
  A B
1 a A
2 b B
3 c C
4 d D
5 e E

(and notice that single-bracket indexing is crucial here)

-pd


From ericjberger at gmail.com  Wed Oct 25 14:15:49 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Wed, 25 Oct 2017 15:15:49 +0300
Subject: [R] as.data.frame doesn't set col.names
In-Reply-To: <1ED27488-3B40-4F08-B892-4B013B7276C9@gmail.com>
References: <CALRb-ocavFTRrOgh1uPtn7q0e_M70f-_WNncKAueNAoxH7O4xA@mail.gmail.com>
 <CALRb-od+pq2-_=KSOL9RJf_mB2y=YwX7ru1VJg9_W_NaZQgxMg@mail.gmail.com>
 <96f7237c48b641fbbad902b245daff68@exch-2p-mbx-w2.ads.tamu.edu>
 <1ED27488-3B40-4F08-B892-4B013B7276C9@gmail.com>
Message-ID: <CAGgJW76KobViHC6icvvGv=Qyx1vDcDxRL1YtEHw-tFx4dA5xTg@mail.gmail.com>

Hi Peter,
Thanks for contributing such a great answer. Can you please provide a
pointer to the documentation where it explains why dd$B <- s and dd["B"] <-
s have such different behavior?

(I am perfectly happy if you write the explanation but if it saves you time
to point to some reference that works fine for me.)

Regards,
Eric


On Wed, Oct 25, 2017 at 2:27 PM, Peter Dalgaard <pdalgd at gmail.com> wrote:

>
> > On 24 Oct 2017, at 22:45 , David L Carlson <dcarlson at tamu.edu> wrote:
> >
> > You left out all the most important bits of information. What is yo? Are
> you trying to assign a data frame to a single column in another data frame?
> Printing head(samples) tells us nothing about what data types you have,
> especially if the things that look like text are really factors that were
> created when you used one of the read.*() functions. Use str(samples) to
> see what you are dealing with.
>
> Actually, I think there is enough information to diagnose this. The main
> issue is as you point out, assignment of an entire data frame to a column
> of another data frame:
>
> > l <- letters[1:5]
> > s <- as.data.frame(sapply(l,toupper))
> > dput(s)
> structure(list(`sapply(l, toupper)` = structure(1:5, .Label = c("A",
> "B", "C", "D", "E"), class = "factor")), .Names = "sapply(l, toupper)",
> row.names = c("a",
> "b", "c", "d", "e"), class = "data.frame")
>
> (incidentally, setting col.names has no effect on this; notice that it is
> only documented as an argument to "list" and "matrix" methods, and sapply()
> returns a vector)
>
> Now, if we do this:
>
> > dd <- data.frame(A=l)
> > dd$B <- s
>
> we end up with a data frame whose B "column" is another data frame
>
> > dput(dd)
> structure(list(A = structure(1:5, .Label = c("a", "b", "c", "d",
> "e"), class = "factor"), B = structure(list(`sapply(l, toupper)` =
> structure(1:5, .Label = c("A",
> "B", "C", "D", "E"), class = "factor")), .Names = "sapply(l, toupper)",
> row.names = c("a",
> "b", "c", "d", "e"), class = "data.frame")), .Names = c("A",
> "B"), row.names = c(NA, -5L), class = "data.frame")
>
> in printing such data frames, the inner frame "wins" the column names,
> which is sensible if you consider what would happen if it had more than one
> column:
>
> > dd
>   A sapply(l, toupper)
> 1 a                  A
> 2 b                  B
> 3 c                  C
> 4 d                  D
> 5 e                  E
>
> To get the effect that Ed probably expected, do
>
> > dd <- data.frame(A=l)
> > dd["B"] <- s
> > dd
>   A B
> 1 a A
> 2 b B
> 3 c C
> 4 d D
> 5 e E
>
> (and notice that single-bracket indexing is crucial here)
>
> -pd
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Wed Oct 25 15:54:11 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 25 Oct 2017 09:54:11 -0400
Subject: [R] as.data.frame doesn't set col.names
In-Reply-To: <CAGgJW76KobViHC6icvvGv=Qyx1vDcDxRL1YtEHw-tFx4dA5xTg@mail.gmail.com>
References: <CALRb-ocavFTRrOgh1uPtn7q0e_M70f-_WNncKAueNAoxH7O4xA@mail.gmail.com>
 <CALRb-od+pq2-_=KSOL9RJf_mB2y=YwX7ru1VJg9_W_NaZQgxMg@mail.gmail.com>
 <96f7237c48b641fbbad902b245daff68@exch-2p-mbx-w2.ads.tamu.edu>
 <1ED27488-3B40-4F08-B892-4B013B7276C9@gmail.com>
 <CAGgJW76KobViHC6icvvGv=Qyx1vDcDxRL1YtEHw-tFx4dA5xTg@mail.gmail.com>
Message-ID: <2f120e58-77c5-0042-c668-f77fc0299b57@gmail.com>

On 25/10/2017 8:15 AM, Eric Berger wrote:
> Hi Peter,
> Thanks for contributing such a great answer. Can you please provide a
> pointer to the documentation where it explains why dd$B <- s and dd["B"] <-
> s have such different behavior?

See Introduction to R, sections 6.1 (Lists) and 6.3 (Data frames).  Note 
that dd$B is nearly the same as dd[["B"]], not dd["B"].

Duncan Murdoch


> 
> (I am perfectly happy if you write the explanation but if it saves you time
> to point to some reference that works fine for me.)
> 
> Regards,
> Eric
> 
> 
> On Wed, Oct 25, 2017 at 2:27 PM, Peter Dalgaard <pdalgd at gmail.com> wrote:
> 
>>
>>> On 24 Oct 2017, at 22:45 , David L Carlson <dcarlson at tamu.edu> wrote:
>>>
>>> You left out all the most important bits of information. What is yo? Are
>> you trying to assign a data frame to a single column in another data frame?
>> Printing head(samples) tells us nothing about what data types you have,
>> especially if the things that look like text are really factors that were
>> created when you used one of the read.*() functions. Use str(samples) to
>> see what you are dealing with.
>>
>> Actually, I think there is enough information to diagnose this. The main
>> issue is as you point out, assignment of an entire data frame to a column
>> of another data frame:
>>
>>> l <- letters[1:5]
>>> s <- as.data.frame(sapply(l,toupper))
>>> dput(s)
>> structure(list(`sapply(l, toupper)` = structure(1:5, .Label = c("A",
>> "B", "C", "D", "E"), class = "factor")), .Names = "sapply(l, toupper)",
>> row.names = c("a",
>> "b", "c", "d", "e"), class = "data.frame")
>>
>> (incidentally, setting col.names has no effect on this; notice that it is
>> only documented as an argument to "list" and "matrix" methods, and sapply()
>> returns a vector)
>>
>> Now, if we do this:
>>
>>> dd <- data.frame(A=l)
>>> dd$B <- s
>>
>> we end up with a data frame whose B "column" is another data frame
>>
>>> dput(dd)
>> structure(list(A = structure(1:5, .Label = c("a", "b", "c", "d",
>> "e"), class = "factor"), B = structure(list(`sapply(l, toupper)` =
>> structure(1:5, .Label = c("A",
>> "B", "C", "D", "E"), class = "factor")), .Names = "sapply(l, toupper)",
>> row.names = c("a",
>> "b", "c", "d", "e"), class = "data.frame")), .Names = c("A",
>> "B"), row.names = c(NA, -5L), class = "data.frame")
>>
>> in printing such data frames, the inner frame "wins" the column names,
>> which is sensible if you consider what would happen if it had more than one
>> column:
>>
>>> dd
>>    A sapply(l, toupper)
>> 1 a                  A
>> 2 b                  B
>> 3 c                  C
>> 4 d                  D
>> 5 e                  E
>>
>> To get the effect that Ed probably expected, do
>>
>>> dd <- data.frame(A=l)
>>> dd["B"] <- s
>>> dd
>>    A B
>> 1 a A
>> 2 b B
>> 3 c C
>> 4 d D
>> 5 e E
>>
>> (and notice that single-bracket indexing is crucial here)
>>
>> -pd
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From booboo at gforcecable.com  Wed Oct 25 15:57:37 2017
From: booboo at gforcecable.com (BooBoo)
Date: Wed, 25 Oct 2017 09:57:37 -0400
Subject: [R] Problem Subsetting Rows that Have NA's
In-Reply-To: <CA+vqiLHpxdwpMmsiX1VmSKUDRzraJtnwQoCTt52=c8uY=TbG0A@mail.gmail.com>
References: <79a2cf04-dd3e-4508-2150-e496fd7bc386@gforcecable.com>
 <CA+vqiLHpxdwpMmsiX1VmSKUDRzraJtnwQoCTt52=c8uY=TbG0A@mail.gmail.com>
Message-ID: <2191cfc8-201c-3421-f94f-d630cd5402e5@gforcecable.com>

On 10/25/2017 4:38 AM, Ista Zahn wrote:
> On Tue, Oct 24, 2017 at 3:05 PM, BooBoo <booboo at gforcecable.com> wrote:
>> This has every appearance of being a bug. If it is not a bug, can someone
>> tell me what I am asking for when I ask for "x[x[,2]==0,]". Thanks.
> You are asking for elements of x where the second column is equal to zero.
>
> help("==")
>
> and
>
> help("[")
>
> explain what happens when missing values are involved. I agree that
> the behavior is surprising, but your first instinct when you discover
> something surprising should be to read the documentation, not to post
> to this list. After having read the documentation you may post back
> here if anything remains unclear.
>
> Best,
> Ista
>
>>> #here is the toy dataset
>>> x <- rbind(c(1,1),c(2,2),c(3,3),c(4,0),c(5,0),c(6,NA),
>> +   c(7,NA),c(8,NA),c(9,NA),c(10,NA)
>> + )
>>> x
>>        [,1] [,2]
>>   [1,]    1    1
>>   [2,]    2    2
>>   [3,]    3    3
>>   [4,]    4    0
>>   [5,]    5    0
>>   [6,]    6   NA
>>   [7,]    7   NA
>>   [8,]    8   NA
>>   [9,]    9   NA
>> [10,]   10   NA
>>> #it contains rows that have NA's
>>> x[is.na(x[,2]),]
>>       [,1] [,2]
>> [1,]    6   NA
>> [2,]    7   NA
>> [3,]    8   NA
>> [4,]    9   NA
>> [5,]   10   NA
>>> #seems like an unreasonable answer to a reasonable question
>>> x[x[,2]==0,]
>>       [,1] [,2]
>> [1,]    4    0
>> [2,]    5    0
>> [3,]   NA   NA
>> [4,]   NA   NA
>> [5,]   NA   NA
>> [6,]   NA   NA
>> [7,]   NA   NA
>>> #this is more what I was expecting
>>> x[which(x[,2]==0),]
>>       [,1] [,2]
>> [1,]    4    0
>> [2,]    5    0
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

I wanted to know if this was a bug so that I could report it if so. You 
say it is not, so you answered my question. As far as me not reading the 
documentation, I challenge anyone to read the cited help pages and 
predict the observed behavior based on the information given in those 
pages.


From dwinsemius at comcast.net  Wed Oct 25 20:17:15 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 25 Oct 2017 11:17:15 -0700
Subject: [R] Problem Subsetting Rows that Have NA's
In-Reply-To: <2191cfc8-201c-3421-f94f-d630cd5402e5@gforcecable.com>
References: <79a2cf04-dd3e-4508-2150-e496fd7bc386@gforcecable.com>
 <CA+vqiLHpxdwpMmsiX1VmSKUDRzraJtnwQoCTt52=c8uY=TbG0A@mail.gmail.com>
 <2191cfc8-201c-3421-f94f-d630cd5402e5@gforcecable.com>
Message-ID: <634DC65A-2043-4742-A7A0-EA942FCA8842@comcast.net>


> On Oct 25, 2017, at 6:57 AM, BooBoo <booboo at gforcecable.com> wrote:
> 
> On 10/25/2017 4:38 AM, Ista Zahn wrote:
>> On Tue, Oct 24, 2017 at 3:05 PM, BooBoo <booboo at gforcecable.com> wrote:
>>> This has every appearance of being a bug. If it is not a bug, can someone
>>> tell me what I am asking for when I ask for "x[x[,2]==0,]". Thanks.
>> You are asking for elements of x where the second column is equal to zero.
>> 
>> help("==")
>> 
>> and
>> 
>> help("[")
>> 
>> explain what happens when missing values are involved. I agree that
>> the behavior is surprising, but your first instinct when you discover
>> something surprising should be to read the documentation, not to post
>> to this list. After having read the documentation you may post back
>> here if anything remains unclear.
>> 
>> Best,
>> Ista
>> 
>>>> #here is the toy dataset
>>>> x <- rbind(c(1,1),c(2,2),c(3,3),c(4,0),c(5,0),c(6,NA),
>>> +   c(7,NA),c(8,NA),c(9,NA),c(10,NA)
>>> + )
>>>> x
>>>       [,1] [,2]
>>>  [1,]    1    1
>>>  [2,]    2    2
>>>  [3,]    3    3
>>>  [4,]    4    0
>>>  [5,]    5    0
>>>  [6,]    6   NA
>>>  [7,]    7   NA
>>>  [8,]    8   NA
>>>  [9,]    9   NA
>>> [10,]   10   NA
>>>> #it contains rows that have NA's
>>>> x[is.na(x[,2]),]
>>>      [,1] [,2]
>>> [1,]    6   NA
>>> [2,]    7   NA
>>> [3,]    8   NA
>>> [4,]    9   NA
>>> [5,]   10   NA
>>>> #seems like an unreasonable answer to a reasonable question
>>>> x[x[,2]==0,]
>>>      [,1] [,2]
>>> [1,]    4    0
>>> [2,]    5    0
>>> [3,]   NA   NA
>>> [4,]   NA   NA
>>> [5,]   NA   NA
>>> [6,]   NA   NA
>>> [7,]   NA   NA
>>>> #this is more what I was expecting
>>>> x[which(x[,2]==0),]
>>>      [,1] [,2]
>>> [1,]    4    0
>>> [2,]    5    0
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
> I wanted to know if this was a bug so that I could report it if so. You say it is not, so you answered my question. As far as me not reading the documentation, I challenge anyone to read the cited help pages and predict the observed behavior based on the information given in those pages.

Some of us do share (or at least remember feeling) your pain. The ?Extract page is long and complex and there are several features that I find non-intuitive. But they are deemed desirable by others. I think I needed to read that page about ten times (with multiple different problems that needed explication) before it started to sink in. You are apparently on that same side of the split opinions on the feature of returning rows with logical NA's as I am. I've learned to use `which`, and I push back when the conoscienti says it's not needed.

 After you read it a few more times you may come to a different opinion. Many people come to R with preconceived notions of what words like "equals" or "list" or "vector" mean and then complain about the documentation. You would be better advised to spend more time studying the language. The help pages are precise but terse, and you need to spend time with the examples and with other tutorial material to recognize the gotcha's.

Here's a couple of possibly helpful rules regarding "[[" and "[" and logical indexing:

Nothing _equals_ NA.
Selection operations with NA logical index item return NA.  (Justified as a warning feature as I understand it.)
"[" always returns a list.
"[[" returns only one thing, but even that thing could be a list.
Generally you want "[[" if you plan on testing for equality with a vector.

The "R Inferno" by Burns is an effort to detail many more of the unexpected or irregular aspects of R (mostly inherited from S).

-- 
Best of luck in your studies.


> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From bgunter.4567 at gmail.com  Wed Oct 25 20:27:33 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 25 Oct 2017 11:27:33 -0700
Subject: [R] Problem Subsetting Rows that Have NA's
In-Reply-To: <634DC65A-2043-4742-A7A0-EA942FCA8842@comcast.net>
References: <79a2cf04-dd3e-4508-2150-e496fd7bc386@gforcecable.com>
 <CA+vqiLHpxdwpMmsiX1VmSKUDRzraJtnwQoCTt52=c8uY=TbG0A@mail.gmail.com>
 <2191cfc8-201c-3421-f94f-d630cd5402e5@gforcecable.com>
 <634DC65A-2043-4742-A7A0-EA942FCA8842@comcast.net>
Message-ID: <CAGxFJbS2oWdXbJiYnttLBMjeRSkNPhzs9oOejz78UHXzEi7pnQ@mail.gmail.com>

... Just to be clear:

David's end summary

"[" always returns a list.
"[[" returns only one thing, but even that thing could be a list.
Generally you want "[[" if you plan on testing for equality with a vector.


applies to indexing on a **list**, of course, and not to vectors, matrices,
etc.

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Wed, Oct 25, 2017 at 11:17 AM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Oct 25, 2017, at 6:57 AM, BooBoo <booboo at gforcecable.com> wrote:
> >
> > On 10/25/2017 4:38 AM, Ista Zahn wrote:
> >> On Tue, Oct 24, 2017 at 3:05 PM, BooBoo <booboo at gforcecable.com> wrote:
> >>> This has every appearance of being a bug. If it is not a bug, can
> someone
> >>> tell me what I am asking for when I ask for "x[x[,2]==0,]". Thanks.
> >> You are asking for elements of x where the second column is equal to
> zero.
> >>
> >> help("==")
> >>
> >> and
> >>
> >> help("[")
> >>
> >> explain what happens when missing values are involved. I agree that
> >> the behavior is surprising, but your first instinct when you discover
> >> something surprising should be to read the documentation, not to post
> >> to this list. After having read the documentation you may post back
> >> here if anything remains unclear.
> >>
> >> Best,
> >> Ista
> >>
> >>>> #here is the toy dataset
> >>>> x <- rbind(c(1,1),c(2,2),c(3,3),c(4,0),c(5,0),c(6,NA),
> >>> +   c(7,NA),c(8,NA),c(9,NA),c(10,NA)
> >>> + )
> >>>> x
> >>>       [,1] [,2]
> >>>  [1,]    1    1
> >>>  [2,]    2    2
> >>>  [3,]    3    3
> >>>  [4,]    4    0
> >>>  [5,]    5    0
> >>>  [6,]    6   NA
> >>>  [7,]    7   NA
> >>>  [8,]    8   NA
> >>>  [9,]    9   NA
> >>> [10,]   10   NA
> >>>> #it contains rows that have NA's
> >>>> x[is.na(x[,2]),]
> >>>      [,1] [,2]
> >>> [1,]    6   NA
> >>> [2,]    7   NA
> >>> [3,]    8   NA
> >>> [4,]    9   NA
> >>> [5,]   10   NA
> >>>> #seems like an unreasonable answer to a reasonable question
> >>>> x[x[,2]==0,]
> >>>      [,1] [,2]
> >>> [1,]    4    0
> >>> [2,]    5    0
> >>> [3,]   NA   NA
> >>> [4,]   NA   NA
> >>> [5,]   NA   NA
> >>> [6,]   NA   NA
> >>> [7,]   NA   NA
> >>>> #this is more what I was expecting
> >>>> x[which(x[,2]==0),]
> >>>      [,1] [,2]
> >>> [1,]    4    0
> >>> [2,]    5    0
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >
> > I wanted to know if this was a bug so that I could report it if so. You
> say it is not, so you answered my question. As far as me not reading the
> documentation, I challenge anyone to read the cited help pages and predict
> the observed behavior based on the information given in those pages.
>
> Some of us do share (or at least remember feeling) your pain. The ?Extract
> page is long and complex and there are several features that I find
> non-intuitive. But they are deemed desirable by others. I think I needed to
> read that page about ten times (with multiple different problems that
> needed explication) before it started to sink in. You are apparently on
> that same side of the split opinions on the feature of returning rows with
> logical NA's as I am. I've learned to use `which`, and I push back when the
> conoscienti says it's not needed.
>
>  After you read it a few more times you may come to a different opinion.
> Many people come to R with preconceived notions of what words like "equals"
> or "list" or "vector" mean and then complain about the documentation. You
> would be better advised to spend more time studying the language. The help
> pages are precise but terse, and you need to spend time with the examples
> and with other tutorial material to recognize the gotcha's.
>
> Here's a couple of possibly helpful rules regarding "[[" and "[" and
> logical indexing:
>
> Nothing _equals_ NA.
> Selection operations with NA logical index item return NA.  (Justified as
> a warning feature as I understand it.)
> "[" always returns a list.
> "[[" returns only one thing, but even that thing could be a list.
> Generally you want "[[" if you plan on testing for equality with a vector.
>
> The "R Inferno" by Burns is an effort to detail many more of the
> unexpected or irregular aspects of R (mostly inherited from S).
>
> --
> Best of luck in your studies.
>
>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> 'Any technology distinguishable from magic is insufficiently advanced.'
>  -Gehm's Corollary to Clarke's Third Law
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Oct 25 20:35:10 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 25 Oct 2017 11:35:10 -0700
Subject: [R] Problem Subsetting Rows that Have NA's
In-Reply-To: <634DC65A-2043-4742-A7A0-EA942FCA8842@comcast.net>
References: <79a2cf04-dd3e-4508-2150-e496fd7bc386@gforcecable.com>
 <CA+vqiLHpxdwpMmsiX1VmSKUDRzraJtnwQoCTt52=c8uY=TbG0A@mail.gmail.com>
 <2191cfc8-201c-3421-f94f-d630cd5402e5@gforcecable.com>
 <634DC65A-2043-4742-A7A0-EA942FCA8842@comcast.net>
Message-ID: <EB42BF2A-999D-4616-9687-BC165300DB00@comcast.net>


> On Oct 25, 2017, at 11:17 AM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Oct 25, 2017, at 6:57 AM, BooBoo <booboo at gforcecable.com> wrote:
>> 
>> On 10/25/2017 4:38 AM, Ista Zahn wrote:
>>> On Tue, Oct 24, 2017 at 3:05 PM, BooBoo <booboo at gforcecable.com> wrote:
>>>> This has every appearance of being a bug. If it is not a bug, can someone
>>>> tell me what I am asking for when I ask for "x[x[,2]==0,]". Thanks.
>>> You are asking for elements of x where the second column is equal to zero.
>>> 
>>> help("==")
>>> 
>>> and
>>> 
>>> help("[")
>>> 
>>> explain what happens when missing values are involved. I agree that
>>> the behavior is surprising, but your first instinct when you discover
>>> something surprising should be to read the documentation, not to post
>>> to this list. After having read the documentation you may post back
>>> here if anything remains unclear.
>>> 
>>> Best,
>>> Ista
>>> 
>>>>> #here is the toy dataset
>>>>> x <- rbind(c(1,1),c(2,2),c(3,3),c(4,0),c(5,0),c(6,NA),
>>>> +   c(7,NA),c(8,NA),c(9,NA),c(10,NA)
>>>> + )
>>>>> x
>>>>      [,1] [,2]
>>>> [1,]    1    1
>>>> [2,]    2    2
>>>> [3,]    3    3
>>>> [4,]    4    0
>>>> [5,]    5    0
>>>> [6,]    6   NA
>>>> [7,]    7   NA
>>>> [8,]    8   NA
>>>> [9,]    9   NA
>>>> [10,]   10   NA
>>>>> #it contains rows that have NA's
>>>>> x[is.na(x[,2]),]
>>>>     [,1] [,2]
>>>> [1,]    6   NA
>>>> [2,]    7   NA
>>>> [3,]    8   NA
>>>> [4,]    9   NA
>>>> [5,]   10   NA
>>>>> #seems like an unreasonable answer to a reasonable question
>>>>> x[x[,2]==0,]
>>>>     [,1] [,2]
>>>> [1,]    4    0
>>>> [2,]    5    0
>>>> [3,]   NA   NA
>>>> [4,]   NA   NA
>>>> [5,]   NA   NA
>>>> [6,]   NA   NA
>>>> [7,]   NA   NA
>>>>> #this is more what I was expecting
>>>>> x[which(x[,2]==0),]
>>>>     [,1] [,2]
>>>> [1,]    4    0
>>>> [2,]    5    0
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> I wanted to know if this was a bug so that I could report it if so. You say it is not, so you answered my question. As far as me not reading the documentation, I challenge anyone to read the cited help pages and predict the observed behavior based on the information given in those pages.
> 
> Some of us do share (or at least remember feeling) your pain. The ?Extract page is long and complex and there are several features that I find non-intuitive. But they are deemed desirable by others. I think I needed to read that page about ten times (with multiple different problems that needed explication) before it started to sink in. You are apparently on that same side of the split opinions on the feature of returning rows with logical NA's as I am. I've learned to use `which`, and I push back when the conoscienti says it's not needed.


horrible misspelling of cognoscenti 


> After you read it a few more times you may come to a different opinion. Many people come to R with preconceived notions of what words like "equals" or "list" or "vector" mean and then complain about the documentation. You would be better advised to spend more time studying the language. The help pages are precise but terse, and you need to spend time with the examples and with other tutorial material to recognize the gotcha's.
> 
> Here's a couple of possibly helpful rules regarding "[[" and "[" and logical indexing:
> 
> Nothing _equals_ NA.
> Selection operations with NA logical index item return NA.  (Justified as a warning feature as I understand it.)
> "[" always returns a list.

That's not true or even half true. "[" always returns a list if it's first argument is a list and it only has two arguments.

If X is a list and you ask for X[vector] you get a list

If you ask for X[vector, ] you may get a list or a vector.

If you ask for X[two_column_matrix] you get a vector.

I should be flogged.


> "[[" returns only one thing, but even that thing could be a list.

Horribl;y imprecise.

> Generally you want "[[" if you plan on testing for equality with a vector.

Don't listen to me. Read ....
> 
> The "R Inferno" by Burns is an effort to detail many more of the unexpected or irregular aspects of R (mostly inherited from S).
> 
> -- 
> Best of luck in your studies.
> 
> 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From pdalgd at gmail.com  Wed Oct 25 22:02:36 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 25 Oct 2017 22:02:36 +0200
Subject: [R] Problem Subsetting Rows that Have NA's
In-Reply-To: <2191cfc8-201c-3421-f94f-d630cd5402e5@gforcecable.com>
References: <79a2cf04-dd3e-4508-2150-e496fd7bc386@gforcecable.com>
 <CA+vqiLHpxdwpMmsiX1VmSKUDRzraJtnwQoCTt52=c8uY=TbG0A@mail.gmail.com>
 <2191cfc8-201c-3421-f94f-d630cd5402e5@gforcecable.com>
Message-ID: <DA4785F5-EE7B-44C2-97F9-B7B1B98994CB@gmail.com>

It's not a bug, and the rationale has been hashed over since the beginning of time...

It is a bit of an annoyance in some contexts and part of the rationale for the existence of subset().

If you need an explanation, start with elementary vector indexing:

colors <- c("red", "green", "blue")
colors[c(1,3,2,NA,3)]

You pretty clearly want the result to be a vector of length 5 with 4th element NA, right?

Same story if you index into a data frame: 

> airquality[c(1,3,2,NA,2),]
    Ozone Solar.R Wind Temp Month Day
1      41     190  7.4   67     5   1
3      12     149 12.6   74     5   3
2      36     118  8.0   72     5   2
NA     NA      NA   NA   NA    NA  NA
2.1    36     118  8.0   72     5   2

Now, that's not an argument that you also get NA rows from logical indexing, but then comes the issue of automatic coercion: In colors[NA], the NA is actually mode "logical". If we removed NA indexes in logical indexing, we would have to explain why colors[c(1,NA)] has length 2 but colors[NA] has length zero (which it currently does not). 

-pd

> On 25 Oct 2017, at 15:57 , BooBoo <booboo at gforcecable.com> wrote:
> 
> On 10/25/2017 4:38 AM, Ista Zahn wrote:
>> On Tue, Oct 24, 2017 at 3:05 PM, BooBoo <booboo at gforcecable.com> wrote:
>>> This has every appearance of being a bug. If it is not a bug, can someone
>>> tell me what I am asking for when I ask for "x[x[,2]==0,]". Thanks.
>> You are asking for elements of x where the second column is equal to zero.
>> 
>> help("==")
>> 
>> and
>> 
>> help("[")
>> 
>> explain what happens when missing values are involved. I agree that
>> the behavior is surprising, but your first instinct when you discover
>> something surprising should be to read the documentation, not to post
>> to this list. After having read the documentation you may post back
>> here if anything remains unclear.
>> 
>> Best,
>> Ista
>> 
>>>> #here is the toy dataset
>>>> x <- rbind(c(1,1),c(2,2),c(3,3),c(4,0),c(5,0),c(6,NA),
>>> +   c(7,NA),c(8,NA),c(9,NA),c(10,NA)
>>> + )
>>>> x
>>>       [,1] [,2]
>>>  [1,]    1    1
>>>  [2,]    2    2
>>>  [3,]    3    3
>>>  [4,]    4    0
>>>  [5,]    5    0
>>>  [6,]    6   NA
>>>  [7,]    7   NA
>>>  [8,]    8   NA
>>>  [9,]    9   NA
>>> [10,]   10   NA
>>>> #it contains rows that have NA's
>>>> x[is.na(x[,2]),]
>>>      [,1] [,2]
>>> [1,]    6   NA
>>> [2,]    7   NA
>>> [3,]    8   NA
>>> [4,]    9   NA
>>> [5,]   10   NA
>>>> #seems like an unreasonable answer to a reasonable question
>>>> x[x[,2]==0,]
>>>      [,1] [,2]
>>> [1,]    4    0
>>> [2,]    5    0
>>> [3,]   NA   NA
>>> [4,]   NA   NA
>>> [5,]   NA   NA
>>> [6,]   NA   NA
>>> [7,]   NA   NA
>>>> #this is more what I was expecting
>>>> x[which(x[,2]==0),]
>>>      [,1] [,2]
>>> [1,]    4    0
>>> [2,]    5    0
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
> I wanted to know if this was a bug so that I could report it if so. You say it is not, so you answered my question. As far as me not reading the documentation, I challenge anyone to read the cited help pages and predict the observed behavior based on the information given in those pages.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From macqueen1 at llnl.gov  Wed Oct 25 23:04:46 2017
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Wed, 25 Oct 2017 21:04:46 +0000
Subject: [R] How to save and restore a workspace
In-Reply-To: <CA+8X3fUdDHpW3-Kip94X8Zz85zMSnQ54b-H7MP5hQWpbUg18fw@mail.gmail.com>
References: <C12851BD-8049-409A-B025-DA6222E955B5@gmail.com>
 <CA+8X3fUdDHpW3-Kip94X8Zz85zMSnQ54b-H7MP5hQWpbUg18fw@mail.gmail.com>
Message-ID: <389CBD89-08A8-4EE2-A9E2-0ED81C89AE5B@llnl.gov>

Saving your workspace means that the variables you currently have defined in your session [ everything that shows up when you type ls() ] are saved to a file, by default named ?.RData?. To restore the workspace, you use the ?Load Workspace? command and navigate to the (same) .RData file. Its default location for Windows, as far as I know, is your ?Documents? folder. So look there.

I see that you tried the Load Workspace command, and didn?t get what you were looking for. I just tested what I described above, and it did work. So either you didn?t navigate to and open .RData, or you have some other idea about what saving and loading a workspace means. Hopefully, it?s the former!

With some effort, you can learn to save different .RData files for different projects; for that see the R Windows FAQ.

By the way, this is much easier on Linux, and on Mac when working in a Terminal (command line) environment.

-Don

--
Don MacQueen
Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062
Lab cell 925-724-7509
 
 

On 10/23/17, 3:45 PM, "R-help on behalf of Jim Lemon" <r-help-bounces at r-project.org on behalf of drjimlemon at gmail.com> wrote:

    Hi Jon,
    Saving your workspace doesn't mean that everything will be rerun when
    you start a new R session. I just means that persistent objects like
    data frames will be there. If you type:
    
    objects()
    
    you will see all of those things that were there when you ended in the
    last session. Things like commands will be in the "history", so you
    can retrieve them just as you did at the end of the last session (up
    arrow). It may be a better solution if you save sequences of commands
    as R script files (e.g. "something.R") as you can run them:
    
    source("something.R")
    
    and edit and save them again.
    
    Jim
    
    On Tue, Oct 24, 2017 at 9:12 AM,  <jonnvan at gmail.com> wrote:
    > Hello,
    > I recently downloaded R in hopes of learning to use it for statistics.
    > I have promptly run into a problem, as I am unable to save, and later recover, a workspace so I can resume work where I left off.
    > I am using Windows.
    > I indicate "yes" to the pop up after q().  Then when I later reopen R Console and click on File, I cannot get my prior workspace to appear in the R Console frame so I can resume work.
    > In the File drop down menu I have tried Load Workspace, Load History, Display file(s)..., opened R Type: R Workspace with no luck.
    > I have read about this in two different books, the R Manual, and R FAQs, used the RGui help function, and still cannot do it.
    > I have used Windows for years, but I am ignorant about programming.
    > Would appreciate any help you might offer.
    > I live in the Denver area, so if there are any local resources you could direct me to, I would be grateful for that as well.
    >
    > Thank you,
    > Jon VanDeventer
    >
    > Sent from my iPad
    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.
    
    ______________________________________________
    R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    https://stat.ethz.ch/mailman/listinfo/r-help
    PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    and provide commented, minimal, self-contained, reproducible code.
    


From jwd at surewest.net  Thu Oct 26 00:59:13 2017
From: jwd at surewest.net (John)
Date: Wed, 25 Oct 2017 15:59:13 -0700
Subject: [R] Problem Subsetting Rows that Have NA's
In-Reply-To: <79a2cf04-dd3e-4508-2150-e496fd7bc386@gforcecable.com>
References: <79a2cf04-dd3e-4508-2150-e496fd7bc386@gforcecable.com>
Message-ID: <20171025155913.78f3410b@Draco.localdomain>

On Tue, 24 Oct 2017 15:05:01 -0400
BooBoo <booboo at gforcecable.com> wrote:

> This has every appearance of being a bug. If it is not a bug, can 
> someone tell me what I am asking for when I ask for "x[x[,2]==0,]".
> Thanks.
> 
As others have pointed out not a bug, but very "unintuitively"
explained in the documentation.  On the other hand, be glad it isn't
a .dbf file and that you are not using dBASE.  Ancient history of
course, but dBASE used to convert missing data (NAs) into 0s when you
weren't looking. If you were extremely unlucky and stuck in the role of
autodidact, your success in both noticing and identifying the problem
could be a long time coming.  In the meantime, your instructors might
be numerate, and point out rudely your inability to carry out even
simple calculations of averages, or they could be innumerate and
happily accept your mistakes as gospel because you included
"quantified" results in you paper.

Good luck.


From tea3rd at gmail.com  Thu Oct 26 06:25:53 2017
From: tea3rd at gmail.com (Thomas Adams)
Date: Thu, 26 Oct 2017 00:25:53 -0400
Subject: [R] Help needed with aggregate or other solution
Message-ID: <CAGxgkWjb8kZkt16OjvzZHZjTG_vbxvjAkfPov=pF9M5MN8vMOQ@mail.gmail.com>

Hello all!

I've been struggling with is for many hours today; I'm close to getting
what I want, but not close enough...

I have a dataframe consisting of two date-time columns followed by two
numeric columns. what I need is the max value (in the first numeric column)
based on the 2nd date-time column, which is essentially a factor. But, I
want the result to provide both date-time values corresponding to the max
value.

My data:

structure(list(date = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
19L, 20L, 21L, 22L, 23L, 24L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
14L, 15L, 16L), .Label = c("2012-01-25 18:00:00", "2012-01-26 00:00:00",
"2012-01-26 06:00:00", "2012-01-26 12:00:00", "2012-01-26 18:00:00",
"2012-01-27 00:00:00", "2012-01-27 06:00:00", "2012-01-27 12:00:00",
"2012-01-27 18:00:00", "2012-01-28 00:00:00", "2012-01-28 06:00:00",
"2012-01-28 12:00:00", "2012-01-28 18:00:00", "2012-01-29 00:00:00",
"2012-01-29 06:00:00", "2012-01-29 12:00:00", "2012-01-29 18:00:00",
"2012-01-30 00:00:00", "2012-01-30 06:00:00", "2012-01-30 12:00:00",
"2012-01-30 18:00:00", "2012-01-31 00:00:00", "2012-01-31 06:00:00",
"2012-01-31 12:00:00", "2012-01-31 13:00:00", "2012-01-31 18:00:00",
"2012-02-01 00:00:00", "2012-02-01 06:00:00", "2012-02-01 12:00:00",
"2012-02-01 18:00:00", "2012-02-02 00:00:00", "2012-02-02 06:00:00",
"2012-02-02 12:00:00", "2012-02-02 18:00:00", "2012-02-03 00:00:00",
"2012-02-03 06:00:00", "2012-02-03 12:00:00", "2012-02-03 18:00:00",
"2012-02-04 00:00:00", "2012-02-04 06:00:00", "2012-02-04 12:00:00",
"2012-02-04 18:00:00", "2012-02-05 00:00:00", "2012-02-05 06:00:00",
"2012-02-05 12:00:00", "2012-02-05 18:00:00", "2012-02-06 00:00:00",
"2012-02-06 06:00:00", "2012-02-06 12:00:00", "2012-02-06 18:00:00",
"2012-02-07 00:00:00", "2012-02-07 06:00:00", "2012-02-07 12:00:00",
"2012-02-07 18:00:00", "2012-02-08 00:00:00", "2012-02-08 06:00:00",
"2012-02-08 12:00:00", "2012-02-08 18:00:00", "2012-02-09 00:00:00",
"2012-02-09 06:00:00", "2012-02-09 12:00:00", "2012-02-09 18:00:00",
"2012-02-10 00:00:00", "2012-02-10 06:00:00", "2012-02-10 12:00:00",
"2012-02-10 18:00:00", "2012-02-11 00:00:00", "2012-02-11 06:00:00",
"2012-02-11 12:00:00", "2012-02-11 18:00:00", "2012-02-12 00:00:00",
"2012-02-12 06:00:00", "2012-02-12 12:00:00", "2012-02-12 18:00:00",
"2012-02-13 00:00:00", "2012-02-13 06:00:00", "2012-02-13 12:00:00",
"2012-02-13 18:00:00", "2012-02-14 00:00:00", "2012-02-14 06:00:00",
"2012-02-14 12:00:00", "2012-02-14 18:00:00", "2012-02-15 00:00:00",
"2012-02-15 06:00:00", "2012-02-15 12:00:00", "2012-02-15 18:00:00",
"2012-02-16 00:00:00", "2012-02-16 06:00:00", "2012-02-16 12:00:00",
"2012-02-16 18:00:00", "2012-02-17 00:00:00", "2012-02-17 06:00:00",
"2012-02-17 12:00:00", "2012-02-17 18:00:00", "2012-02-18 00:00:00",
"2012-02-18 06:00:00", "2012-02-18 12:00:00", "2012-02-18 18:00:00",
"2012-02-19 00:00:00", "2012-02-19 06:00:00", "2012-02-19 12:00:00",
"2012-02-19 18:00:00", "2012-02-20 00:00:00", "2012-02-20 06:00:00",
"2012-02-20 12:00:00", "2012-02-20 18:00:00", "2012-02-21 00:00:00",
"2012-02-21 06:00:00", "2012-02-21 12:00:00", "2012-02-21 18:00:00",
"2012-02-22 00:00:00", "2012-02-22 06:00:00", "2012-02-22 12:00:00",
"2012-02-22 18:00:00", "2012-02-23 00:00:00", "2012-02-23 06:00:00",
"2012-02-23 12:00:00", "2012-02-23 18:00:00", "2012-02-24 00:00:00",
"2012-02-24 06:00:00", "2012-02-24 12:00:00", "2012-02-24 18:00:00",
"2012-02-25 00:00:00", "2012-02-25 06:00:00", "2012-02-25 12:00:00",
"2012-02-25 18:00:00", "2012-02-26 00:00:00", "2012-02-26 06:00:00",
"2012-02-26 12:00:00", "2012-02-26 18:00:00", "2012-02-27 00:00:00",
"2012-02-27 06:00:00", "2012-02-27 12:00:00", "2012-02-27 18:00:00",
"2012-02-28 00:00:00", "2012-02-28 06:00:00", "2012-02-28 12:00:00",
"2012-02-28 18:00:00", "2012-02-29 00:00:00", "2012-02-29 06:00:00",
"2012-02-29 12:00:00", "2012-02-29 18:00:00", "2012-03-01 00:00:00",
"2012-03-01 06:00:00", "2012-03-01 12:00:00", "2012-03-01 18:00:00",
"2012-03-02 00:00:00", "2012-03-02 06:00:00", "2012-03-02 12:00:00",
"2012-03-02 18:00:00", "2012-03-03 00:00:00", "2012-03-03 06:00:00",
"2012-03-03 12:00:00", "2012-03-03 18:00:00", "2012-03-04 00:00:00",
"2012-03-04 06:00:00", "2012-03-04 12:00:00", "2012-03-04 18:00:00",
"2012-03-05 00:00:00", "2012-03-05 06:00:00", "2012-03-05 12:00:00",
"2012-03-05 18:00:00", "2012-03-06 00:00:00", "2012-03-06 06:00:00",
"2012-03-06 12:00:00", "2012-03-06 18:00:00", "2012-03-07 00:00:00",
"2012-03-07 06:00:00", "2012-03-07 12:00:00", "2012-03-07 18:00:00",
"2012-03-08 00:00:00", "2012-03-08 06:00:00", "2012-03-08 12:00:00",
"2012-03-08 18:00:00", "2012-03-09 00:00:00", "2012-03-09 06:00:00",
"2012-03-09 12:00:00", "2012-03-09 18:00:00", "2012-03-10 00:00:00",
"2012-03-10 06:00:00", "2012-03-10 12:00:00", "2012-03-10 18:00:00",
"2012-03-11 00:00:00", "2012-03-11 06:00:00", "2012-03-11 12:00:00",
"2012-03-11 18:00:00", "2012-03-12 00:00:00", "2012-03-12 06:00:00",
"2012-03-12 12:00:00", "2012-03-12 18:00:00", "2012-03-13 00:00:00",
"2012-03-13 06:00:00", "2012-03-13 12:00:00", "2012-03-13 18:00:00",
"2012-03-14 00:00:00", "2012-03-14 06:00:00", "2012-03-14 12:00:00",
"2012-03-14 18:00:00", "2012-03-15 00:00:00", "2012-03-15 06:00:00",
"2012-03-15 12:00:00", "2012-03-15 18:00:00", "2012-03-16 00:00:00",
"2012-03-16 06:00:00", "2012-03-16 12:00:00", "2012-03-16 18:00:00",
"2012-03-17 00:00:00", "2012-03-17 06:00:00", "2012-03-17 12:00:00",
"2012-03-17 18:00:00", "2012-03-18 00:00:00", "2012-03-18 06:00:00",
"2012-03-18 12:00:00", "2012-03-18 18:00:00", "2012-03-19 00:00:00",
"2012-03-19 06:00:00", "2012-03-19 12:00:00", "2012-03-19 18:00:00",
"2012-03-20 00:00:00", "2012-03-20 06:00:00", "2012-03-20 12:00:00",
"2012-03-20 18:00:00", "2012-03-21 00:00:00", "2012-03-21 06:00:00",
"2012-03-21 12:00:00", "2012-03-21 18:00:00", "2012-03-22 00:00:00",
"2012-03-22 06:00:00", "2012-03-22 12:00:00", "2012-03-22 18:00:00",
"2012-03-23 00:00:00", "2012-03-23 06:00:00", "2012-03-23 12:00:00",
"2012-03-23 18:00:00", "2012-03-24 00:00:00", "2012-03-24 06:00:00",
"2012-03-24 12:00:00", "2012-03-24 18:00:00", "2012-03-25 00:00:00",
"2012-03-25 06:00:00", "2012-03-25 12:00:00", "2012-03-25 18:00:00",
"2012-03-26 00:00:00", "2012-03-26 06:00:00", "2012-03-26 12:00:00",
"2012-03-26 18:00:00", "2012-03-27 00:00:00", "2012-03-27 06:00:00",
"2012-03-27 12:00:00", "2012-03-27 18:00:00", "2012-03-28 00:00:00",
"2012-03-28 06:00:00", "2012-03-28 12:00:00", "2012-03-28 18:00:00",
"2012-03-29 00:00:00", "2012-03-29 06:00:00", "2012-03-29 12:00:00",
"2012-03-29 18:00:00", "2012-03-30 00:00:00", "2012-03-30 06:00:00",
"2012-03-30 12:00:00", "2012-03-30 18:00:00", "2012-03-31 00:00:00",
"2012-03-31 06:00:00", "2012-03-31 12:00:00", "2012-03-31 18:00:00",
"2012-04-01 00:00:00", "2012-04-01 06:00:00", "2012-04-01 12:00:00",
"2012-04-01 18:00:00", "2012-04-02 00:00:00", "2012-04-02 06:00:00",
"2012-04-02 12:00:00", "2012-04-02 18:00:00", "2012-04-03 00:00:00",
"2012-04-03 06:00:00", "2012-04-03 12:00:00", "2012-04-03 18:00:00",
"2012-04-04 00:00:00", "2012-04-04 06:00:00", "2012-04-04 12:00:00",
"2012-04-04 18:00:00", "2012-04-05 00:00:00", "2012-04-05 06:00:00",
"2012-04-05 12:00:00", "2012-04-05 18:00:00", "2012-04-06 00:00:00",
"2012-04-06 06:00:00", "2012-04-06 12:00:00", "2012-04-06 18:00:00",
"2012-04-07 00:00:00", "2012-04-07 06:00:00", "2012-04-07 12:00:00",
"2012-04-07 18:00:00", "2012-04-08 00:00:00", "2012-04-08 06:00:00",
"2012-04-08 12:00:00", "2012-04-08 18:00:00", "2012-04-09 00:00:00",
"2012-04-09 06:00:00", "2012-04-09 12:00:00", "2012-04-09 18:00:00",
"2012-04-10 00:00:00", "2012-04-10 06:00:00", "2012-04-10 12:00:00",
"2012-04-10 18:00:00", "2012-04-11 00:00:00", "2012-04-11 06:00:00",
"2012-04-11 12:00:00", "2012-04-11 18:00:00", "2012-04-12 00:00:00",
"2012-04-12 06:00:00", "2012-04-12 12:00:00", "2012-04-12 18:00:00",
"2012-04-13 00:00:00", "2012-04-13 06:00:00", "2012-04-13 12:00:00",
"2012-04-13 18:00:00", "2012-04-14 00:00:00", "2012-04-14 06:00:00",
"2012-04-14 12:00:00", "2012-04-14 18:00:00", "2012-04-15 00:00:00",
"2012-04-15 06:00:00", "2012-04-15 12:00:00", "2012-04-15 18:00:00",
"2012-04-16 00:00:00", "2012-04-16 06:00:00", "2012-04-16 12:00:00",
"2012-04-16 18:00:00", "2012-04-17 00:00:00", "2012-04-17 06:00:00",
"2012-04-17 12:00:00", "2012-04-17 18:00:00", "2012-04-18 00:00:00",
"2012-04-18 06:00:00", "2012-04-18 12:00:00", "2012-04-18 18:00:00",
"2012-04-19 00:00:00", "2012-04-19 06:00:00", "2012-04-19 12:00:00",
"2012-04-19 18:00:00", "2012-04-20 00:00:00", "2012-04-20 06:00:00",
"2012-04-20 12:00:00", "2012-04-20 18:00:00", "2012-04-21 00:00:00",
"2012-04-21 06:00:00", "2012-04-21 12:00:00", "2012-04-21 18:00:00",
"2012-04-22 00:00:00", "2012-04-22 06:00:00", "2012-04-22 12:00:00",
"2012-04-22 18:00:00", "2012-04-23 00:00:00", "2012-04-23 06:00:00",
"2012-04-23 12:00:00", "2012-04-23 18:00:00", "2012-04-24 00:00:00",
"2012-04-24 06:00:00", "2012-04-24 12:00:00", "2012-04-24 18:00:00",
"2012-04-25 00:00:00", "2012-04-25 06:00:00", "2012-04-25 12:00:00",
"2012-04-25 18:00:00", "2012-04-26 00:00:00", "2012-04-26 06:00:00",
"2012-04-26 12:00:00", "2012-04-26 18:00:00", "2012-04-27 00:00:00",
"2012-04-27 06:00:00", "2012-04-27 12:00:00", "2012-04-27 18:00:00",
"2012-04-28 00:00:00", "2012-04-28 06:00:00", "2012-04-28 12:00:00",
"2012-04-28 18:00:00", "2012-04-29 00:00:00", "2012-04-29 06:00:00",
"2012-04-29 12:00:00", "2012-04-29 18:00:00", "2012-04-30 00:00:00",
"2012-04-30 06:00:00", "2012-04-30 12:00:00", "2012-04-30 18:00:00",
"2012-05-01 00:00:00", "2012-05-01 06:00:00", "2012-05-01 12:00:00",
"2012-05-01 18:00:00", "2012-05-02 00:00:00", "2012-05-02 06:00:00",
"2012-05-02 12:00:00", "2012-05-02 18:00:00", "2012-05-03 00:00:00",
"2012-05-03 06:00:00", "2012-05-03 12:00:00", "2012-05-03 18:00:00",
"2012-05-04 00:00:00", "2012-05-04 06:00:00", "2012-05-04 12:00:00",
"2012-05-04 18:00:00", "2012-05-05 00:00:00", "2012-05-05 06:00:00",
"2012-05-05 12:00:00", "2012-05-05 18:00:00", "2012-05-06 00:00:00",
"2012-05-06 06:00:00", "2012-05-06 12:00:00", "2012-05-06 18:00:00",
"2012-05-07 00:00:00", "2012-05-07 06:00:00", "2012-05-07 12:00:00",
"2012-05-07 18:00:00", "2012-05-08 00:00:00", "2012-05-08 06:00:00",
"2012-05-08 12:00:00", "2012-05-08 18:00:00", "2012-05-09 00:00:00",
"2012-05-09 06:00:00", "2012-05-09 12:00:00", "2012-05-09 18:00:00",
"2012-05-10 00:00:00", "2012-05-10 06:00:00", "2012-05-10 12:00:00",
"2012-05-10 18:00:00", "2012-05-11 00:00:00", "2012-05-11 06:00:00",
"2012-05-11 12:00:00", "2012-05-11 18:00:00", "2012-05-12 00:00:00",
"2012-05-12 06:00:00", "2012-05-12 12:00:00", "2012-05-12 18:00:00",
"2012-05-13 00:00:00", "2012-05-13 06:00:00", "2012-05-13 12:00:00",
"2012-05-13 18:00:00", "2012-05-14 00:00:00", "2012-05-14 06:00:00",
"2012-05-14 12:00:00", "2012-05-14 18:00:00", "2012-05-15 00:00:00",
"2012-05-15 06:00:00", "2012-05-15 12:00:00", "2012-05-15 18:00:00",
"2012-05-16 00:00:00", "2012-05-16 06:00:00", "2012-05-16 12:00:00",
"2012-05-16 18:00:00", "2012-05-17 00:00:00", "2012-05-17 06:00:00",
"2012-05-17 12:00:00", "2012-05-17 18:00:00", "2012-05-18 00:00:00",
"2012-05-18 06:00:00", "2012-05-18 12:00:00", "2012-05-18 18:00:00",
"2012-05-19 00:00:00", "2012-05-19 06:00:00", "2012-05-19 12:00:00",
"2012-05-19 18:00:00", "2012-05-20 00:00:00", "2012-05-20 06:00:00",
"2012-05-20 12:00:00", "2012-05-20 18:00:00", "2012-05-21 00:00:00",
"2012-05-21 06:00:00", "2012-05-21 12:00:00", "2012-05-21 18:00:00",
"2012-05-22 00:00:00", "2012-05-22 06:00:00", "2012-05-22 12:00:00",
"2012-05-22 18:00:00", "2012-05-23 00:00:00", "2012-05-23 06:00:00",
"2012-05-23 12:00:00", "2012-05-23 18:00:00", "2012-05-24 00:00:00",
"2012-05-24 06:00:00", "2012-05-24 12:00:00", "2012-05-24 18:00:00",
"2012-05-25 00:00:00", "2012-05-25 06:00:00", "2012-05-25 12:00:00",
"2012-05-25 18:00:00", "2012-05-26 00:00:00", "2012-05-26 06:00:00",
"2012-05-26 12:00:00", "2012-05-26 18:00:00", "2012-05-27 00:00:00",
"2012-05-27 06:00:00", "2012-05-27 12:00:00", "2012-05-27 18:00:00",
"2012-05-28 00:00:00", "2012-05-28 06:00:00", "2012-05-28 12:00:00",
"2012-05-28 18:00:00", "2012-05-29 00:00:00", "2012-05-29 06:00:00",
"2012-05-29 12:00:00"), class = "factor"), basistime = structure(c(1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L), .Label = c("2012-01-25 15:02:00", "2012-01-26 15:11:00",
"2012-01-27 01:41:00", "2012-01-27 10:15:00", "2012-01-27 15:15:00",
"2012-01-28 14:22:00", "2012-01-29 13:33:00", "2012-01-30 14:11:00",
"2012-01-31 14:24:00", "2012-02-01 14:55:00", "2012-02-02 14:56:00",
"2012-02-03 14:40:00", "2012-02-04 15:01:00", "2012-02-05 15:04:00",
"2012-02-06 14:37:00", "2012-02-07 14:42:00", "2012-02-08 14:44:00",
"2012-02-09 14:27:00", "2012-02-10 14:33:00", "2012-02-11 15:05:00",
"2012-02-12 15:09:00", "2012-02-13 15:00:00", "2012-02-14 15:10:00",
"2012-02-15 14:46:00", "2012-02-16 14:42:00", "2012-02-17 14:14:00",
"2012-02-18 15:27:00", "2012-02-19 13:13:00", "2012-02-20 13:56:00",
"2012-02-21 14:39:00", "2012-02-22 15:33:00", "2012-02-23 15:14:00",
"2012-02-24 15:14:00", "2012-02-25 14:25:00", "2012-02-26 14:08:00",
"2012-02-27 14:55:00", "2012-02-28 14:40:00", "2012-02-29 14:45:00",
"2012-03-01 15:18:00", "2012-03-02 15:18:00", "2012-03-03 15:43:00",
"2012-03-04 15:16:00", "2012-03-05 15:17:00", "2012-03-06 15:03:00",
"2012-03-07 14:44:00", "2012-03-08 14:53:00", "2012-03-09 15:10:00",
"2012-03-10 14:43:00", "2012-03-11 13:37:00", "2012-03-12 14:58:00",
"2012-03-13 14:36:00", "2012-03-14 13:49:00", "2012-03-15 14:32:00",
"2012-03-16 15:01:00", "2012-03-17 13:55:00", "2012-03-18 13:44:00",
"2012-03-19 13:55:00", "2012-03-20 13:53:00", "2012-03-21 14:07:00",
"2012-03-22 14:03:00", "2012-03-23 14:30:00", "2012-03-24 14:47:00",
"2012-03-25 14:32:00", "2012-03-26 14:41:00", "2012-03-27 13:51:00",
"2012-03-28 13:32:00", "2012-03-29 13:59:00", "2012-03-30 14:15:00",
"2012-03-31 14:12:00", "2012-04-01 14:30:00", "2012-04-02 13:43:00",
"2012-04-03 13:54:00", "2012-04-04 13:45:00", "2012-04-05 13:46:00",
"2012-04-06 13:41:00", "2012-04-07 13:13:00", "2012-04-08 12:59:00",
"2012-04-09 13:53:00", "2012-04-10 14:15:00", "2012-04-11 14:26:00",
"2012-04-12 14:05:00", "2012-04-13 13:37:00", "2012-04-14 13:52:00",
"2012-04-15 15:00:00", "2012-04-16 14:42:00", "2012-04-17 14:21:00",
"2012-04-18 14:40:00", "2012-04-19 14:35:00", "2012-04-20 15:14:00",
"2012-04-21 14:46:00", "2012-04-22 14:44:00", "2012-04-23 14:33:00",
"2012-04-24 14:41:00", "2012-04-25 14:13:00", "2012-04-26 14:19:00",
"2012-04-27 14:35:00", "2012-04-28 13:48:00", "2012-04-29 14:12:00",
"2012-04-30 13:53:00", "2012-05-02 14:41:00", "2012-05-03 15:01:00",
"2012-05-04 15:21:00", "2012-05-05 14:57:00", "2012-05-06 14:09:00",
"2012-05-07 14:30:00", "2012-05-08 14:05:00", "2012-05-08 21:21:00",
"2012-05-09 15:02:00", "2012-05-10 14:27:00", "2012-05-11 14:47:00",
"2012-05-12 13:24:00", "2012-05-13 14:40:00", "2012-05-13 19:24:00",
"2012-05-14 14:33:00", "2012-05-15 14:39:00", "2012-05-16 14:07:00",
"2012-05-17 13:42:00", "2012-05-18 14:30:00", "2012-05-19 13:59:00",
"2012-05-20 14:14:00", "2012-05-21 14:15:00", "2012-05-22 14:18:00",
"2012-05-23 14:14:00", "2012-05-24 13:52:00"), class = "factor"),
    fcst = c(38.7, 38.9, 39.2, 39.8, 40.5, 41.5, 42.5, 43.1,
    43.9, 44.5, 44.8, 45, 45, 45.1, 45.4, 45.8, 46.4, 47, 47.5,
    47.9, 39.2, 40, 41.8, 42.2, 42.8, 44.4, 45.7, 46.2, 46.2,
    46.2, 46.5, 47, 47.5, 47.9, 48.4, 49, 49.5, 49.9, 50.2, 50.4,
    41.3, 42.5, 42.8, 42.8, 42.8, 42.9, 43, 43.3, 43.5, 43.8),
    usgs = c(38.5, 38.6, 38.6, 38.6, 39.1, 39.8, 41.2, 42.9,
    43.4, 43.6, 43.4, 43.1, 43, 43.1, 43.5, 43.8, 44.1, 44.2,
    44.4, 44.5, 39.1, 39.8, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1,
    43, 43.1, 43.5, 43.8, 44.1, 44.2, 44.4, 44.5, 44.5, 44.5,
    44.6, 44.2, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1, 43, 43.1,
    43.5, 43.8)), .Names = c("date", "basistime", "fcst", "usgs"
), row.names = c(NA, 50L), class = "data.frame")

aggregate(fcst ~ basistime,data=legacy, FUN= max)

A snippet...

              basistime fcst
1   2012-01-25 15:02:00 47.9
2   2012-01-26 15:11:00 50.4
3   2012-01-27 01:41:00 46.0
4   2012-01-27 10:15:00 47.3
5   2012-01-27 15:15:00 47.3
6   2012-01-28 14:22:00 46.2
7   2012-01-29 13:33:00 45.8
8   2012-01-30 14:11:00 44.8
9   2012-01-31 14:24:00 43.9
10  2012-02-01 14:55:00 41.1
11  2012-02-02 14:56:00 38.1
12  2012-02-03 14:40:00 36.2
13  2012-02-04 15:01:00 34.7
14  2012-02-05 15:04:00 33.1
15  2012-02-06 14:37:00 32.2

This is what I want, except I need the other corresponding date-time column
as well. I've tried this with the date-times as factors and as POSIXct
values and using many of the combinations suggested in the documentation
and other examples. The closest I can get returns the second date as a
numeric value:

aggregate(cbind(fcst,date) ~ basistime,data=legacy, FUN= max)

Produces:

              basistime fcst       date
1   2012-01-25 15:02:00 47.9 1327942800
2   2012-01-26 15:11:00 50.4 1328029200
3   2012-01-27 01:41:00 46.0 1328072400
4   2012-01-27 10:15:00 47.3 1328032800
5   2012-01-27 15:15:00 47.3 1328115600
6   2012-01-28 14:22:00 46.2 1328202000
7   2012-01-29 13:33:00 45.8 1328288400
8   2012-01-30 14:11:00 44.8 1328374800
9   2012-01-31 14:24:00 43.9 1328461200
10  2012-02-01 14:55:00 41.1 1328547600
11  2012-02-02 14:56:00 38.1 1328634000
12  2012-02-03 14:40:00 36.2 1328720400
13  2012-02-04 15:01:00 34.7 1328806800
14  2012-02-05 15:04:00 33.1 1328893200
15  2012-02-06 14:37:00 32.2 1328979600
16  2012-02-07 14:42:00 31.2 1329066000
17  2012-02-08 14:44:00 30.4 1329152400
18  2012-02-09 14:27:00 30.0 1329238800


Help is greatly appreciated!

Regards,
Tom

--

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Thu Oct 26 07:28:35 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 25 Oct 2017 22:28:35 -0700 (PDT)
Subject: [R] Help needed with aggregate or other solution
In-Reply-To: <CAGxgkWjb8kZkt16OjvzZHZjTG_vbxvjAkfPov=pF9M5MN8vMOQ@mail.gmail.com>
References: <CAGxgkWjb8kZkt16OjvzZHZjTG_vbxvjAkfPov=pF9M5MN8vMOQ@mail.gmail.com>
Message-ID: <alpine.BSF.2.00.1710252216410.34411@pedal.dcn.davis.ca.us>

Thanks for the dput...

#### reproducible example of split-apply-combine ###

dta <- structure(list(date = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
19L, 20L, 21L, 22L, 23L, 24L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
14L, 15L, 16L), .Label = c("2012-01-25 18:00:00", "2012-01-26 00:00:00",
"2012-01-26 06:00:00", "2012-01-26 12:00:00", "2012-01-26 18:00:00",
"2012-01-27 00:00:00", "2012-01-27 06:00:00", "2012-01-27 12:00:00",
"2012-01-27 18:00:00", "2012-01-28 00:00:00", "2012-01-28 06:00:00",
"2012-01-28 12:00:00", "2012-01-28 18:00:00", "2012-01-29 00:00:00",
"2012-01-29 06:00:00", "2012-01-29 12:00:00", "2012-01-29 18:00:00",
"2012-01-30 00:00:00", "2012-01-30 06:00:00", "2012-01-30 12:00:00",
"2012-01-30 18:00:00", "2012-01-31 00:00:00", "2012-01-31 06:00:00",
"2012-01-31 12:00:00", "2012-01-31 13:00:00", "2012-01-31 18:00:00",
"2012-02-01 00:00:00", "2012-02-01 06:00:00", "2012-02-01 12:00:00",
"2012-02-01 18:00:00", "2012-02-02 00:00:00", "2012-02-02 06:00:00",
"2012-02-02 12:00:00", "2012-02-02 18:00:00", "2012-02-03 00:00:00",
"2012-02-03 06:00:00", "2012-02-03 12:00:00", "2012-02-03 18:00:00",
"2012-02-04 00:00:00", "2012-02-04 06:00:00", "2012-02-04 12:00:00",
"2012-02-04 18:00:00", "2012-02-05 00:00:00", "2012-02-05 06:00:00",
"2012-02-05 12:00:00", "2012-02-05 18:00:00", "2012-02-06 00:00:00",
"2012-02-06 06:00:00", "2012-02-06 12:00:00", "2012-02-06 18:00:00",
"2012-02-07 00:00:00", "2012-02-07 06:00:00", "2012-02-07 12:00:00",
"2012-02-07 18:00:00", "2012-02-08 00:00:00", "2012-02-08 06:00:00",
"2012-02-08 12:00:00", "2012-02-08 18:00:00", "2012-02-09 00:00:00",
"2012-02-09 06:00:00", "2012-02-09 12:00:00", "2012-02-09 18:00:00",
"2012-02-10 00:00:00", "2012-02-10 06:00:00", "2012-02-10 12:00:00",
"2012-02-10 18:00:00", "2012-02-11 00:00:00", "2012-02-11 06:00:00",
"2012-02-11 12:00:00", "2012-02-11 18:00:00", "2012-02-12 00:00:00",
"2012-02-12 06:00:00", "2012-02-12 12:00:00", "2012-02-12 18:00:00",
"2012-02-13 00:00:00", "2012-02-13 06:00:00", "2012-02-13 12:00:00",
"2012-02-13 18:00:00", "2012-02-14 00:00:00", "2012-02-14 06:00:00",
"2012-02-14 12:00:00", "2012-02-14 18:00:00", "2012-02-15 00:00:00",
"2012-02-15 06:00:00", "2012-02-15 12:00:00", "2012-02-15 18:00:00",
"2012-02-16 00:00:00", "2012-02-16 06:00:00", "2012-02-16 12:00:00",
"2012-02-16 18:00:00", "2012-02-17 00:00:00", "2012-02-17 06:00:00",
"2012-02-17 12:00:00", "2012-02-17 18:00:00", "2012-02-18 00:00:00",
"2012-02-18 06:00:00", "2012-02-18 12:00:00", "2012-02-18 18:00:00",
"2012-02-19 00:00:00", "2012-02-19 06:00:00", "2012-02-19 12:00:00",
"2012-02-19 18:00:00", "2012-02-20 00:00:00", "2012-02-20 06:00:00",
"2012-02-20 12:00:00", "2012-02-20 18:00:00", "2012-02-21 00:00:00",
"2012-02-21 06:00:00", "2012-02-21 12:00:00", "2012-02-21 18:00:00",
"2012-02-22 00:00:00", "2012-02-22 06:00:00", "2012-02-22 12:00:00",
"2012-02-22 18:00:00", "2012-02-23 00:00:00", "2012-02-23 06:00:00",
"2012-02-23 12:00:00", "2012-02-23 18:00:00", "2012-02-24 00:00:00",
"2012-02-24 06:00:00", "2012-02-24 12:00:00", "2012-02-24 18:00:00",
"2012-02-25 00:00:00", "2012-02-25 06:00:00", "2012-02-25 12:00:00",
"2012-02-25 18:00:00", "2012-02-26 00:00:00", "2012-02-26 06:00:00",
"2012-02-26 12:00:00", "2012-02-26 18:00:00", "2012-02-27 00:00:00",
"2012-02-27 06:00:00", "2012-02-27 12:00:00", "2012-02-27 18:00:00",
"2012-02-28 00:00:00", "2012-02-28 06:00:00", "2012-02-28 12:00:00",
"2012-02-28 18:00:00", "2012-02-29 00:00:00", "2012-02-29 06:00:00",
"2012-02-29 12:00:00", "2012-02-29 18:00:00", "2012-03-01 00:00:00",
"2012-03-01 06:00:00", "2012-03-01 12:00:00", "2012-03-01 18:00:00",
"2012-03-02 00:00:00", "2012-03-02 06:00:00", "2012-03-02 12:00:00",
"2012-03-02 18:00:00", "2012-03-03 00:00:00", "2012-03-03 06:00:00",
"2012-03-03 12:00:00", "2012-03-03 18:00:00", "2012-03-04 00:00:00",
"2012-03-04 06:00:00", "2012-03-04 12:00:00", "2012-03-04 18:00:00",
"2012-03-05 00:00:00", "2012-03-05 06:00:00", "2012-03-05 12:00:00",
"2012-03-05 18:00:00", "2012-03-06 00:00:00", "2012-03-06 06:00:00",
"2012-03-06 12:00:00", "2012-03-06 18:00:00", "2012-03-07 00:00:00",
"2012-03-07 06:00:00", "2012-03-07 12:00:00", "2012-03-07 18:00:00",
"2012-03-08 00:00:00", "2012-03-08 06:00:00", "2012-03-08 12:00:00",
"2012-03-08 18:00:00", "2012-03-09 00:00:00", "2012-03-09 06:00:00",
"2012-03-09 12:00:00", "2012-03-09 18:00:00", "2012-03-10 00:00:00",
"2012-03-10 06:00:00", "2012-03-10 12:00:00", "2012-03-10 18:00:00",
"2012-03-11 00:00:00", "2012-03-11 06:00:00", "2012-03-11 12:00:00",
"2012-03-11 18:00:00", "2012-03-12 00:00:00", "2012-03-12 06:00:00",
"2012-03-12 12:00:00", "2012-03-12 18:00:00", "2012-03-13 00:00:00",
"2012-03-13 06:00:00", "2012-03-13 12:00:00", "2012-03-13 18:00:00",
"2012-03-14 00:00:00", "2012-03-14 06:00:00", "2012-03-14 12:00:00",
"2012-03-14 18:00:00", "2012-03-15 00:00:00", "2012-03-15 06:00:00",
"2012-03-15 12:00:00", "2012-03-15 18:00:00", "2012-03-16 00:00:00",
"2012-03-16 06:00:00", "2012-03-16 12:00:00", "2012-03-16 18:00:00",
"2012-03-17 00:00:00", "2012-03-17 06:00:00", "2012-03-17 12:00:00",
"2012-03-17 18:00:00", "2012-03-18 00:00:00", "2012-03-18 06:00:00",
"2012-03-18 12:00:00", "2012-03-18 18:00:00", "2012-03-19 00:00:00",
"2012-03-19 06:00:00", "2012-03-19 12:00:00", "2012-03-19 18:00:00",
"2012-03-20 00:00:00", "2012-03-20 06:00:00", "2012-03-20 12:00:00",
"2012-03-20 18:00:00", "2012-03-21 00:00:00", "2012-03-21 06:00:00",
"2012-03-21 12:00:00", "2012-03-21 18:00:00", "2012-03-22 00:00:00",
"2012-03-22 06:00:00", "2012-03-22 12:00:00", "2012-03-22 18:00:00",
"2012-03-23 00:00:00", "2012-03-23 06:00:00", "2012-03-23 12:00:00",
"2012-03-23 18:00:00", "2012-03-24 00:00:00", "2012-03-24 06:00:00",
"2012-03-24 12:00:00", "2012-03-24 18:00:00", "2012-03-25 00:00:00",
"2012-03-25 06:00:00", "2012-03-25 12:00:00", "2012-03-25 18:00:00",
"2012-03-26 00:00:00", "2012-03-26 06:00:00", "2012-03-26 12:00:00",
"2012-03-26 18:00:00", "2012-03-27 00:00:00", "2012-03-27 06:00:00",
"2012-03-27 12:00:00", "2012-03-27 18:00:00", "2012-03-28 00:00:00",
"2012-03-28 06:00:00", "2012-03-28 12:00:00", "2012-03-28 18:00:00",
"2012-03-29 00:00:00", "2012-03-29 06:00:00", "2012-03-29 12:00:00",
"2012-03-29 18:00:00", "2012-03-30 00:00:00", "2012-03-30 06:00:00",
"2012-03-30 12:00:00", "2012-03-30 18:00:00", "2012-03-31 00:00:00",
"2012-03-31 06:00:00", "2012-03-31 12:00:00", "2012-03-31 18:00:00",
"2012-04-01 00:00:00", "2012-04-01 06:00:00", "2012-04-01 12:00:00",
"2012-04-01 18:00:00", "2012-04-02 00:00:00", "2012-04-02 06:00:00",
"2012-04-02 12:00:00", "2012-04-02 18:00:00", "2012-04-03 00:00:00",
"2012-04-03 06:00:00", "2012-04-03 12:00:00", "2012-04-03 18:00:00",
"2012-04-04 00:00:00", "2012-04-04 06:00:00", "2012-04-04 12:00:00",
"2012-04-04 18:00:00", "2012-04-05 00:00:00", "2012-04-05 06:00:00",
"2012-04-05 12:00:00", "2012-04-05 18:00:00", "2012-04-06 00:00:00",
"2012-04-06 06:00:00", "2012-04-06 12:00:00", "2012-04-06 18:00:00",
"2012-04-07 00:00:00", "2012-04-07 06:00:00", "2012-04-07 12:00:00",
"2012-04-07 18:00:00", "2012-04-08 00:00:00", "2012-04-08 06:00:00",
"2012-04-08 12:00:00", "2012-04-08 18:00:00", "2012-04-09 00:00:00",
"2012-04-09 06:00:00", "2012-04-09 12:00:00", "2012-04-09 18:00:00",
"2012-04-10 00:00:00", "2012-04-10 06:00:00", "2012-04-10 12:00:00",
"2012-04-10 18:00:00", "2012-04-11 00:00:00", "2012-04-11 06:00:00",
"2012-04-11 12:00:00", "2012-04-11 18:00:00", "2012-04-12 00:00:00",
"2012-04-12 06:00:00", "2012-04-12 12:00:00", "2012-04-12 18:00:00",
"2012-04-13 00:00:00", "2012-04-13 06:00:00", "2012-04-13 12:00:00",
"2012-04-13 18:00:00", "2012-04-14 00:00:00", "2012-04-14 06:00:00",
"2012-04-14 12:00:00", "2012-04-14 18:00:00", "2012-04-15 00:00:00",
"2012-04-15 06:00:00", "2012-04-15 12:00:00", "2012-04-15 18:00:00",
"2012-04-16 00:00:00", "2012-04-16 06:00:00", "2012-04-16 12:00:00",
"2012-04-16 18:00:00", "2012-04-17 00:00:00", "2012-04-17 06:00:00",
"2012-04-17 12:00:00", "2012-04-17 18:00:00", "2012-04-18 00:00:00",
"2012-04-18 06:00:00", "2012-04-18 12:00:00", "2012-04-18 18:00:00",
"2012-04-19 00:00:00", "2012-04-19 06:00:00", "2012-04-19 12:00:00",
"2012-04-19 18:00:00", "2012-04-20 00:00:00", "2012-04-20 06:00:00",
"2012-04-20 12:00:00", "2012-04-20 18:00:00", "2012-04-21 00:00:00",
"2012-04-21 06:00:00", "2012-04-21 12:00:00", "2012-04-21 18:00:00",
"2012-04-22 00:00:00", "2012-04-22 06:00:00", "2012-04-22 12:00:00",
"2012-04-22 18:00:00", "2012-04-23 00:00:00", "2012-04-23 06:00:00",
"2012-04-23 12:00:00", "2012-04-23 18:00:00", "2012-04-24 00:00:00",
"2012-04-24 06:00:00", "2012-04-24 12:00:00", "2012-04-24 18:00:00",
"2012-04-25 00:00:00", "2012-04-25 06:00:00", "2012-04-25 12:00:00",
"2012-04-25 18:00:00", "2012-04-26 00:00:00", "2012-04-26 06:00:00",
"2012-04-26 12:00:00", "2012-04-26 18:00:00", "2012-04-27 00:00:00",
"2012-04-27 06:00:00", "2012-04-27 12:00:00", "2012-04-27 18:00:00",
"2012-04-28 00:00:00", "2012-04-28 06:00:00", "2012-04-28 12:00:00",
"2012-04-28 18:00:00", "2012-04-29 00:00:00", "2012-04-29 06:00:00",
"2012-04-29 12:00:00", "2012-04-29 18:00:00", "2012-04-30 00:00:00",
"2012-04-30 06:00:00", "2012-04-30 12:00:00", "2012-04-30 18:00:00",
"2012-05-01 00:00:00", "2012-05-01 06:00:00", "2012-05-01 12:00:00",
"2012-05-01 18:00:00", "2012-05-02 00:00:00", "2012-05-02 06:00:00",
"2012-05-02 12:00:00", "2012-05-02 18:00:00", "2012-05-03 00:00:00",
"2012-05-03 06:00:00", "2012-05-03 12:00:00", "2012-05-03 18:00:00",
"2012-05-04 00:00:00", "2012-05-04 06:00:00", "2012-05-04 12:00:00",
"2012-05-04 18:00:00", "2012-05-05 00:00:00", "2012-05-05 06:00:00",
"2012-05-05 12:00:00", "2012-05-05 18:00:00", "2012-05-06 00:00:00",
"2012-05-06 06:00:00", "2012-05-06 12:00:00", "2012-05-06 18:00:00",
"2012-05-07 00:00:00", "2012-05-07 06:00:00", "2012-05-07 12:00:00",
"2012-05-07 18:00:00", "2012-05-08 00:00:00", "2012-05-08 06:00:00",
"2012-05-08 12:00:00", "2012-05-08 18:00:00", "2012-05-09 00:00:00",
"2012-05-09 06:00:00", "2012-05-09 12:00:00", "2012-05-09 18:00:00",
"2012-05-10 00:00:00", "2012-05-10 06:00:00", "2012-05-10 12:00:00",
"2012-05-10 18:00:00", "2012-05-11 00:00:00", "2012-05-11 06:00:00",
"2012-05-11 12:00:00", "2012-05-11 18:00:00", "2012-05-12 00:00:00",
"2012-05-12 06:00:00", "2012-05-12 12:00:00", "2012-05-12 18:00:00",
"2012-05-13 00:00:00", "2012-05-13 06:00:00", "2012-05-13 12:00:00",
"2012-05-13 18:00:00", "2012-05-14 00:00:00", "2012-05-14 06:00:00",
"2012-05-14 12:00:00", "2012-05-14 18:00:00", "2012-05-15 00:00:00",
"2012-05-15 06:00:00", "2012-05-15 12:00:00", "2012-05-15 18:00:00",
"2012-05-16 00:00:00", "2012-05-16 06:00:00", "2012-05-16 12:00:00",
"2012-05-16 18:00:00", "2012-05-17 00:00:00", "2012-05-17 06:00:00",
"2012-05-17 12:00:00", "2012-05-17 18:00:00", "2012-05-18 00:00:00",
"2012-05-18 06:00:00", "2012-05-18 12:00:00", "2012-05-18 18:00:00",
"2012-05-19 00:00:00", "2012-05-19 06:00:00", "2012-05-19 12:00:00",
"2012-05-19 18:00:00", "2012-05-20 00:00:00", "2012-05-20 06:00:00",
"2012-05-20 12:00:00", "2012-05-20 18:00:00", "2012-05-21 00:00:00",
"2012-05-21 06:00:00", "2012-05-21 12:00:00", "2012-05-21 18:00:00",
"2012-05-22 00:00:00", "2012-05-22 06:00:00", "2012-05-22 12:00:00",
"2012-05-22 18:00:00", "2012-05-23 00:00:00", "2012-05-23 06:00:00",
"2012-05-23 12:00:00", "2012-05-23 18:00:00", "2012-05-24 00:00:00",
"2012-05-24 06:00:00", "2012-05-24 12:00:00", "2012-05-24 18:00:00",
"2012-05-25 00:00:00", "2012-05-25 06:00:00", "2012-05-25 12:00:00",
"2012-05-25 18:00:00", "2012-05-26 00:00:00", "2012-05-26 06:00:00",
"2012-05-26 12:00:00", "2012-05-26 18:00:00", "2012-05-27 00:00:00",
"2012-05-27 06:00:00", "2012-05-27 12:00:00", "2012-05-27 18:00:00",
"2012-05-28 00:00:00", "2012-05-28 06:00:00", "2012-05-28 12:00:00",
"2012-05-28 18:00:00", "2012-05-29 00:00:00", "2012-05-29 06:00:00",
"2012-05-29 12:00:00"), class = "factor"), basistime = structure(c(1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L), .Label = c("2012-01-25 15:02:00", "2012-01-26 15:11:00",
"2012-01-27 01:41:00", "2012-01-27 10:15:00", "2012-01-27 15:15:00",
"2012-01-28 14:22:00", "2012-01-29 13:33:00", "2012-01-30 14:11:00",
"2012-01-31 14:24:00", "2012-02-01 14:55:00", "2012-02-02 14:56:00",
"2012-02-03 14:40:00", "2012-02-04 15:01:00", "2012-02-05 15:04:00",
"2012-02-06 14:37:00", "2012-02-07 14:42:00", "2012-02-08 14:44:00",
"2012-02-09 14:27:00", "2012-02-10 14:33:00", "2012-02-11 15:05:00",
"2012-02-12 15:09:00", "2012-02-13 15:00:00", "2012-02-14 15:10:00",
"2012-02-15 14:46:00", "2012-02-16 14:42:00", "2012-02-17 14:14:00",
"2012-02-18 15:27:00", "2012-02-19 13:13:00", "2012-02-20 13:56:00",
"2012-02-21 14:39:00", "2012-02-22 15:33:00", "2012-02-23 15:14:00",
"2012-02-24 15:14:00", "2012-02-25 14:25:00", "2012-02-26 14:08:00",
"2012-02-27 14:55:00", "2012-02-28 14:40:00", "2012-02-29 14:45:00",
"2012-03-01 15:18:00", "2012-03-02 15:18:00", "2012-03-03 15:43:00",
"2012-03-04 15:16:00", "2012-03-05 15:17:00", "2012-03-06 15:03:00",
"2012-03-07 14:44:00", "2012-03-08 14:53:00", "2012-03-09 15:10:00",
"2012-03-10 14:43:00", "2012-03-11 13:37:00", "2012-03-12 14:58:00",
"2012-03-13 14:36:00", "2012-03-14 13:49:00", "2012-03-15 14:32:00",
"2012-03-16 15:01:00", "2012-03-17 13:55:00", "2012-03-18 13:44:00",
"2012-03-19 13:55:00", "2012-03-20 13:53:00", "2012-03-21 14:07:00",
"2012-03-22 14:03:00", "2012-03-23 14:30:00", "2012-03-24 14:47:00",
"2012-03-25 14:32:00", "2012-03-26 14:41:00", "2012-03-27 13:51:00",
"2012-03-28 13:32:00", "2012-03-29 13:59:00", "2012-03-30 14:15:00",
"2012-03-31 14:12:00", "2012-04-01 14:30:00", "2012-04-02 13:43:00",
"2012-04-03 13:54:00", "2012-04-04 13:45:00", "2012-04-05 13:46:00",
"2012-04-06 13:41:00", "2012-04-07 13:13:00", "2012-04-08 12:59:00",
"2012-04-09 13:53:00", "2012-04-10 14:15:00", "2012-04-11 14:26:00",
"2012-04-12 14:05:00", "2012-04-13 13:37:00", "2012-04-14 13:52:00",
"2012-04-15 15:00:00", "2012-04-16 14:42:00", "2012-04-17 14:21:00",
"2012-04-18 14:40:00", "2012-04-19 14:35:00", "2012-04-20 15:14:00",
"2012-04-21 14:46:00", "2012-04-22 14:44:00", "2012-04-23 14:33:00",
"2012-04-24 14:41:00", "2012-04-25 14:13:00", "2012-04-26 14:19:00",
"2012-04-27 14:35:00", "2012-04-28 13:48:00", "2012-04-29 14:12:00",
"2012-04-30 13:53:00", "2012-05-02 14:41:00", "2012-05-03 15:01:00",
"2012-05-04 15:21:00", "2012-05-05 14:57:00", "2012-05-06 14:09:00",
"2012-05-07 14:30:00", "2012-05-08 14:05:00", "2012-05-08 21:21:00",
"2012-05-09 15:02:00", "2012-05-10 14:27:00", "2012-05-11 14:47:00",
"2012-05-12 13:24:00", "2012-05-13 14:40:00", "2012-05-13 19:24:00",
"2012-05-14 14:33:00", "2012-05-15 14:39:00", "2012-05-16 14:07:00",
"2012-05-17 13:42:00", "2012-05-18 14:30:00", "2012-05-19 13:59:00",
"2012-05-20 14:14:00", "2012-05-21 14:15:00", "2012-05-22 14:18:00",
"2012-05-23 14:14:00", "2012-05-24 13:52:00"), class = "factor"),
     fcst = c(38.7, 38.9, 39.2, 39.8, 40.5, 41.5, 42.5, 43.1,
     43.9, 44.5, 44.8, 45, 45, 45.1, 45.4, 45.8, 46.4, 47, 47.5,
     47.9, 39.2, 40, 41.8, 42.2, 42.8, 44.4, 45.7, 46.2, 46.2,
     46.2, 46.5, 47, 47.5, 47.9, 48.4, 49, 49.5, 49.9, 50.2, 50.4,
     41.3, 42.5, 42.8, 42.8, 42.8, 42.9, 43, 43.3, 43.5, 43.8),
     usgs = c(38.5, 38.6, 38.6, 38.6, 39.1, 39.8, 41.2, 42.9,
     43.4, 43.6, 43.4, 43.1, 43, 43.1, 43.5, 43.8, 44.1, 44.2,
     44.4, 44.5, 39.1, 39.8, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1,
     43, 43.1, 43.5, 43.8, 44.1, 44.2, 44.4, 44.5, 44.5, 44.5,
     44.6, 44.2, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1, 43, 43.1,
     43.5, 43.8)), .Names = c("date", "basistime", "fcst", "usgs"
), row.names = c(NA, 50L), class = "data.frame")

Sys.setenv( "Etc/GMT+8" ) # or whatever is appropriate
#> Error in Sys.setenv("Etc/GMT+8"): all arguments must be named

dta2 <- dta
# bad idea to work with dates as factors
dta2$date <- as.POSIXct( as.character( dta2$date ) )
dta2$basistime <- as.POSIXct( as.character( dta2$basistime ) )

# base R solution

dates <- unique( dta2$date )
dta2list <- split( dta2, dta2$date )
grplist <- lapply( dta2list
                  , function( DF ) {
                      DF[ which.max( DF$fcst ), ]
                    }
                  )
result2 <- do.call( rbind, grplist )
result2
#>                                    date           basistime fcst usgs
#> 2012-01-25 18:00:00 2012-01-25 18:00:00 2012-01-25 15:02:00 38.7 38.5
#> 2012-01-26 00:00:00 2012-01-26 00:00:00 2012-01-25 15:02:00 38.9 38.6
#> 2012-01-26 06:00:00 2012-01-26 06:00:00 2012-01-25 15:02:00 39.2 38.6
#> 2012-01-26 12:00:00 2012-01-26 12:00:00 2012-01-25 15:02:00 39.8 38.6
#> 2012-01-26 18:00:00 2012-01-26 18:00:00 2012-01-25 15:02:00 40.5 39.1
#> 2012-01-27 00:00:00 2012-01-27 00:00:00 2012-01-25 15:02:00 41.5 39.8
#> 2012-01-27 06:00:00 2012-01-27 06:00:00 2012-01-25 15:02:00 42.5 41.2
#> 2012-01-27 12:00:00 2012-01-27 12:00:00 2012-01-25 15:02:00 43.1 42.9
#> 2012-01-27 18:00:00 2012-01-27 18:00:00 2012-01-25 15:02:00 43.9 43.4
#> 2012-01-28 00:00:00 2012-01-28 00:00:00 2012-01-25 15:02:00 44.5 43.6
#> 2012-01-28 06:00:00 2012-01-28 06:00:00 2012-01-26 15:11:00 45.7 43.4
#> 2012-01-28 12:00:00 2012-01-28 12:00:00 2012-01-26 15:11:00 46.2 43.1
#> 2012-01-28 18:00:00 2012-01-28 18:00:00 2012-01-26 15:11:00 46.2 43.0
#> 2012-01-29 00:00:00 2012-01-29 00:00:00 2012-01-26 15:11:00 46.2 43.1
#> 2012-01-29 06:00:00 2012-01-29 06:00:00 2012-01-26 15:11:00 46.5 43.5
#> 2012-01-29 12:00:00 2012-01-29 12:00:00 2012-01-26 15:11:00 47.0 43.8
#> 2012-01-29 18:00:00 2012-01-29 18:00:00 2012-01-26 15:11:00 47.5 44.1
#> 2012-01-30 00:00:00 2012-01-30 00:00:00 2012-01-26 15:11:00 47.9 44.2
#> 2012-01-30 06:00:00 2012-01-30 06:00:00 2012-01-26 15:11:00 48.4 44.4
#> 2012-01-30 12:00:00 2012-01-30 12:00:00 2012-01-26 15:11:00 49.0 44.5
#> 2012-01-30 18:00:00 2012-01-30 18:00:00 2012-01-26 15:11:00 49.5 44.5
#> 2012-01-31 00:00:00 2012-01-31 00:00:00 2012-01-26 15:11:00 49.9 44.5
#> 2012-01-31 06:00:00 2012-01-31 06:00:00 2012-01-26 15:11:00 50.2 44.6
#> 2012-01-31 12:00:00 2012-01-31 12:00:00 2012-01-26 15:11:00 50.4 44.2

# alternately, use tidyverse

library(dplyr)
#>
#> Attaching package: 'dplyr'
#> The following objects are masked from 'package:stats':
#>
#>     filter, lag
#> The following objects are masked from 'package:base':
#>
#>     intersect, setdiff, setequal, union
result3 <- (   dta2
            %>% group_by( date )
            %>% do({
                  DF <- .
                  DF[ which.max( DF$fcst ), ]
                })
            %>% ungroup
            %>% as.data.frame
            )
result3
#>                   date           basistime fcst usgs
#> 1  2012-01-25 18:00:00 2012-01-25 15:02:00 38.7 38.5
#> 2  2012-01-26 00:00:00 2012-01-25 15:02:00 38.9 38.6
#> 3  2012-01-26 06:00:00 2012-01-25 15:02:00 39.2 38.6
#> 4  2012-01-26 12:00:00 2012-01-25 15:02:00 39.8 38.6
#> 5  2012-01-26 18:00:00 2012-01-25 15:02:00 40.5 39.1
#> 6  2012-01-27 00:00:00 2012-01-25 15:02:00 41.5 39.8
#> 7  2012-01-27 06:00:00 2012-01-25 15:02:00 42.5 41.2
#> 8  2012-01-27 12:00:00 2012-01-25 15:02:00 43.1 42.9
#> 9  2012-01-27 18:00:00 2012-01-25 15:02:00 43.9 43.4
#> 10 2012-01-28 00:00:00 2012-01-25 15:02:00 44.5 43.6
#> 11 2012-01-28 06:00:00 2012-01-26 15:11:00 45.7 43.4
#> 12 2012-01-28 12:00:00 2012-01-26 15:11:00 46.2 43.1
#> 13 2012-01-28 18:00:00 2012-01-26 15:11:00 46.2 43.0
#> 14 2012-01-29 00:00:00 2012-01-26 15:11:00 46.2 43.1
#> 15 2012-01-29 06:00:00 2012-01-26 15:11:00 46.5 43.5
#> 16 2012-01-29 12:00:00 2012-01-26 15:11:00 47.0 43.8
#> 17 2012-01-29 18:00:00 2012-01-26 15:11:00 47.5 44.1
#> 18 2012-01-30 00:00:00 2012-01-26 15:11:00 47.9 44.2
#> 19 2012-01-30 06:00:00 2012-01-26 15:11:00 48.4 44.4
#> 20 2012-01-30 12:00:00 2012-01-26 15:11:00 49.0 44.5
#> 21 2012-01-30 18:00:00 2012-01-26 15:11:00 49.5 44.5
#> 22 2012-01-31 00:00:00 2012-01-26 15:11:00 49.9 44.5
#> 23 2012-01-31 06:00:00 2012-01-26 15:11:00 50.2 44.6
#> 24 2012-01-31 12:00:00 2012-01-26 15:11:00 50.4 44.2

####################################################


On Thu, 26 Oct 2017, Thomas Adams wrote:

> Hello all!
>
> I've been struggling with is for many hours today; I'm close to getting
> what I want, but not close enough...
>
> I have a dataframe consisting of two date-time columns followed by two
> numeric columns. what I need is the max value (in the first numeric column)
> based on the 2nd date-time column, which is essentially a factor. But, I
> want the result to provide both date-time values corresponding to the max
> value.
>
> My data:
>
> structure(list(date = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L,
> 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
> 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
> 19L, 20L, 21L, 22L, 23L, 24L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
> 14L, 15L, 16L), .Label = c("2012-01-25 18:00:00", "2012-01-26 00:00:00",
> "2012-01-26 06:00:00", "2012-01-26 12:00:00", "2012-01-26 18:00:00",
> "2012-01-27 00:00:00", "2012-01-27 06:00:00", "2012-01-27 12:00:00",
> "2012-01-27 18:00:00", "2012-01-28 00:00:00", "2012-01-28 06:00:00",
> "2012-01-28 12:00:00", "2012-01-28 18:00:00", "2012-01-29 00:00:00",
> "2012-01-29 06:00:00", "2012-01-29 12:00:00", "2012-01-29 18:00:00",
> "2012-01-30 00:00:00", "2012-01-30 06:00:00", "2012-01-30 12:00:00",
> "2012-01-30 18:00:00", "2012-01-31 00:00:00", "2012-01-31 06:00:00",
> "2012-01-31 12:00:00", "2012-01-31 13:00:00", "2012-01-31 18:00:00",
> "2012-02-01 00:00:00", "2012-02-01 06:00:00", "2012-02-01 12:00:00",
> "2012-02-01 18:00:00", "2012-02-02 00:00:00", "2012-02-02 06:00:00",
> "2012-02-02 12:00:00", "2012-02-02 18:00:00", "2012-02-03 00:00:00",
> "2012-02-03 06:00:00", "2012-02-03 12:00:00", "2012-02-03 18:00:00",
> "2012-02-04 00:00:00", "2012-02-04 06:00:00", "2012-02-04 12:00:00",
> "2012-02-04 18:00:00", "2012-02-05 00:00:00", "2012-02-05 06:00:00",
> "2012-02-05 12:00:00", "2012-02-05 18:00:00", "2012-02-06 00:00:00",
> "2012-02-06 06:00:00", "2012-02-06 12:00:00", "2012-02-06 18:00:00",
> "2012-02-07 00:00:00", "2012-02-07 06:00:00", "2012-02-07 12:00:00",
> "2012-02-07 18:00:00", "2012-02-08 00:00:00", "2012-02-08 06:00:00",
> "2012-02-08 12:00:00", "2012-02-08 18:00:00", "2012-02-09 00:00:00",
> "2012-02-09 06:00:00", "2012-02-09 12:00:00", "2012-02-09 18:00:00",
> "2012-02-10 00:00:00", "2012-02-10 06:00:00", "2012-02-10 12:00:00",
> "2012-02-10 18:00:00", "2012-02-11 00:00:00", "2012-02-11 06:00:00",
> "2012-02-11 12:00:00", "2012-02-11 18:00:00", "2012-02-12 00:00:00",
> "2012-02-12 06:00:00", "2012-02-12 12:00:00", "2012-02-12 18:00:00",
> "2012-02-13 00:00:00", "2012-02-13 06:00:00", "2012-02-13 12:00:00",
> "2012-02-13 18:00:00", "2012-02-14 00:00:00", "2012-02-14 06:00:00",
> "2012-02-14 12:00:00", "2012-02-14 18:00:00", "2012-02-15 00:00:00",
> "2012-02-15 06:00:00", "2012-02-15 12:00:00", "2012-02-15 18:00:00",
> "2012-02-16 00:00:00", "2012-02-16 06:00:00", "2012-02-16 12:00:00",
> "2012-02-16 18:00:00", "2012-02-17 00:00:00", "2012-02-17 06:00:00",
> "2012-02-17 12:00:00", "2012-02-17 18:00:00", "2012-02-18 00:00:00",
> "2012-02-18 06:00:00", "2012-02-18 12:00:00", "2012-02-18 18:00:00",
> "2012-02-19 00:00:00", "2012-02-19 06:00:00", "2012-02-19 12:00:00",
> "2012-02-19 18:00:00", "2012-02-20 00:00:00", "2012-02-20 06:00:00",
> "2012-02-20 12:00:00", "2012-02-20 18:00:00", "2012-02-21 00:00:00",
> "2012-02-21 06:00:00", "2012-02-21 12:00:00", "2012-02-21 18:00:00",
> "2012-02-22 00:00:00", "2012-02-22 06:00:00", "2012-02-22 12:00:00",
> "2012-02-22 18:00:00", "2012-02-23 00:00:00", "2012-02-23 06:00:00",
> "2012-02-23 12:00:00", "2012-02-23 18:00:00", "2012-02-24 00:00:00",
> "2012-02-24 06:00:00", "2012-02-24 12:00:00", "2012-02-24 18:00:00",
> "2012-02-25 00:00:00", "2012-02-25 06:00:00", "2012-02-25 12:00:00",
> "2012-02-25 18:00:00", "2012-02-26 00:00:00", "2012-02-26 06:00:00",
> "2012-02-26 12:00:00", "2012-02-26 18:00:00", "2012-02-27 00:00:00",
> "2012-02-27 06:00:00", "2012-02-27 12:00:00", "2012-02-27 18:00:00",
> "2012-02-28 00:00:00", "2012-02-28 06:00:00", "2012-02-28 12:00:00",
> "2012-02-28 18:00:00", "2012-02-29 00:00:00", "2012-02-29 06:00:00",
> "2012-02-29 12:00:00", "2012-02-29 18:00:00", "2012-03-01 00:00:00",
> "2012-03-01 06:00:00", "2012-03-01 12:00:00", "2012-03-01 18:00:00",
> "2012-03-02 00:00:00", "2012-03-02 06:00:00", "2012-03-02 12:00:00",
> "2012-03-02 18:00:00", "2012-03-03 00:00:00", "2012-03-03 06:00:00",
> "2012-03-03 12:00:00", "2012-03-03 18:00:00", "2012-03-04 00:00:00",
> "2012-03-04 06:00:00", "2012-03-04 12:00:00", "2012-03-04 18:00:00",
> "2012-03-05 00:00:00", "2012-03-05 06:00:00", "2012-03-05 12:00:00",
> "2012-03-05 18:00:00", "2012-03-06 00:00:00", "2012-03-06 06:00:00",
> "2012-03-06 12:00:00", "2012-03-06 18:00:00", "2012-03-07 00:00:00",
> "2012-03-07 06:00:00", "2012-03-07 12:00:00", "2012-03-07 18:00:00",
> "2012-03-08 00:00:00", "2012-03-08 06:00:00", "2012-03-08 12:00:00",
> "2012-03-08 18:00:00", "2012-03-09 00:00:00", "2012-03-09 06:00:00",
> "2012-03-09 12:00:00", "2012-03-09 18:00:00", "2012-03-10 00:00:00",
> "2012-03-10 06:00:00", "2012-03-10 12:00:00", "2012-03-10 18:00:00",
> "2012-03-11 00:00:00", "2012-03-11 06:00:00", "2012-03-11 12:00:00",
> "2012-03-11 18:00:00", "2012-03-12 00:00:00", "2012-03-12 06:00:00",
> "2012-03-12 12:00:00", "2012-03-12 18:00:00", "2012-03-13 00:00:00",
> "2012-03-13 06:00:00", "2012-03-13 12:00:00", "2012-03-13 18:00:00",
> "2012-03-14 00:00:00", "2012-03-14 06:00:00", "2012-03-14 12:00:00",
> "2012-03-14 18:00:00", "2012-03-15 00:00:00", "2012-03-15 06:00:00",
> "2012-03-15 12:00:00", "2012-03-15 18:00:00", "2012-03-16 00:00:00",
> "2012-03-16 06:00:00", "2012-03-16 12:00:00", "2012-03-16 18:00:00",
> "2012-03-17 00:00:00", "2012-03-17 06:00:00", "2012-03-17 12:00:00",
> "2012-03-17 18:00:00", "2012-03-18 00:00:00", "2012-03-18 06:00:00",
> "2012-03-18 12:00:00", "2012-03-18 18:00:00", "2012-03-19 00:00:00",
> "2012-03-19 06:00:00", "2012-03-19 12:00:00", "2012-03-19 18:00:00",
> "2012-03-20 00:00:00", "2012-03-20 06:00:00", "2012-03-20 12:00:00",
> "2012-03-20 18:00:00", "2012-03-21 00:00:00", "2012-03-21 06:00:00",
> "2012-03-21 12:00:00", "2012-03-21 18:00:00", "2012-03-22 00:00:00",
> "2012-03-22 06:00:00", "2012-03-22 12:00:00", "2012-03-22 18:00:00",
> "2012-03-23 00:00:00", "2012-03-23 06:00:00", "2012-03-23 12:00:00",
> "2012-03-23 18:00:00", "2012-03-24 00:00:00", "2012-03-24 06:00:00",
> "2012-03-24 12:00:00", "2012-03-24 18:00:00", "2012-03-25 00:00:00",
> "2012-03-25 06:00:00", "2012-03-25 12:00:00", "2012-03-25 18:00:00",
> "2012-03-26 00:00:00", "2012-03-26 06:00:00", "2012-03-26 12:00:00",
> "2012-03-26 18:00:00", "2012-03-27 00:00:00", "2012-03-27 06:00:00",
> "2012-03-27 12:00:00", "2012-03-27 18:00:00", "2012-03-28 00:00:00",
> "2012-03-28 06:00:00", "2012-03-28 12:00:00", "2012-03-28 18:00:00",
> "2012-03-29 00:00:00", "2012-03-29 06:00:00", "2012-03-29 12:00:00",
> "2012-03-29 18:00:00", "2012-03-30 00:00:00", "2012-03-30 06:00:00",
> "2012-03-30 12:00:00", "2012-03-30 18:00:00", "2012-03-31 00:00:00",
> "2012-03-31 06:00:00", "2012-03-31 12:00:00", "2012-03-31 18:00:00",
> "2012-04-01 00:00:00", "2012-04-01 06:00:00", "2012-04-01 12:00:00",
> "2012-04-01 18:00:00", "2012-04-02 00:00:00", "2012-04-02 06:00:00",
> "2012-04-02 12:00:00", "2012-04-02 18:00:00", "2012-04-03 00:00:00",
> "2012-04-03 06:00:00", "2012-04-03 12:00:00", "2012-04-03 18:00:00",
> "2012-04-04 00:00:00", "2012-04-04 06:00:00", "2012-04-04 12:00:00",
> "2012-04-04 18:00:00", "2012-04-05 00:00:00", "2012-04-05 06:00:00",
> "2012-04-05 12:00:00", "2012-04-05 18:00:00", "2012-04-06 00:00:00",
> "2012-04-06 06:00:00", "2012-04-06 12:00:00", "2012-04-06 18:00:00",
> "2012-04-07 00:00:00", "2012-04-07 06:00:00", "2012-04-07 12:00:00",
> "2012-04-07 18:00:00", "2012-04-08 00:00:00", "2012-04-08 06:00:00",
> "2012-04-08 12:00:00", "2012-04-08 18:00:00", "2012-04-09 00:00:00",
> "2012-04-09 06:00:00", "2012-04-09 12:00:00", "2012-04-09 18:00:00",
> "2012-04-10 00:00:00", "2012-04-10 06:00:00", "2012-04-10 12:00:00",
> "2012-04-10 18:00:00", "2012-04-11 00:00:00", "2012-04-11 06:00:00",
> "2012-04-11 12:00:00", "2012-04-11 18:00:00", "2012-04-12 00:00:00",
> "2012-04-12 06:00:00", "2012-04-12 12:00:00", "2012-04-12 18:00:00",
> "2012-04-13 00:00:00", "2012-04-13 06:00:00", "2012-04-13 12:00:00",
> "2012-04-13 18:00:00", "2012-04-14 00:00:00", "2012-04-14 06:00:00",
> "2012-04-14 12:00:00", "2012-04-14 18:00:00", "2012-04-15 00:00:00",
> "2012-04-15 06:00:00", "2012-04-15 12:00:00", "2012-04-15 18:00:00",
> "2012-04-16 00:00:00", "2012-04-16 06:00:00", "2012-04-16 12:00:00",
> "2012-04-16 18:00:00", "2012-04-17 00:00:00", "2012-04-17 06:00:00",
> "2012-04-17 12:00:00", "2012-04-17 18:00:00", "2012-04-18 00:00:00",
> "2012-04-18 06:00:00", "2012-04-18 12:00:00", "2012-04-18 18:00:00",
> "2012-04-19 00:00:00", "2012-04-19 06:00:00", "2012-04-19 12:00:00",
> "2012-04-19 18:00:00", "2012-04-20 00:00:00", "2012-04-20 06:00:00",
> "2012-04-20 12:00:00", "2012-04-20 18:00:00", "2012-04-21 00:00:00",
> "2012-04-21 06:00:00", "2012-04-21 12:00:00", "2012-04-21 18:00:00",
> "2012-04-22 00:00:00", "2012-04-22 06:00:00", "2012-04-22 12:00:00",
> "2012-04-22 18:00:00", "2012-04-23 00:00:00", "2012-04-23 06:00:00",
> "2012-04-23 12:00:00", "2012-04-23 18:00:00", "2012-04-24 00:00:00",
> "2012-04-24 06:00:00", "2012-04-24 12:00:00", "2012-04-24 18:00:00",
> "2012-04-25 00:00:00", "2012-04-25 06:00:00", "2012-04-25 12:00:00",
> "2012-04-25 18:00:00", "2012-04-26 00:00:00", "2012-04-26 06:00:00",
> "2012-04-26 12:00:00", "2012-04-26 18:00:00", "2012-04-27 00:00:00",
> "2012-04-27 06:00:00", "2012-04-27 12:00:00", "2012-04-27 18:00:00",
> "2012-04-28 00:00:00", "2012-04-28 06:00:00", "2012-04-28 12:00:00",
> "2012-04-28 18:00:00", "2012-04-29 00:00:00", "2012-04-29 06:00:00",
> "2012-04-29 12:00:00", "2012-04-29 18:00:00", "2012-04-30 00:00:00",
> "2012-04-30 06:00:00", "2012-04-30 12:00:00", "2012-04-30 18:00:00",
> "2012-05-01 00:00:00", "2012-05-01 06:00:00", "2012-05-01 12:00:00",
> "2012-05-01 18:00:00", "2012-05-02 00:00:00", "2012-05-02 06:00:00",
> "2012-05-02 12:00:00", "2012-05-02 18:00:00", "2012-05-03 00:00:00",
> "2012-05-03 06:00:00", "2012-05-03 12:00:00", "2012-05-03 18:00:00",
> "2012-05-04 00:00:00", "2012-05-04 06:00:00", "2012-05-04 12:00:00",
> "2012-05-04 18:00:00", "2012-05-05 00:00:00", "2012-05-05 06:00:00",
> "2012-05-05 12:00:00", "2012-05-05 18:00:00", "2012-05-06 00:00:00",
> "2012-05-06 06:00:00", "2012-05-06 12:00:00", "2012-05-06 18:00:00",
> "2012-05-07 00:00:00", "2012-05-07 06:00:00", "2012-05-07 12:00:00",
> "2012-05-07 18:00:00", "2012-05-08 00:00:00", "2012-05-08 06:00:00",
> "2012-05-08 12:00:00", "2012-05-08 18:00:00", "2012-05-09 00:00:00",
> "2012-05-09 06:00:00", "2012-05-09 12:00:00", "2012-05-09 18:00:00",
> "2012-05-10 00:00:00", "2012-05-10 06:00:00", "2012-05-10 12:00:00",
> "2012-05-10 18:00:00", "2012-05-11 00:00:00", "2012-05-11 06:00:00",
> "2012-05-11 12:00:00", "2012-05-11 18:00:00", "2012-05-12 00:00:00",
> "2012-05-12 06:00:00", "2012-05-12 12:00:00", "2012-05-12 18:00:00",
> "2012-05-13 00:00:00", "2012-05-13 06:00:00", "2012-05-13 12:00:00",
> "2012-05-13 18:00:00", "2012-05-14 00:00:00", "2012-05-14 06:00:00",
> "2012-05-14 12:00:00", "2012-05-14 18:00:00", "2012-05-15 00:00:00",
> "2012-05-15 06:00:00", "2012-05-15 12:00:00", "2012-05-15 18:00:00",
> "2012-05-16 00:00:00", "2012-05-16 06:00:00", "2012-05-16 12:00:00",
> "2012-05-16 18:00:00", "2012-05-17 00:00:00", "2012-05-17 06:00:00",
> "2012-05-17 12:00:00", "2012-05-17 18:00:00", "2012-05-18 00:00:00",
> "2012-05-18 06:00:00", "2012-05-18 12:00:00", "2012-05-18 18:00:00",
> "2012-05-19 00:00:00", "2012-05-19 06:00:00", "2012-05-19 12:00:00",
> "2012-05-19 18:00:00", "2012-05-20 00:00:00", "2012-05-20 06:00:00",
> "2012-05-20 12:00:00", "2012-05-20 18:00:00", "2012-05-21 00:00:00",
> "2012-05-21 06:00:00", "2012-05-21 12:00:00", "2012-05-21 18:00:00",
> "2012-05-22 00:00:00", "2012-05-22 06:00:00", "2012-05-22 12:00:00",
> "2012-05-22 18:00:00", "2012-05-23 00:00:00", "2012-05-23 06:00:00",
> "2012-05-23 12:00:00", "2012-05-23 18:00:00", "2012-05-24 00:00:00",
> "2012-05-24 06:00:00", "2012-05-24 12:00:00", "2012-05-24 18:00:00",
> "2012-05-25 00:00:00", "2012-05-25 06:00:00", "2012-05-25 12:00:00",
> "2012-05-25 18:00:00", "2012-05-26 00:00:00", "2012-05-26 06:00:00",
> "2012-05-26 12:00:00", "2012-05-26 18:00:00", "2012-05-27 00:00:00",
> "2012-05-27 06:00:00", "2012-05-27 12:00:00", "2012-05-27 18:00:00",
> "2012-05-28 00:00:00", "2012-05-28 06:00:00", "2012-05-28 12:00:00",
> "2012-05-28 18:00:00", "2012-05-29 00:00:00", "2012-05-29 06:00:00",
> "2012-05-29 12:00:00"), class = "factor"), basistime = structure(c(1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L), .Label = c("2012-01-25 15:02:00", "2012-01-26 15:11:00",
> "2012-01-27 01:41:00", "2012-01-27 10:15:00", "2012-01-27 15:15:00",
> "2012-01-28 14:22:00", "2012-01-29 13:33:00", "2012-01-30 14:11:00",
> "2012-01-31 14:24:00", "2012-02-01 14:55:00", "2012-02-02 14:56:00",
> "2012-02-03 14:40:00", "2012-02-04 15:01:00", "2012-02-05 15:04:00",
> "2012-02-06 14:37:00", "2012-02-07 14:42:00", "2012-02-08 14:44:00",
> "2012-02-09 14:27:00", "2012-02-10 14:33:00", "2012-02-11 15:05:00",
> "2012-02-12 15:09:00", "2012-02-13 15:00:00", "2012-02-14 15:10:00",
> "2012-02-15 14:46:00", "2012-02-16 14:42:00", "2012-02-17 14:14:00",
> "2012-02-18 15:27:00", "2012-02-19 13:13:00", "2012-02-20 13:56:00",
> "2012-02-21 14:39:00", "2012-02-22 15:33:00", "2012-02-23 15:14:00",
> "2012-02-24 15:14:00", "2012-02-25 14:25:00", "2012-02-26 14:08:00",
> "2012-02-27 14:55:00", "2012-02-28 14:40:00", "2012-02-29 14:45:00",
> "2012-03-01 15:18:00", "2012-03-02 15:18:00", "2012-03-03 15:43:00",
> "2012-03-04 15:16:00", "2012-03-05 15:17:00", "2012-03-06 15:03:00",
> "2012-03-07 14:44:00", "2012-03-08 14:53:00", "2012-03-09 15:10:00",
> "2012-03-10 14:43:00", "2012-03-11 13:37:00", "2012-03-12 14:58:00",
> "2012-03-13 14:36:00", "2012-03-14 13:49:00", "2012-03-15 14:32:00",
> "2012-03-16 15:01:00", "2012-03-17 13:55:00", "2012-03-18 13:44:00",
> "2012-03-19 13:55:00", "2012-03-20 13:53:00", "2012-03-21 14:07:00",
> "2012-03-22 14:03:00", "2012-03-23 14:30:00", "2012-03-24 14:47:00",
> "2012-03-25 14:32:00", "2012-03-26 14:41:00", "2012-03-27 13:51:00",
> "2012-03-28 13:32:00", "2012-03-29 13:59:00", "2012-03-30 14:15:00",
> "2012-03-31 14:12:00", "2012-04-01 14:30:00", "2012-04-02 13:43:00",
> "2012-04-03 13:54:00", "2012-04-04 13:45:00", "2012-04-05 13:46:00",
> "2012-04-06 13:41:00", "2012-04-07 13:13:00", "2012-04-08 12:59:00",
> "2012-04-09 13:53:00", "2012-04-10 14:15:00", "2012-04-11 14:26:00",
> "2012-04-12 14:05:00", "2012-04-13 13:37:00", "2012-04-14 13:52:00",
> "2012-04-15 15:00:00", "2012-04-16 14:42:00", "2012-04-17 14:21:00",
> "2012-04-18 14:40:00", "2012-04-19 14:35:00", "2012-04-20 15:14:00",
> "2012-04-21 14:46:00", "2012-04-22 14:44:00", "2012-04-23 14:33:00",
> "2012-04-24 14:41:00", "2012-04-25 14:13:00", "2012-04-26 14:19:00",
> "2012-04-27 14:35:00", "2012-04-28 13:48:00", "2012-04-29 14:12:00",
> "2012-04-30 13:53:00", "2012-05-02 14:41:00", "2012-05-03 15:01:00",
> "2012-05-04 15:21:00", "2012-05-05 14:57:00", "2012-05-06 14:09:00",
> "2012-05-07 14:30:00", "2012-05-08 14:05:00", "2012-05-08 21:21:00",
> "2012-05-09 15:02:00", "2012-05-10 14:27:00", "2012-05-11 14:47:00",
> "2012-05-12 13:24:00", "2012-05-13 14:40:00", "2012-05-13 19:24:00",
> "2012-05-14 14:33:00", "2012-05-15 14:39:00", "2012-05-16 14:07:00",
> "2012-05-17 13:42:00", "2012-05-18 14:30:00", "2012-05-19 13:59:00",
> "2012-05-20 14:14:00", "2012-05-21 14:15:00", "2012-05-22 14:18:00",
> "2012-05-23 14:14:00", "2012-05-24 13:52:00"), class = "factor"),
>    fcst = c(38.7, 38.9, 39.2, 39.8, 40.5, 41.5, 42.5, 43.1,
>    43.9, 44.5, 44.8, 45, 45, 45.1, 45.4, 45.8, 46.4, 47, 47.5,
>    47.9, 39.2, 40, 41.8, 42.2, 42.8, 44.4, 45.7, 46.2, 46.2,
>    46.2, 46.5, 47, 47.5, 47.9, 48.4, 49, 49.5, 49.9, 50.2, 50.4,
>    41.3, 42.5, 42.8, 42.8, 42.8, 42.9, 43, 43.3, 43.5, 43.8),
>    usgs = c(38.5, 38.6, 38.6, 38.6, 39.1, 39.8, 41.2, 42.9,
>    43.4, 43.6, 43.4, 43.1, 43, 43.1, 43.5, 43.8, 44.1, 44.2,
>    44.4, 44.5, 39.1, 39.8, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1,
>    43, 43.1, 43.5, 43.8, 44.1, 44.2, 44.4, 44.5, 44.5, 44.5,
>    44.6, 44.2, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1, 43, 43.1,
>    43.5, 43.8)), .Names = c("date", "basistime", "fcst", "usgs"
> ), row.names = c(NA, 50L), class = "data.frame")
>
> aggregate(fcst ~ basistime,data=legacy, FUN= max)
>
> A snippet...
>
>              basistime fcst
> 1   2012-01-25 15:02:00 47.9
> 2   2012-01-26 15:11:00 50.4
> 3   2012-01-27 01:41:00 46.0
> 4   2012-01-27 10:15:00 47.3
> 5   2012-01-27 15:15:00 47.3
> 6   2012-01-28 14:22:00 46.2
> 7   2012-01-29 13:33:00 45.8
> 8   2012-01-30 14:11:00 44.8
> 9   2012-01-31 14:24:00 43.9
> 10  2012-02-01 14:55:00 41.1
> 11  2012-02-02 14:56:00 38.1
> 12  2012-02-03 14:40:00 36.2
> 13  2012-02-04 15:01:00 34.7
> 14  2012-02-05 15:04:00 33.1
> 15  2012-02-06 14:37:00 32.2
>
> This is what I want, except I need the other corresponding date-time column
> as well. I've tried this with the date-times as factors and as POSIXct
> values and using many of the combinations suggested in the documentation
> and other examples. The closest I can get returns the second date as a
> numeric value:
>
> aggregate(cbind(fcst,date) ~ basistime,data=legacy, FUN= max)
>
> Produces:
>
>              basistime fcst       date
> 1   2012-01-25 15:02:00 47.9 1327942800
> 2   2012-01-26 15:11:00 50.4 1328029200
> 3   2012-01-27 01:41:00 46.0 1328072400
> 4   2012-01-27 10:15:00 47.3 1328032800
> 5   2012-01-27 15:15:00 47.3 1328115600
> 6   2012-01-28 14:22:00 46.2 1328202000
> 7   2012-01-29 13:33:00 45.8 1328288400
> 8   2012-01-30 14:11:00 44.8 1328374800
> 9   2012-01-31 14:24:00 43.9 1328461200
> 10  2012-02-01 14:55:00 41.1 1328547600
> 11  2012-02-02 14:56:00 38.1 1328634000
> 12  2012-02-03 14:40:00 36.2 1328720400
> 13  2012-02-04 15:01:00 34.7 1328806800
> 14  2012-02-05 15:04:00 33.1 1328893200
> 15  2012-02-06 14:37:00 32.2 1328979600
> 16  2012-02-07 14:42:00 31.2 1329066000
> 17  2012-02-08 14:44:00 30.4 1329152400
> 18  2012-02-09 14:27:00 30.0 1329238800
>
>
> Help is greatly appreciated!
>
> Regards,
> Tom
>
> --
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From klaus.keller at graduateinstitute.ch  Thu Oct 26 11:59:56 2017
From: klaus.keller at graduateinstitute.ch (Klaus Michael Keller)
Date: Thu, 26 Oct 2017 11:59:56 +0200
Subject: [R] <p>R encountered a fatal error. </p> The session was
 terminated. + *** caught illegal operation ***
Message-ID: <7790C1BE-DA15-46D3-BD7B-54B8A3105F6C@graduateinstitute.ch>

Dear all,

I just installed the "Short Summer" R update last week. Now, my R Studio doesn't open anymore!

--> <p>R encountered a fatal error. </p> The session was terminated.

and my R terminal doesn't close properly

--> *** caught illegal operation ***

I restarted my Mac OS Sierra 10.12.6 and reinstalled both R 3.4.2 and the latest R studio but the problem persists.

How can that issue be solved?

Thanks in advance for your a precious help!

All the best from Switzerland,

Klaus
	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Thu Oct 26 13:11:16 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Thu, 26 Oct 2017 14:11:16 +0300
Subject: [R] <p>R encountered a fatal error. </p> The session was
 terminated. + *** caught illegal operation ***
In-Reply-To: <7790C1BE-DA15-46D3-BD7B-54B8A3105F6C@graduateinstitute.ch>
References: <7790C1BE-DA15-46D3-BD7B-54B8A3105F6C@graduateinstitute.ch>
Message-ID: <CAGgJW74c=Vo5h6ax9-f3amtnGR2VgCcuyPE+Qj5DKmidhFMRvQ@mail.gmail.com>

How about going back to earlier versions if you don't need the latest ones?


On Thu, Oct 26, 2017 at 12:59 PM, Klaus Michael Keller <
klaus.keller at graduateinstitute.ch> wrote:

> Dear all,
>
> I just installed the "Short Summer" R update last week. Now, my R Studio
> doesn't open anymore!
>
> --> <p>R encountered a fatal error. </p> The session was terminated.
>
> and my R terminal doesn't close properly
>
> --> *** caught illegal operation ***
>
> I restarted my Mac OS Sierra 10.12.6 and reinstalled both R 3.4.2 and the
> latest R studio but the problem persists.
>
> How can that issue be solved?
>
> Thanks in advance for your a precious help!
>
> All the best from Switzerland,
>
> Klaus
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Thu Oct 26 13:14:15 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 26 Oct 2017 11:14:15 +0000
Subject: [R] How to save and restore a workspace
In-Reply-To: <389CBD89-08A8-4EE2-A9E2-0ED81C89AE5B@llnl.gov>
References: <C12851BD-8049-409A-B025-DA6222E955B5@gmail.com>
 <CA+8X3fUdDHpW3-Kip94X8Zz85zMSnQ54b-H7MP5hQWpbUg18fw@mail.gmail.com>
 <389CBD89-08A8-4EE2-A9E2-0ED81C89AE5B@llnl.gov>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88FFAB49C8@SRVEXCHCM301.precheza.cz>

Hi Don

This is matter of opinion

> By the way, this is much easier on Linux, and on Mac when working in a
> Terminal (command line) environment.

There is some basic menu items in RGui which is not in terminal. This could be quite confusing if you are used to have each project in separate directory and want to switch to it (I know about setwd command however in that case point and click is much more convenient).

But as I said, it is matter of opinion.

For Jon

I usually do not start R from its icon. I make a new folder for each project for which I want to use R and keep .RData and .Rhistory in it.

Try to set a new folder named e.g. "stat" somewhere.
Locate .RData and .Rhistory files and copy it to this folder.
Double click on .RData to start R (or select R as a program to open this file)
You can open .Rhistory in any suitable text editor (Notepad, Tinn, ...) to reuse commands in it.

Alternative:

After starting R, you can change working directory (setwd or through menu item). After this change, any saved object together with .RData or Rhistory will go into this directory. You could also set a shortcut icon to this directory on your desktop by copiing original icon, renaming it and manually changing desired directory.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of MacQueen,
> Don
> Sent: Wednesday, October 25, 2017 11:05 PM
> To: Jim Lemon <drjimlemon at gmail.com>; jonnvan at gmail.com
> Cc: r-help mailing list <r-help at r-project.org>
> Subject: Re: [R] How to save and restore a workspace
>
> Saving your workspace means that the variables you currently have defined in
> your session [ everything that shows up when you type ls() ] are saved to a file,
> by default named ?.RData?. To restore the workspace, you use the ?Load
> Workspace? command and navigate to the (same) .RData file. Its default
> location for Windows, as far as I know, is your ?Documents? folder. So look
> there.
>
> I see that you tried the Load Workspace command, and didn?t get what you
> were looking for. I just tested what I described above, and it did work. So either
> you didn?t navigate to and open .RData, or you have some other idea about
> what saving and loading a workspace means. Hopefully, it?s the former!
>
> With some effort, you can learn to save different .RData files for different
> projects; for that see the R Windows FAQ.
>
>
> -Don
>
> --
> Don MacQueen
> Lawrence Livermore National Laboratory
> 7000 East Ave., L-627
> Livermore, CA 94550
> 925-423-1062
> Lab cell 925-724-7509
>
>
>
> On 10/23/17, 3:45 PM, "R-help on behalf of Jim Lemon" <r-help-bounces at r-
> project.org on behalf of drjimlemon at gmail.com> wrote:
>
>     Hi Jon,
>     Saving your workspace doesn't mean that everything will be rerun when
>     you start a new R session. I just means that persistent objects like
>     data frames will be there. If you type:
>
>     objects()
>
>     you will see all of those things that were there when you ended in the
>     last session. Things like commands will be in the "history", so you
>     can retrieve them just as you did at the end of the last session (up
>     arrow). It may be a better solution if you save sequences of commands
>     as R script files (e.g. "something.R") as you can run them:
>
>     source("something.R")
>
>     and edit and save them again.
>
>     Jim
>
>     On Tue, Oct 24, 2017 at 9:12 AM,  <jonnvan at gmail.com> wrote:
>     > Hello,
>     > I recently downloaded R in hopes of learning to use it for statistics.
>     > I have promptly run into a problem, as I am unable to save, and later
> recover, a workspace so I can resume work where I left off.
>     > I am using Windows.
>     > I indicate "yes" to the pop up after q().  Then when I later reopen R Console
> and click on File, I cannot get my prior workspace to appear in the R Console
> frame so I can resume work.
>     > In the File drop down menu I have tried Load Workspace, Load History,
> Display file(s)..., opened R Type: R Workspace with no luck.
>     > I have read about this in two different books, the R Manual, and R FAQs,
> used the RGui help function, and still cannot do it.
>     > I have used Windows for years, but I am ignorant about programming.
>     > Would appreciate any help you might offer.
>     > I live in the Denver area, so if there are any local resources you could direct
> me to, I would be grateful for that as well.
>     >
>     > Thank you,
>     > Jon VanDeventer
>     >
>     > Sent from my iPad
>     > ______________________________________________
>     > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>     > https://stat.ethz.ch/mailman/listinfo/r-help
>     > PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
>     > and provide commented, minimal, self-contained, reproducible code.
>
>     ______________________________________________
>     R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
>     and provide commented, minimal, self-contained, reproducible code.
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From tea3rd at gmail.com  Thu Oct 26 13:58:46 2017
From: tea3rd at gmail.com (Thomas Adams)
Date: Thu, 26 Oct 2017 07:58:46 -0400
Subject: [R] Help needed with aggregate or other solution
In-Reply-To: <alpine.BSF.2.00.1710252216410.34411@pedal.dcn.davis.ca.us>
References: <CAGxgkWjb8kZkt16OjvzZHZjTG_vbxvjAkfPov=pF9M5MN8vMOQ@mail.gmail.com>
 <alpine.BSF.2.00.1710252216410.34411@pedal.dcn.davis.ca.us>
Message-ID: <CAGxgkWg=ReVYhW9rvyp6UDOdOL0c6CQQYaHDfGM2tey_iPXgZw@mail.gmail.com>

Hi Jeff,

Thank you for the suggestions -- I appreciate your help. Unfortunately, the
result2 has two problems...

(1) there are now 3 date columns (it looks like 2 cols are merged into 1
col)
(2) the output rows should not have any of the basistime dates repeated
(maybe I misstated the problem); I need the max fcst value by basistime,
but also list the date value for that row; for example:

             basistime fcst
1   2012-01-25 15:02:00 47.9
2   2012-01-26 15:11:00 50.4
3   2012-01-27 01:41:00 46.0
4   2012-01-27 10:15:00 47.3
5   2012-01-27 15:15:00 47.3
6   2012-01-28 14:22:00 46.2
7   2012-01-29 13:33:00 45.8
8   2012-01-30 14:11:00 44.8
9   2012-01-31 14:24:00 43.9
10  2012-02-01 14:55:00 41.1
11  2012-02-02 14:56:00 38.1
12  2012-02-03 14:40:00 36.2
13  2012-02-04 15:01:00 34.7
14  2012-02-05 15:04:00 33.1
15  2012-02-06 14:37:00 32.2

This is very close to what I need. The basistime dates are all unique, with
the max fcst value for the available basistime dates; but I additionally
need the corresponding 'date' value.

Best,
Tom



On Thu, Oct 26, 2017 at 1:28 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Thanks for the dput...
>
> #### reproducible example of split-apply-combine ###
>
> dta <- structure(list(date = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L,
>
> 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
> 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
> 19L, 20L, 21L, 22L, 23L, 24L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
> 14L, 15L, 16L), .Label = c("2012-01-25 18:00:00", "2012-01-26 00:00:00",
> "2012-01-26 06:00:00", "2012-01-26 12:00:00", "2012-01-26 18:00:00",
> "2012-01-27 00:00:00", "2012-01-27 06:00:00", "2012-01-27 12:00:00",
> "2012-01-27 18:00:00", "2012-01-28 00:00:00", "2012-01-28 06:00:00",
> "2012-01-28 12:00:00", "2012-01-28 18:00:00", "2012-01-29 00:00:00",
> "2012-01-29 06:00:00", "2012-01-29 12:00:00", "2012-01-29 18:00:00",
> "2012-01-30 00:00:00", "2012-01-30 06:00:00", "2012-01-30 12:00:00",
> "2012-01-30 18:00:00", "2012-01-31 00:00:00", "2012-01-31 06:00:00",
> "2012-01-31 12:00:00", "2012-01-31 13:00:00", "2012-01-31 18:00:00",
> "2012-02-01 00:00:00", "2012-02-01 06:00:00", "2012-02-01 12:00:00",
> "2012-02-01 18:00:00", "2012-02-02 00:00:00", "2012-02-02 06:00:00",
> "2012-02-02 12:00:00", "2012-02-02 18:00:00", "2012-02-03 00:00:00",
> "2012-02-03 06:00:00", "2012-02-03 12:00:00", "2012-02-03 18:00:00",
> "2012-02-04 00:00:00", "2012-02-04 06:00:00", "2012-02-04 12:00:00",
> "2012-02-04 18:00:00", "2012-02-05 00:00:00", "2012-02-05 06:00:00",
> "2012-02-05 12:00:00", "2012-02-05 18:00:00", "2012-02-06 00:00:00",
> "2012-02-06 06:00:00", "2012-02-06 12:00:00", "2012-02-06 18:00:00",
> "2012-02-07 00:00:00", "2012-02-07 06:00:00", "2012-02-07 12:00:00",
> "2012-02-07 18:00:00", "2012-02-08 00:00:00", "2012-02-08 06:00:00",
> "2012-02-08 12:00:00", "2012-02-08 18:00:00", "2012-02-09 00:00:00",
> "2012-02-09 06:00:00", "2012-02-09 12:00:00", "2012-02-09 18:00:00",
> "2012-02-10 00:00:00", "2012-02-10 06:00:00", "2012-02-10 12:00:00",
> "2012-02-10 18:00:00", "2012-02-11 00:00:00", "2012-02-11 06:00:00",
> "2012-02-11 12:00:00", "2012-02-11 18:00:00", "2012-02-12 00:00:00",
> "2012-02-12 06:00:00", "2012-02-12 12:00:00", "2012-02-12 18:00:00",
> "2012-02-13 00:00:00", "2012-02-13 06:00:00", "2012-02-13 12:00:00",
> "2012-02-13 18:00:00", "2012-02-14 00:00:00", "2012-02-14 06:00:00",
> "2012-02-14 12:00:00", "2012-02-14 18:00:00", "2012-02-15 00:00:00",
> "2012-02-15 06:00:00", "2012-02-15 12:00:00", "2012-02-15 18:00:00",
> "2012-02-16 00:00:00", "2012-02-16 06:00:00", "2012-02-16 12:00:00",
> "2012-02-16 18:00:00", "2012-02-17 00:00:00", "2012-02-17 06:00:00",
> "2012-02-17 12:00:00", "2012-02-17 18:00:00", "2012-02-18 00:00:00",
> "2012-02-18 06:00:00", "2012-02-18 12:00:00", "2012-02-18 18:00:00",
> "2012-02-19 00:00:00", "2012-02-19 06:00:00", "2012-02-19 12:00:00",
> "2012-02-19 18:00:00", "2012-02-20 00:00:00", "2012-02-20 06:00:00",
> "2012-02-20 12:00:00", "2012-02-20 18:00:00", "2012-02-21 00:00:00",
> "2012-02-21 06:00:00", "2012-02-21 12:00:00", "2012-02-21 18:00:00",
> "2012-02-22 00:00:00", "2012-02-22 06:00:00", "2012-02-22 12:00:00",
> "2012-02-22 18:00:00", "2012-02-23 00:00:00", "2012-02-23 06:00:00",
> "2012-02-23 12:00:00", "2012-02-23 18:00:00", "2012-02-24 00:00:00",
> "2012-02-24 06:00:00", "2012-02-24 12:00:00", "2012-02-24 18:00:00",
> "2012-02-25 00:00:00", "2012-02-25 06:00:00", "2012-02-25 12:00:00",
> "2012-02-25 18:00:00", "2012-02-26 00:00:00", "2012-02-26 06:00:00",
> "2012-02-26 12:00:00", "2012-02-26 18:00:00", "2012-02-27 00:00:00",
> "2012-02-27 06:00:00", "2012-02-27 12:00:00", "2012-02-27 18:00:00",
> "2012-02-28 00:00:00", "2012-02-28 06:00:00", "2012-02-28 12:00:00",
> "2012-02-28 18:00:00", "2012-02-29 00:00:00", "2012-02-29 06:00:00",
> "2012-02-29 12:00:00", "2012-02-29 18:00:00", "2012-03-01 00:00:00",
> "2012-03-01 06:00:00", "2012-03-01 12:00:00", "2012-03-01 18:00:00",
> "2012-03-02 00:00:00", "2012-03-02 06:00:00", "2012-03-02 12:00:00",
> "2012-03-02 18:00:00", "2012-03-03 00:00:00", "2012-03-03 06:00:00",
> "2012-03-03 12:00:00", "2012-03-03 18:00:00", "2012-03-04 00:00:00",
> "2012-03-04 06:00:00", "2012-03-04 12:00:00", "2012-03-04 18:00:00",
> "2012-03-05 00:00:00", "2012-03-05 06:00:00", "2012-03-05 12:00:00",
> "2012-03-05 18:00:00", "2012-03-06 00:00:00", "2012-03-06 06:00:00",
> "2012-03-06 12:00:00", "2012-03-06 18:00:00", "2012-03-07 00:00:00",
> "2012-03-07 06:00:00", "2012-03-07 12:00:00", "2012-03-07 18:00:00",
> "2012-03-08 00:00:00", "2012-03-08 06:00:00", "2012-03-08 12:00:00",
> "2012-03-08 18:00:00", "2012-03-09 00:00:00", "2012-03-09 06:00:00",
> "2012-03-09 12:00:00", "2012-03-09 18:00:00", "2012-03-10 00:00:00",
> "2012-03-10 06:00:00", "2012-03-10 12:00:00", "2012-03-10 18:00:00",
> "2012-03-11 00:00:00", "2012-03-11 06:00:00", "2012-03-11 12:00:00",
> "2012-03-11 18:00:00", "2012-03-12 00:00:00", "2012-03-12 06:00:00",
> "2012-03-12 12:00:00", "2012-03-12 18:00:00", "2012-03-13 00:00:00",
> "2012-03-13 06:00:00", "2012-03-13 12:00:00", "2012-03-13 18:00:00",
> "2012-03-14 00:00:00", "2012-03-14 06:00:00", "2012-03-14 12:00:00",
> "2012-03-14 18:00:00", "2012-03-15 00:00:00", "2012-03-15 06:00:00",
> "2012-03-15 12:00:00", "2012-03-15 18:00:00", "2012-03-16 00:00:00",
> "2012-03-16 06:00:00", "2012-03-16 12:00:00", "2012-03-16 18:00:00",
> "2012-03-17 00:00:00", "2012-03-17 06:00:00", "2012-03-17 12:00:00",
> "2012-03-17 18:00:00", "2012-03-18 00:00:00", "2012-03-18 06:00:00",
> "2012-03-18 12:00:00", "2012-03-18 18:00:00", "2012-03-19 00:00:00",
> "2012-03-19 06:00:00", "2012-03-19 12:00:00", "2012-03-19 18:00:00",
> "2012-03-20 00:00:00", "2012-03-20 06:00:00", "2012-03-20 12:00:00",
> "2012-03-20 18:00:00", "2012-03-21 00:00:00", "2012-03-21 06:00:00",
> "2012-03-21 12:00:00", "2012-03-21 18:00:00", "2012-03-22 00:00:00",
> "2012-03-22 06:00:00", "2012-03-22 12:00:00", "2012-03-22 18:00:00",
> "2012-03-23 00:00:00", "2012-03-23 06:00:00", "2012-03-23 12:00:00",
> "2012-03-23 18:00:00", "2012-03-24 00:00:00", "2012-03-24 06:00:00",
> "2012-03-24 12:00:00", "2012-03-24 18:00:00", "2012-03-25 00:00:00",
> "2012-03-25 06:00:00", "2012-03-25 12:00:00", "2012-03-25 18:00:00",
> "2012-03-26 00:00:00", "2012-03-26 06:00:00", "2012-03-26 12:00:00",
> "2012-03-26 18:00:00", "2012-03-27 00:00:00", "2012-03-27 06:00:00",
> "2012-03-27 12:00:00", "2012-03-27 18:00:00", "2012-03-28 00:00:00",
> "2012-03-28 06:00:00", "2012-03-28 12:00:00", "2012-03-28 18:00:00",
> "2012-03-29 00:00:00", "2012-03-29 06:00:00", "2012-03-29 12:00:00",
> "2012-03-29 18:00:00", "2012-03-30 00:00:00", "2012-03-30 06:00:00",
> "2012-03-30 12:00:00", "2012-03-30 18:00:00", "2012-03-31 00:00:00",
> "2012-03-31 06:00:00", "2012-03-31 12:00:00", "2012-03-31 18:00:00",
> "2012-04-01 00:00:00", "2012-04-01 06:00:00", "2012-04-01 12:00:00",
> "2012-04-01 18:00:00", "2012-04-02 00:00:00", "2012-04-02 06:00:00",
> "2012-04-02 12:00:00", "2012-04-02 18:00:00", "2012-04-03 00:00:00",
> "2012-04-03 06:00:00", "2012-04-03 12:00:00", "2012-04-03 18:00:00",
> "2012-04-04 00:00:00", "2012-04-04 06:00:00", "2012-04-04 12:00:00",
> "2012-04-04 18:00:00", "2012-04-05 00:00:00", "2012-04-05 06:00:00",
> "2012-04-05 12:00:00", "2012-04-05 18:00:00", "2012-04-06 00:00:00",
> "2012-04-06 06:00:00", "2012-04-06 12:00:00", "2012-04-06 18:00:00",
> "2012-04-07 00:00:00", "2012-04-07 06:00:00", "2012-04-07 12:00:00",
> "2012-04-07 18:00:00", "2012-04-08 00:00:00", "2012-04-08 06:00:00",
> "2012-04-08 12:00:00", "2012-04-08 18:00:00", "2012-04-09 00:00:00",
> "2012-04-09 06:00:00", "2012-04-09 12:00:00", "2012-04-09 18:00:00",
> "2012-04-10 00:00:00", "2012-04-10 06:00:00", "2012-04-10 12:00:00",
> "2012-04-10 18:00:00", "2012-04-11 00:00:00", "2012-04-11 06:00:00",
> "2012-04-11 12:00:00", "2012-04-11 18:00:00", "2012-04-12 00:00:00",
> "2012-04-12 06:00:00", "2012-04-12 12:00:00", "2012-04-12 18:00:00",
> "2012-04-13 00:00:00", "2012-04-13 06:00:00", "2012-04-13 12:00:00",
> "2012-04-13 18:00:00", "2012-04-14 00:00:00", "2012-04-14 06:00:00",
> "2012-04-14 12:00:00", "2012-04-14 18:00:00", "2012-04-15 00:00:00",
> "2012-04-15 06:00:00", "2012-04-15 12:00:00", "2012-04-15 18:00:00",
> "2012-04-16 00:00:00", "2012-04-16 06:00:00", "2012-04-16 12:00:00",
> "2012-04-16 18:00:00", "2012-04-17 00:00:00", "2012-04-17 06:00:00",
> "2012-04-17 12:00:00", "2012-04-17 18:00:00", "2012-04-18 00:00:00",
> "2012-04-18 06:00:00", "2012-04-18 12:00:00", "2012-04-18 18:00:00",
> "2012-04-19 00:00:00", "2012-04-19 06:00:00", "2012-04-19 12:00:00",
> "2012-04-19 18:00:00", "2012-04-20 00:00:00", "2012-04-20 06:00:00",
> "2012-04-20 12:00:00", "2012-04-20 18:00:00", "2012-04-21 00:00:00",
> "2012-04-21 06:00:00", "2012-04-21 12:00:00", "2012-04-21 18:00:00",
> "2012-04-22 00:00:00", "2012-04-22 06:00:00", "2012-04-22 12:00:00",
> "2012-04-22 18:00:00", "2012-04-23 00:00:00", "2012-04-23 06:00:00",
> "2012-04-23 12:00:00", "2012-04-23 18:00:00", "2012-04-24 00:00:00",
> "2012-04-24 06:00:00", "2012-04-24 12:00:00", "2012-04-24 18:00:00",
> "2012-04-25 00:00:00", "2012-04-25 06:00:00", "2012-04-25 12:00:00",
> "2012-04-25 18:00:00", "2012-04-26 00:00:00", "2012-04-26 06:00:00",
> "2012-04-26 12:00:00", "2012-04-26 18:00:00", "2012-04-27 00:00:00",
> "2012-04-27 06:00:00", "2012-04-27 12:00:00", "2012-04-27 18:00:00",
> "2012-04-28 00:00:00", "2012-04-28 06:00:00", "2012-04-28 12:00:00",
> "2012-04-28 18:00:00", "2012-04-29 00:00:00", "2012-04-29 06:00:00",
> "2012-04-29 12:00:00", "2012-04-29 18:00:00", "2012-04-30 00:00:00",
> "2012-04-30 06:00:00", "2012-04-30 12:00:00", "2012-04-30 18:00:00",
> "2012-05-01 00:00:00", "2012-05-01 06:00:00", "2012-05-01 12:00:00",
> "2012-05-01 18:00:00", "2012-05-02 00:00:00", "2012-05-02 06:00:00",
> "2012-05-02 12:00:00", "2012-05-02 18:00:00", "2012-05-03 00:00:00",
> "2012-05-03 06:00:00", "2012-05-03 12:00:00", "2012-05-03 18:00:00",
> "2012-05-04 00:00:00", "2012-05-04 06:00:00", "2012-05-04 12:00:00",
> "2012-05-04 18:00:00", "2012-05-05 00:00:00", "2012-05-05 06:00:00",
> "2012-05-05 12:00:00", "2012-05-05 18:00:00", "2012-05-06 00:00:00",
> "2012-05-06 06:00:00", "2012-05-06 12:00:00", "2012-05-06 18:00:00",
> "2012-05-07 00:00:00", "2012-05-07 06:00:00", "2012-05-07 12:00:00",
> "2012-05-07 18:00:00", "2012-05-08 00:00:00", "2012-05-08 06:00:00",
> "2012-05-08 12:00:00", "2012-05-08 18:00:00", "2012-05-09 00:00:00",
> "2012-05-09 06:00:00", "2012-05-09 12:00:00", "2012-05-09 18:00:00",
> "2012-05-10 00:00:00", "2012-05-10 06:00:00", "2012-05-10 12:00:00",
> "2012-05-10 18:00:00", "2012-05-11 00:00:00", "2012-05-11 06:00:00",
> "2012-05-11 12:00:00", "2012-05-11 18:00:00", "2012-05-12 00:00:00",
> "2012-05-12 06:00:00", "2012-05-12 12:00:00", "2012-05-12 18:00:00",
> "2012-05-13 00:00:00", "2012-05-13 06:00:00", "2012-05-13 12:00:00",
> "2012-05-13 18:00:00", "2012-05-14 00:00:00", "2012-05-14 06:00:00",
> "2012-05-14 12:00:00", "2012-05-14 18:00:00", "2012-05-15 00:00:00",
> "2012-05-15 06:00:00", "2012-05-15 12:00:00", "2012-05-15 18:00:00",
> "2012-05-16 00:00:00", "2012-05-16 06:00:00", "2012-05-16 12:00:00",
> "2012-05-16 18:00:00", "2012-05-17 00:00:00", "2012-05-17 06:00:00",
> "2012-05-17 12:00:00", "2012-05-17 18:00:00", "2012-05-18 00:00:00",
> "2012-05-18 06:00:00", "2012-05-18 12:00:00", "2012-05-18 18:00:00",
> "2012-05-19 00:00:00", "2012-05-19 06:00:00", "2012-05-19 12:00:00",
> "2012-05-19 18:00:00", "2012-05-20 00:00:00", "2012-05-20 06:00:00",
> "2012-05-20 12:00:00", "2012-05-20 18:00:00", "2012-05-21 00:00:00",
> "2012-05-21 06:00:00", "2012-05-21 12:00:00", "2012-05-21 18:00:00",
> "2012-05-22 00:00:00", "2012-05-22 06:00:00", "2012-05-22 12:00:00",
> "2012-05-22 18:00:00", "2012-05-23 00:00:00", "2012-05-23 06:00:00",
> "2012-05-23 12:00:00", "2012-05-23 18:00:00", "2012-05-24 00:00:00",
> "2012-05-24 06:00:00", "2012-05-24 12:00:00", "2012-05-24 18:00:00",
> "2012-05-25 00:00:00", "2012-05-25 06:00:00", "2012-05-25 12:00:00",
> "2012-05-25 18:00:00", "2012-05-26 00:00:00", "2012-05-26 06:00:00",
> "2012-05-26 12:00:00", "2012-05-26 18:00:00", "2012-05-27 00:00:00",
> "2012-05-27 06:00:00", "2012-05-27 12:00:00", "2012-05-27 18:00:00",
> "2012-05-28 00:00:00", "2012-05-28 06:00:00", "2012-05-28 12:00:00",
> "2012-05-28 18:00:00", "2012-05-29 00:00:00", "2012-05-29 06:00:00",
> "2012-05-29 12:00:00"), class = "factor"), basistime = structure(c(1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L), .Label = c("2012-01-25 15:02:00", "2012-01-26 15:11:00",
> "2012-01-27 01:41:00", "2012-01-27 10:15:00", "2012-01-27 15:15:00",
> "2012-01-28 14:22:00", "2012-01-29 13:33:00", "2012-01-30 14:11:00",
> "2012-01-31 14:24:00", "2012-02-01 14:55:00", "2012-02-02 14:56:00",
> "2012-02-03 14:40:00", "2012-02-04 15:01:00", "2012-02-05 15:04:00",
> "2012-02-06 14:37:00", "2012-02-07 14:42:00", "2012-02-08 14:44:00",
> "2012-02-09 14:27:00", "2012-02-10 14:33:00", "2012-02-11 15:05:00",
> "2012-02-12 15:09:00", "2012-02-13 15:00:00", "2012-02-14 15:10:00",
> "2012-02-15 14:46:00", "2012-02-16 14:42:00", "2012-02-17 14:14:00",
> "2012-02-18 15:27:00", "2012-02-19 13:13:00", "2012-02-20 13:56:00",
> "2012-02-21 14:39:00", "2012-02-22 15:33:00", "2012-02-23 15:14:00",
> "2012-02-24 15:14:00", "2012-02-25 14:25:00", "2012-02-26 14:08:00",
> "2012-02-27 14:55:00", "2012-02-28 14:40:00", "2012-02-29 14:45:00",
> "2012-03-01 15:18:00", "2012-03-02 15:18:00", "2012-03-03 15:43:00",
> "2012-03-04 15:16:00", "2012-03-05 15:17:00", "2012-03-06 15:03:00",
> "2012-03-07 14:44:00", "2012-03-08 14:53:00", "2012-03-09 15:10:00",
> "2012-03-10 14:43:00", "2012-03-11 13:37:00", "2012-03-12 14:58:00",
> "2012-03-13 14:36:00", "2012-03-14 13:49:00", "2012-03-15 14:32:00",
> "2012-03-16 15:01:00", "2012-03-17 13:55:00", "2012-03-18 13:44:00",
> "2012-03-19 13:55:00", "2012-03-20 13:53:00", "2012-03-21 14:07:00",
> "2012-03-22 14:03:00", "2012-03-23 14:30:00", "2012-03-24 14:47:00",
> "2012-03-25 14:32:00", "2012-03-26 14:41:00", "2012-03-27 13:51:00",
> "2012-03-28 13:32:00", "2012-03-29 13:59:00", "2012-03-30 14:15:00",
> "2012-03-31 14:12:00", "2012-04-01 14:30:00", "2012-04-02 13:43:00",
> "2012-04-03 13:54:00", "2012-04-04 13:45:00", "2012-04-05 13:46:00",
> "2012-04-06 13:41:00", "2012-04-07 13:13:00", "2012-04-08 12:59:00",
> "2012-04-09 13:53:00", "2012-04-10 14:15:00", "2012-04-11 14:26:00",
> "2012-04-12 14:05:00", "2012-04-13 13:37:00", "2012-04-14 13:52:00",
> "2012-04-15 15:00:00", "2012-04-16 14:42:00", "2012-04-17 14:21:00",
> "2012-04-18 14:40:00", "2012-04-19 14:35:00", "2012-04-20 15:14:00",
> "2012-04-21 14:46:00", "2012-04-22 14:44:00", "2012-04-23 14:33:00",
> "2012-04-24 14:41:00", "2012-04-25 14:13:00", "2012-04-26 14:19:00",
> "2012-04-27 14:35:00", "2012-04-28 13:48:00", "2012-04-29 14:12:00",
> "2012-04-30 13:53:00", "2012-05-02 14:41:00", "2012-05-03 15:01:00",
> "2012-05-04 15:21:00", "2012-05-05 14:57:00", "2012-05-06 14:09:00",
> "2012-05-07 14:30:00", "2012-05-08 14:05:00", "2012-05-08 21:21:00",
> "2012-05-09 15:02:00", "2012-05-10 14:27:00", "2012-05-11 14:47:00",
> "2012-05-12 13:24:00", "2012-05-13 14:40:00", "2012-05-13 19:24:00",
> "2012-05-14 14:33:00", "2012-05-15 14:39:00", "2012-05-16 14:07:00",
> "2012-05-17 13:42:00", "2012-05-18 14:30:00", "2012-05-19 13:59:00",
> "2012-05-20 14:14:00", "2012-05-21 14:15:00", "2012-05-22 14:18:00",
> "2012-05-23 14:14:00", "2012-05-24 13:52:00"), class = "factor"),
>     fcst = c(38.7, 38.9, 39.2, 39.8, 40.5, 41.5, 42.5, 43.1,
>     43.9, 44.5, 44.8, 45, 45, 45.1, 45.4, 45.8, 46.4, 47, 47.5,
>     47.9, 39.2, 40, 41.8, 42.2, 42.8, 44.4, 45.7, 46.2, 46.2,
>     46.2, 46.5, 47, 47.5, 47.9, 48.4, 49, 49.5, 49.9, 50.2, 50.4,
>     41.3, 42.5, 42.8, 42.8, 42.8, 42.9, 43, 43.3, 43.5, 43.8),
>     usgs = c(38.5, 38.6, 38.6, 38.6, 39.1, 39.8, 41.2, 42.9,
>     43.4, 43.6, 43.4, 43.1, 43, 43.1, 43.5, 43.8, 44.1, 44.2,
>     44.4, 44.5, 39.1, 39.8, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1,
>     43, 43.1, 43.5, 43.8, 44.1, 44.2, 44.4, 44.5, 44.5, 44.5,
>     44.6, 44.2, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1, 43, 43.1,
>     43.5, 43.8)), .Names = c("date", "basistime", "fcst", "usgs"
> ), row.names = c(NA, 50L), class = "data.frame")
>
> Sys.setenv( "Etc/GMT+8" ) # or whatever is appropriate
> #> Error in Sys.setenv("Etc/GMT+8"): all arguments must be named
>
> dta2 <- dta
> # bad idea to work with dates as factors
> dta2$date <- as.POSIXct( as.character( dta2$date ) )
> dta2$basistime <- as.POSIXct( as.character( dta2$basistime ) )
>
> # base R solution
>
> dates <- unique( dta2$date )
> dta2list <- split( dta2, dta2$date )
> grplist <- lapply( dta2list
>                  , function( DF ) {
>                      DF[ which.max( DF$fcst ), ]
>                    }
>                  )
> result2 <- do.call( rbind, grplist )
> result2
> #>                                    date           basistime fcst usgs
> #> 2012-01-25 18:00:00 2012-01-25 18:00:00 2012-01-25 15:02:00 38.7 38.5
> #> 2012-01-26 00:00:00 2012-01-26 00:00:00 2012-01-25 15:02:00 38.9 38.6
> #> 2012-01-26 06:00:00 2012-01-26 06:00:00 2012-01-25 15:02:00 39.2 38.6
> #> 2012-01-26 12:00:00 2012-01-26 12:00:00 2012-01-25 15:02:00 39.8 38.6
> #> 2012-01-26 18:00:00 2012-01-26 18:00:00 2012-01-25 15:02:00 40.5 39.1
> #> 2012-01-27 00:00:00 2012-01-27 00:00:00 2012-01-25 15:02:00 41.5 39.8
> #> 2012-01-27 06:00:00 2012-01-27 06:00:00 2012-01-25 15:02:00 42.5 41.2
> #> 2012-01-27 12:00:00 2012-01-27 12:00:00 2012-01-25 15:02:00 43.1 42.9
> #> 2012-01-27 18:00:00 2012-01-27 18:00:00 2012-01-25 15:02:00 43.9 43.4
> #> 2012-01-28 00:00:00 2012-01-28 00:00:00 2012-01-25 15:02:00 44.5 43.6
> #> 2012-01-28 06:00:00 2012-01-28 06:00:00 2012-01-26 15:11:00 45.7 43.4
> #> 2012-01-28 12:00:00 2012-01-28 12:00:00 2012-01-26 15:11:00 46.2 43.1
> #> 2012-01-28 18:00:00 2012-01-28 18:00:00 2012-01-26 15:11:00 46.2 43.0
> #> 2012-01-29 00:00:00 2012-01-29 00:00:00 2012-01-26 15:11:00 46.2 43.1
> #> 2012-01-29 06:00:00 2012-01-29 06:00:00 2012-01-26 15:11:00 46.5 43.5
> #> 2012-01-29 12:00:00 2012-01-29 12:00:00 2012-01-26 15:11:00 47.0 43.8
> #> 2012-01-29 18:00:00 2012-01-29 18:00:00 2012-01-26 15:11:00 47.5 44.1
> #> 2012-01-30 00:00:00 2012-01-30 00:00:00 2012-01-26 15:11:00 47.9 44.2
> #> 2012-01-30 06:00:00 2012-01-30 06:00:00 2012-01-26 15:11:00 48.4 44.4
> #> 2012-01-30 12:00:00 2012-01-30 12:00:00 2012-01-26 15:11:00 49.0 44.5
> #> 2012-01-30 18:00:00 2012-01-30 18:00:00 2012-01-26 15:11:00 49.5 44.5
> #> 2012-01-31 00:00:00 2012-01-31 00:00:00 2012-01-26 15:11:00 49.9 44.5
> #> 2012-01-31 06:00:00 2012-01-31 06:00:00 2012-01-26 15:11:00 50.2 44.6
> #> 2012-01-31 12:00:00 2012-01-31 12:00:00 2012-01-26 15:11:00 50.4 44.2
>
> # alternately, use tidyverse
>
> library(dplyr)
> #>
> #> Attaching package: 'dplyr'
> #> The following objects are masked from 'package:stats':
> #>
> #>     filter, lag
> #> The following objects are masked from 'package:base':
> #>
> #>     intersect, setdiff, setequal, union
> result3 <- (   dta2
>            %>% group_by( date )
>            %>% do({
>                  DF <- .
>                  DF[ which.max( DF$fcst ), ]
>                })
>            %>% ungroup
>            %>% as.data.frame
>            )
> result3
> #>                   date           basistime fcst usgs
> #> 1  2012-01-25 18:00:00 2012-01-25 15:02:00 38.7 38.5
> #> 2  2012-01-26 00:00:00 2012-01-25 15:02:00 38.9 38.6
> #> 3  2012-01-26 06:00:00 2012-01-25 15:02:00 39.2 38.6
> #> 4  2012-01-26 12:00:00 2012-01-25 15:02:00 39.8 38.6
> #> 5  2012-01-26 18:00:00 2012-01-25 15:02:00 40.5 39.1
> #> 6  2012-01-27 00:00:00 2012-01-25 15:02:00 41.5 39.8
> #> 7  2012-01-27 06:00:00 2012-01-25 15:02:00 42.5 41.2
> #> 8  2012-01-27 12:00:00 2012-01-25 15:02:00 43.1 42.9
> #> 9  2012-01-27 18:00:00 2012-01-25 15:02:00 43.9 43.4
> #> 10 2012-01-28 00:00:00 2012-01-25 15:02:00 44.5 43.6
> #> 11 2012-01-28 06:00:00 2012-01-26 15:11:00 45.7 43.4
> #> 12 2012-01-28 12:00:00 2012-01-26 15:11:00 46.2 43.1
> #> 13 2012-01-28 18:00:00 2012-01-26 15:11:00 46.2 43.0
> #> 14 2012-01-29 00:00:00 2012-01-26 15:11:00 46.2 43.1
> #> 15 2012-01-29 06:00:00 2012-01-26 15:11:00 46.5 43.5
> #> 16 2012-01-29 12:00:00 2012-01-26 15:11:00 47.0 43.8
> #> 17 2012-01-29 18:00:00 2012-01-26 15:11:00 47.5 44.1
> #> 18 2012-01-30 00:00:00 2012-01-26 15:11:00 47.9 44.2
> #> 19 2012-01-30 06:00:00 2012-01-26 15:11:00 48.4 44.4
> #> 20 2012-01-30 12:00:00 2012-01-26 15:11:00 49.0 44.5
> #> 21 2012-01-30 18:00:00 2012-01-26 15:11:00 49.5 44.5
> #> 22 2012-01-31 00:00:00 2012-01-26 15:11:00 49.9 44.5
> #> 23 2012-01-31 06:00:00 2012-01-26 15:11:00 50.2 44.6
> #> 24 2012-01-31 12:00:00 2012-01-26 15:11:00 50.4 44.2
>
> ####################################################
>
>
>
> On Thu, 26 Oct 2017, Thomas Adams wrote:
>
> Hello all!
>>
>> I've been struggling with is for many hours today; I'm close to getting
>> what I want, but not close enough...
>>
>> I have a dataframe consisting of two date-time columns followed by two
>> numeric columns. what I need is the max value (in the first numeric
>> column)
>> based on the 2nd date-time column, which is essentially a factor. But, I
>> want the result to provide both date-time values corresponding to the max
>> value.
>>
>> My data:
>>
>> structure(list(date = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L,
>> 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
>> 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
>> 19L, 20L, 21L, 22L, 23L, 24L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
>> 14L, 15L, 16L), .Label = c("2012-01-25 18:00:00", "2012-01-26 00:00:00",
>> "2012-01-26 06:00:00", "2012-01-26 12:00:00", "2012-01-26 18:00:00",
>> "2012-01-27 00:00:00", "2012-01-27 06:00:00", "2012-01-27 12:00:00",
>> "2012-01-27 18:00:00", "2012-01-28 00:00:00", "2012-01-28 06:00:00",
>> "2012-01-28 12:00:00", "2012-01-28 18:00:00", "2012-01-29 00:00:00",
>> "2012-01-29 06:00:00", "2012-01-29 12:00:00", "2012-01-29 18:00:00",
>> "2012-01-30 00:00:00", "2012-01-30 06:00:00", "2012-01-30 12:00:00",
>> "2012-01-30 18:00:00", "2012-01-31 00:00:00", "2012-01-31 06:00:00",
>> "2012-01-31 12:00:00", "2012-01-31 13:00:00", "2012-01-31 18:00:00",
>> "2012-02-01 00:00:00", "2012-02-01 06:00:00", "2012-02-01 12:00:00",
>> "2012-02-01 18:00:00", "2012-02-02 00:00:00", "2012-02-02 06:00:00",
>> "2012-02-02 12:00:00", "2012-02-02 18:00:00", "2012-02-03 00:00:00",
>> "2012-02-03 06:00:00", "2012-02-03 12:00:00", "2012-02-03 18:00:00",
>> "2012-02-04 00:00:00", "2012-02-04 06:00:00", "2012-02-04 12:00:00",
>> "2012-02-04 18:00:00", "2012-02-05 00:00:00", "2012-02-05 06:00:00",
>> "2012-02-05 12:00:00", "2012-02-05 18:00:00", "2012-02-06 00:00:00",
>> "2012-02-06 06:00:00", "2012-02-06 12:00:00", "2012-02-06 18:00:00",
>> "2012-02-07 00:00:00", "2012-02-07 06:00:00", "2012-02-07 12:00:00",
>> "2012-02-07 18:00:00", "2012-02-08 00:00:00", "2012-02-08 06:00:00",
>> "2012-02-08 12:00:00", "2012-02-08 18:00:00", "2012-02-09 00:00:00",
>> "2012-02-09 06:00:00", "2012-02-09 12:00:00", "2012-02-09 18:00:00",
>> "2012-02-10 00:00:00", "2012-02-10 06:00:00", "2012-02-10 12:00:00",
>> "2012-02-10 18:00:00", "2012-02-11 00:00:00", "2012-02-11 06:00:00",
>> "2012-02-11 12:00:00", "2012-02-11 18:00:00", "2012-02-12 00:00:00",
>> "2012-02-12 06:00:00", "2012-02-12 12:00:00", "2012-02-12 18:00:00",
>> "2012-02-13 00:00:00", "2012-02-13 06:00:00", "2012-02-13 12:00:00",
>> "2012-02-13 18:00:00", "2012-02-14 00:00:00", "2012-02-14 06:00:00",
>> "2012-02-14 12:00:00", "2012-02-14 18:00:00", "2012-02-15 00:00:00",
>> "2012-02-15 06:00:00", "2012-02-15 12:00:00", "2012-02-15 18:00:00",
>> "2012-02-16 00:00:00", "2012-02-16 06:00:00", "2012-02-16 12:00:00",
>> "2012-02-16 18:00:00", "2012-02-17 00:00:00", "2012-02-17 06:00:00",
>> "2012-02-17 12:00:00", "2012-02-17 18:00:00", "2012-02-18 00:00:00",
>> "2012-02-18 06:00:00", "2012-02-18 12:00:00", "2012-02-18 18:00:00",
>> "2012-02-19 00:00:00", "2012-02-19 06:00:00", "2012-02-19 12:00:00",
>> "2012-02-19 18:00:00", "2012-02-20 00:00:00", "2012-02-20 06:00:00",
>> "2012-02-20 12:00:00", "2012-02-20 18:00:00", "2012-02-21 00:00:00",
>> "2012-02-21 06:00:00", "2012-02-21 12:00:00", "2012-02-21 18:00:00",
>> "2012-02-22 00:00:00", "2012-02-22 06:00:00", "2012-02-22 12:00:00",
>> "2012-02-22 18:00:00", "2012-02-23 00:00:00", "2012-02-23 06:00:00",
>> "2012-02-23 12:00:00", "2012-02-23 18:00:00", "2012-02-24 00:00:00",
>> "2012-02-24 06:00:00", "2012-02-24 12:00:00", "2012-02-24 18:00:00",
>> "2012-02-25 00:00:00", "2012-02-25 06:00:00", "2012-02-25 12:00:00",
>> "2012-02-25 18:00:00", "2012-02-26 00:00:00", "2012-02-26 06:00:00",
>> "2012-02-26 12:00:00", "2012-02-26 18:00:00", "2012-02-27 00:00:00",
>> "2012-02-27 06:00:00", "2012-02-27 12:00:00", "2012-02-27 18:00:00",
>> "2012-02-28 00:00:00", "2012-02-28 06:00:00", "2012-02-28 12:00:00",
>> "2012-02-28 18:00:00", "2012-02-29 00:00:00", "2012-02-29 06:00:00",
>> "2012-02-29 12:00:00", "2012-02-29 18:00:00", "2012-03-01 00:00:00",
>> "2012-03-01 06:00:00", "2012-03-01 12:00:00", "2012-03-01 18:00:00",
>> "2012-03-02 00:00:00", "2012-03-02 06:00:00", "2012-03-02 12:00:00",
>> "2012-03-02 18:00:00", "2012-03-03 00:00:00", "2012-03-03 06:00:00",
>> "2012-03-03 12:00:00", "2012-03-03 18:00:00", "2012-03-04 00:00:00",
>> "2012-03-04 06:00:00", "2012-03-04 12:00:00", "2012-03-04 18:00:00",
>> "2012-03-05 00:00:00", "2012-03-05 06:00:00", "2012-03-05 12:00:00",
>> "2012-03-05 18:00:00", "2012-03-06 00:00:00", "2012-03-06 06:00:00",
>> "2012-03-06 12:00:00", "2012-03-06 18:00:00", "2012-03-07 00:00:00",
>> "2012-03-07 06:00:00", "2012-03-07 12:00:00", "2012-03-07 18:00:00",
>> "2012-03-08 00:00:00", "2012-03-08 06:00:00", "2012-03-08 12:00:00",
>> "2012-03-08 18:00:00", "2012-03-09 00:00:00", "2012-03-09 06:00:00",
>> "2012-03-09 12:00:00", "2012-03-09 18:00:00", "2012-03-10 00:00:00",
>> "2012-03-10 06:00:00", "2012-03-10 12:00:00", "2012-03-10 18:00:00",
>> "2012-03-11 00:00:00", "2012-03-11 06:00:00", "2012-03-11 12:00:00",
>> "2012-03-11 18:00:00", "2012-03-12 00:00:00", "2012-03-12 06:00:00",
>> "2012-03-12 12:00:00", "2012-03-12 18:00:00", "2012-03-13 00:00:00",
>> "2012-03-13 06:00:00", "2012-03-13 12:00:00", "2012-03-13 18:00:00",
>> "2012-03-14 00:00:00", "2012-03-14 06:00:00", "2012-03-14 12:00:00",
>> "2012-03-14 18:00:00", "2012-03-15 00:00:00", "2012-03-15 06:00:00",
>> "2012-03-15 12:00:00", "2012-03-15 18:00:00", "2012-03-16 00:00:00",
>> "2012-03-16 06:00:00", "2012-03-16 12:00:00", "2012-03-16 18:00:00",
>> "2012-03-17 00:00:00", "2012-03-17 06:00:00", "2012-03-17 12:00:00",
>> "2012-03-17 18:00:00", "2012-03-18 00:00:00", "2012-03-18 06:00:00",
>> "2012-03-18 12:00:00", "2012-03-18 18:00:00", "2012-03-19 00:00:00",
>> "2012-03-19 06:00:00", "2012-03-19 12:00:00", "2012-03-19 18:00:00",
>> "2012-03-20 00:00:00", "2012-03-20 06:00:00", "2012-03-20 12:00:00",
>> "2012-03-20 18:00:00", "2012-03-21 00:00:00", "2012-03-21 06:00:00",
>> "2012-03-21 12:00:00", "2012-03-21 18:00:00", "2012-03-22 00:00:00",
>> "2012-03-22 06:00:00", "2012-03-22 12:00:00", "2012-03-22 18:00:00",
>> "2012-03-23 00:00:00", "2012-03-23 06:00:00", "2012-03-23 12:00:00",
>> "2012-03-23 18:00:00", "2012-03-24 00:00:00", "2012-03-24 06:00:00",
>> "2012-03-24 12:00:00", "2012-03-24 18:00:00", "2012-03-25 00:00:00",
>> "2012-03-25 06:00:00", "2012-03-25 12:00:00", "2012-03-25 18:00:00",
>> "2012-03-26 00:00:00", "2012-03-26 06:00:00", "2012-03-26 12:00:00",
>> "2012-03-26 18:00:00", "2012-03-27 00:00:00", "2012-03-27 06:00:00",
>> "2012-03-27 12:00:00", "2012-03-27 18:00:00", "2012-03-28 00:00:00",
>> "2012-03-28 06:00:00", "2012-03-28 12:00:00", "2012-03-28 18:00:00",
>> "2012-03-29 00:00:00", "2012-03-29 06:00:00", "2012-03-29 12:00:00",
>> "2012-03-29 18:00:00", "2012-03-30 00:00:00", "2012-03-30 06:00:00",
>> "2012-03-30 12:00:00", "2012-03-30 18:00:00", "2012-03-31 00:00:00",
>> "2012-03-31 06:00:00", "2012-03-31 12:00:00", "2012-03-31 18:00:00",
>> "2012-04-01 00:00:00", "2012-04-01 06:00:00", "2012-04-01 12:00:00",
>> "2012-04-01 18:00:00", "2012-04-02 00:00:00", "2012-04-02 06:00:00",
>> "2012-04-02 12:00:00", "2012-04-02 18:00:00", "2012-04-03 00:00:00",
>> "2012-04-03 06:00:00", "2012-04-03 12:00:00", "2012-04-03 18:00:00",
>> "2012-04-04 00:00:00", "2012-04-04 06:00:00", "2012-04-04 12:00:00",
>> "2012-04-04 18:00:00", "2012-04-05 00:00:00", "2012-04-05 06:00:00",
>> "2012-04-05 12:00:00", "2012-04-05 18:00:00", "2012-04-06 00:00:00",
>> "2012-04-06 06:00:00", "2012-04-06 12:00:00", "2012-04-06 18:00:00",
>> "2012-04-07 00:00:00", "2012-04-07 06:00:00", "2012-04-07 12:00:00",
>> "2012-04-07 18:00:00", "2012-04-08 00:00:00", "2012-04-08 06:00:00",
>> "2012-04-08 12:00:00", "2012-04-08 18:00:00", "2012-04-09 00:00:00",
>> "2012-04-09 06:00:00", "2012-04-09 12:00:00", "2012-04-09 18:00:00",
>> "2012-04-10 00:00:00", "2012-04-10 06:00:00", "2012-04-10 12:00:00",
>> "2012-04-10 18:00:00", "2012-04-11 00:00:00", "2012-04-11 06:00:00",
>> "2012-04-11 12:00:00", "2012-04-11 18:00:00", "2012-04-12 00:00:00",
>> "2012-04-12 06:00:00", "2012-04-12 12:00:00", "2012-04-12 18:00:00",
>> "2012-04-13 00:00:00", "2012-04-13 06:00:00", "2012-04-13 12:00:00",
>> "2012-04-13 18:00:00", "2012-04-14 00:00:00", "2012-04-14 06:00:00",
>> "2012-04-14 12:00:00", "2012-04-14 18:00:00", "2012-04-15 00:00:00",
>> "2012-04-15 06:00:00", "2012-04-15 12:00:00", "2012-04-15 18:00:00",
>> "2012-04-16 00:00:00", "2012-04-16 06:00:00", "2012-04-16 12:00:00",
>> "2012-04-16 18:00:00", "2012-04-17 00:00:00", "2012-04-17 06:00:00",
>> "2012-04-17 12:00:00", "2012-04-17 18:00:00", "2012-04-18 00:00:00",
>> "2012-04-18 06:00:00", "2012-04-18 12:00:00", "2012-04-18 18:00:00",
>> "2012-04-19 00:00:00", "2012-04-19 06:00:00", "2012-04-19 12:00:00",
>> "2012-04-19 18:00:00", "2012-04-20 00:00:00", "2012-04-20 06:00:00",
>> "2012-04-20 12:00:00", "2012-04-20 18:00:00", "2012-04-21 00:00:00",
>> "2012-04-21 06:00:00", "2012-04-21 12:00:00", "2012-04-21 18:00:00",
>> "2012-04-22 00:00:00", "2012-04-22 06:00:00", "2012-04-22 12:00:00",
>> "2012-04-22 18:00:00", "2012-04-23 00:00:00", "2012-04-23 06:00:00",
>> "2012-04-23 12:00:00", "2012-04-23 18:00:00", "2012-04-24 00:00:00",
>> "2012-04-24 06:00:00", "2012-04-24 12:00:00", "2012-04-24 18:00:00",
>> "2012-04-25 00:00:00", "2012-04-25 06:00:00", "2012-04-25 12:00:00",
>> "2012-04-25 18:00:00", "2012-04-26 00:00:00", "2012-04-26 06:00:00",
>> "2012-04-26 12:00:00", "2012-04-26 18:00:00", "2012-04-27 00:00:00",
>> "2012-04-27 06:00:00", "2012-04-27 12:00:00", "2012-04-27 18:00:00",
>> "2012-04-28 00:00:00", "2012-04-28 06:00:00", "2012-04-28 12:00:00",
>> "2012-04-28 18:00:00", "2012-04-29 00:00:00", "2012-04-29 06:00:00",
>> "2012-04-29 12:00:00", "2012-04-29 18:00:00", "2012-04-30 00:00:00",
>> "2012-04-30 06:00:00", "2012-04-30 12:00:00", "2012-04-30 18:00:00",
>> "2012-05-01 00:00:00", "2012-05-01 06:00:00", "2012-05-01 12:00:00",
>> "2012-05-01 18:00:00", "2012-05-02 00:00:00", "2012-05-02 06:00:00",
>> "2012-05-02 12:00:00", "2012-05-02 18:00:00", "2012-05-03 00:00:00",
>> "2012-05-03 06:00:00", "2012-05-03 12:00:00", "2012-05-03 18:00:00",
>> "2012-05-04 00:00:00", "2012-05-04 06:00:00", "2012-05-04 12:00:00",
>> "2012-05-04 18:00:00", "2012-05-05 00:00:00", "2012-05-05 06:00:00",
>> "2012-05-05 12:00:00", "2012-05-05 18:00:00", "2012-05-06 00:00:00",
>> "2012-05-06 06:00:00", "2012-05-06 12:00:00", "2012-05-06 18:00:00",
>> "2012-05-07 00:00:00", "2012-05-07 06:00:00", "2012-05-07 12:00:00",
>> "2012-05-07 18:00:00", "2012-05-08 00:00:00", "2012-05-08 06:00:00",
>> "2012-05-08 12:00:00", "2012-05-08 18:00:00", "2012-05-09 00:00:00",
>> "2012-05-09 06:00:00", "2012-05-09 12:00:00", "2012-05-09 18:00:00",
>> "2012-05-10 00:00:00", "2012-05-10 06:00:00", "2012-05-10 12:00:00",
>> "2012-05-10 18:00:00", "2012-05-11 00:00:00", "2012-05-11 06:00:00",
>> "2012-05-11 12:00:00", "2012-05-11 18:00:00", "2012-05-12 00:00:00",
>> "2012-05-12 06:00:00", "2012-05-12 12:00:00", "2012-05-12 18:00:00",
>> "2012-05-13 00:00:00", "2012-05-13 06:00:00", "2012-05-13 12:00:00",
>> "2012-05-13 18:00:00", "2012-05-14 00:00:00", "2012-05-14 06:00:00",
>> "2012-05-14 12:00:00", "2012-05-14 18:00:00", "2012-05-15 00:00:00",
>> "2012-05-15 06:00:00", "2012-05-15 12:00:00", "2012-05-15 18:00:00",
>> "2012-05-16 00:00:00", "2012-05-16 06:00:00", "2012-05-16 12:00:00",
>> "2012-05-16 18:00:00", "2012-05-17 00:00:00", "2012-05-17 06:00:00",
>> "2012-05-17 12:00:00", "2012-05-17 18:00:00", "2012-05-18 00:00:00",
>> "2012-05-18 06:00:00", "2012-05-18 12:00:00", "2012-05-18 18:00:00",
>> "2012-05-19 00:00:00", "2012-05-19 06:00:00", "2012-05-19 12:00:00",
>> "2012-05-19 18:00:00", "2012-05-20 00:00:00", "2012-05-20 06:00:00",
>> "2012-05-20 12:00:00", "2012-05-20 18:00:00", "2012-05-21 00:00:00",
>> "2012-05-21 06:00:00", "2012-05-21 12:00:00", "2012-05-21 18:00:00",
>> "2012-05-22 00:00:00", "2012-05-22 06:00:00", "2012-05-22 12:00:00",
>> "2012-05-22 18:00:00", "2012-05-23 00:00:00", "2012-05-23 06:00:00",
>> "2012-05-23 12:00:00", "2012-05-23 18:00:00", "2012-05-24 00:00:00",
>> "2012-05-24 06:00:00", "2012-05-24 12:00:00", "2012-05-24 18:00:00",
>> "2012-05-25 00:00:00", "2012-05-25 06:00:00", "2012-05-25 12:00:00",
>> "2012-05-25 18:00:00", "2012-05-26 00:00:00", "2012-05-26 06:00:00",
>> "2012-05-26 12:00:00", "2012-05-26 18:00:00", "2012-05-27 00:00:00",
>> "2012-05-27 06:00:00", "2012-05-27 12:00:00", "2012-05-27 18:00:00",
>> "2012-05-28 00:00:00", "2012-05-28 06:00:00", "2012-05-28 12:00:00",
>> "2012-05-28 18:00:00", "2012-05-29 00:00:00", "2012-05-29 06:00:00",
>> "2012-05-29 12:00:00"), class = "factor"), basistime = structure(c(1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>> 3L), .Label = c("2012-01-25 15:02:00", "2012-01-26 15:11:00",
>> "2012-01-27 01:41:00", "2012-01-27 10:15:00", "2012-01-27 15:15:00",
>> "2012-01-28 14:22:00", "2012-01-29 13:33:00", "2012-01-30 14:11:00",
>> "2012-01-31 14:24:00", "2012-02-01 14:55:00", "2012-02-02 14:56:00",
>> "2012-02-03 14:40:00", "2012-02-04 15:01:00", "2012-02-05 15:04:00",
>> "2012-02-06 14:37:00", "2012-02-07 14:42:00", "2012-02-08 14:44:00",
>> "2012-02-09 14:27:00", "2012-02-10 14:33:00", "2012-02-11 15:05:00",
>> "2012-02-12 15:09:00", "2012-02-13 15:00:00", "2012-02-14 15:10:00",
>> "2012-02-15 14:46:00", "2012-02-16 14:42:00", "2012-02-17 14:14:00",
>> "2012-02-18 15:27:00", "2012-02-19 13:13:00", "2012-02-20 13:56:00",
>> "2012-02-21 14:39:00", "2012-02-22 15:33:00", "2012-02-23 15:14:00",
>> "2012-02-24 15:14:00", "2012-02-25 14:25:00", "2012-02-26 14:08:00",
>> "2012-02-27 14:55:00", "2012-02-28 14:40:00", "2012-02-29 14:45:00",
>> "2012-03-01 15:18:00", "2012-03-02 15:18:00", "2012-03-03 15:43:00",
>> "2012-03-04 15:16:00", "2012-03-05 15:17:00", "2012-03-06 15:03:00",
>> "2012-03-07 14:44:00", "2012-03-08 14:53:00", "2012-03-09 15:10:00",
>> "2012-03-10 14:43:00", "2012-03-11 13:37:00", "2012-03-12 14:58:00",
>> "2012-03-13 14:36:00", "2012-03-14 13:49:00", "2012-03-15 14:32:00",
>> "2012-03-16 15:01:00", "2012-03-17 13:55:00", "2012-03-18 13:44:00",
>> "2012-03-19 13:55:00", "2012-03-20 13:53:00", "2012-03-21 14:07:00",
>> "2012-03-22 14:03:00", "2012-03-23 14:30:00", "2012-03-24 14:47:00",
>> "2012-03-25 14:32:00", "2012-03-26 14:41:00", "2012-03-27 13:51:00",
>> "2012-03-28 13:32:00", "2012-03-29 13:59:00", "2012-03-30 14:15:00",
>> "2012-03-31 14:12:00", "2012-04-01 14:30:00", "2012-04-02 13:43:00",
>> "2012-04-03 13:54:00", "2012-04-04 13:45:00", "2012-04-05 13:46:00",
>> "2012-04-06 13:41:00", "2012-04-07 13:13:00", "2012-04-08 12:59:00",
>> "2012-04-09 13:53:00", "2012-04-10 14:15:00", "2012-04-11 14:26:00",
>> "2012-04-12 14:05:00", "2012-04-13 13:37:00", "2012-04-14 13:52:00",
>> "2012-04-15 15:00:00", "2012-04-16 14:42:00", "2012-04-17 14:21:00",
>> "2012-04-18 14:40:00", "2012-04-19 14:35:00", "2012-04-20 15:14:00",
>> "2012-04-21 14:46:00", "2012-04-22 14:44:00", "2012-04-23 14:33:00",
>> "2012-04-24 14:41:00", "2012-04-25 14:13:00", "2012-04-26 14:19:00",
>> "2012-04-27 14:35:00", "2012-04-28 13:48:00", "2012-04-29 14:12:00",
>> "2012-04-30 13:53:00", "2012-05-02 14:41:00", "2012-05-03 15:01:00",
>> "2012-05-04 15:21:00", "2012-05-05 14:57:00", "2012-05-06 14:09:00",
>> "2012-05-07 14:30:00", "2012-05-08 14:05:00", "2012-05-08 21:21:00",
>> "2012-05-09 15:02:00", "2012-05-10 14:27:00", "2012-05-11 14:47:00",
>> "2012-05-12 13:24:00", "2012-05-13 14:40:00", "2012-05-13 19:24:00",
>> "2012-05-14 14:33:00", "2012-05-15 14:39:00", "2012-05-16 14:07:00",
>> "2012-05-17 13:42:00", "2012-05-18 14:30:00", "2012-05-19 13:59:00",
>> "2012-05-20 14:14:00", "2012-05-21 14:15:00", "2012-05-22 14:18:00",
>> "2012-05-23 14:14:00", "2012-05-24 13:52:00"), class = "factor"),
>>    fcst = c(38.7, 38.9, 39.2, 39.8, 40.5, 41.5, 42.5, 43.1,
>>    43.9, 44.5, 44.8, 45, 45, 45.1, 45.4, 45.8, 46.4, 47, 47.5,
>>    47.9, 39.2, 40, 41.8, 42.2, 42.8, 44.4, 45.7, 46.2, 46.2,
>>    46.2, 46.5, 47, 47.5, 47.9, 48.4, 49, 49.5, 49.9, 50.2, 50.4,
>>    41.3, 42.5, 42.8, 42.8, 42.8, 42.9, 43, 43.3, 43.5, 43.8),
>>    usgs = c(38.5, 38.6, 38.6, 38.6, 39.1, 39.8, 41.2, 42.9,
>>    43.4, 43.6, 43.4, 43.1, 43, 43.1, 43.5, 43.8, 44.1, 44.2,
>>    44.4, 44.5, 39.1, 39.8, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1,
>>    43, 43.1, 43.5, 43.8, 44.1, 44.2, 44.4, 44.5, 44.5, 44.5,
>>    44.6, 44.2, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1, 43, 43.1,
>>    43.5, 43.8)), .Names = c("date", "basistime", "fcst", "usgs"
>> ), row.names = c(NA, 50L), class = "data.frame")
>>
>> aggregate(fcst ~ basistime,data=legacy, FUN= max)
>>
>> A snippet...
>>
>>              basistime fcst
>> 1   2012-01-25 15:02:00 47.9
>> 2   2012-01-26 15:11:00 50.4
>> 3   2012-01-27 01:41:00 46.0
>> 4   2012-01-27 10:15:00 47.3
>> 5   2012-01-27 15:15:00 47.3
>> 6   2012-01-28 14:22:00 46.2
>> 7   2012-01-29 13:33:00 45.8
>> 8   2012-01-30 14:11:00 44.8
>> 9   2012-01-31 14:24:00 43.9
>> 10  2012-02-01 14:55:00 41.1
>> 11  2012-02-02 14:56:00 38.1
>> 12  2012-02-03 14:40:00 36.2
>> 13  2012-02-04 15:01:00 34.7
>> 14  2012-02-05 15:04:00 33.1
>> 15  2012-02-06 14:37:00 32.2
>>
>> This is what I want, except I need the other corresponding date-time
>> column
>> as well. I've tried this with the date-times as factors and as POSIXct
>> values and using many of the combinations suggested in the documentation
>> and other examples. The closest I can get returns the second date as a
>> numeric value:
>>
>> aggregate(cbind(fcst,date) ~ basistime,data=legacy, FUN= max)
>>
>> Produces:
>>
>>              basistime fcst       date
>> 1   2012-01-25 15:02:00 47.9 1327942800
>> 2   2012-01-26 15:11:00 50.4 1328029200
>> 3   2012-01-27 01:41:00 46.0 1328072400
>> 4   2012-01-27 10:15:00 47.3 1328032800
>> 5   2012-01-27 15:15:00 47.3 1328115600
>> 6   2012-01-28 14:22:00 46.2 1328202000
>> 7   2012-01-29 13:33:00 45.8 1328288400
>> 8   2012-01-30 14:11:00 44.8 1328374800
>> 9   2012-01-31 14:24:00 43.9 1328461200
>> 10  2012-02-01 14:55:00 41.1 1328547600
>> 11  2012-02-02 14:56:00 38.1 1328634000
>> 12  2012-02-03 14:40:00 36.2 1328720400
>> 13  2012-02-04 15:01:00 34.7 1328806800
>> 14  2012-02-05 15:04:00 33.1 1328893200
>> 15  2012-02-06 14:37:00 32.2 1328979600
>> 16  2012-02-07 14:42:00 31.2 1329066000
>> 17  2012-02-08 14:44:00 30.4 1329152400
>> 18  2012-02-09 14:27:00 30.0 1329238800
>>
>>
>> Help is greatly appreciated!
>>
>> Regards,
>> Tom
>>
>> --
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> ------------------------------------------------------------
> ---------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ------------------------------------------------------------
> ---------------
>



-- 
Thomas E Adams, III
1724 Sage Lane
Blacksburg, VA 24060
tea3rd at gmail.com (personal)
tea at terrapredictions.org (work)

1 (513) 739-9512 (cell)

	[[alternative HTML version deleted]]


From dnbarron at gmail.com  Thu Oct 26 14:57:39 2017
From: dnbarron at gmail.com (David Barron)
Date: Thu, 26 Oct 2017 13:57:39 +0100
Subject: [R] <p>R encountered a fatal error. </p> The session was
 terminated. + *** caught illegal operation ***
In-Reply-To: <CAGgJW74c=Vo5h6ax9-f3amtnGR2VgCcuyPE+Qj5DKmidhFMRvQ@mail.gmail.com>
References: <7790C1BE-DA15-46D3-BD7B-54B8A3105F6C@graduateinstitute.ch>
 <CAGgJW74c=Vo5h6ax9-f3amtnGR2VgCcuyPE+Qj5DKmidhFMRvQ@mail.gmail.com>
Message-ID: <CAHuze_LdndKMS2tyabH0Qb9xiEOP9Pttd1gWA9mvUHxtNApdhQ@mail.gmail.com>

I've seen similar issues reported on the RStudio community site:
https://community.rstudio.com/.  You might want to check in there, as I
think this may well be an RStudio issue rather than a problem with R itself.

Dave

On 26 October 2017 at 12:11, Eric Berger <ericjberger at gmail.com> wrote:

> How about going back to earlier versions if you don't need the latest ones?
>
>
> On Thu, Oct 26, 2017 at 12:59 PM, Klaus Michael Keller <
> klaus.keller at graduateinstitute.ch> wrote:
>
> > Dear all,
> >
> > I just installed the "Short Summer" R update last week. Now, my R Studio
> > doesn't open anymore!
> >
> > --> <p>R encountered a fatal error. </p> The session was terminated.
> >
> > and my R terminal doesn't close properly
> >
> > --> *** caught illegal operation ***
> >
> > I restarted my Mac OS Sierra 10.12.6 and reinstalled both R 3.4.2 and the
> > latest R studio but the problem persists.
> >
> > How can that issue be solved?
> >
> > Thanks in advance for your a precious help!
> >
> > All the best from Switzerland,
> >
> > Klaus
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> > posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Thu Oct 26 15:04:31 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 26 Oct 2017 15:04:31 +0200
Subject: [R] <p>R encountered a fatal error. </p> The session was
 terminated. + *** caught illegal operation ***
In-Reply-To: <CAHuze_LdndKMS2tyabH0Qb9xiEOP9Pttd1gWA9mvUHxtNApdhQ@mail.gmail.com>
References: <7790C1BE-DA15-46D3-BD7B-54B8A3105F6C@graduateinstitute.ch>
 <CAGgJW74c=Vo5h6ax9-f3amtnGR2VgCcuyPE+Qj5DKmidhFMRvQ@mail.gmail.com>
 <CAHuze_LdndKMS2tyabH0Qb9xiEOP9Pttd1gWA9mvUHxtNApdhQ@mail.gmail.com>
Message-ID: <54244B0D-6E8A-44C8-AF63-D935F59F07FD@gmail.com>

Also, check whether R runs standalone, either as Rgui or in Terminal.app (just start Terminal and type "R" if you haven't tried it before.)

-pd

> On 26 Oct 2017, at 14:57 , David Barron <dnbarron at gmail.com> wrote:
> 
> I've seen similar issues reported on the RStudio community site:
> https://community.rstudio.com/.  You might want to check in there, as I
> think this may well be an RStudio issue rather than a problem with R itself.
> 
> Dave
> 
> On 26 October 2017 at 12:11, Eric Berger <ericjberger at gmail.com> wrote:
> 
>> How about going back to earlier versions if you don't need the latest ones?
>> 
>> 
>> On Thu, Oct 26, 2017 at 12:59 PM, Klaus Michael Keller <
>> klaus.keller at graduateinstitute.ch> wrote:
>> 
>>> Dear all,
>>> 
>>> I just installed the "Short Summer" R update last week. Now, my R Studio
>>> doesn't open anymore!
>>> 
>>> --> <p>R encountered a fatal error. </p> The session was terminated.
>>> 
>>> and my R terminal doesn't close properly
>>> 
>>> --> *** caught illegal operation ***
>>> 
>>> I restarted my Mac OS Sierra 10.12.6 and reinstalled both R 3.4.2 and the
>>> latest R studio but the problem persists.
>>> 
>>> How can that issue be solved?
>>> 
>>> Thanks in advance for your a precious help!
>>> 
>>> All the best from Switzerland,
>>> 
>>> Klaus
>>>        [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/
>>> posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>>        [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pdalgd at gmail.com  Thu Oct 26 15:49:29 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 26 Oct 2017 15:49:29 +0200
Subject: [R] <p>R encountered a fatal error. </p> The session was
 terminated. + *** caught illegal operation ***
In-Reply-To: <54244B0D-6E8A-44C8-AF63-D935F59F07FD@gmail.com>
References: <7790C1BE-DA15-46D3-BD7B-54B8A3105F6C@graduateinstitute.ch>
 <CAGgJW74c=Vo5h6ax9-f3amtnGR2VgCcuyPE+Qj5DKmidhFMRvQ@mail.gmail.com>
 <CAHuze_LdndKMS2tyabH0Qb9xiEOP9Pttd1gWA9mvUHxtNApdhQ@mail.gmail.com>
 <54244B0D-6E8A-44C8-AF63-D935F59F07FD@gmail.com>
Message-ID: <D5A17029-11E2-4BAE-A434-27BD3EB0D323@gmail.com>

Just tried upgrading on this machine (better late than never...) R gave no apparetn problems, either standalone or in RStudio 1.0.143. Newest RStudio gives me a strange warning on startup:

Error in loadNamespace(name) : there is no package called ?.GlobalEnv?

but appears to run sanely otherwise. 

(Looks like I had a script window looking at a function in .GlobalEnv which was no longer there. Closing the script seems to have cured the problem.)

-pd


> On 26 Oct 2017, at 15:04 , peter dalgaard <pdalgd at gmail.com> wrote:
> 
> Also, check whether R runs standalone, either as Rgui or in Terminal.app (just start Terminal and type "R" if you haven't tried it before.)
> 
> -pd
> 
>> On 26 Oct 2017, at 14:57 , David Barron <dnbarron at gmail.com> wrote:
>> 
>> I've seen similar issues reported on the RStudio community site:
>> https://community.rstudio.com/.  You might want to check in there, as I
>> think this may well be an RStudio issue rather than a problem with R itself.
>> 
>> Dave
>> 
>> On 26 October 2017 at 12:11, Eric Berger <ericjberger at gmail.com> wrote:
>> 
>>> How about going back to earlier versions if you don't need the latest ones?
>>> 
>>> 
>>> On Thu, Oct 26, 2017 at 12:59 PM, Klaus Michael Keller <
>>> klaus.keller at graduateinstitute.ch> wrote:
>>> 
>>>> Dear all,
>>>> 
>>>> I just installed the "Short Summer" R update last week. Now, my R Studio
>>>> doesn't open anymore!
>>>> 
>>>> --> <p>R encountered a fatal error. </p> The session was terminated.
>>>> 
>>>> and my R terminal doesn't close properly
>>>> 
>>>> --> *** caught illegal operation ***
>>>> 
>>>> I restarted my Mac OS Sierra 10.12.6 and reinstalled both R 3.4.2 and the
>>>> latest R studio but the problem persists.
>>>> 
>>>> How can that issue be solved?
>>>> 
>>>> Thanks in advance for your a precious help!
>>>> 
>>>> All the best from Switzerland,
>>>> 
>>>> Klaus
>>>>       [[alternative HTML version deleted]]
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/
>>>> posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>> 
>>>       [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/
>>> posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 
> 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From jdnewmil at dcn.davis.ca.us  Thu Oct 26 17:07:40 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 26 Oct 2017 08:07:40 -0700 (PDT)
Subject: [R] Help needed with aggregate or other solution
In-Reply-To: <CAGxgkWg=ReVYhW9rvyp6UDOdOL0c6CQQYaHDfGM2tey_iPXgZw@mail.gmail.com>
References: <CAGxgkWjb8kZkt16OjvzZHZjTG_vbxvjAkfPov=pF9M5MN8vMOQ@mail.gmail.com>
 <alpine.BSF.2.00.1710252216410.34411@pedal.dcn.davis.ca.us>
 <CAGxgkWg=ReVYhW9rvyp6UDOdOL0c6CQQYaHDfGM2tey_iPXgZw@mail.gmail.com>
Message-ID: <alpine.BSF.2.00.1710260743150.58694@pedal.dcn.davis.ca.us>

On Thu, 26 Oct 2017, Thomas Adams wrote:

> Hi Jeff,
> 
> Thank you for the suggestions -- I appreciate your help. Unfortunately, the
> result2 has two problems...
> 
> (1) there are now 3 date columns (it looks like 2 cols are merged into 1
> col)

No, there are two date columns. Result2 includes the grouping value as a 
row name (pulled from the names of the dta2list items by rbind).

> (2) the output rows should not have any of the basistime dates repeated
> (maybe I misstated the problem); I need the max fcst value by basistime, but
> also list the date value for that row; for example:

Then try out my code replacing

dta2list <- split( dta2, dta2$date )

with

dta2list <- split( dta2, dta2$basistime )

and

%>% group_by( date )

with

%>% group_by( basistime )

Please study how the code works and ask questions based on gaps in that 
knowledge rather than how the results are not what you expected. This 
mailing list is not a do-your-work-for-you coding service. Some help pages 
you should look at include:

?rownames
?split
?lapply
?do.call
?rbind
?group_by
?do

You might also find [1] helpful in general, and [2] helpful for 
understanding dplyr.

[1] H. Wickham, The Split-Apply-Combine Strategy for Data Analysis, 
Journal of Statistical Software, vol. 40, no. 1, Apr. 2011.

[2] H. Wickham and G. Grolemund, R for Data Science. OReilly UK Ltd, 2017. 
URL: https://r4ds.had.co.nz.

> 
> ???????????? basistime fcst
> 1?? 2012-01-25 15:02:00 47.9
> 2?? 2012-01-26 15:11:00 50.4
> 3?? 2012-01-27 01:41:00 46.0
> 4?? 2012-01-27 10:15:00 47.3
> 5?? 2012-01-27 15:15:00 47.3
> 6?? 2012-01-28 14:22:00 46.2
> 7?? 2012-01-29 13:33:00 45.8
> 8?? 2012-01-30 14:11:00 44.8
> 9?? 2012-01-31 14:24:00 43.9
> 10? 2012-02-01 14:55:00 41.1
> 11? 2012-02-02 14:56:00 38.1
> 12? 2012-02-03 14:40:00 36.2
> 13? 2012-02-04 15:01:00 34.7
> 14? 2012-02-05 15:04:00 33.1
> 15? 2012-02-06 14:37:00 32.2
> 
> This is very close to what I need. The basistime dates are all unique, with
> the max fcst value for the available basistime dates; but I additionally
> need the corresponding 'date' value.
> 
> Best,
> Tom
> 
> 
> 
> On Thu, Oct 26, 2017 at 1:28 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
>       Thanks for the dput...
>
>       #### reproducible example of split-apply-combine ###
>
>       dta <- structure(list(date = structure(c(1L, 2L, 3L, 4L, 5L, 6L,
>       7L,
>       8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
>       5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
>       19L, 20L, 21L, 22L, 23L, 24L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
>       14L, 15L, 16L), .Label = c("2012-01-25 18:00:00", "2012-01-26
>       00:00:00",
>       "2012-01-26 06:00:00", "2012-01-26 12:00:00", "2012-01-26
>       18:00:00",
>       "2012-01-27 00:00:00", "2012-01-27 06:00:00", "2012-01-27
>       12:00:00",
>       "2012-01-27 18:00:00", "2012-01-28 00:00:00", "2012-01-28
>       06:00:00",
>       "2012-01-28 12:00:00", "2012-01-28 18:00:00", "2012-01-29
>       00:00:00",
>       "2012-01-29 06:00:00", "2012-01-29 12:00:00", "2012-01-29
>       18:00:00",
>       "2012-01-30 00:00:00", "2012-01-30 06:00:00", "2012-01-30
>       12:00:00",
>       "2012-01-30 18:00:00", "2012-01-31 00:00:00", "2012-01-31
>       06:00:00",
>       "2012-01-31 12:00:00", "2012-01-31 13:00:00", "2012-01-31
>       18:00:00",
>       "2012-02-01 00:00:00", "2012-02-01 06:00:00", "2012-02-01
>       12:00:00",
>       "2012-02-01 18:00:00", "2012-02-02 00:00:00", "2012-02-02
>       06:00:00",
>       "2012-02-02 12:00:00", "2012-02-02 18:00:00", "2012-02-03
>       00:00:00",
>       "2012-02-03 06:00:00", "2012-02-03 12:00:00", "2012-02-03
>       18:00:00",
>       "2012-02-04 00:00:00", "2012-02-04 06:00:00", "2012-02-04
>       12:00:00",
>       "2012-02-04 18:00:00", "2012-02-05 00:00:00", "2012-02-05
>       06:00:00",
>       "2012-02-05 12:00:00", "2012-02-05 18:00:00", "2012-02-06
>       00:00:00",
>       "2012-02-06 06:00:00", "2012-02-06 12:00:00", "2012-02-06
>       18:00:00",
>       "2012-02-07 00:00:00", "2012-02-07 06:00:00", "2012-02-07
>       12:00:00",
>       "2012-02-07 18:00:00", "2012-02-08 00:00:00", "2012-02-08
>       06:00:00",
>       "2012-02-08 12:00:00", "2012-02-08 18:00:00", "2012-02-09
>       00:00:00",
>       "2012-02-09 06:00:00", "2012-02-09 12:00:00", "2012-02-09
>       18:00:00",
>       "2012-02-10 00:00:00", "2012-02-10 06:00:00", "2012-02-10
>       12:00:00",
>       "2012-02-10 18:00:00", "2012-02-11 00:00:00", "2012-02-11
>       06:00:00",
>       "2012-02-11 12:00:00", "2012-02-11 18:00:00", "2012-02-12
>       00:00:00",
>       "2012-02-12 06:00:00", "2012-02-12 12:00:00", "2012-02-12
>       18:00:00",
>       "2012-02-13 00:00:00", "2012-02-13 06:00:00", "2012-02-13
>       12:00:00",
>       "2012-02-13 18:00:00", "2012-02-14 00:00:00", "2012-02-14
>       06:00:00",
>       "2012-02-14 12:00:00", "2012-02-14 18:00:00", "2012-02-15
>       00:00:00",
>       "2012-02-15 06:00:00", "2012-02-15 12:00:00", "2012-02-15
>       18:00:00",
>       "2012-02-16 00:00:00", "2012-02-16 06:00:00", "2012-02-16
>       12:00:00",
>       "2012-02-16 18:00:00", "2012-02-17 00:00:00", "2012-02-17
>       06:00:00",
>       "2012-02-17 12:00:00", "2012-02-17 18:00:00", "2012-02-18
>       00:00:00",
>       "2012-02-18 06:00:00", "2012-02-18 12:00:00", "2012-02-18
>       18:00:00",
>       "2012-02-19 00:00:00", "2012-02-19 06:00:00", "2012-02-19
>       12:00:00",
>       "2012-02-19 18:00:00", "2012-02-20 00:00:00", "2012-02-20
>       06:00:00",
>       "2012-02-20 12:00:00", "2012-02-20 18:00:00", "2012-02-21
>       00:00:00",
>       "2012-02-21 06:00:00", "2012-02-21 12:00:00", "2012-02-21
>       18:00:00",
>       "2012-02-22 00:00:00", "2012-02-22 06:00:00", "2012-02-22
>       12:00:00",
>       "2012-02-22 18:00:00", "2012-02-23 00:00:00", "2012-02-23
>       06:00:00",
>       "2012-02-23 12:00:00", "2012-02-23 18:00:00", "2012-02-24
>       00:00:00",
>       "2012-02-24 06:00:00", "2012-02-24 12:00:00", "2012-02-24
>       18:00:00",
>       "2012-02-25 00:00:00", "2012-02-25 06:00:00", "2012-02-25
>       12:00:00",
>       "2012-02-25 18:00:00", "2012-02-26 00:00:00", "2012-02-26
>       06:00:00",
>       "2012-02-26 12:00:00", "2012-02-26 18:00:00", "2012-02-27
>       00:00:00",
>       "2012-02-27 06:00:00", "2012-02-27 12:00:00", "2012-02-27
>       18:00:00",
>       "2012-02-28 00:00:00", "2012-02-28 06:00:00", "2012-02-28
>       12:00:00",
>       "2012-02-28 18:00:00", "2012-02-29 00:00:00", "2012-02-29
>       06:00:00",
>       "2012-02-29 12:00:00", "2012-02-29 18:00:00", "2012-03-01
>       00:00:00",
>       "2012-03-01 06:00:00", "2012-03-01 12:00:00", "2012-03-01
>       18:00:00",
>       "2012-03-02 00:00:00", "2012-03-02 06:00:00", "2012-03-02
>       12:00:00",
>       "2012-03-02 18:00:00", "2012-03-03 00:00:00", "2012-03-03
>       06:00:00",
>       "2012-03-03 12:00:00", "2012-03-03 18:00:00", "2012-03-04
>       00:00:00",
>       "2012-03-04 06:00:00", "2012-03-04 12:00:00", "2012-03-04
>       18:00:00",
>       "2012-03-05 00:00:00", "2012-03-05 06:00:00", "2012-03-05
>       12:00:00",
>       "2012-03-05 18:00:00", "2012-03-06 00:00:00", "2012-03-06
>       06:00:00",
>       "2012-03-06 12:00:00", "2012-03-06 18:00:00", "2012-03-07
>       00:00:00",
>       "2012-03-07 06:00:00", "2012-03-07 12:00:00", "2012-03-07
>       18:00:00",
>       "2012-03-08 00:00:00", "2012-03-08 06:00:00", "2012-03-08
>       12:00:00",
>       "2012-03-08 18:00:00", "2012-03-09 00:00:00", "2012-03-09
>       06:00:00",
>       "2012-03-09 12:00:00", "2012-03-09 18:00:00", "2012-03-10
>       00:00:00",
>       "2012-03-10 06:00:00", "2012-03-10 12:00:00", "2012-03-10
>       18:00:00",
>       "2012-03-11 00:00:00", "2012-03-11 06:00:00", "2012-03-11
>       12:00:00",
>       "2012-03-11 18:00:00", "2012-03-12 00:00:00", "2012-03-12
>       06:00:00",
>       "2012-03-12 12:00:00", "2012-03-12 18:00:00", "2012-03-13
>       00:00:00",
>       "2012-03-13 06:00:00", "2012-03-13 12:00:00", "2012-03-13
>       18:00:00",
>       "2012-03-14 00:00:00", "2012-03-14 06:00:00", "2012-03-14
>       12:00:00",
>       "2012-03-14 18:00:00", "2012-03-15 00:00:00", "2012-03-15
>       06:00:00",
>       "2012-03-15 12:00:00", "2012-03-15 18:00:00", "2012-03-16
>       00:00:00",
>       "2012-03-16 06:00:00", "2012-03-16 12:00:00", "2012-03-16
>       18:00:00",
>       "2012-03-17 00:00:00", "2012-03-17 06:00:00", "2012-03-17
>       12:00:00",
>       "2012-03-17 18:00:00", "2012-03-18 00:00:00", "2012-03-18
>       06:00:00",
>       "2012-03-18 12:00:00", "2012-03-18 18:00:00", "2012-03-19
>       00:00:00",
>       "2012-03-19 06:00:00", "2012-03-19 12:00:00", "2012-03-19
>       18:00:00",
>       "2012-03-20 00:00:00", "2012-03-20 06:00:00", "2012-03-20
>       12:00:00",
>       "2012-03-20 18:00:00", "2012-03-21 00:00:00", "2012-03-21
>       06:00:00",
>       "2012-03-21 12:00:00", "2012-03-21 18:00:00", "2012-03-22
>       00:00:00",
>       "2012-03-22 06:00:00", "2012-03-22 12:00:00", "2012-03-22
>       18:00:00",
>       "2012-03-23 00:00:00", "2012-03-23 06:00:00", "2012-03-23
>       12:00:00",
>       "2012-03-23 18:00:00", "2012-03-24 00:00:00", "2012-03-24
>       06:00:00",
>       "2012-03-24 12:00:00", "2012-03-24 18:00:00", "2012-03-25
>       00:00:00",
>       "2012-03-25 06:00:00", "2012-03-25 12:00:00", "2012-03-25
>       18:00:00",
>       "2012-03-26 00:00:00", "2012-03-26 06:00:00", "2012-03-26
>       12:00:00",
>       "2012-03-26 18:00:00", "2012-03-27 00:00:00", "2012-03-27
>       06:00:00",
>       "2012-03-27 12:00:00", "2012-03-27 18:00:00", "2012-03-28
>       00:00:00",
>       "2012-03-28 06:00:00", "2012-03-28 12:00:00", "2012-03-28
>       18:00:00",
>       "2012-03-29 00:00:00", "2012-03-29 06:00:00", "2012-03-29
>       12:00:00",
>       "2012-03-29 18:00:00", "2012-03-30 00:00:00", "2012-03-30
>       06:00:00",
>       "2012-03-30 12:00:00", "2012-03-30 18:00:00", "2012-03-31
>       00:00:00",
>       "2012-03-31 06:00:00", "2012-03-31 12:00:00", "2012-03-31
>       18:00:00",
>       "2012-04-01 00:00:00", "2012-04-01 06:00:00", "2012-04-01
>       12:00:00",
>       "2012-04-01 18:00:00", "2012-04-02 00:00:00", "2012-04-02
>       06:00:00",
>       "2012-04-02 12:00:00", "2012-04-02 18:00:00", "2012-04-03
>       00:00:00",
>       "2012-04-03 06:00:00", "2012-04-03 12:00:00", "2012-04-03
>       18:00:00",
>       "2012-04-04 00:00:00", "2012-04-04 06:00:00", "2012-04-04
>       12:00:00",
>       "2012-04-04 18:00:00", "2012-04-05 00:00:00", "2012-04-05
>       06:00:00",
>       "2012-04-05 12:00:00", "2012-04-05 18:00:00", "2012-04-06
>       00:00:00",
>       "2012-04-06 06:00:00", "2012-04-06 12:00:00", "2012-04-06
>       18:00:00",
>       "2012-04-07 00:00:00", "2012-04-07 06:00:00", "2012-04-07
>       12:00:00",
>       "2012-04-07 18:00:00", "2012-04-08 00:00:00", "2012-04-08
>       06:00:00",
>       "2012-04-08 12:00:00", "2012-04-08 18:00:00", "2012-04-09
>       00:00:00",
>       "2012-04-09 06:00:00", "2012-04-09 12:00:00", "2012-04-09
>       18:00:00",
>       "2012-04-10 00:00:00", "2012-04-10 06:00:00", "2012-04-10
>       12:00:00",
>       "2012-04-10 18:00:00", "2012-04-11 00:00:00", "2012-04-11
>       06:00:00",
>       "2012-04-11 12:00:00", "2012-04-11 18:00:00", "2012-04-12
>       00:00:00",
>       "2012-04-12 06:00:00", "2012-04-12 12:00:00", "2012-04-12
>       18:00:00",
>       "2012-04-13 00:00:00", "2012-04-13 06:00:00", "2012-04-13
>       12:00:00",
>       "2012-04-13 18:00:00", "2012-04-14 00:00:00", "2012-04-14
>       06:00:00",
>       "2012-04-14 12:00:00", "2012-04-14 18:00:00", "2012-04-15
>       00:00:00",
>       "2012-04-15 06:00:00", "2012-04-15 12:00:00", "2012-04-15
>       18:00:00",
>       "2012-04-16 00:00:00", "2012-04-16 06:00:00", "2012-04-16
>       12:00:00",
>       "2012-04-16 18:00:00", "2012-04-17 00:00:00", "2012-04-17
>       06:00:00",
>       "2012-04-17 12:00:00", "2012-04-17 18:00:00", "2012-04-18
>       00:00:00",
>       "2012-04-18 06:00:00", "2012-04-18 12:00:00", "2012-04-18
>       18:00:00",
>       "2012-04-19 00:00:00", "2012-04-19 06:00:00", "2012-04-19
>       12:00:00",
>       "2012-04-19 18:00:00", "2012-04-20 00:00:00", "2012-04-20
>       06:00:00",
>       "2012-04-20 12:00:00", "2012-04-20 18:00:00", "2012-04-21
>       00:00:00",
>       "2012-04-21 06:00:00", "2012-04-21 12:00:00", "2012-04-21
>       18:00:00",
>       "2012-04-22 00:00:00", "2012-04-22 06:00:00", "2012-04-22
>       12:00:00",
>       "2012-04-22 18:00:00", "2012-04-23 00:00:00", "2012-04-23
>       06:00:00",
>       "2012-04-23 12:00:00", "2012-04-23 18:00:00", "2012-04-24
>       00:00:00",
>       "2012-04-24 06:00:00", "2012-04-24 12:00:00", "2012-04-24
>       18:00:00",
>       "2012-04-25 00:00:00", "2012-04-25 06:00:00", "2012-04-25
>       12:00:00",
>       "2012-04-25 18:00:00", "2012-04-26 00:00:00", "2012-04-26
>       06:00:00",
>       "2012-04-26 12:00:00", "2012-04-26 18:00:00", "2012-04-27
>       00:00:00",
>       "2012-04-27 06:00:00", "2012-04-27 12:00:00", "2012-04-27
>       18:00:00",
>       "2012-04-28 00:00:00", "2012-04-28 06:00:00", "2012-04-28
>       12:00:00",
>       "2012-04-28 18:00:00", "2012-04-29 00:00:00", "2012-04-29
>       06:00:00",
>       "2012-04-29 12:00:00", "2012-04-29 18:00:00", "2012-04-30
>       00:00:00",
>       "2012-04-30 06:00:00", "2012-04-30 12:00:00", "2012-04-30
>       18:00:00",
>       "2012-05-01 00:00:00", "2012-05-01 06:00:00", "2012-05-01
>       12:00:00",
>       "2012-05-01 18:00:00", "2012-05-02 00:00:00", "2012-05-02
>       06:00:00",
>       "2012-05-02 12:00:00", "2012-05-02 18:00:00", "2012-05-03
>       00:00:00",
>       "2012-05-03 06:00:00", "2012-05-03 12:00:00", "2012-05-03
>       18:00:00",
>       "2012-05-04 00:00:00", "2012-05-04 06:00:00", "2012-05-04
>       12:00:00",
>       "2012-05-04 18:00:00", "2012-05-05 00:00:00", "2012-05-05
>       06:00:00",
>       "2012-05-05 12:00:00", "2012-05-05 18:00:00", "2012-05-06
>       00:00:00",
>       "2012-05-06 06:00:00", "2012-05-06 12:00:00", "2012-05-06
>       18:00:00",
>       "2012-05-07 00:00:00", "2012-05-07 06:00:00", "2012-05-07
>       12:00:00",
>       "2012-05-07 18:00:00", "2012-05-08 00:00:00", "2012-05-08
>       06:00:00",
>       "2012-05-08 12:00:00", "2012-05-08 18:00:00", "2012-05-09
>       00:00:00",
>       "2012-05-09 06:00:00", "2012-05-09 12:00:00", "2012-05-09
>       18:00:00",
>       "2012-05-10 00:00:00", "2012-05-10 06:00:00", "2012-05-10
>       12:00:00",
>       "2012-05-10 18:00:00", "2012-05-11 00:00:00", "2012-05-11
>       06:00:00",
>       "2012-05-11 12:00:00", "2012-05-11 18:00:00", "2012-05-12
>       00:00:00",
>       "2012-05-12 06:00:00", "2012-05-12 12:00:00", "2012-05-12
>       18:00:00",
>       "2012-05-13 00:00:00", "2012-05-13 06:00:00", "2012-05-13
>       12:00:00",
>       "2012-05-13 18:00:00", "2012-05-14 00:00:00", "2012-05-14
>       06:00:00",
>       "2012-05-14 12:00:00", "2012-05-14 18:00:00", "2012-05-15
>       00:00:00",
>       "2012-05-15 06:00:00", "2012-05-15 12:00:00", "2012-05-15
>       18:00:00",
>       "2012-05-16 00:00:00", "2012-05-16 06:00:00", "2012-05-16
>       12:00:00",
>       "2012-05-16 18:00:00", "2012-05-17 00:00:00", "2012-05-17
>       06:00:00",
>       "2012-05-17 12:00:00", "2012-05-17 18:00:00", "2012-05-18
>       00:00:00",
>       "2012-05-18 06:00:00", "2012-05-18 12:00:00", "2012-05-18
>       18:00:00",
>       "2012-05-19 00:00:00", "2012-05-19 06:00:00", "2012-05-19
>       12:00:00",
>       "2012-05-19 18:00:00", "2012-05-20 00:00:00", "2012-05-20
>       06:00:00",
>       "2012-05-20 12:00:00", "2012-05-20 18:00:00", "2012-05-21
>       00:00:00",
>       "2012-05-21 06:00:00", "2012-05-21 12:00:00", "2012-05-21
>       18:00:00",
>       "2012-05-22 00:00:00", "2012-05-22 06:00:00", "2012-05-22
>       12:00:00",
>       "2012-05-22 18:00:00", "2012-05-23 00:00:00", "2012-05-23
>       06:00:00",
>       "2012-05-23 12:00:00", "2012-05-23 18:00:00", "2012-05-24
>       00:00:00",
>       "2012-05-24 06:00:00", "2012-05-24 12:00:00", "2012-05-24
>       18:00:00",
>       "2012-05-25 00:00:00", "2012-05-25 06:00:00", "2012-05-25
>       12:00:00",
>       "2012-05-25 18:00:00", "2012-05-26 00:00:00", "2012-05-26
>       06:00:00",
>       "2012-05-26 12:00:00", "2012-05-26 18:00:00", "2012-05-27
>       00:00:00",
>       "2012-05-27 06:00:00", "2012-05-27 12:00:00", "2012-05-27
>       18:00:00",
>       "2012-05-28 00:00:00", "2012-05-28 06:00:00", "2012-05-28
>       12:00:00",
>       "2012-05-28 18:00:00", "2012-05-29 00:00:00", "2012-05-29
>       06:00:00",
>       "2012-05-29 12:00:00"), class = "factor"), basistime =
>       structure(c(1L,
>       1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>       1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>       2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>       3L), .Label = c("2012-01-25 15:02:00", "2012-01-26 15:11:00",
>       "2012-01-27 01:41:00", "2012-01-27 10:15:00", "2012-01-27
>       15:15:00",
>       "2012-01-28 14:22:00", "2012-01-29 13:33:00", "2012-01-30
>       14:11:00",
>       "2012-01-31 14:24:00", "2012-02-01 14:55:00", "2012-02-02
>       14:56:00",
>       "2012-02-03 14:40:00", "2012-02-04 15:01:00", "2012-02-05
>       15:04:00",
>       "2012-02-06 14:37:00", "2012-02-07 14:42:00", "2012-02-08
>       14:44:00",
>       "2012-02-09 14:27:00", "2012-02-10 14:33:00", "2012-02-11
>       15:05:00",
>       "2012-02-12 15:09:00", "2012-02-13 15:00:00", "2012-02-14
>       15:10:00",
>       "2012-02-15 14:46:00", "2012-02-16 14:42:00", "2012-02-17
>       14:14:00",
>       "2012-02-18 15:27:00", "2012-02-19 13:13:00", "2012-02-20
>       13:56:00",
>       "2012-02-21 14:39:00", "2012-02-22 15:33:00", "2012-02-23
>       15:14:00",
>       "2012-02-24 15:14:00", "2012-02-25 14:25:00", "2012-02-26
>       14:08:00",
>       "2012-02-27 14:55:00", "2012-02-28 14:40:00", "2012-02-29
>       14:45:00",
>       "2012-03-01 15:18:00", "2012-03-02 15:18:00", "2012-03-03
>       15:43:00",
>       "2012-03-04 15:16:00", "2012-03-05 15:17:00", "2012-03-06
>       15:03:00",
>       "2012-03-07 14:44:00", "2012-03-08 14:53:00", "2012-03-09
>       15:10:00",
>       "2012-03-10 14:43:00", "2012-03-11 13:37:00", "2012-03-12
>       14:58:00",
>       "2012-03-13 14:36:00", "2012-03-14 13:49:00", "2012-03-15
>       14:32:00",
>       "2012-03-16 15:01:00", "2012-03-17 13:55:00", "2012-03-18
>       13:44:00",
>       "2012-03-19 13:55:00", "2012-03-20 13:53:00", "2012-03-21
>       14:07:00",
>       "2012-03-22 14:03:00", "2012-03-23 14:30:00", "2012-03-24
>       14:47:00",
>       "2012-03-25 14:32:00", "2012-03-26 14:41:00", "2012-03-27
>       13:51:00",
>       "2012-03-28 13:32:00", "2012-03-29 13:59:00", "2012-03-30
>       14:15:00",
>       "2012-03-31 14:12:00", "2012-04-01 14:30:00", "2012-04-02
>       13:43:00",
>       "2012-04-03 13:54:00", "2012-04-04 13:45:00", "2012-04-05
>       13:46:00",
>       "2012-04-06 13:41:00", "2012-04-07 13:13:00", "2012-04-08
>       12:59:00",
>       "2012-04-09 13:53:00", "2012-04-10 14:15:00", "2012-04-11
>       14:26:00",
>       "2012-04-12 14:05:00", "2012-04-13 13:37:00", "2012-04-14
>       13:52:00",
>       "2012-04-15 15:00:00", "2012-04-16 14:42:00", "2012-04-17
>       14:21:00",
>       "2012-04-18 14:40:00", "2012-04-19 14:35:00", "2012-04-20
>       15:14:00",
>       "2012-04-21 14:46:00", "2012-04-22 14:44:00", "2012-04-23
>       14:33:00",
>       "2012-04-24 14:41:00", "2012-04-25 14:13:00", "2012-04-26
>       14:19:00",
>       "2012-04-27 14:35:00", "2012-04-28 13:48:00", "2012-04-29
>       14:12:00",
>       "2012-04-30 13:53:00", "2012-05-02 14:41:00", "2012-05-03
>       15:01:00",
>       "2012-05-04 15:21:00", "2012-05-05 14:57:00", "2012-05-06
>       14:09:00",
>       "2012-05-07 14:30:00", "2012-05-08 14:05:00", "2012-05-08
>       21:21:00",
>       "2012-05-09 15:02:00", "2012-05-10 14:27:00", "2012-05-11
>       14:47:00",
>       "2012-05-12 13:24:00", "2012-05-13 14:40:00", "2012-05-13
>       19:24:00",
>       "2012-05-14 14:33:00", "2012-05-15 14:39:00", "2012-05-16
>       14:07:00",
>       "2012-05-17 13:42:00", "2012-05-18 14:30:00", "2012-05-19
>       13:59:00",
>       "2012-05-20 14:14:00", "2012-05-21 14:15:00", "2012-05-22
>       14:18:00",
>       "2012-05-23 14:14:00", "2012-05-24 13:52:00"), class =
>       "factor"),
>       ? ? fcst = c(38.7, 38.9, 39.2, 39.8, 40.5, 41.5, 42.5, 43.1,
>       ? ? 43.9, 44.5, 44.8, 45, 45, 45.1, 45.4, 45.8, 46.4, 47, 47.5,
>       ? ? 47.9, 39.2, 40, 41.8, 42.2, 42.8, 44.4, 45.7, 46.2, 46.2,
>       ? ? 46.2, 46.5, 47, 47.5, 47.9, 48.4, 49, 49.5, 49.9, 50.2,
>       50.4,
>       ? ? 41.3, 42.5, 42.8, 42.8, 42.8, 42.9, 43, 43.3, 43.5, 43.8),
>       ? ? usgs = c(38.5, 38.6, 38.6, 38.6, 39.1, 39.8, 41.2, 42.9,
>       ? ? 43.4, 43.6, 43.4, 43.1, 43, 43.1, 43.5, 43.8, 44.1, 44.2,
>       ? ? 44.4, 44.5, 39.1, 39.8, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1,
>       ? ? 43, 43.1, 43.5, 43.8, 44.1, 44.2, 44.4, 44.5, 44.5, 44.5,
>       ? ? 44.6, 44.2, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1, 43, 43.1,
>       ? ? 43.5, 43.8)), .Names = c("date", "basistime", "fcst", "usgs"
>       ), row.names = c(NA, 50L), class = "data.frame")
> 
> Sys.setenv( "Etc/GMT+8" ) # or whatever is appropriate
> #> Error in Sys.setenv("Etc/GMT+8"): all arguments must be named
> 
> dta2 <- dta
> # bad idea to work with dates as factors
> dta2$date <- as.POSIXct( as.character( dta2$date ) )
> dta2$basistime <- as.POSIXct( as.character( dta2$basistime ) )
> 
> # base R solution
> 
> dates <- unique( dta2$date )
> dta2list <- split( dta2, dta2$date )
> grplist <- lapply( dta2list
> ? ? ? ? ? ? ? ? ?, function( DF ) {
> ? ? ? ? ? ? ? ? ? ? ?DF[ which.max( DF$fcst ), ]
> ? ? ? ? ? ? ? ? ? ?}
> ? ? ? ? ? ? ? ? ?)
> result2 <- do.call( rbind, grplist )
> result2
> #>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? date? ? ? ? ? ?basistime fcst
> usgs
> #> 2012-01-25 18:00:00 2012-01-25 18:00:00 2012-01-25 15:02:00 38.7
> 38.5
> #> 2012-01-26 00:00:00 2012-01-26 00:00:00 2012-01-25 15:02:00 38.9
> 38.6
> #> 2012-01-26 06:00:00 2012-01-26 06:00:00 2012-01-25 15:02:00 39.2
> 38.6
> #> 2012-01-26 12:00:00 2012-01-26 12:00:00 2012-01-25 15:02:00 39.8
> 38.6
> #> 2012-01-26 18:00:00 2012-01-26 18:00:00 2012-01-25 15:02:00 40.5
> 39.1
> #> 2012-01-27 00:00:00 2012-01-27 00:00:00 2012-01-25 15:02:00 41.5
> 39.8
> #> 2012-01-27 06:00:00 2012-01-27 06:00:00 2012-01-25 15:02:00 42.5
> 41.2
> #> 2012-01-27 12:00:00 2012-01-27 12:00:00 2012-01-25 15:02:00 43.1
> 42.9
> #> 2012-01-27 18:00:00 2012-01-27 18:00:00 2012-01-25 15:02:00 43.9
> 43.4
> #> 2012-01-28 00:00:00 2012-01-28 00:00:00 2012-01-25 15:02:00 44.5
> 43.6
> #> 2012-01-28 06:00:00 2012-01-28 06:00:00 2012-01-26 15:11:00 45.7
> 43.4
> #> 2012-01-28 12:00:00 2012-01-28 12:00:00 2012-01-26 15:11:00 46.2
> 43.1
> #> 2012-01-28 18:00:00 2012-01-28 18:00:00 2012-01-26 15:11:00 46.2
> 43.0
> #> 2012-01-29 00:00:00 2012-01-29 00:00:00 2012-01-26 15:11:00 46.2
> 43.1
> #> 2012-01-29 06:00:00 2012-01-29 06:00:00 2012-01-26 15:11:00 46.5
> 43.5
> #> 2012-01-29 12:00:00 2012-01-29 12:00:00 2012-01-26 15:11:00 47.0
> 43.8
> #> 2012-01-29 18:00:00 2012-01-29 18:00:00 2012-01-26 15:11:00 47.5
> 44.1
> #> 2012-01-30 00:00:00 2012-01-30 00:00:00 2012-01-26 15:11:00 47.9
> 44.2
> #> 2012-01-30 06:00:00 2012-01-30 06:00:00 2012-01-26 15:11:00 48.4
> 44.4
> #> 2012-01-30 12:00:00 2012-01-30 12:00:00 2012-01-26 15:11:00 49.0
> 44.5
> #> 2012-01-30 18:00:00 2012-01-30 18:00:00 2012-01-26 15:11:00 49.5
> 44.5
> #> 2012-01-31 00:00:00 2012-01-31 00:00:00 2012-01-26 15:11:00 49.9
> 44.5
> #> 2012-01-31 06:00:00 2012-01-31 06:00:00 2012-01-26 15:11:00 50.2
> 44.6
> #> 2012-01-31 12:00:00 2012-01-31 12:00:00 2012-01-26 15:11:00 50.4
> 44.2
> 
> # alternately, use tidyverse
> 
> library(dplyr)
> #>
> #> Attaching package: 'dplyr'
> #> The following objects are masked from 'package:stats':
> #>
> #>? ? ?filter, lag
> #> The following objects are masked from 'package:base':
> #>
> #>? ? ?intersect, setdiff, setequal, union
> result3 <- (? ?dta2
> ? ? ? ? ? ?%>% group_by( date )
> ? ? ? ? ? ?%>% do({
> ? ? ? ? ? ? ? ? ?DF <- .
> ? ? ? ? ? ? ? ? ?DF[ which.max( DF$fcst ), ]
> ? ? ? ? ? ? ? ?})
> ? ? ? ? ? ?%>% ungroup
> ? ? ? ? ? ?%>% as.data.frame
> ? ? ? ? ? ?)
> result3
> #>? ? ? ? ? ? ? ? ? ?date? ? ? ? ? ?basistime fcst usgs
> #> 1? 2012-01-25 18:00:00 2012-01-25 15:02:00 38.7 38.5
> #> 2? 2012-01-26 00:00:00 2012-01-25 15:02:00 38.9 38.6
> #> 3? 2012-01-26 06:00:00 2012-01-25 15:02:00 39.2 38.6
> #> 4? 2012-01-26 12:00:00 2012-01-25 15:02:00 39.8 38.6
> #> 5? 2012-01-26 18:00:00 2012-01-25 15:02:00 40.5 39.1
> #> 6? 2012-01-27 00:00:00 2012-01-25 15:02:00 41.5 39.8
> #> 7? 2012-01-27 06:00:00 2012-01-25 15:02:00 42.5 41.2
> #> 8? 2012-01-27 12:00:00 2012-01-25 15:02:00 43.1 42.9
> #> 9? 2012-01-27 18:00:00 2012-01-25 15:02:00 43.9 43.4
> #> 10 2012-01-28 00:00:00 2012-01-25 15:02:00 44.5 43.6
> #> 11 2012-01-28 06:00:00 2012-01-26 15:11:00 45.7 43.4
> #> 12 2012-01-28 12:00:00 2012-01-26 15:11:00 46.2 43.1
> #> 13 2012-01-28 18:00:00 2012-01-26 15:11:00 46.2 43.0
> #> 14 2012-01-29 00:00:00 2012-01-26 15:11:00 46.2 43.1
> #> 15 2012-01-29 06:00:00 2012-01-26 15:11:00 46.5 43.5
> #> 16 2012-01-29 12:00:00 2012-01-26 15:11:00 47.0 43.8
> #> 17 2012-01-29 18:00:00 2012-01-26 15:11:00 47.5 44.1
> #> 18 2012-01-30 00:00:00 2012-01-26 15:11:00 47.9 44.2
> #> 19 2012-01-30 06:00:00 2012-01-26 15:11:00 48.4 44.4
> #> 20 2012-01-30 12:00:00 2012-01-26 15:11:00 49.0 44.5
> #> 21 2012-01-30 18:00:00 2012-01-26 15:11:00 49.5 44.5
> #> 22 2012-01-31 00:00:00 2012-01-26 15:11:00 49.9 44.5
> #> 23 2012-01-31 06:00:00 2012-01-26 15:11:00 50.2 44.6
> #> 24 2012-01-31 12:00:00 2012-01-26 15:11:00 50.4 44.2
> 
> ####################################################
> 
> 
> On Thu, 26 Oct 2017, Thomas Adams wrote:
>
>       Hello all!
>
>       I've been struggling with is for many hours today; I'm
>       close to getting
>       what I want, but not close enough...
>
>       I have a dataframe consisting of two date-time columns
>       followed by two
>       numeric columns. what I need is the max value (in the
>       first numeric column)
>       based on the 2nd date-time column, which is essentially a
>       factor. But, I
>       want the result to provide both date-time values
>       corresponding to the max
>       value.
>
>       My data:
>
>       structure(list(date = structure(c(1L, 2L, 3L, 4L, 5L, 6L,
>       7L,
>       8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L,
>       20L,
>       5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,
>       17L, 18L,
>       19L, 20L, 21L, 22L, 23L, 24L, 7L, 8L, 9L, 10L, 11L, 12L,
>       13L,
>       14L, 15L, 16L), .Label = c("2012-01-25 18:00:00",
>       "2012-01-26 00:00:00",
>       "2012-01-26 06:00:00", "2012-01-26 12:00:00", "2012-01-26
>       18:00:00",
>       "2012-01-27 00:00:00", "2012-01-27 06:00:00", "2012-01-27
>       12:00:00",
>       "2012-01-27 18:00:00", "2012-01-28 00:00:00", "2012-01-28
>       06:00:00",
>       "2012-01-28 12:00:00", "2012-01-28 18:00:00", "2012-01-29
>       00:00:00",
>       "2012-01-29 06:00:00", "2012-01-29 12:00:00", "2012-01-29
>       18:00:00",
>       "2012-01-30 00:00:00", "2012-01-30 06:00:00", "2012-01-30
>       12:00:00",
>       "2012-01-30 18:00:00", "2012-01-31 00:00:00", "2012-01-31
>       06:00:00",
>       "2012-01-31 12:00:00", "2012-01-31 13:00:00", "2012-01-31
>       18:00:00",
>       "2012-02-01 00:00:00", "2012-02-01 06:00:00", "2012-02-01
>       12:00:00",
>       "2012-02-01 18:00:00", "2012-02-02 00:00:00", "2012-02-02
>       06:00:00",
>       "2012-02-02 12:00:00", "2012-02-02 18:00:00", "2012-02-03
>       00:00:00",
>       "2012-02-03 06:00:00", "2012-02-03 12:00:00", "2012-02-03
>       18:00:00",
>       "2012-02-04 00:00:00", "2012-02-04 06:00:00", "2012-02-04
>       12:00:00",
>       "2012-02-04 18:00:00", "2012-02-05 00:00:00", "2012-02-05
>       06:00:00",
>       "2012-02-05 12:00:00", "2012-02-05 18:00:00", "2012-02-06
>       00:00:00",
>       "2012-02-06 06:00:00", "2012-02-06 12:00:00", "2012-02-06
>       18:00:00",
>       "2012-02-07 00:00:00", "2012-02-07 06:00:00", "2012-02-07
>       12:00:00",
>       "2012-02-07 18:00:00", "2012-02-08 00:00:00", "2012-02-08
>       06:00:00",
>       "2012-02-08 12:00:00", "2012-02-08 18:00:00", "2012-02-09
>       00:00:00",
>       "2012-02-09 06:00:00", "2012-02-09 12:00:00", "2012-02-09
>       18:00:00",
>       "2012-02-10 00:00:00", "2012-02-10 06:00:00", "2012-02-10
>       12:00:00",
>       "2012-02-10 18:00:00", "2012-02-11 00:00:00", "2012-02-11
>       06:00:00",
>       "2012-02-11 12:00:00", "2012-02-11 18:00:00", "2012-02-12
>       00:00:00",
>       "2012-02-12 06:00:00", "2012-02-12 12:00:00", "2012-02-12
>       18:00:00",
>       "2012-02-13 00:00:00", "2012-02-13 06:00:00", "2012-02-13
>       12:00:00",
>       "2012-02-13 18:00:00", "2012-02-14 00:00:00", "2012-02-14
>       06:00:00",
>       "2012-02-14 12:00:00", "2012-02-14 18:00:00", "2012-02-15
>       00:00:00",
>       "2012-02-15 06:00:00", "2012-02-15 12:00:00", "2012-02-15
>       18:00:00",
>       "2012-02-16 00:00:00", "2012-02-16 06:00:00", "2012-02-16
>       12:00:00",
>       "2012-02-16 18:00:00", "2012-02-17 00:00:00", "2012-02-17
>       06:00:00",
>       "2012-02-17 12:00:00", "2012-02-17 18:00:00", "2012-02-18
>       00:00:00",
>       "2012-02-18 06:00:00", "2012-02-18 12:00:00", "2012-02-18
>       18:00:00",
>       "2012-02-19 00:00:00", "2012-02-19 06:00:00", "2012-02-19
>       12:00:00",
>       "2012-02-19 18:00:00", "2012-02-20 00:00:00", "2012-02-20
>       06:00:00",
>       "2012-02-20 12:00:00", "2012-02-20 18:00:00", "2012-02-21
>       00:00:00",
>       "2012-02-21 06:00:00", "2012-02-21 12:00:00", "2012-02-21
>       18:00:00",
>       "2012-02-22 00:00:00", "2012-02-22 06:00:00", "2012-02-22
>       12:00:00",
>       "2012-02-22 18:00:00", "2012-02-23 00:00:00", "2012-02-23
>       06:00:00",
>       "2012-02-23 12:00:00", "2012-02-23 18:00:00", "2012-02-24
>       00:00:00",
>       "2012-02-24 06:00:00", "2012-02-24 12:00:00", "2012-02-24
>       18:00:00",
>       "2012-02-25 00:00:00", "2012-02-25 06:00:00", "2012-02-25
>       12:00:00",
>       "2012-02-25 18:00:00", "2012-02-26 00:00:00", "2012-02-26
>       06:00:00",
>       "2012-02-26 12:00:00", "2012-02-26 18:00:00", "2012-02-27
>       00:00:00",
>       "2012-02-27 06:00:00", "2012-02-27 12:00:00", "2012-02-27
>       18:00:00",
>       "2012-02-28 00:00:00", "2012-02-28 06:00:00", "2012-02-28
>       12:00:00",
>       "2012-02-28 18:00:00", "2012-02-29 00:00:00", "2012-02-29
>       06:00:00",
>       "2012-02-29 12:00:00", "2012-02-29 18:00:00", "2012-03-01
>       00:00:00",
>       "2012-03-01 06:00:00", "2012-03-01 12:00:00", "2012-03-01
>       18:00:00",
>       "2012-03-02 00:00:00", "2012-03-02 06:00:00", "2012-03-02
>       12:00:00",
>       "2012-03-02 18:00:00", "2012-03-03 00:00:00", "2012-03-03
>       06:00:00",
>       "2012-03-03 12:00:00", "2012-03-03 18:00:00", "2012-03-04
>       00:00:00",
>       "2012-03-04 06:00:00", "2012-03-04 12:00:00", "2012-03-04
>       18:00:00",
>       "2012-03-05 00:00:00", "2012-03-05 06:00:00", "2012-03-05
>       12:00:00",
>       "2012-03-05 18:00:00", "2012-03-06 00:00:00", "2012-03-06
>       06:00:00",
>       "2012-03-06 12:00:00", "2012-03-06 18:00:00", "2012-03-07
>       00:00:00",
>       "2012-03-07 06:00:00", "2012-03-07 12:00:00", "2012-03-07
>       18:00:00",
>       "2012-03-08 00:00:00", "2012-03-08 06:00:00", "2012-03-08
>       12:00:00",
>       "2012-03-08 18:00:00", "2012-03-09 00:00:00", "2012-03-09
>       06:00:00",
>       "2012-03-09 12:00:00", "2012-03-09 18:00:00", "2012-03-10
>       00:00:00",
>       "2012-03-10 06:00:00", "2012-03-10 12:00:00", "2012-03-10
>       18:00:00",
>       "2012-03-11 00:00:00", "2012-03-11 06:00:00", "2012-03-11
>       12:00:00",
>       "2012-03-11 18:00:00", "2012-03-12 00:00:00", "2012-03-12
>       06:00:00",
>       "2012-03-12 12:00:00", "2012-03-12 18:00:00", "2012-03-13
>       00:00:00",
>       "2012-03-13 06:00:00", "2012-03-13 12:00:00", "2012-03-13
>       18:00:00",
>       "2012-03-14 00:00:00", "2012-03-14 06:00:00", "2012-03-14
>       12:00:00",
>       "2012-03-14 18:00:00", "2012-03-15 00:00:00", "2012-03-15
>       06:00:00",
>       "2012-03-15 12:00:00", "2012-03-15 18:00:00", "2012-03-16
>       00:00:00",
>       "2012-03-16 06:00:00", "2012-03-16 12:00:00", "2012-03-16
>       18:00:00",
>       "2012-03-17 00:00:00", "2012-03-17 06:00:00", "2012-03-17
>       12:00:00",
>       "2012-03-17 18:00:00", "2012-03-18 00:00:00", "2012-03-18
>       06:00:00",
>       "2012-03-18 12:00:00", "2012-03-18 18:00:00", "2012-03-19
>       00:00:00",
>       "2012-03-19 06:00:00", "2012-03-19 12:00:00", "2012-03-19
>       18:00:00",
>       "2012-03-20 00:00:00", "2012-03-20 06:00:00", "2012-03-20
>       12:00:00",
>       "2012-03-20 18:00:00", "2012-03-21 00:00:00", "2012-03-21
>       06:00:00",
>       "2012-03-21 12:00:00", "2012-03-21 18:00:00", "2012-03-22
>       00:00:00",
>       "2012-03-22 06:00:00", "2012-03-22 12:00:00", "2012-03-22
>       18:00:00",
>       "2012-03-23 00:00:00", "2012-03-23 06:00:00", "2012-03-23
>       12:00:00",
>       "2012-03-23 18:00:00", "2012-03-24 00:00:00", "2012-03-24
>       06:00:00",
>       "2012-03-24 12:00:00", "2012-03-24 18:00:00", "2012-03-25
>       00:00:00",
>       "2012-03-25 06:00:00", "2012-03-25 12:00:00", "2012-03-25
>       18:00:00",
>       "2012-03-26 00:00:00", "2012-03-26 06:00:00", "2012-03-26
>       12:00:00",
>       "2012-03-26 18:00:00", "2012-03-27 00:00:00", "2012-03-27
>       06:00:00",
>       "2012-03-27 12:00:00", "2012-03-27 18:00:00", "2012-03-28
>       00:00:00",
>       "2012-03-28 06:00:00", "2012-03-28 12:00:00", "2012-03-28
>       18:00:00",
>       "2012-03-29 00:00:00", "2012-03-29 06:00:00", "2012-03-29
>       12:00:00",
>       "2012-03-29 18:00:00", "2012-03-30 00:00:00", "2012-03-30
>       06:00:00",
>       "2012-03-30 12:00:00", "2012-03-30 18:00:00", "2012-03-31
>       00:00:00",
>       "2012-03-31 06:00:00", "2012-03-31 12:00:00", "2012-03-31
>       18:00:00",
>       "2012-04-01 00:00:00", "2012-04-01 06:00:00", "2012-04-01
>       12:00:00",
>       "2012-04-01 18:00:00", "2012-04-02 00:00:00", "2012-04-02
>       06:00:00",
>       "2012-04-02 12:00:00", "2012-04-02 18:00:00", "2012-04-03
>       00:00:00",
>       "2012-04-03 06:00:00", "2012-04-03 12:00:00", "2012-04-03
>       18:00:00",
>       "2012-04-04 00:00:00", "2012-04-04 06:00:00", "2012-04-04
>       12:00:00",
>       "2012-04-04 18:00:00", "2012-04-05 00:00:00", "2012-04-05
>       06:00:00",
>       "2012-04-05 12:00:00", "2012-04-05 18:00:00", "2012-04-06
>       00:00:00",
>       "2012-04-06 06:00:00", "2012-04-06 12:00:00", "2012-04-06
>       18:00:00",
>       "2012-04-07 00:00:00", "2012-04-07 06:00:00", "2012-04-07
>       12:00:00",
>       "2012-04-07 18:00:00", "2012-04-08 00:00:00", "2012-04-08
>       06:00:00",
>       "2012-04-08 12:00:00", "2012-04-08 18:00:00", "2012-04-09
>       00:00:00",
>       "2012-04-09 06:00:00", "2012-04-09 12:00:00", "2012-04-09
>       18:00:00",
>       "2012-04-10 00:00:00", "2012-04-10 06:00:00", "2012-04-10
>       12:00:00",
>       "2012-04-10 18:00:00", "2012-04-11 00:00:00", "2012-04-11
>       06:00:00",
>       "2012-04-11 12:00:00", "2012-04-11 18:00:00", "2012-04-12
>       00:00:00",
>       "2012-04-12 06:00:00", "2012-04-12 12:00:00", "2012-04-12
>       18:00:00",
>       "2012-04-13 00:00:00", "2012-04-13 06:00:00", "2012-04-13
>       12:00:00",
>       "2012-04-13 18:00:00", "2012-04-14 00:00:00", "2012-04-14
>       06:00:00",
>       "2012-04-14 12:00:00", "2012-04-14 18:00:00", "2012-04-15
>       00:00:00",
>       "2012-04-15 06:00:00", "2012-04-15 12:00:00", "2012-04-15
>       18:00:00",
>       "2012-04-16 00:00:00", "2012-04-16 06:00:00", "2012-04-16
>       12:00:00",
>       "2012-04-16 18:00:00", "2012-04-17 00:00:00", "2012-04-17
>       06:00:00",
>       "2012-04-17 12:00:00", "2012-04-17 18:00:00", "2012-04-18
>       00:00:00",
>       "2012-04-18 06:00:00", "2012-04-18 12:00:00", "2012-04-18
>       18:00:00",
>       "2012-04-19 00:00:00", "2012-04-19 06:00:00", "2012-04-19
>       12:00:00",
>       "2012-04-19 18:00:00", "2012-04-20 00:00:00", "2012-04-20
>       06:00:00",
>       "2012-04-20 12:00:00", "2012-04-20 18:00:00", "2012-04-21
>       00:00:00",
>       "2012-04-21 06:00:00", "2012-04-21 12:00:00", "2012-04-21
>       18:00:00",
>       "2012-04-22 00:00:00", "2012-04-22 06:00:00", "2012-04-22
>       12:00:00",
>       "2012-04-22 18:00:00", "2012-04-23 00:00:00", "2012-04-23
>       06:00:00",
>       "2012-04-23 12:00:00", "2012-04-23 18:00:00", "2012-04-24
>       00:00:00",
>       "2012-04-24 06:00:00", "2012-04-24 12:00:00", "2012-04-24
>       18:00:00",
>       "2012-04-25 00:00:00", "2012-04-25 06:00:00", "2012-04-25
>       12:00:00",
>       "2012-04-25 18:00:00", "2012-04-26 00:00:00", "2012-04-26
>       06:00:00",
>       "2012-04-26 12:00:00", "2012-04-26 18:00:00", "2012-04-27
>       00:00:00",
>       "2012-04-27 06:00:00", "2012-04-27 12:00:00", "2012-04-27
>       18:00:00",
>       "2012-04-28 00:00:00", "2012-04-28 06:00:00", "2012-04-28
>       12:00:00",
>       "2012-04-28 18:00:00", "2012-04-29 00:00:00", "2012-04-29
>       06:00:00",
>       "2012-04-29 12:00:00", "2012-04-29 18:00:00", "2012-04-30
>       00:00:00",
>       "2012-04-30 06:00:00", "2012-04-30 12:00:00", "2012-04-30
>       18:00:00",
>       "2012-05-01 00:00:00", "2012-05-01 06:00:00", "2012-05-01
>       12:00:00",
>       "2012-05-01 18:00:00", "2012-05-02 00:00:00", "2012-05-02
>       06:00:00",
>       "2012-05-02 12:00:00", "2012-05-02 18:00:00", "2012-05-03
>       00:00:00",
>       "2012-05-03 06:00:00", "2012-05-03 12:00:00", "2012-05-03
>       18:00:00",
>       "2012-05-04 00:00:00", "2012-05-04 06:00:00", "2012-05-04
>       12:00:00",
>       "2012-05-04 18:00:00", "2012-05-05 00:00:00", "2012-05-05
>       06:00:00",
>       "2012-05-05 12:00:00", "2012-05-05 18:00:00", "2012-05-06
>       00:00:00",
>       "2012-05-06 06:00:00", "2012-05-06 12:00:00", "2012-05-06
>       18:00:00",
>       "2012-05-07 00:00:00", "2012-05-07 06:00:00", "2012-05-07
>       12:00:00",
>       "2012-05-07 18:00:00", "2012-05-08 00:00:00", "2012-05-08
>       06:00:00",
>       "2012-05-08 12:00:00", "2012-05-08 18:00:00", "2012-05-09
>       00:00:00",
>       "2012-05-09 06:00:00", "2012-05-09 12:00:00", "2012-05-09
>       18:00:00",
>       "2012-05-10 00:00:00", "2012-05-10 06:00:00", "2012-05-10
>       12:00:00",
>       "2012-05-10 18:00:00", "2012-05-11 00:00:00", "2012-05-11
>       06:00:00",
>       "2012-05-11 12:00:00", "2012-05-11 18:00:00", "2012-05-12
>       00:00:00",
>       "2012-05-12 06:00:00", "2012-05-12 12:00:00", "2012-05-12
>       18:00:00",
>       "2012-05-13 00:00:00", "2012-05-13 06:00:00", "2012-05-13
>       12:00:00",
>       "2012-05-13 18:00:00", "2012-05-14 00:00:00", "2012-05-14
>       06:00:00",
>       "2012-05-14 12:00:00", "2012-05-14 18:00:00", "2012-05-15
>       00:00:00",
>       "2012-05-15 06:00:00", "2012-05-15 12:00:00", "2012-05-15
>       18:00:00",
>       "2012-05-16 00:00:00", "2012-05-16 06:00:00", "2012-05-16
>       12:00:00",
>       "2012-05-16 18:00:00", "2012-05-17 00:00:00", "2012-05-17
>       06:00:00",
>       "2012-05-17 12:00:00", "2012-05-17 18:00:00", "2012-05-18
>       00:00:00",
>       "2012-05-18 06:00:00", "2012-05-18 12:00:00", "2012-05-18
>       18:00:00",
>       "2012-05-19 00:00:00", "2012-05-19 06:00:00", "2012-05-19
>       12:00:00",
>       "2012-05-19 18:00:00", "2012-05-20 00:00:00", "2012-05-20
>       06:00:00",
>       "2012-05-20 12:00:00", "2012-05-20 18:00:00", "2012-05-21
>       00:00:00",
>       "2012-05-21 06:00:00", "2012-05-21 12:00:00", "2012-05-21
>       18:00:00",
>       "2012-05-22 00:00:00", "2012-05-22 06:00:00", "2012-05-22
>       12:00:00",
>       "2012-05-22 18:00:00", "2012-05-23 00:00:00", "2012-05-23
>       06:00:00",
>       "2012-05-23 12:00:00", "2012-05-23 18:00:00", "2012-05-24
>       00:00:00",
>       "2012-05-24 06:00:00", "2012-05-24 12:00:00", "2012-05-24
>       18:00:00",
>       "2012-05-25 00:00:00", "2012-05-25 06:00:00", "2012-05-25
>       12:00:00",
>       "2012-05-25 18:00:00", "2012-05-26 00:00:00", "2012-05-26
>       06:00:00",
>       "2012-05-26 12:00:00", "2012-05-26 18:00:00", "2012-05-27
>       00:00:00",
>       "2012-05-27 06:00:00", "2012-05-27 12:00:00", "2012-05-27
>       18:00:00",
>       "2012-05-28 00:00:00", "2012-05-28 06:00:00", "2012-05-28
>       12:00:00",
>       "2012-05-28 18:00:00", "2012-05-29 00:00:00", "2012-05-29
>       06:00:00",
>       "2012-05-29 12:00:00"), class = "factor"), basistime =
>       structure(c(1L,
>       1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>       1L, 1L,
>       1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>       2L, 2L,
>       2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>       3L, 3L,
>       3L), .Label = c("2012-01-25 15:02:00", "2012-01-26
>       15:11:00",
>       "2012-01-27 01:41:00", "2012-01-27 10:15:00", "2012-01-27
>       15:15:00",
>       "2012-01-28 14:22:00", "2012-01-29 13:33:00", "2012-01-30
>       14:11:00",
>       "2012-01-31 14:24:00", "2012-02-01 14:55:00", "2012-02-02
>       14:56:00",
>       "2012-02-03 14:40:00", "2012-02-04 15:01:00", "2012-02-05
>       15:04:00",
>       "2012-02-06 14:37:00", "2012-02-07 14:42:00", "2012-02-08
>       14:44:00",
>       "2012-02-09 14:27:00", "2012-02-10 14:33:00", "2012-02-11
>       15:05:00",
>       "2012-02-12 15:09:00", "2012-02-13 15:00:00", "2012-02-14
>       15:10:00",
>       "2012-02-15 14:46:00", "2012-02-16 14:42:00", "2012-02-17
>       14:14:00",
>       "2012-02-18 15:27:00", "2012-02-19 13:13:00", "2012-02-20
>       13:56:00",
>       "2012-02-21 14:39:00", "2012-02-22 15:33:00", "2012-02-23
>       15:14:00",
>       "2012-02-24 15:14:00", "2012-02-25 14:25:00", "2012-02-26
>       14:08:00",
>       "2012-02-27 14:55:00", "2012-02-28 14:40:00", "2012-02-29
>       14:45:00",
>       "2012-03-01 15:18:00", "2012-03-02 15:18:00", "2012-03-03
>       15:43:00",
>       "2012-03-04 15:16:00", "2012-03-05 15:17:00", "2012-03-06
>       15:03:00",
>       "2012-03-07 14:44:00", "2012-03-08 14:53:00", "2012-03-09
>       15:10:00",
>       "2012-03-10 14:43:00", "2012-03-11 13:37:00", "2012-03-12
>       14:58:00",
>       "2012-03-13 14:36:00", "2012-03-14 13:49:00", "2012-03-15
>       14:32:00",
>       "2012-03-16 15:01:00", "2012-03-17 13:55:00", "2012-03-18
>       13:44:00",
>       "2012-03-19 13:55:00", "2012-03-20 13:53:00", "2012-03-21
>       14:07:00",
>       "2012-03-22 14:03:00", "2012-03-23 14:30:00", "2012-03-24
>       14:47:00",
>       "2012-03-25 14:32:00", "2012-03-26 14:41:00", "2012-03-27
>       13:51:00",
>       "2012-03-28 13:32:00", "2012-03-29 13:59:00", "2012-03-30
>       14:15:00",
>       "2012-03-31 14:12:00", "2012-04-01 14:30:00", "2012-04-02
>       13:43:00",
>       "2012-04-03 13:54:00", "2012-04-04 13:45:00", "2012-04-05
>       13:46:00",
>       "2012-04-06 13:41:00", "2012-04-07 13:13:00", "2012-04-08
>       12:59:00",
>       "2012-04-09 13:53:00", "2012-04-10 14:15:00", "2012-04-11
>       14:26:00",
>       "2012-04-12 14:05:00", "2012-04-13 13:37:00", "2012-04-14
>       13:52:00",
>       "2012-04-15 15:00:00", "2012-04-16 14:42:00", "2012-04-17
>       14:21:00",
>       "2012-04-18 14:40:00", "2012-04-19 14:35:00", "2012-04-20
>       15:14:00",
>       "2012-04-21 14:46:00", "2012-04-22 14:44:00", "2012-04-23
>       14:33:00",
>       "2012-04-24 14:41:00", "2012-04-25 14:13:00", "2012-04-26
>       14:19:00",
>       "2012-04-27 14:35:00", "2012-04-28 13:48:00", "2012-04-29
>       14:12:00",
>       "2012-04-30 13:53:00", "2012-05-02 14:41:00", "2012-05-03
>       15:01:00",
>       "2012-05-04 15:21:00", "2012-05-05 14:57:00", "2012-05-06
>       14:09:00",
>       "2012-05-07 14:30:00", "2012-05-08 14:05:00", "2012-05-08
>       21:21:00",
>       "2012-05-09 15:02:00", "2012-05-10 14:27:00", "2012-05-11
>       14:47:00",
>       "2012-05-12 13:24:00", "2012-05-13 14:40:00", "2012-05-13
>       19:24:00",
>       "2012-05-14 14:33:00", "2012-05-15 14:39:00", "2012-05-16
>       14:07:00",
>       "2012-05-17 13:42:00", "2012-05-18 14:30:00", "2012-05-19
>       13:59:00",
>       "2012-05-20 14:14:00", "2012-05-21 14:15:00", "2012-05-22
>       14:18:00",
>       "2012-05-23 14:14:00", "2012-05-24 13:52:00"), class =
>       "factor"),
>       ? ?fcst = c(38.7, 38.9, 39.2, 39.8, 40.5, 41.5, 42.5,
>       43.1,
>       ? ?43.9, 44.5, 44.8, 45, 45, 45.1, 45.4, 45.8, 46.4, 47,
>       47.5,
>       ? ?47.9, 39.2, 40, 41.8, 42.2, 42.8, 44.4, 45.7, 46.2,
>       46.2,
>       ? ?46.2, 46.5, 47, 47.5, 47.9, 48.4, 49, 49.5, 49.9, 50.2,
>       50.4,
>       ? ?41.3, 42.5, 42.8, 42.8, 42.8, 42.9, 43, 43.3, 43.5,
>       43.8),
>       ? ?usgs = c(38.5, 38.6, 38.6, 38.6, 39.1, 39.8, 41.2,
>       42.9,
>       ? ?43.4, 43.6, 43.4, 43.1, 43, 43.1, 43.5, 43.8, 44.1,
>       44.2,
>       ? ?44.4, 44.5, 39.1, 39.8, 41.2, 42.9, 43.4, 43.6, 43.4,
>       43.1,
>       ? ?43, 43.1, 43.5, 43.8, 44.1, 44.2, 44.4, 44.5, 44.5,
>       44.5,
>       ? ?44.6, 44.2, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1, 43,
>       43.1,
>       ? ?43.5, 43.8)), .Names = c("date", "basistime", "fcst",
>       "usgs"
>       ), row.names = c(NA, 50L), class = "data.frame")
>
>       aggregate(fcst ~ basistime,data=legacy, FUN= max)
>
>       A snippet...
>
>       ? ? ? ? ? ? ?basistime fcst
>       1? ?2012-01-25 15:02:00 47.9
>       2? ?2012-01-26 15:11:00 50.4
>       3? ?2012-01-27 01:41:00 46.0
>       4? ?2012-01-27 10:15:00 47.3
>       5? ?2012-01-27 15:15:00 47.3
>       6? ?2012-01-28 14:22:00 46.2
>       7? ?2012-01-29 13:33:00 45.8
>       8? ?2012-01-30 14:11:00 44.8
>       9? ?2012-01-31 14:24:00 43.9
>       10? 2012-02-01 14:55:00 41.1
>       11? 2012-02-02 14:56:00 38.1
>       12? 2012-02-03 14:40:00 36.2
>       13? 2012-02-04 15:01:00 34.7
>       14? 2012-02-05 15:04:00 33.1
>       15? 2012-02-06 14:37:00 32.2
>
>       This is what I want, except I need the other corresponding
>       date-time column
>       as well. I've tried this with the date-times as factors
>       and as POSIXct
>       values and using many of the combinations suggested in the
>       documentation
>       and other examples. The closest I can get returns the
>       second date as a
>       numeric value:
>
>       aggregate(cbind(fcst,date) ~ basistime,data=legacy, FUN=
>       max)
>
>       Produces:
>
>       ? ? ? ? ? ? ?basistime fcst? ? ? ?date
>       1? ?2012-01-25 15:02:00 47.9 1327942800
>       2? ?2012-01-26 15:11:00 50.4 1328029200
>       3? ?2012-01-27 01:41:00 46.0 1328072400
>       4? ?2012-01-27 10:15:00 47.3 1328032800
>       5? ?2012-01-27 15:15:00 47.3 1328115600
>       6? ?2012-01-28 14:22:00 46.2 1328202000
>       7? ?2012-01-29 13:33:00 45.8 1328288400
>       8? ?2012-01-30 14:11:00 44.8 1328374800
>       9? ?2012-01-31 14:24:00 43.9 1328461200
>       10? 2012-02-01 14:55:00 41.1 1328547600
>       11? 2012-02-02 14:56:00 38.1 1328634000
>       12? 2012-02-03 14:40:00 36.2 1328720400
>       13? 2012-02-04 15:01:00 34.7 1328806800
>       14? 2012-02-05 15:04:00 33.1 1328893200
>       15? 2012-02-06 14:37:00 32.2 1328979600
>       16? 2012-02-07 14:42:00 31.2 1329066000
>       17? 2012-02-08 14:44:00 30.4 1329152400
>       18? 2012-02-09 14:27:00 30.0 1329238800
> 
>
>       Help is greatly appreciated!
>
>       Regards,
>       Tom
>
>       --
> 
> ? ? ? ? [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible
> code.
> 
> 
> ---------------------------------------------------------------------------
> 
> Jeff Newmiller? ? ? ? ? ? ? ? ? ? ? ? The? ? ?.....? ? ? ?.....? Go
> Live...
> DCN:<jdnewmil at dcn.davis.ca.us>? ? ? ? Basics: ##.#.? ? ? ?##.#.? Live
> Go...
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Live:? ?OO#.. Dead: OO#..?
> Playing
> Research Engineer (Solar/Batteries? ? ? ? ? ? O.O#.? ? ? ?#.O#.? with
> /Software/Embedded Controllers)? ? ? ? ? ? ? ?.OO#.? ? ? ?.OO#.?
> rocks...1k
> ---------------------------------------------------------------------------
> 
> 
> 
> 
> --
> Thomas E Adams, III1724 Sage Lane
> Blacksburg, VA 24060
> tea3rd at gmail.com (personal)
> tea at terrapredictions.org (work)
> 
> 1 (513) 739-9512 (cell)
> 
> 
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
---------------------------------------------------------------------------

From djnordlund at gmail.com  Thu Oct 26 19:03:09 2017
From: djnordlund at gmail.com (Daniel Nordlund)
Date: Thu, 26 Oct 2017 10:03:09 -0700
Subject: [R] Help needed with aggregate or other solution
In-Reply-To: <CAGxgkWg=ReVYhW9rvyp6UDOdOL0c6CQQYaHDfGM2tey_iPXgZw@mail.gmail.com>
References: <CAGxgkWjb8kZkt16OjvzZHZjTG_vbxvjAkfPov=pF9M5MN8vMOQ@mail.gmail.com>
 <alpine.BSF.2.00.1710252216410.34411@pedal.dcn.davis.ca.us>
 <CAGxgkWg=ReVYhW9rvyp6UDOdOL0c6CQQYaHDfGM2tey_iPXgZw@mail.gmail.com>
Message-ID: <9c89d25f-318c-be4e-7484-871d25441b2f@gmail.com>

On 10/26/2017 4:58 AM, Thomas Adams wrote:
> Hi Jeff,
> 
> Thank you for the suggestions -- I appreciate your help. Unfortunately, the
> result2 has two problems...
> 
> (1) there are now 3 date columns (it looks like 2 cols are merged into 1
> col)
> (2) the output rows should not have any of the basistime dates repeated
> (maybe I misstated the problem); I need the max fcst value by basistime,
> but also list the date value for that row; for example:
> 
>               basistime fcst
> 1   2012-01-25 15:02:00 47.9
> 2   2012-01-26 15:11:00 50.4
> 3   2012-01-27 01:41:00 46.0
> 4   2012-01-27 10:15:00 47.3
> 5   2012-01-27 15:15:00 47.3
> 6   2012-01-28 14:22:00 46.2
> 7   2012-01-29 13:33:00 45.8
> 8   2012-01-30 14:11:00 44.8
> 9   2012-01-31 14:24:00 43.9
> 10  2012-02-01 14:55:00 41.1
> 11  2012-02-02 14:56:00 38.1
> 12  2012-02-03 14:40:00 36.2
> 13  2012-02-04 15:01:00 34.7
> 14  2012-02-05 15:04:00 33.1
> 15  2012-02-06 14:37:00 32.2
> 
> This is very close to what I need. The basistime dates are all unique, with
> the max fcst value for the available basistime dates; but I additionally
> need the corresponding 'date' value.
> 
> Best,
> Tom
> 
> 
> 
> On Thu, Oct 26, 2017 at 1:28 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
> 
>> Thanks for the dput...
>>
>> #### reproducible example of split-apply-combine ###
>>
>> dta <- structure(list(date = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L,
>>
>> 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
>> 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
>> 19L, 20L, 21L, 22L, 23L, 24L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
>> 14L, 15L, 16L), .Label = c("2012-01-25 18:00:00", "2012-01-26 00:00:00",
>> "2012-01-26 06:00:00", "2012-01-26 12:00:00", "2012-01-26 18:00:00",
>> "2012-01-27 00:00:00", "2012-01-27 06:00:00", "2012-01-27 12:00:00",
>> "2012-01-27 18:00:00", "2012-01-28 00:00:00", "2012-01-28 06:00:00",
>> "2012-01-28 12:00:00", "2012-01-28 18:00:00", "2012-01-29 00:00:00",
>> "2012-01-29 06:00:00", "2012-01-29 12:00:00", "2012-01-29 18:00:00",
>> "2012-01-30 00:00:00", "2012-01-30 06:00:00", "2012-01-30 12:00:00",
>> "2012-01-30 18:00:00", "2012-01-31 00:00:00", "2012-01-31 06:00:00",
>> "2012-01-31 12:00:00", "2012-01-31 13:00:00", "2012-01-31 18:00:00",
>> "2012-02-01 00:00:00", "2012-02-01 06:00:00", "2012-02-01 12:00:00",
>> "2012-02-01 18:00:00", "2012-02-02 00:00:00", "2012-02-02 06:00:00",
>> "2012-02-02 12:00:00", "2012-02-02 18:00:00", "2012-02-03 00:00:00",
>> "2012-02-03 06:00:00", "2012-02-03 12:00:00", "2012-02-03 18:00:00",
>> "2012-02-04 00:00:00", "2012-02-04 06:00:00", "2012-02-04 12:00:00",
>> "2012-02-04 18:00:00", "2012-02-05 00:00:00", "2012-02-05 06:00:00",
>> "2012-02-05 12:00:00", "2012-02-05 18:00:00", "2012-02-06 00:00:00",
>> "2012-02-06 06:00:00", "2012-02-06 12:00:00", "2012-02-06 18:00:00",
>> "2012-02-07 00:00:00", "2012-02-07 06:00:00", "2012-02-07 12:00:00",
>> "2012-02-07 18:00:00", "2012-02-08 00:00:00", "2012-02-08 06:00:00",
>> "2012-02-08 12:00:00", "2012-02-08 18:00:00", "2012-02-09 00:00:00",
>> "2012-02-09 06:00:00", "2012-02-09 12:00:00", "2012-02-09 18:00:00",
>> "2012-02-10 00:00:00", "2012-02-10 06:00:00", "2012-02-10 12:00:00",
>> "2012-02-10 18:00:00", "2012-02-11 00:00:00", "2012-02-11 06:00:00",
>> "2012-02-11 12:00:00", "2012-02-11 18:00:00", "2012-02-12 00:00:00",
>> "2012-02-12 06:00:00", "2012-02-12 12:00:00", "2012-02-12 18:00:00",
>> "2012-02-13 00:00:00", "2012-02-13 06:00:00", "2012-02-13 12:00:00",
>> "2012-02-13 18:00:00", "2012-02-14 00:00:00", "2012-02-14 06:00:00",
>> "2012-02-14 12:00:00", "2012-02-14 18:00:00", "2012-02-15 00:00:00",
>> "2012-02-15 06:00:00", "2012-02-15 12:00:00", "2012-02-15 18:00:00",
>> "2012-02-16 00:00:00", "2012-02-16 06:00:00", "2012-02-16 12:00:00",
>> "2012-02-16 18:00:00", "2012-02-17 00:00:00", "2012-02-17 06:00:00",
>> "2012-02-17 12:00:00", "2012-02-17 18:00:00", "2012-02-18 00:00:00",
>> "2012-02-18 06:00:00", "2012-02-18 12:00:00", "2012-02-18 18:00:00",
>> "2012-02-19 00:00:00", "2012-02-19 06:00:00", "2012-02-19 12:00:00",
>> "2012-02-19 18:00:00", "2012-02-20 00:00:00", "2012-02-20 06:00:00",
>> "2012-02-20 12:00:00", "2012-02-20 18:00:00", "2012-02-21 00:00:00",
>> "2012-02-21 06:00:00", "2012-02-21 12:00:00", "2012-02-21 18:00:00",
>> "2012-02-22 00:00:00", "2012-02-22 06:00:00", "2012-02-22 12:00:00",
>> "2012-02-22 18:00:00", "2012-02-23 00:00:00", "2012-02-23 06:00:00",
>> "2012-02-23 12:00:00", "2012-02-23 18:00:00", "2012-02-24 00:00:00",
>> "2012-02-24 06:00:00", "2012-02-24 12:00:00", "2012-02-24 18:00:00",
>> "2012-02-25 00:00:00", "2012-02-25 06:00:00", "2012-02-25 12:00:00",
>> "2012-02-25 18:00:00", "2012-02-26 00:00:00", "2012-02-26 06:00:00",
>> "2012-02-26 12:00:00", "2012-02-26 18:00:00", "2012-02-27 00:00:00",
>> "2012-02-27 06:00:00", "2012-02-27 12:00:00", "2012-02-27 18:00:00",
>> "2012-02-28 00:00:00", "2012-02-28 06:00:00", "2012-02-28 12:00:00",
>> "2012-02-28 18:00:00", "2012-02-29 00:00:00", "2012-02-29 06:00:00",
>> "2012-02-29 12:00:00", "2012-02-29 18:00:00", "2012-03-01 00:00:00",
>> "2012-03-01 06:00:00", "2012-03-01 12:00:00", "2012-03-01 18:00:00",
>> "2012-03-02 00:00:00", "2012-03-02 06:00:00", "2012-03-02 12:00:00",
>> "2012-03-02 18:00:00", "2012-03-03 00:00:00", "2012-03-03 06:00:00",
>> "2012-03-03 12:00:00", "2012-03-03 18:00:00", "2012-03-04 00:00:00",
>> "2012-03-04 06:00:00", "2012-03-04 12:00:00", "2012-03-04 18:00:00",
>> "2012-03-05 00:00:00", "2012-03-05 06:00:00", "2012-03-05 12:00:00",
>> "2012-03-05 18:00:00", "2012-03-06 00:00:00", "2012-03-06 06:00:00",
>> "2012-03-06 12:00:00", "2012-03-06 18:00:00", "2012-03-07 00:00:00",
>> "2012-03-07 06:00:00", "2012-03-07 12:00:00", "2012-03-07 18:00:00",
>> "2012-03-08 00:00:00", "2012-03-08 06:00:00", "2012-03-08 12:00:00",
>> "2012-03-08 18:00:00", "2012-03-09 00:00:00", "2012-03-09 06:00:00",
>> "2012-03-09 12:00:00", "2012-03-09 18:00:00", "2012-03-10 00:00:00",
>> "2012-03-10 06:00:00", "2012-03-10 12:00:00", "2012-03-10 18:00:00",
>> "2012-03-11 00:00:00", "2012-03-11 06:00:00", "2012-03-11 12:00:00",
>> "2012-03-11 18:00:00", "2012-03-12 00:00:00", "2012-03-12 06:00:00",
>> "2012-03-12 12:00:00", "2012-03-12 18:00:00", "2012-03-13 00:00:00",
>> "2012-03-13 06:00:00", "2012-03-13 12:00:00", "2012-03-13 18:00:00",
>> "2012-03-14 00:00:00", "2012-03-14 06:00:00", "2012-03-14 12:00:00",
>> "2012-03-14 18:00:00", "2012-03-15 00:00:00", "2012-03-15 06:00:00",
>> "2012-03-15 12:00:00", "2012-03-15 18:00:00", "2012-03-16 00:00:00",
>> "2012-03-16 06:00:00", "2012-03-16 12:00:00", "2012-03-16 18:00:00",
>> "2012-03-17 00:00:00", "2012-03-17 06:00:00", "2012-03-17 12:00:00",
>> "2012-03-17 18:00:00", "2012-03-18 00:00:00", "2012-03-18 06:00:00",
>> "2012-03-18 12:00:00", "2012-03-18 18:00:00", "2012-03-19 00:00:00",
>> "2012-03-19 06:00:00", "2012-03-19 12:00:00", "2012-03-19 18:00:00",
>> "2012-03-20 00:00:00", "2012-03-20 06:00:00", "2012-03-20 12:00:00",
>> "2012-03-20 18:00:00", "2012-03-21 00:00:00", "2012-03-21 06:00:00",
>> "2012-03-21 12:00:00", "2012-03-21 18:00:00", "2012-03-22 00:00:00",
>> "2012-03-22 06:00:00", "2012-03-22 12:00:00", "2012-03-22 18:00:00",
>> "2012-03-23 00:00:00", "2012-03-23 06:00:00", "2012-03-23 12:00:00",
>> "2012-03-23 18:00:00", "2012-03-24 00:00:00", "2012-03-24 06:00:00",
>> "2012-03-24 12:00:00", "2012-03-24 18:00:00", "2012-03-25 00:00:00",
>> "2012-03-25 06:00:00", "2012-03-25 12:00:00", "2012-03-25 18:00:00",
>> "2012-03-26 00:00:00", "2012-03-26 06:00:00", "2012-03-26 12:00:00",
>> "2012-03-26 18:00:00", "2012-03-27 00:00:00", "2012-03-27 06:00:00",
>> "2012-03-27 12:00:00", "2012-03-27 18:00:00", "2012-03-28 00:00:00",
>> "2012-03-28 06:00:00", "2012-03-28 12:00:00", "2012-03-28 18:00:00",
>> "2012-03-29 00:00:00", "2012-03-29 06:00:00", "2012-03-29 12:00:00",
>> "2012-03-29 18:00:00", "2012-03-30 00:00:00", "2012-03-30 06:00:00",
>> "2012-03-30 12:00:00", "2012-03-30 18:00:00", "2012-03-31 00:00:00",
>> "2012-03-31 06:00:00", "2012-03-31 12:00:00", "2012-03-31 18:00:00",
>> "2012-04-01 00:00:00", "2012-04-01 06:00:00", "2012-04-01 12:00:00",
>> "2012-04-01 18:00:00", "2012-04-02 00:00:00", "2012-04-02 06:00:00",
>> "2012-04-02 12:00:00", "2012-04-02 18:00:00", "2012-04-03 00:00:00",
>> "2012-04-03 06:00:00", "2012-04-03 12:00:00", "2012-04-03 18:00:00",
>> "2012-04-04 00:00:00", "2012-04-04 06:00:00", "2012-04-04 12:00:00",
>> "2012-04-04 18:00:00", "2012-04-05 00:00:00", "2012-04-05 06:00:00",
>> "2012-04-05 12:00:00", "2012-04-05 18:00:00", "2012-04-06 00:00:00",
>> "2012-04-06 06:00:00", "2012-04-06 12:00:00", "2012-04-06 18:00:00",
>> "2012-04-07 00:00:00", "2012-04-07 06:00:00", "2012-04-07 12:00:00",
>> "2012-04-07 18:00:00", "2012-04-08 00:00:00", "2012-04-08 06:00:00",
>> "2012-04-08 12:00:00", "2012-04-08 18:00:00", "2012-04-09 00:00:00",
>> "2012-04-09 06:00:00", "2012-04-09 12:00:00", "2012-04-09 18:00:00",
>> "2012-04-10 00:00:00", "2012-04-10 06:00:00", "2012-04-10 12:00:00",
>> "2012-04-10 18:00:00", "2012-04-11 00:00:00", "2012-04-11 06:00:00",
>> "2012-04-11 12:00:00", "2012-04-11 18:00:00", "2012-04-12 00:00:00",
>> "2012-04-12 06:00:00", "2012-04-12 12:00:00", "2012-04-12 18:00:00",
>> "2012-04-13 00:00:00", "2012-04-13 06:00:00", "2012-04-13 12:00:00",
>> "2012-04-13 18:00:00", "2012-04-14 00:00:00", "2012-04-14 06:00:00",
>> "2012-04-14 12:00:00", "2012-04-14 18:00:00", "2012-04-15 00:00:00",
>> "2012-04-15 06:00:00", "2012-04-15 12:00:00", "2012-04-15 18:00:00",
>> "2012-04-16 00:00:00", "2012-04-16 06:00:00", "2012-04-16 12:00:00",
>> "2012-04-16 18:00:00", "2012-04-17 00:00:00", "2012-04-17 06:00:00",
>> "2012-04-17 12:00:00", "2012-04-17 18:00:00", "2012-04-18 00:00:00",
>> "2012-04-18 06:00:00", "2012-04-18 12:00:00", "2012-04-18 18:00:00",
>> "2012-04-19 00:00:00", "2012-04-19 06:00:00", "2012-04-19 12:00:00",
>> "2012-04-19 18:00:00", "2012-04-20 00:00:00", "2012-04-20 06:00:00",
>> "2012-04-20 12:00:00", "2012-04-20 18:00:00", "2012-04-21 00:00:00",
>> "2012-04-21 06:00:00", "2012-04-21 12:00:00", "2012-04-21 18:00:00",
>> "2012-04-22 00:00:00", "2012-04-22 06:00:00", "2012-04-22 12:00:00",
>> "2012-04-22 18:00:00", "2012-04-23 00:00:00", "2012-04-23 06:00:00",
>> "2012-04-23 12:00:00", "2012-04-23 18:00:00", "2012-04-24 00:00:00",
>> "2012-04-24 06:00:00", "2012-04-24 12:00:00", "2012-04-24 18:00:00",
>> "2012-04-25 00:00:00", "2012-04-25 06:00:00", "2012-04-25 12:00:00",
>> "2012-04-25 18:00:00", "2012-04-26 00:00:00", "2012-04-26 06:00:00",
>> "2012-04-26 12:00:00", "2012-04-26 18:00:00", "2012-04-27 00:00:00",
>> "2012-04-27 06:00:00", "2012-04-27 12:00:00", "2012-04-27 18:00:00",
>> "2012-04-28 00:00:00", "2012-04-28 06:00:00", "2012-04-28 12:00:00",
>> "2012-04-28 18:00:00", "2012-04-29 00:00:00", "2012-04-29 06:00:00",
>> "2012-04-29 12:00:00", "2012-04-29 18:00:00", "2012-04-30 00:00:00",
>> "2012-04-30 06:00:00", "2012-04-30 12:00:00", "2012-04-30 18:00:00",
>> "2012-05-01 00:00:00", "2012-05-01 06:00:00", "2012-05-01 12:00:00",
>> "2012-05-01 18:00:00", "2012-05-02 00:00:00", "2012-05-02 06:00:00",
>> "2012-05-02 12:00:00", "2012-05-02 18:00:00", "2012-05-03 00:00:00",
>> "2012-05-03 06:00:00", "2012-05-03 12:00:00", "2012-05-03 18:00:00",
>> "2012-05-04 00:00:00", "2012-05-04 06:00:00", "2012-05-04 12:00:00",
>> "2012-05-04 18:00:00", "2012-05-05 00:00:00", "2012-05-05 06:00:00",
>> "2012-05-05 12:00:00", "2012-05-05 18:00:00", "2012-05-06 00:00:00",
>> "2012-05-06 06:00:00", "2012-05-06 12:00:00", "2012-05-06 18:00:00",
>> "2012-05-07 00:00:00", "2012-05-07 06:00:00", "2012-05-07 12:00:00",
>> "2012-05-07 18:00:00", "2012-05-08 00:00:00", "2012-05-08 06:00:00",
>> "2012-05-08 12:00:00", "2012-05-08 18:00:00", "2012-05-09 00:00:00",
>> "2012-05-09 06:00:00", "2012-05-09 12:00:00", "2012-05-09 18:00:00",
>> "2012-05-10 00:00:00", "2012-05-10 06:00:00", "2012-05-10 12:00:00",
>> "2012-05-10 18:00:00", "2012-05-11 00:00:00", "2012-05-11 06:00:00",
>> "2012-05-11 12:00:00", "2012-05-11 18:00:00", "2012-05-12 00:00:00",
>> "2012-05-12 06:00:00", "2012-05-12 12:00:00", "2012-05-12 18:00:00",
>> "2012-05-13 00:00:00", "2012-05-13 06:00:00", "2012-05-13 12:00:00",
>> "2012-05-13 18:00:00", "2012-05-14 00:00:00", "2012-05-14 06:00:00",
>> "2012-05-14 12:00:00", "2012-05-14 18:00:00", "2012-05-15 00:00:00",
>> "2012-05-15 06:00:00", "2012-05-15 12:00:00", "2012-05-15 18:00:00",
>> "2012-05-16 00:00:00", "2012-05-16 06:00:00", "2012-05-16 12:00:00",
>> "2012-05-16 18:00:00", "2012-05-17 00:00:00", "2012-05-17 06:00:00",
>> "2012-05-17 12:00:00", "2012-05-17 18:00:00", "2012-05-18 00:00:00",
>> "2012-05-18 06:00:00", "2012-05-18 12:00:00", "2012-05-18 18:00:00",
>> "2012-05-19 00:00:00", "2012-05-19 06:00:00", "2012-05-19 12:00:00",
>> "2012-05-19 18:00:00", "2012-05-20 00:00:00", "2012-05-20 06:00:00",
>> "2012-05-20 12:00:00", "2012-05-20 18:00:00", "2012-05-21 00:00:00",
>> "2012-05-21 06:00:00", "2012-05-21 12:00:00", "2012-05-21 18:00:00",
>> "2012-05-22 00:00:00", "2012-05-22 06:00:00", "2012-05-22 12:00:00",
>> "2012-05-22 18:00:00", "2012-05-23 00:00:00", "2012-05-23 06:00:00",
>> "2012-05-23 12:00:00", "2012-05-23 18:00:00", "2012-05-24 00:00:00",
>> "2012-05-24 06:00:00", "2012-05-24 12:00:00", "2012-05-24 18:00:00",
>> "2012-05-25 00:00:00", "2012-05-25 06:00:00", "2012-05-25 12:00:00",
>> "2012-05-25 18:00:00", "2012-05-26 00:00:00", "2012-05-26 06:00:00",
>> "2012-05-26 12:00:00", "2012-05-26 18:00:00", "2012-05-27 00:00:00",
>> "2012-05-27 06:00:00", "2012-05-27 12:00:00", "2012-05-27 18:00:00",
>> "2012-05-28 00:00:00", "2012-05-28 06:00:00", "2012-05-28 12:00:00",
>> "2012-05-28 18:00:00", "2012-05-29 00:00:00", "2012-05-29 06:00:00",
>> "2012-05-29 12:00:00"), class = "factor"), basistime = structure(c(1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>> 3L), .Label = c("2012-01-25 15:02:00", "2012-01-26 15:11:00",
>> "2012-01-27 01:41:00", "2012-01-27 10:15:00", "2012-01-27 15:15:00",
>> "2012-01-28 14:22:00", "2012-01-29 13:33:00", "2012-01-30 14:11:00",
>> "2012-01-31 14:24:00", "2012-02-01 14:55:00", "2012-02-02 14:56:00",
>> "2012-02-03 14:40:00", "2012-02-04 15:01:00", "2012-02-05 15:04:00",
>> "2012-02-06 14:37:00", "2012-02-07 14:42:00", "2012-02-08 14:44:00",
>> "2012-02-09 14:27:00", "2012-02-10 14:33:00", "2012-02-11 15:05:00",
>> "2012-02-12 15:09:00", "2012-02-13 15:00:00", "2012-02-14 15:10:00",
>> "2012-02-15 14:46:00", "2012-02-16 14:42:00", "2012-02-17 14:14:00",
>> "2012-02-18 15:27:00", "2012-02-19 13:13:00", "2012-02-20 13:56:00",
>> "2012-02-21 14:39:00", "2012-02-22 15:33:00", "2012-02-23 15:14:00",
>> "2012-02-24 15:14:00", "2012-02-25 14:25:00", "2012-02-26 14:08:00",
>> "2012-02-27 14:55:00", "2012-02-28 14:40:00", "2012-02-29 14:45:00",
>> "2012-03-01 15:18:00", "2012-03-02 15:18:00", "2012-03-03 15:43:00",
>> "2012-03-04 15:16:00", "2012-03-05 15:17:00", "2012-03-06 15:03:00",
>> "2012-03-07 14:44:00", "2012-03-08 14:53:00", "2012-03-09 15:10:00",
>> "2012-03-10 14:43:00", "2012-03-11 13:37:00", "2012-03-12 14:58:00",
>> "2012-03-13 14:36:00", "2012-03-14 13:49:00", "2012-03-15 14:32:00",
>> "2012-03-16 15:01:00", "2012-03-17 13:55:00", "2012-03-18 13:44:00",
>> "2012-03-19 13:55:00", "2012-03-20 13:53:00", "2012-03-21 14:07:00",
>> "2012-03-22 14:03:00", "2012-03-23 14:30:00", "2012-03-24 14:47:00",
>> "2012-03-25 14:32:00", "2012-03-26 14:41:00", "2012-03-27 13:51:00",
>> "2012-03-28 13:32:00", "2012-03-29 13:59:00", "2012-03-30 14:15:00",
>> "2012-03-31 14:12:00", "2012-04-01 14:30:00", "2012-04-02 13:43:00",
>> "2012-04-03 13:54:00", "2012-04-04 13:45:00", "2012-04-05 13:46:00",
>> "2012-04-06 13:41:00", "2012-04-07 13:13:00", "2012-04-08 12:59:00",
>> "2012-04-09 13:53:00", "2012-04-10 14:15:00", "2012-04-11 14:26:00",
>> "2012-04-12 14:05:00", "2012-04-13 13:37:00", "2012-04-14 13:52:00",
>> "2012-04-15 15:00:00", "2012-04-16 14:42:00", "2012-04-17 14:21:00",
>> "2012-04-18 14:40:00", "2012-04-19 14:35:00", "2012-04-20 15:14:00",
>> "2012-04-21 14:46:00", "2012-04-22 14:44:00", "2012-04-23 14:33:00",
>> "2012-04-24 14:41:00", "2012-04-25 14:13:00", "2012-04-26 14:19:00",
>> "2012-04-27 14:35:00", "2012-04-28 13:48:00", "2012-04-29 14:12:00",
>> "2012-04-30 13:53:00", "2012-05-02 14:41:00", "2012-05-03 15:01:00",
>> "2012-05-04 15:21:00", "2012-05-05 14:57:00", "2012-05-06 14:09:00",
>> "2012-05-07 14:30:00", "2012-05-08 14:05:00", "2012-05-08 21:21:00",
>> "2012-05-09 15:02:00", "2012-05-10 14:27:00", "2012-05-11 14:47:00",
>> "2012-05-12 13:24:00", "2012-05-13 14:40:00", "2012-05-13 19:24:00",
>> "2012-05-14 14:33:00", "2012-05-15 14:39:00", "2012-05-16 14:07:00",
>> "2012-05-17 13:42:00", "2012-05-18 14:30:00", "2012-05-19 13:59:00",
>> "2012-05-20 14:14:00", "2012-05-21 14:15:00", "2012-05-22 14:18:00",
>> "2012-05-23 14:14:00", "2012-05-24 13:52:00"), class = "factor"),
>>      fcst = c(38.7, 38.9, 39.2, 39.8, 40.5, 41.5, 42.5, 43.1,
>>      43.9, 44.5, 44.8, 45, 45, 45.1, 45.4, 45.8, 46.4, 47, 47.5,
>>      47.9, 39.2, 40, 41.8, 42.2, 42.8, 44.4, 45.7, 46.2, 46.2,
>>      46.2, 46.5, 47, 47.5, 47.9, 48.4, 49, 49.5, 49.9, 50.2, 50.4,
>>      41.3, 42.5, 42.8, 42.8, 42.8, 42.9, 43, 43.3, 43.5, 43.8),
>>      usgs = c(38.5, 38.6, 38.6, 38.6, 39.1, 39.8, 41.2, 42.9,
>>      43.4, 43.6, 43.4, 43.1, 43, 43.1, 43.5, 43.8, 44.1, 44.2,
>>      44.4, 44.5, 39.1, 39.8, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1,
>>      43, 43.1, 43.5, 43.8, 44.1, 44.2, 44.4, 44.5, 44.5, 44.5,
>>      44.6, 44.2, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1, 43, 43.1,
>>      43.5, 43.8)), .Names = c("date", "basistime", "fcst", "usgs"
>> ), row.names = c(NA, 50L), class = "data.frame")
>>
>> Sys.setenv( "Etc/GMT+8" ) # or whatever is appropriate
>> #> Error in Sys.setenv("Etc/GMT+8"): all arguments must be named
>>
>> dta2 <- dta
>> # bad idea to work with dates as factors
>> dta2$date <- as.POSIXct( as.character( dta2$date ) )
>> dta2$basistime <- as.POSIXct( as.character( dta2$basistime ) )
>>
>> # base R solution
>>
>> dates <- unique( dta2$date )
>> dta2list <- split( dta2, dta2$date )
>> grplist <- lapply( dta2list
>>                   , function( DF ) {
>>                       DF[ which.max( DF$fcst ), ]
>>                     }
>>                   )
>> result2 <- do.call( rbind, grplist )
>> result2
>> #>                                    date           basistime fcst usgs
>> #> 2012-01-25 18:00:00 2012-01-25 18:00:00 2012-01-25 15:02:00 38.7 38.5
>> #> 2012-01-26 00:00:00 2012-01-26 00:00:00 2012-01-25 15:02:00 38.9 38.6
>> #> 2012-01-26 06:00:00 2012-01-26 06:00:00 2012-01-25 15:02:00 39.2 38.6
>> #> 2012-01-26 12:00:00 2012-01-26 12:00:00 2012-01-25 15:02:00 39.8 38.6
>> #> 2012-01-26 18:00:00 2012-01-26 18:00:00 2012-01-25 15:02:00 40.5 39.1
>> #> 2012-01-27 00:00:00 2012-01-27 00:00:00 2012-01-25 15:02:00 41.5 39.8
>> #> 2012-01-27 06:00:00 2012-01-27 06:00:00 2012-01-25 15:02:00 42.5 41.2
>> #> 2012-01-27 12:00:00 2012-01-27 12:00:00 2012-01-25 15:02:00 43.1 42.9
>> #> 2012-01-27 18:00:00 2012-01-27 18:00:00 2012-01-25 15:02:00 43.9 43.4
>> #> 2012-01-28 00:00:00 2012-01-28 00:00:00 2012-01-25 15:02:00 44.5 43.6
>> #> 2012-01-28 06:00:00 2012-01-28 06:00:00 2012-01-26 15:11:00 45.7 43.4
>> #> 2012-01-28 12:00:00 2012-01-28 12:00:00 2012-01-26 15:11:00 46.2 43.1
>> #> 2012-01-28 18:00:00 2012-01-28 18:00:00 2012-01-26 15:11:00 46.2 43.0
>> #> 2012-01-29 00:00:00 2012-01-29 00:00:00 2012-01-26 15:11:00 46.2 43.1
>> #> 2012-01-29 06:00:00 2012-01-29 06:00:00 2012-01-26 15:11:00 46.5 43.5
>> #> 2012-01-29 12:00:00 2012-01-29 12:00:00 2012-01-26 15:11:00 47.0 43.8
>> #> 2012-01-29 18:00:00 2012-01-29 18:00:00 2012-01-26 15:11:00 47.5 44.1
>> #> 2012-01-30 00:00:00 2012-01-30 00:00:00 2012-01-26 15:11:00 47.9 44.2
>> #> 2012-01-30 06:00:00 2012-01-30 06:00:00 2012-01-26 15:11:00 48.4 44.4
>> #> 2012-01-30 12:00:00 2012-01-30 12:00:00 2012-01-26 15:11:00 49.0 44.5
>> #> 2012-01-30 18:00:00 2012-01-30 18:00:00 2012-01-26 15:11:00 49.5 44.5
>> #> 2012-01-31 00:00:00 2012-01-31 00:00:00 2012-01-26 15:11:00 49.9 44.5
>> #> 2012-01-31 06:00:00 2012-01-31 06:00:00 2012-01-26 15:11:00 50.2 44.6
>> #> 2012-01-31 12:00:00 2012-01-31 12:00:00 2012-01-26 15:11:00 50.4 44.2
>>
>> # alternately, use tidyverse
>>
>> library(dplyr)
>> #>
>> #> Attaching package: 'dplyr'
>> #> The following objects are masked from 'package:stats':
>> #>
>> #>     filter, lag
>> #> The following objects are masked from 'package:base':
>> #>
>> #>     intersect, setdiff, setequal, union
>> result3 <- (   dta2
>>             %>% group_by( date )
>>             %>% do({
>>                   DF <- .
>>                   DF[ which.max( DF$fcst ), ]
>>                 })
>>             %>% ungroup
>>             %>% as.data.frame
>>             )
>> result3
>> #>                   date           basistime fcst usgs
>> #> 1  2012-01-25 18:00:00 2012-01-25 15:02:00 38.7 38.5
>> #> 2  2012-01-26 00:00:00 2012-01-25 15:02:00 38.9 38.6
>> #> 3  2012-01-26 06:00:00 2012-01-25 15:02:00 39.2 38.6
>> #> 4  2012-01-26 12:00:00 2012-01-25 15:02:00 39.8 38.6
>> #> 5  2012-01-26 18:00:00 2012-01-25 15:02:00 40.5 39.1
>> #> 6  2012-01-27 00:00:00 2012-01-25 15:02:00 41.5 39.8
>> #> 7  2012-01-27 06:00:00 2012-01-25 15:02:00 42.5 41.2
>> #> 8  2012-01-27 12:00:00 2012-01-25 15:02:00 43.1 42.9
>> #> 9  2012-01-27 18:00:00 2012-01-25 15:02:00 43.9 43.4
>> #> 10 2012-01-28 00:00:00 2012-01-25 15:02:00 44.5 43.6
>> #> 11 2012-01-28 06:00:00 2012-01-26 15:11:00 45.7 43.4
>> #> 12 2012-01-28 12:00:00 2012-01-26 15:11:00 46.2 43.1
>> #> 13 2012-01-28 18:00:00 2012-01-26 15:11:00 46.2 43.0
>> #> 14 2012-01-29 00:00:00 2012-01-26 15:11:00 46.2 43.1
>> #> 15 2012-01-29 06:00:00 2012-01-26 15:11:00 46.5 43.5
>> #> 16 2012-01-29 12:00:00 2012-01-26 15:11:00 47.0 43.8
>> #> 17 2012-01-29 18:00:00 2012-01-26 15:11:00 47.5 44.1
>> #> 18 2012-01-30 00:00:00 2012-01-26 15:11:00 47.9 44.2
>> #> 19 2012-01-30 06:00:00 2012-01-26 15:11:00 48.4 44.4
>> #> 20 2012-01-30 12:00:00 2012-01-26 15:11:00 49.0 44.5
>> #> 21 2012-01-30 18:00:00 2012-01-26 15:11:00 49.5 44.5
>> #> 22 2012-01-31 00:00:00 2012-01-26 15:11:00 49.9 44.5
>> #> 23 2012-01-31 06:00:00 2012-01-26 15:11:00 50.2 44.6
>> #> 24 2012-01-31 12:00:00 2012-01-26 15:11:00 50.4 44.2
>>
>> ####################################################
>>
>>
>>
>> On Thu, 26 Oct 2017, Thomas Adams wrote:
>>
>> Hello all!
>>>
>>> I've been struggling with is for many hours today; I'm close to getting
>>> what I want, but not close enough...
>>>
>>> I have a dataframe consisting of two date-time columns followed by two
>>> numeric columns. what I need is the max value (in the first numeric
>>> column)
>>> based on the 2nd date-time column, which is essentially a factor. But, I
>>> want the result to provide both date-time values corresponding to the max
>>> value.
>>>
>>> My data:
>>>
>>> structure(list(date = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L,
>>> 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
>>> 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
>>> 19L, 20L, 21L, 22L, 23L, 24L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
>>> 14L, 15L, 16L), .Label = c("2012-01-25 18:00:00", "2012-01-26 00:00:00",
>>> "2012-01-26 06:00:00", "2012-01-26 12:00:00", "2012-01-26 18:00:00",
>>> "2012-01-27 00:00:00", "2012-01-27 06:00:00", "2012-01-27 12:00:00",
>>> "2012-01-27 18:00:00", "2012-01-28 00:00:00", "2012-01-28 06:00:00",
>>> "2012-01-28 12:00:00", "2012-01-28 18:00:00", "2012-01-29 00:00:00",
>>> "2012-01-29 06:00:00", "2012-01-29 12:00:00", "2012-01-29 18:00:00",
>>> "2012-01-30 00:00:00", "2012-01-30 06:00:00", "2012-01-30 12:00:00",
>>> "2012-01-30 18:00:00", "2012-01-31 00:00:00", "2012-01-31 06:00:00",
>>> "2012-01-31 12:00:00", "2012-01-31 13:00:00", "2012-01-31 18:00:00",
>>> "2012-02-01 00:00:00", "2012-02-01 06:00:00", "2012-02-01 12:00:00",
>>> "2012-02-01 18:00:00", "2012-02-02 00:00:00", "2012-02-02 06:00:00",
>>> "2012-02-02 12:00:00", "2012-02-02 18:00:00", "2012-02-03 00:00:00",
>>> "2012-02-03 06:00:00", "2012-02-03 12:00:00", "2012-02-03 18:00:00",
>>> "2012-02-04 00:00:00", "2012-02-04 06:00:00", "2012-02-04 12:00:00",
>>> "2012-02-04 18:00:00", "2012-02-05 00:00:00", "2012-02-05 06:00:00",
>>> "2012-02-05 12:00:00", "2012-02-05 18:00:00", "2012-02-06 00:00:00",
>>> "2012-02-06 06:00:00", "2012-02-06 12:00:00", "2012-02-06 18:00:00",
>>> "2012-02-07 00:00:00", "2012-02-07 06:00:00", "2012-02-07 12:00:00",
>>> "2012-02-07 18:00:00", "2012-02-08 00:00:00", "2012-02-08 06:00:00",
>>> "2012-02-08 12:00:00", "2012-02-08 18:00:00", "2012-02-09 00:00:00",
>>> "2012-02-09 06:00:00", "2012-02-09 12:00:00", "2012-02-09 18:00:00",
>>> "2012-02-10 00:00:00", "2012-02-10 06:00:00", "2012-02-10 12:00:00",
>>> "2012-02-10 18:00:00", "2012-02-11 00:00:00", "2012-02-11 06:00:00",
>>> "2012-02-11 12:00:00", "2012-02-11 18:00:00", "2012-02-12 00:00:00",
>>> "2012-02-12 06:00:00", "2012-02-12 12:00:00", "2012-02-12 18:00:00",
>>> "2012-02-13 00:00:00", "2012-02-13 06:00:00", "2012-02-13 12:00:00",
>>> "2012-02-13 18:00:00", "2012-02-14 00:00:00", "2012-02-14 06:00:00",
>>> "2012-02-14 12:00:00", "2012-02-14 18:00:00", "2012-02-15 00:00:00",
>>> "2012-02-15 06:00:00", "2012-02-15 12:00:00", "2012-02-15 18:00:00",
>>> "2012-02-16 00:00:00", "2012-02-16 06:00:00", "2012-02-16 12:00:00",
>>> "2012-02-16 18:00:00", "2012-02-17 00:00:00", "2012-02-17 06:00:00",
>>> "2012-02-17 12:00:00", "2012-02-17 18:00:00", "2012-02-18 00:00:00",
>>> "2012-02-18 06:00:00", "2012-02-18 12:00:00", "2012-02-18 18:00:00",
>>> "2012-02-19 00:00:00", "2012-02-19 06:00:00", "2012-02-19 12:00:00",
>>> "2012-02-19 18:00:00", "2012-02-20 00:00:00", "2012-02-20 06:00:00",
>>> "2012-02-20 12:00:00", "2012-02-20 18:00:00", "2012-02-21 00:00:00",
>>> "2012-02-21 06:00:00", "2012-02-21 12:00:00", "2012-02-21 18:00:00",
>>> "2012-02-22 00:00:00", "2012-02-22 06:00:00", "2012-02-22 12:00:00",
>>> "2012-02-22 18:00:00", "2012-02-23 00:00:00", "2012-02-23 06:00:00",
>>> "2012-02-23 12:00:00", "2012-02-23 18:00:00", "2012-02-24 00:00:00",
>>> "2012-02-24 06:00:00", "2012-02-24 12:00:00", "2012-02-24 18:00:00",
>>> "2012-02-25 00:00:00", "2012-02-25 06:00:00", "2012-02-25 12:00:00",
>>> "2012-02-25 18:00:00", "2012-02-26 00:00:00", "2012-02-26 06:00:00",
>>> "2012-02-26 12:00:00", "2012-02-26 18:00:00", "2012-02-27 00:00:00",
>>> "2012-02-27 06:00:00", "2012-02-27 12:00:00", "2012-02-27 18:00:00",
>>> "2012-02-28 00:00:00", "2012-02-28 06:00:00", "2012-02-28 12:00:00",
>>> "2012-02-28 18:00:00", "2012-02-29 00:00:00", "2012-02-29 06:00:00",
>>> "2012-02-29 12:00:00", "2012-02-29 18:00:00", "2012-03-01 00:00:00",
>>> "2012-03-01 06:00:00", "2012-03-01 12:00:00", "2012-03-01 18:00:00",
>>> "2012-03-02 00:00:00", "2012-03-02 06:00:00", "2012-03-02 12:00:00",
>>> "2012-03-02 18:00:00", "2012-03-03 00:00:00", "2012-03-03 06:00:00",
>>> "2012-03-03 12:00:00", "2012-03-03 18:00:00", "2012-03-04 00:00:00",
>>> "2012-03-04 06:00:00", "2012-03-04 12:00:00", "2012-03-04 18:00:00",
>>> "2012-03-05 00:00:00", "2012-03-05 06:00:00", "2012-03-05 12:00:00",
>>> "2012-03-05 18:00:00", "2012-03-06 00:00:00", "2012-03-06 06:00:00",
>>> "2012-03-06 12:00:00", "2012-03-06 18:00:00", "2012-03-07 00:00:00",
>>> "2012-03-07 06:00:00", "2012-03-07 12:00:00", "2012-03-07 18:00:00",
>>> "2012-03-08 00:00:00", "2012-03-08 06:00:00", "2012-03-08 12:00:00",
>>> "2012-03-08 18:00:00", "2012-03-09 00:00:00", "2012-03-09 06:00:00",
>>> "2012-03-09 12:00:00", "2012-03-09 18:00:00", "2012-03-10 00:00:00",
>>> "2012-03-10 06:00:00", "2012-03-10 12:00:00", "2012-03-10 18:00:00",
>>> "2012-03-11 00:00:00", "2012-03-11 06:00:00", "2012-03-11 12:00:00",
>>> "2012-03-11 18:00:00", "2012-03-12 00:00:00", "2012-03-12 06:00:00",
>>> "2012-03-12 12:00:00", "2012-03-12 18:00:00", "2012-03-13 00:00:00",
>>> "2012-03-13 06:00:00", "2012-03-13 12:00:00", "2012-03-13 18:00:00",
>>> "2012-03-14 00:00:00", "2012-03-14 06:00:00", "2012-03-14 12:00:00",
>>> "2012-03-14 18:00:00", "2012-03-15 00:00:00", "2012-03-15 06:00:00",
>>> "2012-03-15 12:00:00", "2012-03-15 18:00:00", "2012-03-16 00:00:00",
>>> "2012-03-16 06:00:00", "2012-03-16 12:00:00", "2012-03-16 18:00:00",
>>> "2012-03-17 00:00:00", "2012-03-17 06:00:00", "2012-03-17 12:00:00",
>>> "2012-03-17 18:00:00", "2012-03-18 00:00:00", "2012-03-18 06:00:00",
>>> "2012-03-18 12:00:00", "2012-03-18 18:00:00", "2012-03-19 00:00:00",
>>> "2012-03-19 06:00:00", "2012-03-19 12:00:00", "2012-03-19 18:00:00",
>>> "2012-03-20 00:00:00", "2012-03-20 06:00:00", "2012-03-20 12:00:00",
>>> "2012-03-20 18:00:00", "2012-03-21 00:00:00", "2012-03-21 06:00:00",
>>> "2012-03-21 12:00:00", "2012-03-21 18:00:00", "2012-03-22 00:00:00",
>>> "2012-03-22 06:00:00", "2012-03-22 12:00:00", "2012-03-22 18:00:00",
>>> "2012-03-23 00:00:00", "2012-03-23 06:00:00", "2012-03-23 12:00:00",
>>> "2012-03-23 18:00:00", "2012-03-24 00:00:00", "2012-03-24 06:00:00",
>>> "2012-03-24 12:00:00", "2012-03-24 18:00:00", "2012-03-25 00:00:00",
>>> "2012-03-25 06:00:00", "2012-03-25 12:00:00", "2012-03-25 18:00:00",
>>> "2012-03-26 00:00:00", "2012-03-26 06:00:00", "2012-03-26 12:00:00",
>>> "2012-03-26 18:00:00", "2012-03-27 00:00:00", "2012-03-27 06:00:00",
>>> "2012-03-27 12:00:00", "2012-03-27 18:00:00", "2012-03-28 00:00:00",
>>> "2012-03-28 06:00:00", "2012-03-28 12:00:00", "2012-03-28 18:00:00",
>>> "2012-03-29 00:00:00", "2012-03-29 06:00:00", "2012-03-29 12:00:00",
>>> "2012-03-29 18:00:00", "2012-03-30 00:00:00", "2012-03-30 06:00:00",
>>> "2012-03-30 12:00:00", "2012-03-30 18:00:00", "2012-03-31 00:00:00",
>>> "2012-03-31 06:00:00", "2012-03-31 12:00:00", "2012-03-31 18:00:00",
>>> "2012-04-01 00:00:00", "2012-04-01 06:00:00", "2012-04-01 12:00:00",
>>> "2012-04-01 18:00:00", "2012-04-02 00:00:00", "2012-04-02 06:00:00",
>>> "2012-04-02 12:00:00", "2012-04-02 18:00:00", "2012-04-03 00:00:00",
>>> "2012-04-03 06:00:00", "2012-04-03 12:00:00", "2012-04-03 18:00:00",
>>> "2012-04-04 00:00:00", "2012-04-04 06:00:00", "2012-04-04 12:00:00",
>>> "2012-04-04 18:00:00", "2012-04-05 00:00:00", "2012-04-05 06:00:00",
>>> "2012-04-05 12:00:00", "2012-04-05 18:00:00", "2012-04-06 00:00:00",
>>> "2012-04-06 06:00:00", "2012-04-06 12:00:00", "2012-04-06 18:00:00",
>>> "2012-04-07 00:00:00", "2012-04-07 06:00:00", "2012-04-07 12:00:00",
>>> "2012-04-07 18:00:00", "2012-04-08 00:00:00", "2012-04-08 06:00:00",
>>> "2012-04-08 12:00:00", "2012-04-08 18:00:00", "2012-04-09 00:00:00",
>>> "2012-04-09 06:00:00", "2012-04-09 12:00:00", "2012-04-09 18:00:00",
>>> "2012-04-10 00:00:00", "2012-04-10 06:00:00", "2012-04-10 12:00:00",
>>> "2012-04-10 18:00:00", "2012-04-11 00:00:00", "2012-04-11 06:00:00",
>>> "2012-04-11 12:00:00", "2012-04-11 18:00:00", "2012-04-12 00:00:00",
>>> "2012-04-12 06:00:00", "2012-04-12 12:00:00", "2012-04-12 18:00:00",
>>> "2012-04-13 00:00:00", "2012-04-13 06:00:00", "2012-04-13 12:00:00",
>>> "2012-04-13 18:00:00", "2012-04-14 00:00:00", "2012-04-14 06:00:00",
>>> "2012-04-14 12:00:00", "2012-04-14 18:00:00", "2012-04-15 00:00:00",
>>> "2012-04-15 06:00:00", "2012-04-15 12:00:00", "2012-04-15 18:00:00",
>>> "2012-04-16 00:00:00", "2012-04-16 06:00:00", "2012-04-16 12:00:00",
>>> "2012-04-16 18:00:00", "2012-04-17 00:00:00", "2012-04-17 06:00:00",
>>> "2012-04-17 12:00:00", "2012-04-17 18:00:00", "2012-04-18 00:00:00",
>>> "2012-04-18 06:00:00", "2012-04-18 12:00:00", "2012-04-18 18:00:00",
>>> "2012-04-19 00:00:00", "2012-04-19 06:00:00", "2012-04-19 12:00:00",
>>> "2012-04-19 18:00:00", "2012-04-20 00:00:00", "2012-04-20 06:00:00",
>>> "2012-04-20 12:00:00", "2012-04-20 18:00:00", "2012-04-21 00:00:00",
>>> "2012-04-21 06:00:00", "2012-04-21 12:00:00", "2012-04-21 18:00:00",
>>> "2012-04-22 00:00:00", "2012-04-22 06:00:00", "2012-04-22 12:00:00",
>>> "2012-04-22 18:00:00", "2012-04-23 00:00:00", "2012-04-23 06:00:00",
>>> "2012-04-23 12:00:00", "2012-04-23 18:00:00", "2012-04-24 00:00:00",
>>> "2012-04-24 06:00:00", "2012-04-24 12:00:00", "2012-04-24 18:00:00",
>>> "2012-04-25 00:00:00", "2012-04-25 06:00:00", "2012-04-25 12:00:00",
>>> "2012-04-25 18:00:00", "2012-04-26 00:00:00", "2012-04-26 06:00:00",
>>> "2012-04-26 12:00:00", "2012-04-26 18:00:00", "2012-04-27 00:00:00",
>>> "2012-04-27 06:00:00", "2012-04-27 12:00:00", "2012-04-27 18:00:00",
>>> "2012-04-28 00:00:00", "2012-04-28 06:00:00", "2012-04-28 12:00:00",
>>> "2012-04-28 18:00:00", "2012-04-29 00:00:00", "2012-04-29 06:00:00",
>>> "2012-04-29 12:00:00", "2012-04-29 18:00:00", "2012-04-30 00:00:00",
>>> "2012-04-30 06:00:00", "2012-04-30 12:00:00", "2012-04-30 18:00:00",
>>> "2012-05-01 00:00:00", "2012-05-01 06:00:00", "2012-05-01 12:00:00",
>>> "2012-05-01 18:00:00", "2012-05-02 00:00:00", "2012-05-02 06:00:00",
>>> "2012-05-02 12:00:00", "2012-05-02 18:00:00", "2012-05-03 00:00:00",
>>> "2012-05-03 06:00:00", "2012-05-03 12:00:00", "2012-05-03 18:00:00",
>>> "2012-05-04 00:00:00", "2012-05-04 06:00:00", "2012-05-04 12:00:00",
>>> "2012-05-04 18:00:00", "2012-05-05 00:00:00", "2012-05-05 06:00:00",
>>> "2012-05-05 12:00:00", "2012-05-05 18:00:00", "2012-05-06 00:00:00",
>>> "2012-05-06 06:00:00", "2012-05-06 12:00:00", "2012-05-06 18:00:00",
>>> "2012-05-07 00:00:00", "2012-05-07 06:00:00", "2012-05-07 12:00:00",
>>> "2012-05-07 18:00:00", "2012-05-08 00:00:00", "2012-05-08 06:00:00",
>>> "2012-05-08 12:00:00", "2012-05-08 18:00:00", "2012-05-09 00:00:00",
>>> "2012-05-09 06:00:00", "2012-05-09 12:00:00", "2012-05-09 18:00:00",
>>> "2012-05-10 00:00:00", "2012-05-10 06:00:00", "2012-05-10 12:00:00",
>>> "2012-05-10 18:00:00", "2012-05-11 00:00:00", "2012-05-11 06:00:00",
>>> "2012-05-11 12:00:00", "2012-05-11 18:00:00", "2012-05-12 00:00:00",
>>> "2012-05-12 06:00:00", "2012-05-12 12:00:00", "2012-05-12 18:00:00",
>>> "2012-05-13 00:00:00", "2012-05-13 06:00:00", "2012-05-13 12:00:00",
>>> "2012-05-13 18:00:00", "2012-05-14 00:00:00", "2012-05-14 06:00:00",
>>> "2012-05-14 12:00:00", "2012-05-14 18:00:00", "2012-05-15 00:00:00",
>>> "2012-05-15 06:00:00", "2012-05-15 12:00:00", "2012-05-15 18:00:00",
>>> "2012-05-16 00:00:00", "2012-05-16 06:00:00", "2012-05-16 12:00:00",
>>> "2012-05-16 18:00:00", "2012-05-17 00:00:00", "2012-05-17 06:00:00",
>>> "2012-05-17 12:00:00", "2012-05-17 18:00:00", "2012-05-18 00:00:00",
>>> "2012-05-18 06:00:00", "2012-05-18 12:00:00", "2012-05-18 18:00:00",
>>> "2012-05-19 00:00:00", "2012-05-19 06:00:00", "2012-05-19 12:00:00",
>>> "2012-05-19 18:00:00", "2012-05-20 00:00:00", "2012-05-20 06:00:00",
>>> "2012-05-20 12:00:00", "2012-05-20 18:00:00", "2012-05-21 00:00:00",
>>> "2012-05-21 06:00:00", "2012-05-21 12:00:00", "2012-05-21 18:00:00",
>>> "2012-05-22 00:00:00", "2012-05-22 06:00:00", "2012-05-22 12:00:00",
>>> "2012-05-22 18:00:00", "2012-05-23 00:00:00", "2012-05-23 06:00:00",
>>> "2012-05-23 12:00:00", "2012-05-23 18:00:00", "2012-05-24 00:00:00",
>>> "2012-05-24 06:00:00", "2012-05-24 12:00:00", "2012-05-24 18:00:00",
>>> "2012-05-25 00:00:00", "2012-05-25 06:00:00", "2012-05-25 12:00:00",
>>> "2012-05-25 18:00:00", "2012-05-26 00:00:00", "2012-05-26 06:00:00",
>>> "2012-05-26 12:00:00", "2012-05-26 18:00:00", "2012-05-27 00:00:00",
>>> "2012-05-27 06:00:00", "2012-05-27 12:00:00", "2012-05-27 18:00:00",
>>> "2012-05-28 00:00:00", "2012-05-28 06:00:00", "2012-05-28 12:00:00",
>>> "2012-05-28 18:00:00", "2012-05-29 00:00:00", "2012-05-29 06:00:00",
>>> "2012-05-29 12:00:00"), class = "factor"), basistime = structure(c(1L,
>>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>>> 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>>> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>>> 3L), .Label = c("2012-01-25 15:02:00", "2012-01-26 15:11:00",
>>> "2012-01-27 01:41:00", "2012-01-27 10:15:00", "2012-01-27 15:15:00",
>>> "2012-01-28 14:22:00", "2012-01-29 13:33:00", "2012-01-30 14:11:00",
>>> "2012-01-31 14:24:00", "2012-02-01 14:55:00", "2012-02-02 14:56:00",
>>> "2012-02-03 14:40:00", "2012-02-04 15:01:00", "2012-02-05 15:04:00",
>>> "2012-02-06 14:37:00", "2012-02-07 14:42:00", "2012-02-08 14:44:00",
>>> "2012-02-09 14:27:00", "2012-02-10 14:33:00", "2012-02-11 15:05:00",
>>> "2012-02-12 15:09:00", "2012-02-13 15:00:00", "2012-02-14 15:10:00",
>>> "2012-02-15 14:46:00", "2012-02-16 14:42:00", "2012-02-17 14:14:00",
>>> "2012-02-18 15:27:00", "2012-02-19 13:13:00", "2012-02-20 13:56:00",
>>> "2012-02-21 14:39:00", "2012-02-22 15:33:00", "2012-02-23 15:14:00",
>>> "2012-02-24 15:14:00", "2012-02-25 14:25:00", "2012-02-26 14:08:00",
>>> "2012-02-27 14:55:00", "2012-02-28 14:40:00", "2012-02-29 14:45:00",
>>> "2012-03-01 15:18:00", "2012-03-02 15:18:00", "2012-03-03 15:43:00",
>>> "2012-03-04 15:16:00", "2012-03-05 15:17:00", "2012-03-06 15:03:00",
>>> "2012-03-07 14:44:00", "2012-03-08 14:53:00", "2012-03-09 15:10:00",
>>> "2012-03-10 14:43:00", "2012-03-11 13:37:00", "2012-03-12 14:58:00",
>>> "2012-03-13 14:36:00", "2012-03-14 13:49:00", "2012-03-15 14:32:00",
>>> "2012-03-16 15:01:00", "2012-03-17 13:55:00", "2012-03-18 13:44:00",
>>> "2012-03-19 13:55:00", "2012-03-20 13:53:00", "2012-03-21 14:07:00",
>>> "2012-03-22 14:03:00", "2012-03-23 14:30:00", "2012-03-24 14:47:00",
>>> "2012-03-25 14:32:00", "2012-03-26 14:41:00", "2012-03-27 13:51:00",
>>> "2012-03-28 13:32:00", "2012-03-29 13:59:00", "2012-03-30 14:15:00",
>>> "2012-03-31 14:12:00", "2012-04-01 14:30:00", "2012-04-02 13:43:00",
>>> "2012-04-03 13:54:00", "2012-04-04 13:45:00", "2012-04-05 13:46:00",
>>> "2012-04-06 13:41:00", "2012-04-07 13:13:00", "2012-04-08 12:59:00",
>>> "2012-04-09 13:53:00", "2012-04-10 14:15:00", "2012-04-11 14:26:00",
>>> "2012-04-12 14:05:00", "2012-04-13 13:37:00", "2012-04-14 13:52:00",
>>> "2012-04-15 15:00:00", "2012-04-16 14:42:00", "2012-04-17 14:21:00",
>>> "2012-04-18 14:40:00", "2012-04-19 14:35:00", "2012-04-20 15:14:00",
>>> "2012-04-21 14:46:00", "2012-04-22 14:44:00", "2012-04-23 14:33:00",
>>> "2012-04-24 14:41:00", "2012-04-25 14:13:00", "2012-04-26 14:19:00",
>>> "2012-04-27 14:35:00", "2012-04-28 13:48:00", "2012-04-29 14:12:00",
>>> "2012-04-30 13:53:00", "2012-05-02 14:41:00", "2012-05-03 15:01:00",
>>> "2012-05-04 15:21:00", "2012-05-05 14:57:00", "2012-05-06 14:09:00",
>>> "2012-05-07 14:30:00", "2012-05-08 14:05:00", "2012-05-08 21:21:00",
>>> "2012-05-09 15:02:00", "2012-05-10 14:27:00", "2012-05-11 14:47:00",
>>> "2012-05-12 13:24:00", "2012-05-13 14:40:00", "2012-05-13 19:24:00",
>>> "2012-05-14 14:33:00", "2012-05-15 14:39:00", "2012-05-16 14:07:00",
>>> "2012-05-17 13:42:00", "2012-05-18 14:30:00", "2012-05-19 13:59:00",
>>> "2012-05-20 14:14:00", "2012-05-21 14:15:00", "2012-05-22 14:18:00",
>>> "2012-05-23 14:14:00", "2012-05-24 13:52:00"), class = "factor"),
>>>     fcst = c(38.7, 38.9, 39.2, 39.8, 40.5, 41.5, 42.5, 43.1,
>>>     43.9, 44.5, 44.8, 45, 45, 45.1, 45.4, 45.8, 46.4, 47, 47.5,
>>>     47.9, 39.2, 40, 41.8, 42.2, 42.8, 44.4, 45.7, 46.2, 46.2,
>>>     46.2, 46.5, 47, 47.5, 47.9, 48.4, 49, 49.5, 49.9, 50.2, 50.4,
>>>     41.3, 42.5, 42.8, 42.8, 42.8, 42.9, 43, 43.3, 43.5, 43.8),
>>>     usgs = c(38.5, 38.6, 38.6, 38.6, 39.1, 39.8, 41.2, 42.9,
>>>     43.4, 43.6, 43.4, 43.1, 43, 43.1, 43.5, 43.8, 44.1, 44.2,
>>>     44.4, 44.5, 39.1, 39.8, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1,
>>>     43, 43.1, 43.5, 43.8, 44.1, 44.2, 44.4, 44.5, 44.5, 44.5,
>>>     44.6, 44.2, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1, 43, 43.1,
>>>     43.5, 43.8)), .Names = c("date", "basistime", "fcst", "usgs"
>>> ), row.names = c(NA, 50L), class = "data.frame")
>>>
>>> aggregate(fcst ~ basistime,data=legacy, FUN= max)
>>>
>>> A snippet...
>>>
>>>               basistime fcst
>>> 1   2012-01-25 15:02:00 47.9
>>> 2   2012-01-26 15:11:00 50.4
>>> 3   2012-01-27 01:41:00 46.0
>>> 4   2012-01-27 10:15:00 47.3
>>> 5   2012-01-27 15:15:00 47.3
>>> 6   2012-01-28 14:22:00 46.2
>>> 7   2012-01-29 13:33:00 45.8
>>> 8   2012-01-30 14:11:00 44.8
>>> 9   2012-01-31 14:24:00 43.9
>>> 10  2012-02-01 14:55:00 41.1
>>> 11  2012-02-02 14:56:00 38.1
>>> 12  2012-02-03 14:40:00 36.2
>>> 13  2012-02-04 15:01:00 34.7
>>> 14  2012-02-05 15:04:00 33.1
>>> 15  2012-02-06 14:37:00 32.2
>>>
>>> This is what I want, except I need the other corresponding date-time
>>> column
>>> as well. I've tried this with the date-times as factors and as POSIXct
>>> values and using many of the combinations suggested in the documentation
>>> and other examples. The closest I can get returns the second date as a
>>> numeric value:
>>>
>>> aggregate(cbind(fcst,date) ~ basistime,data=legacy, FUN= max)
>>>
>>> Produces:
>>>
>>>               basistime fcst       date
>>> 1   2012-01-25 15:02:00 47.9 1327942800
>>> 2   2012-01-26 15:11:00 50.4 1328029200
>>> 3   2012-01-27 01:41:00 46.0 1328072400
>>> 4   2012-01-27 10:15:00 47.3 1328032800
>>> 5   2012-01-27 15:15:00 47.3 1328115600
>>> 6   2012-01-28 14:22:00 46.2 1328202000
>>> 7   2012-01-29 13:33:00 45.8 1328288400
>>> 8   2012-01-30 14:11:00 44.8 1328374800
>>> 9   2012-01-31 14:24:00 43.9 1328461200
>>> 10  2012-02-01 14:55:00 41.1 1328547600
>>> 11  2012-02-02 14:56:00 38.1 1328634000
>>> 12  2012-02-03 14:40:00 36.2 1328720400
>>> 13  2012-02-04 15:01:00 34.7 1328806800
>>> 14  2012-02-05 15:04:00 33.1 1328893200
>>> 15  2012-02-06 14:37:00 32.2 1328979600
>>> 16  2012-02-07 14:42:00 31.2 1329066000
>>> 17  2012-02-08 14:44:00 30.4 1329152400
>>> 18  2012-02-09 14:27:00 30.0 1329238800
>>>
>>>
>>> Help is greatly appreciated!
>>>
>>> Regards,
>>> Tom
>>>
>>> --
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>> ng-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>> ------------------------------------------------------------
>> ---------------
>> Jeff Newmiller                        The     .....       .....  Go Live...
>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>> Go...
>>                                        Live:   OO#.. Dead: OO#..  Playing
>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
>> ------------------------------------------------------------
>> ---------------
>>
> 
> 
> 

Thomas,

try this.  If your data structure is called dta, then

 > maxes <- aggregate(dta$fcst, list(dta$basistime), FUN=max,   simplify 
= TRUE, drop = TRUE)
 > want <- merge(dta, maxes, by.x=c('fcst', 'basistime'), 
by.y=c('x','Group.1'))
 > want
   fcst           basistime                date usgs
1 43.8 2012-01-27 01:41:00 2012-01-29 12:00:00 43.8
2 47.9 2012-01-25 15:02:00 2012-01-30 12:00:00 44.5
3 50.4 2012-01-26 15:11:00 2012-01-31 12:00:00 44.2
 >

You can drop the usgs column and reorder the other columns if you wish.


Hope this is helpful,

Dan

-- 
Daniel Nordlund
Port Townsend, WA  USA


From annievanderwyden at yahoo.com  Thu Oct 26 18:42:52 2017
From: annievanderwyden at yahoo.com (Annalisa VanderWyden)
Date: Thu, 26 Oct 2017 16:42:52 +0000 (UTC)
Subject: [R] nomogram function error
References: <1953758089.5479680.1509036172243.ref@mail.yahoo.com>
Message-ID: <1953758089.5479680.1509036172243@mail.yahoo.com>


Hi R-help,

?

I have fit a cox ph model to my data, but have beenreceiving an error when trying to fit a model to the nomogram. Here is the codeand corresponding error:

?

?

>nomogramCF = nomogram(cph_age6_40avp4, 

+????????????????????lp.at= seq(-10,10,by =0.5),lp = TRUE,

+??????????????????????

+??????????????????????funlabel="5year survival",

+??????????????????????fun=surv5.CFdata5,fun.at= c(0.01,seq(0.1,0.9,by=0.2),0.99))

Error inapprox(fu[s], xseq[s], fat, ties = mean) : 

??needat least two non-NA values to interpolate

?

?

I have fit similar nomograms based on cox ph models usingsimilar code, so I?m not sure what I?m doing wrong this time.

?

I heard there might be some an issue because of a softwareupdate?

?

Thanks,

Annie


	[[alternative HTML version deleted]]


From laradutrasilva at gmail.com  Thu Oct 26 19:50:59 2017
From: laradutrasilva at gmail.com (Lara Dutra Silva)
Date: Thu, 26 Oct 2017 17:50:59 +0000
Subject: [R] Changes in code_plot
Message-ID: <CAJRXdJB0D6a037LMYocQByXZLzdk2tqRTkh1eJE9qnUVMeWMwg@mail.gmail.com>

Hello,

I am trying to change some aspects in this code.

plot(proj90PT$Acacia_EMmeanByTSS_mergedAlgo_mergedRun_mergedData, main=
"Present",  xlab ="x", ylab="y", cex.main=1.25, cex.lab=1.2, cex.axis=0.75,
family = "arial")


How can I change

- font. I try family "arial"

- size (exemple 14)

of main; xlab; etc ...


Is it possible to set the plot size?


dev. new (width =, height =)


Regards,

Silva

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Oct 26 20:29:39 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 26 Oct 2017 11:29:39 -0700
Subject: [R] Changes in code_plot
In-Reply-To: <CAJRXdJB0D6a037LMYocQByXZLzdk2tqRTkh1eJE9qnUVMeWMwg@mail.gmail.com>
References: <CAJRXdJB0D6a037LMYocQByXZLzdk2tqRTkh1eJE9qnUVMeWMwg@mail.gmail.com>
Message-ID: <CAGxFJbTm_-F4btngubyGm1X+6zkUpTj-BunqUHj2wDaiv9zvaA@mail.gmail.com>

You are using R's traditional graphics engine.  All of your queries are
answered by ?par (where, for example, you can set plot size) and
?plot.default (where, for example, you can change font sizes, e.g. via
cex). Reading the graphics sections in the "Introduction to R" tutorial
that ships with R would also probably be useful for you. This list is for
R- **Help** -- it cannot replace the many good basic R tutorials that are
available and with which we generally expect posters to have spent some
time before seeking help here.

Also, please do read the posting guide (linked at the bottom fo the email)
for suggestions on how to post requests for help that elicit prompt and
helpful responses if you need to post in the future. Especially the part
about "small reproducible examples" (which probably wasn't needed here,
though).

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Thu, Oct 26, 2017 at 10:50 AM, Lara Dutra Silva <laradutrasilva at gmail.com
> wrote:

> Hello,
>
> I am trying to change some aspects in this code.
>
> plot(proj90PT$Acacia_EMmeanByTSS_mergedAlgo_mergedRun_mergedData, main=
> "Present",  xlab ="x", ylab="y", cex.main=1.25, cex.lab=1.2, cex.axis=0.75,
> family = "arial")
>
>
> How can I change
>
> - font. I try family "arial"
>
> - size (exemple 14)
>
> of main; xlab; etc ...
>
>
> Is it possible to set the plot size?
>
>
> dev. new (width =, height =)
>
>
> Regards,
>
> Silva
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Thu Oct 26 20:36:05 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 26 Oct 2017 11:36:05 -0700
Subject: [R] nomogram function error
In-Reply-To: <1953758089.5479680.1509036172243@mail.yahoo.com>
References: <1953758089.5479680.1509036172243.ref@mail.yahoo.com>
 <1953758089.5479680.1509036172243@mail.yahoo.com>
Message-ID: <B921802C-512D-44C7-AA7C-CB39FA1DA162@comcast.net>


> On Oct 26, 2017, at 9:42 AM, Annalisa VanderWyden via R-help <r-help at r-project.org> wrote:
> 
> 
> Hi R-help,
> 
>  
> 
> I have fit a cox ph model to my data, but have beenreceiving an error when trying to fit a model to the nomogram. Here is the codeand corresponding error:
> 
>  
> 
>  
> 
>> nomogramCF = nomogram(cph_age6_40avp4, 

The only package that I know of having a function named `nomogram` is rms. 
> 
> +                    lp.at= seq(-10,10,by =0.5),lp = TRUE,
> 
> +                      
> 
> +                      funlabel="5year survival",
> 
> +                      fun=surv5.CFdata5,

That should be a function name being passed. But you haven't shown us any function definiton.

> fun.at= c(0.01,seq(0.1,0.9,by=0.2),0.99))
> 
> Error inapprox(fu[s], xseq[s], fat, ties = mean) : 
> 
>   needat least two non-NA values to interpolate
> 
>  
> 
>  
> 
> I have fit similar nomograms based on cox ph models usingsimilar code, so I?m not sure what I?m doing wrong this time.

We are at least as unsure as you and most probably more so. You have not offered a reproducible problem.

> I heard there might be some an issue because of a softwareupdate?

There's often a NEWS for packages. Type:

help(pac=rms)

> 
>  
> 
> Thanks,
> 
> Annie
> 
> 
> 	[[alternative HTML version deleted]]

Please read the Posting Guide linked to in every rhelp posting. 


-- 
David.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From Clifton.SothoronJr at logianalytics.com  Thu Oct 26 21:54:56 2017
From: Clifton.SothoronJr at logianalytics.com (Clifton B. Sothoron Jr.)
Date: Thu, 26 Oct 2017 19:54:56 +0000
Subject: [R] Error in UseMethod("xmlAttrs",
 node) : no applicable method for 'xmlAttrs' applied to an object of
 class "NULL"
Message-ID: <6A395F497CE89A4693420F1134B3FDB8CC82DA92@LogiMail10.LogiXML.local>

I'm running R 3.4.1 on Linux.  I'm getting the following error message.

Error in UseMethod("xmlAttrs", node) : no applicable method for 'xmlAttrs' applied to an object of class "NULL"
Calls: rdKMeans -> rdLoadModel -> xmlAttrs

This appears to be a known issue in R Linux that was supposed to be patched.  Is this really fixed?  Was it fixed in 3.4.2?
https://github.com/lbusett/MODIStsp/issues/51

I just ran "yum update R" from my Centos box and V3.4.2 doesn't show up.  I'm still at 3.4.1.  Does anyone know when the Redhat repository will be updated?

Thanks in advance,

Clifton Sothoron
Logi Analytics Inc.


	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Thu Oct 26 22:47:42 2017
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Thu, 26 Oct 2017 21:47:42 +0100
Subject: [R] Help_urgent_how to calculate mean and sd in biomod 2
In-Reply-To: <CAJRXdJANPiayE4Nrft+DSQ-bO3Nhe1TKix2U9Y0JzB9Vi19oPw@mail.gmail.com>
References: <CAJRXdJCS9GRCXVJNhqDcCQTki4c53Cy077v0mjYjVS8oWwRGzg@mail.gmail.com>
 <59EACFCD.1000103@sapo.pt>
 <CAJRXdJB4xgYfVjzRGQjNHp2cvS9QS_T6unB-=PH1Ff6DcTFVcA@mail.gmail.com>
 <CAJRXdJANPiayE4Nrft+DSQ-bO3Nhe1TKix2U9Y0JzB9Vi19oPw@mail.gmail.com>
Message-ID: <59F249EE.2050404@sapo.pt>

Ol?,

Please keep this in the list, I'm cc-ing r-help at r-project.org. And yes, 
I am Portuguese but R-Help is a mailing list in the English language.

As for your new question, I believe that you should start a new thread. 
This is completely different from the question on computing mean and sd. 
Ask a new question.

Font "arial" is a Microsoft font and as far as I know is not supported 
by R. And 'size' is not a graphical parameter. To see the font families 
supported by R see the help page ?par.

Note that width and height are arguments to function ?windows not to 
function plot.

windows(width = 6, height = 5)    # open a new window
plot(1:10, main = "Teste", family = "serif")

#-----------

sessionInfo()
R version 3.4.2 (2017-09-28)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

Matrix products: default

locale:
[1] LC_COLLATE=Portuguese_Portugal.1252 
LC_CTYPE=Portuguese_Portugal.1252
[3] LC_MONETARY=Portuguese_Portugal.1252 LC_NUMERIC=C 

[5] LC_TIME=Portuguese_Portugal.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] ggplot2_2.2.1   lubridate_1.6.0

loaded via a namespace (and not attached):
  [1] Rcpp_0.12.13     digest_0.6.12    grid_3.4.2       plyr_1.8.4
  [5] gtable_0.2.0     magrittr_1.5     scales_0.5.0     rlang_0.1.2
  [9] stringi_1.1.5    lazyeval_0.2.0   labeling_0.3     tools_3.4.2
[13] stringr_1.2.0    munsell_0.4.3    compiler_3.4.2   colorspace_1.3-2
[17] tibble_1.3.4


Hope this helps,

Rui Barradas

Em 26-10-2017 21:09, Lara Dutra Silva escreveu:
> Boa noite,
>
> N?o sei se portugu?s.
>
> Estou a ter algumas dificuldades na altera??o do tamanho, letra de um
> plot, ou seja, as altera??es b?sicas.
> Em anexo envio o plot
>
> O c?digo que estou a utilizar ? o seguinte:
>
>
> plot(proj90PT$Acacia_EMmeanByTSS_mergedAlgo_mergedRun_mergedData,
> main= "Present", xlab ="a", ylab="b",width=6,height=5)
>
> 1) Queria alterar o tipo de letra para "arial"
> family = "arial" - ocorreu erro
>
> 2) O tamanho da letra
> size =14 - ocorreu erro
>
> 3) font
>
> 4) Tamanho do plot
> (width=6,height=5)
>
>
> N?o sei se poder? ajudar-me. Sei que s?o quest?es muito b?sicas.
> Tentei enviar um e-mail para r-help at r-project.org
> <mailto:r-help at r-project.org>
> Cumprimentos,
>
> Lara Silva
>
>
>
> 2017-10-21 13:46 GMT+00:00 Lara Dutra Silva <laradutrasilva at gmail.com
> <mailto:laradutrasilva at gmail.com>>:
>
>     Thank you for the information.
>
>     Regards,
>
>     Lara Silva
>
>     2017-10-21 4:40 GMT+00:00 Rui Barradas <ruipbarradas at sapo.pt
>     <mailto:ruipbarradas at sapo.pt>>:
>
>         Hello,
>
>         This is because myBiomodModelEval_55["ROC","Testing.data",,,] is
>         a vector not an array or matrix. What the error message is
>         saying is that dim() is not returning a value (it's length is
>         not positive, since it cannot be negative length(dim(.)) must be
>         zero). Try it.
>         Or see the class of those objects.
>
>         class(myBiomodModelEval_55["ROC","Testing.data",,,])
>         class(myBiomodModelEval_55["TSS","Testing.data",,,])
>
>
>         You do not apply(), simply do
>
>         mean(myBiomodModelEval_55["ROC","Testing.data",,,])
>
>         And the same for sd().
>
>         Hope this helps,
>
>         Rui Barradas
>
>
>         Em 21-10-2017 01:11, Lara Dutra Silva escreveu:
>
>             Hello
>
>             I am new in R. I am trying to implement Biomod 2 package.
>
>             However, I have a doubt. I want to calculate the mean and sd of
>             "Testing.data"
>             (ROC and TSS)
>
>
>                 # let's print the ROC scores of all selected models
>
>
>                 myBiomodModelEval_55["ROC","Testing.data",,,]
>
>
>                RUN1  RUN2  RUN3  RUN4  RUN5  RUN6  RUN7  RUN8  RUN9 RUN10
>
>             0.938 0.938 0.926 0.931 0.939 0.918 0.920 0.914 0.935 0.919
>
>
>
>                 # let's print the TSS scores
>
>
>                 myBiomodModelEval_55["TSS","Testing.data",,,]
>
>
>                RUN1  RUN2  RUN3  RUN4  RUN5  RUN6  RUN7  RUN8  RUN9 RUN10
>
>             0.746 0.763 0.717 0.758 0.754 0.704 0.700 0.725 0.742 0.721
>
>
>
>
>
>             I try to use "apply"
>
>             apply(myBiomodModelEval_55["TSS","Testing.data",,,], 1, mean)
>
>             apply(myBiomodModelEval_55["ROC","Testing.data",,,],1, mean)
>
>             apply(myBiomodModelEval_55["ROC","Testing.data",,,], 1, sd)
>
>             apply(myBiomodModelEval_55["TSS","Testing.data",,,], 1, sd)
>
>
>
>             I can not figure it out because it runs error.
>             The problem is in the dimension?
>
>             Error in apply(myBiomodModelEval_55["TSS", "Testing.data", ,
>             , ], 1, mean) :
>
>                 dim(X) must have a positive length
>
>
>
>
>             How can I solve this?
>
>             This is the structure of object
>
>                 dimnames(myBiomodModelEval_55)
>
>             [[1]]
>             [1] "ROC" "TSS"
>
>             [[2]]
>             [1] "Testing.data" "Cutoff"       "Sensitivity"  "Specificity"
>
>             [[3]]
>             [1] "GAM"
>
>             [[4]]
>                [1] "RUN1"  "RUN2"  "RUN3"  "RUN4"  "RUN5"  "RUN6"
>             "RUN7"  "RUN8"  "RUN9"
>             [10] "RUN10"
>
>             [[5]]
>             Acacia_AllData
>                    "AllData"
>
>
>             Regards,
>             Silva
>
>                      [[alternative HTML version deleted]]
>
>             ______________________________________________
>             R-help at r-project.org <mailto:R-help at r-project.org> mailing
>             list -- To UNSUBSCRIBE and more, see
>             https://stat.ethz.ch/mailman/listinfo/r-help
>             <https://stat.ethz.ch/mailman/listinfo/r-help>
>             PLEASE do read the posting guide
>             http://www.R-project.org/posting-guide.html
>             <http://www.R-project.org/posting-guide.html>
>             and provide commented, minimal, self-contained, reproducible
>             code.
>
>
>
>
>     --
>
>     /Lara Dutra da Silva/
>
>
>
>
> --
>
> /Lara Dutra da Silva/


From peter.anthoni at kit.edu  Fri Oct 27 08:20:43 2017
From: peter.anthoni at kit.edu (Anthoni, Peter (IMK))
Date: Fri, 27 Oct 2017 06:20:43 +0000
Subject: [R] Help needed with aggregate or other solution
In-Reply-To: <alpine.BSF.2.00.1710260743150.58694@pedal.dcn.davis.ca.us>
References: <CAGxgkWjb8kZkt16OjvzZHZjTG_vbxvjAkfPov=pF9M5MN8vMOQ@mail.gmail.com>
 <alpine.BSF.2.00.1710252216410.34411@pedal.dcn.davis.ca.us>
 <CAGxgkWg=ReVYhW9rvyp6UDOdOL0c6CQQYaHDfGM2tey_iPXgZw@mail.gmail.com>
 <alpine.BSF.2.00.1710260743150.58694@pedal.dcn.davis.ca.us>
Message-ID: <6A0B0D55-0978-47B9-A433-F87B15F25CF0@kit.edu>

Hi Thomas,

Would this work:
res1=aggregate(dta[,"fcst"],by=list(basistime=dta[,"basistime"]),FUN=max)
mm=match(paste(res1[,"basistime"],res1[,"x"]),paste(dta[,"basistime"],dta[,"fcst"]))
dta[mm,]
> dta[mm,]
                  date           basistime fcst usgs
20 2012-01-30 12:00:00 2012-01-25 15:02:00 47.9 44.5
40 2012-01-31 12:00:00 2012-01-26 15:11:00 50.4 44.2
50 2012-01-29 12:00:00 2012-01-27 01:41:00 43.8 43.8

cheers
Peter



> On 26. Oct 2017, at 17:07, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> On Thu, 26 Oct 2017, Thomas Adams wrote:
> 
>> Hi Jeff,
>> Thank you for the suggestions -- I appreciate your help. Unfortunately, the
>> result2 has two problems...
>> (1) there are now 3 date columns (it looks like 2 cols are merged into 1
>> col)
> 
> No, there are two date columns. Result2 includes the grouping value as a row name (pulled from the names of the dta2list items by rbind).
> 
>> (2) the output rows should not have any of the basistime dates repeated
>> (maybe I misstated the problem); I need the max fcst value by basistime, but
>> also list the date value for that row; for example:
> 
> Then try out my code replacing
> 
> dta2list <- split( dta2, dta2$date )
> 
> with
> 
> dta2list <- split( dta2, dta2$basistime )
> 
> and
> 
> %>% group_by( date )
> 
> with
> 
> %>% group_by( basistime )
> 
> Please study how the code works and ask questions based on gaps in that knowledge rather than how the results are not what you expected. This mailing list is not a do-your-work-for-you coding service. Some help pages you should look at include:
> 
> ?rownames
> ?split
> ?lapply
> ?do.call
> ?rbind
> ?group_by
> ?do
> 
> You might also find [1] helpful in general, and [2] helpful for understanding dplyr.
> 
> [1] H. Wickham, The Split-Apply-Combine Strategy for Data Analysis, Journal of Statistical Software, vol. 40, no. 1, Apr. 2011.
> 
> [2] H. Wickham and G. Grolemund, R for Data Science. OReilly UK Ltd, 2017. URL: https://r4ds.had.co.nz.
> 
>>              basistime fcst
>> 1   2012-01-25 15:02:00 47.9
>> 2   2012-01-26 15:11:00 50.4
>> 3   2012-01-27 01:41:00 46.0
>> 4   2012-01-27 10:15:00 47.3
>> 5   2012-01-27 15:15:00 47.3
>> 6   2012-01-28 14:22:00 46.2
>> 7   2012-01-29 13:33:00 45.8
>> 8   2012-01-30 14:11:00 44.8
>> 9   2012-01-31 14:24:00 43.9
>> 10  2012-02-01 14:55:00 41.1
>> 11  2012-02-02 14:56:00 38.1
>> 12  2012-02-03 14:40:00 36.2
>> 13  2012-02-04 15:01:00 34.7
>> 14  2012-02-05 15:04:00 33.1
>> 15  2012-02-06 14:37:00 32.2
>> This is very close to what I need. The basistime dates are all unique, with
>> the max fcst value for the available basistime dates; but I additionally
>> need the corresponding 'date' value.
>> Best,
>> Tom
>> On Thu, Oct 26, 2017 at 1:28 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>> wrote:
>>      Thanks for the dput...
>> 
>>      #### reproducible example of split-apply-combine ###
>> 
>>      dta <- structure(list(date = structure(c(1L, 2L, 3L, 4L, 5L, 6L,
>>      7L,
>>      8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
>>      5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
>>      19L, 20L, 21L, 22L, 23L, 24L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
>>      14L, 15L, 16L), .Label = c("2012-01-25 18:00:00", "2012-01-26
>>      00:00:00",
>>      "2012-01-26 06:00:00", "2012-01-26 12:00:00", "2012-01-26
>>      18:00:00",
>>      "2012-01-27 00:00:00", "2012-01-27 06:00:00", "2012-01-27
>>      12:00:00",
>>      "2012-01-27 18:00:00", "2012-01-28 00:00:00", "2012-01-28
>>      06:00:00",
>>      "2012-01-28 12:00:00", "2012-01-28 18:00:00", "2012-01-29
>>      00:00:00",
>>      "2012-01-29 06:00:00", "2012-01-29 12:00:00", "2012-01-29
>>      18:00:00",
>>      "2012-01-30 00:00:00", "2012-01-30 06:00:00", "2012-01-30
>>      12:00:00",
>>      "2012-01-30 18:00:00", "2012-01-31 00:00:00", "2012-01-31
>>      06:00:00",
>>      "2012-01-31 12:00:00", "2012-01-31 13:00:00", "2012-01-31
>>      18:00:00",
>>      "2012-02-01 00:00:00", "2012-02-01 06:00:00", "2012-02-01
>>      12:00:00",
>>      "2012-02-01 18:00:00", "2012-02-02 00:00:00", "2012-02-02
>>      06:00:00",
>>      "2012-02-02 12:00:00", "2012-02-02 18:00:00", "2012-02-03
>>      00:00:00",
>>      "2012-02-03 06:00:00", "2012-02-03 12:00:00", "2012-02-03
>>      18:00:00",
>>      "2012-02-04 00:00:00", "2012-02-04 06:00:00", "2012-02-04
>>      12:00:00",
>>      "2012-02-04 18:00:00", "2012-02-05 00:00:00", "2012-02-05
>>      06:00:00",
>>      "2012-02-05 12:00:00", "2012-02-05 18:00:00", "2012-02-06
>>      00:00:00",
>>      "2012-02-06 06:00:00", "2012-02-06 12:00:00", "2012-02-06
>>      18:00:00",
>>      "2012-02-07 00:00:00", "2012-02-07 06:00:00", "2012-02-07
>>      12:00:00",
>>      "2012-02-07 18:00:00", "2012-02-08 00:00:00", "2012-02-08
>>      06:00:00",
>>      "2012-02-08 12:00:00", "2012-02-08 18:00:00", "2012-02-09
>>      00:00:00",
>>      "2012-02-09 06:00:00", "2012-02-09 12:00:00", "2012-02-09
>>      18:00:00",
>>      "2012-02-10 00:00:00", "2012-02-10 06:00:00", "2012-02-10
>>      12:00:00",
>>      "2012-02-10 18:00:00", "2012-02-11 00:00:00", "2012-02-11
>>      06:00:00",
>>      "2012-02-11 12:00:00", "2012-02-11 18:00:00", "2012-02-12
>>      00:00:00",
>>      "2012-02-12 06:00:00", "2012-02-12 12:00:00", "2012-02-12
>>      18:00:00",
>>      "2012-02-13 00:00:00", "2012-02-13 06:00:00", "2012-02-13
>>      12:00:00",
>>      "2012-02-13 18:00:00", "2012-02-14 00:00:00", "2012-02-14
>>      06:00:00",
>>      "2012-02-14 12:00:00", "2012-02-14 18:00:00", "2012-02-15
>>      00:00:00",
>>      "2012-02-15 06:00:00", "2012-02-15 12:00:00", "2012-02-15
>>      18:00:00",
>>      "2012-02-16 00:00:00", "2012-02-16 06:00:00", "2012-02-16
>>      12:00:00",
>>      "2012-02-16 18:00:00", "2012-02-17 00:00:00", "2012-02-17
>>      06:00:00",
>>      "2012-02-17 12:00:00", "2012-02-17 18:00:00", "2012-02-18
>>      00:00:00",
>>      "2012-02-18 06:00:00", "2012-02-18 12:00:00", "2012-02-18
>>      18:00:00",
>>      "2012-02-19 00:00:00", "2012-02-19 06:00:00", "2012-02-19
>>      12:00:00",
>>      "2012-02-19 18:00:00", "2012-02-20 00:00:00", "2012-02-20
>>      06:00:00",
>>      "2012-02-20 12:00:00", "2012-02-20 18:00:00", "2012-02-21
>>      00:00:00",
>>      "2012-02-21 06:00:00", "2012-02-21 12:00:00", "2012-02-21
>>      18:00:00",
>>      "2012-02-22 00:00:00", "2012-02-22 06:00:00", "2012-02-22
>>      12:00:00",
>>      "2012-02-22 18:00:00", "2012-02-23 00:00:00", "2012-02-23
>>      06:00:00",
>>      "2012-02-23 12:00:00", "2012-02-23 18:00:00", "2012-02-24
>>      00:00:00",
>>      "2012-02-24 06:00:00", "2012-02-24 12:00:00", "2012-02-24
>>      18:00:00",
>>      "2012-02-25 00:00:00", "2012-02-25 06:00:00", "2012-02-25
>>      12:00:00",
>>      "2012-02-25 18:00:00", "2012-02-26 00:00:00", "2012-02-26
>>      06:00:00",
>>      "2012-02-26 12:00:00", "2012-02-26 18:00:00", "2012-02-27
>>      00:00:00",
>>      "2012-02-27 06:00:00", "2012-02-27 12:00:00", "2012-02-27
>>      18:00:00",
>>      "2012-02-28 00:00:00", "2012-02-28 06:00:00", "2012-02-28
>>      12:00:00",
>>      "2012-02-28 18:00:00", "2012-02-29 00:00:00", "2012-02-29
>>      06:00:00",
>>      "2012-02-29 12:00:00", "2012-02-29 18:00:00", "2012-03-01
>>      00:00:00",
>>      "2012-03-01 06:00:00", "2012-03-01 12:00:00", "2012-03-01
>>      18:00:00",
>>      "2012-03-02 00:00:00", "2012-03-02 06:00:00", "2012-03-02
>>      12:00:00",
>>      "2012-03-02 18:00:00", "2012-03-03 00:00:00", "2012-03-03
>>      06:00:00",
>>      "2012-03-03 12:00:00", "2012-03-03 18:00:00", "2012-03-04
>>      00:00:00",
>>      "2012-03-04 06:00:00", "2012-03-04 12:00:00", "2012-03-04
>>      18:00:00",
>>      "2012-03-05 00:00:00", "2012-03-05 06:00:00", "2012-03-05
>>      12:00:00",
>>      "2012-03-05 18:00:00", "2012-03-06 00:00:00", "2012-03-06
>>      06:00:00",
>>      "2012-03-06 12:00:00", "2012-03-06 18:00:00", "2012-03-07
>>      00:00:00",
>>      "2012-03-07 06:00:00", "2012-03-07 12:00:00", "2012-03-07
>>      18:00:00",
>>      "2012-03-08 00:00:00", "2012-03-08 06:00:00", "2012-03-08
>>      12:00:00",
>>      "2012-03-08 18:00:00", "2012-03-09 00:00:00", "2012-03-09
>>      06:00:00",
>>      "2012-03-09 12:00:00", "2012-03-09 18:00:00", "2012-03-10
>>      00:00:00",
>>      "2012-03-10 06:00:00", "2012-03-10 12:00:00", "2012-03-10
>>      18:00:00",
>>      "2012-03-11 00:00:00", "2012-03-11 06:00:00", "2012-03-11
>>      12:00:00",
>>      "2012-03-11 18:00:00", "2012-03-12 00:00:00", "2012-03-12
>>      06:00:00",
>>      "2012-03-12 12:00:00", "2012-03-12 18:00:00", "2012-03-13
>>      00:00:00",
>>      "2012-03-13 06:00:00", "2012-03-13 12:00:00", "2012-03-13
>>      18:00:00",
>>      "2012-03-14 00:00:00", "2012-03-14 06:00:00", "2012-03-14
>>      12:00:00",
>>      "2012-03-14 18:00:00", "2012-03-15 00:00:00", "2012-03-15
>>      06:00:00",
>>      "2012-03-15 12:00:00", "2012-03-15 18:00:00", "2012-03-16
>>      00:00:00",
>>      "2012-03-16 06:00:00", "2012-03-16 12:00:00", "2012-03-16
>>      18:00:00",
>>      "2012-03-17 00:00:00", "2012-03-17 06:00:00", "2012-03-17
>>      12:00:00",
>>      "2012-03-17 18:00:00", "2012-03-18 00:00:00", "2012-03-18
>>      06:00:00",
>>      "2012-03-18 12:00:00", "2012-03-18 18:00:00", "2012-03-19
>>      00:00:00",
>>      "2012-03-19 06:00:00", "2012-03-19 12:00:00", "2012-03-19
>>      18:00:00",
>>      "2012-03-20 00:00:00", "2012-03-20 06:00:00", "2012-03-20
>>      12:00:00",
>>      "2012-03-20 18:00:00", "2012-03-21 00:00:00", "2012-03-21
>>      06:00:00",
>>      "2012-03-21 12:00:00", "2012-03-21 18:00:00", "2012-03-22
>>      00:00:00",
>>      "2012-03-22 06:00:00", "2012-03-22 12:00:00", "2012-03-22
>>      18:00:00",
>>      "2012-03-23 00:00:00", "2012-03-23 06:00:00", "2012-03-23
>>      12:00:00",
>>      "2012-03-23 18:00:00", "2012-03-24 00:00:00", "2012-03-24
>>      06:00:00",
>>      "2012-03-24 12:00:00", "2012-03-24 18:00:00", "2012-03-25
>>      00:00:00",
>>      "2012-03-25 06:00:00", "2012-03-25 12:00:00", "2012-03-25
>>      18:00:00",
>>      "2012-03-26 00:00:00", "2012-03-26 06:00:00", "2012-03-26
>>      12:00:00",
>>      "2012-03-26 18:00:00", "2012-03-27 00:00:00", "2012-03-27
>>      06:00:00",
>>      "2012-03-27 12:00:00", "2012-03-27 18:00:00", "2012-03-28
>>      00:00:00",
>>      "2012-03-28 06:00:00", "2012-03-28 12:00:00", "2012-03-28
>>      18:00:00",
>>      "2012-03-29 00:00:00", "2012-03-29 06:00:00", "2012-03-29
>>      12:00:00",
>>      "2012-03-29 18:00:00", "2012-03-30 00:00:00", "2012-03-30
>>      06:00:00",
>>      "2012-03-30 12:00:00", "2012-03-30 18:00:00", "2012-03-31
>>      00:00:00",
>>      "2012-03-31 06:00:00", "2012-03-31 12:00:00", "2012-03-31
>>      18:00:00",
>>      "2012-04-01 00:00:00", "2012-04-01 06:00:00", "2012-04-01
>>      12:00:00",
>>      "2012-04-01 18:00:00", "2012-04-02 00:00:00", "2012-04-02
>>      06:00:00",
>>      "2012-04-02 12:00:00", "2012-04-02 18:00:00", "2012-04-03
>>      00:00:00",
>>      "2012-04-03 06:00:00", "2012-04-03 12:00:00", "2012-04-03
>>      18:00:00",
>>      "2012-04-04 00:00:00", "2012-04-04 06:00:00", "2012-04-04
>>      12:00:00",
>>      "2012-04-04 18:00:00", "2012-04-05 00:00:00", "2012-04-05
>>      06:00:00",
>>      "2012-04-05 12:00:00", "2012-04-05 18:00:00", "2012-04-06
>>      00:00:00",
>>      "2012-04-06 06:00:00", "2012-04-06 12:00:00", "2012-04-06
>>      18:00:00",
>>      "2012-04-07 00:00:00", "2012-04-07 06:00:00", "2012-04-07
>>      12:00:00",
>>      "2012-04-07 18:00:00", "2012-04-08 00:00:00", "2012-04-08
>>      06:00:00",
>>      "2012-04-08 12:00:00", "2012-04-08 18:00:00", "2012-04-09
>>      00:00:00",
>>      "2012-04-09 06:00:00", "2012-04-09 12:00:00", "2012-04-09
>>      18:00:00",
>>      "2012-04-10 00:00:00", "2012-04-10 06:00:00", "2012-04-10
>>      12:00:00",
>>      "2012-04-10 18:00:00", "2012-04-11 00:00:00", "2012-04-11
>>      06:00:00",
>>      "2012-04-11 12:00:00", "2012-04-11 18:00:00", "2012-04-12
>>      00:00:00",
>>      "2012-04-12 06:00:00", "2012-04-12 12:00:00", "2012-04-12
>>      18:00:00",
>>      "2012-04-13 00:00:00", "2012-04-13 06:00:00", "2012-04-13
>>      12:00:00",
>>      "2012-04-13 18:00:00", "2012-04-14 00:00:00", "2012-04-14
>>      06:00:00",
>>      "2012-04-14 12:00:00", "2012-04-14 18:00:00", "2012-04-15
>>      00:00:00",
>>      "2012-04-15 06:00:00", "2012-04-15 12:00:00", "2012-04-15
>>      18:00:00",
>>      "2012-04-16 00:00:00", "2012-04-16 06:00:00", "2012-04-16
>>      12:00:00",
>>      "2012-04-16 18:00:00", "2012-04-17 00:00:00", "2012-04-17
>>      06:00:00",
>>      "2012-04-17 12:00:00", "2012-04-17 18:00:00", "2012-04-18
>>      00:00:00",
>>      "2012-04-18 06:00:00", "2012-04-18 12:00:00", "2012-04-18
>>      18:00:00",
>>      "2012-04-19 00:00:00", "2012-04-19 06:00:00", "2012-04-19
>>      12:00:00",
>>      "2012-04-19 18:00:00", "2012-04-20 00:00:00", "2012-04-20
>>      06:00:00",
>>      "2012-04-20 12:00:00", "2012-04-20 18:00:00", "2012-04-21
>>      00:00:00",
>>      "2012-04-21 06:00:00", "2012-04-21 12:00:00", "2012-04-21
>>      18:00:00",
>>      "2012-04-22 00:00:00", "2012-04-22 06:00:00", "2012-04-22
>>      12:00:00",
>>      "2012-04-22 18:00:00", "2012-04-23 00:00:00", "2012-04-23
>>      06:00:00",
>>      "2012-04-23 12:00:00", "2012-04-23 18:00:00", "2012-04-24
>>      00:00:00",
>>      "2012-04-24 06:00:00", "2012-04-24 12:00:00", "2012-04-24
>>      18:00:00",
>>      "2012-04-25 00:00:00", "2012-04-25 06:00:00", "2012-04-25
>>      12:00:00",
>>      "2012-04-25 18:00:00", "2012-04-26 00:00:00", "2012-04-26
>>      06:00:00",
>>      "2012-04-26 12:00:00", "2012-04-26 18:00:00", "2012-04-27
>>      00:00:00",
>>      "2012-04-27 06:00:00", "2012-04-27 12:00:00", "2012-04-27
>>      18:00:00",
>>      "2012-04-28 00:00:00", "2012-04-28 06:00:00", "2012-04-28
>>      12:00:00",
>>      "2012-04-28 18:00:00", "2012-04-29 00:00:00", "2012-04-29
>>      06:00:00",
>>      "2012-04-29 12:00:00", "2012-04-29 18:00:00", "2012-04-30
>>      00:00:00",
>>      "2012-04-30 06:00:00", "2012-04-30 12:00:00", "2012-04-30
>>      18:00:00",
>>      "2012-05-01 00:00:00", "2012-05-01 06:00:00", "2012-05-01
>>      12:00:00",
>>      "2012-05-01 18:00:00", "2012-05-02 00:00:00", "2012-05-02
>>      06:00:00",
>>      "2012-05-02 12:00:00", "2012-05-02 18:00:00", "2012-05-03
>>      00:00:00",
>>      "2012-05-03 06:00:00", "2012-05-03 12:00:00", "2012-05-03
>>      18:00:00",
>>      "2012-05-04 00:00:00", "2012-05-04 06:00:00", "2012-05-04
>>      12:00:00",
>>      "2012-05-04 18:00:00", "2012-05-05 00:00:00", "2012-05-05
>>      06:00:00",
>>      "2012-05-05 12:00:00", "2012-05-05 18:00:00", "2012-05-06
>>      00:00:00",
>>      "2012-05-06 06:00:00", "2012-05-06 12:00:00", "2012-05-06
>>      18:00:00",
>>      "2012-05-07 00:00:00", "2012-05-07 06:00:00", "2012-05-07
>>      12:00:00",
>>      "2012-05-07 18:00:00", "2012-05-08 00:00:00", "2012-05-08
>>      06:00:00",
>>      "2012-05-08 12:00:00", "2012-05-08 18:00:00", "2012-05-09
>>      00:00:00",
>>      "2012-05-09 06:00:00", "2012-05-09 12:00:00", "2012-05-09
>>      18:00:00",
>>      "2012-05-10 00:00:00", "2012-05-10 06:00:00", "2012-05-10
>>      12:00:00",
>>      "2012-05-10 18:00:00", "2012-05-11 00:00:00", "2012-05-11
>>      06:00:00",
>>      "2012-05-11 12:00:00", "2012-05-11 18:00:00", "2012-05-12
>>      00:00:00",
>>      "2012-05-12 06:00:00", "2012-05-12 12:00:00", "2012-05-12
>>      18:00:00",
>>      "2012-05-13 00:00:00", "2012-05-13 06:00:00", "2012-05-13
>>      12:00:00",
>>      "2012-05-13 18:00:00", "2012-05-14 00:00:00", "2012-05-14
>>      06:00:00",
>>      "2012-05-14 12:00:00", "2012-05-14 18:00:00", "2012-05-15
>>      00:00:00",
>>      "2012-05-15 06:00:00", "2012-05-15 12:00:00", "2012-05-15
>>      18:00:00",
>>      "2012-05-16 00:00:00", "2012-05-16 06:00:00", "2012-05-16
>>      12:00:00",
>>      "2012-05-16 18:00:00", "2012-05-17 00:00:00", "2012-05-17
>>      06:00:00",
>>      "2012-05-17 12:00:00", "2012-05-17 18:00:00", "2012-05-18
>>      00:00:00",
>>      "2012-05-18 06:00:00", "2012-05-18 12:00:00", "2012-05-18
>>      18:00:00",
>>      "2012-05-19 00:00:00", "2012-05-19 06:00:00", "2012-05-19
>>      12:00:00",
>>      "2012-05-19 18:00:00", "2012-05-20 00:00:00", "2012-05-20
>>      06:00:00",
>>      "2012-05-20 12:00:00", "2012-05-20 18:00:00", "2012-05-21
>>      00:00:00",
>>      "2012-05-21 06:00:00", "2012-05-21 12:00:00", "2012-05-21
>>      18:00:00",
>>      "2012-05-22 00:00:00", "2012-05-22 06:00:00", "2012-05-22
>>      12:00:00",
>>      "2012-05-22 18:00:00", "2012-05-23 00:00:00", "2012-05-23
>>      06:00:00",
>>      "2012-05-23 12:00:00", "2012-05-23 18:00:00", "2012-05-24
>>      00:00:00",
>>      "2012-05-24 06:00:00", "2012-05-24 12:00:00", "2012-05-24
>>      18:00:00",
>>      "2012-05-25 00:00:00", "2012-05-25 06:00:00", "2012-05-25
>>      12:00:00",
>>      "2012-05-25 18:00:00", "2012-05-26 00:00:00", "2012-05-26
>>      06:00:00",
>>      "2012-05-26 12:00:00", "2012-05-26 18:00:00", "2012-05-27
>>      00:00:00",
>>      "2012-05-27 06:00:00", "2012-05-27 12:00:00", "2012-05-27
>>      18:00:00",
>>      "2012-05-28 00:00:00", "2012-05-28 06:00:00", "2012-05-28
>>      12:00:00",
>>      "2012-05-28 18:00:00", "2012-05-29 00:00:00", "2012-05-29
>>      06:00:00",
>>      "2012-05-29 12:00:00"), class = "factor"), basistime =
>>      structure(c(1L,
>>      1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>>      1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>>      2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>>      3L), .Label = c("2012-01-25 15:02:00", "2012-01-26 15:11:00",
>>      "2012-01-27 01:41:00", "2012-01-27 10:15:00", "2012-01-27
>>      15:15:00",
>>      "2012-01-28 14:22:00", "2012-01-29 13:33:00", "2012-01-30
>>      14:11:00",
>>      "2012-01-31 14:24:00", "2012-02-01 14:55:00", "2012-02-02
>>      14:56:00",
>>      "2012-02-03 14:40:00", "2012-02-04 15:01:00", "2012-02-05
>>      15:04:00",
>>      "2012-02-06 14:37:00", "2012-02-07 14:42:00", "2012-02-08
>>      14:44:00",
>>      "2012-02-09 14:27:00", "2012-02-10 14:33:00", "2012-02-11
>>      15:05:00",
>>      "2012-02-12 15:09:00", "2012-02-13 15:00:00", "2012-02-14
>>      15:10:00",
>>      "2012-02-15 14:46:00", "2012-02-16 14:42:00", "2012-02-17
>>      14:14:00",
>>      "2012-02-18 15:27:00", "2012-02-19 13:13:00", "2012-02-20
>>      13:56:00",
>>      "2012-02-21 14:39:00", "2012-02-22 15:33:00", "2012-02-23
>>      15:14:00",
>>      "2012-02-24 15:14:00", "2012-02-25 14:25:00", "2012-02-26
>>      14:08:00",
>>      "2012-02-27 14:55:00", "2012-02-28 14:40:00", "2012-02-29
>>      14:45:00",
>>      "2012-03-01 15:18:00", "2012-03-02 15:18:00", "2012-03-03
>>      15:43:00",
>>      "2012-03-04 15:16:00", "2012-03-05 15:17:00", "2012-03-06
>>      15:03:00",
>>      "2012-03-07 14:44:00", "2012-03-08 14:53:00", "2012-03-09
>>      15:10:00",
>>      "2012-03-10 14:43:00", "2012-03-11 13:37:00", "2012-03-12
>>      14:58:00",
>>      "2012-03-13 14:36:00", "2012-03-14 13:49:00", "2012-03-15
>>      14:32:00",
>>      "2012-03-16 15:01:00", "2012-03-17 13:55:00", "2012-03-18
>>      13:44:00",
>>      "2012-03-19 13:55:00", "2012-03-20 13:53:00", "2012-03-21
>>      14:07:00",
>>      "2012-03-22 14:03:00", "2012-03-23 14:30:00", "2012-03-24
>>      14:47:00",
>>      "2012-03-25 14:32:00", "2012-03-26 14:41:00", "2012-03-27
>>      13:51:00",
>>      "2012-03-28 13:32:00", "2012-03-29 13:59:00", "2012-03-30
>>      14:15:00",
>>      "2012-03-31 14:12:00", "2012-04-01 14:30:00", "2012-04-02
>>      13:43:00",
>>      "2012-04-03 13:54:00", "2012-04-04 13:45:00", "2012-04-05
>>      13:46:00",
>>      "2012-04-06 13:41:00", "2012-04-07 13:13:00", "2012-04-08
>>      12:59:00",
>>      "2012-04-09 13:53:00", "2012-04-10 14:15:00", "2012-04-11
>>      14:26:00",
>>      "2012-04-12 14:05:00", "2012-04-13 13:37:00", "2012-04-14
>>      13:52:00",
>>      "2012-04-15 15:00:00", "2012-04-16 14:42:00", "2012-04-17
>>      14:21:00",
>>      "2012-04-18 14:40:00", "2012-04-19 14:35:00", "2012-04-20
>>      15:14:00",
>>      "2012-04-21 14:46:00", "2012-04-22 14:44:00", "2012-04-23
>>      14:33:00",
>>      "2012-04-24 14:41:00", "2012-04-25 14:13:00", "2012-04-26
>>      14:19:00",
>>      "2012-04-27 14:35:00", "2012-04-28 13:48:00", "2012-04-29
>>      14:12:00",
>>      "2012-04-30 13:53:00", "2012-05-02 14:41:00", "2012-05-03
>>      15:01:00",
>>      "2012-05-04 15:21:00", "2012-05-05 14:57:00", "2012-05-06
>>      14:09:00",
>>      "2012-05-07 14:30:00", "2012-05-08 14:05:00", "2012-05-08
>>      21:21:00",
>>      "2012-05-09 15:02:00", "2012-05-10 14:27:00", "2012-05-11
>>      14:47:00",
>>      "2012-05-12 13:24:00", "2012-05-13 14:40:00", "2012-05-13
>>      19:24:00",
>>      "2012-05-14 14:33:00", "2012-05-15 14:39:00", "2012-05-16
>>      14:07:00",
>>      "2012-05-17 13:42:00", "2012-05-18 14:30:00", "2012-05-19
>>      13:59:00",
>>      "2012-05-20 14:14:00", "2012-05-21 14:15:00", "2012-05-22
>>      14:18:00",
>>      "2012-05-23 14:14:00", "2012-05-24 13:52:00"), class =
>>      "factor"),
>>          fcst = c(38.7, 38.9, 39.2, 39.8, 40.5, 41.5, 42.5, 43.1,
>>          43.9, 44.5, 44.8, 45, 45, 45.1, 45.4, 45.8, 46.4, 47, 47.5,
>>          47.9, 39.2, 40, 41.8, 42.2, 42.8, 44.4, 45.7, 46.2, 46.2,
>>          46.2, 46.5, 47, 47.5, 47.9, 48.4, 49, 49.5, 49.9, 50.2,
>>      50.4,
>>          41.3, 42.5, 42.8, 42.8, 42.8, 42.9, 43, 43.3, 43.5, 43.8),
>>          usgs = c(38.5, 38.6, 38.6, 38.6, 39.1, 39.8, 41.2, 42.9,
>>          43.4, 43.6, 43.4, 43.1, 43, 43.1, 43.5, 43.8, 44.1, 44.2,
>>          44.4, 44.5, 39.1, 39.8, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1,
>>          43, 43.1, 43.5, 43.8, 44.1, 44.2, 44.4, 44.5, 44.5, 44.5,
>>          44.6, 44.2, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1, 43, 43.1,
>>          43.5, 43.8)), .Names = c("date", "basistime", "fcst", "usgs"
>>      ), row.names = c(NA, 50L), class = "data.frame")
>> Sys.setenv( "Etc/GMT+8" ) # or whatever is appropriate
>> #> Error in Sys.setenv("Etc/GMT+8"): all arguments must be named
>> dta2 <- dta
>> # bad idea to work with dates as factors
>> dta2$date <- as.POSIXct( as.character( dta2$date ) )
>> dta2$basistime <- as.POSIXct( as.character( dta2$basistime ) )
>> # base R solution
>> dates <- unique( dta2$date )
>> dta2list <- split( dta2, dta2$date )
>> grplist <- lapply( dta2list
>>                  , function( DF ) {
>>                      DF[ which.max( DF$fcst ), ]
>>                    }
>>                  )
>> result2 <- do.call( rbind, grplist )
>> result2
>> #>                                    date           basistime fcst
>> usgs
>> #> 2012-01-25 18:00:00 2012-01-25 18:00:00 2012-01-25 15:02:00 38.7
>> 38.5
>> #> 2012-01-26 00:00:00 2012-01-26 00:00:00 2012-01-25 15:02:00 38.9
>> 38.6
>> #> 2012-01-26 06:00:00 2012-01-26 06:00:00 2012-01-25 15:02:00 39.2
>> 38.6
>> #> 2012-01-26 12:00:00 2012-01-26 12:00:00 2012-01-25 15:02:00 39.8
>> 38.6
>> #> 2012-01-26 18:00:00 2012-01-26 18:00:00 2012-01-25 15:02:00 40.5
>> 39.1
>> #> 2012-01-27 00:00:00 2012-01-27 00:00:00 2012-01-25 15:02:00 41.5
>> 39.8
>> #> 2012-01-27 06:00:00 2012-01-27 06:00:00 2012-01-25 15:02:00 42.5
>> 41.2
>> #> 2012-01-27 12:00:00 2012-01-27 12:00:00 2012-01-25 15:02:00 43.1
>> 42.9
>> #> 2012-01-27 18:00:00 2012-01-27 18:00:00 2012-01-25 15:02:00 43.9
>> 43.4
>> #> 2012-01-28 00:00:00 2012-01-28 00:00:00 2012-01-25 15:02:00 44.5
>> 43.6
>> #> 2012-01-28 06:00:00 2012-01-28 06:00:00 2012-01-26 15:11:00 45.7
>> 43.4
>> #> 2012-01-28 12:00:00 2012-01-28 12:00:00 2012-01-26 15:11:00 46.2
>> 43.1
>> #> 2012-01-28 18:00:00 2012-01-28 18:00:00 2012-01-26 15:11:00 46.2
>> 43.0
>> #> 2012-01-29 00:00:00 2012-01-29 00:00:00 2012-01-26 15:11:00 46.2
>> 43.1
>> #> 2012-01-29 06:00:00 2012-01-29 06:00:00 2012-01-26 15:11:00 46.5
>> 43.5
>> #> 2012-01-29 12:00:00 2012-01-29 12:00:00 2012-01-26 15:11:00 47.0
>> 43.8
>> #> 2012-01-29 18:00:00 2012-01-29 18:00:00 2012-01-26 15:11:00 47.5
>> 44.1
>> #> 2012-01-30 00:00:00 2012-01-30 00:00:00 2012-01-26 15:11:00 47.9
>> 44.2
>> #> 2012-01-30 06:00:00 2012-01-30 06:00:00 2012-01-26 15:11:00 48.4
>> 44.4
>> #> 2012-01-30 12:00:00 2012-01-30 12:00:00 2012-01-26 15:11:00 49.0
>> 44.5
>> #> 2012-01-30 18:00:00 2012-01-30 18:00:00 2012-01-26 15:11:00 49.5
>> 44.5
>> #> 2012-01-31 00:00:00 2012-01-31 00:00:00 2012-01-26 15:11:00 49.9
>> 44.5
>> #> 2012-01-31 06:00:00 2012-01-31 06:00:00 2012-01-26 15:11:00 50.2
>> 44.6
>> #> 2012-01-31 12:00:00 2012-01-31 12:00:00 2012-01-26 15:11:00 50.4
>> 44.2
>> # alternately, use tidyverse
>> library(dplyr)
>> #>
>> #> Attaching package: 'dplyr'
>> #> The following objects are masked from 'package:stats':
>> #>
>> #>     filter, lag
>> #> The following objects are masked from 'package:base':
>> #>
>> #>     intersect, setdiff, setequal, union
>> result3 <- (   dta2
>>            %>% group_by( date )
>>            %>% do({
>>                  DF <- .
>>                  DF[ which.max( DF$fcst ), ]
>>                })
>>            %>% ungroup
>>            %>% as.data.frame
>>            )
>> result3
>> #>                   date           basistime fcst usgs
>> #> 1  2012-01-25 18:00:00 2012-01-25 15:02:00 38.7 38.5
>> #> 2  2012-01-26 00:00:00 2012-01-25 15:02:00 38.9 38.6
>> #> 3  2012-01-26 06:00:00 2012-01-25 15:02:00 39.2 38.6
>> #> 4  2012-01-26 12:00:00 2012-01-25 15:02:00 39.8 38.6
>> #> 5  2012-01-26 18:00:00 2012-01-25 15:02:00 40.5 39.1
>> #> 6  2012-01-27 00:00:00 2012-01-25 15:02:00 41.5 39.8
>> #> 7  2012-01-27 06:00:00 2012-01-25 15:02:00 42.5 41.2
>> #> 8  2012-01-27 12:00:00 2012-01-25 15:02:00 43.1 42.9
>> #> 9  2012-01-27 18:00:00 2012-01-25 15:02:00 43.9 43.4
>> #> 10 2012-01-28 00:00:00 2012-01-25 15:02:00 44.5 43.6
>> #> 11 2012-01-28 06:00:00 2012-01-26 15:11:00 45.7 43.4
>> #> 12 2012-01-28 12:00:00 2012-01-26 15:11:00 46.2 43.1
>> #> 13 2012-01-28 18:00:00 2012-01-26 15:11:00 46.2 43.0
>> #> 14 2012-01-29 00:00:00 2012-01-26 15:11:00 46.2 43.1
>> #> 15 2012-01-29 06:00:00 2012-01-26 15:11:00 46.5 43.5
>> #> 16 2012-01-29 12:00:00 2012-01-26 15:11:00 47.0 43.8
>> #> 17 2012-01-29 18:00:00 2012-01-26 15:11:00 47.5 44.1
>> #> 18 2012-01-30 00:00:00 2012-01-26 15:11:00 47.9 44.2
>> #> 19 2012-01-30 06:00:00 2012-01-26 15:11:00 48.4 44.4
>> #> 20 2012-01-30 12:00:00 2012-01-26 15:11:00 49.0 44.5
>> #> 21 2012-01-30 18:00:00 2012-01-26 15:11:00 49.5 44.5
>> #> 22 2012-01-31 00:00:00 2012-01-26 15:11:00 49.9 44.5
>> #> 23 2012-01-31 06:00:00 2012-01-26 15:11:00 50.2 44.6
>> #> 24 2012-01-31 12:00:00 2012-01-26 15:11:00 50.4 44.2
>> ####################################################
>> On Thu, 26 Oct 2017, Thomas Adams wrote:
>> 
>>      Hello all!
>> 
>>      I've been struggling with is for many hours today; I'm
>>      close to getting
>>      what I want, but not close enough...
>> 
>>      I have a dataframe consisting of two date-time columns
>>      followed by two
>>      numeric columns. what I need is the max value (in the
>>      first numeric column)
>>      based on the 2nd date-time column, which is essentially a
>>      factor. But, I
>>      want the result to provide both date-time values
>>      corresponding to the max
>>      value.
>> 
>>      My data:
>> 
>>      structure(list(date = structure(c(1L, 2L, 3L, 4L, 5L, 6L,
>>      7L,
>>      8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L,
>>      20L,
>>      5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,
>>      17L, 18L,
>>      19L, 20L, 21L, 22L, 23L, 24L, 7L, 8L, 9L, 10L, 11L, 12L,
>>      13L,
>>      14L, 15L, 16L), .Label = c("2012-01-25 18:00:00",
>>      "2012-01-26 00:00:00",
>>      "2012-01-26 06:00:00", "2012-01-26 12:00:00", "2012-01-26
>>      18:00:00",
>>      "2012-01-27 00:00:00", "2012-01-27 06:00:00", "2012-01-27
>>      12:00:00",
>>      "2012-01-27 18:00:00", "2012-01-28 00:00:00", "2012-01-28
>>      06:00:00",
>>      "2012-01-28 12:00:00", "2012-01-28 18:00:00", "2012-01-29
>>      00:00:00",
>>      "2012-01-29 06:00:00", "2012-01-29 12:00:00", "2012-01-29
>>      18:00:00",
>>      "2012-01-30 00:00:00", "2012-01-30 06:00:00", "2012-01-30
>>      12:00:00",
>>      "2012-01-30 18:00:00", "2012-01-31 00:00:00", "2012-01-31
>>      06:00:00",
>>      "2012-01-31 12:00:00", "2012-01-31 13:00:00", "2012-01-31
>>      18:00:00",
>>      "2012-02-01 00:00:00", "2012-02-01 06:00:00", "2012-02-01
>>      12:00:00",
>>      "2012-02-01 18:00:00", "2012-02-02 00:00:00", "2012-02-02
>>      06:00:00",
>>      "2012-02-02 12:00:00", "2012-02-02 18:00:00", "2012-02-03
>>      00:00:00",
>>      "2012-02-03 06:00:00", "2012-02-03 12:00:00", "2012-02-03
>>      18:00:00",
>>      "2012-02-04 00:00:00", "2012-02-04 06:00:00", "2012-02-04
>>      12:00:00",
>>      "2012-02-04 18:00:00", "2012-02-05 00:00:00", "2012-02-05
>>      06:00:00",
>>      "2012-02-05 12:00:00", "2012-02-05 18:00:00", "2012-02-06
>>      00:00:00",
>>      "2012-02-06 06:00:00", "2012-02-06 12:00:00", "2012-02-06
>>      18:00:00",
>>      "2012-02-07 00:00:00", "2012-02-07 06:00:00", "2012-02-07
>>      12:00:00",
>>      "2012-02-07 18:00:00", "2012-02-08 00:00:00", "2012-02-08
>>      06:00:00",
>>      "2012-02-08 12:00:00", "2012-02-08 18:00:00", "2012-02-09
>>      00:00:00",
>>      "2012-02-09 06:00:00", "2012-02-09 12:00:00", "2012-02-09
>>      18:00:00",
>>      "2012-02-10 00:00:00", "2012-02-10 06:00:00", "2012-02-10
>>      12:00:00",
>>      "2012-02-10 18:00:00", "2012-02-11 00:00:00", "2012-02-11
>>      06:00:00",
>>      "2012-02-11 12:00:00", "2012-02-11 18:00:00", "2012-02-12
>>      00:00:00",
>>      "2012-02-12 06:00:00", "2012-02-12 12:00:00", "2012-02-12
>>      18:00:00",
>>      "2012-02-13 00:00:00", "2012-02-13 06:00:00", "2012-02-13
>>      12:00:00",
>>      "2012-02-13 18:00:00", "2012-02-14 00:00:00", "2012-02-14
>>      06:00:00",
>>      "2012-02-14 12:00:00", "2012-02-14 18:00:00", "2012-02-15
>>      00:00:00",
>>      "2012-02-15 06:00:00", "2012-02-15 12:00:00", "2012-02-15
>>      18:00:00",
>>      "2012-02-16 00:00:00", "2012-02-16 06:00:00", "2012-02-16
>>      12:00:00",
>>      "2012-02-16 18:00:00", "2012-02-17 00:00:00", "2012-02-17
>>      06:00:00",
>>      "2012-02-17 12:00:00", "2012-02-17 18:00:00", "2012-02-18
>>      00:00:00",
>>      "2012-02-18 06:00:00", "2012-02-18 12:00:00", "2012-02-18
>>      18:00:00",
>>      "2012-02-19 00:00:00", "2012-02-19 06:00:00", "2012-02-19
>>      12:00:00",
>>      "2012-02-19 18:00:00", "2012-02-20 00:00:00", "2012-02-20
>>      06:00:00",
>>      "2012-02-20 12:00:00", "2012-02-20 18:00:00", "2012-02-21
>>      00:00:00",
>>      "2012-02-21 06:00:00", "2012-02-21 12:00:00", "2012-02-21
>>      18:00:00",
>>      "2012-02-22 00:00:00", "2012-02-22 06:00:00", "2012-02-22
>>      12:00:00",
>>      "2012-02-22 18:00:00", "2012-02-23 00:00:00", "2012-02-23
>>      06:00:00",
>>      "2012-02-23 12:00:00", "2012-02-23 18:00:00", "2012-02-24
>>      00:00:00",
>>      "2012-02-24 06:00:00", "2012-02-24 12:00:00", "2012-02-24
>>      18:00:00",
>>      "2012-02-25 00:00:00", "2012-02-25 06:00:00", "2012-02-25
>>      12:00:00",
>>      "2012-02-25 18:00:00", "2012-02-26 00:00:00", "2012-02-26
>>      06:00:00",
>>      "2012-02-26 12:00:00", "2012-02-26 18:00:00", "2012-02-27
>>      00:00:00",
>>      "2012-02-27 06:00:00", "2012-02-27 12:00:00", "2012-02-27
>>      18:00:00",
>>      "2012-02-28 00:00:00", "2012-02-28 06:00:00", "2012-02-28
>>      12:00:00",
>>      "2012-02-28 18:00:00", "2012-02-29 00:00:00", "2012-02-29
>>      06:00:00",
>>      "2012-02-29 12:00:00", "2012-02-29 18:00:00", "2012-03-01
>>      00:00:00",
>>      "2012-03-01 06:00:00", "2012-03-01 12:00:00", "2012-03-01
>>      18:00:00",
>>      "2012-03-02 00:00:00", "2012-03-02 06:00:00", "2012-03-02
>>      12:00:00",
>>      "2012-03-02 18:00:00", "2012-03-03 00:00:00", "2012-03-03
>>      06:00:00",
>>      "2012-03-03 12:00:00", "2012-03-03 18:00:00", "2012-03-04
>>      00:00:00",
>>      "2012-03-04 06:00:00", "2012-03-04 12:00:00", "2012-03-04
>>      18:00:00",
>>      "2012-03-05 00:00:00", "2012-03-05 06:00:00", "2012-03-05
>>      12:00:00",
>>      "2012-03-05 18:00:00", "2012-03-06 00:00:00", "2012-03-06
>>      06:00:00",
>>      "2012-03-06 12:00:00", "2012-03-06 18:00:00", "2012-03-07
>>      00:00:00",
>>      "2012-03-07 06:00:00", "2012-03-07 12:00:00", "2012-03-07
>>      18:00:00",
>>      "2012-03-08 00:00:00", "2012-03-08 06:00:00", "2012-03-08
>>      12:00:00",
>>      "2012-03-08 18:00:00", "2012-03-09 00:00:00", "2012-03-09
>>      06:00:00",
>>      "2012-03-09 12:00:00", "2012-03-09 18:00:00", "2012-03-10
>>      00:00:00",
>>      "2012-03-10 06:00:00", "2012-03-10 12:00:00", "2012-03-10
>>      18:00:00",
>>      "2012-03-11 00:00:00", "2012-03-11 06:00:00", "2012-03-11
>>      12:00:00",
>>      "2012-03-11 18:00:00", "2012-03-12 00:00:00", "2012-03-12
>>      06:00:00",
>>      "2012-03-12 12:00:00", "2012-03-12 18:00:00", "2012-03-13
>>      00:00:00",
>>      "2012-03-13 06:00:00", "2012-03-13 12:00:00", "2012-03-13
>>      18:00:00",
>>      "2012-03-14 00:00:00", "2012-03-14 06:00:00", "2012-03-14
>>      12:00:00",
>>      "2012-03-14 18:00:00", "2012-03-15 00:00:00", "2012-03-15
>>      06:00:00",
>>      "2012-03-15 12:00:00", "2012-03-15 18:00:00", "2012-03-16
>>      00:00:00",
>>      "2012-03-16 06:00:00", "2012-03-16 12:00:00", "2012-03-16
>>      18:00:00",
>>      "2012-03-17 00:00:00", "2012-03-17 06:00:00", "2012-03-17
>>      12:00:00",
>>      "2012-03-17 18:00:00", "2012-03-18 00:00:00", "2012-03-18
>>      06:00:00",
>>      "2012-03-18 12:00:00", "2012-03-18 18:00:00", "2012-03-19
>>      00:00:00",
>>      "2012-03-19 06:00:00", "2012-03-19 12:00:00", "2012-03-19
>>      18:00:00",
>>      "2012-03-20 00:00:00", "2012-03-20 06:00:00", "2012-03-20
>>      12:00:00",
>>      "2012-03-20 18:00:00", "2012-03-21 00:00:00", "2012-03-21
>>      06:00:00",
>>      "2012-03-21 12:00:00", "2012-03-21 18:00:00", "2012-03-22
>>      00:00:00",
>>      "2012-03-22 06:00:00", "2012-03-22 12:00:00", "2012-03-22
>>      18:00:00",
>>      "2012-03-23 00:00:00", "2012-03-23 06:00:00", "2012-03-23
>>      12:00:00",
>>      "2012-03-23 18:00:00", "2012-03-24 00:00:00", "2012-03-24
>>      06:00:00",
>>      "2012-03-24 12:00:00", "2012-03-24 18:00:00", "2012-03-25
>>      00:00:00",
>>      "2012-03-25 06:00:00", "2012-03-25 12:00:00", "2012-03-25
>>      18:00:00",
>>      "2012-03-26 00:00:00", "2012-03-26 06:00:00", "2012-03-26
>>      12:00:00",
>>      "2012-03-26 18:00:00", "2012-03-27 00:00:00", "2012-03-27
>>      06:00:00",
>>      "2012-03-27 12:00:00", "2012-03-27 18:00:00", "2012-03-28
>>      00:00:00",
>>      "2012-03-28 06:00:00", "2012-03-28 12:00:00", "2012-03-28
>>      18:00:00",
>>      "2012-03-29 00:00:00", "2012-03-29 06:00:00", "2012-03-29
>>      12:00:00",
>>      "2012-03-29 18:00:00", "2012-03-30 00:00:00", "2012-03-30
>>      06:00:00",
>>      "2012-03-30 12:00:00", "2012-03-30 18:00:00", "2012-03-31
>>      00:00:00",
>>      "2012-03-31 06:00:00", "2012-03-31 12:00:00", "2012-03-31
>>      18:00:00",
>>      "2012-04-01 00:00:00", "2012-04-01 06:00:00", "2012-04-01
>>      12:00:00",
>>      "2012-04-01 18:00:00", "2012-04-02 00:00:00", "2012-04-02
>>      06:00:00",
>>      "2012-04-02 12:00:00", "2012-04-02 18:00:00", "2012-04-03
>>      00:00:00",
>>      "2012-04-03 06:00:00", "2012-04-03 12:00:00", "2012-04-03
>>      18:00:00",
>>      "2012-04-04 00:00:00", "2012-04-04 06:00:00", "2012-04-04
>>      12:00:00",
>>      "2012-04-04 18:00:00", "2012-04-05 00:00:00", "2012-04-05
>>      06:00:00",
>>      "2012-04-05 12:00:00", "2012-04-05 18:00:00", "2012-04-06
>>      00:00:00",
>>      "2012-04-06 06:00:00", "2012-04-06 12:00:00", "2012-04-06
>>      18:00:00",
>>      "2012-04-07 00:00:00", "2012-04-07 06:00:00", "2012-04-07
>>      12:00:00",
>>      "2012-04-07 18:00:00", "2012-04-08 00:00:00", "2012-04-08
>>      06:00:00",
>>      "2012-04-08 12:00:00", "2012-04-08 18:00:00", "2012-04-09
>>      00:00:00",
>>      "2012-04-09 06:00:00", "2012-04-09 12:00:00", "2012-04-09
>>      18:00:00",
>>      "2012-04-10 00:00:00", "2012-04-10 06:00:00", "2012-04-10
>>      12:00:00",
>>      "2012-04-10 18:00:00", "2012-04-11 00:00:00", "2012-04-11
>>      06:00:00",
>>      "2012-04-11 12:00:00", "2012-04-11 18:00:00", "2012-04-12
>>      00:00:00",
>>      "2012-04-12 06:00:00", "2012-04-12 12:00:00", "2012-04-12
>>      18:00:00",
>>      "2012-04-13 00:00:00", "2012-04-13 06:00:00", "2012-04-13
>>      12:00:00",
>>      "2012-04-13 18:00:00", "2012-04-14 00:00:00", "2012-04-14
>>      06:00:00",
>>      "2012-04-14 12:00:00", "2012-04-14 18:00:00", "2012-04-15
>>      00:00:00",
>>      "2012-04-15 06:00:00", "2012-04-15 12:00:00", "2012-04-15
>>      18:00:00",
>>      "2012-04-16 00:00:00", "2012-04-16 06:00:00", "2012-04-16
>>      12:00:00",
>>      "2012-04-16 18:00:00", "2012-04-17 00:00:00", "2012-04-17
>>      06:00:00",
>>      "2012-04-17 12:00:00", "2012-04-17 18:00:00", "2012-04-18
>>      00:00:00",
>>      "2012-04-18 06:00:00", "2012-04-18 12:00:00", "2012-04-18
>>      18:00:00",
>>      "2012-04-19 00:00:00", "2012-04-19 06:00:00", "2012-04-19
>>      12:00:00",
>>      "2012-04-19 18:00:00", "2012-04-20 00:00:00", "2012-04-20
>>      06:00:00",
>>      "2012-04-20 12:00:00", "2012-04-20 18:00:00", "2012-04-21
>>      00:00:00",
>>      "2012-04-21 06:00:00", "2012-04-21 12:00:00", "2012-04-21
>>      18:00:00",
>>      "2012-04-22 00:00:00", "2012-04-22 06:00:00", "2012-04-22
>>      12:00:00",
>>      "2012-04-22 18:00:00", "2012-04-23 00:00:00", "2012-04-23
>>      06:00:00",
>>      "2012-04-23 12:00:00", "2012-04-23 18:00:00", "2012-04-24
>>      00:00:00",
>>      "2012-04-24 06:00:00", "2012-04-24 12:00:00", "2012-04-24
>>      18:00:00",
>>      "2012-04-25 00:00:00", "2012-04-25 06:00:00", "2012-04-25
>>      12:00:00",
>>      "2012-04-25 18:00:00", "2012-04-26 00:00:00", "2012-04-26
>>      06:00:00",
>>      "2012-04-26 12:00:00", "2012-04-26 18:00:00", "2012-04-27
>>      00:00:00",
>>      "2012-04-27 06:00:00", "2012-04-27 12:00:00", "2012-04-27
>>      18:00:00",
>>      "2012-04-28 00:00:00", "2012-04-28 06:00:00", "2012-04-28
>>      12:00:00",
>>      "2012-04-28 18:00:00", "2012-04-29 00:00:00", "2012-04-29
>>      06:00:00",
>>      "2012-04-29 12:00:00", "2012-04-29 18:00:00", "2012-04-30
>>      00:00:00",
>>      "2012-04-30 06:00:00", "2012-04-30 12:00:00", "2012-04-30
>>      18:00:00",
>>      "2012-05-01 00:00:00", "2012-05-01 06:00:00", "2012-05-01
>>      12:00:00",
>>      "2012-05-01 18:00:00", "2012-05-02 00:00:00", "2012-05-02
>>      06:00:00",
>>      "2012-05-02 12:00:00", "2012-05-02 18:00:00", "2012-05-03
>>      00:00:00",
>>      "2012-05-03 06:00:00", "2012-05-03 12:00:00", "2012-05-03
>>      18:00:00",
>>      "2012-05-04 00:00:00", "2012-05-04 06:00:00", "2012-05-04
>>      12:00:00",
>>      "2012-05-04 18:00:00", "2012-05-05 00:00:00", "2012-05-05
>>      06:00:00",
>>      "2012-05-05 12:00:00", "2012-05-05 18:00:00", "2012-05-06
>>      00:00:00",
>>      "2012-05-06 06:00:00", "2012-05-06 12:00:00", "2012-05-06
>>      18:00:00",
>>      "2012-05-07 00:00:00", "2012-05-07 06:00:00", "2012-05-07
>>      12:00:00",
>>      "2012-05-07 18:00:00", "2012-05-08 00:00:00", "2012-05-08
>>      06:00:00",
>>      "2012-05-08 12:00:00", "2012-05-08 18:00:00", "2012-05-09
>>      00:00:00",
>>      "2012-05-09 06:00:00", "2012-05-09 12:00:00", "2012-05-09
>>      18:00:00",
>>      "2012-05-10 00:00:00", "2012-05-10 06:00:00", "2012-05-10
>>      12:00:00",
>>      "2012-05-10 18:00:00", "2012-05-11 00:00:00", "2012-05-11
>>      06:00:00",
>>      "2012-05-11 12:00:00", "2012-05-11 18:00:00", "2012-05-12
>>      00:00:00",
>>      "2012-05-12 06:00:00", "2012-05-12 12:00:00", "2012-05-12
>>      18:00:00",
>>      "2012-05-13 00:00:00", "2012-05-13 06:00:00", "2012-05-13
>>      12:00:00",
>>      "2012-05-13 18:00:00", "2012-05-14 00:00:00", "2012-05-14
>>      06:00:00",
>>      "2012-05-14 12:00:00", "2012-05-14 18:00:00", "2012-05-15
>>      00:00:00",
>>      "2012-05-15 06:00:00", "2012-05-15 12:00:00", "2012-05-15
>>      18:00:00",
>>      "2012-05-16 00:00:00", "2012-05-16 06:00:00", "2012-05-16
>>      12:00:00",
>>      "2012-05-16 18:00:00", "2012-05-17 00:00:00", "2012-05-17
>>      06:00:00",
>>      "2012-05-17 12:00:00", "2012-05-17 18:00:00", "2012-05-18
>>      00:00:00",
>>      "2012-05-18 06:00:00", "2012-05-18 12:00:00", "2012-05-18
>>      18:00:00",
>>      "2012-05-19 00:00:00", "2012-05-19 06:00:00", "2012-05-19
>>      12:00:00",
>>      "2012-05-19 18:00:00", "2012-05-20 00:00:00", "2012-05-20
>>      06:00:00",
>>      "2012-05-20 12:00:00", "2012-05-20 18:00:00", "2012-05-21
>>      00:00:00",
>>      "2012-05-21 06:00:00", "2012-05-21 12:00:00", "2012-05-21
>>      18:00:00",
>>      "2012-05-22 00:00:00", "2012-05-22 06:00:00", "2012-05-22
>>      12:00:00",
>>      "2012-05-22 18:00:00", "2012-05-23 00:00:00", "2012-05-23
>>      06:00:00",
>>      "2012-05-23 12:00:00", "2012-05-23 18:00:00", "2012-05-24
>>      00:00:00",
>>      "2012-05-24 06:00:00", "2012-05-24 12:00:00", "2012-05-24
>>      18:00:00",
>>      "2012-05-25 00:00:00", "2012-05-25 06:00:00", "2012-05-25
>>      12:00:00",
>>      "2012-05-25 18:00:00", "2012-05-26 00:00:00", "2012-05-26
>>      06:00:00",
>>      "2012-05-26 12:00:00", "2012-05-26 18:00:00", "2012-05-27
>>      00:00:00",
>>      "2012-05-27 06:00:00", "2012-05-27 12:00:00", "2012-05-27
>>      18:00:00",
>>      "2012-05-28 00:00:00", "2012-05-28 06:00:00", "2012-05-28
>>      12:00:00",
>>      "2012-05-28 18:00:00", "2012-05-29 00:00:00", "2012-05-29
>>      06:00:00",
>>      "2012-05-29 12:00:00"), class = "factor"), basistime =
>>      structure(c(1L,
>>      1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>>      1L, 1L,
>>      1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>>      2L, 2L,
>>      2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>>      3L, 3L,
>>      3L), .Label = c("2012-01-25 15:02:00", "2012-01-26
>>      15:11:00",
>>      "2012-01-27 01:41:00", "2012-01-27 10:15:00", "2012-01-27
>>      15:15:00",
>>      "2012-01-28 14:22:00", "2012-01-29 13:33:00", "2012-01-30
>>      14:11:00",
>>      "2012-01-31 14:24:00", "2012-02-01 14:55:00", "2012-02-02
>>      14:56:00",
>>      "2012-02-03 14:40:00", "2012-02-04 15:01:00", "2012-02-05
>>      15:04:00",
>>      "2012-02-06 14:37:00", "2012-02-07 14:42:00", "2012-02-08
>>      14:44:00",
>>      "2012-02-09 14:27:00", "2012-02-10 14:33:00", "2012-02-11
>>      15:05:00",
>>      "2012-02-12 15:09:00", "2012-02-13 15:00:00", "2012-02-14
>>      15:10:00",
>>      "2012-02-15 14:46:00", "2012-02-16 14:42:00", "2012-02-17
>>      14:14:00",
>>      "2012-02-18 15:27:00", "2012-02-19 13:13:00", "2012-02-20
>>      13:56:00",
>>      "2012-02-21 14:39:00", "2012-02-22 15:33:00", "2012-02-23
>>      15:14:00",
>>      "2012-02-24 15:14:00", "2012-02-25 14:25:00", "2012-02-26
>>      14:08:00",
>>      "2012-02-27 14:55:00", "2012-02-28 14:40:00", "2012-02-29
>>      14:45:00",
>>      "2012-03-01 15:18:00", "2012-03-02 15:18:00", "2012-03-03
>>      15:43:00",
>>      "2012-03-04 15:16:00", "2012-03-05 15:17:00", "2012-03-06
>>      15:03:00",
>>      "2012-03-07 14:44:00", "2012-03-08 14:53:00", "2012-03-09
>>      15:10:00",
>>      "2012-03-10 14:43:00", "2012-03-11 13:37:00", "2012-03-12
>>      14:58:00",
>>      "2012-03-13 14:36:00", "2012-03-14 13:49:00", "2012-03-15
>>      14:32:00",
>>      "2012-03-16 15:01:00", "2012-03-17 13:55:00", "2012-03-18
>>      13:44:00",
>>      "2012-03-19 13:55:00", "2012-03-20 13:53:00", "2012-03-21
>>      14:07:00",
>>      "2012-03-22 14:03:00", "2012-03-23 14:30:00", "2012-03-24
>>      14:47:00",
>>      "2012-03-25 14:32:00", "2012-03-26 14:41:00", "2012-03-27
>>      13:51:00",
>>      "2012-03-28 13:32:00", "2012-03-29 13:59:00", "2012-03-30
>>      14:15:00",
>>      "2012-03-31 14:12:00", "2012-04-01 14:30:00", "2012-04-02
>>      13:43:00",
>>      "2012-04-03 13:54:00", "2012-04-04 13:45:00", "2012-04-05
>>      13:46:00",
>>      "2012-04-06 13:41:00", "2012-04-07 13:13:00", "2012-04-08
>>      12:59:00",
>>      "2012-04-09 13:53:00", "2012-04-10 14:15:00", "2012-04-11
>>      14:26:00",
>>      "2012-04-12 14:05:00", "2012-04-13 13:37:00", "2012-04-14
>>      13:52:00",
>>      "2012-04-15 15:00:00", "2012-04-16 14:42:00", "2012-04-17
>>      14:21:00",
>>      "2012-04-18 14:40:00", "2012-04-19 14:35:00", "2012-04-20
>>      15:14:00",
>>      "2012-04-21 14:46:00", "2012-04-22 14:44:00", "2012-04-23
>>      14:33:00",
>>      "2012-04-24 14:41:00", "2012-04-25 14:13:00", "2012-04-26
>>      14:19:00",
>>      "2012-04-27 14:35:00", "2012-04-28 13:48:00", "2012-04-29
>>      14:12:00",
>>      "2012-04-30 13:53:00", "2012-05-02 14:41:00", "2012-05-03
>>      15:01:00",
>>      "2012-05-04 15:21:00", "2012-05-05 14:57:00", "2012-05-06
>>      14:09:00",
>>      "2012-05-07 14:30:00", "2012-05-08 14:05:00", "2012-05-08
>>      21:21:00",
>>      "2012-05-09 15:02:00", "2012-05-10 14:27:00", "2012-05-11
>>      14:47:00",
>>      "2012-05-12 13:24:00", "2012-05-13 14:40:00", "2012-05-13
>>      19:24:00",
>>      "2012-05-14 14:33:00", "2012-05-15 14:39:00", "2012-05-16
>>      14:07:00",
>>      "2012-05-17 13:42:00", "2012-05-18 14:30:00", "2012-05-19
>>      13:59:00",
>>      "2012-05-20 14:14:00", "2012-05-21 14:15:00", "2012-05-22
>>      14:18:00",
>>      "2012-05-23 14:14:00", "2012-05-24 13:52:00"), class =
>>      "factor"),
>>         fcst = c(38.7, 38.9, 39.2, 39.8, 40.5, 41.5, 42.5,
>>      43.1,
>>         43.9, 44.5, 44.8, 45, 45, 45.1, 45.4, 45.8, 46.4, 47,
>>      47.5,
>>         47.9, 39.2, 40, 41.8, 42.2, 42.8, 44.4, 45.7, 46.2,
>>      46.2,
>>         46.2, 46.5, 47, 47.5, 47.9, 48.4, 49, 49.5, 49.9, 50.2,
>>      50.4,
>>         41.3, 42.5, 42.8, 42.8, 42.8, 42.9, 43, 43.3, 43.5,
>>      43.8),
>>         usgs = c(38.5, 38.6, 38.6, 38.6, 39.1, 39.8, 41.2,
>>      42.9,
>>         43.4, 43.6, 43.4, 43.1, 43, 43.1, 43.5, 43.8, 44.1,
>>      44.2,
>>         44.4, 44.5, 39.1, 39.8, 41.2, 42.9, 43.4, 43.6, 43.4,
>>      43.1,
>>         43, 43.1, 43.5, 43.8, 44.1, 44.2, 44.4, 44.5, 44.5,
>>      44.5,
>>         44.6, 44.2, 41.2, 42.9, 43.4, 43.6, 43.4, 43.1, 43,
>>      43.1,
>>         43.5, 43.8)), .Names = c("date", "basistime", "fcst",
>>      "usgs"
>>      ), row.names = c(NA, 50L), class = "data.frame")
>> 
>>      aggregate(fcst ~ basistime,data=legacy, FUN= max)
>> 
>>      A snippet...
>> 
>>                   basistime fcst
>>      1   2012-01-25 15:02:00 47.9
>>      2   2012-01-26 15:11:00 50.4
>>      3   2012-01-27 01:41:00 46.0
>>      4   2012-01-27 10:15:00 47.3
>>      5   2012-01-27 15:15:00 47.3
>>      6   2012-01-28 14:22:00 46.2
>>      7   2012-01-29 13:33:00 45.8
>>      8   2012-01-30 14:11:00 44.8
>>      9   2012-01-31 14:24:00 43.9
>>      10  2012-02-01 14:55:00 41.1
>>      11  2012-02-02 14:56:00 38.1
>>      12  2012-02-03 14:40:00 36.2
>>      13  2012-02-04 15:01:00 34.7
>>      14  2012-02-05 15:04:00 33.1
>>      15  2012-02-06 14:37:00 32.2
>> 
>>      This is what I want, except I need the other corresponding
>>      date-time column
>>      as well. I've tried this with the date-times as factors
>>      and as POSIXct
>>      values and using many of the combinations suggested in the
>>      documentation
>>      and other examples. The closest I can get returns the
>>      second date as a
>>      numeric value:
>> 
>>      aggregate(cbind(fcst,date) ~ basistime,data=legacy, FUN=
>>      max)
>> 
>>      Produces:
>> 
>>                   basistime fcst       date
>>      1   2012-01-25 15:02:00 47.9 1327942800
>>      2   2012-01-26 15:11:00 50.4 1328029200
>>      3   2012-01-27 01:41:00 46.0 1328072400
>>      4   2012-01-27 10:15:00 47.3 1328032800
>>      5   2012-01-27 15:15:00 47.3 1328115600
>>      6   2012-01-28 14:22:00 46.2 1328202000
>>      7   2012-01-29 13:33:00 45.8 1328288400
>>      8   2012-01-30 14:11:00 44.8 1328374800
>>      9   2012-01-31 14:24:00 43.9 1328461200
>>      10  2012-02-01 14:55:00 41.1 1328547600
>>      11  2012-02-02 14:56:00 38.1 1328634000
>>      12  2012-02-03 14:40:00 36.2 1328720400
>>      13  2012-02-04 15:01:00 34.7 1328806800
>>      14  2012-02-05 15:04:00 33.1 1328893200
>>      15  2012-02-06 14:37:00 32.2 1328979600
>>      16  2012-02-07 14:42:00 31.2 1329066000
>>      17  2012-02-08 14:44:00 30.4 1329152400
>>      18  2012-02-09 14:27:00 30.0 1329238800
>> 
>>      Help is greatly appreciated!
>> 
>>      Regards,
>>      Tom
>> 
>>      --
>>         [[alternative HTML version deleted]]
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>> see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible
>> code.
>> ---------------------------------------------------------------------------
>> Jeff Newmiller                        The     .....       .....  Go
>> Live...
>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>> Go...
>>                                       Live:   OO#.. Dead: OO#.. 
>> Playing
>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> /Software/Embedded Controllers)               .OO#.       .OO#. 
>> rocks...1k
>> ---------------------------------------------------------------------------
>> --
>> Thomas E Adams, III1724 Sage Lane
>> Blacksburg, VA 24060
>> tea3rd at gmail.com (personal)
>> tea at terrapredictions.org (work)
>> 1 (513) 739-9512 (cell)
>> 
> 
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                      Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ispanyolcom at gmail.com  Fri Oct 27 09:43:55 2017
From: ispanyolcom at gmail.com (Engin YILMAZ)
Date: Fri, 27 Oct 2017 10:43:55 +0300
Subject: [R] My function and NA Values Problem
Message-ID: <CAMUSX8pDADZRe7b+Dxv-y2orM5Qy46K0b9KKYvBd6RS9LrraMw@mail.gmail.com>

Dear R Staff

My working file is in the annex. "g1.csv"
I have only 2 columns. Rice and coke.
I try to execute following(below) function, but do not work.
Because "Coke" value has NA values.
I try to add "na.rm=True" to the function but do not work
How can I solve this problem with this function or another algorithm?
(Note: I have normally 450 columns)

Sincerely
Engin YILMAZ


apply(g1, 2, function(c) sum(c==0))

Rice Coke
   0   NA

From ericjberger at gmail.com  Fri Oct 27 09:55:43 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Fri, 27 Oct 2017 10:55:43 +0300
Subject: [R] My function and NA Values Problem
In-Reply-To: <CAMUSX8pDADZRe7b+Dxv-y2orM5Qy46K0b9KKYvBd6RS9LrraMw@mail.gmail.com>
References: <CAMUSX8pDADZRe7b+Dxv-y2orM5Qy46K0b9KKYvBd6RS9LrraMw@mail.gmail.com>
Message-ID: <CAGgJW74bk9Sgxt1bpvGHZM-qqkgaw6=98JartmHN54u3x2BwuA@mail.gmail.com>

na.rm=TRUE  (you need to capitalize)


On Fri, Oct 27, 2017 at 10:43 AM, Engin YILMAZ <ispanyolcom at gmail.com>
wrote:

> Dear R Staff
>
> My working file is in the annex. "g1.csv"
> I have only 2 columns. Rice and coke.
> I try to execute following(below) function, but do not work.
> Because "Coke" value has NA values.
> I try to add "na.rm=True" to the function but do not work
> How can I solve this problem with this function or another algorithm?
> (Note: I have normally 450 columns)
>
> Sincerely
> Engin YILMAZ
>
>
> apply(g1, 2, function(c) sum(c==0))
>
> Rice Coke
>    0   NA
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From sezenismail at gmail.com  Fri Oct 27 09:57:39 2017
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Fri, 27 Oct 2017 10:57:39 +0300
Subject: [R] My function and NA Values Problem
In-Reply-To: <CAMUSX8pDADZRe7b+Dxv-y2orM5Qy46K0b9KKYvBd6RS9LrraMw@mail.gmail.com>
References: <CAMUSX8pDADZRe7b+Dxv-y2orM5Qy46K0b9KKYvBd6RS9LrraMw@mail.gmail.com>
Message-ID: <1F21DF63-0DC5-4DB9-AACD-84634CD3CD97@gmail.com>


> On 27 Oct 2017, at 10:43, Engin YILMAZ <ispanyolcom at gmail.com> wrote:
> 
> Dear R Staff
> 
> My working file is in the annex. "g1.csv"
> I have only 2 columns. Rice and coke.
> I try to execute following(below) function, but do not work.
> Because "Coke" value has NA values.
> I try to add "na.rm=True" to the function but do not work
> How can I solve this problem with this function or another algorithm?
> (Note: I have normally 450 columns)
> 
> Sincerely
> Engin YILMAZ
> 
> 
> apply(g1, 2, function(c) sum(c==0))
> 
> Rice Coke
>   0   NA

A simple reproduciple example always works,

# create sample data
df <- data.frame(a = round(runif(100) * 10), b = round(runif(100) * 10))
df[which(df[,2] == 2),2] <- NA # add NA to second column

apply(df, 2, function(x) sum(x == 1, na.rm = TRUE))


From ispanyolcom at gmail.com  Fri Oct 27 10:04:21 2017
From: ispanyolcom at gmail.com (Engin YILMAZ)
Date: Fri, 27 Oct 2017 11:04:21 +0300
Subject: [R] My function and NA Values Problem
In-Reply-To: <1F21DF63-0DC5-4DB9-AACD-84634CD3CD97@gmail.com>
References: <CAMUSX8pDADZRe7b+Dxv-y2orM5Qy46K0b9KKYvBd6RS9LrraMw@mail.gmail.com>
 <1F21DF63-0DC5-4DB9-AACD-84634CD3CD97@gmail.com>
Message-ID: <CAMUSX8r6498+dzS_jdJyHOt-by+FnL20btVCUNB2yGE=76rLAQ@mail.gmail.com>

Thanks SEZEN and BERGER

Now it is working.

My mistake <- I have written  this statement (na.rm=TRUE) to the wrong
location (after the  sum expression)

Thanks for your kindly responses

Sincerely
Engin YILMAZ

2017-10-27 10:57 GMT+03:00 Ismail SEZEN <sezenismail at gmail.com>:

>
> > On 27 Oct 2017, at 10:43, Engin YILMAZ <ispanyolcom at gmail.com> wrote:
> >
> > Dear R Staff
> >
> > My working file is in the annex. "g1.csv"
> > I have only 2 columns. Rice and coke.
> > I try to execute following(below) function, but do not work.
> > Because "Coke" value has NA values.
> > I try to add "na.rm=True" to the function but do not work
> > How can I solve this problem with this function or another algorithm?
> > (Note: I have normally 450 columns)
> >
> > Sincerely
> > Engin YILMAZ
> >
> >
> > apply(g1, 2, function(c) sum(c==0))
> >
> > Rice Coke
> >   0   NA
>
> A simple reproduciple example always works,
>
> # create sample data
> df <- data.frame(a = round(runif(100) * 10), b = round(runif(100) * 10))
> df[which(df[,2] == 2),2] <- NA # add NA to second column
>
> apply(df, 2, function(x) sum(x == 1, na.rm = TRUE))
>
>
>


-- 
*Sayg?lar?mla*
Engin YILMAZ

	[[alternative HTML version deleted]]


From tring at gvdnet.dk  Fri Oct 27 15:16:28 2017
From: tring at gvdnet.dk (Troels Ring)
Date: Fri, 27 Oct 2017 15:16:28 +0200
Subject: [R] genetics: backward haplotype transmission association algorithm
Message-ID: <cf48a0e1-c0fc-cb13-60df-994ff09cd184@gvdnet.dk>

Dear friends - a couple of papers in PNAS (lastly:framework for making 
better predictions by directly estimating variables' predictivity, Lo et 
al PNAS 2016; 113:14277-14282) have focused interest on mapping complex 
traits to multiple loci spread all over the genome. I have been around 
on the relevant taskview(s) I hope but fail to see that the backward 
haplotype transmission association algorithm (BHTA) first described by 
the authors in Hum Hered 2002; 53: 197-215. I wonder whether the 
algorithm has been implemented in R - primarily I ask since I fail to 
understand exactly how the algorithm works and thought R code would help 
there.

All best wishes

Troels Ring
Aalborg, Denmark


From bgunter.4567 at gmail.com  Fri Oct 27 16:38:12 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 27 Oct 2017 07:38:12 -0700
Subject: [R] genetics: backward haplotype transmission association
	algorithm
In-Reply-To: <cf48a0e1-c0fc-cb13-60df-994ff09cd184@gvdnet.dk>
References: <cf48a0e1-c0fc-cb13-60df-994ff09cd184@gvdnet.dk>
Message-ID: <CAGxFJbRmkgpgOa_6s+S-AsdrKBhEH0tF-u+4gG-2REYeEhWPOw@mail.gmail.com>

You may get lucky here, but I recommend that you post this instead on the
Bioconductor list, which is exactly concerned with such things.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Fri, Oct 27, 2017 at 6:16 AM, Troels Ring <tring at gvdnet.dk> wrote:

> Dear friends - a couple of papers in PNAS (lastly:framework for making
> better predictions by directly estimating variables' predictivity, Lo et al
> PNAS 2016; 113:14277-14282) have focused interest on mapping complex traits
> to multiple loci spread all over the genome. I have been around on the
> relevant taskview(s) I hope but fail to see that the backward haplotype
> transmission association algorithm (BHTA) first described by the authors in
> Hum Hered 2002; 53: 197-215. I wonder whether the algorithm has been
> implemented in R - primarily I ask since I fail to understand exactly how
> the algorithm works and thought R code would help there.
>
> All best wishes
>
> Troels Ring
> Aalborg, Denmark
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From alex.restrepo at outlook.com  Fri Oct 27 21:22:13 2017
From: alex.restrepo at outlook.com (Alex Restrepo)
Date: Fri, 27 Oct 2017 19:22:13 +0000
Subject: [R] 3D Plot with Date Axis?
Message-ID: <BLUPR0301MB205271D5D8E385E852558073FA5A0@BLUPR0301MB2052.namprd03.prod.outlook.com>

Hello,

I would like to format the X axis of the plot created via the scatterplot3d function or any other function which will work.

Here is an example of what I am trying to do:

library("scatterplot3d")

mydf=data.frame(rate=c(1,1,4,4), age=c(2,2,5,5), market_date=c('2017-01-01', '2017-02-02', '2017-03-03', '2017-04-04'))

scatterplot3d(mydf$market_date, mydf$rate, mydf$age, xaxt="n")

axis.Date(1, mydf$market_date, format="%Y-%m-%d")

I tried to hide the x axis, but it looks like xaxt is not working for the scatterplot3d function.

Using the above example, could someone please provide me with some guidance and help?

Many Thanks,

Alex


	[[alternative HTML version deleted]]


From Seeliger.Curt at epa.gov  Sat Oct 28 02:41:09 2017
From: Seeliger.Curt at epa.gov (Seeliger, Curt)
Date: Sat, 28 Oct 2017 00:41:09 +0000
Subject: [R] Variable selection in clusters using 1-R2 ratio
Message-ID: <BY2PR09MB0200C3D7DD576B7B66E29085E65B0@BY2PR09MB0200.namprd09.prod.outlook.com>

Folks,
I am looking for a means for calculating the 1-R^2 ratio for variable selection to mimic the values of PROC VARCLUS in SAS. While there may be better methods for variable selection, we are trying to duplicate published  results at this time.

To date, I have been unable to find a way to obtain this value from Hmisc::varclus , stats::hclust and clustvarsel::clustvarsel.  Can any of you give me a leg up on a more appropriate package or suggest another means for obtaining the 1-R^2 ratio from the available outputs?

Thank you for your time,
cur
--
Curt Seeliger, Data Ranger
CSRA, Inc. | Contractor to ORD
seeliger.curt at epa.gov
541-754-4638


	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sat Oct 28 08:46:18 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 27 Oct 2017 23:46:18 -0700
Subject: [R] 3D Plot with Date Axis?
In-Reply-To: <BLUPR0301MB205271D5D8E385E852558073FA5A0@BLUPR0301MB2052.namprd03.prod.outlook.com>
References: <BLUPR0301MB205271D5D8E385E852558073FA5A0@BLUPR0301MB2052.namprd03.prod.outlook.com>
Message-ID: <85C9FCCE-3395-4273-A062-2336E646A849@comcast.net>


> On Oct 27, 2017, at 12:22 PM, Alex Restrepo <alex.restrepo at outlook.com> wrote:
> 
> Hello,
> 
> I would like to format the X axis of the plot created via the scatterplot3d function or any other function which will work.
> 
> Here is an example of what I am trying to do:
> 
> library("scatterplot3d")
> 
> mydf=data.frame(rate=c(1,1,4,4), age=c(2,2,5,5), market_date=c('2017-01-01', '2017-02-02', '2017-03-03', '2017-04-04'))
> 
> scatterplot3d(mydf$market_date, mydf$rate, mydf$age, xaxt="n")
> 
> axis.Date(1, mydf$market_date, format="%Y-%m-%d")
> 
> I tried to hide the x axis, but it looks like xaxt is not working for the scatterplot3d function.

Please read the "?scatterplot3d page more carefully. I think you should be looking for the x.ticklabs parameter.

-- 
David.
> 
> Using the above example, could someone please provide me with some guidance and help?
> 
> Many Thanks,
> 
> Alex
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jeremiejuste at gmail.com  Sat Oct 28 10:18:14 2017
From: jeremiejuste at gmail.com (Jeremie Juste)
Date: Sat, 28 Oct 2017 10:18:14 +0200
Subject: [R] making-a-lisp implementation of R
Message-ID: <87o9orfzy1.fsf@freegnu.noherd.org>


Hello,

I was trying to make my R implementation of making-a-lisp
https://github.com/kanaka/mal/tree/master/

And to my surprise I got stuck in the starting blocks. There is already
a mal implementation of R here
https://github.com/kanaka/mal/tree/master/r but it uses rdyncall package
which is not on CRAN anymore and I thought it would be fun to try my
skills here

The following code is step0_repl.r but it does pass the test. Do you
have any idea why?


READ <- function(arg){
    return(arg)
    }


EVAL <- function(arg){
    return(arg)
}

PRINT <- function(arg){
    return(arg)
}

REP <- function(arg){
    PRINT(EVAL(READ(arg)))
}





tryCatch({
    while(TRUE){
        cat("user> ")
        input <- tryCatch(readLines(n=1),error=function(e) character(0))
        if (identical(input,character(0))) {            
            stop
        } else{
            if(input==""){next} else
                                  cat(REP(input),"\n",sep="")
        }
    }
},error=function(err){
    cat( err$message,"\ninput: ",input,"\n", sep="")
})




Best regards,

Jeremie


From josh.m.ulrich at gmail.com  Sat Oct 28 15:51:29 2017
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Sat, 28 Oct 2017 08:51:29 -0500
Subject: [R] Problem with tq_mutate_xy() from the tidyquant package
In-Reply-To: <CAGgJW76dta1OjUUXzSZerG_43rHbV9pJ6_1=b=pGKB+zaK+7Sg@mail.gmail.com>
References: <CAGgJW76dta1OjUUXzSZerG_43rHbV9pJ6_1=b=pGKB+zaK+7Sg@mail.gmail.com>
Message-ID: <CAPPM_gSBj50zOUpQy3ncRTwz9HePEm9=7g192c0nUScLZEatRg@mail.gmail.com>

On Wed, Oct 18, 2017 at 9:54 AM, Eric Berger <ericjberger at gmail.com> wrote:
> I was able to reproduce the problem with this self-contained example. Maybe
> it could be reproduced with an even smaller one ...
>
Thanks for the reproducible example.

>
> library(tidyquant)  # Loads tidyverse, tidyquant, financial pkgs, xts/zoo
> library(xts)
>
> dtV <- as.Date("2017-01-01") + 1:100
> locL <- list( foo=xts(rnorm(100), order.by=dtV), bar=xts(rnorm(100),
> order.by=dtV) )
> fullXts <- do.call(merge,locL)
> smallXts <- fullXts["2017-02-01::"]
>
> rolling_corrOnePair <- function( aXts, col1, col2, window ) {  #window =
> window size in days
>   tbl  <- timetk::tk_tbl( aXts[,c(col1,col2)], rename_index="date" )
>   colnames(tbl) <- c("date","x","y")
>   naV <- sapply(1:nrow(tbl), function(i) any(is.na(tbl[i,])) )
>   tbl <- tbl[!naV,]# remove any rows with NAs
>    tq_mutate_xy( data=tbl, x = x, y = y, mutate_fun=runCor,
>                 n = window, use="all.obs",  # runCor args
>                 col_rename = "rolling_corr" ) # tq_mutate args
> }
>
> foo <- rolling_corrOnePair( aXts=smallXts, col1="foo", col2="bar",
> window=30 )
>
> ## This produces the error
> # Error in runCov(x, y, n, use = use, sample = sample, cumulative ) :
> #   n = 30 is outside valid range: [1, 1]
>
The error says that n=1 is only valid value for 'n' in the call to
TTR::runCov(), and you have n=30.  TTR::runCor() will throw that error
when there aren't enough observations for a n-period rolling
calculation.

I have no idea why that happens though, because the simple case below works.
corr <- TTR::runCor(fullXts$foo, fullXts$bar, n = 30)  # this is fine

So the problem must be something in your function or in the other
packages you're using.


> Thanks for any help,
>
> Eric
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com
R/Finance 2017 | www.rinfinance.com


From xavier.chiriboga at unine.ch  Sat Oct 28 17:25:18 2017
From: xavier.chiriboga at unine.ch (CHIRIBOGA Xavier)
Date: Sat, 28 Oct 2017 15:25:18 +0000
Subject: [R] Function Relevel DOE NOT FOUND
Message-ID: <1509204329801.68835@unine.ch>

Dear Forum,


Which functions and packages should be installed to make work the function "relevel"?


treatment<-revel(treatment,ref="Db")
Error: no se pudo encontrar la funci?n "revel"


Thank you very much for your help,


Xavier Chiriboga M.
PhD Candidate
Fundamental and Applied Research in Chemical Ecology Lab.
Institute of Biology
University of Neuchatel

Website: https://www.unine.ch/farce/home/membres/xavier-chiriboga.html

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sat Oct 28 17:34:26 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 28 Oct 2017 08:34:26 -0700
Subject: [R] Function Relevel DOE NOT FOUND
In-Reply-To: <1509204329801.68835@unine.ch>
References: <1509204329801.68835@unine.ch>
Message-ID: <CAGxFJbSLj70c2S7nCKghj4wrftHqcr6bSx=s_Mjaj5otZVdrvg@mail.gmail.com>

Check your spelling: "relevel" not "revel"

It's in stats  (as well as probably others).

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Sat, Oct 28, 2017 at 8:25 AM, CHIRIBOGA Xavier <xavier.chiriboga at unine.ch
> wrote:

> Dear Forum,
>
>
> Which functions and packages should be installed to make work the function
> "relevel"?
>
>
> treatment<-revel(treatment,ref="Db")
> Error: no se pudo encontrar la funci?n "revel"
>
>
> Thank you very much for your help,
>
>
> Xavier Chiriboga M.
> PhD Candidate
> Fundamental and Applied Research in Chemical Ecology Lab.
> Institute of Biology
> University of Neuchatel
>
> Website: https://www.unine.ch/farce/home/membres/xavier-chiriboga.html
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From chema at rinzewind.org  Sat Oct 28 17:28:39 2017
From: chema at rinzewind.org (=?iso-8859-1?Q?Jos=E9_Mar=EDa?= Mateos)
Date: Sat, 28 Oct 2017 11:28:39 -0400
Subject: [R] Function Relevel DOE NOT FOUND
In-Reply-To: <1509204329801.68835@unine.ch>
References: <1509204329801.68835@unine.ch>
Message-ID: <20171028152839.GA14304@equipaje>

On Sat, Oct 28, 2017 at 03:25:18PM +0000, CHIRIBOGA Xavier wrote:
>Dear Forum,
>
>
>Which functions and packages should be installed to make work the function "relevel"?
>
>
>treatment<-revel(treatment,ref="Db")
>Error: no se pudo encontrar la funci?n "revel"
>

You have a typo there, it should be "relevel", not "revel".

In my current distribution, it works right from the start, so I don't 
think you'll need to install anything:

$ R
[...]
-> relevel
function (x, ref, ...) UseMethod("relevel")
<bytecode: 0x90f9dc0>
<environment: namespace:stats>

Cheers,

JMM.


From xavier.chiriboga at unine.ch  Sat Oct 28 22:11:51 2017
From: xavier.chiriboga at unine.ch (CHIRIBOGA Xavier)
Date: Sat, 28 Oct 2017 20:11:51 +0000
Subject: [R] HELP relevel INTERCEPT-COMPARISONS
Message-ID: <1509221523264.22912@unine.ch>

Dear colleagues,


How can I do to "relevel" the intercept?

I need that the treatment "Db" be the intercept, and have p-values for the comparisons with the others treatments.

I used the function "relevel" but it did not work out to have what I want.

Thanks for your help,


Xavier


T1 <- read.table(file.choose(), h=T)
> head(T1)
  treatment replicate Time     Ebcg
1   CHA0+Db         1    1  9681172
2   CHA0+Db         2    1  5599000
3   CHA0+Db         3    1 10280480
4   CHA0+Db         4    1  1004526
5   CHA0+Db         5    1 12942018
6   CHA0+Db         6    1  7478773
> attach(T1)
> summary(T1)
   treatment   replicate          Time        Ebcg
 CHA0   :6   Min.   :1.000   Min.   :1   Min.   :       0
 CHA0+Db:9   1st Qu.:2.750   1st Qu.:1   1st Qu.: 1000922
 Db     :9   Median :4.500   Median :1   Median : 4044094
 Healthy:9   Mean   :4.625   Mean   :1   Mean   : 5699717
 PCL    :6   3rd Qu.:6.250   3rd Qu.:1   3rd Qu.: 7785782
 PCL+Db :9   Max.   :9.000   Max.   :1   Max.   :24736254
> hist(Ebcg)
> treatment<-relevel(treatment,ref="Db")
> mT1<-lm(sqrt(Ebcg)~treatment,data=T1)
> summary(mT1)

Call:
lm(formula = sqrt(Ebcg) ~ treatment, data = T1)

Residuals:
     Min       1Q   Median       3Q      Max
-1748.12  -540.19     0.97   545.14  1949.22

Coefficients:
                                 Estimate         Std. Error     t value     Pr(>|t|)
(Intercept)                     532.5          384.4       1.385         0.173296
treatmentCHA0+Db   2048.5          496.3       4.128         0.000170 ***
treatmentDb                2491.9          496.3       5.021         9.93e-06 ***
treatmentHealthy       1215.6          496.3       2.450         0.018552 *
treatmentPCL                120.4          543.6       0.221         0.825816
treatmentPCL+Db        2098.9          496.3       4.229         0.000124 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 941.6 on 42 degrees of freedom
Multiple R-squared:  0.5206,    Adjusted R-squared:  0.4635
F-statistic: 9.121 on 5 and 42 DF,  p-value: 6.227e-06



Xavier Chiriboga M.
PhD Candidate
Fundamental and Applied Research in Chemical Ecology Lab.
Institute of Biology
University of Neuchatel

Website: https://www.unine.ch/farce/home/membres/xavier-chiriboga.html

	[[alternative HTML version deleted]]


From chema at rinzewind.org  Sat Oct 28 22:48:47 2017
From: chema at rinzewind.org (=?iso-8859-1?Q?Jos=E9_Mar=EDa?= Mateos)
Date: Sat, 28 Oct 2017 16:48:47 -0400
Subject: [R] HELP relevel INTERCEPT-COMPARISONS
In-Reply-To: <1509221523264.22912@unine.ch>
References: <1509221523264.22912@unine.ch>
Message-ID: <20171028204847.GA4109@equipaje>

On Sat, Oct 28, 2017 at 08:11:51PM +0000, CHIRIBOGA Xavier wrote:
>> treatment<-relevel(treatment,ref="Db")

Never used relevel myself, but shouldn't this line be instead this one?

T1$treatment <- relevel(T1$treatment, ref = "Db")

Cheers,

JMM.


From ispanyolcom at gmail.com  Sun Oct 29 11:25:53 2017
From: ispanyolcom at gmail.com (Engin YILMAZ)
Date: Sun, 29 Oct 2017 13:25:53 +0300
Subject: [R] Count non-zero values in excluding NA Values
Message-ID: <CAMUSX8rvN8wxXt-NNyBOWD8B+Jri0UrwWSd2nbfvRz4NbNPWTA@mail.gmail.com>

Dear R Staff

You can see my data.csv file in the annex.

I try to count non-zero values in dataset but I need to exclude NA in this
calculation

My code is very long (following),
How can I write this code more efficiently and shortly?

## [NA_Count] - Find NA values

data.na =sapply(data[,3:ncol(data)], function(c) sum(length(which(is.na
(c)))))


## [Zero] - Find zero values

data.z=apply(data[,3:ncol(data)], 2, function(c) sum(c==0))


## [Non-Zero] - Find non-zero values

data.nz=nrow(data[,3:ncol(data)])- (data.na+data.z)


Sincerely
Engin YILMAZ

<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
Virus-free.
www.avast.com
<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
<#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

From ruipbarradas at sapo.pt  Sun Oct 29 12:38:31 2017
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Sun, 29 Oct 2017 11:38:31 +0000
Subject: [R] Count non-zero values in excluding NA Values
In-Reply-To: <CAMUSX8rvN8wxXt-NNyBOWD8B+Jri0UrwWSd2nbfvRz4NbNPWTA@mail.gmail.com>
References: <CAMUSX8rvN8wxXt-NNyBOWD8B+Jri0UrwWSd2nbfvRz4NbNPWTA@mail.gmail.com>
Message-ID: <59F5BDB7.6060004@sapo.pt>

Hello,

Your attachment didn't came through, R-Help strips off most types of 
files, including CSV.
Anyway, the following will do what I understand of your question. Tested 
with a fake dataset.


set.seed(3026)    # make the results reproducible
data <- matrix(1:100, ncol = 10)
data[sample(100, 15)] <- 0
data[sample(100, 10)] <- NA
data <- as.data.frame(data)

zero <- sapply(data, function(x) sum(x == 0, na.rm = TRUE))
na <- sapply(data, function(x) sum(is.na(x)))
totals <- nrow(data) - zero - na  # totals non zero per column
grand_total <- sum(totals)        # total non zero

totals
# V1  V2  V3  V4  V5  V6  V7  V8  V9 V10
#  6   8   8   8   8   7   7   8   6  10

grand_total
#[1] 76

# another way
prod(dim(data)) - sum(zero + na)
#[1] 76


Hope this helps,

Rui Barradas

Em 29-10-2017 10:25, Engin YILMAZ escreveu:
> Dear R Staff
>
> You can see my data.csv file in the annex.
>
> I try to count non-zero values in dataset but I need to exclude NA in this
> calculation
>
> My code is very long (following),
> How can I write this code more efficiently and shortly?
>
> ## [NA_Count] - Find NA values
>
> data.na =sapply(data[,3:ncol(data)], function(c) sum(length(which(is.na
> (c)))))
>
>
> ## [Zero] - Find zero values
>
> data.z=apply(data[,3:ncol(data)], 2, function(c) sum(c==0))
>
>
> ## [Non-Zero] - Find non-zero values
>
> data.nz=nrow(data[,3:ncol(data)])- (data.na+data.z)
>
>
> Sincerely
> Engin YILMAZ
>
> <https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
> Virus-free.
> www.avast.com
> <https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
> <#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ericjberger at gmail.com  Sun Oct 29 12:59:32 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Sun, 29 Oct 2017 14:59:32 +0300
Subject: [R] Count non-zero values in excluding NA Values
In-Reply-To: <59F5BDB7.6060004@sapo.pt>
References: <CAMUSX8rvN8wxXt-NNyBOWD8B+Jri0UrwWSd2nbfvRz4NbNPWTA@mail.gmail.com>
 <59F5BDB7.6060004@sapo.pt>
Message-ID: <CAGgJW74F9PUbX3Vb8y3Z0T+oF6LJ41zq7XfWhYReyCU0Q6=D5w@mail.gmail.com>

If one does not need all the intermediate results then after defining data
just one line:

grand_total <- nrow(data)*ncol(data) - sum( sapply(data, function(x) sum(
is.na(x) | x == 0 ) ) )
# 76




On Sun, Oct 29, 2017 at 2:38 PM, Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> Your attachment didn't came through, R-Help strips off most types of
> files, including CSV.
> Anyway, the following will do what I understand of your question. Tested
> with a fake dataset.
>
>
> set.seed(3026)    # make the results reproducible
> data <- matrix(1:100, ncol = 10)
> data[sample(100, 15)] <- 0
> data[sample(100, 10)] <- NA
> data <- as.data.frame(data)
>
> zero <- sapply(data, function(x) sum(x == 0, na.rm = TRUE))
> na <- sapply(data, function(x) sum(is.na(x)))
> totals <- nrow(data) - zero - na  # totals non zero per column
> grand_total <- sum(totals)        # total non zero
>
> totals
> # V1  V2  V3  V4  V5  V6  V7  V8  V9 V10
> #  6   8   8   8   8   7   7   8   6  10
>
> grand_total
> #[1] 76
>
> # another way
> prod(dim(data)) - sum(zero + na)
> #[1] 76
>
>
> Hope this helps,
>
> Rui Barradas
>
>
> Em 29-10-2017 10:25, Engin YILMAZ escreveu:
>
>> Dear R Staff
>>
>> You can see my data.csv file in the annex.
>>
>> I try to count non-zero values in dataset but I need to exclude NA in this
>> calculation
>>
>> My code is very long (following),
>> How can I write this code more efficiently and shortly?
>>
>> ## [NA_Count] - Find NA values
>>
>> data.na =sapply(data[,3:ncol(data)], function(c) sum(length(which(is.na
>> (c)))))
>>
>>
>> ## [Zero] - Find zero values
>>
>> data.z=apply(data[,3:ncol(data)], 2, function(c) sum(c==0))
>>
>>
>> ## [Non-Zero] - Find non-zero values
>>
>> data.nz=nrow(data[,3:ncol(data)])- (data.na+data.z)
>>
>>
>> Sincerely
>> Engin YILMAZ
>>
>> <https://www.avast.com/sig-email?utm_medium=email&utm_source
>> =link&utm_campaign=sig-email&utm_content=webmail>
>> Virus-free.
>> www.avast.com
>> <https://www.avast.com/sig-email?utm_medium=email&utm_source
>> =link&utm_campaign=sig-email&utm_content=webmail>
>> <#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From esawiek at gmail.com  Sun Oct 29 13:01:35 2017
From: esawiek at gmail.com (Ek Esawi)
Date: Sun, 29 Oct 2017 08:01:35 -0400
Subject: [R] Count non-zero values in excluding NA Values
In-Reply-To: <CAMUSX8rvN8wxXt-NNyBOWD8B+Jri0UrwWSd2nbfvRz4NbNPWTA@mail.gmail.com>
References: <CAMUSX8rvN8wxXt-NNyBOWD8B+Jri0UrwWSd2nbfvRz4NbNPWTA@mail.gmail.com>
Message-ID: <CA+ZkTxuq2hH8YawHLKjvnUz8bf3BkL+miCORBkxOQY82U-oNmQ@mail.gmail.com>

Since i could not see your data, the easiest thing comes to mind is court
values excluding NAs, is something like this
sum(!is.na(x))

Best of luck--EK

On Sun, Oct 29, 2017 at 6:25 AM, Engin YILMAZ <ispanyolcom at gmail.com> wrote:

> Dear R Staff
>
> You can see my data.csv file in the annex.
>
> I try to count non-zero values in dataset but I need to exclude NA in this
> calculation
>
> My code is very long (following),
> How can I write this code more efficiently and shortly?
>
> ## [NA_Count] - Find NA values
>
> data.na =sapply(data[,3:ncol(data)], function(c) sum(length(which(is.na
> (c)))))
>
>
> ## [Zero] - Find zero values
>
> data.z=apply(data[,3:ncol(data)], 2, function(c) sum(c==0))
>
>
> ## [Non-Zero] - Find non-zero values
>
> data.nz=nrow(data[,3:ncol(data)])- (data.na+data.z)
>
>
> Sincerely
> Engin YILMAZ
>
> <https://www.avast.com/sig-email?utm_medium=email&utm_
> source=link&utm_campaign=sig-email&utm_content=webmail>
> Virus-free.
> www.avast.com
> <https://www.avast.com/sig-email?utm_medium=email&utm_
> source=link&utm_campaign=sig-email&utm_content=webmail>
> <#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Sun Oct 29 13:16:18 2017
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Sun, 29 Oct 2017 12:16:18 +0000
Subject: [R] HELP relevel INTERCEPT-COMPARISONS
In-Reply-To: <1509221523264.22912@unine.ch>
References: <1509221523264.22912@unine.ch>
Message-ID: <bb793ce0-ed09-3353-3e95-12724a1c4a07@dewey.myzen.co.uk>

Dear Xavier

You problem comes from using attach(). Do not do that, it usually ends 
in tears. Either use the data parameter to the functions which support 
it or look at with() and within().

On 28/10/2017 21:11, CHIRIBOGA Xavier wrote:
> Dear colleagues,
> 
> 
> How can I do to "relevel" the intercept?
> 
> I need that the treatment "Db" be the intercept, and have p-values for the comparisons with the others treatments.
> 
> I used the function "relevel" but it did not work out to have what I want.
> 
> Thanks for your help,
> 
> 
> Xavier
> 
> 
> T1 <- read.table(file.choose(), h=T)
>> head(T1)
>    treatment replicate Time     Ebcg
> 1   CHA0+Db         1    1  9681172
> 2   CHA0+Db         2    1  5599000
> 3   CHA0+Db         3    1 10280480
> 4   CHA0+Db         4    1  1004526
> 5   CHA0+Db         5    1 12942018
> 6   CHA0+Db         6    1  7478773
>> attach(T1)
>> summary(T1)
>     treatment   replicate          Time        Ebcg
>   CHA0   :6   Min.   :1.000   Min.   :1   Min.   :       0
>   CHA0+Db:9   1st Qu.:2.750   1st Qu.:1   1st Qu.: 1000922
>   Db     :9   Median :4.500   Median :1   Median : 4044094
>   Healthy:9   Mean   :4.625   Mean   :1   Mean   : 5699717
>   PCL    :6   3rd Qu.:6.250   3rd Qu.:1   3rd Qu.: 7785782
>   PCL+Db :9   Max.   :9.000   Max.   :1   Max.   :24736254
>> hist(Ebcg)
>> treatment<-relevel(treatment,ref="Db")
>> mT1<-lm(sqrt(Ebcg)~treatment,data=T1)
>> summary(mT1)
> 
> Call:
> lm(formula = sqrt(Ebcg) ~ treatment, data = T1)
> 
> Residuals:
>       Min       1Q   Median       3Q      Max
> -1748.12  -540.19     0.97   545.14  1949.22
> 
> Coefficients:
>                                   Estimate         Std. Error     t value     Pr(>|t|)
> (Intercept)                     532.5          384.4       1.385         0.173296
> treatmentCHA0+Db   2048.5          496.3       4.128         0.000170 ***
> treatmentDb                2491.9          496.3       5.021         9.93e-06 ***
> treatmentHealthy       1215.6          496.3       2.450         0.018552 *
> treatmentPCL                120.4          543.6       0.221         0.825816
> treatmentPCL+Db        2098.9          496.3       4.229         0.000124 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> Residual standard error: 941.6 on 42 degrees of freedom
> Multiple R-squared:  0.5206,    Adjusted R-squared:  0.4635
> F-statistic: 9.121 on 5 and 42 DF,  p-value: 6.227e-06
> 
> 
> 
> Xavier Chiriboga M.
> PhD Candidate
> Fundamental and Applied Research in Chemical Ecology Lab.
> Institute of Biology
> University of Neuchatel
> 
> Website: https://www.unine.ch/farce/home/membres/xavier-chiriboga.html
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From ispanyolcom at gmail.com  Sun Oct 29 19:48:57 2017
From: ispanyolcom at gmail.com (Engin YILMAZ)
Date: Sun, 29 Oct 2017 21:48:57 +0300
Subject: [R] Count non-zero values in excluding NA Values
In-Reply-To: <CA+ZkTxuq2hH8YawHLKjvnUz8bf3BkL+miCORBkxOQY82U-oNmQ@mail.gmail.com>
References: <CAMUSX8rvN8wxXt-NNyBOWD8B+Jri0UrwWSd2nbfvRz4NbNPWTA@mail.gmail.com>
 <CA+ZkTxuq2hH8YawHLKjvnUz8bf3BkL+miCORBkxOQY82U-oNmQ@mail.gmail.com>
Message-ID: <CAMUSX8oP4FEzycz+83e6m38r-TU8EPuaKL9YQr0RUERD+v-Qpw@mail.gmail.com>

Dear R Staff

This is my file (www.fiscalforecasting.com/data.csv)

if you don't download this file, my dataset same as following

Year

Month

A

B

C

D

E

2005

July

0

*4*

NA

NA

*1*

2005

July

0

NA

NA

0

*9*

2005

July

NA

*4*

0

*1*

0

2005

July

*4*

0

*2*

*9*

NA

I try to count non-zero values which are not NA values for every *column*

*Sincerely*
*Engin YILMAZ*






2017-10-29 15:01 GMT+03:00 Ek Esawi <esawiek at gmail.com>:

> Since i could not see your data, the easiest thing comes to mind is court
> values excluding NAs, is something like this
> sum(!is.na(x))
>
> Best of luck--EK
>
> On Sun, Oct 29, 2017 at 6:25 AM, Engin YILMAZ <ispanyolcom at gmail.com>
> wrote:
>
>> Dear R Staff
>>
>> You can see my data.csv file in the annex.
>>
>> I try to count non-zero values in dataset but I need to exclude NA in this
>> calculation
>>
>> My code is very long (following),
>> How can I write this code more efficiently and shortly?
>>
>> ## [NA_Count] - Find NA values
>>
>> data.na =sapply(data[,3:ncol(data)], function(c) sum(length(which(is.na
>> (c)))))
>>
>>
>> ## [Zero] - Find zero values
>>
>> data.z=apply(data[,3:ncol(data)], 2, function(c) sum(c==0))
>>
>>
>> ## [Non-Zero] - Find non-zero values
>>
>> data.nz=nrow(data[,3:ncol(data)])- (data.na+data.z)
>>
>>
>> Sincerely
>> Engin YILMAZ
>>
>> <https://www.avast.com/sig-email?utm_medium=email&utm_source
>> =link&utm_campaign=sig-email&utm_content=webmail>
>> Virus-free.
>> www.avast.com
>> <https://www.avast.com/sig-email?utm_medium=email&utm_source
>> =link&utm_campaign=sig-email&utm_content=webmail>
>> <#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>


-- 
*Sayg?lar?mla*
Engin YILMAZ

<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
Virus-free.
www.avast.com
<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
<#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

	[[alternative HTML version deleted]]


From esawiek at gmail.com  Sun Oct 29 21:14:42 2017
From: esawiek at gmail.com (Ek Esawi)
Date: Sun, 29 Oct 2017 16:14:42 -0400
Subject: [R] Count non-zero values in excluding NA Values
In-Reply-To: <CAMUSX8rvN8wxXt-NNyBOWD8B+Jri0UrwWSd2nbfvRz4NbNPWTA@mail.gmail.com>
References: <CAMUSX8rvN8wxXt-NNyBOWD8B+Jri0UrwWSd2nbfvRz4NbNPWTA@mail.gmail.com>
Message-ID: <CA+ZkTxumdN2aVG=xPjwJVqRYzChr-vo3iFhsLc0OOw9t6vmcBw@mail.gmail.com>

What was suggested by Eric and Rui works well, but here is a short and may
be simpler answer provided your data is similar what Eric posted. It should
work for your l data too.

aa <- is.na(data)|data==0
nrow(data)-colSums(aa)

EK

On Sun, Oct 29, 2017 at 6:25 AM, Engin YILMAZ <ispanyolcom at gmail.com> wrote:

> Dear R Staff
>
> You can see my data.csv file in the annex.
>
> I try to count non-zero values in dataset but I need to exclude NA in this
> calculation
>
> My code is very long (following),
> How can I write this code more efficiently and shortly?
>
> ## [NA_Count] - Find NA values
>
> data.na =sapply(data[,3:ncol(data)], function(c) sum(length(which(is.na
> (c)))))
>
>
> ## [Zero] - Find zero values
>
> data.z=apply(data[,3:ncol(data)], 2, function(c) sum(c==0))
>
>
> ## [Non-Zero] - Find non-zero values
>
> data.nz=nrow(data[,3:ncol(data)])- (data.na+data.z)
>
>
> Sincerely
> Engin YILMAZ
>
> <https://www.avast.com/sig-email?utm_medium=email&utm_
> source=link&utm_campaign=sig-email&utm_content=webmail>
> Virus-free.
> www.avast.com
> <https://www.avast.com/sig-email?utm_medium=email&utm_
> source=link&utm_campaign=sig-email&utm_content=webmail>
> <#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ispanyolcom at gmail.com  Sun Oct 29 21:21:37 2017
From: ispanyolcom at gmail.com (Engin YILMAZ)
Date: Sun, 29 Oct 2017 23:21:37 +0300
Subject: [R] Count non-zero values in excluding NA Values
In-Reply-To: <CA+ZkTxumdN2aVG=xPjwJVqRYzChr-vo3iFhsLc0OOw9t6vmcBw@mail.gmail.com>
References: <CAMUSX8rvN8wxXt-NNyBOWD8B+Jri0UrwWSd2nbfvRz4NbNPWTA@mail.gmail.com>
 <CA+ZkTxumdN2aVG=xPjwJVqRYzChr-vo3iFhsLc0OOw9t6vmcBw@mail.gmail.com>
Message-ID: <CAMUSX8rH_dh75vg0D278HkPH6jRhv0mOcnnPD4GGKXbgmQvAwQ@mail.gmail.com>

Thanks Esawi,Barradas and Berger

Sincerely
Engin YILMAZ

<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
Virus-free.
www.avast.com
<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
<#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

2017-10-29 23:14 GMT+03:00 Ek Esawi <esawiek at gmail.com>:

> What was suggested by Eric and Rui works well, but here is a short and
> may be simpler answer provided your data is similar what Eric posted. It
> should work for your l data too.
>
> aa <- is.na(data)|data==0
> nrow(data)-colSums(aa)
>
> EK
>
> On Sun, Oct 29, 2017 at 6:25 AM, Engin YILMAZ <ispanyolcom at gmail.com>
> wrote:
>
>> Dear R Staff
>>
>> You can see my data.csv file in the annex.
>>
>> I try to count non-zero values in dataset but I need to exclude NA in this
>> calculation
>>
>> My code is very long (following),
>> How can I write this code more efficiently and shortly?
>>
>> ## [NA_Count] - Find NA values
>>
>> data.na =sapply(data[,3:ncol(data)], function(c) sum(length(which(is.na
>> (c)))))
>>
>>
>> ## [Zero] - Find zero values
>>
>> data.z=apply(data[,3:ncol(data)], 2, function(c) sum(c==0))
>>
>>
>> ## [Non-Zero] - Find non-zero values
>>
>> data.nz=nrow(data[,3:ncol(data)])- (data.na+data.z)
>>
>>
>> Sincerely
>> Engin YILMAZ
>>
>> <https://www.avast.com/sig-email?utm_medium=email&utm_source
>> =link&utm_campaign=sig-email&utm_content=webmail>
>> Virus-free.
>> www.avast.com
>> <https://www.avast.com/sig-email?utm_medium=email&utm_source
>> =link&utm_campaign=sig-email&utm_content=webmail>
>> <#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>


-- 
*Sayg?lar?mla*
Engin YILMAZ

	[[alternative HTML version deleted]]


From djnordlund at gmail.com  Sun Oct 29 23:04:26 2017
From: djnordlund at gmail.com (Daniel Nordlund)
Date: Sun, 29 Oct 2017 15:04:26 -0700
Subject: [R] Count non-zero values in excluding NA Values
In-Reply-To: <CAMUSX8rvN8wxXt-NNyBOWD8B+Jri0UrwWSd2nbfvRz4NbNPWTA@mail.gmail.com>
References: <CAMUSX8rvN8wxXt-NNyBOWD8B+Jri0UrwWSd2nbfvRz4NbNPWTA@mail.gmail.com>
Message-ID: <001aa4eb-da3d-bf91-eb3b-46b38f342e0c@gmail.com>

On 10/29/2017 3:25 AM, Engin YILMAZ wrote:
> Dear R Staff
> 
> You can see my data.csv file in the annex.
> 
> I try to count non-zero values in dataset but I need to exclude NA in this
> calculation
> 
> My code is very long (following),
> How can I write this code more efficiently and shortly?
> 
> ## [NA_Count] - Find NA values
> 
> data.na =sapply(data[,3:ncol(data)], function(c) sum(length(which(is.na
> (c)))))
> 
> 
> ## [Zero] - Find zero values
> 
> data.z=apply(data[,3:ncol(data)], 2, function(c) sum(c==0))
> 
> 
> ## [Non-Zero] - Find non-zero values
> 
> data.nz=nrow(data[,3:ncol(data)])- (data.na+data.z)
> 
> 
> Sincerely
> Engin YILMAZ
> 
> <https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
> Virus-free.
> www.avast.com
> <https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
> <#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

this looks like a good place for apply()

    apply(data,2,function(x) sum(x != 0, na.rm=TRUE))


Hope this is helpful,

Dan

-- 
Daniel Nordlund
Port Townsend, WA  USA


From chalabi.elahe at yahoo.de  Sun Oct 29 15:21:12 2017
From: chalabi.elahe at yahoo.de (Elahe chalabi)
Date: Sun, 29 Oct 2017 14:21:12 +0000 (UTC)
Subject: [R] Counting nuber of sentences by qdap package
References: <1454528057.9568364.1509286872052.ref@mail.yahoo.com>
Message-ID: <1454528057.9568364.1509286872052@mail.yahoo.com>

Hi all,

I have a data frame with a variable Description containing text of speeches and I would like to count number of sentences in each speech,

    
> str(data)
'data.frame':	255 obs. of  3 variables:
$ Group      : Factor w/ 255 levels "AlzheimerGroup1","AlzheimerGroup10",..: 1 112 179 190 201 212 223 234 245 2 ...
$ Gender     : int  1 1 0 0 0 0 0 1 0 0 ...
$ Description: Factor w/ 255 levels "A boy's on the uh falling off the stool picking up cookies . The girl's reaching up for it . The girl the lady "| __truncated__,..: 63 69 38 134 111 242 196 85 84 233 ...

I want to use qdap package.
Does anyone know how should I do this?
Thanks for any help!
Elahe


From varinsacha at yahoo.fr  Sun Oct 29 21:09:28 2017
From: varinsacha at yahoo.fr (varin sacha)
Date: Sun, 29 Oct 2017 20:09:28 +0000 (UTC)
Subject: [R] Add a vertical line and some values on a plot
In-Reply-To: <59ECC6D0.3010601@sapo.pt>
References: <894779362.2554993.1508686400784.ref@mail.yahoo.com>
 <894779362.2554993.1508686400784@mail.yahoo.com> <59ECC6D0.3010601@sapo.pt>
Message-ID: <534001767.9815537.1509307768976@mail.yahoo.com>

Hi Rui, 


Many thanks for your response.




________________________________
De : Rui Barradas <ruipbarradas at sapo.pt>

roject.org> 
Envoy? le : Dimanche 22 octobre 2017 18h27
Objet : Re: [R] Add a vertical line and some values on a plot



Hello,

After the plot just do

abline(v = median(A))

As for how to plot points, see, well, ?points().

Hope this helps,

Rui Barradas


Em 22-10-2017 16:33, varin sacha via R-help escreveu:
> Dear R-experts,
>
> Here below is my code,
> I would like to add a vertical line on my plot, showing the median and I would like to place some values on this graph as well, i.e. 4.3 and -8.4. How can I do ?
> Many thanks for your reply.
>
>
>
> A=c(1,2.3,4,3.5,4.3,2.5,6.3,-0.1,-1.5,3.7,-2.3,-3.5,5.4,3.2, -10.5,-8.4,-9.4)
> d <- density(A)
> plot(d)
> median(A)
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

>


From hannah.hlx at gmail.com  Mon Oct 30 03:24:06 2017
From: hannah.hlx at gmail.com (li li)
Date: Sun, 29 Oct 2017 22:24:06 -0400
Subject: [R] An iterative function
Message-ID: <CAHLnnda=6wtV9m9BVcfoOWZLGevxtu3F-hcVAjsS36vjX-H39Q@mail.gmail.com>

Dear all,
  The function f() below is a function of m1 and m2, both of which are
matrices with 3 rows. The function works sequentially one row after
another.
So altogether there are three stages. I am trying to update the coding to
write a generic function that will work for arbitrary k stages.
  I am hoping to get some suggestion and help. Thanks so much!
         Hanna




 ##x, y are two vectors of the same length
f0 <- function(x, y){
  I <- which((x-y)^2>0.5)
  if (length(I)==0){
    a <- 0; b <-0; c<- 0
  } else {
  a <- min(I)
  b <- x[a]
  c <- y[a]}
  return(list(a=a, b=b, c=c))
}

##both m1 and m2 are matrix with 3 rows and same number of columns
f <- function(m1, m2){
  n <- dim(m1)[2]
  tmp1 <- f0(m1[1,], m2[1,])
  S2 <- which(m1[1,] > tmp1$a)
  if (length(S2) == 0){
    t1 <- c(tmp1$b, 0, 0)
    t2 <- c(tmp1$c, 0, 0)} else {
    tmp2 <- f0(m1[2,S2], m2[2, (n-length(S2)+1):n])
    S3 <- S2[which(m2[2, S2] > tmp2$a)]
    if (length(S3) == 0) {
      t1 <- c(tmp1$b, tmp2$b, 0)
      t2 <- c(tmp1$c, tmp2$c, 0)} else {
        tmp3 <- f0(m1[3,S3], m2[3,  (n-length(S3)+1):n])
        t1 <- c(tmp1$b, tmp2$b, tmp3$c)
        t2 <- c(tmp1$c, tmp2$c, tmp3$b)
      }}
  return(list(t1=t1, t2=t2))
}

	[[alternative HTML version deleted]]


From boris.steipe at utoronto.ca  Mon Oct 30 03:52:19 2017
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Sun, 29 Oct 2017 22:52:19 -0400
Subject: [R] An iterative function
In-Reply-To: <CAHLnnda=6wtV9m9BVcfoOWZLGevxtu3F-hcVAjsS36vjX-H39Q@mail.gmail.com>
References: <CAHLnnda=6wtV9m9BVcfoOWZLGevxtu3F-hcVAjsS36vjX-H39Q@mail.gmail.com>
Message-ID: <7D3C46D6-4771-4BAC-90BC-18088E17A3E7@utoronto.ca>

Write a for-loop that tests your condition and carries the necessary parameters forward. break() when your condition is met.

B.




> On Oct 29, 2017, at 10:24 PM, li li <hannah.hlx at gmail.com> wrote:
> 
> Dear all,
>  The function f() below is a function of m1 and m2, both of which are
> matrices with 3 rows. The function works sequentially one row after
> another.
> So altogether there are three stages. I am trying to update the coding to
> write a generic function that will work for arbitrary k stages.
>  I am hoping to get some suggestion and help. Thanks so much!
>         Hanna
> 
> 
> 
> 
> ##x, y are two vectors of the same length
> f0 <- function(x, y){
>  I <- which((x-y)^2>0.5)
>  if (length(I)==0){
>    a <- 0; b <-0; c<- 0
>  } else {
>  a <- min(I)
>  b <- x[a]
>  c <- y[a]}
>  return(list(a=a, b=b, c=c))
> }
> 
> ##both m1 and m2 are matrix with 3 rows and same number of columns
> f <- function(m1, m2){
>  n <- dim(m1)[2]
>  tmp1 <- f0(m1[1,], m2[1,])
>  S2 <- which(m1[1,] > tmp1$a)
>  if (length(S2) == 0){
>    t1 <- c(tmp1$b, 0, 0)
>    t2 <- c(tmp1$c, 0, 0)} else {
>    tmp2 <- f0(m1[2,S2], m2[2, (n-length(S2)+1):n])
>    S3 <- S2[which(m2[2, S2] > tmp2$a)]
>    if (length(S3) == 0) {
>      t1 <- c(tmp1$b, tmp2$b, 0)
>      t2 <- c(tmp1$c, tmp2$c, 0)} else {
>        tmp3 <- f0(m1[3,S3], m2[3,  (n-length(S3)+1):n])
>        t1 <- c(tmp1$b, tmp2$b, tmp3$c)
>        t2 <- c(tmp1$c, tmp2$c, tmp3$b)
>      }}
>  return(list(t1=t1, t2=t2))
> }
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From p_connolly at slingshot.co.nz  Mon Oct 30 06:24:16 2017
From: p_connolly at slingshot.co.nz (Patrick Connolly)
Date: Mon, 30 Oct 2017 18:24:16 +1300
Subject: [R] How to install packages from GitHubb
Message-ID: <20171030052415.GA6527@slingshot.co.nz>

R-3.4.1

Running the train function from the latest version of the caret
package on CRAN fails with this message:

 unable to find variable "optimismBoot" 
The purported workaround is to use
devtools::install_github('topepo/caret/pkg/caret') to get  the development version.

However, that evidently has the same problem with southern hemisphere
timezones that install.packages() does -- via download.packages().
i.e. it hangs.  The workaround with that one is to use method =
"internal". What is the  equivalent for install_github()?

TIA
-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From frank.ye.mei at gmail.com  Mon Oct 30 01:56:55 2017
From: frank.ye.mei at gmail.com (Frank Mei)
Date: Sun, 29 Oct 2017 20:56:55 -0400
Subject: [R] run r script in r-fiddle
Message-ID: <CAF0J_MviTcFMLEyygkPfDhsSyxgA-qoQrfTB932yuEq1e1t7+Q@mail.gmail.com>

Hi All,

I want to know how to run an R file on my computer in R-Fiddle?

I tried source("filename.r"), but not working.

thanks,
Frank

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Mon Oct 30 07:48:44 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sun, 29 Oct 2017 23:48:44 -0700
Subject: [R] run r script in r-fiddle
In-Reply-To: <CAF0J_MviTcFMLEyygkPfDhsSyxgA-qoQrfTB932yuEq1e1t7+Q@mail.gmail.com>
References: <CAF0J_MviTcFMLEyygkPfDhsSyxgA-qoQrfTB932yuEq1e1t7+Q@mail.gmail.com>
Message-ID: <E5867B62-4C99-4038-AF61-20168BF4DC67@dcn.davis.ca.us>

You can't. Use R on your computer instead. 
-- 
Sent from my phone. Please excuse my brevity.

On October 29, 2017 5:56:55 PM PDT, Frank Mei <frank.ye.mei at gmail.com> wrote:
>Hi All,
>
>I want to know how to run an R file on my computer in R-Fiddle?
>
>I tried source("filename.r"), but not working.
>
>thanks,
>Frank
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From msuzen at gmail.com  Mon Oct 30 11:16:30 2017
From: msuzen at gmail.com (Suzen, Mehmet)
Date: Mon, 30 Oct 2017 11:16:30 +0100
Subject: [R] run r script in r-fiddle
In-Reply-To: <CAF0J_MviTcFMLEyygkPfDhsSyxgA-qoQrfTB932yuEq1e1t7+Q@mail.gmail.com>
References: <CAF0J_MviTcFMLEyygkPfDhsSyxgA-qoQrfTB932yuEq1e1t7+Q@mail.gmail.com>
Message-ID: <CAPtbhHyDh9jOgEmai8XENCX=0RA8YVQUfjwZkEyPCv-3skcCzQ@mail.gmail.com>

Hi Frank,

You could upload your R source file to a public URL, for example to
github and read via RCurl,
as source do not support https as far as I know. Here is a working example.

library('RCurl')
tmatrix <- getURL("https://raw.githubusercontent.com/msuzen/isingLenzMC/master/R/isingUtils.R")
eval(parse(text=tmatrix))

Not that you need to use raw URL for github file.

Best,
-m





On 30 October 2017 at 01:56, Frank Mei <frank.ye.mei at gmail.com> wrote:
> Hi All,
>
> I want to know how to run an R file on my computer in R-Fiddle?
>
> I tried source("filename.r"), but not working.
>
> thanks,
> Frank
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From morkus at protonmail.com  Mon Oct 30 13:30:11 2017
From: morkus at protonmail.com (Morkus)
Date: Mon, 30 Oct 2017 08:30:11 -0400
Subject: [R] Pass Parameters to RScript?
Message-ID: <7b7d2aK3Dn6pRdQEbxfJqT2DQioZWGR1HNr5HVR0S-Wb96HKq5lqQ_zVJExppIYOdF7ZFk61v3tSiJvF-Huxvsp6r_1G_8a3-u8Fdg0JjZM=@protonmail.com>

Is it possible to pass parameters to an R Script, say, from Java or other language?

I did some searches, but came up blank.

Thanks very much in advance,

Sent from [ProtonMail](https://protonmail.com), Swiss-based encrypted email.
	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Mon Oct 30 14:39:43 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Mon, 30 Oct 2017 15:39:43 +0200
Subject: [R] Pass Parameters to RScript?
In-Reply-To: <7b7d2aK3Dn6pRdQEbxfJqT2DQioZWGR1HNr5HVR0S-Wb96HKq5lqQ_zVJExppIYOdF7ZFk61v3tSiJvF-Huxvsp6r_1G_8a3-u8Fdg0JjZM=@protonmail.com>
References: <7b7d2aK3Dn6pRdQEbxfJqT2DQioZWGR1HNr5HVR0S-Wb96HKq5lqQ_zVJExppIYOdF7ZFk61v3tSiJvF-Huxvsp6r_1G_8a3-u8Fdg0JjZM=@protonmail.com>
Message-ID: <CAGgJW77JPinM=fLK2+A17qz2ALnLuMBW9Tvtbz14Ldr-B8e3Zw@mail.gmail.com>

I did a simple search and got  hits immediately, e.g.
https://www.r-bloggers.com/passing-arguments-to-an-r-script-from-command-lines/


On Mon, Oct 30, 2017 at 2:30 PM, Morkus via R-help <r-help at r-project.org>
wrote:

> Is it possible to pass parameters to an R Script, say, from Java or other
> language?
>
> I did some searches, but came up blank.
>
> Thanks very much in advance,
>
> Sent from [ProtonMail](https://protonmail.com), Swiss-based encrypted
> email.
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Mon Oct 30 15:12:17 2017
From: dcarlson at tamu.edu (David L Carlson)
Date: Mon, 30 Oct 2017 14:12:17 +0000
Subject: [R] Count non-zero values in excluding NA Values
In-Reply-To: <CAMUSX8oP4FEzycz+83e6m38r-TU8EPuaKL9YQr0RUERD+v-Qpw@mail.gmail.com>
References: <CAMUSX8rvN8wxXt-NNyBOWD8B+Jri0UrwWSd2nbfvRz4NbNPWTA@mail.gmail.com>
 <CA+ZkTxuq2hH8YawHLKjvnUz8bf3BkL+miCORBkxOQY82U-oNmQ@mail.gmail.com>
 <CAMUSX8oP4FEzycz+83e6m38r-TU8EPuaKL9YQr0RUERD+v-Qpw@mail.gmail.com>
Message-ID: <5b8de78bd0884ea881e6ec3d98fda2bd@exch-2p-mbx-w2.ads.tamu.edu>

You need to send plain text emails so that this does not happen to your data.

Assuming you want to ignore the first 2 columns:

> colSums(dta[, 3:7]>0, na.rm=TRUE)
      Raki     Whisky       Wine       Beer Cigarettes 
       153        153        153        153         32

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Engin YILMAZ
Sent: Sunday, October 29, 2017 1:49 PM
To: Ek Esawi <esawiek at gmail.com>
Cc: r-help at r-project.org
Subject: Re: [R] Count non-zero values in excluding NA Values

Dear R Staff

This is my file (www.fiscalforecasting.com/data.csv)

if you don't download this file, my dataset same as following

Year

Month

A

B

C

D

E

2005

July

0

*4*

NA

NA

*1*

2005

July

0

NA

NA

0

*9*

2005

July

NA

*4*

0

*1*

0

2005

July

*4*

0

*2*

*9*

NA

I try to count non-zero values which are not NA values for every *column*

*Sincerely*
*Engin YILMAZ*






2017-10-29 15:01 GMT+03:00 Ek Esawi <esawiek at gmail.com>:

> Since i could not see your data, the easiest thing comes to mind is 
> court values excluding NAs, is something like this
> sum(!is.na(x))
>
> Best of luck--EK
>
> On Sun, Oct 29, 2017 at 6:25 AM, Engin YILMAZ <ispanyolcom at gmail.com>
> wrote:
>
>> Dear R Staff
>>
>> You can see my data.csv file in the annex.
>>
>> I try to count non-zero values in dataset but I need to exclude NA in 
>> this calculation
>>
>> My code is very long (following),
>> How can I write this code more efficiently and shortly?
>>
>> ## [NA_Count] - Find NA values
>>
>> data.na =sapply(data[,3:ncol(data)], function(c) 
>> sum(length(which(is.na
>> (c)))))
>>
>>
>> ## [Zero] - Find zero values
>>
>> data.z=apply(data[,3:ncol(data)], 2, function(c) sum(c==0))
>>
>>
>> ## [Non-Zero] - Find non-zero values
>>
>> data.nz=nrow(data[,3:ncol(data)])- (data.na+data.z)
>>
>>
>> Sincerely
>> Engin YILMAZ
>>
>> <https://www.avast.com/sig-email?utm_medium=email&utm_source
>> =link&utm_campaign=sig-email&utm_content=webmail>
>> Virus-free.
>> www.avast.com
>> <https://www.avast.com/sig-email?utm_medium=email&utm_source
>> =link&utm_campaign=sig-email&utm_content=webmail>
>> <#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti 
>> ng-guide.html and provide commented, minimal, self-contained, 
>> reproducible code.
>>
>
>


--
*Sayg?lar?mla*
Engin YILMAZ

<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
Virus-free.
www.avast.com
<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
<#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From maechler at stat.math.ethz.ch  Mon Oct 30 15:51:44 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 30 Oct 2017 15:51:44 +0100
Subject: [R] run r script in r-fiddle
In-Reply-To: <CAPtbhHyDh9jOgEmai8XENCX=0RA8YVQUfjwZkEyPCv-3skcCzQ@mail.gmail.com>
References: <CAF0J_MviTcFMLEyygkPfDhsSyxgA-qoQrfTB932yuEq1e1t7+Q@mail.gmail.com>
 <CAPtbhHyDh9jOgEmai8XENCX=0RA8YVQUfjwZkEyPCv-3skcCzQ@mail.gmail.com>
Message-ID: <23031.15488.515464.855674@stat.math.ethz.ch>

>>>>> Suzen, Mehmet <msuzen at gmail.com>
>>>>>     on Mon, 30 Oct 2017 11:16:30 +0100 writes:

    > Hi Frank, You could upload your R source file to a public
    > URL, for example to github and read via RCurl, as source
    > do not support https as far as I know. 

well... but your knowledge is severely (:-) outdated.
Why did you not try first?

source("https://raw.githubusercontent.com/msuzen/isingLenzMC/master/R/isingUtils.R")

works for me even in R 3.3.0 which is really outdated itself!


From msuzen at gmail.com  Mon Oct 30 16:05:18 2017
From: msuzen at gmail.com (Suzen, Mehmet)
Date: Mon, 30 Oct 2017 16:05:18 +0100
Subject: [R] run r script in r-fiddle
In-Reply-To: <23031.15488.515464.855674@stat.math.ethz.ch>
References: <CAF0J_MviTcFMLEyygkPfDhsSyxgA-qoQrfTB932yuEq1e1t7+Q@mail.gmail.com>
 <CAPtbhHyDh9jOgEmai8XENCX=0RA8YVQUfjwZkEyPCv-3skcCzQ@mail.gmail.com>
 <23031.15488.515464.855674@stat.math.ethz.ch>
Message-ID: <CAPtbhHxOFy2-OBeuZnNt9WOV1A8qyCAoerQYGywA_c0f2OY7JA@mail.gmail.com>

We were talking about r-fiddle. It gives error there [*], that's why I
suggested using RCurl.

> source("https://raw.githubusercontent.com/msuzen/isingLenzMC/master/R/isingUtils.R")
...
unsupported URL scheme
Error : cannot open the connection
>

On 30 October 2017 at 15:51, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
>>>>>> Suzen, Mehmet <msuzen at gmail.com>
>>>>>>     on Mon, 30 Oct 2017 11:16:30 +0100 writes:
>
>     > Hi Frank, You could upload your R source file to a public
>     > URL, for example to github and read via RCurl, as source
>     > do not support https as far as I know.
>
> well... but your knowledge is severely (:-) outdated.
> Why did you not try first?
>
> source("https://raw.githubusercontent.com/msuzen/isingLenzMC/master/R/isingUtils.R")
>
> works for me even in R 3.3.0 which is really outdated itself!
>


From morkus at protonmail.com  Mon Oct 30 16:10:49 2017
From: morkus at protonmail.com (Morkus)
Date: Mon, 30 Oct 2017 11:10:49 -0400
Subject: [R] Pass Parameters to RScript?
In-Reply-To: <CAGgJW77JPinM=fLK2+A17qz2ALnLuMBW9Tvtbz14Ldr-B8e3Zw@mail.gmail.com>
References: <7b7d2aK3Dn6pRdQEbxfJqT2DQioZWGR1HNr5HVR0S-Wb96HKq5lqQ_zVJExppIYOdF7ZFk61v3tSiJvF-Huxvsp6r_1G_8a3-u8Fdg0JjZM=@protonmail.com>
 <CAGgJW77JPinM=fLK2+A17qz2ALnLuMBW9Tvtbz14Ldr-B8e3Zw@mail.gmail.com>
Message-ID: <fFe6VQZOCdYI72z6FkGYHAvcwEciZZ_F2KCSRYnIFreYBcze6fouwmUOMq1JpdeK-8I6Blu7suoQyT3-ECImK9FqlSBZr7N1audAXQB8GdU=@protonmail.com>

Thanks Eric,

I saw that page, too, but it states:

"This post describes how to pass external arguments to R when calling a Rscript with a command line."

Not what I'm trying to do.

Thanks for your reply.

Sent from [ProtonMail](https://protonmail.com), Swiss-based encrypted email.

> -------- Original Message --------
> Subject: Re: [R] Pass Parameters to RScript?
> Local Time: October 30, 2017 9:39 AM
> UTC Time: October 30, 2017 1:39 PM
> From: ericjberger at gmail.com
> To: Morkus <morkus at protonmail.com>
> r-help at r-project.org <r-help at r-project.org>
>
> I did a simple search and got  hits immediately, e.g.
> https://www.r-bloggers.com/passing-arguments-to-an-r-script-from-command-lines/
>
> On Mon, Oct 30, 2017 at 2:30 PM, Morkus via R-help <r-help at r-project.org> wrote:
>
>> Is it possible to pass parameters to an R Script, say, from Java or other language?
>>
>> I did some searches, but came up blank.
>>
>> Thanks very much in advance,
>>
>> Sent from [ProtonMail](https://protonmail.com), Swiss-based encrypted email.
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
	[[alternative HTML version deleted]]


From msuzen at gmail.com  Mon Oct 30 16:14:22 2017
From: msuzen at gmail.com (Suzen, Mehmet)
Date: Mon, 30 Oct 2017 16:14:22 +0100
Subject: [R] run r script in r-fiddle
In-Reply-To: <CAPtbhHxOFy2-OBeuZnNt9WOV1A8qyCAoerQYGywA_c0f2OY7JA@mail.gmail.com>
References: <CAF0J_MviTcFMLEyygkPfDhsSyxgA-qoQrfTB932yuEq1e1t7+Q@mail.gmail.com>
 <CAPtbhHyDh9jOgEmai8XENCX=0RA8YVQUfjwZkEyPCv-3skcCzQ@mail.gmail.com>
 <23031.15488.515464.855674@stat.math.ethz.ch>
 <CAPtbhHxOFy2-OBeuZnNt9WOV1A8qyCAoerQYGywA_c0f2OY7JA@mail.gmail.com>
Message-ID: <CAPtbhHzNNrFs7X1D3=73-iCBC2D2OE0Rnh7UVhNX6FV1_BV4dA@mail.gmail.com>

 Note that, looks like r-fiddle runs R 3.1.2.


From ericjberger at gmail.com  Mon Oct 30 16:33:48 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Mon, 30 Oct 2017 17:33:48 +0200
Subject: [R] Pass Parameters to RScript?
In-Reply-To: <fFe6VQZOCdYI72z6FkGYHAvcwEciZZ_F2KCSRYnIFreYBcze6fouwmUOMq1JpdeK-8I6Blu7suoQyT3-ECImK9FqlSBZr7N1audAXQB8GdU=@protonmail.com>
References: <7b7d2aK3Dn6pRdQEbxfJqT2DQioZWGR1HNr5HVR0S-Wb96HKq5lqQ_zVJExppIYOdF7ZFk61v3tSiJvF-Huxvsp6r_1G_8a3-u8Fdg0JjZM=@protonmail.com>
 <CAGgJW77JPinM=fLK2+A17qz2ALnLuMBW9Tvtbz14Ldr-B8e3Zw@mail.gmail.com>
 <fFe6VQZOCdYI72z6FkGYHAvcwEciZZ_F2KCSRYnIFreYBcze6fouwmUOMq1JpdeK-8I6Blu7suoQyT3-ECImK9FqlSBZr7N1audAXQB8GdU=@protonmail.com>
Message-ID: <CAGgJW76Tj501nv14DMS3Dw9ze_c5RcmN5LRH4_tcAmkmfDTBWA@mail.gmail.com>

I do not program in Java but it seems a Java program can make system calls
which would be equivalent to running from the command line, but done from
within a Java program. Not sure whether that would meet your needs and if
not why not. Just a suggestion.

Check out

http://www.java-samples.com/showtutorial.php?tutorialid=8



On Mon, Oct 30, 2017 at 5:10 PM, Morkus <morkus at protonmail.com> wrote:

> Thanks Eric,
>
> I saw that page, too, but it states:
>
> "This post describes how to pass external arguments to *R* when calling a
> Rscript *with a command line.*"
>
> Not what I'm trying to do.
>
> Thanks for your reply.
>
> Sent from ProtonMail <https://protonmail.com>, Swiss-based encrypted
> email.
>
>
> -------- Original Message --------
> Subject: Re: [R] Pass Parameters to RScript?
> Local Time: October 30, 2017 9:39 AM
> UTC Time: October 30, 2017 1:39 PM
> From: ericjberger at gmail.com
> To: Morkus <morkus at protonmail.com>
> r-help at r-project.org <r-help at r-project.org>
>
> I did a simple search and got  hits immediately, e.g.
> https://www.r-bloggers.com/passing-arguments-to-an-r-
> script-from-command-lines/
>
>
> On Mon, Oct 30, 2017 at 2:30 PM, Morkus via R-help <r-help at r-project.org>
> wrote:
>
>> Is it possible to pass parameters to an R Script, say, from Java or other
>> language?
>>
>> I did some searches, but came up blank.
>>
>> Thanks very much in advance,
>>
>> Sent from [ProtonMail](https://protonmail.com), Swiss-based encrypted
>> email.
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Mon Oct 30 16:58:24 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 30 Oct 2017 08:58:24 -0700
Subject: [R] Pass Parameters to RScript?
In-Reply-To: <CAGgJW76Tj501nv14DMS3Dw9ze_c5RcmN5LRH4_tcAmkmfDTBWA@mail.gmail.com>
References: <7b7d2aK3Dn6pRdQEbxfJqT2DQioZWGR1HNr5HVR0S-Wb96HKq5lqQ_zVJExppIYOdF7ZFk61v3tSiJvF-Huxvsp6r_1G_8a3-u8Fdg0JjZM=@protonmail.com>
 <CAGgJW77JPinM=fLK2+A17qz2ALnLuMBW9Tvtbz14Ldr-B8e3Zw@mail.gmail.com>
 <fFe6VQZOCdYI72z6FkGYHAvcwEciZZ_F2KCSRYnIFreYBcze6fouwmUOMq1JpdeK-8I6Blu7suoQyT3-ECImK9FqlSBZr7N1audAXQB8GdU=@protonmail.com>
 <CAGgJW76Tj501nv14DMS3Dw9ze_c5RcmN5LRH4_tcAmkmfDTBWA@mail.gmail.com>
Message-ID: <A3D5965C-AC9C-4CC1-BFB6-5D3EEBBE2936@dcn.davis.ca.us>

I do not do this either, and technically this list is not the right place to ask this multi-language question, rather apparently [1] is because the alternative to using a system call is to use the Rserve service. You should use a search engine to look for info on using rjava and rserve.

[1] https://mailman.rz.uni-augsburg.de/pipermail/stats-rosuda-devel/
-- 
Sent from my phone. Please excuse my brevity.

On October 30, 2017 8:33:48 AM PDT, Eric Berger <ericjberger at gmail.com> wrote:
>I do not program in Java but it seems a Java program can make system
>calls
>which would be equivalent to running from the command line, but done
>from
>within a Java program. Not sure whether that would meet your needs and
>if
>not why not. Just a suggestion.
>
>Check out
>
>http://www.java-samples.com/showtutorial.php?tutorialid=8
>
>
>
>On Mon, Oct 30, 2017 at 5:10 PM, Morkus <morkus at protonmail.com> wrote:
>
>> Thanks Eric,
>>
>> I saw that page, too, but it states:
>>
>> "This post describes how to pass external arguments to *R* when
>calling a
>> Rscript *with a command line.*"
>>
>> Not what I'm trying to do.
>>
>> Thanks for your reply.
>>
>> Sent from ProtonMail <https://protonmail.com>, Swiss-based encrypted
>> email.
>>
>>
>> -------- Original Message --------
>> Subject: Re: [R] Pass Parameters to RScript?
>> Local Time: October 30, 2017 9:39 AM
>> UTC Time: October 30, 2017 1:39 PM
>> From: ericjberger at gmail.com
>> To: Morkus <morkus at protonmail.com>
>> r-help at r-project.org <r-help at r-project.org>
>>
>> I did a simple search and got  hits immediately, e.g.
>> https://www.r-bloggers.com/passing-arguments-to-an-r-
>> script-from-command-lines/
>>
>>
>> On Mon, Oct 30, 2017 at 2:30 PM, Morkus via R-help
><r-help at r-project.org>
>> wrote:
>>
>>> Is it possible to pass parameters to an R Script, say, from Java or
>other
>>> language?
>>>
>>> I did some searches, but came up blank.
>>>
>>> Thanks very much in advance,
>>>
>>> Sent from [ProtonMail](https://protonmail.com), Swiss-based
>encrypted
>>> email.
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>> ng-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From alex.restrepo at outlook.com  Tue Oct 31 00:56:02 2017
From: alex.restrepo at outlook.com (Alex Restrepo)
Date: Mon, 30 Oct 2017 23:56:02 +0000
Subject: [R] Scatterplot3d :: Rotating x tick labels by x degrees
Message-ID: <BLUPR0301MB2052BB1D513419274FB69EB6FA590@BLUPR0301MB2052.namprd03.prod.outlook.com>

Hi,

I would like to rotate the x axis tick labels by 45 degrees.   Using the code below, could someone please provide an example?   Many Thanks In Advance, Alex

library("scatterplot3d")
mydf=data.frame(rate=seq(158, 314)
                ,age=seq(1, 157)
                ,market_date=seq(as.Date("2000/1/1"), as.Date("2003/1/1"), by="7 days"))

mydf$market_date=as.Date(mydf$market_date, format="%Y-%m-%d")

scatterplot3d(mydf$market_date
              ,mydf$rate
              ,mydf$age
              ,x.ticklabs = seq(as.Date("2000/1/1"), as.Date("2003/1/1"), by="330 days"))


	[[alternative HTML version deleted]]


From gmoyeyemi at gmail.com  Tue Oct 31 07:02:40 2017
From: gmoyeyemi at gmail.com (Gafar Matanmi Oyeyemi)
Date: Tue, 31 Oct 2017 07:02:40 +0100
Subject: [R] lasso and ridge regression
Message-ID: <CAFcbP-DK_1ZXCpVhTdNb-gPY2A0nPDCur0Pe6RTxT6mTULaKkQ@mail.gmail.com>

Dear All

The problem is about regularization methods in multiple regression when the
independent variables are collinear. A modified regularization method with
two tuning parameters l1 and l2 and their product l1*l2 (Lambda 1 and
Lambda 2) such that l1 takes care of ridge property and l2 takes care of
LASSO property is proposed

The proposed method is given
<https://i.stack.imgur.com/Ta8FR.jpg>

The problem is how to adapt "glmnet" to accomplish our task.

The extract of the code used is reproduced as follows;

    cv.ridge<- glmnet(x, y, family="gaussian", alpha=0,
    lambda=lambda1, standardize=TRUE)
    cv.lasso<- glmnet(x, y, family="gaussian", alpha=1,
    lambda=lambda2, standardize=TRUE)
    ##weight
    a=1/abs(matrix(coef(cv.ridge, s=lambda1)[, 1][2:(ncol(x)+1)]
    ))^1
    b=1/abs(matrix(coef(cv.lasso, s=lambda2)[, 1][2:(ncol(x)+1)]
    ))^1
    c=a*b
    w4 <-a+b+c
    w4[w4[,1] == Inf] <- 9
    # Fit modified procedure
    fit<- glmnet(x, y, family="gaussian",
    alpha=alpha,lambda=lambda1+lambda2, penalty.factor=w4)

The question is; Does the code address the modified procedure in as shown
in the equation? If not, suggestions are please welcome.

Thanks


-- 
OYEYEMI, Gafar Matanmi (Ph.D)
Reader
Department of Statistics
University of Ilorin.
Area of Specialization: Multivariate Analysis, Statistical Quality Control
& Total Quality Management.
Tel: +2348052278655, +2348068241885

	[[alternative HTML version deleted]]


From p_connolly at slingshot.co.nz  Tue Oct 31 09:10:07 2017
From: p_connolly at slingshot.co.nz (Patrick Connolly)
Date: Tue, 31 Oct 2017 21:10:07 +1300
Subject: [R] Final models from caret's train function
Message-ID: <20171031081007.GB6527@slingshot.co.nz>

Using caret on the Titanic data from Kaggle, I tried various models,
including rfRules which produces a model, partly described as such:

          
> caret.rfRules.cv$finalModel
$model
      len freq     err                 
 [1,] "2" "0.0368" "0"                 
 [2,] "2" "0.032"  "0.05"              
 [3,] "2" "0.1824" "0.0526315789473685"
 [4,] "4" "0.0656" "0.0975609756097561"
 [5,] "4" "0.0304" "0.105263157894737" 
 [6,] "3" "0.4384" "0.105839416058394" 
 [7,] "3" "0.0112" "0.142857142857143" 
 [8,] "4" "0.0256" "0.1875"            
 [9,] "3" "0.1088" "0.279411764705882" 
[10,] "3" "0.056"  "0.342857142857143" 
[11,] "1" "0.0128" "0.25"              
      condition                                                           pred
 [1,] "X[,4]<=7 & X[,11]<=4.5"                                            "1" 
 [2,] "X[,7]<=31.33125 & X[,11]>4.5"                                      "0" 
 [3,] "X[,2]<=0.5 & X[,3]<=0.5"                                           "1" 
 [4,] "X[,2]>0.5 & X[,4]<=30.5 & X[,5]>0.5 & X[,9]>0.5"                   "0" 
 [5,] "X[,3]<=0.5 & X[,4]<=30.2031919426199 & X[,4]>21.5 & X[,9]<=0.5"    "1" 
 [6,] "X[,3]>0.5 & X[,4]>9.5 & X[,7]<=26.26875"                           "0" 
 [7,] "X[,2]>0.5 & X[,7]>13.90835 & X[,7]<=15.3729"                       "0" 
 [8,] "X[,4]<=40.8653667208804 & X[,4]>25 & X[,7]>26.14375 & X[,11]<=1.5" "1" 
 [9,] "X[,3]>0.5 & X[,4]>8.16718191075288 & X[,4]<=77"                    "0" 
[10,] "X[,3]<=0.5 & X[,4]<=38.5 & X[,4]>12.5"                             "1" 
[11,] "X[,1]==X[,1]"                                                      "0" 

[...]

Does that 11th row make sense?  X[,1]==X[,1] will always be true, so
is that saying anything?  Or is it a case of a model for prediction
being useless for inference?


> version
               _                           
platform       x86_64-pc-linux-gnu         
arch           x86_64                      
os             linux-gnu                   
system         x86_64, linux-gnu           
status                                     
major          3                           
minor          4.1                         
year           2017                        
month          06                          
day            30                          
svn rev        72865                       
language       R                           
version.string R version 3.4.1 (2017-06-30)
nickname       Single Candle     

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From highstat at highstat.com  Tue Oct 31 12:40:53 2017
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Tue, 31 Oct 2017 11:40:53 +0000
Subject: [R] Course in Lisbon: Introduction to Linear Mixed Effects Models
 and GLMM with R
Message-ID: <79024998-0c45-1bfd-b6c8-3cc457116b43@highstat.com>

We would like to announce the following statistics course:

Course: Introduction to Linear Mixed Effects Models and GLMM with R
Where:? Lisbon, Portugal
When:?? 19-23 February 2018

Course website: http://highstat.com/index.php/courses
Course flyer: 
http://highstat.com/Courses/Flyers/2018/Flyer2018_02LisbonV2.pdf


Kind regards,

Alain Zuur



-- 

Dr. Alain F. Zuur
Highland Statistics Ltd.
9 St Clair Wynd
AB41 6DZ Newburgh, UK
Email: highstat at highstat.com
URL:   www.highstat.com

And:
NIOZ Royal Netherlands Institute for Sea Research,
Department of Coastal Systems, and Utrecht University,
P.O. Box 59, 1790 AB Den Burg,
Texel, The Netherlands



Author of:
1. Beginner's Guide to Spatial, Temporal and Spatial-Temporal Ecological Data Analysis with R-INLA. (2017).
2. Beginner's Guide to Zero-Inflated Models with R (2016).
3. Beginner's Guide to Data Exploration and Visualisation with R (2015).
4. Beginner's Guide to GAMM with R (2014).
5. Beginner's Guide to GLM and GLMM with R (2013).
6. Beginner's Guide to GAM with R (2012).
7. Zero Inflated Models and GLMM with R (2012).
8. A Beginner's Guide to R (2009).
9. Mixed effects models and extensions in ecology with R (2009).
10. Analysing Ecological Data (2007).


From maechler at stat.math.ethz.ch  Tue Oct 31 12:42:43 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 31 Oct 2017 12:42:43 +0100
Subject: [R] run r script in r-fiddle
In-Reply-To: <CAPtbhHxOFy2-OBeuZnNt9WOV1A8qyCAoerQYGywA_c0f2OY7JA@mail.gmail.com>
References: <CAF0J_MviTcFMLEyygkPfDhsSyxgA-qoQrfTB932yuEq1e1t7+Q@mail.gmail.com>
 <CAPtbhHyDh9jOgEmai8XENCX=0RA8YVQUfjwZkEyPCv-3skcCzQ@mail.gmail.com>
 <23031.15488.515464.855674@stat.math.ethz.ch>
 <CAPtbhHxOFy2-OBeuZnNt9WOV1A8qyCAoerQYGywA_c0f2OY7JA@mail.gmail.com>
Message-ID: <23032.25011.679300.970659@stat.math.ethz.ch>

>>>>> Suzen, Mehmet <msuzen at gmail.com>
>>>>>     on Mon, 30 Oct 2017 16:05:18 +0100 writes:

    > We were talking about r-fiddle. It gives error there [*],

    > that's why I suggested using RCurl.

    >> source("https://raw.githubusercontent.com/msuzen/isingLenzMC/master/R/isingUtils.R")
    > ...  unsupported URL scheme Error : cannot open the
    > connection
    >> 

and later you note that it seems to run R-3.1.2 --- which is
ridiculously outdated.

I think R-help should not be (mis)used at all to talk about
R-Fiddle, then.
Notably as I think it's been provided by a company that no
longer exists under that name, and even if that'd be wrong,  R-Fiddle
does not seem free software (apart from the R parts, I hope !).

If another asked about his/her problems using R 3.1.2, I think
we would all tell him to go away and install a current version
of R, and not waste any more bandwidth and R-help readers' time,
wouldn't we ?


    > On 30 October 2017 at 15:51, Martin Maechler
    > <maechler at stat.math.ethz.ch> wrote:
    >>>>>>> Suzen, Mehmet <msuzen at gmail.com> on Mon, 30 Oct 2017
    >>>>>>> 11:16:30 +0100 writes:
    >> 
    >> > Hi Frank, You could upload your R source file to a
    >> public > URL, for example to github and read via RCurl,
    >> as source > do not support https as far as I know.
    >> 
    >> well... but your knowledge is severely (:-) outdated.
    >> Why did you not try first?
    >> 
    >> source("https://raw.githubusercontent.com/msuzen/isingLenzMC/master/R/isingUtils.R")
    >> 
    >> works for me even in R 3.3.0 which is really outdated
    >> itself!


From cjsilwood at gmail.com  Tue Oct 31 12:43:26 2017
From: cjsilwood at gmail.com (Chris S)
Date: Tue, 31 Oct 2017 11:43:26 +0000
Subject: [R] SamplingStrata R package
Message-ID: <CAHBtsTsXSRTfFqtpZWoVLfdXbiH_Ur2TuXEaZp46DyucH7L1OQ@mail.gmail.com>

Hi all

I am hoping to use the SamplingStrata R package for a dataset describing a
population of businesses wherein I have information on the type of
business, as well as, for designated employment number bands, number of
employees and business turnover information. So in this context the
employment number bands can be described as micro, small, medium and large,
i.e. size of business. Hence I would like the stratification to be business
type X business size. I note that SamplingStrata allows for multivariate
scenarios and the data frame should be straightforward to set up, but in
terms of "domains" which descriptor should be used for this? I am assuming
I can use both number of employees and business turnover information to
optimise the stratification sampling procedure.

thank you in advance

	[[alternative HTML version deleted]]


From apramanik17 at gmail.com  Tue Oct 31 13:41:36 2017
From: apramanik17 at gmail.com (Anima Pramanik)
Date: Tue, 31 Oct 2017 18:11:36 +0530
Subject: [R] error to run this package
Message-ID: <CADhwAg38gYpWJp3p+JjLK-6rcxGuvJEkiaHr2QLhvxqXf0R2Cw@mail.gmail.com>

Error: package or namespace load failed for ?car? in get(Info[i, 1], envir
= env):
 cannot allocate memory block of size 2.5 Gb


please help me to get a solution of this problem

	[[alternative HTML version deleted]]


From msuzen at gmail.com  Tue Oct 31 15:09:07 2017
From: msuzen at gmail.com (Suzen, Mehmet)
Date: Tue, 31 Oct 2017 15:09:07 +0100
Subject: [R] run r script in r-fiddle
In-Reply-To: <23032.25011.679300.970659@stat.math.ethz.ch>
References: <CAF0J_MviTcFMLEyygkPfDhsSyxgA-qoQrfTB932yuEq1e1t7+Q@mail.gmail.com>
 <CAPtbhHyDh9jOgEmai8XENCX=0RA8YVQUfjwZkEyPCv-3skcCzQ@mail.gmail.com>
 <23031.15488.515464.855674@stat.math.ethz.ch>
 <CAPtbhHxOFy2-OBeuZnNt9WOV1A8qyCAoerQYGywA_c0f2OY7JA@mail.gmail.com>
 <23032.25011.679300.970659@stat.math.ethz.ch>
Message-ID: <CAPtbhHz=TXEh6dz+ERtNxp5b0e7J-39x-qEUEXd5hp5v-ZxAsw@mail.gmail.com>

On 31 October 2017 at 12:42, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> Notably as I think it's been provided by a company that no
> longer exists under that name, and even if that'd be wrong,  R-Fiddle
> does not seem free software (apart from the R parts, I hope !).

For the record, r-fiddle is maintained by datacamp:
https://www.datacamp.com/community/blog/r-fiddle-an-online-playground-for-r-code-2


From bgunter.4567 at gmail.com  Tue Oct 31 15:37:21 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 31 Oct 2017 07:37:21 -0700
Subject: [R] SamplingStrata R package
In-Reply-To: <CAHBtsTsXSRTfFqtpZWoVLfdXbiH_Ur2TuXEaZp46DyucH7L1OQ@mail.gmail.com>
References: <CAHBtsTsXSRTfFqtpZWoVLfdXbiH_Ur2TuXEaZp46DyucH7L1OQ@mail.gmail.com>
Message-ID: <CAGxFJbS54b6SfkSpkcRd6_UBgFXina=MQsPmDXXAXx6M4sDbbw@mail.gmail.com>

1.  There is no question here.

2. In any case, this is not a code writing service, so a question about how
to code models without any offering of your own attempts might not be
replied to anyway.

3. For what sorts of queries you might expect replies to, please read and
follow the posting guide below. Also, if you do post, please post in plaint
text, not html, as the latter (especially code) can get mangled by the mail
server.

Cheers,
Bert




Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Tue, Oct 31, 2017 at 4:43 AM, Chris S <cjsilwood at gmail.com> wrote:

> Hi all
>
> I am hoping to use the SamplingStrata R package for a dataset describing a
> population of businesses wherein I have information on the type of
> business, as well as, for designated employment number bands, number of
> employees and business turnover information. So in this context the
> employment number bands can be described as micro, small, medium and large,
> i.e. size of business. Hence I would like the stratification to be business
> type X business size. I note that SamplingStrata allows for multivariate
> scenarios and the data frame should be straightforward to set up, but in
> terms of "domains" which descriptor should be used for this? I am assuming
> I can use both number of employees and business turnover information to
> optimise the stratification sampling procedure.
>
> thank you in advance
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ggrothendieck at gmail.com  Tue Oct 31 15:42:14 2017
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 31 Oct 2017 10:42:14 -0400
Subject: [R] run r script in r-fiddle
In-Reply-To: <CAPtbhHzNNrFs7X1D3=73-iCBC2D2OE0Rnh7UVhNX6FV1_BV4dA@mail.gmail.com>
References: <CAF0J_MviTcFMLEyygkPfDhsSyxgA-qoQrfTB932yuEq1e1t7+Q@mail.gmail.com>
 <CAPtbhHyDh9jOgEmai8XENCX=0RA8YVQUfjwZkEyPCv-3skcCzQ@mail.gmail.com>
 <23031.15488.515464.855674@stat.math.ethz.ch>
 <CAPtbhHxOFy2-OBeuZnNt9WOV1A8qyCAoerQYGywA_c0f2OY7JA@mail.gmail.com>
 <CAPtbhHzNNrFs7X1D3=73-iCBC2D2OE0Rnh7UVhNX6FV1_BV4dA@mail.gmail.com>
Message-ID: <CAP01uRk_tqvsQqoJTK_Y+nSO8jtzf-YHdzSxuhVSkMfcfm7paw@mail.gmail.com>

Try that source statement here -- it is running R 3.4.1:

https://www.tutorialspoint.com/execute_r_online.php

On Mon, Oct 30, 2017 at 11:14 AM, Suzen, Mehmet <msuzen at gmail.com> wrote:
>  Note that, looks like r-fiddle runs R 3.1.2.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From cjsilwood at gmail.com  Tue Oct 31 16:15:10 2017
From: cjsilwood at gmail.com (Chris S)
Date: Tue, 31 Oct 2017 15:15:10 +0000
Subject: [R] SamplingStrata R package
In-Reply-To: <CAGxFJbS54b6SfkSpkcRd6_UBgFXina=MQsPmDXXAXx6M4sDbbw@mail.gmail.com>
References: <CAHBtsTsXSRTfFqtpZWoVLfdXbiH_Ur2TuXEaZp46DyucH7L1OQ@mail.gmail.com>
 <CAGxFJbS54b6SfkSpkcRd6_UBgFXina=MQsPmDXXAXx6M4sDbbw@mail.gmail.com>
Message-ID: <CAHBtsTtrmm0XGWEgh0hm=uwcRwKrZcfUwcG_qxk6ao3wBt0kWw@mail.gmail.com>

Hi Bert
thank you for the reply. Not a coding query as such. Just wanted some
pointers towards how to handle strata using the package in my situation,
i.e. business type X business size with information for count and financial
turnover.

many thanks

On 31 October 2017 at 14:37, Bert Gunter <bgunter.4567 at gmail.com> wrote:

> 1.  There is no question here.
>
> 2. In any case, this is not a code writing service, so a question about
> how to code models without any offering of your own attempts might not be
> replied to anyway.
>
> 3. For what sorts of queries you might expect replies to, please read and
> follow the posting guide below. Also, if you do post, please post in plaint
> text, not html, as the latter (especially code) can get mangled by the mail
> server.
>
> Cheers,
> Bert
>
>
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
> On Tue, Oct 31, 2017 at 4:43 AM, Chris S <cjsilwood at gmail.com> wrote:
>
>> Hi all
>>
>> I am hoping to use the SamplingStrata R package for a dataset describing a
>> population of businesses wherein I have information on the type of
>> business, as well as, for designated employment number bands, number of
>> employees and business turnover information. So in this context the
>> employment number bands can be described as micro, small, medium and
>> large,
>> i.e. size of business. Hence I would like the stratification to be
>> business
>> type X business size. I note that SamplingStrata allows for multivariate
>> scenarios and the data frame should be straightforward to set up, but in
>> terms of "domains" which descriptor should be used for this? I am assuming
>> I can use both number of employees and business turnover information to
>> optimise the stratification sampling procedure.
>>
>> thank you in advance
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Tue Oct 31 16:20:17 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 31 Oct 2017 08:20:17 -0700
Subject: [R] error to run this package
In-Reply-To: <CADhwAg38gYpWJp3p+JjLK-6rcxGuvJEkiaHr2QLhvxqXf0R2Cw@mail.gmail.com>
References: <CADhwAg38gYpWJp3p+JjLK-6rcxGuvJEkiaHr2QLhvxqXf0R2Cw@mail.gmail.com>
Message-ID: <8AB10E73-160E-4530-B6AB-4231D8FEAB0B@dcn.davis.ca.us>

Please read the Posting Guide mentioned at the bottom of this and every message on this list. Things like the following you should consider:

What did you do between the time you started R and this error occurred? There is a presumption that we can reproduce your actions and perhaps get the same result.

What is the output of sessionInfo() just before this error occurs?

Send your email in plain text, since html gets mangled frequently before we see your message. 
-- 
Sent from my phone. Please excuse my brevity.

On October 31, 2017 5:41:36 AM PDT, Anima Pramanik <apramanik17 at gmail.com> wrote:
>Error: package or namespace load failed for ?car? in get(Info[i, 1],
>envir
>= env):
> cannot allocate memory block of size 2.5 Gb
>
>
>please help me to get a solution of this problem
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Tue Oct 31 16:20:56 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 31 Oct 2017 08:20:56 -0700
Subject: [R] SamplingStrata R package
In-Reply-To: <CAHBtsTtrmm0XGWEgh0hm=uwcRwKrZcfUwcG_qxk6ao3wBt0kWw@mail.gmail.com>
References: <CAHBtsTsXSRTfFqtpZWoVLfdXbiH_Ur2TuXEaZp46DyucH7L1OQ@mail.gmail.com>
 <CAGxFJbS54b6SfkSpkcRd6_UBgFXina=MQsPmDXXAXx6M4sDbbw@mail.gmail.com>
 <CAHBtsTtrmm0XGWEgh0hm=uwcRwKrZcfUwcG_qxk6ao3wBt0kWw@mail.gmail.com>
Message-ID: <CAGxFJbTVP2=sEG+71LWzD6VRi52PTzeaCHfJr9PWAhO9WSSdGw@mail.gmail.com>

If yours is primarily a statistics issue (I can't tell), you might do
better posting on a statistics list like stats.stackexchange.com.

-- Bert



On Tue, Oct 31, 2017 at 8:15 AM, Chris S <cjsilwood at gmail.com> wrote:

> Hi Bert
> thank you for the reply. Not a coding query as such. Just wanted some
> pointers towards how to handle strata using the package in my situation,
> i.e. business type X business size with information for count and financial
> turnover.
>
> many thanks
>
> On 31 October 2017 at 14:37, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
>> 1.  There is no question here.
>>
>> 2. In any case, this is not a code writing service, so a question about
>> how to code models without any offering of your own attempts might not be
>> replied to anyway.
>>
>> 3. For what sorts of queries you might expect replies to, please read and
>> follow the posting guide below. Also, if you do post, please post in plaint
>> text, not html, as the latter (especially code) can get mangled by the mail
>> server.
>>
>> Cheers,
>> Bert
>>
>>
>>
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>> On Tue, Oct 31, 2017 at 4:43 AM, Chris S <cjsilwood at gmail.com> wrote:
>>
>>> Hi all
>>>
>>> I am hoping to use the SamplingStrata R package for a dataset describing
>>> a
>>> population of businesses wherein I have information on the type of
>>> business, as well as, for designated employment number bands, number of
>>> employees and business turnover information. So in this context the
>>> employment number bands can be described as micro, small, medium and
>>> large,
>>> i.e. size of business. Hence I would like the stratification to be
>>> business
>>> type X business size. I note that SamplingStrata allows for multivariate
>>> scenarios and the data frame should be straightforward to set up, but in
>>> terms of "domains" which descriptor should be used for this? I am
>>> assuming
>>> I can use both number of employees and business turnover information to
>>> optimise the stratification sampling procedure.
>>>
>>> thank you in advance
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>> ng-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From olivier.crouzet at univ-nantes.fr  Tue Oct 31 16:55:23 2017
From: olivier.crouzet at univ-nantes.fr (Olivier Crouzet)
Date: Tue, 31 Oct 2017 16:55:23 +0100
Subject: [R] Scatterplot3d :: Rotating x tick labels by x degrees
In-Reply-To: <BLUPR0301MB2052BB1D513419274FB69EB6FA590@BLUPR0301MB2052.namprd03.prod.outlook.com>
References: <BLUPR0301MB2052BB1D513419274FB69EB6FA590@BLUPR0301MB2052.namprd03.prod.outlook.com>
Message-ID: <20171031165523.d36d8499bcadc8c33b654470@univ-nantes.fr>

Hi Alex,

this should be related to the "las" argument of "par()" but
actually it does not seem to be parametered in scatterplot3d.
Searching the net for "scatterplot3d las" provides a link to:

https://stackoverflow.com/questions/25458652/specifying-the-orientation-of-the-axes-labels-in-scatterplot3d

You may try the solution that is provided in this link or consider using
alternate packages (like rgl or the plotly packages which one may be
more powerfull as far as I can judge). However I can't help more. It
seems ggplot does not produce 3d plots (but it looks like it can
interact with plotly when using 3d plots).

Olivier.

On Mon, 30 Oct 2017
23:56:02 +0000 Alex Restrepo <alex.restrepo at outlook.com> wrote:

> Hi,
> 
> I would like to rotate the x axis tick labels by 45 degrees.   Using
> the code below, could someone please provide an example?   Many
> Thanks In Advance, Alex
> 
> library("scatterplot3d")
> mydf=data.frame(rate=seq(158, 314)
>                 ,age=seq(1, 157)
>                 ,market_date=seq(as.Date("2000/1/1"), as.Date
> ("2003/1/1"), by="7 days"))
> 
> mydf$market_date=as.Date(mydf$market_date, format="%Y-%m-%d")
> 
> scatterplot3d(mydf$market_date
>               ,mydf$rate
>               ,mydf$age
>               ,x.ticklabs = seq(as.Date("2000/1/1"), as.Date
> ("2003/1/1"), by="330 days"))
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


-- 
  Olivier Crouzet, PhD
  /Assistant Professor/
  @LLING - Laboratoire de Linguistique de Nantes
    UMR6310 CNRS / Universit? de Nantes
  /Guest Researcher/
  @UMCG (University Medical Center Groningen)
    ENT department
    Rijksuniversiteit Groningen


From jdnewmil at dcn.davis.ca.us  Tue Oct 31 17:22:15 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 31 Oct 2017 09:22:15 -0700
Subject: [R] error to run this package
In-Reply-To: <CADhwAg1uzuBWTJoz2_HbXT8ezN5qy_G05fn-JyGe-OVOPme6Pw@mail.gmail.com>
References: <CADhwAg38gYpWJp3p+JjLK-6rcxGuvJEkiaHr2QLhvxqXf0R2Cw@mail.gmail.com>
 <8AB10E73-160E-4530-B6AB-4231D8FEAB0B@dcn.davis.ca.us>
 <CADhwAg1uzuBWTJoz2_HbXT8ezN5qy_G05fn-JyGe-OVOPme6Pw@mail.gmail.com>
Message-ID: <1E91E540-4290-47BF-8080-3D0A1B096460@dcn.davis.ca.us>

Cc'ing back to the list... ALWAYS reply-to-all to keep the list included... I don't know all the answers and I don't reply to every request since this is volunteer time on my part.

That error does not occur when I load that package. Please re-read my previous recommendations and follow all of them... maybe someone else will notice something.
-- 
Sent from my phone. Please excuse my brevity.

On October 31, 2017 9:01:51 AM PDT, Anima Pramanik <apramanik17 at gmail.com> wrote:
>library("car", lib.loc="~/R/win-library/3.4")
>Error: package or namespace load failed for ?car? in get(Info[i, 1],
>envir
>= env):
> cannot allocate memory block of size 2.5 Gb
>
>On Tue, Oct 31, 2017 at 8:50 PM, Jeff Newmiller
><jdnewmil at dcn.davis.ca.us>
>wrote:
>
>> Please read the Posting Guide mentioned at the bottom of this and
>every
>> message on this list. Things like the following you should consider:
>>
>> What did you do between the time you started R and this error
>occurred?
>> There is a presumption that we can reproduce your actions and perhaps
>get
>> the same result.
>>
>> What is the output of sessionInfo() just before this error occurs?
>>
>> Send your email in plain text, since html gets mangled frequently
>before
>> we see your message.
>> --
>> Sent from my phone. Please excuse my brevity.
>>
>> On October 31, 2017 5:41:36 AM PDT, Anima Pramanik
><apramanik17 at gmail.com>
>> wrote:
>> >Error: package or namespace load failed for ?car? in get(Info[i, 1],
>> >envir
>> >= env):
>> > cannot allocate memory block of size 2.5 Gb
>> >
>> >
>> >please help me to get a solution of this problem
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>


From jrkrideau at yahoo.ca  Tue Oct 31 15:46:23 2017
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 31 Oct 2017 14:46:23 +0000 (UTC)
Subject: [R] error to run this package
In-Reply-To: <CADhwAg38gYpWJp3p+JjLK-6rcxGuvJEkiaHr2QLhvxqXf0R2Cw@mail.gmail.com>
References: <CADhwAg38gYpWJp3p+JjLK-6rcxGuvJEkiaHr2QLhvxqXf0R2Cw@mail.gmail.com>
Message-ID: <304925914.7498109.1509461183250@mail.yahoo.com>

Since we don't know what you were doing when this happened it is a bit difficult to guess.
Please supply a minimal set of code that demonstrates what you were doing that gives this error.
The output of sessionInfo() would also be useful.
Have a look at http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
or 
Reproducibility ? Advanced R.for some idea of the type of information that would useful.
The simplist solution might be to make sure that you have enough memory available since the error says, "cannot allocate memory block of size 2.5 Gb".


| 
| 
|  | 
Reproducibility ? Advanced R.


 |

 |

 |



 

    On Tuesday, October 31, 2017, 9:07:04 AM EDT, Anima Pramanik <apramanik17 at gmail.com> wrote:  
 
 Error: package or namespace load failed for ?car? in get(Info[i, 1], envir
= env):
 cannot allocate memory block of size 2.5 Gb


please help me to get a solution of this problem

??? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.  
	[[alternative HTML version deleted]]


From wjm1 at caa.columbia.edu  Tue Oct 31 18:10:20 2017
From: wjm1 at caa.columbia.edu (William Michels)
Date: Tue, 31 Oct 2017 10:10:20 -0700
Subject: [R] Pass Parameters to RScript?
In-Reply-To: <fFe6VQZOCdYI72z6FkGYHAvcwEciZZ_F2KCSRYnIFreYBcze6fouwmUOMq1JpdeK-8I6Blu7suoQyT3-ECImK9FqlSBZr7N1audAXQB8GdU=@protonmail.com>
References: <7b7d2aK3Dn6pRdQEbxfJqT2DQioZWGR1HNr5HVR0S-Wb96HKq5lqQ_zVJExppIYOdF7ZFk61v3tSiJvF-Huxvsp6r_1G_8a3-u8Fdg0JjZM=@protonmail.com>
 <CAGgJW77JPinM=fLK2+A17qz2ALnLuMBW9Tvtbz14Ldr-B8e3Zw@mail.gmail.com>
 <fFe6VQZOCdYI72z6FkGYHAvcwEciZZ_F2KCSRYnIFreYBcze6fouwmUOMq1JpdeK-8I6Blu7suoQyT3-ECImK9FqlSBZr7N1audAXQB8GdU=@protonmail.com>
Message-ID: <CAA99HCyyk4mt9KZCO3pXWrfaEc0FzqLy5bRVt7xM5JF1K_g4UQ@mail.gmail.com>

Hello Morcus,

Is your question really about language inter-operability?
If so, have you checked out rJava?

"rJava: Low-Level R to Java Interface"
 https://CRAN.R-project.org/package=rJava
http://www.rforge.net/rJava/

Regards,

Bill.

W. Michels, Ph.D.


On Mon, Oct 30, 2017 at 8:10 AM, Morkus via R-help <r-help at r-project.org> wrote:
> Thanks Eric,
>
> I saw that page, too, but it states:
>
> "This post describes how to pass external arguments to R when calling a Rscript with a command line."
>
> Not what I'm trying to do.
>
> Thanks for your reply.
>
> Sent from [ProtonMail](https://protonmail.com), Swiss-based encrypted email.
>
>> -------- Original Message --------
>> Subject: Re: [R] Pass Parameters to RScript?
>> Local Time: October 30, 2017 9:39 AM
>> UTC Time: October 30, 2017 1:39 PM
>> From: ericjberger at gmail.com
>> To: Morkus <morkus at protonmail.com>
>> r-help at r-project.org <r-help at r-project.org>
>>
>> I did a simple search and got  hits immediately, e.g.
>> https://www.r-bloggers.com/passing-arguments-to-an-r-script-from-command-lines/
>>
>> On Mon, Oct 30, 2017 at 2:30 PM, Morkus via R-help <r-help at r-project.org> wrote:
>>
>>> Is it possible to pass parameters to an R Script, say, from Java or other language?
>>>
>>> I did some searches, but came up blank.
>>>
>>> Thanks very much in advance,
>>>
>>> Sent from [ProtonMail](https://protonmail.com), Swiss-based encrypted email.
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jfox at mcmaster.ca  Tue Oct 31 19:08:03 2017
From: jfox at mcmaster.ca (Fox, John)
Date: Tue, 31 Oct 2017 18:08:03 +0000
Subject: [R] error to run this package
In-Reply-To: <18732_1509467634_v9VGXsRZ025933_304925914.7498109.1509461183250@mail.yahoo.com>
References: <CADhwAg38gYpWJp3p+JjLK-6rcxGuvJEkiaHr2QLhvxqXf0R2Cw@mail.gmail.com>
 <18732_1509467634_v9VGXsRZ025933_304925914.7498109.1509461183250@mail.yahoo.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC8366F46DB@FHSDB4H16-2.csu.mcmaster.ca>

Dear John and Anima,

I didn't reply earlier because other people got to it before I did and because, given the lack of information in the original post, there wasn't anything to add.

The car package shouldn't require anything near 2.5 Gb to load. Here's what I get under Windows 10 with R 3.4.2:

> memory.size()
[1] 62.08
> library(car)
> memory.size()
[1] 166.09

The units are MB (the ?memory.size help page says "Mb" but I believe that's usually for "megabits" and that it isn't what's intended).

Some more information, as has been suggested by other posters, might help.

Best,
 John

-----------------------------
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
Web: socserv.mcmaster.ca/jfox



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of John Kane
> via R-help
> Sent: Tuesday, October 31, 2017 10:46 AM
> To: r-help at r-project.org; Anima Pramanik <apramanik17 at gmail.com>
> Subject: Re: [R] error to run this package
> 
> Since we don't know what you were doing when this happened it is a bit
> difficult to guess.
> Please supply a minimal set of code that demonstrates what you were doing
> that gives this error.
> The output of sessionInfo() would also be useful.
> Have a look at http://stackoverflow.com/questions/5963269/how-to-make-a-
> great-r-reproducible-example
> or
> Reproducibility ? Advanced R.for some idea of the type of information that
> would useful.
> The simplist solution might be to make sure that you have enough memory
> available since the error says, "cannot allocate memory block of size 2.5 Gb".
> 
> 
> |
> |
> |  |
> Reproducibility ? Advanced R.
> 
> 
>  |
> 
>  |
> 
>  |
> 
> 
> 
> 
> 
>     On Tuesday, October 31, 2017, 9:07:04 AM EDT, Anima Pramanik
> <apramanik17 at gmail.com> wrote:
> 
>  Error: package or namespace load failed for ?car? in get(Info[i, 1], envir
> = env):
>  cannot allocate memory block of size 2.5 Gb
> 
> 
> please help me to get a solution of this problem
> 
> ??? [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From georedraider at yahoo.com  Tue Oct 31 19:04:53 2017
From: georedraider at yahoo.com (Scott Anderwald)
Date: Tue, 31 Oct 2017 13:04:53 -0500
Subject: [R] convertTime package.
Message-ID: <6FAD6550-B165-40D5-9A3C-9FC29CB52A48@yahoo.com>

To whom it might concern.  I am working on a project that needs the convertTime function. I am currently using version 3.4.1 and it says not available for the version.  Two questions is there a work around for the function or is there another package that contains that functions.


Thanks, 


Scott Anderwald

From msuzen at gmail.com  Tue Oct 31 19:27:30 2017
From: msuzen at gmail.com (Suzen, Mehmet)
Date: Tue, 31 Oct 2017 19:27:30 +0100
Subject: [R] run r script in r-fiddle
In-Reply-To: <CAPtbhHz=TXEh6dz+ERtNxp5b0e7J-39x-qEUEXd5hp5v-ZxAsw@mail.gmail.com>
References: <CAF0J_MviTcFMLEyygkPfDhsSyxgA-qoQrfTB932yuEq1e1t7+Q@mail.gmail.com>
 <CAPtbhHyDh9jOgEmai8XENCX=0RA8YVQUfjwZkEyPCv-3skcCzQ@mail.gmail.com>
 <23031.15488.515464.855674@stat.math.ethz.ch>
 <CAPtbhHxOFy2-OBeuZnNt9WOV1A8qyCAoerQYGywA_c0f2OY7JA@mail.gmail.com>
 <23032.25011.679300.970659@stat.math.ethz.ch>
 <CAPtbhHz=TXEh6dz+ERtNxp5b0e7J-39x-qEUEXd5hp5v-ZxAsw@mail.gmail.com>
Message-ID: <CAPtbhHzLZ54YZm-me_xjQP+hF-KB-fZ5uPE_r1hYWPzK4fskag@mail.gmail.com>

Dear List,

According to datacamp support team, r-fiddle.org is not supported. We
asked them to put it down as Professor Maechler suggested it is
a waste of time for the R-help to respond to questions on something
not maintained and severely outdated. If you would like to use
R from your browser, you can embed the following into a web page:

<script src="https://cdn.datacamp.com/datacamp-light-latest.min.js"></script>
<div data-datacamp-exercise data-lang="r"></div>

Currently, it supports R 3.4.0. See the code base, which is open
source, here https://github.com/datacamp/datacamp-light

Hope it helps.

Best,
Mehmet



On 31 October 2017 at 15:09, Suzen, Mehmet <msuzen at gmail.com> wrote:
> On 31 October 2017 at 12:42, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
>> Notably as I think it's been provided by a company that no
>> longer exists under that name, and even if that'd be wrong,  R-Fiddle
>> does not seem free software (apart from the R parts, I hope !).
>
> For the record, r-fiddle is maintained by datacamp:
> https://www.datacamp.com/community/blog/r-fiddle-an-online-playground-for-r-code-2


From sarah.goslee at gmail.com  Tue Oct 31 20:15:57 2017
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 31 Oct 2017 15:15:57 -0400
Subject: [R] convertTime package.
In-Reply-To: <6FAD6550-B165-40D5-9A3C-9FC29CB52A48@yahoo.com>
References: <6FAD6550-B165-40D5-9A3C-9FC29CB52A48@yahoo.com>
Message-ID: <CAM_vjumw5JDC7WysbgBfeMHnHcnMuwv0BL2nd5DuvEWetB+63g@mail.gmail.com>

Hi Scott,

Where did you get this function originally? I can't find anything about it.

What OS are you using?

What says, "not available for the version"? Where are you getting that error?

What are you trying to accomplish? What does that function actually
do? It's impossible to suggest a work-around for a function of unknown
purpose and origin.

(The posting guide for this list suggests you include all of that
information when you inquire.)

Sarah


On Tue, Oct 31, 2017 at 2:04 PM, Scott Anderwald via R-help
<r-help at r-project.org> wrote:
> To whom it might concern.  I am working on a project that needs the convertTime function. I am currently using version 3.4.1 and it says not available for the version.  Two questions is there a work around for the function or is there another package that contains that functions.
>
>
> Thanks,
>
>
> Scott Anderwald



-- 
Sarah Goslee
http://www.functionaldiversity.org


From ericjberger at gmail.com  Tue Oct 31 20:24:40 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Tue, 31 Oct 2017 21:24:40 +0200
Subject: [R] convertTime package.
In-Reply-To: <CAM_vjumw5JDC7WysbgBfeMHnHcnMuwv0BL2nd5DuvEWetB+63g@mail.gmail.com>
References: <6FAD6550-B165-40D5-9A3C-9FC29CB52A48@yahoo.com>
 <CAM_vjumw5JDC7WysbgBfeMHnHcnMuwv0BL2nd5DuvEWetB+63g@mail.gmail.com>
Message-ID: <CAGgJW77n+iNHQwine-tNR=63C5tYqcLkw87t3k-qjvi3pDbfSQ@mail.gmail.com>

If you need a function (e.g. convertTime ) from a package (unknown?) then
you cannot simply instruct R to install the function.
e.g. if you give the command
> install.packages("convertTime")
you will get an error message like
"package 'convertTime' is not available (for R version 3.4.1)"

I did a Google search and found a package called SGP that has a function
"convertTime". I have no idea if it is the function you are looking for but
installing that package worked fine in R version 3.4.1.  So you can try

> install.packages("SGP")

HTH,
Eric


On Tue, Oct 31, 2017 at 9:15 PM, Sarah Goslee <sarah.goslee at gmail.com>
wrote:

> Hi Scott,
>
> Where did you get this function originally? I can't find anything about it.
>
> What OS are you using?
>
> What says, "not available for the version"? Where are you getting that
> error?
>
> What are you trying to accomplish? What does that function actually
> do? It's impossible to suggest a work-around for a function of unknown
> purpose and origin.
>
> (The posting guide for this list suggests you include all of that
> information when you inquire.)
>
> Sarah
>
>
> On Tue, Oct 31, 2017 at 2:04 PM, Scott Anderwald via R-help
> <r-help at r-project.org> wrote:
> > To whom it might concern.  I am working on a project that needs the
> convertTime function. I am currently using version 3.4.1 and it says not
> available for the version.  Two questions is there a work around for the
> function or is there another package that contains that functions.
> >
> >
> > Thanks,
> >
> >
> > Scott Anderwald
>
>
>
> --
> Sarah Goslee
> http://www.functionaldiversity.org
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ligges at statistik.tu-dortmund.de  Tue Oct 31 21:30:15 2017
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Tue, 31 Oct 2017 21:30:15 +0100
Subject: [R] Scatterplot3d :: Rotating x tick labels by x degrees
In-Reply-To: <BLUPR0301MB2052BB1D513419274FB69EB6FA590@BLUPR0301MB2052.namprd03.prod.outlook.com>
References: <BLUPR0301MB2052BB1D513419274FB69EB6FA590@BLUPR0301MB2052.namprd03.prod.outlook.com>
Message-ID: <727ca822-a190-f7a3-c006-f8538513969a@statistik.tu-dortmund.de>



On 31.10.2017 00:56, Alex Restrepo wrote:
> Hi,
> 
> I would like to rotate the x axis tick labels by 45 degrees.   Using the code below, could someone please provide an example?   Many Thanks In Advance, Alex

45 degree rotation is not supported in base R graphics and scatterplot3d 
uses that.

You can use par(las=2)
before your code and get them rrotated by 90 degrees, or you can add 
them individually by

s3d <- scatterplot3d(....

Afterwards you could try along trhese lines:

par(xpd=TRUE)
text(s3d$xyz.convert(desired positions of labels in 3D coordinate 
system), labels, angle=45)

(untested)

Best,
Uwe Ligges





> 
> library("scatterplot3d")
> mydf=data.frame(rate=seq(158, 314)
>                  ,age=seq(1, 157)
>                  ,market_date=seq(as.Date("2000/1/1"), as.Date("2003/1/1"), by="7 days"))
> 
> mydf$market_date=as.Date(mydf$market_date, format="%Y-%m-%d")
> 
> scatterplot3d(mydf$market_date
>                ,mydf$rate
>                ,mydf$age
>                ,x.ticklabs = seq(as.Date("2000/1/1"), as.Date("2003/1/1"), by="330 days"))
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From spankyfingers at yahoo.com  Tue Oct 31 20:00:41 2017
From: spankyfingers at yahoo.com (Edward Guda)
Date: Tue, 31 Oct 2017 22:00:41 +0300
Subject: [R] Help with Nesting
Message-ID: <E27D6052-96F4-4300-9991-63E47C4F47DF@yahoo.com>

How do i resolve this?

symbol <-  c('RRR' ,'GGG')

for(i in seq_along(symbol)) {
  dat <- Quandl("LLL/symbol[i]")
  
}
 
required solutionis a loop where Quandl is a function and it loops as flows,
Quandl("LLL/RRR")
Quandl("LLL/GGG")


From dwinsemius at comcast.net  Tue Oct 31 22:13:34 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 31 Oct 2017 14:13:34 -0700
Subject: [R] Scatterplot3d :: Rotating x tick labels by x degrees
In-Reply-To: <20171031165523.d36d8499bcadc8c33b654470@univ-nantes.fr>
References: <BLUPR0301MB2052BB1D513419274FB69EB6FA590@BLUPR0301MB2052.namprd03.prod.outlook.com>
 <20171031165523.d36d8499bcadc8c33b654470@univ-nantes.fr>
Message-ID: <06AA0B5E-E51A-4D21-A2F3-6C58E54F1A30@comcast.net>


> On Oct 31, 2017, at 8:55 AM, Olivier Crouzet <olivier.crouzet at univ-nantes.fr> wrote:
> 
> Hi Alex,
> 
> this should be related to the "las" argument of "par()" but
> actually it does not seem to be parametered in scatterplot3d.
> Searching the net for "scatterplot3d las" provides a link to:
> 
> https://stackoverflow.com/questions/25458652/specifying-the-orientation-of-the-axes-labels-in-scatterplot3d
> 
> You may try the solution that is provided in this link or consider using
> alternate packages (like rgl or the plotly packages which one may be
> more powerfull as far as I can judge). However I can't help more. It
> seems ggplot does not produce 3d plots (but it looks like it can
> interact with plotly when using 3d plots).

Olivier;

The cited SO solutions (mine being the first one)  were addressing the request for rotated "axis"-labels rather than rotated "axtick"-labels, although the general strategy of looking at the code and using the `text`-function with xpd=TRUE (rather than the `mtext`-function where needed in the definitions of mytext and mytext2) should apply. 

Alex;

I'd encourage you to demonstrate more initiative rather than expecting us to be on-call code servants.  I've decided to limit my gratis coding time to 15 minutes daily. I think this might take me an hour or more. I'm still available for on-list code review on this effort. 

As a start, I'd suggest downloading the scatterplot3d package code and then open up scatterplot3d.R. Find the comment

## label tick marks

... and perhaps decide whether you need to use `text` rather than `mtext`. (`text` can rotate by any amount, 
but may need "xpd" to be set TRUE. `mtext` is limited to 90 degree increments.) I've used up my time, so the next move is yours.
-- 
David.


> 
> Olivier.
> 
> On Mon, 30 Oct 2017
> 23:56:02 +0000 Alex Restrepo <alex.restrepo at outlook.com> wrote:
> 
>> Hi,
>> 
>> I would like to rotate the x axis tick labels by 45 degrees.   Using
>> the code below, could someone please provide an example?   Many
>> Thanks In Advance, Alex
>> 
>> library("scatterplot3d")
>> mydf=data.frame(rate=seq(158, 314)
>>                ,age=seq(1, 157)
>>                ,market_date=seq(as.Date("2000/1/1"), as.Date
>> ("2003/1/1"), by="7 days"))
>> 
>> mydf$market_date=as.Date(mydf$market_date, format="%Y-%m-%d")
>> 
>> scatterplot3d(mydf$market_date
>>              ,mydf$rate
>>              ,mydf$age
>>              ,x.ticklabs = seq(as.Date("2000/1/1"), as.Date
>> ("2003/1/1"), by="330 days"))
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html and provide commented,
>> minimal, self-contained, reproducible code.
> 
> 
> -- 
>  Olivier Crouzet, PhD
>  /Assistant Professor/
>  @LLING - Laboratoire de Linguistique de Nantes
>    UMR6310 CNRS / Universit? de Nantes
>  /Guest Researcher/
>  @UMCG (University Medical Center Groningen)
>    ENT department
>    Rijksuniversiteit Groningen
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From jdnewmil at dcn.davis.ca.us  Tue Oct 31 23:01:02 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 31 Oct 2017 15:01:02 -0700
Subject: [R] Help with Nesting
In-Reply-To: <E27D6052-96F4-4300-9991-63E47C4F47DF@yahoo.com>
References: <E27D6052-96F4-4300-9991-63E47C4F47DF@yahoo.com>
Message-ID: <5DDFB0F5-EC5B-4D79-B551-5EAE60624263@dcn.davis.ca.us>

R does not look inside strings for language objects like your symbol variable. Nor does it magically figure out that it needs to keep previous values in a loop. And you need to be able to use valid syntax to ask clear questions in this list, so some more time with a tutorial should occur before you post again.

Perhaps:

result <- lapply( symbol, function(s) {
   Quandl( paste0( "LLL/", s ) )
}
result
-- 
Sent from my phone. Please excuse my brevity.

On October 31, 2017 12:00:41 PM PDT, Edward Guda via R-help <r-help at r-project.org> wrote:
>How do i resolve this?
>
>symbol <-  c('RRR' ,'GGG')
>
>for(i in seq_along(symbol)) {
>  dat <- Quandl("LLL/symbol[i]")
>  
>}
> 
>required solutionis a loop where Quandl is a function and it loops as
>flows,
>Quandl("LLL/RRR")
>Quandl("LLL/GGG")
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From ruipbarradas at sapo.pt  Tue Oct 31 21:46:44 2017
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Tue, 31 Oct 2017 20:46:44 +0000
Subject: [R] Help with Nesting
In-Reply-To: <E27D6052-96F4-4300-9991-63E47C4F47DF@yahoo.com>
References: <E27D6052-96F4-4300-9991-63E47C4F47DF@yahoo.com>
Message-ID: <59F8E134.5050105@sapo.pt>

Hello,

This is cross-posted from

https://stackoverflow.com/questions/47042591/how-to-resolve-nested-variables-inside-a-loop-in-r

And you already have the answer there. See my comment.

Rui Barradas

Em 31-10-2017 19:00, Edward Guda via R-help escreveu:
> How do i resolve this?
>
> symbol <-  c('RRR' ,'GGG')
>
> for(i in seq_along(symbol)) {
>    dat <- Quandl("LLL/symbol[i]")
>
> }
>
> required solutionis a loop where Quandl is a function and it loops as flows,
> Quandl("LLL/RRR")
> Quandl("LLL/GGG")
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


