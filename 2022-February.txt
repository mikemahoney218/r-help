From yugoh|@v @end|ng |rom gm@||@com  Tue Feb  1 04:50:57 2022
From: yugoh|@v @end|ng |rom gm@||@com (Love Umesi)
Date: Tue, 1 Feb 2022 16:50:57 +1300
Subject: [R] Survey design for multilevel analysis
Message-ID: <CAOTqx+KtV1_WOtffzuZJetE=JFKqVDqV8O15aUs0vBFZa1yyeQ@mail.gmail.com>

I am new in using R, and I need to run a multilevel analysis (two-phase
design on two levels) using Cox frailty survival model on a survey data
(Nigeria Demographic and Health Survey Data).

My problem is how to write the design weight using the two weights needed
and apply them to the analysis.

I have identified the needed variables for the survey design, which
are : psu/cluster=
v021 individual-level weight= wt1_1 cluster-level weigh= wt2_1 strata/stratum=
v022

Please can someone help me with the survey design (svydesign) code and how
to include it and the weights in a model.

I understand I have to use svycoxph in the model. Please how do I include
survey design and account for the 2 weights (individual-level and
cluster-level weights) in the gamma frailty model below?

Frailty1 <- coxph (Surv(study_time, died) ~ factor(v024) + factor(mat_edu)
+ v025 + frailty(v021,distribution="gamma"), data=rcom2018)

Really looking forward to your help as my project is hanging on this.

Many thanks.


library(survival)#> Warning: package 'survival' was built under R version 4.0.5
library(frailtypack)#> Warning: package 'frailtypack' was built under
R version 4.0.5#> Loading required package: boot#> #> Attaching
package: 'boot'#> The following object is masked from
'package:survival':#> #>     aml#> Loading required package: MASS#>
Loading required package: survC1#> Warning: package 'survC1' was built
under R version 4.0.5#> Loading required package: doBy#> Warning:
package 'doBy' was built under R version 4.0.5#> #> Attaching package:
'frailtypack'#> The following object is masked from
'package:survival':#> #>     cluster


rcom1 <- data.frame(
  data.frame(
    pid = c(
      1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,
      14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,
      30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45,
      46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,
      61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76,
      77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92,
      93, 94, 95, 96, 97, 98, 99, 100
    ),
    study_time = c(
      13, 9, 17, 31, 39, 22, 24, 0, 23, 12, 9, 35,
      18, 20, 60, 18, 5, 46, 26, 54, 37, 51, 31, 55, 27, 15, 39, 6,
      29, 0, 9, 40, 23, 12, 35, 56, 14, 40, 57, 42, 5, 42, 39, 39,
      54, 19, 52, 42, 7, 28, 53, 5, 28, 13, 37, 0, 23, 33, 27, 36, 20,
      24, 58, 34, 12, 44, 3, 34, 14, 5, 10, 40, 12, 36, 19, 58, 17,
      40, 39, 58, 53, 53, 1, 50, 2, 28, 24, 13, 13, 50, 46, 46, 19, 6,
      32, 59, 9, 30, 30, 43
    ),
    died = c(
      0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
      0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
      0
    ),
    v021 = c(
      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
      2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3,
      3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,
      4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
      5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,
      6
    ),
    v022 = c(
      "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "2", "2", "2", "2", "2", "2", "2",
      "2", "2", "2"
    ),
    v012 = c(
      40, 37, 27, 27, 24, 32, 35, 35, 34, 20, 28,
      28, 26, 24, 24, 25, 26, 26, 26, 26, 28, 27, 25, 25, 27, 26, 26,
      21, 21, 31, 36, 36, 27, 23, 32, 32, 33, 33, 33, 28, 25, 37,
      33, 34, 33, 28, 28, 29, 33, 33, 33, 39, 38, 38, 38, 38, 24, 27,
      35, 40, 22, 38, 38, 21, 30, 30, 30, 39, 43, 18, 23, 23, 25, 25,
      30, 45, 26, 26, 35, 35, 35, 35, 32, 32, 40, 25, 27, 30, 30, 30,
      28, 28, 18, 27, 30, 30, 27, 21, 21, 30
    ),
    wt2_1 = c(
      401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031,
      401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
      631.818176269531, 631.818176269531, 631.818176269531,
      631.818176269531, 631.818176269531, 631.818176269531, 631.818176269531,
      631.818176269531, 631.818176269531, 631.818176269531
    ),
    wt1_1 = c(
      2.5074667930603, 2.5074667930603,
      2.5074667930603, 2.5074667930603, 2.5074667930603, 2.5074667930603,
      2.5074667930603, 2.5074667930603, 2.5074667930603, 2.5074667930603,
      2.5074667930603, 2.5074667930603, 2.5074667930603,
      2.5074667930603, 2.5074667930603, 5.1194109916687, 5.1194109916687,
      5.1194109916687, 5.1194109916687, 5.1194109916687, 5.1194109916687,
      5.1194109916687, 5.1194109916687, 5.1194109916687,
      5.1194109916687, 5.1194109916687, 5.1194109916687, 5.1194109916687,
      5.1194109916687, 5.1194109916687, 5.1194109916687, 5.1194109916687,
      5.1194109916687, 5.1194109916687, 2.40910983085632,
      2.40910983085632, 2.40910983085632, 2.40910983085632, 2.40910983085632,
      2.40910983085632, 2.40910983085632, 2.40910983085632,
      2.40910983085632, 2.40910983085632, 2.40910983085632, 2.40910983085632,
      2.40910983085632, 2.40910983085632, 1.06203985214233,
      1.06203985214233, 1.06203985214233, 1.06203985214233, 1.06203985214233,
      1.06203985214233, 1.06203985214233, 1.06203985214233,
      1.06203985214233, 1.06203985214233, 1.06203985214233, 1.06203985214233,
      1.06203985214233, 1.06203985214233, 1.06203985214233,
      1.06203985214233, 1.06203985214233, 1.06203985214233, 1.06203985214233,
      2.80098295211792, 2.80098295211792, 2.80098295211792,
      2.80098295211792, 2.80098295211792, 2.80098295211792, 2.80098295211792,
      2.80098295211792, 2.80098295211792, 2.80098295211792,
      2.80098295211792, 2.80098295211792, 2.80098295211792, 2.80098295211792,
      2.80098295211792, 2.80098295211792, 2.80098295211792,
      2.80098295211792, 2.80098295211792, 2.80098295211792, 2.80098295211792,
      2.80098295211792, 2.80098295211792, 1.24210178852081,
      1.24210178852081, 1.24210178852081, 1.24210178852081, 1.24210178852081,
      1.24210178852081, 1.24210178852081, 1.24210178852081,
      1.24210178852081, 1.24210178852081
    ),
    v024 = c(
      "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1"
    ),
    v025 = c(
      "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "1", "1", "1", "1", "2", "2", "2", "2", "2", "2", "2",
      "2", "2", "2"
    ),
    mat_edu = c(
      "5", "5", "5", "4", "4", "5", "4", "4", "4",
      "4", "4", "4", "5", "5", "5", "5", "5", "5", "4", "4", "5",
      "4", "4", "4", "5", "3", "3", "4", "4", "5", "5", "5", "5", "4",
      "2", "2", "0", "0", "0", "5", "5", "0", "1", "5", "5", "3",
      "3", "5", "5", "5", "5", "5", "5", "5", "5", "5", "5", "4", "5",
      "5", "4", "5", "5", "3", "4", "4", "5", "3", "1", "3", "3", "3",
      "1", "3", "2", "1", "3", "3", "4", "4", "0", "0", "2", "2",
      "1", "0", "4", "4", "4", "4", "0", "0", "3", "4", "2", "2", "3",
      "3", "3", "0"
    )
  ))

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Tue Feb  1 16:34:24 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 1 Feb 2022 07:34:24 -0800
Subject: [R] Repeatability Analysis of Ordinal Data
In-Reply-To: <DM6PR01MB4314D3686B97D43F15DF3096EA259@DM6PR01MB4314.prod.exchangelabs.com>
References: <DM6PR01MB4314D3686B97D43F15DF3096EA259@DM6PR01MB4314.prod.exchangelabs.com>
Message-ID: <CAGxFJbTqwHWJnBtX92T+bBH58C2tA4cT613nnv6g=8mO38VYJA@mail.gmail.com>

Package-specific questions for non-standard packages (see
https://cran.r-project.org/doc/FAQ/R-FAQ.html#Add-on-packages-in-R for
the list of current standard packages) are generally off topic here
per the posting guide (linked below). It is suggested there that you
contact the package maintainer (?maintainer) for such questions.

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Feb 1, 2022 at 2:02 AM Sidoti, Salvatore <sidoti.23 at osu.edu> wrote:
>
> Greetings, Colleagues:
>
> I have several Likert-type ordinal data sets consisting of animal responses with repeated measures. I was able to implement a CLMM model easily enough with the package `ordinal`. However, the package does not support repeatability analyses.
>
> Assuming that I subset my data according to treatment and/or sex, I am keen to try the `ordinalRR` package. According to the package documentation (https://cran.r-project.org/web/packages/ordinalRR/ordinalRR.pdf), performing `summary()` on the output from the function `ordinalRR()` returns the point estimates for each rater and for each pairwise combination of raters. However, is it possible to return an overall repeatability value and a 95% credible interval across all raters?
>
> What follows is a stock procedure from the package reference document:
>
> #-------------------------------------------------------------------------------
> library(ordinalRR)
>
> # load the dataset that comes with the package
> data(followup)
>
> # preprocess data to accommodate the package functions
> followup.pre <- preprocess(followup)
>
> # perform the analysis
> followup.random <- ordinalRR(followup.pre)
>
> summary(followup.random)
>
> Call:
>   ordinalRR(followup.pre)
>
> Data: 30 parts, 3 operators, 2 repetitions with 4 ordinal categories.
> Random-effects model MCMC chain: 1000 burn-in and 10000 retained.
>
> Simple repeatability and model parameter estimates by rater:
>   Rater j Repeatability  a_j d_{j,1} d_{j,2} d_{j,3}
> 1         0.900 12.0    -1.5    -0.1     0.6
> 2         0.900 10.9    -1.6    -0.3     0.5
> 3         0.933 12.7    -1.5    -0.2     0.5
>
> Simple repeatability and reproducibility (R&R) point estimates for pairs of raters:
>   Rater j Rater j' (R&R)_{j,j'}
> 1        2        0.808
> 1        3        0.900
> 2        3        0.850
> #-------------------------------------------------------------------------------
>
> Kind Regards,
> Salvatore Sidoti
> PhD Candidate
> The Ohio State University
> Columbus, Ohio USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From nboeger @end|ng |rom gm@||@com  Tue Feb  1 14:45:40 2022
From: nboeger @end|ng |rom gm@||@com (Nathan Boeger)
Date: Tue, 1 Feb 2022 20:45:40 +0700
Subject: [R] Funky calculations
Message-ID: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>

Hello,

I found something strange and maybe I am going nuts but this does not make
sense:

>  (0.4 + 0.2 + 0 + 0.3 + 0 + 0.1) > 1
[1] TRUE

I tried it on my mac M1 (R v4.1.2) and my Linux box (R v4.0.4). If I use
other values, it does not work (see below). It seems only that combination,
granted I did not try them all.

>  (0.4 + 0.2 + 0 + 0.2 + 0 + 0.2) > 1
[1] FALSE

Am I missing something?

Cheers

-nb

	[[alternative HTML version deleted]]


From kev|n@thorpe @end|ng |rom utoronto@c@  Tue Feb  1 17:34:13 2022
From: kev|n@thorpe @end|ng |rom utoronto@c@ (Kevin Thorpe)
Date: Tue, 1 Feb 2022 16:34:13 +0000
Subject: [R] Funky calculations
In-Reply-To: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
Message-ID: <B55BEB40-523F-4362-905A-98AB3BF1ED48@utoronto.ca>

This looks like a version of FAQ 7.31.

> On Feb 1, 2022, at 8:45 AM, Nathan Boeger <nboeger at gmail.com> wrote:
> 
> [You don't often get email from nboeger at gmail.com. Learn why this is important at http://aka.ms/LearnAboutSenderIdentification.]
> 
> Hello,
> 
> I found something strange and maybe I am going nuts but this does not make
> sense:
> 
>> (0.4 + 0.2 + 0 + 0.3 + 0 + 0.1) > 1
> [1] TRUE
> 
> I tried it on my mac M1 (R v4.1.2) and my Linux box (R v4.0.4). If I use
> other values, it does not work (see below). It seems only that combination,
> granted I did not try them all.
> 
>> (0.4 + 0.2 + 0 + 0.2 + 0 + 0.2) > 1
> [1] FALSE
> 
> Am I missing something?
> 
> Cheers
> 
> -nb
> 
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
Kevin E. Thorpe
Head of Biostatistics,  Applied Health Research Centre (AHRC)
Li Ka Shing Knowledge Institute of St. Michael?s Hospital
Assistant Professor, Dalla Lana School of Public Health
University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.3016


From gm@rk@|ow|er @end|ng |rom out|ook@com  Tue Feb  1 17:34:21 2022
From: gm@rk@|ow|er @end|ng |rom out|ook@com (Mark Fowler)
Date: Tue, 1 Feb 2022 16:34:21 +0000
Subject: [R] Funky calculations
In-Reply-To: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
Message-ID: <MN2PR07MB7213D0DFE6763C37A38C8A9985269@MN2PR07MB7213.namprd07.prod.outlook.com>

This a joke? The values differ between the TRUE and FALSE syntaxes. The FALSE computes to 1.0, which is not greater than 0.

Sent from Mail<https://go.microsoft.com/fwlink/?LinkId=550986> for Windows

From: Nathan Boeger<mailto:nboeger at gmail.com>
Sent: Tuesday, February 1, 2022 12:28 PM
To: r-help at r-project.org<mailto:r-help at r-project.org>
Subject: [R] Funky calculations

Hello,

I found something strange and maybe I am going nuts but this does not make
sense:

>  (0.4 + 0.2 + 0 + 0.3 + 0 + 0.1) > 1
[1] TRUE

I tried it on my mac M1 (R v4.1.2) and my Linux box (R v4.0.4). If I use
other values, it does not work (see below). It seems only that combination,
granted I did not try them all.

>  (0.4 + 0.2 + 0 + 0.2 + 0 + 0.2) > 1
[1] FALSE

Am I missing something?

Cheers

-nb

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Tue Feb  1 17:35:57 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Tue, 1 Feb 2022 19:35:57 +0300
Subject: [R] Funky calculations
In-Reply-To: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
Message-ID: <20220201193557.308209e8@Tarkus>

On Tue, 1 Feb 2022 20:45:40 +0700
Nathan Boeger <nboeger at gmail.com> wrote:

> I found something strange and maybe I am going nuts but this does not
> make sense:
> 
> >  (0.4 + 0.2 + 0 + 0.3 + 0 + 0.1) > 1  
> [1] TRUE

Unfortunately, this always happens when computers approximate real
numbers with fractions in binary. See R FAQ 7.31 (RShowDoc('FAQ') or
<https://cran.r-project.org/doc/FAQ/R-FAQ.html>).

-- 
Best regards,
Ivan


From gm@rk@|ow|er @end|ng |rom out|ook@com  Tue Feb  1 17:37:03 2022
From: gm@rk@|ow|er @end|ng |rom out|ook@com (Mark Fowler)
Date: Tue, 1 Feb 2022 16:37:03 +0000
Subject: [R] Funky calculations
In-Reply-To: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
Message-ID: <MN2PR07MB7213E5C0FC4BD36A4034C16985269@MN2PR07MB7213.namprd07.prod.outlook.com>

Sorry, last post I meant not > 1

Sent from Mail<https://go.microsoft.com/fwlink/?LinkId=550986> for Windows

From: Nathan Boeger<mailto:nboeger at gmail.com>
Sent: Tuesday, February 1, 2022 12:28 PM
To: r-help at r-project.org<mailto:r-help at r-project.org>
Subject: [R] Funky calculations

Hello,

I found something strange and maybe I am going nuts but this does not make
sense:

>  (0.4 + 0.2 + 0 + 0.3 + 0 + 0.1) > 1
[1] TRUE

I tried it on my mac M1 (R v4.1.2) and my Linux box (R v4.0.4). If I use
other values, it does not work (see below). It seems only that combination,
granted I did not try them all.

>  (0.4 + 0.2 + 0 + 0.2 + 0 + 0.2) > 1
[1] FALSE

Am I missing something?

Cheers

-nb

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From @v|gro@@ @end|ng |rom ver|zon@net  Tue Feb  1 17:52:56 2022
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Tue, 1 Feb 2022 16:52:56 +0000 (UTC)
Subject: [R] Funky calculations
In-Reply-To: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
Message-ID: <542510676.2533919.1643734376267@mail.yahoo.com>

Yes, Nathan, you are missing something.

You are missing a computer language that does infinite precision arithmetic.

0.1 cannot be stored in binary exactly just as numbers like pi and e cannot be stored exactly as they keep needing more digits. The last digit of the allowed number of digits in floating point may have to be chosen as either a 0 or 1 even though the value is sort of in between. 

Think of what happens if you add in decimal

1/3 + 1/3 + 1/3

You could add

.333
.333
.333

And your handwritten calculation would be .999

And no matter how many more 3's you add, you never get beyond .999999999999999999999999999999999

And obviously the right answer is 1.0000000000000000000000000000

The order the numbers are added can matter. You interspersed a combination of floating point and integers so conversions are happening too. 

R has the ability to test for very near equality like this:

> all.equal(1, (0.4 + 0.2 + 0 + 0.3 + 0 + 0.1))
[1] TRUE

There are some built-in and sort of hidden variables stored in .Machine that provide some tolerances for the data storage on that machine. 

> str(.Machine)
List of 28
 $ double.eps               : num 2.22e-16
 $ double.neg.eps           : num 1.11e-16
 $ double.xmin              : num 2.23e-308
 $ double.xmax              : num 1.8e+308
 $ double.base              : int 2
 $ double.digits            : int 53
 $ double.rounding          : int 5
 $ double.guard             : int 0
 $ double.ulp.digits        : int -52
 $ double.neg.ulp.digits    : int -53
 $ double.exponent          : int 11
 $ double.min.exp           : int -1022
 $ double.max.exp           : int 1024
 $ integer.max              : int 2147483647
 $ sizeof.long              : int 4
 $ sizeof.longlong          : int 8
 $ sizeof.longdouble        : int 16
 $ sizeof.pointer           : int 8
 $ longdouble.eps           : num 1.08e-19
 $ longdouble.neg.eps       : num 5.42e-20
 $ longdouble.digits        : int 64
 $ longdouble.rounding      : int 5
 $ longdouble.guard         : int 0
 $ longdouble.ulp.digits    : int -63
 $ longdouble.neg.ulp.digits: int -64
 $ longdouble.exponent      : int 15
 $ longdouble.min.exp       : int -16382
 $ longdouble.max.exp       : int 16384

Functions like all.equal() can use these to determine a way to compare while ignoring the last.

So yes, R shares this glitch with other programming languages and it is not really safe to compare floating point numbers at times. Here is just a suggestion on say rounding your sum to twelve digits to see how it compares to 1.0:

> round((0.4 + 0.2 + 0 + 0.3 + 0 + 0.1),12) > 1.0
[1] FALSE





-----Original Message-----
From: Nathan Boeger <nboeger at gmail.com>
To: r-help at r-project.org
Sent: Tue, Feb 1, 2022 8:45 am
Subject: [R] Funky calculations

Hello,

I found something strange and maybe I am going nuts but this does not make
sense:

>? (0.4 + 0.2 + 0 + 0.3 + 0 + 0.1) > 1
[1] TRUE

I tried it on my mac M1 (R v4.1.2) and my Linux box (R v4.0.4). If I use
other values, it does not work (see below). It seems only that combination,
granted I did not try them all.

>? (0.4 + 0.2 + 0 + 0.2 + 0 + 0.2) > 1
[1] FALSE

Am I missing something?

Cheers

-nb

??? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Tue Feb  1 18:07:51 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Tue, 1 Feb 2022 17:07:51 +0000
Subject: [R] Funky calculations
In-Reply-To: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
Message-ID: <e414e0f2-129f-e81e-7fa1-ca1f610c6176@sapo.pt>

Hello,

Like others have said, this is FAQ 7.31. Try

(0.4 + 0.2 + 0 + 0.3 + 0 + 0.1) - 1


on both systems, on one of them it's not zero.

Hope this helps,

Rui Barradas

?s 13:45 de 01/02/2022, Nathan Boeger escreveu:
> Hello,
> 
> I found something strange and maybe I am going nuts but this does not make
> sense:
> 
>>   (0.4 + 0.2 + 0 + 0.3 + 0 + 0.1) > 1
> [1] TRUE
> 
> I tried it on my mac M1 (R v4.1.2) and my Linux box (R v4.0.4). If I use
> other values, it does not work (see below). It seems only that combination,
> granted I did not try them all.
> 
>>   (0.4 + 0.2 + 0 + 0.2 + 0 + 0.2) > 1
> [1] FALSE
> 
> Am I missing something?
> 
> Cheers
> 
> -nb
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @v|gro@@ @end|ng |rom ver|zon@net  Tue Feb  1 18:19:47 2022
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Tue, 1 Feb 2022 17:19:47 +0000 (UTC)
Subject: [R] 
 [External] Weird behaviour of order() when having multiple ties
In-Reply-To: <CAFVVV-i83ad8OWs-8veC3dpnFNmaOenuNLWHpksuEH1za=Gr3w@mail.gmail.com>
References: <CAFVVV-gzoE2UDGMga_Pgbix=BmZj5WN2nA1wqY9s75_tcKQiWA@mail.gmail.com>
 <BL1PR11MB5239432B2669D24516983141D2249@BL1PR11MB5239.namprd11.prod.outlook.com>
 <CAFVVV-i83ad8OWs-8veC3dpnFNmaOenuNLWHpksuEH1za=Gr3w@mail.gmail.com>
Message-ID: <1765395989.2546954.1643735987443@mail.yahoo.com>

Stefan,

You are thinking of sorting and indeed if you sort, you get what you think:

> sort(c(2,3,4,1,1,1,1,1))
[1] 1 1 1 1 1 2 3 4

BUT order() does not return any of its values. NONE. It returns the indexes of sorted values and you can, if you choose, use the index to sort one or more values.

Imagine you have two independent vectors (not a data.frame) containing names and ages. You want to rearrange the names in alphabetical order but do not want to lose the ages. When done you want the correspondence to remain.

> Names <- c("Marie", "Jacques", "Zelda", "Jean")
> Ages <- c(12, 32, 3, 102)
> (index <- order(Names))
[1] 2 4 1 3
> (sortedNames <- Names[index])
[1] "Jacques" "Jean"    "Marie"   "Zelda"  
> (sortedAges <- Ages[index])
[1]  32 102  12   3

By getting the order you should take entries from the original, you can apply that order to both of the vectors, or anything else linked such as their birthday. Yes, many people avoid this by simply connecting all the vectors in a data.frame, but under the sheets, the code you use to manipulate things will often do something similar to actually make what you want happen.

In your example, you can get the sorted version like this:

> # Get vector of indices and print
> (index <- order(c(2,3,4,1,1,1,1,1)))
[1] 4 5 6 7 8 1 2 3
> 
> # Use the index on the vector to reaarange the order and print
> (c(2,3,4,1,1,1,1,1)[index])
[1] 1 1 1 1 1 2 3 4
> 
> # Use the index reversed to print in descending order.
> (c(2,3,4,1,1,1,1,1)[rev(index)])
[1] 4 3 2 1 1 1 1 1

And note you can print it forward and backward without calling sort() twice, not that this is important!

I hope that clarifies why the name is "order" and not "sort". 


-----Original Message-----
From: Stefan Fleck <stefan.b.fleck at gmail.com>
To: Richard M. Heiberger <rmh at temple.edu>
Cc: r-help at r-project.org <r-help at r-project.org>
Sent: Sun, Jan 30, 2022 3:07 pm
Subject: Re: [R]  [External] Weird behaviour of order() when having multiple ties

it's not about the sort order of the ties, shouldn't all the 1s in
order(c(2,3,4,1,1,1,1,1)) come before 2,3,4? because that's not what
happening

On Sun, Jan 30, 2022 at 9:00 PM Richard M. Heiberger <rmh at temple.edu> wrote:

> when there are ties it doesn't matter which is first.
> in a situation where it does matter, you will need a tiebreaker column.
> ------------------------------
> *From:* R-help <r-help-bounces at r-project.org> on behalf of Stefan Fleck <
> stefan.b.fleck at gmail.com>
> *Sent:* Sunday, January 30, 2022 4:16:44 AM
> *To:* r-help at r-project.org <r-help at r-project.org>
> *Subject:* [External] [R] Weird behaviour of order() when having multiple
> ties
>
> I am experiencing a weird behavior of `order()` for numeric vectors. I
> tested on 3.6.2 and 4.1.2 for windows and R 4.0.2 on ubuntu. Can anyone
> confirm?
>
> order(
>?  c(
>? ?  0.6,
>? ?  0.5,
>? ?  0.3,
>? ?  0.2,
>? ?  0.1,
>? ?  0.1
>?  )
> )
> ## Result [should be in order]
> [1] 5 6 4 3 2 1
>
> The sort order is obviously wrong. This only occurs if i have multiple
> ties. The problem does _not_ occur for decreasing = TRUE.
>
>? ? ? ?  [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>
> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=04%7C01%7Crmh%40temple.edu%7Cbae20314c2314a5cc7cd08d9e429e33f%7C716e81efb52244738e3110bd02ccf6e5%7C0%7C0%7C637791692024451993%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=O6R%2FNM6IdPzP8RY3JIWfLgmkE%2B0KcVyYBxoRMo8v2dk%3D&reserved=0
> PLEASE do read the posting guide
> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&data=04%7C01%7Crmh%40temple.edu%7Cbae20314c2314a5cc7cd08d9e429e33f%7C716e81efb52244738e3110bd02ccf6e5%7C0%7C0%7C637791692024451993%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=6hlfMjZLzopVzGnFVWlGnoEqvZBQwXPlxMuZ2sglEUk%3D&reserved=0
> and provide commented, minimal, self-contained, reproducible code.
>

??? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From @v|gro@@ @end|ng |rom ver|zon@net  Tue Feb  1 20:07:50 2022
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Tue, 1 Feb 2022 19:07:50 +0000 (UTC)
Subject: [R] Funky calculations
In-Reply-To: <e414e0f2-129f-e81e-7fa1-ca1f610c6176@sapo.pt>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
 <e414e0f2-129f-e81e-7fa1-ca1f610c6176@sapo.pt>
Message-ID: <302884631.2576868.1643742470880@mail.yahoo.com>

I suspect there are examples, RUI, where the same code can produce different results based on the underlying representation or manipulation of floating point. There are two main versions I typically see using what some call a "float" and a "double" and I suspect at some future point someone will take 256 bits or so to make a "superdouble" that allots so many more bits to the mantissa and exponent components that you can use much larger numbers with many more significant digits. But it is the final digits where imprecision enters and also a number can vary whether you do rounding or truncating at that point. I think in binary, you generally truncate. So if you consider a number like 1/7 that has an infinitely long decimal representation in bases like decimal, but does have repeats, compared to a transcendental number like pi that does not really repeat, you can round it in decimal representation depending on where you are in the sequence when you stop.

.142857142857142857...

If you stop at a 5 or 7 or 8 and back up to the previous digit, you round up. Else you leave the previous result alone.

.1
.14
.143
.1429
etc.

I actually wrote a much longer explanation about a similar concern someone had in another language, Python, that was picked up and published in a new daily and can be found as the second article at the following URL if interested.

https://pyherald.com/articles/02_01_2022/

It starts like this:

Representing Numbers: The Complexity Behind
Mail by Avi Gross , src

Note I did not choose that title nor did I choose the original Subject line I was replying to here: https://mail.python.org/pipermail/python-list/2021-November/904404.html

And, no, I was not asked nor given any opportunity to polish it or fix a spelling error. It was just my fairly usual free-floating posting style like I use here. It doe illustrate that we are not talking about a bug in R and that the proper way to avoid it may include being aware that sometimes a test for near-equality may be needed.

Think of how many people write an endless loop by waiting for an asymptotic sum like adding 1/2 + 1/4 + 1/8 ... and breaking out of the loop only when the sum is exactly equal to 1.0. Not only will it mathematically never happen, unless you consider happening AT infinity to qualify, but the addition using floating point numbers may end at 0.9999999999999999999999 or so and anything added to it will underflow and always be shown as adding 0 thereafter. But looked at properly, this is pretty much the same problem being reported here. So some people create a small epsilon such as 1E-15 ad write code like:

if (abs(myval - expected) < epsilon) ...

This is another way of saying, if I am close enough, we are there.

All computer languages are alike; except where they aren't.


-----Original Message-----
From: Rui Barradas <ruipbarradas at sapo.pt>
To: Nathan Boeger <nboeger at gmail.com>; r-help at r-project.org
Sent: Tue, Feb 1, 2022 12:07 pm
Subject: Re: [R] Funky calculations


Hello,

Like others have said, this is FAQ 7.31. Try

(0.4 + 0.2 + 0 + 0.3 + 0 + 0.1) - 1


on both systems, on one of them it's not zero.

Hope this helps,

Rui Barradas

?s 13:45 de 01/02/2022, Nathan Boeger escreveu:
> Hello,
> 
> I found something strange and maybe I am going nuts but this does not make
> sense:
> 
>>?  (0.4 + 0.2 + 0 + 0.3 + 0 + 0.1) > 1
> [1] TRUE
> 
> I tried it on my mac M1 (R v4.1.2) and my Linux box (R v4.0.4). If I use
> other values, it does not work (see below). It seems only that combination,
> granted I did not try them all.
> 
>>?  (0.4 + 0.2 + 0 + 0.2 + 0 + 0.2) > 1
> [1] FALSE
> 
> Am I missing something?
> 
> Cheers
> 
> -nb


 


From rmh @end|ng |rom temp|e@edu  Tue Feb  1 20:44:02 2022
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Tue, 1 Feb 2022 19:44:02 +0000
Subject: [R] [External]  Funky calculations
In-Reply-To: <20220201193557.308209e8@Tarkus>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
 <20220201193557.308209e8@Tarkus>
Message-ID: <D24231AD-5CBF-4595-962E-2009821BD20B@temple.edu>

RShowDoc('FAQ') 

then search for 7.31


This statement
"If you stop at a 5 or 7 or 8 and back up to the previous digit, you round up. Else you leave the previous result alone."
is not quite right.  The recommendation in IEEE 754, and this is how R does arithmetic, is to Round Even.

I ilustrate here with decimal, even though R and other programs use binary.

> x <- c(1.4, 1.5, 1.6, 2.4, 2.5, 2.6, 3.4, 3.5, 3.6, 4.4, 4.5, 4.6)
> r <- round(x)
> cbind(x, r)
        x r
 [1,] 1.4 1
 [2,] 1.5 2
 [3,] 1.6 2
 [4,] 2.4 2
 [5,] 2.5 2
 [6,] 2.6 3
 [7,] 3.4 3
 [8,] 3.5 4
 [9,] 3.6 4
[10,] 4.4 4
[11,] 4.5 4
[12,] 4.6 5
> 

Numbers whose last digit is not 5 (when in decimal) round to the nearest integer.
Numbers who last digit is 5 (1.5, 2.5, 3.5, 4.5 above) 
round to the nearest EVEN integer.
Hence 1.5 and 3.5 round up to the even numbers 2 and 4.
2.5 and 4.5 round down do the even numbers 2 and 4.

This way the round ups and downs average out to 0.  If we always went up from .5 we would have
an updrift over time.

For even more detail click on the link in FAQ 7.31 to my appendix
https:// link.springer.com/content/pdf/bbm%3A978-1-4939-2122-5%2F1.pdf
and search for "Appendix G".

Section G.5 explains Round to Even.
Sections G.6 onward illustrate specific examples, such as the one that started this email thread.

Rich

From drj|m|emon @end|ng |rom gm@||@com  Tue Feb  1 23:19:35 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 2 Feb 2022 09:19:35 +1100
Subject: [R] Survey design for multilevel analysis
In-Reply-To: <CAOTqx+KtV1_WOtffzuZJetE=JFKqVDqV8O15aUs0vBFZa1yyeQ@mail.gmail.com>
References: <CAOTqx+KtV1_WOtffzuZJetE=JFKqVDqV8O15aUs0vBFZa1yyeQ@mail.gmail.com>
Message-ID: <CA+8X3fUCk5FYbXOpTWLnFS3tzf7pSx4DvQrGHa0RcCRgTKJ4Tw@mail.gmail.com>

Hi Love,
I have finally had a chance to look at this more closely. I think that
the following link:

https://cran.r-project.org/web/packages/survival/vignettes/adjcurve.pdf

may be useful. See section 4.2.
This is not my area of expertise, but it seems to be a known problem.
Perhaps by posting to the:

R-SGI-Epi

mailing list you will get responses from specialists in the field.

Jim

On Tue, Feb 1, 2022 at 9:02 PM Love Umesi <yugohlav at gmail.com> wrote:
>
> I am new in using R, and I need to run a multilevel analysis (two-phase
> design on two levels) using Cox frailty survival model on a survey data
> (Nigeria Demographic and Health Survey Data).
>
> My problem is how to write the design weight using the two weights needed
> and apply them to the analysis.
>
> I have identified the needed variables for the survey design, which
> are : psu/cluster=
> v021 individual-level weight= wt1_1 cluster-level weigh= wt2_1 strata/stratum=
> v022
>
> Please can someone help me with the survey design (svydesign) code and how
> to include it and the weights in a model.
>
> I understand I have to use svycoxph in the model. Please how do I include
> survey design and account for the 2 weights (individual-level and
> cluster-level weights) in the gamma frailty model below?
>
> Frailty1 <- coxph (Surv(study_time, died) ~ factor(v024) + factor(mat_edu)
> + v025 + frailty(v021,distribution="gamma"), data=rcom2018)
>
> Really looking forward to your help as my project is hanging on this.
>
> Many thanks.
>
>
> library(survival)#> Warning: package 'survival' was built under R version 4.0.5
> library(frailtypack)#> Warning: package 'frailtypack' was built under
> R version 4.0.5#> Loading required package: boot#> #> Attaching
> package: 'boot'#> The following object is masked from
> 'package:survival':#> #>     aml#> Loading required package: MASS#>
> Loading required package: survC1#> Warning: package 'survC1' was built
> under R version 4.0.5#> Loading required package: doBy#> Warning:
> package 'doBy' was built under R version 4.0.5#> #> Attaching package:
> 'frailtypack'#> The following object is masked from
> 'package:survival':#> #>     cluster
>
>
> rcom1 <- data.frame(
>   data.frame(
>     pid = c(
>       1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,
>       14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,
>       30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45,
>       46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,
>       61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76,
>       77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92,
>       93, 94, 95, 96, 97, 98, 99, 100
>     ),
>     study_time = c(
>       13, 9, 17, 31, 39, 22, 24, 0, 23, 12, 9, 35,
>       18, 20, 60, 18, 5, 46, 26, 54, 37, 51, 31, 55, 27, 15, 39, 6,
>       29, 0, 9, 40, 23, 12, 35, 56, 14, 40, 57, 42, 5, 42, 39, 39,
>       54, 19, 52, 42, 7, 28, 53, 5, 28, 13, 37, 0, 23, 33, 27, 36, 20,
>       24, 58, 34, 12, 44, 3, 34, 14, 5, 10, 40, 12, 36, 19, 58, 17,
>       40, 39, 58, 53, 53, 1, 50, 2, 28, 24, 13, 13, 50, 46, 46, 19, 6,
>       32, 59, 9, 30, 30, 43
>     ),
>     died = c(
>       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
>       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
>       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
>       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>       0
>     ),
>     v021 = c(
>       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3,
>       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,
>       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
>       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,
>       6
>     ),
>     v022 = c(
>       "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "2", "2", "2", "2", "2", "2", "2",
>       "2", "2", "2"
>     ),
>     v012 = c(
>       40, 37, 27, 27, 24, 32, 35, 35, 34, 20, 28,
>       28, 26, 24, 24, 25, 26, 26, 26, 26, 28, 27, 25, 25, 27, 26, 26,
>       21, 21, 31, 36, 36, 27, 23, 32, 32, 33, 33, 33, 28, 25, 37,
>       33, 34, 33, 28, 28, 29, 33, 33, 33, 39, 38, 38, 38, 38, 24, 27,
>       35, 40, 22, 38, 38, 21, 30, 30, 30, 39, 43, 18, 23, 23, 25, 25,
>       30, 45, 26, 26, 35, 35, 35, 35, 32, 32, 40, 25, 27, 30, 30, 30,
>       28, 28, 18, 27, 30, 30, 27, 21, 21, 30
>     ),
>     wt2_1 = c(
>       401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031,
>       401.200012207031, 401.200012207031, 401.200012207031, 401.200012207031,
>       631.818176269531, 631.818176269531, 631.818176269531,
>       631.818176269531, 631.818176269531, 631.818176269531, 631.818176269531,
>       631.818176269531, 631.818176269531, 631.818176269531
>     ),
>     wt1_1 = c(
>       2.5074667930603, 2.5074667930603,
>       2.5074667930603, 2.5074667930603, 2.5074667930603, 2.5074667930603,
>       2.5074667930603, 2.5074667930603, 2.5074667930603, 2.5074667930603,
>       2.5074667930603, 2.5074667930603, 2.5074667930603,
>       2.5074667930603, 2.5074667930603, 5.1194109916687, 5.1194109916687,
>       5.1194109916687, 5.1194109916687, 5.1194109916687, 5.1194109916687,
>       5.1194109916687, 5.1194109916687, 5.1194109916687,
>       5.1194109916687, 5.1194109916687, 5.1194109916687, 5.1194109916687,
>       5.1194109916687, 5.1194109916687, 5.1194109916687, 5.1194109916687,
>       5.1194109916687, 5.1194109916687, 2.40910983085632,
>       2.40910983085632, 2.40910983085632, 2.40910983085632, 2.40910983085632,
>       2.40910983085632, 2.40910983085632, 2.40910983085632,
>       2.40910983085632, 2.40910983085632, 2.40910983085632, 2.40910983085632,
>       2.40910983085632, 2.40910983085632, 1.06203985214233,
>       1.06203985214233, 1.06203985214233, 1.06203985214233, 1.06203985214233,
>       1.06203985214233, 1.06203985214233, 1.06203985214233,
>       1.06203985214233, 1.06203985214233, 1.06203985214233, 1.06203985214233,
>       1.06203985214233, 1.06203985214233, 1.06203985214233,
>       1.06203985214233, 1.06203985214233, 1.06203985214233, 1.06203985214233,
>       2.80098295211792, 2.80098295211792, 2.80098295211792,
>       2.80098295211792, 2.80098295211792, 2.80098295211792, 2.80098295211792,
>       2.80098295211792, 2.80098295211792, 2.80098295211792,
>       2.80098295211792, 2.80098295211792, 2.80098295211792, 2.80098295211792,
>       2.80098295211792, 2.80098295211792, 2.80098295211792,
>       2.80098295211792, 2.80098295211792, 2.80098295211792, 2.80098295211792,
>       2.80098295211792, 2.80098295211792, 1.24210178852081,
>       1.24210178852081, 1.24210178852081, 1.24210178852081, 1.24210178852081,
>       1.24210178852081, 1.24210178852081, 1.24210178852081,
>       1.24210178852081, 1.24210178852081
>     ),
>     v024 = c(
>       "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1"
>     ),
>     v025 = c(
>       "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
>       "1", "1", "1", "1", "1", "1", "2", "2", "2", "2", "2", "2", "2",
>       "2", "2", "2"
>     ),
>     mat_edu = c(
>       "5", "5", "5", "4", "4", "5", "4", "4", "4",
>       "4", "4", "4", "5", "5", "5", "5", "5", "5", "4", "4", "5",
>       "4", "4", "4", "5", "3", "3", "4", "4", "5", "5", "5", "5", "4",
>       "2", "2", "0", "0", "0", "5", "5", "0", "1", "5", "5", "3",
>       "3", "5", "5", "5", "5", "5", "5", "5", "5", "5", "5", "4", "5",
>       "5", "4", "5", "5", "3", "4", "4", "5", "3", "1", "3", "3", "3",
>       "1", "3", "2", "1", "3", "3", "4", "4", "0", "0", "2", "2",
>       "1", "0", "4", "4", "4", "4", "0", "0", "3", "4", "2", "2", "3",
>       "3", "3", "0"
>     )
>   ))
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @v|gro@@ @end|ng |rom ver|zon@net  Wed Feb  2 02:42:12 2022
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Wed, 2 Feb 2022 01:42:12 +0000 (UTC)
Subject: [R] [External]  Funky calculations
In-Reply-To: <D24231AD-5CBF-4595-962E-2009821BD20B@temple.edu>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
 <20220201193557.308209e8@Tarkus>
 <D24231AD-5CBF-4595-962E-2009821BD20B@temple.edu>
Message-ID: <1240726547.2663210.1643766132419@mail.yahoo.com>

Richard,

I think it was fairly clear I was explaining how people do arithmetic manually and often truncate or round to some number of decimal places. I said nothing about what R does or what the IEEE standards say and I do not particularly care when making MY point.

My point is that humans before computers also had trouble writing down any decimals that continue indefinitely. It cannot be expected computer versions of arithmetic can do much better. Different people can opt to do the calculation with the same or different numbers of digits ad when compared to each other they may not match.

I do care what it does in my programs, of course. My goal here was to explain to someone that the anomaly found was not really an anomaly and that careful coding may be required in these situations.


-----Original Message-----
From: Richard M. Heiberger <rmh at temple.edu>
To: Avi Gross <avigross at verizon.net>
Cc: Nathan Boeger <nboeger at gmail.com>; r-help at r-project.org <r-help at r-project.org>
Sent: Tue, Feb 1, 2022 2:44 pm
Subject: Re: [External] [R] Funky calculations


RShowDoc('FAQ') 


then search for 7.31


This statement
"If you stop at a 5 or 7 or 8 and back up to the previous digit, you round up. Else you leave the previous result alone."
is not quite right.? The recommendation in IEEE 754, and this is how R does arithmetic, is to Round Even.

I ilustrate here with decimal, even though R and other programs use binary.

> x <- c(1.4, 1.5, 1.6, 2.4, 2.5, 2.6, 3.4, 3.5, 3.6, 4.4, 4.5, 4.6)
> r <- round(x)
> cbind(x, r)
? ? ? ? x r
 [1,] 1.4 1
 [2,] 1.5 2
 [3,] 1.6 2
 [4,] 2.4 2
 [5,] 2.5 2
 [6,] 2.6 3
 [7,] 3.4 3
 [8,] 3.5 4
 [9,] 3.6 4
[10,] 4.4 4
[11,] 4.5 4
[12,] 4.6 5
> 

Numbers whose last digit is not 5 (when in decimal) round to the nearest integer.
Numbers who last digit is 5 (1.5, 2.5, 3.5, 4.5 above) 
round to the nearest EVEN integer.
Hence 1.5 and 3.5 round up to the even numbers 2 and 4.
2.5 and 4.5 round down do the even numbers 2 and 4.

This way the round ups and downs average out to 0.? If we always went up from .5 we would have
an updrift over time.

For even more detail click on the link in FAQ 7.31 to my appendix
https:// link.springer.com/content/pdf/bbm%3A978-1-4939-2122-5%2F1.pdf
and search for "Appendix G".

Section G.5 explains Round to Even.
Sections G.6 onward illustrate specific examples, such as the one that started this email thread.

Rich
 


From rmh @end|ng |rom temp|e@edu  Wed Feb  2 03:04:45 2022
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Wed, 2 Feb 2022 02:04:45 +0000
Subject: [R] [External]  Funky calculations
In-Reply-To: <1240726547.2663210.1643766132419@mail.yahoo.com>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
 <20220201193557.308209e8@Tarkus>
 <D24231AD-5CBF-4595-962E-2009821BD20B@temple.edu>
 <1240726547.2663210.1643766132419@mail.yahoo.com>
Message-ID: <1A13EFE0-C500-40D0-9BD0-8546E4EECBEA@temple.edu>

I apologize if my tone came across wrong.  I enjoy reading your comments on this list.

My goal was to describe what the IEEE and R interpret "careful coding" to be.

> On Feb 01, 2022, at 20:42, Avi Gross <avigross at verizon.net> wrote:
> 
> Richard,
> 
> I think it was fairly clear I was explaining how people do arithmetic manually and often truncate or round to some number of decimal places. I said nothing about what R does or what the IEEE standards say and I do not particularly care when making MY point.
> 
> My point is that humans before computers also had trouble writing down any decimals that continue indefinitely. It cannot be expected computer versions of arithmetic can do much better. Different people can opt to do the calculation with the same or different numbers of digits ad when compared to each other they may not match.
> 
> I do care what it does in my programs, of course. My goal here was to explain to someone that the anomaly found was not really an anomaly and that careful coding may be required in these situations.
> 
> 
> -----Original Message-----
> From: Richard M. Heiberger <rmh at temple.edu>
> To: Avi Gross <avigross at verizon.net>
> Cc: Nathan Boeger <nboeger at gmail.com>; r-help at r-project.org <r-help at r-project.org>
> Sent: Tue, Feb 1, 2022 2:44 pm
> Subject: Re: [External] [R] Funky calculations
> 
> 
> RShowDoc('FAQ') 
> 
> 
> then search for 7.31
> 
> 
> This statement
> "If you stop at a 5 or 7 or 8 and back up to the previous digit, you round up. Else you leave the previous result alone."
> is not quite right.  The recommendation in IEEE 754, and this is how R does arithmetic, is to Round Even.
> 
> I ilustrate here with decimal, even though R and other programs use binary.
> 
>> x <- c(1.4, 1.5, 1.6, 2.4, 2.5, 2.6, 3.4, 3.5, 3.6, 4.4, 4.5, 4.6)
>> r <- round(x)
>> cbind(x, r)
>         x r
> [1,] 1.4 1
> [2,] 1.5 2
> [3,] 1.6 2
> [4,] 2.4 2
> [5,] 2.5 2
> [6,] 2.6 3
> [7,] 3.4 3
> [8,] 3.5 4
> [9,] 3.6 4
> [10,] 4.4 4
> [11,] 4.5 4
> [12,] 4.6 5
>> 
> 
> Numbers whose last digit is not 5 (when in decimal) round to the nearest integer.
> Numbers who last digit is 5 (1.5, 2.5, 3.5, 4.5 above) 
> round to the nearest EVEN integer.
> Hence 1.5 and 3.5 round up to the even numbers 2 and 4.
> 2.5 and 4.5 round down do the even numbers 2 and 4.
> 
> This way the round ups and downs average out to 0.  If we always went up from .5 we would have
> an updrift over time.
> 
> For even more detail click on the link in FAQ 7.31 to my appendix
> https:// link.springer.com/content/pdf/bbm%3A978-1-4939-2122-5%2F1.pdf
> and search for "Appendix G".
> 
> Section G.5 explains Round to Even.
> Sections G.6 onward illustrate specific examples, such as the one that started this email thread.
> 
> Rich
> 


From nboeger @end|ng |rom gm@||@com  Wed Feb  2 04:00:44 2022
From: nboeger @end|ng |rom gm@||@com (Nathan Boeger)
Date: Wed, 2 Feb 2022 10:00:44 +0700
Subject: [R] [External]  Funky calculations
In-Reply-To: <D24231AD-5CBF-4595-962E-2009821BD20B@temple.edu>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
 <20220201193557.308209e8@Tarkus>
 <D24231AD-5CBF-4595-962E-2009821BD20B@temple.edu>
Message-ID: <CAFxA6Qq1sGwh9ZwEaV6zvtfd0yq6A1W6U4oDMuy1KXs_CV9ryQ@mail.gmail.com>

Thank you for this explanation!

I have a long background in C/C++ and never realized this was such an issue
with some languages. At least, with trivial single digit decimals. I
understand accuracy issues with very large decimals, repeating or
non-terminating rationals and I have handled them in the past. It makes me
worried about all the R scripts I have written before (yikes!).

Cheers

-nb

On Wed, 2 Feb 2022 at 02:44, Richard M. Heiberger <rmh at temple.edu> wrote:

> RShowDoc('FAQ')
>
> then search for 7.31
>
>
> This statement
> "If you stop at a 5 or 7 or 8 and back up to the previous digit, you round
> up. Else you leave the previous result alone."
> is not quite right.  The recommendation in IEEE 754, and this is how R
> does arithmetic, is to Round Even.
>
> I ilustrate here with decimal, even though R and other programs use binary.
>
> > x <- c(1.4, 1.5, 1.6, 2.4, 2.5, 2.6, 3.4, 3.5, 3.6, 4.4, 4.5, 4.6)
> > r <- round(x)
> > cbind(x, r)
>         x r
>  [1,] 1.4 1
>  [2,] 1.5 2
>  [3,] 1.6 2
>  [4,] 2.4 2
>  [5,] 2.5 2
>  [6,] 2.6 3
>  [7,] 3.4 3
>  [8,] 3.5 4
>  [9,] 3.6 4
> [10,] 4.4 4
> [11,] 4.5 4
> [12,] 4.6 5
> >
>
> Numbers whose last digit is not 5 (when in decimal) round to the nearest
> integer.
> Numbers who last digit is 5 (1.5, 2.5, 3.5, 4.5 above)
> round to the nearest EVEN integer.
> Hence 1.5 and 3.5 round up to the even numbers 2 and 4.
> 2.5 and 4.5 round down do the even numbers 2 and 4.
>
> This way the round ups and downs average out to 0.  If we always went up
> from .5 we would have
> an updrift over time.
>
> For even more detail click on the link in FAQ 7.31 to my appendix
> https:// link.springer.com/content/pdf/bbm%3A978-1-4939-2122-5%2F1.pdf
> and search for "Appendix G".
>
> Section G.5 explains Round to Even.
> Sections G.6 onward illustrate specific examples, such as the one that
> started this email thread.
>
> Rich

	[[alternative HTML version deleted]]


From w||||@mwdun|@p @end|ng |rom gm@||@com  Wed Feb  2 04:30:19 2022
From: w||||@mwdun|@p @end|ng |rom gm@||@com (Bill Dunlap)
Date: Tue, 1 Feb 2022 19:30:19 -0800
Subject: [R] [External] Funky calculations
In-Reply-To: <CAFxA6Qq1sGwh9ZwEaV6zvtfd0yq6A1W6U4oDMuy1KXs_CV9ryQ@mail.gmail.com>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
 <20220201193557.308209e8@Tarkus>
 <D24231AD-5CBF-4595-962E-2009821BD20B@temple.edu>
 <CAFxA6Qq1sGwh9ZwEaV6zvtfd0yq6A1W6U4oDMuy1KXs_CV9ryQ@mail.gmail.com>
Message-ID: <CAHqSRuT7KsOnUEKQRz2+Y_Z0ZoE+y3LHm4q9JLzqsSATe-85iA@mail.gmail.com>

The base 2 representation of 0.4 repeats the digit sequence 1001
infinitely, hence must be rounded.  The problem occurs in C the same as it
does in R.

bill at Bill-T490:~$ cat a.c
#include <stdio.h>

int main(int argc, char* argv[])
{
    double d = 0.4 + 0.3 + 0.2 + 0.1;
    printf("0.4+0.3+0.2+0.1 -> %24.17g\n", d);
    printf("0.4+0.3+0.2+0.1 == 1.0 -> %s\n", d == 1.0 ? "true" : "false");
    return 0;
}
bill at Bill-T490:~$ gcc a.c
bill at Bill-T490:~$ ./a.out
0.4+0.3+0.2+0.1 ->      0.99999999999999989
0.4+0.3+0.2+0.1 == 1.0 -> false

-Bill

On Tue, Feb 1, 2022 at 7:01 PM Nathan Boeger <nboeger at gmail.com> wrote:

> Thank you for this explanation!
>
> I have a long background in C/C++ and never realized this was such an issue
> with some languages. At least, with trivial single digit decimals. I
> understand accuracy issues with very large decimals, repeating or
> non-terminating rationals and I have handled them in the past. It makes me
> worried about all the R scripts I have written before (yikes!).
>
> Cheers
>
> -nb
>
> On Wed, 2 Feb 2022 at 02:44, Richard M. Heiberger <rmh at temple.edu> wrote:
>
> > RShowDoc('FAQ')
> >
> > then search for 7.31
> >
> >
> > This statement
> > "If you stop at a 5 or 7 or 8 and back up to the previous digit, you
> round
> > up. Else you leave the previous result alone."
> > is not quite right.  The recommendation in IEEE 754, and this is how R
> > does arithmetic, is to Round Even.
> >
> > I ilustrate here with decimal, even though R and other programs use
> binary.
> >
> > > x <- c(1.4, 1.5, 1.6, 2.4, 2.5, 2.6, 3.4, 3.5, 3.6, 4.4, 4.5, 4.6)
> > > r <- round(x)
> > > cbind(x, r)
> >         x r
> >  [1,] 1.4 1
> >  [2,] 1.5 2
> >  [3,] 1.6 2
> >  [4,] 2.4 2
> >  [5,] 2.5 2
> >  [6,] 2.6 3
> >  [7,] 3.4 3
> >  [8,] 3.5 4
> >  [9,] 3.6 4
> > [10,] 4.4 4
> > [11,] 4.5 4
> > [12,] 4.6 5
> > >
> >
> > Numbers whose last digit is not 5 (when in decimal) round to the nearest
> > integer.
> > Numbers who last digit is 5 (1.5, 2.5, 3.5, 4.5 above)
> > round to the nearest EVEN integer.
> > Hence 1.5 and 3.5 round up to the even numbers 2 and 4.
> > 2.5 and 4.5 round down do the even numbers 2 and 4.
> >
> > This way the round ups and downs average out to 0.  If we always went up
> > from .5 we would have
> > an updrift over time.
> >
> > For even more detail click on the link in FAQ 7.31 to my appendix
> > https:// link.springer.com/content/pdf/bbm%3A978-1-4939-2122-5%2F1.pdf
> > and search for "Appendix G".
> >
> > Section G.5 explains Round to Even.
> > Sections G.6 onward illustrate specific examples, such as the one that
> > started this email thread.
> >
> > Rich
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Feb  2 04:35:56 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 01 Feb 2022 19:35:56 -0800
Subject: [R] [External]  Funky calculations
In-Reply-To: <CAFxA6Qq1sGwh9ZwEaV6zvtfd0yq6A1W6U4oDMuy1KXs_CV9ryQ@mail.gmail.com>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
 <20220201193557.308209e8@Tarkus>
 <D24231AD-5CBF-4595-962E-2009821BD20B@temple.edu>
 <CAFxA6Qq1sGwh9ZwEaV6zvtfd0yq6A1W6U4oDMuy1KXs_CV9ryQ@mail.gmail.com>
Message-ID: <2B4B92D0-499B-4A18-83E7-8F0067E91E50@dcn.davis.ca.us>

Please don't be dense. Not "some languages"... the discussion here has tried very hard to explain why this affects all of them. Including C/C++ (R is written in C). Look at [1] if you don't believe us.

[1] https://0.30000000000000004.com/


On February 1, 2022 7:00:44 PM PST, Nathan Boeger <nboeger at gmail.com> wrote:
>Thank you for this explanation!
>
>I have a long background in C/C++ and never realized this was such an issue
>with some languages. At least, with trivial single digit decimals. I
>understand accuracy issues with very large decimals, repeating or
>non-terminating rationals and I have handled them in the past. It makes me
>worried about all the R scripts I have written before (yikes!).
>
>Cheers
>
>-nb
>
>On Wed, 2 Feb 2022 at 02:44, Richard M. Heiberger <rmh at temple.edu> wrote:
>
>> RShowDoc('FAQ')
>>
>> then search for 7.31
>>
>>
>> This statement
>> "If you stop at a 5 or 7 or 8 and back up to the previous digit, you round
>> up. Else you leave the previous result alone."
>> is not quite right.  The recommendation in IEEE 754, and this is how R
>> does arithmetic, is to Round Even.
>>
>> I ilustrate here with decimal, even though R and other programs use binary.
>>
>> > x <- c(1.4, 1.5, 1.6, 2.4, 2.5, 2.6, 3.4, 3.5, 3.6, 4.4, 4.5, 4.6)
>> > r <- round(x)
>> > cbind(x, r)
>>         x r
>>  [1,] 1.4 1
>>  [2,] 1.5 2
>>  [3,] 1.6 2
>>  [4,] 2.4 2
>>  [5,] 2.5 2
>>  [6,] 2.6 3
>>  [7,] 3.4 3
>>  [8,] 3.5 4
>>  [9,] 3.6 4
>> [10,] 4.4 4
>> [11,] 4.5 4
>> [12,] 4.6 5
>> >
>>
>> Numbers whose last digit is not 5 (when in decimal) round to the nearest
>> integer.
>> Numbers who last digit is 5 (1.5, 2.5, 3.5, 4.5 above)
>> round to the nearest EVEN integer.
>> Hence 1.5 and 3.5 round up to the even numbers 2 and 4.
>> 2.5 and 4.5 round down do the even numbers 2 and 4.
>>
>> This way the round ups and downs average out to 0.  If we always went up
>> from .5 we would have
>> an updrift over time.
>>
>> For even more detail click on the link in FAQ 7.31 to my appendix
>> https:// link.springer.com/content/pdf/bbm%3A978-1-4939-2122-5%2F1.pdf
>> and search for "Appendix G".
>>
>> Section G.5 explains Round to Even.
>> Sections G.6 onward illustrate specific examples, such as the one that
>> started this email thread.
>>
>> Rich
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @v|gro@@ @end|ng |rom ver|zon@net  Wed Feb  2 04:45:08 2022
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Wed, 2 Feb 2022 03:45:08 +0000 (UTC)
Subject: [R] [External]  Funky calculations
In-Reply-To: <1A13EFE0-C500-40D0-9BD0-8546E4EECBEA@temple.edu>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
 <20220201193557.308209e8@Tarkus>
 <D24231AD-5CBF-4595-962E-2009821BD20B@temple.edu>
 <1240726547.2663210.1643766132419@mail.yahoo.com>
 <1A13EFE0-C500-40D0-9BD0-8546E4EECBEA@temple.edu>
Message-ID: <801628629.2676628.1643773508952@mail.yahoo.com>

This is a discussion forum, Richard, and I welcome requests to clarify what I wrote or to be corrected, especially when my words have been read with an attempt to understand. I do get private responses too and some days i wonder if I am not communicating the way people like!

But let me repeat. The question we started with asked about R. My answer applies to quite a few languages besides R and maybe just about all of them.

I got private email insisting the numbers being added were not irrational so why would they not be represented easily as a sum. I know my answers included parts at various levels of abstraction as well as examples of cases when Decimals notation for a number like 1/7 results in an infinite repeating sequence. So, I think it wise to follow up with what binary looks like and why hardly ANYTHING that looks reasonable is hard to represent exactly.

Consider that binary means POWERS OF TWO. The sequence 1101 before a decimal point means (starting from the right and heading left) that you have one ONES and no TWOS and one FOURS and one EIGHTS. Powers of two ranging from 2 to the zero power to two cubed. You can make any integer whatsoever using as long a sequence of zeros and ones as you like. Compare this to decimal notation where you use powers of ten and of course can use any of 0-9.

But looking at fractional numbers, like 1/7 and 1/10, it gets hard and inexact.

Remember now we are in BINARY. Here are some fractions with everything not shown to the right being zeros and thus not needed to be shown explicitly. Starting with the decimal point, read this from left to right to see the powers in the denominator rising so 1/2 then 1/4 then 1/8 ...:

0.0 would be 0.
0.1 would be 1/2
0.101 would be 1/2 + 1/8 or 5/8
0.11 would be 1/2 + 1/4 or 3/4
0.111 would be 1/2 + 1/4 + 1/8 or 7/8

We are now using negative powers where 2 raised to the minus one power is one over two raised to the plus one power, or 1/2 and so on. As you head to the right you get to fairly small numbers like 1/2048 ...

Every single binary fraction is thus a possibly infinite sum of negative powers of two, or rather the reciprocals of those in positive terms.

If you want to make 1/7, to some number of decimal places, it looks like this up to some point where I stop:

0.00100100100100100101

So no halves, no quarters, 1/8, no sixteenths, no thirty-seconds, 1/64, and so on. But if you add all that up, and note the sequence was STOPPED before it could continue further, you get this translated into decimal:

0.142857 55157470703125

Recall 1/7 in decimal notation is
0.142857 142857142857142857...

Note the divergence at the seventh digit after the decimal point. I left a space to show where they diverge. If I used more binary digits, I can get as close as I want but computers these days do not allow too many more digits unless you use highly specialized programs. There are packages that give you access such as "mpfr" but generally nothing can give you infinite precision. R will not handle an infinite number of infinitesimals.

The original problem that began our thread was about numbers like 0.1 and 0.2 and so on. In base ten, they look nice but I repeat in base 2 only powers of TWO reign.

0.1 in base two is about 0.0001100110011001101

that reads as 1/16 + 1/32 + 1/256 + 1/512 + ...

If I convert the above segment, which I repeat was stopped short, I get 0.1000003814697265625 which is a tad over and had I taken the last 1 and changed it to a zero as in 0.0001100110011001100 then we would have a bit under at 0.09999847412109375 

So the only way to write 0.1 exactly is to continue infinitely, again. Do the analysis and understand why most rational numbers will not easily convert to a small number of bits. But the advantages of computers doing operations in binary are huge and need not be explained. You may THINK you are entering numbers in decimal form but they rarely remain that way for long before they simply become binary and often remain binary unless and until you ask to print them out, usually in decimal.

BTW, I used a random web site to do the above conversion calculations:

https://www.rapidtables.com/convert/number/binary-to-decimal.html

Since I am writing in plain text, I cannot show what it says in the box on that page further down under Decimal Calculation Steps so I wonder what the rest of this message looks like:

(0.0001100110011001100)? = (0 ? 2?) + (0 ? 2??) + (0 ? 2??) + (0 ? 2??) + (1 ? 2??) + (1 ? 2??) + (0 ? 2??) + (0 ? 2??) + (1 ? 2??) + (1 ? 2??) + (0 ? 2???) + (0 ? 2???) + (1 ? 2???) + (1 ? 2???) + (0 ? 2???) + (0 ? 2???) + (1 ? 2???) + (1 ? 2???) + (0 ? 2???) + (0 ? 2???) = (0.09999847412109375)??

I think my part in this particular discussion can now finally come to an end. R and everything else can be incomplete. Deal with it!

-----Original Message-----
From: Richard M. Heiberger <rmh at temple.edu>
To: Avi Gross <avigross at verizon.net>
Cc: nboeger at gmail.com <nboeger at gmail.com>; r-help at r-project.org <r-help at r-project.org>
Sent: Tue, Feb 1, 2022 9:04 pm
Subject: Re: [External] [R] Funky calculations


I apologize if my tone came across wrong.? I enjoy reading your comments on this list.

My goal was to describe what the IEEE and R interpret "careful coding" to be.


> On Feb 01, 2022, at 20:42, Avi Gross <avigross at verizon.net> wrote:
> 
> Richard,
> 
> I think it was fairly clear I was explaining how people do arithmetic manually and often truncate or round to some number of decimal places. I said nothing about what R does or what the IEEE standards say and I do not particularly care when making MY point.
> 
> My point is that humans before computers also had trouble writing down any decimals that continue indefinitely. It cannot be expected computer versions of arithmetic can do much better. Different people can opt to do the calculation with the same or different numbers of digits ad when compared to each other they may not match.
> 
> I do care what it does in my programs, of course. My goal here was to explain to someone that the anomaly found was not really an anomaly and that careful coding may be required in these situations.
> 
> 
> -----Original Message-----
> From: Richard M. Heiberger <rmh at temple.edu>
> To: Avi Gross <avigross at verizon.net>
> Cc: Nathan Boeger <nboeger at gmail.com>; r-help at r-project.org <r-help at r-project.org>
> Sent: Tue, Feb 1, 2022 2:44 pm
> Subject: Re: [External] [R] Funky calculations
> 
> 
> RShowDoc('FAQ') 
> 
> 
> then search for 7.31
> 
> 
> This statement
> "If you stop at a 5 or 7 or 8 and back up to the previous digit, you round up. Else you leave the previous result alone."
> is not quite right.? The recommendation in IEEE 754, and this is how R does arithmetic, is to Round Even.
> 
> I ilustrate here with decimal, even though R and other programs use binary.
> 
>> x <- c(1.4, 1.5, 1.6, 2.4, 2.5, 2.6, 3.4, 3.5, 3.6, 4.4, 4.5, 4.6)
>> r <- round(x)
>> cbind(x, r)
>? ? ? ?  x r
> [1,] 1.4 1
> [2,] 1.5 2
> [3,] 1.6 2
> [4,] 2.4 2
> [5,] 2.5 2
> [6,] 2.6 3
> [7,] 3.4 3
> [8,] 3.5 4
> [9,] 3.6 4
> [10,] 4.4 4
> [11,] 4.5 4
> [12,] 4.6 5
>> 
> 
> Numbers whose last digit is not 5 (when in decimal) round to the nearest integer.
> Numbers who last digit is 5 (1.5, 2.5, 3.5, 4.5 above) 
> round to the nearest EVEN integer.
> Hence 1.5 and 3.5 round up to the even numbers 2 and 4.
> 2.5 and 4.5 round down do the even numbers 2 and 4.
> 
> This way the round ups and downs average out to 0.? If we always went up from .5 we would have
> an updrift over time.
> 
> For even more detail click on the link in FAQ 7.31 to my appendix
> https:// link.springer.com/content/pdf/bbm%3A978-1-4939-2122-5%2F1.pdf
> and search for "Appendix G".
> 
> Section G.5 explains Round to Even.
> Sections G.6 onward illustrate specific examples, such as the one that started this email thread.
> 
> Rich
> 


 


From nboeger @end|ng |rom gm@||@com  Wed Feb  2 06:06:10 2022
From: nboeger @end|ng |rom gm@||@com (Nathan Boeger)
Date: Wed, 2 Feb 2022 12:06:10 +0700
Subject: [R] [External] Funky calculations
In-Reply-To: <CAHqSRuT7KsOnUEKQRz2+Y_Z0ZoE+y3LHm4q9JLzqsSATe-85iA@mail.gmail.com>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
 <20220201193557.308209e8@Tarkus>
 <D24231AD-5CBF-4595-962E-2009821BD20B@temple.edu>
 <CAFxA6Qq1sGwh9ZwEaV6zvtfd0yq6A1W6U4oDMuy1KXs_CV9ryQ@mail.gmail.com>
 <CAHqSRuT7KsOnUEKQRz2+Y_Z0ZoE+y3LHm4q9JLzqsSATe-85iA@mail.gmail.com>
Message-ID: <CAFxA6Qr93D4VYjrZDvvrQgnTnToNQj3RQ21pLYrm8tJFHpA=1Q@mail.gmail.com>

I understand this and with C the data type used is important. For this type
of calculation, I would normally use a float (basic single precision is all
I require).

#include <stdio.h>

void main() {
  float foo = (0.4 + 0.2 + 0.30 + 0.1) ;
  printf("foo: %f , foo > 1: %s \n", foo, (foo > 1.0 ? "true" : "false"));
  double bar = (0.4 + 0.2 + 0.30 + 0.1) ;
  printf("bar: %lf , bar > 1: %s \n", bar, (bar > 1.0 ? "true" : "false"));
}

gcc  c-check.c -o c-check
./c-check
foo: 1.000000 , foo > 1: false
bar: 1.000000 , bar > 1: true

Again, it was my mistake for not reading the R-FAQ. I had no idea it would
spark such a long thread.

Cheers

-nb

On Wed, 2 Feb 2022 at 10:30, Bill Dunlap <williamwdunlap at gmail.com> wrote:

> The base 2 representation of 0.4 repeats the digit sequence 1001
> infinitely, hence must be rounded.  The problem occurs in C the same as it
> does in R.
>
> bill at Bill-T490:~$ cat a.c
> #include <stdio.h>
>
> int main(int argc, char* argv[])
> {
>     double d = 0.4 + 0.3 + 0.2 + 0.1;
>     printf("0.4+0.3+0.2+0.1 -> %24.17g\n", d);
>     printf("0.4+0.3+0.2+0.1 == 1.0 -> %s\n", d == 1.0 ? "true" : "false");
>     return 0;
> }
> bill at Bill-T490:~$ gcc a.c
> bill at Bill-T490:~$ ./a.out
> 0.4+0.3+0.2+0.1 ->      0.99999999999999989
> 0.4+0.3+0.2+0.1 == 1.0 -> false
>
> -Bill
>
> On Tue, Feb 1, 2022 at 7:01 PM Nathan Boeger <nboeger at gmail.com> wrote:
>
>> Thank you for this explanation!
>>
>> I have a long background in C/C++ and never realized this was such an
>> issue
>> with some languages. At least, with trivial single digit decimals. I
>> understand accuracy issues with very large decimals, repeating or
>> non-terminating rationals and I have handled them in the past. It makes me
>> worried about all the R scripts I have written before (yikes!).
>>
>> Cheers
>>
>> -nb
>>
>> On Wed, 2 Feb 2022 at 02:44, Richard M. Heiberger <rmh at temple.edu> wrote:
>>
>> > RShowDoc('FAQ')
>> >
>> > then search for 7.31
>> >
>> >
>> > This statement
>> > "If you stop at a 5 or 7 or 8 and back up to the previous digit, you
>> round
>> > up. Else you leave the previous result alone."
>> > is not quite right.  The recommendation in IEEE 754, and this is how R
>> > does arithmetic, is to Round Even.
>> >
>> > I ilustrate here with decimal, even though R and other programs use
>> binary.
>> >
>> > > x <- c(1.4, 1.5, 1.6, 2.4, 2.5, 2.6, 3.4, 3.5, 3.6, 4.4, 4.5, 4.6)
>> > > r <- round(x)
>> > > cbind(x, r)
>> >         x r
>> >  [1,] 1.4 1
>> >  [2,] 1.5 2
>> >  [3,] 1.6 2
>> >  [4,] 2.4 2
>> >  [5,] 2.5 2
>> >  [6,] 2.6 3
>> >  [7,] 3.4 3
>> >  [8,] 3.5 4
>> >  [9,] 3.6 4
>> > [10,] 4.4 4
>> > [11,] 4.5 4
>> > [12,] 4.6 5
>> > >
>> >
>> > Numbers whose last digit is not 5 (when in decimal) round to the nearest
>> > integer.
>> > Numbers who last digit is 5 (1.5, 2.5, 3.5, 4.5 above)
>> > round to the nearest EVEN integer.
>> > Hence 1.5 and 3.5 round up to the even numbers 2 and 4.
>> > 2.5 and 4.5 round down do the even numbers 2 and 4.
>> >
>> > This way the round ups and downs average out to 0.  If we always went up
>> > from .5 we would have
>> > an updrift over time.
>> >
>> > For even more detail click on the link in FAQ 7.31 to my appendix
>> > https:// link.springer.com/content/pdf/bbm%3A978-1-4939-2122-5%2F1.pdf
>> > and search for "Appendix G".
>> >
>> > Section G.5 explains Round to Even.
>> > Sections G.6 onward illustrate specific examples, such as the one that
>> > started this email thread.
>> >
>> > Rich
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From pro|jcn@@h @end|ng |rom gm@||@com  Wed Feb  2 14:35:54 2022
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Wed, 2 Feb 2022 08:35:54 -0500
Subject: [R] [External] Funky calculations
In-Reply-To: <801628629.2676628.1643773508952@mail.yahoo.com>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
 <20220201193557.308209e8@Tarkus>
 <D24231AD-5CBF-4595-962E-2009821BD20B@temple.edu>
 <1240726547.2663210.1643766132419@mail.yahoo.com>
 <1A13EFE0-C500-40D0-9BD0-8546E4EECBEA@temple.edu>
 <801628629.2676628.1643773508952@mail.yahoo.com>
Message-ID: <f7f69f1b-e19c-42e3-863f-27681a8fb618@gmail.com>

I was one of the 31 names on the 1985 IEEE standard. If anyone thinks things are awkward now,
try looking at the swamp we had beforehand.

What I believe IS useful is to provide examples and to explain them in tutorial fashion.
We need to recognize that our computations have limitations. Most common computing platforms
use IEEE binary arithmetic, but not all.

This was much more "in our face" when we used slide rules or hand-crank calculators. I still
have slide rules and a Monroe "Portable" calculator -- 5 kg! It's worth bringing them out every
so often and being thankful for the power and speed of modern computing, while remembering to
watch for the cowpads of REAL and REAL*8 arithmetic.

JN

On 2022-02-01 22:45, Avi Gross via R-help wrote:
> This is a discussion forum, Richard, and I welcome requests to clarify what I wrote or to be corrected, especially when my words have been read with an attempt to understand. I do get private responses too and some days i wonder if I am not communicating the way people like!
> 
> But let me repeat. The question we started with asked about R. My answer applies to quite a few languages besides R and maybe just about all of them.
> 
> I got private email insisting the numbers being added were not irrational so why would they not be represented easily as a sum. I know my answers included parts at various levels of abstraction as well as examples of cases when Decimals notation for a number like 1/7 results in an infinite repeating sequence. So, I think it wise to follow up with what binary looks like and why hardly ANYTHING that looks reasonable is hard to represent exactly.
> 
> Consider that binary means POWERS OF TWO. The sequence 1101 before a decimal point means (starting from the right and heading left) that you have one ONES and no TWOS and one FOURS and one EIGHTS. Powers of two ranging from 2 to the zero power to two cubed. You can make any integer whatsoever using as long a sequence of zeros and ones as you like. Compare this to decimal notation where you use powers of ten and of course can use any of 0-9.
> 
> But looking at fractional numbers, like 1/7 and 1/10, it gets hard and inexact.
> 
> Remember now we are in BINARY. Here are some fractions with everything not shown to the right being zeros and thus not needed to be shown explicitly. Starting with the decimal point, read this from left to right to see the powers in the denominator rising so 1/2 then 1/4 then 1/8 ...:
> 
> 0.0 would be 0.
> 0.1 would be 1/2
> 0.101 would be 1/2 + 1/8 or 5/8
> 0.11 would be 1/2 + 1/4 or 3/4
> 0.111 would be 1/2 + 1/4 + 1/8 or 7/8
> 
> We are now using negative powers where 2 raised to the minus one power is one over two raised to the plus one power, or 1/2 and so on. As you head to the right you get to fairly small numbers like 1/2048 ...
> 
> Every single binary fraction is thus a possibly infinite sum of negative powers of two, or rather the reciprocals of those in positive terms.
> 
> If you want to make 1/7, to some number of decimal places, it looks like this up to some point where I stop:
> 
> 0.00100100100100100101
> 
> So no halves, no quarters, 1/8, no sixteenths, no thirty-seconds, 1/64, and so on. But if you add all that up, and note the sequence was STOPPED before it could continue further, you get this translated into decimal:
> 
> 0.142857 55157470703125
> 
> Recall 1/7 in decimal notation is
> 0.142857 142857142857142857...
> 
> Note the divergence at the seventh digit after the decimal point. I left a space to show where they diverge. If I used more binary digits, I can get as close as I want but computers these days do not allow too many more digits unless you use highly specialized programs. There are packages that give you access such as "mpfr" but generally nothing can give you infinite precision. R will not handle an infinite number of infinitesimals.
> 
> The original problem that began our thread was about numbers like 0.1 and 0.2 and so on. In base ten, they look nice but I repeat in base 2 only powers of TWO reign.
> 
> 0.1 in base two is about 0.0001100110011001101
> 
> that reads as 1/16 + 1/32 + 1/256 + 1/512 + ...
> 
> If I convert the above segment, which I repeat was stopped short, I get 0.1000003814697265625 which is a tad over and had I taken the last 1 and changed it to a zero as in 0.0001100110011001100 then we would have a bit under at 0.09999847412109375
> 
> So the only way to write 0.1 exactly is to continue infinitely, again. Do the analysis and understand why most rational numbers will not easily convert to a small number of bits. But the advantages of computers doing operations in binary are huge and need not be explained. You may THINK you are entering numbers in decimal form but they rarely remain that way for long before they simply become binary and often remain binary unless and until you ask to print them out, usually in decimal.
> 
> BTW, I used a random web site to do the above conversion calculations:
> 
> https://www.rapidtables.com/convert/number/binary-to-decimal.html
> 
> Since I am writing in plain text, I cannot show what it says in the box on that page further down under Decimal Calculation Steps so I wonder what the rest of this message looks like:
> 
> (0.0001100110011001100)? = (0 ? 2?) + (0 ? 2??) + (0 ? 2??) + (0 ? 2??) + (1 ? 2??) + (1 ? 2??) + (0 ? 2??) + (0 ? 2??) + (1 ? 2??) + (1 ? 2??) + (0 ? 2???) + (0 ? 2???) + (1 ? 2???) + (1 ? 2???) + (0 ? 2???) + (0 ? 2???) + (1 ? 2???) + (1 ? 2???) + (0 ? 2???) + (0 ? 2???) = (0.09999847412109375)??
> 
> I think my part in this particular discussion can now finally come to an end. R and everything else can be incomplete. Deal with it!
> 
> -----Original Message-----
> From: Richard M. Heiberger <rmh at temple.edu>
> To: Avi Gross <avigross at verizon.net>
> Cc: nboeger at gmail.com <nboeger at gmail.com>; r-help at r-project.org <r-help at r-project.org>
> Sent: Tue, Feb 1, 2022 9:04 pm
> Subject: Re: [External] [R] Funky calculations
> 
> 
> I apologize if my tone came across wrong.? I enjoy reading your comments on this list.
> 
> My goal was to describe what the IEEE and R interpret "careful coding" to be.
> 
> 
>> On Feb 01, 2022, at 20:42, Avi Gross <avigross at verizon.net> wrote:
>>
>> Richard,
>>
>> I think it was fairly clear I was explaining how people do arithmetic manually and often truncate or round to some number of decimal places. I said nothing about what R does or what the IEEE standards say and I do not particularly care when making MY point.
>>
>> My point is that humans before computers also had trouble writing down any decimals that continue indefinitely. It cannot be expected computer versions of arithmetic can do much better. Different people can opt to do the calculation with the same or different numbers of digits ad when compared to each other they may not match.
>>
>> I do care what it does in my programs, of course. My goal here was to explain to someone that the anomaly found was not really an anomaly and that careful coding may be required in these situations.
>>
>>
>> -----Original Message-----
>> From: Richard M. Heiberger <rmh at temple.edu>
>> To: Avi Gross <avigross at verizon.net>
>> Cc: Nathan Boeger <nboeger at gmail.com>; r-help at r-project.org <r-help at r-project.org>
>> Sent: Tue, Feb 1, 2022 2:44 pm
>> Subject: Re: [External] [R] Funky calculations
>>
>>
>> RShowDoc('FAQ')
>>
>>
>> then search for 7.31
>>
>>
>> This statement
>> "If you stop at a 5 or 7 or 8 and back up to the previous digit, you round up. Else you leave the previous result alone."
>> is not quite right.? The recommendation in IEEE 754, and this is how R does arithmetic, is to Round Even.
>>
>> I ilustrate here with decimal, even though R and other programs use binary.
>>
>>> x <- c(1.4, 1.5, 1.6, 2.4, 2.5, 2.6, 3.4, 3.5, 3.6, 4.4, 4.5, 4.6)
>>> r <- round(x)
>>> cbind(x, r)
>>  ? ? ? ?  x r
>> [1,] 1.4 1
>> [2,] 1.5 2
>> [3,] 1.6 2
>> [4,] 2.4 2
>> [5,] 2.5 2
>> [6,] 2.6 3
>> [7,] 3.4 3
>> [8,] 3.5 4
>> [9,] 3.6 4
>> [10,] 4.4 4
>> [11,] 4.5 4
>> [12,] 4.6 5
>>>
>>
>> Numbers whose last digit is not 5 (when in decimal) round to the nearest integer.
>> Numbers who last digit is 5 (1.5, 2.5, 3.5, 4.5 above)
>> round to the nearest EVEN integer.
>> Hence 1.5 and 3.5 round up to the even numbers 2 and 4.
>> 2.5 and 4.5 round down do the even numbers 2 and 4.
>>
>> This way the round ups and downs average out to 0.? If we always went up from .5 we would have
>> an updrift over time.
>>
>> For even more detail click on the link in FAQ 7.31 to my appendix
>> https:// link.springer.com/content/pdf/bbm%3A978-1-4939-2122-5%2F1.pdf
>> and search for "Appendix G".
>>
>> Section G.5 explains Round to Even.
>> Sections G.6 onward illustrate specific examples, such as the one that started this email thread.
>>
>> Rich
>>
> 
> 
>   
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @@e||ck @end|ng |rom gm@||@com  Wed Feb  2 14:39:09 2022
From: @@e||ck @end|ng |rom gm@||@com (stephen sefick)
Date: Wed, 2 Feb 2022 08:39:09 -0500
Subject: [R] Funky calculations
In-Reply-To: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
Message-ID: <CADKEMqjLh+dcwvusYCCMqhUjqaL8pQaTV04NBptaoKN1aY26fQ@mail.gmail.com>

I have not looked into this, but maybe this is related to floating point
rounding?
Kindest regards,

Stephen Sefick, PhD

On Tue, Feb 1, 2022, 11:28 Nathan Boeger <nboeger at gmail.com> wrote:

> Hello,
>
> I found something strange and maybe I am going nuts but this does not make
> sense:
>
> >  (0.4 + 0.2 + 0 + 0.3 + 0 + 0.1) > 1
> [1] TRUE
>
> I tried it on my mac M1 (R v4.1.2) and my Linux box (R v4.0.4). If I use
> other values, it does not work (see below). It seems only that combination,
> granted I did not try them all.
>
> >  (0.4 + 0.2 + 0 + 0.2 + 0 + 0.2) > 1
> [1] FALSE
>
> Am I missing something?
>
> Cheers
>
> -nb
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From w||||@mwdun|@p @end|ng |rom gm@||@com  Wed Feb  2 15:23:54 2022
From: w||||@mwdun|@p @end|ng |rom gm@||@com (Bill Dunlap)
Date: Wed, 2 Feb 2022 06:23:54 -0800
Subject: [R] [External] Funky calculations
In-Reply-To: <CAFxA6Qr93D4VYjrZDvvrQgnTnToNQj3RQ21pLYrm8tJFHpA=1Q@mail.gmail.com>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
 <20220201193557.308209e8@Tarkus>
 <D24231AD-5CBF-4595-962E-2009821BD20B@temple.edu>
 <CAFxA6Qq1sGwh9ZwEaV6zvtfd0yq6A1W6U4oDMuy1KXs_CV9ryQ@mail.gmail.com>
 <CAHqSRuT7KsOnUEKQRz2+Y_Z0ZoE+y3LHm4q9JLzqsSATe-85iA@mail.gmail.com>
 <CAFxA6Qr93D4VYjrZDvvrQgnTnToNQj3RQ21pLYrm8tJFHpA=1Q@mail.gmail.com>
Message-ID: <CAHqSRuTcqLizbAFy2jRBKoqFTNuV6mi08au6ox2iwbLgWew1QA@mail.gmail.com>

Floats have 23 bits of precision so the rounding is done there instead of
at 52 bits, hence a different example is needed to show the problem with
floats.

bill at Bill-T490:~$ cat b.c
#include <stdio.h>

int main(int argc, char* argv[])
{
    float d = 0.4 + 0.4 + 0.4 + 0.4;
    printf("0.4+0.4+0.4+0.4 -> %24.17g\n", (double)d);
    printf("0.4+0.4+0.4+0.4 == 1.6 -> %s\n", d == 1.0 ? "true" : "false");
    return 0;
}
bill at Bill-T490:~$ gcc b.c
bill at Bill-T490:~$ ./a.out
0.4+0.4+0.4+0.4 ->       1.6000000238418579
0.4+0.4+0.4+0.4 == 1.6 -> false

There is no getting around the fact that rounding will happen.

-Bill

On Tue, Feb 1, 2022 at 9:06 PM Nathan Boeger <nboeger at gmail.com> wrote:

>
> I understand this and with C the data type used is important. For this
> type of calculation, I would normally use a float (basic single precision
> is all I require).
>
> #include <stdio.h>
>
> void main() {
>   float foo = (0.4 + 0.2 + 0.30 + 0.1) ;
>   printf("foo: %f , foo > 1: %s \n", foo, (foo > 1.0 ? "true" : "false"));
>   double bar = (0.4 + 0.2 + 0.30 + 0.1) ;
>   printf("bar: %lf , bar > 1: %s \n", bar, (bar > 1.0 ? "true" : "false"));
> }
>
> gcc  c-check.c -o c-check
> ./c-check
> foo: 1.000000 , foo > 1: false
> bar: 1.000000 , bar > 1: true
>
> Again, it was my mistake for not reading the R-FAQ. I had no idea it would
> spark such a long thread.
>
> Cheers
>
> -nb
>
> On Wed, 2 Feb 2022 at 10:30, Bill Dunlap <williamwdunlap at gmail.com> wrote:
>
>> The base 2 representation of 0.4 repeats the digit sequence 1001
>> infinitely, hence must be rounded.  The problem occurs in C the same as it
>> does in R.
>>
>> bill at Bill-T490:~$ cat a.c
>> #include <stdio.h>
>>
>> int main(int argc, char* argv[])
>> {
>>     double d = 0.4 + 0.3 + 0.2 + 0.1;
>>     printf("0.4+0.3+0.2+0.1 -> %24.17g\n", d);
>>     printf("0.4+0.3+0.2+0.1 == 1.0 -> %s\n", d == 1.0 ? "true" : "false");
>>     return 0;
>> }
>> bill at Bill-T490:~$ gcc a.c
>> bill at Bill-T490:~$ ./a.out
>> 0.4+0.3+0.2+0.1 ->      0.99999999999999989
>> 0.4+0.3+0.2+0.1 == 1.0 -> false
>>
>> -Bill
>>
>> On Tue, Feb 1, 2022 at 7:01 PM Nathan Boeger <nboeger at gmail.com> wrote:
>>
>>> Thank you for this explanation!
>>>
>>> I have a long background in C/C++ and never realized this was such an
>>> issue
>>> with some languages. At least, with trivial single digit decimals. I
>>> understand accuracy issues with very large decimals, repeating or
>>> non-terminating rationals and I have handled them in the past. It makes
>>> me
>>> worried about all the R scripts I have written before (yikes!).
>>>
>>> Cheers
>>>
>>> -nb
>>>
>>> On Wed, 2 Feb 2022 at 02:44, Richard M. Heiberger <rmh at temple.edu>
>>> wrote:
>>>
>>> > RShowDoc('FAQ')
>>> >
>>> > then search for 7.31
>>> >
>>> >
>>> > This statement
>>> > "If you stop at a 5 or 7 or 8 and back up to the previous digit, you
>>> round
>>> > up. Else you leave the previous result alone."
>>> > is not quite right.  The recommendation in IEEE 754, and this is how R
>>> > does arithmetic, is to Round Even.
>>> >
>>> > I ilustrate here with decimal, even though R and other programs use
>>> binary.
>>> >
>>> > > x <- c(1.4, 1.5, 1.6, 2.4, 2.5, 2.6, 3.4, 3.5, 3.6, 4.4, 4.5, 4.6)
>>> > > r <- round(x)
>>> > > cbind(x, r)
>>> >         x r
>>> >  [1,] 1.4 1
>>> >  [2,] 1.5 2
>>> >  [3,] 1.6 2
>>> >  [4,] 2.4 2
>>> >  [5,] 2.5 2
>>> >  [6,] 2.6 3
>>> >  [7,] 3.4 3
>>> >  [8,] 3.5 4
>>> >  [9,] 3.6 4
>>> > [10,] 4.4 4
>>> > [11,] 4.5 4
>>> > [12,] 4.6 5
>>> > >
>>> >
>>> > Numbers whose last digit is not 5 (when in decimal) round to the
>>> nearest
>>> > integer.
>>> > Numbers who last digit is 5 (1.5, 2.5, 3.5, 4.5 above)
>>> > round to the nearest EVEN integer.
>>> > Hence 1.5 and 3.5 round up to the even numbers 2 and 4.
>>> > 2.5 and 4.5 round down do the even numbers 2 and 4.
>>> >
>>> > This way the round ups and downs average out to 0.  If we always went
>>> up
>>> > from .5 we would have
>>> > an updrift over time.
>>> >
>>> > For even more detail click on the link in FAQ 7.31 to my appendix
>>> > https:// link.springer.com/content/pdf/bbm%3A978-1-4939-2122-5%2F1.pdf
>>> > and search for "Appendix G".
>>> >
>>> > Section G.5 explains Round to Even.
>>> > Sections G.6 onward illustrate specific examples, such as the one that
>>> > started this email thread.
>>> >
>>> > Rich
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>

	[[alternative HTML version deleted]]


From @v|gro@@ @end|ng |rom ver|zon@net  Wed Feb  2 19:07:02 2022
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Wed, 2 Feb 2022 18:07:02 +0000 (UTC)
Subject: [R] [External] Funky calculations
In-Reply-To: <f7f69f1b-e19c-42e3-863f-27681a8fb618@gmail.com>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
 <20220201193557.308209e8@Tarkus>
 <D24231AD-5CBF-4595-962E-2009821BD20B@temple.edu>
 <1240726547.2663210.1643766132419@mail.yahoo.com>
 <1A13EFE0-C500-40D0-9BD0-8546E4EECBEA@temple.edu>
 <801628629.2676628.1643773508952@mail.yahoo.com>
 <f7f69f1b-e19c-42e3-863f-27681a8fb618@gmail.com>
Message-ID: <1219089011.2808560.1643825222712@mail.yahoo.com>

JC,

Interesting. People often misunderstand standards and the need for them as well as the way they are a compromise by lots of people representing different interests. You say you were one of 31, but in my experience, many more people are involved than the ones who show up. I had some involvements with standards back in the nineties and sometimes the results made sense and sometimes we ended up with designing a camel with stripes while trying to design a horse.

The issue here that needs repeating is that some things are basically impossible and the best we can do is to go part-way and make it as sane as we can.

Binary is NOT a must but for now it is. Electronic circuits that work in two modes we call zero and one are not the only choices and we may someday work with more complex circuits. Some mechanical devices have been built that work in a decimal manner. But given what we are using, a standard that lets you work with numbers to a reasonable level of representation is the best we can do. As has been pointed out, mathematically, only selected real numbers can be represented exactly in base two, and even fewer if you limit the number of bits you can use.

So floating point can be used with caution. I have seen some problems where it was chosen to deliberately convert all the numbers used to integers by say changing from measuring in continuous ways in meters to measuring in millimeters with no fractional millimeters allowed. All calculations done (excluding standard division) could now be compared to others without ambiguity. But for most cases, this is not necessary or even possible.

-----Original Message-----
From: J C Nash <profjcnash at gmail.com>
To: r-help at r-project.org
Sent: Wed, Feb 2, 2022 8:35 am
Subject: Re: [R] [External] Funky calculations


I was one of the 31 names on the 1985 IEEE standard. If anyone thinks things are awkward now,
try looking at the swamp we had beforehand.

What I believe IS useful is to provide examples and to explain them in tutorial fashion.
We need to recognize that our computations have limitations. Most common computing platforms
use IEEE binary arithmetic, but not all.

This was much more "in our face" when we used slide rules or hand-crank calculators. I still
have slide rules and a Monroe "Portable" calculator -- 5 kg! It's worth bringing them out every
so often and being thankful for the power and speed of modern computing, while remembering to
watch for the cowpads of REAL and REAL*8 arithmetic.

JN

On 2022-02-01 22:45, Avi Gross via R-help wrote:
> This is a discussion forum, Richard, and I welcome requests to clarify what I wrote or to be corrected, especially when my words have been read with an attempt to understand. I do get private responses too and some days i wonder if I am not communicating the way people like!
> 
> But let me repeat. The question we started with asked about R. My answer applies to quite a few languages besides R and maybe just about all of them.
> 
> I got private email insisting the numbers being added were not irrational so why would they not be represented easily as a sum. I know my answers included parts at various levels of abstraction as well as examples of cases when Decimals notation for a number like 1/7 results in an infinite repeating sequence. So, I think it wise to follow up with what binary looks like and why hardly ANYTHING that looks reasonable is hard to represent exactly.
> 
> Consider that binary means POWERS OF TWO. The sequence 1101 before a decimal point means (starting from the right and heading left) that you have one ONES and no TWOS and one FOURS and one EIGHTS. Powers of two ranging from 2 to the zero power to two cubed. You can make any integer whatsoever using as long a sequence of zeros and ones as you like. Compare this to decimal notation where you use powers of ten and of course can use any of 0-9.
> 
> But looking at fractional numbers, like 1/7 and 1/10, it gets hard and inexact.
> 
> Remember now we are in BINARY. Here are some fractions with everything not shown to the right being zeros and thus not needed to be shown explicitly. Starting with the decimal point, read this from left to right to see the powers in the denominator rising so 1/2 then 1/4 then 1/8 ...:
> 
> 0.0 would be 0.
> 0.1 would be 1/2
> 0.101 would be 1/2 + 1/8 or 5/8
> 0.11 would be 1/2 + 1/4 or 3/4
> 0.111 would be 1/2 + 1/4 + 1/8 or 7/8
> 
> We are now using negative powers where 2 raised to the minus one power is one over two raised to the plus one power, or 1/2 and so on. As you head to the right you get to fairly small numbers like 1/2048 ...
> 
> Every single binary fraction is thus a possibly infinite sum of negative powers of two, or rather the reciprocals of those in positive terms.
> 
> If you want to make 1/7, to some number of decimal places, it looks like this up to some point where I stop:
> 
> 0.00100100100100100101
> 
> So no halves, no quarters, 1/8, no sixteenths, no thirty-seconds, 1/64, and so on. But if you add all that up, and note the sequence was STOPPED before it could continue further, you get this translated into decimal:
> 
> 0.142857 55157470703125
> 
> Recall 1/7 in decimal notation is
> 0.142857 142857142857142857...
> 
> Note the divergence at the seventh digit after the decimal point. I left a space to show where they diverge. If I used more binary digits, I can get as close as I want but computers these days do not allow too many more digits unless you use highly specialized programs. There are packages that give you access such as "mpfr" but generally nothing can give you infinite precision. R will not handle an infinite number of infinitesimals.
> 
> The original problem that began our thread was about numbers like 0.1 and 0.2 and so on. In base ten, they look nice but I repeat in base 2 only powers of TWO reign.
> 
> 0.1 in base two is about 0.0001100110011001101
> 
> that reads as 1/16 + 1/32 + 1/256 + 1/512 + ...
> 
> If I convert the above segment, which I repeat was stopped short, I get 0.1000003814697265625 which is a tad over and had I taken the last 1 and changed it to a zero as in 0.0001100110011001100 then we would have a bit under at 0.09999847412109375
> 
> So the only way to write 0.1 exactly is to continue infinitely, again. Do the analysis and understand why most rational numbers will not easily convert to a small number of bits. But the advantages of computers doing operations in binary are huge and need not be explained. You may THINK you are entering numbers in decimal form but they rarely remain that way for long before they simply become binary and often remain binary unless and until you ask to print them out, usually in decimal.
> 
> BTW, I used a random web site to do the above conversion calculations:
> 
> https://www.rapidtables.com/convert/number/binary-to-decimal.html
> 
> Since I am writing in plain text, I cannot show what it says in the box on that page further down under Decimal Calculation Steps so I wonder what the rest of this message looks like:
> 
> (0.0001100110011001100)? = (0 ? 2?) + (0 ? 2??) + (0 ? 2??) + (0 ? 2??) + (1 ? 2??) + (1 ? 2??) + (0 ? 2??) + (0 ? 2??) + (1 ? 2??) + (1 ? 2??) + (0 ? 2???) + (0 ? 2???) + (1 ? 2???) + (1 ? 2???) + (0 ? 2???) + (0 ? 2???) + (1 ? 2???) + (1 ? 2???) + (0 ? 2???) + (0 ? 2???) = (0.09999847412109375)??
> 
> I think my part in this particular discussion can now finally come to an end. R and everything else can be incomplete. Deal with it!
> 
> -----Original Message-----
> From: Richard M. Heiberger <rmh at temple.edu>
> To: Avi Gross <avigross at verizon.net>
> Cc: nboeger at gmail.com <nboeger at gmail.com>; r-help at r-project.org <r-help at r-project.org>
> Sent: Tue, Feb 1, 2022 9:04 pm
> Subject: Re: [External] [R] Funky calculations
> 
> 
> I apologize if my tone came across wrong.? I enjoy reading your comments on this list.
> 
> My goal was to describe what the IEEE and R interpret "careful coding" to be.
> 
> 
>> On Feb 01, 2022, at 20:42, Avi Gross <avigross at verizon.net> wrote:
>>
>> Richard,
>>
>> I think it was fairly clear I was explaining how people do arithmetic manually and often truncate or round to some number of decimal places. I said nothing about what R does or what the IEEE standards say and I do not particularly care when making MY point.
>>
>> My point is that humans before computers also had trouble writing down any decimals that continue indefinitely. It cannot be expected computer versions of arithmetic can do much better. Different people can opt to do the calculation with the same or different numbers of digits ad when compared to each other they may not match.
>>
>> I do care what it does in my programs, of course. My goal here was to explain to someone that the anomaly found was not really an anomaly and that careful coding may be required in these situations.
>>
>>
>> -----Original Message-----
>> From: Richard M. Heiberger <rmh at temple.edu>
>> To: Avi Gross <avigross at verizon.net>
>> Cc: Nathan Boeger <nboeger at gmail.com>; r-help at r-project.org <r-help at r-project.org>
>> Sent: Tue, Feb 1, 2022 2:44 pm
>> Subject: Re: [External] [R] Funky calculations
>>
>>
>> RShowDoc('FAQ')
>>
>>
>> then search for 7.31
>>
>>
>> This statement
>> "If you stop at a 5 or 7 or 8 and back up to the previous digit, you round up. Else you leave the previous result alone."
>> is not quite right.? The recommendation in IEEE 754, and this is how R does arithmetic, is to Round Even.
>>
>> I ilustrate here with decimal, even though R and other programs use binary.
>>
>>> x <- c(1.4, 1.5, 1.6, 2.4, 2.5, 2.6, 3.4, 3.5, 3.6, 4.4, 4.5, 4.6)
>>> r <- round(x)
>>> cbind(x, r)
>>? ? ? ? ?? x r
>> [1,] 1.4 1
>> [2,] 1.5 2
>> [3,] 1.6 2
>> [4,] 2.4 2
>> [5,] 2.5 2
>> [6,] 2.6 3
>> [7,] 3.4 3
>> [8,] 3.5 4
>> [9,] 3.6 4
>> [10,] 4.4 4
>> [11,] 4.5 4
>> [12,] 4.6 5
>>>
>>
>> Numbers whose last digit is not 5 (when in decimal) round to the nearest integer.
>> Numbers who last digit is 5 (1.5, 2.5, 3.5, 4.5 above)
>> round to the nearest EVEN integer.
>> Hence 1.5 and 3.5 round up to the even numbers 2 and 4.
>> 2.5 and 4.5 round down do the even numbers 2 and 4.
>>
>> This way the round ups and downs average out to 0.? If we always went up from .5 we would have
>> an updrift over time.
>>
>> For even more detail click on the link in FAQ 7.31 to my appendix
>> https:// link.springer.com/content/pdf/bbm%3A978-1-4939-2122-5%2F1.pdf
>> and search for "Appendix G".
>>
>> Section G.5 explains Round to Even.
>> Sections G.6 onward illustrate specific examples, such as the one that started this email thread.
>>
>> Rich


From tebert @end|ng |rom u||@edu  Wed Feb  2 19:27:28 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Wed, 2 Feb 2022 18:27:28 +0000
Subject: [R] [External] Funky calculations
In-Reply-To: <f7f69f1b-e19c-42e3-863f-27681a8fb618@gmail.com>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
 <20220201193557.308209e8@Tarkus>
 <D24231AD-5CBF-4595-962E-2009821BD20B@temple.edu>
 <1240726547.2663210.1643766132419@mail.yahoo.com>
 <1A13EFE0-C500-40D0-9BD0-8546E4EECBEA@temple.edu>
 <801628629.2676628.1643773508952@mail.yahoo.com>
 <f7f69f1b-e19c-42e3-863f-27681a8fb618@gmail.com>
Message-ID: <BN6PR2201MB155343686950421C0B16BDFCCF279@BN6PR2201MB1553.namprd22.prod.outlook.com>

Punch cards and reams of green and white striped fanfold. A portable Cromemco we called a boat anchor, and hardly used because the Apple II was more in line with our computing tasks/skills. The relevant part is learning assembly and machine language on the Apple. Trying to describe an infinite value in a finite space results in inaccuracy no matter how you work it. The problem applies to all programs in all languages. The problem, while present, does not always matter. One way to identify the limits of accuracy is something like this:
1 + 1 == 2   #  returns TRUE
1 + 0.9 == 2 # returns FALSE
1 + 0.9999999 == 2 should return FALSE
Keep going and at some point you will get TRUE.
In my version of R, here is the last time I get FALSE.
1+ 0.999999999999999 == 2    #15 digits after decimal place

I can do something similar in Excel. In printing Excel rounds up to 1 when 10 nines are present. The if test quits returning FALSE after 14 decimal places. =if(1 + 0.9999999 = 2, "TRUE","FALSE")

One can play the game another way. This matters because it is NOT the number of digits after the decimal point, it is the total number of digits.

Here is the last correct answer in this game:
999999999999999 == 1000000000000000
That is 15 digits, add another 9 on the left and a zero on the right and you get TRUE.

123456789 + 0.00000001 == 123456789
Here is the last FALSE, but note that I have only added a number with 8 decimal places.

There are many other ways to play this game:
log10(10.00000000000001)==1 # add a zero before the decimal point, how many can one have after?

In statistics it is well know that it is a bad idea to model data that spans many orders of magnitude. At least for me, this is the simplest presentation for why this is the case. For a more exact reason consider the above results when taking the inverse of a matrix with elements that differ by several orders of magnitude.

The issue is well known, and programs like R and SAS and SPSS, (etc...) take care of the issue, up to a point. However, it is not hard to break the system if you try or are not careful. The point at which the system breaks is easy to identify in simple terms. However, just because R's limit is 15 digits does not mean that all packages in R will also have that limit. Just keep in mind that your data range in R needs to be such that the operations on that data all fit within this 15 digit limitation. Other programs may have different limitations, but there is always a limitation.

Tim 



-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of J C Nash
Sent: Wednesday, February 2, 2022 8:36 AM
To: r-help at r-project.org
Subject: Re: [R] [External] Funky calculations

[External Email]

I was one of the 31 names on the 1985 IEEE standard. If anyone thinks things are awkward now, try looking at the swamp we had beforehand.

What I believe IS useful is to provide examples and to explain them in tutorial fashion.
We need to recognize that our computations have limitations. Most common computing platforms use IEEE binary arithmetic, but not all.

This was much more "in our face" when we used slide rules or hand-crank calculators. I still have slide rules and a Monroe "Portable" calculator -- 5 kg! It's worth bringing them out every so often and being thankful for the power and speed of modern computing, while remembering to watch for the cowpads of REAL and REAL*8 arithmetic.

JN

On 2022-02-01 22:45, Avi Gross via R-help wrote:
> This is a discussion forum, Richard, and I welcome requests to clarify what I wrote or to be corrected, especially when my words have been read with an attempt to understand. I do get private responses too and some days i wonder if I am not communicating the way people like!
>
> But let me repeat. The question we started with asked about R. My answer applies to quite a few languages besides R and maybe just about all of them.
>
> I got private email insisting the numbers being added were not irrational so why would they not be represented easily as a sum. I know my answers included parts at various levels of abstraction as well as examples of cases when Decimals notation for a number like 1/7 results in an infinite repeating sequence. So, I think it wise to follow up with what binary looks like and why hardly ANYTHING that looks reasonable is hard to represent exactly.
>
> Consider that binary means POWERS OF TWO. The sequence 1101 before a decimal point means (starting from the right and heading left) that you have one ONES and no TWOS and one FOURS and one EIGHTS. Powers of two ranging from 2 to the zero power to two cubed. You can make any integer whatsoever using as long a sequence of zeros and ones as you like. Compare this to decimal notation where you use powers of ten and of course can use any of 0-9.
>
> But looking at fractional numbers, like 1/7 and 1/10, it gets hard and inexact.
>
> Remember now we are in BINARY. Here are some fractions with everything not shown to the right being zeros and thus not needed to be shown explicitly. Starting with the decimal point, read this from left to right to see the powers in the denominator rising so 1/2 then 1/4 then 1/8 ...:
>
> 0.0 would be 0.
> 0.1 would be 1/2
> 0.101 would be 1/2 + 1/8 or 5/8
> 0.11 would be 1/2 + 1/4 or 3/4
> 0.111 would be 1/2 + 1/4 + 1/8 or 7/8
>
> We are now using negative powers where 2 raised to the minus one power is one over two raised to the plus one power, or 1/2 and so on. As you head to the right you get to fairly small numbers like 1/2048 ...
>
> Every single binary fraction is thus a possibly infinite sum of negative powers of two, or rather the reciprocals of those in positive terms.
>
> If you want to make 1/7, to some number of decimal places, it looks like this up to some point where I stop:
>
> 0.00100100100100100101
>
> So no halves, no quarters, 1/8, no sixteenths, no thirty-seconds, 1/64, and so on. But if you add all that up, and note the sequence was STOPPED before it could continue further, you get this translated into decimal:
>
> 0.142857 55157470703125
>
> Recall 1/7 in decimal notation is
> 0.142857 142857142857142857...
>
> Note the divergence at the seventh digit after the decimal point. I left a space to show where they diverge. If I used more binary digits, I can get as close as I want but computers these days do not allow too many more digits unless you use highly specialized programs. There are packages that give you access such as "mpfr" but generally nothing can give you infinite precision. R will not handle an infinite number of infinitesimals.
>
> The original problem that began our thread was about numbers like 0.1 and 0.2 and so on. In base ten, they look nice but I repeat in base 2 only powers of TWO reign.
>
> 0.1 in base two is about 0.0001100110011001101
>
> that reads as 1/16 + 1/32 + 1/256 + 1/512 + ...
>
> If I convert the above segment, which I repeat was stopped short, I 
> get 0.1000003814697265625 which is a tad over and had I taken the last 
> 1 and changed it to a zero as in 0.0001100110011001100 then we would 
> have a bit under at 0.09999847412109375
>
> So the only way to write 0.1 exactly is to continue infinitely, again. Do the analysis and understand why most rational numbers will not easily convert to a small number of bits. But the advantages of computers doing operations in binary are huge and need not be explained. You may THINK you are entering numbers in decimal form but they rarely remain that way for long before they simply become binary and often remain binary unless and until you ask to print them out, usually in decimal.
>
> BTW, I used a random web site to do the above conversion calculations:
>
> https://urldefense.proofpoint.com/v2/url?u=https-3A__www.rapidtables.c
> om_convert_number_binary-2Dto-2Ddecimal.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3
> EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=m7vgw0V3XLMYZ26IDQFrX6s6LeffgRVmWQ
> d4FGv5GUaI_W1jJ_i7QSJKQrB8fON4&s=_HQxpbll2YuLGvrwfc0AJPK4NAQ9xCcqvfl0c
> 1i-8Tg&e=
>
> Since I am writing in plain text, I cannot show what it says in the box on that page further down under Decimal Calculation Steps so I wonder what the rest of this message looks like:
>
> (0.0001100110011001100)? = (0 ? 2?) + (0 ? 2??) + (0 ? 2??) + (0 ? 
> 2??) + (1 ? 2??) + (1 ? 2??) + (0 ? 2??) + (0 ? 2??) + (1 ? 2??) + (1 
> ? 2??) + (0 ? 2???) + (0 ? 2???) + (1 ? 2???) + (1 ? 2???) + (0 ? 
> 2???) + (0 ? 2???) + (1 ? 2???) + (1 ? 2???) + (0 ? 2???) + (0 ? 2???) 
> = (0.09999847412109375)??
>
> I think my part in this particular discussion can now finally come to an end. R and everything else can be incomplete. Deal with it!
>
> -----Original Message-----
> From: Richard M. Heiberger <rmh at temple.edu>
> To: Avi Gross <avigross at verizon.net>
> Cc: nboeger at gmail.com <nboeger at gmail.com>; r-help at r-project.org 
> <r-help at r-project.org>
> Sent: Tue, Feb 1, 2022 9:04 pm
> Subject: Re: [External] [R] Funky calculations
>
>
> I apologize if my tone came across wrong.  I enjoy reading your comments on this list.
>
> My goal was to describe what the IEEE and R interpret "careful coding" to be.
>
>
>> On Feb 01, 2022, at 20:42, Avi Gross <avigross at verizon.net> wrote:
>>
>> Richard,
>>
>> I think it was fairly clear I was explaining how people do arithmetic manually and often truncate or round to some number of decimal places. I said nothing about what R does or what the IEEE standards say and I do not particularly care when making MY point.
>>
>> My point is that humans before computers also had trouble writing down any decimals that continue indefinitely. It cannot be expected computer versions of arithmetic can do much better. Different people can opt to do the calculation with the same or different numbers of digits ad when compared to each other they may not match.
>>
>> I do care what it does in my programs, of course. My goal here was to explain to someone that the anomaly found was not really an anomaly and that careful coding may be required in these situations.
>>
>>
>> -----Original Message-----
>> From: Richard M. Heiberger <rmh at temple.edu>
>> To: Avi Gross <avigross at verizon.net>
>> Cc: Nathan Boeger <nboeger at gmail.com>; r-help at r-project.org 
>> <r-help at r-project.org>
>> Sent: Tue, Feb 1, 2022 2:44 pm
>> Subject: Re: [External] [R] Funky calculations
>>
>>
>> RShowDoc('FAQ')
>>
>>
>> then search for 7.31
>>
>>
>> This statement
>> "If you stop at a 5 or 7 or 8 and back up to the previous digit, you round up. Else you leave the previous result alone."
>> is not quite right.  The recommendation in IEEE 754, and this is how R does arithmetic, is to Round Even.
>>
>> I ilustrate here with decimal, even though R and other programs use binary.
>>
>>> x <- c(1.4, 1.5, 1.6, 2.4, 2.5, 2.6, 3.4, 3.5, 3.6, 4.4, 4.5, 4.6) r 
>>> <- round(x) cbind(x, r)
>>           x r
>> [1,] 1.4 1
>> [2,] 1.5 2
>> [3,] 1.6 2
>> [4,] 2.4 2
>> [5,] 2.5 2
>> [6,] 2.6 3
>> [7,] 3.4 3
>> [8,] 3.5 4
>> [9,] 3.6 4
>> [10,] 4.4 4
>> [11,] 4.5 4
>> [12,] 4.6 5
>>>
>>
>> Numbers whose last digit is not 5 (when in decimal) round to the nearest integer.
>> Numbers who last digit is 5 (1.5, 2.5, 3.5, 4.5 above) round to the 
>> nearest EVEN integer.
>> Hence 1.5 and 3.5 round up to the even numbers 2 and 4.
>> 2.5 and 4.5 round down do the even numbers 2 and 4.
>>
>> This way the round ups and downs average out to 0.  If we always went 
>> up from .5 we would have an updrift over time.
>>
>> For even more detail click on the link in FAQ 7.31 to my appendix 
>> https:// 
>> link.springer.com/content/pdf/bbm%3A978-1-4939-2122-5%2F1.pdf
>> and search for "Appendix G".
>>
>> Section G.5 explains Round to Even.
>> Sections G.6 onward illustrate specific examples, such as the one that started this email thread.
>>
>> Rich
>>
>
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=m7vgw0V3XLMYZ26IDQFrX6s6LeffgRVmWQd4FGv5GUaI_W1jJ_i7QSJKQ
> rB8fON4&s=HX4jIGehR02lpSThBSeyCJjGktkizJEOcq7FpBq2mOY&e=
> PLEASE do read the posting guide 
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=m7vgw0V3XLMYZ26IDQFrX6s6LeffgRVmWQd4FGv5GUaI_W1jJ_i7QSJK
> QrB8fON4&s=GDNe-9c0Z0dm3zV0Nqlnil_EDvpqQgJybFt5DdfZ3v4&e=
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=m7vgw0V3XLMYZ26IDQFrX6s6LeffgRVmWQd4FGv5GUaI_W1jJ_i7QSJKQrB8fON4&s=HX4jIGehR02lpSThBSeyCJjGktkizJEOcq7FpBq2mOY&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=m7vgw0V3XLMYZ26IDQFrX6s6LeffgRVmWQd4FGv5GUaI_W1jJ_i7QSJKQrB8fON4&s=GDNe-9c0Z0dm3zV0Nqlnil_EDvpqQgJybFt5DdfZ3v4&e=
and provide commented, minimal, self-contained, reproducible code.

From @v|gro@@ @end|ng |rom ver|zon@net  Wed Feb  2 20:23:47 2022
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Wed, 2 Feb 2022 19:23:47 +0000 (UTC)
Subject: [R] [External] Funky calculations
In-Reply-To: <BN6PR2201MB155343686950421C0B16BDFCCF279@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
 <20220201193557.308209e8@Tarkus>
 <D24231AD-5CBF-4595-962E-2009821BD20B@temple.edu>
 <1240726547.2663210.1643766132419@mail.yahoo.com>
 <1A13EFE0-C500-40D0-9BD0-8546E4EECBEA@temple.edu>
 <801628629.2676628.1643773508952@mail.yahoo.com>
 <f7f69f1b-e19c-42e3-863f-27681a8fb618@gmail.com>
 <BN6PR2201MB155343686950421C0B16BDFCCF279@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <88067166.2834644.1643829827893@mail.yahoo.com>

As discussed, Tim, your version of R has already built-in all kinds of values and limits and even ways to do approximate tests in the variable .Machine such as:

> .Machine$double.eps
[1] 2.220446e-16
> .Machine$sizeof.longdouble
[1] 16
> .Machine$double.eps
[1] 2.220446e-16
> .Machine$longdouble.eps
[1] 1.084202e-19

If you can figure out which ones correspond to what you are looking for, you can stay within bounds or set up your code to catch the rare cases when you are in a gray area where comparisons are not reliable and assume all other areas are valid for tests of equality or inequality.

Consider another common problem. You want to multiply two integers that are stored in integer variables of some capacity ranging from a byte to 4 or even more bytes. On many machines, if the multiplication needs even more digits than are available, the result is to return just the bits that fit and toss the others away. Your result is effectively nonsense. You may also throw an error and have to catch it.

But if your program runs portably on many machines, how can you be sure it worked properly if you multiply say a billion by a billion in an integer that only fits the following number on my machine at this time:

> .Machine$integer.max
[1] 2147483647

The above number is a bit more than 2 billion so clearly multiplying those two numbers will not fit. One solution, and not a great one, is to verify that each number being multiplied is less than the square root of the limit. Another is to do the multiplication and then divide the result by one of the multiplicands and verify the result is the same as the other multiplicand. If not, you had a problem, probably an overflow situation.

It should not be necessary to do this and in some ways it isn't. There are probably R packages supporting larger integers (and I don't mean by converting to floating point) and I know Python integers already are of any length and only limited by available memory.

But it is an example of how real computers using current methods are not mathematically whole and you can not demand they do what is mathematically possible and especially not in areas where things are truly meant to be continuous with no gaps or of infinite duration in various ways. The real world is not the ideal Platonic Realm.

Now I have to think about what may happen with quantum computers and algorithms using them. QBITS have a certain sense of infinity in them and can be a sort of superposition of many quantum states and thus may turn out to allow arbitrary precision, unless they hit some quantum wall.

But that is a topic for some futuristic R.



-----Original Message-----
From: Ebert,Timothy Aaron <tebert at ufl.edu>
To: J C Nash <profjcnash at gmail.com>; r-help at r-project.org <r-help at r-project.org>
Sent: Wed, Feb 2, 2022 1:27 pm
Subject: Re: [R] [External] Funky calculations


Punch cards and reams of green and white striped fanfold. A portable Cromemco we called a boat anchor, and hardly used because the Apple II was more in line with our computing tasks/skills. The relevant part is learning assembly and machine language on the Apple. Trying to describe an infinite value in a finite space results in inaccuracy no matter how you work it. The problem applies to all programs in all languages. The problem, while present, does not always matter. One way to identify the limits of accuracy is something like this:
1 + 1 == 2?  #? returns TRUE
1 + 0.9 == 2 # returns FALSE
1 + 0.9999999 == 2 should return FALSE
Keep going and at some point you will get TRUE.
In my version of R, here is the last time I get FALSE.
1+ 0.999999999999999 == 2? ? #15 digits after decimal place

I can do something similar in Excel. In printing Excel rounds up to 1 when 10 nines are present. The if test quits returning FALSE after 14 decimal places. =if(1 + 0.9999999 = 2, "TRUE","FALSE")

One can play the game another way. This matters because it is NOT the number of digits after the decimal point, it is the total number of digits.

Here is the last correct answer in this game:
999999999999999 == 1000000000000000
That is 15 digits, add another 9 on the left and a zero on the right and you get TRUE.

123456789 + 0.00000001 == 123456789
Here is the last FALSE, but note that I have only added a number with 8 decimal places.

There are many other ways to play this game:
log10(10.00000000000001)==1 # add a zero before the decimal point, how many can one have after?

In statistics it is well know that it is a bad idea to model data that spans many orders of magnitude. At least for me, this is the simplest presentation for why this is the case. For a more exact reason consider the above results when taking the inverse of a matrix with elements that differ by several orders of magnitude.

The issue is well known, and programs like R and SAS and SPSS, (etc...) take care of the issue, up to a point. However, it is not hard to break the system if you try or are not careful. The point at which the system breaks is easy to identify in simple terms. However, just because R's limit is 15 digits does not mean that all packages in R will also have that limit. Just keep in mind that your data range in R needs to be such that the operations on that data all fit within this 15 digit limitation. Other programs may have different limitations, but there is always a limitation.

Tim 



-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of J C Nash
Sent: Wednesday, February 2, 2022 8:36 AM
To: r-help at r-project.org
Subject: Re: [R] [External] Funky calculations

[External Email]

I was one of the 31 names on the 1985 IEEE standard. If anyone thinks things are awkward now, try looking at the swamp we had beforehand.

What I believe IS useful is to provide examples and to explain them in tutorial fashion.
We need to recognize that our computations have limitations. Most common computing platforms use IEEE binary arithmetic, but not all.

This was much more "in our face" when we used slide rules or hand-crank calculators. I still have slide rules and a Monroe "Portable" calculator -- 5 kg! It's worth bringing them out every so often and being thankful for the power and speed of modern computing, while remembering to watch for the cowpads of REAL and REAL*8 arithmetic.

JN

On 2022-02-01 22:45, Avi Gross via R-help wrote:
> This is a discussion forum, Richard, and I welcome requests to clarify what I wrote or to be corrected, especially when my words have been read with an attempt to understand. I do get private responses too and some days i wonder if I am not communicating the way people like!
>
> But let me repeat. The question we started with asked about R. My answer applies to quite a few languages besides R and maybe just about all of them.
>
> I got private email insisting the numbers being added were not irrational so why would they not be represented easily as a sum. I know my answers included parts at various levels of abstraction as well as examples of cases when Decimals notation for a number like 1/7 results in an infinite repeating sequence. So, I think it wise to follow up with what binary looks like and why hardly ANYTHING that looks reasonable is hard to represent exactly.
>
> Consider that binary means POWERS OF TWO. The sequence 1101 before a decimal point means (starting from the right and heading left) that you have one ONES and no TWOS and one FOURS and one EIGHTS. Powers of two ranging from 2 to the zero power to two cubed. You can make any integer whatsoever using as long a sequence of zeros and ones as you like. Compare this to decimal notation where you use powers of ten and of course can use any of 0-9.
>
> But looking at fractional numbers, like 1/7 and 1/10, it gets hard and inexact.
>
> Remember now we are in BINARY. Here are some fractions with everything not shown to the right being zeros and thus not needed to be shown explicitly. Starting with the decimal point, read this from left to right to see the powers in the denominator rising so 1/2 then 1/4 then 1/8 ...:
>
> 0.0 would be 0.
> 0.1 would be 1/2
> 0.101 would be 1/2 + 1/8 or 5/8
> 0.11 would be 1/2 + 1/4 or 3/4
> 0.111 would be 1/2 + 1/4 + 1/8 or 7/8
>
> We are now using negative powers where 2 raised to the minus one power is one over two raised to the plus one power, or 1/2 and so on. As you head to the right you get to fairly small numbers like 1/2048 ...
>
> Every single binary fraction is thus a possibly infinite sum of negative powers of two, or rather the reciprocals of those in positive terms.
>
> If you want to make 1/7, to some number of decimal places, it looks like this up to some point where I stop:
>
> 0.00100100100100100101
>
> So no halves, no quarters, 1/8, no sixteenths, no thirty-seconds, 1/64, and so on. But if you add all that up, and note the sequence was STOPPED before it could continue further, you get this translated into decimal:
>
> 0.142857 55157470703125
>
> Recall 1/7 in decimal notation is
> 0.142857 142857142857142857...
>
> Note the divergence at the seventh digit after the decimal point. I left a space to show where they diverge. If I used more binary digits, I can get as close as I want but computers these days do not allow too many more digits unless you use highly specialized programs. There are packages that give you access such as "mpfr" but generally nothing can give you infinite precision. R will not handle an infinite number of infinitesimals.
>
> The original problem that began our thread was about numbers like 0.1 and 0.2 and so on. In base ten, they look nice but I repeat in base 2 only powers of TWO reign.
>
> 0.1 in base two is about 0.0001100110011001101
>
> that reads as 1/16 + 1/32 + 1/256 + 1/512 + ...
>
> If I convert the above segment, which I repeat was stopped short, I 
> get 0.1000003814697265625 which is a tad over and had I taken the last 
> 1 and changed it to a zero as in 0.0001100110011001100 then we would 
> have a bit under at 0.09999847412109375
>
> So the only way to write 0.1 exactly is to continue infinitely, again. Do the analysis and understand why most rational numbers will not easily convert to a small number of bits. But the advantages of computers doing operations in binary are huge and need not be explained. You may THINK you are entering numbers in decimal form but they rarely remain that way for long before they simply become binary and often remain binary unless and until you ask to print them out, usually in decimal.
>
> BTW, I used a random web site to do the above conversion calculations:
>
> https://urldefense.proofpoint.com/v2/url?u=https-3A__www.rapidtables.c
> om_convert_number_binary-2Dto-2Ddecimal.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3
> EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=m7vgw0V3XLMYZ26IDQFrX6s6LeffgRVmWQ
> d4FGv5GUaI_W1jJ_i7QSJKQrB8fON4&s=_HQxpbll2YuLGvrwfc0AJPK4NAQ9xCcqvfl0c
> 1i-8Tg&e=
>
> Since I am writing in plain text, I cannot show what it says in the box on that page further down under Decimal Calculation Steps so I wonder what the rest of this message looks like:
>
> (0.0001100110011001100)? = (0 ? 2?) + (0 ? 2??) + (0 ? 2??) + (0 ? 
> 2??) + (1 ? 2??) + (1 ? 2??) + (0 ? 2??) + (0 ? 2??) + (1 ? 2??) + (1 
> ? 2??) + (0 ? 2???) + (0 ? 2???) + (1 ? 2???) + (1 ? 2???) + (0 ? 
> 2???) + (0 ? 2???) + (1 ? 2???) + (1 ? 2???) + (0 ? 2???) + (0 ? 2???) 
> = (0.09999847412109375)??
>
> I think my part in this particular discussion can now finally come to an end. R and everything else can be incomplete. Deal with it!
>
> -----Original Message-----
> From: Richard M. Heiberger <rmh at temple.edu>
> To: Avi Gross <avigross at verizon.net>
> Cc: nboeger at gmail.com <nboeger at gmail.com>; r-help at r-project.org 
> <r-help at r-project.org>
> Sent: Tue, Feb 1, 2022 9:04 pm
> Subject: Re: [External] [R] Funky calculations
>
>
> I apologize if my tone came across wrong.? I enjoy reading your comments on this list.
>
> My goal was to describe what the IEEE and R interpret "careful coding" to be.
>
>
>> On Feb 01, 2022, at 20:42, Avi Gross <avigross at verizon.net> wrote:
>>
>> Richard,
>>
>> I think it was fairly clear I was explaining how people do arithmetic manually and often truncate or round to some number of decimal places. I said nothing about what R does or what the IEEE standards say and I do not particularly care when making MY point.
>>
>> My point is that humans before computers also had trouble writing down any decimals that continue indefinitely. It cannot be expected computer versions of arithmetic can do much better. Different people can opt to do the calculation with the same or different numbers of digits ad when compared to each other they may not match.
>>
>> I do care what it does in my programs, of course. My goal here was to explain to someone that the anomaly found was not really an anomaly and that careful coding may be required in these situations.
>>
>>
>> -----Original Message-----
>> From: Richard M. Heiberger <rmh at temple.edu>
>> To: Avi Gross <avigross at verizon.net>
>> Cc: Nathan Boeger <nboeger at gmail.com>; r-help at r-project.org 
>> <r-help at r-project.org>
>> Sent: Tue, Feb 1, 2022 2:44 pm
>> Subject: Re: [External] [R] Funky calculations
>>
>>
>> RShowDoc('FAQ')
>>
>>
>> then search for 7.31
>>
>>
>> This statement
>> "If you stop at a 5 or 7 or 8 and back up to the previous digit, you round up. Else you leave the previous result alone."
>> is not quite right.? The recommendation in IEEE 754, and this is how R does arithmetic, is to Round Even.
>>
>> I ilustrate here with decimal, even though R and other programs use binary.
>>
>>> x <- c(1.4, 1.5, 1.6, 2.4, 2.5, 2.6, 3.4, 3.5, 3.6, 4.4, 4.5, 4.6) r 
>>> <- round(x) cbind(x, r)
>>? ? ? ? ?  x r
>> [1,] 1.4 1
>> [2,] 1.5 2
>> [3,] 1.6 2
>> [4,] 2.4 2
>> [5,] 2.5 2
>> [6,] 2.6 3
>> [7,] 3.4 3
>> [8,] 3.5 4
>> [9,] 3.6 4
>> [10,] 4.4 4
>> [11,] 4.5 4
>> [12,] 4.6 5
>>>
>>
>> Numbers whose last digit is not 5 (when in decimal) round to the nearest integer.
>> Numbers who last digit is 5 (1.5, 2.5, 3.5, 4.5 above) round to the 
>> nearest EVEN integer.
>> Hence 1.5 and 3.5 round up to the even numbers 2 and 4.
>> 2.5 and 4.5 round down do the even numbers 2 and 4.
>>
>> This way the round ups and downs average out to 0.? If we always went 
>> up from .5 we would have an updrift over time.
>>
>> For even more detail click on the link in FAQ 7.31 to my appendix 
>> https:// 
>> link.springer.com/content/pdf/bbm%3A978-1-4939-2122-5%2F1.pdf
>> and search for "Appendix G".
>>
>> Section G.5 explains Round to Even.
>> Sections G.6 onward illustrate specific examples, such as the one that started this email thread.
>>
>> Rich
>>
>
>
>


From nboeger @end|ng |rom gm@||@com  Thu Feb  3 03:14:01 2022
From: nboeger @end|ng |rom gm@||@com (Nathan Boeger)
Date: Thu, 3 Feb 2022 09:14:01 +0700
Subject: [R] [External] Funky calculations
In-Reply-To: <CAHqSRuTcqLizbAFy2jRBKoqFTNuV6mi08au6ox2iwbLgWew1QA@mail.gmail.com>
References: <CAFxA6QotuTYyJ8Jz_9SeeGZNR_LRxCXHKXYO+o3JihnQdnQL+g@mail.gmail.com>
 <20220201193557.308209e8@Tarkus>
 <D24231AD-5CBF-4595-962E-2009821BD20B@temple.edu>
 <CAFxA6Qq1sGwh9ZwEaV6zvtfd0yq6A1W6U4oDMuy1KXs_CV9ryQ@mail.gmail.com>
 <CAHqSRuT7KsOnUEKQRz2+Y_Z0ZoE+y3LHm4q9JLzqsSATe-85iA@mail.gmail.com>
 <CAFxA6Qr93D4VYjrZDvvrQgnTnToNQj3RQ21pLYrm8tJFHpA=1Q@mail.gmail.com>
 <CAHqSRuTcqLizbAFy2jRBKoqFTNuV6mi08au6ox2iwbLgWew1QA@mail.gmail.com>
Message-ID: <CAFxA6Qo+Mq0H2Hhdb9K5FKgc_sq9fr1TbZHydN9Pw43DChCrfg@mail.gmail.com>

You have a typo:

   printf("0.4+0.4+0.4+0.4 == 1.6 -> %s\n", d == 1.0 ? "true" : "false");

should be:

   printf("0.4+0.4+0.4+0.4 == 1.6 -> %s\n", d == 1.6 ? "true" : "false");

The comparison was with 1.0 but should be 1.6. I ran it and it has the same
output as you show above. Very interesting.

-nb

On Wed, 2 Feb 2022 at 21:24, Bill Dunlap <williamwdunlap at gmail.com> wrote:

> Floats have 23 bits of precision so the rounding is done there instead of
> at 52 bits, hence a different example is needed to show the problem with
> floats.
>
> bill at Bill-T490:~$ cat b.c
> #include <stdio.h>
>
> int main(int argc, char* argv[])
> {
>     float d = 0.4 + 0.4 + 0.4 + 0.4;
>     printf("0.4+0.4+0.4+0.4 -> %24.17g\n", (double)d);
>     printf("0.4+0.4+0.4+0.4 == 1.6 -> %s\n", d == 1.0 ? "true" : "false");
>     return 0;
> }
> bill at Bill-T490:~$ gcc b.c
> bill at Bill-T490:~$ ./a.out
> 0.4+0.4+0.4+0.4 ->       1.6000000238418579
> 0.4+0.4+0.4+0.4 == 1.6 -> false
>
> There is no getting around the fact that rounding will happen.
>
> -Bill
>
> On Tue, Feb 1, 2022 at 9:06 PM Nathan Boeger <nboeger at gmail.com> wrote:
>
>>
>> I understand this and with C the data type used is important. For this
>> type of calculation, I would normally use a float (basic single precision
>> is all I require).
>>
>> #include <stdio.h>
>>
>> void main() {
>>   float foo = (0.4 + 0.2 + 0.30 + 0.1) ;
>>   printf("foo: %f , foo > 1: %s \n", foo, (foo > 1.0 ? "true" : "false"));
>>   double bar = (0.4 + 0.2 + 0.30 + 0.1) ;
>>   printf("bar: %lf , bar > 1: %s \n", bar, (bar > 1.0 ? "true" :
>> "false"));
>> }
>>
>> gcc  c-check.c -o c-check
>> ./c-check
>> foo: 1.000000 , foo > 1: false
>> bar: 1.000000 , bar > 1: true
>>
>> Again, it was my mistake for not reading the R-FAQ. I had no idea it
>> would spark such a long thread.
>>
>> Cheers
>>
>> -nb
>>
>> On Wed, 2 Feb 2022 at 10:30, Bill Dunlap <williamwdunlap at gmail.com>
>> wrote:
>>
>>> The base 2 representation of 0.4 repeats the digit sequence 1001
>>> infinitely, hence must be rounded.  The problem occurs in C the same as it
>>> does in R.
>>>
>>> bill at Bill-T490:~$ cat a.c
>>> #include <stdio.h>
>>>
>>> int main(int argc, char* argv[])
>>> {
>>>     double d = 0.4 + 0.3 + 0.2 + 0.1;
>>>     printf("0.4+0.3+0.2+0.1 -> %24.17g\n", d);
>>>     printf("0.4+0.3+0.2+0.1 == 1.0 -> %s\n", d == 1.0 ? "true" :
>>> "false");
>>>     return 0;
>>> }
>>> bill at Bill-T490:~$ gcc a.c
>>> bill at Bill-T490:~$ ./a.out
>>> 0.4+0.3+0.2+0.1 ->      0.99999999999999989
>>> 0.4+0.3+0.2+0.1 == 1.0 -> false
>>>
>>> -Bill
>>>
>>> On Tue, Feb 1, 2022 at 7:01 PM Nathan Boeger <nboeger at gmail.com> wrote:
>>>
>>>> Thank you for this explanation!
>>>>
>>>> I have a long background in C/C++ and never realized this was such an
>>>> issue
>>>> with some languages. At least, with trivial single digit decimals. I
>>>> understand accuracy issues with very large decimals, repeating or
>>>> non-terminating rationals and I have handled them in the past. It makes
>>>> me
>>>> worried about all the R scripts I have written before (yikes!).
>>>>
>>>> Cheers
>>>>
>>>> -nb
>>>>
>>>> On Wed, 2 Feb 2022 at 02:44, Richard M. Heiberger <rmh at temple.edu>
>>>> wrote:
>>>>
>>>> > RShowDoc('FAQ')
>>>> >
>>>> > then search for 7.31
>>>> >
>>>> >
>>>> > This statement
>>>> > "If you stop at a 5 or 7 or 8 and back up to the previous digit, you
>>>> round
>>>> > up. Else you leave the previous result alone."
>>>> > is not quite right.  The recommendation in IEEE 754, and this is how R
>>>> > does arithmetic, is to Round Even.
>>>> >
>>>> > I ilustrate here with decimal, even though R and other programs use
>>>> binary.
>>>> >
>>>> > > x <- c(1.4, 1.5, 1.6, 2.4, 2.5, 2.6, 3.4, 3.5, 3.6, 4.4, 4.5, 4.6)
>>>> > > r <- round(x)
>>>> > > cbind(x, r)
>>>> >         x r
>>>> >  [1,] 1.4 1
>>>> >  [2,] 1.5 2
>>>> >  [3,] 1.6 2
>>>> >  [4,] 2.4 2
>>>> >  [5,] 2.5 2
>>>> >  [6,] 2.6 3
>>>> >  [7,] 3.4 3
>>>> >  [8,] 3.5 4
>>>> >  [9,] 3.6 4
>>>> > [10,] 4.4 4
>>>> > [11,] 4.5 4
>>>> > [12,] 4.6 5
>>>> > >
>>>> >
>>>> > Numbers whose last digit is not 5 (when in decimal) round to the
>>>> nearest
>>>> > integer.
>>>> > Numbers who last digit is 5 (1.5, 2.5, 3.5, 4.5 above)
>>>> > round to the nearest EVEN integer.
>>>> > Hence 1.5 and 3.5 round up to the even numbers 2 and 4.
>>>> > 2.5 and 4.5 round down do the even numbers 2 and 4.
>>>> >
>>>> > This way the round ups and downs average out to 0.  If we always went
>>>> up
>>>> > from .5 we would have
>>>> > an updrift over time.
>>>> >
>>>> > For even more detail click on the link in FAQ 7.31 to my appendix
>>>> > https://
>>>> link.springer.com/content/pdf/bbm%3A978-1-4939-2122-5%2F1.pdf
>>>> > and search for "Appendix G".
>>>> >
>>>> > Section G.5 explains Round to Even.
>>>> > Sections G.6 onward illustrate specific examples, such as the one that
>>>> > started this email thread.
>>>> >
>>>> > Rich
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>

	[[alternative HTML version deleted]]


From d@nny @end|ng |rom |dem@@|ntern@t|on@|  Wed Feb  2 17:10:41 2022
From: d@nny @end|ng |rom |dem@@|ntern@t|on@| (Danny Parsons)
Date: Wed, 2 Feb 2022 16:10:41 +0000
Subject: [R] [R-pkgs] Package "naflex": Flexible Options for Handling
 Missing Values
Message-ID: <CABxkZL5b3Vfa=Q44iAgwQ0T6gK8BNU8W_3o_vzyb=azx=+t46g@mail.gmail.com>

Dear all,

I have recently released a new package "naflex" on CRAN:
https://cran.r-project.org/web/packages/naflex/index.html.

"naflex" provides helper functions that give additional flexibility for
handling missing values in summary functions (beyond the current extremes
of na.rm = TRUE/FALSE).

For example, allow up to 20% of values to be missing:

library(naflex)
library(magrittr)
x <- c(1, 3, NA, 4, 3, 2, NA, 5, 8, 7)
mean(x %>% na_omit_if(prop = 0.2))
# 4.125
mean(x %>% na_omit_if(prop = 0.1))
# NA

Please see the package details or vignette for the full set of checks
available.

I hope this may be of use to some as a convenient way to work with various
missing value rules.

Feedback, suggestions and bug reports are welcome at
https://github.com/dannyparsons/naflex or by email
(danny at idems.international)

Best regards,

Danny Parsons

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From d@rgo@ch @end|ng |rom gm@||@com  Thu Feb  3 11:06:04 2022
From: d@rgo@ch @end|ng |rom gm@||@com (Fredrik Karlsson)
Date: Thu, 3 Feb 2022 11:06:04 +0100
Subject: [R] exp / log scaled version of cut?
Message-ID: <CANO=ohJ_79HSXzBZDR+=B+8PmbcW8YXDmnE1ahKddTF=6i_++Q@mail.gmail.com>

Dear list,

For tasks involving people its often good to work with ages in exp or log
scale, since either not much or very much happens in the first 10 years of
life, and much less (or much more) happens the following years. For tables
with age range summary statistics, I sometimes would like to have a
function that would for instance cut a range of values into breaks exp()
scale equidistance bins, but with unconverted values in the labels.

Similar to the way log / exp scaling of figure axis tick marks work, but
with a factor returned.

i have not been able to find such a function. Is there one?

Thanks!

Fredrik Karlsson


-- 
"Life is like a trumpet - if you don't put anything into it, you don't get
anything out of it."

	[[alternative HTML version deleted]]


From ||gge@ @end|ng |rom @t@t|@t|k@tu-dortmund@de  Thu Feb  3 13:41:35 2022
From: ||gge@ @end|ng |rom @t@t|@t|k@tu-dortmund@de (Uwe Ligges)
Date: Thu, 3 Feb 2022 13:41:35 +0100
Subject: [R] exp / log scaled version of cut?
In-Reply-To: <CANO=ohJ_79HSXzBZDR+=B+8PmbcW8YXDmnE1ahKddTF=6i_++Q@mail.gmail.com>
References: <CANO=ohJ_79HSXzBZDR+=B+8PmbcW8YXDmnE1ahKddTF=6i_++Q@mail.gmail.com>
Message-ID: <c8773396-b593-c801-4949-67d32b7ccaef@statistik.tu-dortmund.de>



On 03.02.2022 11:06, Fredrik Karlsson wrote:
> Dear list,
> 
> For tasks involving people its often good to work with ages in exp or log
> scale, since either not much or very much happens in the first 10 years of
> life, and much less (or much more) happens the following years. For tables
> with age range summary statistics, I sometimes would like to have a
> function that would for instance cut a range of values into breaks exp()
> scale equidistance bins, but with unconverted values in the labels.
> 
> Similar to the way log / exp scaling of figure axis tick marks work, but
> with a factor returned.
> 
> i have not been able to find such a function. Is there one?
> 
> Thanks!
> 
> Fredrik Karlsson
> 
> 


Yes, cut():

x <- runif(100, 0, 100)
cut(x, c(0, 10, 100))

Best,
Uwe Ligges


From tebert @end|ng |rom u||@edu  Thu Feb  3 16:22:32 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Thu, 3 Feb 2022 15:22:32 +0000
Subject: [R] exp / log scaled version of cut?
In-Reply-To: <c8773396-b593-c801-4949-67d32b7ccaef@statistik.tu-dortmund.de>
References: <CANO=ohJ_79HSXzBZDR+=B+8PmbcW8YXDmnE1ahKddTF=6i_++Q@mail.gmail.com>
 <c8773396-b593-c801-4949-67d32b7ccaef@statistik.tu-dortmund.de>
Message-ID: <BN6PR2201MB15532150D3FABC1DADD7A94FCF289@BN6PR2201MB1553.namprd22.prod.outlook.com>

In plotting with ggplot the graphing function geom_histogram() can do this for you. https://www.tutorialgateway.org/r-ggplot2-histogram/#:~:text=The%20syntax%20to%20draw%20a%20ggplot%20Histogram%20in,%3D%20FALSE%2C%20show.legend%20%3D%20NA%2C%20inherit.aes%20%3D%20TRUE%29

Binning the raw data could be done using case_when() see https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/case_when

With case_when() you could create a new variable so that you have the original data to refer back to. You might be able to overlay a plot of the axes in the raw data and the graph in binned data. I am not sure that would make much sense, and might be a way to trick readers into making bad conclusions: plotting binned data onto a continuous axis. Might work better to plot the binned data and then change labels using commands like 
xlab(), ylab(), theme(axis_title.x=element_blank()), axis_title.x=element_text(), labs(x="my x", y="my y")

http://www.sthda.com/english/wiki/ggplot2-axis-ticks-a-guide-to-customize-tick-marks-and-labels
https://www.datanovia.com/en/blog/ggplot-axis-labels/

Depending on exactly what you want to do here.

Regards,
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Uwe Ligges
Sent: Thursday, February 3, 2022 7:42 AM
To: Fredrik Karlsson <dargosch at gmail.com>; R-help <r-help at r-project.org>
Subject: Re: [R] exp / log scaled version of cut?

[External Email]

On 03.02.2022 11:06, Fredrik Karlsson wrote:
> Dear list,
>
> For tasks involving people its often good to work with ages in exp or 
> log scale, since either not much or very much happens in the first 10 
> years of life, and much less (or much more) happens the following 
> years. For tables with age range summary statistics, I sometimes would 
> like to have a function that would for instance cut a range of values 
> into breaks exp() scale equidistance bins, but with unconverted values in the labels.
>
> Similar to the way log / exp scaling of figure axis tick marks work, 
> but with a factor returned.
>
> i have not been able to find such a function. Is there one?
>
> Thanks!
>
> Fredrik Karlsson
>
>


Yes, cut():

x <- runif(100, 0, 100)
cut(x, c(0, 10, 100))

Best,
Uwe Ligges

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=E5bX46POI6aWCVW2Sn1gYLSC19ff-eoqeKiPmpipinQEE2CU38mPX2ReGsGWTfwG&s=MsUQlOyfUie2u5p7BJAJzW-BHNtXOlFZMiq-hPxTRVQ&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=E5bX46POI6aWCVW2Sn1gYLSC19ff-eoqeKiPmpipinQEE2CU38mPX2ReGsGWTfwG&s=q8534zhXd2FeFS4Z_Dth5YadYUs4P_9ZXjxdohleFB0&e=
and provide commented, minimal, self-contained, reproducible code.


From petr@p|k@| @end|ng |rom prechez@@cz  Thu Feb  3 16:52:12 2022
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Thu, 3 Feb 2022 15:52:12 +0000
Subject: [R] generate distribution based on summary data and add random noise
Message-ID: <9e7449825b9844678143a79a7b61fe04@SRVEXCHCM1301.precheza.cz>

Hallo all

I have summary data with size bins and percentage below that size.

dat <- structure(list(size = c(10L, 20L, 30L, 40L, 50L, 60L, 70L, 80L,
90L, 100L, 110L, 120L, 130L, 140L, 150L, 160L, 170L, 180L, 190L,
200L, 250L, 300L, 400L, 500L), percent = c(0L, 0L, 0L, 1L, 1L,
2L, 4L, 8L, 13L, 18L, 24L, 31L, 38L, 44L, 50L, 57L, 65L, 72L,
76L, 83L, 95L, 98L, 100L, 100L)), class = "data.frame", row.names = c(NA,
-24L))

#I want to generate original distribution (I know it is better not to do it but I have no other choice) so I calculated #mids of those bins

xd <-dat$size-c(5,diff(dat$size)/2)
xd<- xd[-1]

#I can sample the size bins with probability given by percent.
Result <- sample(xd, 1000, rep=T, prob=diff(dat$percent)/100)
plot(ecdf(Result))

#and I can add some noise to it, which is satisfactory with lower size bins but not enough for higher size bins.

Result <- sample(xd, 1000, rep=T, prob=diff(dat$percent)/100)+rnorm(1000, mean=0, sd=5)
plot(ecdf(Result))
I can increase sd to satisfy bigger bin size but in that case noise is too big for lower bin size.

I would like to add smaller random noise to lower size bins and bigger random noise to higher size bins, which seems to be easy task but I am stuck how to do it. It should be somehow proportional to size value.
The only way forward I see is to sort generated result and to use something like

+ rnorm(1000, mean=xd, sd=xd/10)
But it is not correct.

I'd appreciate any hint how to add random noise to values in ordered manner.

Best regards.
Petr

Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From bgunter@4567 @end|ng |rom gm@||@com  Thu Feb  3 17:09:43 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 3 Feb 2022 08:09:43 -0800
Subject: [R] generate distribution based on summary data and add random
 noise
In-Reply-To: <9e7449825b9844678143a79a7b61fe04@SRVEXCHCM1301.precheza.cz>
References: <9e7449825b9844678143a79a7b61fe04@SRVEXCHCM1301.precheza.cz>
Message-ID: <CAGxFJbT55JBCC9DCRC2xo8t7gUq8u9K6u1v8LOhJU2D_UrRnEA@mail.gmail.com>

If I understand correctly:
To generate a sample of total size N, generate a uniform sample of size p*N
for a bin with proportion p?
?runif

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Feb 3, 2022 at 7:52 AM PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hallo all
>
> I have summary data with size bins and percentage below that size.
>
> dat <- structure(list(size = c(10L, 20L, 30L, 40L, 50L, 60L, 70L, 80L,
> 90L, 100L, 110L, 120L, 130L, 140L, 150L, 160L, 170L, 180L, 190L,
> 200L, 250L, 300L, 400L, 500L), percent = c(0L, 0L, 0L, 1L, 1L,
> 2L, 4L, 8L, 13L, 18L, 24L, 31L, 38L, 44L, 50L, 57L, 65L, 72L,
> 76L, 83L, 95L, 98L, 100L, 100L)), class = "data.frame", row.names = c(NA,
> -24L))
>
> #I want to generate original distribution (I know it is better not to do
> it but I have no other choice) so I calculated #mids of those bins
>
> xd <-dat$size-c(5,diff(dat$size)/2)
> xd<- xd[-1]
>
> #I can sample the size bins with probability given by percent.
> Result <- sample(xd, 1000, rep=T, prob=diff(dat$percent)/100)
> plot(ecdf(Result))
>
> #and I can add some noise to it, which is satisfactory with lower size
> bins but not enough for higher size bins.
>
> Result <- sample(xd, 1000, rep=T, prob=diff(dat$percent)/100)+rnorm(1000,
> mean=0, sd=5)
> plot(ecdf(Result))
> I can increase sd to satisfy bigger bin size but in that case noise is too
> big for lower bin size.
>
> I would like to add smaller random noise to lower size bins and bigger
> random noise to higher size bins, which seems to be easy task but I am
> stuck how to do it. It should be somehow proportional to size value.
> The only way forward I see is to sort generated result and to use
> something like
>
> + rnorm(1000, mean=xd, sd=xd/10)
> But it is not correct.
>
> I'd appreciate any hint how to add random noise to values in ordered
> manner.
>
> Best regards.
> Petr
>
> Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch
> partner? PRECHEZA a.s. jsou zve?ejn?ny na:
> https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information
> about processing and protection of business partner?s personal data are
> available on website:
> https://www.precheza.cz/en/personal-data-protection-principles/
> D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou
> d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en?
> odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any
> documents attached to it may be confidential and are subject to the legally
> binding disclaimer: https://www.precheza.cz/en/01-disclaimer/
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From petr@p|k@| @end|ng |rom prechez@@cz  Thu Feb  3 17:44:51 2022
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Thu, 3 Feb 2022 16:44:51 +0000
Subject: [R] generate distribution based on summary data and add random
 noise
In-Reply-To: <CAGxFJbT55JBCC9DCRC2xo8t7gUq8u9K6u1v8LOhJU2D_UrRnEA@mail.gmail.com>
References: <9e7449825b9844678143a79a7b61fe04@SRVEXCHCM1301.precheza.cz>
 <CAGxFJbT55JBCC9DCRC2xo8t7gUq8u9K6u1v8LOhJU2D_UrRnEA@mail.gmail.com>
Message-ID: <0299ea9503e64c03b9fe606a9c12ea11@SRVEXCHCM1301.precheza.cz>

Hallo Bert

probably not, sorry. Did you try my examples?

To make it maybe simpler
1. sample a vector with given proportion and generate new data
2. add random noise to each generated value with sd given by value of a vector.

let say

x <- c(10, 100)
y <- c(.6, .4)
set.seed(200)
z <- sample(x, 10, rep=TRUE, prob=y)
ind <- order(z)
bins <- rle(z[ind])
bin1 <- rnorm(bins$lengths[1], mean = 0, sd=bins$values[1]/5)
bin2 <- rnorm(bins$lengths[2], mean = 0, sd=bins$values[2]/5)
z[ind] + c(bin1, bin2)

Sorry that I did not explain myself more clearly, I hoped that example showed what I have on mind.

Basically it is particle size cumulative distribution but size is expressed as size bins. Normally I have exact size measurement for each particle.

S pozdravem | Best Regards
RNDr. Petr PIKAL
Vedouc? V?zkumu a v?voje | Research Manager
PRECHEZA a.s.
n?b?. Dr. Edvarda Bene?e 1170/24 | 750 02 P?erov | Czech Republic
Tel: +420 581 252 256 | GSM: +420 724 008 364
mailto:petr.pikal at precheza.cz | https://www.precheza.cz/

Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/

From: Bert Gunter <bgunter.4567 at gmail.com> 
Sent: Thursday, February 3, 2022 5:10 PM
To: PIKAL Petr <petr.pikal at precheza.cz>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] generate distribution based on summary data and add random noise

If I understand correctly:
To generate a sample of total size N, generate a uniform sample of size p*N for a bin with proportion p?
?runif


Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Feb 3, 2022 at 7:52 AM PIKAL Petr <mailto:petr.pikal at precheza.cz> wrote:
Hallo all

I have summary data with size bins and percentage below that size.

dat <- structure(list(size = c(10L, 20L, 30L, 40L, 50L, 60L, 70L, 80L,
90L, 100L, 110L, 120L, 130L, 140L, 150L, 160L, 170L, 180L, 190L,
200L, 250L, 300L, 400L, 500L), percent = c(0L, 0L, 0L, 1L, 1L,
2L, 4L, 8L, 13L, 18L, 24L, 31L, 38L, 44L, 50L, 57L, 65L, 72L,
76L, 83L, 95L, 98L, 100L, 100L)), class = "data.frame", row.names = c(NA,
-24L))

#I want to generate original distribution (I know it is better not to do it but I have no other choice) so I calculated #mids of those bins

xd <-dat$size-c(5,diff(dat$size)/2)
xd<- xd[-1]

#I can sample the size bins with probability given by percent.
Result <- sample(xd, 1000, rep=T, prob=diff(dat$percent)/100)
plot(ecdf(Result))

#and I can add some noise to it, which is satisfactory with lower size bins but not enough for higher size bins.

Result <- sample(xd, 1000, rep=T, prob=diff(dat$percent)/100)+rnorm(1000, mean=0, sd=5)
plot(ecdf(Result))
I can increase sd to satisfy bigger bin size but in that case noise is too big for lower bin size.

I would like to add smaller random noise to lower size bins and bigger random noise to higher size bins, which seems to be easy task but I am stuck how to do it. It should be somehow proportional to size value.
The only way forward I see is to sort generated result and to use something like

+ rnorm(1000, mean=xd, sd=xd/10)
But it is not correct.

I'd appreciate any hint how to add random noise to values in ordered manner.

Best regards.
Petr

Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/

______________________________________________
mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From bgunter@4567 @end|ng |rom gm@||@com  Thu Feb  3 18:34:39 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 3 Feb 2022 09:34:39 -0800
Subject: [R] generate distribution based on summary data and add random
 noise
In-Reply-To: <0299ea9503e64c03b9fe606a9c12ea11@SRVEXCHCM1301.precheza.cz>
References: <9e7449825b9844678143a79a7b61fe04@SRVEXCHCM1301.precheza.cz>
 <CAGxFJbT55JBCC9DCRC2xo8t7gUq8u9K6u1v8LOhJU2D_UrRnEA@mail.gmail.com>
 <0299ea9503e64c03b9fe606a9c12ea11@SRVEXCHCM1301.precheza.cz>
Message-ID: <CAGxFJbQ7WcTt_fMFwRRez2TV+JNov92Hak1Y1Mnc7TUb+Bw+2A@mail.gmail.com>

Nope. I think I provided what you asked for, random data in each bin with
the amount of data proportional to bin percentage and the distribution of
that data uniform (nor normal) within the bin. So maybe someone else can
give you what you want if this ain't it.

Cheers,
Bert

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Feb 3, 2022 at 8:44 AM PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hallo Bert
>
> probably not, sorry. Did you try my examples?
>
> To make it maybe simpler
> 1. sample a vector with given proportion and generate new data
> 2. add random noise to each generated value with sd given by value of a
> vector.
>
> let say
>
> x <- c(10, 100)
> y <- c(.6, .4)
> set.seed(200)
> z <- sample(x, 10, rep=TRUE, prob=y)
> ind <- order(z)
> bins <- rle(z[ind])
> bin1 <- rnorm(bins$lengths[1], mean = 0, sd=bins$values[1]/5)
> bin2 <- rnorm(bins$lengths[2], mean = 0, sd=bins$values[2]/5)
> z[ind] + c(bin1, bin2)
>
> Sorry that I did not explain myself more clearly, I hoped that example
> showed what I have on mind.
>
> Basically it is particle size cumulative distribution but size is
> expressed as size bins. Normally I have exact size measurement for each
> particle.
>
> S pozdravem | Best Regards
> RNDr. Petr PIKAL
> Vedouc? V?zkumu a v?voje | Research Manager
> PRECHEZA a.s.
> n?b?. Dr. Edvarda Bene?e 1170/24 | 750 02 P?erov | Czech Republic
> Tel: +420 581 252 256 | GSM: +420 724 008 364
> mailto:petr.pikal at precheza.cz | https://www.precheza.cz/
>
> Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch
> partner? PRECHEZA a.s. jsou zve?ejn?ny na:
> https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information
> about processing and protection of business partner?s personal data are
> available on website:
> https://www.precheza.cz/en/personal-data-protection-principles/
> D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou
> d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en?
> odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any
> documents attached to it may be confidential and are subject to the legally
> binding disclaimer: https://www.precheza.cz/en/01-disclaimer/
>
> From: Bert Gunter <bgunter.4567 at gmail.com>
> Sent: Thursday, February 3, 2022 5:10 PM
> To: PIKAL Petr <petr.pikal at precheza.cz>
> Cc: R-help <r-help at r-project.org>
> Subject: Re: [R] generate distribution based on summary data and add
> random noise
>
> If I understand correctly:
> To generate a sample of total size N, generate a uniform sample of size
> p*N for a bin with proportion p?
> ?runif
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Thu, Feb 3, 2022 at 7:52 AM PIKAL Petr <mailto:petr.pikal at precheza.cz>
> wrote:
> Hallo all
>
> I have summary data with size bins and percentage below that size.
>
> dat <- structure(list(size = c(10L, 20L, 30L, 40L, 50L, 60L, 70L, 80L,
> 90L, 100L, 110L, 120L, 130L, 140L, 150L, 160L, 170L, 180L, 190L,
> 200L, 250L, 300L, 400L, 500L), percent = c(0L, 0L, 0L, 1L, 1L,
> 2L, 4L, 8L, 13L, 18L, 24L, 31L, 38L, 44L, 50L, 57L, 65L, 72L,
> 76L, 83L, 95L, 98L, 100L, 100L)), class = "data.frame", row.names = c(NA,
> -24L))
>
> #I want to generate original distribution (I know it is better not to do
> it but I have no other choice) so I calculated #mids of those bins
>
> xd <-dat$size-c(5,diff(dat$size)/2)
> xd<- xd[-1]
>
> #I can sample the size bins with probability given by percent.
> Result <- sample(xd, 1000, rep=T, prob=diff(dat$percent)/100)
> plot(ecdf(Result))
>
> #and I can add some noise to it, which is satisfactory with lower size
> bins but not enough for higher size bins.
>
> Result <- sample(xd, 1000, rep=T, prob=diff(dat$percent)/100)+rnorm(1000,
> mean=0, sd=5)
> plot(ecdf(Result))
> I can increase sd to satisfy bigger bin size but in that case noise is too
> big for lower bin size.
>
> I would like to add smaller random noise to lower size bins and bigger
> random noise to higher size bins, which seems to be easy task but I am
> stuck how to do it. It should be somehow proportional to size value.
> The only way forward I see is to sort generated result and to use
> something like
>
> + rnorm(1000, mean=xd, sd=xd/10)
> But it is not correct.
>
> I'd appreciate any hint how to add random noise to values in ordered
> manner.
>
> Best regards.
> Petr
>
> Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch
> partner? PRECHEZA a.s. jsou zve?ejn?ny na:
> https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information
> about processing and protection of business partner?s personal data are
> available on website:
> https://www.precheza.cz/en/personal-data-protection-principles/
> D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou
> d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en?
> odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any
> documents attached to it may be confidential and are subject to the legally
> binding disclaimer: https://www.precheza.cz/en/01-disclaimer/
>
> ______________________________________________
> mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Thu Feb  3 22:16:47 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Thu, 3 Feb 2022 21:16:47 +0000
Subject: [R] generate distribution based on summary data and add random
 noise
In-Reply-To: <CAGxFJbQ7WcTt_fMFwRRez2TV+JNov92Hak1Y1Mnc7TUb+Bw+2A@mail.gmail.com>
References: <9e7449825b9844678143a79a7b61fe04@SRVEXCHCM1301.precheza.cz>
 <CAGxFJbT55JBCC9DCRC2xo8t7gUq8u9K6u1v8LOhJU2D_UrRnEA@mail.gmail.com>
 <0299ea9503e64c03b9fe606a9c12ea11@SRVEXCHCM1301.precheza.cz>
 <CAGxFJbQ7WcTt_fMFwRRez2TV+JNov92Hak1Y1Mnc7TUb+Bw+2A@mail.gmail.com>
Message-ID: <BN6PR2201MB155369689F1BEE40D401F054CF289@BN6PR2201MB1553.namprd22.prod.outlook.com>

I suggest taking Bert's suggestion and looking more closely. Take a different dataset where you have measures for each particle. Then apply the binning function. Use Bert's approach to recovering the "raw" data and then plot the real raw data and the approximated raw data. If you are happy with the result then proceed. If you are not happy, then consider that there is always the option not to do something. 

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Bert Gunter
Sent: Thursday, February 3, 2022 12:35 PM
To: PIKAL Petr <petr.pikal at precheza.cz>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] generate distribution based on summary data and add random noise

[External Email]

Nope. I think I provided what you asked for, random data in each bin with the amount of data proportional to bin percentage and the distribution of that data uniform (nor normal) within the bin. So maybe someone else can give you what you want if this ain't it.

Cheers,
Bert

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Feb 3, 2022 at 8:44 AM PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hallo Bert
>
> probably not, sorry. Did you try my examples?
>
> To make it maybe simpler
> 1. sample a vector with given proportion and generate new data 2. add 
> random noise to each generated value with sd given by value of a 
> vector.
>
> let say
>
> x <- c(10, 100)
> y <- c(.6, .4)
> set.seed(200)
> z <- sample(x, 10, rep=TRUE, prob=y)
> ind <- order(z)
> bins <- rle(z[ind])
> bin1 <- rnorm(bins$lengths[1], mean = 0, sd=bins$values[1]/5)
> bin2 <- rnorm(bins$lengths[2], mean = 0, sd=bins$values[2]/5) z[ind] + 
> c(bin1, bin2)
>
> Sorry that I did not explain myself more clearly, I hoped that example 
> showed what I have on mind.
>
> Basically it is particle size cumulative distribution but size is 
> expressed as size bins. Normally I have exact size measurement for 
> each particle.
>
> S pozdravem | Best Regards
> RNDr. Petr PIKAL
> Vedouc? V?zkumu a v?voje | Research Manager PRECHEZA a.s.
> n?b?. Dr. Edvarda Bene?e 1170/24 | 750 02 P?erov | Czech Republic
> Tel: +420 581 252 256 | GSM: +420 724 008 364 
> mailto:petr.pikal at precheza.cz | 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__www.precheza.cz_&
> d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=-AB8wceVe
> 94Vlj5r4Ys5b6hqY2KsI4sq--H-Gl-PqL2ayWSE3H0EGJ3NJxR-_CUq&s=2A8zlu8YYruq
> c_HvJ_ZM1ZUyHYUxhnkzEcK0r7gqw1U&e=
>
> Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? 
> obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na:
> https://urldefense.proofpoint.com/v2/url?u=https-3A__www.precheza.cz_z
> asady-2Dochrany-2Dosobnich-2Dudaju_&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&
> r=9PEhQh2kVeAsRzsn7AkP-g&m=-AB8wceVe94Vlj5r4Ys5b6hqY2KsI4sq--H-Gl-PqL2ayWSE3H0EGJ3NJxR-_CUq&s=LnvV0OF3Gt0WokwkVLzk8zlw5EIaPMiJnIlIV4FQWzs&e=  | Information about processing and protection of business partner?s personal data are available on website:
> https://urldefense.proofpoint.com/v2/url?u=https-3A__www.precheza.cz_e
> n_personal-2Ddata-2Dprotection-2Dprinciples_&d=DwIFaQ&c=sJ6xIWYx-zLMB3
> EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=-AB8wceVe94Vlj5r4Ys5b6hqY2KsI4sq--
> H-Gl-PqL2ayWSE3H0EGJ3NJxR-_CUq&s=1ctBn30QSySXAHR0-xjdk_VQKrru3bF2TNX4j
> 2buz7I&e=
> D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou 
> d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en?
> odpov?dnosti: 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__www.precheza.cz_0
> 1-2Ddovetek_&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-
> g&m=-AB8wceVe94Vlj5r4Ys5b6hqY2KsI4sq--H-Gl-PqL2ayWSE3H0EGJ3NJxR-_CUq&s
> =Xyy9O_noGq_K3Nto-64iGqZC0R-JhoAwiMSNQIixvZU&e=  | This email and any 
> documents attached to it may be confidential and are subject to the 
> legally binding disclaimer: 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__www.precheza.cz_e
> n_01-2Ddisclaimer_&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzs
> n7AkP-g&m=-AB8wceVe94Vlj5r4Ys5b6hqY2KsI4sq--H-Gl-PqL2ayWSE3H0EGJ3NJxR-
> _CUq&s=FQbPQ1XP9xb1RJMbKZlXhQQaa9zSnUxJOevqrzrypRo&e=
>
> From: Bert Gunter <bgunter.4567 at gmail.com>
> Sent: Thursday, February 3, 2022 5:10 PM
> To: PIKAL Petr <petr.pikal at precheza.cz>
> Cc: R-help <r-help at r-project.org>
> Subject: Re: [R] generate distribution based on summary data and add 
> random noise
>
> If I understand correctly:
> To generate a sample of total size N, generate a uniform sample of 
> size p*N for a bin with proportion p?
> ?runif
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along 
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Thu, Feb 3, 2022 at 7:52 AM PIKAL Petr 
> <mailto:petr.pikal at precheza.cz>
> wrote:
> Hallo all
>
> I have summary data with size bins and percentage below that size.
>
> dat <- structure(list(size = c(10L, 20L, 30L, 40L, 50L, 60L, 70L, 80L, 
> 90L, 100L, 110L, 120L, 130L, 140L, 150L, 160L, 170L, 180L, 190L, 200L, 
> 250L, 300L, 400L, 500L), percent = c(0L, 0L, 0L, 1L, 1L, 2L, 4L, 8L, 
> 13L, 18L, 24L, 31L, 38L, 44L, 50L, 57L, 65L, 72L, 76L, 83L, 95L, 98L, 
> 100L, 100L)), class = "data.frame", row.names = c(NA,
> -24L))
>
> #I want to generate original distribution (I know it is better not to 
> do it but I have no other choice) so I calculated #mids of those bins
>
> xd <-dat$size-c(5,diff(dat$size)/2)
> xd<- xd[-1]
>
> #I can sample the size bins with probability given by percent.
> Result <- sample(xd, 1000, rep=T, prob=diff(dat$percent)/100)
> plot(ecdf(Result))
>
> #and I can add some noise to it, which is satisfactory with lower size 
> bins but not enough for higher size bins.
>
> Result <- sample(xd, 1000, rep=T, 
> prob=diff(dat$percent)/100)+rnorm(1000,
> mean=0, sd=5)
> plot(ecdf(Result))
> I can increase sd to satisfy bigger bin size but in that case noise is 
> too big for lower bin size.
>
> I would like to add smaller random noise to lower size bins and bigger 
> random noise to higher size bins, which seems to be easy task but I am 
> stuck how to do it. It should be somehow proportional to size value.
> The only way forward I see is to sort generated result and to use 
> something like
>
> + rnorm(1000, mean=xd, sd=xd/10)
> But it is not correct.
>
> I'd appreciate any hint how to add random noise to values in ordered 
> manner.
>
> Best regards.
> Petr
>
> Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? 
> obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na:
> https://urldefense.proofpoint.com/v2/url?u=https-3A__www.precheza.cz_z
> asady-2Dochrany-2Dosobnich-2Dudaju_&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&
> r=9PEhQh2kVeAsRzsn7AkP-g&m=-AB8wceVe94Vlj5r4Ys5b6hqY2KsI4sq--H-Gl-PqL2ayWSE3H0EGJ3NJxR-_CUq&s=LnvV0OF3Gt0WokwkVLzk8zlw5EIaPMiJnIlIV4FQWzs&e=  | Information about processing and protection of business partner?s personal data are available on website:
> https://urldefense.proofpoint.com/v2/url?u=https-3A__www.precheza.cz_e
> n_personal-2Ddata-2Dprotection-2Dprinciples_&d=DwIFaQ&c=sJ6xIWYx-zLMB3
> EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=-AB8wceVe94Vlj5r4Ys5b6hqY2KsI4sq--
> H-Gl-PqL2ayWSE3H0EGJ3NJxR-_CUq&s=1ctBn30QSySXAHR0-xjdk_VQKrru3bF2TNX4j
> 2buz7I&e=
> D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou 
> d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en?
> odpov?dnosti: 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__www.precheza.cz_0
> 1-2Ddovetek_&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-
> g&m=-AB8wceVe94Vlj5r4Ys5b6hqY2KsI4sq--H-Gl-PqL2ayWSE3H0EGJ3NJxR-_CUq&s
> =Xyy9O_noGq_K3Nto-64iGqZC0R-JhoAwiMSNQIixvZU&e=  | This email and any 
> documents attached to it may be confidential and are subject to the 
> legally binding disclaimer: 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__www.precheza.cz_e
> n_01-2Ddisclaimer_&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzs
> n7AkP-g&m=-AB8wceVe94Vlj5r4Ys5b6hqY2KsI4sq--H-Gl-PqL2ayWSE3H0EGJ3NJxR-
> _CUq&s=FQbPQ1XP9xb1RJMbKZlXhQQaa9zSnUxJOevqrzrypRo&e=
>
> ______________________________________________
> mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, 
> see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=-AB8wceVe94Vlj5r4Ys5b6hqY2KsI4sq--H-Gl-PqL2ayWSE3H0EGJ3NJ
> xR-_CUq&s=G7mrJcOOgGTjgbVjY_TNsusk-0cEKAFjvYGjiD5RZeM&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=-AB8wceVe94Vlj5r4Ys5b6hqY2KsI4sq--H-Gl-PqL2ayWSE3H0EGJ3N
> JxR-_CUq&s=b-P7V72w6IHT7gpAGwzaTN42gMwGRy9jkOWeQ4dX1QI&e=
> and provide commented, minimal, self-contained, reproducible code.
>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=-AB8wceVe94Vlj5r4Ys5b6hqY2KsI4sq--H-Gl-PqL2ayWSE3H0EGJ3NJxR-_CUq&s=G7mrJcOOgGTjgbVjY_TNsusk-0cEKAFjvYGjiD5RZeM&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=-AB8wceVe94Vlj5r4Ys5b6hqY2KsI4sq--H-Gl-PqL2ayWSE3H0EGJ3NJxR-_CUq&s=b-P7V72w6IHT7gpAGwzaTN42gMwGRy9jkOWeQ4dX1QI&e=
and provide commented, minimal, self-contained, reproducible code.

