From h.wickham at gmail.com  Fri Dec  1 00:42:25 2017
From: h.wickham at gmail.com (Hadley Wickham)
Date: Thu, 30 Nov 2017 17:42:25 -0600
Subject: [R] Fwd: Wanted to learn R Language
In-Reply-To: <CA+8X3fVDgXHBuZiht+NDQzjeZ45SSGP_G=dTD6+QX4BVWi6XtA@mail.gmail.com>
References: <CAPEb7FTDxOiS2i7KAox5-ejQp3=3NiDBhw12w592Eo6HbSG7AQ@mail.gmail.com>
 <CAPo+ggsvtO-eDFMwkh4kxaJ+xH4yoWMLMJaFSV6q7OdXbkYZXA@mail.gmail.com>
 <CA+8X3fVDgXHBuZiht+NDQzjeZ45SSGP_G=dTD6+QX4BVWi6XtA@mail.gmail.com>
Message-ID: <CABdHhvGmri8cNFPqEFfxQY9SGHpbrqM7Z2ZsLs3otm-aut2XKA@mail.gmail.com>

Or try haven::read_xpt(): http://haven.tidyverse.org/reference/read_xpt.html

Hadley

On Thu, Nov 30, 2017 at 3:13 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
> Hi SAS_learner,
> Have a look at the read.xport function in the foreign package.
>
> Jim
>
> On Fri, Dec 1, 2017 at 7:50 AM, SAS_learner <proccontents at gmail.com> wrote:
>> Hello all ,
>>
>> I am a SAS user for a while and wanted to learn to program in R . My
>> biggest hurdle to start, is to get the data (I work in clinical domain
>> ) that too inside VPN secured access. The only way I can learn during
>> my work time is create my own data frames and create programs that can
>> be used for data ( either SDTM or AdAM data ) validation or checking
>> Table counts . For this I need to imitate the clinical data structure
>> . If there is any place or a package that help to start. I have couple
>> of dummy SAS datasets in my work area , but not sure how can I can
>> access them . Can anybody help me . Thanks ahead .
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
http://hadley.nz


From fisher at plessthan.com  Fri Dec  1 03:37:02 2017
From: fisher at plessthan.com (Dennis Fisher)
Date: Thu, 30 Nov 2017 18:37:02 -0800
Subject: [R] Timezone problem with 3.4.2
Message-ID: <4C71EDB9-FFCA-40D1-82EC-5C20D6D826E5@plessthan.com>

Colleagues

I just installed 3.4.2 on a Mac running High Sierra.

I encountered the following:

R version 3.4.2 (2017-09-28) -- "Short Summer"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> Sys.Date()
[1] "2017-12-01"
Warning message:
In as.POSIXlt.POSIXct(Sys.time()) :
  unknown timezone 'zone/tz/2017c.1.0/zoneinfo/America/Los_Angeles'

There is nothing odd about the date/time settings on the computer.

I then tried:
> Sys.timezone()
[1] NA

Previously, there was not a problem with timezones.

Can I override this?  

Any thoughts?

Dennis

Dennis Fisher MD
P < (The "P Less Than" Company)
Phone / Fax: 1-866-PLessThan (1-866-753-7784)
www.PLessThan.com


From fisher at plessthan.com  Fri Dec  1 03:47:52 2017
From: fisher at plessthan.com (Dennis Fisher)
Date: Thu, 30 Nov 2017 18:47:52 -0800
Subject: [R] Timezone problem with 3.4.2
In-Reply-To: <27F72C67-771E-4301-8327-279A9CB32A44@me.com>
References: <4C71EDB9-FFCA-40D1-82EC-5C20D6D826E5@plessthan.com>
 <27F72C67-771E-4301-8327-279A9CB32A44@me.com>
Message-ID: <22E85028-26A0-4A90-891D-7995351D0507@plessthan.com>

Mark

Thanks for pointing this out.  I did a default installation of R.  Does this mean that I need to reinstall from the command line?

Dennis

Dennis Fisher MD
P < (The "P Less Than" Company)
Phone / Fax: 1-866-PLessThan (1-866-753-7784)
www.PLessThan.com <http://www.plessthan.com/>




> On Nov 30, 2017, at 6:42 PM, R. Mark Sharp <rmsharp at me.com> wrote:
> 
> From Peter Dalgaard announcement earlier today.
> 
> 
> CHANGES IN R 3.4.3:
> 
> INSTALLATION on a UNIX-ALIKE:
> 
>   * A workaround has been added for the changes in location of
>     time-zone files in macOS 10.13 'High Sierra' and again in
>     10.13.1, so the default time zone is deduced correctly from the
>     system setting when R is configured with --with-internal-tzcode
>     (the default on macOS).
> 
> R. Mark Sharp, Ph.D.
> Data Scientist and Biomedical Statistical Consultant
> 7526 Meadow Green St.
> San Antonio, TX 78251
> mobile: 210-218-2868
> rmsharp at me.com
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
>> On Nov 30, 2017, at 8:37 PM, Dennis Fisher <fisher at plessthan.com> wrote:
>> 
>> Colleagues
>> 
>> I just installed 3.4.2 on a Mac running High Sierra.
>> 
>> I encountered the following:
>> 
>> R version 3.4.2 (2017-09-28) -- "Short Summer"
>> Copyright (C) 2017 The R Foundation for Statistical Computing
>> Platform: x86_64-apple-darwin15.6.0 (64-bit)
>> 
>> R is free software and comes with ABSOLUTELY NO WARRANTY.
>> You are welcome to redistribute it under certain conditions.
>> Type 'license()' or 'licence()' for distribution details.
>> 
>> Natural language support but running in an English locale
>> 
>> R is a collaborative project with many contributors.
>> Type 'contributors()' for more information and
>> 'citation()' on how to cite R or R packages in publications.
>> 
>> Type 'demo()' for some demos, 'help()' for on-line help, or
>> 'help.start()' for an HTML browser interface to help.
>> Type 'q()' to quit R.
>> 
>>> Sys.Date()
>> [1] "2017-12-01"
>> Warning message:
>> In as.POSIXlt.POSIXct(Sys.time()) :
>> unknown timezone 'zone/tz/2017c.1.0/zoneinfo/America/Los_Angeles'
>> 
>> There is nothing odd about the date/time settings on the computer.
>> 
>> I then tried:
>>> Sys.timezone()
>> [1] NA
>> 
>> Previously, there was not a problem with timezones.
>> 
>> Can I override this?
>> 
>> Any thoughts?
>> 
>> Dennis
>> 
>> Dennis Fisher MD
>> P < (The "P Less Than" Company)
>> Phone / Fax: 1-866-PLessThan (1-866-753-7784)
>> www.PLessThan.com
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> CONFIDENTIALITY NOTICE: This e-mail and any files and/or attachments transmitted, may contain privileged and confidential information and is intended solely for the exclusive use of the individual or entity to whom it is addressed. If you are not the intended recipient, you are hereby notified that any review, dissemination, distribution or copying of this e-mail and/or attachments is strictly prohibited. If you have received this e-mail in error, please immediately notify the sender stating that this transmission was misdirected; return the e-mail to sender; destroy all paper copies and delete all electronic copies from your system without disclosing its contents.
> 


	[[alternative HTML version deleted]]


From rmsharp at me.com  Fri Dec  1 03:42:29 2017
From: rmsharp at me.com (R. Mark Sharp)
Date: Thu, 30 Nov 2017 20:42:29 -0600
Subject: [R] Timezone problem with 3.4.2
In-Reply-To: <4C71EDB9-FFCA-40D1-82EC-5C20D6D826E5@plessthan.com>
References: <4C71EDB9-FFCA-40D1-82EC-5C20D6D826E5@plessthan.com>
Message-ID: <27F72C67-771E-4301-8327-279A9CB32A44@me.com>

From Peter Dalgaard announcement earlier today.


CHANGES IN R 3.4.3:

 INSTALLATION on a UNIX-ALIKE:

   * A workaround has been added for the changes in location of
     time-zone files in macOS 10.13 'High Sierra' and again in
     10.13.1, so the default time zone is deduced correctly from the
     system setting when R is configured with --with-internal-tzcode
     (the default on macOS).

R. Mark Sharp, Ph.D.
Data Scientist and Biomedical Statistical Consultant
7526 Meadow Green St.
San Antonio, TX 78251
mobile: 210-218-2868
rmsharp at me.com










> On Nov 30, 2017, at 8:37 PM, Dennis Fisher <fisher at plessthan.com> wrote:
> 
> Colleagues
> 
> I just installed 3.4.2 on a Mac running High Sierra.
> 
> I encountered the following:
> 
> R version 3.4.2 (2017-09-28) -- "Short Summer"
> Copyright (C) 2017 The R Foundation for Statistical Computing
> Platform: x86_64-apple-darwin15.6.0 (64-bit)
> 
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
> 
>  Natural language support but running in an English locale
> 
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and
> 'citation()' on how to cite R or R packages in publications.
> 
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for an HTML browser interface to help.
> Type 'q()' to quit R.
> 
>> Sys.Date()
> [1] "2017-12-01"
> Warning message:
> In as.POSIXlt.POSIXct(Sys.time()) :
>  unknown timezone 'zone/tz/2017c.1.0/zoneinfo/America/Los_Angeles'
> 
> There is nothing odd about the date/time settings on the computer.
> 
> I then tried:
>> Sys.timezone()
> [1] NA
> 
> Previously, there was not a problem with timezones.
> 
> Can I override this?
> 
> Any thoughts?
> 
> Dennis
> 
> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone / Fax: 1-866-PLessThan (1-866-753-7784)
> www.PLessThan.com
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> CONFIDENTIALITY NOTICE: This e-mail and any files and/or attachments transmitted, may contain privileged and confidential information and is intended solely for the exclusive use of the individual or entity to whom it is addressed. If you are not the intended recipient, you are hereby notified that any review, dissemination, distribution or copying of this e-mail and/or attachments is strictly prohibited. If you have received this e-mail in error, please immediately notify the sender stating that this transmission was misdirected; return the e-mail to sender; destroy all paper copies and delete all electronic copies from your system without disclosing its contents.


From rmsharp at me.com  Fri Dec  1 03:57:14 2017
From: rmsharp at me.com (R. Mark Sharp)
Date: Thu, 30 Nov 2017 20:57:14 -0600
Subject: [R] Timezone problem with 3.4.2
In-Reply-To: <22E85028-26A0-4A90-891D-7995351D0507@plessthan.com>
References: <4C71EDB9-FFCA-40D1-82EC-5C20D6D826E5@plessthan.com>
 <27F72C67-771E-4301-8327-279A9CB32A44@me.com>
 <22E85028-26A0-4A90-891D-7995351D0507@plessthan.com>
Message-ID: <A38778CF-380F-4003-8331-003D0420FB29@me.com>

Dennis,

Brian Ripley pointed out shortly after 3.4.2 was released that the timezone was not being set correctly because of last minute changes to MacOS. You will have to install 3.4.3. I am waiting for the native installer, which will likely be out within a few days.

Mark
R. Mark Sharp, Ph.D.
Data Scientist and Biomedical Statistical Consultant
7526 Meadow Green St.
San Antonio, TX 78251
mobile: 210-218-2868
rmsharp at me.com










> On Nov 30, 2017, at 8:47 PM, Dennis Fisher <fisher at plessthan.com> wrote:
> 
> Mark
> 
> Thanks for pointing this out.  I did a default installation of R.  Does this mean that I need to reinstall from the command line?
> 
> Dennis
> 
> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone / Fax: 1-866-PLessThan (1-866-753-7784)
> www.PLessThan.com <http://www.plessthan.com/>
> 
> 
> 
> 
>> On Nov 30, 2017, at 6:42 PM, R. Mark Sharp <rmsharp at me.com> wrote:
>> 
>> From Peter Dalgaard announcement earlier today.
>> 
>> 
>> CHANGES IN R 3.4.3:
>> 
>> INSTALLATION on a UNIX-ALIKE:
>> 
>>  * A workaround has been added for the changes in location of
>>    time-zone files in macOS 10.13 'High Sierra' and again in
>>    10.13.1, so the default time zone is deduced correctly from the
>>    system setting when R is configured with --with-internal-tzcode
>>    (the default on macOS).
>> 
>> R. Mark Sharp, Ph.D.
>> Data Scientist and Biomedical Statistical Consultant
>> 7526 Meadow Green St.
>> San Antonio, TX 78251
>> mobile: 210-218-2868
>> rmsharp at me.com
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>>> On Nov 30, 2017, at 8:37 PM, Dennis Fisher <fisher at plessthan.com> wrote:
>>> 
>>> Colleagues
>>> 
>>> I just installed 3.4.2 on a Mac running High Sierra.
>>> 
>>> I encountered the following:
>>> 
>>> R version 3.4.2 (2017-09-28) -- "Short Summer"
>>> Copyright (C) 2017 The R Foundation for Statistical Computing
>>> Platform: x86_64-apple-darwin15.6.0 (64-bit)
>>> 
>>> R is free software and comes with ABSOLUTELY NO WARRANTY.
>>> You are welcome to redistribute it under certain conditions.
>>> Type 'license()' or 'licence()' for distribution details.
>>> 
>>> Natural language support but running in an English locale
>>> 
>>> R is a collaborative project with many contributors.
>>> Type 'contributors()' for more information and
>>> 'citation()' on how to cite R or R packages in publications.
>>> 
>>> Type 'demo()' for some demos, 'help()' for on-line help, or
>>> 'help.start()' for an HTML browser interface to help.
>>> Type 'q()' to quit R.
>>> 
>>>> Sys.Date()
>>> [1] "2017-12-01"
>>> Warning message:
>>> In as.POSIXlt.POSIXct(Sys.time()) :
>>> unknown timezone 'zone/tz/2017c.1.0/zoneinfo/America/Los_Angeles'
>>> 
>>> There is nothing odd about the date/time settings on the computer.
>>> 
>>> I then tried:
>>>> Sys.timezone()
>>> [1] NA
>>> 
>>> Previously, there was not a problem with timezones.
>>> 
>>> Can I override this?
>>> 
>>> Any thoughts?
>>> 
>>> Dennis
>>> 
>>> Dennis Fisher MD
>>> P < (The "P Less Than" Company)
>>> Phone / Fax: 1-866-PLessThan (1-866-753-7784)
>>> www.PLessThan.com
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> CONFIDENTIALITY NOTICE: This e-mail and any files and/or attachments transmitted, may contain privileged and confidential information and is intended solely for the exclusive use of the individual or entity to whom it is addressed. If you are not the intended recipient, you are hereby notified that any review, dissemination, distribution or copying of this e-mail and/or attachments is strictly prohibited. If you have received this e-mail in error, please immediately notify the sender stating that this transmission was misdirected; return the e-mail to sender; destroy all paper copies and delete all electronic copies from your system without disclosing its contents.
>> 
> 
> 
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> CONFIDENTIALITY NOTICE: This e-mail and any files and/or attachments transmitted, may contain privileged and confidential information and is intended solely for the exclusive use of the individual or entity to whom it is addressed. If you are not the intended recipient, you are hereby notified that any review, dissemination, distribution or copying of this e-mail and/or attachments is strictly prohibited. If you have received this e-mail in error, please immediately notify the sender stating that this transmission was misdirected; return the e-mail to sender; destroy all paper copies and delete all electronic copies from your system without disclosing its contents.


From alihassanshabir at gmail.com  Fri Dec  1 06:30:33 2017
From: alihassanshabir at gmail.com (Ali Hassan Shabbir)
Date: Fri, 1 Dec 2017 13:30:33 +0800
Subject: [R] Need Guidance
Message-ID: <CAG_Gom8H8uE=ezjHi_=ihcdGNX5+BnqQhMX3vREqjHCVhWxq1w@mail.gmail.com>

Respected Sir
I hope you will be fine and enjoy good health.
I am PhD student in Northeast Normal University, China. Kindly guide me,
How can I extract a time series of variable for a specific location
(specific longitude and latitude) from a CMIP5 experiment by using  R?  And
I brief my query which you can understand easily,  how to find monthly time
series data of lat=42.5 and long=116.25. for example
2006-1-1  280
2006-2-1  344
.
.
.
2100 -12-31   350
I shall be very thankful to you for this act of kindness.

Best Wishes
Ali Hassan Shabbir

	[[alternative HTML version deleted]]


From hotprojects at nyc.rr.com  Fri Dec  1 07:28:29 2017
From: hotprojects at nyc.rr.com (hotprojects at nyc.rr.com)
Date: Fri, 1 Dec 2017 01:28:29 -0500
Subject: [R] R vs PYTHON vs SAS vs SPSS?
Message-ID: <FA6EA000ABFF4AA4AEDC574799E849A2@OwnerPC>

I am a mature learner; 3 masters
some doctoral work ? statistics for social sciences; psychological statistics ?
worked in spss and sas 2005 ? 2006
now have forgotten ; relearning
my question is this can I do everything in R and Python and SAS studio
that I did in SPSS and the paid variation of SAS we used in doctoral statistics class?
Can I do in Stat in R or Python or SAS studio
everything I need to do in multiple regression; ANOVA; ANCOVA etc
?
that I did/was starting to do in SPSS?
	[[alternative HTML version deleted]]


From hasan.diwan at gmail.com  Fri Dec  1 08:33:06 2017
From: hasan.diwan at gmail.com (Hasan Diwan)
Date: Thu, 30 Nov 2017 23:33:06 -0800
Subject: [R] R vs PYTHON vs SAS vs SPSS?
In-Reply-To: <FA6EA000ABFF4AA4AEDC574799E849A2@OwnerPC>
References: <FA6EA000ABFF4AA4AEDC574799E849A2@OwnerPC>
Message-ID: <CAP+bYWC_=YmLvcxkJ_KbpDeee1N2WbibPLb4EX55dTb-vU4FPQ@mail.gmail.com>

Yes

On 30 November 2017 at 22:28, <hotprojects at nyc.rr.com> wrote:

> I am a mature learner; 3 masters
> some doctoral work ? statistics for social sciences; psychological
> statistics ?
> worked in spss and sas 2005 ? 2006
> now have forgotten ; relearning
> my question is this can I do everything in R and Python and SAS studio
> that I did in SPSS and the paid variation of SAS we used in doctoral
> statistics class?
> Can I do in Stat in R or Python or SAS studio
> everything I need to do in multiple regression; ANOVA; ANCOVA etc
> ?
> that I did/was starting to do in SPSS?
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.




-- 
OpenPGP:
https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1
If you wish to request my time, please do so using
http://bit.ly/hd1ScheduleRequest.
Si vous voudrais faire connnaisance, allez a
http://bit.ly/hd1ScheduleRequest.

<https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1>Sent
from my mobile device
Envoye de mon portable

	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Fri Dec  1 10:52:43 2017
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 1 Dec 2017 22:52:43 +1300
Subject: [R] [FORGED] Re:  R vs PYTHON vs SAS vs SPSS?
In-Reply-To: <CAP+bYWC_=YmLvcxkJ_KbpDeee1N2WbibPLb4EX55dTb-vU4FPQ@mail.gmail.com>
References: <FA6EA000ABFF4AA4AEDC574799E849A2@OwnerPC>
 <CAP+bYWC_=YmLvcxkJ_KbpDeee1N2WbibPLb4EX55dTb-vU4FPQ@mail.gmail.com>
Message-ID: <ab5c9535-35bf-ede3-2df7-372ee5f32c32@auckland.ac.nz>


On 01/12/17 20:33, Hasan Diwan wrote:

> Yes.

Very true.  But some *thinking* is required; that often proves to be a 
formidable stumbling block.

cheers,

Rolf Turner

> 
> On 30 November 2017 at 22:28, <hotprojects at nyc.rr.com> wrote:
> 
>> I am a mature learner; 3 masters
>> some doctoral work ? statistics for social sciences; psychological
>> statistics ?
>> worked in spss and sas 2005 ? 2006
>> now have forgotten ; relearning
>> my question is this can I do everything in R and Python and SAS studio
>> that I did in SPSS and the paid variation of SAS we used in doctoral
>> statistics class?
>> Can I do in Stat in R or Python or SAS studio
>> everything I need to do in multiple regression; ANOVA; ANCOVA etc
>> ?
>> that I did/was starting to do in SPSS?


From phillip.alday at mpi.nl  Fri Dec  1 11:11:39 2017
From: phillip.alday at mpi.nl (Phillip Alday)
Date: Fri, 1 Dec 2017 11:11:39 +0100
Subject: [R] How to extract coefficients from sequential (type 1),
 ANOVAs using lmer and lme
In-Reply-To: <1791113954.6386453.1512057407767@mail.yahoo.com>
References: <mailman.1.1511866801.58392.r-help@r-project.org>
 <2e617573-081d-30ec-33b7-e1db1f32c927@mpi.nl>
 <1791113954.6386453.1512057407767@mail.yahoo.com>
Message-ID: <81752a8f-57e0-1776-d6ba-62709a5767fa@mpi.nl>

Please reread my point #1: the tests of the (individual) coefficients in
the model summary are not the same as the ANOVA tests. There is a
certain correspondence between the two (i.e. between the coding of your
categorical variables and the type of sum of squares; and for a model
with a single predictor, F=t^2), but they are not the same in general.
The t-test in the model coefficients is simply the ratio of the estimate
to the standard error (i.e. a Wald test), and the standard errors, like
the estimates, are all calculated at the same time. So in that sense,
the t-tests are always marginal (cf. Pinheiro and Bates 2000, pp. 90-91).

I would also encourage you to focus more on your estimates (the Value
column) and less on p-values.

All that said, it seems you only care about the significance of model
terms and not the estimates, so there are two possibilities for
sequential tests:

1. Nested model comparison (likelihood-ratio test) with either the
anova() function or drop1(model,test='Chisq')

2. use the p-values from the ANOVA results

(see Pinheiro and Bates 2000, pp. 90-91, for some notes on which test is
preferred as well as the GLMM FAQ:
https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html)

But please please note that it would be misleading to say that these are
the p-values for the coefficients in your model. These issues are the
same for both mixed and 'normal' regression models.

Phillip

On 30/11/17 16:56, Akihiro Koyama wrote:
> Hi Phillip,
> 
> Thank you very much for informative comments. But I still cannot find a
> way to extract coefficients from sequential ANOVAs.
> 
> I have many data sets which all give different p-values for "sequential"
> and "marginal" options in anova(). But summary() command looks only
> provide me coefficients associated with "marginal" ANOVAs. 
> 
> 
> 
> 
> On Tuesday, November 28, 2017 6:51 AM, Phillip Alday
> <phillip.alday at mpi.nl> wrote:
> 
> 
> Be careful when mixing lme4 and lmerTest together -- lmerTest extends
> and changes the behavior of various lme4 functions.
> 
> From the help page for lme4-anova (?lme4::anova.merMod)
> 
>>      ?anova?: returns the sequential decomposition of the contributions
>>          of fixed-effects terms or, for multiple arguments, model
>>          comparison statistics.  For objects of class ?lmerMod? the
>>          default behavior is to refit the models with ML if fitted
>>          with ?REML = TRUE?, this can be controlled via the ?refit?
>>          argument. See also ?anova?.
> 
> So lme4-anova will give you sequential tests; note, however, that lme4
> won't calculate the denominator degrees of freedom for you and thus
> won't give p-values. See the FAQ
> (https://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-are-p_002dvalues-not-displayed-when-using-lmer_0028_0029_003f)
> 
> From the help page for lmerTest-anova (?lmerTest::anova.merModLmerTest):
>> Usage:
>>
>>      ## S4 method for signature 'merModLmerTest'
>>      anova(object, ... , ddf="Satterthwaite",
>>      type=3)
>>     
>> Arguments:
>>
> ...
>>    type: type of hypothesis to be tested. Could be type=3 or type=2 or
>>          type = 1 (The definition comes from SAS theory)
> 
> 
> So lmerTest-anova by default gives you Type III ('marginal', although
> Type II is what actually gives you tests that respect the Principle of
> Marginality; see John Fox's Applied Regression Analysis (book) or
> Venables' "Exegeses on Linear Models"
> (https://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf) for more information
> on that. Type I tests are the sequential tests, so with anova(model,
> type=1), you will get the sequential tests you want. lmerTest will
> approximate the denominator degrees of freedom for you (using
> Satterthwaite method by default, or the more computationally intensive
> Kenward-Roger method), so you'll get p-values if that's what you want.
> 
> Finally, it's important to note two things:
> 
> 1. The "type"-argument for nlme::summary doesn't actually do anything
> (see ?nlme::summary.lme). It's just passed onto the 'print' method,
> where it's silently ignored. The 'type' of sum of squares is an
> ANOVA-thing; the closest correspondence in terms of model coefficients
> is the coding of your categorical contrasts. See the literature
> mentioned above for more details as well as Dale Barr's discussion on
> simple vs. main effects in regression models
> (http://talklab.psy.gla.ac.uk/tvw/catpred/).
> 
> (?nlme::anova.lme does have indeed have a 'type' argument.)
> 
> 2. It is possible for the sequential tests and the marginal tests to
> yield the same results. Again, see the above literature. You have no
> interactions in your model and continuous (i.e. not-categorical)
> predictors, so if they're orthogonal, then the sequential and marginal
> tests will be numerically the same, even if they test different
> hypotheses. (See section 5.2, starting on page 14; the sequential tests
> are the "eliminating" tests, while the marginal tests are the "ignoring"
> tests in that explanation.)
> 
> Best,
> Phillip
> 
> 
> On 28/11/17 12:00, r-help-request at r-project.org
> <mailto:r-help-request at r-project.org> wrote:
>> I wantto run sequential ANOVAs (i.e. type I sums of squares), and
> trying to getresults including ANOVA tables and associated coefficients
> for predictive variables(I am using the R 3.4.2 version). I think ANOVA
> tables look right, but believecoefficients are wrong. Specifically, it
> looks like that the coefficients arefrom ANOVA with ?marginal? (type III
> sums of squares). I have tried both lme (nlmepackage) and lmer (lme4 +
> lmerTEST packages). Examples of the results arebelow:
>>
> 
> 
> <snip>
> 
> 
>> Ibelieve the results from summary() are for ?marginal? instead of
> ?sequential?ANOVA because the p-value (i.e., 0.237 for narea) in summary
> are identical tothose in tables from ?marginal?. I also used lmer in the
> lme4 pacakge to findthe same results (summary() results look like from
> ?marginal?).
>>
>>
>> Cananybody tell me how to get coefficients for ?sequential? ANOVAs?
> Thank you.
>>
> 
> 
>


From margarida.soares at biol.lu.se  Fri Dec  1 13:17:38 2017
From: margarida.soares at biol.lu.se (Margarida Soares)
Date: Fri, 1 Dec 2017 12:17:38 +0000
Subject: [R] pls in r
Message-ID: <801680F9-781E-41F0-A543-7D8E8CED4159@biol.lu.se>

Hello, 

I am a beginner in R, and I wonder if anyone could help me with a partial least square regression in R. I have looked up the instructions and the manual from Bjorn Mevi and Ron Wehrens. However, I think I managed to write the script correctly, but I dont understand the output on the R environment, and also how to decide on the number of components to use (from the RMSEP), and also how to do a correlation plot.

I welcome any help or advice!
Thanks!

Margarida Soares
PhD Student 
MEMEG, Department of Biology
Lund University 


From traxplayer at gmail.com  Fri Dec  1 14:22:38 2017
From: traxplayer at gmail.com (=?UTF-8?Q?Martin_M=C3=B8ller_Skarbiniks_Pedersen?=)
Date: Fri, 1 Dec 2017 14:22:38 +0100
Subject: [R] pls in r
In-Reply-To: <801680F9-781E-41F0-A543-7D8E8CED4159@biol.lu.se>
References: <801680F9-781E-41F0-A543-7D8E8CED4159@biol.lu.se>
Message-ID: <CAGAA5bd1Bf-uoSdCuZH8WhJzTT64_jjvycnYYMY=Pf5+MKVWYw@mail.gmail.com>

Hi,

On 1 December 2017 at 13:17, Margarida Soares <margarida.soares at biol.lu.se>
wrote:

> I am a beginner in R, and I wonder if anyone could help me with a partial
least square regression in R.
> I have looked up the instructions and the manual from Bjorn Mevi and Ron
Wehrens.

So you have read the vignette also ?
https://cran.r-project.org/web/packages/pls/vignettes/pls-manual.pdf

> However, I think I managed to write the script correctly,

You are welcome to post your script here then it would easier to help you.

>
> but I dont understand the output on the R environment, and also how to
decide on the number of components to use (from the RMSEP), and also how to
do a correlation plot.


Maybe this can help you:
https://www.r-bloggers.com/partial-least-squares-regression-in-r/

Regards
Martin

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Fri Dec  1 18:17:24 2017
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 1 Dec 2017 12:17:24 -0500
Subject: [R] Need Guidance
In-Reply-To: <CAG_Gom8H8uE=ezjHi_=ihcdGNX5+BnqQhMX3vREqjHCVhWxq1w@mail.gmail.com>
References: <CAG_Gom8H8uE=ezjHi_=ihcdGNX5+BnqQhMX3vREqjHCVhWxq1w@mail.gmail.com>
Message-ID: <CAM_vju=3KOQgfGH6WkTkT=o+FuD4wcf-gvwxLywbBPeesxQgQg@mail.gmail.com>

Have you successfully imported the dataset into R?
What have you tried?

If you have not imported the dataset into R, what format are your CMIP5 data in?

They are distributed various ways, and it's impossible to answer your
question without knowing what your data are.


On Fri, Dec 1, 2017 at 12:30 AM, Ali Hassan Shabbir
<alihassanshabir at gmail.com> wrote:
> Respected Sir
> I hope you will be fine and enjoy good health.
> I am PhD student in Northeast Normal University, China. Kindly guide me,
> How can I extract a time series of variable for a specific location
> (specific longitude and latitude) from a CMIP5 experiment by using  R?  And
> I brief my query which you can understand easily,  how to find monthly time
> series data of lat=42.5 and long=116.25. for example
> 2006-1-1  280
> 2006-2-1  344
> .
> .
> .
> 2100 -12-31   350
> I shall be very thankful to you for this act of kindness.
>
> Best Wishes
> Ali Hassan Shabbir

-- 
Sarah Goslee
http://www.functionaldiversity.org


From goran.brostrom at umu.se  Fri Dec  1 18:51:00 2017
From: goran.brostrom at umu.se (=?UTF-8?Q?G=c3=b6ran_Brostr=c3=b6m?=)
Date: Fri, 1 Dec 2017 18:51:00 +0100
Subject: [R] R 3.4.3 is released
In-Reply-To: <53ED0ECA-CE49-4CFB-93E6-8967C83F4492@cbs.dk>
References: <53ED0ECA-CE49-4CFB-93E6-8967C83F4492@cbs.dk>
Message-ID: <583ec8c7-64cb-41e6-155e-d3d02791287f@umu.se>

Thanks: I installed from source and got an error when loading a package:

--------------------------------------------------------------------
 > library(eha)
Loading required package: survival
Error: package or namespace load failed for ?eha? in dyn.load(file, 
DLLpath = DLLpath, ...):
  unable to load shared object 
'/usr/local/lib/R/site-library/eha/libs/eha.so':
   /usr/lib/x86_64-linux-gnu/libblas.so.3: undefined symbol: sgemv_thread_n
--------------------------------------------------------------------

Never seen this one before...

I'm on Ubuntu artful, and upgraded with 'apt'. Then

----------------------------------------------------------------
goran at M6800:~/src/R-3.4.3$ /usr/bin/R
/usr/lib/R/bin/exec/R: symbol lookup error: 
/usr/lib/x86_64-linux-gnu/libblas.so.3: undefined symbol: sgemv_thread_n
----------------------------------------------------------------

What can I do?

G?ran Brostr?m

On 2017-11-30 10:56, Peter Dalgaard wrote:
> The build system rolled up R-3.4.3.tar.gz (codename "Kite-Eating Tree") this morning.
> 
> The list below details the changes in this release.
> 
> You can get the source code from
> 
> http://cran.r-project.org/src/base/R-3/R-3.4.3.tar.gz
> 
> or wait for it to be mirrored at a CRAN site nearer to you.
> 
> Binaries for various platforms will appear in due course.
> 
> 
> For the R Core Team,
> 
> Peter Dalgaard
> 
> 
> 
> These are the checksums (md5 and SHA-256) for the freshly created files, in case you wish
> to check that they are uncorrupted:
> 
> MD5 (AUTHORS) = f12a9c3881197b20b08dd3d1f9d005e6
> MD5 (COPYING) = eb723b61539feef013de476e68b5c50a
> MD5 (COPYING.LIB) = a6f89e2100d9b6cdffcea4f398e37343
> MD5 (FAQ) = 32a94aba902b293cf8b8dbbf4113f2ab
> MD5 (INSTALL) = 7893f754308ca31f1ccf62055090ad7b
> MD5 (NEWS) = cd01b91d7a7acd614ad72bd571bf26b3
> MD5 (NEWS.0) = bfcd7c147251b5474d96848c6f57e5a8
> MD5 (NEWS.1) = eb78c4d053ec9c32b815cf0c2ebea801
> MD5 (NEWS.2) = 71562183d75dd2080d86c42bbf733bb7
> MD5 (R-latest.tar.gz) = bc55db54f992fda9049201ca62d2a584
> MD5 (README) = f468f281c919665e276a1b691decbbe6
> MD5 (RESOURCES) = 529223fd3ffef95731d0a87353108435
> MD5 (THANKS) = f60d286bb7294cef00cb0eed4052a66f
> MD5 (VERSION-INFO.dcf) = 2f9cc25704594a615a8c8248d0dcd804
> MD5 (R-3/R-3.4.3.tar.gz) = bc55db54f992fda9049201ca62d2a584
> 
> 6474d9791fff6a74936296bde3fcb569477f5958e4326189bd6e5ab878e0cd4f  AUTHORS
> e6d6a009505e345fe949e1310334fcb0747f28dae2856759de102ab66b722cb4  COPYING
> 6095e9ffa777dd22839f7801aa845b31c9ed07f3d6bf8a26dc5d2dec8ccc0ef3  COPYING.LIB
> 7936facb07e752869342808b9c8879d0e270b1a9ec92b67ef4dd87496abfef0a  FAQ
> f87461be6cbaecc4dce44ac58e5bd52364b0491ccdadaf846cb9b452e9550f31  INSTALL
> 97a082deb0a67a341f2c88a881c56a409a81c1af697732c3b15d8226d3970231  NEWS
> 4e21b62f515b749f80997063fceab626d7258c7d650e81a662ba8e0640f12f62  NEWS.0
> 12b30c724117b1b2b11484673906a6dcd48a361f69fc420b36194f9218692d01  NEWS.1
> a10f84be31f897456a31d31690df2fdc3f21a197f28b4d04332cc85005dcd0d2  NEWS.2
> 7a3cb831de5b4151e1f890113ed207527b7d4b16df9ec6b35e0964170007f426  R-latest.tar.gz
> 2fdd3e90f23f32692d4b3a0c0452f2c219a10882033d1774f8cadf25886c3ddc  README
> 408737572ecc6e1135fdb2cf7a9dbb1a6cb27967c757f1771b8c39d1fd2f1ab9  RESOURCES
> 52f934a4e8581945cbc1ba234932749066b5744cbd3b1cb467ba6ef164975163  THANKS
> 598f9c6b562c7106f741b9cdc2cc7d0bae364645f103e6ecb49e57625e28308b  VERSION-INFO.dcf
> 7a3cb831de5b4151e1f890113ed207527b7d4b16df9ec6b35e0964170007f426  R-3/R-3.4.3.tar.gz
> 
> This is the relevant part of the NEWS file
> 
> CHANGES IN R 3.4.3:
> 
>    INSTALLATION on a UNIX-ALIKE:
> 
>      * A workaround has been added for the changes in location of
>        time-zone files in macOS 10.13 'High Sierra' and again in
>        10.13.1, so the default time zone is deduced correctly from the
>        system setting when R is configured with --with-internal-tzcode
>        (the default on macOS).
> 
>      * R CMD javareconf has been updated to recognize the use of a Java
>        9 SDK on macOS.
> 
>    BUG FIXES:
> 
>      * raw(0) & raw(0) and raw(0) | raw(0) again return raw(0) (rather
>        than logical(0)).
> 
>      * intToUtf8() converts integers corresponding to surrogate code
>        points to NA rather than invalid UTF-8, as well as values larger
>        than the current Unicode maximum of 0x10FFFF.  (This aligns with
>        the current RFC3629.)
> 
>      * Fix calling of methods on S4 generics that dispatch on ... when
>        the call contains ....
> 
>      * Following Unicode 'Corrigendum 9', the UTF-8 representations of
>        U+FFFE and U+FFFF are now regarded as valid by utf8ToInt().
> 
>      * range(c(TRUE, NA), finite = TRUE) and similar no longer return
>        NA. (Reported by Lukas Stadler.)
> 
>      * The self starting function attr(SSlogis, "initial") now also
>        works when the y values have exact minimum zero and is slightly
>        changed in general, behaving symmetrically in the y range.
> 
>      * The printing of named raw vectors is now formatted nicely as for
>        other such atomic vectors, thanks to Lukas Stadler.
>


From jun.yan at uconn.edu  Fri Dec  1 11:07:23 2017
From: jun.yan at uconn.edu (Yan, Jun)
Date: Fri, 1 Dec 2017 10:07:23 +0000
Subject: [R] 2018 ASA Computing/Graphics: Chambers Software Award and
 Student Paper Competition
In-Reply-To: <BN6PR05MB3267B377502548C00B3BF21BF04D0@BN6PR05MB3267.namprd05.prod.outlook.com>
References: <BN6PR05MB3267B377502548C00B3BF21BF04D0@BN6PR05MB3267.namprd05.prod.outlook.com>
Message-ID: <BN6PR05MB326743C20A9D1ED44321F8EAF0390@BN6PR05MB3267.namprd05.prod.outlook.com>

Dear R-help Listers,

The submission site for the two awards (Chambers Student Software;
Computing/Graphics Student Paper) is open at http://asa.stat.uconn.edu<http://asa.stat.uconn.edu/>
until the deadline, 5:00 pm EST, December 15, 2017. The results will
be announced by January 15, 2018. The award recipients are expected to
present in a topic-contributed session at the 2018 JSM in Vancouver.

I would appreciate your spreading the words.


Jun Yan

________________________________
From: Yan, Jun
Sent: Wednesday, October 18, 2017 6:37:19 AM
To: r-help at R-project.org
Subject: 2018 ASA Computing/Graphics: Chambers Software Award and Student Paper Competition


Dear R-help Listers,


The following two student competitions are of interests to the now many student R package developers. I'd appreciate your help in spreading them.


#1. John M. Chambers Statistical Software Award 2018

The Statistical Computing Section of the American Statistical
Association announces the competition for the John M. Chambers
Statistical Software Award. In 1998 the Association for Computing
Machinery (ACM) presented the ACM Software System Award to John
Chambers for the design and development of S. Dr. Chambers generously
donated his award to the Statistical Computing Section to endow an
annual prize for statistical software written by, or in collaboration
with, an undergraduate or graduate student. The prize carries with it
a cash award, which has been increased from $1,000 to $2,000 starting
from 2018. See http://stat-computing.org/awards/jmc/history.html for

the history of the award.

Both individuals and teams are eligible to participate in the
competition. To be eligible, at least one individual within the team
must have begun the development while a student and must either
currently be a student, or have completed all requirements for her/his
last degree after January 1, 2017. The award will be given to the
student, or split between student team members if the team consists of
multiple students, up to a maximum of three students. If the software
was created by a team, the contribution of the student(s) must be
substantial.

To apply for the award, teams must provide the following materials:

Current CVs of all team members.

A letter from a faculty mentor at the academic institution of one of
the students. The letter should confirm that the student had
substantial participation in the development of the software, certify
her/his student status when the software began to be developed,
confirm that he/she is still a student (or provide a date of degree
completion), and briefly discuss the importance of the software to
statistical practice.

A brief, one to two page description of the software, summarizing what
it does, how it does it, and why it is an important contribution. If
any student team member has continued developing the software after
finishing her/his studies, the description should indicate what was
developed when the individual was a student and what has been added
since.

An installable software package with its source code for use by the
award committee. It should be accompanied by enough information to
allow the judges to effectively use and evaluate the software
(including its design considerations). This information can be
provided in a variety of ways, including but not limited to: a user
manual, a manuscript, a URL, and online help to the system.

All materials must be in English. We prefer that electronic text be
submitted as PDF files. The entries will be judged on a variety of
dimensions, including the importance and relevance for statistical
practice of the tasks performed by the software, ease of use, clarity
of description, elegance and availability for use by the statistical
community. Preference will be given to those entries that are grounded
in software design rather than calculation. The decision of the award
committee is final.

All application materials MUST BE RECEIVED by 5:00pm EST, Friday,
December 15, 2017. The submission window will be open at
http://asa.stat.uconn.edu on December 1, 2017. Questions are to be


emailed to Professor Jun Yan.

#2. Student Paper Competition 2018

The Statistical Computing and Statistical Graphics Sections of the ASA
are co-sponsoring a student paper competition on the topics of
Statistical Computing and Statistical Graphics. Students are
encouraged to submit a paper in one of these areas, which might be
original methodological research, some novel computing or graphical
application in statistics, or any other suitable contribution (for
example, a software-related project). The selected winners will
present their papers in a topic-contributed session at the 2017 Joint
Statistical Meetings. The prize carries with it a cash award of
$1,000.

Anyone who is a student (graduate or undergraduate) on or after
September 1, 2017 is eligible to participate. An entry must include an
abstract, a six page manuscript (including figures, tables and
references; a two-column format is acceptable), blinded versions of
the abstract and manuscript (with no author names or other information
that easily identifies the authors), a CV, and a letter from a faculty
member familiar with the student's work. The applicant must be the
first author of the paper. The faculty letter must include a
verification of the applicant's student status and, in the case of
joint authorship, should indicate what fraction of the contribution is
attributable to the applicant. We prefer that electronic submissions
of papers consist of PDF files. All materials must be in English.

Students may submit papers to no more than two sections and may accept
only one section's award. Students must inform both sections applied
to when he or she wins and accepts an award, thereby removing the
student from the award competition for the second section.

All application materials MUST BE RECEIVED by 5:00 PM EST, Friday,
December 15, 2017. The submission window will be open at
http://asa.stat.uconn.edu on December 1, 2017. They will be reviewed

by the Student Paper Competition Award committee of the Statistical
Computing and Graphics Sections. The selection criteria used by the
committee will include innovation and significance of the contribution
as well as the professional quality of the manuscript. Award
announcements will be made by January 15th, 2018.

Additional important information on the competition may be found at
http://stat-computing.org/awards/student/faq.html and, for all ASA
sponsored student paper competitions Additional important information
on the competition may be found at
http://stat-computing.org/awards/student/faq.html and, for all ASA

sponsored student paper competitions, at
http://amstat.org/ASA/Your-Career/Student-Paper-Competitions.aspx?hkey=6481db83-6316-44b1-be71-d4796b76f583.

Jun Yan

Department of Statistics

University of Connecticut

jun.yan at uconn.edu


	[[alternative HTML version deleted]]


From kthralls at spu.edu  Fri Dec  1 19:32:00 2017
From: kthralls at spu.edu (Thralls, Katie)
Date: Fri, 1 Dec 2017 18:32:00 +0000
Subject: [R] R-help: ActivPALProcessing
Message-ID: <CO2PR07MB2645878D14CAE17D92F7EA1AC1390@CO2PR07MB2645.namprd07.prod.outlook.com>

Hi,

I am having trouble with the function process.AP  in the ActivPALProcessing package. Based on the detailed instruction attached by the author, it is my understanding that process.AP will use the information provided in the log of subjects to batch process AP events files saved in the working directory.  Results of the batch processing will be summarized in three files: 1) Sleep Wake Wear Table, 2) Results Table and 3) Means Table.

However, I have followed the directions in the attachment but have only received a few outcomes (I copied and pasted what came out from my code below.) I have checked and rechecked my files to be sure their formatting is correct and I am not sure what else to try. I e-mailed the author about a month ago but have not heard anything either. Let me know if you have some recommendations on how I can move forward.

Thanks!
Katie Thralls
9254139791



CODE:

install.packages("activpalProcessing")
library("activpalProcessing")

  list.files("~/Dropbox/R_code/AP_Directory/")
directory <- identifyDirectory("~/Dropbox/R_code/AP_Directory/")
setwd("~/Dropbox/R_code/AP_Directory/")
  list.files()
  #Here I open "log.subjects.csv" in a text editor and a return at the end of the last (only) data row. Then resave as csv. Then the error go away. See:
  read.csv("log.subjects.csv")
read.table("log.subjects.csv")
read.csv("SB_KT_1.csv")
process.AP(directory,name.of.log.subjects="log.subjects",name.of.log.bed=log.bed,name.of.log.on.off=log.on.off)
#no errors



Output in R Console:
> process.AP(directory,name.of.log.subjects="log.subjects",name.of.log.bed=log.bed,name.of.log.on.off=log.on.off)
[1] 1
[1] "1"
[1] "SB"


From ashenkin at ufl.edu  Sat Dec  2 11:48:05 2017
From: ashenkin at ufl.edu (Alexander Shenkin)
Date: Sat, 2 Dec 2017 10:48:05 +0000
Subject: [R] source files in temp environment
Message-ID: <94ce2684-e1e6-e1a1-ec1f-0f7c7053a850@ufl.edu>

Hi all,

I often keep code in separate files for organizational purposes, and 
source() that code from higher level scripts.  One problem is that those 
sourced files often create temporary variables that I don't want to keep 
around.  I could clean up after myself with lots of rm()'s, but that's a 
pain, and is messy.

I'm wondering if one solution might be to source the code in a temporary 
environment, assign outputs of interest to the .GlobalEnv with <<-, and 
then delete the environment afterwards.  One way to do this:

file.r:
temp1 = 1
temp2 = 2
desired_var <<- temp1 + temp2

console:
temp_e = new.env()
source("file.r", local = temp_e)
rm(temp_e)

It's a bit messy to create and delete environments, so I tried what 
others have referred to:

source("file.r", local = attach(NULL))

This, however, results in a persistent "NULL" environment in the search 
path.

 > search()
".GlobalEnv"            "package:bindrcpp"      "NULL"
"tools:rstudio"         "package:stats"         "package:graphics" 
"package:grDevices"     "package:utils"         "package:datasets"
"package:methods"       "Autoloads"             "package:base"

Of course, functions are built to encapsulate like this (and do so in 
their own temporary environment), but in many cases, turning the sourced 
code into functions is possible but clunky.

Any thoughts or suggestions would be much appreciated.

Thanks,
Allie


From murdoch.duncan at gmail.com  Sat Dec  2 12:01:41 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 2 Dec 2017 06:01:41 -0500
Subject: [R] source files in temp environment
In-Reply-To: <94ce2684-e1e6-e1a1-ec1f-0f7c7053a850@ufl.edu>
References: <94ce2684-e1e6-e1a1-ec1f-0f7c7053a850@ufl.edu>
Message-ID: <64bb6367-f912-1228-0f67-73f7fb8f7308@gmail.com>

On 02/12/2017 5:48 AM, Alexander Shenkin wrote:
> Hi all,
> 
> I often keep code in separate files for organizational purposes, and
> source() that code from higher level scripts.  One problem is that those
> sourced files often create temporary variables that I don't want to keep
> around.  I could clean up after myself with lots of rm()'s, but that's a
> pain, and is messy.
> 
> I'm wondering if one solution might be to source the code in a temporary
> environment, assign outputs of interest to the .GlobalEnv with <<-, and
> then delete the environment afterwards.  One way to do this:
> 
> file.r:
> temp1 = 1
> temp2 = 2
> desired_var <<- temp1 + temp2
> 
> console:
> temp_e = new.env()
> source("file.r", local = temp_e)
> rm(temp_e)
> 
> It's a bit messy to create and delete environments, so I tried what
> others have referred to:
> 
> source("file.r", local = attach(NULL))
> 
> This, however, results in a persistent "NULL" environment in the search
> path.
> 
>   > search()
> ".GlobalEnv"            "package:bindrcpp"      "NULL"
> "tools:rstudio"         "package:stats"         "package:graphics"
> "package:grDevices"     "package:utils"         "package:datasets"
> "package:methods"       "Autoloads"             "package:base"
> 
> Of course, functions are built to encapsulate like this (and do so in
> their own temporary environment), but in many cases, turning the sourced
> code into functions is possible but clunky.
> 
> Any thoughts or suggestions would be much appreciated.

I would wrap the calls in the local() function, or put them in a 
function and call that.  That is,

local({
   source("file.R", local = TRUE)
})

or

sourceit <- function() {
   source("file.R", local = TRUE)
}
sourceit()

With respect to your last comment (turning the code in file.R into 
functions which don't leave their locals behind):  I think that would be 
the best solution.  You may find it clunky now, but in the long run it 
likely will help you to make better code.

Duncan Murdoch


From christian at echoffmann.ch  Sat Dec  2 10:37:11 2017
From: christian at echoffmann.ch (Christian)
Date: Sat, 2 Dec 2017 10:37:11 +0100
Subject: [R] Where is R 3.4.3 (announced for 30.November)?
Message-ID: <2430c66a-e541-65b7-3025-bc3e133a1b19@echoffmann.ch>

Havin Porblems with R 3.4.2, I am waiting for R 3.4.3 to be available. 
Nothing appers on CRAN for R 3.4.3.

When will R 3.4.3 be avalable?

Best   C.
-- 
Christian Hoffmann
Rigiblickstrasse 15b
CH-8915 Hausen am Albis
Switzerland
Telefon +41-(0)44-7640853


From mahmoudrayan.an313 at gmail.com  Sat Dec  2 11:37:39 2017
From: mahmoudrayan.an313 at gmail.com (Mahmoud Aslani)
Date: Sat, 2 Dec 2017 14:07:39 +0330
Subject: [R] DEA CCR stochastice
Message-ID: <CAE30chDYKcPyxLtE5uH3R06ve7+Bx+kMjC8VRV1Pr9n+p6gLBA@mail.gmail.com>

Hello,
Good time,

I want to write a source code with the model DEA CCR stochastice in R
software. But I could not find any example that guides me. I need this
model for my article.
Where can I find the source code for this model? Is it possible to send me
an example of this source code?

Help me please.
Thanks

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Sat Dec  2 13:03:23 2017
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Sat, 2 Dec 2017 12:03:23 +0000
Subject: [R] Where is R 3.4.3 (announced for 30.November)?
In-Reply-To: <2430c66a-e541-65b7-3025-bc3e133a1b19@echoffmann.ch>
References: <2430c66a-e541-65b7-3025-bc3e133a1b19@echoffmann.ch>
Message-ID: <b43b2e63-a180-0de0-2f9b-fe6026756c97@dewey.myzen.co.uk>

Dear Christian

It is available now. Perhaps the mirror you are using has not quite 
caught up yet?

On 02/12/2017 09:37, Christian wrote:
> Havin Porblems with R 3.4.2, I am waiting for R 3.4.3 to be available. 
> Nothing appers on CRAN for R 3.4.3.
> 
> When will R 3.4.3 be avalable?
> 
> Best?? C.

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From goran.brostrom at umu.se  Sat Dec  2 13:24:09 2017
From: goran.brostrom at umu.se (=?UTF-8?Q?G=c3=b6ran_Brostr=c3=b6m?=)
Date: Sat, 2 Dec 2017 13:24:09 +0100
Subject: [R] R 3.4.3 is released
In-Reply-To: <583ec8c7-64cb-41e6-155e-d3d02791287f@umu.se>
References: <53ED0ECA-CE49-4CFB-93E6-8967C83F4492@cbs.dk>
 <583ec8c7-64cb-41e6-155e-d3d02791287f@umu.se>
Message-ID: <0bf57cf1-d4e9-1b00-d561-ca2839cf76b7@umu.se>

Removing libopenblas-dev and libopenblas-base solved my problem.

G?ran

On 2017-12-01 18:51, G?ran Brostr?m wrote:
> Thanks: I installed from source and got an error when loading a package:
> 
> --------------------------------------------------------------------
>  > library(eha)
> Loading required package: survival
> Error: package or namespace load failed for ?eha? in dyn.load(file, 
> DLLpath = DLLpath, ...):
>  ?unable to load shared object 
> '/usr/local/lib/R/site-library/eha/libs/eha.so':
>  ? /usr/lib/x86_64-linux-gnu/libblas.so.3: undefined symbol: sgemv_thread_n
> --------------------------------------------------------------------
> 
> Never seen this one before...
> 
> I'm on Ubuntu artful, and upgraded with 'apt'. Then
> 
> ----------------------------------------------------------------
> goran at M6800:~/src/R-3.4.3$ /usr/bin/R
> /usr/lib/R/bin/exec/R: symbol lookup error: 
> /usr/lib/x86_64-linux-gnu/libblas.so.3: undefined symbol: sgemv_thread_n
> ----------------------------------------------------------------
> 
> What can I do?
> 
> G?ran Brostr?m
> 
> On 2017-11-30 10:56, Peter Dalgaard wrote:
>> The build system rolled up R-3.4.3.tar.gz (codename "Kite-Eating 
>> Tree") this morning.
>>
>> The list below details the changes in this release.
>>
>> You can get the source code from
>>
>> http://cran.r-project.org/src/base/R-3/R-3.4.3.tar.gz
>>
>> or wait for it to be mirrored at a CRAN site nearer to you.
>>
>> Binaries for various platforms will appear in due course.
>>
>>
>> For the R Core Team,
>>
>> Peter Dalgaard
>>
>>
>>
>> These are the checksums (md5 and SHA-256) for the freshly created 
>> files, in case you wish
>> to check that they are uncorrupted:
>>
>> MD5 (AUTHORS) = f12a9c3881197b20b08dd3d1f9d005e6
>> MD5 (COPYING) = eb723b61539feef013de476e68b5c50a
>> MD5 (COPYING.LIB) = a6f89e2100d9b6cdffcea4f398e37343
>> MD5 (FAQ) = 32a94aba902b293cf8b8dbbf4113f2ab
>> MD5 (INSTALL) = 7893f754308ca31f1ccf62055090ad7b
>> MD5 (NEWS) = cd01b91d7a7acd614ad72bd571bf26b3
>> MD5 (NEWS.0) = bfcd7c147251b5474d96848c6f57e5a8
>> MD5 (NEWS.1) = eb78c4d053ec9c32b815cf0c2ebea801
>> MD5 (NEWS.2) = 71562183d75dd2080d86c42bbf733bb7
>> MD5 (R-latest.tar.gz) = bc55db54f992fda9049201ca62d2a584
>> MD5 (README) = f468f281c919665e276a1b691decbbe6
>> MD5 (RESOURCES) = 529223fd3ffef95731d0a87353108435
>> MD5 (THANKS) = f60d286bb7294cef00cb0eed4052a66f
>> MD5 (VERSION-INFO.dcf) = 2f9cc25704594a615a8c8248d0dcd804
>> MD5 (R-3/R-3.4.3.tar.gz) = bc55db54f992fda9049201ca62d2a584
>>
>> 6474d9791fff6a74936296bde3fcb569477f5958e4326189bd6e5ab878e0cd4f? AUTHORS
>> e6d6a009505e345fe949e1310334fcb0747f28dae2856759de102ab66b722cb4? COPYING
>> 6095e9ffa777dd22839f7801aa845b31c9ed07f3d6bf8a26dc5d2dec8ccc0ef3  
>> COPYING.LIB
>> 7936facb07e752869342808b9c8879d0e270b1a9ec92b67ef4dd87496abfef0a? FAQ
>> f87461be6cbaecc4dce44ac58e5bd52364b0491ccdadaf846cb9b452e9550f31? INSTALL
>> 97a082deb0a67a341f2c88a881c56a409a81c1af697732c3b15d8226d3970231? NEWS
>> 4e21b62f515b749f80997063fceab626d7258c7d650e81a662ba8e0640f12f62? NEWS.0
>> 12b30c724117b1b2b11484673906a6dcd48a361f69fc420b36194f9218692d01? NEWS.1
>> a10f84be31f897456a31d31690df2fdc3f21a197f28b4d04332cc85005dcd0d2? NEWS.2
>> 7a3cb831de5b4151e1f890113ed207527b7d4b16df9ec6b35e0964170007f426  
>> R-latest.tar.gz
>> 2fdd3e90f23f32692d4b3a0c0452f2c219a10882033d1774f8cadf25886c3ddc? README
>> 408737572ecc6e1135fdb2cf7a9dbb1a6cb27967c757f1771b8c39d1fd2f1ab9  
>> RESOURCES
>> 52f934a4e8581945cbc1ba234932749066b5744cbd3b1cb467ba6ef164975163? THANKS
>> 598f9c6b562c7106f741b9cdc2cc7d0bae364645f103e6ecb49e57625e28308b  
>> VERSION-INFO.dcf
>> 7a3cb831de5b4151e1f890113ed207527b7d4b16df9ec6b35e0964170007f426  
>> R-3/R-3.4.3.tar.gz
>>
>> This is the relevant part of the NEWS file
>>
>> CHANGES IN R 3.4.3:
>>
>> ?? INSTALLATION on a UNIX-ALIKE:
>>
>> ???? * A workaround has been added for the changes in location of
>> ?????? time-zone files in macOS 10.13 'High Sierra' and again in
>> ?????? 10.13.1, so the default time zone is deduced correctly from the
>> ?????? system setting when R is configured with --with-internal-tzcode
>> ?????? (the default on macOS).
>>
>> ???? * R CMD javareconf has been updated to recognize the use of a Java
>> ?????? 9 SDK on macOS.
>>
>> ?? BUG FIXES:
>>
>> ???? * raw(0) & raw(0) and raw(0) | raw(0) again return raw(0) (rather
>> ?????? than logical(0)).
>>
>> ???? * intToUtf8() converts integers corresponding to surrogate code
>> ?????? points to NA rather than invalid UTF-8, as well as values larger
>> ?????? than the current Unicode maximum of 0x10FFFF.? (This aligns with
>> ?????? the current RFC3629.)
>>
>> ???? * Fix calling of methods on S4 generics that dispatch on ... when
>> ?????? the call contains ....
>>
>> ???? * Following Unicode 'Corrigendum 9', the UTF-8 representations of
>> ?????? U+FFFE and U+FFFF are now regarded as valid by utf8ToInt().
>>
>> ???? * range(c(TRUE, NA), finite = TRUE) and similar no longer return
>> ?????? NA. (Reported by Lukas Stadler.)
>>
>> ???? * The self starting function attr(SSlogis, "initial") now also
>> ?????? works when the y values have exact minimum zero and is slightly
>> ?????? changed in general, behaving symmetrically in the y range.
>>
>> ???? * The printing of named raw vectors is now formatted nicely as for
>> ?????? other such atomic vectors, thanks to Lukas Stadler.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jackarnestad at gmail.com  Sat Dec  2 19:43:06 2017
From: jackarnestad at gmail.com (Jack Arnestad)
Date: Sat, 2 Dec 2017 13:43:06 -0500
Subject: [R] How can you find the optimal number of values to randomly
 sample to optimize random forest classification without trial and error?
Message-ID: <CADbGCViBpZoqW-iNhi1JR=25K-+mTvyrS7zzjuNg4FZaUZMK4A@mail.gmail.com>

I have data set up like the following:

control1 <- sample(1:75, 3947398, replace=TRUE)
control2 <- sample(1:75, 28793, replace=TRUE)
control3 <- sample(1:100, 392733, replace=TRUE)
control4 <- sample(1:75, 858383, replace=TRUE)
patient1 <- sample(1:100, 28048, replace=TRUE)
patient2 <- sample(1:50, 80400, replace=TRUE)
patient3 <- sample(1:100, 48239, replace=TRUE)
control <- list(control1, control2, control3, control4)
patient <- list(patient1, patient2, patient3)

To classify these samples as either control or patient, I want make
frequency distributions of presence of each of the 100 variables being
considered. To do this, I randomly sample "s" values from each sample and
generate a frequency vector of length 100. This is how I would do it:

control_s <- list()
patient_s <- list()for (i in 1:length(control))
        control_s[[i]] <- sample(control[[i]], s)for (i in 1:length(patient))
        patient_s[[i]] <- sample(patient[[i]], s)

Once I do this, I generate the frequency vector of length 100 as follows:

controlfreq <- list()for (i in 1:length(control_s)){
controlfreq[[i]] <-
    as.data.frame(prop.table(table(factor(
        control_s[[i]], levels = 1:100
    ))))[,2]}
patientfreq <- list()for (i in 1:length(patient_s)){
patientfreq[[i]] <-
    as.data.frame(prop.table(table(factor(
        patient_s[[i]], levels = 1:100
    ))))[,2]}
controlfreq <- t(as.data.frame(controlfreq))
controltrainingset <- transform(controlfreq, status = "control")
patientfreq <- t(as.data.frame(patientfreq))
patienttrainingset <- transform(patientfreq, status = "patient")

dataset <- rbind(controltrainingset, patienttrainingset)

This is the final data frame being used in the classification algorithm. My
goal of this post is to figure out how to identify the optimal "s" value so
that the highest ROC is achieved. I am using "rf" from the caret package to
do classification.

library(caret)
fitControl <-trainControl(method = "LOOCV", classProbs = T, savePredictions = T)
model <- train(status ~ ., data = dataset, method = "rf", trControl =
fitControl)

How can I automate it to start "s" at 5000, change it to another value, and
based on the change in ROC, keep changing "s" to work towards the best
possible "s" value?

Thanks!

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Sat Dec  2 21:35:28 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Sat, 2 Dec 2017 22:35:28 +0200
Subject: [R] source files in temp environment
In-Reply-To: <64bb6367-f912-1228-0f67-73f7fb8f7308@gmail.com>
References: <94ce2684-e1e6-e1a1-ec1f-0f7c7053a850@ufl.edu>
 <64bb6367-f912-1228-0f67-73f7fb8f7308@gmail.com>
Message-ID: <CAGgJW77BhiM8Fm9_GeJ=GvL2fCKJS9Sqa90LfOD1rxL9b2tgtA@mail.gmail.com>

I totally agree with Duncan's last point. I find it hard to reconcile your
early remarks (which indicate a deep knowledge of programming) with the
idea that your code is not built up from combining small(ish) functions.
Small functions would generally be considered best practices. Try searching
on this topic to see discussions and pointers.

On Sat, Dec 2, 2017 at 1:01 PM, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 02/12/2017 5:48 AM, Alexander Shenkin wrote:
>
>> Hi all,
>>
>> I often keep code in separate files for organizational purposes, and
>> source() that code from higher level scripts.  One problem is that those
>> sourced files often create temporary variables that I don't want to keep
>> around.  I could clean up after myself with lots of rm()'s, but that's a
>> pain, and is messy.
>>
>> I'm wondering if one solution might be to source the code in a temporary
>> environment, assign outputs of interest to the .GlobalEnv with <<-, and
>> then delete the environment afterwards.  One way to do this:
>>
>> file.r:
>> temp1 = 1
>> temp2 = 2
>> desired_var <<- temp1 + temp2
>>
>> console:
>> temp_e = new.env()
>> source("file.r", local = temp_e)
>> rm(temp_e)
>>
>> It's a bit messy to create and delete environments, so I tried what
>> others have referred to:
>>
>> source("file.r", local = attach(NULL))
>>
>> This, however, results in a persistent "NULL" environment in the search
>> path.
>>
>>   > search()
>> ".GlobalEnv"            "package:bindrcpp"      "NULL"
>> "tools:rstudio"         "package:stats"         "package:graphics"
>> "package:grDevices"     "package:utils"         "package:datasets"
>> "package:methods"       "Autoloads"             "package:base"
>>
>> Of course, functions are built to encapsulate like this (and do so in
>> their own temporary environment), but in many cases, turning the sourced
>> code into functions is possible but clunky.
>>
>> Any thoughts or suggestions would be much appreciated.
>>
>
> I would wrap the calls in the local() function, or put them in a function
> and call that.  That is,
>
> local({
>   source("file.R", local = TRUE)
> })
>
> or
>
> sourceit <- function() {
>   source("file.R", local = TRUE)
> }
> sourceit()
>
> With respect to your last comment (turning the code in file.R into
> functions which don't leave their locals behind):  I think that would be
> the best solution.  You may find it clunky now, but in the long run it
> likely will help you to make better code.
>
> Duncan Murdoch
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From traxplayer at gmail.com  Sun Dec  3 02:06:08 2017
From: traxplayer at gmail.com (=?UTF-8?Q?Martin_M=C3=B8ller_Skarbiniks_Pedersen?=)
Date: Sun, 3 Dec 2017 02:06:08 +0100
Subject: [R] Rcpp, dyn.load and C++ problems
Message-ID: <CAGAA5bd0dThYikRtvuhNwtzDDo-1t0pub6+saU7heYxXMEAfjQ@mail.gmail.com>

Hi,

  I have written a small C++ function and compile it.
  However in R I can't see the function I have defined in C++.
  I have read some web-pages about Rcpp and C++ but it is a bit confusion
for me.

Anyway,
  This is the C++-code:

#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
List compute_values_cpp(int totalPoints = 1e5, double angle_increment =
0.01, int radius = 400, double grow = 3.64) {
  double xn = 0.5;
  double angle = 0.1;
  double xn_plus_one, yn_plus_one;
  NumericVector x(totalPoints);
  NumericVector y(totalPoints);

  for (int i=0; i<totalPoints; i++) {
    xn_plus_one = xn*cos(angle)*radius;
    yn_plus_one = xn*sin(angle)*radius;
    angle += angle_increment;
    xn = grow*xn*(1-xn);
    x[i] = xn_plus_one;
    y[i] = yn_plus_one;
  }
  return List::create(Rcpp::Named("x") = x, Rcpp::Named("y") = y);
}

And I compile it like this:
PKG_CXXFLAGS=$(Rscript -e 'Rcpp:::CxxFlags()') \
PKG_LIBS=$(Rscript -e 'Rcpp:::LdFlags()')  \
R CMD SHLIB logistic_map.cpp
without problems and I get a logistic_map.so file as expected.

However in R:
R> dyn.load("logistic_map.so")
R> compute_values_cpp()
Error in compute_values_cpp() :
  could not find function "compute_values_cpp"

Please advise,
  What piece of the puzzle is missing?

Regards
Martin M. S. Pedersen

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Sun Dec  3 05:23:47 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Sun, 3 Dec 2017 06:23:47 +0200
Subject: [R] Rcpp, dyn.load and C++ problems
In-Reply-To: <CAGAA5bd0dThYikRtvuhNwtzDDo-1t0pub6+saU7heYxXMEAfjQ@mail.gmail.com>
References: <CAGAA5bd0dThYikRtvuhNwtzDDo-1t0pub6+saU7heYxXMEAfjQ@mail.gmail.com>
Message-ID: <CAGgJW74kdFOovifZDX0HLAv4cEBssadYchjD=bTUzFo-vDnfJw@mail.gmail.com>

.Call("compute_values_cpp")
Also, if you were passing arguments to the C++ function you would need to
declare the function differently.
Do a search on "Rcpp calling C++ functions from R"

HTH,
Eric


On Sun, Dec 3, 2017 at 3:06 AM, Martin M?ller Skarbiniks Pedersen <
traxplayer at gmail.com> wrote:

> Hi,
>
>   I have written a small C++ function and compile it.
>   However in R I can't see the function I have defined in C++.
>   I have read some web-pages about Rcpp and C++ but it is a bit confusion
> for me.
>
> Anyway,
>   This is the C++-code:
>
> #include <Rcpp.h>
> using namespace Rcpp;
>
> // [[Rcpp::export]]
> List compute_values_cpp(int totalPoints = 1e5, double angle_increment =
> 0.01, int radius = 400, double grow = 3.64) {
>   double xn = 0.5;
>   double angle = 0.1;
>   double xn_plus_one, yn_plus_one;
>   NumericVector x(totalPoints);
>   NumericVector y(totalPoints);
>
>   for (int i=0; i<totalPoints; i++) {
>     xn_plus_one = xn*cos(angle)*radius;
>     yn_plus_one = xn*sin(angle)*radius;
>     angle += angle_increment;
>     xn = grow*xn*(1-xn);
>     x[i] = xn_plus_one;
>     y[i] = yn_plus_one;
>   }
>   return List::create(Rcpp::Named("x") = x, Rcpp::Named("y") = y);
> }
>
> And I compile it like this:
> PKG_CXXFLAGS=$(Rscript -e 'Rcpp:::CxxFlags()') \
> PKG_LIBS=$(Rscript -e 'Rcpp:::LdFlags()')  \
> R CMD SHLIB logistic_map.cpp
> without problems and I get a logistic_map.so file as expected.
>
> However in R:
> R> dyn.load("logistic_map.so")
> R> compute_values_cpp()
> Error in compute_values_cpp() :
>   could not find function "compute_values_cpp"
>
> Please advise,
>   What piece of the puzzle is missing?
>
> Regards
> Martin M. S. Pedersen
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From traxplayer at gmail.com  Sun Dec  3 20:00:13 2017
From: traxplayer at gmail.com (=?UTF-8?Q?Martin_M=C3=B8ller_Skarbiniks_Pedersen?=)
Date: Sun, 3 Dec 2017 20:00:13 +0100
Subject: [R] Rcpp, dyn.load and C++ problems
In-Reply-To: <CAGgJW74kdFOovifZDX0HLAv4cEBssadYchjD=bTUzFo-vDnfJw@mail.gmail.com>
References: <CAGAA5bd0dThYikRtvuhNwtzDDo-1t0pub6+saU7heYxXMEAfjQ@mail.gmail.com>
 <CAGgJW74kdFOovifZDX0HLAv4cEBssadYchjD=bTUzFo-vDnfJw@mail.gmail.com>
Message-ID: <CAGAA5bfcjpwpg3KcYG7zX3grZxdVW7_79QKXUDZA3nrBgyPf8Q@mail.gmail.com>

On 3 December 2017 at 05:23, Eric Berger <ericjberger at gmail.com> wrote:

> .Call("compute_values_cpp")
> Also, if you were passing arguments to the C++ function you would need to
> declare the function differently.
> Do a search on "Rcpp calling C++ functions from R"
>
>
Hi,
  It is still not working.
  $ ./compile.sh
g++  -I/usr/include/R/ -DNDEBUG   -D_FORTIFY_SOURCE=2
-I/home/tusk/R/x86_64-pc-linux-gnu-library/3.4/Rcpp/include -fpic
-march=x86-64 -mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt
-c logistic_map.cpp -o logistic_map.o
g++ -shared -L/usr/lib64/R/lib
-Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -o logistic_map.so
logistic_map.o -L/usr/lib64/R/lib -lR

$ readelf -Ws logistic_map.so  | grep -i comp
    89: 0000000000002a50  2356 FUNC    GLOBAL DEFAULT    9
_Z18compute_values_cppidid
   141: 0000000000002a50  2356 FUNC    GLOBAL DEFAULT    9
_Z18compute_values_cppidid

$ R
R> dyn.load("logistic_map.so")
R> .Call("compute_values_cpp")
Error in .Call("compute_values_cpp") :
  C symbol name "compute_values_cpp" not in load table

Hmm.



> HTH,
> Eric
>
>
> On Sun, Dec 3, 2017 at 3:06 AM, Martin M?ller Skarbiniks Pedersen <
> traxplayer at gmail.com> wrote:
>
>> Hi,
>>
>>   I have written a small C++ function and compile it.
>>   However in R I can't see the function I have defined in C++.
>>   I have read some web-pages about Rcpp and C++ but it is a bit confusion
>> for me.
>>
>> Anyway,
>>   This is the C++-code:
>>
>> #include <Rcpp.h>
>> using namespace Rcpp;
>>
>> // [[Rcpp::export]]
>> List compute_values_cpp(int totalPoints = 1e5, double angle_increment =
>> 0.01, int radius = 400, double grow = 3.64) {
>>   double xn = 0.5;
>>   double angle = 0.1;
>>   double xn_plus_one, yn_plus_one;
>>   NumericVector x(totalPoints);
>>   NumericVector y(totalPoints);
>>
>>   for (int i=0; i<totalPoints; i++) {
>>     xn_plus_one = xn*cos(angle)*radius;
>>     yn_plus_one = xn*sin(angle)*radius;
>>     angle += angle_increment;
>>     xn = grow*xn*(1-xn);
>>     x[i] = xn_plus_one;
>>     y[i] = yn_plus_one;
>>   }
>>   return List::create(Rcpp::Named("x") = x, Rcpp::Named("y") = y);
>> }
>>
>> And I compile it like this:
>> PKG_CXXFLAGS=$(Rscript -e 'Rcpp:::CxxFlags()') \
>> PKG_LIBS=$(Rscript -e 'Rcpp:::LdFlags()')  \
>> R CMD SHLIB logistic_map.cpp
>> without problems and I get a logistic_map.so file as expected.
>>
>> However in R:
>> R> dyn.load("logistic_map.so")
>> R> compute_values_cpp()
>> Error in compute_values_cpp() :
>>   could not find function "compute_values_cpp"
>>
>> Please advise,
>>   What piece of the puzzle is missing?
>>
>> Regards
>> Martin M. S. Pedersen
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>


-- 
Til uvedkommende, der l?ser med: Der er ingen grund til at l?se min
mail. Jeg har intet at g?re med FARC, al-Jihad, al-Qaida, Hamas, Hizb
al-Mujahidin eller ETA. Jeg har aldrig gjort Zakat, g?r ikke ind for
Istishad, har ikke lavet en bilbombe eller kernev?ben og jeg ved
d?rligt nok, hvad Al Manar og ????? betyder.  Men tak for den udviste
interesse.

Leve Ligemageriet!
Styrk p?belv?ldet!
Bevar misundelsesafgifterne og cafepengene!
Hurra for ?ldrebyrden!

	[[alternative HTML version deleted]]


From traxplayer at gmail.com  Sun Dec  3 20:04:49 2017
From: traxplayer at gmail.com (=?UTF-8?Q?Martin_M=C3=B8ller_Skarbiniks_Pedersen?=)
Date: Sun, 3 Dec 2017 20:04:49 +0100
Subject: [R] Rcpp, dyn.load and C++ problems
In-Reply-To: <CAGgJW74kdFOovifZDX0HLAv4cEBssadYchjD=bTUzFo-vDnfJw@mail.gmail.com>
References: <CAGAA5bd0dThYikRtvuhNwtzDDo-1t0pub6+saU7heYxXMEAfjQ@mail.gmail.com>
 <CAGgJW74kdFOovifZDX0HLAv4cEBssadYchjD=bTUzFo-vDnfJw@mail.gmail.com>
Message-ID: <CAGAA5bet+de2LLiLfV0Op32gLE0kT-Tfnjt84kELv4B6PhrfEA@mail.gmail.com>

On 3 December 2017 at 05:23, Eric Berger <ericjberger at gmail.com> wrote:

>
> Do a search on "Rcpp calling C++ functions from R"
>

Thanks. However search for "Rcpp calling C++ functions from R" gives a lot
of result but I think
some of them are outdated and others don't agree with each other.

Can you point to a specific good on-line guide for me?

Regards
Martin

	[[alternative HTML version deleted]]


From peter.langfelder at gmail.com  Sun Dec  3 20:08:27 2017
From: peter.langfelder at gmail.com (Peter Langfelder)
Date: Sun, 3 Dec 2017 11:08:27 -0800
Subject: [R] Rcpp, dyn.load and C++ problems
In-Reply-To: <CAGAA5bet+de2LLiLfV0Op32gLE0kT-Tfnjt84kELv4B6PhrfEA@mail.gmail.com>
References: <CAGAA5bd0dThYikRtvuhNwtzDDo-1t0pub6+saU7heYxXMEAfjQ@mail.gmail.com>
 <CAGgJW74kdFOovifZDX0HLAv4cEBssadYchjD=bTUzFo-vDnfJw@mail.gmail.com>
 <CAGAA5bet+de2LLiLfV0Op32gLE0kT-Tfnjt84kELv4B6PhrfEA@mail.gmail.com>
Message-ID: <CA+hbrhUrSrKUwkMh65dDFnegU0ZHf20y2ZB0nfV87nqxwnJh2w@mail.gmail.com>

I would go to the source, in this case Dirk Eddelbuettel's (I hope I
spelled it correctly) documentation for Rcpp:

http://dirk.eddelbuettel.com/code/rcpp/Rcpp-attributes.pdf

Note that you need to do

sourceCpp("logistic_map.cpp")

in R instead of building and dyn.load()-ing the object.

HTH,

Peter

On Sun, Dec 3, 2017 at 11:04 AM, Martin M?ller Skarbiniks Pedersen
<traxplayer at gmail.com> wrote:
> On 3 December 2017 at 05:23, Eric Berger <ericjberger at gmail.com> wrote:
>
>>
>> Do a search on "Rcpp calling C++ functions from R"
>>
>
> Thanks. However search for "Rcpp calling C++ functions from R" gives a lot
> of result but I think
> some of them are outdated and others don't agree with each other.
>
> Can you point to a specific good on-line guide for me?
>
> Regards
> Martin
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From edd at debian.org  Sun Dec  3 20:19:52 2017
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 3 Dec 2017 13:19:52 -0600
Subject: [R] Rcpp, dyn.load and C++ problems
In-Reply-To: <CAGAA5bd0dThYikRtvuhNwtzDDo-1t0pub6+saU7heYxXMEAfjQ@mail.gmail.com>
References: <CAGAA5bd0dThYikRtvuhNwtzDDo-1t0pub6+saU7heYxXMEAfjQ@mail.gmail.com>
Message-ID: <23076.20056.49654.905318@bud.eddelbuettel.com>


Martin,

You are making your life way too complicated.

There are a number of things I would do differently:

0) Wrong list. Rcpp has its down, rcpp-devel, and I basically do not read
this and would have missed this were it not for luck.

On 3 December 2017 at 02:06, Martin M?ller Skarbiniks Pedersen wrote:
|   I have read some web-pages about Rcpp and C++ but it is a bit confusion
| for me.

1) Keep reading.

| And I compile it like this:
| PKG_CXXFLAGS=$(Rscript -e 'Rcpp:::CxxFlags()') \
| PKG_LIBS=$(Rscript -e 'Rcpp:::LdFlags()')  \
| R CMD SHLIB logistic_map.cpp
| without problems and I get a logistic_map.so file as expected.

2) Possible but too complicated. Read on.
 
| However in R:
| R> dyn.load("logistic_map.so")

3) You never ever need that with Rcpp and its tool, unless you insist on
redoing thing by hand in which case you _must_ use SEXP .Call(SEXP a, ...)

| Please advise,
|   What piece of the puzzle is missing?

4) Keep reading. I will pivot to you other mails. Solution below.

On 3 December 2017 at 20:00, Martin M?ller Skarbiniks Pedersen wrote:
| Hi,
|   It is still not working.
|   $ ./compile.sh

5) Still wrong.

On 3 December 2017 at 20:04, Martin M?ller Skarbiniks Pedersen wrote:
| Thanks. However search for "Rcpp calling C++ functions from R" gives a lot
| of result but I think
| some of them are outdated and others don't agree with each other.
| 
| Can you point to a specific good on-line guide for me?

6) Call me crazy but maybe the nine vignettes included with the package?

In essence you _complitely_ missed what Rcpp Attributes does and shows, as
does the (newer) Rcpp Introduction vignette.

More crazy, your file was actually 100% correct. I just added three lines to
_also_ execute R code (and I indented just for clarity)

    /*** R
    res <- compute_values_cpp()
    str(res)
    */

Then in R:

    R> library(Rcpp)
    R> sourceCpp("/tmp/mmsp.cpp")
    
    R> res <- compute_values_cpp()
    
    R> str(res)
    List of 2
     $ x: num [1:100000] 199 362 118 302 262 ...
     $ y: num [1:100000] 20 40 14.3 39.5 36.9 ...
    
    R> 

_One call_ of sourceCpp() compiles AND links AND loads AND runs the example R
code (which is optional).

For reference, the current mmsp.cpp follows.

Dirk


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
List compute_values_cpp(int totalPoints = 1e5, double angle_increment =
0.01, int radius = 400, double grow = 3.64) {
  double xn = 0.5;
  double angle = 0.1;
  double xn_plus_one, yn_plus_one;
  NumericVector x(totalPoints);
  NumericVector y(totalPoints);

  for (int i=0; i<totalPoints; i++) {
    xn_plus_one = xn*cos(angle)*radius;
    yn_plus_one = xn*sin(angle)*radius;
    angle += angle_increment;
    xn = grow*xn*(1-xn);
    x[i] = xn_plus_one;
    y[i] = yn_plus_one;
  }
  return List::create(Rcpp::Named("x") = x, Rcpp::Named("y") = y);
}

/*** R
res <- compute_values_cpp()
str(res)
*/

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From traxplayer at gmail.com  Sun Dec  3 20:30:46 2017
From: traxplayer at gmail.com (=?UTF-8?Q?Martin_M=C3=B8ller_Skarbiniks_Pedersen?=)
Date: Sun, 3 Dec 2017 20:30:46 +0100
Subject: [R] Rcpp, dyn.load and C++ problems
In-Reply-To: <23076.20056.49654.905318@bud.eddelbuettel.com>
References: <CAGAA5bd0dThYikRtvuhNwtzDDo-1t0pub6+saU7heYxXMEAfjQ@mail.gmail.com>
 <23076.20056.49654.905318@bud.eddelbuettel.com>
Message-ID: <CAGAA5bfwYEUhXp0wQOOfCDwD9EUUkDHCRDf8HmjDWiiF-JaGSw@mail.gmail.com>

On 3 December 2017 at 20:19, Dirk Eddelbuettel <edd at debian.org> wrote:

Hi Dirk,
  Thanks for your answers. I got a few more questions.

>
> 0) Wrong list. Rcpp has its down, rcpp-devel, and I basically do not read
> this and would have missed this were it not for luck.

OK. I did found the rcpp-devel mailing-list.
But I though it was a developers of the rcpp-package.
So it is ok to post beginners questions to rcpp-devel?

>
> 6) Call me crazy but maybe the nine vignettes included with the package?
>

OK. I will print them and read them.

[...]

>
> Then in R:
>
>     R> library(Rcpp)
>     R> sourceCpp("/tmp/mmsp.cpp")
>

> [...]


>
> _One call_ of sourceCpp() compiles AND links AND loads AND runs the
example R
> code (which is optional).


Basically I was searching for ways to compile to C++ code a single time and
not everything the R code runs.

Like I don't recompile my pure C++ programs everything I run them, only
if I change something.

But that is different with C++ in R? That it is normal to compile the C++
code
for each run?

Regards
Martin

	[[alternative HTML version deleted]]


From edd at debian.org  Sun Dec  3 20:33:18 2017
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 3 Dec 2017 13:33:18 -0600
Subject: [R] Rcpp, dyn.load and C++ problems
In-Reply-To: <CA+hbrhUrSrKUwkMh65dDFnegU0ZHf20y2ZB0nfV87nqxwnJh2w@mail.gmail.com>
References: <CAGAA5bd0dThYikRtvuhNwtzDDo-1t0pub6+saU7heYxXMEAfjQ@mail.gmail.com>
 <CAGgJW74kdFOovifZDX0HLAv4cEBssadYchjD=bTUzFo-vDnfJw@mail.gmail.com>
 <CAGAA5bet+de2LLiLfV0Op32gLE0kT-Tfnjt84kELv4B6PhrfEA@mail.gmail.com>
 <CA+hbrhUrSrKUwkMh65dDFnegU0ZHf20y2ZB0nfV87nqxwnJh2w@mail.gmail.com>
Message-ID: <23076.20862.960363.495463@bud.eddelbuettel.com>


On 3 December 2017 at 11:08, Peter Langfelder wrote:
| I would go to the source, in this case Dirk Eddelbuettel's (I hope I
| spelled it correctly)

You did. Take a point :)

| documentation for Rcpp:
| 
| http://dirk.eddelbuettel.com/code/rcpp/Rcpp-attributes.pdf

Yup.  And

    RShowDoc("Rcpp-attributes", package="Rcpp")

in R is even easier.

On 3 December 2017 at 13:19, Dirk Eddelbuettel wrote:
| 0) Wrong list. Rcpp has its down, rcpp-devel, and I basically do not read
| this and would have missed this were it not for luck.

"R has its own list, rcpp-devel" is what I meant to write.  For completeness,
we can also test on the command-line as Martin and what I do a lot myself:

edd at bud:~$ r -lRcpp -e'sourceCpp("/tmp/mmsp.cpp")'

R> res <- compute_values_cpp()

R> str(res)

Attaching package: ?utils?

The following objects are masked from ?package:Rcpp?:

    .DollarNames, prompt

List of 2
 $ x: num [1:100000] 199 362 118 302 262 ...
 $ y: num [1:100000] 20 40 14.3 39.5 36.9 ...
You have mail in /var/mail/edd
edd at bud:~$


That uses littler which I like, Rscript is almost the same (but has no -l
switch for libraries and fails to load the methods package which we sometimes
need).

Dirk


-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From traxplayer at gmail.com  Sun Dec  3 20:42:58 2017
From: traxplayer at gmail.com (=?UTF-8?Q?Martin_M=C3=B8ller_Skarbiniks_Pedersen?=)
Date: Sun, 3 Dec 2017 20:42:58 +0100
Subject: [R] Rcpp, dyn.load and C++ problems
In-Reply-To: <23076.20862.960363.495463@bud.eddelbuettel.com>
References: <CAGAA5bd0dThYikRtvuhNwtzDDo-1t0pub6+saU7heYxXMEAfjQ@mail.gmail.com>
 <CAGgJW74kdFOovifZDX0HLAv4cEBssadYchjD=bTUzFo-vDnfJw@mail.gmail.com>
 <CAGAA5bet+de2LLiLfV0Op32gLE0kT-Tfnjt84kELv4B6PhrfEA@mail.gmail.com>
 <CA+hbrhUrSrKUwkMh65dDFnegU0ZHf20y2ZB0nfV87nqxwnJh2w@mail.gmail.com>
 <23076.20862.960363.495463@bud.eddelbuettel.com>
Message-ID: <CAGAA5beVBS44JoPTEDH1Sm=Ft9NXJN2nFJfVh+owntMX9CZLvQ@mail.gmail.com>

>
> > edd at bud:~$ r -lRcpp -e'sourceCpp("/tmp/mmsp.cpp")'


???

What is r in this case ? A alias for something ?

Regards
Martin

	[[alternative HTML version deleted]]


From jeremiejuste at gmail.com  Sun Dec  3 20:55:29 2017
From: jeremiejuste at gmail.com (Jeremie Juste)
Date: Sun, 03 Dec 2017 20:55:29 +0100
Subject: [R] [FORGED] Re:  R vs PYTHON vs SAS vs SPSS?
In-Reply-To: <ab5c9535-35bf-ede3-2df7-372ee5f32c32@auckland.ac.nz> (Rolf
 Turner's message of "Fri, 1 Dec 2017 22:52:43 +1300")
References: <FA6EA000ABFF4AA4AEDC574799E849A2@OwnerPC>
 <CAP+bYWC_=YmLvcxkJ_KbpDeee1N2WbibPLb4EX55dTb-vU4FPQ@mail.gmail.com>
 <ab5c9535-35bf-ede3-2df7-372ee5f32c32@auckland.ac.nz>
Message-ID: <874lp7powe.fsf@freegnu.noherd.org>

Rolf Turner <r.turner at auckland.ac.nz> writes:

> On 01/12/17 20:33, Hasan Diwan wrote:
>
>> Yes.
>
> Very true.  But some *thinking* is required; that often proves to be a
> formidable stumbling block.

Or one of the best decision you'll ever take.   You cannot master SAS without expensive
courses and information does not spread as fast as in R. The real power
in R is it's community in my opinion. "We think therefore we
R". We think as a community. Welcome :-)


Best,
Jeremie


From edd at debian.org  Sun Dec  3 20:59:14 2017
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 3 Dec 2017 13:59:14 -0600
Subject: [R] Rcpp, dyn.load and C++ problems
In-Reply-To: <CAGAA5bfwYEUhXp0wQOOfCDwD9EUUkDHCRDf8HmjDWiiF-JaGSw@mail.gmail.com>
References: <CAGAA5bd0dThYikRtvuhNwtzDDo-1t0pub6+saU7heYxXMEAfjQ@mail.gmail.com>
 <23076.20056.49654.905318@bud.eddelbuettel.com>
 <CAGAA5bfwYEUhXp0wQOOfCDwD9EUUkDHCRDf8HmjDWiiF-JaGSw@mail.gmail.com>
Message-ID: <23076.22418.548676.771159@bud.eddelbuettel.com>


On 3 December 2017 at 20:30, Martin M?ller Skarbiniks Pedersen wrote:
| On 3 December 2017 at 20:19, Dirk Eddelbuettel <edd at debian.org> wrote:
|   Thanks for your answers. I got a few more questions.
| 
| >
| > 0) Wrong list. Rcpp has its down, rcpp-devel, and I basically do not read
| > this and would have missed this were it not for luck.
| 
| OK. I did found the rcpp-devel mailing-list.
| But I though it was a developers of the rcpp-package.
| So it is ok to post beginners questions to rcpp-devel?

Very much so. Just peruse the archive. We have a non-public and very quiet
list Rcpp-core for development of Rcpp -- as opposed 'developing with Rcpp'.

| Basically I was searching for ways to compile to C++ code a single time and
| not everything the R code runs.

A package does that for you. Just how Rcpp itself contains compiled code. Or
any of the ~ 1250 packages on CRAN. Each compiled once, running as often as
you want thereafter.
 
On 3 December 2017 at 20:42, Martin M?ller Skarbiniks Pedersen wrote:
| >
| > > edd at bud:~$ r -lRcpp -e'sourceCpp("/tmp/mmsp.cpp")'
| 
| 
| ???
| 
| What is r in this case ? A alias for something ?

Littler, another (CRAN) package of mine. I explained that a few lines
lower. If it still confuses you imagine you saw

$ Rscript -e 'Rcpp::sourceCpp("/tmp/mmsp.cpp")'

Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From betsy.mccoach at uconn.edu  Sun Dec  3 23:42:36 2017
From: betsy.mccoach at uconn.edu (Mccoach, D. Betsy)
Date: Sun, 3 Dec 2017 22:42:36 +0000
Subject: [R] DATIC 2018 Summer Workshops Using R
Message-ID: <BLUPR0501MB868F6E0BC6CF475D678F6D5E63F0@BLUPR0501MB868.namprd05.prod.outlook.com>

DATIC (www.datic.uconn.edu<http://www.datic.uconn.edu>) is offering 4 workshops at the University of Connecticut in June, 2018: Mixture Modeling, Introduction to Data Analysis in R, Multilevel Modeling in R, and Dyadic Analysis with R.  Registration is now open.  Go to www.datic.uconn.edu<http://www.datic.uconn.edu>  for more information and to register for the workshops.

Mixture Modeling
June 4-6, 2018
Dr. Eric Loken

This 3-day mixture modeling workshop will survey techniques for exploring heterogeneous latent structure in data. We will begin by defining a variety of mixture models. The main focus will be on latent class analysis (LCA) and latent profile analysis (LPA), with applications in health and education. Additional models will include mixture regression models, mixture IRT, k-means clustering, and growth mixture models for longitudinal data. The course will emphasize hands-on work by participants, who will also be encouraged to make connections to their own data, learning to execute many of these models in R. Particular attention will be paid to issues that arise in applied settings including model assumptions, parameter estimation, and interpretation.

Introduction to Data Analysis in R
Instructor: Dr. Randi L. Garcia
Two separate sessions of the R workshop are being offered.
Session 1: June 7 ? June 8, 2018
?        Thursday and Friday prior to Multilevel Modeling with R Workshop
Session 2: June 21 ? June 22, 2018
?         Thursday and Friday prior to Dyadic Data Analysis with R workshop

Are you curious about using R for data analysis? Have you been thinking about making the switch to R, but don?t know where to start? This two-day workshop is the perfect quick start guide to analyzing your data with R. We will cover the fundamentals of data analysis in R with a special focus on translating your existing knowledge and skills from other software (e.g., SPSS) into R. The goal of this workshop is to develop proficiency in R for data preparation and preliminary data analysis. We will build confidence in importing data from different sources into RStudio and getting that data ready for any advanced technique you might then employ. Among the topics to be covered are intro to the RStudio environment, packages, and RMarkdown, data manipulation, data visualization, correlations, reliability tests, basic inference tests, ANOVA, linear regression, Exploratory Factor Analysis (EFA), Confirmatory Factor Analysis (CFA), and more. Instruction on the specific statistics and statistical models will be minimal to zero. It is assumed that you already know how to do these analyses, but you want to see how to do them in R. You do not need to be registered for any other DATIC workshops to enroll in the 2 day Introduction to Data Analysis in R workshop.

Multilevel Modeling Using R Workshop
June 11-15, 2018
Drs. D. Betsy McCoach & Randi Garcia


This workshop covers the basics and applications of multilevel modeling with extensions to more complex designs. Participants will learn how to analyze both organizational and longitudinal (mostly growth curve) data using multilevel modeling and to interpret the results from their analyses. Although the workshop does not require any prior knowledge or experience with multilevel modeling, participants are expected to have a working knowledge of multiple regression. The emphasis will be practical with minimal emphasis on statistical theory, but those seeking more statistical information can arrange an individualized session with the instructors. All analyses will be demonstrated using R. Instruction will consist of lectures, computer demonstrations of data analyses, and hands-on opportunities to analyze practice data sets using R. The workshop emphasizes practical applications and places minimal emphasis on statistical theory.   No prior familiarity with R is required, but if you have never used R and want to gain a general proficiency working with data in R, we encourage you to take the two-day DATIC Intro to R and RStudio workshop held on Thursday, June 7, through Friday, June 8, 2018.



Dyadic Data Analysis with R
June 25 ? June 29, 2018
Instructors: Drs. Randi L. Garcia and David A. Kenny



The Dyadic Data Analysis workshop focuses on the analysis of dyadic data when both members of a dyad are measured on the same variables. All analyses will use multilevel modeling in R via the RStudio graphical interface. Participants will learn how to analyze dyadic data and to interpret the results from their analyses. Among the topics to be covered are the vocabulary of dyadic analysis, non-independence, data structures, and the Actor-Partner Interdependence Model. We also discuss mediation and moderation of dyadic effects. On day 4, participants choose from one of two break-out sessions: 1) the analysis of over-time dyadic data (e.g., growth curve models) or 2) dyadic data analysis with SEM using the lavaan R package (e.g., Actor?Partner Interdependence Model and Common Fate Model). The discussion of over?time data is limited to one day so the workshop should not be construed as workshop on longitudinal dyadic analysis. Participants should have a working knowledge of multiple regression. No prior familiarity with R is required, but if you have never used R and want to gain a general proficiency working with data in R, we encourage you to take the two-day DATIC Intro to R and RStudio workshop.


D. Betsy McCoach
Professor, Measurement, Evaluation, and Assessment program
Department of Educational Psychology
University of Connecticut
249 Glenbrook Road, Unit 3064
Storrs, CT 06269-3064
860-486-0183
betsy at uconn.edu<mailto:betsy at uconn.edu>

Here?s a link to my newest article with former student Jessica Flake (she is first author, and it is her dissertation): http://www.tandfonline.com/eprint/tPmpDnkNXfpfPMrXfJ3N/full


	[[alternative HTML version deleted]]


From andreas.leha at med.uni-goettingen.de  Mon Dec  4 04:35:39 2017
From: andreas.leha at med.uni-goettingen.de (Andreas Leha)
Date: Mon, 4 Dec 2017 04:35:39 +0100
Subject: [R] PSOCK cluster and renice
Message-ID: <87d13vtbas.fsf@ukes-ams26-134.ams.med.uni.goettingen.de>

Hi all,

Is it possible to use the 'renice' option together with parallel
clusters of type 'PSOCK'?  The help page for parallel::makeCluster is
not specific about which options are supported on which types and I am
getting the following message when passing renice = 19 :

> cl <- parallel::makeCluster(2, renice = 19)
nice: ?+19?: No such file or directory

Kind regards,
Andreas


From henrik.bengtsson at gmail.com  Mon Dec  4 05:15:14 2017
From: henrik.bengtsson at gmail.com (Henrik Bengtsson)
Date: Sun, 3 Dec 2017 20:15:14 -0800
Subject: [R] PSOCK cluster and renice
In-Reply-To: <87d13vtbas.fsf@ukes-ams26-134.ams.med.uni.goettingen.de>
References: <87d13vtbas.fsf@ukes-ams26-134.ams.med.uni.goettingen.de>
Message-ID: <CAFDcVCTNPKx=xPnt7rbYVw6AXCYj4kqaf-fDJdRnzqNg-9ENZw@mail.gmail.com>

Looks like a bug to me due to wrong assumptions about 'nice'
arguments, but could be because a "non-standard" 'nice' is used.  If
we do:

> trace(system, tracer = quote(print(command)))
Tracing function "system" in package "base"

we see that the system call used is:

> cl <- parallel::makePSOCKcluster(2L, renice = 19)
Tracing system(cmd, wait = FALSE) on entry
[1] "nice +19 '/usr/lib/R/bin/Rscript'
--default-packages=datasets,utils,grDevices,graphics,stats,methods -e
'parallel:::.slaveRSOCK()' MASTER=localhost PORT=11146 OUT=/dev/null
TIMEOUT=2592000 XDR=TRUE"
nice: ?+19?: No such file or directory
^C

The code that prepends that 'nice +19' is in parallel:::newPSOCKnode:

    if (!is.na(renice) && renice)
        cmd <- sprintf("nice +%d %s", as.integer(renice), cmd)

I don't know where that originates from and on what platform it was
tests/validated.  On Ubuntu 16.04, CentOS 6.6, and CentOS 7.4, I have
'nice' from "GNU coreutils" and they all complain about using '+',
e.g.

$ nice +19 date
nice: +19: No such file or directory

but '-' works:

$ nice -19 date
Sun Dec  3 20:01:31 PST 2017

Neither 'nice --help' nor 'man help' mention the use of a +n option.


WORKAROUND:  As a workaround, you can use:

cl <- future::makeClusterPSOCK(2L, rscript = c("nice",
"--adjustment=10", file.path(R.home("bin"), "Rscript")))

which is backward compatible with parallel::makePSOCKcluster() but
provides you with more detailed control.  Try adding verbose = TRUE to
see what the exact call looks like.

/Henrik


On Sun, Dec 3, 2017 at 7:35 PM, Andreas Leha
<andreas.leha at med.uni-goettingen.de> wrote:
> Hi all,
>
> Is it possible to use the 'renice' option together with parallel
> clusters of type 'PSOCK'?  The help page for parallel::makeCluster is
> not specific about which options are supported on which types and I am
> getting the following message when passing renice = 19 :
>
>> cl <- parallel::makeCluster(2, renice = 19)
> nice: ?+19?: No such file or directory
>
> Kind regards,
> Andreas
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From andreas.leha at med.uni-goettingen.de  Mon Dec  4 06:06:25 2017
From: andreas.leha at med.uni-goettingen.de (Andreas Leha)
Date: Mon, 4 Dec 2017 06:06:25 +0100
Subject: [R] PSOCK cluster and renice
References: <87d13vtbas.fsf@ukes-ams26-134.ams.med.uni.goettingen.de>
 <CAFDcVCTNPKx=xPnt7rbYVw6AXCYj4kqaf-fDJdRnzqNg-9ENZw@mail.gmail.com>
Message-ID: <87609nt73i.fsf@ukes-ams26-134.ams.med.uni.goettingen.de>

Hi Henrik,

Thanks for the detailed in fast reply!

My guess would be that the confusion comes from the different use of nice and renice.

The workraund you provided work fine!  Thanks a lot.

Best,
Andreas



Henrik Bengtsson <henrik.bengtsson at gmail.com> writes:

> Looks like a bug to me due to wrong assumptions about 'nice'
> arguments, but could be because a "non-standard" 'nice' is used.  If
> we do:
>
>> trace(system, tracer = quote(print(command)))
> Tracing function "system" in package "base"
>
> we see that the system call used is:
>
>> cl <- parallel::makePSOCKcluster(2L, renice = 19)
> Tracing system(cmd, wait = FALSE) on entry
> [1] "nice +19 '/usr/lib/R/bin/Rscript'
> --default-packages=datasets,utils,grDevices,graphics,stats,methods -e
> 'parallel:::.slaveRSOCK()' MASTER=localhost PORT=11146 OUT=/dev/null
> TIMEOUT=2592000 XDR=TRUE"
> nice: ?+19?: No such file or directory
> ^C
>
> The code that prepends that 'nice +19' is in parallel:::newPSOCKnode:
>
>     if (!is.na(renice) && renice)
>         cmd <- sprintf("nice +%d %s", as.integer(renice), cmd)
>
> I don't know where that originates from and on what platform it was
> tests/validated.  On Ubuntu 16.04, CentOS 6.6, and CentOS 7.4, I have
> 'nice' from "GNU coreutils" and they all complain about using '+',
> e.g.
>
> $ nice +19 date
> nice: +19: No such file or directory
>
> but '-' works:
>
> $ nice -19 date
> Sun Dec  3 20:01:31 PST 2017
>
> Neither 'nice --help' nor 'man help' mention the use of a +n option.
>
>
> WORKAROUND:  As a workaround, you can use:
>
> cl <- future::makeClusterPSOCK(2L, rscript = c("nice",
> "--adjustment=10", file.path(R.home("bin"), "Rscript")))
>
> which is backward compatible with parallel::makePSOCKcluster() but
> provides you with more detailed control.  Try adding verbose = TRUE to
> see what the exact call looks like.
>
> /Henrik
>
>
> On Sun, Dec 3, 2017 at 7:35 PM, Andreas Leha
> <andreas.leha at med.uni-goettingen.de> wrote:
>> Hi all,
>>
>> Is it possible to use the 'renice' option together with parallel
>> clusters of type 'PSOCK'?  The help page for parallel::makeCluster is
>> not specific about which options are supported on which types and I am
>> getting the following message when passing renice = 19 :
>>
>>> cl <- parallel::makeCluster(2, renice = 19)
>> nice: ?+19?: No such file or directory
>>
>> Kind regards,
>> Andreas
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From lterlemez at anadolu.edu.tr  Mon Dec  4 11:58:56 2017
From: lterlemez at anadolu.edu.tr (Levent TERLEMEZ)
Date: Mon, 4 Dec 2017 10:58:56 +0000
Subject: [R] ggtern and bquote...
Message-ID: <D64B051E.1CE5A%lterlemez@anadolu.edu.tr>

Dear Users,

What is the proper way to write symbol, superscript, subscript in ggtern/ggplot? I tried every given example, every possible features of ggplot but couldn?t achived. I just want to write P_a, sigma^2, etc, would you please advise me about this problem.

Thanks in advance,
Levent TERLEMEZ




________________________________

Bu elektronik posta ve onunla iletilen b?t?n dosyalar sadece yukar?da isimleri belirtilen ki?iler aras?nda ?zel haberle?me amac?n? ta??makta olup g?nderici taraf?ndan al?nmas? ama?lanan yetkili ger?ek ya da t?zel ki?inin kullan?m?na aittir. E?er bu elektronik posta size yanl??l?kla ula?m??sa, elektronik postan?n i?eri?ini a??klaman?z, kopyalaman?z, y?nlendirmeniz ve kullanman?z kesinlikle yasakt?r. Bu durumda, l?tfen mesaj? geri g?nderiniz ve sisteminizden siliniz. Anadolu ?niversitesi bu mesaj?n i?erdi?i bilgilerin do?rulu?u veya eksiksiz oldu?u konusunda herhangi bir garanti vermemektedir. Bu nedenle bu bilgilerin ne ?ekilde olursa olsun i?eri?inden, iletilmesinden, al?nmas?ndan ve saklanmas?ndan sorumlu de?ildir. Bu mesajdaki g?r??ler yaln?zca g?nderen ki?iye aittir ve Anadolu ?niversitesinin g?r??lerini yans?tmayabilir.

This electronic mail and any files transmitted with it are intended for the private use of the people named above. If you are not the intended recipient and received this message in error, forwarding, copying or use of any of the information is strictly prohibited. Any dissemination or use of this information by a person other than the intended recipient is unauthorized and may be illegal. In this case, please immediately notify the sender and delete it from your system. Anadolu University does not guarantee the accuracy or completeness of any information included in this message. Therefore, by any means Anadolu University is not responsible for the content of the message, and the transmission, reception, storage, and use of the information. The opinions expressed in this message only belong to the sender of it and may not reflect the opinions of Anadolu University.

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Mon Dec  4 14:55:19 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 4 Dec 2017 14:55:19 +0100
Subject: [R] ggtern and bquote...
In-Reply-To: <D64B051E.1CE5A%lterlemez@anadolu.edu.tr>
References: <D64B051E.1CE5A%lterlemez@anadolu.edu.tr>
Message-ID: <B8CC130C-AFFF-4B51-84F4-EF57C3C7F04D@gmail.com>



> On 4 Dec 2017, at 11:58 , Levent TERLEMEZ via R-help <r-help at r-project.org> wrote:
> 
> Dear Users,
> 
> What is the proper way to write symbol, superscript, subscript in ggtern/ggplot? I tried every given example, every possible features of ggplot but couldn?t achived. I just want to write P_a, sigma^2, etc, would you please advise me about this problem.

Did you try expression(P_a)? I don't do much gg-stuff, but I seem to recall that quote() doesn't quite cut it the way it does in base graphics.

-pd

> 
> Thanks in advance,
> Levent TERLEMEZ
> 
> 
> 
> 
> ________________________________
> 
> Bu elektronik posta ve onunla iletilen b?t?n dosyalar sadece yukar?da isimleri belirtilen ki?iler aras?nda ?zel haberle?me amac?n? ta??makta olup g?nderici taraf?ndan al?nmas? ama?lanan yetkili ger?ek ya da t?zel ki?inin kullan?m?na aittir. E?er bu elektronik posta size yanl??l?kla ula?m??sa, elektronik postan?n i?eri?ini a??klaman?z, kopyalaman?z, y?nlendirmeniz ve kullanman?z kesinlikle yasakt?r. Bu durumda, l?tfen mesaj? geri g?nderiniz ve sisteminizden siliniz. Anadolu ?niversitesi bu mesaj?n i?erdi?i bilgilerin do?rulu?u veya eksiksiz oldu?u konusunda herhangi bir garanti vermemektedir. Bu nedenle bu bilgilerin ne ?ekilde olursa olsun i?eri?inden, iletilmesinden, al?nmas?ndan ve saklanmas?ndan sorumlu de?ildir. Bu mesajdaki g?r??ler yaln?zca g?nderen ki?iye aittir ve Anadolu ?niversitesinin g?r??lerini yans?tmayabilir.
> 
> This electronic mail and any files transmitted with it are intended for the private use of the people named above. If you are not the intended recipient and received this message in error, forwarding, copying or use of any of the information is strictly prohibited. Any dissemination or use of this information by a person other than the intended recipient is unauthorized and may be illegal. In this case, please immediately notify the sender and delete it from your system. Anadolu University does not guarantee the accuracy or completeness of any information included in this message. Therefore, by any means Anadolu University is not responsible for the content of the message, and the transmission, reception, storage, and use of the information. The opinions expressed in this message only belong to the sender of it and may not reflect the opinions of Anadolu University.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From love.bohman at sociology.su.se  Mon Dec  4 12:33:25 2017
From: love.bohman at sociology.su.se (Love Bohman)
Date: Mon, 4 Dec 2017 11:33:25 +0000
Subject: [R] Dynamic reference, right-hand side of function
Message-ID: <d200347436c940f695583220f01ac86c@ebox-prod-srv09.win.su.se>

Hi R-users!
Being new to R, and a fairly advanced Stata-user, I guess part of my problem is that my mindset (and probably my language as well) is wrong. Anyway, I have what I guess is a rather simple problem, that I now without success spent days trying to solve.

I have a bunch of datasets imported from Stata that is labelled aa_2000 aa_2001 aa_2002, etc. Each dataset is imported as a matrix, and consists of one column only. The columns consists of integer numbers. I need to convert the data to vectors, which I found several ways to do. I use, for example:
aa_2000 <- as.numeric(aa_2000[,1])
However, when trying to automate the task, so I don't have to write a line of code for each dataset, I get stuck. Since I'm a Stata user, my first attempt is trying to make a loop in order to loop over all datasets. However, I manage to write a loop that works for the left-hand side of the syntax, but not for the right-hand side.
I have included some examples from my struggles to solve the issue below, what they all have in common is that I don't manage to call for any "macro" (is that only a Stata-word?) in the right hand side of the functions. When I try to replace the static reference with a dynamic one (like in the left-hand side), the syntax just doesn't work.

I would very much appreciate some help with this issue!
All the best,
Love

year <- 2002
dataname <- paste0("aa_",year)
assign(paste0(dataname), as.numeric(aa_2002[,1]))

year <- 2003
assign(paste0("aa_",year), as.numeric(aa_2003))

year <- 2005
assign(paste0("aa_",year), aa_2005[,1])

list1 <- c(2000:2007)
list1[c(7)]
assign(paste0("aa_",list1[c(7)]), as.numeric(paste0(aa_2006)))


	[[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Mon Dec  4 15:16:00 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 4 Dec 2017 15:16:00 +0100
Subject: [R] ggtern and bquote...
In-Reply-To: <B8CC130C-AFFF-4B51-84F4-EF57C3C7F04D@gmail.com>
References: <D64B051E.1CE5A%lterlemez@anadolu.edu.tr>
 <B8CC130C-AFFF-4B51-84F4-EF57C3C7F04D@gmail.com>
Message-ID: <23077.22688.642047.971753@stat.math.ethz.ch>

>>>>> peter dalgaard <pdalgd at gmail.com>
>>>>>     on Mon, 4 Dec 2017 14:55:19 +0100 writes:

    >> On 4 Dec 2017, at 11:58 , Levent TERLEMEZ via R-help
    >> <r-help at r-project.org> wrote:
    >> 
    >> Dear Users,
    >> 
    >> What is the proper way to write symbol, superscript,
    >> subscript in ggtern/ggplot? I tried every given example,
    >> every possible features of ggplot but couldn?t achived. I
    >> just want to write P_a, sigma^2, etc, would you please
    >> advise me about this problem.

    > Did you try expression(P_a)? I don't do much gg-stuff, but
    > I seem to recall that quote() doesn't quite cut it the way
    > it does in base graphics.

    > -pd

Yes, I vaguely remember that indeed also for the lattice package
(which is based on 'grid' the same as 'ggplot2' is ..) sometimes
expressions instead of calls are needed, i.e., expression(*)
instead of just quote(*).

However, I think Levent really meant what you'd get by
 expression(P[a]) ?

@Levent: The clue is the need for valid R syntax, and indeed, as
   in LaTeX  x_i often is the i-th element of x,  the R syntax for
   indexing/subsetting is used here, i.e.
    x[i]  for LaTeX  x_i


Last but not least, if Levent really needs bquote() [i.e. substitute()]
then, a final
      as.expression(.)
may be needed :

identical(as.expression(quote(a == 1)),
	     expression(      a == 1))  # --> TRUE

--
Martin Maechler, ETH Zurich


From lterlemez at anadolu.edu.tr  Mon Dec  4 15:37:55 2017
From: lterlemez at anadolu.edu.tr (Levent TERLEMEZ)
Date: Mon, 4 Dec 2017 14:37:55 +0000
Subject: [R] YNT:  ggtern and bquote...
In-Reply-To: <23077.22688.642047.971753@stat.math.ethz.ch>
References: <D64B051E.1CE5A%lterlemez@anadolu.edu.tr>
 <B8CC130C-AFFF-4B51-84F4-EF57C3C7F04D@gmail.com>,
 <23077.22688.642047.971753@stat.math.ethz.ch>
Message-ID: <37273DBD29A7A2408A7736A073A9FA3BE2723E73@mb06.porsuk.anadolu.edu.tr>

Hi,

My example code is this;

x11<-data.frame(A=c(.6,.6,.6),B=c(.20,.20,.20),C=c(0.20,.20,.20))
ggtern(data=x11,aes(A,B,C,xend = c(0.7,.00,0.7),yend = c(.30,.50,.0),zend =c(.0,.50,0.3)))+
    geom_point()+
    theme_showarrows()+geom_segment(size=.5)+
    geom_text_viewport(x=c(.45,.27,.37),y=c(.32,.29,.22),label=as.expression("P_a","P_b","P_c"))

ggtern(data=x11,aes(A,B,C,xend = c(0.7,.00,0.7),yend = c(.30,.50,.0),zend =c(.0,.50,0.3)))+
    geom_point()+
    theme_showarrows()+geom_segment(size=.5)+
    geom_text_viewport(x=c(.45,.27,.37),y=c(.32,.29,.22),label=as.expression(quote(c("P_a","P_b","P_c"))))

In geom_text_viewport (I also tried geom_label and geom_text versions) tried all possible solutions, but i couldn't achieved. R command outputs are like this:

Error in stats::complete.cases(df[, vars, drop = FALSE]) :
  invalid 'type' (expression) of argument


Maybe i am writing the code wrong, i couldn't figure out.

Thanks for your kind answers.



________________________________________
Kimden: Martin Maechler [maechler at stat.math.ethz.ch]
G?nderildi: 04 Aral?k 2017 Pazartesi 16:16
Kime: peter dalgaard
Bilgi: Levent TERLEMEZ; R-help at r-project.org
Konu: Re: [R] ggtern and bquote...

>>>>> peter dalgaard <pdalgd at gmail.com>
>>>>>     on Mon, 4 Dec 2017 14:55:19 +0100 writes:

    >> On 4 Dec 2017, at 11:58 , Levent TERLEMEZ via R-help
    >> <r-help at r-project.org> wrote:
    >>
    >> Dear Users,
    >>
    >> What is the proper way to write symbol, superscript,
    >> subscript in ggtern/ggplot? I tried every given example,
    >> every possible features of ggplot but couldn?t achived. I
    >> just want to write P_a, sigma^2, etc, would you please
    >> advise me about this problem.

    > Did you try expression(P_a)? I don't do much gg-stuff, but
    > I seem to recall that quote() doesn't quite cut it the way
    > it does in base graphics.

    > -pd

Yes, I vaguely remember that indeed also for the lattice package
(which is based on 'grid' the same as 'ggplot2' is ..) sometimes
expressions instead of calls are needed, i.e., expression(*)
instead of just quote(*).

However, I think Levent really meant what you'd get by
 expression(P[a]) ?

@Levent: The clue is the need for valid R syntax, and indeed, as
   in LaTeX  x_i often is the i-th element of x,  the R syntax for
   indexing/subsetting is used here, i.e.
    x[i]  for LaTeX  x_i


Last but not least, if Levent really needs bquote() [i.e. substitute()]
then, a final
      as.expression(.)
may be needed :

identical(as.expression(quote(a == 1)),
             expression(      a == 1))  # --> TRUE

--
Martin Maechler, ETH Zurich


________________________________

Bu elektronik posta ve onunla iletilen b?t?n dosyalar sadece yukar?da isimleri belirtilen ki?iler aras?nda ?zel haberle?me amac?n? ta??makta olup g?nderici taraf?ndan al?nmas? ama?lanan yetkili ger?ek ya da t?zel ki?inin kullan?m?na aittir. E?er bu elektronik posta size yanl??l?kla ula?m??sa, elektronik postan?n i?eri?ini a??klaman?z, kopyalaman?z, y?nlendirmeniz ve kullanman?z kesinlikle yasakt?r. Bu durumda, l?tfen mesaj? geri g?nderiniz ve sisteminizden siliniz. Anadolu ?niversitesi bu mesaj?n i?erdi?i bilgilerin do?rulu?u veya eksiksiz oldu?u konusunda herhangi bir garanti vermemektedir. Bu nedenle bu bilgilerin ne ?ekilde olursa olsun i?eri?inden, iletilmesinden, al?nmas?ndan ve saklanmas?ndan sorumlu de?ildir. Bu mesajdaki g?r??ler yaln?zca g?nderen ki?iye aittir ve Anadolu ?niversitesinin g?r??lerini yans?tmayabilir.

This electronic mail and any files transmitted with it are intended for the private use of the people named above. If you are not the intended recipient and received this message in error, forwarding, copying or use of any of the information is strictly prohibited. Any dissemination or use of this information by a person other than the intended recipient is unauthorized and may be illegal. In this case, please immediately notify the sender and delete it from your system. Anadolu University does not guarantee the accuracy or completeness of any information included in this message. Therefore, by any means Anadolu University is not responsible for the content of the message, and the transmission, reception, storage, and use of the information. The opinions expressed in this message only belong to the sender of it and may not reflect the opinions of Anadolu University.


From esawiek at gmail.com  Mon Dec  4 15:47:14 2017
From: esawiek at gmail.com (Ek Esawi)
Date: Mon, 4 Dec 2017 09:47:14 -0500
Subject: [R] Dynamic reference, right-hand side of function
In-Reply-To: <d200347436c940f695583220f01ac86c@ebox-prod-srv09.win.su.se>
References: <d200347436c940f695583220f01ac86c@ebox-prod-srv09.win.su.se>
Message-ID: <CA+ZkTxtW-VO+5AEBQ1kdwCXwaUVAfJ0AMuU42V_kCotGWfJfYw@mail.gmail.com>

Hi Love,

I am not sure if I understand your question and it will help if you
provided a sample data frame, sample of your code and sample of your output
and the output you desire. Having said that, I think you could use cbind to
join all datasets into a matrix and use the apply family of functions to
achieve what you are after.

On Mon, Dec 4, 2017 at 6:33 AM, Love Bohman <love.bohman at sociology.su.se>
wrote:

> Hi R-users!
> Being new to R, and a fairly advanced Stata-user, I guess part of my
> problem is that my mindset (and probably my language as well) is wrong.
> Anyway, I have what I guess is a rather simple problem, that I now without
> success spent days trying to solve.
>
> I have a bunch of datasets imported from Stata that is labelled aa_2000
> aa_2001 aa_2002, etc. Each dataset is imported as a matrix, and consists of
> one column only. The columns consists of integer numbers. I need to convert
> the data to vectors, which I found several ways to do. I use, for example:
> aa_2000 <- as.numeric(aa_2000[,1])
> However, when trying to automate the task, so I don't have to write a line
> of code for each dataset, I get stuck. Since I'm a Stata user, my first
> attempt is trying to make a loop in order to loop over all datasets.
> However, I manage to write a loop that works for the left-hand side of the
> syntax, but not for the right-hand side.
> I have included some examples from my struggles to solve the issue below,
> what they all have in common is that I don't manage to call for any "macro"
> (is that only a Stata-word?) in the right hand side of the functions. When
> I try to replace the static reference with a dynamic one (like in the
> left-hand side), the syntax just doesn't work.
>
> I would very much appreciate some help with this issue!
> All the best,
> Love
>
> year <- 2002
> dataname <- paste0("aa_",year)
> assign(paste0(dataname), as.numeric(aa_2002[,1]))
>
> year <- 2003
> assign(paste0("aa_",year), as.numeric(aa_2003))
>
> year <- 2005
> assign(paste0("aa_",year), aa_2005[,1])
>
> list1 <- c(2000:2007)
> list1[c(7)]
> assign(paste0("aa_",list1[c(7)]), as.numeric(paste0(aa_2006)))
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From E.Vettorazzi at uke.de  Mon Dec  4 15:48:59 2017
From: E.Vettorazzi at uke.de (Eik Vettorazzi)
Date: Mon, 4 Dec 2017 15:48:59 +0100
Subject: [R] YNT: ggtern and bquote...
In-Reply-To: <37273DBD29A7A2408A7736A073A9FA3BE2723E73@mb06.porsuk.anadolu.edu.tr>
References: <D64B051E.1CE5A%lterlemez@anadolu.edu.tr>
 <B8CC130C-AFFF-4B51-84F4-EF57C3C7F04D@gmail.com>
 <23077.22688.642047.971753@stat.math.ethz.ch>
 <37273DBD29A7A2408A7736A073A9FA3BE2723E73@mb06.porsuk.anadolu.edu.tr>
Message-ID: <5b5d0ad2-4dc9-f7fe-7c9d-8016b5d31358@uke.de>

reading ?plotmath you might notice that "_" isn't the propper syntax for
subscripts. This will work:

ggtern(data=x11,aes(A,B,C,xend = c(0.7,.00,0.7),yend =
c(.30,.50,.0),zend =c(.0,.50,0.3)))+
  geom_point()+
  theme_showarrows()+geom_segment(size=.5)+
geom_text_viewport(x=c(.45,.27,.37),y=c(.32,.29,.22),label=c("P[a]","P[b]","P[c]"),
parse=TRUE)

cheers.

Am 04.12.2017 um 15:37 schrieb Levent TERLEMEZ via R-help:
> Hi,
> 
> My example code is this;
> 
> x11<-data.frame(A=c(.6,.6,.6),B=c(.20,.20,.20),C=c(0.20,.20,.20))
> ggtern(data=x11,aes(A,B,C,xend = c(0.7,.00,0.7),yend = c(.30,.50,.0),zend =c(.0,.50,0.3)))+
>     geom_point()+
>     theme_showarrows()+geom_segment(size=.5)+
>     geom_text_viewport(x=c(.45,.27,.37),y=c(.32,.29,.22),label=as.expression("P_a","P_b","P_c"))
> 
> ggtern(data=x11,aes(A,B,C,xend = c(0.7,.00,0.7),yend = c(.30,.50,.0),zend =c(.0,.50,0.3)))+
>     geom_point()+
>     theme_showarrows()+geom_segment(size=.5)+
>     geom_text_viewport(x=c(.45,.27,.37),y=c(.32,.29,.22),label=as.expression(quote(c("P_a","P_b","P_c"))))
> 
> In geom_text_viewport (I also tried geom_label and geom_text versions) tried all possible solutions, but i couldn't achieved. R command outputs are like this:
> 
> Error in stats::complete.cases(df[, vars, drop = FALSE]) :
>   invalid 'type' (expression) of argument
> 
> 
> Maybe i am writing the code wrong, i couldn't figure out.
> 
> Thanks for your kind answers.
> 
> 
> 
> ________________________________________
> Kimden: Martin Maechler [maechler at stat.math.ethz.ch]
> G?nderildi: 04 Aral?k 2017 Pazartesi 16:16
> Kime: peter dalgaard
> Bilgi: Levent TERLEMEZ; R-help at r-project.org
> Konu: Re: [R] ggtern and bquote...
> 
>>>>>> peter dalgaard <pdalgd at gmail.com>
>>>>>>     on Mon, 4 Dec 2017 14:55:19 +0100 writes:
> 
>     >> On 4 Dec 2017, at 11:58 , Levent TERLEMEZ via R-help
>     >> <r-help at r-project.org> wrote:
>     >>
>     >> Dear Users,
>     >>
>     >> What is the proper way to write symbol, superscript,
>     >> subscript in ggtern/ggplot? I tried every given example,
>     >> every possible features of ggplot but couldn?t achived. I
>     >> just want to write P_a, sigma^2, etc, would you please
>     >> advise me about this problem.
> 
>     > Did you try expression(P_a)? I don't do much gg-stuff, but
>     > I seem to recall that quote() doesn't quite cut it the way
>     > it does in base graphics.
> 
>     > -pd
> 
> Yes, I vaguely remember that indeed also for the lattice package
> (which is based on 'grid' the same as 'ggplot2' is ..) sometimes
> expressions instead of calls are needed, i.e., expression(*)
> instead of just quote(*).
> 
> However, I think Levent really meant what you'd get by
>  expression(P[a]) ?
> 
> @Levent: The clue is the need for valid R syntax, and indeed, as
>    in LaTeX  x_i often is the i-th element of x,  the R syntax for
>    indexing/subsetting is used here, i.e.
>     x[i]  for LaTeX  x_i
> 
> 
> Last but not least, if Levent really needs bquote() [i.e. substitute()]
> then, a final
>       as.expression(.)
> may be needed :
> 
> identical(as.expression(quote(a == 1)),
>              expression(      a == 1))  # --> TRUE
> 
> --
> Martin Maechler, ETH Zurich
> 
> 
> ________________________________
> 
> Bu elektronik posta ve onunla iletilen b?t?n dosyalar sadece yukar?da isimleri belirtilen ki?iler aras?nda ?zel haberle?me amac?n? ta??makta olup g?nderici taraf?ndan al?nmas? ama?lanan yetkili ger?ek ya da t?zel ki?inin kullan?m?na aittir. E?er bu elektronik posta size yanl??l?kla ula?m??sa, elektronik postan?n i?eri?ini a??klaman?z, kopyalaman?z, y?nlendirmeniz ve kullanman?z kesinlikle yasakt?r. Bu durumda, l?tfen mesaj? geri g?nderiniz ve sisteminizden siliniz. Anadolu ?niversitesi bu mesaj?n i?erdi?i bilgilerin do?rulu?u veya eksiksiz oldu?u konusunda herhangi bir garanti vermemektedir. Bu nedenle bu bilgilerin ne ?ekilde olursa olsun i?eri?inden, iletilmesinden, al?nmas?ndan ve saklanmas?ndan sorumlu de?ildir. Bu mesajdaki g?r??ler yaln?zca g?nderen ki?iye aittir ve Anadolu ?niversitesinin g?r??lerini yans?tmayabilir.
> 
> This electronic mail and any files transmitted with it are intended for the private use of the people named above. If you are not the intended recipient and received this message in error, forwarding, copying or use of any of the information is strictly prohibited. Any dissemination or use of this information by a person other than the intended recipient is unauthorized and may be illegal. In this case, please immediately notify the sender and delete it from your system. Anadolu University does not guarantee the accuracy or completeness of any information included in this message. Therefore, by any means Anadolu University is not responsible for the content of the message, and the transmission, reception, storage, and use of the information. The opinions expressed in this message only belong to the sender of it and may not reflect the opinions of Anadolu University.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Eik Vettorazzi

Department of Medical Biometry and Epidemiology
University Medical Center Hamburg-Eppendorf

Martinistrasse 52
building W 34
20246 Hamburg

Phone: +49 (0) 40 7410 - 58243
Fax:   +49 (0) 40 7410 - 57790
Web: www.uke.de/imbe
--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Martina Saurin (komm.)
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING

From pdalgd at gmail.com  Mon Dec  4 16:23:38 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 4 Dec 2017 16:23:38 +0100
Subject: [R] ggtern and bquote...
In-Reply-To: <5b5d0ad2-4dc9-f7fe-7c9d-8016b5d31358@uke.de>
References: <D64B051E.1CE5A%lterlemez@anadolu.edu.tr>
 <B8CC130C-AFFF-4B51-84F4-EF57C3C7F04D@gmail.com>
 <23077.22688.642047.971753@stat.math.ethz.ch>
 <37273DBD29A7A2408A7736A073A9FA3BE2723E73@mb06.porsuk.anadolu.edu.tr>
 <5b5d0ad2-4dc9-f7fe-7c9d-8016b5d31358@uke.de>
Message-ID: <E7E03EB3-F8D9-423B-B06B-4CAB93F60CEA@gmail.com>

D'oh! Thanks for pointing this out. I blame caffeine depletion at the time...

-pd

> On 4 Dec 2017, at 15:48 , Eik Vettorazzi <E.Vettorazzi at uke.de> wrote:
> 
> reading ?plotmath you might notice that "_" isn't the propper syntax for
> subscripts. This will work:
> 
> ggtern(data=x11,aes(A,B,C,xend = c(0.7,.00,0.7),yend =
> c(.30,.50,.0),zend =c(.0,.50,0.3)))+
>  geom_point()+
>  theme_showarrows()+geom_segment(size=.5)+
> geom_text_viewport(x=c(.45,.27,.37),y=c(.32,.29,.22),label=c("P[a]","P[b]","P[c]"),
> parse=TRUE)
> 
> cheers.
> 
> Am 04.12.2017 um 15:37 schrieb Levent TERLEMEZ via R-help:
>> Hi,
>> 
>> My example code is this;
>> 
>> x11<-data.frame(A=c(.6,.6,.6),B=c(.20,.20,.20),C=c(0.20,.20,.20))
>> ggtern(data=x11,aes(A,B,C,xend = c(0.7,.00,0.7),yend = c(.30,.50,.0),zend =c(.0,.50,0.3)))+
>>    geom_point()+
>>    theme_showarrows()+geom_segment(size=.5)+
>>    geom_text_viewport(x=c(.45,.27,.37),y=c(.32,.29,.22),label=as.expression("P_a","P_b","P_c"))
>> 
>> ggtern(data=x11,aes(A,B,C,xend = c(0.7,.00,0.7),yend = c(.30,.50,.0),zend =c(.0,.50,0.3)))+
>>    geom_point()+
>>    theme_showarrows()+geom_segment(size=.5)+
>>    geom_text_viewport(x=c(.45,.27,.37),y=c(.32,.29,.22),label=as.expression(quote(c("P_a","P_b","P_c"))))
>> 
>> In geom_text_viewport (I also tried geom_label and geom_text versions) tried all possible solutions, but i couldn't achieved. R command outputs are like this:
>> 
>> Error in stats::complete.cases(df[, vars, drop = FALSE]) :
>>  invalid 'type' (expression) of argument
>> 
>> 
>> Maybe i am writing the code wrong, i couldn't figure out.
>> 
>> Thanks for your kind answers.
>> 
>> 
>> 
>> ________________________________________
>> Kimden: Martin Maechler [maechler at stat.math.ethz.ch]
>> G?nderildi: 04 Aral?k 2017 Pazartesi 16:16
>> Kime: peter dalgaard
>> Bilgi: Levent TERLEMEZ; R-help at r-project.org
>> Konu: Re: [R] ggtern and bquote...
>> 
>>>>>>> peter dalgaard <pdalgd at gmail.com>
>>>>>>>    on Mon, 4 Dec 2017 14:55:19 +0100 writes:
>> 
>>>> On 4 Dec 2017, at 11:58 , Levent TERLEMEZ via R-help
>>>> <r-help at r-project.org> wrote:
>>>> 
>>>> Dear Users,
>>>> 
>>>> What is the proper way to write symbol, superscript,
>>>> subscript in ggtern/ggplot? I tried every given example,
>>>> every possible features of ggplot but couldn?t achived. I
>>>> just want to write P_a, sigma^2, etc, would you please
>>>> advise me about this problem.
>> 
>>> Did you try expression(P_a)? I don't do much gg-stuff, but
>>> I seem to recall that quote() doesn't quite cut it the way
>>> it does in base graphics.
>> 
>>> -pd
>> 
>> Yes, I vaguely remember that indeed also for the lattice package
>> (which is based on 'grid' the same as 'ggplot2' is ..) sometimes
>> expressions instead of calls are needed, i.e., expression(*)
>> instead of just quote(*).
>> 
>> However, I think Levent really meant what you'd get by
>> expression(P[a]) ?
>> 
>> @Levent: The clue is the need for valid R syntax, and indeed, as
>>   in LaTeX  x_i often is the i-th element of x,  the R syntax for
>>   indexing/subsetting is used here, i.e.
>>    x[i]  for LaTeX  x_i
>> 
>> 
>> Last but not least, if Levent really needs bquote() [i.e. substitute()]
>> then, a final
>>      as.expression(.)
>> may be needed :
>> 
>> identical(as.expression(quote(a == 1)),
>>             expression(      a == 1))  # --> TRUE
>> 
>> --
>> Martin Maechler, ETH Zurich
>> 
>> 
>> ________________________________
>> 
>> Bu elektronik posta ve onunla iletilen b?t?n dosyalar sadece yukar?da isimleri belirtilen ki?iler aras?nda ?zel haberle?me amac?n? ta??makta olup g?nderici taraf?ndan al?nmas? ama?lanan yetkili ger?ek ya da t?zel ki?inin kullan?m?na aittir. E?er bu elektronik posta size yanl??l?kla ula?m??sa, elektronik postan?n i?eri?ini a??klaman?z, kopyalaman?z, y?nlendirmeniz ve kullanman?z kesinlikle yasakt?r. Bu durumda, l?tfen mesaj? geri g?nderiniz ve sisteminizden siliniz. Anadolu ?niversitesi bu mesaj?n i?erdi?i bilgilerin do?rulu?u veya eksiksiz oldu?u konusunda herhangi bir garanti vermemektedir. Bu nedenle bu bilgilerin ne ?ekilde olursa olsun i?eri?inden, iletilmesinden, al?nmas?ndan ve saklanmas?ndan sorumlu de?ildir. Bu mesajdaki g?r??ler yaln?zca g?nderen ki?iye aittir ve Anadolu ?niversitesinin g?r??lerini yans?tmayabilir.
>> 
>> This electronic mail and any files transmitted with it are intended for the private use of the people named above. If you are not the intended recipient and received this message in error, forwarding, copying or use of any of the information is strictly prohibited. Any dissemination or use of this information by a person other than the intended recipient is unauthorized and may be illegal. In this case, please immediately notify the sender and delete it from your system. Anadolu University does not guarantee the accuracy or completeness of any information included in this message. Therefore, by any means Anadolu University is not responsible for the content of the message, and the transmission, reception, storage, and use of the information. The opinions expressed in this message only belong to the sender of it and may not reflect the opinions of Anadolu University.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> -- 
> Eik Vettorazzi
> 
> Department of Medical Biometry and Epidemiology
> University Medical Center Hamburg-Eppendorf
> 
> Martinistrasse 52
> building W 34
> 20246 Hamburg
> 
> Phone: +49 (0) 40 7410 - 58243
> Fax:   +49 (0) 40 7410 - 57790
> Web: www.uke.de/imbe
> --
> 
> _____________________________________________________________________
> 
> Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
> Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Martina Saurin (komm.)
> _____________________________________________________________________
> 
> SAVE PAPER - THINK BEFORE PRINTING
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pdalgd at gmail.com  Mon Dec  4 16:38:52 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 4 Dec 2017 16:38:52 +0100
Subject: [R] Dynamic reference, right-hand side of function
In-Reply-To: <d200347436c940f695583220f01ac86c@ebox-prod-srv09.win.su.se>
References: <d200347436c940f695583220f01ac86c@ebox-prod-srv09.win.su.se>
Message-ID: <368012C4-4CB5-406E-84CE-019D0986CC99@gmail.com>

The generic rule is that R is not a macro language, so looping of names of things gets awkward. It is usually easier to use compound objects like lists and iterate over them. E.g.

datanames <- paste0("aa_", 2000:2007)
datalist <- lapply(datanames, get) 
names(datalist) <- datanames
col1 <- lapply(datalist, "[[", 1) 
colnum <- lapply(col1, as.numeric)

(The 2nd line assumes that the damage has already been done so that you have aa_2000 ... aa_2007 in your workspace. You might alternatively create the list directly while importing the data.)

-pd

> On 4 Dec 2017, at 12:33 , Love Bohman <love.bohman at sociology.su.se> wrote:
> 
> Hi R-users!
> Being new to R, and a fairly advanced Stata-user, I guess part of my problem is that my mindset (and probably my language as well) is wrong. Anyway, I have what I guess is a rather simple problem, that I now without success spent days trying to solve.
> 
> I have a bunch of datasets imported from Stata that is labelled aa_2000 aa_2001 aa_2002, etc. Each dataset is imported as a matrix, and consists of one column only. The columns consists of integer numbers. I need to convert the data to vectors, which I found several ways to do. I use, for example:
> aa_2000 <- as.numeric(aa_2000[,1])
> However, when trying to automate the task, so I don't have to write a line of code for each dataset, I get stuck. Since I'm a Stata user, my first attempt is trying to make a loop in order to loop over all datasets. However, I manage to write a loop that works for the left-hand side of the syntax, but not for the right-hand side.
> I have included some examples from my struggles to solve the issue below, what they all have in common is that I don't manage to call for any "macro" (is that only a Stata-word?) in the right hand side of the functions. When I try to replace the static reference with a dynamic one (like in the left-hand side), the syntax just doesn't work.
> 
> I would very much appreciate some help with this issue!
> All the best,
> Love
> 
> year <- 2002
> dataname <- paste0("aa_",year)
> assign(paste0(dataname), as.numeric(aa_2002[,1]))
> 
> year <- 2003
> assign(paste0("aa_",year), as.numeric(aa_2003))
> 
> year <- 2005
> assign(paste0("aa_",year), aa_2005[,1])
> 
> list1 <- c(2000:2007)
> list1[c(7)]
> assign(paste0("aa_",list1[c(7)]), as.numeric(paste0(aa_2006)))
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From lutipilotto at yahoo.com.br  Mon Dec  4 15:56:44 2017
From: lutipilotto at yahoo.com.br (Luciane Maria Pilotto)
Date: Mon, 4 Dec 2017 14:56:44 +0000 (UTC)
Subject: [R] svyglm
References: <1859697545.1252505.1512399404304.ref@mail.yahoo.com>
Message-ID: <1859697545.1252505.1512399404304@mail.yahoo.com>

Hi,
I am trying to run analyzes incorporating sample weight, strata and cluster (three-stage sample) with PNS data (national health survey) and is giving error. I describe below the commands used. I could not make the code reproducible properly.
Thanks,
#################################################
library(survey)####change to 0 and 1 variable outcomedent2<-ifelse(consdentcat2==2,0,1)table(dent2)?dent2<-as.factor(dent2)str(dent2)reg<-cbind(reg, dent2)
#tchange to factor?str(sexo)reg$sexo <- as.factor((reg$sexo))
#################
pns2013design<-svydesign(id=~upa, nest=TRUE, strata = estrato, weight = peso, data = reg)

PNS<-svyglm(dent2~sexo,design=pns2013design, method="logistic", data = reg)

Error in logistic(x = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,? :?? unused arguments (x = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,?1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,################################################################################################################################################################

###NOT running SVYGLM command#data for R-helpbteste <- reg[1:10, c(1, 4, 26, 27, 28)]#dput(bteste)###omitting most of the lines?"9967.223281", "9967.870849", "9968.207979", "997.0045537",?? ? "997.1451224", "997.1574275", "997.1782368", "997.1812338",?? ? "997.2480231", "997.2531051", "997.3803145", "997.4345619",?? ? "997.4363662", "997.5344599", "997.5964572", "997.7439813",?? ? "997.8360773", "997.8770935", "997.8811374", "997.9147096",?? ? "997.9563562", "9974.30392", "9974.344482", "9975.656981",?? ? "9977.382263", "9979.999691", "998.0053953", "998.1069038",?? ? "998.2140192", "998.2655421", "998.308316", "998.3090242",?? ? "998.3579509", "998.3656231", "998.3766007", "998.6844831",?? ? "998.7030027", "998.7112321", "998.8021132", "998.8839799",?? ? "998.9225688", "998.9270228", "998.9337225", "9983.555066",?? ? "9985.353117", "9989.517638", "999.0713699", "999.0771916",?? ? "999.1021413", "999.1779133", "999.2539765", "999.3435971",?? ? "999.3809978", "999.6348707", "999.7597985", "999.8002746",?? ? "999.8819267", "999.8821907", "999.8921074", "999.9211427",?? ? "9991.102816", "9991.440035", "9994.626337", "9994.723654",?? ? "9996.637923", "9998.491819"), class = "factor")), .Names = c("consdentcat2",?"sexo", "estrato", "upa", "pesomorcc"), row.names = c(NA, 10L), class = "data.frame")##################################################
bteste1 <-bteste[1:10, ]#bteste1? ?consdentcat2 sexo estrato? ? ?upa? ?pesomorcc1? ? ? ? ? ? ?2? ? 1 1110011 1100002 418.76819022? ? ? ? ? ? ?2? ? 1 1110011 1100002 317.13175793? ? ? ? ? ? ?2? ? 1 1110011 1100002 467.09452884? ? ? ? ? ? ?1? ? 1 1110011 1100002 209.38409515? ? ? ? ? ? ?2? ? 1 1110011 1100002 209.38409516? ? ? ? ? ? ?2? ? 1 1110011 1100002 418.76819027? ? ? ? ? ? ?2? ? 1 1110011 1100002 233.54726448? ? ? ? ? ? ?2? ? 1 1110011 1100002 628.15228539? ? ? ? ? ? ?2? ? 1 1110011 1100002 317.131757910? ? ? ? ? ? 2? ? 2 1110011 1100002 321.5014524>?? ? ? ???? ? ? ? ? ? ? ? ? ?_________________________________________
Luciane Maria Pilotto



|  | Livre de v?rus. www.avast.com.  |


	[[alternative HTML version deleted]]


From lterlemez at anadolu.edu.tr  Mon Dec  4 17:39:59 2017
From: lterlemez at anadolu.edu.tr (Levent TERLEMEZ)
Date: Mon, 4 Dec 2017 16:39:59 +0000
Subject: [R] YNT:  ggtern and bquote...
In-Reply-To: <E7E03EB3-F8D9-423B-B06B-4CAB93F60CEA@gmail.com>
References: <D64B051E.1CE5A%lterlemez@anadolu.edu.tr>
 <B8CC130C-AFFF-4B51-84F4-EF57C3C7F04D@gmail.com>
 <23077.22688.642047.971753@stat.math.ethz.ch>
 <37273DBD29A7A2408A7736A073A9FA3BE2723E73@mb06.porsuk.anadolu.edu.tr>
 <5b5d0ad2-4dc9-f7fe-7c9d-8016b5d31358@uke.de>,
 <E7E03EB3-F8D9-423B-B06B-4CAB93F60CEA@gmail.com>
Message-ID: <37273DBD29A7A2408A7736A073A9FA3BE2723F05@mb06.porsuk.anadolu.edu.tr>

Hi, thanks to everybody for pointing the issue and their kind answers. I appreciated, it is solved.

x11<-data.frame(A=c(.6,.6,.6),B=c(.20,.20,.20),C=c(0.20,.20,.20))
ggtern(data=x11,aes(A,B,C,xend = c(0.7,.00,0.7),yend= c(.30,.50,.0),zend =c(.0,.50,0.3)))+
  geom_point()+
  theme_showarrows()+geom_segment(size=.5)+
  geom_text_viewport(x=c(.45,.27,.37),y=c(.32,.29,.22),label=c("P[a]","P[b]","P[c]"),parse=TRUE)


My best regards,
Levent.
________________________________________
Kimden: peter dalgaard [pdalgd at gmail.com]
G?nderildi: 04 Aral?k 2017 Pazartesi 17:23
Kime: Eik Vettorazzi
Bilgi: Levent TERLEMEZ; R-help at r-project.org
Konu: Re: [R] ggtern and bquote...

D'oh! Thanks for pointing this out. I blame caffeine depletion at the time...

-pd

> On 4 Dec 2017, at 15:48 , Eik Vettorazzi <E.Vettorazzi at uke.de> wrote:
>
> reading ?plotmath you might notice that "_" isn't the propper syntax for
> subscripts. This will work:
>
> ggtern(data=x11,aes(A,B,C,xend = c(0.7,.00,0.7),yend =
> c(.30,.50,.0),zend =c(.0,.50,0.3)))+
>  geom_point()+
>  theme_showarrows()+geom_segment(size=.5)+
> geom_text_viewport(x=c(.45,.27,.37),y=c(.32,.29,.22),label=c("P[a]","P[b]","P[c]"),
> parse=TRUE)
>
> cheers.
>
> Am 04.12.2017 um 15:37 schrieb Levent TERLEMEZ via R-help:
>> Hi,
>>
>> My example code is this;
>>
>> x11<-data.frame(A=c(.6,.6,.6),B=c(.20,.20,.20),C=c(0.20,.20,.20))
>> ggtern(data=x11,aes(A,B,C,xend = c(0.7,.00,0.7),yend = c(.30,.50,.0),zend =c(.0,.50,0.3)))+
>>    geom_point()+
>>    theme_showarrows()+geom_segment(size=.5)+
>>    geom_text_viewport(x=c(.45,.27,.37),y=c(.32,.29,.22),label=as.expression("P_a","P_b","P_c"))
>>
>> ggtern(data=x11,aes(A,B,C,xend = c(0.7,.00,0.7),yend = c(.30,.50,.0),zend =c(.0,.50,0.3)))+
>>    geom_point()+
>>    theme_showarrows()+geom_segment(size=.5)+
>>    geom_text_viewport(x=c(.45,.27,.37),y=c(.32,.29,.22),label=as.expression(quote(c("P_a","P_b","P_c"))))
>>
>> In geom_text_viewport (I also tried geom_label and geom_text versions) tried all possible solutions, but i couldn't achieved. R command outputs are like this:
>>
>> Error in stats::complete.cases(df[, vars, drop = FALSE]) :
>>  invalid 'type' (expression) of argument
>>
>>
>> Maybe i am writing the code wrong, i couldn't figure out.
>>
>> Thanks for your kind answers.
>>
>>
>>
>> ________________________________________
>> Kimden: Martin Maechler [maechler at stat.math.ethz.ch]
>> G?nderildi: 04 Aral?k 2017 Pazartesi 16:16
>> Kime: peter dalgaard
>> Bilgi: Levent TERLEMEZ; R-help at r-project.org
>> Konu: Re: [R] ggtern and bquote...
>>
>>>>>>> peter dalgaard <pdalgd at gmail.com>
>>>>>>>    on Mon, 4 Dec 2017 14:55:19 +0100 writes:
>>
>>>> On 4 Dec 2017, at 11:58 , Levent TERLEMEZ via R-help
>>>> <r-help at r-project.org> wrote:
>>>>
>>>> Dear Users,
>>>>
>>>> What is the proper way to write symbol, superscript,
>>>> subscript in ggtern/ggplot? I tried every given example,
>>>> every possible features of ggplot but couldn?t achived. I
>>>> just want to write P_a, sigma^2, etc, would you please
>>>> advise me about this problem.
>>
>>> Did you try expression(P_a)? I don't do much gg-stuff, but
>>> I seem to recall that quote() doesn't quite cut it the way
>>> it does in base graphics.
>>
>>> -pd
>>
>> Yes, I vaguely remember that indeed also for the lattice package
>> (which is based on 'grid' the same as 'ggplot2' is ..) sometimes
>> expressions instead of calls are needed, i.e., expression(*)
>> instead of just quote(*).
>>
>> However, I think Levent really meant what you'd get by
>> expression(P[a]) ?
>>
>> @Levent: The clue is the need for valid R syntax, and indeed, as
>>   in LaTeX  x_i often is the i-th element of x,  the R syntax for
>>   indexing/subsetting is used here, i.e.
>>    x[i]  for LaTeX  x_i
>>
>>
>> Last but not least, if Levent really needs bquote() [i.e. substitute()]
>> then, a final
>>      as.expression(.)
>> may be needed :
>>
>> identical(as.expression(quote(a == 1)),
>>             expression(      a == 1))  # --> TRUE
>>
>> --
>> Martin Maechler, ETH Zurich
>>
>>
>> ________________________________
>>
>> Bu elektronik posta ve onunla iletilen b?t?n dosyalar sadece yukar?da isimleri belirtilen ki?iler aras?nda ?zel haberle?me amac?n? ta??makta olup g?nderici taraf?ndan al?nmas? ama?lanan yetkili ger?ek ya da t?zel ki?inin kullan?m?na aittir. E?er bu elektronik posta size yanl??l?kla ula?m??sa, elektronik postan?n i?eri?ini a??klaman?z, kopyalaman?z, y?nlendirmeniz ve kullanman?z kesinlikle yasakt?r. Bu durumda, l?tfen mesaj? geri g?nderiniz ve sisteminizden siliniz. Anadolu ?niversitesi bu mesaj?n i?erdi?i bilgilerin do?rulu?u veya eksiksiz oldu?u konusunda herhangi bir garanti vermemektedir. Bu nedenle bu bilgilerin ne ?ekilde olursa olsun i?eri?inden, iletilmesinden, al?nmas?ndan ve saklanmas?ndan sorumlu de?ildir. Bu mesajdaki g?r??ler yaln?zca g?nderen ki?iye aittir ve Anadolu ?niversitesinin g?r??lerini yans?tmayabilir.
>>
>> This electronic mail and any files transmitted with it are intended for the private use of the people named above. If you are not the intended recipient and received this message in error, forwarding, copying or use of any of the information is strictly prohibited. Any dissemination or use of this information by a person other than the intended recipient is unauthorized and may be illegal. In this case, please immediately notify the sender and delete it from your system. Anadolu University does not guarantee the accuracy or completeness of any information included in this message. Therefore, by any means Anadolu University is not responsible for the content of the message, and the transmission, reception, storage, and use of the information. The opinions expressed in this message only belong to the sender of it and may not reflect the opinions of Anadolu University.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> --
> Eik Vettorazzi
>
> Department of Medical Biometry and Epidemiology
> University Medical Center Hamburg-Eppendorf
>
> Martinistrasse 52
> building W 34
> 20246 Hamburg
>
> Phone: +49 (0) 40 7410 - 58243
> Fax:   +49 (0) 40 7410 - 57790
> Web: www.uke.de/imbe
> --
>
> _____________________________________________________________________
>
> Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
> Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Martina Saurin (komm.)
> _____________________________________________________________________
>
> SAVE PAPER - THINK BEFORE PRINTING
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com











From jeanphilippe.fontaine at gssi.infn.it  Mon Dec  4 18:13:31 2017
From: jeanphilippe.fontaine at gssi.infn.it (jean-philippe)
Date: Mon, 4 Dec 2017 18:13:31 +0100
Subject: [R] problem with the behaviour of dashed lines in R plots
Message-ID: <5A25823B.7030700@gssi.infn.it>

dear R users,

I am performing a linear regression with lm, and I would like to plot 
the regressor in dashed lines. I know that the lty=2 option is the way 
out, but it has a very strange behaviour: the line starts dashed but 
then the spaces between each dash becomes very tiny and so the line 
become somehow continuous for the human eye. Do you know how to fix that 
problem, in order to have a dashed line with big enough spaces between 
the dashes?
Here is a MWE (don't mind if clearly a linear model will not fit these 
"fake randomly generated" data).

Also, changing the plot(...,type="l") to abline(regressor,lty=2,...) 
helps and draw a pure dashed line but it is impossible to force it to 
stay in the bounds of the data.
Changing to line instead shows the same problem as mentioned here in the 
MWE.


pdf("reproducableex.pdf")
df1<-data.frame(B=runif(20,1.4,1.6),A=runif(20,-19.5,-9.8))
regressor<-lm(A~B,data = df1)
plot(df1$B,predict(regressor,df1),type="l", col="black", 
mgp=c(2,0.5,0),cex.lab=1.6, lwd=2, 
lty=2,xlim=range(c(1.2,1.7)),ylim=rev(range(c(-19,-8))))
par(new = TRUE)
plot(df1$B,as.numeric(df1$A),type="p", col="black", 
mgp=c(2,0.5,0),cex.lab=1.6,cex=2, xlab = "", ylab = 
"",xlim=range(c(1.2,1.7)),ylim=rev(range(c(-19,-8))),pch=17)
box(lwd=3)
dev.off()


Thanks in advance, best regards


Jean-Philippe Fontaine

-- 
Jean-Philippe Fontaine
PhD Student in Astroparticle Physics,
Gran Sasso Science Institute (GSSI),
Viale Francesco Crispi 7,
67100 L'Aquila, Italy
Mobile: +393487128593, +33615653774


From sarah.goslee at gmail.com  Mon Dec  4 18:30:31 2017
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 4 Dec 2017 12:30:31 -0500
Subject: [R] problem with the behaviour of dashed lines in R plots
In-Reply-To: <5A25823B.7030700@gssi.infn.it>
References: <5A25823B.7030700@gssi.infn.it>
Message-ID: <CAM_vjunC1BcSmMMxsP8isB-4YdY1US3-FXZ2ZrNOmbRjKtP23w@mail.gmail.com>

Hi,

It's because you are plotting a line between each of the points in
your data frame, and they are very close together:

> cbind(df1$B,predict(regressor,df1))
       [,1]      [,2]
1  1.410832 -13.96466
2  1.589383 -15.21169
3  1.446662 -14.21491
4  1.488665 -14.50826
5  1.487035 -14.49687
6  1.497347 -14.56890
7  1.458070 -14.29458
8  1.568134 -15.06328
9  1.543364 -14.89029
10 1.513473 -14.68152
11 1.465462 -14.34621
12 1.506752 -14.63458
13 1.434703 -14.13139
14 1.584011 -15.17417
15 1.585621 -15.18542
16 1.542410 -14.88362
17 1.430091 -14.09917
18 1.474529 -14.40953
19 1.431341 -14.10790
20 1.425015 -14.06372

If instead you use abline(regressor, lty=2) you will get a
nice-looking dashed line.

Or, if you want only the data extent, you could use just the points
for the minimum and maximum values of x.

Sarah

On Mon, Dec 4, 2017 at 12:13 PM, jean-philippe
<jeanphilippe.fontaine at gssi.infn.it> wrote:
> dear R users,
>
> I am performing a linear regression with lm, and I would like to plot the
> regressor in dashed lines. I know that the lty=2 option is the way out, but
> it has a very strange behaviour: the line starts dashed but then the spaces
> between each dash becomes very tiny and so the line become somehow
> continuous for the human eye. Do you know how to fix that problem, in order
> to have a dashed line with big enough spaces between the dashes?
> Here is a MWE (don't mind if clearly a linear model will not fit these "fake
> randomly generated" data).
>
> Also, changing the plot(...,type="l") to abline(regressor,lty=2,...) helps
> and draw a pure dashed line but it is impossible to force it to stay in the
> bounds of the data.
> Changing to line instead shows the same problem as mentioned here in the
> MWE.
>
>
> pdf("reproducableex.pdf")
> df1<-data.frame(B=runif(20,1.4,1.6),A=runif(20,-19.5,-9.8))
> regressor<-lm(A~B,data = df1)
> plot(df1$B,predict(regressor,df1),type="l", col="black",
> mgp=c(2,0.5,0),cex.lab=1.6, lwd=2,
> lty=2,xlim=range(c(1.2,1.7)),ylim=rev(range(c(-19,-8))))
> par(new = TRUE)
> plot(df1$B,as.numeric(df1$A),type="p", col="black",
> mgp=c(2,0.5,0),cex.lab=1.6,cex=2, xlab = "", ylab =
> "",xlim=range(c(1.2,1.7)),ylim=rev(range(c(-19,-8))),pch=17)
> box(lwd=3)
> dev.off()
>
>
> Thanks in advance, best regards
>
>
> Jean-Philippe Fontaine
>


-- 
Sarah Goslee
http://www.functionaldiversity.org


From jeanphilippe.fontaine at gssi.infn.it  Mon Dec  4 19:30:32 2017
From: jeanphilippe.fontaine at gssi.infn.it (jean-philippe)
Date: Mon, 4 Dec 2017 19:30:32 +0100
Subject: [R] problem with the behaviour of dashed lines in R plots
In-Reply-To: <CAM_vjunC1BcSmMMxsP8isB-4YdY1US3-FXZ2ZrNOmbRjKtP23w@mail.gmail.com>
References: <5A25823B.7030700@gssi.infn.it>
 <CAM_vjunC1BcSmMMxsP8isB-4YdY1US3-FXZ2ZrNOmbRjKtP23w@mail.gmail.com>
Message-ID: <5A259448.5060009@gssi.infn.it>

hi Sarah,

Thanks a lot for having taken time to answer me and for your reply. I 
wonder how I missed this solution. Indeed plotting the line with the 2 
extreme data points works perfectly.


Best,


Jean-Philippe Fontaine



On 04/12/2017 18:30, Sarah Goslee wrote:
> It's because you are plotting a line between each of the points in
> your data frame, and they are very close togethe

-- 
Jean-Philippe Fontaine
PhD Student in Astroparticle Physics,
Gran Sasso Science Institute (GSSI),
Viale Francesco Crispi 7,
67100 L'Aquila, Italy
Mobile: +393487128593, +33615653774


From ericjberger at gmail.com  Mon Dec  4 20:49:30 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Mon, 4 Dec 2017 21:49:30 +0200
Subject: [R] problem with the behaviour of dashed lines in R plots
In-Reply-To: <5A259448.5060009@gssi.infn.it>
References: <5A25823B.7030700@gssi.infn.it>
 <CAM_vjunC1BcSmMMxsP8isB-4YdY1US3-FXZ2ZrNOmbRjKtP23w@mail.gmail.com>
 <5A259448.5060009@gssi.infn.it>
Message-ID: <CAGgJW74HqCY1toUbR6BNH7yBS-fuqbi0yCocvQXz0T+C13jZJg@mail.gmail.com>

Hi,
Sarah's last comment about using min/max x got me thinking. It's not that
the points are "very close together", it's that the x-values are not
ordered. So the plot is actually drawing a dashed line back-and-forth
between different points on the line, which has the effect of making the
result appear non-dashed. If you sort by the x-values before plotting you
will see the output is very different. I am not saying this is a better
solution than Sarah's regarding just using the end-points, but at least it
partially explains the relevance of that suggestion.

For example, here is a slightly modified version of the code that does an
ordering before plotting:

df1<-data.frame(B=runif(20,1.4,1.6),A=runif(20,-19.5,-9.8))
regressor<-lm(A~B,data = df1)
df2 <- data.frame( x=df1$B, yhat= predict(regressor,df1))
df2 <- df2[ order(df2$x), ]
plot(df2$x,df2$yhat,type="l", col="black", mgp=c(2,0.5,0),cex.lab=1.6,
lwd=2, lty=2,xlim=range(c(1.2,1.7)),ylim=rev(range(c(-19,-8))))
par(new = TRUE)
plot(df1$B,as.numeric(df1$A),type="p", col="black",
mgp=c(2,0.5,0),cex.lab=1.6,cex=2, xlab = "", ylab =
"",xlim=range(c(1.2,1.7)),ylim=rev(range(c(-19,-8))),pch=17)
box(lwd=3)


HTH,
Eric

On Mon, Dec 4, 2017 at 8:30 PM, jean-philippe <
jeanphilippe.fontaine at gssi.infn.it> wrote:

> hi Sarah,
>
> Thanks a lot for having taken time to answer me and for your reply. I
> wonder how I missed this solution. Indeed plotting the line with the 2
> extreme data points works perfectly.
>
>
> Best,
>
>
> Jean-Philippe Fontaine
>
>
>
> On 04/12/2017 18:30, Sarah Goslee wrote:
>
>> It's because you are plotting a line between each of the points in
>> your data frame, and they are very close togethe
>>
>
> --
> Jean-Philippe Fontaine
> PhD Student in Astroparticle Physics,
> Gran Sasso Science Institute (GSSI),
> Viale Francesco Crispi 7,
> 67100 L'Aquila, Italy
> Mobile: +393487128593, +33615653774
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From laradutrasilva at gmail.com  Mon Dec  4 18:41:06 2017
From: laradutrasilva at gmail.com (Lara Dutra Silva)
Date: Mon, 4 Dec 2017 16:41:06 -0100
Subject: [R] Boundary_maps
Message-ID: <CAJRXdJA7hsjmf4KaXH_dv37ygFBPHKkpbw8-79nQ4u5tdM6qeA@mail.gmail.com>

Hello,

I have a question. Is there any code in R, so I can delimiy the study area
(bounder)?



plot(proj63PA$Pittosporum_EMmeanByROC_mergedAlgo_mergedRun_mergedData,
main= "", xlab ="x.coords", ylab="y.coords", cex.axis=caxis)
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Map.PNG
Type: image/png
Size: 16753 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20171204/67243e6a/attachment.png>

From love.bohman at sociology.su.se  Mon Dec  4 22:46:06 2017
From: love.bohman at sociology.su.se (Love Bohman)
Date: Mon, 4 Dec 2017 21:46:06 +0000
Subject: [R] Dynamic reference, right-hand side of function
In-Reply-To: <368012C4-4CB5-406E-84CE-019D0986CC99@gmail.com>
References: <d200347436c940f695583220f01ac86c@ebox-prod-srv09.win.su.se>
 <368012C4-4CB5-406E-84CE-019D0986CC99@gmail.com>
Message-ID: <dd6a7dcd237542eebeb064aa3493c7bf@ebox-prod-srv09.win.su.se>

Hi!
Thanks for the replies!
I understand people more accustomed to R doesn't like looping much, and that thinking about loops is something I do since I worked with Stata a lot. The syntax from Peter Dalgaard was really clever, and I learned a lot from it, even though it didn't solve my problem (I guess it wasn't very well explained). My problem was basically that I have a data matrix consisting of just 1 row, and I want to convert that row into a vector. However, when trying to do that dynamically, I couldn't get R to read the right hand side of the syntax as a variable name instead of a string. However, together with a colleague I finally solved it with the (eval(as.name()) function (I include the loop I used below). I understand that looping isn't kosher among you more devoted R-users, and eventually I hope I will learn to use lists in the future instead.

Thanks!
Love


for (year in 2000:2007){
varname <- paste0("aa_",year)
assign(paste0(varname), as.vector(eval(as.name(varname))))
}

-----Ursprungligt meddelande-----
Fr?n: peter dalgaard [mailto:pdalgd at gmail.com] 
Skickat: den 4 december 2017 16:39
Till: Love Bohman <love.bohman at sociology.su.se>
Kopia: r-help at r-project.org
?mne: Re: [R] Dynamic reference, right-hand side of function

The generic rule is that R is not a macro language, so looping of names of things gets awkward. It is usually easier to use compound objects like lists and iterate over them. E.g.

datanames <- paste0("aa_", 2000:2007)
datalist <- lapply(datanames, get)
names(datalist) <- datanames
col1 <- lapply(datalist, "[[", 1)
colnum <- lapply(col1, as.numeric)

(The 2nd line assumes that the damage has already been done so that you have aa_2000 ... aa_2007 in your workspace. You might alternatively create the list directly while importing the data.)

-pd

> On 4 Dec 2017, at 12:33 , Love Bohman <love.bohman at sociology.su.se> wrote:
> 
> Hi R-users!
> Being new to R, and a fairly advanced Stata-user, I guess part of my problem is that my mindset (and probably my language as well) is wrong. Anyway, I have what I guess is a rather simple problem, that I now without success spent days trying to solve.
> 
> I have a bunch of datasets imported from Stata that is labelled aa_2000 aa_2001 aa_2002, etc. Each dataset is imported as a matrix, and consists of one column only. The columns consists of integer numbers. I need to convert the data to vectors, which I found several ways to do. I use, for example:
> aa_2000 <- as.numeric(aa_2000[,1])
> However, when trying to automate the task, so I don't have to write a line of code for each dataset, I get stuck. Since I'm a Stata user, my first attempt is trying to make a loop in order to loop over all datasets. However, I manage to write a loop that works for the left-hand side of the syntax, but not for the right-hand side.
> I have included some examples from my struggles to solve the issue below, what they all have in common is that I don't manage to call for any "macro" (is that only a Stata-word?) in the right hand side of the functions. When I try to replace the static reference with a dynamic one (like in the left-hand side), the syntax just doesn't work.
> 
> I would very much appreciate some help with this issue!
> All the best,
> Love
> 
> year <- 2002
> dataname <- paste0("aa_",year)
> assign(paste0(dataname), as.numeric(aa_2002[,1]))
> 
> year <- 2003
> assign(paste0("aa_",year), as.numeric(aa_2003))
> 
> year <- 2005
> assign(paste0("aa_",year), aa_2005[,1])
> 
> list1 <- c(2000:2007)
> list1[c(7)]
> assign(paste0("aa_",list1[c(7)]), as.numeric(paste0(aa_2006)))
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pdalgd at gmail.com  Mon Dec  4 23:09:23 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 4 Dec 2017 23:09:23 +0100
Subject: [R] Dynamic reference, right-hand side of function
In-Reply-To: <dd6a7dcd237542eebeb064aa3493c7bf@ebox-prod-srv09.win.su.se>
References: <d200347436c940f695583220f01ac86c@ebox-prod-srv09.win.su.se>
 <368012C4-4CB5-406E-84CE-019D0986CC99@gmail.com>
 <dd6a7dcd237542eebeb064aa3493c7bf@ebox-prod-srv09.win.su.se>
Message-ID: <816C0DF8-27F5-443A-80F4-8FE9633CA648@gmail.com>

Um, if you insist on doing it that way, at least use

assign(varname, as.vector(get(varname)))

-pd

> On 4 Dec 2017, at 22:46 , Love Bohman <love.bohman at sociology.su.se> wrote:
> 
> Hi!
> Thanks for the replies!
> I understand people more accustomed to R doesn't like looping much, and that thinking about loops is something I do since I worked with Stata a lot. The syntax from Peter Dalgaard was really clever, and I learned a lot from it, even though it didn't solve my problem (I guess it wasn't very well explained). My problem was basically that I have a data matrix consisting of just 1 row, and I want to convert that row into a vector. However, when trying to do that dynamically, I couldn't get R to read the right hand side of the syntax as a variable name instead of a string. However, together with a colleague I finally solved it with the (eval(as.name()) function (I include the loop I used below). I understand that looping isn't kosher among you more devoted R-users, and eventually I hope I will learn to use lists in the future instead.
> 
> Thanks!
> Love
> 
> 
> for (year in 2000:2007){
> varname <- paste0("aa_",year)
> assign(paste0(varname), as.vector(eval(as.name(varname))))
> }
> 
> -----Ursprungligt meddelande-----
> Fr?n: peter dalgaard [mailto:pdalgd at gmail.com] 
> Skickat: den 4 december 2017 16:39
> Till: Love Bohman <love.bohman at sociology.su.se>
> Kopia: r-help at r-project.org
> ?mne: Re: [R] Dynamic reference, right-hand side of function
> 
> The generic rule is that R is not a macro language, so looping of names of things gets awkward. It is usually easier to use compound objects like lists and iterate over them. E.g.
> 
> datanames <- paste0("aa_", 2000:2007)
> datalist <- lapply(datanames, get)
> names(datalist) <- datanames
> col1 <- lapply(datalist, "[[", 1)
> colnum <- lapply(col1, as.numeric)
> 
> (The 2nd line assumes that the damage has already been done so that you have aa_2000 ... aa_2007 in your workspace. You might alternatively create the list directly while importing the data.)
> 
> -pd
> 
>> On 4 Dec 2017, at 12:33 , Love Bohman <love.bohman at sociology.su.se> wrote:
>> 
>> Hi R-users!
>> Being new to R, and a fairly advanced Stata-user, I guess part of my problem is that my mindset (and probably my language as well) is wrong. Anyway, I have what I guess is a rather simple problem, that I now without success spent days trying to solve.
>> 
>> I have a bunch of datasets imported from Stata that is labelled aa_2000 aa_2001 aa_2002, etc. Each dataset is imported as a matrix, and consists of one column only. The columns consists of integer numbers. I need to convert the data to vectors, which I found several ways to do. I use, for example:
>> aa_2000 <- as.numeric(aa_2000[,1])
>> However, when trying to automate the task, so I don't have to write a line of code for each dataset, I get stuck. Since I'm a Stata user, my first attempt is trying to make a loop in order to loop over all datasets. However, I manage to write a loop that works for the left-hand side of the syntax, but not for the right-hand side.
>> I have included some examples from my struggles to solve the issue below, what they all have in common is that I don't manage to call for any "macro" (is that only a Stata-word?) in the right hand side of the functions. When I try to replace the static reference with a dynamic one (like in the left-hand side), the syntax just doesn't work.
>> 
>> I would very much appreciate some help with this issue!
>> All the best,
>> Love
>> 
>> year <- 2002
>> dataname <- paste0("aa_",year)
>> assign(paste0(dataname), as.numeric(aa_2002[,1]))
>> 
>> year <- 2003
>> assign(paste0("aa_",year), as.numeric(aa_2003))
>> 
>> year <- 2005
>> assign(paste0("aa_",year), aa_2005[,1])
>> 
>> list1 <- c(2000:2007)
>> list1[c(7)]
>> assign(paste0("aa_",list1[c(7)]), as.numeric(paste0(aa_2006)))
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 
> 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From love.bohman at sociology.su.se  Mon Dec  4 23:20:39 2017
From: love.bohman at sociology.su.se (Love Bohman)
Date: Mon, 4 Dec 2017 22:20:39 +0000
Subject: [R] Dynamic reference, right-hand side of function
In-Reply-To: <816C0DF8-27F5-443A-80F4-8FE9633CA648@gmail.com>
References: <d200347436c940f695583220f01ac86c@ebox-prod-srv09.win.su.se>
 <368012C4-4CB5-406E-84CE-019D0986CC99@gmail.com>
 <dd6a7dcd237542eebeb064aa3493c7bf@ebox-prod-srv09.win.su.se>
 <816C0DF8-27F5-443A-80F4-8FE9633CA648@gmail.com>
Message-ID: <283ecb0b898e414295cc21d47c8584ce@ebox-prod-srv09.win.su.se>

:-)
I don't insist on anything, I'm just struggling to learn a new language and partly a new way of thinking, and I really appreciate the corrections. I hope I someday will be able to handle lists in R as easy as I handle loops in Stata...
Thanks again!

Love
  

-----Ursprungligt meddelande-----
Fr?n: peter dalgaard [mailto:pdalgd at gmail.com] 
Skickat: den 4 december 2017 23:09
Till: Love Bohman <love.bohman at sociology.su.se>
Kopia: r-help at r-project.org
?mne: Re: [R] Dynamic reference, right-hand side of function

Um, if you insist on doing it that way, at least use

assign(varname, as.vector(get(varname)))

-pd

> On 4 Dec 2017, at 22:46 , Love Bohman <love.bohman at sociology.su.se> wrote:
> 
> Hi!
> Thanks for the replies!
> I understand people more accustomed to R doesn't like looping much, and that thinking about loops is something I do since I worked with Stata a lot. The syntax from Peter Dalgaard was really clever, and I learned a lot from it, even though it didn't solve my problem (I guess it wasn't very well explained). My problem was basically that I have a data matrix consisting of just 1 row, and I want to convert that row into a vector. However, when trying to do that dynamically, I couldn't get R to read the right hand side of the syntax as a variable name instead of a string. However, together with a colleague I finally solved it with the (eval(as.name()) function (I include the loop I used below). I understand that looping isn't kosher among you more devoted R-users, and eventually I hope I will learn to use lists in the future instead.
> 
> Thanks!
> Love
> 
> 
> for (year in 2000:2007){
> varname <- paste0("aa_",year)
> assign(paste0(varname), as.vector(eval(as.name(varname))))
> }
> 
> -----Ursprungligt meddelande-----
> Fr?n: peter dalgaard [mailto:pdalgd at gmail.com]
> Skickat: den 4 december 2017 16:39
> Till: Love Bohman <love.bohman at sociology.su.se>
> Kopia: r-help at r-project.org
> ?mne: Re: [R] Dynamic reference, right-hand side of function
> 
> The generic rule is that R is not a macro language, so looping of names of things gets awkward. It is usually easier to use compound objects like lists and iterate over them. E.g.
> 
> datanames <- paste0("aa_", 2000:2007)
> datalist <- lapply(datanames, get)
> names(datalist) <- datanames
> col1 <- lapply(datalist, "[[", 1)
> colnum <- lapply(col1, as.numeric)
> 
> (The 2nd line assumes that the damage has already been done so that 
> you have aa_2000 ... aa_2007 in your workspace. You might 
> alternatively create the list directly while importing the data.)
> 
> -pd
> 
>> On 4 Dec 2017, at 12:33 , Love Bohman <love.bohman at sociology.su.se> wrote:
>> 
>> Hi R-users!
>> Being new to R, and a fairly advanced Stata-user, I guess part of my problem is that my mindset (and probably my language as well) is wrong. Anyway, I have what I guess is a rather simple problem, that I now without success spent days trying to solve.
>> 
>> I have a bunch of datasets imported from Stata that is labelled aa_2000 aa_2001 aa_2002, etc. Each dataset is imported as a matrix, and consists of one column only. The columns consists of integer numbers. I need to convert the data to vectors, which I found several ways to do. I use, for example:
>> aa_2000 <- as.numeric(aa_2000[,1])
>> However, when trying to automate the task, so I don't have to write a line of code for each dataset, I get stuck. Since I'm a Stata user, my first attempt is trying to make a loop in order to loop over all datasets. However, I manage to write a loop that works for the left-hand side of the syntax, but not for the right-hand side.
>> I have included some examples from my struggles to solve the issue below, what they all have in common is that I don't manage to call for any "macro" (is that only a Stata-word?) in the right hand side of the functions. When I try to replace the static reference with a dynamic one (like in the left-hand side), the syntax just doesn't work.
>> 
>> I would very much appreciate some help with this issue!
>> All the best,
>> Love
>> 
>> year <- 2002
>> dataname <- paste0("aa_",year)
>> assign(paste0(dataname), as.numeric(aa_2002[,1]))
>> 
>> year <- 2003
>> assign(paste0("aa_",year), as.numeric(aa_2003))
>> 
>> year <- 2005
>> assign(paste0("aa_",year), aa_2005[,1])
>> 
>> list1 <- c(2000:2007)
>> list1[c(7)]
>> assign(paste0("aa_",list1[c(7)]), as.numeric(paste0(aa_2006)))
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School Solbjerg Plads 3, 
> 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 
> 

--
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com










From dulcalma at bigpond.com  Mon Dec  4 23:41:17 2017
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Tue, 5 Dec 2017 09:41:17 +1100
Subject: [R] problem with the behaviour of dashed lines in R plots
In-Reply-To: <5A25823B.7030700@gssi.infn.it>
References: <5A25823B.7030700@gssi.infn.it>
Message-ID: <000901d36d50$ffbacd80$ff306880$@bigpond.com>

If you can change the line colour try this

options(mgp=c(2,0.5,0),cex.lab=1.6)

plot(df1$B, predict(regressor,df1),
     type="l",
     col="grey",
     lwd=2,
     lty=1,
     xlim=range(c(1.2,1.7)),
     ylim=rev(range(c(-19,-8))))
     
lines(df1$B,as.numeric(df1$A),
      type="p",
      col="black",
      cex=2,
      pch=17)

box(lwd=3)

Saves typing by using options

Regards

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2350


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
jean-philippe
Sent: Tuesday, 5 December 2017 04:14
To: R mailing list
Subject: [R] problem with the behaviour of dashed lines in R plots

dear R users,

I am performing a linear regression with lm, and I would like to plot 
the regressor in dashed lines. I know that the lty=2 option is the way 
out, but it has a very strange behaviour: the line starts dashed but 
then the spaces between each dash becomes very tiny and so the line 
become somehow continuous for the human eye. Do you know how to fix that 
problem, in order to have a dashed line with big enough spaces between 
the dashes?
Here is a MWE (don't mind if clearly a linear model will not fit these 
"fake randomly generated" data).

Also, changing the plot(...,type="l") to abline(regressor,lty=2,...) 
helps and draw a pure dashed line but it is impossible to force it to 
stay in the bounds of the data.
Changing to line instead shows the same problem as mentioned here in the 
MWE.


pdf("reproducableex.pdf")
df1<-data.frame(B=runif(20,1.4,1.6),A=runif(20,-19.5,-9.8))
regressor<-lm(A~B,data = df1)
plot(df1$B,predict(regressor,df1),type="l", col="black", 
mgp=c(2,0.5,0),cex.lab=1.6, lwd=2, 
lty=2,xlim=range(c(1.2,1.7)),ylim=rev(range(c(-19,-8))))
par(new = TRUE)
plot(df1$B,as.numeric(df1$A),type="p", col="black", 
mgp=c(2,0.5,0),cex.lab=1.6,cex=2, xlab = "", ylab = 
"",xlim=range(c(1.2,1.7)),ylim=rev(range(c(-19,-8))),pch=17)
box(lwd=3)
dev.off()


Thanks in advance, best regards


Jean-Philippe Fontaine

-- 
Jean-Philippe Fontaine
PhD Student in Astroparticle Physics,
Gran Sasso Science Institute (GSSI),
Viale Francesco Crispi 7,
67100 L'Aquila, Italy
Mobile: +393487128593, +33615653774

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Mon Dec  4 23:56:40 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 4 Dec 2017 14:56:40 -0800
Subject: [R] Dynamic reference, right-hand side of function
In-Reply-To: <dd6a7dcd237542eebeb064aa3493c7bf@ebox-prod-srv09.win.su.se>
References: <d200347436c940f695583220f01ac86c@ebox-prod-srv09.win.su.se>
 <368012C4-4CB5-406E-84CE-019D0986CC99@gmail.com>
 <dd6a7dcd237542eebeb064aa3493c7bf@ebox-prod-srv09.win.su.se>
Message-ID: <CAF8bMcbvE3v=CbEt1Bt3F8E0E_Gu8YEsJzggy0bv7TQoOontFw@mail.gmail.com>

Note that in
    for (year in 2000:2007){
        varname <- paste0("aa_",year)
        assign(paste0(varname), as.vector(eval(as.name(varname))))
    }
the paste0(varname) is redundant - varname was just computed as the return
value of paste0().

People are trying to steer you towards making a list or environment that
contains
only your aa_XXXX objects because that will make subsequent code more
readable and maintainable.  (The initial setup can be weird because of the
string manipulations required.)

E.g., suppose you have datasets aa_XXXX for some arbitrary set of years.  If
you did not create a list of them when reading in the data
    aa_1970 <- scan(quiet=TRUE, text="2 3 5 8 10")
    aa_1981 <- scan(quiet=TRUE, text="11 17 18 19 23")
then you can package all of them in the current environment into a list with
    aa_names <- objects(pattern="^aa_[[:digit:]]{4}$")
    names(aa_names) <- sub("^aa_", "", aa_names) # just the year
    aa_list <- lapply(aa_names, get)
Once you have a list of all your datasets then you can do operations on all
the
elements of the list without having to remember which years you have data
for.
    aa_list <- lapply(aa_list, as.numeric)
    aa_trends <- lapply(aa_list, function(x) { iota <- seq_along(x) ;
coef(lm(x ~ iota)) })

To read in data from a bunch of files called aa_XXXX.txt, making the names
of
the list be the XXXX part of the file name, do
    aa_files <- dir(pattern="^aa_[[:digit:]]{4}\\.txt$")
    names(aa_files) <- sub("aa_([[:digit:]]{4})\\.txt$", "\\1", aa_files)
    aa_list <- lapply(aa_files, readAnAAFile)
where 'readAnAAFile' is the function you use to read one such file.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Dec 4, 2017 at 1:46 PM, Love Bohman <love.bohman at sociology.su.se>
wrote:

> Hi!
> Thanks for the replies!
> I understand people more accustomed to R doesn't like looping much, and
> that thinking about loops is something I do since I worked with Stata a
> lot. The syntax from Peter Dalgaard was really clever, and I learned a lot
> from it, even though it didn't solve my problem (I guess it wasn't very
> well explained). My problem was basically that I have a data matrix
> consisting of just 1 row, and I want to convert that row into a vector.
> However, when trying to do that dynamically, I couldn't get R to read the
> right hand side of the syntax as a variable name instead of a string.
> However, together with a colleague I finally solved it with the (eval(
> as.name()) function (I include the loop I used below). I understand that
> looping isn't kosher among you more devoted R-users, and eventually I hope
> I will learn to use lists in the future instead.
>
> Thanks!
> Love
>
>
> for (year in 2000:2007){
> varname <- paste0("aa_",year)
> assign(paste0(varname), as.vector(eval(as.name(varname))))
> }
>
> -----Ursprungligt meddelande-----
> Fr?n: peter dalgaard [mailto:pdalgd at gmail.com]
> Skickat: den 4 december 2017 16:39
> Till: Love Bohman <love.bohman at sociology.su.se>
> Kopia: r-help at r-project.org
> ?mne: Re: [R] Dynamic reference, right-hand side of function
>
> The generic rule is that R is not a macro language, so looping of names of
> things gets awkward. It is usually easier to use compound objects like
> lists and iterate over them. E.g.
>
> datanames <- paste0("aa_", 2000:2007)
> datalist <- lapply(datanames, get)
> names(datalist) <- datanames
> col1 <- lapply(datalist, "[[", 1)
> colnum <- lapply(col1, as.numeric)
>
> (The 2nd line assumes that the damage has already been done so that you
> have aa_2000 ... aa_2007 in your workspace. You might alternatively create
> the list directly while importing the data.)
>
> -pd
>
> > On 4 Dec 2017, at 12:33 , Love Bohman <love.bohman at sociology.su.se>
> wrote:
> >
> > Hi R-users!
> > Being new to R, and a fairly advanced Stata-user, I guess part of my
> problem is that my mindset (and probably my language as well) is wrong.
> Anyway, I have what I guess is a rather simple problem, that I now without
> success spent days trying to solve.
> >
> > I have a bunch of datasets imported from Stata that is labelled aa_2000
> aa_2001 aa_2002, etc. Each dataset is imported as a matrix, and consists of
> one column only. The columns consists of integer numbers. I need to convert
> the data to vectors, which I found several ways to do. I use, for example:
> > aa_2000 <- as.numeric(aa_2000[,1])
> > However, when trying to automate the task, so I don't have to write a
> line of code for each dataset, I get stuck. Since I'm a Stata user, my
> first attempt is trying to make a loop in order to loop over all datasets.
> However, I manage to write a loop that works for the left-hand side of the
> syntax, but not for the right-hand side.
> > I have included some examples from my struggles to solve the issue
> below, what they all have in common is that I don't manage to call for any
> "macro" (is that only a Stata-word?) in the right hand side of the
> functions. When I try to replace the static reference with a dynamic one
> (like in the left-hand side), the syntax just doesn't work.
> >
> > I would very much appreciate some help with this issue!
> > All the best,
> > Love
> >
> > year <- 2002
> > dataname <- paste0("aa_",year)
> > assign(paste0(dataname), as.numeric(aa_2002[,1]))
> >
> > year <- 2003
> > assign(paste0("aa_",year), as.numeric(aa_2003))
> >
> > year <- 2005
> > assign(paste0("aa_",year), aa_2005[,1])
> >
> > list1 <- c(2000:2007)
> > list1[c(7)]
> > assign(paste0("aa_",list1[c(7)]), as.numeric(paste0(aa_2006)))
> >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School Solbjerg Plads 3, 2000
> Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Tue Dec  5 00:00:56 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 4 Dec 2017 15:00:56 -0800 (PST)
Subject: [R] Dynamic reference, right-hand side of function
In-Reply-To: <283ecb0b898e414295cc21d47c8584ce@ebox-prod-srv09.win.su.se>
References: <d200347436c940f695583220f01ac86c@ebox-prod-srv09.win.su.se>
 <368012C4-4CB5-406E-84CE-019D0986CC99@gmail.com>
 <dd6a7dcd237542eebeb064aa3493c7bf@ebox-prod-srv09.win.su.se>
 <816C0DF8-27F5-443A-80F4-8FE9633CA648@gmail.com>
 <283ecb0b898e414295cc21d47c8584ce@ebox-prod-srv09.win.su.se>
Message-ID: <alpine.BSF.2.00.1712041430410.70142@pedal.dcn.davis.ca.us>

Loops are not evil, and no-one in this thread said they are. But I 
believe your failure to provide a reproducible example is creating 
confusion, since you may be using words that mean one thing to you and 
something else to the readers here.

################################
# A reproducible example includes a tiny set of sample data
# Since we cannot reproducibly refer to filenames (your directories
# and files in them are unlikely to be like mine), I will
# use a little trick to read from data in the example:

dta <- read.csv( text=
"1.1,3.0,5,7.4,4,2.2,0
", header=FALSE)
str(dta)
#> 'data.frame':    1 obs. of  7 variables:
#>  $ V1: num 1.1
#>  $ V2: num 3
#>  $ V3: int 5
#>  $ V4: num 7.4
#>  $ V5: int 4
#>  $ V6: num 2.2
#>  $ V7: int 0

# note that I did not use "data" as the name because
# there is a commonly-used function by that name in R
# that could be confused with your variable

# if you have your object already in memory, you can use the
# dput function to create R code that will re-create it in our
# working environments.

dput( dta )
#> structure(list(V1 = 1.1, V2 = 3, V3 = 5L, V4 = 7.4, V5 = 4L,
#>     V6 = 2.2, V7 = 0L), .Names = c("V1", "V2", "V3", "V4", "V5",
#> "V6", "V7"), class = "data.frame", row.names = c(NA, -1L))

# which you can put a variable name in front of in your example code:

dtasample <- structure(list( V1 = 1.1, V2 = 3, V3 = 5L
, V4 = 7.4, V5 = 4L, V6 = 2.2, V7 = 0L ) , .Names = c( "V1"
, "V2", "V3", "V4", "V5", "V6", "V7" ), class = "data.frame"
, row.names = c(NA, -1L) )

# and starting with that line you can make a self-contained
# (reproducible) example for us to investigate your problem with

# Note that reading a single row of data into R usually gets
# a data frame, which looks like a matrix but is not a matrix.
# Read the Introduction to R about these two types carefully.
# Each column in a data frame can have a different type of data,
# but in a vector or a matrix all rows and columns must be of
# the same type.

dtam <- as.matrix( dta )

# If you have any values that R cannot clearly identify as numeric
# or integer, then the next most general type of variable is
# character... and that is often something that trips up newbies,
# though I have no evidence that you have any non-numeric columns
# in your data frames.

dtax <- as.vector( dta )
str(dtax)
#> 'data.frame':    1 obs. of  7 variables:
#>  $ V1: num 1.1
#>  $ V2: num 3
#>  $ V3: int 5
#>  $ V4: num 7.4
#>  $ V5: int 4
#>  $ V6: num 2.2
#>  $ V7: int 0

# This actually makes no change to dta, because a data frame is already
# a list of columns, and lists are just vectors that can hold different
# types of variables, so dta is already a kind of vector.

dtan <- as.numeric( dta )
str(dtan)
#>  num [1:7] 1.1 3 5 7.4 4 2.2 0

# I suspect this is what you are trying to accomplish... but really,
# if we had an example of the data you are working with, we would
# already know.
################################

Some more explanations of reproducibility [1][2][3]

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

[2] http://adv-r.had.co.nz/Reproducibility.html

[3] https://cran.r-project.org/web/packages/reprex/index.html (read the 
vignette)


On Mon, 4 Dec 2017, Love Bohman wrote:

> :-)
> I don't insist on anything, I'm just struggling to learn a new language and partly a new way of thinking, and I really appreciate the corrections. I hope I someday will be able to handle lists in R as easy as I handle loops in Stata...
> Thanks again!
>
> Love
>
>
> -----Ursprungligt meddelande-----
> Fr?n: peter dalgaard [mailto:pdalgd at gmail.com]
> Skickat: den 4 december 2017 23:09
> Till: Love Bohman <love.bohman at sociology.su.se>
> Kopia: r-help at r-project.org
> ?mne: Re: [R] Dynamic reference, right-hand side of function
>
> Um, if you insist on doing it that way, at least use
>
> assign(varname, as.vector(get(varname)))
>
> -pd
>
>> On 4 Dec 2017, at 22:46 , Love Bohman <love.bohman at sociology.su.se> wrote:
>>
>> Hi!
>> Thanks for the replies!
>> I understand people more accustomed to R doesn't like looping much, and that thinking about loops is something I do since I worked with Stata a lot. The syntax from Peter Dalgaard was really clever, and I learned a lot from it, even though it didn't solve my problem (I guess it wasn't very well explained). My problem was basically that I have a data matrix consisting of just 1 row, and I want to convert that row into a vector. However, when trying to do that dynamically, I couldn't get R to read the right hand side of the syntax as a variable name instead of a string. However, together with a colleague I finally solved it with the (eval(as.name()) function (I include the loop I used below). I understand that looping isn't kosher among you more devoted R-users, and eventually I hope I will learn to use lists in the future instead.
>>
>> Thanks!
>> Love
>>
>>
>> for (year in 2000:2007){
>> varname <- paste0("aa_",year)
>> assign(paste0(varname), as.vector(eval(as.name(varname))))
>> }
>>
>> -----Ursprungligt meddelande-----
>> Fr?n: peter dalgaard [mailto:pdalgd at gmail.com]
>> Skickat: den 4 december 2017 16:39
>> Till: Love Bohman <love.bohman at sociology.su.se>
>> Kopia: r-help at r-project.org
>> ?mne: Re: [R] Dynamic reference, right-hand side of function
>>
>> The generic rule is that R is not a macro language, so looping of names of things gets awkward. It is usually easier to use compound objects like lists and iterate over them. E.g.
>>
>> datanames <- paste0("aa_", 2000:2007)
>> datalist <- lapply(datanames, get)
>> names(datalist) <- datanames
>> col1 <- lapply(datalist, "[[", 1)
>> colnum <- lapply(col1, as.numeric)
>>
>> (The 2nd line assumes that the damage has already been done so that
>> you have aa_2000 ... aa_2007 in your workspace. You might
>> alternatively create the list directly while importing the data.)
>>
>> -pd
>>
>>> On 4 Dec 2017, at 12:33 , Love Bohman <love.bohman at sociology.su.se> wrote:
>>>
>>> Hi R-users!
>>> Being new to R, and a fairly advanced Stata-user, I guess part of my problem is that my mindset (and probably my language as well) is wrong. Anyway, I have what I guess is a rather simple problem, that I now without success spent days trying to solve.
>>>
>>> I have a bunch of datasets imported from Stata that is labelled aa_2000 aa_2001 aa_2002, etc. Each dataset is imported as a matrix, and consists of one column only. The columns consists of integer numbers. I need to convert the data to vectors, which I found several ways to do. I use, for example:
>>> aa_2000 <- as.numeric(aa_2000[,1])
>>> However, when trying to automate the task, so I don't have to write a line of code for each dataset, I get stuck. Since I'm a Stata user, my first attempt is trying to make a loop in order to loop over all datasets. However, I manage to write a loop that works for the left-hand side of the syntax, but not for the right-hand side.
>>> I have included some examples from my struggles to solve the issue below, what they all have in common is that I don't manage to call for any "macro" (is that only a Stata-word?) in the right hand side of the functions. When I try to replace the static reference with a dynamic one (like in the left-hand side), the syntax just doesn't work.
>>>
>>> I would very much appreciate some help with this issue!
>>> All the best,
>>> Love
>>>
>>> year <- 2002
>>> dataname <- paste0("aa_",year)
>>> assign(paste0(dataname), as.numeric(aa_2002[,1]))
>>>
>>> year <- 2003
>>> assign(paste0("aa_",year), as.numeric(aa_2003))
>>>
>>> year <- 2005
>>> assign(paste0("aa_",year), aa_2005[,1])
>>>
>>> list1 <- c(2000:2007)
>>> list1[c(7)]
>>> assign(paste0("aa_",list1[c(7)]), as.numeric(paste0(aa_2006)))
>>>
>>>
>>> 	[[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School Solbjerg Plads 3,
>> 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>
>>
>>
>>
>>
>>
>>
>>
>>
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From bgunter.4567 at gmail.com  Tue Dec  5 00:37:58 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 4 Dec 2017 15:37:58 -0800
Subject: [R] Boundary_maps
In-Reply-To: <CAJRXdJA7hsjmf4KaXH_dv37ygFBPHKkpbw8-79nQ4u5tdM6qeA@mail.gmail.com>
References: <CAJRXdJA7hsjmf4KaXH_dv37ygFBPHKkpbw8-79nQ4u5tdM6qeA@mail.gmail.com>
Message-ID: <CAGxFJbRXWugJorzmKvPgbbGWuCT70QM7xv7kTCV5YwN6jzvCuA@mail.gmail.com>

See the CRAN Spatial task view:
https://cran.r-project.org/web/views/Spatial.html for relevant packages (I
think).

Further queries should probably be directed to the r-sig-geo list, where
the relevant expertise is more likely to be found.

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Mon, Dec 4, 2017 at 9:41 AM, Lara Dutra Silva <laradutrasilva at gmail.com>
wrote:

> Hello,
>
> I have a question. Is there any code in R, so I can delimiy the study area
> (bounder)?
>
>
>
> plot(proj63PA$Pittosporum_EMmeanByROC_mergedAlgo_mergedRun_mergedData,
> main= "", xlab ="x.coords", ylab="y.coords", cex.axis=caxis)
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From love.bohman at sociology.su.se  Tue Dec  5 01:23:24 2017
From: love.bohman at sociology.su.se (Love Bohman)
Date: Tue, 5 Dec 2017 00:23:24 +0000
Subject: [R] Dynamic reference, right-hand side of function
In-Reply-To: <alpine.BSF.2.00.1712041430410.70142@pedal.dcn.davis.ca.us>
References: <d200347436c940f695583220f01ac86c@ebox-prod-srv09.win.su.se>
 <368012C4-4CB5-406E-84CE-019D0986CC99@gmail.com>
 <dd6a7dcd237542eebeb064aa3493c7bf@ebox-prod-srv09.win.su.se>
 <816C0DF8-27F5-443A-80F4-8FE9633CA648@gmail.com>
 <283ecb0b898e414295cc21d47c8584ce@ebox-prod-srv09.win.su.se>
 <alpine.BSF.2.00.1712041430410.70142@pedal.dcn.davis.ca.us>
Message-ID: <53a206a2b0f64b3788aab0b542c09d8d@ebox-prod-srv09.win.su.se>

Hi again!
I know you don't find loops evil (well, at least not diabolic :-) ). (After many hours googling I have realized that thinking about loops rather than lists is a newbie thing we Stata-users do, I just jokingly pointed it out). Anyway, I'm really happy that you try to teach me some R-manners. Since I still get questions about what the h**k I mean by my strange question, I sort it out with an example:

I had a number of matrices, named in a consecutive manner:

aa_2000 <- as.matrix(read.csv(text="1,0,1,1,0,0,0,0,0,0,1,0,0", header=FALSE))
aa_2001 <- as.matrix(read.csv( text="0,0,0,1,0,1,1,0,0,0,0,1,0,0", header=FALSE))
aa_2002 <- as.matrix(read.csv( text="1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0", header=FALSE))

I needed them to be vectors, and they weren't:

is.vector(aa_2000)

I finally solved it with this loop (well, I admit I shaped up the last line thanks to William Dunlap):

for (year in 2000:2002){
varname <- paste0("aa_",year)
assign(varname, as.vector(eval(as.name(varname))))
}

The loop obviously solved the problem:

is.vector(aa_2000)                     

However, you have taught me that I should have solved it more elegant with a data list:
  
bb_2000 <- as.matrix(read.csv(text="1,0,1,1,0,0,0,0,0,0,1,0,0", header=FALSE))
bb_2001 <- as.matrix(read.csv( text="0,0,0,1,0,1,1,0,0,0,0,1,0,0", header=FALSE))
bb_2002 <- as.matrix(read.csv( text="1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0", header=FALSE))

is.vector(bb_2000)                     

datanames <- paste0("bb_", 2000:2002)
datalist <- lapply(datanames, get)
is.vector(datalist[1])


I learned a lot of code today, and I really appreciate it! A million thanks!
My R-superpowers are, well, not as minuscule as when I woke up this morning.

All the best,
Love (or maybe LoveR, my future superhero name)



-----Ursprungligt meddelande-----
Fr?n: Jeff Newmiller [mailto:jdnewmil at dcn.davis.ca.us] 
Skickat: den 5 december 2017 00:01
Till: Love Bohman <love.bohman at sociology.su.se>
Kopia: peter dalgaard <pdalgd at gmail.com>; r-help at r-project.org
?mne: Re: [R] Dynamic reference, right-hand side of function

Loops are not evil, and no-one in this thread said they are. But I believe your failure to provide a reproducible example is creating confusion, since you may be using words that mean one thing to you and something else to the readers here.

################################
# A reproducible example includes a tiny set of sample data # Since we cannot reproducibly refer to filenames (your directories # and files in them are unlikely to be like mine), I will # use a little trick to read from data in the example:

dta <- read.csv( text=
"1.1,3.0,5,7.4,4,2.2,0
", header=FALSE)
str(dta)
#> 'data.frame':    1 obs. of  7 variables:
#>  $ V1: num 1.1
#>  $ V2: num 3
#>  $ V3: int 5
#>  $ V4: num 7.4
#>  $ V5: int 4
#>  $ V6: num 2.2
#>  $ V7: int 0

# note that I did not use "data" as the name because # there is a commonly-used function by that name in R # that could be confused with your variable

# if you have your object already in memory, you can use the # dput function to create R code that will re-create it in our # working environments.

dput( dta )
#> structure(list(V1 = 1.1, V2 = 3, V3 = 5L, V4 = 7.4, V5 = 4L,
#>     V6 = 2.2, V7 = 0L), .Names = c("V1", "V2", "V3", "V4", "V5",
#> "V6", "V7"), class = "data.frame", row.names = c(NA, -1L))

# which you can put a variable name in front of in your example code:

dtasample <- structure(list( V1 = 1.1, V2 = 3, V3 = 5L , V4 = 7.4, V5 = 4L, V6 = 2.2, V7 = 0L ) , .Names = c( "V1"
, "V2", "V3", "V4", "V5", "V6", "V7" ), class = "data.frame"
, row.names = c(NA, -1L) )

# and starting with that line you can make a self-contained # (reproducible) example for us to investigate your problem with

# Note that reading a single row of data into R usually gets # a data frame, which looks like a matrix but is not a matrix.
# Read the Introduction to R about these two types carefully.
# Each column in a data frame can have a different type of data, # but in a vector or a matrix all rows and columns must be of # the same type.

dtam <- as.matrix( dta )

# If you have any values that R cannot clearly identify as numeric # or integer, then the next most general type of variable is # character... and that is often something that trips up newbies, # though I have no evidence that you have any non-numeric columns # in your data frames.

dtax <- as.vector( dta )
str(dtax)
#> 'data.frame':    1 obs. of  7 variables:
#>  $ V1: num 1.1
#>  $ V2: num 3
#>  $ V3: int 5
#>  $ V4: num 7.4
#>  $ V5: int 4
#>  $ V6: num 2.2
#>  $ V7: int 0

# This actually makes no change to dta, because a data frame is already # a list of columns, and lists are just vectors that can hold different # types of variables, so dta is already a kind of vector.

dtan <- as.numeric( dta )
str(dtan)
#>  num [1:7] 1.1 3 5 7.4 4 2.2 0

# I suspect this is what you are trying to accomplish... but really, # if we had an example of the data you are working with, we would # already know.
################################

Some more explanations of reproducibility [1][2][3]

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

[2] http://adv-r.had.co.nz/Reproducibility.html

[3] https://cran.r-project.org/web/packages/reprex/index.html (read the
vignette)


On Mon, 4 Dec 2017, Love Bohman wrote:

> :-)
> I don't insist on anything, I'm just struggling to learn a new language and partly a new way of thinking, and I really appreciate the corrections. I hope I someday will be able to handle lists in R as easy as I handle loops in Stata...
> Thanks again!
>
> Love
>
>
> -----Ursprungligt meddelande-----
> Fr?n: peter dalgaard [mailto:pdalgd at gmail.com]
> Skickat: den 4 december 2017 23:09
> Till: Love Bohman <love.bohman at sociology.su.se>
> Kopia: r-help at r-project.org
> ?mne: Re: [R] Dynamic reference, right-hand side of function
>
> Um, if you insist on doing it that way, at least use
>
> assign(varname, as.vector(get(varname)))
>
> -pd
>
>> On 4 Dec 2017, at 22:46 , Love Bohman <love.bohman at sociology.su.se> wrote:
>>
>> Hi!
>> Thanks for the replies!
>> I understand people more accustomed to R doesn't like looping much, and that thinking about loops is something I do since I worked with Stata a lot. The syntax from Peter Dalgaard was really clever, and I learned a lot from it, even though it didn't solve my problem (I guess it wasn't very well explained). My problem was basically that I have a data matrix consisting of just 1 row, and I want to convert that row into a vector. However, when trying to do that dynamically, I couldn't get R to read the right hand side of the syntax as a variable name instead of a string. However, together with a colleague I finally solved it with the (eval(as.name()) function (I include the loop I used below). I understand that looping isn't kosher among you more devoted R-users, and eventually I hope I will learn to use lists in the future instead.
>>
>> Thanks!
>> Love
>>
>>
>> for (year in 2000:2007){
>> varname <- paste0("aa_",year)
>> assign(paste0(varname), as.vector(eval(as.name(varname))))
>> }
>>
>> -----Ursprungligt meddelande-----
>> Fr?n: peter dalgaard [mailto:pdalgd at gmail.com]
>> Skickat: den 4 december 2017 16:39
>> Till: Love Bohman <love.bohman at sociology.su.se>
>> Kopia: r-help at r-project.org
>> ?mne: Re: [R] Dynamic reference, right-hand side of function
>>
>> The generic rule is that R is not a macro language, so looping of names of things gets awkward. It is usually easier to use compound objects like lists and iterate over them. E.g.
>>
>> datanames <- paste0("aa_", 2000:2007)
>> datalist <- lapply(datanames, get)
>> names(datalist) <- datanames
>> col1 <- lapply(datalist, "[[", 1)
>> colnum <- lapply(col1, as.numeric)
>>
>> (The 2nd line assumes that the damage has already been done so that
>> you have aa_2000 ... aa_2007 in your workspace. You might
>> alternatively create the list directly while importing the data.)
>>
>> -pd
>>
>>> On 4 Dec 2017, at 12:33 , Love Bohman <love.bohman at sociology.su.se> wrote:
>>>
>>> Hi R-users!
>>> Being new to R, and a fairly advanced Stata-user, I guess part of my problem is that my mindset (and probably my language as well) is wrong. Anyway, I have what I guess is a rather simple problem, that I now without success spent days trying to solve.
>>>
>>> I have a bunch of datasets imported from Stata that is labelled aa_2000 aa_2001 aa_2002, etc. Each dataset is imported as a matrix, and consists of one column only. The columns consists of integer numbers. I need to convert the data to vectors, which I found several ways to do. I use, for example:
>>> aa_2000 <- as.numeric(aa_2000[,1])
>>> However, when trying to automate the task, so I don't have to write a line of code for each dataset, I get stuck. Since I'm a Stata user, my first attempt is trying to make a loop in order to loop over all datasets. However, I manage to write a loop that works for the left-hand side of the syntax, but not for the right-hand side.
>>> I have included some examples from my struggles to solve the issue below, what they all have in common is that I don't manage to call for any "macro" (is that only a Stata-word?) in the right hand side of the functions. When I try to replace the static reference with a dynamic one (like in the left-hand side), the syntax just doesn't work.
>>>
>>> I would very much appreciate some help with this issue!
>>> All the best,
>>> Love
>>>
>>> year <- 2002
>>> dataname <- paste0("aa_",year)
>>> assign(paste0(dataname), as.numeric(aa_2002[,1]))
>>>
>>> year <- 2003
>>> assign(paste0("aa_",year), as.numeric(aa_2003))
>>>
>>> year <- 2005
>>> assign(paste0("aa_",year), aa_2005[,1])
>>>
>>> list1 <- c(2000:2007)
>>> list1[c(7)]
>>> assign(paste0("aa_",list1[c(7)]), as.numeric(paste0(aa_2006)))
>>>
>>>
>>> 	[[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School Solbjerg Plads 3,
>> 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>
>>
>>
>>
>>
>>
>>
>>
>>
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From wdunlap at tibco.com  Tue Dec  5 01:40:40 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 4 Dec 2017 16:40:40 -0800
Subject: [R] Dynamic reference, right-hand side of function
In-Reply-To: <53a206a2b0f64b3788aab0b542c09d8d@ebox-prod-srv09.win.su.se>
References: <d200347436c940f695583220f01ac86c@ebox-prod-srv09.win.su.se>
 <368012C4-4CB5-406E-84CE-019D0986CC99@gmail.com>
 <dd6a7dcd237542eebeb064aa3493c7bf@ebox-prod-srv09.win.su.se>
 <816C0DF8-27F5-443A-80F4-8FE9633CA648@gmail.com>
 <283ecb0b898e414295cc21d47c8584ce@ebox-prod-srv09.win.su.se>
 <alpine.BSF.2.00.1712041430410.70142@pedal.dcn.davis.ca.us>
 <53a206a2b0f64b3788aab0b542c09d8d@ebox-prod-srv09.win.su.se>
Message-ID: <CAF8bMcYF9SNdfCs+pe1YpPhci1J89E1cz3TXp4j0xPYLPq6Nng@mail.gmail.com>

By the way, R 'vectors' are not the equivalents of mathematical 'vectors'.
In R, a vector is something that can have arbitrary length and which has
no 'attributes', other than perhaps element names.  Vectors can be numeric,
character,
complex, lists, etc.   Functions, names, and NULL are not vectors.  In my
opinion,
the typical data scientist will rarely find the R vector concept useful and
it more
likely to make mistakes with it.

Instead of using as.vector and is.vector, use as.numeric or as.character,
etc.,
and is.numeric or is.character, naming the type you want.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Dec 4, 2017 at 4:23 PM, Love Bohman <love.bohman at sociology.su.se>
wrote:

> Hi again!
> I know you don't find loops evil (well, at least not diabolic :-) ).
> (After many hours googling I have realized that thinking about loops rather
> than lists is a newbie thing we Stata-users do, I just jokingly pointed it
> out). Anyway, I'm really happy that you try to teach me some R-manners.
> Since I still get questions about what the h**k I mean by my strange
> question, I sort it out with an example:
>
> I had a number of matrices, named in a consecutive manner:
>
> aa_2000 <- as.matrix(read.csv(text="1,0,1,1,0,0,0,0,0,0,1,0,0",
> header=FALSE))
> aa_2001 <- as.matrix(read.csv( text="0,0,0,1,0,1,1,0,0,0,0,1,0,0",
> header=FALSE))
> aa_2002 <- as.matrix(read.csv( text="1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0",
> header=FALSE))
>
> I needed them to be vectors, and they weren't:
>
> is.vector(aa_2000)
>
> I finally solved it with this loop (well, I admit I shaped up the last
> line thanks to William Dunlap):
>
> for (year in 2000:2002){
> varname <- paste0("aa_",year)
> assign(varname, as.vector(eval(as.name(varname))))
> }
>
> The loop obviously solved the problem:
>
> is.vector(aa_2000)
>
> However, you have taught me that I should have solved it more elegant with
> a data list:
>
> bb_2000 <- as.matrix(read.csv(text="1,0,1,1,0,0,0,0,0,0,1,0,0",
> header=FALSE))
> bb_2001 <- as.matrix(read.csv( text="0,0,0,1,0,1,1,0,0,0,0,1,0,0",
> header=FALSE))
> bb_2002 <- as.matrix(read.csv( text="1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0",
> header=FALSE))
>
> is.vector(bb_2000)
>
> datanames <- paste0("bb_", 2000:2002)
> datalist <- lapply(datanames, get)
> is.vector(datalist[1])
>
>
> I learned a lot of code today, and I really appreciate it! A million
> thanks!
> My R-superpowers are, well, not as minuscule as when I woke up this
> morning.
>
> All the best,
> Love (or maybe LoveR, my future superhero name)
>
>
>
> -----Ursprungligt meddelande-----
> Fr?n: Jeff Newmiller [mailto:jdnewmil at dcn.davis.ca.us]
> Skickat: den 5 december 2017 00:01
> Till: Love Bohman <love.bohman at sociology.su.se>
> Kopia: peter dalgaard <pdalgd at gmail.com>; r-help at r-project.org
> ?mne: Re: [R] Dynamic reference, right-hand side of function
>
> Loops are not evil, and no-one in this thread said they are. But I believe
> your failure to provide a reproducible example is creating confusion, since
> you may be using words that mean one thing to you and something else to the
> readers here.
>
> ################################
> # A reproducible example includes a tiny set of sample data # Since we
> cannot reproducibly refer to filenames (your directories # and files in
> them are unlikely to be like mine), I will # use a little trick to read
> from data in the example:
>
> dta <- read.csv( text=
> "1.1,3.0,5,7.4,4,2.2,0
> ", header=FALSE)
> str(dta)
> #> 'data.frame':    1 obs. of  7 variables:
> #>  $ V1: num 1.1
> #>  $ V2: num 3
> #>  $ V3: int 5
> #>  $ V4: num 7.4
> #>  $ V5: int 4
> #>  $ V6: num 2.2
> #>  $ V7: int 0
>
> # note that I did not use "data" as the name because # there is a
> commonly-used function by that name in R # that could be confused with your
> variable
>
> # if you have your object already in memory, you can use the # dput
> function to create R code that will re-create it in our # working
> environments.
>
> dput( dta )
> #> structure(list(V1 = 1.1, V2 = 3, V3 = 5L, V4 = 7.4, V5 = 4L,
> #>     V6 = 2.2, V7 = 0L), .Names = c("V1", "V2", "V3", "V4", "V5",
> #> "V6", "V7"), class = "data.frame", row.names = c(NA, -1L))
>
> # which you can put a variable name in front of in your example code:
>
> dtasample <- structure(list( V1 = 1.1, V2 = 3, V3 = 5L , V4 = 7.4, V5 =
> 4L, V6 = 2.2, V7 = 0L ) , .Names = c( "V1"
> , "V2", "V3", "V4", "V5", "V6", "V7" ), class = "data.frame"
> , row.names = c(NA, -1L) )
>
> # and starting with that line you can make a self-contained #
> (reproducible) example for us to investigate your problem with
>
> # Note that reading a single row of data into R usually gets # a data
> frame, which looks like a matrix but is not a matrix.
> # Read the Introduction to R about these two types carefully.
> # Each column in a data frame can have a different type of data, # but in
> a vector or a matrix all rows and columns must be of # the same type.
>
> dtam <- as.matrix( dta )
>
> # If you have any values that R cannot clearly identify as numeric # or
> integer, then the next most general type of variable is # character... and
> that is often something that trips up newbies, # though I have no evidence
> that you have any non-numeric columns # in your data frames.
>
> dtax <- as.vector( dta )
> str(dtax)
> #> 'data.frame':    1 obs. of  7 variables:
> #>  $ V1: num 1.1
> #>  $ V2: num 3
> #>  $ V3: int 5
> #>  $ V4: num 7.4
> #>  $ V5: int 4
> #>  $ V6: num 2.2
> #>  $ V7: int 0
>
> # This actually makes no change to dta, because a data frame is already #
> a list of columns, and lists are just vectors that can hold different #
> types of variables, so dta is already a kind of vector.
>
> dtan <- as.numeric( dta )
> str(dtan)
> #>  num [1:7] 1.1 3 5 7.4 4 2.2 0
>
> # I suspect this is what you are trying to accomplish... but really, # if
> we had an example of the data you are working with, we would # already know.
> ################################
>
> Some more explanations of reproducibility [1][2][3]
>
> [1] http://stackoverflow.com/questions/5963269/how-to-make-
> a-great-r-reproducible-example
>
> [2] http://adv-r.had.co.nz/Reproducibility.html
>
> [3] https://cran.r-project.org/web/packages/reprex/index.html (read the
> vignette)
>
>
> On Mon, 4 Dec 2017, Love Bohman wrote:
>
> > :-)
> > I don't insist on anything, I'm just struggling to learn a new language
> and partly a new way of thinking, and I really appreciate the corrections.
> I hope I someday will be able to handle lists in R as easy as I handle
> loops in Stata...
> > Thanks again!
> >
> > Love
> >
> >
> > -----Ursprungligt meddelande-----
> > Fr?n: peter dalgaard [mailto:pdalgd at gmail.com]
> > Skickat: den 4 december 2017 23:09
> > Till: Love Bohman <love.bohman at sociology.su.se>
> > Kopia: r-help at r-project.org
> > ?mne: Re: [R] Dynamic reference, right-hand side of function
> >
> > Um, if you insist on doing it that way, at least use
> >
> > assign(varname, as.vector(get(varname)))
> >
> > -pd
> >
> >> On 4 Dec 2017, at 22:46 , Love Bohman <love.bohman at sociology.su.se>
> wrote:
> >>
> >> Hi!
> >> Thanks for the replies!
> >> I understand people more accustomed to R doesn't like looping much, and
> that thinking about loops is something I do since I worked with Stata a
> lot. The syntax from Peter Dalgaard was really clever, and I learned a lot
> from it, even though it didn't solve my problem (I guess it wasn't very
> well explained). My problem was basically that I have a data matrix
> consisting of just 1 row, and I want to convert that row into a vector.
> However, when trying to do that dynamically, I couldn't get R to read the
> right hand side of the syntax as a variable name instead of a string.
> However, together with a colleague I finally solved it with the (eval(
> as.name()) function (I include the loop I used below). I understand that
> looping isn't kosher among you more devoted R-users, and eventually I hope
> I will learn to use lists in the future instead.
> >>
> >> Thanks!
> >> Love
> >>
> >>
> >> for (year in 2000:2007){
> >> varname <- paste0("aa_",year)
> >> assign(paste0(varname), as.vector(eval(as.name(varname))))
> >> }
> >>
> >> -----Ursprungligt meddelande-----
> >> Fr?n: peter dalgaard [mailto:pdalgd at gmail.com]
> >> Skickat: den 4 december 2017 16:39
> >> Till: Love Bohman <love.bohman at sociology.su.se>
> >> Kopia: r-help at r-project.org
> >> ?mne: Re: [R] Dynamic reference, right-hand side of function
> >>
> >> The generic rule is that R is not a macro language, so looping of names
> of things gets awkward. It is usually easier to use compound objects like
> lists and iterate over them. E.g.
> >>
> >> datanames <- paste0("aa_", 2000:2007)
> >> datalist <- lapply(datanames, get)
> >> names(datalist) <- datanames
> >> col1 <- lapply(datalist, "[[", 1)
> >> colnum <- lapply(col1, as.numeric)
> >>
> >> (The 2nd line assumes that the damage has already been done so that
> >> you have aa_2000 ... aa_2007 in your workspace. You might
> >> alternatively create the list directly while importing the data.)
> >>
> >> -pd
> >>
> >>> On 4 Dec 2017, at 12:33 , Love Bohman <love.bohman at sociology.su.se>
> wrote:
> >>>
> >>> Hi R-users!
> >>> Being new to R, and a fairly advanced Stata-user, I guess part of my
> problem is that my mindset (and probably my language as well) is wrong.
> Anyway, I have what I guess is a rather simple problem, that I now without
> success spent days trying to solve.
> >>>
> >>> I have a bunch of datasets imported from Stata that is labelled
> aa_2000 aa_2001 aa_2002, etc. Each dataset is imported as a matrix, and
> consists of one column only. The columns consists of integer numbers. I
> need to convert the data to vectors, which I found several ways to do. I
> use, for example:
> >>> aa_2000 <- as.numeric(aa_2000[,1])
> >>> However, when trying to automate the task, so I don't have to write a
> line of code for each dataset, I get stuck. Since I'm a Stata user, my
> first attempt is trying to make a loop in order to loop over all datasets.
> However, I manage to write a loop that works for the left-hand side of the
> syntax, but not for the right-hand side.
> >>> I have included some examples from my struggles to solve the issue
> below, what they all have in common is that I don't manage to call for any
> "macro" (is that only a Stata-word?) in the right hand side of the
> functions. When I try to replace the static reference with a dynamic one
> (like in the left-hand side), the syntax just doesn't work.
> >>>
> >>> I would very much appreciate some help with this issue!
> >>> All the best,
> >>> Love
> >>>
> >>> year <- 2002
> >>> dataname <- paste0("aa_",year)
> >>> assign(paste0(dataname), as.numeric(aa_2002[,1]))
> >>>
> >>> year <- 2003
> >>> assign(paste0("aa_",year), as.numeric(aa_2003))
> >>>
> >>> year <- 2005
> >>> assign(paste0("aa_",year), aa_2005[,1])
> >>>
> >>> list1 <- c(2000:2007)
> >>> list1[c(7)]
> >>> assign(paste0("aa_",list1[c(7)]), as.numeric(paste0(aa_2006)))
> >>>
> >>>
> >>>     [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Peter Dalgaard, Professor,
> >> Center for Statistics, Copenhagen Business School Solbjerg Plads 3,
> >> 2000 Frederiksberg, Denmark
> >> Phone: (+45)38153501
> >> Office: A 4.23
> >> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >
> > --
> > Peter Dalgaard, Professor,
> > Center for Statistics, Copenhagen Business School Solbjerg Plads 3, 2000
> Frederiksberg, Denmark
> > Phone: (+45)38153501
> > Office: A 4.23
> > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >
> >
> >
> >
> >
> >
> >
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ------------------------------------------------------------
> ---------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> Go...
>                                        Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Dec  5 02:11:47 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 4 Dec 2017 17:11:47 -0800
Subject: [R] Dynamic reference, right-hand side of function
In-Reply-To: <CAF8bMcYF9SNdfCs+pe1YpPhci1J89E1cz3TXp4j0xPYLPq6Nng@mail.gmail.com>
References: <d200347436c940f695583220f01ac86c@ebox-prod-srv09.win.su.se>
 <368012C4-4CB5-406E-84CE-019D0986CC99@gmail.com>
 <dd6a7dcd237542eebeb064aa3493c7bf@ebox-prod-srv09.win.su.se>
 <816C0DF8-27F5-443A-80F4-8FE9633CA648@gmail.com>
 <283ecb0b898e414295cc21d47c8584ce@ebox-prod-srv09.win.su.se>
 <alpine.BSF.2.00.1712041430410.70142@pedal.dcn.davis.ca.us>
 <53a206a2b0f64b3788aab0b542c09d8d@ebox-prod-srv09.win.su.se>
 <CAF8bMcYF9SNdfCs+pe1YpPhci1J89E1cz3TXp4j0xPYLPq6Nng@mail.gmail.com>
Message-ID: <CAGxFJbQRBhwxBF19TVsx9ny+2nwnycqrgSb-trs=Vp7i6_z18g@mail.gmail.com>

It appears that much of the OP's confusion could be cleared up by studying
relevant resources, e.g. online tutorials or the R Language Maual that
ships with R. This list is not meant to replace such "homework" by users,
as this lengthy and confusing interchange demonstrates.

Having said that, the OP can check that a matrix of any dimensions is
represented in R as a vector in column major order with a "dim" attribute.
So:

> a <- matrix(1:10, nr = 5)
> dim(a)
[1] 5 2
> a ## invokes the print() method for matrices
     [,1] [,2]
[1,]    1    6
[2,]    2    7
[3,]    3    8
[4,]    4    9
[5,]    5   10
> is.vector(a)
[1] FALSE
> dim(a) <- NULL ## removes the 'dim' attribute
> is.vector(a)
[1] TRUE
> a
 [1]  1  2  3  4  5  6  7  8  9 10

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Mon, Dec 4, 2017 at 4:40 PM, William Dunlap via R-help <
r-help at r-project.org> wrote:

> By the way, R 'vectors' are not the equivalents of mathematical 'vectors'.
> In R, a vector is something that can have arbitrary length and which has
> no 'attributes', other than perhaps element names.  Vectors can be numeric,
> character,
> complex, lists, etc.   Functions, names, and NULL are not vectors.  In my
> opinion,
> the typical data scientist will rarely find the R vector concept useful and
> it more
> likely to make mistakes with it.
>
> Instead of using as.vector and is.vector, use as.numeric or as.character,
> etc.,
> and is.numeric or is.character, naming the type you want.
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Mon, Dec 4, 2017 at 4:23 PM, Love Bohman <love.bohman at sociology.su.se>
> wrote:
>
> > Hi again!
> > I know you don't find loops evil (well, at least not diabolic :-) ).
> > (After many hours googling I have realized that thinking about loops
> rather
> > than lists is a newbie thing we Stata-users do, I just jokingly pointed
> it
> > out). Anyway, I'm really happy that you try to teach me some R-manners.
> > Since I still get questions about what the h**k I mean by my strange
> > question, I sort it out with an example:
> >
> > I had a number of matrices, named in a consecutive manner:
> >
> > aa_2000 <- as.matrix(read.csv(text="1,0,1,1,0,0,0,0,0,0,1,0,0",
> > header=FALSE))
> > aa_2001 <- as.matrix(read.csv( text="0,0,0,1,0,1,1,0,0,0,0,1,0,0",
> > header=FALSE))
> > aa_2002 <- as.matrix(read.csv( text="1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0",
> > header=FALSE))
> >
> > I needed them to be vectors, and they weren't:
> >
> > is.vector(aa_2000)
> >
> > I finally solved it with this loop (well, I admit I shaped up the last
> > line thanks to William Dunlap):
> >
> > for (year in 2000:2002){
> > varname <- paste0("aa_",year)
> > assign(varname, as.vector(eval(as.name(varname))))
> > }
> >
> > The loop obviously solved the problem:
> >
> > is.vector(aa_2000)
> >
> > However, you have taught me that I should have solved it more elegant
> with
> > a data list:
> >
> > bb_2000 <- as.matrix(read.csv(text="1,0,1,1,0,0,0,0,0,0,1,0,0",
> > header=FALSE))
> > bb_2001 <- as.matrix(read.csv( text="0,0,0,1,0,1,1,0,0,0,0,1,0,0",
> > header=FALSE))
> > bb_2002 <- as.matrix(read.csv( text="1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0",
> > header=FALSE))
> >
> > is.vector(bb_2000)
> >
> > datanames <- paste0("bb_", 2000:2002)
> > datalist <- lapply(datanames, get)
> > is.vector(datalist[1])
> >
> >
> > I learned a lot of code today, and I really appreciate it! A million
> > thanks!
> > My R-superpowers are, well, not as minuscule as when I woke up this
> > morning.
> >
> > All the best,
> > Love (or maybe LoveR, my future superhero name)
> >
> >
> >
> > -----Ursprungligt meddelande-----
> > Fr?n: Jeff Newmiller [mailto:jdnewmil at dcn.davis.ca.us]
> > Skickat: den 5 december 2017 00:01
> > Till: Love Bohman <love.bohman at sociology.su.se>
> > Kopia: peter dalgaard <pdalgd at gmail.com>; r-help at r-project.org
> > ?mne: Re: [R] Dynamic reference, right-hand side of function
> >
> > Loops are not evil, and no-one in this thread said they are. But I
> believe
> > your failure to provide a reproducible example is creating confusion,
> since
> > you may be using words that mean one thing to you and something else to
> the
> > readers here.
> >
> > ################################
> > # A reproducible example includes a tiny set of sample data # Since we
> > cannot reproducibly refer to filenames (your directories # and files in
> > them are unlikely to be like mine), I will # use a little trick to read
> > from data in the example:
> >
> > dta <- read.csv( text=
> > "1.1,3.0,5,7.4,4,2.2,0
> > ", header=FALSE)
> > str(dta)
> > #> 'data.frame':    1 obs. of  7 variables:
> > #>  $ V1: num 1.1
> > #>  $ V2: num 3
> > #>  $ V3: int 5
> > #>  $ V4: num 7.4
> > #>  $ V5: int 4
> > #>  $ V6: num 2.2
> > #>  $ V7: int 0
> >
> > # note that I did not use "data" as the name because # there is a
> > commonly-used function by that name in R # that could be confused with
> your
> > variable
> >
> > # if you have your object already in memory, you can use the # dput
> > function to create R code that will re-create it in our # working
> > environments.
> >
> > dput( dta )
> > #> structure(list(V1 = 1.1, V2 = 3, V3 = 5L, V4 = 7.4, V5 = 4L,
> > #>     V6 = 2.2, V7 = 0L), .Names = c("V1", "V2", "V3", "V4", "V5",
> > #> "V6", "V7"), class = "data.frame", row.names = c(NA, -1L))
> >
> > # which you can put a variable name in front of in your example code:
> >
> > dtasample <- structure(list( V1 = 1.1, V2 = 3, V3 = 5L , V4 = 7.4, V5 =
> > 4L, V6 = 2.2, V7 = 0L ) , .Names = c( "V1"
> > , "V2", "V3", "V4", "V5", "V6", "V7" ), class = "data.frame"
> > , row.names = c(NA, -1L) )
> >
> > # and starting with that line you can make a self-contained #
> > (reproducible) example for us to investigate your problem with
> >
> > # Note that reading a single row of data into R usually gets # a data
> > frame, which looks like a matrix but is not a matrix.
> > # Read the Introduction to R about these two types carefully.
> > # Each column in a data frame can have a different type of data, # but in
> > a vector or a matrix all rows and columns must be of # the same type.
> >
> > dtam <- as.matrix( dta )
> >
> > # If you have any values that R cannot clearly identify as numeric # or
> > integer, then the next most general type of variable is # character...
> and
> > that is often something that trips up newbies, # though I have no
> evidence
> > that you have any non-numeric columns # in your data frames.
> >
> > dtax <- as.vector( dta )
> > str(dtax)
> > #> 'data.frame':    1 obs. of  7 variables:
> > #>  $ V1: num 1.1
> > #>  $ V2: num 3
> > #>  $ V3: int 5
> > #>  $ V4: num 7.4
> > #>  $ V5: int 4
> > #>  $ V6: num 2.2
> > #>  $ V7: int 0
> >
> > # This actually makes no change to dta, because a data frame is already #
> > a list of columns, and lists are just vectors that can hold different #
> > types of variables, so dta is already a kind of vector.
> >
> > dtan <- as.numeric( dta )
> > str(dtan)
> > #>  num [1:7] 1.1 3 5 7.4 4 2.2 0
> >
> > # I suspect this is what you are trying to accomplish... but really, # if
> > we had an example of the data you are working with, we would # already
> know.
> > ################################
> >
> > Some more explanations of reproducibility [1][2][3]
> >
> > [1] http://stackoverflow.com/questions/5963269/how-to-make-
> > a-great-r-reproducible-example
> >
> > [2] http://adv-r.had.co.nz/Reproducibility.html
> >
> > [3] https://cran.r-project.org/web/packages/reprex/index.html (read the
> > vignette)
> >
> >
> > On Mon, 4 Dec 2017, Love Bohman wrote:
> >
> > > :-)
> > > I don't insist on anything, I'm just struggling to learn a new language
> > and partly a new way of thinking, and I really appreciate the
> corrections.
> > I hope I someday will be able to handle lists in R as easy as I handle
> > loops in Stata...
> > > Thanks again!
> > >
> > > Love
> > >
> > >
> > > -----Ursprungligt meddelande-----
> > > Fr?n: peter dalgaard [mailto:pdalgd at gmail.com]
> > > Skickat: den 4 december 2017 23:09
> > > Till: Love Bohman <love.bohman at sociology.su.se>
> > > Kopia: r-help at r-project.org
> > > ?mne: Re: [R] Dynamic reference, right-hand side of function
> > >
> > > Um, if you insist on doing it that way, at least use
> > >
> > > assign(varname, as.vector(get(varname)))
> > >
> > > -pd
> > >
> > >> On 4 Dec 2017, at 22:46 , Love Bohman <love.bohman at sociology.su.se>
> > wrote:
> > >>
> > >> Hi!
> > >> Thanks for the replies!
> > >> I understand people more accustomed to R doesn't like looping much,
> and
> > that thinking about loops is something I do since I worked with Stata a
> > lot. The syntax from Peter Dalgaard was really clever, and I learned a
> lot
> > from it, even though it didn't solve my problem (I guess it wasn't very
> > well explained). My problem was basically that I have a data matrix
> > consisting of just 1 row, and I want to convert that row into a vector.
> > However, when trying to do that dynamically, I couldn't get R to read the
> > right hand side of the syntax as a variable name instead of a string.
> > However, together with a colleague I finally solved it with the (eval(
> > as.name()) function (I include the loop I used below). I understand that
> > looping isn't kosher among you more devoted R-users, and eventually I
> hope
> > I will learn to use lists in the future instead.
> > >>
> > >> Thanks!
> > >> Love
> > >>
> > >>
> > >> for (year in 2000:2007){
> > >> varname <- paste0("aa_",year)
> > >> assign(paste0(varname), as.vector(eval(as.name(varname))))
> > >> }
> > >>
> > >> -----Ursprungligt meddelande-----
> > >> Fr?n: peter dalgaard [mailto:pdalgd at gmail.com]
> > >> Skickat: den 4 december 2017 16:39
> > >> Till: Love Bohman <love.bohman at sociology.su.se>
> > >> Kopia: r-help at r-project.org
> > >> ?mne: Re: [R] Dynamic reference, right-hand side of function
> > >>
> > >> The generic rule is that R is not a macro language, so looping of
> names
> > of things gets awkward. It is usually easier to use compound objects like
> > lists and iterate over them. E.g.
> > >>
> > >> datanames <- paste0("aa_", 2000:2007)
> > >> datalist <- lapply(datanames, get)
> > >> names(datalist) <- datanames
> > >> col1 <- lapply(datalist, "[[", 1)
> > >> colnum <- lapply(col1, as.numeric)
> > >>
> > >> (The 2nd line assumes that the damage has already been done so that
> > >> you have aa_2000 ... aa_2007 in your workspace. You might
> > >> alternatively create the list directly while importing the data.)
> > >>
> > >> -pd
> > >>
> > >>> On 4 Dec 2017, at 12:33 , Love Bohman <love.bohman at sociology.su.se>
> > wrote:
> > >>>
> > >>> Hi R-users!
> > >>> Being new to R, and a fairly advanced Stata-user, I guess part of my
> > problem is that my mindset (and probably my language as well) is wrong.
> > Anyway, I have what I guess is a rather simple problem, that I now
> without
> > success spent days trying to solve.
> > >>>
> > >>> I have a bunch of datasets imported from Stata that is labelled
> > aa_2000 aa_2001 aa_2002, etc. Each dataset is imported as a matrix, and
> > consists of one column only. The columns consists of integer numbers. I
> > need to convert the data to vectors, which I found several ways to do. I
> > use, for example:
> > >>> aa_2000 <- as.numeric(aa_2000[,1])
> > >>> However, when trying to automate the task, so I don't have to write a
> > line of code for each dataset, I get stuck. Since I'm a Stata user, my
> > first attempt is trying to make a loop in order to loop over all
> datasets.
> > However, I manage to write a loop that works for the left-hand side of
> the
> > syntax, but not for the right-hand side.
> > >>> I have included some examples from my struggles to solve the issue
> > below, what they all have in common is that I don't manage to call for
> any
> > "macro" (is that only a Stata-word?) in the right hand side of the
> > functions. When I try to replace the static reference with a dynamic one
> > (like in the left-hand side), the syntax just doesn't work.
> > >>>
> > >>> I would very much appreciate some help with this issue!
> > >>> All the best,
> > >>> Love
> > >>>
> > >>> year <- 2002
> > >>> dataname <- paste0("aa_",year)
> > >>> assign(paste0(dataname), as.numeric(aa_2002[,1]))
> > >>>
> > >>> year <- 2003
> > >>> assign(paste0("aa_",year), as.numeric(aa_2003))
> > >>>
> > >>> year <- 2005
> > >>> assign(paste0("aa_",year), aa_2005[,1])
> > >>>
> > >>> list1 <- c(2000:2007)
> > >>> list1[c(7)]
> > >>> assign(paste0("aa_",list1[c(7)]), as.numeric(paste0(aa_2006)))
> > >>>
> > >>>
> > >>>     [[alternative HTML version deleted]]
> > >>>
> > >>> ______________________________________________
> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide
> > >>> http://www.R-project.org/posting-guide.html
> > >>> and provide commented, minimal, self-contained, reproducible code.
> > >>
> > >> --
> > >> Peter Dalgaard, Professor,
> > >> Center for Statistics, Copenhagen Business School Solbjerg Plads 3,
> > >> 2000 Frederiksberg, Denmark
> > >> Phone: (+45)38153501
> > >> Office: A 4.23
> > >> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> > >>
> > >>
> > >>
> > >>
> > >>
> > >>
> > >>
> > >>
> > >>
> > >
> > > --
> > > Peter Dalgaard, Professor,
> > > Center for Statistics, Copenhagen Business School Solbjerg Plads 3,
> 2000
> > Frederiksberg, Denmark
> > > Phone: (+45)38153501
> > > Office: A 4.23
> > > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/
> > posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > ------------------------------------------------------------
> > ---------------
> > Jeff Newmiller                        The     .....       .....  Go
> Live...
> > DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> > Go...
> >                                        Live:   OO#.. Dead: OO#..  Playing
> > Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> > /Software/Embedded Controllers)               .OO#.       .OO#.
> rocks...1k
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> > posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From s.anssarii at gmail.com  Tue Dec  5 08:47:24 2017
From: s.anssarii at gmail.com (Sahar Ansari)
Date: Tue, 5 Dec 2017 11:17:24 +0330
Subject: [R] cannot allocate vector of size n
Message-ID: <CALfjpQyC9502vsPSKFDb6J_BO1KPccmj=y-XgG9KFneMKv=DQQ@mail.gmail.com>

Hello

I have a memory problem,when I am running my code with RStudio-1.0.143
(64bit) on windows 2012 server with 6 GB ram (VPS)

I receive the following error message:

Error: cannot allocate vector of size 2.5 Gb

I used to run this source code on this machine with no difficulty . I
haven't changed the source code and the specification of the machine
whereas I'm facing this error recently.

I'd greatly appreciate any thoughts or suggestions that you might have.



Sincerely Yours,

	[[alternative HTML version deleted]]


From jmhannon.ucdavis at gmail.com  Tue Dec  5 10:47:52 2017
From: jmhannon.ucdavis at gmail.com (Michael Hannon)
Date: Tue, 5 Dec 2017 01:47:52 -0800
Subject: [R] cannot allocate vector of size n
In-Reply-To: <CALfjpQyC9502vsPSKFDb6J_BO1KPccmj=y-XgG9KFneMKv=DQQ@mail.gmail.com>
References: <CALfjpQyC9502vsPSKFDb6J_BO1KPccmj=y-XgG9KFneMKv=DQQ@mail.gmail.com>
Message-ID: <CACdH2ZZjB+eqwNn=DYCot0Wufr4wq6gkz+rA30O4VXZeCHtEHg@mail.gmail.com>

(1) That's an old version of R Studio, although I doubt that that's
the source of your problem.

(2) What is your session info?

    > sessionInfo()
or
    > devtools::session_info()

I just allocated a numeric vector of size 2.5e9 on a 16GB linux box (R
3.4.3).  It worked, but it pretty much exhausted my memory.

-- Mike


On Mon, Dec 4, 2017 at 11:47 PM, Sahar Ansari <s.anssarii at gmail.com> wrote:
> Hello
>
> I have a memory problem,when I am running my code with RStudio-1.0.143
> (64bit) on windows 2012 server with 6 GB ram (VPS)
>
> I receive the following error message:
>
> Error: cannot allocate vector of size 2.5 Gb
>
> I used to run this source code on this machine with no difficulty . I
> haven't changed the source code and the specification of the machine
> whereas I'm facing this error recently.
>
> I'd greatly appreciate any thoughts or suggestions that you might have.
>
>
>
> Sincerely Yours,
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From margaridapmsoares at gmail.com  Tue Dec  5 11:30:06 2017
From: margaridapmsoares at gmail.com (Margarida Soares)
Date: Tue, 5 Dec 2017 11:30:06 +0100
Subject: [R] PLS in R
Message-ID: <CACtBhRNLBzSzbgWEwCQONLDxKXv8U4DoF76hqa7i75XGgmkSPQ@mail.gmail.com>

Hello, I need help with a partial least square regression in R. I have read
both the vignette and the post on R bloggers but it is hard to figure out
how to do it. Here is the script I wrote:

library(pls)
plsrcue<- plsr(cue~fb+cn+n+ph+fung+bact+resp, data = cue, ncomp=7,
na.action = NULL, method = "kernelpls", scale=FALSE, validation = "LOO",
model = TRUE, x = FALSE, y = FALSE)
summary(plsrcue)

and I got this output, where I think I can choose the number of components
based on RMSEP, but how do I choose it?

Data:  X dimension: 33 7
Y dimension: 33 1
Fit method: kernelpls
Number of components considered: 7

VALIDATION: RMSEP
Cross-validated using 33 leave-one-out segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7
comps
CV         0.09854  0.07014  0.05366  0.04712  0.01935  0.01943  0.01882
 0.01900
adjCV      0.09854  0.06999  0.05357  0.04703  0.01930  0.01942  0.01876
 0.01893

TRAINING: % variance explained
     1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps
X      42.33    78.82    99.15    99.95   100.00   100.00   100.00
cue    56.77    76.14    81.98    97.05    97.11    97.56    97.75


- and also, how to proceed from here?

- and how to make a correlation plot?

- what to do with the values, coefficients that I get in the Environment
(pls values)


Thanks for your help!

margarida soares

	[[alternative HTML version deleted]]


From cadeb at usgs.gov  Tue Dec  5 21:17:00 2017
From: cadeb at usgs.gov (Cade, Brian)
Date: Tue, 5 Dec 2017 13:17:00 -0700
Subject: [R] warnings about factor levels dropped from predict.glm
Message-ID: <CAM5M9BQj_U44G1F416ERsbm0if=xR=iT9KU8eZ9KSp1_DUpMxg@mail.gmail.com>

I am helping a student with some logistic regression analyses and we are
getting some strange inconsistencies regarding a warning about factor
levels being dropped when running predict.glm(, newdata = ournewdata) on
the logistic regression model object.  We have checked multiple times that
the factor levels have been defined similarly on both data sets (one used
to estimate model and the newdata) and that values occur for all factor
levels in both data sets.  When I run these commands on my version of R
(3.2.5) on a Windows 7 OS I do not get the warnings.  When the student runs
them on her version of R (not sure what number hers is) on her Mac, she
gets these warnings constantly.  I've checked some records manually by
doing the algebra and the predict.glm() function is working correctly
incorporating the factor levels on my machine.  Any thoughts???

Brian

Brian S. Cade, PhD

U. S. Geological Survey
Fort Collins Science Center
2150 Centre Ave., Bldg. C
Fort Collins, CO  80526-8818

email:  cadeb at usgs.gov <brian_cade at usgs.gov>
tel:  970 226-9326

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Dec  5 21:37:47 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 5 Dec 2017 12:37:47 -0800
Subject: [R] warnings about factor levels dropped from predict.glm
In-Reply-To: <CAM5M9BQj_U44G1F416ERsbm0if=xR=iT9KU8eZ9KSp1_DUpMxg@mail.gmail.com>
References: <CAM5M9BQj_U44G1F416ERsbm0if=xR=iT9KU8eZ9KSp1_DUpMxg@mail.gmail.com>
Message-ID: <CAGxFJbTtO-bfzHHPPwURqiYad8NveBjvdRx7z+undfdAr=F5qA@mail.gmail.com>

A guess (treat accordingly):

Different BLAS versions are in use on the two different machines/versions.
In one, near singularities are handled, and in the other they are not,
percolating up to warnings at the R level.

You can check this by seeing whether the estimated fit is the same on the 2
machines. If so, ignore the above.

-- Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Tue, Dec 5, 2017 at 12:17 PM, Cade, Brian <cadeb at usgs.gov> wrote:

> I am helping a student with some logistic regression analyses and we are
> getting some strange inconsistencies regarding a warning about factor
> levels being dropped when running predict.glm(, newdata = ournewdata) on
> the logistic regression model object.  We have checked multiple times that
> the factor levels have been defined similarly on both data sets (one used
> to estimate model and the newdata) and that values occur for all factor
> levels in both data sets.  When I run these commands on my version of R
> (3.2.5) on a Windows 7 OS I do not get the warnings.  When the student runs
> them on her version of R (not sure what number hers is) on her Mac, she
> gets these warnings constantly.  I've checked some records manually by
> doing the algebra and the predict.glm() function is working correctly
> incorporating the factor levels on my machine.  Any thoughts???
>
> Brian
>
> Brian S. Cade, PhD
>
> U. S. Geological Survey
> Fort Collins Science Center
> 2150 Centre Ave., Bldg. C
> Fort Collins, CO  80526-8818
>
> email:  cadeb at usgs.gov <brian_cade at usgs.gov>
> tel:  970 226-9326
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From marc_schwartz at me.com  Tue Dec  5 22:13:03 2017
From: marc_schwartz at me.com (Marc Schwartz)
Date: Tue, 05 Dec 2017 16:13:03 -0500
Subject: [R] warnings about factor levels dropped from predict.glm
In-Reply-To: <CAGxFJbTtO-bfzHHPPwURqiYad8NveBjvdRx7z+undfdAr=F5qA@mail.gmail.com>
References: <CAM5M9BQj_U44G1F416ERsbm0if=xR=iT9KU8eZ9KSp1_DUpMxg@mail.gmail.com>
 <CAGxFJbTtO-bfzHHPPwURqiYad8NveBjvdRx7z+undfdAr=F5qA@mail.gmail.com>
Message-ID: <D2569DEF-2DE2-4D8E-95C1-EBC9D9D2C0B9@me.com>

Hi,

I suspect that the warning may be coming from stats::model.frame.default(), with text along the lines of:

  "contrasts dropped from factor YOUR.FACTOR.NAME due to missing levels"

You might want to see if the student has a ~/.Rprofile file that has some modified default options regarding contrasts, etc.

Check to see if there is some change/difference in the structure of the data frames in use, specifically any contrast related attributes on the relevant data frame columns that are different on the two systems. See ?str.

Have them open an R session from the macOS terminal and run R using:

  R --vanilla

to see if you get the same errors on their system. If not, it suggests that perhaps their .Rprofile file has something non-default in it, and/or perhaps there is a .RData file in their working directory that has some saved workspace objects causing a conflict, as that file will be loaded by default with a new R session.

Regards,

Marc Schwartz
  

> On Dec 5, 2017, at 3:37 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> A guess (treat accordingly):
> 
> Different BLAS versions are in use on the two different machines/versions.
> In one, near singularities are handled, and in the other they are not,
> percolating up to warnings at the R level.
> 
> You can check this by seeing whether the estimated fit is the same on the 2
> machines. If so, ignore the above.
> 
> -- Bert
> 
> 
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> On Tue, Dec 5, 2017 at 12:17 PM, Cade, Brian <cadeb at usgs.gov> wrote:
> 
>> I am helping a student with some logistic regression analyses and we are
>> getting some strange inconsistencies regarding a warning about factor
>> levels being dropped when running predict.glm(, newdata = ournewdata) on
>> the logistic regression model object.  We have checked multiple times that
>> the factor levels have been defined similarly on both data sets (one used
>> to estimate model and the newdata) and that values occur for all factor
>> levels in both data sets.  When I run these commands on my version of R
>> (3.2.5) on a Windows 7 OS I do not get the warnings.  When the student runs
>> them on her version of R (not sure what number hers is) on her Mac, she
>> gets these warnings constantly.  I've checked some records manually by
>> doing the algebra and the predict.glm() function is working correctly
>> incorporating the factor levels on my machine.  Any thoughts???
>> 
>> Brian
>> 
>> Brian S. Cade, PhD
>> 
>> U. S. Geological Survey
>> Fort Collins Science Center
>> 2150 Centre Ave., Bldg. C
>> Fort Collins, CO  80526-8818
>> 
>> email:  cadeb at usgs.gov <brian_cade at usgs.gov>
>> tel:  970 226-9326


From paulbernal07 at gmail.com  Wed Dec  6 14:07:58 2017
From: paulbernal07 at gmail.com (Paul Bernal)
Date: Wed, 6 Dec 2017 08:07:58 -0500
Subject: [R] Odd dates generated in Forecasts
Message-ID: <CAMOcQfON+4vvLDCHAKsvnRhW1B1L7=evX3nmBdBr4DuB58qvYQ@mail.gmail.com>

Dear friends,

I have a weekly time series which starts on Jan 4th, 2003 and ends on
december 31st, 2016.

I set up my ts object as follows:

MyTseries <- ts(mydataset, start=2003, end=2016, frequency=52)

MyModel <- auto.arima(MyTseries, d=1, D=1)

MyModelForecast <- forecast (MyModel, h=12)

Since my last observation was on december 31st, 2016 I expected my forecast
date to start on 2017, but instead it returned odd dates.

This is my dataset

dput(dataset):

structure(list(Date = structure(c(8L, 22L, 36L, 50L, 64L, 78L,
92L, 106L, 120L, 134L, 148L, 162L, 176L, 190L, 204L, 218L, 232L,
246L, 260L, 274L, 288L, 302L, 316L, 330L, 344L, 358L, 372L, 386L,
400L, 414L, 428L, 442L, 456L, 470L, 484L, 498L, 512L, 526L, 540L,
554L, 568L, 582L, 596L, 610L, 624L, 638L, 652L, 666L, 680L, 694L,
708L, 722L, 5L, 19L, 33L, 47L, 61L, 75L, 89L, 103L, 117L, 130L,
144L, 158L, 172L, 186L, 200L, 214L, 228L, 242L, 256L, 270L, 284L,
298L, 312L, 326L, 340L, 354L, 368L, 382L, 396L, 410L, 424L, 438L,
452L, 466L, 480L, 494L, 508L, 522L, 536L, 550L, 564L, 578L, 592L,
606L, 620L, 634L, 648L, 662L, 676L, 690L, 704L, 718L, 1L, 15L,
29L, 43L, 57L, 71L, 85L, 99L, 113L, 127L, 141L, 155L, 169L, 183L,
197L, 211L, 225L, 239L, 253L, 267L, 281L, 295L, 309L, 323L, 337L,
351L, 365L, 379L, 393L, 407L, 421L, 435L, 449L, 463L, 477L, 491L,
505L, 519L, 533L, 547L, 561L, 575L, 589L, 603L, 617L, 631L, 645L,
659L, 673L, 687L, 701L, 715L, 729L, 13L, 27L, 41L, 55L, 69L,
83L, 97L, 111L, 126L, 140L, 154L, 168L, 182L, 196L, 210L, 224L,
238L, 252L, 266L, 280L, 294L, 308L, 322L, 336L, 350L, 364L, 378L,
392L, 406L, 420L, 434L, 448L, 462L, 476L, 490L, 504L, 518L, 532L,
546L, 560L, 574L, 588L, 602L, 616L, 630L, 644L, 658L, 672L, 686L,
700L, 714L, 728L, 12L, 26L, 40L, 54L, 68L, 82L, 96L, 110L, 124L,
138L, 152L, 166L, 180L, 194L, 208L, 222L, 236L, 250L, 264L, 278L,
292L, 306L, 320L, 334L, 348L, 362L, 376L, 390L, 404L, 418L, 432L,
446L, 460L, 474L, 488L, 502L, 516L, 530L, 544L, 558L, 572L, 586L,
600L, 614L, 628L, 642L, 656L, 670L, 684L, 698L, 712L, 726L, 10L,
24L, 38L, 52L, 66L, 80L, 94L, 108L, 121L, 135L, 149L, 163L, 177L,
191L, 205L, 219L, 233L, 247L, 261L, 275L, 289L, 303L, 317L, 331L,
345L, 359L, 373L, 387L, 401L, 415L, 429L, 443L, 457L, 471L, 485L,
499L, 513L, 527L, 541L, 555L, 569L, 583L, 597L, 611L, 625L, 639L,
653L, 667L, 681L, 695L, 709L, 723L, 6L, 20L, 34L, 48L, 62L, 76L,
90L, 104L, 118L, 132L, 146L, 160L, 174L, 188L, 202L, 216L, 230L,
244L, 258L, 272L, 286L, 300L, 314L, 328L, 342L, 356L, 370L, 384L,
398L, 412L, 426L, 440L, 454L, 468L, 482L, 496L, 510L, 524L, 538L,
552L, 566L, 580L, 594L, 608L, 622L, 636L, 650L, 664L, 678L, 692L,
706L, 720L, 3L, 17L, 31L, 45L, 59L, 73L, 87L, 101L, 115L, 131L,
145L, 159L, 173L, 187L, 201L, 215L, 229L, 243L, 257L, 271L, 285L,
299L, 313L, 327L, 341L, 355L, 369L, 383L, 397L, 411L, 425L, 439L,
453L, 467L, 481L, 495L, 509L, 523L, 537L, 551L, 565L, 579L, 593L,
607L, 621L, 635L, 649L, 663L, 677L, 691L, 705L, 719L, 2L, 16L,
30L, 44L, 58L, 72L, 86L, 100L, 114L, 128L, 142L, 156L, 170L,
184L, 198L, 212L, 226L, 240L, 254L, 268L, 282L, 296L, 310L, 324L,
338L, 352L, 366L, 380L, 394L, 408L, 422L, 436L, 450L, 464L, 478L,
492L, 506L, 520L, 534L, 548L, 562L, 576L, 590L, 604L, 618L, 632L,
646L, 660L, 674L, 688L, 702L, 716L, 730L, 14L, 28L, 42L, 56L,
70L, 84L, 98L, 112L, 125L, 139L, 153L, 167L, 181L, 195L, 209L,
223L, 237L, 251L, 265L, 279L, 293L, 307L, 321L, 335L, 349L, 363L,
377L, 391L, 405L, 419L, 433L, 447L, 461L, 475L, 489L, 503L, 517L,
531L, 545L, 559L, 573L, 587L, 601L, 615L, 629L, 643L, 657L, 671L,
685L, 699L, 713L, 727L, 11L, 25L, 39L, 53L, 67L, 81L, 95L, 109L,
123L, 137L, 151L, 165L, 179L, 193L, 207L, 221L, 235L, 249L, 263L,
277L, 291L, 305L, 319L, 333L, 347L, 361L, 375L, 389L, 403L, 417L,
431L, 445L, 459L, 473L, 487L, 501L, 515L, 529L, 543L, 557L, 571L,
585L, 599L, 613L, 627L, 641L, 655L, 669L, 683L, 697L, 711L, 725L,
9L, 23L, 37L, 51L, 65L, 79L, 93L, 107L, 122L, 136L, 150L, 164L,
178L, 192L, 206L, 220L, 234L, 248L, 262L, 276L, 290L, 304L, 318L,
332L, 346L, 360L, 374L, 388L, 402L, 416L, 430L, 444L, 458L, 472L,
486L, 500L, 514L, 528L, 542L, 556L, 570L, 584L, 598L, 612L, 626L,
640L, 654L, 668L, 682L, 696L, 710L, 724L, 7L, 21L, 35L, 49L,
63L, 77L, 91L, 105L, 119L, 133L, 147L, 161L, 175L, 189L, 203L,
217L, 231L, 245L, 259L, 273L, 287L, 301L, 315L, 329L, 343L, 357L,
371L, 385L, 399L, 413L, 427L, 441L, 455L, 469L, 483L, 497L, 511L,
525L, 539L, 553L, 567L, 581L, 595L, 609L, 623L, 637L, 651L, 665L,
679L, 693L, 707L, 721L, 4L, 18L, 32L, 46L, 60L, 74L, 88L, 102L,
116L, 129L, 143L, 157L, 171L, 185L, 199L, 213L, 227L, 241L, 255L,
269L, 283L, 297L, 311L, 325L, 339L, 353L, 367L, 381L, 395L, 409L,
423L, 437L, 451L, 465L, 479L, 493L, 507L, 521L, 535L, 549L, 563L,
577L, 591L, 605L, 619L, 633L, 647L, 661L, 675L, 689L, 703L, 717L,
731L), .Label = c("01/01/05", "01/01/11", "01/02/10", "01/02/16",
"01/03/04", "01/03/09", "01/03/15", "01/04/03", "01/04/14", "01/05/08",
"01/05/13", "01/06/07", "01/07/06", "01/07/12", "01/08/05", "01/08/11",
"01/09/10", "01/09/16", "01/10/04", "01/10/09", "01/10/15", "01/11/03",
"01/11/14", "01/12/08", "01/12/13", "01/13/07", "01/14/06", "01/14/12",
"01/15/05", "01/15/11", "01/16/10", "01/16/16", "01/17/04", "01/17/09",
"01/17/15", "01/18/03", "01/18/14", "01/19/08", "01/19/13", "01/20/07",
"01/21/06", "01/21/12", "01/22/05", "01/22/11", "01/23/10", "01/23/16",
"01/24/04", "01/24/09", "01/24/15", "01/25/03", "01/25/14", "01/26/08",
"01/26/13", "01/27/07", "01/28/06", "01/28/12", "01/29/05", "01/29/11",
"01/30/10", "01/30/16", "01/31/04", "01/31/09", "01/31/15", "02/01/03",
"02/01/14", "02/02/08", "02/02/13", "02/03/07", "02/04/06", "02/04/12",
"02/05/05", "02/05/11", "02/06/10", "02/06/16", "02/07/04", "02/07/09",
"02/07/15", "02/08/03", "02/08/14", "02/09/08", "02/09/13", "02/10/07",
"02/11/06", "02/11/12", "02/12/05", "02/12/11", "02/13/10", "02/13/16",
"02/14/04", "02/14/09", "02/14/15", "02/15/03", "02/15/14", "02/16/08",
"02/16/13", "02/17/07", "02/18/06", "02/18/12", "02/19/05", "02/19/11",
"02/20/10", "02/20/16", "02/21/04", "02/21/09", "02/21/15", "02/22/03",
"02/22/14", "02/23/08", "02/23/13", "02/24/07", "02/25/06", "02/25/12",
"02/26/05", "02/26/11", "02/27/10", "02/27/16", "02/28/04", "02/28/09",
"02/28/15", "03/01/03", "03/01/08", "03/01/14", "03/02/13", "03/03/07",
"03/03/12", "03/04/06", "03/05/05", "03/05/11", "03/05/16", "03/06/04",
"03/06/10", "03/07/09", "03/07/15", "03/08/03", "03/08/08", "03/08/14",
"03/09/13", "03/10/07", "03/10/12", "03/11/06", "03/12/05", "03/12/11",
"03/12/16", "03/13/04", "03/13/10", "03/14/09", "03/14/15", "03/15/03",
"03/15/08", "03/15/14", "03/16/13", "03/17/07", "03/17/12", "03/18/06",
"03/19/05", "03/19/11", "03/19/16", "03/20/04", "03/20/10", "03/21/09",
"03/21/15", "03/22/03", "03/22/08", "03/22/14", "03/23/13", "03/24/07",
"03/24/12", "03/25/06", "03/26/05", "03/26/11", "03/26/16", "03/27/04",
"03/27/10", "03/28/09", "03/28/15", "03/29/03", "03/29/08", "03/29/14",
"03/30/13", "03/31/07", "03/31/12", "04/01/06", "04/02/05", "04/02/11",
"04/02/16", "04/03/04", "04/03/10", "04/04/09", "04/04/15", "04/05/03",
"04/05/08", "04/05/14", "04/06/13", "04/07/07", "04/07/12", "04/08/06",
"04/09/05", "04/09/11", "04/09/16", "04/10/04", "04/10/10", "04/11/09",
"04/11/15", "04/12/03", "04/12/08", "04/12/14", "04/13/13", "04/14/07",
"04/14/12", "04/15/06", "04/16/05", "04/16/11", "04/16/16", "04/17/04",
"04/17/10", "04/18/09", "04/18/15", "04/19/03", "04/19/08", "04/19/14",
"04/20/13", "04/21/07", "04/21/12", "04/22/06", "04/23/05", "04/23/11",
"04/23/16", "04/24/04", "04/24/10", "04/25/09", "04/25/15", "04/26/03",
"04/26/08", "04/26/14", "04/27/13", "04/28/07", "04/28/12", "04/29/06",
"04/30/05", "04/30/11", "04/30/16", "05/01/04", "05/01/10", "05/02/09",
"05/02/15", "05/03/03", "05/03/08", "05/03/14", "05/04/13", "05/05/07",
"05/05/12", "05/06/06", "05/07/05", "05/07/11", "05/07/16", "05/08/04",
"05/08/10", "05/09/09", "05/09/15", "05/10/03", "05/10/08", "05/10/14",
"05/11/13", "05/12/07", "05/12/12", "05/13/06", "05/14/05", "05/14/11",
"05/14/16", "05/15/04", "05/15/10", "05/16/09", "05/16/15", "05/17/03",
"05/17/08", "05/17/14", "05/18/13", "05/19/07", "05/19/12", "05/20/06",
"05/21/05", "05/21/11", "05/21/16", "05/22/04", "05/22/10", "05/23/09",
"05/23/15", "05/24/03", "05/24/08", "05/24/14", "05/25/13", "05/26/07",
"05/26/12", "05/27/06", "05/28/05", "05/28/11", "05/28/16", "05/29/04",
"05/29/10", "05/30/09", "05/30/15", "05/31/03", "05/31/08", "05/31/14",
"06/01/13", "06/02/07", "06/02/12", "06/03/06", "06/04/05", "06/04/11",
"06/04/16", "06/05/04", "06/05/10", "06/06/09", "06/06/15", "06/07/03",
"06/07/08", "06/07/14", "06/08/13", "06/09/07", "06/09/12", "06/10/06",
"06/11/05", "06/11/11", "06/11/16", "06/12/04", "06/12/10", "06/13/09",
"06/13/15", "06/14/03", "06/14/08", "06/14/14", "06/15/13", "06/16/07",
"06/16/12", "06/17/06", "06/18/05", "06/18/11", "06/18/16", "06/19/04",
"06/19/10", "06/20/09", "06/20/15", "06/21/03", "06/21/08", "06/21/14",
"06/22/13", "06/23/07", "06/23/12", "06/24/06", "06/25/05", "06/25/11",
"06/25/16", "06/26/04", "06/26/10", "06/27/09", "06/27/15", "06/28/03",
"06/28/08", "06/28/14", "06/29/13", "06/30/07", "06/30/12", "07/01/06",
"07/02/05", "07/02/11", "07/02/16", "07/03/04", "07/03/10", "07/04/09",
"07/04/15", "07/05/03", "07/05/08", "07/05/14", "07/06/13", "07/07/07",
"07/07/12", "07/08/06", "07/09/05", "07/09/11", "07/09/16", "07/10/04",
"07/10/10", "07/11/09", "07/11/15", "07/12/03", "07/12/08", "07/12/14",
"07/13/13", "07/14/07", "07/14/12", "07/15/06", "07/16/05", "07/16/11",
"07/16/16", "07/17/04", "07/17/10", "07/18/09", "07/18/15", "07/19/03",
"07/19/08", "07/19/14", "07/20/13", "07/21/07", "07/21/12", "07/22/06",
"07/23/05", "07/23/11", "07/23/16", "07/24/04", "07/24/10", "07/25/09",
"07/25/15", "07/26/03", "07/26/08", "07/26/14", "07/27/13", "07/28/07",
"07/28/12", "07/29/06", "07/30/05", "07/30/11", "07/30/16", "07/31/04",
"07/31/10", "08/01/09", "08/01/15", "08/02/03", "08/02/08", "08/02/14",
"08/03/13", "08/04/07", "08/04/12", "08/05/06", "08/06/05", "08/06/11",
"08/06/16", "08/07/04", "08/07/10", "08/08/09", "08/08/15", "08/09/03",
"08/09/08", "08/09/14", "08/10/13", "08/11/07", "08/11/12", "08/12/06",
"08/13/05", "08/13/11", "08/13/16", "08/14/04", "08/14/10", "08/15/09",
"08/15/15", "08/16/03", "08/16/08", "08/16/14", "08/17/13", "08/18/07",
"08/18/12", "08/19/06", "08/20/05", "08/20/11", "08/20/16", "08/21/04",
"08/21/10", "08/22/09", "08/22/15", "08/23/03", "08/23/08", "08/23/14",
"08/24/13", "08/25/07", "08/25/12", "08/26/06", "08/27/05", "08/27/11",
"08/27/16", "08/28/04", "08/28/10", "08/29/09", "08/29/15", "08/30/03",
"08/30/08", "08/30/14", "08/31/13", "09/01/07", "09/01/12", "09/02/06",
"09/03/05", "09/03/11", "09/03/16", "09/04/04", "09/04/10", "09/05/09",
"09/05/15", "09/06/03", "09/06/08", "09/06/14", "09/07/13", "09/08/07",
"09/08/12", "09/09/06", "09/10/05", "09/10/11", "09/10/16", "09/11/04",
"09/11/10", "09/12/09", "09/12/15", "09/13/03", "09/13/08", "09/13/14",
"09/14/13", "09/15/07", "09/15/12", "09/16/06", "09/17/05", "09/17/11",
"09/17/16", "09/18/04", "09/18/10", "09/19/09", "09/19/15", "09/20/03",
"09/20/08", "09/20/14", "09/21/13", "09/22/07", "09/22/12", "09/23/06",
"09/24/05", "09/24/11", "09/24/16", "09/25/04", "09/25/10", "09/26/09",
"09/26/15", "09/27/03", "09/27/08", "09/27/14", "09/28/13", "09/29/07",
"09/29/12", "09/30/06", "10/01/05", "10/01/11", "10/01/16", "10/02/04",
"10/02/10", "10/03/09", "10/03/15", "10/04/03", "10/04/08", "10/04/14",
"10/05/13", "10/06/07", "10/06/12", "10/07/06", "10/08/05", "10/08/11",
"10/08/16", "10/09/04", "10/09/10", "10/10/09", "10/10/15", "10/11/03",
"10/11/08", "10/11/14", "10/12/13", "10/13/07", "10/13/12", "10/14/06",
"10/15/05", "10/15/11", "10/15/16", "10/16/04", "10/16/10", "10/17/09",
"10/17/15", "10/18/03", "10/18/08", "10/18/14", "10/19/13", "10/20/07",
"10/20/12", "10/21/06", "10/22/05", "10/22/11", "10/22/16", "10/23/04",
"10/23/10", "10/24/09", "10/24/15", "10/25/03", "10/25/08", "10/25/14",
"10/26/13", "10/27/07", "10/27/12", "10/28/06", "10/29/05", "10/29/11",
"10/29/16", "10/30/04", "10/30/10", "10/31/09", "10/31/15", "11/01/03",
"11/01/08", "11/01/14", "11/02/13", "11/03/07", "11/03/12", "11/04/06",
"11/05/05", "11/05/11", "11/05/16", "11/06/04", "11/06/10", "11/07/09",
"11/07/15", "11/08/03", "11/08/08", "11/08/14", "11/09/13", "11/10/07",
"11/10/12", "11/11/06", "11/12/05", "11/12/11", "11/12/16", "11/13/04",
"11/13/10", "11/14/09", "11/14/15", "11/15/03", "11/15/08", "11/15/14",
"11/16/13", "11/17/07", "11/17/12", "11/18/06", "11/19/05", "11/19/11",
"11/19/16", "11/20/04", "11/20/10", "11/21/09", "11/21/15", "11/22/03",
"11/22/08", "11/22/14", "11/23/13", "11/24/07", "11/24/12", "11/25/06",
"11/26/05", "11/26/11", "11/26/16", "11/27/04", "11/27/10", "11/28/09",
"11/28/15", "11/29/03", "11/29/08", "11/29/14", "11/30/13", "12/01/07",
"12/01/12", "12/02/06", "12/03/05", "12/03/11", "12/03/16", "12/04/04",
"12/04/10", "12/05/09", "12/05/15", "12/06/03", "12/06/08", "12/06/14",
"12/07/13", "12/08/07", "12/08/12", "12/09/06", "12/10/05", "12/10/11",
"12/10/16", "12/11/04", "12/11/10", "12/12/09", "12/12/15", "12/13/03",
"12/13/08", "12/13/14", "12/14/13", "12/15/07", "12/15/12", "12/16/06",
"12/17/05", "12/17/11", "12/17/16", "12/18/04", "12/18/10", "12/19/09",
"12/19/15", "12/20/03", "12/20/08", "12/20/14", "12/21/13", "12/22/07",
"12/22/12", "12/23/06", "12/24/05", "12/24/11", "12/24/16", "12/25/04",
"12/25/10", "12/26/09", "12/26/15", "12/27/03", "12/27/08", "12/27/14",
"12/28/13", "12/29/07", "12/29/12", "12/30/06", "12/31/05", "12/31/11",
"12/31/16"), class = "factor"), Transits = c(11L, 17L, 14L, 18L,
12L, 14L, 21L, 14L, 16L, 10L, 11L, 10L, 6L, 8L, 12L, 11L, 8L,
6L, 7L, 9L, 6L, 5L, 3L, 12L, 10L, 5L, 6L, 5L, 9L, 14L, 6L, 6L,
5L, 6L, 7L, 6L, 5L, 6L, 4L, 14L, 10L, 7L, 9L, 8L, 9L, 12L, 12L,
13L, 9L, 15L, 19L, 9L, 8L, 7L, 8L, 16L, 19L, 14L, 15L, 7L, 12L,
16L, 15L, 10L, 17L, 7L, 5L, 6L, 10L, 4L, 5L, 5L, 9L, 5L, 2L,
6L, 8L, 5L, 4L, 8L, 7L, 9L, 6L, 7L, 8L, 5L, 9L, 7L, 5L, 13L,
11L, 15L, 9L, 11L, 17L, 17L, 10L, 14L, 20L, 18L, 8L, 17L, 7L,
12L, 9L, 10L, 6L, 7L, 11L, 9L, 7L, 10L, 7L, 10L, 13L, 12L, 10L,
8L, 11L, 10L, 9L, 10L, 8L, 9L, 6L, 9L, 7L, 6L, 6L, 11L, 10L,
5L, 4L, 7L, 14L, 11L, 8L, 10L, 7L, 5L, 4L, 8L, 6L, 15L, 11L,
14L, 16L, 18L, 20L, 14L, 18L, 14L, 13L, 16L, 17L, 9L, 11L, 16L,
13L, 12L, 11L, 16L, 16L, 17L, 21L, 13L, 19L, 16L, 14L, 13L, 11L,
8L, 8L, 19L, 9L, 11L, 10L, 17L, 12L, 10L, 11L, 13L, 12L, 11L,
8L, 12L, 19L, 14L, 9L, 9L, 16L, 20L, 13L, 12L, 14L, 13L, 12L,
18L, 13L, 12L, 19L, 16L, 16L, 17L, 16L, 20L, 20L, 13L, 14L, 13L,
7L, 16L, 15L, 17L, 16L, 15L, 17L, 18L, 17L, 16L, 14L, 12L, 9L,
9L, 6L, 14L, 8L, 17L, 9L, 12L, 11L, 8L, 11L, 10L, 13L, 7L, 4L,
10L, 12L, 11L, 8L, 8L, 13L, 6L, 9L, 6L, 18L, 11L, 13L, 10L, 17L,
14L, 10L, 14L, 11L, 15L, 19L, 17L, 15L, 18L, 14L, 12L, 20L, 12L,
12L, 16L, 16L, 17L, 11L, 10L, 11L, 13L, 16L, 17L, 9L, 11L, 13L,
7L, 7L, 7L, 10L, 9L, 5L, 12L, 8L, 8L, 10L, 10L, 12L, 9L, 8L,
9L, 12L, 10L, 9L, 6L, 9L, 2L, 9L, 8L, 10L, 12L, 13L, 15L, 12L,
13L, 12L, 17L, 18L, 14L, 21L, 15L, 19L, 15L, 11L, 18L, 20L, 18L,
19L, 19L, 18L, 20L, 16L, 15L, 11L, 20L, 12L, 13L, 12L, 10L, 11L,
12L, 14L, 9L, 7L, 11L, 12L, 14L, 11L, 11L, 9L, 20L, 9L, 11L,
9L, 10L, 9L, 15L, 8L, 8L, 13L, 10L, 15L, 11L, 14L, 16L, 25L,
20L, 23L, 31L, 29L, 21L, 20L, 16L, 25L, 20L, 16L, 20L, 21L, 19L,
16L, 22L, 17L, 18L, 20L, 16L, 15L, 14L, 15L, 10L, 7L, 15L, 10L,
5L, 14L, 7L, 16L, 11L, 12L, 8L, 8L, 8L, 9L, 9L, 9L, 14L, 5L,
12L, 9L, 8L, 11L, 11L, 7L, 11L, 16L, 18L, 17L, 31L, 21L, 30L,
21L, 29L, 26L, 22L, 14L, 26L, 19L, 20L, 10L, 15L, 22L, 15L, 21L,
13L, 19L, 19L, 22L, 19L, 15L, 16L, 18L, 15L, 13L, 13L, 8L, 6L,
9L, 9L, 11L, 10L, 7L, 7L, 8L, 11L, 7L, 7L, 10L, 6L, 8L, 9L, 5L,
10L, 6L, 8L, 12L, 6L, 17L, 12L, 16L, 13L, 14L, 18L, 25L, 19L,
19L, 24L, 14L, 18L, 20L, 18L, 16L, 17L, 12L, 21L, 20L, 18L, 15L,
12L, 13L, 11L, 9L, 10L, 11L, 8L, 12L, 12L, 4L, 5L, 5L, 11L, 9L,
10L, 7L, 3L, 13L, 6L, 10L, 9L, 9L, 10L, 10L, 8L, 10L, 6L, 12L,
5L, 12L, 13L, 19L, 10L, 14L, 20L, 19L, 20L, 16L, 22L, 16L, 24L,
16L, 20L, 20L, 13L, 9L, 11L, 8L, 13L, 16L, 10L, 13L, 11L, 15L,
6L, 6L, 5L, 7L, 3L, 7L, 4L, 7L, 6L, 5L, 4L, 7L, 10L, 5L, 7L,
1L, 6L, 10L, 5L, 8L, 8L, 10L, 6L, 6L, 11L, 8L, 7L, 10L, 12L,
11L, 14L, 17L, 20L, 29L, 21L, 24L, 25L, 32L, 26L, 29L, 18L, 22L,
19L, 16L, 17L, 25L, 16L, 24L, 21L, 16L, 25L, 16L, 23L, 23L, 16L,
14L, 18L, 12L, 17L, 16L, 9L, 12L, 5L, 11L, 9L, 8L, 13L, 10L,
6L, 6L, 10L, 14L, 12L, 7L, 13L, 10L, 14L, 13L, 4L, 16L, 19L,
16L, 15L, 30L, 26L, 29L, 31L, 22L, 26L, 35L, 35L, 30L, 21L, 25L,
31L, 20L, 21L, 23L, 27L, 21L, 24L, 16L, 28L, 11L, 19L, 16L, 13L,
14L, 13L, 19L, 15L, 15L, 22L, 14L, 24L, 16L, 16L, 12L, 12L, 14L,
14L, 12L, 16L, 8L, 10L, 15L, 16L, 11L, 16L, 9L, 15L, 12L, 15L,
6L, 15L, 19L, 19L, 20L, 19L, 21L, 20L, 23L, 21L, 15L, 15L, 24L,
13L, 9L, 11L, 10L, 11L, 13L, 15L, 9L, 17L, 22L, 16L, 16L, 13L,
15L, 15L, 13L, 11L, 11L, 15L, 10L, 12L, 14L, 6L, 11L, 9L, 12L,
4L, 12L, 7L, 9L, 7L, 15L, 13L, 12L, 8L, 5L, 16L, 13L, 13L, 15L,
16L, 16L, 13L, 21L, 15L, 18L, 22L, 12L, 16L, 25L, 14L, 17L, 12L,
10L)), .Names = c("Date", "Transits"), class = "data.frame", row.names =
c(NA,
-731L))


This is the result I get from the forecast:

dput(Forecasts):

structure(list(Point.Forecast = c(19.4253367238618, 12.2907914272766,
12.9624076300088, 18.0807466065309, 15.9843908802223, 16.5321579531827,
16.7184964071333, 10.1201838392972, 20.7203714449961, 7.45002607776652,
14.9836817432811, 13.3820472411712), Lo.80 = c(13.9434152149432,
6.74027066336727, 7.25612824519892, 12.2923074967212, 10.0690364921587,
10.501665758874, 10.5705243712663, 3.85957289110304, 14.3484486381226,
0.968987377892339, 8.39518758108775, 6.6878669395083), Hi.80 =
c(24.9072582327803,
17.841312191186, 18.6686870148187, 23.8691857163406, 21.8997452682858,
22.5626501474914, 22.8664684430002, 16.3807947874913, 27.0922942518695,
13.9310647776407, 21.5721759054744, 20.0762275428341), Lo.95 =
c(11.0414612619408,
3.80200245835122, 4.23540640692652, 9.2280929272317, 6.93763703269479,
7.30931602651216, 7.31598456740248, 0.545405648875148, 10.975356457297,
-2.4618672675205, 4.90744944999539, 3.14418194567588), Hi.95 =
c(27.8092121857828,
20.779580396202, 21.6894088530911, 26.9334002858301, 25.0311447277497,
25.7549998798532, 26.121008246864, 19.6949620297192, 30.4653864326951,
17.3619194230535, 25.0599140365668, 23.6199125366665)), .Names =
c("Point.Forecast",
"Lo.80", "Hi.80", "Lo.95", "Hi.95"), row.names = c("2016.019",
"2016.038", "2016.058", "2016.077", "2016.096", "2016.115", "2016.135",
"2016.154", "2016.173", "2016.192", "2016.212", "2016.231"), class =
"data.frame")

Thank you beforehand for your valuable support,

Best regards,

Paul

	[[alternative HTML version deleted]]


From serpalma.v at gmail.com  Wed Dec  6 15:17:14 2017
From: serpalma.v at gmail.com (Sergio PV)
Date: Wed, 6 Dec 2017 15:17:14 +0100
Subject: [R] Coeficients estimation in a repeated measures linear model
Message-ID: <CAGriyTN9Lq-g48S5Oe9R7vE7pr75ZtBC7bp-FR0sBgt+4ru1Hg@mail.gmail.com>

Dear Users,

I am trying to understand the inner workings of a repeated measures linear
model. Take for example a situation with 6 individuals sampled twice for
two conditions (control and treated).

set.seed(12)
ctrl <- rnorm(n = 6, mean = 2)
ttd <- rnorm(n = 6, mean = 10)
dat <- data.frame(vals = c(ctrl, ttd),
                  group = c(rep("ctrl", 6), rep("ttd", 6)),
                  ind = factor(rep(1:6, 2)))

fit <- lm(vals ~ ind + group, data = dat)
model.matrix(~ ind + group, data = dat)

I am puzzled on how the coeficients are calculated. For example, according
to the model matrix, I thought the intercept would be individual 1 control.
But that is clearly not the case.
For the last coeficient, I understand it as the mean of all differences
between treated vs control at each individual.

I would greatly appreciate if someone could clarify to me how the
coefficients in this situation are estimated.

Thanks

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Dec  6 18:03:03 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 6 Dec 2017 09:03:03 -0800
Subject: [R] Odd dates generated in Forecasts
In-Reply-To: <CAMOcQfON+4vvLDCHAKsvnRhW1B1L7=evX3nmBdBr4DuB58qvYQ@mail.gmail.com>
References: <CAMOcQfON+4vvLDCHAKsvnRhW1B1L7=evX3nmBdBr4DuB58qvYQ@mail.gmail.com>
Message-ID: <F178806B-58EC-4A8C-BAC6-0D063CAB77F0@comcast.net>


> On Dec 6, 2017, at 5:07 AM, Paul Bernal <paulbernal07 at gmail.com> wrote:
> 
> Dear friends,
> 
> I have a weekly time series which starts on Jan 4th, 2003 and ends on
> december 31st, 2016.
> 
> I set up my ts object as follows:
> 
> MyTseries <- ts(mydataset, start=2003, end=2016, frequency=52)
> 
> MyModel <- auto.arima(MyTseries, d=1, D=1)
> 
> MyModelForecast <- forecast (MyModel, h=12)
> 
> Since my last observation was on december 31st, 2016 I expected my forecast
> date to start on 2017, but instead it returned odd dates.
> 
> This is my dataset
> 
> dput(dataset):
> 
> structure(list(Date = structure(c(8L, 22L, 36L, 50L, 64L, 78L,
> 92L, 106L, 120L, 134L, 148L, 162L, 176L, 190L, 204L, 218L, 232L,
> 246L, 260L, 274L, 288L, 302L, 316L, 330L, 344L, 358L, 372L, 386L,
> 400L, 414L, 428L, 442L, 456L, 470L, 484L, 498L, 512L, 526L, 540L,
> 554L, 568L, 582L, 596L, 610L, 624L, 638L, 652L, 666L, 680L, 694L,
> 708L, 722L, 5L, 19L, 33L, 47L, 61L, 75L, 89L, 103L, 117L, 130L,
> 144L, 158L, 172L, 186L, 200L, 214L, 228L, 242L, 256L, 270L, 284L,
> 298L, 312L, 326L, 340L, 354L, 368L, 382L, 396L, 410L, 424L, 438L,
> 452L, 466L, 480L, 494L, 508L, 522L, 536L, 550L, 564L, 578L, 592L,
> 606L, 620L, 634L, 648L, 662L, 676L, 690L, 704L, 718L, 1L, 15L,
> 29L, 43L, 57L, 71L, 85L, 99L, 113L, 127L, 141L, 155L, 169L, 183L,
> 197L, 211L, 225L, 239L, 253L, 267L, 281L, 295L, 309L, 323L, 337L,
> 351L, 365L, 379L, 393L, 407L, 421L, 435L, 449L, 463L, 477L, 491L,
> 505L, 519L, 533L, 547L, 561L, 575L, 589L, 603L, 617L, 631L, 645L,
> 659L, 673L, 687L, 701L, 715L, 729L, 13L, 27L, 41L, 55L, 69L,
> 83L, 97L, 111L, 126L, 140L, 154L, 168L, 182L, 196L, 210L, 224L,
> 238L, 252L, 266L, 280L, 294L, 308L, 322L, 336L, 350L, 364L, 378L,
> 392L, 406L, 420L, 434L, 448L, 462L, 476L, 490L, 504L, 518L, 532L,
> 546L, 560L, 574L, 588L, 602L, 616L, 630L, 644L, 658L, 672L, 686L,
> 700L, 714L, 728L, 12L, 26L, 40L, 54L, 68L, 82L, 96L, 110L, 124L,
> 138L, 152L, 166L, 180L, 194L, 208L, 222L, 236L, 250L, 264L, 278L,
> 292L, 306L, 320L, 334L, 348L, 362L, 376L, 390L, 404L, 418L, 432L,
> 446L, 460L, 474L, 488L, 502L, 516L, 530L, 544L, 558L, 572L, 586L,
> 600L, 614L, 628L, 642L, 656L, 670L, 684L, 698L, 712L, 726L, 10L,
> 24L, 38L, 52L, 66L, 80L, 94L, 108L, 121L, 135L, 149L, 163L, 177L,
> 191L, 205L, 219L, 233L, 247L, 261L, 275L, 289L, 303L, 317L, 331L,
> 345L, 359L, 373L, 387L, 401L, 415L, 429L, 443L, 457L, 471L, 485L,
> 499L, 513L, 527L, 541L, 555L, 569L, 583L, 597L, 611L, 625L, 639L,
> 653L, 667L, 681L, 695L, 709L, 723L, 6L, 20L, 34L, 48L, 62L, 76L,
> 90L, 104L, 118L, 132L, 146L, 160L, 174L, 188L, 202L, 216L, 230L,
> 244L, 258L, 272L, 286L, 300L, 314L, 328L, 342L, 356L, 370L, 384L,
> 398L, 412L, 426L, 440L, 454L, 468L, 482L, 496L, 510L, 524L, 538L,
> 552L, 566L, 580L, 594L, 608L, 622L, 636L, 650L, 664L, 678L, 692L,
> 706L, 720L, 3L, 17L, 31L, 45L, 59L, 73L, 87L, 101L, 115L, 131L,
> 145L, 159L, 173L, 187L, 201L, 215L, 229L, 243L, 257L, 271L, 285L,
> 299L, 313L, 327L, 341L, 355L, 369L, 383L, 397L, 411L, 425L, 439L,
> 453L, 467L, 481L, 495L, 509L, 523L, 537L, 551L, 565L, 579L, 593L,
> 607L, 621L, 635L, 649L, 663L, 677L, 691L, 705L, 719L, 2L, 16L,
> 30L, 44L, 58L, 72L, 86L, 100L, 114L, 128L, 142L, 156L, 170L,
> 184L, 198L, 212L, 226L, 240L, 254L, 268L, 282L, 296L, 310L, 324L,
> 338L, 352L, 366L, 380L, 394L, 408L, 422L, 436L, 450L, 464L, 478L,
> 492L, 506L, 520L, 534L, 548L, 562L, 576L, 590L, 604L, 618L, 632L,
> 646L, 660L, 674L, 688L, 702L, 716L, 730L, 14L, 28L, 42L, 56L,
> 70L, 84L, 98L, 112L, 125L, 139L, 153L, 167L, 181L, 195L, 209L,
> 223L, 237L, 251L, 265L, 279L, 293L, 307L, 321L, 335L, 349L, 363L,
> 377L, 391L, 405L, 419L, 433L, 447L, 461L, 475L, 489L, 503L, 517L,
> 531L, 545L, 559L, 573L, 587L, 601L, 615L, 629L, 643L, 657L, 671L,
> 685L, 699L, 713L, 727L, 11L, 25L, 39L, 53L, 67L, 81L, 95L, 109L,
> 123L, 137L, 151L, 165L, 179L, 193L, 207L, 221L, 235L, 249L, 263L,
> 277L, 291L, 305L, 319L, 333L, 347L, 361L, 375L, 389L, 403L, 417L,
> 431L, 445L, 459L, 473L, 487L, 501L, 515L, 529L, 543L, 557L, 571L,
> 585L, 599L, 613L, 627L, 641L, 655L, 669L, 683L, 697L, 711L, 725L,
> 9L, 23L, 37L, 51L, 65L, 79L, 93L, 107L, 122L, 136L, 150L, 164L,
> 178L, 192L, 206L, 220L, 234L, 248L, 262L, 276L, 290L, 304L, 318L,
> 332L, 346L, 360L, 374L, 388L, 402L, 416L, 430L, 444L, 458L, 472L,
> 486L, 500L, 514L, 528L, 542L, 556L, 570L, 584L, 598L, 612L, 626L,
> 640L, 654L, 668L, 682L, 696L, 710L, 724L, 7L, 21L, 35L, 49L,
> 63L, 77L, 91L, 105L, 119L, 133L, 147L, 161L, 175L, 189L, 203L,
> 217L, 231L, 245L, 259L, 273L, 287L, 301L, 315L, 329L, 343L, 357L,
> 371L, 385L, 399L, 413L, 427L, 441L, 455L, 469L, 483L, 497L, 511L,
> 525L, 539L, 553L, 567L, 581L, 595L, 609L, 623L, 637L, 651L, 665L,
> 679L, 693L, 707L, 721L, 4L, 18L, 32L, 46L, 60L, 74L, 88L, 102L,
> 116L, 129L, 143L, 157L, 171L, 185L, 199L, 213L, 227L, 241L, 255L,
> 269L, 283L, 297L, 311L, 325L, 339L, 353L, 367L, 381L, 395L, 409L,
> 423L, 437L, 451L, 465L, 479L, 493L, 507L, 521L, 535L, 549L, 563L,
> 577L, 591L, 605L, 619L, 633L, 647L, 661L, 675L, 689L, 703L, 717L,
> 731L), .Label = c("01/01/05", "01/01/11", "01/02/10", "01/02/16",
> "01/03/04", "01/03/09", "01/03/15", "01/04/03", "01/04/14", "01/05/08",
> "01/05/13", "01/06/07", "01/07/06", "01/07/12", "01/08/05", "01/08/11",
> "01/09/10", "01/09/16", "01/10/04", "01/10/09", "01/10/15", "01/11/03",
> "01/11/14", "01/12/08", "01/12/13", "01/13/07", "01/14/06", "01/14/12",
> "01/15/05", "01/15/11", "01/16/10", "01/16/16", "01/17/04", "01/17/09",
> "01/17/15", "01/18/03", "01/18/14", "01/19/08", "01/19/13", "01/20/07",
> "01/21/06", "01/21/12", "01/22/05", "01/22/11", "01/23/10", "01/23/16",
> "01/24/04", "01/24/09", "01/24/15", "01/25/03", "01/25/14", "01/26/08",
> "01/26/13", "01/27/07", "01/28/06", "01/28/12", "01/29/05", "01/29/11",
> "01/30/10", "01/30/16", "01/31/04", "01/31/09", "01/31/15", "02/01/03",
> "02/01/14", "02/02/08", "02/02/13", "02/03/07", "02/04/06", "02/04/12",
> "02/05/05", "02/05/11", "02/06/10", "02/06/16", "02/07/04", "02/07/09",
> "02/07/15", "02/08/03", "02/08/14", "02/09/08", "02/09/13", "02/10/07",
> "02/11/06", "02/11/12", "02/12/05", "02/12/11", "02/13/10", "02/13/16",
> "02/14/04", "02/14/09", "02/14/15", "02/15/03", "02/15/14", "02/16/08",
> "02/16/13", "02/17/07", "02/18/06", "02/18/12", "02/19/05", "02/19/11",
> "02/20/10", "02/20/16", "02/21/04", "02/21/09", "02/21/15", "02/22/03",
> "02/22/14", "02/23/08", "02/23/13", "02/24/07", "02/25/06", "02/25/12",
> "02/26/05", "02/26/11", "02/27/10", "02/27/16", "02/28/04", "02/28/09",
> "02/28/15", "03/01/03", "03/01/08", "03/01/14", "03/02/13", "03/03/07",
> "03/03/12", "03/04/06", "03/05/05", "03/05/11", "03/05/16", "03/06/04",
> "03/06/10", "03/07/09", "03/07/15", "03/08/03", "03/08/08", "03/08/14",
> "03/09/13", "03/10/07", "03/10/12", "03/11/06", "03/12/05", "03/12/11",
> "03/12/16", "03/13/04", "03/13/10", "03/14/09", "03/14/15", "03/15/03",
> "03/15/08", "03/15/14", "03/16/13", "03/17/07", "03/17/12", "03/18/06",
> "03/19/05", "03/19/11", "03/19/16", "03/20/04", "03/20/10", "03/21/09",
> "03/21/15", "03/22/03", "03/22/08", "03/22/14", "03/23/13", "03/24/07",
> "03/24/12", "03/25/06", "03/26/05", "03/26/11", "03/26/16", "03/27/04",
> "03/27/10", "03/28/09", "03/28/15", "03/29/03", "03/29/08", "03/29/14",
> "03/30/13", "03/31/07", "03/31/12", "04/01/06", "04/02/05", "04/02/11",
> "04/02/16", "04/03/04", "04/03/10", "04/04/09", "04/04/15", "04/05/03",
> "04/05/08", "04/05/14", "04/06/13", "04/07/07", "04/07/12", "04/08/06",
> "04/09/05", "04/09/11", "04/09/16", "04/10/04", "04/10/10", "04/11/09",
> "04/11/15", "04/12/03", "04/12/08", "04/12/14", "04/13/13", "04/14/07",
> "04/14/12", "04/15/06", "04/16/05", "04/16/11", "04/16/16", "04/17/04",
> "04/17/10", "04/18/09", "04/18/15", "04/19/03", "04/19/08", "04/19/14",
> "04/20/13", "04/21/07", "04/21/12", "04/22/06", "04/23/05", "04/23/11",
> "04/23/16", "04/24/04", "04/24/10", "04/25/09", "04/25/15", "04/26/03",
> "04/26/08", "04/26/14", "04/27/13", "04/28/07", "04/28/12", "04/29/06",
> "04/30/05", "04/30/11", "04/30/16", "05/01/04", "05/01/10", "05/02/09",
> "05/02/15", "05/03/03", "05/03/08", "05/03/14", "05/04/13", "05/05/07",
> "05/05/12", "05/06/06", "05/07/05", "05/07/11", "05/07/16", "05/08/04",
> "05/08/10", "05/09/09", "05/09/15", "05/10/03", "05/10/08", "05/10/14",
> "05/11/13", "05/12/07", "05/12/12", "05/13/06", "05/14/05", "05/14/11",
> "05/14/16", "05/15/04", "05/15/10", "05/16/09", "05/16/15", "05/17/03",
> "05/17/08", "05/17/14", "05/18/13", "05/19/07", "05/19/12", "05/20/06",
> "05/21/05", "05/21/11", "05/21/16", "05/22/04", "05/22/10", "05/23/09",
> "05/23/15", "05/24/03", "05/24/08", "05/24/14", "05/25/13", "05/26/07",
> "05/26/12", "05/27/06", "05/28/05", "05/28/11", "05/28/16", "05/29/04",
> "05/29/10", "05/30/09", "05/30/15", "05/31/03", "05/31/08", "05/31/14",
> "06/01/13", "06/02/07", "06/02/12", "06/03/06", "06/04/05", "06/04/11",
> "06/04/16", "06/05/04", "06/05/10", "06/06/09", "06/06/15", "06/07/03",
> "06/07/08", "06/07/14", "06/08/13", "06/09/07", "06/09/12", "06/10/06",
> "06/11/05", "06/11/11", "06/11/16", "06/12/04", "06/12/10", "06/13/09",
> "06/13/15", "06/14/03", "06/14/08", "06/14/14", "06/15/13", "06/16/07",
> "06/16/12", "06/17/06", "06/18/05", "06/18/11", "06/18/16", "06/19/04",
> "06/19/10", "06/20/09", "06/20/15", "06/21/03", "06/21/08", "06/21/14",
> "06/22/13", "06/23/07", "06/23/12", "06/24/06", "06/25/05", "06/25/11",
> "06/25/16", "06/26/04", "06/26/10", "06/27/09", "06/27/15", "06/28/03",
> "06/28/08", "06/28/14", "06/29/13", "06/30/07", "06/30/12", "07/01/06",
> "07/02/05", "07/02/11", "07/02/16", "07/03/04", "07/03/10", "07/04/09",
> "07/04/15", "07/05/03", "07/05/08", "07/05/14", "07/06/13", "07/07/07",
> "07/07/12", "07/08/06", "07/09/05", "07/09/11", "07/09/16", "07/10/04",
> "07/10/10", "07/11/09", "07/11/15", "07/12/03", "07/12/08", "07/12/14",
> "07/13/13", "07/14/07", "07/14/12", "07/15/06", "07/16/05", "07/16/11",
> "07/16/16", "07/17/04", "07/17/10", "07/18/09", "07/18/15", "07/19/03",
> "07/19/08", "07/19/14", "07/20/13", "07/21/07", "07/21/12", "07/22/06",
> "07/23/05", "07/23/11", "07/23/16", "07/24/04", "07/24/10", "07/25/09",
> "07/25/15", "07/26/03", "07/26/08", "07/26/14", "07/27/13", "07/28/07",
> "07/28/12", "07/29/06", "07/30/05", "07/30/11", "07/30/16", "07/31/04",
> "07/31/10", "08/01/09", "08/01/15", "08/02/03", "08/02/08", "08/02/14",
> "08/03/13", "08/04/07", "08/04/12", "08/05/06", "08/06/05", "08/06/11",
> "08/06/16", "08/07/04", "08/07/10", "08/08/09", "08/08/15", "08/09/03",
> "08/09/08", "08/09/14", "08/10/13", "08/11/07", "08/11/12", "08/12/06",
> "08/13/05", "08/13/11", "08/13/16", "08/14/04", "08/14/10", "08/15/09",
> "08/15/15", "08/16/03", "08/16/08", "08/16/14", "08/17/13", "08/18/07",
> "08/18/12", "08/19/06", "08/20/05", "08/20/11", "08/20/16", "08/21/04",
> "08/21/10", "08/22/09", "08/22/15", "08/23/03", "08/23/08", "08/23/14",
> "08/24/13", "08/25/07", "08/25/12", "08/26/06", "08/27/05", "08/27/11",
> "08/27/16", "08/28/04", "08/28/10", "08/29/09", "08/29/15", "08/30/03",
> "08/30/08", "08/30/14", "08/31/13", "09/01/07", "09/01/12", "09/02/06",
> "09/03/05", "09/03/11", "09/03/16", "09/04/04", "09/04/10", "09/05/09",
> "09/05/15", "09/06/03", "09/06/08", "09/06/14", "09/07/13", "09/08/07",
> "09/08/12", "09/09/06", "09/10/05", "09/10/11", "09/10/16", "09/11/04",
> "09/11/10", "09/12/09", "09/12/15", "09/13/03", "09/13/08", "09/13/14",
> "09/14/13", "09/15/07", "09/15/12", "09/16/06", "09/17/05", "09/17/11",
> "09/17/16", "09/18/04", "09/18/10", "09/19/09", "09/19/15", "09/20/03",
> "09/20/08", "09/20/14", "09/21/13", "09/22/07", "09/22/12", "09/23/06",
> "09/24/05", "09/24/11", "09/24/16", "09/25/04", "09/25/10", "09/26/09",
> "09/26/15", "09/27/03", "09/27/08", "09/27/14", "09/28/13", "09/29/07",
> "09/29/12", "09/30/06", "10/01/05", "10/01/11", "10/01/16", "10/02/04",
> "10/02/10", "10/03/09", "10/03/15", "10/04/03", "10/04/08", "10/04/14",
> "10/05/13", "10/06/07", "10/06/12", "10/07/06", "10/08/05", "10/08/11",
> "10/08/16", "10/09/04", "10/09/10", "10/10/09", "10/10/15", "10/11/03",
> "10/11/08", "10/11/14", "10/12/13", "10/13/07", "10/13/12", "10/14/06",
> "10/15/05", "10/15/11", "10/15/16", "10/16/04", "10/16/10", "10/17/09",
> "10/17/15", "10/18/03", "10/18/08", "10/18/14", "10/19/13", "10/20/07",
> "10/20/12", "10/21/06", "10/22/05", "10/22/11", "10/22/16", "10/23/04",
> "10/23/10", "10/24/09", "10/24/15", "10/25/03", "10/25/08", "10/25/14",
> "10/26/13", "10/27/07", "10/27/12", "10/28/06", "10/29/05", "10/29/11",
> "10/29/16", "10/30/04", "10/30/10", "10/31/09", "10/31/15", "11/01/03",
> "11/01/08", "11/01/14", "11/02/13", "11/03/07", "11/03/12", "11/04/06",
> "11/05/05", "11/05/11", "11/05/16", "11/06/04", "11/06/10", "11/07/09",
> "11/07/15", "11/08/03", "11/08/08", "11/08/14", "11/09/13", "11/10/07",
> "11/10/12", "11/11/06", "11/12/05", "11/12/11", "11/12/16", "11/13/04",
> "11/13/10", "11/14/09", "11/14/15", "11/15/03", "11/15/08", "11/15/14",
> "11/16/13", "11/17/07", "11/17/12", "11/18/06", "11/19/05", "11/19/11",
> "11/19/16", "11/20/04", "11/20/10", "11/21/09", "11/21/15", "11/22/03",
> "11/22/08", "11/22/14", "11/23/13", "11/24/07", "11/24/12", "11/25/06",
> "11/26/05", "11/26/11", "11/26/16", "11/27/04", "11/27/10", "11/28/09",
> "11/28/15", "11/29/03", "11/29/08", "11/29/14", "11/30/13", "12/01/07",
> "12/01/12", "12/02/06", "12/03/05", "12/03/11", "12/03/16", "12/04/04",
> "12/04/10", "12/05/09", "12/05/15", "12/06/03", "12/06/08", "12/06/14",
> "12/07/13", "12/08/07", "12/08/12", "12/09/06", "12/10/05", "12/10/11",
> "12/10/16", "12/11/04", "12/11/10", "12/12/09", "12/12/15", "12/13/03",
> "12/13/08", "12/13/14", "12/14/13", "12/15/07", "12/15/12", "12/16/06",
> "12/17/05", "12/17/11", "12/17/16", "12/18/04", "12/18/10", "12/19/09",
> "12/19/15", "12/20/03", "12/20/08", "12/20/14", "12/21/13", "12/22/07",
> "12/22/12", "12/23/06", "12/24/05", "12/24/11", "12/24/16", "12/25/04",
> "12/25/10", "12/26/09", "12/26/15", "12/27/03", "12/27/08", "12/27/14",
> "12/28/13", "12/29/07", "12/29/12", "12/30/06", "12/31/05", "12/31/11",
> "12/31/16"), class = "factor"),

Those are not R dates. Unless you convert to dates then they will have alpha sort order which is probably NOT what you expected.

Perhaps:

dat$fixed_dates <- as.Date( as.character(dat$Date), format="%m/%d/%y")



-- 
David
> Transits = c(11L, 17L, 14L, 18L,
> 12L, 14L, 21L, 14L, 16L, 10L, 11L, 10L, 6L, 8L, 12L, 11L, 8L,
> 6L, 7L, 9L, 6L, 5L, 3L, 12L, 10L, 5L, 6L, 5L, 9L, 14L, 6L, 6L,
> 5L, 6L, 7L, 6L, 5L, 6L, 4L, 14L, 10L, 7L, 9L, 8L, 9L, 12L, 12L,
> 13L, 9L, 15L, 19L, 9L, 8L, 7L, 8L, 16L, 19L, 14L, 15L, 7L, 12L,
> 16L, 15L, 10L, 17L, 7L, 5L, 6L, 10L, 4L, 5L, 5L, 9L, 5L, 2L,
> 6L, 8L, 5L, 4L, 8L, 7L, 9L, 6L, 7L, 8L, 5L, 9L, 7L, 5L, 13L,
> 11L, 15L, 9L, 11L, 17L, 17L, 10L, 14L, 20L, 18L, 8L, 17L, 7L,
> 12L, 9L, 10L, 6L, 7L, 11L, 9L, 7L, 10L, 7L, 10L, 13L, 12L, 10L,
> 8L, 11L, 10L, 9L, 10L, 8L, 9L, 6L, 9L, 7L, 6L, 6L, 11L, 10L,
> 5L, 4L, 7L, 14L, 11L, 8L, 10L, 7L, 5L, 4L, 8L, 6L, 15L, 11L,
> 14L, 16L, 18L, 20L, 14L, 18L, 14L, 13L, 16L, 17L, 9L, 11L, 16L,
> 13L, 12L, 11L, 16L, 16L, 17L, 21L, 13L, 19L, 16L, 14L, 13L, 11L,
> 8L, 8L, 19L, 9L, 11L, 10L, 17L, 12L, 10L, 11L, 13L, 12L, 11L,
> 8L, 12L, 19L, 14L, 9L, 9L, 16L, 20L, 13L, 12L, 14L, 13L, 12L,
> 18L, 13L, 12L, 19L, 16L, 16L, 17L, 16L, 20L, 20L, 13L, 14L, 13L,
> 7L, 16L, 15L, 17L, 16L, 15L, 17L, 18L, 17L, 16L, 14L, 12L, 9L,
> 9L, 6L, 14L, 8L, 17L, 9L, 12L, 11L, 8L, 11L, 10L, 13L, 7L, 4L,
> 10L, 12L, 11L, 8L, 8L, 13L, 6L, 9L, 6L, 18L, 11L, 13L, 10L, 17L,
> 14L, 10L, 14L, 11L, 15L, 19L, 17L, 15L, 18L, 14L, 12L, 20L, 12L,
> 12L, 16L, 16L, 17L, 11L, 10L, 11L, 13L, 16L, 17L, 9L, 11L, 13L,
> 7L, 7L, 7L, 10L, 9L, 5L, 12L, 8L, 8L, 10L, 10L, 12L, 9L, 8L,
> 9L, 12L, 10L, 9L, 6L, 9L, 2L, 9L, 8L, 10L, 12L, 13L, 15L, 12L,
> 13L, 12L, 17L, 18L, 14L, 21L, 15L, 19L, 15L, 11L, 18L, 20L, 18L,
> 19L, 19L, 18L, 20L, 16L, 15L, 11L, 20L, 12L, 13L, 12L, 10L, 11L,
> 12L, 14L, 9L, 7L, 11L, 12L, 14L, 11L, 11L, 9L, 20L, 9L, 11L,
> 9L, 10L, 9L, 15L, 8L, 8L, 13L, 10L, 15L, 11L, 14L, 16L, 25L,
> 20L, 23L, 31L, 29L, 21L, 20L, 16L, 25L, 20L, 16L, 20L, 21L, 19L,
> 16L, 22L, 17L, 18L, 20L, 16L, 15L, 14L, 15L, 10L, 7L, 15L, 10L,
> 5L, 14L, 7L, 16L, 11L, 12L, 8L, 8L, 8L, 9L, 9L, 9L, 14L, 5L,
> 12L, 9L, 8L, 11L, 11L, 7L, 11L, 16L, 18L, 17L, 31L, 21L, 30L,
> 21L, 29L, 26L, 22L, 14L, 26L, 19L, 20L, 10L, 15L, 22L, 15L, 21L,
> 13L, 19L, 19L, 22L, 19L, 15L, 16L, 18L, 15L, 13L, 13L, 8L, 6L,
> 9L, 9L, 11L, 10L, 7L, 7L, 8L, 11L, 7L, 7L, 10L, 6L, 8L, 9L, 5L,
> 10L, 6L, 8L, 12L, 6L, 17L, 12L, 16L, 13L, 14L, 18L, 25L, 19L,
> 19L, 24L, 14L, 18L, 20L, 18L, 16L, 17L, 12L, 21L, 20L, 18L, 15L,
> 12L, 13L, 11L, 9L, 10L, 11L, 8L, 12L, 12L, 4L, 5L, 5L, 11L, 9L,
> 10L, 7L, 3L, 13L, 6L, 10L, 9L, 9L, 10L, 10L, 8L, 10L, 6L, 12L,
> 5L, 12L, 13L, 19L, 10L, 14L, 20L, 19L, 20L, 16L, 22L, 16L, 24L,
> 16L, 20L, 20L, 13L, 9L, 11L, 8L, 13L, 16L, 10L, 13L, 11L, 15L,
> 6L, 6L, 5L, 7L, 3L, 7L, 4L, 7L, 6L, 5L, 4L, 7L, 10L, 5L, 7L,
> 1L, 6L, 10L, 5L, 8L, 8L, 10L, 6L, 6L, 11L, 8L, 7L, 10L, 12L,
> 11L, 14L, 17L, 20L, 29L, 21L, 24L, 25L, 32L, 26L, 29L, 18L, 22L,
> 19L, 16L, 17L, 25L, 16L, 24L, 21L, 16L, 25L, 16L, 23L, 23L, 16L,
> 14L, 18L, 12L, 17L, 16L, 9L, 12L, 5L, 11L, 9L, 8L, 13L, 10L,
> 6L, 6L, 10L, 14L, 12L, 7L, 13L, 10L, 14L, 13L, 4L, 16L, 19L,
> 16L, 15L, 30L, 26L, 29L, 31L, 22L, 26L, 35L, 35L, 30L, 21L, 25L,
> 31L, 20L, 21L, 23L, 27L, 21L, 24L, 16L, 28L, 11L, 19L, 16L, 13L,
> 14L, 13L, 19L, 15L, 15L, 22L, 14L, 24L, 16L, 16L, 12L, 12L, 14L,
> 14L, 12L, 16L, 8L, 10L, 15L, 16L, 11L, 16L, 9L, 15L, 12L, 15L,
> 6L, 15L, 19L, 19L, 20L, 19L, 21L, 20L, 23L, 21L, 15L, 15L, 24L,
> 13L, 9L, 11L, 10L, 11L, 13L, 15L, 9L, 17L, 22L, 16L, 16L, 13L,
> 15L, 15L, 13L, 11L, 11L, 15L, 10L, 12L, 14L, 6L, 11L, 9L, 12L,
> 4L, 12L, 7L, 9L, 7L, 15L, 13L, 12L, 8L, 5L, 16L, 13L, 13L, 15L,
> 16L, 16L, 13L, 21L, 15L, 18L, 22L, 12L, 16L, 25L, 14L, 17L, 12L,
> 10L)), .Names = c("Date", "Transits"), class = "data.frame", row.names =
> c(NA,
> -731L))
> 
> 
> This is the result I get from the forecast:
> 
> dput(Forecasts):
> 
> structure(list(Point.Forecast = c(19.4253367238618, 12.2907914272766,
> 12.9624076300088, 18.0807466065309, 15.9843908802223, 16.5321579531827,
> 16.7184964071333, 10.1201838392972, 20.7203714449961, 7.45002607776652,
> 14.9836817432811, 13.3820472411712), Lo.80 = c(13.9434152149432,
> 6.74027066336727, 7.25612824519892, 12.2923074967212, 10.0690364921587,
> 10.501665758874, 10.5705243712663, 3.85957289110304, 14.3484486381226,
> 0.968987377892339, 8.39518758108775, 6.6878669395083), Hi.80 =
> c(24.9072582327803,
> 17.841312191186, 18.6686870148187, 23.8691857163406, 21.8997452682858,
> 22.5626501474914, 22.8664684430002, 16.3807947874913, 27.0922942518695,
> 13.9310647776407, 21.5721759054744, 20.0762275428341), Lo.95 =
> c(11.0414612619408,
> 3.80200245835122, 4.23540640692652, 9.2280929272317, 6.93763703269479,
> 7.30931602651216, 7.31598456740248, 0.545405648875148, 10.975356457297,
> -2.4618672675205, 4.90744944999539, 3.14418194567588), Hi.95 =
> c(27.8092121857828,
> 20.779580396202, 21.6894088530911, 26.9334002858301, 25.0311447277497,
> 25.7549998798532, 26.121008246864, 19.6949620297192, 30.4653864326951,
> 17.3619194230535, 25.0599140365668, 23.6199125366665)), .Names =
> c("Point.Forecast",
> "Lo.80", "Hi.80", "Lo.95", "Hi.95"), row.names = c("2016.019",
> "2016.038", "2016.058", "2016.077", "2016.096", "2016.115", "2016.135",
> "2016.154", "2016.173", "2016.192", "2016.212", "2016.231"), class =
> "data.frame")
> 
> Thank you beforehand for your valuable support,
> 
> Best regards,
> 
> Paul
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From bgunter.4567 at gmail.com  Wed Dec  6 17:41:29 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 6 Dec 2017 08:41:29 -0800
Subject: [R] Coeficients estimation in a repeated measures linear model
In-Reply-To: <CAGriyTN9Lq-g48S5Oe9R7vE7pr75ZtBC7bp-FR0sBgt+4ru1Hg@mail.gmail.com>
References: <CAGriyTN9Lq-g48S5Oe9R7vE7pr75ZtBC7bp-FR0sBgt+4ru1Hg@mail.gmail.com>
Message-ID: <CAGxFJbTLo5cJ1TM9F_m7+arBffRWnKwRUc2wMSYjSQXmGzN_3g@mail.gmail.com>

Sergio:

1. You do not have a "repeated measures linear model" .

2. This list is not designed to replace your own efforts to learn the
necessary R background, in this case, factor coding and contrasts in linear
models. I would suggest you spend some time with any of the many fine R
linear model tutorials that can be found on the web. Here is one place to
look for suggestions: https://www.rstudio.com/online-learning/#R  . But
just googling around you'll probably find something that may suit even
better.

3. This list is primarily for R programming help, not statistics help
(although they do sometimes intersect). For the latter, try a statistics
site like stats.stackexchange.com  .

4. Finally, as always, consulting with a local statistical resource, if
available, is always worth considering.

HTH.

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Wed, Dec 6, 2017 at 6:17 AM, Sergio PV <serpalma.v at gmail.com> wrote:

> Dear Users,
>
> I am trying to understand the inner workings of a repeated measures linear
> model. Take for example a situation with 6 individuals sampled twice for
> two conditions (control and treated).
>
> set.seed(12)
> ctrl <- rnorm(n = 6, mean = 2)
> ttd <- rnorm(n = 6, mean = 10)
> dat <- data.frame(vals = c(ctrl, ttd),
>                   group = c(rep("ctrl", 6), rep("ttd", 6)),
>                   ind = factor(rep(1:6, 2)))
>
> fit <- lm(vals ~ ind + group, data = dat)
> model.matrix(~ ind + group, data = dat)
>
> I am puzzled on how the coeficients are calculated. For example, according
> to the model matrix, I thought the intercept would be individual 1 control.
> But that is clearly not the case.
> For the last coeficient, I understand it as the mean of all differences
> between treated vs control at each individual.
>
> I would greatly appreciate if someone could clarify to me how the
> coefficients in this situation are estimated.
>
> Thanks
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From paulbernal07 at gmail.com  Wed Dec  6 20:09:17 2017
From: paulbernal07 at gmail.com (Paul Bernal)
Date: Wed, 6 Dec 2017 14:09:17 -0500
Subject: [R] Odd dates generated in Forecasts
In-Reply-To: <F178806B-58EC-4A8C-BAC6-0D063CAB77F0@comcast.net>
References: <CAMOcQfON+4vvLDCHAKsvnRhW1B1L7=evX3nmBdBr4DuB58qvYQ@mail.gmail.com>
 <F178806B-58EC-4A8C-BAC6-0D063CAB77F0@comcast.net>
Message-ID: <CAMOcQfPvy=Gwz=gdWZMXjipW4-u8ihEKjQNr7733JZMju5Q01A@mail.gmail.com>

Thank you very much David. As a matter of fact, I solved it by doing the
following:

MyTimeSeriesObj <- ts(MyData, freq=365.25/7,
start=decimal_date(mdy("01-04-2003")))

After doing that adjustment, my forecasts dates started from 2017 on.

Cheers,

Paul

2017-12-06 12:03 GMT-05:00 David Winsemius <dwinsemius at comcast.net>:

>
> > On Dec 6, 2017, at 5:07 AM, Paul Bernal <paulbernal07 at gmail.com> wrote:
> >
> > Dear friends,
> >
> > I have a weekly time series which starts on Jan 4th, 2003 and ends on
> > december 31st, 2016.
> >
> > I set up my ts object as follows:
> >
> > MyTseries <- ts(mydataset, start=2003, end=2016, frequency=52)
> >
> > MyModel <- auto.arima(MyTseries, d=1, D=1)
> >
> > MyModelForecast <- forecast (MyModel, h=12)
> >
> > Since my last observation was on december 31st, 2016 I expected my
> forecast
> > date to start on 2017, but instead it returned odd dates.
> >
> > This is my dataset
> >
> > dput(dataset):
> >
> > structure(list(Date = structure(c(8L, 22L, 36L, 50L, 64L, 78L,
> > 92L, 106L, 120L, 134L, 148L, 162L, 176L, 190L, 204L, 218L, 232L,
> > 246L, 260L, 274L, 288L, 302L, 316L, 330L, 344L, 358L, 372L, 386L,
> > 400L, 414L, 428L, 442L, 456L, 470L, 484L, 498L, 512L, 526L, 540L,
> > 554L, 568L, 582L, 596L, 610L, 624L, 638L, 652L, 666L, 680L, 694L,
> > 708L, 722L, 5L, 19L, 33L, 47L, 61L, 75L, 89L, 103L, 117L, 130L,
> > 144L, 158L, 172L, 186L, 200L, 214L, 228L, 242L, 256L, 270L, 284L,
> > 298L, 312L, 326L, 340L, 354L, 368L, 382L, 396L, 410L, 424L, 438L,
> > 452L, 466L, 480L, 494L, 508L, 522L, 536L, 550L, 564L, 578L, 592L,
> > 606L, 620L, 634L, 648L, 662L, 676L, 690L, 704L, 718L, 1L, 15L,
> > 29L, 43L, 57L, 71L, 85L, 99L, 113L, 127L, 141L, 155L, 169L, 183L,
> > 197L, 211L, 225L, 239L, 253L, 267L, 281L, 295L, 309L, 323L, 337L,
> > 351L, 365L, 379L, 393L, 407L, 421L, 435L, 449L, 463L, 477L, 491L,
> > 505L, 519L, 533L, 547L, 561L, 575L, 589L, 603L, 617L, 631L, 645L,
> > 659L, 673L, 687L, 701L, 715L, 729L, 13L, 27L, 41L, 55L, 69L,
> > 83L, 97L, 111L, 126L, 140L, 154L, 168L, 182L, 196L, 210L, 224L,
> > 238L, 252L, 266L, 280L, 294L, 308L, 322L, 336L, 350L, 364L, 378L,
> > 392L, 406L, 420L, 434L, 448L, 462L, 476L, 490L, 504L, 518L, 532L,
> > 546L, 560L, 574L, 588L, 602L, 616L, 630L, 644L, 658L, 672L, 686L,
> > 700L, 714L, 728L, 12L, 26L, 40L, 54L, 68L, 82L, 96L, 110L, 124L,
> > 138L, 152L, 166L, 180L, 194L, 208L, 222L, 236L, 250L, 264L, 278L,
> > 292L, 306L, 320L, 334L, 348L, 362L, 376L, 390L, 404L, 418L, 432L,
> > 446L, 460L, 474L, 488L, 502L, 516L, 530L, 544L, 558L, 572L, 586L,
> > 600L, 614L, 628L, 642L, 656L, 670L, 684L, 698L, 712L, 726L, 10L,
> > 24L, 38L, 52L, 66L, 80L, 94L, 108L, 121L, 135L, 149L, 163L, 177L,
> > 191L, 205L, 219L, 233L, 247L, 261L, 275L, 289L, 303L, 317L, 331L,
> > 345L, 359L, 373L, 387L, 401L, 415L, 429L, 443L, 457L, 471L, 485L,
> > 499L, 513L, 527L, 541L, 555L, 569L, 583L, 597L, 611L, 625L, 639L,
> > 653L, 667L, 681L, 695L, 709L, 723L, 6L, 20L, 34L, 48L, 62L, 76L,
> > 90L, 104L, 118L, 132L, 146L, 160L, 174L, 188L, 202L, 216L, 230L,
> > 244L, 258L, 272L, 286L, 300L, 314L, 328L, 342L, 356L, 370L, 384L,
> > 398L, 412L, 426L, 440L, 454L, 468L, 482L, 496L, 510L, 524L, 538L,
> > 552L, 566L, 580L, 594L, 608L, 622L, 636L, 650L, 664L, 678L, 692L,
> > 706L, 720L, 3L, 17L, 31L, 45L, 59L, 73L, 87L, 101L, 115L, 131L,
> > 145L, 159L, 173L, 187L, 201L, 215L, 229L, 243L, 257L, 271L, 285L,
> > 299L, 313L, 327L, 341L, 355L, 369L, 383L, 397L, 411L, 425L, 439L,
> > 453L, 467L, 481L, 495L, 509L, 523L, 537L, 551L, 565L, 579L, 593L,
> > 607L, 621L, 635L, 649L, 663L, 677L, 691L, 705L, 719L, 2L, 16L,
> > 30L, 44L, 58L, 72L, 86L, 100L, 114L, 128L, 142L, 156L, 170L,
> > 184L, 198L, 212L, 226L, 240L, 254L, 268L, 282L, 296L, 310L, 324L,
> > 338L, 352L, 366L, 380L, 394L, 408L, 422L, 436L, 450L, 464L, 478L,
> > 492L, 506L, 520L, 534L, 548L, 562L, 576L, 590L, 604L, 618L, 632L,
> > 646L, 660L, 674L, 688L, 702L, 716L, 730L, 14L, 28L, 42L, 56L,
> > 70L, 84L, 98L, 112L, 125L, 139L, 153L, 167L, 181L, 195L, 209L,
> > 223L, 237L, 251L, 265L, 279L, 293L, 307L, 321L, 335L, 349L, 363L,
> > 377L, 391L, 405L, 419L, 433L, 447L, 461L, 475L, 489L, 503L, 517L,
> > 531L, 545L, 559L, 573L, 587L, 601L, 615L, 629L, 643L, 657L, 671L,
> > 685L, 699L, 713L, 727L, 11L, 25L, 39L, 53L, 67L, 81L, 95L, 109L,
> > 123L, 137L, 151L, 165L, 179L, 193L, 207L, 221L, 235L, 249L, 263L,
> > 277L, 291L, 305L, 319L, 333L, 347L, 361L, 375L, 389L, 403L, 417L,
> > 431L, 445L, 459L, 473L, 487L, 501L, 515L, 529L, 543L, 557L, 571L,
> > 585L, 599L, 613L, 627L, 641L, 655L, 669L, 683L, 697L, 711L, 725L,
> > 9L, 23L, 37L, 51L, 65L, 79L, 93L, 107L, 122L, 136L, 150L, 164L,
> > 178L, 192L, 206L, 220L, 234L, 248L, 262L, 276L, 290L, 304L, 318L,
> > 332L, 346L, 360L, 374L, 388L, 402L, 416L, 430L, 444L, 458L, 472L,
> > 486L, 500L, 514L, 528L, 542L, 556L, 570L, 584L, 598L, 612L, 626L,
> > 640L, 654L, 668L, 682L, 696L, 710L, 724L, 7L, 21L, 35L, 49L,
> > 63L, 77L, 91L, 105L, 119L, 133L, 147L, 161L, 175L, 189L, 203L,
> > 217L, 231L, 245L, 259L, 273L, 287L, 301L, 315L, 329L, 343L, 357L,
> > 371L, 385L, 399L, 413L, 427L, 441L, 455L, 469L, 483L, 497L, 511L,
> > 525L, 539L, 553L, 567L, 581L, 595L, 609L, 623L, 637L, 651L, 665L,
> > 679L, 693L, 707L, 721L, 4L, 18L, 32L, 46L, 60L, 74L, 88L, 102L,
> > 116L, 129L, 143L, 157L, 171L, 185L, 199L, 213L, 227L, 241L, 255L,
> > 269L, 283L, 297L, 311L, 325L, 339L, 353L, 367L, 381L, 395L, 409L,
> > 423L, 437L, 451L, 465L, 479L, 493L, 507L, 521L, 535L, 549L, 563L,
> > 577L, 591L, 605L, 619L, 633L, 647L, 661L, 675L, 689L, 703L, 717L,
> > 731L), .Label = c("01/01/05", "01/01/11", "01/02/10", "01/02/16",
> > "01/03/04", "01/03/09", "01/03/15", "01/04/03", "01/04/14", "01/05/08",
> > "01/05/13", "01/06/07", "01/07/06", "01/07/12", "01/08/05", "01/08/11",
> > "01/09/10", "01/09/16", "01/10/04", "01/10/09", "01/10/15", "01/11/03",
> > "01/11/14", "01/12/08", "01/12/13", "01/13/07", "01/14/06", "01/14/12",
> > "01/15/05", "01/15/11", "01/16/10", "01/16/16", "01/17/04", "01/17/09",
> > "01/17/15", "01/18/03", "01/18/14", "01/19/08", "01/19/13", "01/20/07",
> > "01/21/06", "01/21/12", "01/22/05", "01/22/11", "01/23/10", "01/23/16",
> > "01/24/04", "01/24/09", "01/24/15", "01/25/03", "01/25/14", "01/26/08",
> > "01/26/13", "01/27/07", "01/28/06", "01/28/12", "01/29/05", "01/29/11",
> > "01/30/10", "01/30/16", "01/31/04", "01/31/09", "01/31/15", "02/01/03",
> > "02/01/14", "02/02/08", "02/02/13", "02/03/07", "02/04/06", "02/04/12",
> > "02/05/05", "02/05/11", "02/06/10", "02/06/16", "02/07/04", "02/07/09",
> > "02/07/15", "02/08/03", "02/08/14", "02/09/08", "02/09/13", "02/10/07",
> > "02/11/06", "02/11/12", "02/12/05", "02/12/11", "02/13/10", "02/13/16",
> > "02/14/04", "02/14/09", "02/14/15", "02/15/03", "02/15/14", "02/16/08",
> > "02/16/13", "02/17/07", "02/18/06", "02/18/12", "02/19/05", "02/19/11",
> > "02/20/10", "02/20/16", "02/21/04", "02/21/09", "02/21/15", "02/22/03",
> > "02/22/14", "02/23/08", "02/23/13", "02/24/07", "02/25/06", "02/25/12",
> > "02/26/05", "02/26/11", "02/27/10", "02/27/16", "02/28/04", "02/28/09",
> > "02/28/15", "03/01/03", "03/01/08", "03/01/14", "03/02/13", "03/03/07",
> > "03/03/12", "03/04/06", "03/05/05", "03/05/11", "03/05/16", "03/06/04",
> > "03/06/10", "03/07/09", "03/07/15", "03/08/03", "03/08/08", "03/08/14",
> > "03/09/13", "03/10/07", "03/10/12", "03/11/06", "03/12/05", "03/12/11",
> > "03/12/16", "03/13/04", "03/13/10", "03/14/09", "03/14/15", "03/15/03",
> > "03/15/08", "03/15/14", "03/16/13", "03/17/07", "03/17/12", "03/18/06",
> > "03/19/05", "03/19/11", "03/19/16", "03/20/04", "03/20/10", "03/21/09",
> > "03/21/15", "03/22/03", "03/22/08", "03/22/14", "03/23/13", "03/24/07",
> > "03/24/12", "03/25/06", "03/26/05", "03/26/11", "03/26/16", "03/27/04",
> > "03/27/10", "03/28/09", "03/28/15", "03/29/03", "03/29/08", "03/29/14",
> > "03/30/13", "03/31/07", "03/31/12", "04/01/06", "04/02/05", "04/02/11",
> > "04/02/16", "04/03/04", "04/03/10", "04/04/09", "04/04/15", "04/05/03",
> > "04/05/08", "04/05/14", "04/06/13", "04/07/07", "04/07/12", "04/08/06",
> > "04/09/05", "04/09/11", "04/09/16", "04/10/04", "04/10/10", "04/11/09",
> > "04/11/15", "04/12/03", "04/12/08", "04/12/14", "04/13/13", "04/14/07",
> > "04/14/12", "04/15/06", "04/16/05", "04/16/11", "04/16/16", "04/17/04",
> > "04/17/10", "04/18/09", "04/18/15", "04/19/03", "04/19/08", "04/19/14",
> > "04/20/13", "04/21/07", "04/21/12", "04/22/06", "04/23/05", "04/23/11",
> > "04/23/16", "04/24/04", "04/24/10", "04/25/09", "04/25/15", "04/26/03",
> > "04/26/08", "04/26/14", "04/27/13", "04/28/07", "04/28/12", "04/29/06",
> > "04/30/05", "04/30/11", "04/30/16", "05/01/04", "05/01/10", "05/02/09",
> > "05/02/15", "05/03/03", "05/03/08", "05/03/14", "05/04/13", "05/05/07",
> > "05/05/12", "05/06/06", "05/07/05", "05/07/11", "05/07/16", "05/08/04",
> > "05/08/10", "05/09/09", "05/09/15", "05/10/03", "05/10/08", "05/10/14",
> > "05/11/13", "05/12/07", "05/12/12", "05/13/06", "05/14/05", "05/14/11",
> > "05/14/16", "05/15/04", "05/15/10", "05/16/09", "05/16/15", "05/17/03",
> > "05/17/08", "05/17/14", "05/18/13", "05/19/07", "05/19/12", "05/20/06",
> > "05/21/05", "05/21/11", "05/21/16", "05/22/04", "05/22/10", "05/23/09",
> > "05/23/15", "05/24/03", "05/24/08", "05/24/14", "05/25/13", "05/26/07",
> > "05/26/12", "05/27/06", "05/28/05", "05/28/11", "05/28/16", "05/29/04",
> > "05/29/10", "05/30/09", "05/30/15", "05/31/03", "05/31/08", "05/31/14",
> > "06/01/13", "06/02/07", "06/02/12", "06/03/06", "06/04/05", "06/04/11",
> > "06/04/16", "06/05/04", "06/05/10", "06/06/09", "06/06/15", "06/07/03",
> > "06/07/08", "06/07/14", "06/08/13", "06/09/07", "06/09/12", "06/10/06",
> > "06/11/05", "06/11/11", "06/11/16", "06/12/04", "06/12/10", "06/13/09",
> > "06/13/15", "06/14/03", "06/14/08", "06/14/14", "06/15/13", "06/16/07",
> > "06/16/12", "06/17/06", "06/18/05", "06/18/11", "06/18/16", "06/19/04",
> > "06/19/10", "06/20/09", "06/20/15", "06/21/03", "06/21/08", "06/21/14",
> > "06/22/13", "06/23/07", "06/23/12", "06/24/06", "06/25/05", "06/25/11",
> > "06/25/16", "06/26/04", "06/26/10", "06/27/09", "06/27/15", "06/28/03",
> > "06/28/08", "06/28/14", "06/29/13", "06/30/07", "06/30/12", "07/01/06",
> > "07/02/05", "07/02/11", "07/02/16", "07/03/04", "07/03/10", "07/04/09",
> > "07/04/15", "07/05/03", "07/05/08", "07/05/14", "07/06/13", "07/07/07",
> > "07/07/12", "07/08/06", "07/09/05", "07/09/11", "07/09/16", "07/10/04",
> > "07/10/10", "07/11/09", "07/11/15", "07/12/03", "07/12/08", "07/12/14",
> > "07/13/13", "07/14/07", "07/14/12", "07/15/06", "07/16/05", "07/16/11",
> > "07/16/16", "07/17/04", "07/17/10", "07/18/09", "07/18/15", "07/19/03",
> > "07/19/08", "07/19/14", "07/20/13", "07/21/07", "07/21/12", "07/22/06",
> > "07/23/05", "07/23/11", "07/23/16", "07/24/04", "07/24/10", "07/25/09",
> > "07/25/15", "07/26/03", "07/26/08", "07/26/14", "07/27/13", "07/28/07",
> > "07/28/12", "07/29/06", "07/30/05", "07/30/11", "07/30/16", "07/31/04",
> > "07/31/10", "08/01/09", "08/01/15", "08/02/03", "08/02/08", "08/02/14",
> > "08/03/13", "08/04/07", "08/04/12", "08/05/06", "08/06/05", "08/06/11",
> > "08/06/16", "08/07/04", "08/07/10", "08/08/09", "08/08/15", "08/09/03",
> > "08/09/08", "08/09/14", "08/10/13", "08/11/07", "08/11/12", "08/12/06",
> > "08/13/05", "08/13/11", "08/13/16", "08/14/04", "08/14/10", "08/15/09",
> > "08/15/15", "08/16/03", "08/16/08", "08/16/14", "08/17/13", "08/18/07",
> > "08/18/12", "08/19/06", "08/20/05", "08/20/11", "08/20/16", "08/21/04",
> > "08/21/10", "08/22/09", "08/22/15", "08/23/03", "08/23/08", "08/23/14",
> > "08/24/13", "08/25/07", "08/25/12", "08/26/06", "08/27/05", "08/27/11",
> > "08/27/16", "08/28/04", "08/28/10", "08/29/09", "08/29/15", "08/30/03",
> > "08/30/08", "08/30/14", "08/31/13", "09/01/07", "09/01/12", "09/02/06",
> > "09/03/05", "09/03/11", "09/03/16", "09/04/04", "09/04/10", "09/05/09",
> > "09/05/15", "09/06/03", "09/06/08", "09/06/14", "09/07/13", "09/08/07",
> > "09/08/12", "09/09/06", "09/10/05", "09/10/11", "09/10/16", "09/11/04",
> > "09/11/10", "09/12/09", "09/12/15", "09/13/03", "09/13/08", "09/13/14",
> > "09/14/13", "09/15/07", "09/15/12", "09/16/06", "09/17/05", "09/17/11",
> > "09/17/16", "09/18/04", "09/18/10", "09/19/09", "09/19/15", "09/20/03",
> > "09/20/08", "09/20/14", "09/21/13", "09/22/07", "09/22/12", "09/23/06",
> > "09/24/05", "09/24/11", "09/24/16", "09/25/04", "09/25/10", "09/26/09",
> > "09/26/15", "09/27/03", "09/27/08", "09/27/14", "09/28/13", "09/29/07",
> > "09/29/12", "09/30/06", "10/01/05", "10/01/11", "10/01/16", "10/02/04",
> > "10/02/10", "10/03/09", "10/03/15", "10/04/03", "10/04/08", "10/04/14",
> > "10/05/13", "10/06/07", "10/06/12", "10/07/06", "10/08/05", "10/08/11",
> > "10/08/16", "10/09/04", "10/09/10", "10/10/09", "10/10/15", "10/11/03",
> > "10/11/08", "10/11/14", "10/12/13", "10/13/07", "10/13/12", "10/14/06",
> > "10/15/05", "10/15/11", "10/15/16", "10/16/04", "10/16/10", "10/17/09",
> > "10/17/15", "10/18/03", "10/18/08", "10/18/14", "10/19/13", "10/20/07",
> > "10/20/12", "10/21/06", "10/22/05", "10/22/11", "10/22/16", "10/23/04",
> > "10/23/10", "10/24/09", "10/24/15", "10/25/03", "10/25/08", "10/25/14",
> > "10/26/13", "10/27/07", "10/27/12", "10/28/06", "10/29/05", "10/29/11",
> > "10/29/16", "10/30/04", "10/30/10", "10/31/09", "10/31/15", "11/01/03",
> > "11/01/08", "11/01/14", "11/02/13", "11/03/07", "11/03/12", "11/04/06",
> > "11/05/05", "11/05/11", "11/05/16", "11/06/04", "11/06/10", "11/07/09",
> > "11/07/15", "11/08/03", "11/08/08", "11/08/14", "11/09/13", "11/10/07",
> > "11/10/12", "11/11/06", "11/12/05", "11/12/11", "11/12/16", "11/13/04",
> > "11/13/10", "11/14/09", "11/14/15", "11/15/03", "11/15/08", "11/15/14",
> > "11/16/13", "11/17/07", "11/17/12", "11/18/06", "11/19/05", "11/19/11",
> > "11/19/16", "11/20/04", "11/20/10", "11/21/09", "11/21/15", "11/22/03",
> > "11/22/08", "11/22/14", "11/23/13", "11/24/07", "11/24/12", "11/25/06",
> > "11/26/05", "11/26/11", "11/26/16", "11/27/04", "11/27/10", "11/28/09",
> > "11/28/15", "11/29/03", "11/29/08", "11/29/14", "11/30/13", "12/01/07",
> > "12/01/12", "12/02/06", "12/03/05", "12/03/11", "12/03/16", "12/04/04",
> > "12/04/10", "12/05/09", "12/05/15", "12/06/03", "12/06/08", "12/06/14",
> > "12/07/13", "12/08/07", "12/08/12", "12/09/06", "12/10/05", "12/10/11",
> > "12/10/16", "12/11/04", "12/11/10", "12/12/09", "12/12/15", "12/13/03",
> > "12/13/08", "12/13/14", "12/14/13", "12/15/07", "12/15/12", "12/16/06",
> > "12/17/05", "12/17/11", "12/17/16", "12/18/04", "12/18/10", "12/19/09",
> > "12/19/15", "12/20/03", "12/20/08", "12/20/14", "12/21/13", "12/22/07",
> > "12/22/12", "12/23/06", "12/24/05", "12/24/11", "12/24/16", "12/25/04",
> > "12/25/10", "12/26/09", "12/26/15", "12/27/03", "12/27/08", "12/27/14",
> > "12/28/13", "12/29/07", "12/29/12", "12/30/06", "12/31/05", "12/31/11",
> > "12/31/16"), class = "factor"),
>
> Those are not R dates. Unless you convert to dates then they will have
> alpha sort order which is probably NOT what you expected.
>
> Perhaps:
>
> dat$fixed_dates <- as.Date( as.character(dat$Date), format="%m/%d/%y")
>
>
>
> --
> David
> > Transits = c(11L, 17L, 14L, 18L,
> > 12L, 14L, 21L, 14L, 16L, 10L, 11L, 10L, 6L, 8L, 12L, 11L, 8L,
> > 6L, 7L, 9L, 6L, 5L, 3L, 12L, 10L, 5L, 6L, 5L, 9L, 14L, 6L, 6L,
> > 5L, 6L, 7L, 6L, 5L, 6L, 4L, 14L, 10L, 7L, 9L, 8L, 9L, 12L, 12L,
> > 13L, 9L, 15L, 19L, 9L, 8L, 7L, 8L, 16L, 19L, 14L, 15L, 7L, 12L,
> > 16L, 15L, 10L, 17L, 7L, 5L, 6L, 10L, 4L, 5L, 5L, 9L, 5L, 2L,
> > 6L, 8L, 5L, 4L, 8L, 7L, 9L, 6L, 7L, 8L, 5L, 9L, 7L, 5L, 13L,
> > 11L, 15L, 9L, 11L, 17L, 17L, 10L, 14L, 20L, 18L, 8L, 17L, 7L,
> > 12L, 9L, 10L, 6L, 7L, 11L, 9L, 7L, 10L, 7L, 10L, 13L, 12L, 10L,
> > 8L, 11L, 10L, 9L, 10L, 8L, 9L, 6L, 9L, 7L, 6L, 6L, 11L, 10L,
> > 5L, 4L, 7L, 14L, 11L, 8L, 10L, 7L, 5L, 4L, 8L, 6L, 15L, 11L,
> > 14L, 16L, 18L, 20L, 14L, 18L, 14L, 13L, 16L, 17L, 9L, 11L, 16L,
> > 13L, 12L, 11L, 16L, 16L, 17L, 21L, 13L, 19L, 16L, 14L, 13L, 11L,
> > 8L, 8L, 19L, 9L, 11L, 10L, 17L, 12L, 10L, 11L, 13L, 12L, 11L,
> > 8L, 12L, 19L, 14L, 9L, 9L, 16L, 20L, 13L, 12L, 14L, 13L, 12L,
> > 18L, 13L, 12L, 19L, 16L, 16L, 17L, 16L, 20L, 20L, 13L, 14L, 13L,
> > 7L, 16L, 15L, 17L, 16L, 15L, 17L, 18L, 17L, 16L, 14L, 12L, 9L,
> > 9L, 6L, 14L, 8L, 17L, 9L, 12L, 11L, 8L, 11L, 10L, 13L, 7L, 4L,
> > 10L, 12L, 11L, 8L, 8L, 13L, 6L, 9L, 6L, 18L, 11L, 13L, 10L, 17L,
> > 14L, 10L, 14L, 11L, 15L, 19L, 17L, 15L, 18L, 14L, 12L, 20L, 12L,
> > 12L, 16L, 16L, 17L, 11L, 10L, 11L, 13L, 16L, 17L, 9L, 11L, 13L,
> > 7L, 7L, 7L, 10L, 9L, 5L, 12L, 8L, 8L, 10L, 10L, 12L, 9L, 8L,
> > 9L, 12L, 10L, 9L, 6L, 9L, 2L, 9L, 8L, 10L, 12L, 13L, 15L, 12L,
> > 13L, 12L, 17L, 18L, 14L, 21L, 15L, 19L, 15L, 11L, 18L, 20L, 18L,
> > 19L, 19L, 18L, 20L, 16L, 15L, 11L, 20L, 12L, 13L, 12L, 10L, 11L,
> > 12L, 14L, 9L, 7L, 11L, 12L, 14L, 11L, 11L, 9L, 20L, 9L, 11L,
> > 9L, 10L, 9L, 15L, 8L, 8L, 13L, 10L, 15L, 11L, 14L, 16L, 25L,
> > 20L, 23L, 31L, 29L, 21L, 20L, 16L, 25L, 20L, 16L, 20L, 21L, 19L,
> > 16L, 22L, 17L, 18L, 20L, 16L, 15L, 14L, 15L, 10L, 7L, 15L, 10L,
> > 5L, 14L, 7L, 16L, 11L, 12L, 8L, 8L, 8L, 9L, 9L, 9L, 14L, 5L,
> > 12L, 9L, 8L, 11L, 11L, 7L, 11L, 16L, 18L, 17L, 31L, 21L, 30L,
> > 21L, 29L, 26L, 22L, 14L, 26L, 19L, 20L, 10L, 15L, 22L, 15L, 21L,
> > 13L, 19L, 19L, 22L, 19L, 15L, 16L, 18L, 15L, 13L, 13L, 8L, 6L,
> > 9L, 9L, 11L, 10L, 7L, 7L, 8L, 11L, 7L, 7L, 10L, 6L, 8L, 9L, 5L,
> > 10L, 6L, 8L, 12L, 6L, 17L, 12L, 16L, 13L, 14L, 18L, 25L, 19L,
> > 19L, 24L, 14L, 18L, 20L, 18L, 16L, 17L, 12L, 21L, 20L, 18L, 15L,
> > 12L, 13L, 11L, 9L, 10L, 11L, 8L, 12L, 12L, 4L, 5L, 5L, 11L, 9L,
> > 10L, 7L, 3L, 13L, 6L, 10L, 9L, 9L, 10L, 10L, 8L, 10L, 6L, 12L,
> > 5L, 12L, 13L, 19L, 10L, 14L, 20L, 19L, 20L, 16L, 22L, 16L, 24L,
> > 16L, 20L, 20L, 13L, 9L, 11L, 8L, 13L, 16L, 10L, 13L, 11L, 15L,
> > 6L, 6L, 5L, 7L, 3L, 7L, 4L, 7L, 6L, 5L, 4L, 7L, 10L, 5L, 7L,
> > 1L, 6L, 10L, 5L, 8L, 8L, 10L, 6L, 6L, 11L, 8L, 7L, 10L, 12L,
> > 11L, 14L, 17L, 20L, 29L, 21L, 24L, 25L, 32L, 26L, 29L, 18L, 22L,
> > 19L, 16L, 17L, 25L, 16L, 24L, 21L, 16L, 25L, 16L, 23L, 23L, 16L,
> > 14L, 18L, 12L, 17L, 16L, 9L, 12L, 5L, 11L, 9L, 8L, 13L, 10L,
> > 6L, 6L, 10L, 14L, 12L, 7L, 13L, 10L, 14L, 13L, 4L, 16L, 19L,
> > 16L, 15L, 30L, 26L, 29L, 31L, 22L, 26L, 35L, 35L, 30L, 21L, 25L,
> > 31L, 20L, 21L, 23L, 27L, 21L, 24L, 16L, 28L, 11L, 19L, 16L, 13L,
> > 14L, 13L, 19L, 15L, 15L, 22L, 14L, 24L, 16L, 16L, 12L, 12L, 14L,
> > 14L, 12L, 16L, 8L, 10L, 15L, 16L, 11L, 16L, 9L, 15L, 12L, 15L,
> > 6L, 15L, 19L, 19L, 20L, 19L, 21L, 20L, 23L, 21L, 15L, 15L, 24L,
> > 13L, 9L, 11L, 10L, 11L, 13L, 15L, 9L, 17L, 22L, 16L, 16L, 13L,
> > 15L, 15L, 13L, 11L, 11L, 15L, 10L, 12L, 14L, 6L, 11L, 9L, 12L,
> > 4L, 12L, 7L, 9L, 7L, 15L, 13L, 12L, 8L, 5L, 16L, 13L, 13L, 15L,
> > 16L, 16L, 13L, 21L, 15L, 18L, 22L, 12L, 16L, 25L, 14L, 17L, 12L,
> > 10L)), .Names = c("Date", "Transits"), class = "data.frame", row.names =
> > c(NA,
> > -731L))
> >
> >
> > This is the result I get from the forecast:
> >
> > dput(Forecasts):
> >
> > structure(list(Point.Forecast = c(19.4253367238618, 12.2907914272766,
> > 12.9624076300088, 18.0807466065309, 15.9843908802223, 16.5321579531827,
> > 16.7184964071333, 10.1201838392972, 20.7203714449961, 7.45002607776652,
> > 14.9836817432811, 13.3820472411712), Lo.80 = c(13.9434152149432,
> > 6.74027066336727, 7.25612824519892, 12.2923074967212, 10.0690364921587,
> > 10.501665758874, 10.5705243712663, 3.85957289110304, 14.3484486381226,
> > 0.968987377892339, 8.39518758108775, 6.6878669395083), Hi.80 =
> > c(24.9072582327803,
> > 17.841312191186, 18.6686870148187, 23.8691857163406, 21.8997452682858,
> > 22.5626501474914, 22.8664684430002, 16.3807947874913, 27.0922942518695,
> > 13.9310647776407, 21.5721759054744, 20.0762275428341), Lo.95 =
> > c(11.0414612619408,
> > 3.80200245835122, 4.23540640692652, 9.2280929272317, 6.93763703269479,
> > 7.30931602651216, 7.31598456740248, 0.545405648875148, 10.975356457297,
> > -2.4618672675205, 4.90744944999539, 3.14418194567588), Hi.95 =
> > c(27.8092121857828,
> > 20.779580396202, 21.6894088530911, 26.9334002858301, 25.0311447277497,
> > 25.7549998798532, 26.121008246864, 19.6949620297192, 30.4653864326951,
> > 17.3619194230535, 25.0599140365668, 23.6199125366665)), .Names =
> > c("Point.Forecast",
> > "Lo.80", "Hi.80", "Lo.95", "Hi.95"), row.names = c("2016.019",
> > "2016.038", "2016.058", "2016.077", "2016.096", "2016.115", "2016.135",
> > "2016.154", "2016.173", "2016.192", "2016.212", "2016.231"), class =
> > "data.frame")
> >
> > Thank you beforehand for your valuable support,
> >
> > Best regards,
> >
> > Paul
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> 'Any technology distinguishable from magic is insufficiently advanced.'
>  -Gehm's Corollary to Clarke's Third Law
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Dec  6 20:32:59 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 6 Dec 2017 11:32:59 -0800
Subject: [R] Odd dates generated in Forecasts
In-Reply-To: <CAMOcQfPvy=Gwz=gdWZMXjipW4-u8ihEKjQNr7733JZMju5Q01A@mail.gmail.com>
References: <CAMOcQfON+4vvLDCHAKsvnRhW1B1L7=evX3nmBdBr4DuB58qvYQ@mail.gmail.com>
 <F178806B-58EC-4A8C-BAC6-0D063CAB77F0@comcast.net>
 <CAMOcQfPvy=Gwz=gdWZMXjipW4-u8ihEKjQNr7733JZMju5Q01A@mail.gmail.com>
Message-ID: <8B08AE1D-31A5-4C61-A1A8-548830DC30B8@comcast.net>


> On Dec 6, 2017, at 11:09 AM, Paul Bernal <paulbernal07 at gmail.com> wrote:
> 
> Thank you very much David. As a matter of fact, I solved it by doing the following:
> 
> MyTimeSeriesObj <- ts(MyData, freq=365.25/7, start=decimal_date(mdy("01-04-2003")))
> 
> After doing that adjustment, my forecasts dates started from 2017 on.

Not clear what MyData consisted of. If it was the factor variable in the `dataset` object in your earlier communication, then you gave the `ts`  function garbage. The fact that it was labeled to your liking would not create a palatable result.

-- 
David.
> 
> Cheers,
> 
> Paul
> 
> 2017-12-06 12:03 GMT-05:00 David Winsemius <dwinsemius at comcast.net>:
> 
> > On Dec 6, 2017, at 5:07 AM, Paul Bernal <paulbernal07 at gmail.com> wrote:
> >
> > Dear friends,
> >
> > I have a weekly time series which starts on Jan 4th, 2003 and ends on
> > december 31st, 2016.
> >
> > I set up my ts object as follows:
> >
> > MyTseries <- ts(mydataset, start=2003, end=2016, frequency=52)
> >
> > MyModel <- auto.arima(MyTseries, d=1, D=1)
> >
> > MyModelForecast <- forecast (MyModel, h=12)
> >
> > Since my last observation was on december 31st, 2016 I expected my forecast
> > date to start on 2017, but instead it returned odd dates.
> >
> > This is my dataset
> >
> > dput(dataset):
> >
> > structure(list(Date = structure(c(8L, 22L, 36L, 50L, 64L, 78L,
> > 92L, 106L, 120L, 134L, 148L, 162L, 176L, 190L, 204L, 218L, 232L,
> > 246L, 260L, 274L, 288L, 302L, 316L, 330L, 344L, 358L, 372L, 386L,
> > 400L, 414L, 428L, 442L, 456L, 470L, 484L, 498L, 512L, 526L, 540L,
> > 554L, 568L, 582L, 596L, 610L, 624L, 638L, 652L, 666L, 680L, 694L,
> > 708L, 722L, 5L, 19L, 33L, 47L, 61L, 75L, 89L, 103L, 117L, 130L,
> > 144L, 158L, 172L, 186L, 200L, 214L, 228L, 242L, 256L, 270L, 284L,
> > 298L, 312L, 326L, 340L, 354L, 368L, 382L, 396L, 410L, 424L, 438L,
> > 452L, 466L, 480L, 494L, 508L, 522L, 536L, 550L, 564L, 578L, 592L,
> > 606L, 620L, 634L, 648L, 662L, 676L, 690L, 704L, 718L, 1L, 15L,
> > 29L, 43L, 57L, 71L, 85L, 99L, 113L, 127L, 141L, 155L, 169L, 183L,
> > 197L, 211L, 225L, 239L, 253L, 267L, 281L, 295L, 309L, 323L, 337L,
> > 351L, 365L, 379L, 393L, 407L, 421L, 435L, 449L, 463L, 477L, 491L,
> > 505L, 519L, 533L, 547L, 561L, 575L, 589L, 603L, 617L, 631L, 645L,
> > 659L, 673L, 687L, 701L, 715L, 729L, 13L, 27L, 41L, 55L, 69L,
> > 83L, 97L, 111L, 126L, 140L, 154L, 168L, 182L, 196L, 210L, 224L,
> > 238L, 252L, 266L, 280L, 294L, 308L, 322L, 336L, 350L, 364L, 378L,
> > 392L, 406L, 420L, 434L, 448L, 462L, 476L, 490L, 504L, 518L, 532L,
> > 546L, 560L, 574L, 588L, 602L, 616L, 630L, 644L, 658L, 672L, 686L,
> > 700L, 714L, 728L, 12L, 26L, 40L, 54L, 68L, 82L, 96L, 110L, 124L,
> > 138L, 152L, 166L, 180L, 194L, 208L, 222L, 236L, 250L, 264L, 278L,
> > 292L, 306L, 320L, 334L, 348L, 362L, 376L, 390L, 404L, 418L, 432L,
> > 446L, 460L, 474L, 488L, 502L, 516L, 530L, 544L, 558L, 572L, 586L,
> > 600L, 614L, 628L, 642L, 656L, 670L, 684L, 698L, 712L, 726L, 10L,
> > 24L, 38L, 52L, 66L, 80L, 94L, 108L, 121L, 135L, 149L, 163L, 177L,
> > 191L, 205L, 219L, 233L, 247L, 261L, 275L, 289L, 303L, 317L, 331L,
> > 345L, 359L, 373L, 387L, 401L, 415L, 429L, 443L, 457L, 471L, 485L,
> > 499L, 513L, 527L, 541L, 555L, 569L, 583L, 597L, 611L, 625L, 639L,
> > 653L, 667L, 681L, 695L, 709L, 723L, 6L, 20L, 34L, 48L, 62L, 76L,
> > 90L, 104L, 118L, 132L, 146L, 160L, 174L, 188L, 202L, 216L, 230L,
> > 244L, 258L, 272L, 286L, 300L, 314L, 328L, 342L, 356L, 370L, 384L,
> > 398L, 412L, 426L, 440L, 454L, 468L, 482L, 496L, 510L, 524L, 538L,
> > 552L, 566L, 580L, 594L, 608L, 622L, 636L, 650L, 664L, 678L, 692L,
> > 706L, 720L, 3L, 17L, 31L, 45L, 59L, 73L, 87L, 101L, 115L, 131L,
> > 145L, 159L, 173L, 187L, 201L, 215L, 229L, 243L, 257L, 271L, 285L,
> > 299L, 313L, 327L, 341L, 355L, 369L, 383L, 397L, 411L, 425L, 439L,
> > 453L, 467L, 481L, 495L, 509L, 523L, 537L, 551L, 565L, 579L, 593L,
> > 607L, 621L, 635L, 649L, 663L, 677L, 691L, 705L, 719L, 2L, 16L,
> > 30L, 44L, 58L, 72L, 86L, 100L, 114L, 128L, 142L, 156L, 170L,
> > 184L, 198L, 212L, 226L, 240L, 254L, 268L, 282L, 296L, 310L, 324L,
> > 338L, 352L, 366L, 380L, 394L, 408L, 422L, 436L, 450L, 464L, 478L,
> > 492L, 506L, 520L, 534L, 548L, 562L, 576L, 590L, 604L, 618L, 632L,
> > 646L, 660L, 674L, 688L, 702L, 716L, 730L, 14L, 28L, 42L, 56L,
> > 70L, 84L, 98L, 112L, 125L, 139L, 153L, 167L, 181L, 195L, 209L,
> > 223L, 237L, 251L, 265L, 279L, 293L, 307L, 321L, 335L, 349L, 363L,
> > 377L, 391L, 405L, 419L, 433L, 447L, 461L, 475L, 489L, 503L, 517L,
> > 531L, 545L, 559L, 573L, 587L, 601L, 615L, 629L, 643L, 657L, 671L,
> > 685L, 699L, 713L, 727L, 11L, 25L, 39L, 53L, 67L, 81L, 95L, 109L,
> > 123L, 137L, 151L, 165L, 179L, 193L, 207L, 221L, 235L, 249L, 263L,
> > 277L, 291L, 305L, 319L, 333L, 347L, 361L, 375L, 389L, 403L, 417L,
> > 431L, 445L, 459L, 473L, 487L, 501L, 515L, 529L, 543L, 557L, 571L,
> > 585L, 599L, 613L, 627L, 641L, 655L, 669L, 683L, 697L, 711L, 725L,
> > 9L, 23L, 37L, 51L, 65L, 79L, 93L, 107L, 122L, 136L, 150L, 164L,
> > 178L, 192L, 206L, 220L, 234L, 248L, 262L, 276L, 290L, 304L, 318L,
> > 332L, 346L, 360L, 374L, 388L, 402L, 416L, 430L, 444L, 458L, 472L,
> > 486L, 500L, 514L, 528L, 542L, 556L, 570L, 584L, 598L, 612L, 626L,
> > 640L, 654L, 668L, 682L, 696L, 710L, 724L, 7L, 21L, 35L, 49L,
> > 63L, 77L, 91L, 105L, 119L, 133L, 147L, 161L, 175L, 189L, 203L,
> > 217L, 231L, 245L, 259L, 273L, 287L, 301L, 315L, 329L, 343L, 357L,
> > 371L, 385L, 399L, 413L, 427L, 441L, 455L, 469L, 483L, 497L, 511L,
> > 525L, 539L, 553L, 567L, 581L, 595L, 609L, 623L, 637L, 651L, 665L,
> > 679L, 693L, 707L, 721L, 4L, 18L, 32L, 46L, 60L, 74L, 88L, 102L,
> > 116L, 129L, 143L, 157L, 171L, 185L, 199L, 213L, 227L, 241L, 255L,
> > 269L, 283L, 297L, 311L, 325L, 339L, 353L, 367L, 381L, 395L, 409L,
> > 423L, 437L, 451L, 465L, 479L, 493L, 507L, 521L, 535L, 549L, 563L,
> > 577L, 591L, 605L, 619L, 633L, 647L, 661L, 675L, 689L, 703L, 717L,
> > 731L), .Label = c("01/01/05", "01/01/11", "01/02/10", "01/02/16",
> > "01/03/04", "01/03/09", "01/03/15", "01/04/03", "01/04/14", "01/05/08",
> > "01/05/13", "01/06/07", "01/07/06", "01/07/12", "01/08/05", "01/08/11",
> > "01/09/10", "01/09/16", "01/10/04", "01/10/09", "01/10/15", "01/11/03",
> > "01/11/14", "01/12/08", "01/12/13", "01/13/07", "01/14/06", "01/14/12",
> > "01/15/05", "01/15/11", "01/16/10", "01/16/16", "01/17/04", "01/17/09",
> > "01/17/15", "01/18/03", "01/18/14", "01/19/08", "01/19/13", "01/20/07",
> > "01/21/06", "01/21/12", "01/22/05", "01/22/11", "01/23/10", "01/23/16",
> > "01/24/04", "01/24/09", "01/24/15", "01/25/03", "01/25/14", "01/26/08",
> > "01/26/13", "01/27/07", "01/28/06", "01/28/12", "01/29/05", "01/29/11",
> > "01/30/10", "01/30/16", "01/31/04", "01/31/09", "01/31/15", "02/01/03",
> > "02/01/14", "02/02/08", "02/02/13", "02/03/07", "02/04/06", "02/04/12",
> > "02/05/05", "02/05/11", "02/06/10", "02/06/16", "02/07/04", "02/07/09",
> > "02/07/15", "02/08/03", "02/08/14", "02/09/08", "02/09/13", "02/10/07",
> > "02/11/06", "02/11/12", "02/12/05", "02/12/11", "02/13/10", "02/13/16",
> > "02/14/04", "02/14/09", "02/14/15", "02/15/03", "02/15/14", "02/16/08",
> > "02/16/13", "02/17/07", "02/18/06", "02/18/12", "02/19/05", "02/19/11",
> > "02/20/10", "02/20/16", "02/21/04", "02/21/09", "02/21/15", "02/22/03",
> > "02/22/14", "02/23/08", "02/23/13", "02/24/07", "02/25/06", "02/25/12",
> > "02/26/05", "02/26/11", "02/27/10", "02/27/16", "02/28/04", "02/28/09",
> > "02/28/15", "03/01/03", "03/01/08", "03/01/14", "03/02/13", "03/03/07",
> > "03/03/12", "03/04/06", "03/05/05", "03/05/11", "03/05/16", "03/06/04",
> > "03/06/10", "03/07/09", "03/07/15", "03/08/03", "03/08/08", "03/08/14",
> > "03/09/13", "03/10/07", "03/10/12", "03/11/06", "03/12/05", "03/12/11",
> > "03/12/16", "03/13/04", "03/13/10", "03/14/09", "03/14/15", "03/15/03",
> > "03/15/08", "03/15/14", "03/16/13", "03/17/07", "03/17/12", "03/18/06",
> > "03/19/05", "03/19/11", "03/19/16", "03/20/04", "03/20/10", "03/21/09",
> > "03/21/15", "03/22/03", "03/22/08", "03/22/14", "03/23/13", "03/24/07",
> > "03/24/12", "03/25/06", "03/26/05", "03/26/11", "03/26/16", "03/27/04",
> > "03/27/10", "03/28/09", "03/28/15", "03/29/03", "03/29/08", "03/29/14",
> > "03/30/13", "03/31/07", "03/31/12", "04/01/06", "04/02/05", "04/02/11",
> > "04/02/16", "04/03/04", "04/03/10", "04/04/09", "04/04/15", "04/05/03",
> > "04/05/08", "04/05/14", "04/06/13", "04/07/07", "04/07/12", "04/08/06",
> > "04/09/05", "04/09/11", "04/09/16", "04/10/04", "04/10/10", "04/11/09",
> > "04/11/15", "04/12/03", "04/12/08", "04/12/14", "04/13/13", "04/14/07",
> > "04/14/12", "04/15/06", "04/16/05", "04/16/11", "04/16/16", "04/17/04",
> > "04/17/10", "04/18/09", "04/18/15", "04/19/03", "04/19/08", "04/19/14",
> > "04/20/13", "04/21/07", "04/21/12", "04/22/06", "04/23/05", "04/23/11",
> > "04/23/16", "04/24/04", "04/24/10", "04/25/09", "04/25/15", "04/26/03",
> > "04/26/08", "04/26/14", "04/27/13", "04/28/07", "04/28/12", "04/29/06",
> > "04/30/05", "04/30/11", "04/30/16", "05/01/04", "05/01/10", "05/02/09",
> > "05/02/15", "05/03/03", "05/03/08", "05/03/14", "05/04/13", "05/05/07",
> > "05/05/12", "05/06/06", "05/07/05", "05/07/11", "05/07/16", "05/08/04",
> > "05/08/10", "05/09/09", "05/09/15", "05/10/03", "05/10/08", "05/10/14",
> > "05/11/13", "05/12/07", "05/12/12", "05/13/06", "05/14/05", "05/14/11",
> > "05/14/16", "05/15/04", "05/15/10", "05/16/09", "05/16/15", "05/17/03",
> > "05/17/08", "05/17/14", "05/18/13", "05/19/07", "05/19/12", "05/20/06",
> > "05/21/05", "05/21/11", "05/21/16", "05/22/04", "05/22/10", "05/23/09",
> > "05/23/15", "05/24/03", "05/24/08", "05/24/14", "05/25/13", "05/26/07",
> > "05/26/12", "05/27/06", "05/28/05", "05/28/11", "05/28/16", "05/29/04",
> > "05/29/10", "05/30/09", "05/30/15", "05/31/03", "05/31/08", "05/31/14",
> > "06/01/13", "06/02/07", "06/02/12", "06/03/06", "06/04/05", "06/04/11",
> > "06/04/16", "06/05/04", "06/05/10", "06/06/09", "06/06/15", "06/07/03",
> > "06/07/08", "06/07/14", "06/08/13", "06/09/07", "06/09/12", "06/10/06",
> > "06/11/05", "06/11/11", "06/11/16", "06/12/04", "06/12/10", "06/13/09",
> > "06/13/15", "06/14/03", "06/14/08", "06/14/14", "06/15/13", "06/16/07",
> > "06/16/12", "06/17/06", "06/18/05", "06/18/11", "06/18/16", "06/19/04",
> > "06/19/10", "06/20/09", "06/20/15", "06/21/03", "06/21/08", "06/21/14",
> > "06/22/13", "06/23/07", "06/23/12", "06/24/06", "06/25/05", "06/25/11",
> > "06/25/16", "06/26/04", "06/26/10", "06/27/09", "06/27/15", "06/28/03",
> > "06/28/08", "06/28/14", "06/29/13", "06/30/07", "06/30/12", "07/01/06",
> > "07/02/05", "07/02/11", "07/02/16", "07/03/04", "07/03/10", "07/04/09",
> > "07/04/15", "07/05/03", "07/05/08", "07/05/14", "07/06/13", "07/07/07",
> > "07/07/12", "07/08/06", "07/09/05", "07/09/11", "07/09/16", "07/10/04",
> > "07/10/10", "07/11/09", "07/11/15", "07/12/03", "07/12/08", "07/12/14",
> > "07/13/13", "07/14/07", "07/14/12", "07/15/06", "07/16/05", "07/16/11",
> > "07/16/16", "07/17/04", "07/17/10", "07/18/09", "07/18/15", "07/19/03",
> > "07/19/08", "07/19/14", "07/20/13", "07/21/07", "07/21/12", "07/22/06",
> > "07/23/05", "07/23/11", "07/23/16", "07/24/04", "07/24/10", "07/25/09",
> > "07/25/15", "07/26/03", "07/26/08", "07/26/14", "07/27/13", "07/28/07",
> > "07/28/12", "07/29/06", "07/30/05", "07/30/11", "07/30/16", "07/31/04",
> > "07/31/10", "08/01/09", "08/01/15", "08/02/03", "08/02/08", "08/02/14",
> > "08/03/13", "08/04/07", "08/04/12", "08/05/06", "08/06/05", "08/06/11",
> > "08/06/16", "08/07/04", "08/07/10", "08/08/09", "08/08/15", "08/09/03",
> > "08/09/08", "08/09/14", "08/10/13", "08/11/07", "08/11/12", "08/12/06",
> > "08/13/05", "08/13/11", "08/13/16", "08/14/04", "08/14/10", "08/15/09",
> > "08/15/15", "08/16/03", "08/16/08", "08/16/14", "08/17/13", "08/18/07",
> > "08/18/12", "08/19/06", "08/20/05", "08/20/11", "08/20/16", "08/21/04",
> > "08/21/10", "08/22/09", "08/22/15", "08/23/03", "08/23/08", "08/23/14",
> > "08/24/13", "08/25/07", "08/25/12", "08/26/06", "08/27/05", "08/27/11",
> > "08/27/16", "08/28/04", "08/28/10", "08/29/09", "08/29/15", "08/30/03",
> > "08/30/08", "08/30/14", "08/31/13", "09/01/07", "09/01/12", "09/02/06",
> > "09/03/05", "09/03/11", "09/03/16", "09/04/04", "09/04/10", "09/05/09",
> > "09/05/15", "09/06/03", "09/06/08", "09/06/14", "09/07/13", "09/08/07",
> > "09/08/12", "09/09/06", "09/10/05", "09/10/11", "09/10/16", "09/11/04",
> > "09/11/10", "09/12/09", "09/12/15", "09/13/03", "09/13/08", "09/13/14",
> > "09/14/13", "09/15/07", "09/15/12", "09/16/06", "09/17/05", "09/17/11",
> > "09/17/16", "09/18/04", "09/18/10", "09/19/09", "09/19/15", "09/20/03",
> > "09/20/08", "09/20/14", "09/21/13", "09/22/07", "09/22/12", "09/23/06",
> > "09/24/05", "09/24/11", "09/24/16", "09/25/04", "09/25/10", "09/26/09",
> > "09/26/15", "09/27/03", "09/27/08", "09/27/14", "09/28/13", "09/29/07",
> > "09/29/12", "09/30/06", "10/01/05", "10/01/11", "10/01/16", "10/02/04",
> > "10/02/10", "10/03/09", "10/03/15", "10/04/03", "10/04/08", "10/04/14",
> > "10/05/13", "10/06/07", "10/06/12", "10/07/06", "10/08/05", "10/08/11",
> > "10/08/16", "10/09/04", "10/09/10", "10/10/09", "10/10/15", "10/11/03",
> > "10/11/08", "10/11/14", "10/12/13", "10/13/07", "10/13/12", "10/14/06",
> > "10/15/05", "10/15/11", "10/15/16", "10/16/04", "10/16/10", "10/17/09",
> > "10/17/15", "10/18/03", "10/18/08", "10/18/14", "10/19/13", "10/20/07",
> > "10/20/12", "10/21/06", "10/22/05", "10/22/11", "10/22/16", "10/23/04",
> > "10/23/10", "10/24/09", "10/24/15", "10/25/03", "10/25/08", "10/25/14",
> > "10/26/13", "10/27/07", "10/27/12", "10/28/06", "10/29/05", "10/29/11",
> > "10/29/16", "10/30/04", "10/30/10", "10/31/09", "10/31/15", "11/01/03",
> > "11/01/08", "11/01/14", "11/02/13", "11/03/07", "11/03/12", "11/04/06",
> > "11/05/05", "11/05/11", "11/05/16", "11/06/04", "11/06/10", "11/07/09",
> > "11/07/15", "11/08/03", "11/08/08", "11/08/14", "11/09/13", "11/10/07",
> > "11/10/12", "11/11/06", "11/12/05", "11/12/11", "11/12/16", "11/13/04",
> > "11/13/10", "11/14/09", "11/14/15", "11/15/03", "11/15/08", "11/15/14",
> > "11/16/13", "11/17/07", "11/17/12", "11/18/06", "11/19/05", "11/19/11",
> > "11/19/16", "11/20/04", "11/20/10", "11/21/09", "11/21/15", "11/22/03",
> > "11/22/08", "11/22/14", "11/23/13", "11/24/07", "11/24/12", "11/25/06",
> > "11/26/05", "11/26/11", "11/26/16", "11/27/04", "11/27/10", "11/28/09",
> > "11/28/15", "11/29/03", "11/29/08", "11/29/14", "11/30/13", "12/01/07",
> > "12/01/12", "12/02/06", "12/03/05", "12/03/11", "12/03/16", "12/04/04",
> > "12/04/10", "12/05/09", "12/05/15", "12/06/03", "12/06/08", "12/06/14",
> > "12/07/13", "12/08/07", "12/08/12", "12/09/06", "12/10/05", "12/10/11",
> > "12/10/16", "12/11/04", "12/11/10", "12/12/09", "12/12/15", "12/13/03",
> > "12/13/08", "12/13/14", "12/14/13", "12/15/07", "12/15/12", "12/16/06",
> > "12/17/05", "12/17/11", "12/17/16", "12/18/04", "12/18/10", "12/19/09",
> > "12/19/15", "12/20/03", "12/20/08", "12/20/14", "12/21/13", "12/22/07",
> > "12/22/12", "12/23/06", "12/24/05", "12/24/11", "12/24/16", "12/25/04",
> > "12/25/10", "12/26/09", "12/26/15", "12/27/03", "12/27/08", "12/27/14",
> > "12/28/13", "12/29/07", "12/29/12", "12/30/06", "12/31/05", "12/31/11",
> > "12/31/16"), class = "factor"),
> 
> Those are not R dates. Unless you convert to dates then they will have alpha sort order which is probably NOT what you expected.
> 
> Perhaps:
> 
> dat$fixed_dates <- as.Date( as.character(dat$Date), format="%m/%d/%y")
> 
> 
> 
> --
> David
> > Transits = c(11L, 17L, 14L, 18L,
> > 12L, 14L, 21L, 14L, 16L, 10L, 11L, 10L, 6L, 8L, 12L, 11L, 8L,
> > 6L, 7L, 9L, 6L, 5L, 3L, 12L, 10L, 5L, 6L, 5L, 9L, 14L, 6L, 6L,
> > 5L, 6L, 7L, 6L, 5L, 6L, 4L, 14L, 10L, 7L, 9L, 8L, 9L, 12L, 12L,
> > 13L, 9L, 15L, 19L, 9L, 8L, 7L, 8L, 16L, 19L, 14L, 15L, 7L, 12L,
> > 16L, 15L, 10L, 17L, 7L, 5L, 6L, 10L, 4L, 5L, 5L, 9L, 5L, 2L,
> > 6L, 8L, 5L, 4L, 8L, 7L, 9L, 6L, 7L, 8L, 5L, 9L, 7L, 5L, 13L,
> > 11L, 15L, 9L, 11L, 17L, 17L, 10L, 14L, 20L, 18L, 8L, 17L, 7L,
> > 12L, 9L, 10L, 6L, 7L, 11L, 9L, 7L, 10L, 7L, 10L, 13L, 12L, 10L,
> > 8L, 11L, 10L, 9L, 10L, 8L, 9L, 6L, 9L, 7L, 6L, 6L, 11L, 10L,
> > 5L, 4L, 7L, 14L, 11L, 8L, 10L, 7L, 5L, 4L, 8L, 6L, 15L, 11L,
> > 14L, 16L, 18L, 20L, 14L, 18L, 14L, 13L, 16L, 17L, 9L, 11L, 16L,
> > 13L, 12L, 11L, 16L, 16L, 17L, 21L, 13L, 19L, 16L, 14L, 13L, 11L,
> > 8L, 8L, 19L, 9L, 11L, 10L, 17L, 12L, 10L, 11L, 13L, 12L, 11L,
> > 8L, 12L, 19L, 14L, 9L, 9L, 16L, 20L, 13L, 12L, 14L, 13L, 12L,
> > 18L, 13L, 12L, 19L, 16L, 16L, 17L, 16L, 20L, 20L, 13L, 14L, 13L,
> > 7L, 16L, 15L, 17L, 16L, 15L, 17L, 18L, 17L, 16L, 14L, 12L, 9L,
> > 9L, 6L, 14L, 8L, 17L, 9L, 12L, 11L, 8L, 11L, 10L, 13L, 7L, 4L,
> > 10L, 12L, 11L, 8L, 8L, 13L, 6L, 9L, 6L, 18L, 11L, 13L, 10L, 17L,
> > 14L, 10L, 14L, 11L, 15L, 19L, 17L, 15L, 18L, 14L, 12L, 20L, 12L,
> > 12L, 16L, 16L, 17L, 11L, 10L, 11L, 13L, 16L, 17L, 9L, 11L, 13L,
> > 7L, 7L, 7L, 10L, 9L, 5L, 12L, 8L, 8L, 10L, 10L, 12L, 9L, 8L,
> > 9L, 12L, 10L, 9L, 6L, 9L, 2L, 9L, 8L, 10L, 12L, 13L, 15L, 12L,
> > 13L, 12L, 17L, 18L, 14L, 21L, 15L, 19L, 15L, 11L, 18L, 20L, 18L,
> > 19L, 19L, 18L, 20L, 16L, 15L, 11L, 20L, 12L, 13L, 12L, 10L, 11L,
> > 12L, 14L, 9L, 7L, 11L, 12L, 14L, 11L, 11L, 9L, 20L, 9L, 11L,
> > 9L, 10L, 9L, 15L, 8L, 8L, 13L, 10L, 15L, 11L, 14L, 16L, 25L,
> > 20L, 23L, 31L, 29L, 21L, 20L, 16L, 25L, 20L, 16L, 20L, 21L, 19L,
> > 16L, 22L, 17L, 18L, 20L, 16L, 15L, 14L, 15L, 10L, 7L, 15L, 10L,
> > 5L, 14L, 7L, 16L, 11L, 12L, 8L, 8L, 8L, 9L, 9L, 9L, 14L, 5L,
> > 12L, 9L, 8L, 11L, 11L, 7L, 11L, 16L, 18L, 17L, 31L, 21L, 30L,
> > 21L, 29L, 26L, 22L, 14L, 26L, 19L, 20L, 10L, 15L, 22L, 15L, 21L,
> > 13L, 19L, 19L, 22L, 19L, 15L, 16L, 18L, 15L, 13L, 13L, 8L, 6L,
> > 9L, 9L, 11L, 10L, 7L, 7L, 8L, 11L, 7L, 7L, 10L, 6L, 8L, 9L, 5L,
> > 10L, 6L, 8L, 12L, 6L, 17L, 12L, 16L, 13L, 14L, 18L, 25L, 19L,
> > 19L, 24L, 14L, 18L, 20L, 18L, 16L, 17L, 12L, 21L, 20L, 18L, 15L,
> > 12L, 13L, 11L, 9L, 10L, 11L, 8L, 12L, 12L, 4L, 5L, 5L, 11L, 9L,
> > 10L, 7L, 3L, 13L, 6L, 10L, 9L, 9L, 10L, 10L, 8L, 10L, 6L, 12L,
> > 5L, 12L, 13L, 19L, 10L, 14L, 20L, 19L, 20L, 16L, 22L, 16L, 24L,
> > 16L, 20L, 20L, 13L, 9L, 11L, 8L, 13L, 16L, 10L, 13L, 11L, 15L,
> > 6L, 6L, 5L, 7L, 3L, 7L, 4L, 7L, 6L, 5L, 4L, 7L, 10L, 5L, 7L,
> > 1L, 6L, 10L, 5L, 8L, 8L, 10L, 6L, 6L, 11L, 8L, 7L, 10L, 12L,
> > 11L, 14L, 17L, 20L, 29L, 21L, 24L, 25L, 32L, 26L, 29L, 18L, 22L,
> > 19L, 16L, 17L, 25L, 16L, 24L, 21L, 16L, 25L, 16L, 23L, 23L, 16L,
> > 14L, 18L, 12L, 17L, 16L, 9L, 12L, 5L, 11L, 9L, 8L, 13L, 10L,
> > 6L, 6L, 10L, 14L, 12L, 7L, 13L, 10L, 14L, 13L, 4L, 16L, 19L,
> > 16L, 15L, 30L, 26L, 29L, 31L, 22L, 26L, 35L, 35L, 30L, 21L, 25L,
> > 31L, 20L, 21L, 23L, 27L, 21L, 24L, 16L, 28L, 11L, 19L, 16L, 13L,
> > 14L, 13L, 19L, 15L, 15L, 22L, 14L, 24L, 16L, 16L, 12L, 12L, 14L,
> > 14L, 12L, 16L, 8L, 10L, 15L, 16L, 11L, 16L, 9L, 15L, 12L, 15L,
> > 6L, 15L, 19L, 19L, 20L, 19L, 21L, 20L, 23L, 21L, 15L, 15L, 24L,
> > 13L, 9L, 11L, 10L, 11L, 13L, 15L, 9L, 17L, 22L, 16L, 16L, 13L,
> > 15L, 15L, 13L, 11L, 11L, 15L, 10L, 12L, 14L, 6L, 11L, 9L, 12L,
> > 4L, 12L, 7L, 9L, 7L, 15L, 13L, 12L, 8L, 5L, 16L, 13L, 13L, 15L,
> > 16L, 16L, 13L, 21L, 15L, 18L, 22L, 12L, 16L, 25L, 14L, 17L, 12L,
> > 10L)), .Names = c("Date", "Transits"), class = "data.frame", row.names =
> > c(NA,
> > -731L))
> >
> >
> > This is the result I get from the forecast:
> >
> > dput(Forecasts):
> >
> > structure(list(Point.Forecast = c(19.4253367238618, 12.2907914272766,
> > 12.9624076300088, 18.0807466065309, 15.9843908802223, 16.5321579531827,
> > 16.7184964071333, 10.1201838392972, 20.7203714449961, 7.45002607776652,
> > 14.9836817432811, 13.3820472411712), Lo.80 = c(13.9434152149432,
> > 6.74027066336727, 7.25612824519892, 12.2923074967212, 10.0690364921587,
> > 10.501665758874, 10.5705243712663, 3.85957289110304, 14.3484486381226,
> > 0.968987377892339, 8.39518758108775, 6.6878669395083), Hi.80 =
> > c(24.9072582327803,
> > 17.841312191186, 18.6686870148187, 23.8691857163406, 21.8997452682858,
> > 22.5626501474914, 22.8664684430002, 16.3807947874913, 27.0922942518695,
> > 13.9310647776407, 21.5721759054744, 20.0762275428341), Lo.95 =
> > c(11.0414612619408,
> > 3.80200245835122, 4.23540640692652, 9.2280929272317, 6.93763703269479,
> > 7.30931602651216, 7.31598456740248, 0.545405648875148, 10.975356457297,
> > -2.4618672675205, 4.90744944999539, 3.14418194567588), Hi.95 =
> > c(27.8092121857828,
> > 20.779580396202, 21.6894088530911, 26.9334002858301, 25.0311447277497,
> > 25.7549998798532, 26.121008246864, 19.6949620297192, 30.4653864326951,
> > 17.3619194230535, 25.0599140365668, 23.6199125366665)), .Names =
> > c("Point.Forecast",
> > "Lo.80", "Hi.80", "Lo.95", "Hi.95"), row.names = c("2016.019",
> > "2016.038", "2016.058", "2016.077", "2016.096", "2016.115", "2016.135",
> > "2016.154", "2016.173", "2016.192", "2016.212", "2016.231"), class =
> > "data.frame")
> >
> > Thank you beforehand for your valuable support,
> >
> > Best regards,
> >
> > Paul
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
> 
> 
> 
> 
> 
> 

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From drjimlemon at gmail.com  Wed Dec  6 22:21:24 2017
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 7 Dec 2017 08:21:24 +1100
Subject: [R] Coeficients estimation in a repeated measures linear model
In-Reply-To: <CAGriyTN9Lq-g48S5Oe9R7vE7pr75ZtBC7bp-FR0sBgt+4ru1Hg@mail.gmail.com>
References: <CAGriyTN9Lq-g48S5Oe9R7vE7pr75ZtBC7bp-FR0sBgt+4ru1Hg@mail.gmail.com>
Message-ID: <CA+8X3fXzCDEXtTkM6rQdTV=zErNT9guSui=KjUQBfUY22S87FQ@mail.gmail.com>

Hi Sergio,
You seem to be aiming for a univariate repeated measures analysis.
Maybe this will help:

subno<-rep(1:6,2)
dat <- data.frame(subno=rep(1:6,2),,vals = c(ctrl, ttd),
   cond = c(rep("ctrl", 6), rep("ttd", 6)), ind = factor(rep(1:6, 2)))
fit<-aov(vals~ind+cond+Error(subno),data=dat)
fit
summary(fit)

Note that the assumptions of this model are easy to violate.

Jim


On Thu, Dec 7, 2017 at 1:17 AM, Sergio PV <serpalma.v at gmail.com> wrote:
> Dear Users,
>
> I am trying to understand the inner workings of a repeated measures linear
> model. Take for example a situation with 6 individuals sampled twice for
> two conditions (control and treated).
>
> set.seed(12)
> ctrl <- rnorm(n = 6, mean = 2)
> ttd <- rnorm(n = 6, mean = 10)
> dat <- data.frame(vals = c(ctrl, ttd),
>                   group = c(rep("ctrl", 6), rep("ttd", 6)),
>                   ind = factor(rep(1:6, 2)))
>
> fit <- lm(vals ~ ind + group, data = dat)
> model.matrix(~ ind + group, data = dat)
>
> I am puzzled on how the coeficients are calculated. For example, according
> to the model matrix, I thought the intercept would be individual 1 control.
> But that is clearly not the case.
> For the last coeficient, I understand it as the mean of all differences
> between treated vs control at each individual.
>
> I would greatly appreciate if someone could clarify to me how the
> coefficients in this situation are estimated.
>
> Thanks
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sewashm at gmail.com  Thu Dec  7 00:15:54 2017
From: sewashm at gmail.com (Ashta)
Date: Wed, 6 Dec 2017 17:15:54 -0600
Subject: [R] Remove
Message-ID: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>

Hi all,
In a data set I have group(GR) and two variables   x and y. I want to
remove a  group that have  the same record for the x variable in each
row.

DM <- read.table( text='GR x y
A 25 125
A 23 135
A 14 145
A 12 230
B 25 321
B 25 512
B 25 123
B 25 451
C 11 521
C 14 235
C 15 258
C 10 654',header = TRUE, stringsAsFactors = FALSE)

In this example the output should contain group A and C  as group B
has   the same record  for the variable x .

The result will be
A 25 125
A 23 135
A 14 145
A 12 230
C 11 521
C 14 235
C 15 258
C 10 654

How do I do it R?
Thank you.


From dwinsemius at comcast.net  Thu Dec  7 00:21:12 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 6 Dec 2017 15:21:12 -0800
Subject: [R] Remove
In-Reply-To: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>
References: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>
Message-ID: <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>


> On Dec 6, 2017, at 3:15 PM, Ashta <sewashm at gmail.com> wrote:
> 
> Hi all,
> In a data set I have group(GR) and two variables   x and y. I want to
> remove a  group that have  the same record for the x variable in each
> row.
> 
> DM <- read.table( text='GR x y
> A 25 125
> A 23 135
> A 14 145
> A 12 230
> B 25 321
> B 25 512
> B 25 123
> B 25 451
> C 11 521
> C 14 235
> C 15 258
> C 10 654',header = TRUE, stringsAsFactors = FALSE)
> 
> In this example the output should contain group A and C  as group B
> has   the same record  for the variable x .
> 
> The result will be
> A 25 125
> A 23 135
> A 14 145
> A 12 230
> C 11 521
> C 14 235
> C 15 258
> C 10 654

Try:

DM[ !duplicated(DM$x) , ]
> 
> How do I do it R?
> Thank you.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From sewashm at gmail.com  Thu Dec  7 00:44:38 2017
From: sewashm at gmail.com (Ashta)
Date: Wed, 6 Dec 2017 17:44:38 -0600
Subject: [R] Remove
In-Reply-To: <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>
References: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>
 <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>
Message-ID: <CADDFq31Tnc-QzPgw4Ucf3-jzdNDicjx0QESWY9HRy+H4MSSbpQ@mail.gmail.com>

Thank you David.
This will not work.  Tthis removes only duplicate records.
DM[ !duplicated(DM$x) , ]

My goal is to remove the group if all elements of x in that group have
 the same value.


On Wed, Dec 6, 2017 at 5:21 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>
>> On Dec 6, 2017, at 3:15 PM, Ashta <sewashm at gmail.com> wrote:
>>
>> Hi all,
>> In a data set I have group(GR) and two variables   x and y. I want to
>> remove a  group that have  the same record for the x variable in each
>> row.
>>
>> DM <- read.table( text='GR x y
>> A 25 125
>> A 23 135
>> A 14 145
>> A 12 230
>> B 25 321
>> B 25 512
>> B 25 123
>> B 25 451
>> C 11 521
>> C 14 235
>> C 15 258
>> C 10 654',header = TRUE, stringsAsFactors = FALSE)
>>
>> In this example the output should contain group A and C  as group B
>> has   the same record  for the variable x .
>>
>> The result will be
>> A 25 125
>> A 23 135
>> A 14 145
>> A 12 230
>> C 11 521
>> C 14 235
>> C 15 258
>> C 10 654
>
> Try:
>
> DM[ !duplicated(DM$x) , ]
>>
>> How do I do it R?
>> Thank you.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
>
>
>
>
>


From jdnewmil at dcn.davis.ca.us  Thu Dec  7 00:48:14 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 06 Dec 2017 15:48:14 -0800
Subject: [R] Remove
In-Reply-To: <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>
References: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>
 <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>
Message-ID: <3296595B-6750-425D-9C2E-AD1697307689@dcn.davis.ca.us>

subset( DM, "B" != x )

This is covered in the Introduction to R document that comes with R.
-- 
Sent from my phone. Please excuse my brevity.

On December 6, 2017 3:21:12 PM PST, David Winsemius <dwinsemius at comcast.net> wrote:
>
>> On Dec 6, 2017, at 3:15 PM, Ashta <sewashm at gmail.com> wrote:
>> 
>> Hi all,
>> In a data set I have group(GR) and two variables   x and y. I want to
>> remove a  group that have  the same record for the x variable in each
>> row.
>> 
>> DM <- read.table( text='GR x y
>> A 25 125
>> A 23 135
>> A 14 145
>> A 12 230
>> B 25 321
>> B 25 512
>> B 25 123
>> B 25 451
>> C 11 521
>> C 14 235
>> C 15 258
>> C 10 654',header = TRUE, stringsAsFactors = FALSE)
>> 
>> In this example the output should contain group A and C  as group B
>> has   the same record  for the variable x .
>> 
>> The result will be
>> A 25 125
>> A 23 135
>> A 14 145
>> A 12 230
>> C 11 521
>> C 14 235
>> C 15 258
>> C 10 654
>
>Try:
>
>DM[ !duplicated(DM$x) , ]
>> 
>> How do I do it R?
>> Thank you.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>David Winsemius
>Alameda, CA, USA
>
>'Any technology distinguishable from magic is insufficiently advanced.'
>  -Gehm's Corollary to Clarke's Third Law
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From sewashm at gmail.com  Thu Dec  7 00:58:22 2017
From: sewashm at gmail.com (Ashta)
Date: Wed, 6 Dec 2017 17:58:22 -0600
Subject: [R] Remove
In-Reply-To: <3296595B-6750-425D-9C2E-AD1697307689@dcn.davis.ca.us>
References: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>
 <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>
 <3296595B-6750-425D-9C2E-AD1697307689@dcn.davis.ca.us>
Message-ID: <CADDFq31sV_u3NLhU=T-ko=71abBdN4A8LaG8otMtsBwanQBHfQ@mail.gmail.com>

Thank you Jeff,

subset( DM, "B" != x ), this works if I know the group only.
But if I don't know that group in this case "B", how do I identify
group(s) that  all elements of x have the same value?

On Wed, Dec 6, 2017 at 5:48 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> subset( DM, "B" != x )
>
> This is covered in the Introduction to R document that comes with R.
> --
> Sent from my phone. Please excuse my brevity.
>
> On December 6, 2017 3:21:12 PM PST, David Winsemius <dwinsemius at comcast.net> wrote:
>>
>>> On Dec 6, 2017, at 3:15 PM, Ashta <sewashm at gmail.com> wrote:
>>>
>>> Hi all,
>>> In a data set I have group(GR) and two variables   x and y. I want to
>>> remove a  group that have  the same record for the x variable in each
>>> row.
>>>
>>> DM <- read.table( text='GR x y
>>> A 25 125
>>> A 23 135
>>> A 14 145
>>> A 12 230
>>> B 25 321
>>> B 25 512
>>> B 25 123
>>> B 25 451
>>> C 11 521
>>> C 14 235
>>> C 15 258
>>> C 10 654',header = TRUE, stringsAsFactors = FALSE)
>>>
>>> In this example the output should contain group A and C  as group B
>>> has   the same record  for the variable x .
>>>
>>> The result will be
>>> A 25 125
>>> A 23 135
>>> A 14 145
>>> A 12 230
>>> C 11 521
>>> C 14 235
>>> C 15 258
>>> C 10 654
>>
>>Try:
>>
>>DM[ !duplicated(DM$x) , ]
>>>
>>> How do I do it R?
>>> Thank you.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>David Winsemius
>>Alameda, CA, USA
>>
>>'Any technology distinguishable from magic is insufficiently advanced.'
>>  -Gehm's Corollary to Clarke's Third Law
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.


From istazahn at gmail.com  Thu Dec  7 00:59:55 2017
From: istazahn at gmail.com (Ista Zahn)
Date: Wed, 6 Dec 2017 18:59:55 -0500
Subject: [R] Remove
In-Reply-To: <CADDFq31sV_u3NLhU=T-ko=71abBdN4A8LaG8otMtsBwanQBHfQ@mail.gmail.com>
References: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>
 <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>
 <3296595B-6750-425D-9C2E-AD1697307689@dcn.davis.ca.us>
 <CADDFq31sV_u3NLhU=T-ko=71abBdN4A8LaG8otMtsBwanQBHfQ@mail.gmail.com>
Message-ID: <CA+vqiLG+Tx_73yR=M8uYM4toTXO02CDK3t0yK2AG_xn0uxrkhw@mail.gmail.com>

Hi Ashta,

There are many ways to do it. Here is one:

vars <- sapply(split(DM$x, DM$GR), var)
DM[DM$GR %in% names(vars[vars > 0]), ]

Best
Ista

On Wed, Dec 6, 2017 at 6:58 PM, Ashta <sewashm at gmail.com> wrote:
> Thank you Jeff,
>
> subset( DM, "B" != x ), this works if I know the group only.
> But if I don't know that group in this case "B", how do I identify
> group(s) that  all elements of x have the same value?
>
> On Wed, Dec 6, 2017 at 5:48 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>> subset( DM, "B" != x )
>>
>> This is covered in the Introduction to R document that comes with R.
>> --
>> Sent from my phone. Please excuse my brevity.
>>
>> On December 6, 2017 3:21:12 PM PST, David Winsemius <dwinsemius at comcast.net> wrote:
>>>
>>>> On Dec 6, 2017, at 3:15 PM, Ashta <sewashm at gmail.com> wrote:
>>>>
>>>> Hi all,
>>>> In a data set I have group(GR) and two variables   x and y. I want to
>>>> remove a  group that have  the same record for the x variable in each
>>>> row.
>>>>
>>>> DM <- read.table( text='GR x y
>>>> A 25 125
>>>> A 23 135
>>>> A 14 145
>>>> A 12 230
>>>> B 25 321
>>>> B 25 512
>>>> B 25 123
>>>> B 25 451
>>>> C 11 521
>>>> C 14 235
>>>> C 15 258
>>>> C 10 654',header = TRUE, stringsAsFactors = FALSE)
>>>>
>>>> In this example the output should contain group A and C  as group B
>>>> has   the same record  for the variable x .
>>>>
>>>> The result will be
>>>> A 25 125
>>>> A 23 135
>>>> A 14 145
>>>> A 12 230
>>>> C 11 521
>>>> C 14 235
>>>> C 15 258
>>>> C 10 654
>>>
>>>Try:
>>>
>>>DM[ !duplicated(DM$x) , ]
>>>>
>>>> How do I do it R?
>>>> Thank you.
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>David Winsemius
>>>Alameda, CA, USA
>>>
>>>'Any technology distinguishable from magic is insufficiently advanced.'
>>>  -Gehm's Corollary to Clarke's Third Law
>>>
>>>______________________________________________
>>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide
>>>http://www.R-project.org/posting-guide.html
>>>and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sewashm at gmail.com  Thu Dec  7 01:27:11 2017
From: sewashm at gmail.com (Ashta)
Date: Wed, 6 Dec 2017 18:27:11 -0600
Subject: [R] Remove
In-Reply-To: <CA+vqiLG+Tx_73yR=M8uYM4toTXO02CDK3t0yK2AG_xn0uxrkhw@mail.gmail.com>
References: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>
 <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>
 <3296595B-6750-425D-9C2E-AD1697307689@dcn.davis.ca.us>
 <CADDFq31sV_u3NLhU=T-ko=71abBdN4A8LaG8otMtsBwanQBHfQ@mail.gmail.com>
 <CA+vqiLG+Tx_73yR=M8uYM4toTXO02CDK3t0yK2AG_xn0uxrkhw@mail.gmail.com>
Message-ID: <CADDFq30GCH8r=tE4UW08kpSjyym2J-USvVugLUhNMsk3tED_MQ@mail.gmail.com>

Thank you Ista! Worked fine.

On Wed, Dec 6, 2017 at 5:59 PM, Ista Zahn <istazahn at gmail.com> wrote:
> Hi Ashta,
>
> There are many ways to do it. Here is one:
>
> vars <- sapply(split(DM$x, DM$GR), var)
> DM[DM$GR %in% names(vars[vars > 0]), ]
>
> Best
> Ista
>
> On Wed, Dec 6, 2017 at 6:58 PM, Ashta <sewashm at gmail.com> wrote:
>> Thank you Jeff,
>>
>> subset( DM, "B" != x ), this works if I know the group only.
>> But if I don't know that group in this case "B", how do I identify
>> group(s) that  all elements of x have the same value?
>>
>> On Wed, Dec 6, 2017 at 5:48 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>> subset( DM, "B" != x )
>>>
>>> This is covered in the Introduction to R document that comes with R.
>>> --
>>> Sent from my phone. Please excuse my brevity.
>>>
>>> On December 6, 2017 3:21:12 PM PST, David Winsemius <dwinsemius at comcast.net> wrote:
>>>>
>>>>> On Dec 6, 2017, at 3:15 PM, Ashta <sewashm at gmail.com> wrote:
>>>>>
>>>>> Hi all,
>>>>> In a data set I have group(GR) and two variables   x and y. I want to
>>>>> remove a  group that have  the same record for the x variable in each
>>>>> row.
>>>>>
>>>>> DM <- read.table( text='GR x y
>>>>> A 25 125
>>>>> A 23 135
>>>>> A 14 145
>>>>> A 12 230
>>>>> B 25 321
>>>>> B 25 512
>>>>> B 25 123
>>>>> B 25 451
>>>>> C 11 521
>>>>> C 14 235
>>>>> C 15 258
>>>>> C 10 654',header = TRUE, stringsAsFactors = FALSE)
>>>>>
>>>>> In this example the output should contain group A and C  as group B
>>>>> has   the same record  for the variable x .
>>>>>
>>>>> The result will be
>>>>> A 25 125
>>>>> A 23 135
>>>>> A 14 145
>>>>> A 12 230
>>>>> C 11 521
>>>>> C 14 235
>>>>> C 15 258
>>>>> C 10 654
>>>>
>>>>Try:
>>>>
>>>>DM[ !duplicated(DM$x) , ]
>>>>>
>>>>> How do I do it R?
>>>>> Thank you.
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>David Winsemius
>>>>Alameda, CA, USA
>>>>
>>>>'Any technology distinguishable from magic is insufficiently advanced.'
>>>>  -Gehm's Corollary to Clarke's Third Law
>>>>
>>>>______________________________________________
>>>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide
>>>>http://www.R-project.org/posting-guide.html
>>>>and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Thu Dec  7 05:34:25 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 6 Dec 2017 20:34:25 -0800
Subject: [R] Remove
In-Reply-To: <CADDFq30GCH8r=tE4UW08kpSjyym2J-USvVugLUhNMsk3tED_MQ@mail.gmail.com>
References: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>
 <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>
 <3296595B-6750-425D-9C2E-AD1697307689@dcn.davis.ca.us>
 <CADDFq31sV_u3NLhU=T-ko=71abBdN4A8LaG8otMtsBwanQBHfQ@mail.gmail.com>
 <CA+vqiLG+Tx_73yR=M8uYM4toTXO02CDK3t0yK2AG_xn0uxrkhw@mail.gmail.com>
 <CADDFq30GCH8r=tE4UW08kpSjyym2J-USvVugLUhNMsk3tED_MQ@mail.gmail.com>
Message-ID: <FD98DBA2-0D1A-4958-AC3F-B06469E28596@comcast.net>


> On Dec 6, 2017, at 4:27 PM, Ashta <sewashm at gmail.com> wrote:
> 
> Thank you Ista! Worked fine.

Here's another (possibly more direct in its logic?):

 DM[ !ave(DM$x, DM$GR, FUN= function(x) {!length(unique(x))==1}), ]
  GR  x   y
5  B 25 321
6  B 25 512
7  B 25 123
8  B 25 451

-- 
David

> On Wed, Dec 6, 2017 at 5:59 PM, Ista Zahn <istazahn at gmail.com> wrote:
>> Hi Ashta,
>> 
>> There are many ways to do it. Here is one:
>> 
>> vars <- sapply(split(DM$x, DM$GR), var)
>> DM[DM$GR %in% names(vars[vars > 0]), ]
>> 
>> Best
>> Ista
>> 
>> On Wed, Dec 6, 2017 at 6:58 PM, Ashta <sewashm at gmail.com> wrote:
>>> Thank you Jeff,
>>> 
>>> subset( DM, "B" != x ), this works if I know the group only.
>>> But if I don't know that group in this case "B", how do I identify
>>> group(s) that  all elements of x have the same value?
>>> 
>>> On Wed, Dec 6, 2017 at 5:48 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>> subset( DM, "B" != x )
>>>> 
>>>> This is covered in the Introduction to R document that comes with R.
>>>> --
>>>> Sent from my phone. Please excuse my brevity.
>>>> 
>>>> On December 6, 2017 3:21:12 PM PST, David Winsemius <dwinsemius at comcast.net> wrote:
>>>>> 
>>>>>> On Dec 6, 2017, at 3:15 PM, Ashta <sewashm at gmail.com> wrote:
>>>>>> 
>>>>>> Hi all,
>>>>>> In a data set I have group(GR) and two variables   x and y. I want to
>>>>>> remove a  group that have  the same record for the x variable in each
>>>>>> row.
>>>>>> 
>>>>>> DM <- read.table( text='GR x y
>>>>>> A 25 125
>>>>>> A 23 135
>>>>>> A 14 145
>>>>>> A 12 230
>>>>>> B 25 321
>>>>>> B 25 512
>>>>>> B 25 123
>>>>>> B 25 451
>>>>>> C 11 521
>>>>>> C 14 235
>>>>>> C 15 258
>>>>>> C 10 654',header = TRUE, stringsAsFactors = FALSE)
>>>>>> 
>>>>>> In this example the output should contain group A and C  as group B
>>>>>> has   the same record  for the variable x .
>>>>>> 
>>>>>> The result will be
>>>>>> A 25 125
>>>>>> A 23 135
>>>>>> A 14 145
>>>>>> A 12 230
>>>>>> C 11 521
>>>>>> C 14 235
>>>>>> C 15 258
>>>>>> C 10 654
>>>>> 
>>>>> Try:
>>>>> 
>>>>> DM[ !duplicated(DM$x) , ]
>>>>>> 
>>>>>> How do I do it R?
>>>>>> Thank you.
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>>> David Winsemius
>>>>> Alameda, CA, USA
>>>>> 
>>>>> 'Any technology distinguishable from magic is insufficiently advanced.'
>>>>> -Gehm's Corollary to Clarke's Third Law
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From kpmainali at gmail.com  Thu Dec  7 07:03:46 2017
From: kpmainali at gmail.com (Kumar Mainali)
Date: Thu, 7 Dec 2017 01:03:46 -0500
Subject: [R] parallel computing with foreach()
Message-ID: <CABK368hj9eBM3GOMr=X=EFw8D49F4ks5_2Szs5Cvt4v-07YM5g@mail.gmail.com>

I have used foreach() for parallel computing but in the current problem, it
is not working. Given the volume and type of the data involved in the
analysis, I will try to give below the complete code without reproducible
example.

In short, each R environment will draw a set of separate files, perform the
analysis and dump in separate folders.

splist <- c("juoc", "juos", "jusc", "pico", "pifl", "pipo", "pire", "psme")
covset <- c("PEN", "Thorn")

foreach(i = 1:length(splist)) %:%
foreach(j = 1:length(covset)) %dopar% {

spname <- splist[i]; spname
myTorP <- covset[j]; myTorP

DataSpecies = data.frame(prsabs = rep(1, 10), lon = rep(30, 10), lat =
rep(80, 10))
myResp = as.numeric(DataSpecies[,1])
myRespXY = DataSpecies[, c("lon", "lat")]
# directory of a bunch of raster files specific to each R environment
rastdir <- paste0(rootdir, "Current/", myTorP); rastdir
rasterc = list.files(rastdir, pattern="\\.tif$", full.names = T)
print(rasterc)
myExplc = stack(rasterc, RAT=FALSE)
}

I get the following error message that most likely generates while stacking
rasters because there are 25 rasters in the folder of each environment.
Also, in the normal for loop, this reads all fine.
Error in { :
  task 1 failed - "arguments imply differing number of rows: 25, 0"

Thank you.
?

	[[alternative HTML version deleted]]


From peter.langfelder at gmail.com  Thu Dec  7 07:52:03 2017
From: peter.langfelder at gmail.com (Peter Langfelder)
Date: Wed, 6 Dec 2017 22:52:03 -0800
Subject: [R] parallel computing with foreach()
In-Reply-To: <CABK368hj9eBM3GOMr=X=EFw8D49F4ks5_2Szs5Cvt4v-07YM5g@mail.gmail.com>
References: <CABK368hj9eBM3GOMr=X=EFw8D49F4ks5_2Szs5Cvt4v-07YM5g@mail.gmail.com>
Message-ID: <CA+hbrhWR=+r1vJPYSdSqgRj+Q8cuDYctnn7NjtC=0avSkZT_hw@mail.gmail.com>

Your code generates an error that has nothing to do with dopar. I have
no idea what your function stack is supposed to do; you may be
inadvertently calling utils::stack which would produce this kind of
error:

> stack(1:25, RAT = FALSE)
Error in data.frame(values = unlist(unname(x)), ind, stringsAsFactors = FALSE) :
  arguments imply differing number of rows: 25, 0

HTH,

Peter

On Wed, Dec 6, 2017 at 10:03 PM, Kumar Mainali <kpmainali at gmail.com> wrote:
> I have used foreach() for parallel computing but in the current problem, it
> is not working. Given the volume and type of the data involved in the
> analysis, I will try to give below the complete code without reproducible
> example.
>
> In short, each R environment will draw a set of separate files, perform the
> analysis and dump in separate folders.
>
> splist <- c("juoc", "juos", "jusc", "pico", "pifl", "pipo", "pire", "psme")
> covset <- c("PEN", "Thorn")
>
> foreach(i = 1:length(splist)) %:%
> foreach(j = 1:length(covset)) %dopar% {
>
> spname <- splist[i]; spname
> myTorP <- covset[j]; myTorP
>
> DataSpecies = data.frame(prsabs = rep(1, 10), lon = rep(30, 10), lat =
> rep(80, 10))
> myResp = as.numeric(DataSpecies[,1])
> myRespXY = DataSpecies[, c("lon", "lat")]
> # directory of a bunch of raster files specific to each R environment
> rastdir <- paste0(rootdir, "Current/", myTorP); rastdir
> rasterc = list.files(rastdir, pattern="\\.tif$", full.names = T)
> print(rasterc)
> myExplc = stack(rasterc, RAT=FALSE)
> }
>
> I get the following error message that most likely generates while stacking
> rasters because there are 25 rasters in the folder of each environment.
> Also, in the normal for loop, this reads all fine.
> Error in { :
>   task 1 failed - "arguments imply differing number of rows: 25, 0"
>
> Thank you.
> ?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From b.h.mevik at usit.uio.no  Thu Dec  7 08:02:30 2017
From: b.h.mevik at usit.uio.no (=?utf-8?Q?Bj=C3=B8rn-Helge_Mevik?=)
Date: Thu, 07 Dec 2017 08:02:30 +0100
Subject: [R] PLS in R
In-Reply-To: <CACtBhRNLBzSzbgWEwCQONLDxKXv8U4DoF76hqa7i75XGgmkSPQ@mail.gmail.com>
 (Margarida Soares's message of "Tue, 5 Dec 2017 11:30:06 +0100")
References: <CACtBhRNLBzSzbgWEwCQONLDxKXv8U4DoF76hqa7i75XGgmkSPQ@mail.gmail.com>
Message-ID: <s3s1sk7hvg9.fsf@varelg.uio.no>

Margarida Soares <margaridapmsoares at gmail.com> writes:

> library(pls)
> plsrcue<- plsr(cue~fb+cn+n+ph+fung+bact+resp, data = cue, ncomp=7,
> na.action = NULL, method = "kernelpls", scale=FALSE, validation = "LOO",
> model = TRUE, x = FALSE, y = FALSE)
> summary(plsrcue)
>
> and I got this output, where I think I can choose the number of components
> based on RMSEP, but how do I choose it?

There are no "hard" rules for how to choose the number of components,
but one rule of thumb is to stop when the RMSEP starts to flatten out,
or to increase.  In your case, I would say 4 components.  An easier way
to look at the RMSEP values is with plot(RMSEP(plsrcue)).

(There are some algorithms that can suggest the number of components for
you.  Two of those are implemented in the development of the plsr
package (hopefully released during Christmas).  You can check it out
here if you wish: https://github.com/bhmevik/pls .  Disclaimer: I am the
maintainer of the package. :) )

> - and also, how to proceed from here?

That depends on what you want to do/learn about the system you
aremodelling.  Many researchers in fields like spectroscopy or
chemometrics (where PLSR originated) plot loadings and scores and infer
things graphically.)

> - and how to make a correlation plot?

corrplot(plsrcue) - at least if you mean a correlation loadings plot.
See ?corrplot for details

> - what to do with the values, coefficients that I get in the Environment
> (pls values)

Again, that depends on what you want with your model.

-- 
Regards,
Bj?rn-Helge Mevik
-------------- neste del --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 800 bytes
Desc: ikke tilgjengelig
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20171207/bd29fd10/attachment.sig>

From petr.pikal at precheza.cz  Thu Dec  7 09:57:59 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 7 Dec 2017 08:57:59 +0000
Subject: [R] Dynamic reference, right-hand side of function
In-Reply-To: <53a206a2b0f64b3788aab0b542c09d8d@ebox-prod-srv09.win.su.se>
References: <d200347436c940f695583220f01ac86c@ebox-prod-srv09.win.su.se>
 <368012C4-4CB5-406E-84CE-019D0986CC99@gmail.com>
 <dd6a7dcd237542eebeb064aa3493c7bf@ebox-prod-srv09.win.su.se>
 <816C0DF8-27F5-443A-80F4-8FE9633CA648@gmail.com>
 <283ecb0b898e414295cc21d47c8584ce@ebox-prod-srv09.win.su.se>
 <alpine.BSF.2.00.1712041430410.70142@pedal.dcn.davis.ca.us>
 <53a206a2b0f64b3788aab0b542c09d8d@ebox-prod-srv09.win.su.se>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF880155FC97DF@SRVEXCHCM301.precheza.cz>

Hi

I am just curious why not to set proper form of object directly when you read it to R?

Based on your example

aa_2000 <- as.vector(as.matrix(read.csv(text="1,0,1,1,0,0,0,0,0,0,1,0,0", header=FALSE)))

or perhaps

> aa_2000<-unlist(read.csv(text="1,0,1,1,0,0,0,0,0,0,1,0,0", header=FALSE))
> is.vector(aa_2000)
[1] TRUE

But as others pointed out you should understand what are R objects and how you could use them.

When you want to read several files to R, my concept would be to put them in separate directory, change the working directory in R to this directory with my files,

?setwd

make vector of file names
myfiles <- list.files()

?list.files

and iterate through this vector of files

for (i in myfiles) {

do read stuff
do transform stuff
do concatenation stuff
}

to end with one object (list, data.frame, ...) which is suitable for further analysis and/or manipulation.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Love Bohman
> Sent: Tuesday, December 5, 2017 1:23 AM
> To: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> Cc: r-help at r-project.org; peter dalgaard <pdalgd at gmail.com>
> Subject: Re: [R] Dynamic reference, right-hand side of function
>
> Hi again!
> I know you don't find loops evil (well, at least not diabolic :-) ). (After many
> hours googling I have realized that thinking about loops rather than lists is a
> newbie thing we Stata-users do, I just jokingly pointed it out). Anyway, I'm
> really happy that you try to teach me some R-manners. Since I still get
> questions about what the h**k I mean by my strange question, I sort it out with
> an example:
>
> I had a number of matrices, named in a consecutive manner:
>
> aa_2000 <- as.matrix(read.csv(text="1,0,1,1,0,0,0,0,0,0,1,0,0", header=FALSE))
> aa_2001 <- as.matrix(read.csv( text="0,0,0,1,0,1,1,0,0,0,0,1,0,0",
> header=FALSE))
> aa_2002 <- as.matrix(read.csv( text="1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0",
> header=FALSE))
>
> I needed them to be vectors, and they weren't:
>
> is.vector(aa_2000)
>
> I finally solved it with this loop (well, I admit I shaped up the last line thanks to
> William Dunlap):
>
> for (year in 2000:2002){
> varname <- paste0("aa_",year)
> assign(varname, as.vector(eval(as.name(varname))))
> }
>
> The loop obviously solved the problem:
>
> is.vector(aa_2000)
>
> However, you have taught me that I should have solved it more elegant with a
> data list:
>
> bb_2000 <- as.matrix(read.csv(text="1,0,1,1,0,0,0,0,0,0,1,0,0", header=FALSE))
> bb_2001 <- as.matrix(read.csv( text="0,0,0,1,0,1,1,0,0,0,0,1,0,0",
> header=FALSE))
> bb_2002 <- as.matrix(read.csv( text="1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0",
> header=FALSE))
>
> is.vector(bb_2000)
>
> datanames <- paste0("bb_", 2000:2002)
> datalist <- lapply(datanames, get)
> is.vector(datalist[1])
>
>
> I learned a lot of code today, and I really appreciate it! A million thanks!
> My R-superpowers are, well, not as minuscule as when I woke up this morning.
>
> All the best,
> Love (or maybe LoveR, my future superhero name)
>
>
>
> -----Ursprungligt meddelande-----
> Fr?n: Jeff Newmiller [mailto:jdnewmil at dcn.davis.ca.us]
> Skickat: den 5 december 2017 00:01
> Till: Love Bohman <love.bohman at sociology.su.se>
> Kopia: peter dalgaard <pdalgd at gmail.com>; r-help at r-project.org
> ?mne: Re: [R] Dynamic reference, right-hand side of function
>
> Loops are not evil, and no-one in this thread said they are. But I believe your
> failure to provide a reproducible example is creating confusion, since you may
> be using words that mean one thing to you and something else to the readers
> here.
>
> ################################
> # A reproducible example includes a tiny set of sample data # Since we cannot
> reproducibly refer to filenames (your directories # and files in them are unlikely
> to be like mine), I will # use a little trick to read from data in the example:
>
> dta <- read.csv( text=
> "1.1,3.0,5,7.4,4,2.2,0
> ", header=FALSE)
> str(dta)
> #> 'data.frame':    1 obs. of  7 variables:
> #>  $ V1: num 1.1
> #>  $ V2: num 3
> #>  $ V3: int 5
> #>  $ V4: num 7.4
> #>  $ V5: int 4
> #>  $ V6: num 2.2
> #>  $ V7: int 0
>
> # note that I did not use "data" as the name because # there is a commonly-
> used function by that name in R # that could be confused with your variable
>
> # if you have your object already in memory, you can use the # dput function to
> create R code that will re-create it in our # working environments.
>
> dput( dta )
> #> structure(list(V1 = 1.1, V2 = 3, V3 = 5L, V4 = 7.4, V5 = 4L,
> #>     V6 = 2.2, V7 = 0L), .Names = c("V1", "V2", "V3", "V4", "V5",
> #> "V6", "V7"), class = "data.frame", row.names = c(NA, -1L))
>
> # which you can put a variable name in front of in your example code:
>
> dtasample <- structure(list( V1 = 1.1, V2 = 3, V3 = 5L , V4 = 7.4, V5 = 4L, V6 =
> 2.2, V7 = 0L ) , .Names = c( "V1"
> , "V2", "V3", "V4", "V5", "V6", "V7" ), class = "data.frame"
> , row.names = c(NA, -1L) )
>
> # and starting with that line you can make a self-contained # (reproducible)
> example for us to investigate your problem with
>
> # Note that reading a single row of data into R usually gets # a data frame,
> which looks like a matrix but is not a matrix.
> # Read the Introduction to R about these two types carefully.
> # Each column in a data frame can have a different type of data, # but in a
> vector or a matrix all rows and columns must be of # the same type.
>
> dtam <- as.matrix( dta )
>
> # If you have any values that R cannot clearly identify as numeric # or integer,
> then the next most general type of variable is # character... and that is often
> something that trips up newbies, # though I have no evidence that you have
> any non-numeric columns # in your data frames.
>
> dtax <- as.vector( dta )
> str(dtax)
> #> 'data.frame':    1 obs. of  7 variables:
> #>  $ V1: num 1.1
> #>  $ V2: num 3
> #>  $ V3: int 5
> #>  $ V4: num 7.4
> #>  $ V5: int 4
> #>  $ V6: num 2.2
> #>  $ V7: int 0
>
> # This actually makes no change to dta, because a data frame is already # a list
> of columns, and lists are just vectors that can hold different # types of
> variables, so dta is already a kind of vector.
>
> dtan <- as.numeric( dta )
> str(dtan)
> #>  num [1:7] 1.1 3 5 7.4 4 2.2 0
>
> # I suspect this is what you are trying to accomplish... but really, # if we had an
> example of the data you are working with, we would # already know.
> ################################
>
> Some more explanations of reproducibility [1][2][3]
>
> [1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-
> reproducible-example
>
> [2] http://adv-r.had.co.nz/Reproducibility.html
>
> [3] https://cran.r-project.org/web/packages/reprex/index.html (read the
> vignette)
>
>
> On Mon, 4 Dec 2017, Love Bohman wrote:
>
> > :-)
> > I don't insist on anything, I'm just struggling to learn a new language and
> partly a new way of thinking, and I really appreciate the corrections. I hope I
> someday will be able to handle lists in R as easy as I handle loops in Stata...
> > Thanks again!
> >
> > Love
> >
> >
> > -----Ursprungligt meddelande-----
> > Fr?n: peter dalgaard [mailto:pdalgd at gmail.com]
> > Skickat: den 4 december 2017 23:09
> > Till: Love Bohman <love.bohman at sociology.su.se>
> > Kopia: r-help at r-project.org
> > ?mne: Re: [R] Dynamic reference, right-hand side of function
> >
> > Um, if you insist on doing it that way, at least use
> >
> > assign(varname, as.vector(get(varname)))
> >
> > -pd
> >
> >> On 4 Dec 2017, at 22:46 , Love Bohman <love.bohman at sociology.su.se>
> wrote:
> >>
> >> Hi!
> >> Thanks for the replies!
> >> I understand people more accustomed to R doesn't like looping much, and
> that thinking about loops is something I do since I worked with Stata a lot. The
> syntax from Peter Dalgaard was really clever, and I learned a lot from it, even
> though it didn't solve my problem (I guess it wasn't very well explained). My
> problem was basically that I have a data matrix consisting of just 1 row, and I
> want to convert that row into a vector. However, when trying to do that
> dynamically, I couldn't get R to read the right hand side of the syntax as a
> variable name instead of a string. However, together with a colleague I finally
> solved it with the (eval(as.name()) function (I include the loop I used below). I
> understand that looping isn't kosher among you more devoted R-users, and
> eventually I hope I will learn to use lists in the future instead.
> >>
> >> Thanks!
> >> Love
> >>
> >>
> >> for (year in 2000:2007){
> >> varname <- paste0("aa_",year)
> >> assign(paste0(varname), as.vector(eval(as.name(varname))))
> >> }
> >>
> >> -----Ursprungligt meddelande-----
> >> Fr?n: peter dalgaard [mailto:pdalgd at gmail.com]
> >> Skickat: den 4 december 2017 16:39
> >> Till: Love Bohman <love.bohman at sociology.su.se>
> >> Kopia: r-help at r-project.org
> >> ?mne: Re: [R] Dynamic reference, right-hand side of function
> >>
> >> The generic rule is that R is not a macro language, so looping of names of
> things gets awkward. It is usually easier to use compound objects like lists and
> iterate over them. E.g.
> >>
> >> datanames <- paste0("aa_", 2000:2007) datalist <- lapply(datanames,
> >> get)
> >> names(datalist) <- datanames
> >> col1 <- lapply(datalist, "[[", 1)
> >> colnum <- lapply(col1, as.numeric)
> >>
> >> (The 2nd line assumes that the damage has already been done so that
> >> you have aa_2000 ... aa_2007 in your workspace. You might
> >> alternatively create the list directly while importing the data.)
> >>
> >> -pd
> >>
> >>> On 4 Dec 2017, at 12:33 , Love Bohman <love.bohman at sociology.su.se>
> wrote:
> >>>
> >>> Hi R-users!
> >>> Being new to R, and a fairly advanced Stata-user, I guess part of my
> problem is that my mindset (and probably my language as well) is wrong.
> Anyway, I have what I guess is a rather simple problem, that I now without
> success spent days trying to solve.
> >>>
> >>> I have a bunch of datasets imported from Stata that is labelled aa_2000
> aa_2001 aa_2002, etc. Each dataset is imported as a matrix, and consists of
> one column only. The columns consists of integer numbers. I need to convert
> the data to vectors, which I found several ways to do. I use, for example:
> >>> aa_2000 <- as.numeric(aa_2000[,1])
> >>> However, when trying to automate the task, so I don't have to write a line
> of code for each dataset, I get stuck. Since I'm a Stata user, my first attempt is
> trying to make a loop in order to loop over all datasets. However, I manage to
> write a loop that works for the left-hand side of the syntax, but not for the
> right-hand side.
> >>> I have included some examples from my struggles to solve the issue below,
> what they all have in common is that I don't manage to call for any "macro" (is
> that only a Stata-word?) in the right hand side of the functions. When I try to
> replace the static reference with a dynamic one (like in the left-hand side), the
> syntax just doesn't work.
> >>>
> >>> I would very much appreciate some help with this issue!
> >>> All the best,
> >>> Love
> >>>
> >>> year <- 2002
> >>> dataname <- paste0("aa_",year)
> >>> assign(paste0(dataname), as.numeric(aa_2002[,1]))
> >>>
> >>> year <- 2003
> >>> assign(paste0("aa_",year), as.numeric(aa_2003))
> >>>
> >>> year <- 2005
> >>> assign(paste0("aa_",year), aa_2005[,1])
> >>>
> >>> list1 <- c(2000:2007)
> >>> list1[c(7)]
> >>> assign(paste0("aa_",list1[c(7)]), as.numeric(paste0(aa_2006)))
> >>>
> >>>
> >>>   [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Peter Dalgaard, Professor,
> >> Center for Statistics, Copenhagen Business School Solbjerg Plads 3,
> >> 2000 Frederiksberg, Denmark
> >> Phone: (+45)38153501
> >> Office: A 4.23
> >> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >
> > --
> > Peter Dalgaard, Professor,
> > Center for Statistics, Copenhagen Business School Solbjerg Plads 3,
> > 2000 Frederiksberg, Denmark
> > Phone: (+45)38153501
> > Office: A 4.23
> > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >
> >
> >
> >
> >
> >
> >
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                        Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From topijush at gmail.com  Thu Dec  7 10:43:42 2017
From: topijush at gmail.com (Pijush Das)
Date: Thu, 7 Dec 2017 15:13:42 +0530
Subject: [R] Please to remove the warning when trying to check a package.
Message-ID: <CAGa91zgxJp9k99UBuCsWRnF6yZ7fSCgyX7kE7eiAvziQFyFDHQ@mail.gmail.com>

Hello Sir,



I have been trying to create a package in R.
When I have put the check option in R studio to check everything is ok that
time I have found
two warnings and three notes. Those are given below.

warnings:
1) * checking dependencies in R code ... WARNING
'::' or ':::' import not declared from: 'SparseM'
'loadNamespace' or 'requireNamespace' calls not declared from:
  'Matrix' 'SparseM'

2) * checking files in 'vignettes' ... WARNING
Files in the 'vignettes' directory but no files in 'inst/doc':
  'vignettes.Rmd', 'vignettes.pdf'

Notes:
1) * checking foreign function calls ... NOTE
Foreign function call to a different package:
  .C("svmpredict", ..., PACKAGE = "e1071")
See chapter 'System and foreign language interfaces' in the 'Writing R
Extensions' manual.

2) * checking R code for possible problems ... NOTE
predict.packageXXX: no visible global function definition for 'as'
predict.packageXXX: no visible global function definition for 'new'
Undefined global functions or variables:
  as new
Consider adding

3) * checking DESCRIPTION meta-information ... NOTE
Packages listed in more than one of Depends, Imports, Suggests, Enhances:
  'e1071' 'nlme' 'openxlsx' 'pheatmap' 'RColorBrewer' 'R.rsp'
A package should be listed in only one of these fields.



Can anybody help me to short out those problem please ?


Thank you very much.


regards
Pijush

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Thu Dec  7 15:13:26 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 7 Dec 2017 09:13:26 -0500
Subject: [R] Please to remove the warning when trying to check a package.
In-Reply-To: <CAGa91zgxJp9k99UBuCsWRnF6yZ7fSCgyX7kE7eiAvziQFyFDHQ@mail.gmail.com>
References: <CAGa91zgxJp9k99UBuCsWRnF6yZ7fSCgyX7kE7eiAvziQFyFDHQ@mail.gmail.com>
Message-ID: <97d57927-5cc6-041d-662f-9a6a8735b9d8@gmail.com>

R-help is the wrong list for questions about developing packages.  I'll 
send a reply to you and to R-package-devel, which is the right list.

Duncan Murdoch

On 07/12/2017 4:43 AM, Pijush Das wrote:
> Hello Sir,
> 
> 
> 
> I have been trying to create a package in R.
> When I have put the check option in R studio to check everything is ok that
> time I have found
> two warnings and three notes. Those are given below.
> 
> warnings:
> 1) * checking dependencies in R code ... WARNING
> '::' or ':::' import not declared from: 'SparseM'
> 'loadNamespace' or 'requireNamespace' calls not declared from:
>    'Matrix' 'SparseM'
> 
> 2) * checking files in 'vignettes' ... WARNING
> Files in the 'vignettes' directory but no files in 'inst/doc':
>    'vignettes.Rmd', 'vignettes.pdf'
> 
> Notes:
> 1) * checking foreign function calls ... NOTE
> Foreign function call to a different package:
>    .C("svmpredict", ..., PACKAGE = "e1071")
> See chapter 'System and foreign language interfaces' in the 'Writing R
> Extensions' manual.
> 
> 2) * checking R code for possible problems ... NOTE
> predict.packageXXX: no visible global function definition for 'as'
> predict.packageXXX: no visible global function definition for 'new'
> Undefined global functions or variables:
>    as new
> Consider adding
> 
> 3) * checking DESCRIPTION meta-information ... NOTE
> Packages listed in more than one of Depends, Imports, Suggests, Enhances:
>    'e1071' 'nlme' 'openxlsx' 'pheatmap' 'RColorBrewer' 'R.rsp'
> A package should be listed in only one of these fields.
> 
> 
> 
> Can anybody help me to short out those problem please ?
> 
> 
> Thank you very much.
> 
> 
> regards
> Pijush
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jfhenson1 at gmail.com  Thu Dec  7 19:47:14 2017
From: jfhenson1 at gmail.com (James Henson)
Date: Thu, 7 Dec 2017 12:47:14 -0600
Subject: [R] Error in loadNamespace
Message-ID: <CABPq8JPBML1fvrKFrRNfJpth_DJFfBYv612j1WE4VGEn88wOKg@mail.gmail.com>

Hello R Community,

I inadvertently updated packages via R Studio when a package was open.  Now
when R Studio is opened the message below appears in the console panel.

Error in loadNamespace(name) : there is no package called ?yaml?

Error in loadNamespace(name) : there is no package called ?yaml?

When running R code, so far the only function that has not worked is
datatable () in the ?DT? package.  Removing the ?DT? package and
reinstalling it had no effect.  A possible fix is to update R from an older
version, which was not running when I made the mistake.  Is this a good
idea?

Thanks for your help.

James F. Henson

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Thu Dec  7 20:19:46 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 7 Dec 2017 11:19:46 -0800
Subject: [R] Error in loadNamespace
In-Reply-To: <CABPq8JPBML1fvrKFrRNfJpth_DJFfBYv612j1WE4VGEn88wOKg@mail.gmail.com>
References: <CABPq8JPBML1fvrKFrRNfJpth_DJFfBYv612j1WE4VGEn88wOKg@mail.gmail.com>
Message-ID: <15C9B687-588F-47E7-B5E2-9ED5A8AEF921@comcast.net>


> On Dec 7, 2017, at 10:47 AM, James Henson <jfhenson1 at gmail.com> wrote:
> 
> Hello R Community,
> 
> I inadvertently updated packages via R Studio when a package was open.  Now
> when R Studio is opened the message below appears in the console panel.
> 
> Error in loadNamespace(name) : there is no package called ?yaml?
> 
> Error in loadNamespace(name) : there is no package called ?yaml?
> 
> When running R code, so far the only function that has not worked is
> datatable () in the ?DT? package.  Removing the ?DT? package and
> reinstalling it had no effect.  A possible fix is to update R from an older
> version, which was not running when I made the mistake.  Is this a good
> idea?

There is a package on CRAN named "yaml". Have your tried installing it?
> 
> Thanks for your help.
> 
> James F. Henson
> 
> 	[[alternative HTML version deleted]]

Rhelp is a plain text mailing list.

> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From jfhenson1 at gmail.com  Thu Dec  7 21:10:40 2017
From: jfhenson1 at gmail.com (James Henson)
Date: Thu, 7 Dec 2017 14:10:40 -0600
Subject: [R] Error in loadNamespace
In-Reply-To: <15C9B687-588F-47E7-B5E2-9ED5A8AEF921@comcast.net>
References: <CABPq8JPBML1fvrKFrRNfJpth_DJFfBYv612j1WE4VGEn88wOKg@mail.gmail.com>
 <15C9B687-588F-47E7-B5E2-9ED5A8AEF921@comcast.net>
Message-ID: <CABPq8JNvWMSwMhgqBXNjrbu5Dw3qqbs6J9ob--3reMZqp1oEpA@mail.gmail.com>

Thanks David!, that fixed the problem.
Best regards,
James

On Thu, Dec 7, 2017 at 1:19 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Dec 7, 2017, at 10:47 AM, James Henson <jfhenson1 at gmail.com> wrote:
> >
> > Hello R Community,
> >
> > I inadvertently updated packages via R Studio when a package was open.
> Now
> > when R Studio is opened the message below appears in the console panel.
> >
> > Error in loadNamespace(name) : there is no package called ?yaml?
> >
> > Error in loadNamespace(name) : there is no package called ?yaml?
> >
> > When running R code, so far the only function that has not worked is
> > datatable () in the ?DT? package.  Removing the ?DT? package and
> > reinstalling it had no effect.  A possible fix is to update R from an
> older
> > version, which was not running when I made the mistake.  Is this a good
> > idea?
>
> There is a package on CRAN named "yaml". Have your tried installing it?
> >
> > Thanks for your help.
> >
> > James F. Henson
> >
> >       [[alternative HTML version deleted]]
>
> Rhelp is a plain text mailing list.
>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> 'Any technology distinguishable from magic is insufficiently advanced.'
>  -Gehm's Corollary to Clarke's Third Law
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From davidsmi at microsoft.com  Thu Dec  7 22:44:10 2017
From: davidsmi at microsoft.com (David Smith (CLOUD AI))
Date: Thu, 7 Dec 2017 21:44:10 +0000
Subject: [R] Revolutions blog: November 2017 roundup
Message-ID: <BN6PR21MB0497C137CE3D2B98BE7D141CC8330@BN6PR21MB0497.namprd21.prod.outlook.com>

Since 2008, Microsoft (formerly Revolution Analytics) staff and guests have
written about R at the Revolutions blog (http://blog.revolutionanalytics.com)
and every month I post a summary of articles from the previous month of
particular interest to readers of r-help.

In case you missed them, here are some articles related to R from the
month of November:

R 3.4.3 "Kite Eating Tree" has been released:
http://blog.revolutionanalytics.com/2017/11/r-343-released.html

Several approaches for generating a "Secret Santa" list with R:
http://blog.revolutionanalytics.com/2017/11/how-to-generate-a-secret-santa-list-with-r.html

The "RevoScaleR" package from Microsoft R Server has now been ported to Python:
http://blog.revolutionanalytics.com/2017/11/revoscalepy.html

The call for papers for the R/Finance 2018 conference in Chicago is now open:
http://blog.revolutionanalytics.com/2017/11/rfinance-2018.html

Give thanks to the volunteers behind R:
http://blog.revolutionanalytics.com/2017/11/happy-thanksgiving.html

Advice for R user groups from the organizer of R-Ladies Chicago:
http://blog.revolutionanalytics.com/2017/11/r-ladies-chicago-part-1.html

Use containers to build R clusters for parallel workloads in Azure with the
doAzureParallel package:
http://blog.revolutionanalytics.com/2017/11/doazureparallel-containers.html 

A collection of R scripts for interesting visualizations that fit into a
280-character Tweet:
http://blog.revolutionanalytics.com/2017/11/charts-in-280-chars.html

R is featured in a StackOverflow case study at the Microsoft Connect conference:
http://blog.revolutionanalytics.com/2017/11/connect-highlights.html

The City of Chicago uses R to forecast water quality and issue beach safety
alerts: http://blog.revolutionanalytics.com/2017/11/chicago-water.html

A collection of best practices for sharing data in spreadsheets, from a paper by
Karl Broman and Kara Woo:
http://blog.revolutionanalytics.com/2017/11/good-practices-spreadsheets.html

The MRAN website has been updated with faster package search and other
improvements:
http://blog.revolutionanalytics.com/2017/11/an-update-for-mran.html

The curl package has been updated to use the built-in winSSL library on Windows:
http://blog.revolutionanalytics.com/2017/11/curl-updated.html

Beginner, intermediate and advanced on-line learning plans for developing AI
applications on Azure:
http://blog.revolutionanalytics.com/2017/11/azure-learning-plans.html

A recap of the EARL (Effective Applications of the R Language) conference in
Boston: http://blog.revolutionanalytics.com/2017/11/recap-earl-boston-2017.html

Giora Simchoni uses R to calculate the expected payout from a slot machine:
http://blog.revolutionanalytics.com/2017/11/slot-machine-edge.html  

An introductory R tutorial by Jesse Sadler focuses on the analysis of historical
documents: http://blog.revolutionanalytics.com/2017/11/r-excel-history.html

A new RStudio cheat sheet "Working with Strings":
http://blog.revolutionanalytics.com/2017/11/strings-in-r.html

An overview of generating distributions in R via simulated gaming dice:
http://blog.revolutionanalytics.com/2017/11/role-playing-with-probabilities.html

An analysis of StackOverflow survey data ranks R and Python among the most-liked
and least-disliked languages:
http://blog.revolutionanalytics.com/2017/11/r-is-the-least-disliked-programming-language.html

And some general interest stories (not necessarily related to R):

* Siri transcribes a trombone player:
  http://blog.revolutionanalytics.com/2017/11/because-its-friday-trombone.html

* A collection of short videos of interesting chemical reactions:
  http://blog.revolutionanalytics.com/2017/11/because-its-friday-chemical-reactions.html

* An animation shows the impact of a rogue drone on Gatwick airport:
  http://blog.revolutionanalytics.com/2017/11/how-a-drone-impacted-flight-paths.html  

* An AI sythesizes novel images of furniture, animals, and celebrities:
  http://blog.revolutionanalytics.com/2017/11/because-its-friday-fake-celebrities.html

As always, thanks for the comments and please keep sending suggestions to
me at davidsmi at microsoft.com or via Twitter (I'm @revodavid).

Cheers,
# David

-- 
David M Smith <davidsmi at microsoft.com>
R Community Lead, Microsoft AI & Research? 
Tel: +1 (312) 9205766 (Chicago IL, USA)
Twitter: @revodavid | Blog: ?http://blog.revolutionanalytics.com


From stsalwa at hotmail.com  Thu Dec  7 23:51:55 2017
From: stsalwa at hotmail.com (Stephanie Tsalwa)
Date: Thu, 7 Dec 2017 22:51:55 +0000
Subject: [R] Seeking help with code
Message-ID: <AM5P194MB0161F52E08A9BD67DBB1B71DA0330@AM5P194MB0161.EURP194.PROD.OUTLOOK.COM>

Assuming the days of raining during half a year of all states(provinces) of a country is normally distributed (mean=?, standard deviation=?) with sigma (?) equals to 2. We now have 10 data points here: 26.64, 30.65, 31.27, 33.04, 32.56, 29.10, 28.96, 26.44, 27.76, 32.27. Try to get the 95% level of CI for ?, using parametric Bootstrap method with bootstrap size B=8000.

my code - what am i doing wrong

#set sample size n, bootstrap size B
n = 10
b = 8000


set a vector of days of rain into "drain"
drain = c(26.64, 30.65, 31.27, 33.04, 32.56, 29.10, 28.96, 26.44, 27.76, 32.27)

#calculate mean of the sample for days of rain
mdr=mean(drain)
mdr

#calculate the parameter of the exponential distribution
lambdahat = 1.0/mdr
lambdahat

#draw the bootstrap sample from Exponential
x = rexp(n*b, lambdahat)
x

bootstrapsample = matrix(x, nrow=n, ncol=b)
bootstrapsample

# Compute the bootstrap lambdastar
lambdastar = 1.0/colMeans(bootstrapsample)
lambdastar

# Compute the differences
deltastar = lambdastar - lambdahat
deltastar

# Find the 0.05 and 0.95 quantile for deltastar
d = quantile(deltastar, c(0.05,0.95))
d

# Calculate the 95% confidence interval for lambda.
ci = lambdahat - c(d[2], d[1])
ci


	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Dec  8 00:24:25 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 7 Dec 2017 15:24:25 -0800
Subject: [R] Seeking help with code
In-Reply-To: <AM5P194MB0161F52E08A9BD67DBB1B71DA0330@AM5P194MB0161.EURP194.PROD.OUTLOOK.COM>
References: <AM5P194MB0161F52E08A9BD67DBB1B71DA0330@AM5P194MB0161.EURP194.PROD.OUTLOOK.COM>
Message-ID: <15D4963F-788A-4291-8268-106865D764EE@comcast.net>


> On Dec 7, 2017, at 2:51 PM, Stephanie Tsalwa <stsalwa at hotmail.com> wrote:
> 
> Assuming the days of raining during half a year of all states(provinces) of a country is normally distributed (mean=?, standard deviation=?) with sigma (?) equals to 2. We now have 10 data points here: 26.64, 30.65, 31.27, 33.04, 32.56, 29.10, 28.96, 26.44, 27.76, 32.27. Try to get the 95% level of CI for ?, using parametric Bootstrap method with bootstrap size B=8000.

This has the definite appearance of a homework assignment. It also has several other features which likewise suggest a failure to read the Posting Guide, a link to which is at the bottom of every posting sent out from the Rhelp mail-server.

You are advised to post credential/affiliation information and more context of the wider goals if my assumption of HW-status is incorrect.

-- 
David.

(And yes I do realize that I am also not posting any affiliation information, either, but I'm retired and am not asking for free advice, am I? Don't follow my bad example.)
> 
> my code - what am i doing wrong
> 
> #set sample size n, bootstrap size B
> n = 10
> b = 8000
> 
> 
> set a vector of days of rain into "drain"
> drain = c(26.64, 30.65, 31.27, 33.04, 32.56, 29.10, 28.96, 26.44, 27.76, 32.27)
> 
> #calculate mean of the sample for days of rain
> mdr=mean(drain)
> mdr
> 
> #calculate the parameter of the exponential distribution
> lambdahat = 1.0/mdr
> lambdahat
> 
> #draw the bootstrap sample from Exponential
> x = rexp(n*b, lambdahat)
> x
> 
> bootstrapsample = matrix(x, nrow=n, ncol=b)
> bootstrapsample
> 
> # Compute the bootstrap lambdastar
> lambdastar = 1.0/colMeans(bootstrapsample)
> lambdastar
> 
> # Compute the differences
> deltastar = lambdastar - lambdahat
> deltastar
> 
> # Find the 0.05 and 0.95 quantile for deltastar
> d = quantile(deltastar, c(0.05,0.95))
> d
> 
> # Calculate the 95% confidence interval for lambda.
> ci = lambdahat - c(d[2], d[1])
> ci
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From larry.martell at gmail.com  Fri Dec  8 02:29:05 2017
From: larry.martell at gmail.com (Larry Martell)
Date: Thu, 7 Dec 2017 20:29:05 -0500
Subject: [R] Preventing repeated package installation,
	or pre installing packages
In-Reply-To: <2AA35173-BBBF-4580-A366-16E14AC61EE2@krugs.de>
References: <CACwCsY4h5vKiHmpnhULXT=_sgJrGc6i+Tf9acCZdg4exXR1AbQ@mail.gmail.com>
 <2AA35173-BBBF-4580-A366-16E14AC61EE2@krugs.de>
Message-ID: <CACwCsY6Tgic1VkMuV6SC0B++uGKxEWDRr=HJko3Kj4Nkydg5hQ@mail.gmail.com>

On Wed, Nov 29, 2017 at 11:14 AM, Rainer Krug <Rainer at krugs.de> wrote:
>
>
> On 29 Nov 2017, at 15:28, Larry Martell <larry.martell at gmail.com> wrote:
>
> I have a R script that I call from python using rpy2. It uses dplyr, doBy,
> and ggplot2. The script has install.packages commands for these 3 packages.
> Even thought the packages are already installed it still downloads,
> builds, and installs them, which is very time consuming. Is there a way to
> have it only do the install if the package is not already installed?
>
>
> You could use something like
>
>
> if (!require(dplyr)) {
> install.packages(?dplyr?)
> library(dplyr)
> }
>
> where require() returns FALSE if it fails to load the package.

Thanks that worked perfectly.


> Also, I run in a docker container, so after the container is instantiated
> the packages are not there the first time the script runs. Is there a way
> to pre load the packages, in which case I would not need the
> install.packages commands for these packages and my above question would
> become moot.
>
>
> Yes - add them to you Docker file, but this is a docker question, not R.
> Check out the Rocker Dockerfiles to see how you can do this.

What I did not know was how to load a R package from the command line.
Thanks to Thierry Onkelinx's answer I now do know.


From larry.martell at gmail.com  Fri Dec  8 02:30:09 2017
From: larry.martell at gmail.com (Larry Martell)
Date: Thu, 7 Dec 2017 20:30:09 -0500
Subject: [R] Preventing repeated package installation,
	or pre installing packages
In-Reply-To: <CAJuCY5xZNOcpzD_DfQ6cd3ROH6hYJLx9fbsCpG7aVvN3PY9wtQ@mail.gmail.com>
References: <CACwCsY4h5vKiHmpnhULXT=_sgJrGc6i+Tf9acCZdg4exXR1AbQ@mail.gmail.com>
 <CAJuCY5xZNOcpzD_DfQ6cd3ROH6hYJLx9fbsCpG7aVvN3PY9wtQ@mail.gmail.com>
Message-ID: <CACwCsY7wLu672-F6LPLX7PK7=8V9qCKOt1Fwp3JL_0wpb+_j5A@mail.gmail.com>

On Wed, Nov 29, 2017 at 11:20 AM, Thierry Onkelinx
<thierry.onkelinx at inbo.be> wrote:
> Dear Larry,
>
> Have a look at https://github.com/inbo/rstable That is a dockerfile
> with a stable version of R and a set of packages.

Thank you very much. This is very useful to me.

> 2017-11-29 15:28 GMT+01:00 Larry Martell <larry.martell at gmail.com>:
>> I have a R script that I call from python using rpy2. It uses dplyr, doBy,
>> and ggplot2. The script has install.packages commands for these 3 packages.
>> Even thought the packages are already installed it still downloads,
>> builds, and installs them, which is very time consuming. Is there a way to
>> have it only do the install if the package is not already installed?
>>
>> Also, I run in a docker container, so after the container is instantiated
>> the packages are not there the first time the script runs. Is there a way
>> to pre load the packages, in which case I would not need the
>> install.packages commands for these packages and my above question would
>> become moot.
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From boris.steipe at utoronto.ca  Fri Dec  8 05:39:11 2017
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Fri, 8 Dec 2017 14:39:11 +1000
Subject: [R] Curiously short cycles in iterated permutations with the same
	seed
Message-ID: <B6A04D10-189B-4F99-8B37-D7697CD6E0F5@utoronto.ca>

I have noticed that when I iterate permutations of short vectors with the same seed, the cycle lengths are much shorter than I would expect by chance. For example:

X <- 1:10
Xorig <- X
start <- 112358
N <- 10

for (i in 1:N) {
  seed <- start + i
  for (j in 1:1000) { # Maximum cycle length to consider
    set.seed(seed)    # Re-seed RNG to same initial state
    X <- sample(X)    # Permute X and iterate
    if (all(X == Xorig)) {
      cat(sprintf("Seed:\t%d\tCycle: %d\n", seed, j))
      break()
    }
  }
}

Seed:	112359	Cycle: 14
Seed:	112360	Cycle: 14
Seed:	112361	Cycle: 8
Seed:	112362	Cycle: 14
Seed:	112363	Cycle: 8
Seed:	112364	Cycle: 10
Seed:	112365	Cycle: 10
Seed:	112366	Cycle: 10
Seed:	112367	Cycle: 9
Seed:	112368	Cycle: 12

I understand that I am performing the same permutation operation over and over again - but I don't see why that would lead to such a short cycle (in fact the cycle for the first 100,000 seeds is never longer than 30). Does this have a straightforward explanation?


Thanks!
Boris


From kpmainali at gmail.com  Fri Dec  8 06:11:48 2017
From: kpmainali at gmail.com (Kumar Mainali)
Date: Fri, 8 Dec 2017 00:11:48 -0500
Subject: [R] parallel computing with foreach()
In-Reply-To: <CA+hbrhWR=+r1vJPYSdSqgRj+Q8cuDYctnn7NjtC=0avSkZT_hw@mail.gmail.com>
References: <CABK368hj9eBM3GOMr=X=EFw8D49F4ks5_2Szs5Cvt4v-07YM5g@mail.gmail.com>
 <CA+hbrhWR=+r1vJPYSdSqgRj+Q8cuDYctnn7NjtC=0avSkZT_hw@mail.gmail.com>
Message-ID: <CABK368h57-p30F0F0PT7m3SCmNkCrBz24JMAtcAQ+YHLzRMCbQ@mail.gmail.com>

Thanks Peter. I failed to realize earlier that one of the functions I used
came from a package. The following solved the problem.

foreach(i = 1:length(splist)) %:%

foreach(j = 1:length(covset), .packages = c("raster")) %dopar% {

.......


On Thu, Dec 7, 2017 at 1:52 AM, Peter Langfelder <peter.langfelder at gmail.com
> wrote:

> Your code generates an error that has nothing to do with dopar. I have
> no idea what your function stack is supposed to do; you may be
> inadvertently calling utils::stack which would produce this kind of
> error:
>
> > stack(1:25, RAT = FALSE)
> Error in data.frame(values = unlist(unname(x)), ind, stringsAsFactors =
> FALSE) :
>   arguments imply differing number of rows: 25, 0
>
> HTH,
>
> Peter
>
> On Wed, Dec 6, 2017 at 10:03 PM, Kumar Mainali <kpmainali at gmail.com>
> wrote:
> > I have used foreach() for parallel computing but in the current problem,
> it
> > is not working. Given the volume and type of the data involved in the
> > analysis, I will try to give below the complete code without reproducible
> > example.
> >
> > In short, each R environment will draw a set of separate files, perform
> the
> > analysis and dump in separate folders.
> >
> > splist <- c("juoc", "juos", "jusc", "pico", "pifl", "pipo", "pire",
> "psme")
> > covset <- c("PEN", "Thorn")
> >
> > foreach(i = 1:length(splist)) %:%
> > foreach(j = 1:length(covset)) %dopar% {
> >
> > spname <- splist[i]; spname
> > myTorP <- covset[j]; myTorP
> >
> > DataSpecies = data.frame(prsabs = rep(1, 10), lon = rep(30, 10), lat =
> > rep(80, 10))
> > myResp = as.numeric(DataSpecies[,1])
> > myRespXY = DataSpecies[, c("lon", "lat")]
> > # directory of a bunch of raster files specific to each R environment
> > rastdir <- paste0(rootdir, "Current/", myTorP); rastdir
> > rasterc = list.files(rastdir, pattern="\\.tif$", full.names = T)
> > print(rasterc)
> > myExplc = stack(rasterc, RAT=FALSE)
> > }
> >
> > I get the following error message that most likely generates while
> stacking
> > rasters because there are 25 rasters in the folder of each environment.
> > Also, in the normal for loop, this reads all fine.
> > Error in { :
> >   task 1 failed - "arguments imply differing number of rows: 25, 0"
> >
> > Thank you.
> > ?
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>



-- 
Postdoctoral Associate
Department of Biology
University of Maryland, College Park
?

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Fri Dec  8 06:34:48 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Fri, 8 Dec 2017 07:34:48 +0200
Subject: [R] Curiously short cycles in iterated permutations with the
	same seed
In-Reply-To: <B6A04D10-189B-4F99-8B37-D7697CD6E0F5@utoronto.ca>
References: <B6A04D10-189B-4F99-8B37-D7697CD6E0F5@utoronto.ca>
Message-ID: <CAGgJW74J=u7=JjFPhfph_vMajujhUNS5vEcGvQ+h-MCS86yHHw@mail.gmail.com>

Hi Boris,
Do a search on "the order of elements of the symmetric group". (This search
will also turn up homework questions and solutions.) You will understand
why you are seeing this once you understand how a permutation is decomposed
into cycles and how the order relates to a partition of n (n=10 in your
case).

Enjoy!
Eric


On Fri, Dec 8, 2017 at 6:39 AM, Boris Steipe <boris.steipe at utoronto.ca>
wrote:

> I have noticed that when I iterate permutations of short vectors with the
> same seed, the cycle lengths are much shorter than I would expect by
> chance. For example:
>
> X <- 1:10
> Xorig <- X
> start <- 112358
> N <- 10
>
> for (i in 1:N) {
>   seed <- start + i
>   for (j in 1:1000) { # Maximum cycle length to consider
>     set.seed(seed)    # Re-seed RNG to same initial state
>     X <- sample(X)    # Permute X and iterate
>     if (all(X == Xorig)) {
>       cat(sprintf("Seed:\t%d\tCycle: %d\n", seed, j))
>       break()
>     }
>   }
> }
>
> Seed:   112359  Cycle: 14
> Seed:   112360  Cycle: 14
> Seed:   112361  Cycle: 8
> Seed:   112362  Cycle: 14
> Seed:   112363  Cycle: 8
> Seed:   112364  Cycle: 10
> Seed:   112365  Cycle: 10
> Seed:   112366  Cycle: 10
> Seed:   112367  Cycle: 9
> Seed:   112368  Cycle: 12
>
> I understand that I am performing the same permutation operation over and
> over again - but I don't see why that would lead to such a short cycle (in
> fact the cycle for the first 100,000 seeds is never longer than 30). Does
> this have a straightforward explanation?
>
>
> Thanks!
> Boris
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From sunscape1 at hotmail.com  Fri Dec  8 07:37:37 2017
From: sunscape1 at hotmail.com (Benjamin Sabatini)
Date: Fri, 8 Dec 2017 06:37:37 +0000
Subject: [R] trying to find the multiple combinations...
Message-ID: <BN6PR22MB0098C0690657E78C83601B0FF2300@BN6PR22MB0098.namprd22.prod.outlook.com>

Hi,

I'm trying to find a way to determine what multiples of the combination of three or more numbers equals a forth number.

So, if I had a number set like:

c(13.4689, 12.85212, 17.05071)

What combination and multiples of these numbers would average to 15.0078? (so, something that would tell me x, y, and z in (x*13.4689 + y*12.85212+ z*17.05071) / x+y+z) = 15.0078

I think this is doable with aggregate?

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Fri Dec  8 09:19:30 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 08 Dec 2017 00:19:30 -0800
Subject: [R] trying to find the multiple combinations...
In-Reply-To: <BN6PR22MB0098C0690657E78C83601B0FF2300@BN6PR22MB0098.namprd22.prod.outlook.com>
References: <BN6PR22MB0098C0690657E78C83601B0FF2300@BN6PR22MB0098.namprd22.prod.outlook.com>
Message-ID: <7A43238B-E099-4C0F-B4D3-3742AFFCD801@dcn.davis.ca.us>

Solve for one of your variables and it will be given in terms of the other two. That is, there is a whole infinite plane of solutions. No, aggregate will not be sufficient to enumerate the solution set.. 
-- 
Sent from my phone. Please excuse my brevity.

On December 7, 2017 10:37:37 PM PST, Benjamin Sabatini <sunscape1 at hotmail.com> wrote:
>Hi,
>
>I'm trying to find a way to determine what multiples of the combination
>of three or more numbers equals a forth number.
>
>So, if I had a number set like:
>
>c(13.4689, 12.85212, 17.05071)
>
>What combination and multiples of these numbers would average to
>15.0078? (so, something that would tell me x, y, and z in (x*13.4689 +
>y*12.85212+ z*17.05071) / x+y+z) = 15.0078
>
>I think this is doable with aggregate?
>
>	[[alternative HTML version deleted]]

This is a plain text mailing list. Please learn how to use your email program. 

>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Fri Dec  8 16:16:37 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 8 Dec 2017 07:16:37 -0800
Subject: [R] trying to find the multiple combinations...
In-Reply-To: <7A43238B-E099-4C0F-B4D3-3742AFFCD801@dcn.davis.ca.us>
References: <BN6PR22MB0098C0690657E78C83601B0FF2300@BN6PR22MB0098.namprd22.prod.outlook.com>
 <7A43238B-E099-4C0F-B4D3-3742AFFCD801@dcn.davis.ca.us>
Message-ID: <CAGxFJbRku9B_YTp=8xw44kicA=4mmEA_i8naSNb4EzNP+md+Kg@mail.gmail.com>

Are x,y, and z supposed to be positive whole numbers? If so, there may be
no solutions. If there is a solution set, of course any multiple of the set
is a solution set, so presumably you want a minimal set in some sense. This
strikes me as a hard problem mathematically, but maybe there is some
obvious way to set an upper bound on a minimal x,y, and z, in which case a
simple grid search could then be used.

Naturally, if any real numbers are sought, Jeff is correct.

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Fri, Dec 8, 2017 at 12:19 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Solve for one of your variables and it will be given in terms of the other
> two. That is, there is a whole infinite plane of solutions. No, aggregate
> will not be sufficient to enumerate the solution set..
> --
> Sent from my phone. Please excuse my brevity.
>
> On December 7, 2017 10:37:37 PM PST, Benjamin Sabatini <
> sunscape1 at hotmail.com> wrote:
> >Hi,
> >
> >I'm trying to find a way to determine what multiples of the combination
> >of three or more numbers equals a forth number.
> >
> >So, if I had a number set like:
> >
> >c(13.4689, 12.85212, 17.05071)
> >
> >What combination and multiples of these numbers would average to
> >15.0078? (so, something that would tell me x, y, and z in (x*13.4689 +
> >y*12.85212+ z*17.05071) / x+y+z) = 15.0078
> >
> >I think this is doable with aggregate?
> >
> >       [[alternative HTML version deleted]]
>
> This is a plain text mailing list. Please learn how to use your email
> program.
>
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Fri Dec  8 17:48:44 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 8 Dec 2017 08:48:44 -0800
Subject: [R] Curiously short cycles in iterated permutations with the
	same seed
In-Reply-To: <CAGgJW74J=u7=JjFPhfph_vMajujhUNS5vEcGvQ+h-MCS86yHHw@mail.gmail.com>
References: <B6A04D10-189B-4F99-8B37-D7697CD6E0F5@utoronto.ca>
 <CAGgJW74J=u7=JjFPhfph_vMajujhUNS5vEcGvQ+h-MCS86yHHw@mail.gmail.com>
Message-ID: <CAF8bMcb-NB7DpRH-936KEkbyXCyxNEmfdppDh2ZbSgR19H3Pxg@mail.gmail.com>

Landau's function gives the maximum cycle length of a permutation.
See.,e.g.,
http://mathworld.wolfram.com/LandausFunction.html.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Dec 7, 2017 at 9:34 PM, Eric Berger <ericjberger at gmail.com> wrote:

> Hi Boris,
> Do a search on "the order of elements of the symmetric group". (This search
> will also turn up homework questions and solutions.) You will understand
> why you are seeing this once you understand how a permutation is decomposed
> into cycles and how the order relates to a partition of n (n=10 in your
> case).
>
> Enjoy!
> Eric
>
>
> On Fri, Dec 8, 2017 at 6:39 AM, Boris Steipe <boris.steipe at utoronto.ca>
> wrote:
>
> > I have noticed that when I iterate permutations of short vectors with the
> > same seed, the cycle lengths are much shorter than I would expect by
> > chance. For example:
> >
> > X <- 1:10
> > Xorig <- X
> > start <- 112358
> > N <- 10
> >
> > for (i in 1:N) {
> >   seed <- start + i
> >   for (j in 1:1000) { # Maximum cycle length to consider
> >     set.seed(seed)    # Re-seed RNG to same initial state
> >     X <- sample(X)    # Permute X and iterate
> >     if (all(X == Xorig)) {
> >       cat(sprintf("Seed:\t%d\tCycle: %d\n", seed, j))
> >       break()
> >     }
> >   }
> > }
> >
> > Seed:   112359  Cycle: 14
> > Seed:   112360  Cycle: 14
> > Seed:   112361  Cycle: 8
> > Seed:   112362  Cycle: 14
> > Seed:   112363  Cycle: 8
> > Seed:   112364  Cycle: 10
> > Seed:   112365  Cycle: 10
> > Seed:   112366  Cycle: 10
> > Seed:   112367  Cycle: 9
> > Seed:   112368  Cycle: 12
> >
> > I understand that I am performing the same permutation operation over and
> > over again - but I don't see why that would lead to such a short cycle
> (in
> > fact the cycle for the first 100,000 seeds is never longer than 30). Does
> > this have a straightforward explanation?
> >
> >
> > Thanks!
> > Boris
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> > posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Fri Dec  8 18:47:59 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 8 Dec 2017 09:47:59 -0800
Subject: [R] trying to find the multiple combinations...
In-Reply-To: <BN6PR22MB0098AC7C5D2704E72E77E8DCF2300@BN6PR22MB0098.namprd22.prod.outlook.com>
References: <BN6PR22MB0098C0690657E78C83601B0FF2300@BN6PR22MB0098.namprd22.prod.outlook.com>
 <7A43238B-E099-4C0F-B4D3-3742AFFCD801@dcn.davis.ca.us>
 <CAGxFJbRku9B_YTp=8xw44kicA=4mmEA_i8naSNb4EzNP+md+Kg@mail.gmail.com>
 <BN6PR22MB0098AC7C5D2704E72E77E8DCF2300@BN6PR22MB0098.namprd22.prod.outlook.com>
Message-ID: <CAGxFJbRvD-LQGOYJsaj+gURtY83WxPmxoPCwW9hGnB=d49oZPA@mail.gmail.com>

Please keep all replies onlist if there is no reason to keep them private.
I am not a free, private consultant (and so might choose to ignore your
followups); and I don't have all or necessarily the best answers anyway. So
give yourself the maximal chance to be helped by others.

Anyway,

?expand.grid is what you're looking for I think as an alternative to nested
loops. If "results" is a vector of calculated results, i.e. the averages
for the grid of combinations you generate, and "target" is your desired
target (average), here of 15.0078, then

which.min(abs(results - target))
gives you the index of the closest results and

abs(results - target) < tol
gives you a vector of logicals of results that are within tol of the target.

This is all pretty basic stuff, which suggests that you really need to
spend some time with an R tutorial or two.  Here are some suggestions, but
a web search would uncover many more, some of which might be more suitable
for you:
https://www.rstudio.com/online-learning/#R

This list can help (not sure if I did here), but it cannot replace such
homework on your own.

Cheers,
Bert




Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Fri, Dec 8, 2017 at 9:15 AM, Benjamin Sabatini <sunscape1 at hotmail.com>
wrote:

> Hi,
>
> Yes, actually, I could set an upper limit and grind through the
> possibilities to find a minimal set or a few if that's what you mean. Close
> to the result would be OK, too. Otherwise it would go on forever, I
> suppose.
>
> At first I was thinking of just trying to write three for loops to test
> for every set of the multiples of x, y, and z between something like 1 and
> 10,000, but I understand that this is not at all efficient in R. So, (1*13.4689
> + 1*12.85212+ 1*17.05071) / 1+1+1), (1*13.4689 + 2*12.85212+ 1*17.05071)
> / 1+2+1)...
>
> Is there a better way? If I solve for z is it then easier with an upper
> limit? So, z = x*0.753288 + y*1.0552
> and then loop it?
>
> ------------------------------
> *From:* Bert Gunter <bgunter.4567 at gmail.com>
> *Sent:* Friday, December 8, 2017 3:16 PM
> *To:* Jeff Newmiller
> *Cc:* R-help; Benjamin Sabatini
> *Subject:* Re: [R] trying to find the multiple combinations...
>
> Are x,y, and z supposed to be positive whole numbers? If so, there may be
> no solutions. If there is a solution set, of course any multiple of the set
> is a solution set, so presumably you want a minimal set in some sense. This
> strikes me as a hard problem mathematically, but maybe there is some
> obvious way to set an upper bound on a minimal x,y, and z, in which case a
> simple grid search could then be used.
>
> Naturally, if any real numbers are sought, Jeff is correct.
>
> Cheers,
> Bert
>
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
> On Fri, Dec 8, 2017 at 12:19 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
>
> Solve for one of your variables and it will be given in terms of the other
> two. That is, there is a whole infinite plane of solutions. No, aggregate
> will not be sufficient to enumerate the solution set..
> --
> Sent from my phone. Please excuse my brevity.
>
> On December 7, 2017 10:37:37 PM PST, Benjamin Sabatini <
> sunscape1 at hotmail.com> wrote:
> >Hi,
> >
> >I'm trying to find a way to determine what multiples of the combination
> >of three or more numbers equals a forth number.
> >
> >So, if I had a number set like:
> >
> >c(13.4689, 12.85212, 17.05071)
> >
> >What combination and multiples of these numbers would average to
> >15.0078? (so, something that would tell me x, y, and z in (x*13.4689 +
> >y*12.85212+ z*17.05071) / x+y+z) = 15.0078
> >
> >I think this is doable with aggregate?
> >
> >       [[alternative HTML version deleted]]
>
> This is a plain text mailing list. Please learn how to use your email
> program.
>
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>

	[[alternative HTML version deleted]]


From woleogunjobi at gmail.com  Fri Dec  8 17:25:17 2017
From: woleogunjobi at gmail.com (woleogunjobi at gmail.com)
Date: Fri, 8 Dec 2017 16:25:17 +0000 (UTC)
Subject: [R] Elastic net
References: <1290271566.748913.1512750317302.ref@mail.yahoo.com>
Message-ID: <1290271566.748913.1512750317302@mail.yahoo.com>

Dear R users,?
? ? ? ? ? ? ? ? ? ? ? ? I am using "Glmnet" package in R for applying "elastic net" method. In elastic net, two penalities are applied one is lambda1 for?LASSO and lambda2 for ridge ( zou, 2005) penalty.?How can I? write the code to? pre-chose the? lambda1 for?LASSO and lambda2 for ridge without using cross-validation
Thanks in advance?

Tayo?


	[[alternative HTML version deleted]]


From esawiek at gmail.com  Fri Dec  8 19:37:50 2017
From: esawiek at gmail.com (Ek Esawi)
Date: Fri, 8 Dec 2017 13:37:50 -0500
Subject: [R] trying to find the multiple combinations...
In-Reply-To: <BN6PR22MB0098C0690657E78C83601B0FF2300@BN6PR22MB0098.namprd22.prod.outlook.com>
References: <BN6PR22MB0098C0690657E78C83601B0FF2300@BN6PR22MB0098.namprd22.prod.outlook.com>
Message-ID: <CA+ZkTxsrfkLh1czM4VPtpdtshvb11TXWSdWcahOdBYJvDn2c4w@mail.gmail.com>

As Burt and Jeff stated that there is an infinite set of solutions. If you
are interested in a particular solution, such as getting 15.0078, you can
easily achieve that by trial and error; that is fix 1 or 2 variables and
change the the rest till you get the desired solution.
I tried that and came up with the same solution as you listed. if you are
interested in a set of solutions within a tolerance, then that will be a
different story.


HTH
EK

On Fri, Dec 8, 2017 at 1:37 AM, Benjamin Sabatini <sunscape1 at hotmail.com>
wrote:

> Hi,
>
> I'm trying to find a way to determine what multiples of the combination of
> three or more numbers equals a forth number.
>
> So, if I had a number set like:
>
> c(13.4689, 12.85212, 17.05071)
>
> What combination and multiples of these numbers would average to 15.0078?
> (so, something that would tell me x, y, and z in (x*13.4689 + y*12.85212+
> z*17.05071) / x+y+z) = 15.0078
>
> I think this is doable with aggregate?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From neotropical.bats at gmail.com  Fri Dec  8 19:48:38 2017
From: neotropical.bats at gmail.com (Neotropical bat risk assessments)
Date: Fri, 8 Dec 2017 13:48:38 -0500
Subject: [R] Need help with labeling a plot correctly
In-Reply-To: <mailman.1.1512730801.23563.r-help@r-project.org>
References: <mailman.1.1512730801.23563.r-help@r-project.org>
Message-ID: <f808d58c-86ef-5670-bba7-f7db27c886ff@gmail.com>

Hi all,
Eons ago (~2007) Hadley Wickham extremely generously helped me developed 
a means for plotting temporal activity w/ ggplot.
I need to revisit this and have it working again.? I failed to copy the 
entire legacy code over to my traveling laptop from my workstation, but 
found a partial bit of the code in an old 2009 Gmail attachment.? I am 
slated to do a graduate seminar at UF G'ville and needed to discuss 
temporal activity.

I have been working my feeble brain again and revisiting the almost 
complete code and have it partially working by doing step by step 
sections of code and carefully making sure no typos that could screw up 
syntax.

The partial code Hadley developed for me is below and is now working 
more or less with R 3.4.3 (2017-11-30).
What I need to do is have the missing bit of code so that the Y- Axis of 
the plots has time of night? rather than the "elapsed seconds."
As a note this is for bat activity data so data points begin after 
sunset on a given night and end at sunrise the following day so 
date/times span 2 calender days with a midnight roll-over.

The code below was provided by Hadley and works except for the Y axis 
labeling with correct times.
I can email a sample of the raw data if anyone is able to help.

Cheers,

Bruce
++++++++++++++++++++++

library(zoo)
library(cluster)
library(lattice)
library(vegan)
library(chron)
library(MASS)
library(ggplot2)

# activity plots that Hadley Wickham helped develop back in ~2007
#New from Hadley
*s*etwd("C:/Bats/Temporal data")

#C:\Bats\Temporal data
source("date.r")
All <- read.csv("C:/Bats/Temporal data/TECTemporal.CSV")

 ?Ptepar <-subset(All, Species == "Ptepar")
str(Ptepar)

# Notice that date and time are factors - you need to convert them
# into dates.? This doesn't happen automatically in R.? Also, the syntax to
# make dates isn't exactly intuitive.

Ptepar$Date <- as.Date(Ptepar$Date, "%m/%d/%Y")

Ptepar$Time <- as.POSIXct(strptime(as.character(Ptepar$Time), "%H:%M"))
Ptepar$datetime <- as.POSIXct(strptime(paste(Ptepar$Date, 
format(Ptepar$Time,
"%H:%M")), "%Y-%m-%d %H:%M"))

# For a given time, give the time the evening started
# (this will be the previous day if the time is in the morning)
evening <- function(date) {
 ?update(date,
 ? ?yday = yday(date) - ifelse(am(date), 1, 0),
 ? ?hour = 18,
 ? ?minute = 0,
 ? ?second = 0)
}

# Now we calculate the number of seconds since the start of the evening
Ptepar$elapsed <- as.double(Ptepar$datetime - evening(Ptepar$datetime), 
"secs")
ggplot(Ptepar, aes(Date, elapsed, colour = Location)) +
 ?geom_jitter() +
 ?facet_wrap(~ Species)

The resulting plot appears fine however I need to have the Y axis 
display time of night rather than the elapsed seconds.





From andrewharmon42 at gmail.com  Fri Dec  8 23:30:09 2017
From: andrewharmon42 at gmail.com (Andrew Harmon)
Date: Fri, 8 Dec 2017 16:30:09 -0600
Subject: [R] lmerTest Issues
Message-ID: <CAB4=Ug=CSKW+-AOKZBkYNPAxdHDOBKamhJs836AffeQjh4OMXw@mail.gmail.com>

Hello all,

Everything was working very well. Now when I try to load lmerTest using:
library("lmerTest"), I get this error:

Error: package or namespace load failed for ?lmerTest? in
loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck =
vI[[j]]):
 there is no package called ?purrr?


Nothing I've done has worked. I have uninstalled both R and R studio. I've
updated packages as well, but nothing works.

Any suggestions?

Thanks,

Drew

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Sat Dec  9 01:02:12 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 08 Dec 2017 16:02:12 -0800
Subject: [R] lmerTest Issues
In-Reply-To: <CAB4=Ug=CSKW+-AOKZBkYNPAxdHDOBKamhJs836AffeQjh4OMXw@mail.gmail.com>
References: <CAB4=Ug=CSKW+-AOKZBkYNPAxdHDOBKamhJs836AffeQjh4OMXw@mail.gmail.com>
Message-ID: <1B482791-F2C1-4D2C-9BBA-0FB33F669041@dcn.davis.ca.us>

Have you read the error and installed package "purrr"? On Windows at least, previously-installed packages can get removed if you attempt to update them while you have another instance of R open at that time using said packages. Best to close all your instances of R before updating, but you can do that now to recover. 
-- 
Sent from my phone. Please excuse my brevity.

On December 8, 2017 2:30:09 PM PST, Andrew Harmon <andrewharmon42 at gmail.com> wrote:
>Hello all,
>
>Everything was working very well. Now when I try to load lmerTest
>using:
>library("lmerTest"), I get this error:
>
>Error: package or namespace load failed for ?lmerTest? in
>loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck =
>vI[[j]]):
> there is no package called ?purrr?
>
>
>Nothing I've done has worked. I have uninstalled both R and R studio.
>I've
>updated packages as well, but nothing works.
>
>Any suggestions?
>
>Thanks,
>
>Drew
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From sewashm at gmail.com  Sat Dec  9 01:48:35 2017
From: sewashm at gmail.com (Ashta)
Date: Fri, 8 Dec 2017 18:48:35 -0600
Subject: [R] Remove
In-Reply-To: <FD98DBA2-0D1A-4958-AC3F-B06469E28596@comcast.net>
References: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>
 <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>
 <3296595B-6750-425D-9C2E-AD1697307689@dcn.davis.ca.us>
 <CADDFq31sV_u3NLhU=T-ko=71abBdN4A8LaG8otMtsBwanQBHfQ@mail.gmail.com>
 <CA+vqiLG+Tx_73yR=M8uYM4toTXO02CDK3t0yK2AG_xn0uxrkhw@mail.gmail.com>
 <CADDFq30GCH8r=tE4UW08kpSjyym2J-USvVugLUhNMsk3tED_MQ@mail.gmail.com>
 <FD98DBA2-0D1A-4958-AC3F-B06469E28596@comcast.net>
Message-ID: <CADDFq33=ztwKPyEH7k6_y8hHaXZefnr1uQ5PDxxW3QeWg7qp8A@mail.gmail.com>

Hi David, Ista and all,

I  have one related question  Within one group I want to keep records
conditionally.
example within
group A I want keep rows that have  " x" values  ranged  between 15 and 30.
group B I want keep rows that have  " x" values  ranged  between  40 and 50.
group C I want keep rows that have  " x" values  ranged  between  60 and 75.


DM <- read.table( text='GR x y
A 25 125
A 23 135
A 14 145
A 35 230
B 45 321
B 47 512
B 53 123
B 55 451
C 61 521
C 68 235
C 85 258
C 80 654',header = TRUE, stringsAsFactors = FALSE)


The end result will be
A 25 125
A 23 135
B 45 321
B 47 512
C 61 521
C 68 235

Thank you

On Wed, Dec 6, 2017 at 10:34 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>
>> On Dec 6, 2017, at 4:27 PM, Ashta <sewashm at gmail.com> wrote:
>>
>> Thank you Ista! Worked fine.
>
> Here's another (possibly more direct in its logic?):
>
>  DM[ !ave(DM$x, DM$GR, FUN= function(x) {!length(unique(x))==1}), ]
>   GR  x   y
> 5  B 25 321
> 6  B 25 512
> 7  B 25 123
> 8  B 25 451
>
> --
> David
>
>> On Wed, Dec 6, 2017 at 5:59 PM, Ista Zahn <istazahn at gmail.com> wrote:
>>> Hi Ashta,
>>>
>>> There are many ways to do it. Here is one:
>>>
>>> vars <- sapply(split(DM$x, DM$GR), var)
>>> DM[DM$GR %in% names(vars[vars > 0]), ]
>>>
>>> Best
>>> Ista
>>>
>>> On Wed, Dec 6, 2017 at 6:58 PM, Ashta <sewashm at gmail.com> wrote:
>>>> Thank you Jeff,
>>>>
>>>> subset( DM, "B" != x ), this works if I know the group only.
>>>> But if I don't know that group in this case "B", how do I identify
>>>> group(s) that  all elements of x have the same value?
>>>>
>>>> On Wed, Dec 6, 2017 at 5:48 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>>> subset( DM, "B" != x )
>>>>>
>>>>> This is covered in the Introduction to R document that comes with R.
>>>>> --
>>>>> Sent from my phone. Please excuse my brevity.
>>>>>
>>>>> On December 6, 2017 3:21:12 PM PST, David Winsemius <dwinsemius at comcast.net> wrote:
>>>>>>
>>>>>>> On Dec 6, 2017, at 3:15 PM, Ashta <sewashm at gmail.com> wrote:
>>>>>>>
>>>>>>> Hi all,
>>>>>>> In a data set I have group(GR) and two variables   x and y. I want to
>>>>>>> remove a  group that have  the same record for the x variable in each
>>>>>>> row.
>>>>>>>
>>>>>>> DM <- read.table( text='GR x y
>>>>>>> A 25 125
>>>>>>> A 23 135
>>>>>>> A 14 145
>>>>>>> A 12 230
>>>>>>> B 25 321
>>>>>>> B 25 512
>>>>>>> B 25 123
>>>>>>> B 25 451
>>>>>>> C 11 521
>>>>>>> C 14 235
>>>>>>> C 15 258
>>>>>>> C 10 654',header = TRUE, stringsAsFactors = FALSE)
>>>>>>>
>>>>>>> In this example the output should contain group A and C  as group B
>>>>>>> has   the same record  for the variable x .
>>>>>>>
>>>>>>> The result will be
>>>>>>> A 25 125
>>>>>>> A 23 135
>>>>>>> A 14 145
>>>>>>> A 12 230
>>>>>>> C 11 521
>>>>>>> C 14 235
>>>>>>> C 15 258
>>>>>>> C 10 654
>>>>>>
>>>>>> Try:
>>>>>>
>>>>>> DM[ !duplicated(DM$x) , ]
>>>>>>>
>>>>>>> How do I do it R?
>>>>>>> Thank you.
>>>>>>>
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>>> David Winsemius
>>>>>> Alameda, CA, USA
>>>>>>
>>>>>> 'Any technology distinguishable from magic is insufficiently advanced.'
>>>>>> -Gehm's Corollary to Clarke's Third Law
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
>
>
>
>
>


From dwinsemius at comcast.net  Sat Dec  9 03:16:18 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 8 Dec 2017 18:16:18 -0800
Subject: [R] Remove
In-Reply-To: <CADDFq33=ztwKPyEH7k6_y8hHaXZefnr1uQ5PDxxW3QeWg7qp8A@mail.gmail.com>
References: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>
 <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>
 <3296595B-6750-425D-9C2E-AD1697307689@dcn.davis.ca.us>
 <CADDFq31sV_u3NLhU=T-ko=71abBdN4A8LaG8otMtsBwanQBHfQ@mail.gmail.com>
 <CA+vqiLG+Tx_73yR=M8uYM4toTXO02CDK3t0yK2AG_xn0uxrkhw@mail.gmail.com>
 <CADDFq30GCH8r=tE4UW08kpSjyym2J-USvVugLUhNMsk3tED_MQ@mail.gmail.com>
 <FD98DBA2-0D1A-4958-AC3F-B06469E28596@comcast.net>
 <CADDFq33=ztwKPyEH7k6_y8hHaXZefnr1uQ5PDxxW3QeWg7qp8A@mail.gmail.com>
Message-ID: <546E1142-F2D3-49D5-86E5-EC802E1FDF7C@comcast.net>


> On Dec 8, 2017, at 4:48 PM, Ashta <sewashm at gmail.com> wrote:
> 
> Hi David, Ista and all,
> 
> I  have one related question  Within one group I want to keep records
> conditionally.
> example within
> group A I want keep rows that have  " x" values  ranged  between 15 and 30.
> group B I want keep rows that have  " x" values  ranged  between  40 and 50.
> group C I want keep rows that have  " x" values  ranged  between  60 and 75.

When you have a problem where there are multiple "parallel: parameters, the function to "reach for" is `mapply`. 

    mapply( your_selection_func, group_vec, min_vec, max_vec)

... and this will probably return the values as a list (of dataframes if you build the function correctly,  so you may may need to then do:

    do.call(rbind, ...)

-- 
David.
> 
> 
> DM <- read.table( text='GR x y
> A 25 125
> A 23 135
> A 14 145
> A 35 230
> B 45 321
> B 47 512
> B 53 123
> B 55 451
> C 61 521
> C 68 235
> C 85 258
> C 80 654',header = TRUE, stringsAsFactors = FALSE)
> 
> 
> The end result will be
> A 25 125
> A 23 135
> B 45 321
> B 47 512
> C 61 521
> C 68 235
> 
> Thank you
> 
> On Wed, Dec 6, 2017 at 10:34 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>> 
>>> On Dec 6, 2017, at 4:27 PM, Ashta <sewashm at gmail.com> wrote:
>>> 
>>> Thank you Ista! Worked fine.
>> 
>> Here's another (possibly more direct in its logic?):
>> 
>> DM[ !ave(DM$x, DM$GR, FUN= function(x) {!length(unique(x))==1}), ]
>>  GR  x   y
>> 5  B 25 321
>> 6  B 25 512
>> 7  B 25 123
>> 8  B 25 451
>> 
>> --
>> David
>> 
>>> On Wed, Dec 6, 2017 at 5:59 PM, Ista Zahn <istazahn at gmail.com> wrote:
>>>> Hi Ashta,
>>>> 
>>>> There are many ways to do it. Here is one:
>>>> 
>>>> vars <- sapply(split(DM$x, DM$GR), var)
>>>> DM[DM$GR %in% names(vars[vars > 0]), ]
>>>> 
>>>> Best
>>>> Ista
>>>> 
>>>> On Wed, Dec 6, 2017 at 6:58 PM, Ashta <sewashm at gmail.com> wrote:
>>>>> Thank you Jeff,
>>>>> 
>>>>> subset( DM, "B" != x ), this works if I know the group only.
>>>>> But if I don't know that group in this case "B", how do I identify
>>>>> group(s) that  all elements of x have the same value?
>>>>> 
>>>>> On Wed, Dec 6, 2017 at 5:48 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>>>> subset( DM, "B" != x )
>>>>>> 
>>>>>> This is covered in the Introduction to R document that comes with R.
>>>>>> --
>>>>>> Sent from my phone. Please excuse my brevity.
>>>>>> 
>>>>>> On December 6, 2017 3:21:12 PM PST, David Winsemius <dwinsemius at comcast.net> wrote:
>>>>>>> 
>>>>>>>> On Dec 6, 2017, at 3:15 PM, Ashta <sewashm at gmail.com> wrote:
>>>>>>>> 
>>>>>>>> Hi all,
>>>>>>>> In a data set I have group(GR) and two variables   x and y. I want to
>>>>>>>> remove a  group that have  the same record for the x variable in each
>>>>>>>> row.
>>>>>>>> 
>>>>>>>> DM <- read.table( text='GR x y
>>>>>>>> A 25 125
>>>>>>>> A 23 135
>>>>>>>> A 14 145
>>>>>>>> A 12 230
>>>>>>>> B 25 321
>>>>>>>> B 25 512
>>>>>>>> B 25 123
>>>>>>>> B 25 451
>>>>>>>> C 11 521
>>>>>>>> C 14 235
>>>>>>>> C 15 258
>>>>>>>> C 10 654',header = TRUE, stringsAsFactors = FALSE)
>>>>>>>> 
>>>>>>>> In this example the output should contain group A and C  as group B
>>>>>>>> has   the same record  for the variable x .
>>>>>>>> 
>>>>>>>> The result will be
>>>>>>>> A 25 125
>>>>>>>> A 23 135
>>>>>>>> A 14 145
>>>>>>>> A 12 230
>>>>>>>> C 11 521
>>>>>>>> C 14 235
>>>>>>>> C 15 258
>>>>>>>> C 10 654
>>>>>>> 
>>>>>>> Try:
>>>>>>> 
>>>>>>> DM[ !duplicated(DM$x) , ]
>>>>>>>> 
>>>>>>>> How do I do it R?
>>>>>>>> Thank you.
>>>>>>>> 
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>> 
>>>>>>> David Winsemius
>>>>>>> Alameda, CA, USA
>>>>>>> 
>>>>>>> 'Any technology distinguishable from magic is insufficiently advanced.'
>>>>>>> -Gehm's Corollary to Clarke's Third Law
>>>>>>> 
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide
>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> David Winsemius
>> Alameda, CA, USA
>> 
>> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
>> 
>> 
>> 
>> 
>> 

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From jmhannon.ucdavis at gmail.com  Sat Dec  9 06:56:09 2017
From: jmhannon.ucdavis at gmail.com (Michael Hannon)
Date: Fri, 8 Dec 2017 21:56:09 -0800
Subject: [R] Remove
In-Reply-To: <CADDFq33=ztwKPyEH7k6_y8hHaXZefnr1uQ5PDxxW3QeWg7qp8A@mail.gmail.com>
References: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>
 <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>
 <3296595B-6750-425D-9C2E-AD1697307689@dcn.davis.ca.us>
 <CADDFq31sV_u3NLhU=T-ko=71abBdN4A8LaG8otMtsBwanQBHfQ@mail.gmail.com>
 <CA+vqiLG+Tx_73yR=M8uYM4toTXO02CDK3t0yK2AG_xn0uxrkhw@mail.gmail.com>
 <CADDFq30GCH8r=tE4UW08kpSjyym2J-USvVugLUhNMsk3tED_MQ@mail.gmail.com>
 <FD98DBA2-0D1A-4958-AC3F-B06469E28596@comcast.net>
 <CADDFq33=ztwKPyEH7k6_y8hHaXZefnr1uQ5PDxxW3QeWg7qp8A@mail.gmail.com>
Message-ID: <CACdH2ZZsvCvR3RRcw2J4bVS7G+aNLW0Cdice3wiarL5vA7e2sQ@mail.gmail.com>

library(dplyr)

DM <- read.table( text='GR x y
A 25 125
A 23 135
.
.
.
)

DM %>% filter((GR == "A" & (x >= 15) & (x <= 30)) |
                        (GR == "B" & (x >= 40) & (x <= 50)) |
                        (GR == "C" & (x >= 60) & (x <= 75)))


On Fri, Dec 8, 2017 at 4:48 PM, Ashta <sewashm at gmail.com> wrote:
> Hi David, Ista and all,
>
> I  have one related question  Within one group I want to keep records
> conditionally.
> example within
> group A I want keep rows that have  " x" values  ranged  between 15 and 30.
> group B I want keep rows that have  " x" values  ranged  between  40 and 50.
> group C I want keep rows that have  " x" values  ranged  between  60 and 75.
>
>
> DM <- read.table( text='GR x y
> A 25 125
> A 23 135
> A 14 145
> A 35 230
> B 45 321
> B 47 512
> B 53 123
> B 55 451
> C 61 521
> C 68 235
> C 85 258
> C 80 654',header = TRUE, stringsAsFactors = FALSE)
>
>
> The end result will be
> A 25 125
> A 23 135
> B 45 321
> B 47 512
> C 61 521
> C 68 235
>
> Thank you
>
> On Wed, Dec 6, 2017 at 10:34 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>
>>> On Dec 6, 2017, at 4:27 PM, Ashta <sewashm at gmail.com> wrote:
>>>
>>> Thank you Ista! Worked fine.
>>
>> Here's another (possibly more direct in its logic?):
>>
>>  DM[ !ave(DM$x, DM$GR, FUN= function(x) {!length(unique(x))==1}), ]
>>   GR  x   y
>> 5  B 25 321
>> 6  B 25 512
>> 7  B 25 123
>> 8  B 25 451
>>
>> --
>> David
>>
>>> On Wed, Dec 6, 2017 at 5:59 PM, Ista Zahn <istazahn at gmail.com> wrote:
>>>> Hi Ashta,
>>>>
>>>> There are many ways to do it. Here is one:
>>>>
>>>> vars <- sapply(split(DM$x, DM$GR), var)
>>>> DM[DM$GR %in% names(vars[vars > 0]), ]
>>>>
>>>> Best
>>>> Ista
>>>>
>>>> On Wed, Dec 6, 2017 at 6:58 PM, Ashta <sewashm at gmail.com> wrote:
>>>>> Thank you Jeff,
>>>>>
>>>>> subset( DM, "B" != x ), this works if I know the group only.
>>>>> But if I don't know that group in this case "B", how do I identify
>>>>> group(s) that  all elements of x have the same value?
>>>>>
>>>>> On Wed, Dec 6, 2017 at 5:48 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>>>> subset( DM, "B" != x )
>>>>>>
>>>>>> This is covered in the Introduction to R document that comes with R.
>>>>>> --
>>>>>> Sent from my phone. Please excuse my brevity.
>>>>>>
>>>>>> On December 6, 2017 3:21:12 PM PST, David Winsemius <dwinsemius at comcast.net> wrote:
>>>>>>>
>>>>>>>> On Dec 6, 2017, at 3:15 PM, Ashta <sewashm at gmail.com> wrote:
>>>>>>>>
>>>>>>>> Hi all,
>>>>>>>> In a data set I have group(GR) and two variables   x and y. I want to
>>>>>>>> remove a  group that have  the same record for the x variable in each
>>>>>>>> row.
>>>>>>>>
>>>>>>>> DM <- read.table( text='GR x y
>>>>>>>> A 25 125
>>>>>>>> A 23 135
>>>>>>>> A 14 145
>>>>>>>> A 12 230
>>>>>>>> B 25 321
>>>>>>>> B 25 512
>>>>>>>> B 25 123
>>>>>>>> B 25 451
>>>>>>>> C 11 521
>>>>>>>> C 14 235
>>>>>>>> C 15 258
>>>>>>>> C 10 654',header = TRUE, stringsAsFactors = FALSE)
>>>>>>>>
>>>>>>>> In this example the output should contain group A and C  as group B
>>>>>>>> has   the same record  for the variable x .
>>>>>>>>
>>>>>>>> The result will be
>>>>>>>> A 25 125
>>>>>>>> A 23 135
>>>>>>>> A 14 145
>>>>>>>> A 12 230
>>>>>>>> C 11 521
>>>>>>>> C 14 235
>>>>>>>> C 15 258
>>>>>>>> C 10 654
>>>>>>>
>>>>>>> Try:
>>>>>>>
>>>>>>> DM[ !duplicated(DM$x) , ]
>>>>>>>>
>>>>>>>> How do I do it R?
>>>>>>>> Thank you.
>>>>>>>>
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>
>>>>>>> David Winsemius
>>>>>>> Alameda, CA, USA
>>>>>>>
>>>>>>> 'Any technology distinguishable from magic is insufficiently advanced.'
>>>>>>> -Gehm's Corollary to Clarke's Third Law
>>>>>>>
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide
>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> David Winsemius
>> Alameda, CA, USA
>>
>> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
>>
>>
>>
>>
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Sat Dec  9 07:27:04 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 8 Dec 2017 22:27:04 -0800 (PST)
Subject: [R] Remove
In-Reply-To: <CACdH2ZZsvCvR3RRcw2J4bVS7G+aNLW0Cdice3wiarL5vA7e2sQ@mail.gmail.com>
References: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>
 <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>
 <3296595B-6750-425D-9C2E-AD1697307689@dcn.davis.ca.us>
 <CADDFq31sV_u3NLhU=T-ko=71abBdN4A8LaG8otMtsBwanQBHfQ@mail.gmail.com>
 <CA+vqiLG+Tx_73yR=M8uYM4toTXO02CDK3t0yK2AG_xn0uxrkhw@mail.gmail.com>
 <CADDFq30GCH8r=tE4UW08kpSjyym2J-USvVugLUhNMsk3tED_MQ@mail.gmail.com>
 <FD98DBA2-0D1A-4958-AC3F-B06469E28596@comcast.net>
 <CADDFq33=ztwKPyEH7k6_y8hHaXZefnr1uQ5PDxxW3QeWg7qp8A@mail.gmail.com>
 <CACdH2ZZsvCvR3RRcw2J4bVS7G+aNLW0Cdice3wiarL5vA7e2sQ@mail.gmail.com>
Message-ID: <alpine.BSF.2.00.1712082214220.5895@pedal.dcn.davis.ca.us>

In this case I cannot see an advantage to using dplyr over subset, other 
than if dplyr is your hammer then the problem will look like a nail, or if 
this is one step in a larger context where dplyr is more useful.

Nor do I think this is a good use for mapply (or dplyr::group_by) because 
the groups are handled differently... better to introduce a data-driven 
columnar approach than to have three separate algorithms and bind the data 
frames together again.

Here are three ways I came up with. I sometimes use a variation of method 
3 when the logical tests are rather more complicated than this and I want 
to characterize those tests in the final report.

####### reprex
DM <- read.table( text =
"GR x y
A 25 125
A 23 135
A 14 145
A 35 230
B 45 321
B 47 512
B 53 123
B 55 451
C 61 521
C 68 235
C 85 258
C 80 654", header = TRUE, stringsAsFactors = FALSE )

# 1 Hardcoded logic
DM1 <- subset( DM
              ,   "A" == GR & 15 <= x & x <= 30
                | "B" == GR & 40 <= x & x <= 50
                | "C" == GR & 60 <= x & x <= 75
              )
DM1
#>    GR  x   y
#> 1   A 25 125
#> 2   A 23 135
#> 5   B 45 321
#> 6   B 47 512
#> 9   C 61 521
#> 10  C 68 235

# 2 relational approach
cond <- read.table( text =
"GR minx maxx
A   15   30
B   40   50
C   60   75
", header = TRUE )
DM2 <- merge( DM, cond, by = "GR" )
DM2 <- subset( DM2, minx <= x & x <= maxx, select = -c( minx, maxx ) )
DM2
#>    GR  x   y
#> 1   A 25 125
#> 2   A 23 135
#> 5   B 45 321
#> 6   B 47 512
#> 9   C 61 521
#> 10  C 68 235

# 3 Construct selection vector
sel <- rep( FALSE, nrow( DM ) )
for ( i in seq.int( nrow( cond ) ) ) {
     sel <- sel | ( cond$GR[ i ] == DM$GR
                  & cond$minx[ i ] <= DM$x
                  & DM$x <= cond$maxx[ i ]
                  )
}
DM3 <- DM[ sel, ]
DM3
#>    GR  x   y
#> 1   A 25 125
#> 2   A 23 135
#> 5   B 45 321
#> 6   B 47 512
#> 9   C 61 521
#> 10  C 68 235
#######


On Fri, 8 Dec 2017, Michael Hannon wrote:

> library(dplyr)
>
> DM <- read.table( text='GR x y
> A 25 125
> A 23 135
> .
> .
> .
> )
>
> DM %>% filter((GR == "A" & (x >= 15) & (x <= 30)) |
>                        (GR == "B" & (x >= 40) & (x <= 50)) |
>                        (GR == "C" & (x >= 60) & (x <= 75)))
>
>
> On Fri, Dec 8, 2017 at 4:48 PM, Ashta <sewashm at gmail.com> wrote:
>> Hi David, Ista and all,
>>
>> I  have one related question  Within one group I want to keep records
>> conditionally.
>> example within
>> group A I want keep rows that have  " x" values  ranged  between 15 and 30.
>> group B I want keep rows that have  " x" values  ranged  between  40 and 50.
>> group C I want keep rows that have  " x" values  ranged  between  60 and 75.
>>
>>
>> DM <- read.table( text='GR x y
>> A 25 125
>> A 23 135
>> A 14 145
>> A 35 230
>> B 45 321
>> B 47 512
>> B 53 123
>> B 55 451
>> C 61 521
>> C 68 235
>> C 85 258
>> C 80 654',header = TRUE, stringsAsFactors = FALSE)
>>
>>
>> The end result will be
>> A 25 125
>> A 23 135
>> B 45 321
>> B 47 512
>> C 61 521
>> C 68 235
>>
>> Thank you
>>
>> On Wed, Dec 6, 2017 at 10:34 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>>
>>>> On Dec 6, 2017, at 4:27 PM, Ashta <sewashm at gmail.com> wrote:
>>>>
>>>> Thank you Ista! Worked fine.
>>>
>>> Here's another (possibly more direct in its logic?):
>>>
>>>  DM[ !ave(DM$x, DM$GR, FUN= function(x) {!length(unique(x))==1}), ]
>>>   GR  x   y
>>> 5  B 25 321
>>> 6  B 25 512
>>> 7  B 25 123
>>> 8  B 25 451
>>>
>>> --
>>> David
>>>
>>>> On Wed, Dec 6, 2017 at 5:59 PM, Ista Zahn <istazahn at gmail.com> wrote:
>>>>> Hi Ashta,
>>>>>
>>>>> There are many ways to do it. Here is one:
>>>>>
>>>>> vars <- sapply(split(DM$x, DM$GR), var)
>>>>> DM[DM$GR %in% names(vars[vars > 0]), ]
>>>>>
>>>>> Best
>>>>> Ista
>>>>>
>>>>> On Wed, Dec 6, 2017 at 6:58 PM, Ashta <sewashm at gmail.com> wrote:
>>>>>> Thank you Jeff,
>>>>>>
>>>>>> subset( DM, "B" != x ), this works if I know the group only.
>>>>>> But if I don't know that group in this case "B", how do I identify
>>>>>> group(s) that  all elements of x have the same value?
>>>>>>
>>>>>> On Wed, Dec 6, 2017 at 5:48 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>>>>> subset( DM, "B" != x )
>>>>>>>
>>>>>>> This is covered in the Introduction to R document that comes with R.
>>>>>>> --
>>>>>>> Sent from my phone. Please excuse my brevity.
>>>>>>>
>>>>>>> On December 6, 2017 3:21:12 PM PST, David Winsemius <dwinsemius at comcast.net> wrote:
>>>>>>>>
>>>>>>>>> On Dec 6, 2017, at 3:15 PM, Ashta <sewashm at gmail.com> wrote:
>>>>>>>>>
>>>>>>>>> Hi all,
>>>>>>>>> In a data set I have group(GR) and two variables   x and y. I want to
>>>>>>>>> remove a  group that have  the same record for the x variable in each
>>>>>>>>> row.
>>>>>>>>>
>>>>>>>>> DM <- read.table( text='GR x y
>>>>>>>>> A 25 125
>>>>>>>>> A 23 135
>>>>>>>>> A 14 145
>>>>>>>>> A 12 230
>>>>>>>>> B 25 321
>>>>>>>>> B 25 512
>>>>>>>>> B 25 123
>>>>>>>>> B 25 451
>>>>>>>>> C 11 521
>>>>>>>>> C 14 235
>>>>>>>>> C 15 258
>>>>>>>>> C 10 654',header = TRUE, stringsAsFactors = FALSE)
>>>>>>>>>
>>>>>>>>> In this example the output should contain group A and C  as group B
>>>>>>>>> has   the same record  for the variable x .
>>>>>>>>>
>>>>>>>>> The result will be
>>>>>>>>> A 25 125
>>>>>>>>> A 23 135
>>>>>>>>> A 14 145
>>>>>>>>> A 12 230
>>>>>>>>> C 11 521
>>>>>>>>> C 14 235
>>>>>>>>> C 15 258
>>>>>>>>> C 10 654
>>>>>>>>
>>>>>>>> Try:
>>>>>>>>
>>>>>>>> DM[ !duplicated(DM$x) , ]
>>>>>>>>>
>>>>>>>>> How do I do it R?
>>>>>>>>> Thank you.
>>>>>>>>>
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>>
>>>>>>>> David Winsemius
>>>>>>>> Alameda, CA, USA
>>>>>>>>
>>>>>>>> 'Any technology distinguishable from magic is insufficiently advanced.'
>>>>>>>> -Gehm's Corollary to Clarke's Third Law
>>>>>>>>
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> David Winsemius
>>> Alameda, CA, USA
>>>
>>> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
>>>
>>>
>>>
>>>
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From arabianahmad at googlemail.com  Sat Dec  9 06:46:13 2017
From: arabianahmad at googlemail.com (ah a)
Date: Sat, 9 Dec 2017 09:16:13 +0330
Subject: [R] MSVAR
Message-ID: <CALjW5c+5u4L1Hg41hb_rJhHsOAbn0Dr7Oc--f+Gqp0JrvUniDw@mail.gmail.com>

Hello,

As I'm interested to search about the monetary transmission channel in our
country by MSVAR model,Could you do me favor and tell me How I can run
different types of MSVAR model (such as MSIAH(2)-VAR(2)) and finding
impulse response function in different regimes and also variance
decomposition?

Thank you very much in advance.

Best Regards,
Ahmad

	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Sat Dec  9 11:00:05 2017
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Sat, 9 Dec 2017 10:00:05 +0000
Subject: [R] Remove
In-Reply-To: <CADDFq33=ztwKPyEH7k6_y8hHaXZefnr1uQ5PDxxW3QeWg7qp8A@mail.gmail.com>
References: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>
 <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>
 <3296595B-6750-425D-9C2E-AD1697307689@dcn.davis.ca.us>
 <CADDFq31sV_u3NLhU=T-ko=71abBdN4A8LaG8otMtsBwanQBHfQ@mail.gmail.com>
 <CA+vqiLG+Tx_73yR=M8uYM4toTXO02CDK3t0yK2AG_xn0uxrkhw@mail.gmail.com>
 <CADDFq30GCH8r=tE4UW08kpSjyym2J-USvVugLUhNMsk3tED_MQ@mail.gmail.com>
 <FD98DBA2-0D1A-4958-AC3F-B06469E28596@comcast.net>
 <CADDFq33=ztwKPyEH7k6_y8hHaXZefnr1uQ5PDxxW3QeWg7qp8A@mail.gmail.com>
Message-ID: <041c8657-ecdf-1b11-71c5-8998fb6fa07a@sapo.pt>

Hello,

Try the following.

keep <- list(A = c(15, 30), B = c(40, 50), C = c(60, 75))
sp <- split(DM$x, DM$GR)
inx <- unlist(lapply(seq_along(sp), function(i) keep[[i]][1] <= sp[[i]] 
& sp[[i]] <= keep[[i]][2]))
DM[inx, ]
#   GR  x   y
#1   A 25 125
#2   A 23 135
#5   B 45 321
#6   B 47 512
#9   C 61 521
#10  C 68 235

Hope this helps,

Rui Barradas

On 12/9/2017 12:48 AM, Ashta wrote:
> Hi David, Ista and all,
> 
> I  have one related question  Within one group I want to keep records
> conditionally.
> example within
> group A I want keep rows that have  " x" values  ranged  between 15 and 30.
> group B I want keep rows that have  " x" values  ranged  between  40 and 50.
> group C I want keep rows that have  " x" values  ranged  between  60 and 75.
> 
> 
> DM <- read.table( text='GR x y
> A 25 125
> A 23 135
> A 14 145
> A 35 230
> B 45 321
> B 47 512
> B 53 123
> B 55 451
> C 61 521
> C 68 235
> C 85 258
> C 80 654',header = TRUE, stringsAsFactors = FALSE)
> 
> 
> The end result will be
> A 25 125
> A 23 135
> B 45 321
> B 47 512
> C 61 521
> C 68 235
> 
> Thank you
> 
> On Wed, Dec 6, 2017 at 10:34 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>
>>> On Dec 6, 2017, at 4:27 PM, Ashta <sewashm at gmail.com> wrote:
>>>
>>> Thank you Ista! Worked fine.
>>
>> Here's another (possibly more direct in its logic?):
>>
>>   DM[ !ave(DM$x, DM$GR, FUN= function(x) {!length(unique(x))==1}), ]
>>    GR  x   y
>> 5  B 25 321
>> 6  B 25 512
>> 7  B 25 123
>> 8  B 25 451
>>
>> --
>> David
>>
>>> On Wed, Dec 6, 2017 at 5:59 PM, Ista Zahn <istazahn at gmail.com> wrote:
>>>> Hi Ashta,
>>>>
>>>> There are many ways to do it. Here is one:
>>>>
>>>> vars <- sapply(split(DM$x, DM$GR), var)
>>>> DM[DM$GR %in% names(vars[vars > 0]), ]
>>>>
>>>> Best
>>>> Ista
>>>>
>>>> On Wed, Dec 6, 2017 at 6:58 PM, Ashta <sewashm at gmail.com> wrote:
>>>>> Thank you Jeff,
>>>>>
>>>>> subset( DM, "B" != x ), this works if I know the group only.
>>>>> But if I don't know that group in this case "B", how do I identify
>>>>> group(s) that  all elements of x have the same value?
>>>>>
>>>>> On Wed, Dec 6, 2017 at 5:48 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>>>> subset( DM, "B" != x )
>>>>>>
>>>>>> This is covered in the Introduction to R document that comes with R.
>>>>>> --
>>>>>> Sent from my phone. Please excuse my brevity.
>>>>>>
>>>>>> On December 6, 2017 3:21:12 PM PST, David Winsemius <dwinsemius at comcast.net> wrote:
>>>>>>>
>>>>>>>> On Dec 6, 2017, at 3:15 PM, Ashta <sewashm at gmail.com> wrote:
>>>>>>>>
>>>>>>>> Hi all,
>>>>>>>> In a data set I have group(GR) and two variables   x and y. I want to
>>>>>>>> remove a  group that have  the same record for the x variable in each
>>>>>>>> row.
>>>>>>>>
>>>>>>>> DM <- read.table( text='GR x y
>>>>>>>> A 25 125
>>>>>>>> A 23 135
>>>>>>>> A 14 145
>>>>>>>> A 12 230
>>>>>>>> B 25 321
>>>>>>>> B 25 512
>>>>>>>> B 25 123
>>>>>>>> B 25 451
>>>>>>>> C 11 521
>>>>>>>> C 14 235
>>>>>>>> C 15 258
>>>>>>>> C 10 654',header = TRUE, stringsAsFactors = FALSE)
>>>>>>>>
>>>>>>>> In this example the output should contain group A and C  as group B
>>>>>>>> has   the same record  for the variable x .
>>>>>>>>
>>>>>>>> The result will be
>>>>>>>> A 25 125
>>>>>>>> A 23 135
>>>>>>>> A 14 145
>>>>>>>> A 12 230
>>>>>>>> C 11 521
>>>>>>>> C 14 235
>>>>>>>> C 15 258
>>>>>>>> C 10 654
>>>>>>>
>>>>>>> Try:
>>>>>>>
>>>>>>> DM[ !duplicated(DM$x) , ]
>>>>>>>>
>>>>>>>> How do I do it R?
>>>>>>>> Thank you.
>>>>>>>>
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>
>>>>>>> David Winsemius
>>>>>>> Alameda, CA, USA
>>>>>>>
>>>>>>> 'Any technology distinguishable from magic is insufficiently advanced.'
>>>>>>> -Gehm's Corollary to Clarke's Third Law
>>>>>>>
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide
>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> David Winsemius
>> Alameda, CA, USA
>>
>> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
>>
>>
>>
>>
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From yadavneog at gmail.com  Sat Dec  9 10:59:22 2017
From: yadavneog at gmail.com (yadav neog)
Date: Sat, 9 Dec 2017 15:29:22 +0530
Subject: [R] help
Message-ID: <CACdLcRz3GK30u48n8HzYiY7ruN5XczkkUPA0EEz0DGjWQ-Z24w@mail.gmail.com>

dear members, I want to run Toda Yamamoto causal test in my data. I have
gone through the some of the examples. but unable to understand wald.test.
especially the 'term' argument. kindly help me in understand wald.test ??

-- 
Yadawananda Neog
Research Scholar
Department of Economics
Banaras Hindu University
Mob. 9838545073

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sat Dec  9 16:58:44 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 9 Dec 2017 07:58:44 -0800
Subject: [R] trying to find the multiple combinations...
In-Reply-To: <CAGxFJbRvD-LQGOYJsaj+gURtY83WxPmxoPCwW9hGnB=d49oZPA@mail.gmail.com>
References: <BN6PR22MB0098C0690657E78C83601B0FF2300@BN6PR22MB0098.namprd22.prod.outlook.com>
 <7A43238B-E099-4C0F-B4D3-3742AFFCD801@dcn.davis.ca.us>
 <CAGxFJbRku9B_YTp=8xw44kicA=4mmEA_i8naSNb4EzNP+md+Kg@mail.gmail.com>
 <BN6PR22MB0098AC7C5D2704E72E77E8DCF2300@BN6PR22MB0098.namprd22.prod.outlook.com>
 <CAGxFJbRvD-LQGOYJsaj+gURtY83WxPmxoPCwW9hGnB=d49oZPA@mail.gmail.com>
Message-ID: <CAGxFJbTA6pyYS2aA95PxtssAvRJOEtYUxDPZrArsPe=Vyi_XXw@mail.gmail.com>

Off topic, but for the record...

As Jeff already noted,the equation reduces to a single linear equation with
rational coefficients, so of course there are infinitely many integer
solutions.

Apologies for my dummheit.

-- Bert

On Fri, Dec 8, 2017 at 9:47 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Please keep all replies onlist if there is no reason to keep them private.
> I am not a free, private consultant (and so might choose to ignore your
> followups); and I don't have all or necessarily the best answers anyway. So
> give yourself the maximal chance to be helped by others.
>
> Anyway,
>
> ?expand.grid is what you're looking for I think as an alternative to
> nested loops. If "results" is a vector of calculated results, i.e. the
> averages for the grid of combinations you generate, and "target" is your
> desired target (average), here of 15.0078, then
>
> which.min(abs(results - target))
> gives you the index of the closest results and
>
> abs(results - target) < tol
> gives you a vector of logicals of results that are within tol of the
> target.
>
> This is all pretty basic stuff, which suggests that you really need to
> spend some time with an R tutorial or two.  Here are some suggestions, but
> a web search would uncover many more, some of which might be more suitable
> for you:
> https://www.rstudio.com/online-learning/#R
>
> This list can help (not sure if I did here), but it cannot replace such
> homework on your own.
>
> Cheers,
> Bert
>
>
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
> On Fri, Dec 8, 2017 at 9:15 AM, Benjamin Sabatini <sunscape1 at hotmail.com>
> wrote:
>
>> Hi,
>>
>> Yes, actually, I could set an upper limit and grind through the
>> possibilities to find a minimal set or a few if that's what you mean. Close
>> to the result would be OK, too. Otherwise it would go on forever, I
>> suppose.
>>
>> At first I was thinking of just trying to write three for loops to test
>> for every set of the multiples of x, y, and z between something like 1 and
>> 10,000, but I understand that this is not at all efficient in R. So, (1*13.4689
>> + 1*12.85212+ 1*17.05071) / 1+1+1), (1*13.4689 + 2*12.85212+ 1*17.05071)
>> / 1+2+1)...
>>
>> Is there a better way? If I solve for z is it then easier with an upper
>> limit? So, z = x*0.753288 + y*1.0552
>> and then loop it?
>>
>> ------------------------------
>> *From:* Bert Gunter <bgunter.4567 at gmail.com>
>> *Sent:* Friday, December 8, 2017 3:16 PM
>> *To:* Jeff Newmiller
>> *Cc:* R-help; Benjamin Sabatini
>> *Subject:* Re: [R] trying to find the multiple combinations...
>>
>> Are x,y, and z supposed to be positive whole numbers? If so, there may be
>> no solutions. If there is a solution set, of course any multiple of the set
>> is a solution set, so presumably you want a minimal set in some sense. This
>> strikes me as a hard problem mathematically, but maybe there is some
>> obvious way to set an upper bound on a minimal x,y, and z, in which case a
>> simple grid search could then be used.
>>
>> Naturally, if any real numbers are sought, Jeff is correct.
>>
>> Cheers,
>> Bert
>>
>>
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>> On Fri, Dec 8, 2017 at 12:19 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us
>> > wrote:
>>
>> Solve for one of your variables and it will be given in terms of the
>> other two. That is, there is a whole infinite plane of solutions. No,
>> aggregate will not be sufficient to enumerate the solution set..
>> --
>> Sent from my phone. Please excuse my brevity.
>>
>> On December 7, 2017 10:37:37 PM PST, Benjamin Sabatini <
>> sunscape1 at hotmail.com> wrote:
>> >Hi,
>> >
>> >I'm trying to find a way to determine what multiples of the combination
>> >of three or more numbers equals a forth number.
>> >
>> >So, if I had a number set like:
>> >
>> >c(13.4689, 12.85212, 17.05071)
>> >
>> >What combination and multiples of these numbers would average to
>> >15.0078? (so, something that would tell me x, y, and z in (x*13.4689 +
>> >y*12.85212+ z*17.05071) / x+y+z) = 15.0078
>> >
>> >I think this is doable with aggregate?
>> >
>> >       [[alternative HTML version deleted]]
>>
>> This is a plain text mailing list. Please learn how to use your email
>> program.
>>
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>

	[[alternative HTML version deleted]]


From esawiek at gmail.com  Sat Dec  9 18:21:44 2017
From: esawiek at gmail.com (Ek Esawi)
Date: Sat, 9 Dec 2017 12:21:44 -0500
Subject: [R] Remove
In-Reply-To: <041c8657-ecdf-1b11-71c5-8998fb6fa07a@sapo.pt>
References: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>
 <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>
 <3296595B-6750-425D-9C2E-AD1697307689@dcn.davis.ca.us>
 <CADDFq31sV_u3NLhU=T-ko=71abBdN4A8LaG8otMtsBwanQBHfQ@mail.gmail.com>
 <CA+vqiLG+Tx_73yR=M8uYM4toTXO02CDK3t0yK2AG_xn0uxrkhw@mail.gmail.com>
 <CADDFq30GCH8r=tE4UW08kpSjyym2J-USvVugLUhNMsk3tED_MQ@mail.gmail.com>
 <FD98DBA2-0D1A-4958-AC3F-B06469E28596@comcast.net>
 <CADDFq33=ztwKPyEH7k6_y8hHaXZefnr1uQ5PDxxW3QeWg7qp8A@mail.gmail.com>
 <041c8657-ecdf-1b11-71c5-8998fb6fa07a@sapo.pt>
Message-ID: <CA+ZkTxuU6vt2Ve7i5vcM9WSAgg+Zt+YXRVTOHn3V2CHrBOgLAg@mail.gmail.com>

HI--

How about this one. It produces the desired result. If you have more
conditions, you can put them in a matrix/DF form and subset as suggested by
one of the previous suggestion.

DM[(DM$GR=="A"&DM$x>=15&DM$x<=30)|(DM$GR=="B"&DM$x>=40&DM$x<=50)|(DM
$GR=="C"&DM$x>=60&DM$x<=70),]


EK

On Sat, Dec 9, 2017 at 5:00 AM, Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> Try the following.
>
> keep <- list(A = c(15, 30), B = c(40, 50), C = c(60, 75))
> sp <- split(DM$x, DM$GR)
> inx <- unlist(lapply(seq_along(sp), function(i) keep[[i]][1] <= sp[[i]] &
> sp[[i]] <= keep[[i]][2]))
> DM[inx, ]
> #   GR  x   y
> #1   A 25 125
> #2   A 23 135
> #5   B 45 321
> #6   B 47 512
> #9   C 61 521
> #10  C 68 235
>
> Hope this helps,
>
> Rui Barradas
>
>
> On 12/9/2017 12:48 AM, Ashta wrote:
>
>> Hi David, Ista and all,
>>
>> I  have one related question  Within one group I want to keep records
>> conditionally.
>> example within
>> group A I want keep rows that have  " x" values  ranged  between 15 and
>> 30.
>> group B I want keep rows that have  " x" values  ranged  between  40 and
>> 50.
>> group C I want keep rows that have  " x" values  ranged  between  60 and
>> 75.
>>
>>
>> DM <- read.table( text='GR x y
>> A 25 125
>> A 23 135
>> A 14 145
>> A 35 230
>> B 45 321
>> B 47 512
>> B 53 123
>> B 55 451
>> C 61 521
>> C 68 235
>> C 85 258
>> C 80 654',header = TRUE, stringsAsFactors = FALSE)
>>
>>
>> The end result will be
>> A 25 125
>> A 23 135
>> B 45 321
>> B 47 512
>> C 61 521
>> C 68 235
>>
>> Thank you
>>
>> On Wed, Dec 6, 2017 at 10:34 PM, David Winsemius <dwinsemius at comcast.net>
>> wrote:
>>
>>>
>>> On Dec 6, 2017, at 4:27 PM, Ashta <sewashm at gmail.com> wrote:
>>>>
>>>> Thank you Ista! Worked fine.
>>>>
>>>
>>> Here's another (possibly more direct in its logic?):
>>>
>>>   DM[ !ave(DM$x, DM$GR, FUN= function(x) {!length(unique(x))==1}), ]
>>>    GR  x   y
>>> 5  B 25 321
>>> 6  B 25 512
>>> 7  B 25 123
>>> 8  B 25 451
>>>
>>> --
>>> David
>>>
>>> On Wed, Dec 6, 2017 at 5:59 PM, Ista Zahn <istazahn at gmail.com> wrote:
>>>>
>>>>> Hi Ashta,
>>>>>
>>>>> There are many ways to do it. Here is one:
>>>>>
>>>>> vars <- sapply(split(DM$x, DM$GR), var)
>>>>> DM[DM$GR %in% names(vars[vars > 0]), ]
>>>>>
>>>>> Best
>>>>> Ista
>>>>>
>>>>> On Wed, Dec 6, 2017 at 6:58 PM, Ashta <sewashm at gmail.com> wrote:
>>>>>
>>>>>> Thank you Jeff,
>>>>>>
>>>>>> subset( DM, "B" != x ), this works if I know the group only.
>>>>>> But if I don't know that group in this case "B", how do I identify
>>>>>> group(s) that  all elements of x have the same value?
>>>>>>
>>>>>> On Wed, Dec 6, 2017 at 5:48 PM, Jeff Newmiller <
>>>>>> jdnewmil at dcn.davis.ca.us> wrote:
>>>>>>
>>>>>>> subset( DM, "B" != x )
>>>>>>>
>>>>>>> This is covered in the Introduction to R document that comes with R.
>>>>>>> --
>>>>>>> Sent from my phone. Please excuse my brevity.
>>>>>>>
>>>>>>> On December 6, 2017 3:21:12 PM PST, David Winsemius <
>>>>>>> dwinsemius at comcast.net> wrote:
>>>>>>>
>>>>>>>>
>>>>>>>> On Dec 6, 2017, at 3:15 PM, Ashta <sewashm at gmail.com> wrote:
>>>>>>>>>
>>>>>>>>> Hi all,
>>>>>>>>> In a data set I have group(GR) and two variables   x and y. I want
>>>>>>>>> to
>>>>>>>>> remove a  group that have  the same record for the x variable in
>>>>>>>>> each
>>>>>>>>> row.
>>>>>>>>>
>>>>>>>>> DM <- read.table( text='GR x y
>>>>>>>>> A 25 125
>>>>>>>>> A 23 135
>>>>>>>>> A 14 145
>>>>>>>>> A 12 230
>>>>>>>>> B 25 321
>>>>>>>>> B 25 512
>>>>>>>>> B 25 123
>>>>>>>>> B 25 451
>>>>>>>>> C 11 521
>>>>>>>>> C 14 235
>>>>>>>>> C 15 258
>>>>>>>>> C 10 654',header = TRUE, stringsAsFactors = FALSE)
>>>>>>>>>
>>>>>>>>> In this example the output should contain group A and C  as group B
>>>>>>>>> has   the same record  for the variable x .
>>>>>>>>>
>>>>>>>>> The result will be
>>>>>>>>> A 25 125
>>>>>>>>> A 23 135
>>>>>>>>> A 14 145
>>>>>>>>> A 12 230
>>>>>>>>> C 11 521
>>>>>>>>> C 14 235
>>>>>>>>> C 15 258
>>>>>>>>> C 10 654
>>>>>>>>>
>>>>>>>>
>>>>>>>> Try:
>>>>>>>>
>>>>>>>> DM[ !duplicated(DM$x) , ]
>>>>>>>>
>>>>>>>>>
>>>>>>>>> How do I do it R?
>>>>>>>>> Thank you.
>>>>>>>>>
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>
>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>>>
>>>>>>>>
>>>>>>>> David Winsemius
>>>>>>>> Alameda, CA, USA
>>>>>>>>
>>>>>>>> 'Any technology distinguishable from magic is insufficiently
>>>>>>>> advanced.'
>>>>>>>> -Gehm's Corollary to Clarke's Third Law
>>>>>>>>
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>>
>>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>>>>> ng-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>>> ng-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>> David Winsemius
>>> Alameda, CA, USA
>>>
>>> 'Any technology distinguishable from magic is insufficiently advanced.'
>>>  -Gehm's Corollary to Clarke's Third Law
>>>
>>>
>>>
>>>
>>>
>>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sat Dec  9 18:38:26 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 9 Dec 2017 09:38:26 -0800
Subject: [R] Remove
In-Reply-To: <546E1142-F2D3-49D5-86E5-EC802E1FDF7C@comcast.net>
References: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>
 <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>
 <3296595B-6750-425D-9C2E-AD1697307689@dcn.davis.ca.us>
 <CADDFq31sV_u3NLhU=T-ko=71abBdN4A8LaG8otMtsBwanQBHfQ@mail.gmail.com>
 <CA+vqiLG+Tx_73yR=M8uYM4toTXO02CDK3t0yK2AG_xn0uxrkhw@mail.gmail.com>
 <CADDFq30GCH8r=tE4UW08kpSjyym2J-USvVugLUhNMsk3tED_MQ@mail.gmail.com>
 <FD98DBA2-0D1A-4958-AC3F-B06469E28596@comcast.net>
 <CADDFq33=ztwKPyEH7k6_y8hHaXZefnr1uQ5PDxxW3QeWg7qp8A@mail.gmail.com>
 <546E1142-F2D3-49D5-86E5-EC802E1FDF7C@comcast.net>
Message-ID: <8673592E-7CF9-40AA-951F-D0212CEDB179@comcast.net>


> On Dec 8, 2017, at 6:16 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Dec 8, 2017, at 4:48 PM, Ashta <sewashm at gmail.com> wrote:
>> 
>> Hi David, Ista and all,
>> 
>> I  have one related question  Within one group I want to keep records
>> conditionally.
>> example within
>> group A I want keep rows that have  " x" values  ranged  between 15 and 30.
>> group B I want keep rows that have  " x" values  ranged  between  40 and 50.
>> group C I want keep rows that have  " x" values  ranged  between  60 and 75.
> 
> When you have a problem where there are multiple "parallel: parameters, the function to "reach for" is `mapply`. 
> 
>    mapply( your_selection_func, group_vec, min_vec, max_vec)
> 
> ... and this will probably return the values as a list (of dataframes if you build the function correctly,  so you may may need to then do:
> 
>    do.call(rbind, ...)

 do.call( rbind, 
    mapply( function(dat, grp, minx, maxx) {dat[ dat$GR==grp & dat$x >= minx & dat$x <= maxx, ]}, 
            grp=LETTERS[1:3], minx=c(15,40,60), maxx=c(30,50,75) ,
            MoreArgs=list(dat=DM),
            IMPLIFY=FALSE))
     GR  x   y
A.1   A 25 125
A.2   A 23 135
B.5   B 45 321
B.6   B 47 512
C.9   C 61 521
C.10  C 68 235

> 
> -- 
> David.
>> 
>> 
>> DM <- read.table( text='GR x y
>> A 25 125
>> A 23 135
>> A 14 145
>> A 35 230
>> B 45 321
>> B 47 512
>> B 53 123
>> B 55 451
>> C 61 521
>> C 68 235
>> C 85 258
>> C 80 654',header = TRUE, stringsAsFactors = FALSE)
>> 
>> 
>> The end result will be
>> A 25 125
>> A 23 135
>> B 45 321
>> B 47 512
>> C 61 521
>> C 68 235
>> 
>> Thank you
>> 
>> On Wed, Dec 6, 2017 at 10:34 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>> 
>>>> On Dec 6, 2017, at 4:27 PM, Ashta <sewashm at gmail.com> wrote:
>>>> 
>>>> Thank you Ista! Worked fine.
>>> 
>>> Here's another (possibly more direct in its logic?):
>>> 
>>> DM[ !ave(DM$x, DM$GR, FUN= function(x) {!length(unique(x))==1}), ]
>>> GR  x   y
>>> 5  B 25 321
>>> 6  B 25 512
>>> 7  B 25 123
>>> 8  B 25 451
>>> 
>>> --
>>> David
>>> 
>>>> On Wed, Dec 6, 2017 at 5:59 PM, Ista Zahn <istazahn at gmail.com> wrote:
>>>>> Hi Ashta,
>>>>> 
>>>>> There are many ways to do it. Here is one:
>>>>> 
>>>>> vars <- sapply(split(DM$x, DM$GR), var)
>>>>> DM[DM$GR %in% names(vars[vars > 0]), ]
>>>>> 
>>>>> Best
>>>>> Ista
>>>>> 
>>>>> On Wed, Dec 6, 2017 at 6:58 PM, Ashta <sewashm at gmail.com> wrote:
>>>>>> Thank you Jeff,
>>>>>> 
>>>>>> subset( DM, "B" != x ), this works if I know the group only.
>>>>>> But if I don't know that group in this case "B", how do I identify
>>>>>> group(s) that  all elements of x have the same value?
>>>>>> 
>>>>>> On Wed, Dec 6, 2017 at 5:48 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>>>>> subset( DM, "B" != x )
>>>>>>> 
>>>>>>> This is covered in the Introduction to R document that comes with R.
>>>>>>> --
>>>>>>> Sent from my phone. Please excuse my brevity.
>>>>>>> 
>>>>>>> On December 6, 2017 3:21:12 PM PST, David Winsemius <dwinsemius at comcast.net> wrote:
>>>>>>>> 
>>>>>>>>> On Dec 6, 2017, at 3:15 PM, Ashta <sewashm at gmail.com> wrote:
>>>>>>>>> 
>>>>>>>>> Hi all,
>>>>>>>>> In a data set I have group(GR) and two variables   x and y. I want to
>>>>>>>>> remove a  group that have  the same record for the x variable in each
>>>>>>>>> row.
>>>>>>>>> 
>>>>>>>>> DM <- read.table( text='GR x y
>>>>>>>>> A 25 125
>>>>>>>>> A 23 135
>>>>>>>>> A 14 145
>>>>>>>>> A 12 230
>>>>>>>>> B 25 321
>>>>>>>>> B 25 512
>>>>>>>>> B 25 123
>>>>>>>>> B 25 451
>>>>>>>>> C 11 521
>>>>>>>>> C 14 235
>>>>>>>>> C 15 258
>>>>>>>>> C 10 654',header = TRUE, stringsAsFactors = FALSE)
>>>>>>>>> 
>>>>>>>>> In this example the output should contain group A and C  as group B
>>>>>>>>> has   the same record  for the variable x .
>>>>>>>>> 
>>>>>>>>> The result will be
>>>>>>>>> A 25 125
>>>>>>>>> A 23 135
>>>>>>>>> A 14 145
>>>>>>>>> A 12 230
>>>>>>>>> C 11 521
>>>>>>>>> C 14 235
>>>>>>>>> C 15 258
>>>>>>>>> C 10 654
>>>>>>>> 
>>>>>>>> Try:
>>>>>>>> 
>>>>>>>> DM[ !duplicated(DM$x) , ]
>>>>>>>>> 
>>>>>>>>> How do I do it R?
>>>>>>>>> Thank you.
>>>>>>>>> 
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>> 
>>>>>>>> David Winsemius
>>>>>>>> Alameda, CA, USA
>>>>>>>> 
>>>>>>>> 'Any technology distinguishable from magic is insufficiently advanced.'
>>>>>>>> -Gehm's Corollary to Clarke's Third Law
>>>>>>>> 
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> David Winsemius
>>> Alameda, CA, USA
>>> 
>>> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
>>> 
>>> 
>>> 
>>> 
>>> 
> 
> David Winsemius
> Alameda, CA, USA
> 
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From wdunlap at tibco.com  Sat Dec  9 20:21:20 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Sat, 9 Dec 2017 11:21:20 -0800
Subject: [R] Remove
In-Reply-To: <8673592E-7CF9-40AA-951F-D0212CEDB179@comcast.net>
References: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>
 <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>
 <3296595B-6750-425D-9C2E-AD1697307689@dcn.davis.ca.us>
 <CADDFq31sV_u3NLhU=T-ko=71abBdN4A8LaG8otMtsBwanQBHfQ@mail.gmail.com>
 <CA+vqiLG+Tx_73yR=M8uYM4toTXO02CDK3t0yK2AG_xn0uxrkhw@mail.gmail.com>
 <CADDFq30GCH8r=tE4UW08kpSjyym2J-USvVugLUhNMsk3tED_MQ@mail.gmail.com>
 <FD98DBA2-0D1A-4958-AC3F-B06469E28596@comcast.net>
 <CADDFq33=ztwKPyEH7k6_y8hHaXZefnr1uQ5PDxxW3QeWg7qp8A@mail.gmail.com>
 <546E1142-F2D3-49D5-86E5-EC802E1FDF7C@comcast.net>
 <8673592E-7CF9-40AA-951F-D0212CEDB179@comcast.net>
Message-ID: <CAF8bMcZ-DgEQ3M1zy9t=Nit0vp1-YWvOVjtVfpjn4YfHWbChXQ@mail.gmail.com>

You could make numeric vectors, named by the group identifier, of the
contraints
and subscript it by group name:

> DM <- read.table( text='GR x y
+ A 25 125
+ A 23 135
+ A 14 145
+ A 35 230
+ B 45 321
+ B 47 512
+ B 53 123
+ B 55 451
+ C 61 521
+ C 68 235
+ C 85 258
+ C 80 654',header = TRUE, stringsAsFactors = FALSE)
>
> GRmin <- c(A=15, B=40, C=60)
> GRmax <- c(A=30, B=50, C=75)
> subset(DM, x>=GRmin[GR] & x <=GRmax[GR])
   GR  x   y
1   A 25 125
2   A 23 135
5   B 45 321
6   B 47 512
9   C 61 521
10  C 68 235

Or, if you want to completely avoid non-standard evaluation:
> DM[ DM$x >= GRmin[DM$GR] & DM$x <= GRmax[DM$GR], ]
   GR  x   y
1   A 25 125
2   A 23 135
5   B 45 321
6   B 47 512
9   C 61 521
10  C 68 235




Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Sat, Dec 9, 2017 at 9:38 AM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Dec 8, 2017, at 6:16 PM, David Winsemius <dwinsemius at comcast.net>
> wrote:
> >
> >
> >> On Dec 8, 2017, at 4:48 PM, Ashta <sewashm at gmail.com> wrote:
> >>
> >> Hi David, Ista and all,
> >>
> >> I  have one related question  Within one group I want to keep records
> >> conditionally.
> >> example within
> >> group A I want keep rows that have  " x" values  ranged  between 15 and
> 30.
> >> group B I want keep rows that have  " x" values  ranged  between  40
> and 50.
> >> group C I want keep rows that have  " x" values  ranged  between  60
> and 75.
> >
> > When you have a problem where there are multiple "parallel: parameters,
> the function to "reach for" is `mapply`.
> >
> >    mapply( your_selection_func, group_vec, min_vec, max_vec)
> >
> > ... and this will probably return the values as a list (of dataframes if
> you build the function correctly,  so you may may need to then do:
> >
> >    do.call(rbind, ...)
>
>  do.call( rbind,
>     mapply( function(dat, grp, minx, maxx) {dat[ dat$GR==grp & dat$x >=
> minx & dat$x <= maxx, ]},
>             grp=LETTERS[1:3], minx=c(15,40,60), maxx=c(30,50,75) ,
>             MoreArgs=list(dat=DM),
>             IMPLIFY=FALSE))
>      GR  x   y
> A.1   A 25 125
> A.2   A 23 135
> B.5   B 45 321
> B.6   B 47 512
> C.9   C 61 521
> C.10  C 68 235
>
> >
> > --
> > David.
> >>
> >>
> >> DM <- read.table( text='GR x y
> >> A 25 125
> >> A 23 135
> >> A 14 145
> >> A 35 230
> >> B 45 321
> >> B 47 512
> >> B 53 123
> >> B 55 451
> >> C 61 521
> >> C 68 235
> >> C 85 258
> >> C 80 654',header = TRUE, stringsAsFactors = FALSE)
> >>
> >>
> >> The end result will be
> >> A 25 125
> >> A 23 135
> >> B 45 321
> >> B 47 512
> >> C 61 521
> >> C 68 235
> >>
> >> Thank you
> >>
> >> On Wed, Dec 6, 2017 at 10:34 PM, David Winsemius <
> dwinsemius at comcast.net> wrote:
> >>>
> >>>> On Dec 6, 2017, at 4:27 PM, Ashta <sewashm at gmail.com> wrote:
> >>>>
> >>>> Thank you Ista! Worked fine.
> >>>
> >>> Here's another (possibly more direct in its logic?):
> >>>
> >>> DM[ !ave(DM$x, DM$GR, FUN= function(x) {!length(unique(x))==1}), ]
> >>> GR  x   y
> >>> 5  B 25 321
> >>> 6  B 25 512
> >>> 7  B 25 123
> >>> 8  B 25 451
> >>>
> >>> --
> >>> David
> >>>
> >>>> On Wed, Dec 6, 2017 at 5:59 PM, Ista Zahn <istazahn at gmail.com> wrote:
> >>>>> Hi Ashta,
> >>>>>
> >>>>> There are many ways to do it. Here is one:
> >>>>>
> >>>>> vars <- sapply(split(DM$x, DM$GR), var)
> >>>>> DM[DM$GR %in% names(vars[vars > 0]), ]
> >>>>>
> >>>>> Best
> >>>>> Ista
> >>>>>
> >>>>> On Wed, Dec 6, 2017 at 6:58 PM, Ashta <sewashm at gmail.com> wrote:
> >>>>>> Thank you Jeff,
> >>>>>>
> >>>>>> subset( DM, "B" != x ), this works if I know the group only.
> >>>>>> But if I don't know that group in this case "B", how do I identify
> >>>>>> group(s) that  all elements of x have the same value?
> >>>>>>
> >>>>>> On Wed, Dec 6, 2017 at 5:48 PM, Jeff Newmiller <
> jdnewmil at dcn.davis.ca.us> wrote:
> >>>>>>> subset( DM, "B" != x )
> >>>>>>>
> >>>>>>> This is covered in the Introduction to R document that comes with
> R.
> >>>>>>> --
> >>>>>>> Sent from my phone. Please excuse my brevity.
> >>>>>>>
> >>>>>>> On December 6, 2017 3:21:12 PM PST, David Winsemius <
> dwinsemius at comcast.net> wrote:
> >>>>>>>>
> >>>>>>>>> On Dec 6, 2017, at 3:15 PM, Ashta <sewashm at gmail.com> wrote:
> >>>>>>>>>
> >>>>>>>>> Hi all,
> >>>>>>>>> In a data set I have group(GR) and two variables   x and y. I
> want to
> >>>>>>>>> remove a  group that have  the same record for the x variable in
> each
> >>>>>>>>> row.
> >>>>>>>>>
> >>>>>>>>> DM <- read.table( text='GR x y
> >>>>>>>>> A 25 125
> >>>>>>>>> A 23 135
> >>>>>>>>> A 14 145
> >>>>>>>>> A 12 230
> >>>>>>>>> B 25 321
> >>>>>>>>> B 25 512
> >>>>>>>>> B 25 123
> >>>>>>>>> B 25 451
> >>>>>>>>> C 11 521
> >>>>>>>>> C 14 235
> >>>>>>>>> C 15 258
> >>>>>>>>> C 10 654',header = TRUE, stringsAsFactors = FALSE)
> >>>>>>>>>
> >>>>>>>>> In this example the output should contain group A and C  as
> group B
> >>>>>>>>> has   the same record  for the variable x .
> >>>>>>>>>
> >>>>>>>>> The result will be
> >>>>>>>>> A 25 125
> >>>>>>>>> A 23 135
> >>>>>>>>> A 14 145
> >>>>>>>>> A 12 230
> >>>>>>>>> C 11 521
> >>>>>>>>> C 14 235
> >>>>>>>>> C 15 258
> >>>>>>>>> C 10 654
> >>>>>>>>
> >>>>>>>> Try:
> >>>>>>>>
> >>>>>>>> DM[ !duplicated(DM$x) , ]
> >>>>>>>>>
> >>>>>>>>> How do I do it R?
> >>>>>>>>> Thank you.
> >>>>>>>>>
> >>>>>>>>> ______________________________________________
> >>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> see
> >>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>> PLEASE do read the posting guide
> >>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>> and provide commented, minimal, self-contained, reproducible
> code.
> >>>>>>>>
> >>>>>>>> David Winsemius
> >>>>>>>> Alameda, CA, USA
> >>>>>>>>
> >>>>>>>> 'Any technology distinguishable from magic is insufficiently
> advanced.'
> >>>>>>>> -Gehm's Corollary to Clarke's Third Law
> >>>>>>>>
> >>>>>>>> ______________________________________________
> >>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>> PLEASE do read the posting guide
> >>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>> David Winsemius
> >>> Alameda, CA, USA
> >>>
> >>> 'Any technology distinguishable from magic is insufficiently
> advanced.'   -Gehm's Corollary to Clarke's Third Law
> >>>
> >>>
> >>>
> >>>
> >>>
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> > 'Any technology distinguishable from magic is insufficiently advanced.'
>  -Gehm's Corollary to Clarke's Third Law
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> 'Any technology distinguishable from magic is insufficiently advanced.'
>  -Gehm's Corollary to Clarke's Third Law
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From sewashm at gmail.com  Sat Dec  9 22:53:27 2017
From: sewashm at gmail.com (Ashta)
Date: Sat, 9 Dec 2017 15:53:27 -0600
Subject: [R] Remove
In-Reply-To: <CAF8bMcZ-DgEQ3M1zy9t=Nit0vp1-YWvOVjtVfpjn4YfHWbChXQ@mail.gmail.com>
References: <CADDFq33bJnOv_=Q3taq6rQVrtesuSPrRcJB=frsDg_msPNZ2BQ@mail.gmail.com>
 <90A3E93F-0D48-48E8-931F-F3160C661D33@comcast.net>
 <3296595B-6750-425D-9C2E-AD1697307689@dcn.davis.ca.us>
 <CADDFq31sV_u3NLhU=T-ko=71abBdN4A8LaG8otMtsBwanQBHfQ@mail.gmail.com>
 <CA+vqiLG+Tx_73yR=M8uYM4toTXO02CDK3t0yK2AG_xn0uxrkhw@mail.gmail.com>
 <CADDFq30GCH8r=tE4UW08kpSjyym2J-USvVugLUhNMsk3tED_MQ@mail.gmail.com>
 <FD98DBA2-0D1A-4958-AC3F-B06469E28596@comcast.net>
 <CADDFq33=ztwKPyEH7k6_y8hHaXZefnr1uQ5PDxxW3QeWg7qp8A@mail.gmail.com>
 <546E1142-F2D3-49D5-86E5-EC802E1FDF7C@comcast.net>
 <8673592E-7CF9-40AA-951F-D0212CEDB179@comcast.net>
 <CAF8bMcZ-DgEQ3M1zy9t=Nit0vp1-YWvOVjtVfpjn4YfHWbChXQ@mail.gmail.com>
Message-ID: <CADDFq329C7kSU0zwZsEW01+1ZxA5FYVT4arrpE3sg=hrfPjOLA@mail.gmail.com>

Thank you All !!

Now, I have plenty of options to chose.


On Sat, Dec 9, 2017 at 1:21 PM, William Dunlap <wdunlap at tibco.com> wrote:
> You could make numeric vectors, named by the group identifier, of the
> contraints
> and subscript it by group name:
>
>> DM <- read.table( text='GR x y
> + A 25 125
> + A 23 135
> + A 14 145
> + A 35 230
> + B 45 321
> + B 47 512
> + B 53 123
> + B 55 451
> + C 61 521
> + C 68 235
> + C 85 258
> + C 80 654',header = TRUE, stringsAsFactors = FALSE)
>>
>> GRmin <- c(A=15, B=40, C=60)
>> GRmax <- c(A=30, B=50, C=75)
>> subset(DM, x>=GRmin[GR] & x <=GRmax[GR])
>    GR  x   y
> 1   A 25 125
> 2   A 23 135
> 5   B 45 321
> 6   B 47 512
> 9   C 61 521
> 10  C 68 235
>
> Or, if you want to completely avoid non-standard evaluation:
>> DM[ DM$x >= GRmin[DM$GR] & DM$x <= GRmax[DM$GR], ]
>    GR  x   y
> 1   A 25 125
> 2   A 23 135
> 5   B 45 321
> 6   B 47 512
> 9   C 61 521
> 10  C 68 235
>
>
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Sat, Dec 9, 2017 at 9:38 AM, David Winsemius <dwinsemius at comcast.net>
> wrote:
>>
>>
>> > On Dec 8, 2017, at 6:16 PM, David Winsemius <dwinsemius at comcast.net>
>> > wrote:
>> >
>> >
>> >> On Dec 8, 2017, at 4:48 PM, Ashta <sewashm at gmail.com> wrote:
>> >>
>> >> Hi David, Ista and all,
>> >>
>> >> I  have one related question  Within one group I want to keep records
>> >> conditionally.
>> >> example within
>> >> group A I want keep rows that have  " x" values  ranged  between 15 and
>> >> 30.
>> >> group B I want keep rows that have  " x" values  ranged  between  40
>> >> and 50.
>> >> group C I want keep rows that have  " x" values  ranged  between  60
>> >> and 75.
>> >
>> > When you have a problem where there are multiple "parallel: parameters,
>> > the function to "reach for" is `mapply`.
>> >
>> >    mapply( your_selection_func, group_vec, min_vec, max_vec)
>> >
>> > ... and this will probably return the values as a list (of dataframes if
>> > you build the function correctly,  so you may may need to then do:
>> >
>> >    do.call(rbind, ...)
>>
>>  do.call( rbind,
>>     mapply( function(dat, grp, minx, maxx) {dat[ dat$GR==grp & dat$x >=
>> minx & dat$x <= maxx, ]},
>>             grp=LETTERS[1:3], minx=c(15,40,60), maxx=c(30,50,75) ,
>>             MoreArgs=list(dat=DM),
>>             IMPLIFY=FALSE))
>>      GR  x   y
>> A.1   A 25 125
>> A.2   A 23 135
>> B.5   B 45 321
>> B.6   B 47 512
>> C.9   C 61 521
>> C.10  C 68 235
>>
>> >
>> > --
>> > David.
>> >>
>> >>
>> >> DM <- read.table( text='GR x y
>> >> A 25 125
>> >> A 23 135
>> >> A 14 145
>> >> A 35 230
>> >> B 45 321
>> >> B 47 512
>> >> B 53 123
>> >> B 55 451
>> >> C 61 521
>> >> C 68 235
>> >> C 85 258
>> >> C 80 654',header = TRUE, stringsAsFactors = FALSE)
>> >>
>> >>
>> >> The end result will be
>> >> A 25 125
>> >> A 23 135
>> >> B 45 321
>> >> B 47 512
>> >> C 61 521
>> >> C 68 235
>> >>
>> >> Thank you
>> >>
>> >> On Wed, Dec 6, 2017 at 10:34 PM, David Winsemius
>> >> <dwinsemius at comcast.net> wrote:
>> >>>
>> >>>> On Dec 6, 2017, at 4:27 PM, Ashta <sewashm at gmail.com> wrote:
>> >>>>
>> >>>> Thank you Ista! Worked fine.
>> >>>
>> >>> Here's another (possibly more direct in its logic?):
>> >>>
>> >>> DM[ !ave(DM$x, DM$GR, FUN= function(x) {!length(unique(x))==1}), ]
>> >>> GR  x   y
>> >>> 5  B 25 321
>> >>> 6  B 25 512
>> >>> 7  B 25 123
>> >>> 8  B 25 451
>> >>>
>> >>> --
>> >>> David
>> >>>
>> >>>> On Wed, Dec 6, 2017 at 5:59 PM, Ista Zahn <istazahn at gmail.com> wrote:
>> >>>>> Hi Ashta,
>> >>>>>
>> >>>>> There are many ways to do it. Here is one:
>> >>>>>
>> >>>>> vars <- sapply(split(DM$x, DM$GR), var)
>> >>>>> DM[DM$GR %in% names(vars[vars > 0]), ]
>> >>>>>
>> >>>>> Best
>> >>>>> Ista
>> >>>>>
>> >>>>> On Wed, Dec 6, 2017 at 6:58 PM, Ashta <sewashm at gmail.com> wrote:
>> >>>>>> Thank you Jeff,
>> >>>>>>
>> >>>>>> subset( DM, "B" != x ), this works if I know the group only.
>> >>>>>> But if I don't know that group in this case "B", how do I identify
>> >>>>>> group(s) that  all elements of x have the same value?
>> >>>>>>
>> >>>>>> On Wed, Dec 6, 2017 at 5:48 PM, Jeff Newmiller
>> >>>>>> <jdnewmil at dcn.davis.ca.us> wrote:
>> >>>>>>> subset( DM, "B" != x )
>> >>>>>>>
>> >>>>>>> This is covered in the Introduction to R document that comes with
>> >>>>>>> R.
>> >>>>>>> --
>> >>>>>>> Sent from my phone. Please excuse my brevity.
>> >>>>>>>
>> >>>>>>> On December 6, 2017 3:21:12 PM PST, David Winsemius
>> >>>>>>> <dwinsemius at comcast.net> wrote:
>> >>>>>>>>
>> >>>>>>>>> On Dec 6, 2017, at 3:15 PM, Ashta <sewashm at gmail.com> wrote:
>> >>>>>>>>>
>> >>>>>>>>> Hi all,
>> >>>>>>>>> In a data set I have group(GR) and two variables   x and y. I
>> >>>>>>>>> want to
>> >>>>>>>>> remove a  group that have  the same record for the x variable in
>> >>>>>>>>> each
>> >>>>>>>>> row.
>> >>>>>>>>>
>> >>>>>>>>> DM <- read.table( text='GR x y
>> >>>>>>>>> A 25 125
>> >>>>>>>>> A 23 135
>> >>>>>>>>> A 14 145
>> >>>>>>>>> A 12 230
>> >>>>>>>>> B 25 321
>> >>>>>>>>> B 25 512
>> >>>>>>>>> B 25 123
>> >>>>>>>>> B 25 451
>> >>>>>>>>> C 11 521
>> >>>>>>>>> C 14 235
>> >>>>>>>>> C 15 258
>> >>>>>>>>> C 10 654',header = TRUE, stringsAsFactors = FALSE)
>> >>>>>>>>>
>> >>>>>>>>> In this example the output should contain group A and C  as
>> >>>>>>>>> group B
>> >>>>>>>>> has   the same record  for the variable x .
>> >>>>>>>>>
>> >>>>>>>>> The result will be
>> >>>>>>>>> A 25 125
>> >>>>>>>>> A 23 135
>> >>>>>>>>> A 14 145
>> >>>>>>>>> A 12 230
>> >>>>>>>>> C 11 521
>> >>>>>>>>> C 14 235
>> >>>>>>>>> C 15 258
>> >>>>>>>>> C 10 654
>> >>>>>>>>
>> >>>>>>>> Try:
>> >>>>>>>>
>> >>>>>>>> DM[ !duplicated(DM$x) , ]
>> >>>>>>>>>
>> >>>>>>>>> How do I do it R?
>> >>>>>>>>> Thank you.
>> >>>>>>>>>
>> >>>>>>>>> ______________________________________________
>> >>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>> >>>>>>>>> see
>> >>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>>>>>> PLEASE do read the posting guide
>> >>>>>>>> http://www.R-project.org/posting-guide.html
>> >>>>>>>>> and provide commented, minimal, self-contained, reproducible
>> >>>>>>>>> code.
>> >>>>>>>>
>> >>>>>>>> David Winsemius
>> >>>>>>>> Alameda, CA, USA
>> >>>>>>>>
>> >>>>>>>> 'Any technology distinguishable from magic is insufficiently
>> >>>>>>>> advanced.'
>> >>>>>>>> -Gehm's Corollary to Clarke's Third Law
>> >>>>>>>>
>> >>>>>>>> ______________________________________________
>> >>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>>>>> PLEASE do read the posting guide
>> >>>>>>>> http://www.R-project.org/posting-guide.html
>> >>>>>>>> and provide commented, minimal, self-contained, reproducible
>> >>>>>>>> code.
>> >>>>>>
>> >>>>>> ______________________________________________
>> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>>> PLEASE do read the posting guide
>> >>>>>> http://www.R-project.org/posting-guide.html
>> >>>>>> and provide commented, minimal, self-contained, reproducible code.
>> >>>>
>> >>>> ______________________________________________
>> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>> PLEASE do read the posting guide
>> >>>> http://www.R-project.org/posting-guide.html
>> >>>> and provide commented, minimal, self-contained, reproducible code.
>> >>>
>> >>> David Winsemius
>> >>> Alameda, CA, USA
>> >>>
>> >>> 'Any technology distinguishable from magic is insufficiently
>> >>> advanced.'   -Gehm's Corollary to Clarke's Third Law
>> >>>
>> >>>
>> >>>
>> >>>
>> >>>
>> >
>> > David Winsemius
>> > Alameda, CA, USA
>> >
>> > 'Any technology distinguishable from magic is insufficiently advanced.'
>> > -Gehm's Corollary to Clarke's Third Law
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> David Winsemius
>> Alameda, CA, USA
>>
>> 'Any technology distinguishable from magic is insufficiently advanced.'
>> -Gehm's Corollary to Clarke's Third Law
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>


From ajdamico at gmail.com  Sun Dec 10 06:06:33 2017
From: ajdamico at gmail.com (Anthony Damico)
Date: Sun, 10 Dec 2017 00:06:33 -0500
Subject: [R] svyglm
In-Reply-To: <1859697545.1252505.1512399404304@mail.yahoo.com>
References: <1859697545.1252505.1512399404304.ref@mail.yahoo.com>
 <1859697545.1252505.1512399404304@mail.yahoo.com>
Message-ID: <CAOwvMDx2QOoD08wREYw9g+Oe0HG567P8dioc1TXr20HTw3vbGQ@mail.gmail.com>

hi, could you create a reproducible example starting from
http://asdfree.com/pesquisa-nacional-de-saude-pns.html  ?  thanks

On Mon, Dec 4, 2017 at 9:56 AM, Luciane Maria Pilotto via R-help <
r-help at r-project.org> wrote:

> Hi,
> I am trying to run analyzes incorporating sample weight, strata and
> cluster (three-stage sample) with PNS data (national health survey) and is
> giving error. I describe below the commands used. I could not make the code
> reproducible properly.
> Thanks,
> #################################################
> library(survey)####change to 0 and 1 variable outcomedent2<-ifelse(
> consdentcat2==2,0,1)table(dent2) dent2<-as.factor(dent2)str(dent2)reg<-cbind(reg,
> dent2)
> #tchange to factor str(sexo)reg$sexo <- as.factor((reg$sexo))
> #################
> pns2013design<-svydesign(id=~upa, nest=TRUE, strata = estrato, weight =
> peso, data = reg)
>
> PNS<-svyglm(dent2~sexo,design=pns2013design, method="logistic", data =
> reg)
>
> Error in logistic(x = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> :   unused arguments (x = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1,############################
> ############################################################
> ########################################################################
>
> ###NOT running SVYGLM command#data for R-helpbteste <- reg[1:10, c(1, 4,
> 26, 27, 28)]#dput(bteste)###omitting most of the lines "9967.223281",
> "9967.870849", "9968.207979", "997.0045537",     "997.1451224",
> "997.1574275", "997.1782368", "997.1812338",     "997.2480231",
> "997.2531051", "997.3803145", "997.4345619",     "997.4363662",
> "997.5344599", "997.5964572", "997.7439813",     "997.8360773",
> "997.8770935", "997.8811374", "997.9147096",     "997.9563562",
> "9974.30392", "9974.344482", "9975.656981",     "9977.382263",
> "9979.999691", "998.0053953", "998.1069038",     "998.2140192",
> "998.2655421", "998.308316", "998.3090242",     "998.3579509",
> "998.3656231", "998.3766007", "998.6844831",     "998.7030027",
> "998.7112321", "998.8021132", "998.8839799",     "998.9225688",
> "998.9270228", "998.9337225", "9983.555066",     "9985.353117",
> "9989.517638", "999.0713699", "999.0771916",     "999.1021413",
> "999.1779133", "999.2539765", "999.3435971",     "999.3809978",
> "999.6348707", "999.7597985", "999.8002746",     "999.8819267",
> "999.8821907", "999.8921074", "999.9211427",     "9991.102816",
> "9991.440035", "9994.626337", "9994.723654",     "9996.637923",
> "9998.491819"), class = "factor")), .Names = c("consdentcat2", "sexo",
> "estrato", "upa", "pesomorcc"), row.names = c(NA, 10L), class =
> "data.frame")##################################################
> bteste1 <-bteste[1:10, ]#bteste1   consdentcat2 sexo estrato     upa
>  pesomorcc1             2    1 1110011 1100002 418.76819022             2
>   1 1110011 1100002 317.13175793             2    1 1110011 1100002
> 467.09452884             1    1 1110011 1100002 209.38409515             2
>   1 1110011 1100002 209.38409516             2    1 1110011 1100002
> 418.76819027             2    1 1110011 1100002 233.54726448             2
>   1 1110011 1100002 628.15228539             2    1 1110011 1100002
> 317.131757910            2    2 1110011 1100002 321.5014524>
>              _________________________________________
> Luciane Maria Pilotto
>
>
>
> |  | Livre de v?rus. www.avast.com.  |
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From arabianahmad at googlemail.com  Sun Dec 10 05:51:17 2017
From: arabianahmad at googlemail.com (ah a)
Date: Sun, 10 Dec 2017 08:21:17 +0330
Subject: [R] MSVAR model
Message-ID: <CALjW5c+z1Byyv9V=wzvT+F-kDRPKcwRqzJ3J+MPYq8Na9e-Mwg@mail.gmail.com>

Hello,

As I'm interested to search about the monetary transmission channel in our
country by MSVAR model,Could you do me favor and tell me How I can run
different types of MSVAR model (such as MSIAH(2)-VAR(2)) and finding
impulse response function in different regimes and also variance
decomposition?

Thank you very much in advance.

Best Regards,
Ahmad

	[[alternative HTML version deleted]]


From varinsacha at yahoo.fr  Sun Dec 10 16:19:59 2017
From: varinsacha at yahoo.fr (varin sacha)
Date: Sun, 10 Dec 2017 15:19:59 +0000 (UTC)
Subject: [R] Confidence intervals around the MIC (Maximal information
 coefficient)
References: <1832704878.3094043.1512919199329.ref@mail.yahoo.com>
Message-ID: <1832704878.3094043.1512919199329@mail.yahoo.com>

Dear R-Experts,

Here below is my R code (reproducible example) to calculate the confidence intervals around the spearman coefficient.

##########
C=c(2,4,5,6,3,4,5,7,8,7,6,5,6,7,7,8,5,4,3,2)
D=c(3,5,4,6,7,2,3,1,2,4,5,4,6,4,5,4,3,2,8,9)
cor(C,D,method= "spearman")
library(boot)
myCor=function(data,index){
cor(data[index, ])[1,2]
}
results=boot(data=cbind(C,D),statistic=myCor, R=2000)
boot.ci(results,type="all")
##########


Now, I would like to calculate the CIs around the MIC (Maximal information coefficient). The MIC can be calculated thanks to the library(minerva). I don?t get the CIs for the MIC, I don?t know how to change my R codes to get the CIs around the MIC. Any help would be highly appreciated  :

##########
library(minerva)
mine(C,D)
library(boot)
myCor=function(data,index){
mic(data[index, ])[1,2]
}
results=boot(data=cbind(C,D),statistic=myCor, R=2000)
boot.ci(results,type="all")
##########


From ruipbarradas at sapo.pt  Sun Dec 10 16:34:07 2017
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Sun, 10 Dec 2017 15:34:07 +0000
Subject: [R] Confidence intervals around the MIC (Maximal information
 coefficient)
In-Reply-To: <1832704878.3094043.1512919199329@mail.yahoo.com>
References: <1832704878.3094043.1512919199329.ref@mail.yahoo.com>
 <1832704878.3094043.1512919199329@mail.yahoo.com>
Message-ID: <54d46947-e138-854d-320d-9a2b2a23072a@sapo.pt>

Hello,

First of all, when I tried to use function mic I got an error.

  mic(cbind(C, D))
Error in mic(cbind(C, D)) : could not find function "mic"

So I've changed your function myCor and all went well, with a warning 
relative to BCa intervals.

myCor <- function(data, index){
     mine(data[index, ])$MIC
}
results=boot(data = cbind(C,D), statistic = myCor, R = 2000)
boot.ci(results,type="all")


Hope this helps,

Rui Barradas

On 12/10/2017 3:19 PM, varin sacha via R-help wrote:
> Dear R-Experts,
> 
> Here below is my R code (reproducible example) to calculate the confidence intervals around the spearman coefficient.
> 
> ##########
> C=c(2,4,5,6,3,4,5,7,8,7,6,5,6,7,7,8,5,4,3,2)
> D=c(3,5,4,6,7,2,3,1,2,4,5,4,6,4,5,4,3,2,8,9)
> cor(C,D,method= "spearman")
> library(boot)
> myCor=function(data,index){
> cor(data[index, ])[1,2]
> }
> results=boot(data=cbind(C,D),statistic=myCor, R=2000)
> boot.ci(results,type="all")
> ##########
> 
> 
> Now, I would like to calculate the CIs around the MIC (Maximal information coefficient). The MIC can be calculated thanks to the library(minerva). I don?t get the CIs for the MIC, I don?t know how to change my R codes to get the CIs around the MIC. Any help would be highly appreciated  :
> 
> ##########
> library(minerva)
> mine(C,D)
> library(boot)
> myCor=function(data,index){
> mic(data[index, ])[1,2]
> }
> results=boot(data=cbind(C,D),statistic=myCor, R=2000)
> boot.ci(results,type="all")
> ##########
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From varinsacha at yahoo.fr  Sun Dec 10 18:07:13 2017
From: varinsacha at yahoo.fr (varin sacha)
Date: Sun, 10 Dec 2017 17:07:13 +0000 (UTC)
Subject: [R] Confidence intervals around the MIC (Maximal information
 coefficient)
In-Reply-To: <54d46947-e138-854d-320d-9a2b2a23072a@sapo.pt>
References: <1832704878.3094043.1512919199329.ref@mail.yahoo.com>
 <1832704878.3094043.1512919199329@mail.yahoo.com>
 <54d46947-e138-854d-320d-9a2b2a23072a@sapo.pt>
Message-ID: <968782862.3188807.1512925633456@mail.yahoo.com>

Hi Rui,

Many thanks. The R code works BUT the results I get are quite weird I guess !

MIC = 0.2650

Normal 95% CI = (0.9614,  1.0398)

The MIC is not inside the confidence intervals !


Is there something wrong in the R code ?


Here is the reproducible example :

##########

C=c(2,4,5,6,3,4,5,7,8,7,6,5,6,7,7,8,5,4,3,2)
D=c(3,5,4,6,7,2,3,1,2,4,5,4,6,4,5,4,3,2,8,9)
library(minerva)

mine(C,D)$MIC
library(boot)
myCor <- function(data, index){
mine(data[index, ])$MIC
}
results=boot(data = cbind(C,D), statistic = myCor, R = 2000)
boot.ci(results,type="all")

##########






________________________________
De : Rui Barradas <ruipbarradas at sapo.pt>

roject.org> 
Envoy? le : Dimanche 10 d?cembre 2017 16h34
Objet : Re: [R] Confidence intervals around the MIC (Maximal information coefficient)



Hello,

First of all, when I tried to use function mic I got an error.

  mic(cbind(C, D))
Error in mic(cbind(C, D)) : could not find function "mic"

So I've changed your function myCor and all went well, with a warning 
relative to BCa intervals.

myCor <- function(data, index){
     mine(data[index, ])$MIC
}
results=boot(data = cbind(C,D), statistic = myCor, R = 2000)
boot.ci(results,type="all")


Hope this helps,

Rui Barradas


On 12/10/2017 3:19 PM, varin sacha via R-help wrote:
> Dear R-Experts,
> 
> Here below is my R code (reproducible example) to calculate the confidence intervals around the spearman coefficient.
> 
> ##########
> C=c(2,4,5,6,3,4,5,7,8,7,6,5,6,7,7,8,5,4,3,2)
> D=c(3,5,4,6,7,2,3,1,2,4,5,4,6,4,5,4,3,2,8,9)
> cor(C,D,method= "spearman")
> library(boot)
> myCor=function(data,index){
> cor(data[index, ])[1,2]
> }
> results=boot(data=cbind(C,D),statistic=myCor, R=2000)
> boot.ci(results,type="all")
> ##########
> 
> 
> Now, I would like to calculate the CIs around the MIC (Maximal information coefficient). The MIC can be calculated thanks to the library(minerva). I don?t get the CIs for the MIC, I don?t know how to change my R codes to get the CIs around the MIC. Any help would be highly appreciated  :
> 
> ##########
> library(minerva)
> mine(C,D)
> library(boot)
> myCor=function(data,index){
> mic(data[index, ])[1,2]
> }
> results=boot(data=cbind(C,D),statistic=myCor, R=2000)
> boot.ci(results,type="all")
> ##########
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From dcarlson at tamu.edu  Sun Dec 10 19:04:38 2017
From: dcarlson at tamu.edu (David L Carlson)
Date: Sun, 10 Dec 2017 18:04:38 +0000
Subject: [R] Confidence intervals around the MIC (Maximal information
 coefficient)
In-Reply-To: <968782862.3188807.1512925633456@mail.yahoo.com>
References: <1832704878.3094043.1512919199329.ref@mail.yahoo.com>
 <1832704878.3094043.1512919199329@mail.yahoo.com>
 <54d46947-e138-854d-320d-9a2b2a23072a@sapo.pt>
 <968782862.3188807.1512925633456@mail.yahoo.com>
Message-ID: <2ef8294fc88d43f79ceb254d5062fdef@exch-2p-mbx-w2.ads.tamu.edu>

You need:

myCor <- function(data, index){
    mine(data[index, ])$MIC[1, 2]
}
results=boot(data = cbind(C,D), statistic = myCor, R = 2000)
boot.ci(results,type="all")

Look at the differences between:
mine(C, D)

and

mine(cbind(C, D))

The first returns a value, the second returns a symmetric matrix. Just like cor()



David L. Carlson
Department of Anthropology
Texas A&M University

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of varin sacha via R-help
Sent: Sunday, December 10, 2017 11:07 AM
To: Rui Barradas <ruipbarradas at sapo.pt>; R-help Mailing List <r-help at r-project.org>
Subject: Re: [R] Confidence intervals around the MIC (Maximal information coefficient)

Hi Rui,

Many thanks. The R code works BUT the results I get are quite weird I guess !

MIC = 0.2650

Normal 95% CI = (0.9614,  1.0398)

The MIC is not inside the confidence intervals !


Is there something wrong in the R code ?


Here is the reproducible example :

##########

C=c(2,4,5,6,3,4,5,7,8,7,6,5,6,7,7,8,5,4,3,2)
D=c(3,5,4,6,7,2,3,1,2,4,5,4,6,4,5,4,3,2,8,9)
library(minerva)

mine(C,D)$MIC
library(boot)
myCor <- function(data, index){
mine(data[index, ])$MIC
}
results=boot(data = cbind(C,D), statistic = myCor, R = 2000)
boot.ci(results,type="all")

##########






________________________________
De : Rui Barradas <ruipbarradas at sapo.pt>

roject.org> 
Envoy? le : Dimanche 10 d?cembre 2017 16h34
Objet : Re: [R] Confidence intervals around the MIC (Maximal information coefficient)



Hello,

First of all, when I tried to use function mic I got an error.

  mic(cbind(C, D))
Error in mic(cbind(C, D)) : could not find function "mic"

So I've changed your function myCor and all went well, with a warning 
relative to BCa intervals.

myCor <- function(data, index){
     mine(data[index, ])$MIC
}
results=boot(data = cbind(C,D), statistic = myCor, R = 2000)
boot.ci(results,type="all")


Hope this helps,

Rui Barradas


On 12/10/2017 3:19 PM, varin sacha via R-help wrote:
> Dear R-Experts,
> 
> Here below is my R code (reproducible example) to calculate the confidence intervals around the spearman coefficient.
> 
> ##########
> C=c(2,4,5,6,3,4,5,7,8,7,6,5,6,7,7,8,5,4,3,2)
> D=c(3,5,4,6,7,2,3,1,2,4,5,4,6,4,5,4,3,2,8,9)
> cor(C,D,method= "spearman")
> library(boot)
> myCor=function(data,index){
> cor(data[index, ])[1,2]
> }
> results=boot(data=cbind(C,D),statistic=myCor, R=2000)
> boot.ci(results,type="all")
> ##########
> 
> 
> Now, I would like to calculate the CIs around the MIC (Maximal information coefficient). The MIC can be calculated thanks to the library(minerva). I don?t get the CIs for the MIC, I don?t know how to change my R codes to get the CIs around the MIC. Any help would be highly appreciated  :
> 
> ##########
> library(minerva)
> mine(C,D)
> library(boot)
> myCor=function(data,index){
> mic(data[index, ])[1,2]
> }
> results=boot(data=cbind(C,D),statistic=myCor, R=2000)
> boot.ci(results,type="all")
> ##########
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From dwinsemius at comcast.net  Sun Dec 10 19:23:17 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 10 Dec 2017 10:23:17 -0800
Subject: [R] MSVAR model
In-Reply-To: <CALjW5c+z1Byyv9V=wzvT+F-kDRPKcwRqzJ3J+MPYq8Na9e-Mwg@mail.gmail.com>
References: <CALjW5c+z1Byyv9V=wzvT+F-kDRPKcwRqzJ3J+MPYq8Na9e-Mwg@mail.gmail.com>
Message-ID: <F964CFD9-5C95-4D53-9EA2-BAA33AAF637A@comcast.net>


> On Dec 9, 2017, at 8:51 PM, ah a via R-help <r-help at r-project.org> wrote:
> 
> Hello,
> 
> As I'm interested to search about the monetary transmission channel in our
> country by MSVAR model,Could you do me favor and tell me How I can run
> different types of MSVAR model (such as MSIAH(2)-VAR(2)) and finding
> impulse response function in different regimes and also variance
> decomposition?

This is uncomfortably close to an existing fortune:

fortunes::fortune("brain surgery")

Why don't you read the Posting Guide. Then perhaps you can improve your question?

-- 
David.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From varinsacha at yahoo.fr  Sun Dec 10 20:55:49 2017
From: varinsacha at yahoo.fr (varin sacha)
Date: Sun, 10 Dec 2017 19:55:49 +0000 (UTC)
Subject: [R] Confidence intervals around the MIC (Maximal information
 coefficient)
In-Reply-To: <2ef8294fc88d43f79ceb254d5062fdef@exch-2p-mbx-w2.ads.tamu.edu>
References: <1832704878.3094043.1512919199329.ref@mail.yahoo.com>
 <1832704878.3094043.1512919199329@mail.yahoo.com>
 <54d46947-e138-854d-320d-9a2b2a23072a@sapo.pt>
 <968782862.3188807.1512925633456@mail.yahoo.com>
 <2ef8294fc88d43f79ceb254d5062fdef@exch-2p-mbx-w2.ads.tamu.edu>
Message-ID: <1211457719.3337336.1512935749582@mail.yahoo.com>

Hi David, Rui,

Thanks for your precious responses. It works !

Best,


________________________________
De : David L Carlson <dcarlson at tamu.edu>

.pt> 
Cc : "r-help at r-project.org" <r-help at r-project.org>
Envoy? le : Dimanche 10 d?cembre 2017 19h05
Objet : RE: [R] Confidence intervals around the MIC (Maximal information coefficient)



You need:

myCor <- function(data, index){
    mine(data[index, ])$MIC[1, 2]
}
results=boot(data = cbind(C,D), statistic = myCor, R = 2000)
boot.ci(results,type="all")

Look at the differences between:
mine(C, D)

and

mine(cbind(C, D))

The first returns a value, the second returns a symmetric matrix. Just like cor()



David L. Carlson
Department of Anthropology
Texas A&M University


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of varin sacha via R-help
Sent: Sunday, December 10, 2017 11:07 AM
To: Rui Barradas <ruipbarradas at sapo.pt>; R-help Mailing List <r-help at r-project.org>
Subject: Re: [R] Confidence intervals around the MIC (Maximal information coefficient)

Hi Rui,

Many thanks. The R code works BUT the results I get are quite weird I guess !

MIC = 0.2650

Normal 95% CI = (0.9614,  1.0398)

[[elided Yahoo spam]]


Is there something wrong in the R code ?


Here is the reproducible example :

##########

C=c(2,4,5,6,3,4,5,7,8,7,6,5,6,7,7,8,5,4,3,2)
D=c(3,5,4,6,7,2,3,1,2,4,5,4,6,4,5,4,3,2,8,9)
library(minerva)

mine(C,D)$MIC
library(boot)
myCor <- function(data, index){
mine(data[index, ])$MIC
}
results=boot(data = cbind(C,D), statistic = myCor, R = 2000)
boot.ci(results,type="all")

##########






________________________________
De : Rui Barradas <ruipbarradas at sapo.pt>

roject.org> 
Envoy? le : Dimanche 10 d?cembre 2017 16h34
Objet : Re: [R] Confidence intervals around the MIC (Maximal information coefficient)



Hello,

First of all, when I tried to use function mic I got an error.

  mic(cbind(C, D))
Error in mic(cbind(C, D)) : could not find function "mic"

So I've changed your function myCor and all went well, with a warning 
relative to BCa intervals.

myCor <- function(data, index){
     mine(data[index, ])$MIC
}
results=boot(data = cbind(C,D), statistic = myCor, R = 2000)
boot.ci(results,type="all")


Hope this helps,

Rui Barradas


On 12/10/2017 3:19 PM, varin sacha via R-help wrote:
> Dear R-Experts,
> 
> Here below is my R code (reproducible example) to calculate the confidence intervals around the spearman coefficient.
> 
> ##########
> C=c(2,4,5,6,3,4,5,7,8,7,6,5,6,7,7,8,5,4,3,2)
> D=c(3,5,4,6,7,2,3,1,2,4,5,4,6,4,5,4,3,2,8,9)
> cor(C,D,method= "spearman")
> library(boot)
> myCor=function(data,index){
> cor(data[index, ])[1,2]
> }
> results=boot(data=cbind(C,D),statistic=myCor, R=2000)
> boot.ci(results,type="all")
> ##########
> 
> 
> Now, I would like to calculate the CIs around the MIC (Maximal information coefficient). The MIC can be calculated thanks to the library(minerva). I don?t get the CIs for the MIC, I don?t know how to change my R codes to get the CIs around the MIC. Any help would be highly appreciated  :
> 
> ##########
> library(minerva)
> mine(C,D)
> library(boot)
> myCor=function(data,index){
> mic(data[index, ])[1,2]
> }
> results=boot(data=cbind(C,D),statistic=myCor, R=2000)
> boot.ci(results,type="all")
> ##########
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From pnsinha68 at gmail.com  Mon Dec 11 08:12:26 2017
From: pnsinha68 at gmail.com (Partha Sinha)
Date: Mon, 11 Dec 2017 12:42:26 +0530
Subject: [R] file creating
Message-ID: <CADcgpJcck27VyP0YeBoBrGMgBEKKAHWCZTuZMGF=wirU7W0NnQ@mail.gmail.com>

I am using R(3.4.3), Win 7(extreme edition) 32 bit,
I have 10 excel files data1,xls, data2.xls .. till data10.xls.
I want to create 10 dataframes . How to do ?
regards
Parth

	[[alternative HTML version deleted]]


From ulrik.stervbo at gmail.com  Mon Dec 11 08:20:11 2017
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Mon, 11 Dec 2017 07:20:11 +0000
Subject: [R] file creating
In-Reply-To: <CADcgpJcck27VyP0YeBoBrGMgBEKKAHWCZTuZMGF=wirU7W0NnQ@mail.gmail.com>
References: <CADcgpJcck27VyP0YeBoBrGMgBEKKAHWCZTuZMGF=wirU7W0NnQ@mail.gmail.com>
Message-ID: <CAKVAULPRZmrGVQy9crMhTHdVNU9h2SQ=_eB1OMn+3Tunm-4qpQ@mail.gmail.com>

You could loop over the file names, read each excel file and store the
individual data frames in a list using lapply.

I prefer to read excel files with the package readxl.

The code could be along the lines of

library(readxl)
my_files <- c("file1", "file2")

lapply(my_files, read_excel)

HTH
Ulrik

Partha Sinha <pnsinha68 at gmail.com> schrieb am Mo., 11. Dez. 2017, 08:13:

> I am using R(3.4.3), Win 7(extreme edition) 32 bit,
> I have 10 excel files data1,xls, data2.xls .. till data10.xls.
> I want to create 10 dataframes . How to do ?
> regards
> Parth
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dkrstajic at hotmail.com  Mon Dec 11 13:53:25 2017
From: dkrstajic at hotmail.com (Damjan Krstajic)
Date: Mon, 11 Dec 2017 12:53:25 +0000
Subject: [R] Gaussian Process Classification R packages
Message-ID: <VI1PR08MB105439B29680FF36A9DAB23BB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>

Dear All,


I am struggling to find an R package which contains a function for building a Gaussian Process model for binary classification which may produce prediction intervals for predicted probabilities. I would be grateful if somebody could point me to such package.


Thank you very much in advance.

DK

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Mon Dec 11 16:50:47 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 11 Dec 2017 07:50:47 -0800
Subject: [R] Gaussian Process Classification R packages
In-Reply-To: <VI1PR08MB105439B29680FF36A9DAB23BB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
References: <VI1PR08MB105439B29680FF36A9DAB23BB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
Message-ID: <CAGxFJbTNJ-q0f3fkKMONVHOVHsFBovzm8r5S5BwTrcRT3A8hyg@mail.gmail.com>

Google it!

"R Gaussian process model binary classification."

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Mon, Dec 11, 2017 at 4:53 AM, Damjan Krstajic <dkrstajic at hotmail.com>
wrote:

> Dear All,
>
>
> I am struggling to find an R package which contains a function for
> building a Gaussian Process model for binary classification which may
> produce prediction intervals for predicted probabilities. I would be
> grateful if somebody could point me to such package.
>
>
> Thank you very much in advance.
>
> DK
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dkrstajic at hotmail.com  Mon Dec 11 17:06:43 2017
From: dkrstajic at hotmail.com (Damjan Krstajic)
Date: Mon, 11 Dec 2017 16:06:43 +0000
Subject: [R] Gaussian Process Classification R packages
In-Reply-To: <CAGxFJbTNJ-q0f3fkKMONVHOVHsFBovzm8r5S5BwTrcRT3A8hyg@mail.gmail.com>
References: <VI1PR08MB105439B29680FF36A9DAB23BB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>,
 <CAGxFJbTNJ-q0f3fkKMONVHOVHsFBovzm8r5S5BwTrcRT3A8hyg@mail.gmail.com>
Message-ID: <VI1PR08MB1054AF96FA95CCF8F83D502FB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>

I have kindly asked for help and I am sad to receive such a reply from some on the r-help list.


I did google it prior to sending my request, and I could not find any R package which provides GP classification model which produces prediction intervals for each sample. I would be grateful if anybody could inform me about it. Thank you.


________________________________
From: Bert Gunter <bgunter.4567 at gmail.com>
Sent: 11 December 2017 15:50
To: Damjan Krstajic
Cc: r-help at r-project.org
Subject: Re: [R] Gaussian Process Classification R packages

Google it!

"R Gaussian process model binary classification."

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Mon, Dec 11, 2017 at 4:53 AM, Damjan Krstajic <dkrstajic at hotmail.com<mailto:dkrstajic at hotmail.com>> wrote:
Dear All,


I am struggling to find an R package which contains a function for building a Gaussian Process model for binary classification which may produce prediction intervals for predicted probabilities. I would be grateful if somebody could point me to such package.


Thank you very much in advance.

DK

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From ccberry at ucsd.edu  Mon Dec 11 18:04:54 2017
From: ccberry at ucsd.edu (Berry, Charles)
Date: Mon, 11 Dec 2017 17:04:54 +0000
Subject: [R] Gaussian Process Classification R packages
In-Reply-To: <VI1PR08MB1054AF96FA95CCF8F83D502FB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
References: <VI1PR08MB105439B29680FF36A9DAB23BB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
 <CAGxFJbTNJ-q0f3fkKMONVHOVHsFBovzm8r5S5BwTrcRT3A8hyg@mail.gmail.com>
 <VI1PR08MB1054AF96FA95CCF8F83D502FB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
Message-ID: <16D9D99C-1D42-45B7-A9D3-33359021C4B3@ucsd.edu>


> On Dec 11, 2017, at 8:06 AM, Damjan Krstajic <dkrstajic at hotmail.com> wrote:
> 
> I have kindly asked for help and I am sad to receive such a reply from some on the r-help list.
> 
> 

Well, you only said you were `struggling' to find a package.

Bert may well have done the Google search himself and found numerous resources on such models including links to R (as I did, see below).  If so, his response seems quite natural.

Perhaps, you need to say what is wrong with the hits you got and the packages that they describe to keep a potential response from running in the wrong direction. Perhaps, you have misunderstood the capabilities of a package or failed to grasp an inobvious way to use the package to reach your goal.  In any case, providing some background of why you think the obvious leads do not work in your case can be helpful.

Doing that search myself I see links to R packages, R functions, and to "The Gaussian Processes Web Site" which has a table of possibly relevant softwares.  It seems like there is a lot there to digest.

HTH,

Chuck

> I did google it prior to sending my request, and I could not find any R package which provides GP classification model which produces prediction intervals for each sample. I would be grateful if anybody could inform me about it. Thank you.
> 
> 
> ________________________________
> From: Bert Gunter <bgunter.4567 at gmail.com>
> Sent: 11 December 2017 15:50
> To: Damjan Krstajic
> Cc: r-help at r-project.org
> Subject: Re: [R] Gaussian Process Classification R packages
> 
> Google it!
> 
> "R Gaussian process model binary classification."
> 
> Cheers,
> Bert
> 


From iwritecode2 at gmail.com  Mon Dec 11 18:35:45 2017
From: iwritecode2 at gmail.com (Robert Wilkins)
Date: Mon, 11 Dec 2017 12:35:45 -0500
Subject: [R] Data cleaning & Data preparation, what do R users want?
In-Reply-To: <CAF1jk_=S4PatyvoR8tUys5d8gJxyEVnbu9WTZ17MQtS2_N_01g@mail.gmail.com>
References: <CAGW5CW9iGfDpLBu89Soe_81JUBZo86pfxAj+ynfPxdVA+8WBkQ@mail.gmail.com>
 <CA+8X3fXU300O-R2KTZw2GpoWWnWvMS4htqvdBySOudVrrTG6hg@mail.gmail.com>
 <CA+8X3fXHb_OcMYJd9B9=5TsZTpmMbBgwF85T-Z3uhyXL7VxJrw@mail.gmail.com>
 <CAF1jk_=S4PatyvoR8tUys5d8gJxyEVnbu9WTZ17MQtS2_N_01g@mail.gmail.com>
Message-ID: <CAGW5CW_PiXgfs5Tuhh_-BnPrrQ+RDmuVUx_qF_NMUD_8Tjfkvw@mail.gmail.com>

Dominik (and others)

If it is indeed still the biggest paint point, even in 2017, then maybe we
can do something about that, with more efforts at different user interface
design and try-outs with them on specialized datasets.
[ The fact that in some specialties, such as clinical trials, for example,
getting access to public domain datasets (and not having to use a tiny
"toy" dataset, which nobody will pay attention to, does make it harder].

It would help if academia (both comp-sci and statistics departments) would
support those who invest resources in drafting and test-driving new product
designs. If, in the year 2017, it is still a big pain point, doesn't that
make sense. More speculative work in statistical programming language
design has not been a priority in academia since before 1980.

On Thu, Nov 30, 2017 at 4:11 AM, Dominik Schneider <
dominik.schneider at colorado.edu> wrote:

> I would agree that getting data into R from various sources is the biggest
> pain point. Even if there is an api, the results are not always consistent
> and you have to do lots of dimension checking to get it right. Or there
> isn't an open api at all and you have to hack it by web scraping or
> otherwise- http://enpiar.com/2017/08/11/one-hour-package/
>
> On Thu, Nov 30, 2017 at 1:00 AM, Jim Lemon <drjimlemon at gmail.com> wrote:
>
>> Hi again,
>> Typo in the last email. Should read "about 40 standard deviations".
>>
>> Jim
>>
>> On Thu, Nov 30, 2017 at 10:54 AM, Jim Lemon <drjimlemon at gmail.com> wrote:
>> > Hi Robert,
>> > People want different levels of automation in the software they use.
>> > What concerns many of us is the desire for the function
>> > "figure-out-what-this-data-is-import-it-and-get-rid-of-bad-values".
>> > Such users typically want something that justifies its use by being
>> > written by someone who seems to know what they're doing and lots of
>> > other people use it. One advantage of many R functions is their
>> > modular construction. This encourages users to at least consider the
>> > steps that are taken rather than just accept what comes out of that
>> > long tube.
>> >
>> > Take the contentious problem of outlier identification. If I just let
>> > the black box peel off some values, I don't know what I have lost. On
>> > the other hand, if I import data and examine it with a summary
>> > function, I may find that one woman has a height of 5.2 meters. I can
>> > range check by looking up the Guinness Book of Records. It's an
>> > outlier. I can estimate the probability of such a height.  Hmm, about
>> > 4 standard deviations above the mean. It's an outlier. I can attempt a
>> > Sherlock Holmes. "Watson, I conclude that an imperial measure (5'2")
>> > has been recorded as a metric value". It's not an outlier.
>> >
>> > The more R gravitates toward "black box" functions, the more some
>> > users are encouraged to let them do the work.You pays your money and
>> > you takes your chances.
>> >
>> > Jim
>> >
>> >
>> > On Thu, Nov 30, 2017 at 3:37 AM, Robert Wilkins <iwritecode2 at gmail.com>
>> wrote:
>> >> R has a very wide audience, clinical research, astronomy, psychology,
>> and
>> >> so on and so on.
>> >> I would consider data analysis work to be three stages: data
>> preparation,
>> >> statistical analysis, and producing the report.
>> >> This regards the process of getting the data ready for analysis and
>> >> reporting, sometimes called "data cleaning" or "data munging" or "data
>> >> wrangling".
>> >>
>> >> So as regards tools for data preparation, speaking to the highly
>> diverse
>> >> audience mentioned, here is my question:
>> >>
>> >> What do you want?
>> >> Or are you already quite happy with the range of tools that is
>> currently
>> >> before you?
>> >>
>> >> [BTW,  I posed the same question last week to the r-devel list, and was
>> >> advised that r-help might be a more suitable audience by one of the
>> >> moderators.]
>> >>
>> >> Robert Wilkins
>> >>
>> >>         [[alternative HTML version deleted]]
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From dkrstajic at hotmail.com  Mon Dec 11 18:53:01 2017
From: dkrstajic at hotmail.com (Damjan Krstajic)
Date: Mon, 11 Dec 2017 17:53:01 +0000
Subject: [R] Gaussian Process Classification R packages
In-Reply-To: <16D9D99C-1D42-45B7-A9D3-33359021C4B3@ucsd.edu>
References: <VI1PR08MB105439B29680FF36A9DAB23BB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
 <CAGxFJbTNJ-q0f3fkKMONVHOVHsFBovzm8r5S5BwTrcRT3A8hyg@mail.gmail.com>
 <VI1PR08MB1054AF96FA95CCF8F83D502FB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>,
 <16D9D99C-1D42-45B7-A9D3-33359021C4B3@ucsd.edu>
Message-ID: <VI1PR08MB105471AB66A0433D4D98D2B5B4370@VI1PR08MB1054.eurprd08.prod.outlook.com>

Thank you Charles Berry for your kind reply. I don't see anything wrong with the word "struggling". I have spent several hours trying various R packages like kernlab and GPfit to use GP to create a binary classification model which produces a prediction interval for each sample. I have been struggling because with all of them you may create a GP classification model but it only produces a single prediction probability, and not a prediction interval of probabilities. Packages that I have tried may provide a prediction interval for regression but not for binary classification.


You mention "The Gaussian Processes Web Site", have you checked how many R packages are listed there?


I have been coding in R for more than a decade and contact r-help when I am struggling (I don't see anything wrong with this word) to find a solution in R. Replies like "Google it!" are below my level of my communication and understanding of others.


Best wishes

DK


________________________________
From: Berry, Charles <ccberry at ucsd.edu>
Sent: 11 December 2017 17:04
To: Damjan Krstajic
Cc: Bert Gunter; r-help at r-project.org
Subject: Re: Gaussian Process Classification R packages


> On Dec 11, 2017, at 8:06 AM, Damjan Krstajic <dkrstajic at hotmail.com> wrote:
>
> I have kindly asked for help and I am sad to receive such a reply from some on the r-help list.
>
>

Well, you only said you were `struggling' to find a package.

Bert may well have done the Google search himself and found numerous resources on such models including links to R (as I did, see below).  If so, his response seems quite natural.

Perhaps, you need to say what is wrong with the hits you got and the packages that they describe to keep a potential response from running in the wrong direction. Perhaps, you have misunderstood the capabilities of a package or failed to grasp an inobvious way to use the package to reach your goal.  In any case, providing some background of why you think the obvious leads do not work in your case can be helpful.

Doing that search myself I see links to R packages, R functions, and to "The Gaussian Processes Web Site" which has a table of possibly relevant softwares.  It seems like there is a lot there to digest.

HTH,

Chuck

> I did google it prior to sending my request, and I could not find any R package which provides GP classification model which produces prediction intervals for each sample. I would be grateful if anybody could inform me about it. Thank you.
>
>
> ________________________________
> From: Bert Gunter <bgunter.4567 at gmail.com>
> Sent: 11 December 2017 15:50
> To: Damjan Krstajic
> Cc: r-help at r-project.org
> Subject: Re: [R] Gaussian Process Classification R packages
>
> Google it!
>
> "R Gaussian process model binary classification."
>
> Cheers,
> Bert
>



	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Mon Dec 11 19:15:48 2017
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 12 Dec 2017 07:15:48 +1300
Subject: [R] OT -- isotonic regression subject to bound constraints.
Message-ID: <d94aab6d-387e-e84d-c29c-0ac3f271277a@auckland.ac.nz>


Well, I could argue that it's not *completely* OT since my question is 
motivated by an enquiry that I received in respect of a CRAN package 
"Iso" that I wrote and maintain.

The question is this:  Given observations y_1, ..., y_n, what is the 
solution to the problem:

   minimise \sum_{i=1}^n (y_i - y_i^*)^2

with respect to y_1^*, ..., y_n^* subject to the "isotonic" constraint
y_1^* <= y_2^* <= ... <= y_n^* and the *additional8 bound constraint
a <= y_1^* and y_n^* <= b, where a and b are given constants?

I have googled around a bit (unsuccessfully) and have asked this 
question on crossvalidated a couple of days ago, with no response whatever.

So I thought that I might try the super-knowledgeable R community, in 
the hope that someone out there might be able to tell me something useful.

Note that the question can be expressed as finding the projection of the 
point (y_1, ..., y_n) onto the intersection of the isotonic cone and
the hypercube [a,b]^n.

At first I thought that protecting onto the isotonic cone and then 
projection that result onto the hypercube might work, but I am now 
pretty sure that is hopelessly naive.

Any hints?  Ta.

cheers,

Rolf Turner


-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r.turner at auckland.ac.nz  Mon Dec 11 19:23:15 2017
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 12 Dec 2017 07:23:15 +1300
Subject: [R] OT -- isotonic regression subject to bound constraints.
In-Reply-To: <d94aab6d-387e-e84d-c29c-0ac3f271277a@auckland.ac.nz>
References: <d94aab6d-387e-e84d-c29c-0ac3f271277a@auckland.ac.nz>
Message-ID: <c2431e81-4ea0-92ca-103c-20226393e6ba@auckland.ac.nz>

On 12/12/17 07:15, Rolf Turner wrote:
> 
> Well, I could argue that it's not *completely* OT since my question is 
> motivated by an enquiry that I received in respect of a CRAN package 
> "Iso" that I wrote and maintain.
> 
> The question is this:? Given observations y_1, ..., y_n, what is the 
> solution to the problem:
> 
>  ? minimise \sum_{i=1}^n (y_i - y_i^*)^2
> 
> with respect to y_1^*, ..., y_n^* subject to the "isotonic" constraint
> y_1^* <= y_2^* <= ... <= y_n^* and the *additional8 bound constraint
> a <= y_1^* and y_n^* <= b, where a and b are given constants?

<SNIP>

Scrub that question!  *Just* after I sent it (wouldn't you know!) I got 
an email from my original enquirer telling me that he'd found the 
solution in the package OrdMonReg on CRAN.

Sorry for the noise.

cheers,

Rolf Turner


-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From jdnewmil at dcn.davis.ca.us  Tue Dec 12 00:48:35 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 11 Dec 2017 15:48:35 -0800
Subject: [R] Gaussian Process Classification R packages
In-Reply-To: <VI1PR08MB105471AB66A0433D4D98D2B5B4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
References: <VI1PR08MB105439B29680FF36A9DAB23BB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
 <CAGxFJbTNJ-q0f3fkKMONVHOVHsFBovzm8r5S5BwTrcRT3A8hyg@mail.gmail.com>
 <VI1PR08MB1054AF96FA95CCF8F83D502FB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>,
 <16D9D99C-1D42-45B7-A9D3-33359021C4B3@ucsd.edu>
 <VI1PR08MB105471AB66A0433D4D98D2B5B4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
Message-ID: <7D0FD9B8-B729-4835-A191-19BE599875F4@dcn.davis.ca.us>

While a plea about struggling may seem appropriate to you, it is just as content-free as a reply telling you to use Google... and like it or not, that tit-for-tat arises due to frustration with lack of specificity as detailed by Charles. That is, if you are constructive about documenting your issue with a reproducible example and mentioning what you have tried and how it failed, you won't prompt such frustrated/unhelpful responses in the future.

Did you find [1] or [2]?

[1] https://stats.stackexchange.com/questions/177677/gaussian-process-prediction-interval

[2] https://stats.stackexchange.com/questions/9131/obtaining-a-formula-for-prediction-limits-in-a-linear-model/9144#9144
-- 
Sent from my phone. Please excuse my brevity.

On December 11, 2017 9:53:01 AM PST, Damjan Krstajic <dkrstajic at hotmail.com> wrote:
>Thank you Charles Berry for your kind reply. I don't see anything wrong
>with the word "struggling". I have spent several hours trying various R
>packages like kernlab and GPfit to use GP to create a binary
>classification model which produces a prediction interval for each
>sample. I have been struggling because with all of them you may create
>a GP classification model but it only produces a single prediction
>probability, and not a prediction interval of probabilities. Packages
>that I have tried may provide a prediction interval for regression but
>not for binary classification.
>
>
>You mention "The Gaussian Processes Web Site", have you checked how
>many R packages are listed there?
>
>
>I have been coding in R for more than a decade and contact r-help when
>I am struggling (I don't see anything wrong with this word) to find a
>solution in R. Replies like "Google it!" are below my level of my
>communication and understanding of others.
>
>
>Best wishes
>
>DK
>
>
>________________________________
>From: Berry, Charles <ccberry at ucsd.edu>
>Sent: 11 December 2017 17:04
>To: Damjan Krstajic
>Cc: Bert Gunter; r-help at r-project.org
>Subject: Re: Gaussian Process Classification R packages
>
>
>> On Dec 11, 2017, at 8:06 AM, Damjan Krstajic <dkrstajic at hotmail.com>
>wrote:
>>
>> I have kindly asked for help and I am sad to receive such a reply
>from some on the r-help list.
>>
>>
>
>Well, you only said you were `struggling' to find a package.
>
>Bert may well have done the Google search himself and found numerous
>resources on such models including links to R (as I did, see below). 
>If so, his response seems quite natural.
>
>Perhaps, you need to say what is wrong with the hits you got and the
>packages that they describe to keep a potential response from running
>in the wrong direction. Perhaps, you have misunderstood the
>capabilities of a package or failed to grasp an inobvious way to use
>the package to reach your goal.  In any case, providing some background
>of why you think the obvious leads do not work in your case can be
>helpful.
>
>Doing that search myself I see links to R packages, R functions, and to
>"The Gaussian Processes Web Site" which has a table of possibly
>relevant softwares.  It seems like there is a lot there to digest.
>
>HTH,
>
>Chuck
>
>> I did google it prior to sending my request, and I could not find any
>R package which provides GP classification model which produces
>prediction intervals for each sample. I would be grateful if anybody
>could inform me about it. Thank you.
>>
>>
>> ________________________________
>> From: Bert Gunter <bgunter.4567 at gmail.com>
>> Sent: 11 December 2017 15:50
>> To: Damjan Krstajic
>> Cc: r-help at r-project.org
>> Subject: Re: [R] Gaussian Process Classification R packages
>>
>> Google it!
>>
>> "R Gaussian process model binary classification."
>>
>> Cheers,
>> Bert
>>
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Tue Dec 12 01:01:45 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 11 Dec 2017 16:01:45 -0800
Subject: [R] Gaussian Process Classification R packages
In-Reply-To: <7D0FD9B8-B729-4835-A191-19BE599875F4@dcn.davis.ca.us>
References: <VI1PR08MB105439B29680FF36A9DAB23BB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
 <CAGxFJbTNJ-q0f3fkKMONVHOVHsFBovzm8r5S5BwTrcRT3A8hyg@mail.gmail.com>
 <VI1PR08MB1054AF96FA95CCF8F83D502FB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
 <16D9D99C-1D42-45B7-A9D3-33359021C4B3@ucsd.edu>
 <VI1PR08MB105471AB66A0433D4D98D2B5B4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
 <7D0FD9B8-B729-4835-A191-19BE599875F4@dcn.davis.ca.us>
Message-ID: <CAGxFJbT+-gE8dJi6xXUbSNrgUSLUq_9PF45Y0d=zgSmT9boYbg@mail.gmail.com>

For the record:

I **was** trying to be helpful. I simply didn't know whether "I struggled"
meant that the OP had done a web search; as Chuck mentioned, when I did
one, I found what looked like possibly helpful hits. The OP's hostile
response frankly surprised me, but I see no reason to respond in kind.

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Mon, Dec 11, 2017 at 3:48 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> While a plea about struggling may seem appropriate to you, it is just as
> content-free as a reply telling you to use Google... and like it or not,
> that tit-for-tat arises due to frustration with lack of specificity as
> detailed by Charles. That is, if you are constructive about documenting
> your issue with a reproducible example and mentioning what you have tried
> and how it failed, you won't prompt such frustrated/unhelpful responses in
> the future.
>
> Did you find [1] or [2]?
>
> [1] https://stats.stackexchange.com/questions/177677/gaussian-
> process-prediction-interval
>
> [2] https://stats.stackexchange.com/questions/9131/obtaining-
> a-formula-for-prediction-limits-in-a-linear-model/9144#9144
> --
> Sent from my phone. Please excuse my brevity.
>
> On December 11, 2017 9:53:01 AM PST, Damjan Krstajic <
> dkrstajic at hotmail.com> wrote:
> >Thank you Charles Berry for your kind reply. I don't see anything wrong
> >with the word "struggling". I have spent several hours trying various R
> >packages like kernlab and GPfit to use GP to create a binary
> >classification model which produces a prediction interval for each
> >sample. I have been struggling because with all of them you may create
> >a GP classification model but it only produces a single prediction
> >probability, and not a prediction interval of probabilities. Packages
> >that I have tried may provide a prediction interval for regression but
> >not for binary classification.
> >
> >
> >You mention "The Gaussian Processes Web Site", have you checked how
> >many R packages are listed there?
> >
> >
> >I have been coding in R for more than a decade and contact r-help when
> >I am struggling (I don't see anything wrong with this word) to find a
> >solution in R. Replies like "Google it!" are below my level of my
> >communication and understanding of others.
> >
> >
> >Best wishes
> >
> >DK
> >
> >
> >________________________________
> >From: Berry, Charles <ccberry at ucsd.edu>
> >Sent: 11 December 2017 17:04
> >To: Damjan Krstajic
> >Cc: Bert Gunter; r-help at r-project.org
> >Subject: Re: Gaussian Process Classification R packages
> >
> >
> >> On Dec 11, 2017, at 8:06 AM, Damjan Krstajic <dkrstajic at hotmail.com>
> >wrote:
> >>
> >> I have kindly asked for help and I am sad to receive such a reply
> >from some on the r-help list.
> >>
> >>
> >
> >Well, you only said you were `struggling' to find a package.
> >
> >Bert may well have done the Google search himself and found numerous
> >resources on such models including links to R (as I did, see below).
> >If so, his response seems quite natural.
> >
> >Perhaps, you need to say what is wrong with the hits you got and the
> >packages that they describe to keep a potential response from running
> >in the wrong direction. Perhaps, you have misunderstood the
> >capabilities of a package or failed to grasp an inobvious way to use
> >the package to reach your goal.  In any case, providing some background
> >of why you think the obvious leads do not work in your case can be
> >helpful.
> >
> >Doing that search myself I see links to R packages, R functions, and to
> >"The Gaussian Processes Web Site" which has a table of possibly
> >relevant softwares.  It seems like there is a lot there to digest.
> >
> >HTH,
> >
> >Chuck
> >
> >> I did google it prior to sending my request, and I could not find any
> >R package which provides GP classification model which produces
> >prediction intervals for each sample. I would be grateful if anybody
> >could inform me about it. Thank you.
> >>
> >>
> >> ________________________________
> >> From: Bert Gunter <bgunter.4567 at gmail.com>
> >> Sent: 11 December 2017 15:50
> >> To: Damjan Krstajic
> >> Cc: r-help at r-project.org
> >> Subject: Re: [R] Gaussian Process Classification R packages
> >>
> >> Google it!
> >>
> >> "R Gaussian process model binary classification."
> >>
> >> Cheers,
> >> Bert
> >>
> >
> >
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dkrstajic at hotmail.com  Tue Dec 12 01:36:43 2017
From: dkrstajic at hotmail.com (Damjan Krstajic)
Date: Tue, 12 Dec 2017 00:36:43 +0000
Subject: [R] Gaussian Process Classification R packages
In-Reply-To: <CAGxFJbT+-gE8dJi6xXUbSNrgUSLUq_9PF45Y0d=zgSmT9boYbg@mail.gmail.com>
References: <VI1PR08MB105439B29680FF36A9DAB23BB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
 <CAGxFJbTNJ-q0f3fkKMONVHOVHsFBovzm8r5S5BwTrcRT3A8hyg@mail.gmail.com>
 <VI1PR08MB1054AF96FA95CCF8F83D502FB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
 <16D9D99C-1D42-45B7-A9D3-33359021C4B3@ucsd.edu>
 <VI1PR08MB105471AB66A0433D4D98D2B5B4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
 <7D0FD9B8-B729-4835-A191-19BE599875F4@dcn.davis.ca.us>,
 <CAGxFJbT+-gE8dJi6xXUbSNrgUSLUq_9PF45Y0d=zgSmT9boYbg@mail.gmail.com>
Message-ID: <DB5PR08MB1047E3385C4B080ABF4A6AECB4340@DB5PR08MB1047.eurprd08.prod.outlook.com>

For the record please re-read my original message. It is clear, concise, polite and thankful for future help. I received a reply "Google it!". Thank you!


Thank you Jeff for your links. I am aware of them. However, they do not point to an R package for  GP for binary classification which produces prediction intervals.


It seems that r-help is not as it was before. Wish you all the best. Roger and out.

________________________________
From: Bert Gunter <bgunter.4567 at gmail.com>
Sent: 12 December 2017 00:01
To: Jeff Newmiller
Cc: R-help; Damjan Krstajic; Berry, Charles
Subject: Re: [R] Gaussian Process Classification R packages

For the record:

I **was** trying to be helpful. I simply didn't know whether "I struggled" meant that the OP had done a web search; as Chuck mentioned, when I did one, I found what looked like possibly helpful hits. The OP's hostile response frankly surprised me, but I see no reason to respond in kind.

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Mon, Dec 11, 2017 at 3:48 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>> wrote:
While a plea about struggling may seem appropriate to you, it is just as content-free as a reply telling you to use Google... and like it or not, that tit-for-tat arises due to frustration with lack of specificity as detailed by Charles. That is, if you are constructive about documenting your issue with a reproducible example and mentioning what you have tried and how it failed, you won't prompt such frustrated/unhelpful responses in the future.

Did you find [1] or [2]?

[1] https://stats.stackexchange.com/questions/177677/gaussian-process-prediction-interval

[2] https://stats.stackexchange.com/questions/9131/obtaining-a-formula-for-prediction-limits-in-a-linear-model/9144#9144
--
Sent from my phone. Please excuse my brevity.

On December 11, 2017 9:53:01 AM PST, Damjan Krstajic <dkrstajic at hotmail.com<mailto:dkrstajic at hotmail.com>> wrote:
>Thank you Charles Berry for your kind reply. I don't see anything wrong
>with the word "struggling". I have spent several hours trying various R
>packages like kernlab and GPfit to use GP to create a binary
>classification model which produces a prediction interval for each
>sample. I have been struggling because with all of them you may create
>a GP classification model but it only produces a single prediction
>probability, and not a prediction interval of probabilities. Packages
>that I have tried may provide a prediction interval for regression but
>not for binary classification.
>
>
>You mention "The Gaussian Processes Web Site", have you checked how
>many R packages are listed there?
>
>
>I have been coding in R for more than a decade and contact r-help when
>I am struggling (I don't see anything wrong with this word) to find a
>solution in R. Replies like "Google it!" are below my level of my
>communication and understanding of others.
>
>
>Best wishes
>
>DK
>
>
>________________________________
>From: Berry, Charles <ccberry at ucsd.edu<mailto:ccberry at ucsd.edu>>
>Sent: 11 December 2017 17:04
>To: Damjan Krstajic
>Cc: Bert Gunter; r-help at r-project.org<mailto:r-help at r-project.org>
>Subject: Re: Gaussian Process Classification R packages
>
>
>> On Dec 11, 2017, at 8:06 AM, Damjan Krstajic <dkrstajic at hotmail.com<mailto:dkrstajic at hotmail.com>>
>wrote:
>>
>> I have kindly asked for help and I am sad to receive such a reply
>from some on the r-help list.
>>
>>
>
>Well, you only said you were `struggling' to find a package.
>
>Bert may well have done the Google search himself and found numerous
>resources on such models including links to R (as I did, see below).
>If so, his response seems quite natural.
>
>Perhaps, you need to say what is wrong with the hits you got and the
>packages that they describe to keep a potential response from running
>in the wrong direction. Perhaps, you have misunderstood the
>capabilities of a package or failed to grasp an inobvious way to use
>the package to reach your goal.  In any case, providing some background
>of why you think the obvious leads do not work in your case can be
>helpful.
>
>Doing that search myself I see links to R packages, R functions, and to
>"The Gaussian Processes Web Site" which has a table of possibly
>relevant softwares.  It seems like there is a lot there to digest.
>
>HTH,
>
>Chuck
>
>> I did google it prior to sending my request, and I could not find any
>R package which provides GP classification model which produces
>prediction intervals for each sample. I would be grateful if anybody
>could inform me about it. Thank you.
>>
>>
>> ________________________________
>> From: Bert Gunter <bgunter.4567 at gmail.com<mailto:bgunter.4567 at gmail.com>>
>> Sent: 11 December 2017 15:50
>> To: Damjan Krstajic
>> Cc: r-help at r-project.org<mailto:r-help at r-project.org>
>> Subject: Re: [R] Gaussian Process Classification R packages
>>
>> Google it!
>>
>> "R Gaussian process model binary classification."
>>
>> Cheers,
>> Bert
>>
>
>
>
>       [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From gwblack001 at sbcglobal.net  Tue Dec 12 02:54:51 2017
From: gwblack001 at sbcglobal.net (Gary Black)
Date: Mon, 11 Dec 2017 19:54:51 -0600
Subject: [R] Gaussian Process Classification R packages
In-Reply-To: <DB5PR08MB1047E3385C4B080ABF4A6AECB4340@DB5PR08MB1047.eurprd08.prod.outlook.com>
References: <VI1PR08MB105439B29680FF36A9DAB23BB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
 <CAGxFJbTNJ-q0f3fkKMONVHOVHsFBovzm8r5S5BwTrcRT3A8hyg@mail.gmail.com>
 <VI1PR08MB1054AF96FA95CCF8F83D502FB4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
 <16D9D99C-1D42-45B7-A9D3-33359021C4B3@ucsd.edu>
 <VI1PR08MB105471AB66A0433D4D98D2B5B4370@VI1PR08MB1054.eurprd08.prod.outlook.com>
 <7D0FD9B8-B729-4835-A191-19BE599875F4@dcn.davis.ca.us>,
 <CAGxFJbT+-gE8dJi6xXUbSNrgUSLUq_9PF45Y0d=zgSmT9boYbg@mail.gmail.com>
 <DB5PR08MB1047E3385C4B080ABF4A6AECB4340@DB5PR08MB1047.eurprd08.prod.outlook.com>
Message-ID: <000001d372ec$33990130$9acb0390$@sbcglobal.net>

As a total novice and somebody lurking in the background who doesn't have a
need to use R regularly except for a one-time project, I also am surprised
by (really mainly one or two) people who answer most posts by telling people
to do their homework, spend more time studying R, etc.  Even novices know
these things, and so those replies are totally unhelpful.

For novices who have a "one time" need to use R for a project, we cannot
spend months studying as if we were going to use it every day in our jobs.
We simply read some tutorials, some books, etc. and when we run into an
issue that we cannot resolve, we ask for help.  I thought that was what
forums such as this were about - people who voluntarily answer questions for
free because they are interested in the subject.

That said, I agree totally about the need to be clear and concise when
posting questions.  But as a novice, we sometimes are not sure exactly what
the relevant information is that is needed to answer our question.

In summary, how about cutting novices some slack?  If you don't want to
answer a post in a helpful way, then just ignore it.  You don't need to
interject yourself into every post.  


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Damjan
Krstajic
Sent: Monday, December 11, 2017 6:37 PM
To: Bert Gunter <bgunter.4567 at gmail.com>; Jeff Newmiller
<jdnewmil at dcn.davis.ca.us>
Cc: R-help <r-help at r-project.org>; Berry, Charles <ccberry at ucsd.edu>
Subject: Re: [R] Gaussian Process Classification R packages

For the record please re-read my original message. It is clear, concise,
polite and thankful for future help. I received a reply "Google it!". Thank
you!


Thank you Jeff for your links. I am aware of them. However, they do not
point to an R package for  GP for binary classification which produces
prediction intervals.


It seems that r-help is not as it was before. Wish you all the best. Roger
and out.

________________________________
From: Bert Gunter <bgunter.4567 at gmail.com>
Sent: 12 December 2017 00:01
To: Jeff Newmiller
Cc: R-help; Damjan Krstajic; Berry, Charles
Subject: Re: [R] Gaussian Process Classification R packages

For the record:

I **was** trying to be helpful. I simply didn't know whether "I struggled"
meant that the OP had done a web search; as Chuck mentioned, when I did one,
I found what looked like possibly helpful hits. The OP's hostile response
frankly surprised me, but I see no reason to respond in kind.

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Mon, Dec 11, 2017 at 3:48 PM, Jeff Newmiller
<jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>> wrote:
While a plea about struggling may seem appropriate to you, it is just as
content-free as a reply telling you to use Google... and like it or not,
that tit-for-tat arises due to frustration with lack of specificity as
detailed by Charles. That is, if you are constructive about documenting your
issue with a reproducible example and mentioning what you have tried and how
it failed, you won't prompt such frustrated/unhelpful responses in the
future.

Did you find [1] or [2]?

[1]
https://stats.stackexchange.com/questions/177677/gaussian-process-prediction
-interval

[2]
https://stats.stackexchange.com/questions/9131/obtaining-a-formula-for-predi
ction-limits-in-a-linear-model/9144#9144
--
Sent from my phone. Please excuse my brevity.

On December 11, 2017 9:53:01 AM PST, Damjan Krstajic
<dkrstajic at hotmail.com<mailto:dkrstajic at hotmail.com>> wrote:
>Thank you Charles Berry for your kind reply. I don't see anything wrong 
>with the word "struggling". I have spent several hours trying various R 
>packages like kernlab and GPfit to use GP to create a binary 
>classification model which produces a prediction interval for each 
>sample. I have been struggling because with all of them you may create 
>a GP classification model but it only produces a single prediction 
>probability, and not a prediction interval of probabilities. Packages 
>that I have tried may provide a prediction interval for regression but 
>not for binary classification.
>
>
>You mention "The Gaussian Processes Web Site", have you checked how 
>many R packages are listed there?
>
>
>I have been coding in R for more than a decade and contact r-help when 
>I am struggling (I don't see anything wrong with this word) to find a 
>solution in R. Replies like "Google it!" are below my level of my 
>communication and understanding of others.
>
>
>Best wishes
>
>DK
>
>
>________________________________
>From: Berry, Charles <ccberry at ucsd.edu<mailto:ccberry at ucsd.edu>>
>Sent: 11 December 2017 17:04
>To: Damjan Krstajic
>Cc: Bert Gunter; r-help at r-project.org<mailto:r-help at r-project.org>
>Subject: Re: Gaussian Process Classification R packages
>
>
>> On Dec 11, 2017, at 8:06 AM, Damjan Krstajic 
>> <dkrstajic at hotmail.com<mailto:dkrstajic at hotmail.com>>
>wrote:
>>
>> I have kindly asked for help and I am sad to receive such a reply
>from some on the r-help list.
>>
>>
>
>Well, you only said you were `struggling' to find a package.
>
>Bert may well have done the Google search himself and found numerous 
>resources on such models including links to R (as I did, see below).
>If so, his response seems quite natural.
>
>Perhaps, you need to say what is wrong with the hits you got and the 
>packages that they describe to keep a potential response from running 
>in the wrong direction. Perhaps, you have misunderstood the 
>capabilities of a package or failed to grasp an inobvious way to use 
>the package to reach your goal.  In any case, providing some background 
>of why you think the obvious leads do not work in your case can be 
>helpful.
>
>Doing that search myself I see links to R packages, R functions, and to 
>"The Gaussian Processes Web Site" which has a table of possibly 
>relevant softwares.  It seems like there is a lot there to digest.
>
>HTH,
>
>Chuck
>
>> I did google it prior to sending my request, and I could not find any
>R package which provides GP classification model which produces 
>prediction intervals for each sample. I would be grateful if anybody 
>could inform me about it. Thank you.
>>
>>
>> ________________________________
>> From: Bert Gunter 
>> <bgunter.4567 at gmail.com<mailto:bgunter.4567 at gmail.com>>
>> Sent: 11 December 2017 15:50
>> To: Damjan Krstajic
>> Cc: r-help at r-project.org<mailto:r-help at r-project.org>
>> Subject: Re: [R] Gaussian Process Classification R packages
>>
>> Google it!
>>
>> "R Gaussian process model binary classification."
>>
>> Cheers,
>> Bert
>>
>
>
>
>       [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To 
>UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


---
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus


From wolfgang.viechtbauer at maastrichtuniversity.nl  Tue Dec 12 23:24:41 2017
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (SP))
Date: Tue, 12 Dec 2017 22:24:41 +0000
Subject: [R] xyTable(x,y) versus table(x,y) with NAs
Message-ID: <c9e0e831545344bcae2c4e735ae91281@UM-MAIL3216.unimaas.nl>

Hi All,

It seems to me that xyTable() gets thrown off by NAs:

x <- c(1, 1, 2, 2,  2, 3)
y <- c(1, 2, 1, 3, NA, 3)
table(x, y, useNA="always")
xyTable(x, y)

Is this intended behavior?

Best,
Wolfgang


From emorway at usgs.gov  Wed Dec 13 02:36:02 2017
From: emorway at usgs.gov (Morway, Eric)
Date: Tue, 12 Dec 2017 17:36:02 -0800
Subject: [R] inefficient for loop, is there a better way?
Message-ID: <CAPoqHzqGvbkO-kBRxJPpwQ7bk1ukU51jROYdKsgFgbPX7=vKkg@mail.gmail.com>

The code below is a small reproducible example of a much larger problem.
While the script below works, it is really slow on the true dataset with
many more rows and columns.  I'm hoping to get the same result to examp,
but with significant time savings.

The example below is setting up a data.frame for an ensuing regression
analysis.  The purpose of the script below is to appends columns to 'examp'
that contain values corresponding to the total number of days in the
previous 7 ('per') above some stage ('elev1' or 'elev2').  Is there a
faster method that leverages existing R functionality?  I feel like the
hack below is pretty clunky and can be sped up on the true dataset.  I
would like to run a more efficient script many times adjusting the value of
'per'.

ts <- 1:1000
examp <- data.frame(ts=ts, stage=sin(ts))

hi1 <- list()
hi2 <- list()
per <- 7
elev1 <- 0.6
elev2 <- 0.85
for(i in per:nrow(examp)){
    examp_per <- examp[seq(i - (per - 1), i, by=1),]
    stg_hi_cond1 <- subset(examp_per, examp_per$stage > elev1)
    stg_hi_cond2 <- subset(examp_per, examp_per$stage > elev2)

    hi1 <- c(hi1, nrow(stg_hi_cond1))
    hi2 <- c(hi2, nrow(stg_hi_cond2))
}
examp$days_abv_0.6_in_last_7   <- c(rep(NA, times=per-1), unlist(hi1))
examp$days_abv_0.85_in_last_7  <- c(rep(NA, times=per-1), unlist(hi2))

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Wed Dec 13 03:03:02 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 12 Dec 2017 18:03:02 -0800
Subject: [R] inefficient for loop, is there a better way?
In-Reply-To: <CAPoqHzqGvbkO-kBRxJPpwQ7bk1ukU51jROYdKsgFgbPX7=vKkg@mail.gmail.com>
References: <CAPoqHzqGvbkO-kBRxJPpwQ7bk1ukU51jROYdKsgFgbPX7=vKkg@mail.gmail.com>
Message-ID: <CAF8bMcYkH6a8+naAjo_rLkQXz9LDTjWPNbiaon19ma2H0Qb9cQ@mail.gmail.com>

Try using stats::filter (not the unfortunately named dplyr::filter, which
is entirely different).
state>elev is a logical vector, but filter(), like most numerical
functions, treats TRUEs as 1s
and FALSEs as 0s.

E.g.,

> str( stats::filter( x=examp$stage>elev1, filter=rep(1,7),
method="convolution", sides=1) )
 Time-Series [1:1000] from 1 to 1000: NA NA NA NA NA NA 3 3 2 2 ...
> str( stats::filter( x=examp$stage>elev2, filter=rep(1,7),
method="convolution", sides=1) )
 Time-Series [1:1000] from 1 to 1000: NA NA NA NA NA NA 1 2 1 1 ...


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Dec 12, 2017 at 5:36 PM, Morway, Eric <emorway at usgs.gov> wrote:

> The code below is a small reproducible example of a much larger problem.
> While the script below works, it is really slow on the true dataset with
> many more rows and columns.  I'm hoping to get the same result to examp,
> but with significant time savings.
>
> The example below is setting up a data.frame for an ensuing regression
> analysis.  The purpose of the script below is to appends columns to 'examp'
> that contain values corresponding to the total number of days in the
> previous 7 ('per') above some stage ('elev1' or 'elev2').  Is there a
> faster method that leverages existing R functionality?  I feel like the
> hack below is pretty clunky and can be sped up on the true dataset.  I
> would like to run a more efficient script many times adjusting the value of
> 'per'.
>
> ts <- 1:1000
> examp <- data.frame(ts=ts, stage=sin(ts))
>
> hi1 <- list()
> hi2 <- list()
> per <- 7
> elev1 <- 0.6
> elev2 <- 0.85
> for(i in per:nrow(examp)){
>     examp_per <- examp[seq(i - (per - 1), i, by=1),]
>     stg_hi_cond1 <- subset(examp_per, examp_per$stage > elev1)
>     stg_hi_cond2 <- subset(examp_per, examp_per$stage > elev2)
>
>     hi1 <- c(hi1, nrow(stg_hi_cond1))
>     hi2 <- c(hi2, nrow(stg_hi_cond2))
> }
> examp$days_abv_0.6_in_last_7   <- c(rep(NA, times=per-1), unlist(hi1))
> examp$days_abv_0.85_in_last_7  <- c(rep(NA, times=per-1), unlist(hi2))
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From yvan at dragonfly.co.nz  Wed Dec 13 03:26:23 2017
From: yvan at dragonfly.co.nz (Yvan Richard)
Date: Wed, 13 Dec 2017 15:26:23 +1300
Subject: [R] inefficient for loop, is there a better way?
In-Reply-To: <CAPoqHzqGvbkO-kBRxJPpwQ7bk1ukU51jROYdKsgFgbPX7=vKkg@mail.gmail.com>
References: <CAPoqHzqGvbkO-kBRxJPpwQ7bk1ukU51jROYdKsgFgbPX7=vKkg@mail.gmail.com>
Message-ID: <CAMrYPbFdSbXG2P2WQRbFmTE0+SOsKe818hA8znFbHbNvY_NeoQ@mail.gmail.com>

One way of doing it with data.table. It seems to scale up pretty well.
It takes 4 seconds on my computer with ts <- 1:1e6.

library(data.table)
per <- 7
elev1 <- 0.6
elev2 <- 0.85

ts <- 1:1000

examp <- data.table(ts=ts, stage=sin(ts))
examp[, `:=`(days_abv_0.6_in_last_7  = apply(do.call('cbind',
shift(stage, 1:per)), 1, function(x) sum(x > elev1)),
                   days_abv_0.85_in_last_7 = apply(do.call('cbind',
shift(stage, 1:per)), 1, function(x) sum(x > elev2)))]



On 13 December 2017 at 14:36, Morway, Eric <emorway at usgs.gov> wrote:
> The code below is a small reproducible example of a much larger problem.
> While the script below works, it is really slow on the true dataset with
> many more rows and columns.  I'm hoping to get the same result to examp,
> but with significant time savings.
>
> The example below is setting up a data.frame for an ensuing regression
> analysis.  The purpose of the script below is to appends columns to 'examp'
> that contain values corresponding to the total number of days in the
> previous 7 ('per') above some stage ('elev1' or 'elev2').  Is there a
> faster method that leverages existing R functionality?  I feel like the
> hack below is pretty clunky and can be sped up on the true dataset.  I
> would like to run a more efficient script many times adjusting the value of
> 'per'.
>
> ts <- 1:1000
> examp <- data.frame(ts=ts, stage=sin(ts))
>
> hi1 <- list()
> hi2 <- list()
> per <- 7
> elev1 <- 0.6
> elev2 <- 0.85
> for(i in per:nrow(examp)){
>     examp_per <- examp[seq(i - (per - 1), i, by=1),]
>     stg_hi_cond1 <- subset(examp_per, examp_per$stage > elev1)
>     stg_hi_cond2 <- subset(examp_per, examp_per$stage > elev2)
>
>     hi1 <- c(hi1, nrow(stg_hi_cond1))
>     hi2 <- c(hi2, nrow(stg_hi_cond2))
> }
> examp$days_abv_0.6_in_last_7   <- c(rep(NA, times=per-1), unlist(hi1))
> examp$days_abv_0.85_in_last_7  <- c(rep(NA, times=per-1), unlist(hi2))
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Yvan Richard, PhD
Environmental data scientist



Physical address: Level 4, 158 Victoria St, Te Aro, Wellington, New Zealand
Postal address: PO Box 27535, Wellington 6141, New Zealand
Phone: 022 643 7881


From bgunter.4567 at gmail.com  Wed Dec 13 03:47:00 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 12 Dec 2017 18:47:00 -0800
Subject: [R] inefficient for loop, is there a better way?
In-Reply-To: <CAPoqHzqGvbkO-kBRxJPpwQ7bk1ukU51jROYdKsgFgbPX7=vKkg@mail.gmail.com>
References: <CAPoqHzqGvbkO-kBRxJPpwQ7bk1ukU51jROYdKsgFgbPX7=vKkg@mail.gmail.com>
Message-ID: <CAGxFJbS_xkgzQV-iZWgR13dqcbTSSo_fUjU1GEe2S1ZjFG-7Aw@mail.gmail.com>

I believe ?filter will do what you want.

I used  n = 100 instead of 1000:

ts <- 1:100
examp <- data.frame(ts=ts, stage=sin(ts))
examp <- within(examp, {
  abv_1 <- filter(stage > 0.6, rep(1,7),sides =1)
  abv_2 <- filter(stage > .85, rep(1,7), sides =1)
   })
examp

I think this should be fairly fast, but let us know if not. There may be
other alternatives that might be faster.
Assuming it does what you wanted, of course.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Tue, Dec 12, 2017 at 5:36 PM, Morway, Eric <emorway at usgs.gov> wrote:

> The code below is a small reproducible example of a much larger problem.
> While the script below works, it is really slow on the true dataset with
> many more rows and columns.  I'm hoping to get the same result to examp,
> but with significant time savings.
>
> The example below is setting up a data.frame for an ensuing regression
> analysis.  The purpose of the script below is to appends columns to 'examp'
> that contain values corresponding to the total number of days in the
> previous 7 ('per') above some stage ('elev1' or 'elev2').  Is there a
> faster method that leverages existing R functionality?  I feel like the
> hack below is pretty clunky and can be sped up on the true dataset.  I
> would like to run a more efficient script many times adjusting the value of
> 'per'.
>
> ts <- 1:1000
> examp <- data.frame(ts=ts, stage=sin(ts))
>
> hi1 <- list()
> hi2 <- list()
> per <- 7
> elev1 <- 0.6
> elev2 <- 0.85
> for(i in per:nrow(examp)){
>     examp_per <- examp[seq(i - (per - 1), i, by=1),]
>     stg_hi_cond1 <- subset(examp_per, examp_per$stage > elev1)
>     stg_hi_cond2 <- subset(examp_per, examp_per$stage > elev2)
>
>     hi1 <- c(hi1, nrow(stg_hi_cond1))
>     hi2 <- c(hi2, nrow(stg_hi_cond2))
> }
> examp$days_abv_0.6_in_last_7   <- c(rep(NA, times=per-1), unlist(hi1))
> examp$days_abv_0.85_in_last_7  <- c(rep(NA, times=per-1), unlist(hi2))
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From maingo at protonmail.com  Wed Dec 13 06:41:06 2017
From: maingo at protonmail.com (Maingo)
Date: Wed, 13 Dec 2017 00:41:06 -0500
Subject: [R] Add vectors of unequal length without recycling?
Message-ID: <jkOWiOMgaIdgmQJBkGQi98m9V0ZDOKov3ioB_7ZV4wVkVDKlCuksj5dqw_kgdmvFcYubWyG3DBNShT2og494s0U4D5aFJRHrO71x1wU6So4=@protonmail.com>

I'm a newbie for R lang. And I recently came across the "Recycling Rule" when adding two vectors of unequal length.

I learned from this tutor [ http://www.r-tutor.com/r-introduction/vector/vector-arithmetics ] that:

""""""

If two vectors are of unequal length, the shorter one will be recycled in order to match the longer vector. For example, the following vectors u and v have different lengths, and their sum is computed by recycling values of the shorter vector u.

> u = c(10, 20, 30)

> v = c(1, 2, 3, 4, 5, 6, 7, 8, 9)

> u + v

[1] 11 22 33 14 25 36 17 28 39

""""""

And I wondered, why the shorter vecter u should be recycled? Why not just leave the extra values(4,5,6,7,8,9) in the longer vector untouched by default?

Otherwise is it better to have another function that could add vectors without recycling? Right now the recycling feature bugs me a lot.

Sent with [ProtonMail](https://protonmail.com) Secure Email.
	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Wed Dec 13 07:16:14 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 12 Dec 2017 22:16:14 -0800
Subject: [R] Add vectors of unequal length without recycling?
In-Reply-To: <jkOWiOMgaIdgmQJBkGQi98m9V0ZDOKov3ioB_7ZV4wVkVDKlCuksj5dqw_kgdmvFcYubWyG3DBNShT2og494s0U4D5aFJRHrO71x1wU6So4=@protonmail.com>
References: <jkOWiOMgaIdgmQJBkGQi98m9V0ZDOKov3ioB_7ZV4wVkVDKlCuksj5dqw_kgdmvFcYubWyG3DBNShT2og494s0U4D5aFJRHrO71x1wU6So4=@protonmail.com>
Message-ID: <E180AFE4-7136-490A-9025-5A17054CFEAF@dcn.davis.ca.us>

Better get over it, because it isn't going to change. To avoid it, always work with vectors of the same length. 

This is a logical extension of the idea that a scalar adds to every element of a vector. 
-- 
Sent from my phone. Please excuse my brevity.

On December 12, 2017 9:41:06 PM PST, Maingo via R-help <r-help at r-project.org> wrote:
>I'm a newbie for R lang. And I recently came across the "Recycling
>Rule" when adding two vectors of unequal length.
>
>I learned from this tutor [
>http://www.r-tutor.com/r-introduction/vector/vector-arithmetics ] that:
>
>""""""
>
>If two vectors are of unequal length, the shorter one will be recycled
>in order to match the longer vector. For example, the following vectors
>u and v have different lengths, and their sum is computed by recycling
>values of the shorter vector u.
>
>> u = c(10, 20, 30)
>
>> v = c(1, 2, 3, 4, 5, 6, 7, 8, 9)
>
>> u + v
>
>[1] 11 22 33 14 25 36 17 28 39
>
>""""""
>
>And I wondered, why the shorter vecter u should be recycled? Why not
>just leave the extra values(4,5,6,7,8,9) in the longer vector untouched
>by default?
>
>Otherwise is it better to have another function that could add vectors
>without recycling? Right now the recycling feature bugs me a lot.
>
>Sent with [ProtonMail](https://protonmail.com) Secure Email.
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Wed Dec 13 08:19:08 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 13 Dec 2017 07:19:08 +0000
Subject: [R] Add vectors of unequal length without recycling?
In-Reply-To: <jkOWiOMgaIdgmQJBkGQi98m9V0ZDOKov3ioB_7ZV4wVkVDKlCuksj5dqw_kgdmvFcYubWyG3DBNShT2og494s0U4D5aFJRHrO71x1wU6So4=@protonmail.com>
References: <jkOWiOMgaIdgmQJBkGQi98m9V0ZDOKov3ioB_7ZV4wVkVDKlCuksj5dqw_kgdmvFcYubWyG3DBNShT2og494s0U4D5aFJRHrO71x1wU6So4=@protonmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF880155FCB373@SRVEXCHCM301.precheza.cz>

Hi

If some feature does not suit your intentions you can make your own. Especially in this simple case.

myadd<-function(x,y) {

if(length(x)!=length(y)) {

n <- max(length(x), length(y))
length(x) <- n
length(y) <- n
x[is.na(x)]<-0
y[is.na(u)]<-0
}
x+y
}

> myadd(u,v)
[1] 11 22 33  4  5  6  7  8  9
>

But usually data to R are imported as data frames and missing values are set to NA.

In such case you could use rowSums with na.rm argument.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Maingo via R-
> help
> Sent: Wednesday, December 13, 2017 6:41 AM
> To: r-help at r-project.org
> Subject: [R] Add vectors of unequal length without recycling?
>
> I'm a newbie for R lang. And I recently came across the "Recycling Rule" when
> adding two vectors of unequal length.
>
> I learned from this tutor [ http://www.r-tutor.com/r-
> introduction/vector/vector-arithmetics ] that:
>
> """"""
>
> If two vectors are of unequal length, the shorter one will be recycled in order to
> match the longer vector. For example, the following vectors u and v have
> different lengths, and their sum is computed by recycling values of the shorter
> vector u.
>
> > u = c(10, 20, 30)
>
> > v = c(1, 2, 3, 4, 5, 6, 7, 8, 9)
>
> > u + v
>
> [1] 11 22 33 14 25 36 17 28 39
>
> """"""
>
> And I wondered, why the shorter vecter u should be recycled? Why not just
> leave the extra values(4,5,6,7,8,9) in the longer vector untouched by default?
>
> Otherwise is it better to have another function that could add vectors without
> recycling? Right now the recycling feature bugs me a lot.
>
> Sent with [ProtonMail](https://protonmail.com) Secure Email.
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From b.h.mevik at usit.uio.no  Wed Dec 13 08:52:02 2017
From: b.h.mevik at usit.uio.no (=?utf-8?Q?Bj=C3=B8rn-Helge_Mevik?=)
Date: Wed, 13 Dec 2017 08:52:02 +0100
Subject: [R] PLS in R
In-Reply-To: <CACtBhRPhpDytqNr4h=pRJ31+_60Sxs5rLki4yk9uQfe6pLhypg@mail.gmail.com>
 (Margarida Soares's message of "Tue, 12 Dec 2017 16:10:43 +0100")
References: <CACtBhRNLBzSzbgWEwCQONLDxKXv8U4DoF76hqa7i75XGgmkSPQ@mail.gmail.com>
 <s3s1sk7hvg9.fsf@varelg.uio.no>
 <CACtBhRPhpDytqNr4h=pRJ31+_60Sxs5rLki4yk9uQfe6pLhypg@mail.gmail.com>
Message-ID: <s3so9n3dpzx.fsf@varelg.uio.no>

Margarida Soares <margaridapmsoares at gmail.com> writes:

> Thanks for your reply on pls!
> I have tried to do a correlation plot but I get the following group of
> graphs. Any way of having only 1 plot?
> This is my script:
>
> corrplot(plsrcue1, comp = 1:4, radii = c(sqrt(1/2), 1), identify = FALSE,
> type = "p" )

"Correlation loadings" are the correlations between each variable and
the selected components, so I don't see how you can have more than two
sets of correlations (i.e., more than two components) in a single
scatter plot.  You could have three sets in a 3d plot, of course, but
that you would have to implement yourself. :)

-- 
Regards,
Bj?rn-Helge Mevik
-------------- neste del --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 800 bytes
Desc: ikke tilgjengelig
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20171213/162e860b/attachment.sig>

From jtelleriar at gmail.com  Wed Dec 13 12:04:51 2017
From: jtelleriar at gmail.com (Juan Telleria)
Date: Wed, 13 Dec 2017 12:04:51 +0100
Subject: [R] Best R GUIs
In-Reply-To: <CANNd7==hD5bxGx21684nWNv=x+-ZLjCi6teDRoob2aNEe86B-g@mail.gmail.com>
References: <CANNd7=mDae-CnAkPgJwkG1UQ-Xj_K6uRZqdBy_K4EhR2nSwE_Q@mail.gmail.com>
 <CANNd7=n785afAiSCasvTeCUVrLESoQD29bk_q9SL+9CvH0fRWQ@mail.gmail.com>
 <CANNd7=kOaKonKX2aS_71qT5v2FWN6YJL_s5LmgiT8+BOqX6fBw@mail.gmail.com>
 <CANNd7=nkFxijw9NPv5NnB6nDHiDb9-pZ0TDyGcR1wY2KCD4xUw@mail.gmail.com>
 <CANNd7=nGAWqXGJRs7sNmXm_rqO=U0-FSYqdj=Rpy=mqrw97CYQ@mail.gmail.com>
 <CANNd7==hD5bxGx21684nWNv=x+-ZLjCi6teDRoob2aNEe86B-g@mail.gmail.com>
Message-ID: <CANNd7=mE-Uoyx2prSTewFcppPf7isCf-=kpP_+L6bjw6xHVJZw@mail.gmail.com>

Dear R Community Members,

I would like to add to one article I have written the best Graphical User
Interfaces the R programming language has.

For the moment I know:
A) Rstudio.
B) R Tools for Visual Studio.
C) Open Analytics Architect.

Are there others worth to mention?

Thank you.

Kind regards,
Juan Telleria

	[[alternative HTML version deleted]]


From istazahn at gmail.com  Wed Dec 13 13:03:25 2017
From: istazahn at gmail.com (Ista Zahn)
Date: Wed, 13 Dec 2017 07:03:25 -0500
Subject: [R] Best R GUIs
In-Reply-To: <CANNd7=mE-Uoyx2prSTewFcppPf7isCf-=kpP_+L6bjw6xHVJZw@mail.gmail.com>
References: <CANNd7=mDae-CnAkPgJwkG1UQ-Xj_K6uRZqdBy_K4EhR2nSwE_Q@mail.gmail.com>
 <CANNd7=n785afAiSCasvTeCUVrLESoQD29bk_q9SL+9CvH0fRWQ@mail.gmail.com>
 <CANNd7=kOaKonKX2aS_71qT5v2FWN6YJL_s5LmgiT8+BOqX6fBw@mail.gmail.com>
 <CANNd7=nkFxijw9NPv5NnB6nDHiDb9-pZ0TDyGcR1wY2KCD4xUw@mail.gmail.com>
 <CANNd7=nGAWqXGJRs7sNmXm_rqO=U0-FSYqdj=Rpy=mqrw97CYQ@mail.gmail.com>
 <CANNd7==hD5bxGx21684nWNv=x+-ZLjCi6teDRoob2aNEe86B-g@mail.gmail.com>
 <CANNd7=mE-Uoyx2prSTewFcppPf7isCf-=kpP_+L6bjw6xHVJZw@mail.gmail.com>
Message-ID: <CA+vqiLFcaO3qzuyqsZj=qP-f5NOSnLe2n+gY_gztPAHJ2-_PRg@mail.gmail.com>

On Dec 13, 2017 6:05 AM, "Juan Telleria" <jtelleriar at gmail.com> wrote:

Dear R Community Members,

I would like to add to one article I have written the best Graphical User
Interfaces the R programming language has.

For the moment I know:
A) Rstudio.
B) R Tools for Visual Studio.
C) Open Analytics Architect.


Many editors have plugins for working with R, including

D) ESS for Emacs
E) IRkernel for Jupyter notebooks
F) StatET for Eclipse
G) Vim-R for vim


Are there others worth to mention?

Thank you.

Kind regards,
Juan Telleria

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From chalabi.elahe at yahoo.de  Wed Dec 13 13:14:01 2017
From: chalabi.elahe at yahoo.de (Elahe chalabi)
Date: Wed, 13 Dec 2017 12:14:01 +0000 (UTC)
Subject: [R] overlay two histograms ggplot
References: <123746066.6750128.1513167241106.ref@mail.yahoo.com>
Message-ID: <123746066.6750128.1513167241106@mail.yahoo.com>

Hi all,

How can I overlay these two histograms? 

ggplot(gg, aes(gg$Alz, fill = gg$veg)) + geom_histogram(alpha = 0.2)
ggplot(tt, aes(tt$Cont, fill = tt$veg)) + geom_histogram(alpha = 0.2)

thanks for any help!
Elahe


From Gaurang.Mehta at royallondon.com  Wed Dec 13 13:15:55 2017
From: Gaurang.Mehta at royallondon.com (Mehta, Gaurang)
Date: Wed, 13 Dec 2017 12:15:55 +0000
Subject: [R] Best R GUIs
In-Reply-To: <CA+vqiLFcaO3qzuyqsZj=qP-f5NOSnLe2n+gY_gztPAHJ2-_PRg@mail.gmail.com>
References: <CANNd7=mDae-CnAkPgJwkG1UQ-Xj_K6uRZqdBy_K4EhR2nSwE_Q@mail.gmail.com> 
 <CANNd7=n785afAiSCasvTeCUVrLESoQD29bk_q9SL+9CvH0fRWQ@mail.gmail.com> 
 <CANNd7=kOaKonKX2aS_71qT5v2FWN6YJL_s5LmgiT8+BOqX6fBw@mail.gmail.com> 
 <CANNd7=nkFxijw9NPv5NnB6nDHiDb9-pZ0TDyGcR1wY2KCD4xUw@mail.gmail.com> 
 <CANNd7=nGAWqXGJRs7sNmXm_rqO=U0-FSYqdj=Rpy=mqrw97CYQ@mail.gmail.com> 
 <CANNd7==hD5bxGx21684nWNv=x+-ZLjCi6teDRoob2aNEe86B-g@mail.gmail.com> 
 <CANNd7=mE-Uoyx2prSTewFcppPf7isCf-=kpP_+L6bjw6xHVJZw@mail.gmail.com> 
 <CA+vqiLFcaO3qzuyqsZj=qP-f5NOSnLe2n+gY_gztPAHJ2-_PRg@mail.gmail.com>
Message-ID: <f17b340d70164ca0b391d75eff03e6c0@WCPRDSHWWEDW01.royallondongroup.com>

Hi Zahn,
I thought Shiny package in R was the GUI equivalent for R applications.
Tools such as Rstudio is an IDE (integrated development environment) to me.
Regards,
Gaurang Mehta

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ista Zahn
Sent: 13 December 2017 12:03
To: Juan Telleria
Cc: r-help at r-project.org
Subject: Re: [R] Best R GUIs

On Dec 13, 2017 6:05 AM, "Juan Telleria" <jtelleriar at gmail.com> wrote:

Dear R Community Members,

I would like to add to one article I have written the best Graphical User Interfaces the R programming language has.

For the moment I know:
A) Rstudio.
B) R Tools for Visual Studio.
C) Open Analytics Architect.


Many editors have plugins for working with R, including

D) ESS for Emacs
E) IRkernel for Jupyter notebooks
F) StatET for Eclipse
G) Vim-R for vim


Are there others worth to mention?

Thank you.

Kind regards,
Juan Telleria

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

This email is intended for the person or company named and access by anyone else is unauthorised. If you are not the person or company named, please delete this email and notify the sender.

The information in this email, including any attachments, may be confidential or legally privileged (meaning that its disclosure is protected in law). Its unauthorised disclosure, copying, distribution or use is prohibited and may be unlawful.

Email communications sent over the internet are not guaranteed to be secure or virus-free and such messages are potentially at risk.  The Royal London Group accepts no liability for any claims arising from use of the internet to transmit messages by or to any company within the Royal London Group.

The Royal London Group consists of The Royal London Mutual Insurance Society Limited and its subsidiaries.

The Royal London Mutual Insurance Society Limited is authorised by the Prudential Regulation Authority and regulated by the Financial Conduct Authority and the Prudential Regulation Authority and provides life assurance and pensions.

Registered in England and Wales number 99064.

Registered office: 55 Gracechurch Street, London, EC3V 0RL.

In the Republic of Ireland: The Royal London Mutual Insurance Society Limited is authorised by the Prudential Regulation Authority in the UK and is regulated by the Central Bank of Ireland for conduct of business rules.


From jtelleriar at gmail.com  Wed Dec 13 16:01:51 2017
From: jtelleriar at gmail.com (Juan Telleria)
Date: Wed, 13 Dec 2017 16:01:51 +0100
Subject: [R] Best R GUIs
In-Reply-To: <CA+vqiLFcaO3qzuyqsZj=qP-f5NOSnLe2n+gY_gztPAHJ2-_PRg@mail.gmail.com>
References: <CANNd7=mDae-CnAkPgJwkG1UQ-Xj_K6uRZqdBy_K4EhR2nSwE_Q@mail.gmail.com>
 <CANNd7=n785afAiSCasvTeCUVrLESoQD29bk_q9SL+9CvH0fRWQ@mail.gmail.com>
 <CANNd7=kOaKonKX2aS_71qT5v2FWN6YJL_s5LmgiT8+BOqX6fBw@mail.gmail.com>
 <CANNd7=nkFxijw9NPv5NnB6nDHiDb9-pZ0TDyGcR1wY2KCD4xUw@mail.gmail.com>
 <CANNd7=nGAWqXGJRs7sNmXm_rqO=U0-FSYqdj=Rpy=mqrw97CYQ@mail.gmail.com>
 <CANNd7==hD5bxGx21684nWNv=x+-ZLjCi6teDRoob2aNEe86B-g@mail.gmail.com>
 <CANNd7=mE-Uoyx2prSTewFcppPf7isCf-=kpP_+L6bjw6xHVJZw@mail.gmail.com>
 <CA+vqiLFcaO3qzuyqsZj=qP-f5NOSnLe2n+gY_gztPAHJ2-_PRg@mail.gmail.com>
Message-ID: <CANNd7=kLr5=qkf2WbVpJN7rAf9TRVM0Y60m1LEj7fyhQybCppw@mail.gmail.com>

Thank you all, some of the best free IDEs are specified in the following
article:

http://www.linuxlinks.com/article/20110306113701179/GUIsforR.html

Plus pluggable IDEs:
A) ESS for Emacs
B) IRkernel for Jupyter notebooks
C) StatET for Eclipse
D) Vim-R for vim


Kind regards,
Juan

	[[alternative HTML version deleted]]


From jszhao at yeah.net  Wed Dec 13 16:31:29 2017
From: jszhao at yeah.net (Jinsong Zhao)
Date: Wed, 13 Dec 2017 23:31:29 +0800
Subject: [R] difference between ifelse and if...else?
Message-ID: <c297d21e-3a0d-75c0-895a-0e0d5e56e0e2@yeah.net>

Hi there,

I don't know why the following codes are return different results.

 > ifelse(3 > 2, 1:3, length(1:3))
[1] 1
 > if (3 > 2) 1:3 else length(1:3)
[1] 1 2 3

Any hints?

Best,
Jinsong


From roundsjeremiah at gmail.com  Wed Dec 13 16:33:38 2017
From: roundsjeremiah at gmail.com (jeremiah rounds)
Date: Wed, 13 Dec 2017 07:33:38 -0800
Subject: [R] difference between ifelse and if...else?
In-Reply-To: <c297d21e-3a0d-75c0-895a-0e0d5e56e0e2@yeah.net>
References: <c297d21e-3a0d-75c0-895a-0e0d5e56e0e2@yeah.net>
Message-ID: <CAOjnRsYtFeRmX8T_Zk3gFVikb=Yd0K7JY7k3TQ77MJTLZ6X9Mw@mail.gmail.com>

ifelse is vectorized.

On Wed, Dec 13, 2017 at 7:31 AM, Jinsong Zhao <jszhao at yeah.net> wrote:

> Hi there,
>
> I don't know why the following codes are return different results.
>
> > ifelse(3 > 2, 1:3, length(1:3))
> [1] 1
> > if (3 > 2) 1:3 else length(1:3)
> [1] 1 2 3
>
> Any hints?
>
> Best,
> Jinsong
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Wed Dec 13 16:40:19 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Wed, 13 Dec 2017 17:40:19 +0200
Subject: [R] difference between ifelse and if...else?
In-Reply-To: <CAOjnRsYtFeRmX8T_Zk3gFVikb=Yd0K7JY7k3TQ77MJTLZ6X9Mw@mail.gmail.com>
References: <c297d21e-3a0d-75c0-895a-0e0d5e56e0e2@yeah.net>
 <CAOjnRsYtFeRmX8T_Zk3gFVikb=Yd0K7JY7k3TQ77MJTLZ6X9Mw@mail.gmail.com>
Message-ID: <CAGgJW75FdjbgCbkyq+RoXOb2xbJCNY-sCxd-jLtUm+bygrbGeQ@mail.gmail.com>

ifelse returns the "shape" of the first argument

In your ifelse the shape of "3 > 2" is a vector of length one, so it will
return a vector length one.

Avoid "ifelse" until you are very comfortable with it. It can often burn
you.




On Wed, Dec 13, 2017 at 5:33 PM, jeremiah rounds <roundsjeremiah at gmail.com>
wrote:

> ifelse is vectorized.
>
> On Wed, Dec 13, 2017 at 7:31 AM, Jinsong Zhao <jszhao at yeah.net> wrote:
>
> > Hi there,
> >
> > I don't know why the following codes are return different results.
> >
> > > ifelse(3 > 2, 1:3, length(1:3))
> > [1] 1
> > > if (3 > 2) 1:3 else length(1:3)
> > [1] 1 2 3
> >
> > Any hints?
> >
> > Best,
> > Jinsong
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posti
> > ng-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From pauljohn32 at gmail.com  Wed Dec 13 16:56:17 2017
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Wed, 13 Dec 2017 09:56:17 -0600
Subject: [R] [ESS]  M-x R gives no choice of starting dir
In-Reply-To: <87h8w9rnlh.fsf@enricoschumann.net>
References: <65f0acc4-ea1c-0a62-7092-d5b071c8d62c@echoffmann.ch>
 <87h8w9rnlh.fsf@enricoschumann.net>
Message-ID: <CAErODj_90FoBSvEMpbTCDb9TC44A-J5LniuLPSR7hN2GBM466w@mail.gmail.com>

If Emacs is not asking for starting directory, it is very likely your
init file has this somewhere:

(setq ess-ask-for-ess-directory nil)


On Mon, Sep 11, 2017 at 3:23 PM, Enrico Schumann <es at enricoschumann.net> wrote:
> On Mon, 11 Sep 2017, Christian writes:
>
>> Hi,
>>
>> I experienced a sudden change in the behavior of M-x R in not giving
>> me the choice where to start R. May be that I botched my
>> preferences. I am using Aquamacs 3.3 on MacOS 10.12.6
>>
>> Christian
>
> I suppose you are using ESS? There is a variable called
> 'ess-ask-for-ess-directory', which controls whether
> M-x R prompts for a directory. Perhaps you have set
> this to nil?
>
> I also Cc the ESS-help mailing list and suggest that
> follow-up be sent there.
>
>
> Kind regards
>      Enrico
>
> --
> Enrico Schumann
> Lucerne, Switzerland
> http://enricoschumann.net
>
> ______________________________________________
> ESS-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/ess-help



-- 
Paul E. Johnson   http://pj.freefaculty.org
Director, Center for Research Methods and Data Analysis http://crmda.ku.edu

To write to me directly, please address me at pauljohn at ku.edu.


From wdunlap at tibco.com  Wed Dec 13 17:34:42 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 13 Dec 2017 08:34:42 -0800
Subject: [R] Add vectors of unequal length without recycling?
In-Reply-To: <jkOWiOMgaIdgmQJBkGQi98m9V0ZDOKov3ioB_7ZV4wVkVDKlCuksj5dqw_kgdmvFcYubWyG3DBNShT2og494s0U4D5aFJRHrO71x1wU6So4=@protonmail.com>
References: <jkOWiOMgaIdgmQJBkGQi98m9V0ZDOKov3ioB_7ZV4wVkVDKlCuksj5dqw_kgdmvFcYubWyG3DBNShT2og494s0U4D5aFJRHrO71x1wU6So4=@protonmail.com>
Message-ID: <CAF8bMcaj-eP7TMWEP3ckpbnQR_yNkuQuK_=EDJ3KsU+NytveTw@mail.gmail.com>

Without recycling you would get:
    u <- c(10, 20, 30)
    u + 1
    #[1] 11 20 30
which would be pretty inconvenient.

(Note that the recycling rule has to make a special case for when one
argument has length zero - the output then has length zero as well.)


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Dec 12, 2017 at 9:41 PM, Maingo via R-help <r-help at r-project.org>
wrote:

> I'm a newbie for R lang. And I recently came across the "Recycling Rule"
> when adding two vectors of unequal length.
>
> I learned from this tutor [ http://www.r-tutor.com/r-
> introduction/vector/vector-arithmetics ] that:
>
> """"""
>
> If two vectors are of unequal length, the shorter one will be recycled in
> order to match the longer vector. For example, the following vectors u and
> v have different lengths, and their sum is computed by recycling values of
> the shorter vector u.
>
> > u = c(10, 20, 30)
>
> > v = c(1, 2, 3, 4, 5, 6, 7, 8, 9)
>
> > u + v
>
> [1] 11 22 33 14 25 36 17 28 39
>
> """"""
>
> And I wondered, why the shorter vecter u should be recycled? Why not just
> leave the extra values(4,5,6,7,8,9) in the longer vector untouched by
> default?
>
> Otherwise is it better to have another function that could add vectors
> without recycling? Right now the recycling feature bugs me a lot.
>
> Sent with [ProtonMail](https://protonmail.com) Secure Email.
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Wed Dec 13 17:37:21 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 13 Dec 2017 11:37:21 -0500
Subject: [R] difference between ifelse and if...else?
In-Reply-To: <c297d21e-3a0d-75c0-895a-0e0d5e56e0e2@yeah.net>
References: <c297d21e-3a0d-75c0-895a-0e0d5e56e0e2@yeah.net>
Message-ID: <c99c95ec-d93e-cc8d-ea6c-49bf400c5979@gmail.com>

On 13/12/2017 10:31 AM, Jinsong Zhao wrote:
> Hi there,
> 
> I don't know why the following codes are return different results.
> 
>   > ifelse(3 > 2, 1:3, length(1:3))
> [1] 1
>   > if (3 > 2) 1:3 else length(1:3)
> [1] 1 2 3
> 
> Any hints?

The documentation in the help page ?ifelse and ?"if" explains it pretty 
clearly.  Think of ifelse() as a function with vector inputs and a 
vector output, and if() as a flow control construction.

Duncan Murdoch


From wjm1 at caa.columbia.edu  Wed Dec 13 19:17:51 2017
From: wjm1 at caa.columbia.edu (William Michels)
Date: Wed, 13 Dec 2017 10:17:51 -0800
Subject: [R] Add vectors of unequal length without recycling?
In-Reply-To: <CAF8bMcaj-eP7TMWEP3ckpbnQR_yNkuQuK_=EDJ3KsU+NytveTw@mail.gmail.com>
References: <jkOWiOMgaIdgmQJBkGQi98m9V0ZDOKov3ioB_7ZV4wVkVDKlCuksj5dqw_kgdmvFcYubWyG3DBNShT2og494s0U4D5aFJRHrO71x1wU6So4=@protonmail.com>
 <CAF8bMcaj-eP7TMWEP3ckpbnQR_yNkuQuK_=EDJ3KsU+NytveTw@mail.gmail.com>
Message-ID: <CAA99HCxJo4jUqaikEY9tdE3avAHShrBTbvzhbh6-q_OU1L0SQg@mail.gmail.com>

Maingo,

See previous discussion below on rbind.na() and cbind.na() scripts:

https://stat.ethz.ch/pipermail/r-help/2016-December/443790.html

You might consider binding first then adding orthogonally.
So rbind.na() then colSums(), OR cbind.na() then rowSums().

Best of luck,

W Michels, Ph.D.



On Wed, Dec 13, 2017 at 8:34 AM, William Dunlap via R-help
<r-help at r-project.org> wrote:
> Without recycling you would get:
>     u <- c(10, 20, 30)
>     u + 1
>     #[1] 11 20 30
> which would be pretty inconvenient.
>
> (Note that the recycling rule has to make a special case for when one
> argument has length zero - the output then has length zero as well.)
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Tue, Dec 12, 2017 at 9:41 PM, Maingo via R-help <r-help at r-project.org>
> wrote:
>
>> I'm a newbie for R lang. And I recently came across the "Recycling Rule"
>> when adding two vectors of unequal length.
>>
>> I learned from this tutor [ http://www.r-tutor.com/r-
>> introduction/vector/vector-arithmetics ] that:
>>
>> """"""
>>
>> If two vectors are of unequal length, the shorter one will be recycled in
>> order to match the longer vector. For example, the following vectors u and
>> v have different lengths, and their sum is computed by recycling values of
>> the shorter vector u.
>>
>> > u = c(10, 20, 30)
>>
>> > v = c(1, 2, 3, 4, 5, 6, 7, 8, 9)
>>
>> > u + v
>>
>> [1] 11 22 33 14 25 36 17 28 39
>>
>> """"""
>>
>> And I wondered, why the shorter vecter u should be recycled? Why not just
>> leave the extra values(4,5,6,7,8,9) in the longer vector untouched by
>> default?
>>
>> Otherwise is it better to have another function that could add vectors
>> without recycling? Right now the recycling feature bugs me a lot.
>>
>> Sent with [ProtonMail](https://protonmail.com) Secure Email.
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From liuwensui at gmail.com  Wed Dec 13 19:25:50 2017
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 13 Dec 2017 12:25:50 -0600
Subject: [R] Best R GUIs
In-Reply-To: <CANNd7=mE-Uoyx2prSTewFcppPf7isCf-=kpP_+L6bjw6xHVJZw@mail.gmail.com>
References: <CANNd7=mDae-CnAkPgJwkG1UQ-Xj_K6uRZqdBy_K4EhR2nSwE_Q@mail.gmail.com>
 <CANNd7=n785afAiSCasvTeCUVrLESoQD29bk_q9SL+9CvH0fRWQ@mail.gmail.com>
 <CANNd7=kOaKonKX2aS_71qT5v2FWN6YJL_s5LmgiT8+BOqX6fBw@mail.gmail.com>
 <CANNd7=nkFxijw9NPv5NnB6nDHiDb9-pZ0TDyGcR1wY2KCD4xUw@mail.gmail.com>
 <CANNd7=nGAWqXGJRs7sNmXm_rqO=U0-FSYqdj=Rpy=mqrw97CYQ@mail.gmail.com>
 <CANNd7==hD5bxGx21684nWNv=x+-ZLjCi6teDRoob2aNEe86B-g@mail.gmail.com>
 <CANNd7=mE-Uoyx2prSTewFcppPf7isCf-=kpP_+L6bjw6xHVJZw@mail.gmail.com>
Message-ID: <CAKyN3iDooNzteXfjoM-ty8c=1_nQgvq8OLDqSx8wKonZ1bndRg@mail.gmail.com>

how could you miss emacs + ess?

On Wed, Dec 13, 2017 at 5:04 AM, Juan Telleria <jtelleriar at gmail.com> wrote:

> Dear R Community Members,
>
> I would like to add to one article I have written the best Graphical User
> Interfaces the R programming language has.
>
> For the moment I know:
> A) Rstudio.
> B) R Tools for Visual Studio.
> C) Open Analytics Architect.
>
> Are there others worth to mention?
>
> Thank you.
>
> Kind regards,
> Juan Telleria
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ycding at coh.org  Wed Dec 13 18:38:56 2017
From: ycding at coh.org (Ding, Yuan Chun)
Date: Wed, 13 Dec 2017 17:38:56 +0000
Subject: [R] running Cox regression model for 1000 markers
Message-ID: <A86C6438FB909A409DDEF926277952B6A4FED9@PPWEXCH2KX14.coh.org>

HI All,

Sorry to bother you.

I have a data matrix with 1000 genes in rows and 100 samples in columns;  I need to run Cox regression model for each of the 1000 markers.  I know how to run Cox regression model for individual marker. Does anyone know a R package or method to run Cox model for those 1000 markers and then output results in a data frame?

Thank you,

Ding


---------------------------------------------------------------------
-SECURITY/CONFIDENTIALITY WARNING-
This message (and any attachments) are intended solely f...{{dropped:22}}


From valkremk at gmail.com  Wed Dec 13 22:36:18 2017
From: valkremk at gmail.com (Val)
Date: Wed, 13 Dec 2017 15:36:18 -0600
Subject: [R] match and new columns
Message-ID: <CAJOiR6Zq05JLpx60xFWpbzByFOhuh-K7-ZkGcu=-BF9sRzt6aQ@mail.gmail.com>

Hi all,

I have a data frame
tdat <- read.table(textConnection("A B C Y
A12 B03 C04 0.70
A23 B05 C06 0.05
A14 B06 C07 1.20
A25 A23 A12 3.51
A16 A25 A14 2,16"),header = TRUE)

I want match tdat$B with tdat$A and populate the  column   values of tdat$A
( col A and Col B) in the newly created columns (col D and col  E).  please
find my attempt and the desired output below

Desired output
A B C Y  D E
A12 B03 C04 0.70  0  0
A23 B05 C06 0.05  0  0
A14 B06 C07 1.20  0  0
A25 A23 A12 3.51 B05 C06
A16 A25 A14 2,16 A23 A12

my attempt,

tdat$D <- 0
tdat$E <- 0

if(tdat$B %in% tdat$A)
  {
  tdat$D <- tdat$A[tdat$B]
  tdat$E <- tdat$A[tdat$C]
}
 but did not work.

Thank you in advance

	[[alternative HTML version deleted]]


From macqueen1 at llnl.gov  Wed Dec 13 22:46:58 2017
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Wed, 13 Dec 2017 21:46:58 +0000
Subject: [R] difference between ifelse and if...else?
In-Reply-To: <c297d21e-3a0d-75c0-895a-0e0d5e56e0e2@yeah.net>
References: <c297d21e-3a0d-75c0-895a-0e0d5e56e0e2@yeah.net>
Message-ID: <150769A9-9320-40F7-9E48-9D1E549F5DCB@llnl.gov>

Because ifelse is not intended to be an alternative to if ... else. They exist for different purposes.

(besides the other replies, a careful reading of their help pages, and trying the examples, should explain the different purposes).

--
Don MacQueen
Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062
Lab cell 925-724-7509
 
 

On 12/13/17, 7:31 AM, "R-help on behalf of Jinsong Zhao" <r-help-bounces at r-project.org on behalf of jszhao at yeah.net> wrote:

    Hi there,
    
    I don't know why the following codes are return different results.
    
     > ifelse(3 > 2, 1:3, length(1:3))
    [1] 1
     > if (3 > 2) 1:3 else length(1:3)
    [1] 1 2 3
    
    Any hints?
    
    Best,
    Jinsong
    
    ______________________________________________
    R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    https://stat.ethz.ch/mailman/listinfo/r-help
    PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    and provide commented, minimal, self-contained, reproducible code.
    


From ruipbarradas at sapo.pt  Wed Dec 13 23:36:24 2017
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Wed, 13 Dec 2017 22:36:24 +0000
Subject: [R] match and new columns
In-Reply-To: <CAJOiR6Zq05JLpx60xFWpbzByFOhuh-K7-ZkGcu=-BF9sRzt6aQ@mail.gmail.com>
References: <CAJOiR6Zq05JLpx60xFWpbzByFOhuh-K7-ZkGcu=-BF9sRzt6aQ@mail.gmail.com>
Message-ID: <9f2555fe-bc0f-7914-327b-b43097ad9626@sapo.pt>

Hello,

Here is one way.

tdat$D <- ifelse(tdat$B %in% tdat$A, tdat$A[tdat$B], 0)
tdat$E <- ifelse(tdat$B %in% tdat$A, tdat$A[tdat$C], 0)


Hope this helps,

Rui Barradas

On 12/13/2017 9:36 PM, Val wrote:
> Hi all,
> 
> I have a data frame
> tdat <- read.table(textConnection("A B C Y
> A12 B03 C04 0.70
> A23 B05 C06 0.05
> A14 B06 C07 1.20
> A25 A23 A12 3.51
> A16 A25 A14 2,16"),header = TRUE)
> 
> I want match tdat$B with tdat$A and populate the  column   values of tdat$A
> ( col A and Col B) in the newly created columns (col D and col  E).  please
> find my attempt and the desired output below
> 
> Desired output
> A B C Y  D E
> A12 B03 C04 0.70  0  0
> A23 B05 C06 0.05  0  0
> A14 B06 C07 1.20  0  0
> A25 A23 A12 3.51 B05 C06
> A16 A25 A14 2,16 A23 A12
> 
> my attempt,
> 
> tdat$D <- 0
> tdat$E <- 0
> 
> if(tdat$B %in% tdat$A)
>    {
>    tdat$D <- tdat$A[tdat$B]
>    tdat$E <- tdat$A[tdat$C]
> }
>   but did not work.
> 
> Thank you in advance
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From valkremk at gmail.com  Thu Dec 14 00:02:10 2017
From: valkremk at gmail.com (Val)
Date: Wed, 13 Dec 2017 17:02:10 -0600
Subject: [R] match and new columns
In-Reply-To: <9f2555fe-bc0f-7914-327b-b43097ad9626@sapo.pt>
References: <CAJOiR6Zq05JLpx60xFWpbzByFOhuh-K7-ZkGcu=-BF9sRzt6aQ@mail.gmail.com>
 <9f2555fe-bc0f-7914-327b-b43097ad9626@sapo.pt>
Message-ID: <CAJOiR6aw-Q4xoW3CQC=XL5eDzSAGfdngPb9=hNp0+259uXBiJw@mail.gmail.com>

Thank you Rui,
I did not get the desired result. Here is the output from your script

   A   B   C    Y D E
1 A12 B03 C04 0.70 0 0
2 A23 B05 C06 0.05 0 0
3 A14 B06 C07 1.20 0 0
4 A25 A23 A12 3.51 1 1
5 A16 A25 A14 2,16 4 4


On Wed, Dec 13, 2017 at 4:36 PM, Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> Here is one way.
>
> tdat$D <- ifelse(tdat$B %in% tdat$A, tdat$A[tdat$B], 0)
> tdat$E <- ifelse(tdat$B %in% tdat$A, tdat$A[tdat$C], 0)
>
>
> Hope this helps,
>
> Rui Barradas
>
>
> On 12/13/2017 9:36 PM, Val wrote:
>
>> Hi all,
>>
>> I have a data frame
>> tdat <- read.table(textConnection("A B C Y
>> A12 B03 C04 0.70
>> A23 B05 C06 0.05
>> A14 B06 C07 1.20
>> A25 A23 A12 3.51
>> A16 A25 A14 2,16"),header = TRUE)
>>
>> I want match tdat$B with tdat$A and populate the  column   values of
>> tdat$A
>> ( col A and Col B) in the newly created columns (col D and col  E).
>> please
>> find my attempt and the desired output below
>>
>> Desired output
>> A B C Y  D E
>> A12 B03 C04 0.70  0  0
>> A23 B05 C06 0.05  0  0
>> A14 B06 C07 1.20  0  0
>> A25 A23 A12 3.51 B05 C06
>> A16 A25 A14 2,16 A23 A12
>>
>> my attempt,
>>
>> tdat$D <- 0
>> tdat$E <- 0
>>
>> if(tdat$B %in% tdat$A)
>>    {
>>    tdat$D <- tdat$A[tdat$B]
>>    tdat$E <- tdat$A[tdat$C]
>> }
>>   but did not work.
>>
>> Thank you in advance
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Thu Dec 14 02:23:41 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 13 Dec 2017 17:23:41 -0800
Subject: [R] match and new columns
In-Reply-To: <CAJOiR6aw-Q4xoW3CQC=XL5eDzSAGfdngPb9=hNp0+259uXBiJw@mail.gmail.com>
References: <CAJOiR6Zq05JLpx60xFWpbzByFOhuh-K7-ZkGcu=-BF9sRzt6aQ@mail.gmail.com>
 <9f2555fe-bc0f-7914-327b-b43097ad9626@sapo.pt>
 <CAJOiR6aw-Q4xoW3CQC=XL5eDzSAGfdngPb9=hNp0+259uXBiJw@mail.gmail.com>
Message-ID: <CAF8bMcYK-uctBOZfOJ8=VWX2mSq2_=j+ap0_=H1myo4S=tRoEg@mail.gmail.com>

Use the stringsAsFactors=FALSE argument to read.table when
making your data.frame - factors are getting in your way here.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Wed, Dec 13, 2017 at 3:02 PM, Val <valkremk at gmail.com> wrote:

> Thank you Rui,
> I did not get the desired result. Here is the output from your script
>
>    A   B   C    Y D E
> 1 A12 B03 C04 0.70 0 0
> 2 A23 B05 C06 0.05 0 0
> 3 A14 B06 C07 1.20 0 0
> 4 A25 A23 A12 3.51 1 1
> 5 A16 A25 A14 2,16 4 4
>
>
> On Wed, Dec 13, 2017 at 4:36 PM, Rui Barradas <ruipbarradas at sapo.pt>
> wrote:
>
> > Hello,
> >
> > Here is one way.
> >
> > tdat$D <- ifelse(tdat$B %in% tdat$A, tdat$A[tdat$B], 0)
> > tdat$E <- ifelse(tdat$B %in% tdat$A, tdat$A[tdat$C], 0)
> >
> >
> > Hope this helps,
> >
> > Rui Barradas
> >
> >
> > On 12/13/2017 9:36 PM, Val wrote:
> >
> >> Hi all,
> >>
> >> I have a data frame
> >> tdat <- read.table(textConnection("A B C Y
> >> A12 B03 C04 0.70
> >> A23 B05 C06 0.05
> >> A14 B06 C07 1.20
> >> A25 A23 A12 3.51
> >> A16 A25 A14 2,16"),header = TRUE)
> >>
> >> I want match tdat$B with tdat$A and populate the  column   values of
> >> tdat$A
> >> ( col A and Col B) in the newly created columns (col D and col  E).
> >> please
> >> find my attempt and the desired output below
> >>
> >> Desired output
> >> A B C Y  D E
> >> A12 B03 C04 0.70  0  0
> >> A23 B05 C06 0.05  0  0
> >> A14 B06 C07 1.20  0  0
> >> A25 A23 A12 3.51 B05 C06
> >> A16 A25 A14 2,16 A23 A12
> >>
> >> my attempt,
> >>
> >> tdat$D <- 0
> >> tdat$E <- 0
> >>
> >> if(tdat$B %in% tdat$A)
> >>    {
> >>    tdat$D <- tdat$A[tdat$B]
> >>    tdat$E <- tdat$A[tdat$C]
> >> }
> >>   but did not work.
> >>
> >> Thank you in advance
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posti
> >> ng-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From valkremk at gmail.com  Thu Dec 14 02:37:28 2017
From: valkremk at gmail.com (Val)
Date: Wed, 13 Dec 2017 19:37:28 -0600
Subject: [R] match and new columns
In-Reply-To: <CAF8bMcYK-uctBOZfOJ8=VWX2mSq2_=j+ap0_=H1myo4S=tRoEg@mail.gmail.com>
References: <CAJOiR6Zq05JLpx60xFWpbzByFOhuh-K7-ZkGcu=-BF9sRzt6aQ@mail.gmail.com>
 <9f2555fe-bc0f-7914-327b-b43097ad9626@sapo.pt>
 <CAJOiR6aw-Q4xoW3CQC=XL5eDzSAGfdngPb9=hNp0+259uXBiJw@mail.gmail.com>
 <CAF8bMcYK-uctBOZfOJ8=VWX2mSq2_=j+ap0_=H1myo4S=tRoEg@mail.gmail.com>
Message-ID: <CAJOiR6Y1+K=62XQmUu5Bngcme5XVQpBsW3Ao0aDSDXT=UK5WRw@mail.gmail.com>

Hi Bill,

I put stringsAsFactors = FALSE
 still did not work.

tdat <- read.table(textConnection("A B C Y
A12 B03 C04 0.70
A23 B05 C06 0.05
A14 B06 C07 1.20
A25 A23 A12 3.51
A16 A25 A14 2,16"),header = TRUE ,stringsAsFactors = FALSE)
tdat$D <- 0
tdat$E <- 0

tdat$D <- (ifelse(tdat$B %in% tdat$A, tdat$A[tdat$B], 0))
tdat$E <- (ifelse(tdat$B %in% tdat$A, tdat$A[tdat$C], 0))
tdat

I got this,

 A  B  C   Y   D    E
1 A12 B03 C04 0.70    0    0
2 A23 B05 C06 0.05    0    0
3 A14 B06 C07 1.20    0    0
4 A25 A23 A12 3.51 <NA> <NA>
5 A16 A25 A14 2,16 <NA> <NA>





On Wed, Dec 13, 2017 at 7:23 PM, William Dunlap <wdunlap at tibco.com> wrote:

> Use the stringsAsFactors=FALSE argument to read.table when
> making your data.frame - factors are getting in your way here.
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Wed, Dec 13, 2017 at 3:02 PM, Val <valkremk at gmail.com> wrote:
>
>> Thank you Rui,
>> I did not get the desired result. Here is the output from your script
>>
>>    A   B   C    Y D E
>> 1 A12 <https://maps.google.com/?q=1+A12&entry=gmail&source=g> B03 C04
>> 0.70 0 0
>> 2 A23 B05 C06 0.05 0 0
>> 3 A14 <https://maps.google.com/?q=3+A14&entry=gmail&source=g> B06 C07
>> 1.20 0 0
>> 4 A25 A23 A12 3.51 1 1
>> 5 A16 A25 A14 2,16 4
>> <https://maps.google.com/?q=A14+2,16+4&entry=gmail&source=g> 4
>>
>>
>> On Wed, Dec 13, 2017 at 4:36 PM, Rui Barradas <ruipbarradas at sapo.pt>
>> wrote:
>>
>> > Hello,
>> >
>> > Here is one way.
>> >
>> > tdat$D <- ifelse(tdat$B %in% tdat$A, tdat$A[tdat$B], 0)
>> > tdat$E <- ifelse(tdat$B %in% tdat$A, tdat$A[tdat$C], 0)
>> >
>> >
>> > Hope this helps,
>> >
>> > Rui Barradas
>> >
>> >
>> > On 12/13/2017 9:36 PM, Val wrote:
>> >
>> >> Hi all,
>> >>
>> >> I have a data frame
>> >> tdat <- read.table(textConnection("A B C Y
>> >> A12 B03 C04 0.70
>> >> A23 B05 C06 0.05
>> >> A14 B06 C07 1.20
>> >> A25 A23 A12 3.51
>> >> A16 A25 A14 2,16
>> <https://maps.google.com/?q=A14+2,16&entry=gmail&source=g>"),header =
>> TRUE)
>> >>
>> >> I want match tdat$B with tdat$A and populate the  column   values of
>> >> tdat$A
>> >> ( col A and Col B) in the newly created columns (col D and col  E).
>> >> please
>> >> find my attempt and the desired output below
>> >>
>> >> Desired output
>> >> A B C Y  D E
>> >> A12 B03 C04 0.70  0  0
>> >> A23 B05 C06 0.05  0  0
>> >> A14 B06 C07 1.20  0  0
>> >> A25 A23 A12 3.51 B05 C06
>> >> A16 A25 A14 2,16 A23 A12
>> <https://maps.google.com/?q=2,16+A23+A12&entry=gmail&source=g>
>> >>
>> >> my attempt,
>> >>
>> >> tdat$D <- 0
>> >> tdat$E <- 0
>> >>
>> >> if(tdat$B %in% tdat$A)
>> >>    {
>> >>    tdat$D <- tdat$A[tdat$B]
>> >>    tdat$E <- tdat$A[tdat$C]
>> >> }
>> >>   but did not work.
>> >>
>> >> Thank you in advance
>> >>
>> >>         [[alternative HTML version deleted]]
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide http://www.R-project.org/posti
>> >> ng-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Thu Dec 14 03:18:10 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 13 Dec 2017 18:18:10 -0800
Subject: [R] match and new columns
In-Reply-To: <CAJOiR6Y1+K=62XQmUu5Bngcme5XVQpBsW3Ao0aDSDXT=UK5WRw@mail.gmail.com>
References: <CAJOiR6Zq05JLpx60xFWpbzByFOhuh-K7-ZkGcu=-BF9sRzt6aQ@mail.gmail.com>
 <9f2555fe-bc0f-7914-327b-b43097ad9626@sapo.pt>
 <CAJOiR6aw-Q4xoW3CQC=XL5eDzSAGfdngPb9=hNp0+259uXBiJw@mail.gmail.com>
 <CAF8bMcYK-uctBOZfOJ8=VWX2mSq2_=j+ap0_=H1myo4S=tRoEg@mail.gmail.com>
 <CAJOiR6Y1+K=62XQmUu5Bngcme5XVQpBsW3Ao0aDSDXT=UK5WRw@mail.gmail.com>
Message-ID: <CAF8bMcYxk_Mb1gumYxSNGpR7hdttPQz0+1_pmRQhG8Y6VzY+FQ@mail.gmail.com>

Try the following (which won't work with factors):

> i <- match(tdat$B, tdat$A)
> newColumns <- tdat[i, c("B", "C")]
> newColumns[is.na(newColumns)] <- "0"
> transform(tdat, D=newColumns[["B"]], E=newColumns[["C"]])
    A   B   C    Y   D   E
1 A12 B03 C04 0.70   0   0
2 A23 B05 C06 0.05   0   0
3 A14 B06 C07 1.20   0   0
4 A25 A23 A12 3.51 B05 C06
5 A16 A25 A14 2,16 A23 A12


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Wed, Dec 13, 2017 at 5:37 PM, Val <valkremk at gmail.com> wrote:

> Hi Bill,
>
> I put stringsAsFactors = FALSE
>  still did not work.
>
> tdat <- read.table(textConnection("A B C Y
> A12 B03 C04 0.70
> A23 B05 C06 0.05
> A14 B06 C07 1.20
> A25 A23 A12 3.51
> A16 A25 A14 2,16
> <https://maps.google.com/?q=A14+2,16&entry=gmail&source=g>"),header =
> TRUE ,stringsAsFactors = FALSE)
> tdat$D <- 0
> tdat$E <- 0
>
> tdat$D <- (ifelse(tdat$B %in% tdat$A, tdat$A[tdat$B], 0))
> tdat$E <- (ifelse(tdat$B %in% tdat$A, tdat$A[tdat$C], 0))
> tdat
>
> I got this,
>
>  A  B  C   Y   D    E1 A12 <https://maps.google.com/?q=1+A12&entry=gmail&source=g> B03 C04 0.70    0    0
> 2 A23 B05 C06 0.05    0    03 A14 <https://maps.google.com/?q=3+A14&entry=gmail&source=g> B06 C07 1.20    0    0
> 4 A25 A23 A12 3.51 <NA> <NA>
> 5 A16 A25 A14 2,16 <NA> <NA>
>
>
>
>
>
> On Wed, Dec 13, 2017 at 7:23 PM, William Dunlap <wdunlap at tibco.com> wrote:
>
>> Use the stringsAsFactors=FALSE argument to read.table when
>> making your data.frame - factors are getting in your way here.
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>> On Wed, Dec 13, 2017 at 3:02 PM, Val <valkremk at gmail.com> wrote:
>>
>>> Thank you Rui,
>>> I did not get the desired result. Here is the output from your script
>>>
>>>    A   B   C    Y D E
>>> 1 A12 <https://maps.google.com/?q=1+A12&entry=gmail&source=g> B03 C04
>>> 0.70 0 0
>>> 2 A23 B05 C06 0.05 0 0
>>> 3 A14 <https://maps.google.com/?q=3+A14&entry=gmail&source=g> B06 C07
>>> 1.20 0 0
>>> 4 A25 A23 A12 3.51 1 1
>>> 5 A16 A25 A14 2,16 4
>>> <https://maps.google.com/?q=A14+2,16+4&entry=gmail&source=g> 4
>>>
>>>
>>> On Wed, Dec 13, 2017 at 4:36 PM, Rui Barradas <ruipbarradas at sapo.pt>
>>> wrote:
>>>
>>> > Hello,
>>> >
>>> > Here is one way.
>>> >
>>> > tdat$D <- ifelse(tdat$B %in% tdat$A, tdat$A[tdat$B], 0)
>>> > tdat$E <- ifelse(tdat$B %in% tdat$A, tdat$A[tdat$C], 0)
>>> >
>>> >
>>> > Hope this helps,
>>> >
>>> > Rui Barradas
>>> >
>>> >
>>> > On 12/13/2017 9:36 PM, Val wrote:
>>> >
>>> >> Hi all,
>>> >>
>>> >> I have a data frame
>>> >> tdat <- read.table(textConnection("A B C Y
>>> >> A12 B03 C04 0.70
>>> >> A23 B05 C06 0.05
>>> >> A14 B06 C07 1.20
>>> >> A25 A23 A12 3.51
>>> >> A16 A25 A14 2,16
>>> <https://maps.google.com/?q=A14+2,16&entry=gmail&source=g>"),header =
>>> TRUE)
>>> >>
>>> >> I want match tdat$B with tdat$A and populate the  column   values of
>>> >> tdat$A
>>> >> ( col A and Col B) in the newly created columns (col D and col  E).
>>> >> please
>>> >> find my attempt and the desired output below
>>> >>
>>> >> Desired output
>>> >> A B C Y  D E
>>> >> A12 B03 C04 0.70  0  0
>>> >> A23 B05 C06 0.05  0  0
>>> >> A14 B06 C07 1.20  0  0
>>> >> A25 A23 A12 3.51 B05 C06
>>> >> A16 A25 A14 2,16 A23 A12
>>> <https://maps.google.com/?q=2,16+A23+A12&entry=gmail&source=g>
>>> >>
>>> >> my attempt,
>>> >>
>>> >> tdat$D <- 0
>>> >> tdat$E <- 0
>>> >>
>>> >> if(tdat$B %in% tdat$A)
>>> >>    {
>>> >>    tdat$D <- tdat$A[tdat$B]
>>> >>    tdat$E <- tdat$A[tdat$C]
>>> >> }
>>> >>   but did not work.
>>> >>
>>> >> Thank you in advance
>>> >>
>>> >>         [[alternative HTML version deleted]]
>>> >>
>>> >> ______________________________________________
>>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >> PLEASE do read the posting guide http://www.R-project.org/posti
>>> >> ng-guide.html
>>> >> and provide commented, minimal, self-contained, reproducible code.
>>> >>
>>> >>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>> ng-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From rmh at temple.edu  Thu Dec 14 07:12:07 2017
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu, 14 Dec 2017 01:12:07 -0500
Subject: [R] change in behavior of c.trellis
Message-ID: <CAGx1TMB0GtTiqSiNs9HSWiKE4fqbA77zspwb9yo35p-FVZXZYQ@mail.gmail.com>

> library(latticeExtra)
Loading required package: lattice
Loading required package: RColorBrewer
> t11 <- xyplot(1 ~ 1)
> t11
> c(t11, t11)
Warning message:
In formals(fun) : argument is not a function
> version
               _
platform       x86_64-w64-mingw32
arch           x86_64
os             mingw32
system         x86_64, mingw32
status         Patched
major          3
minor          4.3
year           2017
month          12
day            12
svn rev        73903
language       R
version.string R version 3.4.3 Patched (2017-12-12 r73903)
nickname       Kite-Eating Tree
>


the c.trellis seems to work correctly except for the new warning.

There was no warning in 3.3.3.
I first noticed it in 3.4.1

the c.trellis function itself is not changed between 3.3.3 and 3.4.3 Patched.


From haenlein at escpeurope.eu  Thu Dec 14 08:48:29 2017
From: haenlein at escpeurope.eu (Michael Haenlein)
Date: Thu, 14 Dec 2017 08:48:29 +0100
Subject: [R] Aggregation across two variables in data.table
Message-ID: <CAOyz9G7xgaMLdfEKngsHpcwPotL+WS441w5ymnyppA_Dawyz1Q@mail.gmail.com>

Dear all,

I have a data.frame that includes a series of demographic variables for a
set of respondents plus a dependent variable (Theta). For example:

   Age                                Education       Marital Familysize
Income                Housing    Theta
1:  50                         Associate degree      Divorced          4
 70K+    Owned with mortgage 9.147777
2:  65                          Bachelor degree       Married          1
10-15K Owned without mortgage 7.345036
3:  33                          Bachelor degree       Married          2
30-40K    Owned with mortgage 7.974937
4:  69                          Bachelor degree Never married          1
 70K+    Owned with mortgage 7.733053
5:  54 Some college, less than college graduate Never married          3
30-40K                 Rented 7.648642
6:  35                         Associate degree     Separated          2
10-15K                 Rented 7.496411

My objective is to calculate the average of Theta across all pairs of two
demographics.

For 1 demographic this is straightforward:

Demo_names <- c("Age", "Education", "Marital", "Familysize", "Income",
"Housing")
means1 <- as.list(rep(0, length(Demo_names)))
for (i in 1:length(Demo_names)) {
Demo_tmp <- Demo_names[i]
means1[[i]] <- data_tmp[,list(mean(Theta)),by=Demo_tmp]}

Is there an easy way to extent this logic to more than 1 variable? I know
how to do this manually, e.g.,
data_tmp[,list(mean(Theta)),by=list(Marital, Education)]

But I don't know how to integrate this into a loop.

Thanks,

Michael

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Thu Dec 14 09:08:23 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 14 Dec 2017 08:08:23 +0000
Subject: [R] Aggregation across two variables in data.table
In-Reply-To: <CAOyz9G7xgaMLdfEKngsHpcwPotL+WS441w5ymnyppA_Dawyz1Q@mail.gmail.com>
References: <CAOyz9G7xgaMLdfEKngsHpcwPotL+WS441w5ymnyppA_Dawyz1Q@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF880155FCB71E@SRVEXCHCM301.precheza.cz>

Hi

Are you aware of function aggregate?

result <- with(data_tmp, aggregate(Theta, list(Marital, Education), mean))

should do the trick.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Michael
> Haenlein
> Sent: Thursday, December 14, 2017 8:48 AM
> To: r-help at r-project.org
> Cc: Michael Haenlein <haenlein at escpeurope.eu>
> Subject: [R] Aggregation across two variables in data.table
>
> Dear all,
>
> I have a data.frame that includes a series of demographic variables for a
> set of respondents plus a dependent variable (Theta). For example:
>
>    Age                                Education       Marital Familysize
> Income                Housing    Theta
> 1:  50                         Associate degree      Divorced          4
>  70K+    Owned with mortgage 9.147777
> 2:  65                          Bachelor degree       Married          1
> 10-15K Owned without mortgage 7.345036
> 3:  33                          Bachelor degree       Married          2
> 30-40K    Owned with mortgage 7.974937
> 4:  69                          Bachelor degree Never married          1
>  70K+    Owned with mortgage 7.733053
> 5:  54 Some college, less than college graduate Never married          3
> 30-40K                 Rented 7.648642
> 6:  35                         Associate degree     Separated          2
> 10-15K                 Rented 7.496411
>
> My objective is to calculate the average of Theta across all pairs of two
> demographics.
>
> For 1 demographic this is straightforward:
>
> Demo_names <- c("Age", "Education", "Marital", "Familysize", "Income",
> "Housing")
> means1 <- as.list(rep(0, length(Demo_names)))
> for (i in 1:length(Demo_names)) {
> Demo_tmp <- Demo_names[i]
> means1[[i]] <- data_tmp[,list(mean(Theta)),by=Demo_tmp]}
>
> Is there an easy way to extent this logic to more than 1 variable? I know
> how to do this manually, e.g.,
> data_tmp[,list(mean(Theta)),by=list(Marital, Education)]
>
> But I don't know how to integrate this into a loop.
>
> Thanks,
>
> Michael
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From p_connolly at slingshot.co.nz  Thu Dec 14 09:28:22 2017
From: p_connolly at slingshot.co.nz (Patrick Connolly)
Date: Thu, 14 Dec 2017 21:28:22 +1300
Subject: [R] Distributions for gbm models
Message-ID: <20171214082822.GB4428@slingshot.co.nz>

On page 409 of "Applied Predictive Modeling" by Max Kuhn, it states
that the gbm function can accomodate only two class problems when
referring to the distribution parameter.  

>From gbm help re: the distribution parameter:


          Currently available options are "gaussian" (squared error),
          "laplace" (absolute loss), "tdist" (t-distribution loss),
          "bernoulli" (logistic regression for 0-1 outcomes),
          "huberized" (huberized hinge loss for 0-1 outcomes),
          "multinomial" (classification when there are more than 2
          classes), "adaboost" (the AdaBoost exponential loss for 0-1
          outcomes), "poisson" (count outcomes), "coxph" (right
          censored observations), "quantile", or "pairwise" (ranking
          measure using the LambdaMart algorithm).


I would have thought that huberized and multinomial would also be
possible.  Is that not so?  In any case, how would anything different
from bernoulli (the default) be specified when using the caret train
function since distribution appears not to be among the list of
parameters that caret recognises?

> getModelInfo("gbm")[["gbm"]]$parameters
          parameter   class                   label
1           n.trees numeric   # Boosting Iterations
2 interaction.depth numeric          Max Tree Depth
3         shrinkage numeric               Shrinkage
4    n.minobsinnode numeric Min. Terminal Node Size

Is that a limitation of the caret package?  Or is there something I'm
not getting?

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From thierry.onkelinx at inbo.be  Thu Dec 14 10:13:03 2017
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Thu, 14 Dec 2017 10:13:03 +0100
Subject: [R] overlay two histograms ggplot
In-Reply-To: <123746066.6750128.1513167241106@mail.yahoo.com>
References: <123746066.6750128.1513167241106.ref@mail.yahoo.com>
 <123746066.6750128.1513167241106@mail.yahoo.com>
Message-ID: <CAJuCY5wAOM2PvYAKSYjmRE0uvDeHc8ejsGz_VP_FPGnfBh=M0w@mail.gmail.com>

Dear Elahe,

Something like

joint <- rbind(
  data.frame(x = gg$Az, veg = gg$veg, type = "Alz"),
  data.frame(x = tt$Cont, veg = gg$veg, type = "Cont")
)
ggplot(joint, aes(x = x, fill = veg, colour = type)) + geom_plot(alpha = 0.2)

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
AND FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Kliniekstraat 25, B-1070 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no
more than asking him to perform a post-mortem examination: he may be
able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does
not ensure that a reasonable answer can be extracted from a given body
of data. ~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////


Van 14 tot en met 19 december 2017 verhuizen we uit onze vestiging in
Brussel naar het Herman Teirlinckgebouw op de site Thurn & Taxis.
Vanaf dan ben je welkom op het nieuwe adres: Havenlaan 88 bus 73, 1000 Brussel.

///////////////////////////////////////////////////////////////////////////////////////////



2017-12-13 13:14 GMT+01:00 Elahe chalabi via R-help <r-help at r-project.org>:
> Hi all,
>
> How can I overlay these two histograms?
>
> ggplot(gg, aes(gg$Alz, fill = gg$veg)) + geom_histogram(alpha = 0.2)
> ggplot(tt, aes(tt$Cont, fill = tt$veg)) + geom_histogram(alpha = 0.2)
>
> thanks for any help!
> Elahe
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Thu Dec 14 10:54:10 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 14 Dec 2017 04:54:10 -0500
Subject: [R] change in behavior of c.trellis
In-Reply-To: <CAGx1TMB0GtTiqSiNs9HSWiKE4fqbA77zspwb9yo35p-FVZXZYQ@mail.gmail.com>
References: <CAGx1TMB0GtTiqSiNs9HSWiKE4fqbA77zspwb9yo35p-FVZXZYQ@mail.gmail.com>
Message-ID: <6c9b3b79-9834-a125-877d-7e8638db0a12@gmail.com>

This looks like a bug in latticeExtra:  in the c.trellis function there 
are these lines:

     ## some prepanel functions require a 'subscripts' argument in each 
'panel.args'
     if ("subscripts" %in% c(names(formals(obj1$prepanel.default)),
                             names(formals(obj1$prepanel)))) {

But obj1$prepanel is NULL in your test case.  NULL is not a legal 
argument to formals(), and it now warns about it.

Duncan Murdoch

On 14/12/2017 1:12 AM, Richard M. Heiberger wrote:
>> library(latticeExtra)
> Loading required package: lattice
> Loading required package: RColorBrewer
>> t11 <- xyplot(1 ~ 1)
>> t11
>> c(t11, t11)
> Warning message:
> In formals(fun) : argument is not a function
>> version
>                 _
> platform       x86_64-w64-mingw32
> arch           x86_64
> os             mingw32
> system         x86_64, mingw32
> status         Patched
> major          3
> minor          4.3
> year           2017
> month          12
> day            12
> svn rev        73903
> language       R
> version.string R version 3.4.3 Patched (2017-12-12 r73903)
> nickname       Kite-Eating Tree
>>
> 
> 
> the c.trellis seems to work correctly except for the new warning.
> 
> There was no warning in 3.3.3.
> I first noticed it in 3.4.1
> 
> the c.trellis function itself is not changed between 3.3.3 and 3.4.3 Patched.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ghiaco at gmail.com  Thu Dec 14 13:23:43 2017
From: ghiaco at gmail.com (Sima Usvyatsov)
Date: Thu, 14 Dec 2017 08:23:43 -0400
Subject: [R] multiple instances of predictor variable per model
Message-ID: <CAFGTTqRrSr5VoN0qPow3cdb1OuNyc3Km577K5ot0ObdKv+WDvw@mail.gmail.com>

I?m running a model on animal behavior in response to shipping. In most
cases, there is only one ship in the study area at one time. Ship length,
distance from the animals, speed, angle from animals, and ship direction
(as east/west bound) are among shipping-related covariates (with multiple
interactions).

The tricky part is that sometimes there are 2 ships in the area. I could
add all the same covariates, but doubled-up for the second ship. However,
this really hurts convergence. And - conceptually - why would ship length
affect the animals differently between ship 1 and ship 2? I would think
that animals would react similarly to both ships (and the effect would just
add up), so I don?t want the model to estimate two covariates that I think
are the same. And if I had 5 ships instead of 2, those dfs would really
rack up.

Note that I can?t just double the vessel values, since their speeds,
directions, lengths, etc all differ.

Here's a little mock data set for 3 surveys - 2 have a single ship, and 1
has two ships. Note that each survey is only done once, so if there are 2
ships (or more), the number of animals (and all other survey-related info)
is just copied over on another line

df <- data.frame(Survey = c(1, 1, 2, 3), NAnimals = c(10, 10, 1, 0),
Vessel = c("A", "B", "C", "D"), VesselLength = c(20, 50, 40, 70),
VesselSpeed = c(10, 5, 4, 5), Direction = c("West", "East", "West", "West"))

Disclaimer: this is a crosspost from here (
https://stats.stackexchange.com/questions/318667/multiple-instances-of-predictor-variables
).

Many thanks.

	[[alternative HTML version deleted]]


From nilesh.dighe at monsanto.com  Thu Dec 14 14:26:44 2017
From: nilesh.dighe at monsanto.com (DIGHE, NILESH [AG/2362])
Date: Thu, 14 Dec 2017 13:26:44 +0000
Subject: [R] help with recursive function
Message-ID: <CY1PR0101MB1018F79F34CFE3142A8592ADE20A0@CY1PR0101MB1018.prod.exchangelabs.com>

Hi, I need some help with running a recursive function. I like to run funlp2 recursively.
When I try to run recursive function in another function named "calclp" I get this "Error: any(!dat2$norm_sd) >= 1 is not TRUE".

I have never built a recursive function before so having trouble executing it in this case.  I would appreciate any help or guidance to resolve this issue. Please see my data and the three functions that I am using below.
Please note that calclp is the function I am running and the other two functions are within this calclp function.

# code:
Test<- calclp(dataset = dat)

# calclp function

calclp<- function (dataset)

{

    dat1 <- funlp1(dataset)

    recursive_funlp <- function(dataset = dat1, func = funlp2) {

        dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%

            mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%

            spread(key = field_rep, value = lp) %>% mutate_at(.vars = grep("_",

            names(.)), funs(norm = round(scale(.), 3)))

        dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],

            1, sd, na.rm = TRUE), 3)

        dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],

            1, function(x) {

                max(abs(x), na.rm = TRUE)

            }), 3)

        stopifnot(any(!dat2$norm_sd) >= 1)

        if (any(!dat2$norm_sd) >= 1) {

            df1 <- dat1

            return(df1)

        }

        else {

            df2 <- recursive_funlp()

            return(df2)

        }

    }

    df3 <- recursive_funlp(dataset = dat1, func = funlp2)

    df3

}


# funlp1 function

funlp1<- function (dataset)

{

    dat2 <- dataset %>% select(field, set, ent_num, rep_num,

        lp) %>% unite(uniqueid, set, ent_num, sep = ".") %>%

        unite(field_rep, field, rep_num) %>% mutate(field_rep = paste(field_rep,

        "lp", sep = ".")) %>% spread(key = field_rep, value = lp) %>%

        mutate_at(.vars = grep("_", names(.)), funs(norm = round(scale(.),

            3)))

    dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],

        1, sd, na.rm = TRUE), 3)

    dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],

        1, function(x) {

            max(abs(x), na.rm = TRUE)

        }), 3)

    data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,

        -norm_sd) %>% separate(key, c("field_rep", "treatment"),

        "\\.") %>% spread(treatment, value) %>% mutate(outlier = NA)

    df_clean <- with(data1, data1[norm_sd < 1, ])

    datD <- with(data1, data1[norm_sd >= 1, ])

    s <- split(datD, datD$uniqueid)

    sdf <- lapply(s, function(x) {

        data.frame(x, x$outlier <- ifelse(is.na(x$lp_norm), NA,

            ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),

            x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))

        x

    })

    sdf2 <- bind_rows(sdf)

    all_dat <- bind_rows(df_clean, sdf2)

    all_dat

}


# funlp2 function

funlp2<-function (dataset)

{

    data1 <- dataset

    df_clean <- with(data1, data1[norm_sd < 1, ])

    datD <- with(data1, data1[norm_sd >= 1, ])

    s <- split(datD, datD$uniqueid)

    sdf <- lapply(s, function(x) {

        data.frame(x, x$outlier <- ifelse(is.na(x$lp_norm), NA,

            ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),

            x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))

        x

    })

    sdf2 <- bind_rows(sdf)

    all_dat <- bind_rows(df_clean, sdf2)

    all_dat

}


# dataset
dput(dat)
structure(list(field = c("LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01"), set = c("seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta"), ent_num = c(23L, 14L, 43L, 30L, 44L, 60L, 17L,
34L, 41L, 40L, 9L, 36L, 38L, 19L, 61L, 51L, 45L, 42L, 3L, 39L,
21L, 11L, 12L, 7L, 35L, 5L, 70L, 47L, 28L, 16L, 72L, 13L, 49L,
67L, 56L, 32L, 27L, 46L, 24L, 63L, 15L, 66L, 26L, 29L, 48L, 1L,
54L, 37L, 2L, 50L, 52L, 31L, 33L, 25L, 6L, 69L, 53L, 10L, 18L,
55L, 59L, 4L, 58L, 22L, 20L, 64L, 71L, 57L, 11L, 17L, 43L, 13L,
1L, 16L, 34L, 47L, 52L, 72L, 59L, 22L, 54L, 18L, 25L, 61L, 56L,
41L, 27L, 14L, 49L, 19L, 29L, 31L, 64L, 6L, 53L, 35L, 37L, 67L,
39L, 51L, 40L, 15L, 69L, 60L, 38L, 4L, 23L, 3L, 48L, 32L, 42L,
24L, 28L, 33L, 57L, 8L, 21L, 46L, 7L, 30L, 45L, 2L, 63L, 36L,
68L, 20L, 66L, 70L, 58L, 5L, 10L, 12L, 62L, 50L, 71L, 9L, 55L,
26L, 44L, 65L, 14L, 63L, 46L, 58L, 62L, 19L, 59L, 2L, 5L, 6L,
40L, 21L, 44L, 37L, 55L, 35L, 71L, 56L, 10L, 36L, 53L, 25L, 61L,
12L, 26L, 23L, 4L, 13L, 28L, 38L, 57L, 54L, 72L, 48L, 66L, 9L,
70L, 15L, 39L, 60L, 17L, 34L, 51L, 67L, 42L, 49L, 31L, 30L, 3L,
18L, 65L, 32L, 27L, 52L, 22L, 11L, 47L, 64L, 8L, 43L, 41L, 16L,
20L, 33L, 7L, 50L, 68L, 24L, 1L, 69L, 45L, 29L, 37L, 30L, 55L,
54L, 43L, 32L, 21L, 27L, 33L, 40L, 67L, 57L, 68L, 31L, 17L, 13L,
6L, 62L, 19L, 22L, 3L, 10L, 44L, 34L, 69L, 70L, 4L, 1L, 25L,
11L, 51L, 5L, 63L, 71L, 12L, 38L, 58L, 39L, 49L, 59L, 56L, 65L,
2L, 64L, 8L, 35L, 46L, 45L, 29L, 53L, 36L, 42L, 23L, 18L, 50L,
26L, 14L, 48L, 66L, 20L, 24L, 7L, 15L, 53L, 22L, 39L, 20L, 60L,
59L, 43L, 19L, 41L, 6L, 62L, 1L, 55L, 34L, 50L, 38L, 40L, 44L,
4L, 46L, 29L, 65L, 57L, 48L, 33L, 69L, 14L, 35L, 67L, 72L, 54L,
3L, 49L, 2L, 12L, 18L, 30L, 10L, 70L, 31L, 15L, 63L, 71L, 21L,
45L, 28L, 56L, 27L, 64L, 61L, 51L, 5L, 24L, 68L, 25L, 66L, 16L,
36L, 58L, 37L, 52L, 26L, 9L, 42L, 7L, 11L, 8L, 32L, 23L, 13L,
47L, 17L, 61L, 72L, 47L, 60L, 16L, 9L, 28L, 52L, 41L, 1L, 61L,
6L, 23L, 58L, 63L, 25L, 28L, 30L, 36L, 62L, 9L, 32L, 19L, 31L,
56L, 45L, 2L, 22L, 27L, 40L, 14L, 11L, 50L, 13L, 70L, 20L, 64L,
39L, 26L, 21L, 43L, 29L, 35L, 54L, 52L, 37L, 17L, 16L, 72L, 48L,
12L, 18L, 44L, 42L, 49L, 68L, 5L, 55L, 69L, 51L, 66L, 59L, 53L,
15L, 71L, 41L, 57L, 4L, 60L, 8L, 7L, 33L, 34L, 24L, 10L, 67L,
47L, 38L, 3L, 65L, 46L, 20L, 34L, 71L, 1L, 33L, 57L, 13L, 21L,
66L, 29L, 3L, 61L, 69L, 24L, 62L, 39L, 49L, 47L, 31L, 53L, 52L,
43L, 17L, 7L, 8L, 12L, 60L, 63L, 50L, 2L, 51L, 46L, 10L, 23L,
48L, 11L, 26L, 40L, 70L, 42L, 59L, 15L, 56L, 58L, 27L, 6L, 35L,
4L, 37L, 5L, 65L, 44L, 28L, 14L, 32L, 36L, 45L, 9L, 18L, 55L,
68L, 30L, 54L, 41L, 25L, 22L, 38L, 16L, 67L, 64L, 19L, 72L, 68L,
28L, 33L, 15L, 51L, 4L, 47L, 36L, 8L, 57L, 48L, 1L, 52L, 39L,
32L, 50L, 13L, 30L, 63L, 2L, 9L, 62L, 22L, 6L, 61L, 16L, 53L,
38L, 37L, 20L, 69L, 44L, 56L, 29L, 26L, 14L, 17L, 46L, 66L, 58L,
42L, 60L, 19L, 45L, 3L, 59L, 70L, 31L, 24L, 55L, 40L, 43L, 25L,
65L, 12L, 67L, 21L, 7L, 27L, 49L, 72L, 54L, 41L, 23L, 34L, 5L,
64L, 35L, 18L, 71L, 11L, 24L, 19L, 38L, 14L, 4L, 56L, 5L, 54L,
34L, 64L, 55L, 33L, 69L, 71L, 52L, 61L, 48L, 23L, 43L, 41L, 20L,
39L, 11L, 63L, 36L, 22L, 9L, 25L, 27L, 51L, 53L, 37L, 57L, 13L,
18L, 64L, 22L, 53L, 16L, 5L, 28L, 60L, 31L, 11L, 29L, 45L, 59L,
72L, 49L, 67L, 13L, 20L, 3L, 42L, 44L, 69L, 33L, 38L, 15L, 70L,
35L, 48L, 26L, 56L, 19L, 39L, 43L, 40L, 14L, 2L, 68L, 51L, 12L,
47L, 10L, 55L, 23L, 4L, 71L, 41L, 50L, 7L, 24L, 61L, 27L, 54L,
46L, 58L, 37L, 66L, 57L, 1L, 36L, 32L, 18L, 62L, 9L, 30L, 21L,
6L, 52L, 8L, 65L, 17L, 25L, 63L, 34L, 65L, 22L, 56L, 9L, 7L,
11L, 31L, 4L, 63L, 29L, 61L, 54L, 12L, 62L, 59L, 5L, 23L, 53L,
36L, 24L, 35L, 66L, 49L, 72L, 18L, 70L, 32L, 43L, 20L, 45L, 34L,
46L, 28L, 6L, 44L, 71L, 39L, 13L, 27L, 1L, 58L, 30L, 68L, 17L,
33L, 26L, 57L, 15L, 21L, 52L, 48L, 42L, 16L, 40L, 38L, 8L, 69L,
2L, 51L, 67L, 55L, 64L, 47L, 60L, 19L, 41L, 50L, 3L, 14L, 25L,
10L, 37L, 6L, 15L, 45L, 49L, 8L, 17L, 50L, 16L, 58L, 72L, 26L,
60L, 7L, 32L, 1L, 46L, 66L, 68L, 62L, 47L, 35L, 70L, 10L, 31L,
65L, 2L, 3L, 21L, 12L, 30L, 40L, 28L, 59L, 42L, 67L, 44L, 29L
), rep_num = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L), lp = c(NA, 41.64, 38.8, 44.45, 40.54,
38.54, 41.94, 39.6, 37.39, 40.95, 38.45, 43.47, 41.66, 40.91,
42.68, 43.12, 38.22, 40.95, 46.24, 42.95, 38.95, 39.88, 40.57,
40.13, 38.57, 45.45, 40.78, 43.52, 39.75, 39.93, 40.35, 37.6,
37.7, 43.05, 43.32, 41.31, 39.03, 42.5, 43.18, 41.32, 39.58,
40.62, 39.64, 39.85, 38.75, 40.18, 41.44, 40.5, 40.87, 40.75,
38.37, 40.26, 35.11, 40.89, 41.67, 38.87, 37.32, 35.85, 38.25,
42.21, 43.15, 38.69, 38.8, 38.77, 37.98, 39.8, 33.37, 40.09,
42.87, 44.07, 43.78, 42.47, 42.8, 41.1, 41.17, 44.07, 44.24,
43.16, 46.12, 42.64, 43.92, 42.16, 43.69, 42.89, 42.63, 43.58,
44.4, 43.17, 43.3, 42.5, 42.6, 44.05, 44.63, 43.09, 45.17, 45.17,
42.78, 42.38, 45.76, 42.16, 44.22, 43.67, 40.45, 41.95, 42.49,
41.98, 45.15, 46.01, 38.51, 43.08, 45.19, 44.53, 43.14, 39.93,
46.84, 43.59, 41.68, 43.7, 44.63, 44.02, 42.07, 43.88, 43.2,
46.2, 40.84, 39.14, 43.89, 42.58, 41.53, 45.32, 39.56, 43.77,
45.45, 45.16, 43.4, 40.08, 42.27, 43.74, 43.77, 41.31, 41.38,
45.01, 45.76, 40.4, 48.39, 46.27, 15.44, 44.76, 47.45, 44.87,
46.8, 41.83, 43.03, 43.96, 42.51, 45.06, 45.55, 44.9, 42.47,
44.9, 45, 44.77, 43.79, 44.26, 44.57, 44.4, 43.42, 42.31, 47.18,
43.83, 45.72, 44.83, 44.96, 40.28, 42.85, 41.23, 45.23, 47.09,
43.61, 42.69, 46.27, 45.16, 44.14, 43.34, 45.97, 43.81, 43.01,
39.82, 45.91, 45.97, 43.61, 45.12, 46.37, 42.67, 42.47, 45.86,
44.19, 44.46, 42.64, 43.95, 44.93, 40.33, 42.75, 39.92, 44.17,
44.49, 41.51, 42.43, 44.14, 40.5, 41.29, 44.89, 37.98, 39.02,
39.62, 42.13, 39.03, 44.16, 39.15, 41.49, 42.63, 40.11, 39.97,
42.85, 35.98, 39.45, 40.99, 42.44, 42.11, 37.36, 40.63, 40.69,
43.57, 39.04, 39.3, 42.19, 36.88, 40.39, 37.78, 38.6, 40.2, 40.98,
36.58, 43.59, 42.49, 39.96, 39.84, 40.43, 38.94, 42.72, 39.43,
42.13, 40.36, 40.58, 40.01, 42.17, 42.17, 41.35, 43.27, 40.15,
39.76, 40.94, 40.87, 42.32, 41.81, 41.97, 43.72, 41.32, 40.83,
37.64, 41.03, 38.98, 40.61, 41.17, 41.96, 40.07, 38.48, 42.85,
38.68, 39.09, 42.16, 38.14, 37.99, 41.06, 37.4, 41.88, 39.35,
39.73, 38.38, 41.34, 40.67, 40.89, 39.28, 37.59, 39.3, 39.72,
40.79, 39.42, 34.5, 37.61, 37.76, 40.24, 41.17, 41.24, 42.14,
42.53, 39.71, 39.44, 40.19, 42.51, 40.15, 36.26, 37.48, 40.43,
37.5, 42.11, 41.44, 40.29, 39.75, 37.58, 41.25, 40.39, 41.63,
42.18, 43.71, 38.69, 43.47, 41.98, 35.51, 42.22, 38.51, 40.17,
39.4, 39.54, 41.7, 40.93, 40.01, 35.97, 41.44, 41.89, 40.9, 39.95,
40.69, 43.18, 37.03, 39.91, 36.75, 42.75, 41.61, 42.6, 38.55,
42.84, 38.41, 42.55, 41.44, 41.03, 40, 41.35, 42.18, 42.66, 40.06,
42.48, 41.33, 41.92, 40.88, 40.99, 42.78, 38.26, 40.81, 39.95,
37.89, 40.81, 38.33, 39.33, 39.67, 40.12, 41.75, 39.81, 41.92,
44.59, 37.8, 40.54, 40.49, 41.82, 42.13, 39.93, 39.94, 42.54,
41.74, 43.06, 41.72, 41.27, 39.42, 42.07, 40.06, 42.1, 38.91,
39.28, 42.94, 39.94, 41.86, 38.56, 38.15, 41.47, 42.34, 38.14,
40.19, 40.2, 43.01, 42.35, 40.89, 41.44, 42.89, 44.49, 39.1,
38.42, 37.44, 43.28, 37.96, 40.56, 41.53, 41.59, 41.45, 42.55,
40.6, 44.04, 39.31, 41.08, 37.14, 40.03, 41.49, 40.82, 38.47,
43.43, 38.82, 40.4, 41.09, 42.26, 41.85, 41.13, 37.27, 41.97,
45.24, 43.35, 40.53, 43.14, 39.71, 42.77, 42.2, 42.13, 41.91,
42.59, 41.99, 42.11, 40.43, 40.02, 43.53, 43.01, 39.55, 43.2,
39.95, 42.51, 42, 42.26, 42.39, 42.64, 39.9, 41.89, 43.05, 41.02,
41.95, 39.92, 43.42, 44.01, 42.6, 40.84, 42.86, 44.31, 42.24,
41.25, 42.38, 44.99, 41.24, 43.97, 41.12, 37.88, 41.53, 41.56,
39.18, 40.83, 42.96, 41.92, 43.86, 42.48, 42.65, 43.3, 41.99,
42.51, 40.65, 42.77, 34.71, 40.97, 38.15, 39.76, 36.74, 37.95,
39.17, 38.22, 39.31, 43.57, 37.21, 39.35, 42.37, 42.01, 42.39,
43.21, 36.91, 36.69, 41.07, 41.91, 34.63, 40.61, 36.23, 38.12,
40.76, 39.14, 41.81, 39.14, 41.04, 37.32, 40.83, 40.81, 38.29,
39.61, 40.96, 40.71, 40.47, 38.64, 39.76, 38.19, 39.05, 38.04,
41.14, 38.35, 42.3, 34.44, 40.93, 39.3, 41.44, 38.3, 42.74, 36.66,
40.02, 36.62, 40.48, 41.72, 41.23, 41.81, 42.07, 40.22, 37.83,
36.54, 37.77, 40.49, 38.65, 43.2, 43.32, 40.67, 41.95, 36.11,
39.28, 42.38, 40.35, 40.3, 43.48, 40.55, 40.54, 44.03, 41.94,
37.97, 41.98, 41.53, 38.19, 38.66, 41.18, 41.95, 42.53, 38.7,
44.55, 42.39, 41.55, 38.46, 42.27, 42.19, 41.95, 41.81, 39.81,
38.12, 42.94, 42.99, 38.85, 41.26, 43.13, 44.21, 38.54, 44.02,
43.46, 41.64, 42.06, 42.11, 42.34, 41.86, 37.91, 40.89, 40.5,
39.54, 37.87, 40.86, 41.36, 41.77, 42.03, 39.15, 40.04, 44.11,
41.34, 42.97, 38.42, 37.28, 41.04, 41.48, 38.82, 41.94, 37.95,
40.9, 40.39, 40.31, 43.19, 41.22, 41.49, 41.25, 40.07, 36.7,
39.97, 39.99, 41.7, 37.09, 42.58, 43.01, 37.7, 41.81, 39.99,
42.95, 43.19, 42.69, 41.5, 40.64, 43.24, 41.14, 41.21, 41.29,
41.43, 44.21, 38.52, 42.54, 40.54, 42.49, 43.2, 38.12, 40.08,
39.02, 41.45, 42.33, 41.11, 38.93, 41.63, 44.22, 41.41, 39.08,
40.9, 41.1, 43.88, 40.96, 46.75, 47.54, 40.35, 41.97, 44.94,
44.91, 44.66, 44.5, 44.4, 46.4, 47.97, 46.05, 45.57, 42.83, 41.48,
47.48, 45.43, 41.98, 43.14, 45.6, 44.78, 45.45, 45.69, 44.82,
44.24, 41.14, 43.14, 46.61, 43.92, 43.56, 43.68, 45.37, 45.15,
40.76, 43.78, 44.67, 41.36, 41.4, 40.97, 41.87, 39.83, 43.8,
48.36, 44.28, 43.29, 44.69, 43.26, 43.35, 44.34, 45.08, 42.26,
39.7, 42.4, 44.03, 43.22, 42.71, 45.89, 44.89, 44.81, 42.59,
40.85, 43.82, 44.85, 47.47, 43.64, 42.65, 45.67, 43.24, 42.33,
40.61, 38.3, 39.84, 41.08, 42.33, 44.44, 40.85, 39.58, 42.55,
41.75, 39.44, 41.79, 39.31, 41.34, 42.76, 40.79, 37.31, 42.85,
42.88, 42.01, 44.63, 38.85, 41.13, 40.43, 41.34, 43.14, 40.58,
42.21, 38.94, 44.88, 42.33, 42.61, 41.88, 41.13, 41.83, 42.8)), .Names = c("field",
"set", "ent_num", "rep_num", "lp"), class = "data.frame", row.names = c(NA,
-787L))

# session info

R version 3.4.1 (2017-06-30)

Platform: i386-w64-mingw32/i386 (32-bit)

Running under: Windows 7 x64 (build 7601) Service Pack 1



Matrix products: default



locale:

[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252

[3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C

[5] LC_TIME=English_United States.1252



attached base packages:

[1] stats     graphics  grDevices utils     datasets  methods   base



other attached packages:

[1] bindrcpp_0.2 tidyr_0.6.3  dplyr_0.7.4



loaded via a namespace (and not attached):

 [1] compiler_3.4.1   magrittr_1.5     assertthat_0.2.0 R6_2.2.2         tools_3.4.1

 [6] glue_1.1.1       tibble_1.3.3     Rcpp_0.12.11     stringi_1.1.5    pkgconfig_2.0.1

[11] rlang_0.1.2      bindr_0.1

This email and any attachments were sent from a Monsanto email account and may contain confidential and/or privileged information. If you are not the intended recipient, please contact the sender and delete this email and any attachments immediately. Any unauthorized use, including disclosing, printing, storing, copying or distributing this email, is prohibited. All emails and attachments sent to or from Monsanto email accounts may be subject to monitoring, reading, and archiving by Monsanto, including its affiliates and subsidiaries, as permitted by applicable law. Thank you.

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Thu Dec 14 14:43:53 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Thu, 14 Dec 2017 15:43:53 +0200
Subject: [R] help with recursive function
In-Reply-To: <CY1PR0101MB1018F79F34CFE3142A8592ADE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
References: <CY1PR0101MB1018F79F34CFE3142A8592ADE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
Message-ID: <CAGgJW772Nk+Z3XamiGF-2r6Ta9Cud1VOhUvRr4p4k0epTX4xtA@mail.gmail.com>

You seem to have a typo at this expression (and some others like it)

Namely, you write

any(!dat2$norm_sd) >= 1

when you possibly meant to write

!( any(dat2$norm_sd) >= 1 )

i.e. I think your ! seems to be in the wrong place.

HTH,
Eric


On Thu, Dec 14, 2017 at 3:26 PM, DIGHE, NILESH [AG/2362] <
nilesh.dighe at monsanto.com> wrote:

> Hi, I need some help with running a recursive function. I like to run
> funlp2 recursively.
> When I try to run recursive function in another function named "calclp" I
> get this "Error: any(!dat2$norm_sd) >= 1 is not TRUE".
>
> I have never built a recursive function before so having trouble executing
> it in this case.  I would appreciate any help or guidance to resolve this
> issue. Please see my data and the three functions that I am using below.
> Please note that calclp is the function I am running and the other two
> functions are within this calclp function.
>
> # code:
> Test<- calclp(dataset = dat)
>
> # calclp function
>
> calclp<- function (dataset)
>
> {
>
>     dat1 <- funlp1(dataset)
>
>     recursive_funlp <- function(dataset = dat1, func = funlp2) {
>
>         dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
>
>             mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
>
>             spread(key = field_rep, value = lp) %>% mutate_at(.vars =
> grep("_",
>
>             names(.)), funs(norm = round(scale(.), 3)))
>
>         dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>             1, sd, na.rm = TRUE), 3)
>
>         dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>             1, function(x) {
>
>                 max(abs(x), na.rm = TRUE)
>
>             }), 3)
>
>         stopifnot(any(!dat2$norm_sd) >= 1)
>
>         if (any(!dat2$norm_sd) >= 1) {
>
>             df1 <- dat1
>
>             return(df1)
>
>         }
>
>         else {
>
>             df2 <- recursive_funlp()
>
>             return(df2)
>
>         }
>
>     }
>
>     df3 <- recursive_funlp(dataset = dat1, func = funlp2)
>
>     df3
>
> }
>
>
> # funlp1 function
>
> funlp1<- function (dataset)
>
> {
>
>     dat2 <- dataset %>% select(field, set, ent_num, rep_num,
>
>         lp) %>% unite(uniqueid, set, ent_num, sep = ".") %>%
>
>         unite(field_rep, field, rep_num) %>% mutate(field_rep =
> paste(field_rep,
>
>         "lp", sep = ".")) %>% spread(key = field_rep, value = lp) %>%
>
>         mutate_at(.vars = grep("_", names(.)), funs(norm = round(scale(.),
>
>             3)))
>
>     dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>         1, sd, na.rm = TRUE), 3)
>
>     dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>         1, function(x) {
>
>             max(abs(x), na.rm = TRUE)
>
>         }), 3)
>
>     data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,
>
>         -norm_sd) %>% separate(key, c("field_rep", "treatment"),
>
>         "\\.") %>% spread(treatment, value) %>% mutate(outlier = NA)
>
>     df_clean <- with(data1, data1[norm_sd < 1, ])
>
>     datD <- with(data1, data1[norm_sd >= 1, ])
>
>     s <- split(datD, datD$uniqueid)
>
>     sdf <- lapply(s, function(x) {
>
>         data.frame(x, x$outlier <- ifelse(is.na(x$lp_norm), NA,
>
>             ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),
>
>             x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))
>
>         x
>
>     })
>
>     sdf2 <- bind_rows(sdf)
>
>     all_dat <- bind_rows(df_clean, sdf2)
>
>     all_dat
>
> }
>
>
> # funlp2 function
>
> funlp2<-function (dataset)
>
> {
>
>     data1 <- dataset
>
>     df_clean <- with(data1, data1[norm_sd < 1, ])
>
>     datD <- with(data1, data1[norm_sd >= 1, ])
>
>     s <- split(datD, datD$uniqueid)
>
>     sdf <- lapply(s, function(x) {
>
>         data.frame(x, x$outlier <- ifelse(is.na(x$lp_norm), NA,
>
>             ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),
>
>             x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))
>
>         x
>
>     })
>
>     sdf2 <- bind_rows(sdf)
>
>     all_dat <- bind_rows(df_clean, sdf2)
>
>     all_dat
>
> }
>
>
> # dataset
> dput(dat)
> structure(list(field = c("LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01"), set = c("seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta"), ent_num = c(23L, 14L, 43L, 30L, 44L, 60L, 17L,
> 34L, 41L, 40L, 9L, 36L, 38L, 19L, 61L, 51L, 45L, 42L, 3L, 39L,
> 21L, 11L, 12L, 7L, 35L, 5L, 70L, 47L, 28L, 16L, 72L, 13L, 49L,
> 67L, 56L, 32L, 27L, 46L, 24L, 63L, 15L, 66L, 26L, 29L, 48L, 1L,
> 54L, 37L, 2L, 50L, 52L, 31L, 33L, 25L, 6L, 69L, 53L, 10L, 18L,
> 55L, 59L, 4L, 58L, 22L, 20L, 64L, 71L, 57L, 11L, 17L, 43L, 13L,
> 1L, 16L, 34L, 47L, 52L, 72L, 59L, 22L, 54L, 18L, 25L, 61L, 56L,
> 41L, 27L, 14L, 49L, 19L, 29L, 31L, 64L, 6L, 53L, 35L, 37L, 67L,
> 39L, 51L, 40L, 15L, 69L, 60L, 38L, 4L, 23L, 3L, 48L, 32L, 42L,
> 24L, 28L, 33L, 57L, 8L, 21L, 46L, 7L, 30L, 45L, 2L, 63L, 36L,
> 68L, 20L, 66L, 70L, 58L, 5L, 10L, 12L, 62L, 50L, 71L, 9L, 55L,
> 26L, 44L, 65L, 14L, 63L, 46L, 58L, 62L, 19L, 59L, 2L, 5L, 6L,
> 40L, 21L, 44L, 37L, 55L, 35L, 71L, 56L, 10L, 36L, 53L, 25L, 61L,
> 12L, 26L, 23L, 4L, 13L, 28L, 38L, 57L, 54L, 72L, 48L, 66L, 9L,
> 70L, 15L, 39L, 60L, 17L, 34L, 51L, 67L, 42L, 49L, 31L, 30L, 3L,
> 18L, 65L, 32L, 27L, 52L, 22L, 11L, 47L, 64L, 8L, 43L, 41L, 16L,
> 20L, 33L, 7L, 50L, 68L, 24L, 1L, 69L, 45L, 29L, 37L, 30L, 55L,
> 54L, 43L, 32L, 21L, 27L, 33L, 40L, 67L, 57L, 68L, 31L, 17L, 13L,
> 6L, 62L, 19L, 22L, 3L, 10L, 44L, 34L, 69L, 70L, 4L, 1L, 25L,
> 11L, 51L, 5L, 63L, 71L, 12L, 38L, 58L, 39L, 49L, 59L, 56L, 65L,
> 2L, 64L, 8L, 35L, 46L, 45L, 29L, 53L, 36L, 42L, 23L, 18L, 50L,
> 26L, 14L, 48L, 66L, 20L, 24L, 7L, 15L, 53L, 22L, 39L, 20L, 60L,
> 59L, 43L, 19L, 41L, 6L, 62L, 1L, 55L, 34L, 50L, 38L, 40L, 44L,
> 4L, 46L, 29L, 65L, 57L, 48L, 33L, 69L, 14L, 35L, 67L, 72L, 54L,
> 3L, 49L, 2L, 12L, 18L, 30L, 10L, 70L, 31L, 15L, 63L, 71L, 21L,
> 45L, 28L, 56L, 27L, 64L, 61L, 51L, 5L, 24L, 68L, 25L, 66L, 16L,
> 36L, 58L, 37L, 52L, 26L, 9L, 42L, 7L, 11L, 8L, 32L, 23L, 13L,
> 47L, 17L, 61L, 72L, 47L, 60L, 16L, 9L, 28L, 52L, 41L, 1L, 61L,
> 6L, 23L, 58L, 63L, 25L, 28L, 30L, 36L, 62L, 9L, 32L, 19L, 31L,
> 56L, 45L, 2L, 22L, 27L, 40L, 14L, 11L, 50L, 13L, 70L, 20L, 64L,
> 39L, 26L, 21L, 43L, 29L, 35L, 54L, 52L, 37L, 17L, 16L, 72L, 48L,
> 12L, 18L, 44L, 42L, 49L, 68L, 5L, 55L, 69L, 51L, 66L, 59L, 53L,
> 15L, 71L, 41L, 57L, 4L, 60L, 8L, 7L, 33L, 34L, 24L, 10L, 67L,
> 47L, 38L, 3L, 65L, 46L, 20L, 34L, 71L, 1L, 33L, 57L, 13L, 21L,
> 66L, 29L, 3L, 61L, 69L, 24L, 62L, 39L, 49L, 47L, 31L, 53L, 52L,
> 43L, 17L, 7L, 8L, 12L, 60L, 63L, 50L, 2L, 51L, 46L, 10L, 23L,
> 48L, 11L, 26L, 40L, 70L, 42L, 59L, 15L, 56L, 58L, 27L, 6L, 35L,
> 4L, 37L, 5L, 65L, 44L, 28L, 14L, 32L, 36L, 45L, 9L, 18L, 55L,
> 68L, 30L, 54L, 41L, 25L, 22L, 38L, 16L, 67L, 64L, 19L, 72L, 68L,
> 28L, 33L, 15L, 51L, 4L, 47L, 36L, 8L, 57L, 48L, 1L, 52L, 39L,
> 32L, 50L, 13L, 30L, 63L, 2L, 9L, 62L, 22L, 6L, 61L, 16L, 53L,
> 38L, 37L, 20L, 69L, 44L, 56L, 29L, 26L, 14L, 17L, 46L, 66L, 58L,
> 42L, 60L, 19L, 45L, 3L, 59L, 70L, 31L, 24L, 55L, 40L, 43L, 25L,
> 65L, 12L, 67L, 21L, 7L, 27L, 49L, 72L, 54L, 41L, 23L, 34L, 5L,
> 64L, 35L, 18L, 71L, 11L, 24L, 19L, 38L, 14L, 4L, 56L, 5L, 54L,
> 34L, 64L, 55L, 33L, 69L, 71L, 52L, 61L, 48L, 23L, 43L, 41L, 20L,
> 39L, 11L, 63L, 36L, 22L, 9L, 25L, 27L, 51L, 53L, 37L, 57L, 13L,
> 18L, 64L, 22L, 53L, 16L, 5L, 28L, 60L, 31L, 11L, 29L, 45L, 59L,
> 72L, 49L, 67L, 13L, 20L, 3L, 42L, 44L, 69L, 33L, 38L, 15L, 70L,
> 35L, 48L, 26L, 56L, 19L, 39L, 43L, 40L, 14L, 2L, 68L, 51L, 12L,
> 47L, 10L, 55L, 23L, 4L, 71L, 41L, 50L, 7L, 24L, 61L, 27L, 54L,
> 46L, 58L, 37L, 66L, 57L, 1L, 36L, 32L, 18L, 62L, 9L, 30L, 21L,
> 6L, 52L, 8L, 65L, 17L, 25L, 63L, 34L, 65L, 22L, 56L, 9L, 7L,
> 11L, 31L, 4L, 63L, 29L, 61L, 54L, 12L, 62L, 59L, 5L, 23L, 53L,
> 36L, 24L, 35L, 66L, 49L, 72L, 18L, 70L, 32L, 43L, 20L, 45L, 34L,
> 46L, 28L, 6L, 44L, 71L, 39L, 13L, 27L, 1L, 58L, 30L, 68L, 17L,
> 33L, 26L, 57L, 15L, 21L, 52L, 48L, 42L, 16L, 40L, 38L, 8L, 69L,
> 2L, 51L, 67L, 55L, 64L, 47L, 60L, 19L, 41L, 50L, 3L, 14L, 25L,
> 10L, 37L, 6L, 15L, 45L, 49L, 8L, 17L, 50L, 16L, 58L, 72L, 26L,
> 60L, 7L, 32L, 1L, 46L, 66L, 68L, 62L, 47L, 35L, 70L, 10L, 31L,
> 65L, 2L, 3L, 21L, 12L, 30L, 40L, 28L, 59L, 42L, 67L, 44L, 29L
> ), rep_num = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L), lp = c(NA, 41.64, 38.8, 44.45, 40.54,
> 38.54, 41.94, 39.6, 37.39, 40.95, 38.45, 43.47, 41.66, 40.91,
> 42.68, 43.12, 38.22, 40.95, 46.24, 42.95, 38.95, 39.88, 40.57,
> 40.13, 38.57, 45.45, 40.78, 43.52, 39.75, 39.93, 40.35, 37.6,
> 37.7, 43.05, 43.32, 41.31, 39.03, 42.5, 43.18, 41.32, 39.58,
> 40.62, 39.64, 39.85, 38.75, 40.18, 41.44, 40.5, 40.87, 40.75,
> 38.37, 40.26, 35.11, 40.89, 41.67, 38.87, 37.32, 35.85, 38.25,
> 42.21, 43.15, 38.69, 38.8, 38.77, 37.98, 39.8, 33.37, 40.09,
> 42.87, 44.07, 43.78, 42.47, 42.8, 41.1, 41.17, 44.07, 44.24,
> 43.16, 46.12, 42.64, 43.92, 42.16, 43.69, 42.89, 42.63, 43.58,
> 44.4, 43.17, 43.3, 42.5, 42.6, 44.05, 44.63, 43.09, 45.17, 45.17,
> 42.78, 42.38, 45.76, 42.16, 44.22, 43.67, 40.45, 41.95, 42.49,
> 41.98, 45.15, 46.01, 38.51, 43.08, 45.19, 44.53, 43.14, 39.93,
> 46.84, 43.59, 41.68, 43.7, 44.63, 44.02, 42.07, 43.88, 43.2,
> 46.2, 40.84, 39.14, 43.89, 42.58, 41.53, 45.32, 39.56, 43.77,
> 45.45, 45.16, 43.4, 40.08, 42.27, 43.74, 43.77, 41.31, 41.38,
> 45.01, 45.76, 40.4, 48.39, 46.27, 15.44, 44.76, 47.45, 44.87,
> 46.8, 41.83, 43.03, 43.96, 42.51, 45.06, 45.55, 44.9, 42.47,
> 44.9, 45, 44.77, 43.79, 44.26, 44.57, 44.4, 43.42, 42.31, 47.18,
> 43.83, 45.72, 44.83, 44.96, 40.28, 42.85, 41.23, 45.23, 47.09,
> 43.61, 42.69, 46.27, 45.16, 44.14, 43.34, 45.97, 43.81, 43.01,
> 39.82, 45.91, 45.97, 43.61, 45.12, 46.37, 42.67, 42.47, 45.86,
> 44.19, 44.46, 42.64, 43.95, 44.93, 40.33, 42.75, 39.92, 44.17,
> 44.49, 41.51, 42.43, 44.14, 40.5, 41.29, 44.89, 37.98, 39.02,
> 39.62, 42.13, 39.03, 44.16, 39.15, 41.49, 42.63, 40.11, 39.97,
> 42.85, 35.98, 39.45, 40.99, 42.44, 42.11, 37.36, 40.63, 40.69,
> 43.57, 39.04, 39.3, 42.19, 36.88, 40.39, 37.78, 38.6, 40.2, 40.98,
> 36.58, 43.59, 42.49, 39.96, 39.84, 40.43, 38.94, 42.72, 39.43,
> 42.13, 40.36, 40.58, 40.01, 42.17, 42.17, 41.35, 43.27, 40.15,
> 39.76, 40.94, 40.87, 42.32, 41.81, 41.97, 43.72, 41.32, 40.83,
> 37.64, 41.03, 38.98, 40.61, 41.17, 41.96, 40.07, 38.48, 42.85,
> 38.68, 39.09, 42.16, 38.14, 37.99, 41.06, 37.4, 41.88, 39.35,
> 39.73, 38.38, 41.34, 40.67, 40.89, 39.28, 37.59, 39.3, 39.72,
> 40.79, 39.42, 34.5, 37.61, 37.76, 40.24, 41.17, 41.24, 42.14,
> 42.53, 39.71, 39.44, 40.19, 42.51, 40.15, 36.26, 37.48, 40.43,
> 37.5, 42.11, 41.44, 40.29, 39.75, 37.58, 41.25, 40.39, 41.63,
> 42.18, 43.71, 38.69, 43.47, 41.98, 35.51, 42.22, 38.51, 40.17,
> 39.4, 39.54, 41.7, 40.93, 40.01, 35.97, 41.44, 41.89, 40.9, 39.95,
> 40.69, 43.18, 37.03, 39.91, 36.75, 42.75, 41.61, 42.6, 38.55,
> 42.84, 38.41, 42.55, 41.44, 41.03, 40, 41.35, 42.18, 42.66, 40.06,
> 42.48, 41.33, 41.92, 40.88, 40.99, 42.78, 38.26, 40.81, 39.95,
> 37.89, 40.81, 38.33, 39.33, 39.67, 40.12, 41.75, 39.81, 41.92,
> 44.59, 37.8, 40.54, 40.49, 41.82, 42.13, 39.93, 39.94, 42.54,
> 41.74, 43.06, 41.72, 41.27, 39.42, 42.07, 40.06, 42.1, 38.91,
> 39.28, 42.94, 39.94, 41.86, 38.56, 38.15, 41.47, 42.34, 38.14,
> 40.19, 40.2, 43.01, 42.35, 40.89, 41.44, 42.89, 44.49, 39.1,
> 38.42, 37.44, 43.28, 37.96, 40.56, 41.53, 41.59, 41.45, 42.55,
> 40.6, 44.04, 39.31, 41.08, 37.14, 40.03, 41.49, 40.82, 38.47,
> 43.43, 38.82, 40.4, 41.09, 42.26, 41.85, 41.13, 37.27, 41.97,
> 45.24, 43.35, 40.53, 43.14, 39.71, 42.77, 42.2, 42.13, 41.91,
> 42.59, 41.99, 42.11, 40.43, 40.02, 43.53, 43.01, 39.55, 43.2,
> 39.95, 42.51, 42, 42.26, 42.39, 42.64, 39.9, 41.89, 43.05, 41.02,
> 41.95, 39.92, 43.42, 44.01, 42.6, 40.84, 42.86, 44.31, 42.24,
> 41.25, 42.38, 44.99, 41.24, 43.97, 41.12, 37.88, 41.53, 41.56,
> 39.18, 40.83, 42.96, 41.92, 43.86, 42.48, 42.65, 43.3, 41.99,
> 42.51, 40.65, 42.77, 34.71, 40.97, 38.15, 39.76, 36.74, 37.95,
> 39.17, 38.22, 39.31, 43.57, 37.21, 39.35, 42.37, 42.01, 42.39,
> 43.21, 36.91, 36.69, 41.07, 41.91, 34.63, 40.61, 36.23, 38.12,
> 40.76, 39.14, 41.81, 39.14, 41.04, 37.32, 40.83, 40.81, 38.29,
> 39.61, 40.96, 40.71, 40.47, 38.64, 39.76, 38.19, 39.05, 38.04,
> 41.14, 38.35, 42.3, 34.44, 40.93, 39.3, 41.44, 38.3, 42.74, 36.66,
> 40.02, 36.62, 40.48, 41.72, 41.23, 41.81, 42.07, 40.22, 37.83,
> 36.54, 37.77, 40.49, 38.65, 43.2, 43.32, 40.67, 41.95, 36.11,
> 39.28, 42.38, 40.35, 40.3, 43.48, 40.55, 40.54, 44.03, 41.94,
> 37.97, 41.98, 41.53, 38.19, 38.66, 41.18, 41.95, 42.53, 38.7,
> 44.55, 42.39, 41.55, 38.46, 42.27, 42.19, 41.95, 41.81, 39.81,
> 38.12, 42.94, 42.99, 38.85, 41.26, 43.13, 44.21, 38.54, 44.02,
> 43.46, 41.64, 42.06, 42.11, 42.34, 41.86, 37.91, 40.89, 40.5,
> 39.54, 37.87, 40.86, 41.36, 41.77, 42.03, 39.15, 40.04, 44.11,
> 41.34, 42.97, 38.42, 37.28, 41.04, 41.48, 38.82, 41.94, 37.95,
> 40.9, 40.39, 40.31, 43.19, 41.22, 41.49, 41.25, 40.07, 36.7,
> 39.97, 39.99, 41.7, 37.09, 42.58, 43.01, 37.7, 41.81, 39.99,
> 42.95, 43.19, 42.69, 41.5, 40.64, 43.24, 41.14, 41.21, 41.29,
> 41.43, 44.21, 38.52, 42.54, 40.54, 42.49, 43.2, 38.12, 40.08,
> 39.02, 41.45, 42.33, 41.11, 38.93, 41.63, 44.22, 41.41, 39.08,
> 40.9, 41.1, 43.88, 40.96, 46.75, 47.54, 40.35, 41.97, 44.94,
> 44.91, 44.66, 44.5, 44.4, 46.4, 47.97, 46.05, 45.57, 42.83, 41.48,
> 47.48, 45.43, 41.98, 43.14, 45.6, 44.78, 45.45, 45.69, 44.82,
> 44.24, 41.14, 43.14, 46.61, 43.92, 43.56, 43.68, 45.37, 45.15,
> 40.76, 43.78, 44.67, 41.36, 41.4, 40.97, 41.87, 39.83, 43.8,
> 48.36, 44.28, 43.29, 44.69, 43.26, 43.35, 44.34, 45.08, 42.26,
> 39.7, 42.4, 44.03, 43.22, 42.71, 45.89, 44.89, 44.81, 42.59,
> 40.85, 43.82, 44.85, 47.47, 43.64, 42.65, 45.67, 43.24, 42.33,
> 40.61, 38.3, 39.84, 41.08, 42.33, 44.44, 40.85, 39.58, 42.55,
> 41.75, 39.44, 41.79, 39.31, 41.34, 42.76, 40.79, 37.31, 42.85,
> 42.88, 42.01, 44.63, 38.85, 41.13, 40.43, 41.34, 43.14, 40.58,
> 42.21, 38.94, 44.88, 42.33, 42.61, 41.88, 41.13, 41.83, 42.8)), .Names =
> c("field",
> "set", "ent_num", "rep_num", "lp"), class = "data.frame", row.names = c(NA,
> -787L))
>
> # session info
>
> R version 3.4.1 (2017-06-30)
>
> Platform: i386-w64-mingw32/i386 (32-bit)
>
> Running under: Windows 7 x64 (build 7601) Service Pack 1
>
>
>
> Matrix products: default
>
>
>
> locale:
>
> [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
> States.1252
>
> [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C
>
> [5] LC_TIME=English_United States.1252
>
>
>
> attached base packages:
>
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
>
>
> other attached packages:
>
> [1] bindrcpp_0.2 tidyr_0.6.3  dplyr_0.7.4
>
>
>
> loaded via a namespace (and not attached):
>
>  [1] compiler_3.4.1   magrittr_1.5     assertthat_0.2.0 R6_2.2.2
>  tools_3.4.1
>
>  [6] glue_1.1.1       tibble_1.3.3     Rcpp_0.12.11     stringi_1.1.5
> pkgconfig_2.0.1
>
> [11] rlang_0.1.2      bindr_0.1
>
> This email and any attachments were sent from a Monsanto email account and
> may contain confidential and/or privileged information. If you are not the
> intended recipient, please contact the sender and delete this email and any
> attachments immediately. Any unauthorized use, including disclosing,
> printing, storing, copying or distributing this email, is prohibited. All
> emails and attachments sent to or from Monsanto email accounts may be
> subject to monitoring, reading, and archiving by Monsanto, including its
> affiliates and subsidiaries, as permitted by applicable law. Thank you.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Thu Dec 14 15:16:58 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Thu, 14 Dec 2017 16:16:58 +0200
Subject: [R] help with recursive function
In-Reply-To: <CAGgJW772Nk+Z3XamiGF-2r6Ta9Cud1VOhUvRr4p4k0epTX4xtA@mail.gmail.com>
References: <CY1PR0101MB1018F79F34CFE3142A8592ADE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CAGgJW772Nk+Z3XamiGF-2r6Ta9Cud1VOhUvRr4p4k0epTX4xtA@mail.gmail.com>
Message-ID: <CAGgJW77AAvtOiCwLz=XEvsU+AuqHA=SK8RWLcBD5Ka7P8OFrPA@mail.gmail.com>

My own typo ... whoops ...

!( any(dat2$norm_sd >= 1 ))



On Thu, Dec 14, 2017 at 3:43 PM, Eric Berger <ericjberger at gmail.com> wrote:

> You seem to have a typo at this expression (and some others like it)
>
> Namely, you write
>
> any(!dat2$norm_sd) >= 1
>
> when you possibly meant to write
>
> !( any(dat2$norm_sd) >= 1 )
>
> i.e. I think your ! seems to be in the wrong place.
>
> HTH,
> Eric
>
>
> On Thu, Dec 14, 2017 at 3:26 PM, DIGHE, NILESH [AG/2362] <
> nilesh.dighe at monsanto.com> wrote:
>
>> Hi, I need some help with running a recursive function. I like to run
>> funlp2 recursively.
>> When I try to run recursive function in another function named "calclp" I
>> get this "Error: any(!dat2$norm_sd) >= 1 is not TRUE".
>>
>> I have never built a recursive function before so having trouble
>> executing it in this case.  I would appreciate any help or guidance to
>> resolve this issue. Please see my data and the three functions that I am
>> using below.
>> Please note that calclp is the function I am running and the other two
>> functions are within this calclp function.
>>
>> # code:
>> Test<- calclp(dataset = dat)
>>
>> # calclp function
>>
>> calclp<- function (dataset)
>>
>> {
>>
>>     dat1 <- funlp1(dataset)
>>
>>     recursive_funlp <- function(dataset = dat1, func = funlp2) {
>>
>>         dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
>>
>>             mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
>>
>>             spread(key = field_rep, value = lp) %>% mutate_at(.vars =
>> grep("_",
>>
>>             names(.)), funs(norm = round(scale(.), 3)))
>>
>>         dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>>
>>             1, sd, na.rm = TRUE), 3)
>>
>>         dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>>
>>             1, function(x) {
>>
>>                 max(abs(x), na.rm = TRUE)
>>
>>             }), 3)
>>
>>         stopifnot(any(!dat2$norm_sd) >= 1)
>>
>>         if (any(!dat2$norm_sd) >= 1) {
>>
>>             df1 <- dat1
>>
>>             return(df1)
>>
>>         }
>>
>>         else {
>>
>>             df2 <- recursive_funlp()
>>
>>             return(df2)
>>
>>         }
>>
>>     }
>>
>>     df3 <- recursive_funlp(dataset = dat1, func = funlp2)
>>
>>     df3
>>
>> }
>>
>>
>> # funlp1 function
>>
>> funlp1<- function (dataset)
>>
>> {
>>
>>     dat2 <- dataset %>% select(field, set, ent_num, rep_num,
>>
>>         lp) %>% unite(uniqueid, set, ent_num, sep = ".") %>%
>>
>>         unite(field_rep, field, rep_num) %>% mutate(field_rep =
>> paste(field_rep,
>>
>>         "lp", sep = ".")) %>% spread(key = field_rep, value = lp) %>%
>>
>>         mutate_at(.vars = grep("_", names(.)), funs(norm = round(scale(.),
>>
>>             3)))
>>
>>     dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>>
>>         1, sd, na.rm = TRUE), 3)
>>
>>     dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>>
>>         1, function(x) {
>>
>>             max(abs(x), na.rm = TRUE)
>>
>>         }), 3)
>>
>>     data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,
>>
>>         -norm_sd) %>% separate(key, c("field_rep", "treatment"),
>>
>>         "\\.") %>% spread(treatment, value) %>% mutate(outlier = NA)
>>
>>     df_clean <- with(data1, data1[norm_sd < 1, ])
>>
>>     datD <- with(data1, data1[norm_sd >= 1, ])
>>
>>     s <- split(datD, datD$uniqueid)
>>
>>     sdf <- lapply(s, function(x) {
>>
>>         data.frame(x, x$outlier <- ifelse(is.na(x$lp_norm), NA,
>>
>>             ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),
>>
>>             x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))
>>
>>         x
>>
>>     })
>>
>>     sdf2 <- bind_rows(sdf)
>>
>>     all_dat <- bind_rows(df_clean, sdf2)
>>
>>     all_dat
>>
>> }
>>
>>
>> # funlp2 function
>>
>> funlp2<-function (dataset)
>>
>> {
>>
>>     data1 <- dataset
>>
>>     df_clean <- with(data1, data1[norm_sd < 1, ])
>>
>>     datD <- with(data1, data1[norm_sd >= 1, ])
>>
>>     s <- split(datD, datD$uniqueid)
>>
>>     sdf <- lapply(s, function(x) {
>>
>>         data.frame(x, x$outlier <- ifelse(is.na(x$lp_norm), NA,
>>
>>             ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),
>>
>>             x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))
>>
>>         x
>>
>>     })
>>
>>     sdf2 <- bind_rows(sdf)
>>
>>     all_dat <- bind_rows(df_clean, sdf2)
>>
>>     all_dat
>>
>> }
>>
>>
>> # dataset
>> dput(dat)
>> structure(list(field = c("LM01", "LM01", "LM01", "LM01", "LM01",
>> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
>> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
>> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
>> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
>> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
>> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
>> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
>> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "B2NW", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "EX01", "EX01",
>> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
>> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
>> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
>> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
>> "EX01", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
>> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
>> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
>> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
>> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01"), set = c("seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta"), ent_num = c(23L, 14L, 43L, 30L, 44L, 60L, 17L,
>> 34L, 41L, 40L, 9L, 36L, 38L, 19L, 61L, 51L, 45L, 42L, 3L, 39L,
>> 21L, 11L, 12L, 7L, 35L, 5L, 70L, 47L, 28L, 16L, 72L, 13L, 49L,
>> 67L, 56L, 32L, 27L, 46L, 24L, 63L, 15L, 66L, 26L, 29L, 48L, 1L,
>> 54L, 37L, 2L, 50L, 52L, 31L, 33L, 25L, 6L, 69L, 53L, 10L, 18L,
>> 55L, 59L, 4L, 58L, 22L, 20L, 64L, 71L, 57L, 11L, 17L, 43L, 13L,
>> 1L, 16L, 34L, 47L, 52L, 72L, 59L, 22L, 54L, 18L, 25L, 61L, 56L,
>> 41L, 27L, 14L, 49L, 19L, 29L, 31L, 64L, 6L, 53L, 35L, 37L, 67L,
>> 39L, 51L, 40L, 15L, 69L, 60L, 38L, 4L, 23L, 3L, 48L, 32L, 42L,
>> 24L, 28L, 33L, 57L, 8L, 21L, 46L, 7L, 30L, 45L, 2L, 63L, 36L,
>> 68L, 20L, 66L, 70L, 58L, 5L, 10L, 12L, 62L, 50L, 71L, 9L, 55L,
>> 26L, 44L, 65L, 14L, 63L, 46L, 58L, 62L, 19L, 59L, 2L, 5L, 6L,
>> 40L, 21L, 44L, 37L, 55L, 35L, 71L, 56L, 10L, 36L, 53L, 25L, 61L,
>> 12L, 26L, 23L, 4L, 13L, 28L, 38L, 57L, 54L, 72L, 48L, 66L, 9L,
>> 70L, 15L, 39L, 60L, 17L, 34L, 51L, 67L, 42L, 49L, 31L, 30L, 3L,
>> 18L, 65L, 32L, 27L, 52L, 22L, 11L, 47L, 64L, 8L, 43L, 41L, 16L,
>> 20L, 33L, 7L, 50L, 68L, 24L, 1L, 69L, 45L, 29L, 37L, 30L, 55L,
>> 54L, 43L, 32L, 21L, 27L, 33L, 40L, 67L, 57L, 68L, 31L, 17L, 13L,
>> 6L, 62L, 19L, 22L, 3L, 10L, 44L, 34L, 69L, 70L, 4L, 1L, 25L,
>> 11L, 51L, 5L, 63L, 71L, 12L, 38L, 58L, 39L, 49L, 59L, 56L, 65L,
>> 2L, 64L, 8L, 35L, 46L, 45L, 29L, 53L, 36L, 42L, 23L, 18L, 50L,
>> 26L, 14L, 48L, 66L, 20L, 24L, 7L, 15L, 53L, 22L, 39L, 20L, 60L,
>> 59L, 43L, 19L, 41L, 6L, 62L, 1L, 55L, 34L, 50L, 38L, 40L, 44L,
>> 4L, 46L, 29L, 65L, 57L, 48L, 33L, 69L, 14L, 35L, 67L, 72L, 54L,
>> 3L, 49L, 2L, 12L, 18L, 30L, 10L, 70L, 31L, 15L, 63L, 71L, 21L,
>> 45L, 28L, 56L, 27L, 64L, 61L, 51L, 5L, 24L, 68L, 25L, 66L, 16L,
>> 36L, 58L, 37L, 52L, 26L, 9L, 42L, 7L, 11L, 8L, 32L, 23L, 13L,
>> 47L, 17L, 61L, 72L, 47L, 60L, 16L, 9L, 28L, 52L, 41L, 1L, 61L,
>> 6L, 23L, 58L, 63L, 25L, 28L, 30L, 36L, 62L, 9L, 32L, 19L, 31L,
>> 56L, 45L, 2L, 22L, 27L, 40L, 14L, 11L, 50L, 13L, 70L, 20L, 64L,
>> 39L, 26L, 21L, 43L, 29L, 35L, 54L, 52L, 37L, 17L, 16L, 72L, 48L,
>> 12L, 18L, 44L, 42L, 49L, 68L, 5L, 55L, 69L, 51L, 66L, 59L, 53L,
>> 15L, 71L, 41L, 57L, 4L, 60L, 8L, 7L, 33L, 34L, 24L, 10L, 67L,
>> 47L, 38L, 3L, 65L, 46L, 20L, 34L, 71L, 1L, 33L, 57L, 13L, 21L,
>> 66L, 29L, 3L, 61L, 69L, 24L, 62L, 39L, 49L, 47L, 31L, 53L, 52L,
>> 43L, 17L, 7L, 8L, 12L, 60L, 63L, 50L, 2L, 51L, 46L, 10L, 23L,
>> 48L, 11L, 26L, 40L, 70L, 42L, 59L, 15L, 56L, 58L, 27L, 6L, 35L,
>> 4L, 37L, 5L, 65L, 44L, 28L, 14L, 32L, 36L, 45L, 9L, 18L, 55L,
>> 68L, 30L, 54L, 41L, 25L, 22L, 38L, 16L, 67L, 64L, 19L, 72L, 68L,
>> 28L, 33L, 15L, 51L, 4L, 47L, 36L, 8L, 57L, 48L, 1L, 52L, 39L,
>> 32L, 50L, 13L, 30L, 63L, 2L, 9L, 62L, 22L, 6L, 61L, 16L, 53L,
>> 38L, 37L, 20L, 69L, 44L, 56L, 29L, 26L, 14L, 17L, 46L, 66L, 58L,
>> 42L, 60L, 19L, 45L, 3L, 59L, 70L, 31L, 24L, 55L, 40L, 43L, 25L,
>> 65L, 12L, 67L, 21L, 7L, 27L, 49L, 72L, 54L, 41L, 23L, 34L, 5L,
>> 64L, 35L, 18L, 71L, 11L, 24L, 19L, 38L, 14L, 4L, 56L, 5L, 54L,
>> 34L, 64L, 55L, 33L, 69L, 71L, 52L, 61L, 48L, 23L, 43L, 41L, 20L,
>> 39L, 11L, 63L, 36L, 22L, 9L, 25L, 27L, 51L, 53L, 37L, 57L, 13L,
>> 18L, 64L, 22L, 53L, 16L, 5L, 28L, 60L, 31L, 11L, 29L, 45L, 59L,
>> 72L, 49L, 67L, 13L, 20L, 3L, 42L, 44L, 69L, 33L, 38L, 15L, 70L,
>> 35L, 48L, 26L, 56L, 19L, 39L, 43L, 40L, 14L, 2L, 68L, 51L, 12L,
>> 47L, 10L, 55L, 23L, 4L, 71L, 41L, 50L, 7L, 24L, 61L, 27L, 54L,
>> 46L, 58L, 37L, 66L, 57L, 1L, 36L, 32L, 18L, 62L, 9L, 30L, 21L,
>> 6L, 52L, 8L, 65L, 17L, 25L, 63L, 34L, 65L, 22L, 56L, 9L, 7L,
>> 11L, 31L, 4L, 63L, 29L, 61L, 54L, 12L, 62L, 59L, 5L, 23L, 53L,
>> 36L, 24L, 35L, 66L, 49L, 72L, 18L, 70L, 32L, 43L, 20L, 45L, 34L,
>> 46L, 28L, 6L, 44L, 71L, 39L, 13L, 27L, 1L, 58L, 30L, 68L, 17L,
>> 33L, 26L, 57L, 15L, 21L, 52L, 48L, 42L, 16L, 40L, 38L, 8L, 69L,
>> 2L, 51L, 67L, 55L, 64L, 47L, 60L, 19L, 41L, 50L, 3L, 14L, 25L,
>> 10L, 37L, 6L, 15L, 45L, 49L, 8L, 17L, 50L, 16L, 58L, 72L, 26L,
>> 60L, 7L, 32L, 1L, 46L, 66L, 68L, 62L, 47L, 35L, 70L, 10L, 31L,
>> 65L, 2L, 3L, 21L, 12L, 30L, 40L, 28L, 59L, 42L, 67L, 44L, 29L
>> ), rep_num = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L), lp = c(NA, 41.64, 38.8, 44.45, 40.54,
>> 38.54, 41.94, 39.6, 37.39, 40.95, 38.45, 43.47, 41.66, 40.91,
>> 42.68, 43.12, 38.22, 40.95, 46.24, 42.95, 38.95, 39.88, 40.57,
>> 40.13, 38.57, 45.45, 40.78, 43.52, 39.75, 39.93, 40.35, 37.6,
>> 37.7, 43.05, 43.32, 41.31, 39.03, 42.5, 43.18, 41.32, 39.58,
>> 40.62, 39.64, 39.85, 38.75, 40.18, 41.44, 40.5, 40.87, 40.75,
>> 38.37, 40.26, 35.11, 40.89, 41.67, 38.87, 37.32, 35.85, 38.25,
>> 42.21, 43.15, 38.69, 38.8, 38.77, 37.98, 39.8, 33.37, 40.09,
>> 42.87, 44.07, 43.78, 42.47, 42.8, 41.1, 41.17, 44.07, 44.24,
>> 43.16, 46.12, 42.64, 43.92, 42.16, 43.69, 42.89, 42.63, 43.58,
>> 44.4, 43.17, 43.3, 42.5, 42.6, 44.05, 44.63, 43.09, 45.17, 45.17,
>> 42.78, 42.38, 45.76, 42.16, 44.22, 43.67, 40.45, 41.95, 42.49,
>> 41.98, 45.15, 46.01, 38.51, 43.08, 45.19, 44.53, 43.14, 39.93,
>> 46.84, 43.59, 41.68, 43.7, 44.63, 44.02, 42.07, 43.88, 43.2,
>> 46.2, 40.84, 39.14, 43.89, 42.58, 41.53, 45.32, 39.56, 43.77,
>> 45.45, 45.16, 43.4, 40.08, 42.27, 43.74, 43.77, 41.31, 41.38,
>> 45.01, 45.76, 40.4, 48.39, 46.27, 15.44, 44.76, 47.45, 44.87,
>> 46.8, 41.83, 43.03, 43.96, 42.51, 45.06, 45.55, 44.9, 42.47,
>> 44.9, 45, 44.77, 43.79, 44.26, 44.57, 44.4, 43.42, 42.31, 47.18,
>> 43.83, 45.72, 44.83, 44.96, 40.28, 42.85, 41.23, 45.23, 47.09,
>> 43.61, 42.69, 46.27, 45.16, 44.14, 43.34, 45.97, 43.81, 43.01,
>> 39.82, 45.91, 45.97, 43.61, 45.12, 46.37, 42.67, 42.47, 45.86,
>> 44.19, 44.46, 42.64, 43.95, 44.93, 40.33, 42.75, 39.92, 44.17,
>> 44.49, 41.51, 42.43, 44.14, 40.5, 41.29, 44.89, 37.98, 39.02,
>> 39.62, 42.13, 39.03, 44.16, 39.15, 41.49, 42.63, 40.11, 39.97,
>> 42.85, 35.98, 39.45, 40.99, 42.44, 42.11, 37.36, 40.63, 40.69,
>> 43.57, 39.04, 39.3, 42.19, 36.88, 40.39, 37.78, 38.6, 40.2, 40.98,
>> 36.58, 43.59, 42.49, 39.96, 39.84, 40.43, 38.94, 42.72, 39.43,
>> 42.13, 40.36, 40.58, 40.01, 42.17, 42.17, 41.35, 43.27, 40.15,
>> 39.76, 40.94, 40.87, 42.32, 41.81, 41.97, 43.72, 41.32, 40.83,
>> 37.64, 41.03, 38.98, 40.61, 41.17, 41.96, 40.07, 38.48, 42.85,
>> 38.68, 39.09, 42.16, 38.14, 37.99, 41.06, 37.4, 41.88, 39.35,
>> 39.73, 38.38, 41.34, 40.67, 40.89, 39.28, 37.59, 39.3, 39.72,
>> 40.79, 39.42, 34.5, 37.61, 37.76, 40.24, 41.17, 41.24, 42.14,
>> 42.53, 39.71, 39.44, 40.19, 42.51, 40.15, 36.26, 37.48, 40.43,
>> 37.5, 42.11, 41.44, 40.29, 39.75, 37.58, 41.25, 40.39, 41.63,
>> 42.18, 43.71, 38.69, 43.47, 41.98, 35.51, 42.22, 38.51, 40.17,
>> 39.4, 39.54, 41.7, 40.93, 40.01, 35.97, 41.44, 41.89, 40.9, 39.95,
>> 40.69, 43.18, 37.03, 39.91, 36.75, 42.75, 41.61, 42.6, 38.55,
>> 42.84, 38.41, 42.55, 41.44, 41.03, 40, 41.35, 42.18, 42.66, 40.06,
>> 42.48, 41.33, 41.92, 40.88, 40.99, 42.78, 38.26, 40.81, 39.95,
>> 37.89, 40.81, 38.33, 39.33, 39.67, 40.12, 41.75, 39.81, 41.92,
>> 44.59, 37.8, 40.54, 40.49, 41.82, 42.13, 39.93, 39.94, 42.54,
>> 41.74, 43.06, 41.72, 41.27, 39.42, 42.07, 40.06, 42.1, 38.91,
>> 39.28, 42.94, 39.94, 41.86, 38.56, 38.15, 41.47, 42.34, 38.14,
>> 40.19, 40.2, 43.01, 42.35, 40.89, 41.44, 42.89, 44.49, 39.1,
>> 38.42, 37.44, 43.28, 37.96, 40.56, 41.53, 41.59, 41.45, 42.55,
>> 40.6, 44.04, 39.31, 41.08, 37.14, 40.03, 41.49, 40.82, 38.47,
>> 43.43, 38.82, 40.4, 41.09, 42.26, 41.85, 41.13, 37.27, 41.97,
>> 45.24, 43.35, 40.53, 43.14, 39.71, 42.77, 42.2, 42.13, 41.91,
>> 42.59, 41.99, 42.11, 40.43, 40.02, 43.53, 43.01, 39.55, 43.2,
>> 39.95, 42.51, 42, 42.26, 42.39, 42.64, 39.9, 41.89, 43.05, 41.02,
>> 41.95, 39.92, 43.42, 44.01, 42.6, 40.84, 42.86, 44.31, 42.24,
>> 41.25, 42.38, 44.99, 41.24, 43.97, 41.12, 37.88, 41.53, 41.56,
>> 39.18, 40.83, 42.96, 41.92, 43.86, 42.48, 42.65, 43.3, 41.99,
>> 42.51, 40.65, 42.77, 34.71, 40.97, 38.15, 39.76, 36.74, 37.95,
>> 39.17, 38.22, 39.31, 43.57, 37.21, 39.35, 42.37, 42.01, 42.39,
>> 43.21, 36.91, 36.69, 41.07, 41.91, 34.63, 40.61, 36.23, 38.12,
>> 40.76, 39.14, 41.81, 39.14, 41.04, 37.32, 40.83, 40.81, 38.29,
>> 39.61, 40.96, 40.71, 40.47, 38.64, 39.76, 38.19, 39.05, 38.04,
>> 41.14, 38.35, 42.3, 34.44, 40.93, 39.3, 41.44, 38.3, 42.74, 36.66,
>> 40.02, 36.62, 40.48, 41.72, 41.23, 41.81, 42.07, 40.22, 37.83,
>> 36.54, 37.77, 40.49, 38.65, 43.2, 43.32, 40.67, 41.95, 36.11,
>> 39.28, 42.38, 40.35, 40.3, 43.48, 40.55, 40.54, 44.03, 41.94,
>> 37.97, 41.98, 41.53, 38.19, 38.66, 41.18, 41.95, 42.53, 38.7,
>> 44.55, 42.39, 41.55, 38.46, 42.27, 42.19, 41.95, 41.81, 39.81,
>> 38.12, 42.94, 42.99, 38.85, 41.26, 43.13, 44.21, 38.54, 44.02,
>> 43.46, 41.64, 42.06, 42.11, 42.34, 41.86, 37.91, 40.89, 40.5,
>> 39.54, 37.87, 40.86, 41.36, 41.77, 42.03, 39.15, 40.04, 44.11,
>> 41.34, 42.97, 38.42, 37.28, 41.04, 41.48, 38.82, 41.94, 37.95,
>> 40.9, 40.39, 40.31, 43.19, 41.22, 41.49, 41.25, 40.07, 36.7,
>> 39.97, 39.99, 41.7, 37.09, 42.58, 43.01, 37.7, 41.81, 39.99,
>> 42.95, 43.19, 42.69, 41.5, 40.64, 43.24, 41.14, 41.21, 41.29,
>> 41.43, 44.21, 38.52, 42.54, 40.54, 42.49, 43.2, 38.12, 40.08,
>> 39.02, 41.45, 42.33, 41.11, 38.93, 41.63, 44.22, 41.41, 39.08,
>> 40.9, 41.1, 43.88, 40.96, 46.75, 47.54, 40.35, 41.97, 44.94,
>> 44.91, 44.66, 44.5, 44.4, 46.4, 47.97, 46.05, 45.57, 42.83, 41.48,
>> 47.48, 45.43, 41.98, 43.14, 45.6, 44.78, 45.45, 45.69, 44.82,
>> 44.24, 41.14, 43.14, 46.61, 43.92, 43.56, 43.68, 45.37, 45.15,
>> 40.76, 43.78, 44.67, 41.36, 41.4, 40.97, 41.87, 39.83, 43.8,
>> 48.36, 44.28, 43.29, 44.69, 43.26, 43.35, 44.34, 45.08, 42.26,
>> 39.7, 42.4, 44.03, 43.22, 42.71, 45.89, 44.89, 44.81, 42.59,
>> 40.85, 43.82, 44.85, 47.47, 43.64, 42.65, 45.67, 43.24, 42.33,
>> 40.61, 38.3, 39.84, 41.08, 42.33, 44.44, 40.85, 39.58, 42.55,
>> 41.75, 39.44, 41.79, 39.31, 41.34, 42.76, 40.79, 37.31, 42.85,
>> 42.88, 42.01, 44.63, 38.85, 41.13, 40.43, 41.34, 43.14, 40.58,
>> 42.21, 38.94, 44.88, 42.33, 42.61, 41.88, 41.13, 41.83, 42.8)), .Names =
>> c("field",
>> "set", "ent_num", "rep_num", "lp"), class = "data.frame", row.names =
>> c(NA,
>> -787L))
>>
>> # session info
>>
>> R version 3.4.1 (2017-06-30)
>>
>> Platform: i386-w64-mingw32/i386 (32-bit)
>>
>> Running under: Windows 7 x64 (build 7601) Service Pack 1
>>
>>
>>
>> Matrix products: default
>>
>>
>>
>> locale:
>>
>> [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
>> States.1252
>>
>> [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C
>>
>> [5] LC_TIME=English_United States.1252
>>
>>
>>
>> attached base packages:
>>
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>>
>>
>> other attached packages:
>>
>> [1] bindrcpp_0.2 tidyr_0.6.3  dplyr_0.7.4
>>
>>
>>
>> loaded via a namespace (and not attached):
>>
>>  [1] compiler_3.4.1   magrittr_1.5     assertthat_0.2.0 R6_2.2.2
>>  tools_3.4.1
>>
>>  [6] glue_1.1.1       tibble_1.3.3     Rcpp_0.12.11     stringi_1.1.5
>> pkgconfig_2.0.1
>>
>> [11] rlang_0.1.2      bindr_0.1
>>
>> This email and any attachments were sent from a Monsanto email account
>> and may contain confidential and/or privileged information. If you are not
>> the intended recipient, please contact the sender and delete this email and
>> any attachments immediately. Any unauthorized use, including disclosing,
>> printing, storing, copying or distributing this email, is prohibited. All
>> emails and attachments sent to or from Monsanto email accounts may be
>> subject to monitoring, reading, and archiving by Monsanto, including its
>> affiliates and subsidiaries, as permitted by applicable law. Thank you.
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From nilesh.dighe at monsanto.com  Thu Dec 14 16:01:10 2017
From: nilesh.dighe at monsanto.com (DIGHE, NILESH [AG/2362])
Date: Thu, 14 Dec 2017 15:01:10 +0000
Subject: [R] help with recursive function
In-Reply-To: <CAGgJW77AAvtOiCwLz=XEvsU+AuqHA=SK8RWLcBD5Ka7P8OFrPA@mail.gmail.com>
References: <CY1PR0101MB1018F79F34CFE3142A8592ADE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CAGgJW772Nk+Z3XamiGF-2r6Ta9Cud1VOhUvRr4p4k0epTX4xtA@mail.gmail.com>
 <CAGgJW77AAvtOiCwLz=XEvsU+AuqHA=SK8RWLcBD5Ka7P8OFrPA@mail.gmail.com>
Message-ID: <CY1PR0101MB1018FB7B65C9BDBE0ECA5DCBE20A0@CY1PR0101MB1018.prod.exchangelabs.com>

Eric:  Thanks for taking time to look into my problem.  Despite of making the change you suggested, I am still getting the same error.  I am wondering if the logic I am using in the stopifnot and if functions is a problem.
I like the recursive function to stop whenever the norm_sd column has zero values that are above or equal to 1. Below is the calclp function after the changes you suggested.
Thanks. Nilesh

dput(calclp)
function (dataset)
{
    dat1 <- funlp1(dataset)
    recursive_funlp <- function(dataset = dat1, func = funlp2) {
        dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
            mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
            spread(key = field_rep, value = lp) %>% mutate_at(.vars = grep("_",
            names(.)), funs(norm = round(scale(.), 3)))
        dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
            1, sd, na.rm = TRUE), 3)
        dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
            1, function(x) {
                max(abs(x), na.rm = TRUE)
            }), 3)
        stopifnot(!(any(dat2$norm_sd >= 1)))
        if (!(any(dat2$norm_sd >= 1))) {
            df1 <- dat1
            return(df1)
        }
        else {
            df2 <- recursive_funlp()
            return(df2)
        }
    }
    df3 <- recursive_funlp(dataset = dat1, func = funlp2)
    df3
}


From: Eric Berger [mailto:ericjberger at gmail.com]
Sent: Thursday, December 14, 2017 8:17 AM
To: DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] help with recursive function

My own typo ... whoops ...

!( any(dat2$norm_sd >= 1 ))



On Thu, Dec 14, 2017 at 3:43 PM, Eric Berger <ericjberger at gmail.com<mailto:ericjberger at gmail.com>> wrote:
You seem to have a typo at this expression (and some others like it)

Namely, you write

any(!dat2$norm_sd) >= 1

when you possibly meant to write

!( any(dat2$norm_sd) >= 1 )

i.e. I think your ! seems to be in the wrong place.

HTH,
Eric


On Thu, Dec 14, 2017 at 3:26 PM, DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com>> wrote:
Hi, I need some help with running a recursive function. I like to run funlp2 recursively.
When I try to run recursive function in another function named "calclp" I get this "Error: any(!dat2$norm_sd) >= 1 is not TRUE".

I have never built a recursive function before so having trouble executing it in this case.  I would appreciate any help or guidance to resolve this issue. Please see my data and the three functions that I am using below.
Please note that calclp is the function I am running and the other two functions are within this calclp function.

# code:
Test<- calclp(dataset = dat)

# calclp function

calclp<- function (dataset)

{

    dat1 <- funlp1(dataset)

    recursive_funlp <- function(dataset = dat1, func = funlp2) {

        dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%

            mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%

            spread(key = field_rep, value = lp) %>% mutate_at(.vars = grep("_",

            names(.)), funs(norm = round(scale(.), 3)))

        dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],

            1, sd, na.rm = TRUE), 3)

        dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],

            1, function(x) {

                max(abs(x), na.rm = TRUE)

            }), 3)

        stopifnot(any(!dat2$norm_sd) >= 1)

        if (any(!dat2$norm_sd) >= 1) {

            df1 <- dat1

            return(df1)

        }

        else {

            df2 <- recursive_funlp()

            return(df2)

        }

    }

    df3 <- recursive_funlp(dataset = dat1, func = funlp2)

    df3

}


# funlp1 function

funlp1<- function (dataset)

{

    dat2 <- dataset %>% select(field, set, ent_num, rep_num,

        lp) %>% unite(uniqueid, set, ent_num, sep = ".") %>%

        unite(field_rep, field, rep_num) %>% mutate(field_rep = paste(field_rep,

        "lp", sep = ".")) %>% spread(key = field_rep, value = lp) %>%

        mutate_at(.vars = grep("_", names(.)), funs(norm = round(scale(.),

            3)))

    dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],

        1, sd, na.rm = TRUE), 3)

    dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],

        1, function(x) {

            max(abs(x), na.rm = TRUE)

        }), 3)

    data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,

        -norm_sd) %>% separate(key, c("field_rep", "treatment"),

        "\\.<file://.>") %>% spread(treatment, value) %>% mutate(outlier = NA)

    df_clean <- with(data1, data1[norm_sd < 1, ])

    datD <- with(data1, data1[norm_sd >= 1, ])

    s <- split(datD, datD$uniqueid)

    sdf <- lapply(s, function(x) {

        data.frame(x, x$outlier <- ifelse(is.na<http://is.na>(x$lp_norm), NA,

            ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),

            x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))

        x

    })

    sdf2 <- bind_rows(sdf)

    all_dat <- bind_rows(df_clean, sdf2)

    all_dat

}


# funlp2 function

funlp2<-function (dataset)

{

    data1 <- dataset

    df_clean <- with(data1, data1[norm_sd < 1, ])

    datD <- with(data1, data1[norm_sd >= 1, ])

    s <- split(datD, datD$uniqueid)

    sdf <- lapply(s, function(x) {

        data.frame(x, x$outlier <- ifelse(is.na<http://is.na>(x$lp_norm), NA,

            ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),

            x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))

        x

    })

    sdf2 <- bind_rows(sdf)

    all_dat <- bind_rows(df_clean, sdf2)

    all_dat

}


# dataset
dput(dat)
structure(list(field = c("LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01"), set = c("seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta"), ent_num = c(23L, 14L, 43L, 30L, 44L, 60L, 17L,
34L, 41L, 40L, 9L, 36L, 38L, 19L, 61L, 51L, 45L, 42L, 3L, 39L,
21L, 11L, 12L, 7L, 35L, 5L, 70L, 47L, 28L, 16L, 72L, 13L, 49L,
67L, 56L, 32L, 27L, 46L, 24L, 63L, 15L, 66L, 26L, 29L, 48L, 1L,
54L, 37L, 2L, 50L, 52L, 31L, 33L, 25L, 6L, 69L, 53L, 10L, 18L,
55L, 59L, 4L, 58L, 22L, 20L, 64L, 71L, 57L, 11L, 17L, 43L, 13L,
1L, 16L, 34L, 47L, 52L, 72L, 59L, 22L, 54L, 18L, 25L, 61L, 56L,
41L, 27L, 14L, 49L, 19L, 29L, 31L, 64L, 6L, 53L, 35L, 37L, 67L,
39L, 51L, 40L, 15L, 69L, 60L, 38L, 4L, 23L, 3L, 48L, 32L, 42L,
24L, 28L, 33L, 57L, 8L, 21L, 46L, 7L, 30L, 45L, 2L, 63L, 36L,
68L, 20L, 66L, 70L, 58L, 5L, 10L, 12L, 62L, 50L, 71L, 9L, 55L,
26L, 44L, 65L, 14L, 63L, 46L, 58L, 62L, 19L, 59L, 2L, 5L, 6L,
40L, 21L, 44L, 37L, 55L, 35L, 71L, 56L, 10L, 36L, 53L, 25L, 61L,
12L, 26L, 23L, 4L, 13L, 28L, 38L, 57L, 54L, 72L, 48L, 66L, 9L,
70L, 15L, 39L, 60L, 17L, 34L, 51L, 67L, 42L, 49L, 31L, 30L, 3L,
18L, 65L, 32L, 27L, 52L, 22L, 11L, 47L, 64L, 8L, 43L, 41L, 16L,
20L, 33L, 7L, 50L, 68L, 24L, 1L, 69L, 45L, 29L, 37L, 30L, 55L,
54L, 43L, 32L, 21L, 27L, 33L, 40L, 67L, 57L, 68L, 31L, 17L, 13L,
6L, 62L, 19L, 22L, 3L, 10L, 44L, 34L, 69L, 70L, 4L, 1L, 25L,
11L, 51L, 5L, 63L, 71L, 12L, 38L, 58L, 39L, 49L, 59L, 56L, 65L,
2L, 64L, 8L, 35L, 46L, 45L, 29L, 53L, 36L, 42L, 23L, 18L, 50L,
26L, 14L, 48L, 66L, 20L, 24L, 7L, 15L, 53L, 22L, 39L, 20L, 60L,
59L, 43L, 19L, 41L, 6L, 62L, 1L, 55L, 34L, 50L, 38L, 40L, 44L,
4L, 46L, 29L, 65L, 57L, 48L, 33L, 69L, 14L, 35L, 67L, 72L, 54L,
3L, 49L, 2L, 12L, 18L, 30L, 10L, 70L, 31L, 15L, 63L, 71L, 21L,
45L, 28L, 56L, 27L, 64L, 61L, 51L, 5L, 24L, 68L, 25L, 66L, 16L,
36L, 58L, 37L, 52L, 26L, 9L, 42L, 7L, 11L, 8L, 32L, 23L, 13L,
47L, 17L, 61L, 72L, 47L, 60L, 16L, 9L, 28L, 52L, 41L, 1L, 61L,
6L, 23L, 58L, 63L, 25L, 28L, 30L, 36L, 62L, 9L, 32L, 19L, 31L,
56L, 45L, 2L, 22L, 27L, 40L, 14L, 11L, 50L, 13L, 70L, 20L, 64L,
39L, 26L, 21L, 43L, 29L, 35L, 54L, 52L, 37L, 17L, 16L, 72L, 48L,
12L, 18L, 44L, 42L, 49L, 68L, 5L, 55L, 69L, 51L, 66L, 59L, 53L,
15L, 71L, 41L, 57L, 4L, 60L, 8L, 7L, 33L, 34L, 24L, 10L, 67L,
47L, 38L, 3L, 65L, 46L, 20L, 34L, 71L, 1L, 33L, 57L, 13L, 21L,
66L, 29L, 3L, 61L, 69L, 24L, 62L, 39L, 49L, 47L, 31L, 53L, 52L,
43L, 17L, 7L, 8L, 12L, 60L, 63L, 50L, 2L, 51L, 46L, 10L, 23L,
48L, 11L, 26L, 40L, 70L, 42L, 59L, 15L, 56L, 58L, 27L, 6L, 35L,
4L, 37L, 5L, 65L, 44L, 28L, 14L, 32L, 36L, 45L, 9L, 18L, 55L,
68L, 30L, 54L, 41L, 25L, 22L, 38L, 16L, 67L, 64L, 19L, 72L, 68L,
28L, 33L, 15L, 51L, 4L, 47L, 36L, 8L, 57L, 48L, 1L, 52L, 39L,
32L, 50L, 13L, 30L, 63L, 2L, 9L, 62L, 22L, 6L, 61L, 16L, 53L,
38L, 37L, 20L, 69L, 44L, 56L, 29L, 26L, 14L, 17L, 46L, 66L, 58L,
42L, 60L, 19L, 45L, 3L, 59L, 70L, 31L, 24L, 55L, 40L, 43L, 25L,
65L, 12L, 67L, 21L, 7L, 27L, 49L, 72L, 54L, 41L, 23L, 34L, 5L,
64L, 35L, 18L, 71L, 11L, 24L, 19L, 38L, 14L, 4L, 56L, 5L, 54L,
34L, 64L, 55L, 33L, 69L, 71L, 52L, 61L, 48L, 23L, 43L, 41L, 20L,
39L, 11L, 63L, 36L, 22L, 9L, 25L, 27L, 51L, 53L, 37L, 57L, 13L,
18L, 64L, 22L, 53L, 16L, 5L, 28L, 60L, 31L, 11L, 29L, 45L, 59L,
72L, 49L, 67L, 13L, 20L, 3L, 42L, 44L, 69L, 33L, 38L, 15L, 70L,
35L, 48L, 26L, 56L, 19L, 39L, 43L, 40L, 14L, 2L, 68L, 51L, 12L,
47L, 10L, 55L, 23L, 4L, 71L, 41L, 50L, 7L, 24L, 61L, 27L, 54L,
46L, 58L, 37L, 66L, 57L, 1L, 36L, 32L, 18L, 62L, 9L, 30L, 21L,
6L, 52L, 8L, 65L, 17L, 25L, 63L, 34L, 65L, 22L, 56L, 9L, 7L,
11L, 31L, 4L, 63L, 29L, 61L, 54L, 12L, 62L, 59L, 5L, 23L, 53L,
36L, 24L, 35L, 66L, 49L, 72L, 18L, 70L, 32L, 43L, 20L, 45L, 34L,
46L, 28L, 6L, 44L, 71L, 39L, 13L, 27L, 1L, 58L, 30L, 68L, 17L,
33L, 26L, 57L, 15L, 21L, 52L, 48L, 42L, 16L, 40L, 38L, 8L, 69L,
2L, 51L, 67L, 55L, 64L, 47L, 60L, 19L, 41L, 50L, 3L, 14L, 25L,
10L, 37L, 6L, 15L, 45L, 49L, 8L, 17L, 50L, 16L, 58L, 72L, 26L,
60L, 7L, 32L, 1L, 46L, 66L, 68L, 62L, 47L, 35L, 70L, 10L, 31L,
65L, 2L, 3L, 21L, 12L, 30L, 40L, 28L, 59L, 42L, 67L, 44L, 29L
), rep_num = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L), lp = c(NA, 41.64, 38.8, 44.45, 40.54,
38.54, 41.94, 39.6, 37.39, 40.95, 38.45, 43.47, 41.66, 40.91,
42.68, 43.12, 38.22, 40.95, 46.24, 42.95, 38.95, 39.88, 40.57,
40.13, 38.57, 45.45, 40.78, 43.52, 39.75, 39.93, 40.35, 37.6,
37.7, 43.05, 43.32, 41.31, 39.03, 42.5, 43.18, 41.32, 39.58,
40.62, 39.64, 39.85, 38.75, 40.18, 41.44, 40.5, 40.87, 40.75,
38.37, 40.26, 35.11, 40.89, 41.67, 38.87, 37.32, 35.85, 38.25,
42.21, 43.15, 38.69, 38.8, 38.77, 37.98, 39.8, 33.37, 40.09,
42.87, 44.07, 43.78, 42.47, 42.8, 41.1, 41.17, 44.07, 44.24,
43.16, 46.12, 42.64, 43.92, 42.16, 43.69, 42.89, 42.63, 43.58,
44.4, 43.17, 43.3, 42.5, 42.6, 44.05, 44.63, 43.09, 45.17, 45.17,
42.78, 42.38, 45.76, 42.16, 44.22, 43.67, 40.45, 41.95, 42.49,
41.98, 45.15, 46.01, 38.51, 43.08, 45.19, 44.53, 43.14, 39.93,
46.84, 43.59, 41.68, 43.7, 44.63, 44.02, 42.07, 43.88, 43.2,
46.2, 40.84, 39.14, 43.89, 42.58, 41.53, 45.32, 39.56, 43.77,
45.45, 45.16, 43.4, 40.08, 42.27, 43.74, 43.77, 41.31, 41.38,
45.01, 45.76, 40.4, 48.39, 46.27, 15.44, 44.76, 47.45, 44.87,
46.8, 41.83, 43.03, 43.96, 42.51, 45.06, 45.55, 44.9, 42.47,
44.9, 45, 44.77, 43.79, 44.26, 44.57, 44.4, 43.42, 42.31, 47.18,
43.83, 45.72, 44.83, 44.96, 40.28, 42.85, 41.23, 45.23, 47.09,
43.61, 42.69, 46.27, 45.16, 44.14, 43.34, 45.97, 43.81, 43.01,
39.82, 45.91, 45.97, 43.61, 45.12, 46.37, 42.67, 42.47, 45.86,
44.19, 44.46, 42.64, 43.95, 44.93, 40.33, 42.75, 39.92, 44.17,
44.49, 41.51, 42.43, 44.14, 40.5, 41.29, 44.89, 37.98, 39.02,
39.62, 42.13, 39.03, 44.16, 39.15, 41.49, 42.63, 40.11, 39.97,
42.85, 35.98, 39.45, 40.99, 42.44, 42.11, 37.36, 40.63, 40.69,
43.57, 39.04, 39.3, 42.19, 36.88, 40.39, 37.78, 38.6, 40.2, 40.98,
36.58, 43.59, 42.49, 39.96, 39.84, 40.43, 38.94, 42.72, 39.43,
42.13, 40.36, 40.58, 40.01, 42.17, 42.17, 41.35, 43.27, 40.15,
39.76, 40.94, 40.87, 42.32, 41.81, 41.97, 43.72, 41.32, 40.83,
37.64, 41.03, 38.98, 40.61, 41.17, 41.96, 40.07, 38.48, 42.85,
38.68, 39.09, 42.16, 38.14, 37.99, 41.06, 37.4, 41.88, 39.35,
39.73, 38.38, 41.34, 40.67, 40.89, 39.28, 37.59, 39.3, 39.72,
40.79, 39.42, 34.5, 37.61, 37.76, 40.24, 41.17, 41.24, 42.14,
42.53, 39.71, 39.44, 40.19, 42.51, 40.15, 36.26, 37.48, 40.43,
37.5, 42.11, 41.44, 40.29, 39.75, 37.58, 41.25, 40.39, 41.63,
42.18, 43.71, 38.69, 43.47, 41.98, 35.51, 42.22, 38.51, 40.17,
39.4, 39.54, 41.7, 40.93, 40.01, 35.97, 41.44, 41.89, 40.9, 39.95,
40.69, 43.18, 37.03, 39.91, 36.75, 42.75, 41.61, 42.6, 38.55,
42.84, 38.41, 42.55, 41.44, 41.03, 40, 41.35, 42.18, 42.66, 40.06,
42.48, 41.33, 41.92, 40.88, 40.99, 42.78, 38.26, 40.81, 39.95,
37.89, 40.81, 38.33, 39.33, 39.67, 40.12, 41.75, 39.81, 41.92,
44.59, 37.8, 40.54, 40.49, 41.82, 42.13, 39.93, 39.94, 42.54,
41.74, 43.06, 41.72, 41.27, 39.42, 42.07, 40.06, 42.1, 38.91,
39.28, 42.94, 39.94, 41.86, 38.56, 38.15, 41.47, 42.34, 38.14,
40.19, 40.2, 43.01, 42.35, 40.89, 41.44, 42.89, 44.49, 39.1,
38.42, 37.44, 43.28, 37.96, 40.56, 41.53, 41.59, 41.45, 42.55,
40.6, 44.04, 39.31, 41.08, 37.14, 40.03, 41.49, 40.82, 38.47,
43.43, 38.82, 40.4, 41.09, 42.26, 41.85, 41.13, 37.27, 41.97,
45.24, 43.35, 40.53, 43.14, 39.71, 42.77, 42.2, 42.13, 41.91,
42.59, 41.99, 42.11, 40.43, 40.02, 43.53, 43.01, 39.55, 43.2,
39.95, 42.51, 42, 42.26, 42.39, 42.64, 39.9, 41.89, 43.05, 41.02,
41.95, 39.92, 43.42, 44.01, 42.6, 40.84, 42.86, 44.31, 42.24,
41.25, 42.38, 44.99, 41.24, 43.97, 41.12, 37.88, 41.53, 41.56,
39.18, 40.83, 42.96, 41.92, 43.86, 42.48, 42.65, 43.3, 41.99,
42.51, 40.65, 42.77, 34.71, 40.97, 38.15, 39.76, 36.74, 37.95,
39.17, 38.22, 39.31, 43.57, 37.21, 39.35, 42.37, 42.01, 42.39,
43.21, 36.91, 36.69, 41.07, 41.91, 34.63, 40.61, 36.23, 38.12,
40.76, 39.14, 41.81, 39.14, 41.04, 37.32, 40.83, 40.81, 38.29,
39.61, 40.96, 40.71, 40.47, 38.64, 39.76, 38.19, 39.05, 38.04,
41.14, 38.35, 42.3, 34.44, 40.93, 39.3, 41.44, 38.3, 42.74, 36.66,
40.02, 36.62, 40.48, 41.72, 41.23, 41.81, 42.07, 40.22, 37.83,
36.54, 37.77, 40.49, 38.65, 43.2, 43.32, 40.67, 41.95, 36.11,
39.28, 42.38, 40.35, 40.3, 43.48, 40.55, 40.54, 44.03, 41.94,
37.97, 41.98, 41.53, 38.19, 38.66, 41.18, 41.95, 42.53, 38.7,
44.55, 42.39, 41.55, 38.46, 42.27, 42.19, 41.95, 41.81, 39.81,
38.12, 42.94, 42.99, 38.85, 41.26, 43.13, 44.21, 38.54, 44.02,
43.46, 41.64, 42.06, 42.11, 42.34, 41.86, 37.91, 40.89, 40.5,
39.54, 37.87, 40.86, 41.36, 41.77, 42.03, 39.15, 40.04, 44.11,
41.34, 42.97, 38.42, 37.28, 41.04, 41.48, 38.82, 41.94, 37.95,
40.9, 40.39, 40.31, 43.19, 41.22, 41.49, 41.25, 40.07, 36.7,
39.97, 39.99, 41.7, 37.09, 42.58, 43.01, 37.7, 41.81, 39.99,
42.95, 43.19, 42.69, 41.5, 40.64, 43.24, 41.14, 41.21, 41.29,
41.43, 44.21, 38.52, 42.54, 40.54, 42.49, 43.2, 38.12, 40.08,
39.02, 41.45, 42.33, 41.11, 38.93, 41.63, 44.22, 41.41, 39.08,
40.9, 41.1, 43.88, 40.96, 46.75, 47.54, 40.35, 41.97, 44.94,
44.91, 44.66, 44.5, 44.4, 46.4, 47.97, 46.05, 45.57, 42.83, 41.48,
47.48, 45.43, 41.98, 43.14, 45.6, 44.78, 45.45, 45.69, 44.82,
44.24, 41.14, 43.14, 46.61, 43.92, 43.56, 43.68, 45.37, 45.15,
40.76, 43.78, 44.67, 41.36, 41.4, 40.97, 41.87, 39.83, 43.8,
48.36, 44.28, 43.29, 44.69, 43.26, 43.35, 44.34, 45.08, 42.26,
39.7, 42.4, 44.03, 43.22, 42.71, 45.89, 44.89, 44.81, 42.59,
40.85, 43.82, 44.85, 47.47, 43.64, 42.65, 45.67, 43.24, 42.33,
40.61, 38.3, 39.84, 41.08, 42.33, 44.44, 40.85, 39.58, 42.55,
41.75, 39.44, 41.79, 39.31, 41.34, 42.76, 40.79, 37.31, 42.85,
42.88, 42.01, 44.63, 38.85, 41.13, 40.43, 41.34, 43.14, 40.58,
42.21, 38.94, 44.88, 42.33, 42.61, 41.88, 41.13, 41.83, 42.8)), .Names = c("field",
"set", "ent_num", "rep_num", "lp"), class = "data.frame", row.names = c(NA,
-787L))

# session info

R version 3.4.1 (2017-06-30)

Platform: i386-w64-mingw32/i386 (32-bit)

Running under: Windows 7 x64 (build 7601) Service Pack 1



Matrix products: default



locale:

[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252

[3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C

[5] LC_TIME=English_United States.1252



attached base packages:

[1] stats     graphics  grDevices utils     datasets  methods   base



other attached packages:

[1] bindrcpp_0.2 tidyr_0.6.3  dplyr_0.7.4



loaded via a namespace (and not attached):

 [1] compiler_3.4.1   magrittr_1.5     assertthat_0.2.0 R6_2.2.2         tools_3.4.1

 [6] glue_1.1.1       tibble_1.3.3     Rcpp_0.12.11     stringi_1.1.5    pkgconfig_2.0.1

[11] rlang_0.1.2      bindr_0.1

This email and any attachments were sent from a Monsanto email account and may contain confidential and/or privileged information. If you are not the intended recipient, please contact the sender and delete this email and any attachments immediately. Any unauthorized use, including disclosing, printing, storing, copying or distributing this email, is prohibited. All emails and attachments sent to or from Monsanto email accounts may be subject to monitoring, reading, and archiving by Monsanto, including its affiliates and subsidiaries, as permitted by applicable law. Thank you.

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


This email and any attachments were sent from a Monsanto email account and may contain confidential and/or privileged information. If you are not the intended recipient, please contact the sender and delete this email and any attachments immediately. Any unauthorized use, including disclosing, printing, storing, copying or distributing this email, is prohibited. All emails and attachments sent to or from Monsanto email accounts may be subject to monitoring, reading, and archiving by Monsanto, including its affiliates and subsidiaries, as permitted by applicable law. Thank you.

	[[alternative HTML version deleted]]


From nilesh.dighe at monsanto.com  Thu Dec 14 16:31:07 2017
From: nilesh.dighe at monsanto.com (DIGHE, NILESH [AG/2362])
Date: Thu, 14 Dec 2017 15:31:07 +0000
Subject: [R] help with recursive function
In-Reply-To: <CY1PR0101MB1018FB7B65C9BDBE0ECA5DCBE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
References: <CY1PR0101MB1018F79F34CFE3142A8592ADE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CAGgJW772Nk+Z3XamiGF-2r6Ta9Cud1VOhUvRr4p4k0epTX4xtA@mail.gmail.com>
 <CAGgJW77AAvtOiCwLz=XEvsU+AuqHA=SK8RWLcBD5Ka7P8OFrPA@mail.gmail.com>
 <CY1PR0101MB1018FB7B65C9BDBE0ECA5DCBE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
Message-ID: <CY1PR0101MB1018B00187792A9F236E549EE20A0@CY1PR0101MB1018.prod.exchangelabs.com>

Hi, I accidently left out few lines of code from the calclp function.  Updated function is pasted below.
I am still getting the same error ?Error: !(any(data1$norm_sd >= 1)) is not TRUE?

I would appreciate any help.
Nilesh
dput(calclp)
function (dataset)
{
    dat1 <- funlp1(dataset)
    recursive_funlp <- function(dataset = dat1, func = funlp2) {
        dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
            mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
            spread(key = field_rep, value = lp) %>% mutate_at(.vars = grep("_",
            names(.)), funs(norm = round(scale(.), 3)))
        dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
            1, sd, na.rm = TRUE), 3)
        dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
            1, function(x) {
                max(abs(x), na.rm = TRUE)
            }), 3)
        data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,
            -norm_sd) %>% separate(key, c("field_rep", "treatment"),
            "\\.") %>% spread(treatment, value) %>% mutate(outlier = NA)
        stopifnot(!(any(data1$norm_sd >= 1)))
        if (!(any(data1$norm_sd >= 1))) {
            df1 <- dat1
            return(df1)
        }
       else {
            df2 <- recursive_funlp()
            return(df2)
        }
    }
    df3 <- recursive_funlp(dataset = dat1, func = funlp2)
    df3
}


From: DIGHE, NILESH [AG/2362]
Sent: Thursday, December 14, 2017 9:01 AM
To: 'Eric Berger' <ericjberger at gmail.com>
Cc: r-help <r-help at r-project.org>
Subject: RE: [R] help with recursive function

Eric:  Thanks for taking time to look into my problem.  Despite of making the change you suggested, I am still getting the same error.  I am wondering if the logic I am using in the stopifnot and if functions is a problem.
I like the recursive function to stop whenever the norm_sd column has zero values that are above or equal to 1. Below is the calclp function after the changes you suggested.
Thanks. Nilesh

dput(calclp)
function (dataset)
{
    dat1 <- funlp1(dataset)
    recursive_funlp <- function(dataset = dat1, func = funlp2) {
        dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
            mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
            spread(key = field_rep, value = lp) %>% mutate_at(.vars = grep("_",
            names(.)), funs(norm = round(scale(.), 3)))
        dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
            1, sd, na.rm = TRUE), 3)
        dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
            1, function(x) {
                max(abs(x), na.rm = TRUE)
            }), 3)
        stopifnot(!(any(dat2$norm_sd >= 1)))
        if (!(any(dat2$norm_sd >= 1))) {
            df1 <- dat1
            return(df1)
        }
        else {
            df2 <- recursive_funlp()
            return(df2)
        }
    }
    df3 <- recursive_funlp(dataset = dat1, func = funlp2)
    df3
}


From: Eric Berger [mailto:ericjberger at gmail.com]
Sent: Thursday, December 14, 2017 8:17 AM
To: DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com>>
Cc: r-help <r-help at r-project.org<mailto:r-help at r-project.org>>
Subject: Re: [R] help with recursive function

My own typo ... whoops ...

!( any(dat2$norm_sd >= 1 ))



On Thu, Dec 14, 2017 at 3:43 PM, Eric Berger <ericjberger at gmail.com<mailto:ericjberger at gmail.com>> wrote:
You seem to have a typo at this expression (and some others like it)

Namely, you write

any(!dat2$norm_sd) >= 1

when you possibly meant to write

!( any(dat2$norm_sd) >= 1 )

i.e. I think your ! seems to be in the wrong place.

HTH,
Eric


On Thu, Dec 14, 2017 at 3:26 PM, DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com>> wrote:
Hi, I need some help with running a recursive function. I like to run funlp2 recursively.
When I try to run recursive function in another function named "calclp" I get this "Error: any(!dat2$norm_sd) >= 1 is not TRUE".

I have never built a recursive function before so having trouble executing it in this case.  I would appreciate any help or guidance to resolve this issue. Please see my data and the three functions that I am using below.
Please note that calclp is the function I am running and the other two functions are within this calclp function.

# code:
Test<- calclp(dataset = dat)

# calclp function

calclp<- function (dataset)

{

    dat1 <- funlp1(dataset)

    recursive_funlp <- function(dataset = dat1, func = funlp2) {

        dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%

            mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%

            spread(key = field_rep, value = lp) %>% mutate_at(.vars = grep("_",

            names(.)), funs(norm = round(scale(.), 3)))

        dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],

            1, sd, na.rm = TRUE), 3)

        dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],

            1, function(x) {

                max(abs(x), na.rm = TRUE)

            }), 3)

        stopifnot(any(!dat2$norm_sd) >= 1)

        if (any(!dat2$norm_sd) >= 1) {

            df1 <- dat1

            return(df1)

        }

        else {

            df2 <- recursive_funlp()

            return(df2)

        }

    }

    df3 <- recursive_funlp(dataset = dat1, func = funlp2)

    df3

}


# funlp1 function

funlp1<- function (dataset)

{

    dat2 <- dataset %>% select(field, set, ent_num, rep_num,

        lp) %>% unite(uniqueid, set, ent_num, sep = ".") %>%

        unite(field_rep, field, rep_num) %>% mutate(field_rep = paste(field_rep,

        "lp", sep = ".")) %>% spread(key = field_rep, value = lp) %>%

        mutate_at(.vars = grep("_", names(.)), funs(norm = round(scale(.),

            3)))

    dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],

        1, sd, na.rm = TRUE), 3)

    dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],

        1, function(x) {

            max(abs(x), na.rm = TRUE)

        }), 3)

    data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,

        -norm_sd) %>% separate(key, c("field_rep", "treatment"),

        "\\.<file://.>") %>% spread(treatment, value) %>% mutate(outlier = NA)

    df_clean <- with(data1, data1[norm_sd < 1, ])

    datD <- with(data1, data1[norm_sd >= 1, ])

    s <- split(datD, datD$uniqueid)

    sdf <- lapply(s, function(x) {

        data.frame(x, x$outlier <- ifelse(is.na<http://is.na>(x$lp_norm), NA,

            ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),

            x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))

        x

    })

    sdf2 <- bind_rows(sdf)

    all_dat <- bind_rows(df_clean, sdf2)

    all_dat

}


# funlp2 function

funlp2<-function (dataset)

{

    data1 <- dataset

    df_clean <- with(data1, data1[norm_sd < 1, ])

    datD <- with(data1, data1[norm_sd >= 1, ])

    s <- split(datD, datD$uniqueid)

    sdf <- lapply(s, function(x) {

        data.frame(x, x$outlier <- ifelse(is.na<http://is.na>(x$lp_norm), NA,

            ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),

            x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))

        x

    })

    sdf2 <- bind_rows(sdf)

    all_dat <- bind_rows(df_clean, sdf2)

    all_dat

}


# dataset
dput(dat)
structure(list(field = c("LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01"), set = c("seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta"), ent_num = c(23L, 14L, 43L, 30L, 44L, 60L, 17L,
34L, 41L, 40L, 9L, 36L, 38L, 19L, 61L, 51L, 45L, 42L, 3L, 39L,
21L, 11L, 12L, 7L, 35L, 5L, 70L, 47L, 28L, 16L, 72L, 13L, 49L,
67L, 56L, 32L, 27L, 46L, 24L, 63L, 15L, 66L, 26L, 29L, 48L, 1L,
54L, 37L, 2L, 50L, 52L, 31L, 33L, 25L, 6L, 69L, 53L, 10L, 18L,
55L, 59L, 4L, 58L, 22L, 20L, 64L, 71L, 57L, 11L, 17L, 43L, 13L,
1L, 16L, 34L, 47L, 52L, 72L, 59L, 22L, 54L, 18L, 25L, 61L, 56L,
41L, 27L, 14L, 49L, 19L, 29L, 31L, 64L, 6L, 53L, 35L, 37L, 67L,
39L, 51L, 40L, 15L, 69L, 60L, 38L, 4L, 23L, 3L, 48L, 32L, 42L,
24L, 28L, 33L, 57L, 8L, 21L, 46L, 7L, 30L, 45L, 2L, 63L, 36L,
68L, 20L, 66L, 70L, 58L, 5L, 10L, 12L, 62L, 50L, 71L, 9L, 55L,
26L, 44L, 65L, 14L, 63L, 46L, 58L, 62L, 19L, 59L, 2L, 5L, 6L,
40L, 21L, 44L, 37L, 55L, 35L, 71L, 56L, 10L, 36L, 53L, 25L, 61L,
12L, 26L, 23L, 4L, 13L, 28L, 38L, 57L, 54L, 72L, 48L, 66L, 9L,
70L, 15L, 39L, 60L, 17L, 34L, 51L, 67L, 42L, 49L, 31L, 30L, 3L,
18L, 65L, 32L, 27L, 52L, 22L, 11L, 47L, 64L, 8L, 43L, 41L, 16L,
20L, 33L, 7L, 50L, 68L, 24L, 1L, 69L, 45L, 29L, 37L, 30L, 55L,
54L, 43L, 32L, 21L, 27L, 33L, 40L, 67L, 57L, 68L, 31L, 17L, 13L,
6L, 62L, 19L, 22L, 3L, 10L, 44L, 34L, 69L, 70L, 4L, 1L, 25L,
11L, 51L, 5L, 63L, 71L, 12L, 38L, 58L, 39L, 49L, 59L, 56L, 65L,
2L, 64L, 8L, 35L, 46L, 45L, 29L, 53L, 36L, 42L, 23L, 18L, 50L,
26L, 14L, 48L, 66L, 20L, 24L, 7L, 15L, 53L, 22L, 39L, 20L, 60L,
59L, 43L, 19L, 41L, 6L, 62L, 1L, 55L, 34L, 50L, 38L, 40L, 44L,
4L, 46L, 29L, 65L, 57L, 48L, 33L, 69L, 14L, 35L, 67L, 72L, 54L,
3L, 49L, 2L, 12L, 18L, 30L, 10L, 70L, 31L, 15L, 63L, 71L, 21L,
45L, 28L, 56L, 27L, 64L, 61L, 51L, 5L, 24L, 68L, 25L, 66L, 16L,
36L, 58L, 37L, 52L, 26L, 9L, 42L, 7L, 11L, 8L, 32L, 23L, 13L,
47L, 17L, 61L, 72L, 47L, 60L, 16L, 9L, 28L, 52L, 41L, 1L, 61L,
6L, 23L, 58L, 63L, 25L, 28L, 30L, 36L, 62L, 9L, 32L, 19L, 31L,
56L, 45L, 2L, 22L, 27L, 40L, 14L, 11L, 50L, 13L, 70L, 20L, 64L,
39L, 26L, 21L, 43L, 29L, 35L, 54L, 52L, 37L, 17L, 16L, 72L, 48L,
12L, 18L, 44L, 42L, 49L, 68L, 5L, 55L, 69L, 51L, 66L, 59L, 53L,
15L, 71L, 41L, 57L, 4L, 60L, 8L, 7L, 33L, 34L, 24L, 10L, 67L,
47L, 38L, 3L, 65L, 46L, 20L, 34L, 71L, 1L, 33L, 57L, 13L, 21L,
66L, 29L, 3L, 61L, 69L, 24L, 62L, 39L, 49L, 47L, 31L, 53L, 52L,
43L, 17L, 7L, 8L, 12L, 60L, 63L, 50L, 2L, 51L, 46L, 10L, 23L,
48L, 11L, 26L, 40L, 70L, 42L, 59L, 15L, 56L, 58L, 27L, 6L, 35L,
4L, 37L, 5L, 65L, 44L, 28L, 14L, 32L, 36L, 45L, 9L, 18L, 55L,
68L, 30L, 54L, 41L, 25L, 22L, 38L, 16L, 67L, 64L, 19L, 72L, 68L,
28L, 33L, 15L, 51L, 4L, 47L, 36L, 8L, 57L, 48L, 1L, 52L, 39L,
32L, 50L, 13L, 30L, 63L, 2L, 9L, 62L, 22L, 6L, 61L, 16L, 53L,
38L, 37L, 20L, 69L, 44L, 56L, 29L, 26L, 14L, 17L, 46L, 66L, 58L,
42L, 60L, 19L, 45L, 3L, 59L, 70L, 31L, 24L, 55L, 40L, 43L, 25L,
65L, 12L, 67L, 21L, 7L, 27L, 49L, 72L, 54L, 41L, 23L, 34L, 5L,
64L, 35L, 18L, 71L, 11L, 24L, 19L, 38L, 14L, 4L, 56L, 5L, 54L,
34L, 64L, 55L, 33L, 69L, 71L, 52L, 61L, 48L, 23L, 43L, 41L, 20L,
39L, 11L, 63L, 36L, 22L, 9L, 25L, 27L, 51L, 53L, 37L, 57L, 13L,
18L, 64L, 22L, 53L, 16L, 5L, 28L, 60L, 31L, 11L, 29L, 45L, 59L,
72L, 49L, 67L, 13L, 20L, 3L, 42L, 44L, 69L, 33L, 38L, 15L, 70L,
35L, 48L, 26L, 56L, 19L, 39L, 43L, 40L, 14L, 2L, 68L, 51L, 12L,
47L, 10L, 55L, 23L, 4L, 71L, 41L, 50L, 7L, 24L, 61L, 27L, 54L,
46L, 58L, 37L, 66L, 57L, 1L, 36L, 32L, 18L, 62L, 9L, 30L, 21L,
6L, 52L, 8L, 65L, 17L, 25L, 63L, 34L, 65L, 22L, 56L, 9L, 7L,
11L, 31L, 4L, 63L, 29L, 61L, 54L, 12L, 62L, 59L, 5L, 23L, 53L,
36L, 24L, 35L, 66L, 49L, 72L, 18L, 70L, 32L, 43L, 20L, 45L, 34L,
46L, 28L, 6L, 44L, 71L, 39L, 13L, 27L, 1L, 58L, 30L, 68L, 17L,
33L, 26L, 57L, 15L, 21L, 52L, 48L, 42L, 16L, 40L, 38L, 8L, 69L,
2L, 51L, 67L, 55L, 64L, 47L, 60L, 19L, 41L, 50L, 3L, 14L, 25L,
10L, 37L, 6L, 15L, 45L, 49L, 8L, 17L, 50L, 16L, 58L, 72L, 26L,
60L, 7L, 32L, 1L, 46L, 66L, 68L, 62L, 47L, 35L, 70L, 10L, 31L,
65L, 2L, 3L, 21L, 12L, 30L, 40L, 28L, 59L, 42L, 67L, 44L, 29L
), rep_num = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L), lp = c(NA, 41.64, 38.8, 44.45, 40.54,
38.54, 41.94, 39.6, 37.39, 40.95, 38.45, 43.47, 41.66, 40.91,
42.68, 43.12, 38.22, 40.95, 46.24, 42.95, 38.95, 39.88, 40.57,
40.13, 38.57, 45.45, 40.78, 43.52, 39.75, 39.93, 40.35, 37.6,
37.7, 43.05, 43.32, 41.31, 39.03, 42.5, 43.18, 41.32, 39.58,
40.62, 39.64, 39.85, 38.75, 40.18, 41.44, 40.5, 40.87, 40.75,
38.37, 40.26, 35.11, 40.89, 41.67, 38.87, 37.32, 35.85, 38.25,
42.21, 43.15, 38.69, 38.8, 38.77, 37.98, 39.8, 33.37, 40.09,
42.87, 44.07, 43.78, 42.47, 42.8, 41.1, 41.17, 44.07, 44.24,
43.16, 46.12, 42.64, 43.92, 42.16, 43.69, 42.89, 42.63, 43.58,
44.4, 43.17, 43.3, 42.5, 42.6, 44.05, 44.63, 43.09, 45.17, 45.17,
42.78, 42.38, 45.76, 42.16, 44.22, 43.67, 40.45, 41.95, 42.49,
41.98, 45.15, 46.01, 38.51, 43.08, 45.19, 44.53, 43.14, 39.93,
46.84, 43.59, 41.68, 43.7, 44.63, 44.02, 42.07, 43.88, 43.2,
46.2, 40.84, 39.14, 43.89, 42.58, 41.53, 45.32, 39.56, 43.77,
45.45, 45.16, 43.4, 40.08, 42.27, 43.74, 43.77, 41.31, 41.38,
45.01, 45.76, 40.4, 48.39, 46.27, 15.44, 44.76, 47.45, 44.87,
46.8, 41.83, 43.03, 43.96, 42.51, 45.06, 45.55, 44.9, 42.47,
44.9, 45, 44.77, 43.79, 44.26, 44.57, 44.4, 43.42, 42.31, 47.18,
43.83, 45.72, 44.83, 44.96, 40.28, 42.85, 41.23, 45.23, 47.09,
43.61, 42.69, 46.27, 45.16, 44.14, 43.34, 45.97, 43.81, 43.01,
39.82, 45.91, 45.97, 43.61, 45.12, 46.37, 42.67, 42.47, 45.86,
44.19, 44.46, 42.64, 43.95, 44.93, 40.33, 42.75, 39.92, 44.17,
44.49, 41.51, 42.43, 44.14, 40.5, 41.29, 44.89, 37.98, 39.02,
39.62, 42.13, 39.03, 44.16, 39.15, 41.49, 42.63, 40.11, 39.97,
42.85, 35.98, 39.45, 40.99, 42.44, 42.11, 37.36, 40.63, 40.69,
43.57, 39.04, 39.3, 42.19, 36.88, 40.39, 37.78, 38.6, 40.2, 40.98,
36.58, 43.59, 42.49, 39.96, 39.84, 40.43, 38.94, 42.72, 39.43,
42.13, 40.36, 40.58, 40.01, 42.17, 42.17, 41.35, 43.27, 40.15,
39.76, 40.94, 40.87, 42.32, 41.81, 41.97, 43.72, 41.32, 40.83,
37.64, 41.03, 38.98, 40.61, 41.17, 41.96, 40.07, 38.48, 42.85,
38.68, 39.09, 42.16, 38.14, 37.99, 41.06, 37.4, 41.88, 39.35,
39.73, 38.38, 41.34, 40.67, 40.89, 39.28, 37.59, 39.3, 39.72,
40.79, 39.42, 34.5, 37.61, 37.76, 40.24, 41.17, 41.24, 42.14,
42.53, 39.71, 39.44, 40.19, 42.51, 40.15, 36.26, 37.48, 40.43,
37.5, 42.11, 41.44, 40.29, 39.75, 37.58, 41.25, 40.39, 41.63,
42.18, 43.71, 38.69, 43.47, 41.98, 35.51, 42.22, 38.51, 40.17,
39.4, 39.54, 41.7, 40.93, 40.01, 35.97, 41.44, 41.89, 40.9, 39.95,
40.69, 43.18, 37.03, 39.91, 36.75, 42.75, 41.61, 42.6, 38.55,
42.84, 38.41, 42.55, 41.44, 41.03, 40, 41.35, 42.18, 42.66, 40.06,
42.48, 41.33, 41.92, 40.88, 40.99, 42.78, 38.26, 40.81, 39.95,
37.89, 40.81, 38.33, 39.33, 39.67, 40.12, 41.75, 39.81, 41.92,
44.59, 37.8, 40.54, 40.49, 41.82, 42.13, 39.93, 39.94, 42.54,
41.74, 43.06, 41.72, 41.27, 39.42, 42.07, 40.06, 42.1, 38.91,
39.28, 42.94, 39.94, 41.86, 38.56, 38.15, 41.47, 42.34, 38.14,
40.19, 40.2, 43.01, 42.35, 40.89, 41.44, 42.89, 44.49, 39.1,
38.42, 37.44, 43.28, 37.96, 40.56, 41.53, 41.59, 41.45, 42.55,
40.6, 44.04, 39.31, 41.08, 37.14, 40.03, 41.49, 40.82, 38.47,
43.43, 38.82, 40.4, 41.09, 42.26, 41.85, 41.13, 37.27, 41.97,
45.24, 43.35, 40.53, 43.14, 39.71, 42.77, 42.2, 42.13, 41.91,
42.59, 41.99, 42.11, 40.43, 40.02, 43.53, 43.01, 39.55, 43.2,
39.95, 42.51, 42, 42.26, 42.39, 42.64, 39.9, 41.89, 43.05, 41.02,
41.95, 39.92, 43.42, 44.01, 42.6, 40.84, 42.86, 44.31, 42.24,
41.25, 42.38, 44.99, 41.24, 43.97, 41.12, 37.88, 41.53, 41.56,
39.18, 40.83, 42.96, 41.92, 43.86, 42.48, 42.65, 43.3, 41.99,
42.51, 40.65, 42.77, 34.71, 40.97, 38.15, 39.76, 36.74, 37.95,
39.17, 38.22, 39.31, 43.57, 37.21, 39.35, 42.37, 42.01, 42.39,
43.21, 36.91, 36.69, 41.07, 41.91, 34.63, 40.61, 36.23, 38.12,
40.76, 39.14, 41.81, 39.14, 41.04, 37.32, 40.83, 40.81, 38.29,
39.61, 40.96, 40.71, 40.47, 38.64, 39.76, 38.19, 39.05, 38.04,
41.14, 38.35, 42.3, 34.44, 40.93, 39.3, 41.44, 38.3, 42.74, 36.66,
40.02, 36.62, 40.48, 41.72, 41.23, 41.81, 42.07, 40.22, 37.83,
36.54, 37.77, 40.49, 38.65, 43.2, 43.32, 40.67, 41.95, 36.11,
39.28, 42.38, 40.35, 40.3, 43.48, 40.55, 40.54, 44.03, 41.94,
37.97, 41.98, 41.53, 38.19, 38.66, 41.18, 41.95, 42.53, 38.7,
44.55, 42.39, 41.55, 38.46, 42.27, 42.19, 41.95, 41.81, 39.81,
38.12, 42.94, 42.99, 38.85, 41.26, 43.13, 44.21, 38.54, 44.02,
43.46, 41.64, 42.06, 42.11, 42.34, 41.86, 37.91, 40.89, 40.5,
39.54, 37.87, 40.86, 41.36, 41.77, 42.03, 39.15, 40.04, 44.11,
41.34, 42.97, 38.42, 37.28, 41.04, 41.48, 38.82, 41.94, 37.95,
40.9, 40.39, 40.31, 43.19, 41.22, 41.49, 41.25, 40.07, 36.7,
39.97, 39.99, 41.7, 37.09, 42.58, 43.01, 37.7, 41.81, 39.99,
42.95, 43.19, 42.69, 41.5, 40.64, 43.24, 41.14, 41.21, 41.29,
41.43, 44.21, 38.52, 42.54, 40.54, 42.49, 43.2, 38.12, 40.08,
39.02, 41.45, 42.33, 41.11, 38.93, 41.63, 44.22, 41.41, 39.08,
40.9, 41.1, 43.88, 40.96, 46.75, 47.54, 40.35, 41.97, 44.94,
44.91, 44.66, 44.5, 44.4, 46.4, 47.97, 46.05, 45.57, 42.83, 41.48,
47.48, 45.43, 41.98, 43.14, 45.6, 44.78, 45.45, 45.69, 44.82,
44.24, 41.14, 43.14, 46.61, 43.92, 43.56, 43.68, 45.37, 45.15,
40.76, 43.78, 44.67, 41.36, 41.4, 40.97, 41.87, 39.83, 43.8,
48.36, 44.28, 43.29, 44.69, 43.26, 43.35, 44.34, 45.08, 42.26,
39.7, 42.4, 44.03, 43.22, 42.71, 45.89, 44.89, 44.81, 42.59,
40.85, 43.82, 44.85, 47.47, 43.64, 42.65, 45.67, 43.24, 42.33,
40.61, 38.3, 39.84, 41.08, 42.33, 44.44, 40.85, 39.58, 42.55,
41.75, 39.44, 41.79, 39.31, 41.34, 42.76, 40.79, 37.31, 42.85,
42.88, 42.01, 44.63, 38.85, 41.13, 40.43, 41.34, 43.14, 40.58,
42.21, 38.94, 44.88, 42.33, 42.61, 41.88, 41.13, 41.83, 42.8)), .Names = c("field",
"set", "ent_num", "rep_num", "lp"), class = "data.frame", row.names = c(NA,
-787L))

# session info

R version 3.4.1 (2017-06-30)

Platform: i386-w64-mingw32/i386 (32-bit)

Running under: Windows 7 x64 (build 7601) Service Pack 1



Matrix products: default



locale:

[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252

[3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C

[5] LC_TIME=English_United States.1252



attached base packages:

[1] stats     graphics  grDevices utils     datasets  methods   base



other attached packages:

[1] bindrcpp_0.2 tidyr_0.6.3  dplyr_0.7.4



loaded via a namespace (and not attached):

 [1] compiler_3.4.1   magrittr_1.5     assertthat_0.2.0 R6_2.2.2         tools_3.4.1

 [6] glue_1.1.1       tibble_1.3.3     Rcpp_0.12.11     stringi_1.1.5    pkgconfig_2.0.1

[11] rlang_0.1.2      bindr_0.1

This email and any attachments were sent from a Monsanto email account and may contain confidential and/or privileged information. If you are not the intended recipient, please contact the sender and delete this email and any attachments immediately. Any unauthorized use, including disclosing, printing, storing, copying or distributing this email, is prohibited. All emails and attachments sent to or from Monsanto email accounts may be subject to monitoring, reading, and archiving by Monsanto, including its affiliates and subsidiaries, as permitted by applicable law. Thank you.

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


This email and any attachments were sent from a Monsanto email account and may contain confidential and/or privileged information. If you are not the intended recipient, please contact the sender and delete this email and any attachments immediately. Any unauthorized use, including disclosing, printing, storing, copying or distributing this email, is prohibited. All emails and attachments sent to or from Monsanto email accounts may be subject to monitoring, reading, and archiving by Monsanto, including its affiliates and subsidiaries, as permitted by applicable law. Thank you.

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Thu Dec 14 16:33:53 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Thu, 14 Dec 2017 17:33:53 +0200
Subject: [R] help with recursive function
In-Reply-To: <CY1PR0101MB1018B00187792A9F236E549EE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
References: <CY1PR0101MB1018F79F34CFE3142A8592ADE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CAGgJW772Nk+Z3XamiGF-2r6Ta9Cud1VOhUvRr4p4k0epTX4xtA@mail.gmail.com>
 <CAGgJW77AAvtOiCwLz=XEvsU+AuqHA=SK8RWLcBD5Ka7P8OFrPA@mail.gmail.com>
 <CY1PR0101MB1018FB7B65C9BDBE0ECA5DCBE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CY1PR0101MB1018B00187792A9F236E549EE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
Message-ID: <CAGgJW76zkxSQjH+XafCF58xoK8qYT9O0AfijT8cTxFZkYzQpPg@mail.gmail.com>

The message is coming from your stopifnot() condition being met.


On Thu, Dec 14, 2017 at 5:31 PM, DIGHE, NILESH [AG/2362] <
nilesh.dighe at monsanto.com> wrote:

> Hi, I accidently left out few lines of code from the calclp function.
> Updated function is pasted below.
>
> I am still getting the same error ?Error: !(any(data1$norm_sd >= 1)) is
> not TRUE?
>
>
>
> I would appreciate any help.
>
> Nilesh
>
> dput(calclp)
>
> function (dataset)
>
> {
>
>     dat1 <- funlp1(dataset)
>
>     recursive_funlp <- function(dataset = dat1, func = funlp2) {
>
>         dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
>
>             mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
>
>             spread(key = field_rep, value = lp) %>% mutate_at(.vars =
> grep("_",
>
>             names(.)), funs(norm = round(scale(.), 3)))
>
>         dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>             1, sd, na.rm = TRUE), 3)
>
>         dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>             1, function(x) {
>
>                 max(abs(x), na.rm = TRUE)
>
>             }), 3)
>
>         data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,
>
>             -norm_sd) %>% separate(key, c("field_rep", "treatment"),
>
>             "\\.") %>% spread(treatment, value) %>% mutate(outlier = NA)
>
>         stopifnot(!(any(data1$norm_sd >= 1)))
>
>         if (!(any(data1$norm_sd >= 1))) {
>
>             df1 <- dat1
>
>             return(df1)
>
>         }
>
>        else {
>
>             df2 <- recursive_funlp()
>
>             return(df2)
>
>         }
>
>     }
>
>     df3 <- recursive_funlp(dataset = dat1, func = funlp2)
>
>     df3
>
> }
>
>
>
>
>
> *From:* DIGHE, NILESH [AG/2362]
> *Sent:* Thursday, December 14, 2017 9:01 AM
> *To:* 'Eric Berger' <ericjberger at gmail.com>
> *Cc:* r-help <r-help at r-project.org>
> *Subject:* RE: [R] help with recursive function
>
>
>
> Eric:  Thanks for taking time to look into my problem.  Despite of making
> the change you suggested, I am still getting the same error.  I am
> wondering if the logic I am using in the stopifnot and if functions is a
> problem.
>
> I like the recursive function to stop whenever the norm_sd column has zero
> values that are above or equal to 1. Below is the calclp function after the
> changes you suggested.
>
> Thanks. Nilesh
>
>
>
> dput(calclp)
>
> function (dataset)
>
> {
>
>     dat1 <- funlp1(dataset)
>
>     recursive_funlp <- function(dataset = dat1, func = funlp2) {
>
>         dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
>
>             mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
>
>             spread(key = field_rep, value = lp) %>% mutate_at(.vars =
> grep("_",
>
>             names(.)), funs(norm = round(scale(.), 3)))
>
>         dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>             1, sd, na.rm = TRUE), 3)
>
>         dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>             1, function(x) {
>
>                 max(abs(x), na.rm = TRUE)
>
>             }), 3)
>
>         stopifnot(!(any(dat2$norm_sd >= 1)))
>
>         if (!(any(dat2$norm_sd >= 1))) {
>
>             df1 <- dat1
>
>             return(df1)
>
>         }
>
>         else {
>
>             df2 <- recursive_funlp()
>
>             return(df2)
>
>         }
>
>     }
>
>     df3 <- recursive_funlp(dataset = dat1, func = funlp2)
>
>     df3
>
> }
>
>
>
>
>
> *From:* Eric Berger [mailto:ericjberger at gmail.com <ericjberger at gmail.com>]
>
> *Sent:* Thursday, December 14, 2017 8:17 AM
> *To:* DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com>
> *Cc:* r-help <r-help at r-project.org>
> *Subject:* Re: [R] help with recursive function
>
>
>
> My own typo ... whoops ...
>
>
>
> !( any(dat2$norm_sd >= 1 ))
>
>
>
>
>
>
>
> On Thu, Dec 14, 2017 at 3:43 PM, Eric Berger <ericjberger at gmail.com>
> wrote:
>
> You seem to have a typo at this expression (and some others like it)
>
>
>
> Namely, you write
>
>
>
> any(!dat2$norm_sd) >= 1
>
>
>
> when you possibly meant to write
>
>
>
> !( any(dat2$norm_sd) >= 1 )
>
>
>
> i.e. I think your ! seems to be in the wrong place.
>
>
>
> HTH,
> Eric
>
>
>
>
>
> On Thu, Dec 14, 2017 at 3:26 PM, DIGHE, NILESH [AG/2362] <
> nilesh.dighe at monsanto.com> wrote:
>
> Hi, I need some help with running a recursive function. I like to run
> funlp2 recursively.
> When I try to run recursive function in another function named "calclp" I
> get this "Error: any(!dat2$norm_sd) >= 1 is not TRUE".
>
> I have never built a recursive function before so having trouble executing
> it in this case.  I would appreciate any help or guidance to resolve this
> issue. Please see my data and the three functions that I am using below.
> Please note that calclp is the function I am running and the other two
> functions are within this calclp function.
>
> # code:
> Test<- calclp(dataset = dat)
>
> # calclp function
>
> calclp<- function (dataset)
>
> {
>
>     dat1 <- funlp1(dataset)
>
>     recursive_funlp <- function(dataset = dat1, func = funlp2) {
>
>         dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
>
>             mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
>
>             spread(key = field_rep, value = lp) %>% mutate_at(.vars =
> grep("_",
>
>             names(.)), funs(norm = round(scale(.), 3)))
>
>         dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>             1, sd, na.rm = TRUE), 3)
>
>         dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>             1, function(x) {
>
>                 max(abs(x), na.rm = TRUE)
>
>             }), 3)
>
>         stopifnot(any(!dat2$norm_sd) >= 1)
>
>         if (any(!dat2$norm_sd) >= 1) {
>
>             df1 <- dat1
>
>             return(df1)
>
>         }
>
>         else {
>
>             df2 <- recursive_funlp()
>
>             return(df2)
>
>         }
>
>     }
>
>     df3 <- recursive_funlp(dataset = dat1, func = funlp2)
>
>     df3
>
> }
>
>
> # funlp1 function
>
> funlp1<- function (dataset)
>
> {
>
>     dat2 <- dataset %>% select(field, set, ent_num, rep_num,
>
>         lp) %>% unite(uniqueid, set, ent_num, sep = ".") %>%
>
>         unite(field_rep, field, rep_num) %>% mutate(field_rep =
> paste(field_rep,
>
>         "lp", sep = ".")) %>% spread(key = field_rep, value = lp) %>%
>
>         mutate_at(.vars = grep("_", names(.)), funs(norm = round(scale(.),
>
>             3)))
>
>     dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>         1, sd, na.rm = TRUE), 3)
>
>     dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>         1, function(x) {
>
>             max(abs(x), na.rm = TRUE)
>
>         }), 3)
>
>     data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,
>
>         -norm_sd) %>% separate(key, c("field_rep", "treatment"),
>
>         "\\.") %>% spread(treatment, value) %>% mutate(outlier = NA)
>
>     df_clean <- with(data1, data1[norm_sd < 1, ])
>
>     datD <- with(data1, data1[norm_sd >= 1, ])
>
>     s <- split(datD, datD$uniqueid)
>
>     sdf <- lapply(s, function(x) {
>
>         data.frame(x, x$outlier <- ifelse(is.na(x$lp_norm), NA,
>
>             ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),
>
>             x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))
>
>         x
>
>     })
>
>     sdf2 <- bind_rows(sdf)
>
>     all_dat <- bind_rows(df_clean, sdf2)
>
>     all_dat
>
> }
>
>
> # funlp2 function
>
> funlp2<-function (dataset)
>
> {
>
>     data1 <- dataset
>
>     df_clean <- with(data1, data1[norm_sd < 1, ])
>
>     datD <- with(data1, data1[norm_sd >= 1, ])
>
>     s <- split(datD, datD$uniqueid)
>
>     sdf <- lapply(s, function(x) {
>
>         data.frame(x, x$outlier <- ifelse(is.na(x$lp_norm), NA,
>
>             ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),
>
>             x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))
>
>         x
>
>     })
>
>     sdf2 <- bind_rows(sdf)
>
>     all_dat <- bind_rows(df_clean, sdf2)
>
>     all_dat
>
> }
>
>
> # dataset
> dput(dat)
> structure(list(field = c("LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01"), set = c("seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta"), ent_num = c(23L, 14L, 43L, 30L, 44L, 60L, 17L,
> 34L, 41L, 40L, 9L, 36L, 38L, 19L, 61L, 51L, 45L, 42L, 3L, 39L,
> 21L, 11L, 12L, 7L, 35L, 5L, 70L, 47L, 28L, 16L, 72L, 13L, 49L,
> 67L, 56L, 32L, 27L, 46L, 24L, 63L, 15L, 66L, 26L, 29L, 48L, 1L,
> 54L, 37L, 2L, 50L, 52L, 31L, 33L, 25L, 6L, 69L, 53L, 10L, 18L,
> 55L, 59L, 4L, 58L, 22L, 20L, 64L, 71L, 57L, 11L, 17L, 43L, 13L,
> 1L, 16L, 34L, 47L, 52L, 72L, 59L, 22L, 54L, 18L, 25L, 61L, 56L,
> 41L, 27L, 14L, 49L, 19L, 29L, 31L, 64L, 6L, 53L, 35L, 37L, 67L,
> 39L, 51L, 40L, 15L, 69L, 60L, 38L, 4L, 23L, 3L, 48L, 32L, 42L,
> 24L, 28L, 33L, 57L, 8L, 21L, 46L, 7L, 30L, 45L, 2L, 63L, 36L,
> 68L, 20L, 66L, 70L, 58L, 5L, 10L, 12L, 62L, 50L, 71L, 9L, 55L,
> 26L, 44L, 65L, 14L, 63L, 46L, 58L, 62L, 19L, 59L, 2L, 5L, 6L,
> 40L, 21L, 44L, 37L, 55L, 35L, 71L, 56L, 10L, 36L, 53L, 25L, 61L,
> 12L, 26L, 23L, 4L, 13L, 28L, 38L, 57L, 54L, 72L, 48L, 66L, 9L,
> 70L, 15L, 39L, 60L, 17L, 34L, 51L, 67L, 42L, 49L, 31L, 30L, 3L,
> 18L, 65L, 32L, 27L, 52L, 22L, 11L, 47L, 64L, 8L, 43L, 41L, 16L,
> 20L, 33L, 7L, 50L, 68L, 24L, 1L, 69L, 45L, 29L, 37L, 30L, 55L,
> 54L, 43L, 32L, 21L, 27L, 33L, 40L, 67L, 57L, 68L, 31L, 17L, 13L,
> 6L, 62L, 19L, 22L, 3L, 10L, 44L, 34L, 69L, 70L, 4L, 1L, 25L,
> 11L, 51L, 5L, 63L, 71L, 12L, 38L, 58L, 39L, 49L, 59L, 56L, 65L,
> 2L, 64L, 8L, 35L, 46L, 45L, 29L, 53L, 36L, 42L, 23L, 18L, 50L,
> 26L, 14L, 48L, 66L, 20L, 24L, 7L, 15L, 53L, 22L, 39L, 20L, 60L,
> 59L, 43L, 19L, 41L, 6L, 62L, 1L, 55L, 34L, 50L, 38L, 40L, 44L,
> 4L, 46L, 29L, 65L, 57L, 48L, 33L, 69L, 14L, 35L, 67L, 72L, 54L,
> 3L, 49L, 2L, 12L, 18L, 30L, 10L, 70L, 31L, 15L, 63L, 71L, 21L,
> 45L, 28L, 56L, 27L, 64L, 61L, 51L, 5L, 24L, 68L, 25L, 66L, 16L,
> 36L, 58L, 37L, 52L, 26L, 9L, 42L, 7L, 11L, 8L, 32L, 23L, 13L,
> 47L, 17L, 61L, 72L, 47L, 60L, 16L, 9L, 28L, 52L, 41L, 1L, 61L,
> 6L, 23L, 58L, 63L, 25L, 28L, 30L, 36L, 62L, 9L, 32L, 19L, 31L,
> 56L, 45L, 2L, 22L, 27L, 40L, 14L, 11L, 50L, 13L, 70L, 20L, 64L,
> 39L, 26L, 21L, 43L, 29L, 35L, 54L, 52L, 37L, 17L, 16L, 72L, 48L,
> 12L, 18L, 44L, 42L, 49L, 68L, 5L, 55L, 69L, 51L, 66L, 59L, 53L,
> 15L, 71L, 41L, 57L, 4L, 60L, 8L, 7L, 33L, 34L, 24L, 10L, 67L,
> 47L, 38L, 3L, 65L, 46L, 20L, 34L, 71L, 1L, 33L, 57L, 13L, 21L,
> 66L, 29L, 3L, 61L, 69L, 24L, 62L, 39L, 49L, 47L, 31L, 53L, 52L,
> 43L, 17L, 7L, 8L, 12L, 60L, 63L, 50L, 2L, 51L, 46L, 10L, 23L,
> 48L, 11L, 26L, 40L, 70L, 42L, 59L, 15L, 56L, 58L, 27L, 6L, 35L,
> 4L, 37L, 5L, 65L, 44L, 28L, 14L, 32L, 36L, 45L, 9L, 18L, 55L,
> 68L, 30L, 54L, 41L, 25L, 22L, 38L, 16L, 67L, 64L, 19L, 72L, 68L,
> 28L, 33L, 15L, 51L, 4L, 47L, 36L, 8L, 57L, 48L, 1L, 52L, 39L,
> 32L, 50L, 13L, 30L, 63L, 2L, 9L, 62L, 22L, 6L, 61L, 16L, 53L,
> 38L, 37L, 20L, 69L, 44L, 56L, 29L, 26L, 14L, 17L, 46L, 66L, 58L,
> 42L, 60L, 19L, 45L, 3L, 59L, 70L, 31L, 24L, 55L, 40L, 43L, 25L,
> 65L, 12L, 67L, 21L, 7L, 27L, 49L, 72L, 54L, 41L, 23L, 34L, 5L,
> 64L, 35L, 18L, 71L, 11L, 24L, 19L, 38L, 14L, 4L, 56L, 5L, 54L,
> 34L, 64L, 55L, 33L, 69L, 71L, 52L, 61L, 48L, 23L, 43L, 41L, 20L,
> 39L, 11L, 63L, 36L, 22L, 9L, 25L, 27L, 51L, 53L, 37L, 57L, 13L,
> 18L, 64L, 22L, 53L, 16L, 5L, 28L, 60L, 31L, 11L, 29L, 45L, 59L,
> 72L, 49L, 67L, 13L, 20L, 3L, 42L, 44L, 69L, 33L, 38L, 15L, 70L,
> 35L, 48L, 26L, 56L, 19L, 39L, 43L, 40L, 14L, 2L, 68L, 51L, 12L,
> 47L, 10L, 55L, 23L, 4L, 71L, 41L, 50L, 7L, 24L, 61L, 27L, 54L,
> 46L, 58L, 37L, 66L, 57L, 1L, 36L, 32L, 18L, 62L, 9L, 30L, 21L,
> 6L, 52L, 8L, 65L, 17L, 25L, 63L, 34L, 65L, 22L, 56L, 9L, 7L,
> 11L, 31L, 4L, 63L, 29L, 61L, 54L, 12L, 62L, 59L, 5L, 23L, 53L,
> 36L, 24L, 35L, 66L, 49L, 72L, 18L, 70L, 32L, 43L, 20L, 45L, 34L,
> 46L, 28L, 6L, 44L, 71L, 39L, 13L, 27L, 1L, 58L, 30L, 68L, 17L,
> 33L, 26L, 57L, 15L, 21L, 52L, 48L, 42L, 16L, 40L, 38L, 8L, 69L,
> 2L, 51L, 67L, 55L, 64L, 47L, 60L, 19L, 41L, 50L, 3L, 14L, 25L,
> 10L, 37L, 6L, 15L, 45L, 49L, 8L, 17L, 50L, 16L, 58L, 72L, 26L,
> 60L, 7L, 32L, 1L, 46L, 66L, 68L, 62L, 47L, 35L, 70L, 10L, 31L,
> 65L, 2L, 3L, 21L, 12L, 30L, 40L, 28L, 59L, 42L, 67L, 44L, 29L
> ), rep_num = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L), lp = c(NA, 41.64, 38.8, 44.45, 40.54,
> 38.54, 41.94, 39.6, 37.39, 40.95, 38.45, 43.47, 41.66, 40.91,
> 42.68, 43.12, 38.22, 40.95, 46.24, 42.95, 38.95, 39.88, 40.57,
> 40.13, 38.57, 45.45, 40.78, 43.52, 39.75, 39.93, 40.35, 37.6,
> 37.7, 43.05, 43.32, 41.31, 39.03, 42.5, 43.18, 41.32, 39.58,
> 40.62, 39.64, 39.85, 38.75, 40.18, 41.44, 40.5, 40.87, 40.75,
> 38.37, 40.26, 35.11, 40.89, 41.67, 38.87, 37.32, 35.85, 38.25,
> 42.21, 43.15, 38.69, 38.8, 38.77, 37.98, 39.8, 33.37, 40.09,
> 42.87, 44.07, 43.78, 42.47, 42.8, 41.1, 41.17, 44.07, 44.24,
> 43.16, 46.12, 42.64, 43.92, 42.16, 43.69, 42.89, 42.63, 43.58,
> 44.4, 43.17, 43.3, 42.5, 42.6, 44.05, 44.63, 43.09, 45.17, 45.17,
> 42.78, 42.38, 45.76, 42.16, 44.22, 43.67, 40.45, 41.95, 42.49,
> 41.98, 45.15, 46.01, 38.51, 43.08, 45.19, 44.53, 43.14, 39.93,
> 46.84, 43.59, 41.68, 43.7, 44.63, 44.02, 42.07, 43.88, 43.2,
> 46.2, 40.84, 39.14, 43.89, 42.58, 41.53, 45.32, 39.56, 43.77,
> 45.45, 45.16, 43.4, 40.08, 42.27, 43.74, 43.77, 41.31, 41.38,
> 45.01, 45.76, 40.4, 48.39, 46.27, 15.44, 44.76, 47.45, 44.87,
> 46.8, 41.83, 43.03, 43.96, 42.51, 45.06, 45.55, 44.9, 42.47,
> 44.9, 45, 44.77, 43.79, 44.26, 44.57, 44.4, 43.42, 42.31, 47.18,
> 43.83, 45.72, 44.83, 44.96, 40.28, 42.85, 41.23, 45.23, 47.09,
> 43.61, 42.69, 46.27, 45.16, 44.14, 43.34, 45.97, 43.81, 43.01,
> 39.82, 45.91, 45.97, 43.61, 45.12, 46.37, 42.67, 42.47, 45.86,
> 44.19, 44.46, 42.64, 43.95, 44.93, 40.33, 42.75, 39.92, 44.17,
> 44.49, 41.51, 42.43, 44.14, 40.5, 41.29, 44.89, 37.98, 39.02,
> 39.62, 42.13, 39.03, 44.16, 39.15, 41.49, 42.63, 40.11, 39.97,
> 42.85, 35.98, 39.45, 40.99, 42.44, 42.11, 37.36, 40.63, 40.69,
> 43.57, 39.04, 39.3, 42.19, 36.88, 40.39, 37.78, 38.6, 40.2, 40.98,
> 36.58, 43.59, 42.49, 39.96, 39.84, 40.43, 38.94, 42.72, 39.43,
> 42.13, 40.36, 40.58, 40.01, 42.17, 42.17, 41.35, 43.27, 40.15,
> 39.76, 40.94, 40.87, 42.32, 41.81, 41.97, 43.72, 41.32, 40.83,
> 37.64, 41.03, 38.98, 40.61, 41.17, 41.96, 40.07, 38.48, 42.85,
> 38.68, 39.09, 42.16, 38.14, 37.99, 41.06, 37.4, 41.88, 39.35,
> 39.73, 38.38, 41.34, 40.67, 40.89, 39.28, 37.59, 39.3, 39.72,
> 40.79, 39.42, 34.5, 37.61, 37.76, 40.24, 41.17, 41.24, 42.14,
> 42.53, 39.71, 39.44, 40.19, 42.51, 40.15, 36.26, 37.48, 40.43,
> 37.5, 42.11, 41.44, 40.29, 39.75, 37.58, 41.25, 40.39, 41.63,
> 42.18, 43.71, 38.69, 43.47, 41.98, 35.51, 42.22, 38.51, 40.17,
> 39.4, 39.54, 41.7, 40.93, 40.01, 35.97, 41.44, 41.89, 40.9, 39.95,
> 40.69, 43.18, 37.03, 39.91, 36.75, 42.75, 41.61, 42.6, 38.55,
> 42.84, 38.41, 42.55, 41.44, 41.03, 40, 41.35, 42.18, 42.66, 40.06,
> 42.48, 41.33, 41.92, 40.88, 40.99, 42.78, 38.26, 40.81, 39.95,
> 37.89, 40.81, 38.33, 39.33, 39.67, 40.12, 41.75, 39.81, 41.92,
> 44.59, 37.8, 40.54, 40.49, 41.82, 42.13, 39.93, 39.94, 42.54,
> 41.74, 43.06, 41.72, 41.27, 39.42, 42.07, 40.06, 42.1, 38.91,
> 39.28, 42.94, 39.94, 41.86, 38.56, 38.15, 41.47, 42.34, 38.14,
> 40.19, 40.2, 43.01, 42.35, 40.89, 41.44, 42.89, 44.49, 39.1,
> 38.42, 37.44, 43.28, 37.96, 40.56, 41.53, 41.59, 41.45, 42.55,
> 40.6, 44.04, 39.31, 41.08, 37.14, 40.03, 41.49, 40.82, 38.47,
> 43.43, 38.82, 40.4, 41.09, 42.26, 41.85, 41.13, 37.27, 41.97,
> 45.24, 43.35, 40.53, 43.14, 39.71, 42.77, 42.2, 42.13, 41.91,
> 42.59, 41.99, 42.11, 40.43, 40.02, 43.53, 43.01, 39.55, 43.2,
> 39.95, 42.51, 42, 42.26, 42.39, 42.64, 39.9, 41.89, 43.05, 41.02,
> 41.95, 39.92, 43.42, 44.01, 42.6, 40.84, 42.86, 44.31, 42.24,
> 41.25, 42.38, 44.99, 41.24, 43.97, 41.12, 37.88, 41.53, 41.56,
> 39.18, 40.83, 42.96, 41.92, 43.86, 42.48, 42.65, 43.3, 41.99,
> 42.51, 40.65, 42.77, 34.71, 40.97, 38.15, 39.76, 36.74, 37.95,
> 39.17, 38.22, 39.31, 43.57, 37.21, 39.35, 42.37, 42.01, 42.39,
> 43.21, 36.91, 36.69, 41.07, 41.91, 34.63, 40.61, 36.23, 38.12,
> 40.76, 39.14, 41.81, 39.14, 41.04, 37.32, 40.83, 40.81, 38.29,
> 39.61, 40.96, 40.71, 40.47, 38.64, 39.76, 38.19, 39.05, 38.04,
> 41.14, 38.35, 42.3, 34.44, 40.93, 39.3, 41.44, 38.3, 42.74, 36.66,
> 40.02, 36.62, 40.48, 41.72, 41.23, 41.81, 42.07, 40.22, 37.83,
> 36.54, 37.77, 40.49, 38.65, 43.2, 43.32, 40.67, 41.95, 36.11,
> 39.28, 42.38, 40.35, 40.3, 43.48, 40.55, 40.54, 44.03, 41.94,
> 37.97, 41.98, 41.53, 38.19, 38.66, 41.18, 41.95, 42.53, 38.7,
> 44.55, 42.39, 41.55, 38.46, 42.27, 42.19, 41.95, 41.81, 39.81,
> 38.12, 42.94, 42.99, 38.85, 41.26, 43.13, 44.21, 38.54, 44.02,
> 43.46, 41.64, 42.06, 42.11, 42.34, 41.86, 37.91, 40.89, 40.5,
> 39.54, 37.87, 40.86, 41.36, 41.77, 42.03, 39.15, 40.04, 44.11,
> 41.34, 42.97, 38.42, 37.28, 41.04, 41.48, 38.82, 41.94, 37.95,
> 40.9, 40.39, 40.31, 43.19, 41.22, 41.49, 41.25, 40.07, 36.7,
> 39.97, 39.99, 41.7, 37.09, 42.58, 43.01, 37.7, 41.81, 39.99,
> 42.95, 43.19, 42.69, 41.5, 40.64, 43.24, 41.14, 41.21, 41.29,
> 41.43, 44.21, 38.52, 42.54, 40.54, 42.49, 43.2, 38.12, 40.08,
> 39.02, 41.45, 42.33, 41.11, 38.93, 41.63, 44.22, 41.41, 39.08,
> 40.9, 41.1, 43.88, 40.96, 46.75, 47.54, 40.35, 41.97, 44.94,
> 44.91, 44.66, 44.5, 44.4, 46.4, 47.97, 46.05, 45.57, 42.83, 41.48,
> 47.48, 45.43, 41.98, 43.14, 45.6, 44.78, 45.45, 45.69, 44.82,
> 44.24, 41.14, 43.14, 46.61, 43.92, 43.56, 43.68, 45.37, 45.15,
> 40.76, 43.78, 44.67, 41.36, 41.4, 40.97, 41.87, 39.83, 43.8,
> 48.36, 44.28, 43.29, 44.69, 43.26, 43.35, 44.34, 45.08, 42.26,
> 39.7, 42.4, 44.03, 43.22, 42.71, 45.89, 44.89, 44.81, 42.59,
> 40.85, 43.82, 44.85, 47.47, 43.64, 42.65, 45.67, 43.24, 42.33,
> 40.61, 38.3, 39.84, 41.08, 42.33, 44.44, 40.85, 39.58, 42.55,
> 41.75, 39.44, 41.79, 39.31, 41.34, 42.76, 40.79, 37.31, 42.85,
> 42.88, 42.01, 44.63, 38.85, 41.13, 40.43, 41.34, 43.14, 40.58,
> 42.21, 38.94, 44.88, 42.33, 42.61, 41.88, 41.13, 41.83, 42.8)), .Names =
> c("field",
> "set", "ent_num", "rep_num", "lp"), class = "data.frame", row.names = c(NA,
> -787L))
>
> # session info
>
> R version 3.4.1 (2017-06-30)
>
> Platform: i386-w64-mingw32/i386 (32-bit)
>
> Running under: Windows 7 x64 (build 7601) Service Pack 1
>
>
>
> Matrix products: default
>
>
>
> locale:
>
> [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
> States.1252
>
> [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C
>
> [5] LC_TIME=English_United States.1252
>
>
>
> attached base packages:
>
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
>
>
> other attached packages:
>
> [1] bindrcpp_0.2 tidyr_0.6.3  dplyr_0.7.4
>
>
>
> loaded via a namespace (and not attached):
>
>  [1] compiler_3.4.1   magrittr_1.5     assertthat_0.2.0 R6_2.2.2
>  tools_3.4.1
>
>  [6] glue_1.1.1       tibble_1.3.3     Rcpp_0.12.11     stringi_1.1.5
> pkgconfig_2.0.1
>
> [11] rlang_0.1.2      bindr_0.1
>
> This email and any attachments were sent from a Monsanto email account and
> may contain confidential and/or privileged information. If you are not the
> intended recipient, please contact the sender and delete this email and any
> attachments immediately. Any unauthorized use, including disclosing,
> printing, storing, copying or distributing this email, is prohibited. All
> emails and attachments sent to or from Monsanto email accounts may be
> subject to monitoring, reading, and archiving by Monsanto, including its
> affiliates and subsidiaries, as permitted by applicable law. Thank you.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
>
>
> This email and any attachments were sent from a Monsanto email account and may contain confidential and/or privileged information. If you are not the intended recipient, please contact the sender and delete this email and any attachments immediately. Any unauthorized use, including disclosing, printing, storing, copying or distributing this email, is prohibited. All emails and attachments sent to or from Monsanto email accounts may be subject to monitoring, reading, and archiving by Monsanto, including its affiliates and subsidiaries, as permitted by applicable law. Thank you.
>
>
>
>

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Thu Dec 14 17:28:41 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Thu, 14 Dec 2017 18:28:41 +0200
Subject: [R] help with recursive function
In-Reply-To: <CAGgJW76zkxSQjH+XafCF58xoK8qYT9O0AfijT8cTxFZkYzQpPg@mail.gmail.com>
References: <CY1PR0101MB1018F79F34CFE3142A8592ADE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CAGgJW772Nk+Z3XamiGF-2r6Ta9Cud1VOhUvRr4p4k0epTX4xtA@mail.gmail.com>
 <CAGgJW77AAvtOiCwLz=XEvsU+AuqHA=SK8RWLcBD5Ka7P8OFrPA@mail.gmail.com>
 <CY1PR0101MB1018FB7B65C9BDBE0ECA5DCBE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CY1PR0101MB1018B00187792A9F236E549EE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CAGgJW76zkxSQjH+XafCF58xoK8qYT9O0AfijT8cTxFZkYzQpPg@mail.gmail.com>
Message-ID: <CAGgJW75kvGt527SWM6gaNSDkAMg=5Uf3NGAybVY43WzJ65_=2g@mail.gmail.com>

If you are trying to understand why the "stopifnot" condition is met you
can replace it by something like:

if ( any(dat2$norm_sd >= 1) )
   browser()

This will put you in a debugging session where you can examine your
variables, e.g.

> dat$norm_sd

HTH,
Eric



On Thu, Dec 14, 2017 at 5:33 PM, Eric Berger <ericjberger at gmail.com> wrote:

> The message is coming from your stopifnot() condition being met.
>
>
> On Thu, Dec 14, 2017 at 5:31 PM, DIGHE, NILESH [AG/2362] <
> nilesh.dighe at monsanto.com> wrote:
>
>> Hi, I accidently left out few lines of code from the calclp function.
>> Updated function is pasted below.
>>
>> I am still getting the same error ?Error: !(any(data1$norm_sd >= 1)) is
>> not TRUE?
>>
>>
>>
>> I would appreciate any help.
>>
>> Nilesh
>>
>> dput(calclp)
>>
>> function (dataset)
>>
>> {
>>
>>     dat1 <- funlp1(dataset)
>>
>>     recursive_funlp <- function(dataset = dat1, func = funlp2) {
>>
>>         dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
>>
>>             mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
>>
>>             spread(key = field_rep, value = lp) %>% mutate_at(.vars =
>> grep("_",
>>
>>             names(.)), funs(norm = round(scale(.), 3)))
>>
>>         dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>>
>>             1, sd, na.rm = TRUE), 3)
>>
>>         dat2$norm_max <- round(apply(dat2[, grep("lp_norm",
>> names(dat2))],
>>
>>             1, function(x) {
>>
>>                 max(abs(x), na.rm = TRUE)
>>
>>             }), 3)
>>
>>         data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,
>>
>>             -norm_sd) %>% separate(key, c("field_rep", "treatment"),
>>
>>             "\\.") %>% spread(treatment, value) %>% mutate(outlier = NA)
>>
>>         stopifnot(!(any(data1$norm_sd >= 1)))
>>
>>         if (!(any(data1$norm_sd >= 1))) {
>>
>>             df1 <- dat1
>>
>>             return(df1)
>>
>>         }
>>
>>        else {
>>
>>             df2 <- recursive_funlp()
>>
>>             return(df2)
>>
>>         }
>>
>>     }
>>
>>     df3 <- recursive_funlp(dataset = dat1, func = funlp2)
>>
>>     df3
>>
>> }
>>
>>
>>
>>
>>
>> *From:* DIGHE, NILESH [AG/2362]
>> *Sent:* Thursday, December 14, 2017 9:01 AM
>> *To:* 'Eric Berger' <ericjberger at gmail.com>
>> *Cc:* r-help <r-help at r-project.org>
>> *Subject:* RE: [R] help with recursive function
>>
>>
>>
>> Eric:  Thanks for taking time to look into my problem.  Despite of making
>> the change you suggested, I am still getting the same error.  I am
>> wondering if the logic I am using in the stopifnot and if functions is a
>> problem.
>>
>> I like the recursive function to stop whenever the norm_sd column has
>> zero values that are above or equal to 1. Below is the calclp function
>> after the changes you suggested.
>>
>> Thanks. Nilesh
>>
>>
>>
>> dput(calclp)
>>
>> function (dataset)
>>
>> {
>>
>>     dat1 <- funlp1(dataset)
>>
>>     recursive_funlp <- function(dataset = dat1, func = funlp2) {
>>
>>         dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
>>
>>             mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
>>
>>             spread(key = field_rep, value = lp) %>% mutate_at(.vars =
>> grep("_",
>>
>>             names(.)), funs(norm = round(scale(.), 3)))
>>
>>         dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>>
>>             1, sd, na.rm = TRUE), 3)
>>
>>         dat2$norm_max <- round(apply(dat2[, grep("lp_norm",
>> names(dat2))],
>>
>>             1, function(x) {
>>
>>                 max(abs(x), na.rm = TRUE)
>>
>>             }), 3)
>>
>>         stopifnot(!(any(dat2$norm_sd >= 1)))
>>
>>         if (!(any(dat2$norm_sd >= 1))) {
>>
>>             df1 <- dat1
>>
>>             return(df1)
>>
>>         }
>>
>>         else {
>>
>>             df2 <- recursive_funlp()
>>
>>             return(df2)
>>
>>         }
>>
>>     }
>>
>>     df3 <- recursive_funlp(dataset = dat1, func = funlp2)
>>
>>     df3
>>
>> }
>>
>>
>>
>>
>>
>> *From:* Eric Berger [mailto:ericjberger at gmail.com <ericjberger at gmail.com>]
>>
>> *Sent:* Thursday, December 14, 2017 8:17 AM
>> *To:* DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com>
>> *Cc:* r-help <r-help at r-project.org>
>> *Subject:* Re: [R] help with recursive function
>>
>>
>>
>> My own typo ... whoops ...
>>
>>
>>
>> !( any(dat2$norm_sd >= 1 ))
>>
>>
>>
>>
>>
>>
>>
>> On Thu, Dec 14, 2017 at 3:43 PM, Eric Berger <ericjberger at gmail.com>
>> wrote:
>>
>> You seem to have a typo at this expression (and some others like it)
>>
>>
>>
>> Namely, you write
>>
>>
>>
>> any(!dat2$norm_sd) >= 1
>>
>>
>>
>> when you possibly meant to write
>>
>>
>>
>> !( any(dat2$norm_sd) >= 1 )
>>
>>
>>
>> i.e. I think your ! seems to be in the wrong place.
>>
>>
>>
>> HTH,
>> Eric
>>
>>
>>
>>
>>
>> On Thu, Dec 14, 2017 at 3:26 PM, DIGHE, NILESH [AG/2362] <
>> nilesh.dighe at monsanto.com> wrote:
>>
>> Hi, I need some help with running a recursive function. I like to run
>> funlp2 recursively.
>> When I try to run recursive function in another function named "calclp" I
>> get this "Error: any(!dat2$norm_sd) >= 1 is not TRUE".
>>
>> I have never built a recursive function before so having trouble
>> executing it in this case.  I would appreciate any help or guidance to
>> resolve this issue. Please see my data and the three functions that I am
>> using below.
>> Please note that calclp is the function I am running and the other two
>> functions are within this calclp function.
>>
>> # code:
>> Test<- calclp(dataset = dat)
>>
>> # calclp function
>>
>> calclp<- function (dataset)
>>
>> {
>>
>>     dat1 <- funlp1(dataset)
>>
>>     recursive_funlp <- function(dataset = dat1, func = funlp2) {
>>
>>         dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
>>
>>             mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
>>
>>             spread(key = field_rep, value = lp) %>% mutate_at(.vars =
>> grep("_",
>>
>>             names(.)), funs(norm = round(scale(.), 3)))
>>
>>         dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>>
>>             1, sd, na.rm = TRUE), 3)
>>
>>         dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>>
>>             1, function(x) {
>>
>>                 max(abs(x), na.rm = TRUE)
>>
>>             }), 3)
>>
>>         stopifnot(any(!dat2$norm_sd) >= 1)
>>
>>         if (any(!dat2$norm_sd) >= 1) {
>>
>>             df1 <- dat1
>>
>>             return(df1)
>>
>>         }
>>
>>         else {
>>
>>             df2 <- recursive_funlp()
>>
>>             return(df2)
>>
>>         }
>>
>>     }
>>
>>     df3 <- recursive_funlp(dataset = dat1, func = funlp2)
>>
>>     df3
>>
>> }
>>
>>
>> # funlp1 function
>>
>> funlp1<- function (dataset)
>>
>> {
>>
>>     dat2 <- dataset %>% select(field, set, ent_num, rep_num,
>>
>>         lp) %>% unite(uniqueid, set, ent_num, sep = ".") %>%
>>
>>         unite(field_rep, field, rep_num) %>% mutate(field_rep =
>> paste(field_rep,
>>
>>         "lp", sep = ".")) %>% spread(key = field_rep, value = lp) %>%
>>
>>         mutate_at(.vars = grep("_", names(.)), funs(norm = round(scale(.),
>>
>>             3)))
>>
>>     dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>>
>>         1, sd, na.rm = TRUE), 3)
>>
>>     dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>>
>>         1, function(x) {
>>
>>             max(abs(x), na.rm = TRUE)
>>
>>         }), 3)
>>
>>     data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,
>>
>>         -norm_sd) %>% separate(key, c("field_rep", "treatment"),
>>
>>         "\\.") %>% spread(treatment, value) %>% mutate(outlier = NA)
>>
>>     df_clean <- with(data1, data1[norm_sd < 1, ])
>>
>>     datD <- with(data1, data1[norm_sd >= 1, ])
>>
>>     s <- split(datD, datD$uniqueid)
>>
>>     sdf <- lapply(s, function(x) {
>>
>>         data.frame(x, x$outlier <- ifelse(is.na(x$lp_norm), NA,
>>
>>             ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),
>>
>>             x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))
>>
>>         x
>>
>>     })
>>
>>     sdf2 <- bind_rows(sdf)
>>
>>     all_dat <- bind_rows(df_clean, sdf2)
>>
>>     all_dat
>>
>> }
>>
>>
>> # funlp2 function
>>
>> funlp2<-function (dataset)
>>
>> {
>>
>>     data1 <- dataset
>>
>>     df_clean <- with(data1, data1[norm_sd < 1, ])
>>
>>     datD <- with(data1, data1[norm_sd >= 1, ])
>>
>>     s <- split(datD, datD$uniqueid)
>>
>>     sdf <- lapply(s, function(x) {
>>
>>         data.frame(x, x$outlier <- ifelse(is.na(x$lp_norm), NA,
>>
>>             ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),
>>
>>             x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))
>>
>>         x
>>
>>     })
>>
>>     sdf2 <- bind_rows(sdf)
>>
>>     all_dat <- bind_rows(df_clean, sdf2)
>>
>>     all_dat
>>
>> }
>>
>>
>> # dataset
>> dput(dat)
>> structure(list(field = c("LM01", "LM01", "LM01", "LM01", "LM01",
>> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
>> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
>> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
>> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
>> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
>> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
>> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
>> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
>> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
>> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
>> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "B2NW", "B2NW",
>> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
>> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
>> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
>> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "EX01", "EX01",
>> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
>> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
>> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
>> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
>> "EX01", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
>> "MAND", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
>> "28EP", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
>> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
>> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
>> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
>> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01"), set = c("seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
>> "seta", "seta"), ent_num = c(23L, 14L, 43L, 30L, 44L, 60L, 17L,
>> 34L, 41L, 40L, 9L, 36L, 38L, 19L, 61L, 51L, 45L, 42L, 3L, 39L,
>> 21L, 11L, 12L, 7L, 35L, 5L, 70L, 47L, 28L, 16L, 72L, 13L, 49L,
>> 67L, 56L, 32L, 27L, 46L, 24L, 63L, 15L, 66L, 26L, 29L, 48L, 1L,
>> 54L, 37L, 2L, 50L, 52L, 31L, 33L, 25L, 6L, 69L, 53L, 10L, 18L,
>> 55L, 59L, 4L, 58L, 22L, 20L, 64L, 71L, 57L, 11L, 17L, 43L, 13L,
>> 1L, 16L, 34L, 47L, 52L, 72L, 59L, 22L, 54L, 18L, 25L, 61L, 56L,
>> 41L, 27L, 14L, 49L, 19L, 29L, 31L, 64L, 6L, 53L, 35L, 37L, 67L,
>> 39L, 51L, 40L, 15L, 69L, 60L, 38L, 4L, 23L, 3L, 48L, 32L, 42L,
>> 24L, 28L, 33L, 57L, 8L, 21L, 46L, 7L, 30L, 45L, 2L, 63L, 36L,
>> 68L, 20L, 66L, 70L, 58L, 5L, 10L, 12L, 62L, 50L, 71L, 9L, 55L,
>> 26L, 44L, 65L, 14L, 63L, 46L, 58L, 62L, 19L, 59L, 2L, 5L, 6L,
>> 40L, 21L, 44L, 37L, 55L, 35L, 71L, 56L, 10L, 36L, 53L, 25L, 61L,
>> 12L, 26L, 23L, 4L, 13L, 28L, 38L, 57L, 54L, 72L, 48L, 66L, 9L,
>> 70L, 15L, 39L, 60L, 17L, 34L, 51L, 67L, 42L, 49L, 31L, 30L, 3L,
>> 18L, 65L, 32L, 27L, 52L, 22L, 11L, 47L, 64L, 8L, 43L, 41L, 16L,
>> 20L, 33L, 7L, 50L, 68L, 24L, 1L, 69L, 45L, 29L, 37L, 30L, 55L,
>> 54L, 43L, 32L, 21L, 27L, 33L, 40L, 67L, 57L, 68L, 31L, 17L, 13L,
>> 6L, 62L, 19L, 22L, 3L, 10L, 44L, 34L, 69L, 70L, 4L, 1L, 25L,
>> 11L, 51L, 5L, 63L, 71L, 12L, 38L, 58L, 39L, 49L, 59L, 56L, 65L,
>> 2L, 64L, 8L, 35L, 46L, 45L, 29L, 53L, 36L, 42L, 23L, 18L, 50L,
>> 26L, 14L, 48L, 66L, 20L, 24L, 7L, 15L, 53L, 22L, 39L, 20L, 60L,
>> 59L, 43L, 19L, 41L, 6L, 62L, 1L, 55L, 34L, 50L, 38L, 40L, 44L,
>> 4L, 46L, 29L, 65L, 57L, 48L, 33L, 69L, 14L, 35L, 67L, 72L, 54L,
>> 3L, 49L, 2L, 12L, 18L, 30L, 10L, 70L, 31L, 15L, 63L, 71L, 21L,
>> 45L, 28L, 56L, 27L, 64L, 61L, 51L, 5L, 24L, 68L, 25L, 66L, 16L,
>> 36L, 58L, 37L, 52L, 26L, 9L, 42L, 7L, 11L, 8L, 32L, 23L, 13L,
>> 47L, 17L, 61L, 72L, 47L, 60L, 16L, 9L, 28L, 52L, 41L, 1L, 61L,
>> 6L, 23L, 58L, 63L, 25L, 28L, 30L, 36L, 62L, 9L, 32L, 19L, 31L,
>> 56L, 45L, 2L, 22L, 27L, 40L, 14L, 11L, 50L, 13L, 70L, 20L, 64L,
>> 39L, 26L, 21L, 43L, 29L, 35L, 54L, 52L, 37L, 17L, 16L, 72L, 48L,
>> 12L, 18L, 44L, 42L, 49L, 68L, 5L, 55L, 69L, 51L, 66L, 59L, 53L,
>> 15L, 71L, 41L, 57L, 4L, 60L, 8L, 7L, 33L, 34L, 24L, 10L, 67L,
>> 47L, 38L, 3L, 65L, 46L, 20L, 34L, 71L, 1L, 33L, 57L, 13L, 21L,
>> 66L, 29L, 3L, 61L, 69L, 24L, 62L, 39L, 49L, 47L, 31L, 53L, 52L,
>> 43L, 17L, 7L, 8L, 12L, 60L, 63L, 50L, 2L, 51L, 46L, 10L, 23L,
>> 48L, 11L, 26L, 40L, 70L, 42L, 59L, 15L, 56L, 58L, 27L, 6L, 35L,
>> 4L, 37L, 5L, 65L, 44L, 28L, 14L, 32L, 36L, 45L, 9L, 18L, 55L,
>> 68L, 30L, 54L, 41L, 25L, 22L, 38L, 16L, 67L, 64L, 19L, 72L, 68L,
>> 28L, 33L, 15L, 51L, 4L, 47L, 36L, 8L, 57L, 48L, 1L, 52L, 39L,
>> 32L, 50L, 13L, 30L, 63L, 2L, 9L, 62L, 22L, 6L, 61L, 16L, 53L,
>> 38L, 37L, 20L, 69L, 44L, 56L, 29L, 26L, 14L, 17L, 46L, 66L, 58L,
>> 42L, 60L, 19L, 45L, 3L, 59L, 70L, 31L, 24L, 55L, 40L, 43L, 25L,
>> 65L, 12L, 67L, 21L, 7L, 27L, 49L, 72L, 54L, 41L, 23L, 34L, 5L,
>> 64L, 35L, 18L, 71L, 11L, 24L, 19L, 38L, 14L, 4L, 56L, 5L, 54L,
>> 34L, 64L, 55L, 33L, 69L, 71L, 52L, 61L, 48L, 23L, 43L, 41L, 20L,
>> 39L, 11L, 63L, 36L, 22L, 9L, 25L, 27L, 51L, 53L, 37L, 57L, 13L,
>> 18L, 64L, 22L, 53L, 16L, 5L, 28L, 60L, 31L, 11L, 29L, 45L, 59L,
>> 72L, 49L, 67L, 13L, 20L, 3L, 42L, 44L, 69L, 33L, 38L, 15L, 70L,
>> 35L, 48L, 26L, 56L, 19L, 39L, 43L, 40L, 14L, 2L, 68L, 51L, 12L,
>> 47L, 10L, 55L, 23L, 4L, 71L, 41L, 50L, 7L, 24L, 61L, 27L, 54L,
>> 46L, 58L, 37L, 66L, 57L, 1L, 36L, 32L, 18L, 62L, 9L, 30L, 21L,
>> 6L, 52L, 8L, 65L, 17L, 25L, 63L, 34L, 65L, 22L, 56L, 9L, 7L,
>> 11L, 31L, 4L, 63L, 29L, 61L, 54L, 12L, 62L, 59L, 5L, 23L, 53L,
>> 36L, 24L, 35L, 66L, 49L, 72L, 18L, 70L, 32L, 43L, 20L, 45L, 34L,
>> 46L, 28L, 6L, 44L, 71L, 39L, 13L, 27L, 1L, 58L, 30L, 68L, 17L,
>> 33L, 26L, 57L, 15L, 21L, 52L, 48L, 42L, 16L, 40L, 38L, 8L, 69L,
>> 2L, 51L, 67L, 55L, 64L, 47L, 60L, 19L, 41L, 50L, 3L, 14L, 25L,
>> 10L, 37L, 6L, 15L, 45L, 49L, 8L, 17L, 50L, 16L, 58L, 72L, 26L,
>> 60L, 7L, 32L, 1L, 46L, 66L, 68L, 62L, 47L, 35L, 70L, 10L, 31L,
>> 65L, 2L, 3L, 21L, 12L, 30L, 40L, 28L, 59L, 42L, 67L, 44L, 29L
>> ), rep_num = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L), lp = c(NA, 41.64, 38.8, 44.45, 40.54,
>> 38.54, 41.94, 39.6, 37.39, 40.95, 38.45, 43.47, 41.66, 40.91,
>> 42.68, 43.12, 38.22, 40.95, 46.24, 42.95, 38.95, 39.88, 40.57,
>> 40.13, 38.57, 45.45, 40.78, 43.52, 39.75, 39.93, 40.35, 37.6,
>> 37.7, 43.05, 43.32, 41.31, 39.03, 42.5, 43.18, 41.32, 39.58,
>> 40.62, 39.64, 39.85, 38.75, 40.18, 41.44, 40.5, 40.87, 40.75,
>> 38.37, 40.26, 35.11, 40.89, 41.67, 38.87, 37.32, 35.85, 38.25,
>> 42.21, 43.15, 38.69, 38.8, 38.77, 37.98, 39.8, 33.37, 40.09,
>> 42.87, 44.07, 43.78, 42.47, 42.8, 41.1, 41.17, 44.07, 44.24,
>> 43.16, 46.12, 42.64, 43.92, 42.16, 43.69, 42.89, 42.63, 43.58,
>> 44.4, 43.17, 43.3, 42.5, 42.6, 44.05, 44.63, 43.09, 45.17, 45.17,
>> 42.78, 42.38, 45.76, 42.16, 44.22, 43.67, 40.45, 41.95, 42.49,
>> 41.98, 45.15, 46.01, 38.51, 43.08, 45.19, 44.53, 43.14, 39.93,
>> 46.84, 43.59, 41.68, 43.7, 44.63, 44.02, 42.07, 43.88, 43.2,
>> 46.2, 40.84, 39.14, 43.89, 42.58, 41.53, 45.32, 39.56, 43.77,
>> 45.45, 45.16, 43.4, 40.08, 42.27, 43.74, 43.77, 41.31, 41.38,
>> 45.01, 45.76, 40.4, 48.39, 46.27, 15.44, 44.76, 47.45, 44.87,
>> 46.8, 41.83, 43.03, 43.96, 42.51, 45.06, 45.55, 44.9, 42.47,
>> 44.9, 45, 44.77, 43.79, 44.26, 44.57, 44.4, 43.42, 42.31, 47.18,
>> 43.83, 45.72, 44.83, 44.96, 40.28, 42.85, 41.23, 45.23, 47.09,
>> 43.61, 42.69, 46.27, 45.16, 44.14, 43.34, 45.97, 43.81, 43.01,
>> 39.82, 45.91, 45.97, 43.61, 45.12, 46.37, 42.67, 42.47, 45.86,
>> 44.19, 44.46, 42.64, 43.95, 44.93, 40.33, 42.75, 39.92, 44.17,
>> 44.49, 41.51, 42.43, 44.14, 40.5, 41.29, 44.89, 37.98, 39.02,
>> 39.62, 42.13, 39.03, 44.16, 39.15, 41.49, 42.63, 40.11, 39.97,
>> 42.85, 35.98, 39.45, 40.99, 42.44, 42.11, 37.36, 40.63, 40.69,
>> 43.57, 39.04, 39.3, 42.19, 36.88, 40.39, 37.78, 38.6, 40.2, 40.98,
>> 36.58, 43.59, 42.49, 39.96, 39.84, 40.43, 38.94, 42.72, 39.43,
>> 42.13, 40.36, 40.58, 40.01, 42.17, 42.17, 41.35, 43.27, 40.15,
>> 39.76, 40.94, 40.87, 42.32, 41.81, 41.97, 43.72, 41.32, 40.83,
>> 37.64, 41.03, 38.98, 40.61, 41.17, 41.96, 40.07, 38.48, 42.85,
>> 38.68, 39.09, 42.16, 38.14, 37.99, 41.06, 37.4, 41.88, 39.35,
>> 39.73, 38.38, 41.34, 40.67, 40.89, 39.28, 37.59, 39.3, 39.72,
>> 40.79, 39.42, 34.5, 37.61, 37.76, 40.24, 41.17, 41.24, 42.14,
>> 42.53, 39.71, 39.44, 40.19, 42.51, 40.15, 36.26, 37.48, 40.43,
>> 37.5, 42.11, 41.44, 40.29, 39.75, 37.58, 41.25, 40.39, 41.63,
>> 42.18, 43.71, 38.69, 43.47, 41.98, 35.51, 42.22, 38.51, 40.17,
>> 39.4, 39.54, 41.7, 40.93, 40.01, 35.97, 41.44, 41.89, 40.9, 39.95,
>> 40.69, 43.18, 37.03, 39.91, 36.75, 42.75, 41.61, 42.6, 38.55,
>> 42.84, 38.41, 42.55, 41.44, 41.03, 40, 41.35, 42.18, 42.66, 40.06,
>> 42.48, 41.33, 41.92, 40.88, 40.99, 42.78, 38.26, 40.81, 39.95,
>> 37.89, 40.81, 38.33, 39.33, 39.67, 40.12, 41.75, 39.81, 41.92,
>> 44.59, 37.8, 40.54, 40.49, 41.82, 42.13, 39.93, 39.94, 42.54,
>> 41.74, 43.06, 41.72, 41.27, 39.42, 42.07, 40.06, 42.1, 38.91,
>> 39.28, 42.94, 39.94, 41.86, 38.56, 38.15, 41.47, 42.34, 38.14,
>> 40.19, 40.2, 43.01, 42.35, 40.89, 41.44, 42.89, 44.49, 39.1,
>> 38.42, 37.44, 43.28, 37.96, 40.56, 41.53, 41.59, 41.45, 42.55,
>> 40.6, 44.04, 39.31, 41.08, 37.14, 40.03, 41.49, 40.82, 38.47,
>> 43.43, 38.82, 40.4, 41.09, 42.26, 41.85, 41.13, 37.27, 41.97,
>> 45.24, 43.35, 40.53, 43.14, 39.71, 42.77, 42.2, 42.13, 41.91,
>> 42.59, 41.99, 42.11, 40.43, 40.02, 43.53, 43.01, 39.55, 43.2,
>> 39.95, 42.51, 42, 42.26, 42.39, 42.64, 39.9, 41.89, 43.05, 41.02,
>> 41.95, 39.92, 43.42, 44.01, 42.6, 40.84, 42.86, 44.31, 42.24,
>> 41.25, 42.38, 44.99, 41.24, 43.97, 41.12, 37.88, 41.53, 41.56,
>> 39.18, 40.83, 42.96, 41.92, 43.86, 42.48, 42.65, 43.3, 41.99,
>> 42.51, 40.65, 42.77, 34.71, 40.97, 38.15, 39.76, 36.74, 37.95,
>> 39.17, 38.22, 39.31, 43.57, 37.21, 39.35, 42.37, 42.01, 42.39,
>> 43.21, 36.91, 36.69, 41.07, 41.91, 34.63, 40.61, 36.23, 38.12,
>> 40.76, 39.14, 41.81, 39.14, 41.04, 37.32, 40.83, 40.81, 38.29,
>> 39.61, 40.96, 40.71, 40.47, 38.64, 39.76, 38.19, 39.05, 38.04,
>> 41.14, 38.35, 42.3, 34.44, 40.93, 39.3, 41.44, 38.3, 42.74, 36.66,
>> 40.02, 36.62, 40.48, 41.72, 41.23, 41.81, 42.07, 40.22, 37.83,
>> 36.54, 37.77, 40.49, 38.65, 43.2, 43.32, 40.67, 41.95, 36.11,
>> 39.28, 42.38, 40.35, 40.3, 43.48, 40.55, 40.54, 44.03, 41.94,
>> 37.97, 41.98, 41.53, 38.19, 38.66, 41.18, 41.95, 42.53, 38.7,
>> 44.55, 42.39, 41.55, 38.46, 42.27, 42.19, 41.95, 41.81, 39.81,
>> 38.12, 42.94, 42.99, 38.85, 41.26, 43.13, 44.21, 38.54, 44.02,
>> 43.46, 41.64, 42.06, 42.11, 42.34, 41.86, 37.91, 40.89, 40.5,
>> 39.54, 37.87, 40.86, 41.36, 41.77, 42.03, 39.15, 40.04, 44.11,
>> 41.34, 42.97, 38.42, 37.28, 41.04, 41.48, 38.82, 41.94, 37.95,
>> 40.9, 40.39, 40.31, 43.19, 41.22, 41.49, 41.25, 40.07, 36.7,
>> 39.97, 39.99, 41.7, 37.09, 42.58, 43.01, 37.7, 41.81, 39.99,
>> 42.95, 43.19, 42.69, 41.5, 40.64, 43.24, 41.14, 41.21, 41.29,
>> 41.43, 44.21, 38.52, 42.54, 40.54, 42.49, 43.2, 38.12, 40.08,
>> 39.02, 41.45, 42.33, 41.11, 38.93, 41.63, 44.22, 41.41, 39.08,
>> 40.9, 41.1, 43.88, 40.96, 46.75, 47.54, 40.35, 41.97, 44.94,
>> 44.91, 44.66, 44.5, 44.4, 46.4, 47.97, 46.05, 45.57, 42.83, 41.48,
>> 47.48, 45.43, 41.98, 43.14, 45.6, 44.78, 45.45, 45.69, 44.82,
>> 44.24, 41.14, 43.14, 46.61, 43.92, 43.56, 43.68, 45.37, 45.15,
>> 40.76, 43.78, 44.67, 41.36, 41.4, 40.97, 41.87, 39.83, 43.8,
>> 48.36, 44.28, 43.29, 44.69, 43.26, 43.35, 44.34, 45.08, 42.26,
>> 39.7, 42.4, 44.03, 43.22, 42.71, 45.89, 44.89, 44.81, 42.59,
>> 40.85, 43.82, 44.85, 47.47, 43.64, 42.65, 45.67, 43.24, 42.33,
>> 40.61, 38.3, 39.84, 41.08, 42.33, 44.44, 40.85, 39.58, 42.55,
>> 41.75, 39.44, 41.79, 39.31, 41.34, 42.76, 40.79, 37.31, 42.85,
>> 42.88, 42.01, 44.63, 38.85, 41.13, 40.43, 41.34, 43.14, 40.58,
>> 42.21, 38.94, 44.88, 42.33, 42.61, 41.88, 41.13, 41.83, 42.8)), .Names =
>> c("field",
>> "set", "ent_num", "rep_num", "lp"), class = "data.frame", row.names =
>> c(NA,
>> -787L))
>>
>> # session info
>>
>> R version 3.4.1 (2017-06-30)
>>
>> Platform: i386-w64-mingw32/i386 (32-bit)
>>
>> Running under: Windows 7 x64 (build 7601) Service Pack 1
>>
>>
>>
>> Matrix products: default
>>
>>
>>
>> locale:
>>
>> [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
>> States.1252
>>
>> [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C
>>
>> [5] LC_TIME=English_United States.1252
>>
>>
>>
>> attached base packages:
>>
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>>
>>
>> other attached packages:
>>
>> [1] bindrcpp_0.2 tidyr_0.6.3  dplyr_0.7.4
>>
>>
>>
>> loaded via a namespace (and not attached):
>>
>>  [1] compiler_3.4.1   magrittr_1.5     assertthat_0.2.0 R6_2.2.2
>>  tools_3.4.1
>>
>>  [6] glue_1.1.1       tibble_1.3.3     Rcpp_0.12.11     stringi_1.1.5
>> pkgconfig_2.0.1
>>
>> [11] rlang_0.1.2      bindr_0.1
>>
>> This email and any attachments were sent from a Monsanto email account
>> and may contain confidential and/or privileged information. If you are not
>> the intended recipient, please contact the sender and delete this email and
>> any attachments immediately. Any unauthorized use, including disclosing,
>> printing, storing, copying or distributing this email, is prohibited. All
>> emails and attachments sent to or from Monsanto email accounts may be
>> subject to monitoring, reading, and archiving by Monsanto, including its
>> affiliates and subsidiaries, as permitted by applicable law. Thank you.
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>>
>>
>>
>> This email and any attachments were sent from a Monsanto email account and may contain confidential and/or privileged information. If you are not the intended recipient, please contact the sender and delete this email and any attachments immediately. Any unauthorized use, including disclosing, printing, storing, copying or distributing this email, is prohibited. All emails and attachments sent to or from Monsanto email accounts may be subject to monitoring, reading, and archiving by Monsanto, including its affiliates and subsidiaries, as permitted by applicable law. Thank you.
>>
>>
>>
>>
>

	[[alternative HTML version deleted]]


From nilesh.dighe at monsanto.com  Thu Dec 14 18:11:20 2017
From: nilesh.dighe at monsanto.com (DIGHE, NILESH [AG/2362])
Date: Thu, 14 Dec 2017 17:11:20 +0000
Subject: [R] help with recursive function
In-Reply-To: <CAGgJW75kvGt527SWM6gaNSDkAMg=5Uf3NGAybVY43WzJ65_=2g@mail.gmail.com>
References: <CY1PR0101MB1018F79F34CFE3142A8592ADE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CAGgJW772Nk+Z3XamiGF-2r6Ta9Cud1VOhUvRr4p4k0epTX4xtA@mail.gmail.com>
 <CAGgJW77AAvtOiCwLz=XEvsU+AuqHA=SK8RWLcBD5Ka7P8OFrPA@mail.gmail.com>
 <CY1PR0101MB1018FB7B65C9BDBE0ECA5DCBE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CY1PR0101MB1018B00187792A9F236E549EE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CAGgJW76zkxSQjH+XafCF58xoK8qYT9O0AfijT8cTxFZkYzQpPg@mail.gmail.com>
 <CAGgJW75kvGt527SWM6gaNSDkAMg=5Uf3NGAybVY43WzJ65_=2g@mail.gmail.com>
Message-ID: <CY1PR0101MB10186D7B1B69C7234B36FCBEE20A0@CY1PR0101MB1018.prod.exchangelabs.com>

Eric:  I will try and see if I can figure out the issue by debugging as you suggested. I don?t know why my code after stopifnot is not getting executed where I like the code to run the funlp2 function when the if statement is TRUE but when it is false, I like it to keep running until the stopifnot condition is met.

When the stopifnot condition is met, I like to get the output from if statement saved.
Anyway,  I will keep trying.
Again, Thanks for your help!
Nilesh

From: Eric Berger [mailto:ericjberger at gmail.com]
Sent: Thursday, December 14, 2017 10:29 AM
To: DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] help with recursive function

If you are trying to understand why the "stopifnot" condition is met you can replace it by something like:

if ( any(dat2$norm_sd >= 1) )
   browser()

This will put you in a debugging session where you can examine your variables, e.g.

> dat$norm_sd

HTH,
Eric



On Thu, Dec 14, 2017 at 5:33 PM, Eric Berger <ericjberger at gmail.com<mailto:ericjberger at gmail.com>> wrote:
The message is coming from your stopifnot() condition being met.


On Thu, Dec 14, 2017 at 5:31 PM, DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com>> wrote:
Hi, I accidently left out few lines of code from the calclp function.  Updated function is pasted below.
I am still getting the same error ?Error: !(any(data1$norm_sd >= 1)) is not TRUE?

I would appreciate any help.
Nilesh
dput(calclp)
function (dataset)
{
    dat1 <- funlp1(dataset)
    recursive_funlp <- function(dataset = dat1, func = funlp2) {
        dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
            mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
            spread(key = field_rep, value = lp) %>% mutate_at(.vars = grep("_",
            names(.)), funs(norm = round(scale(.), 3)))
        dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
            1, sd, na.rm = TRUE), 3)
        dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
            1, function(x) {
                max(abs(x), na.rm = TRUE)
            }), 3)
        data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,
            -norm_sd) %>% separate(key, c("field_rep", "treatment"),
            "\\.<file://.>") %>% spread(treatment, value) %>% mutate(outlier = NA)
        stopifnot(!(any(data1$norm_sd >= 1)))
        if (!(any(data1$norm_sd >= 1))) {
            df1 <- dat1
            return(df1)
        }
       else {
            df2 <- recursive_funlp()
            return(df2)
        }
    }
    df3 <- recursive_funlp(dataset = dat1, func = funlp2)
    df3
}


From: DIGHE, NILESH [AG/2362]
Sent: Thursday, December 14, 2017 9:01 AM
To: 'Eric Berger' <ericjberger at gmail.com<mailto:ericjberger at gmail.com>>
Cc: r-help <r-help at r-project.org<mailto:r-help at r-project.org>>
Subject: RE: [R] help with recursive function

Eric:  Thanks for taking time to look into my problem.  Despite of making the change you suggested, I am still getting the same error.  I am wondering if the logic I am using in the stopifnot and if functions is a problem.
I like the recursive function to stop whenever the norm_sd column has zero values that are above or equal to 1. Below is the calclp function after the changes you suggested.
Thanks. Nilesh

dput(calclp)
function (dataset)
{
    dat1 <- funlp1(dataset)
    recursive_funlp <- function(dataset = dat1, func = funlp2) {
        dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
            mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
            spread(key = field_rep, value = lp) %>% mutate_at(.vars = grep("_",
            names(.)), funs(norm = round(scale(.), 3)))
        dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
            1, sd, na.rm = TRUE), 3)
        dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
            1, function(x) {
                max(abs(x), na.rm = TRUE)
            }), 3)
        stopifnot(!(any(dat2$norm_sd >= 1)))
        if (!(any(dat2$norm_sd >= 1))) {
            df1 <- dat1
            return(df1)
        }
        else {
            df2 <- recursive_funlp()
            return(df2)
        }
    }
    df3 <- recursive_funlp(dataset = dat1, func = funlp2)
    df3
}


From: Eric Berger [mailto:ericjberger at gmail.com]
Sent: Thursday, December 14, 2017 8:17 AM
To: DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com>>
Cc: r-help <r-help at r-project.org<mailto:r-help at r-project.org>>
Subject: Re: [R] help with recursive function

My own typo ... whoops ...

!( any(dat2$norm_sd >= 1 ))



On Thu, Dec 14, 2017 at 3:43 PM, Eric Berger <ericjberger at gmail.com<mailto:ericjberger at gmail.com>> wrote:
You seem to have a typo at this expression (and some others like it)

Namely, you write

any(!dat2$norm_sd) >= 1

when you possibly meant to write

!( any(dat2$norm_sd) >= 1 )

i.e. I think your ! seems to be in the wrong place.

HTH,
Eric


On Thu, Dec 14, 2017 at 3:26 PM, DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com>> wrote:
Hi, I need some help with running a recursive function. I like to run funlp2 recursively.
When I try to run recursive function in another function named "calclp" I get this "Error: any(!dat2$norm_sd) >= 1 is not TRUE".

I have never built a recursive function before so having trouble executing it in this case.  I would appreciate any help or guidance to resolve this issue. Please see my data and the three functions that I am using below.
Please note that calclp is the function I am running and the other two functions are within this calclp function.

# code:
Test<- calclp(dataset = dat)

# calclp function

calclp<- function (dataset)

{

    dat1 <- funlp1(dataset)

    recursive_funlp <- function(dataset = dat1, func = funlp2) {

        dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%

            mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%

            spread(key = field_rep, value = lp) %>% mutate_at(.vars = grep("_",

            names(.)), funs(norm = round(scale(.), 3)))

        dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],

            1, sd, na.rm = TRUE), 3)

        dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],

            1, function(x) {

                max(abs(x), na.rm = TRUE)

            }), 3)

        stopifnot(any(!dat2$norm_sd) >= 1)

        if (any(!dat2$norm_sd) >= 1) {

            df1 <- dat1

            return(df1)

        }

        else {

            df2 <- recursive_funlp()

            return(df2)

        }

    }

    df3 <- recursive_funlp(dataset = dat1, func = funlp2)

    df3

}


# funlp1 function

funlp1<- function (dataset)

{

    dat2 <- dataset %>% select(field, set, ent_num, rep_num,

        lp) %>% unite(uniqueid, set, ent_num, sep = ".") %>%

        unite(field_rep, field, rep_num) %>% mutate(field_rep = paste(field_rep,

        "lp", sep = ".")) %>% spread(key = field_rep, value = lp) %>%

        mutate_at(.vars = grep("_", names(.)), funs(norm = round(scale(.),

            3)))

    dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],

        1, sd, na.rm = TRUE), 3)

    dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],

        1, function(x) {

            max(abs(x), na.rm = TRUE)

        }), 3)

    data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,

        -norm_sd) %>% separate(key, c("field_rep", "treatment"),

        "\\.<file://.>") %>% spread(treatment, value) %>% mutate(outlier = NA)

    df_clean <- with(data1, data1[norm_sd < 1, ])

    datD <- with(data1, data1[norm_sd >= 1, ])

    s <- split(datD, datD$uniqueid)

    sdf <- lapply(s, function(x) {

        data.frame(x, x$outlier <- ifelse(is.na<http://is.na>(x$lp_norm), NA,

            ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),

            x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))

        x

    })

    sdf2 <- bind_rows(sdf)

    all_dat <- bind_rows(df_clean, sdf2)

    all_dat

}


# funlp2 function

funlp2<-function (dataset)

{

    data1 <- dataset

    df_clean <- with(data1, data1[norm_sd < 1, ])

    datD <- with(data1, data1[norm_sd >= 1, ])

    s <- split(datD, datD$uniqueid)

    sdf <- lapply(s, function(x) {

        data.frame(x, x$outlier <- ifelse(is.na<http://is.na>(x$lp_norm), NA,

            ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),

            x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))

        x

    })

    sdf2 <- bind_rows(sdf)

    all_dat <- bind_rows(df_clean, sdf2)

    all_dat

}


# dataset
dput(dat)
structure(list(field = c("LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01"), set = c("seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta"), ent_num = c(23L, 14L, 43L, 30L, 44L, 60L, 17L,
34L, 41L, 40L, 9L, 36L, 38L, 19L, 61L, 51L, 45L, 42L, 3L, 39L,
21L, 11L, 12L, 7L, 35L, 5L, 70L, 47L, 28L, 16L, 72L, 13L, 49L,
67L, 56L, 32L, 27L, 46L, 24L, 63L, 15L, 66L, 26L, 29L, 48L, 1L,
54L, 37L, 2L, 50L, 52L, 31L, 33L, 25L, 6L, 69L, 53L, 10L, 18L,
55L, 59L, 4L, 58L, 22L, 20L, 64L, 71L, 57L, 11L, 17L, 43L, 13L,
1L, 16L, 34L, 47L, 52L, 72L, 59L, 22L, 54L, 18L, 25L, 61L, 56L,
41L, 27L, 14L, 49L, 19L, 29L, 31L, 64L, 6L, 53L, 35L, 37L, 67L,
39L, 51L, 40L, 15L, 69L, 60L, 38L, 4L, 23L, 3L, 48L, 32L, 42L,
24L, 28L, 33L, 57L, 8L, 21L, 46L, 7L, 30L, 45L, 2L, 63L, 36L,
68L, 20L, 66L, 70L, 58L, 5L, 10L, 12L, 62L, 50L, 71L, 9L, 55L,
26L, 44L, 65L, 14L, 63L, 46L, 58L, 62L, 19L, 59L, 2L, 5L, 6L,
40L, 21L, 44L, 37L, 55L, 35L, 71L, 56L, 10L, 36L, 53L, 25L, 61L,
12L, 26L, 23L, 4L, 13L, 28L, 38L, 57L, 54L, 72L, 48L, 66L, 9L,
70L, 15L, 39L, 60L, 17L, 34L, 51L, 67L, 42L, 49L, 31L, 30L, 3L,
18L, 65L, 32L, 27L, 52L, 22L, 11L, 47L, 64L, 8L, 43L, 41L, 16L,
20L, 33L, 7L, 50L, 68L, 24L, 1L, 69L, 45L, 29L, 37L, 30L, 55L,
54L, 43L, 32L, 21L, 27L, 33L, 40L, 67L, 57L, 68L, 31L, 17L, 13L,
6L, 62L, 19L, 22L, 3L, 10L, 44L, 34L, 69L, 70L, 4L, 1L, 25L,
11L, 51L, 5L, 63L, 71L, 12L, 38L, 58L, 39L, 49L, 59L, 56L, 65L,
2L, 64L, 8L, 35L, 46L, 45L, 29L, 53L, 36L, 42L, 23L, 18L, 50L,
26L, 14L, 48L, 66L, 20L, 24L, 7L, 15L, 53L, 22L, 39L, 20L, 60L,
59L, 43L, 19L, 41L, 6L, 62L, 1L, 55L, 34L, 50L, 38L, 40L, 44L,
4L, 46L, 29L, 65L, 57L, 48L, 33L, 69L, 14L, 35L, 67L, 72L, 54L,
3L, 49L, 2L, 12L, 18L, 30L, 10L, 70L, 31L, 15L, 63L, 71L, 21L,
45L, 28L, 56L, 27L, 64L, 61L, 51L, 5L, 24L, 68L, 25L, 66L, 16L,
36L, 58L, 37L, 52L, 26L, 9L, 42L, 7L, 11L, 8L, 32L, 23L, 13L,
47L, 17L, 61L, 72L, 47L, 60L, 16L, 9L, 28L, 52L, 41L, 1L, 61L,
6L, 23L, 58L, 63L, 25L, 28L, 30L, 36L, 62L, 9L, 32L, 19L, 31L,
56L, 45L, 2L, 22L, 27L, 40L, 14L, 11L, 50L, 13L, 70L, 20L, 64L,
39L, 26L, 21L, 43L, 29L, 35L, 54L, 52L, 37L, 17L, 16L, 72L, 48L,
12L, 18L, 44L, 42L, 49L, 68L, 5L, 55L, 69L, 51L, 66L, 59L, 53L,
15L, 71L, 41L, 57L, 4L, 60L, 8L, 7L, 33L, 34L, 24L, 10L, 67L,
47L, 38L, 3L, 65L, 46L, 20L, 34L, 71L, 1L, 33L, 57L, 13L, 21L,
66L, 29L, 3L, 61L, 69L, 24L, 62L, 39L, 49L, 47L, 31L, 53L, 52L,
43L, 17L, 7L, 8L, 12L, 60L, 63L, 50L, 2L, 51L, 46L, 10L, 23L,
48L, 11L, 26L, 40L, 70L, 42L, 59L, 15L, 56L, 58L, 27L, 6L, 35L,
4L, 37L, 5L, 65L, 44L, 28L, 14L, 32L, 36L, 45L, 9L, 18L, 55L,
68L, 30L, 54L, 41L, 25L, 22L, 38L, 16L, 67L, 64L, 19L, 72L, 68L,
28L, 33L, 15L, 51L, 4L, 47L, 36L, 8L, 57L, 48L, 1L, 52L, 39L,
32L, 50L, 13L, 30L, 63L, 2L, 9L, 62L, 22L, 6L, 61L, 16L, 53L,
38L, 37L, 20L, 69L, 44L, 56L, 29L, 26L, 14L, 17L, 46L, 66L, 58L,
42L, 60L, 19L, 45L, 3L, 59L, 70L, 31L, 24L, 55L, 40L, 43L, 25L,
65L, 12L, 67L, 21L, 7L, 27L, 49L, 72L, 54L, 41L, 23L, 34L, 5L,
64L, 35L, 18L, 71L, 11L, 24L, 19L, 38L, 14L, 4L, 56L, 5L, 54L,
34L, 64L, 55L, 33L, 69L, 71L, 52L, 61L, 48L, 23L, 43L, 41L, 20L,
39L, 11L, 63L, 36L, 22L, 9L, 25L, 27L, 51L, 53L, 37L, 57L, 13L,
18L, 64L, 22L, 53L, 16L, 5L, 28L, 60L, 31L, 11L, 29L, 45L, 59L,
72L, 49L, 67L, 13L, 20L, 3L, 42L, 44L, 69L, 33L, 38L, 15L, 70L,
35L, 48L, 26L, 56L, 19L, 39L, 43L, 40L, 14L, 2L, 68L, 51L, 12L,
47L, 10L, 55L, 23L, 4L, 71L, 41L, 50L, 7L, 24L, 61L, 27L, 54L,
46L, 58L, 37L, 66L, 57L, 1L, 36L, 32L, 18L, 62L, 9L, 30L, 21L,
6L, 52L, 8L, 65L, 17L, 25L, 63L, 34L, 65L, 22L, 56L, 9L, 7L,
11L, 31L, 4L, 63L, 29L, 61L, 54L, 12L, 62L, 59L, 5L, 23L, 53L,
36L, 24L, 35L, 66L, 49L, 72L, 18L, 70L, 32L, 43L, 20L, 45L, 34L,
46L, 28L, 6L, 44L, 71L, 39L, 13L, 27L, 1L, 58L, 30L, 68L, 17L,
33L, 26L, 57L, 15L, 21L, 52L, 48L, 42L, 16L, 40L, 38L, 8L, 69L,
2L, 51L, 67L, 55L, 64L, 47L, 60L, 19L, 41L, 50L, 3L, 14L, 25L,
10L, 37L, 6L, 15L, 45L, 49L, 8L, 17L, 50L, 16L, 58L, 72L, 26L,
60L, 7L, 32L, 1L, 46L, 66L, 68L, 62L, 47L, 35L, 70L, 10L, 31L,
65L, 2L, 3L, 21L, 12L, 30L, 40L, 28L, 59L, 42L, 67L, 44L, 29L
), rep_num = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L), lp = c(NA, 41.64, 38.8, 44.45, 40.54,
38.54, 41.94, 39.6, 37.39, 40.95, 38.45, 43.47, 41.66, 40.91,
42.68, 43.12, 38.22, 40.95, 46.24, 42.95, 38.95, 39.88, 40.57,
40.13, 38.57, 45.45, 40.78, 43.52, 39.75, 39.93, 40.35, 37.6,
37.7, 43.05, 43.32, 41.31, 39.03, 42.5, 43.18, 41.32, 39.58,
40.62, 39.64, 39.85, 38.75, 40.18, 41.44, 40.5, 40.87, 40.75,
38.37, 40.26, 35.11, 40.89, 41.67, 38.87, 37.32, 35.85, 38.25,
42.21, 43.15, 38.69, 38.8, 38.77, 37.98, 39.8, 33.37, 40.09,
42.87, 44.07, 43.78, 42.47, 42.8, 41.1, 41.17, 44.07, 44.24,
43.16, 46.12, 42.64, 43.92, 42.16, 43.69, 42.89, 42.63, 43.58,
44.4, 43.17, 43.3, 42.5, 42.6, 44.05, 44.63, 43.09, 45.17, 45.17,
42.78, 42.38, 45.76, 42.16, 44.22, 43.67, 40.45, 41.95, 42.49,
41.98, 45.15, 46.01, 38.51, 43.08, 45.19, 44.53, 43.14, 39.93,
46.84, 43.59, 41.68, 43.7, 44.63, 44.02, 42.07, 43.88, 43.2,
46.2, 40.84, 39.14, 43.89, 42.58, 41.53, 45.32, 39.56, 43.77,
45.45, 45.16, 43.4, 40.08, 42.27, 43.74, 43.77, 41.31, 41.38,
45.01, 45.76, 40.4, 48.39, 46.27, 15.44, 44.76, 47.45, 44.87,
46.8, 41.83, 43.03, 43.96, 42.51, 45.06, 45.55, 44.9, 42.47,
44.9, 45, 44.77, 43.79, 44.26, 44.57, 44.4, 43.42, 42.31, 47.18,
43.83, 45.72, 44.83, 44.96, 40.28, 42.85, 41.23, 45.23, 47.09,
43.61, 42.69, 46.27, 45.16, 44.14, 43.34, 45.97, 43.81, 43.01,
39.82, 45.91, 45.97, 43.61, 45.12, 46.37, 42.67, 42.47, 45.86,
44.19, 44.46, 42.64, 43.95, 44.93, 40.33, 42.75, 39.92, 44.17,
44.49, 41.51, 42.43, 44.14, 40.5, 41.29, 44.89, 37.98, 39.02,
39.62, 42.13, 39.03, 44.16, 39.15, 41.49, 42.63, 40.11, 39.97,
42.85, 35.98, 39.45, 40.99, 42.44, 42.11, 37.36, 40.63, 40.69,
43.57, 39.04, 39.3, 42.19, 36.88, 40.39, 37.78, 38.6, 40.2, 40.98,
36.58, 43.59, 42.49, 39.96, 39.84, 40.43, 38.94, 42.72, 39.43,
42.13, 40.36, 40.58, 40.01, 42.17, 42.17, 41.35, 43.27, 40.15,
39.76, 40.94, 40.87, 42.32, 41.81, 41.97, 43.72, 41.32, 40.83,
37.64, 41.03, 38.98, 40.61, 41.17, 41.96, 40.07, 38.48, 42.85,
38.68, 39.09, 42.16, 38.14, 37.99, 41.06, 37.4, 41.88, 39.35,
39.73, 38.38, 41.34, 40.67, 40.89, 39.28, 37.59, 39.3, 39.72,
40.79, 39.42, 34.5, 37.61, 37.76, 40.24, 41.17, 41.24, 42.14,
42.53, 39.71, 39.44, 40.19, 42.51, 40.15, 36.26, 37.48, 40.43,
37.5, 42.11, 41.44, 40.29, 39.75, 37.58, 41.25, 40.39, 41.63,
42.18, 43.71, 38.69, 43.47, 41.98, 35.51, 42.22, 38.51, 40.17,
39.4, 39.54, 41.7, 40.93, 40.01, 35.97, 41.44, 41.89, 40.9, 39.95,
40.69, 43.18, 37.03, 39.91, 36.75, 42.75, 41.61, 42.6, 38.55,
42.84, 38.41, 42.55, 41.44, 41.03, 40, 41.35, 42.18, 42.66, 40.06,
42.48, 41.33, 41.92, 40.88, 40.99, 42.78, 38.26, 40.81, 39.95,
37.89, 40.81, 38.33, 39.33, 39.67, 40.12, 41.75, 39.81, 41.92,
44.59, 37.8, 40.54, 40.49, 41.82, 42.13, 39.93, 39.94, 42.54,
41.74, 43.06, 41.72, 41.27, 39.42, 42.07, 40.06, 42.1, 38.91,
39.28, 42.94, 39.94, 41.86, 38.56, 38.15, 41.47, 42.34, 38.14,
40.19, 40.2, 43.01, 42.35, 40.89, 41.44, 42.89, 44.49, 39.1,
38.42, 37.44, 43.28, 37.96, 40.56, 41.53, 41.59, 41.45, 42.55,
40.6, 44.04, 39.31, 41.08, 37.14, 40.03, 41.49, 40.82, 38.47,
43.43, 38.82, 40.4, 41.09, 42.26, 41.85, 41.13, 37.27, 41.97,
45.24, 43.35, 40.53, 43.14, 39.71, 42.77, 42.2, 42.13, 41.91,
42.59, 41.99, 42.11, 40.43, 40.02, 43.53, 43.01, 39.55, 43.2,
39.95, 42.51, 42, 42.26, 42.39, 42.64, 39.9, 41.89, 43.05, 41.02,
41.95, 39.92, 43.42, 44.01, 42.6, 40.84, 42.86, 44.31, 42.24,
41.25, 42.38, 44.99, 41.24, 43.97, 41.12, 37.88, 41.53, 41.56,
39.18, 40.83, 42.96, 41.92, 43.86, 42.48, 42.65, 43.3, 41.99,
42.51, 40.65, 42.77, 34.71, 40.97, 38.15, 39.76, 36.74, 37.95,
39.17, 38.22, 39.31, 43.57, 37.21, 39.35, 42.37, 42.01, 42.39,
43.21, 36.91, 36.69, 41.07, 41.91, 34.63, 40.61, 36.23, 38.12,
40.76, 39.14, 41.81, 39.14, 41.04, 37.32, 40.83, 40.81, 38.29,
39.61, 40.96, 40.71, 40.47, 38.64, 39.76, 38.19, 39.05, 38.04,
41.14, 38.35, 42.3, 34.44, 40.93, 39.3, 41.44, 38.3, 42.74, 36.66,
40.02, 36.62, 40.48, 41.72, 41.23, 41.81, 42.07, 40.22, 37.83,
36.54, 37.77, 40.49, 38.65, 43.2, 43.32, 40.67, 41.95, 36.11,
39.28, 42.38, 40.35, 40.3, 43.48, 40.55, 40.54, 44.03, 41.94,
37.97, 41.98, 41.53, 38.19, 38.66, 41.18, 41.95, 42.53, 38.7,
44.55, 42.39, 41.55, 38.46, 42.27, 42.19, 41.95, 41.81, 39.81,
38.12, 42.94, 42.99, 38.85, 41.26, 43.13, 44.21, 38.54, 44.02,
43.46, 41.64, 42.06, 42.11, 42.34, 41.86, 37.91, 40.89, 40.5,
39.54, 37.87, 40.86, 41.36, 41.77, 42.03, 39.15, 40.04, 44.11,
41.34, 42.97, 38.42, 37.28, 41.04, 41.48, 38.82, 41.94, 37.95,
40.9, 40.39, 40.31, 43.19, 41.22, 41.49, 41.25, 40.07, 36.7,
39.97, 39.99, 41.7, 37.09, 42.58, 43.01, 37.7, 41.81, 39.99,
42.95, 43.19, 42.69, 41.5, 40.64, 43.24, 41.14, 41.21, 41.29,
41.43, 44.21, 38.52, 42.54, 40.54, 42.49, 43.2, 38.12, 40.08,
39.02, 41.45, 42.33, 41.11, 38.93, 41.63, 44.22, 41.41, 39.08,
40.9, 41.1, 43.88, 40.96, 46.75, 47.54, 40.35, 41.97, 44.94,
44.91, 44.66, 44.5, 44.4, 46.4, 47.97, 46.05, 45.57, 42.83, 41.48,
47.48, 45.43, 41.98, 43.14, 45.6, 44.78, 45.45, 45.69, 44.82,
44.24, 41.14, 43.14, 46.61, 43.92, 43.56, 43.68, 45.37, 45.15,
40.76, 43.78, 44.67, 41.36, 41.4, 40.97, 41.87, 39.83, 43.8,
48.36, 44.28, 43.29, 44.69, 43.26, 43.35, 44.34, 45.08, 42.26,
39.7, 42.4, 44.03, 43.22, 42.71, 45.89, 44.89, 44.81, 42.59,
40.85, 43.82, 44.85, 47.47, 43.64, 42.65, 45.67, 43.24, 42.33,
40.61, 38.3, 39.84, 41.08, 42.33, 44.44, 40.85, 39.58, 42.55,
41.75, 39.44, 41.79, 39.31, 41.34, 42.76, 40.79, 37.31, 42.85,
42.88, 42.01, 44.63, 38.85, 41.13, 40.43, 41.34, 43.14, 40.58,
42.21, 38.94, 44.88, 42.33, 42.61, 41.88, 41.13, 41.83, 42.8)), .Names = c("field",
"set", "ent_num", "rep_num", "lp"), class = "data.frame", row.names = c(NA,
-787L))

# session info

R version 3.4.1 (2017-06-30)

Platform: i386-w64-mingw32/i386 (32-bit)

Running under: Windows 7 x64 (build 7601) Service Pack 1



Matrix products: default



locale:

[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252

[3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C

[5] LC_TIME=English_United States.1252



attached base packages:

[1] stats     graphics  grDevices utils     datasets  methods   base



other attached packages:

[1] bindrcpp_0.2 tidyr_0.6.3  dplyr_0.7.4



loaded via a namespace (and not attached):

 [1] compiler_3.4.1   magrittr_1.5     assertthat_0.2.0 R6_2.2.2         tools_3.4.1

 [6] glue_1.1.1       tibble_1.3.3     Rcpp_0.12.11     stringi_1.1.5    pkgconfig_2.0.1

[11] rlang_0.1.2      bindr_0.1

This email and any attachments were sent from a Monsanto email account and may contain confidential and/or privileged information. If you are not the intended recipient, please contact the sender and delete this email and any attachments immediately. Any unauthorized use, including disclosing, printing, storing, copying or distributing this email, is prohibited. All emails and attachments sent to or from Monsanto email accounts may be subject to monitoring, reading, and archiving by Monsanto, including its affiliates and subsidiaries, as permitted by applicable law. Thank you.

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.






This email and any attachments were sent from a Monsanto email account and may contain confidential and/or privileged information. If you are not the intended recipient, please contact the sender and delete this email and any attachments immediately. Any unauthorized use, including disclosing, printing, storing, copying or distributing this email, is prohibited. All emails and attachments sent to or from Monsanto email accounts may be subject to monitoring, reading, and archiving by Monsanto, including its affiliates and subsidiaries, as permitted by applicable law. Thank you.





This email and any attachments were sent from a Monsanto email account and may contain confidential and/or privileged information. If you are not the intended recipient, please contact the sender and delete this email and any attachments immediately. Any unauthorized use, including disclosing, printing, storing, copying or distributing this email, is prohibited. All emails and attachments sent to or from Monsanto email accounts may be subject to monitoring, reading, and archiving by Monsanto, including its affiliates and subsidiaries, as permitted by applicable law. Thank you.

	[[alternative HTML version deleted]]


From porzalinda at gmail.com  Thu Dec 14 15:35:36 2017
From: porzalinda at gmail.com (linda porz)
Date: Thu, 14 Dec 2017 14:35:36 +0000
Subject: [R] permutation test for Cox proportional hazards regression model
Message-ID: <CALoJSBeU+0_C8JMHA_rewvUf2jsfL9yG_xB9c+Pxf52fHSySQQ@mail.gmail.com>

I would like to perform a permutation test for Cox proportional hazards
regression model. I only find it for t-test and other tests (e.g. comparing
two medians).

Is there a way that I can perform a Cox PH model in R or SAS for the
LR-test?

I am doing the following

B <- 1000; LRtestx <- rep(NA,B);

Srv <- Surv(Time, Event);

for(j in 1:B){ LRtestx[j] <- cph(Srv~sample(x,length(x),replace=F))$stat[3]};

LRtest <- cph(Srv~x)$stat[3];

sum(LRtestx > LRtest)/B

Many thanks

Linda

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Thu Dec 14 18:26:23 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 14 Dec 2017 09:26:23 -0800
Subject: [R] help with recursive function
In-Reply-To: <CY1PR0101MB10186D7B1B69C7234B36FCBEE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
References: <CY1PR0101MB1018F79F34CFE3142A8592ADE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CAGgJW772Nk+Z3XamiGF-2r6Ta9Cud1VOhUvRr4p4k0epTX4xtA@mail.gmail.com>
 <CAGgJW77AAvtOiCwLz=XEvsU+AuqHA=SK8RWLcBD5Ka7P8OFrPA@mail.gmail.com>
 <CY1PR0101MB1018FB7B65C9BDBE0ECA5DCBE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CY1PR0101MB1018B00187792A9F236E549EE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CAGgJW76zkxSQjH+XafCF58xoK8qYT9O0AfijT8cTxFZkYzQpPg@mail.gmail.com>
 <CAGgJW75kvGt527SWM6gaNSDkAMg=5Uf3NGAybVY43WzJ65_=2g@mail.gmail.com>
 <CY1PR0101MB10186D7B1B69C7234B36FCBEE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
Message-ID: <CAF8bMcaucmVz-V4-C2YB15tJt43WmdAwxXFwdSnxSqDfXZwYyA@mail.gmail.com>

Your code contains the lines
        stopifnot(!(any(data1$norm_sd >= 1)))
        if            (!(any(data1$norm_sd >= 1))) {
            df1 <- dat1
            return(df1)
        }

stop() "throws an error", causing the current function and all functions in
the call
stack to abort and return nothing.  It does not mean to stop now and return
a result.
Does the function give the correct results if you just leave out the
stopifnot line?


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Dec 14, 2017 at 9:11 AM, DIGHE, NILESH [AG/2362] <
nilesh.dighe at monsanto.com> wrote:

> Eric:  I will try and see if I can figure out the issue by debugging as
> you suggested. I don?t know why my code after stopifnot is not getting
> executed where I like the code to run the funlp2 function when the if
> statement is TRUE but when it is false, I like it to keep running until the
> stopifnot condition is met.
>
> When the stopifnot condition is met, I like to get the output from if
> statement saved.
> Anyway,  I will keep trying.
> Again, Thanks for your help!
> Nilesh
>
> From: Eric Berger [mailto:ericjberger at gmail.com]
> Sent: Thursday, December 14, 2017 10:29 AM
> To: DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com>
> Cc: r-help <r-help at r-project.org>
> Subject: Re: [R] help with recursive function
>
> If you are trying to understand why the "stopifnot" condition is met you
> can replace it by something like:
>
> if ( any(dat2$norm_sd >= 1) )
>    browser()
>
> This will put you in a debugging session where you can examine your
> variables, e.g.
>
> > dat$norm_sd
>
> HTH,
> Eric
>
>
>
> On Thu, Dec 14, 2017 at 5:33 PM, Eric Berger <ericjberger at gmail.com
> <mailto:ericjberger at gmail.com>> wrote:
> The message is coming from your stopifnot() condition being met.
>
>
> On Thu, Dec 14, 2017 at 5:31 PM, DIGHE, NILESH [AG/2362] <
> nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com>> wrote:
> Hi, I accidently left out few lines of code from the calclp function.
> Updated function is pasted below.
> I am still getting the same error ?Error: !(any(data1$norm_sd >= 1)) is
> not TRUE?
>
> I would appreciate any help.
> Nilesh
> dput(calclp)
> function (dataset)
> {
>     dat1 <- funlp1(dataset)
>     recursive_funlp <- function(dataset = dat1, func = funlp2) {
>         dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
>             mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
>             spread(key = field_rep, value = lp) %>% mutate_at(.vars =
> grep("_",
>             names(.)), funs(norm = round(scale(.), 3)))
>         dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>             1, sd, na.rm = TRUE), 3)
>         dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>             1, function(x) {
>                 max(abs(x), na.rm = TRUE)
>             }), 3)
>         data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,
>             -norm_sd) %>% separate(key, c("field_rep", "treatment"),
>             "\\.<file://.>") %>% spread(treatment, value) %>%
> mutate(outlier = NA)
>         stopifnot(!(any(data1$norm_sd >= 1)))
>         if (!(any(data1$norm_sd >= 1))) {
>             df1 <- dat1
>             return(df1)
>         }
>        else {
>             df2 <- recursive_funlp()
>             return(df2)
>         }
>     }
>     df3 <- recursive_funlp(dataset = dat1, func = funlp2)
>     df3
> }
>
>
> From: DIGHE, NILESH [AG/2362]
> Sent: Thursday, December 14, 2017 9:01 AM
> To: 'Eric Berger' <ericjberger at gmail.com<mailto:ericjberger at gmail.com>>
> Cc: r-help <r-help at r-project.org<mailto:r-help at r-project.org>>
> Subject: RE: [R] help with recursive function
>
> Eric:  Thanks for taking time to look into my problem.  Despite of making
> the change you suggested, I am still getting the same error.  I am
> wondering if the logic I am using in the stopifnot and if functions is a
> problem.
> I like the recursive function to stop whenever the norm_sd column has zero
> values that are above or equal to 1. Below is the calclp function after the
> changes you suggested.
> Thanks. Nilesh
>
> dput(calclp)
> function (dataset)
> {
>     dat1 <- funlp1(dataset)
>     recursive_funlp <- function(dataset = dat1, func = funlp2) {
>         dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
>             mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
>             spread(key = field_rep, value = lp) %>% mutate_at(.vars =
> grep("_",
>             names(.)), funs(norm = round(scale(.), 3)))
>         dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>             1, sd, na.rm = TRUE), 3)
>         dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>             1, function(x) {
>                 max(abs(x), na.rm = TRUE)
>             }), 3)
>         stopifnot(!(any(dat2$norm_sd >= 1)))
>         if (!(any(dat2$norm_sd >= 1))) {
>             df1 <- dat1
>             return(df1)
>         }
>         else {
>             df2 <- recursive_funlp()
>             return(df2)
>         }
>     }
>     df3 <- recursive_funlp(dataset = dat1, func = funlp2)
>     df3
> }
>
>
> From: Eric Berger [mailto:ericjberger at gmail.com]
> Sent: Thursday, December 14, 2017 8:17 AM
> To: DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com<mailto:
> nilesh.dighe at monsanto.com>>
> Cc: r-help <r-help at r-project.org<mailto:r-help at r-project.org>>
> Subject: Re: [R] help with recursive function
>
> My own typo ... whoops ...
>
> !( any(dat2$norm_sd >= 1 ))
>
>
>
> On Thu, Dec 14, 2017 at 3:43 PM, Eric Berger <ericjberger at gmail.com
> <mailto:ericjberger at gmail.com>> wrote:
> You seem to have a typo at this expression (and some others like it)
>
> Namely, you write
>
> any(!dat2$norm_sd) >= 1
>
> when you possibly meant to write
>
> !( any(dat2$norm_sd) >= 1 )
>
> i.e. I think your ! seems to be in the wrong place.
>
> HTH,
> Eric
>
>
> On Thu, Dec 14, 2017 at 3:26 PM, DIGHE, NILESH [AG/2362] <
> nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com>> wrote:
> Hi, I need some help with running a recursive function. I like to run
> funlp2 recursively.
> When I try to run recursive function in another function named "calclp" I
> get this "Error: any(!dat2$norm_sd) >= 1 is not TRUE".
>
> I have never built a recursive function before so having trouble executing
> it in this case.  I would appreciate any help or guidance to resolve this
> issue. Please see my data and the three functions that I am using below.
> Please note that calclp is the function I am running and the other two
> functions are within this calclp function.
>
> # code:
> Test<- calclp(dataset = dat)
>
> # calclp function
>
> calclp<- function (dataset)
>
> {
>
>     dat1 <- funlp1(dataset)
>
>     recursive_funlp <- function(dataset = dat1, func = funlp2) {
>
>         dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
>
>             mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
>
>             spread(key = field_rep, value = lp) %>% mutate_at(.vars =
> grep("_",
>
>             names(.)), funs(norm = round(scale(.), 3)))
>
>         dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>             1, sd, na.rm = TRUE), 3)
>
>         dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>             1, function(x) {
>
>                 max(abs(x), na.rm = TRUE)
>
>             }), 3)
>
>         stopifnot(any(!dat2$norm_sd) >= 1)
>
>         if (any(!dat2$norm_sd) >= 1) {
>
>             df1 <- dat1
>
>             return(df1)
>
>         }
>
>         else {
>
>             df2 <- recursive_funlp()
>
>             return(df2)
>
>         }
>
>     }
>
>     df3 <- recursive_funlp(dataset = dat1, func = funlp2)
>
>     df3
>
> }
>
>
> # funlp1 function
>
> funlp1<- function (dataset)
>
> {
>
>     dat2 <- dataset %>% select(field, set, ent_num, rep_num,
>
>         lp) %>% unite(uniqueid, set, ent_num, sep = ".") %>%
>
>         unite(field_rep, field, rep_num) %>% mutate(field_rep =
> paste(field_rep,
>
>         "lp", sep = ".")) %>% spread(key = field_rep, value = lp) %>%
>
>         mutate_at(.vars = grep("_", names(.)), funs(norm = round(scale(.),
>
>             3)))
>
>     dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>         1, sd, na.rm = TRUE), 3)
>
>     dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>         1, function(x) {
>
>             max(abs(x), na.rm = TRUE)
>
>         }), 3)
>
>     data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,
>
>         -norm_sd) %>% separate(key, c("field_rep", "treatment"),
>
>         "\\.<file://.>") %>% spread(treatment, value) %>% mutate(outlier =
> NA)
>
>     df_clean <- with(data1, data1[norm_sd < 1, ])
>
>     datD <- with(data1, data1[norm_sd >= 1, ])
>
>     s <- split(datD, datD$uniqueid)
>
>     sdf <- lapply(s, function(x) {
>
>         data.frame(x, x$outlier <- ifelse(is.na<http://is.na>(x$lp_norm),
> NA,
>
>             ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),
>
>             x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))
>
>         x
>
>     })
>
>     sdf2 <- bind_rows(sdf)
>
>     all_dat <- bind_rows(df_clean, sdf2)
>
>     all_dat
>
> }
>
>
> # funlp2 function
>
> funlp2<-function (dataset)
>
> {
>
>     data1 <- dataset
>
>     df_clean <- with(data1, data1[norm_sd < 1, ])
>
>     datD <- with(data1, data1[norm_sd >= 1, ])
>
>     s <- split(datD, datD$uniqueid)
>
>     sdf <- lapply(s, function(x) {
>
>         data.frame(x, x$outlier <- ifelse(is.na<http://is.na>(x$lp_norm),
> NA,
>
>             ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),
>
>             x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))
>
>         x
>
>     })
>
>     sdf2 <- bind_rows(sdf)
>
>     all_dat <- bind_rows(df_clean, sdf2)
>
>     all_dat
>
> }
>
>
> # dataset
> dput(dat)
> structure(list(field = c("LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01"), set = c("seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta"), ent_num = c(23L, 14L, 43L, 30L, 44L, 60L, 17L,
> 34L, 41L, 40L, 9L, 36L, 38L, 19L, 61L, 51L, 45L, 42L, 3L, 39L,
> 21L, 11L, 12L, 7L, 35L, 5L, 70L, 47L, 28L, 16L, 72L, 13L, 49L,
> 67L, 56L, 32L, 27L, 46L, 24L, 63L, 15L, 66L, 26L, 29L, 48L, 1L,
> 54L, 37L, 2L, 50L, 52L, 31L, 33L, 25L, 6L, 69L, 53L, 10L, 18L,
> 55L, 59L, 4L, 58L, 22L, 20L, 64L, 71L, 57L, 11L, 17L, 43L, 13L,
> 1L, 16L, 34L, 47L, 52L, 72L, 59L, 22L, 54L, 18L, 25L, 61L, 56L,
> 41L, 27L, 14L, 49L, 19L, 29L, 31L, 64L, 6L, 53L, 35L, 37L, 67L,
> 39L, 51L, 40L, 15L, 69L, 60L, 38L, 4L, 23L, 3L, 48L, 32L, 42L,
> 24L, 28L, 33L, 57L, 8L, 21L, 46L, 7L, 30L, 45L, 2L, 63L, 36L,
> 68L, 20L, 66L, 70L, 58L, 5L, 10L, 12L, 62L, 50L, 71L, 9L, 55L,
> 26L, 44L, 65L, 14L, 63L, 46L, 58L, 62L, 19L, 59L, 2L, 5L, 6L,
> 40L, 21L, 44L, 37L, 55L, 35L, 71L, 56L, 10L, 36L, 53L, 25L, 61L,
> 12L, 26L, 23L, 4L, 13L, 28L, 38L, 57L, 54L, 72L, 48L, 66L, 9L,
> 70L, 15L, 39L, 60L, 17L, 34L, 51L, 67L, 42L, 49L, 31L, 30L, 3L,
> 18L, 65L, 32L, 27L, 52L, 22L, 11L, 47L, 64L, 8L, 43L, 41L, 16L,
> 20L, 33L, 7L, 50L, 68L, 24L, 1L, 69L, 45L, 29L, 37L, 30L, 55L,
> 54L, 43L, 32L, 21L, 27L, 33L, 40L, 67L, 57L, 68L, 31L, 17L, 13L,
> 6L, 62L, 19L, 22L, 3L, 10L, 44L, 34L, 69L, 70L, 4L, 1L, 25L,
> 11L, 51L, 5L, 63L, 71L, 12L, 38L, 58L, 39L, 49L, 59L, 56L, 65L,
> 2L, 64L, 8L, 35L, 46L, 45L, 29L, 53L, 36L, 42L, 23L, 18L, 50L,
> 26L, 14L, 48L, 66L, 20L, 24L, 7L, 15L, 53L, 22L, 39L, 20L, 60L,
> 59L, 43L, 19L, 41L, 6L, 62L, 1L, 55L, 34L, 50L, 38L, 40L, 44L,
> 4L, 46L, 29L, 65L, 57L, 48L, 33L, 69L, 14L, 35L, 67L, 72L, 54L,
> 3L, 49L, 2L, 12L, 18L, 30L, 10L, 70L, 31L, 15L, 63L, 71L, 21L,
> 45L, 28L, 56L, 27L, 64L, 61L, 51L, 5L, 24L, 68L, 25L, 66L, 16L,
> 36L, 58L, 37L, 52L, 26L, 9L, 42L, 7L, 11L, 8L, 32L, 23L, 13L,
> 47L, 17L, 61L, 72L, 47L, 60L, 16L, 9L, 28L, 52L, 41L, 1L, 61L,
> 6L, 23L, 58L, 63L, 25L, 28L, 30L, 36L, 62L, 9L, 32L, 19L, 31L,
> 56L, 45L, 2L, 22L, 27L, 40L, 14L, 11L, 50L, 13L, 70L, 20L, 64L,
> 39L, 26L, 21L, 43L, 29L, 35L, 54L, 52L, 37L, 17L, 16L, 72L, 48L,
> 12L, 18L, 44L, 42L, 49L, 68L, 5L, 55L, 69L, 51L, 66L, 59L, 53L,
> 15L, 71L, 41L, 57L, 4L, 60L, 8L, 7L, 33L, 34L, 24L, 10L, 67L,
> 47L, 38L, 3L, 65L, 46L, 20L, 34L, 71L, 1L, 33L, 57L, 13L, 21L,
> 66L, 29L, 3L, 61L, 69L, 24L, 62L, 39L, 49L, 47L, 31L, 53L, 52L,
> 43L, 17L, 7L, 8L, 12L, 60L, 63L, 50L, 2L, 51L, 46L, 10L, 23L,
> 48L, 11L, 26L, 40L, 70L, 42L, 59L, 15L, 56L, 58L, 27L, 6L, 35L,
> 4L, 37L, 5L, 65L, 44L, 28L, 14L, 32L, 36L, 45L, 9L, 18L, 55L,
> 68L, 30L, 54L, 41L, 25L, 22L, 38L, 16L, 67L, 64L, 19L, 72L, 68L,
> 28L, 33L, 15L, 51L, 4L, 47L, 36L, 8L, 57L, 48L, 1L, 52L, 39L,
> 32L, 50L, 13L, 30L, 63L, 2L, 9L, 62L, 22L, 6L, 61L, 16L, 53L,
> 38L, 37L, 20L, 69L, 44L, 56L, 29L, 26L, 14L, 17L, 46L, 66L, 58L,
> 42L, 60L, 19L, 45L, 3L, 59L, 70L, 31L, 24L, 55L, 40L, 43L, 25L,
> 65L, 12L, 67L, 21L, 7L, 27L, 49L, 72L, 54L, 41L, 23L, 34L, 5L,
> 64L, 35L, 18L, 71L, 11L, 24L, 19L, 38L, 14L, 4L, 56L, 5L, 54L,
> 34L, 64L, 55L, 33L, 69L, 71L, 52L, 61L, 48L, 23L, 43L, 41L, 20L,
> 39L, 11L, 63L, 36L, 22L, 9L, 25L, 27L, 51L, 53L, 37L, 57L, 13L,
> 18L, 64L, 22L, 53L, 16L, 5L, 28L, 60L, 31L, 11L, 29L, 45L, 59L,
> 72L, 49L, 67L, 13L, 20L, 3L, 42L, 44L, 69L, 33L, 38L, 15L, 70L,
> 35L, 48L, 26L, 56L, 19L, 39L, 43L, 40L, 14L, 2L, 68L, 51L, 12L,
> 47L, 10L, 55L, 23L, 4L, 71L, 41L, 50L, 7L, 24L, 61L, 27L, 54L,
> 46L, 58L, 37L, 66L, 57L, 1L, 36L, 32L, 18L, 62L, 9L, 30L, 21L,
> 6L, 52L, 8L, 65L, 17L, 25L, 63L, 34L, 65L, 22L, 56L, 9L, 7L,
> 11L, 31L, 4L, 63L, 29L, 61L, 54L, 12L, 62L, 59L, 5L, 23L, 53L,
> 36L, 24L, 35L, 66L, 49L, 72L, 18L, 70L, 32L, 43L, 20L, 45L, 34L,
> 46L, 28L, 6L, 44L, 71L, 39L, 13L, 27L, 1L, 58L, 30L, 68L, 17L,
> 33L, 26L, 57L, 15L, 21L, 52L, 48L, 42L, 16L, 40L, 38L, 8L, 69L,
> 2L, 51L, 67L, 55L, 64L, 47L, 60L, 19L, 41L, 50L, 3L, 14L, 25L,
> 10L, 37L, 6L, 15L, 45L, 49L, 8L, 17L, 50L, 16L, 58L, 72L, 26L,
> 60L, 7L, 32L, 1L, 46L, 66L, 68L, 62L, 47L, 35L, 70L, 10L, 31L,
> 65L, 2L, 3L, 21L, 12L, 30L, 40L, 28L, 59L, 42L, 67L, 44L, 29L
> ), rep_num = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L), lp = c(NA, 41.64, 38.8, 44.45, 40.54,
> 38.54, 41.94, 39.6, 37.39, 40.95, 38.45, 43.47, 41.66, 40.91,
> 42.68, 43.12, 38.22, 40.95, 46.24, 42.95, 38.95, 39.88, 40.57,
> 40.13, 38.57, 45.45, 40.78, 43.52, 39.75, 39.93, 40.35, 37.6,
> 37.7, 43.05, 43.32, 41.31, 39.03, 42.5, 43.18, 41.32, 39.58,
> 40.62, 39.64, 39.85, 38.75, 40.18, 41.44, 40.5, 40.87, 40.75,
> 38.37, 40.26, 35.11, 40.89, 41.67, 38.87, 37.32, 35.85, 38.25,
> 42.21, 43.15, 38.69, 38.8, 38.77, 37.98, 39.8, 33.37, 40.09,
> 42.87, 44.07, 43.78, 42.47, 42.8, 41.1, 41.17, 44.07, 44.24,
> 43.16, 46.12, 42.64, 43.92, 42.16, 43.69, 42.89, 42.63, 43.58,
> 44.4, 43.17, 43.3, 42.5, 42.6, 44.05, 44.63, 43.09, 45.17, 45.17,
> 42.78, 42.38, 45.76, 42.16, 44.22, 43.67, 40.45, 41.95, 42.49,
> 41.98, 45.15, 46.01, 38.51, 43.08, 45.19, 44.53, 43.14, 39.93,
> 46.84, 43.59, 41.68, 43.7, 44.63, 44.02, 42.07, 43.88, 43.2,
> 46.2, 40.84, 39.14, 43.89, 42.58, 41.53, 45.32, 39.56, 43.77,
> 45.45, 45.16, 43.4, 40.08, 42.27, 43.74, 43.77, 41.31, 41.38,
> 45.01, 45.76, 40.4, 48.39, 46.27, 15.44, 44.76, 47.45, 44.87,
> 46.8, 41.83, 43.03, 43.96, 42.51, 45.06, 45.55, 44.9, 42.47,
> 44.9, 45, 44.77, 43.79, 44.26, 44.57, 44.4, 43.42, 42.31, 47.18,
> 43.83, 45.72, 44.83, 44.96, 40.28, 42.85, 41.23, 45.23, 47.09,
> 43.61, 42.69, 46.27, 45.16, 44.14, 43.34, 45.97, 43.81, 43.01,
> 39.82, 45.91, 45.97, 43.61, 45.12, 46.37, 42.67, 42.47, 45.86,
> 44.19, 44.46, 42.64, 43.95, 44.93, 40.33, 42.75, 39.92, 44.17,
> 44.49, 41.51, 42.43, 44.14, 40.5, 41.29, 44.89, 37.98, 39.02,
> 39.62, 42.13, 39.03, 44.16, 39.15, 41.49, 42.63, 40.11, 39.97,
> 42.85, 35.98, 39.45, 40.99, 42.44, 42.11, 37.36, 40.63, 40.69,
> 43.57, 39.04, 39.3, 42.19, 36.88, 40.39, 37.78, 38.6, 40.2, 40.98,
> 36.58, 43.59, 42.49, 39.96, 39.84, 40.43, 38.94, 42.72, 39.43,
> 42.13, 40.36, 40.58, 40.01, 42.17, 42.17, 41.35, 43.27, 40.15,
> 39.76, 40.94, 40.87, 42.32, 41.81, 41.97, 43.72, 41.32, 40.83,
> 37.64, 41.03, 38.98, 40.61, 41.17, 41.96, 40.07, 38.48, 42.85,
> 38.68, 39.09, 42.16, 38.14, 37.99, 41.06, 37.4, 41.88, 39.35,
> 39.73, 38.38, 41.34, 40.67, 40.89, 39.28, 37.59, 39.3, 39.72,
> 40.79, 39.42, 34.5, 37.61, 37.76, 40.24, 41.17, 41.24, 42.14,
> 42.53, 39.71, 39.44, 40.19, 42.51, 40.15, 36.26, 37.48, 40.43,
> 37.5, 42.11, 41.44, 40.29, 39.75, 37.58, 41.25, 40.39, 41.63,
> 42.18, 43.71, 38.69, 43.47, 41.98, 35.51, 42.22, 38.51, 40.17,
> 39.4, 39.54, 41.7, 40.93, 40.01, 35.97, 41.44, 41.89, 40.9, 39.95,
> 40.69, 43.18, 37.03, 39.91, 36.75, 42.75, 41.61, 42.6, 38.55,
> 42.84, 38.41, 42.55, 41.44, 41.03, 40, 41.35, 42.18, 42.66, 40.06,
> 42.48, 41.33, 41.92, 40.88, 40.99, 42.78, 38.26, 40.81, 39.95,
> 37.89, 40.81, 38.33, 39.33, 39.67, 40.12, 41.75, 39.81, 41.92,
> 44.59, 37.8, 40.54, 40.49, 41.82, 42.13, 39.93, 39.94, 42.54,
> 41.74, 43.06, 41.72, 41.27, 39.42, 42.07, 40.06, 42.1, 38.91,
> 39.28, 42.94, 39.94, 41.86, 38.56, 38.15, 41.47, 42.34, 38.14,
> 40.19, 40.2, 43.01, 42.35, 40.89, 41.44, 42.89, 44.49, 39.1,
> 38.42, 37.44, 43.28, 37.96, 40.56, 41.53, 41.59, 41.45, 42.55,
> 40.6, 44.04, 39.31, 41.08, 37.14, 40.03, 41.49, 40.82, 38.47,
> 43.43, 38.82, 40.4, 41.09, 42.26, 41.85, 41.13, 37.27, 41.97,
> 45.24, 43.35, 40.53, 43.14, 39.71, 42.77, 42.2, 42.13, 41.91,
> 42.59, 41.99, 42.11, 40.43, 40.02, 43.53, 43.01, 39.55, 43.2,
> 39.95, 42.51, 42, 42.26, 42.39, 42.64, 39.9, 41.89, 43.05, 41.02,
> 41.95, 39.92, 43.42, 44.01, 42.6, 40.84, 42.86, 44.31, 42.24,
> 41.25, 42.38, 44.99, 41.24, 43.97, 41.12, 37.88, 41.53, 41.56,
> 39.18, 40.83, 42.96, 41.92, 43.86, 42.48, 42.65, 43.3, 41.99,
> 42.51, 40.65, 42.77, 34.71, 40.97, 38.15, 39.76, 36.74, 37.95,
> 39.17, 38.22, 39.31, 43.57, 37.21, 39.35, 42.37, 42.01, 42.39,
> 43.21, 36.91, 36.69, 41.07, 41.91, 34.63, 40.61, 36.23, 38.12,
> 40.76, 39.14, 41.81, 39.14, 41.04, 37.32, 40.83, 40.81, 38.29,
> 39.61, 40.96, 40.71, 40.47, 38.64, 39.76, 38.19, 39.05, 38.04,
> 41.14, 38.35, 42.3, 34.44, 40.93, 39.3, 41.44, 38.3, 42.74, 36.66,
> 40.02, 36.62, 40.48, 41.72, 41.23, 41.81, 42.07, 40.22, 37.83,
> 36.54, 37.77, 40.49, 38.65, 43.2, 43.32, 40.67, 41.95, 36.11,
> 39.28, 42.38, 40.35, 40.3, 43.48, 40.55, 40.54, 44.03, 41.94,
> 37.97, 41.98, 41.53, 38.19, 38.66, 41.18, 41.95, 42.53, 38.7,
> 44.55, 42.39, 41.55, 38.46, 42.27, 42.19, 41.95, 41.81, 39.81,
> 38.12, 42.94, 42.99, 38.85, 41.26, 43.13, 44.21, 38.54, 44.02,
> 43.46, 41.64, 42.06, 42.11, 42.34, 41.86, 37.91, 40.89, 40.5,
> 39.54, 37.87, 40.86, 41.36, 41.77, 42.03, 39.15, 40.04, 44.11,
> 41.34, 42.97, 38.42, 37.28, 41.04, 41.48, 38.82, 41.94, 37.95,
> 40.9, 40.39, 40.31, 43.19, 41.22, 41.49, 41.25, 40.07, 36.7,
> 39.97, 39.99, 41.7, 37.09, 42.58, 43.01, 37.7, 41.81, 39.99,
> 42.95, 43.19, 42.69, 41.5, 40.64, 43.24, 41.14, 41.21, 41.29,
> 41.43, 44.21, 38.52, 42.54, 40.54, 42.49, 43.2, 38.12, 40.08,
> 39.02, 41.45, 42.33, 41.11, 38.93, 41.63, 44.22, 41.41, 39.08,
> 40.9, 41.1, 43.88, 40.96, 46.75, 47.54, 40.35, 41.97, 44.94,
> 44.91, 44.66, 44.5, 44.4, 46.4, 47.97, 46.05, 45.57, 42.83, 41.48,
> 47.48, 45.43, 41.98, 43.14, 45.6, 44.78, 45.45, 45.69, 44.82,
> 44.24, 41.14, 43.14, 46.61, 43.92, 43.56, 43.68, 45.37, 45.15,
> 40.76, 43.78, 44.67, 41.36, 41.4, 40.97, 41.87, 39.83, 43.8,
> 48.36, 44.28, 43.29, 44.69, 43.26, 43.35, 44.34, 45.08, 42.26,
> 39.7, 42.4, 44.03, 43.22, 42.71, 45.89, 44.89, 44.81, 42.59,
> 40.85, 43.82, 44.85, 47.47, 43.64, 42.65, 45.67, 43.24, 42.33,
> 40.61, 38.3, 39.84, 41.08, 42.33, 44.44, 40.85, 39.58, 42.55,
> 41.75, 39.44, 41.79, 39.31, 41.34, 42.76, 40.79, 37.31, 42.85,
> 42.88, 42.01, 44.63, 38.85, 41.13, 40.43, 41.34, 43.14, 40.58,
> 42.21, 38.94, 44.88, 42.33, 42.61, 41.88, 41.13, 41.83, 42.8)), .Names =
> c("field",
> "set", "ent_num", "rep_num", "lp"), class = "data.frame", row.names = c(NA,
> -787L))
>
> # session info
>
> R version 3.4.1 (2017-06-30)
>
> Platform: i386-w64-mingw32/i386 (32-bit)
>
> Running under: Windows 7 x64 (build 7601) Service Pack 1
>
>
>
> Matrix products: default
>
>
>
> locale:
>
> [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
> States.1252
>
> [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C
>
> [5] LC_TIME=English_United States.1252
>
>
>
> attached base packages:
>
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
>
>
> other attached packages:
>
> [1] bindrcpp_0.2 tidyr_0.6.3  dplyr_0.7.4
>
>
>
> loaded via a namespace (and not attached):
>
>  [1] compiler_3.4.1   magrittr_1.5     assertthat_0.2.0 R6_2.2.2
>  tools_3.4.1
>
>  [6] glue_1.1.1       tibble_1.3.3     Rcpp_0.12.11     stringi_1.1.5
> pkgconfig_2.0.1
>
> [11] rlang_0.1.2      bindr_0.1
>
> This email and any attachments were sent from a Monsanto email account and
> may contain confidential and/or privileged information. If you are not the
> intended recipient, please contact the sender and delete this email and any
> attachments immediately. Any unauthorized use, including disclosing,
> printing, storing, copying or distributing this email, is prohibited. All
> emails and attachments sent to or from Monsanto email accounts may be
> subject to monitoring, reading, and archiving by Monsanto, including its
> affiliates and subsidiaries, as permitted by applicable law. Thank you.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
> UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
>
>
> This email and any attachments were sent from a Monsanto email account and
> may contain confidential and/or privileged information. If you are not the
> intended recipient, please contact the sender and delete this email and any
> attachments immediately. Any unauthorized use, including disclosing,
> printing, storing, copying or distributing this email, is prohibited. All
> emails and attachments sent to or from Monsanto email accounts may be
> subject to monitoring, reading, and archiving by Monsanto, including its
> affiliates and subsidiaries, as permitted by applicable law. Thank you.
>
>
>
>
>
> This email and any attachments were sent from a Monsanto email account and
> may contain confidential and/or privileged information. If you are not the
> intended recipient, please contact the sender and delete this email and any
> attachments immediately. Any unauthorized use, including disclosing,
> printing, storing, copying or distributing this email, is prohibited. All
> emails and attachments sent to or from Monsanto email accounts may be
> subject to monitoring, reading, and archiving by Monsanto, including its
> affiliates and subsidiaries, as permitted by applicable law. Thank you.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From nilesh.dighe at monsanto.com  Thu Dec 14 19:14:24 2017
From: nilesh.dighe at monsanto.com (DIGHE, NILESH [AG/2362])
Date: Thu, 14 Dec 2017 18:14:24 +0000
Subject: [R] help with recursive function
In-Reply-To: <CAF8bMcaucmVz-V4-C2YB15tJt43WmdAwxXFwdSnxSqDfXZwYyA@mail.gmail.com>
References: <CY1PR0101MB1018F79F34CFE3142A8592ADE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CAGgJW772Nk+Z3XamiGF-2r6Ta9Cud1VOhUvRr4p4k0epTX4xtA@mail.gmail.com>
 <CAGgJW77AAvtOiCwLz=XEvsU+AuqHA=SK8RWLcBD5Ka7P8OFrPA@mail.gmail.com>
 <CY1PR0101MB1018FB7B65C9BDBE0ECA5DCBE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CY1PR0101MB1018B00187792A9F236E549EE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CAGgJW76zkxSQjH+XafCF58xoK8qYT9O0AfijT8cTxFZkYzQpPg@mail.gmail.com>
 <CAGgJW75kvGt527SWM6gaNSDkAMg=5Uf3NGAybVY43WzJ65_=2g@mail.gmail.com>
 <CY1PR0101MB10186D7B1B69C7234B36FCBEE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CAF8bMcaucmVz-V4-C2YB15tJt43WmdAwxXFwdSnxSqDfXZwYyA@mail.gmail.com>
Message-ID: <CY1PR0101MB1018D3A7F86D030725EE826CE20A0@CY1PR0101MB1018.prod.exchangelabs.com>

When I run the code without stopifnot, the code takes 5 min to run and then it throws an error listed below without producing any results.
Error: node stack overflow
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Error during wrapup: node stack overflow

Thanks.
Nilesh

From: William Dunlap [mailto:wdunlap at tibco.com]
Sent: Thursday, December 14, 2017 11:26 AM
To: DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com>
Cc: Eric Berger <ericjberger at gmail.com>; r-help <r-help at r-project.org>
Subject: Re: [R] help with recursive function

Your code contains the lines
        stopifnot(!(any(data1$norm_sd >= 1)))
        if            (!(any(data1$norm_sd >= 1))) {
            df1 <- dat1
            return(df1)
        }

stop() "throws an error", causing the current function and all functions in the call
stack to abort and return nothing.  It does not mean to stop now and return a result.
Does the function give the correct results if you just leave out the stopifnot line?


Bill Dunlap
TIBCO Software
wdunlap tibco.com<http://tibco.com>

On Thu, Dec 14, 2017 at 9:11 AM, DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com>> wrote:
Eric:  I will try and see if I can figure out the issue by debugging as you suggested. I don?t know why my code after stopifnot is not getting executed where I like the code to run the funlp2 function when the if statement is TRUE but when it is false, I like it to keep running until the stopifnot condition is met.

When the stopifnot condition is met, I like to get the output from if statement saved.
Anyway,  I will keep trying.
Again, Thanks for your help!
Nilesh

From: Eric Berger [mailto:ericjberger at gmail.com<mailto:ericjberger at gmail.com>]
Sent: Thursday, December 14, 2017 10:29 AM
To: DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com>>
Cc: r-help <r-help at r-project.org<mailto:r-help at r-project.org>>
Subject: Re: [R] help with recursive function

If you are trying to understand why the "stopifnot" condition is met you can replace it by something like:

if ( any(dat2$norm_sd >= 1) )
   browser()

This will put you in a debugging session where you can examine your variables, e.g.

> dat$norm_sd

HTH,
Eric



On Thu, Dec 14, 2017 at 5:33 PM, Eric Berger <ericjberger at gmail.com<mailto:ericjberger at gmail.com><mailto:ericjberger at gmail.com<mailto:ericjberger at gmail.com>>> wrote:
The message is coming from your stopifnot() condition being met.


On Thu, Dec 14, 2017 at 5:31 PM, DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com><mailto:nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com>>> wrote:
Hi, I accidently left out few lines of code from the calclp function.  Updated function is pasted below.
I am still getting the same error ?Error: !(any(data1$norm_sd >= 1)) is not TRUE?

I would appreciate any help.
Nilesh
dput(calclp)
function (dataset)
{
    dat1 <- funlp1(dataset)
    recursive_funlp <- function(dataset = dat1, func = funlp2) {
        dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
            mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
            spread(key = field_rep, value = lp) %>% mutate_at(.vars = grep("_",
            names(.)), funs(norm = round(scale(.), 3)))
        dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
            1, sd, na.rm = TRUE), 3)
        dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
            1, function(x) {
                max(abs(x), na.rm = TRUE)
            }), 3)
        data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,
            -norm_sd) %>% separate(key, c("field_rep", "treatment"),
            "\\.<file://.><file://.%3cfile:/.%3e>") %>% spread(treatment, value) %>% mutate(outlier = NA)
        stopifnot(!(any(data1$norm_sd >= 1)))
        if (!(any(data1$norm_sd >= 1))) {
            df1 <- dat1
            return(df1)
        }
       else {
            df2 <- recursive_funlp()
            return(df2)
        }
    }
    df3 <- recursive_funlp(dataset = dat1, func = funlp2)
    df3
}


From: DIGHE, NILESH [AG/2362]
Sent: Thursday, December 14, 2017 9:01 AM
To: 'Eric Berger' <ericjberger at gmail.com<mailto:ericjberger at gmail.com><mailto:ericjberger at gmail.com<mailto:ericjberger at gmail.com>>>
Cc: r-help <r-help at r-project.org<mailto:r-help at r-project.org><mailto:r-help at r-project.org<mailto:r-help at r-project.org>>>
Subject: RE: [R] help with recursive function

Eric:  Thanks for taking time to look into my problem.  Despite of making the change you suggested, I am still getting the same error.  I am wondering if the logic I am using in the stopifnot and if functions is a problem.
I like the recursive function to stop whenever the norm_sd column has zero values that are above or equal to 1. Below is the calclp function after the changes you suggested.
Thanks. Nilesh

dput(calclp)
function (dataset)
{
    dat1 <- funlp1(dataset)
    recursive_funlp <- function(dataset = dat1, func = funlp2) {
        dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
            mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
            spread(key = field_rep, value = lp) %>% mutate_at(.vars = grep("_",
            names(.)), funs(norm = round(scale(.), 3)))
        dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
            1, sd, na.rm = TRUE), 3)
        dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
            1, function(x) {
                max(abs(x), na.rm = TRUE)
            }), 3)
        stopifnot(!(any(dat2$norm_sd >= 1)))
        if (!(any(dat2$norm_sd >= 1))) {
            df1 <- dat1
            return(df1)
        }
        else {
            df2 <- recursive_funlp()
            return(df2)
        }
    }
    df3 <- recursive_funlp(dataset = dat1, func = funlp2)
    df3
}


From: Eric Berger [mailto:ericjberger at gmail.com<mailto:ericjberger at gmail.com>]
Sent: Thursday, December 14, 2017 8:17 AM
To: DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com><mailto:nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com>>>
Cc: r-help <r-help at r-project.org<mailto:r-help at r-project.org><mailto:r-help at r-project.org<mailto:r-help at r-project.org>>>
Subject: Re: [R] help with recursive function

My own typo ... whoops ...

!( any(dat2$norm_sd >= 1 ))



On Thu, Dec 14, 2017 at 3:43 PM, Eric Berger <ericjberger at gmail.com<mailto:ericjberger at gmail.com><mailto:ericjberger at gmail.com<mailto:ericjberger at gmail.com>>> wrote:
You seem to have a typo at this expression (and some others like it)

Namely, you write

any(!dat2$norm_sd) >= 1

when you possibly meant to write

!( any(dat2$norm_sd) >= 1 )

i.e. I think your ! seems to be in the wrong place.

HTH,
Eric


On Thu, Dec 14, 2017 at 3:26 PM, DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com><mailto:nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com>>> wrote:
Hi, I need some help with running a recursive function. I like to run funlp2 recursively.
When I try to run recursive function in another function named "calclp" I get this "Error: any(!dat2$norm_sd) >= 1 is not TRUE".

I have never built a recursive function before so having trouble executing it in this case.  I would appreciate any help or guidance to resolve this issue. Please see my data and the three functions that I am using below.
Please note that calclp is the function I am running and the other two functions are within this calclp function.

# code:
Test<- calclp(dataset = dat)

# calclp function

calclp<- function (dataset)

{

    dat1 <- funlp1(dataset)

    recursive_funlp <- function(dataset = dat1, func = funlp2) {

        dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%

            mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%

            spread(key = field_rep, value = lp) %>% mutate_at(.vars = grep("_",

            names(.)), funs(norm = round(scale(.), 3)))

        dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],

            1, sd, na.rm = TRUE), 3)

        dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],

            1, function(x) {

                max(abs(x), na.rm = TRUE)

            }), 3)

        stopifnot(any(!dat2$norm_sd) >= 1)

        if (any(!dat2$norm_sd) >= 1) {

            df1 <- dat1

            return(df1)

        }

        else {

            df2 <- recursive_funlp()

            return(df2)

        }

    }

    df3 <- recursive_funlp(dataset = dat1, func = funlp2)

    df3

}


# funlp1 function

funlp1<- function (dataset)

{

    dat2 <- dataset %>% select(field, set, ent_num, rep_num,

        lp) %>% unite(uniqueid, set, ent_num, sep = ".") %>%

        unite(field_rep, field, rep_num) %>% mutate(field_rep = paste(field_rep,

        "lp", sep = ".")) %>% spread(key = field_rep, value = lp) %>%

        mutate_at(.vars = grep("_", names(.)), funs(norm = round(scale(.),

            3)))

    dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],

        1, sd, na.rm = TRUE), 3)

    dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],

        1, function(x) {

            max(abs(x), na.rm = TRUE)

        }), 3)

    data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,

        -norm_sd) %>% separate(key, c("field_rep", "treatment"),

        "\\.<file://.><file://.%3cfile:/.%3e>") %>% spread(treatment, value) %>% mutate(outlier = NA)

    df_clean <- with(data1, data1[norm_sd < 1, ])

    datD <- with(data1, data1[norm_sd >= 1, ])

    s <- split(datD, datD$uniqueid)

    sdf <- lapply(s, function(x) {

        data.frame(x, x$outlier <- ifelse(is.na<http://is.na><http://is.na>(x$lp_norm), NA,

            ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),

            x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))

        x

    })

    sdf2 <- bind_rows(sdf)

    all_dat <- bind_rows(df_clean, sdf2)

    all_dat

}


# funlp2 function

funlp2<-function (dataset)

{

    data1 <- dataset

    df_clean <- with(data1, data1[norm_sd < 1, ])

    datD <- with(data1, data1[norm_sd >= 1, ])

    s <- split(datD, datD$uniqueid)

    sdf <- lapply(s, function(x) {

        data.frame(x, x$outlier <- ifelse(is.na<http://is.na><http://is.na>(x$lp_norm), NA,

            ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),

            x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))

        x

    })

    sdf2 <- bind_rows(sdf)

    all_dat <- bind_rows(df_clean, sdf2)

    all_dat

}


# dataset
dput(dat)
structure(list(field = c("LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
"LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
"OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
"SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
"O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "B2NW", "B2NW",
"B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
"17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
"WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
"NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
"MAND", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
"28EP", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
"EX01", "EX01", "EX01", "EX01", "EX01", "EX01"), set = c("seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
"seta", "seta"), ent_num = c(23L, 14L, 43L, 30L, 44L, 60L, 17L,
34L, 41L, 40L, 9L, 36L, 38L, 19L, 61L, 51L, 45L, 42L, 3L, 39L,
21L, 11L, 12L, 7L, 35L, 5L, 70L, 47L, 28L, 16L, 72L, 13L, 49L,
67L, 56L, 32L, 27L, 46L, 24L, 63L, 15L, 66L, 26L, 29L, 48L, 1L,
54L, 37L, 2L, 50L, 52L, 31L, 33L, 25L, 6L, 69L, 53L, 10L, 18L,
55L, 59L, 4L, 58L, 22L, 20L, 64L, 71L, 57L, 11L, 17L, 43L, 13L,
1L, 16L, 34L, 47L, 52L, 72L, 59L, 22L, 54L, 18L, 25L, 61L, 56L,
41L, 27L, 14L, 49L, 19L, 29L, 31L, 64L, 6L, 53L, 35L, 37L, 67L,
39L, 51L, 40L, 15L, 69L, 60L, 38L, 4L, 23L, 3L, 48L, 32L, 42L,
24L, 28L, 33L, 57L, 8L, 21L, 46L, 7L, 30L, 45L, 2L, 63L, 36L,
68L, 20L, 66L, 70L, 58L, 5L, 10L, 12L, 62L, 50L, 71L, 9L, 55L,
26L, 44L, 65L, 14L, 63L, 46L, 58L, 62L, 19L, 59L, 2L, 5L, 6L,
40L, 21L, 44L, 37L, 55L, 35L, 71L, 56L, 10L, 36L, 53L, 25L, 61L,
12L, 26L, 23L, 4L, 13L, 28L, 38L, 57L, 54L, 72L, 48L, 66L, 9L,
70L, 15L, 39L, 60L, 17L, 34L, 51L, 67L, 42L, 49L, 31L, 30L, 3L,
18L, 65L, 32L, 27L, 52L, 22L, 11L, 47L, 64L, 8L, 43L, 41L, 16L,
20L, 33L, 7L, 50L, 68L, 24L, 1L, 69L, 45L, 29L, 37L, 30L, 55L,
54L, 43L, 32L, 21L, 27L, 33L, 40L, 67L, 57L, 68L, 31L, 17L, 13L,
6L, 62L, 19L, 22L, 3L, 10L, 44L, 34L, 69L, 70L, 4L, 1L, 25L,
11L, 51L, 5L, 63L, 71L, 12L, 38L, 58L, 39L, 49L, 59L, 56L, 65L,
2L, 64L, 8L, 35L, 46L, 45L, 29L, 53L, 36L, 42L, 23L, 18L, 50L,
26L, 14L, 48L, 66L, 20L, 24L, 7L, 15L, 53L, 22L, 39L, 20L, 60L,
59L, 43L, 19L, 41L, 6L, 62L, 1L, 55L, 34L, 50L, 38L, 40L, 44L,
4L, 46L, 29L, 65L, 57L, 48L, 33L, 69L, 14L, 35L, 67L, 72L, 54L,
3L, 49L, 2L, 12L, 18L, 30L, 10L, 70L, 31L, 15L, 63L, 71L, 21L,
45L, 28L, 56L, 27L, 64L, 61L, 51L, 5L, 24L, 68L, 25L, 66L, 16L,
36L, 58L, 37L, 52L, 26L, 9L, 42L, 7L, 11L, 8L, 32L, 23L, 13L,
47L, 17L, 61L, 72L, 47L, 60L, 16L, 9L, 28L, 52L, 41L, 1L, 61L,
6L, 23L, 58L, 63L, 25L, 28L, 30L, 36L, 62L, 9L, 32L, 19L, 31L,
56L, 45L, 2L, 22L, 27L, 40L, 14L, 11L, 50L, 13L, 70L, 20L, 64L,
39L, 26L, 21L, 43L, 29L, 35L, 54L, 52L, 37L, 17L, 16L, 72L, 48L,
12L, 18L, 44L, 42L, 49L, 68L, 5L, 55L, 69L, 51L, 66L, 59L, 53L,
15L, 71L, 41L, 57L, 4L, 60L, 8L, 7L, 33L, 34L, 24L, 10L, 67L,
47L, 38L, 3L, 65L, 46L, 20L, 34L, 71L, 1L, 33L, 57L, 13L, 21L,
66L, 29L, 3L, 61L, 69L, 24L, 62L, 39L, 49L, 47L, 31L, 53L, 52L,
43L, 17L, 7L, 8L, 12L, 60L, 63L, 50L, 2L, 51L, 46L, 10L, 23L,
48L, 11L, 26L, 40L, 70L, 42L, 59L, 15L, 56L, 58L, 27L, 6L, 35L,
4L, 37L, 5L, 65L, 44L, 28L, 14L, 32L, 36L, 45L, 9L, 18L, 55L,
68L, 30L, 54L, 41L, 25L, 22L, 38L, 16L, 67L, 64L, 19L, 72L, 68L,
28L, 33L, 15L, 51L, 4L, 47L, 36L, 8L, 57L, 48L, 1L, 52L, 39L,
32L, 50L, 13L, 30L, 63L, 2L, 9L, 62L, 22L, 6L, 61L, 16L, 53L,
38L, 37L, 20L, 69L, 44L, 56L, 29L, 26L, 14L, 17L, 46L, 66L, 58L,
42L, 60L, 19L, 45L, 3L, 59L, 70L, 31L, 24L, 55L, 40L, 43L, 25L,
65L, 12L, 67L, 21L, 7L, 27L, 49L, 72L, 54L, 41L, 23L, 34L, 5L,
64L, 35L, 18L, 71L, 11L, 24L, 19L, 38L, 14L, 4L, 56L, 5L, 54L,
34L, 64L, 55L, 33L, 69L, 71L, 52L, 61L, 48L, 23L, 43L, 41L, 20L,
39L, 11L, 63L, 36L, 22L, 9L, 25L, 27L, 51L, 53L, 37L, 57L, 13L,
18L, 64L, 22L, 53L, 16L, 5L, 28L, 60L, 31L, 11L, 29L, 45L, 59L,
72L, 49L, 67L, 13L, 20L, 3L, 42L, 44L, 69L, 33L, 38L, 15L, 70L,
35L, 48L, 26L, 56L, 19L, 39L, 43L, 40L, 14L, 2L, 68L, 51L, 12L,
47L, 10L, 55L, 23L, 4L, 71L, 41L, 50L, 7L, 24L, 61L, 27L, 54L,
46L, 58L, 37L, 66L, 57L, 1L, 36L, 32L, 18L, 62L, 9L, 30L, 21L,
6L, 52L, 8L, 65L, 17L, 25L, 63L, 34L, 65L, 22L, 56L, 9L, 7L,
11L, 31L, 4L, 63L, 29L, 61L, 54L, 12L, 62L, 59L, 5L, 23L, 53L,
36L, 24L, 35L, 66L, 49L, 72L, 18L, 70L, 32L, 43L, 20L, 45L, 34L,
46L, 28L, 6L, 44L, 71L, 39L, 13L, 27L, 1L, 58L, 30L, 68L, 17L,
33L, 26L, 57L, 15L, 21L, 52L, 48L, 42L, 16L, 40L, 38L, 8L, 69L,
2L, 51L, 67L, 55L, 64L, 47L, 60L, 19L, 41L, 50L, 3L, 14L, 25L,
10L, 37L, 6L, 15L, 45L, 49L, 8L, 17L, 50L, 16L, 58L, 72L, 26L,
60L, 7L, 32L, 1L, 46L, 66L, 68L, 62L, 47L, 35L, 70L, 10L, 31L,
65L, 2L, 3L, 21L, 12L, 30L, 40L, 28L, 59L, 42L, 67L, 44L, 29L
), rep_num = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L), lp = c(NA, 41.64, 38.8, 44.45, 40.54,
38.54, 41.94, 39.6, 37.39, 40.95, 38.45, 43.47, 41.66, 40.91,
42.68, 43.12, 38.22, 40.95, 46.24, 42.95, 38.95, 39.88, 40.57,
40.13, 38.57, 45.45, 40.78, 43.52, 39.75, 39.93, 40.35, 37.6,
37.7, 43.05, 43.32, 41.31, 39.03, 42.5, 43.18, 41.32, 39.58,
40.62, 39.64, 39.85, 38.75, 40.18, 41.44, 40.5, 40.87, 40.75,
38.37, 40.26, 35.11, 40.89, 41.67, 38.87, 37.32, 35.85, 38.25,
42.21, 43.15, 38.69, 38.8, 38.77, 37.98, 39.8, 33.37, 40.09,
42.87, 44.07, 43.78, 42.47, 42.8, 41.1, 41.17, 44.07, 44.24,
43.16, 46.12, 42.64, 43.92, 42.16, 43.69, 42.89, 42.63, 43.58,
44.4, 43.17, 43.3, 42.5, 42.6, 44.05, 44.63, 43.09, 45.17, 45.17,
42.78, 42.38, 45.76, 42.16, 44.22, 43.67, 40.45, 41.95, 42.49,
41.98, 45.15, 46.01, 38.51, 43.08, 45.19, 44.53, 43.14, 39.93,
46.84, 43.59, 41.68, 43.7, 44.63, 44.02, 42.07, 43.88, 43.2,
46.2, 40.84, 39.14, 43.89, 42.58, 41.53, 45.32, 39.56, 43.77,
45.45, 45.16, 43.4, 40.08, 42.27, 43.74, 43.77, 41.31, 41.38,
45.01, 45.76, 40.4, 48.39, 46.27, 15.44, 44.76, 47.45, 44.87,
46.8, 41.83, 43.03, 43.96, 42.51, 45.06, 45.55, 44.9, 42.47,
44.9, 45, 44.77, 43.79, 44.26, 44.57, 44.4, 43.42, 42.31, 47.18,
43.83, 45.72, 44.83, 44.96, 40.28, 42.85, 41.23, 45.23, 47.09,
43.61, 42.69, 46.27, 45.16, 44.14, 43.34, 45.97, 43.81, 43.01,
39.82, 45.91, 45.97, 43.61, 45.12, 46.37, 42.67, 42.47, 45.86,
44.19, 44.46, 42.64, 43.95, 44.93, 40.33, 42.75, 39.92, 44.17,
44.49, 41.51, 42.43, 44.14, 40.5, 41.29, 44.89, 37.98, 39.02,
39.62, 42.13, 39.03, 44.16, 39.15, 41.49, 42.63, 40.11, 39.97,
42.85, 35.98, 39.45, 40.99, 42.44, 42.11, 37.36, 40.63, 40.69,
43.57, 39.04, 39.3, 42.19, 36.88, 40.39, 37.78, 38.6, 40.2, 40.98,
36.58, 43.59, 42.49, 39.96, 39.84, 40.43, 38.94, 42.72, 39.43,
42.13, 40.36, 40.58, 40.01, 42.17, 42.17, 41.35, 43.27, 40.15,
39.76, 40.94, 40.87, 42.32, 41.81, 41.97, 43.72, 41.32, 40.83,
37.64, 41.03, 38.98, 40.61, 41.17, 41.96, 40.07, 38.48, 42.85,
38.68, 39.09, 42.16, 38.14, 37.99, 41.06, 37.4, 41.88, 39.35,
39.73, 38.38, 41.34, 40.67, 40.89, 39.28, 37.59, 39.3, 39.72,
40.79, 39.42, 34.5, 37.61, 37.76, 40.24, 41.17, 41.24, 42.14,
42.53, 39.71, 39.44, 40.19, 42.51, 40.15, 36.26, 37.48, 40.43,
37.5, 42.11, 41.44, 40.29, 39.75, 37.58, 41.25, 40.39, 41.63,
42.18, 43.71, 38.69, 43.47, 41.98, 35.51, 42.22, 38.51, 40.17,
39.4, 39.54, 41.7, 40.93, 40.01, 35.97, 41.44, 41.89, 40.9, 39.95,
40.69, 43.18, 37.03, 39.91, 36.75, 42.75, 41.61, 42.6, 38.55,
42.84, 38.41, 42.55, 41.44, 41.03, 40, 41.35, 42.18, 42.66, 40.06,
42.48, 41.33, 41.92, 40.88, 40.99, 42.78, 38.26, 40.81, 39.95,
37.89, 40.81, 38.33, 39.33, 39.67, 40.12, 41.75, 39.81, 41.92,
44.59, 37.8, 40.54, 40.49, 41.82, 42.13, 39.93, 39.94, 42.54,
41.74, 43.06, 41.72, 41.27, 39.42, 42.07, 40.06, 42.1, 38.91,
39.28, 42.94, 39.94, 41.86, 38.56, 38.15, 41.47, 42.34, 38.14,
40.19, 40.2, 43.01, 42.35, 40.89, 41.44, 42.89, 44.49, 39.1,
38.42, 37.44, 43.28, 37.96, 40.56, 41.53, 41.59, 41.45, 42.55,
40.6, 44.04, 39.31, 41.08, 37.14, 40.03, 41.49, 40.82, 38.47,
43.43, 38.82, 40.4, 41.09, 42.26, 41.85, 41.13, 37.27, 41.97,
45.24, 43.35, 40.53, 43.14, 39.71, 42.77, 42.2, 42.13, 41.91,
42.59, 41.99, 42.11, 40.43, 40.02, 43.53, 43.01, 39.55, 43.2,
39.95, 42.51, 42, 42.26, 42.39, 42.64, 39.9, 41.89, 43.05, 41.02,
41.95, 39.92, 43.42, 44.01, 42.6, 40.84, 42.86, 44.31, 42.24,
41.25, 42.38, 44.99, 41.24, 43.97, 41.12, 37.88, 41.53, 41.56,
39.18, 40.83, 42.96, 41.92, 43.86, 42.48, 42.65, 43.3, 41.99,
42.51, 40.65, 42.77, 34.71, 40.97, 38.15, 39.76, 36.74, 37.95,
39.17, 38.22, 39.31, 43.57, 37.21, 39.35, 42.37, 42.01, 42.39,
43.21, 36.91, 36.69, 41.07, 41.91, 34.63, 40.61, 36.23, 38.12,
40.76, 39.14, 41.81, 39.14, 41.04, 37.32, 40.83, 40.81, 38.29,
39.61, 40.96, 40.71, 40.47, 38.64, 39.76, 38.19, 39.05, 38.04,
41.14, 38.35, 42.3, 34.44, 40.93, 39.3, 41.44, 38.3, 42.74, 36.66,
40.02, 36.62, 40.48, 41.72, 41.23, 41.81, 42.07, 40.22, 37.83,
36.54, 37.77, 40.49, 38.65, 43.2, 43.32, 40.67, 41.95, 36.11,
39.28, 42.38, 40.35, 40.3, 43.48, 40.55, 40.54, 44.03, 41.94,
37.97, 41.98, 41.53, 38.19, 38.66, 41.18, 41.95, 42.53, 38.7,
44.55, 42.39, 41.55, 38.46, 42.27, 42.19, 41.95, 41.81, 39.81,
38.12, 42.94, 42.99, 38.85, 41.26, 43.13, 44.21, 38.54, 44.02,
43.46, 41.64, 42.06, 42.11, 42.34, 41.86, 37.91, 40.89, 40.5,
39.54, 37.87, 40.86, 41.36, 41.77, 42.03, 39.15, 40.04, 44.11,
41.34, 42.97, 38.42, 37.28, 41.04, 41.48, 38.82, 41.94, 37.95,
40.9, 40.39, 40.31, 43.19, 41.22, 41.49, 41.25, 40.07, 36.7,
39.97, 39.99, 41.7, 37.09, 42.58, 43.01, 37.7, 41.81, 39.99,
42.95, 43.19, 42.69, 41.5, 40.64, 43.24, 41.14, 41.21, 41.29,
41.43, 44.21, 38.52, 42.54, 40.54, 42.49, 43.2, 38.12, 40.08,
39.02, 41.45, 42.33, 41.11, 38.93, 41.63, 44.22, 41.41, 39.08,
40.9, 41.1, 43.88, 40.96, 46.75, 47.54, 40.35, 41.97, 44.94,
44.91, 44.66, 44.5, 44.4, 46.4, 47.97, 46.05, 45.57, 42.83, 41.48,
47.48, 45.43, 41.98, 43.14, 45.6, 44.78, 45.45, 45.69, 44.82,
44.24, 41.14, 43.14, 46.61, 43.92, 43.56, 43.68, 45.37, 45.15,
40.76, 43.78, 44.67, 41.36, 41.4, 40.97, 41.87, 39.83, 43.8,
48.36, 44.28, 43.29, 44.69, 43.26, 43.35, 44.34, 45.08, 42.26,
39.7, 42.4, 44.03, 43.22, 42.71, 45.89, 44.89, 44.81, 42.59,
40.85, 43.82, 44.85, 47.47, 43.64, 42.65, 45.67, 43.24, 42.33,
40.61, 38.3, 39.84, 41.08, 42.33, 44.44, 40.85, 39.58, 42.55,
41.75, 39.44, 41.79, 39.31, 41.34, 42.76, 40.79, 37.31, 42.85,
42.88, 42.01, 44.63, 38.85, 41.13, 40.43, 41.34, 43.14, 40.58,
42.21, 38.94, 44.88, 42.33, 42.61, 41.88, 41.13, 41.83, 42.8)), .Names = c("field",
"set", "ent_num", "rep_num", "lp"), class = "data.frame", row.names = c(NA,
-787L))

# session info

R version 3.4.1 (2017-06-30)

Platform: i386-w64-mingw32/i386 (32-bit)

Running under: Windows 7 x64 (build 7601) Service Pack 1



Matrix products: default



locale:

[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252

[3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C

[5] LC_TIME=English_United States.1252



attached base packages:

[1] stats     graphics  grDevices utils     datasets  methods   base



other attached packages:

[1] bindrcpp_0.2 tidyr_0.6.3  dplyr_0.7.4



loaded via a namespace (and not attached):

 [1] compiler_3.4.1   magrittr_1.5     assertthat_0.2.0 R6_2.2.2         tools_3.4.1

 [6] glue_1.1.1       tibble_1.3.3     Rcpp_0.12.11     stringi_1.1.5    pkgconfig_2.0.1

[11] rlang_0.1.2      bindr_0.1

This email and any attachments were sent from a Monsanto email account and may contain confidential and/or privileged information. If you are not the intended recipient, please contact the sender and delete this email and any attachments immediately. Any unauthorized use, including disclosing, printing, storing, copying or distributing this email, is prohibited. All emails and attachments sent to or from Monsanto email accounts may be subject to monitoring, reading, and archiving by Monsanto, including its affiliates and subsidiaries, as permitted by applicable law. Thank you.

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org><mailto:R-help at r-project.org<mailto:R-help at r-project.org>> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.






This email and any attachments were sent from a Monsanto email account and may contain confidential and/or privileged information. If you are not the intended recipient, please contact the sender and delete this email and any attachments immediately. Any unauthorized use, including disclosing, printing, storing, copying or distributing this email, is prohibited. All emails and attachments sent to or from Monsanto email accounts may be subject to monitoring, reading, and archiving by Monsanto, including its affiliates and subsidiaries, as permitted by applicable law. Thank you.





This email and any attachments were sent from a Monsanto email account and may contain confidential and/or privileged information. If you are not the intended recipient, please contact the sender and delete this email and any attachments immediately. Any unauthorized use, including disclosing, printing, storing, copying or distributing this email, is prohibited. All emails and attachments sent to or from Monsanto email accounts may be subject to monitoring, reading, and archiving by Monsanto, including its affiliates and subsidiaries, as permitted by applicable law. Thank you.

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

This email and any attachments were sent from a Monsanto email account and may contain confidential and/or privileged information. If you are not the intended recipient, please contact the sender and delete this email and any attachments immediately. Any unauthorized use, including disclosing, printing, storing, copying or distributing this email, is prohibited. All emails and attachments sent to or from Monsanto email accounts may be subject to monitoring, reading, and archiving by Monsanto, including its affiliates and subsidiaries, as permitted by applicable law. Thank you.

	[[alternative HTML version deleted]]


From chocold12 at gmail.com  Thu Dec 14 19:36:46 2017
From: chocold12 at gmail.com (lily li)
Date: Thu, 14 Dec 2017 11:36:46 -0700
Subject: [R] Errors in reading in txt files
Message-ID: <CAN5afy_Sewg6z1P+n-u9qHk29+1UjaddH7ZKGbSPn9APVXwyiQ@mail.gmail.com>

Hi R users,

I have a question about reading from text files. The file has the structure
below:

Time                            Column1   Column2
01.01.2001-12:00:00
01.01.2001-24:00:00        12             11
01.02.2001-12:00:00        13             10
01.02.2001-24:00:00        11             12
01.03.2001-12:00:00        15             11
01.03.2001-24:00:00        16             10
...

I just use the simple script to open it: df = read.table('DATAM', head=T).

But it has the error and thus cannot read the file:
Error in scan(file = file, what = what, sep = sep, quote = quote, dec =
dec,  :
  line 1 did not have 3 elements

How to read it with three fixed columns, and how to read the time format in
the first column correctly? Thanks for your help.

	[[alternative HTML version deleted]]


From bhh at xs4all.nl  Thu Dec 14 19:58:15 2017
From: bhh at xs4all.nl (Berend Hasselman)
Date: Thu, 14 Dec 2017 19:58:15 +0100
Subject: [R] Errors in reading in txt files
In-Reply-To: <CAN5afy_Sewg6z1P+n-u9qHk29+1UjaddH7ZKGbSPn9APVXwyiQ@mail.gmail.com>
References: <CAN5afy_Sewg6z1P+n-u9qHk29+1UjaddH7ZKGbSPn9APVXwyiQ@mail.gmail.com>
Message-ID: <C883D7B3-521E-4AA8-ABFB-14DBE5C483C1@xs4all.nl>


> On 14 Dec 2017, at 19:36, lily li <chocold12 at gmail.com> wrote:
> 
> Hi R users,
> 
> I have a question about reading from text files. The file has the structure
> below:
> 
> Time                            Column1   Column2
> 01.01.2001-12:00:00

This line does not contain 3 elements; only one.
You'll have to fix that line. Delete it, prepend it with a comment character of add enough columns.


Berend

> 01.01.2001-24:00:00        12             11
> 01.02.2001-12:00:00        13             10
> 01.02.2001-24:00:00        11             12
> 01.03.2001-12:00:00        15             11
> 01.03.2001-24:00:00        16             10
> ...
> 
> I just use the simple script to open it: df = read.table('DATAM', head=T).
> 
> But it has the error and thus cannot read the file:
> Error in scan(file = file, what = what, sep = sep, quote = quote, dec =
> dec,  :
>  line 1 did not have 3 elements
> 
> How to read it with three fixed columns, and how to read the time format in
> the first column correctly? Thanks for your help.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Thu Dec 14 19:58:41 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 14 Dec 2017 10:58:41 -0800
Subject: [R] help with recursive function
In-Reply-To: <CY1PR0101MB10186D7B1B69C7234B36FCBEE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
References: <CY1PR0101MB1018F79F34CFE3142A8592ADE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CAGgJW772Nk+Z3XamiGF-2r6Ta9Cud1VOhUvRr4p4k0epTX4xtA@mail.gmail.com>
 <CAGgJW77AAvtOiCwLz=XEvsU+AuqHA=SK8RWLcBD5Ka7P8OFrPA@mail.gmail.com>
 <CY1PR0101MB1018FB7B65C9BDBE0ECA5DCBE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CY1PR0101MB1018B00187792A9F236E549EE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
 <CAGgJW76zkxSQjH+XafCF58xoK8qYT9O0AfijT8cTxFZkYzQpPg@mail.gmail.com>
 <CAGgJW75kvGt527SWM6gaNSDkAMg=5Uf3NGAybVY43WzJ65_=2g@mail.gmail.com>
 <CY1PR0101MB10186D7B1B69C7234B36FCBEE20A0@CY1PR0101MB1018.prod.exchangelabs.com>
Message-ID: <CAF8bMca6=ZPQ54mM6mWCk5fPsnF0cdfd6uoM+8ENOi0MjMHgAQ@mail.gmail.com>

    recursive_funlp <- function(dataset = dat1, func = funlp2) {
        ...
        if (!(any(data1$norm_sd >= 1))) {
            df1 <- dat1
            return(df1)
        }
       else {
            df2 <- recursive_funlp() # GIVE SOME ARGUMENTS HERE
            return(df2)
        }
    }

When you recurse into recursve_funip(), give it at least a new dataset
argument.
Otherwise it just processes the entire dat1 again and the recursion never
stops.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Dec 14, 2017 at 9:11 AM, DIGHE, NILESH [AG/2362] <
nilesh.dighe at monsanto.com> wrote:

> Eric:  I will try and see if I can figure out the issue by debugging as
> you suggested. I don?t know why my code after stopifnot is not getting
> executed where I like the code to run the funlp2 function when the if
> statement is TRUE but when it is false, I like it to keep running until the
> stopifnot condition is met.
>
> When the stopifnot condition is met, I like to get the output from if
> statement saved.
> Anyway,  I will keep trying.
> Again, Thanks for your help!
> Nilesh
>
> From: Eric Berger [mailto:ericjberger at gmail.com]
> Sent: Thursday, December 14, 2017 10:29 AM
> To: DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com>
> Cc: r-help <r-help at r-project.org>
> Subject: Re: [R] help with recursive function
>
> If you are trying to understand why the "stopifnot" condition is met you
> can replace it by something like:
>
> if ( any(dat2$norm_sd >= 1) )
>    browser()
>
> This will put you in a debugging session where you can examine your
> variables, e.g.
>
> > dat$norm_sd
>
> HTH,
> Eric
>
>
>
> On Thu, Dec 14, 2017 at 5:33 PM, Eric Berger <ericjberger at gmail.com
> <mailto:ericjberger at gmail.com>> wrote:
> The message is coming from your stopifnot() condition being met.
>
>
> On Thu, Dec 14, 2017 at 5:31 PM, DIGHE, NILESH [AG/2362] <
> nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com>> wrote:
> Hi, I accidently left out few lines of code from the calclp function.
> Updated function is pasted below.
> I am still getting the same error ?Error: !(any(data1$norm_sd >= 1)) is
> not TRUE?
>
> I would appreciate any help.
> Nilesh
> dput(calclp)
> function (dataset)
> {
>     dat1 <- funlp1(dataset)
>     recursive_funlp <- function(dataset = dat1, func = funlp2) {
>         dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
>             mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
>             spread(key = field_rep, value = lp) %>% mutate_at(.vars =
> grep("_",
>             names(.)), funs(norm = round(scale(.), 3)))
>         dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>             1, sd, na.rm = TRUE), 3)
>         dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>             1, function(x) {
>                 max(abs(x), na.rm = TRUE)
>             }), 3)
>         data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,
>             -norm_sd) %>% separate(key, c("field_rep", "treatment"),
>             "\\.<file://.>") %>% spread(treatment, value) %>%
> mutate(outlier = NA)
>         stopifnot(!(any(data1$norm_sd >= 1)))
>         if (!(any(data1$norm_sd >= 1))) {
>             df1 <- dat1
>             return(df1)
>         }
>        else {
>             df2 <- recursive_funlp()
>             return(df2)
>         }
>     }
>     df3 <- recursive_funlp(dataset = dat1, func = funlp2)
>     df3
> }
>
>
> From: DIGHE, NILESH [AG/2362]
> Sent: Thursday, December 14, 2017 9:01 AM
> To: 'Eric Berger' <ericjberger at gmail.com<mailto:ericjberger at gmail.com>>
> Cc: r-help <r-help at r-project.org<mailto:r-help at r-project.org>>
> Subject: RE: [R] help with recursive function
>
> Eric:  Thanks for taking time to look into my problem.  Despite of making
> the change you suggested, I am still getting the same error.  I am
> wondering if the logic I am using in the stopifnot and if functions is a
> problem.
> I like the recursive function to stop whenever the norm_sd column has zero
> values that are above or equal to 1. Below is the calclp function after the
> changes you suggested.
> Thanks. Nilesh
>
> dput(calclp)
> function (dataset)
> {
>     dat1 <- funlp1(dataset)
>     recursive_funlp <- function(dataset = dat1, func = funlp2) {
>         dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
>             mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
>             spread(key = field_rep, value = lp) %>% mutate_at(.vars =
> grep("_",
>             names(.)), funs(norm = round(scale(.), 3)))
>         dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>             1, sd, na.rm = TRUE), 3)
>         dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>             1, function(x) {
>                 max(abs(x), na.rm = TRUE)
>             }), 3)
>         stopifnot(!(any(dat2$norm_sd >= 1)))
>         if (!(any(dat2$norm_sd >= 1))) {
>             df1 <- dat1
>             return(df1)
>         }
>         else {
>             df2 <- recursive_funlp()
>             return(df2)
>         }
>     }
>     df3 <- recursive_funlp(dataset = dat1, func = funlp2)
>     df3
> }
>
>
> From: Eric Berger [mailto:ericjberger at gmail.com]
> Sent: Thursday, December 14, 2017 8:17 AM
> To: DIGHE, NILESH [AG/2362] <nilesh.dighe at monsanto.com<mailto:
> nilesh.dighe at monsanto.com>>
> Cc: r-help <r-help at r-project.org<mailto:r-help at r-project.org>>
> Subject: Re: [R] help with recursive function
>
> My own typo ... whoops ...
>
> !( any(dat2$norm_sd >= 1 ))
>
>
>
> On Thu, Dec 14, 2017 at 3:43 PM, Eric Berger <ericjberger at gmail.com
> <mailto:ericjberger at gmail.com>> wrote:
> You seem to have a typo at this expression (and some others like it)
>
> Namely, you write
>
> any(!dat2$norm_sd) >= 1
>
> when you possibly meant to write
>
> !( any(dat2$norm_sd) >= 1 )
>
> i.e. I think your ! seems to be in the wrong place.
>
> HTH,
> Eric
>
>
> On Thu, Dec 14, 2017 at 3:26 PM, DIGHE, NILESH [AG/2362] <
> nilesh.dighe at monsanto.com<mailto:nilesh.dighe at monsanto.com>> wrote:
> Hi, I need some help with running a recursive function. I like to run
> funlp2 recursively.
> When I try to run recursive function in another function named "calclp" I
> get this "Error: any(!dat2$norm_sd) >= 1 is not TRUE".
>
> I have never built a recursive function before so having trouble executing
> it in this case.  I would appreciate any help or guidance to resolve this
> issue. Please see my data and the three functions that I am using below.
> Please note that calclp is the function I am running and the other two
> functions are within this calclp function.
>
> # code:
> Test<- calclp(dataset = dat)
>
> # calclp function
>
> calclp<- function (dataset)
>
> {
>
>     dat1 <- funlp1(dataset)
>
>     recursive_funlp <- function(dataset = dat1, func = funlp2) {
>
>         dat2 <- dataset %>% select(uniqueid, field_rep, lp) %>%
>
>             mutate(field_rep = paste(field_rep, "lp", sep = ".")) %>%
>
>             spread(key = field_rep, value = lp) %>% mutate_at(.vars =
> grep("_",
>
>             names(.)), funs(norm = round(scale(.), 3)))
>
>         dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>             1, sd, na.rm = TRUE), 3)
>
>         dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>             1, function(x) {
>
>                 max(abs(x), na.rm = TRUE)
>
>             }), 3)
>
>         stopifnot(any(!dat2$norm_sd) >= 1)
>
>         if (any(!dat2$norm_sd) >= 1) {
>
>             df1 <- dat1
>
>             return(df1)
>
>         }
>
>         else {
>
>             df2 <- recursive_funlp()
>
>             return(df2)
>
>         }
>
>     }
>
>     df3 <- recursive_funlp(dataset = dat1, func = funlp2)
>
>     df3
>
> }
>
>
> # funlp1 function
>
> funlp1<- function (dataset)
>
> {
>
>     dat2 <- dataset %>% select(field, set, ent_num, rep_num,
>
>         lp) %>% unite(uniqueid, set, ent_num, sep = ".") %>%
>
>         unite(field_rep, field, rep_num) %>% mutate(field_rep =
> paste(field_rep,
>
>         "lp", sep = ".")) %>% spread(key = field_rep, value = lp) %>%
>
>         mutate_at(.vars = grep("_", names(.)), funs(norm = round(scale(.),
>
>             3)))
>
>     dat2$norm_sd <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>         1, sd, na.rm = TRUE), 3)
>
>     dat2$norm_max <- round(apply(dat2[, grep("lp_norm", names(dat2))],
>
>         1, function(x) {
>
>             max(abs(x), na.rm = TRUE)
>
>         }), 3)
>
>     data1 <- dat2 %>% gather(key, value, -uniqueid, -norm_max,
>
>         -norm_sd) %>% separate(key, c("field_rep", "treatment"),
>
>         "\\.<file://.>") %>% spread(treatment, value) %>% mutate(outlier =
> NA)
>
>     df_clean <- with(data1, data1[norm_sd < 1, ])
>
>     datD <- with(data1, data1[norm_sd >= 1, ])
>
>     s <- split(datD, datD$uniqueid)
>
>     sdf <- lapply(s, function(x) {
>
>         data.frame(x, x$outlier <- ifelse(is.na<http://is.na>(x$lp_norm),
> NA,
>
>             ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),
>
>             x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))
>
>         x
>
>     })
>
>     sdf2 <- bind_rows(sdf)
>
>     all_dat <- bind_rows(df_clean, sdf2)
>
>     all_dat
>
> }
>
>
> # funlp2 function
>
> funlp2<-function (dataset)
>
> {
>
>     data1 <- dataset
>
>     df_clean <- with(data1, data1[norm_sd < 1, ])
>
>     datD <- with(data1, data1[norm_sd >= 1, ])
>
>     s <- split(datD, datD$uniqueid)
>
>     sdf <- lapply(s, function(x) {
>
>         data.frame(x, x$outlier <- ifelse(is.na<http://is.na>(x$lp_norm),
> NA,
>
>             ifelse(abs(x$lp_norm) == x$norm_max, "yes", "no")),
>
>             x$lp <- with(x, ifelse(outlier == "yes", NA, lp)))
>
>         x
>
>     })
>
>     sdf2 <- bind_rows(sdf)
>
>     all_dat <- bind_rows(df_clean, sdf2)
>
>     all_dat
>
> }
>
>
> # dataset
> dput(dat)
> structure(list(field = c("LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01",
> "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "LM01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01",
> "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "OL01", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1",
> "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "SGI1", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS",
> "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "O2LS", "B2NW", "B2NW",
> "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "B2NW", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3",
> "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "17C3", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1",
> "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "WCI1", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "NY01",
> "NY01", "NY01", "NY01", "NY01", "NY01", "NY01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND", "MAND",
> "MAND", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP", "28EP",
> "28EP", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01", "EX01",
> "EX01", "EX01", "EX01", "EX01", "EX01", "EX01"), set = c("seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta", "seta", "seta", "seta", "seta", "seta", "seta",
> "seta", "seta"), ent_num = c(23L, 14L, 43L, 30L, 44L, 60L, 17L,
> 34L, 41L, 40L, 9L, 36L, 38L, 19L, 61L, 51L, 45L, 42L, 3L, 39L,
> 21L, 11L, 12L, 7L, 35L, 5L, 70L, 47L, 28L, 16L, 72L, 13L, 49L,
> 67L, 56L, 32L, 27L, 46L, 24L, 63L, 15L, 66L, 26L, 29L, 48L, 1L,
> 54L, 37L, 2L, 50L, 52L, 31L, 33L, 25L, 6L, 69L, 53L, 10L, 18L,
> 55L, 59L, 4L, 58L, 22L, 20L, 64L, 71L, 57L, 11L, 17L, 43L, 13L,
> 1L, 16L, 34L, 47L, 52L, 72L, 59L, 22L, 54L, 18L, 25L, 61L, 56L,
> 41L, 27L, 14L, 49L, 19L, 29L, 31L, 64L, 6L, 53L, 35L, 37L, 67L,
> 39L, 51L, 40L, 15L, 69L, 60L, 38L, 4L, 23L, 3L, 48L, 32L, 42L,
> 24L, 28L, 33L, 57L, 8L, 21L, 46L, 7L, 30L, 45L, 2L, 63L, 36L,
> 68L, 20L, 66L, 70L, 58L, 5L, 10L, 12L, 62L, 50L, 71L, 9L, 55L,
> 26L, 44L, 65L, 14L, 63L, 46L, 58L, 62L, 19L, 59L, 2L, 5L, 6L,
> 40L, 21L, 44L, 37L, 55L, 35L, 71L, 56L, 10L, 36L, 53L, 25L, 61L,
> 12L, 26L, 23L, 4L, 13L, 28L, 38L, 57L, 54L, 72L, 48L, 66L, 9L,
> 70L, 15L, 39L, 60L, 17L, 34L, 51L, 67L, 42L, 49L, 31L, 30L, 3L,
> 18L, 65L, 32L, 27L, 52L, 22L, 11L, 47L, 64L, 8L, 43L, 41L, 16L,
> 20L, 33L, 7L, 50L, 68L, 24L, 1L, 69L, 45L, 29L, 37L, 30L, 55L,
> 54L, 43L, 32L, 21L, 27L, 33L, 40L, 67L, 57L, 68L, 31L, 17L, 13L,
> 6L, 62L, 19L, 22L, 3L, 10L, 44L, 34L, 69L, 70L, 4L, 1L, 25L,
> 11L, 51L, 5L, 63L, 71L, 12L, 38L, 58L, 39L, 49L, 59L, 56L, 65L,
> 2L, 64L, 8L, 35L, 46L, 45L, 29L, 53L, 36L, 42L, 23L, 18L, 50L,
> 26L, 14L, 48L, 66L, 20L, 24L, 7L, 15L, 53L, 22L, 39L, 20L, 60L,
> 59L, 43L, 19L, 41L, 6L, 62L, 1L, 55L, 34L, 50L, 38L, 40L, 44L,
> 4L, 46L, 29L, 65L, 57L, 48L, 33L, 69L, 14L, 35L, 67L, 72L, 54L,
> 3L, 49L, 2L, 12L, 18L, 30L, 10L, 70L, 31L, 15L, 63L, 71L, 21L,
> 45L, 28L, 56L, 27L, 64L, 61L, 51L, 5L, 24L, 68L, 25L, 66L, 16L,
> 36L, 58L, 37L, 52L, 26L, 9L, 42L, 7L, 11L, 8L, 32L, 23L, 13L,
> 47L, 17L, 61L, 72L, 47L, 60L, 16L, 9L, 28L, 52L, 41L, 1L, 61L,
> 6L, 23L, 58L, 63L, 25L, 28L, 30L, 36L, 62L, 9L, 32L, 19L, 31L,
> 56L, 45L, 2L, 22L, 27L, 40L, 14L, 11L, 50L, 13L, 70L, 20L, 64L,
> 39L, 26L, 21L, 43L, 29L, 35L, 54L, 52L, 37L, 17L, 16L, 72L, 48L,
> 12L, 18L, 44L, 42L, 49L, 68L, 5L, 55L, 69L, 51L, 66L, 59L, 53L,
> 15L, 71L, 41L, 57L, 4L, 60L, 8L, 7L, 33L, 34L, 24L, 10L, 67L,
> 47L, 38L, 3L, 65L, 46L, 20L, 34L, 71L, 1L, 33L, 57L, 13L, 21L,
> 66L, 29L, 3L, 61L, 69L, 24L, 62L, 39L, 49L, 47L, 31L, 53L, 52L,
> 43L, 17L, 7L, 8L, 12L, 60L, 63L, 50L, 2L, 51L, 46L, 10L, 23L,
> 48L, 11L, 26L, 40L, 70L, 42L, 59L, 15L, 56L, 58L, 27L, 6L, 35L,
> 4L, 37L, 5L, 65L, 44L, 28L, 14L, 32L, 36L, 45L, 9L, 18L, 55L,
> 68L, 30L, 54L, 41L, 25L, 22L, 38L, 16L, 67L, 64L, 19L, 72L, 68L,
> 28L, 33L, 15L, 51L, 4L, 47L, 36L, 8L, 57L, 48L, 1L, 52L, 39L,
> 32L, 50L, 13L, 30L, 63L, 2L, 9L, 62L, 22L, 6L, 61L, 16L, 53L,
> 38L, 37L, 20L, 69L, 44L, 56L, 29L, 26L, 14L, 17L, 46L, 66L, 58L,
> 42L, 60L, 19L, 45L, 3L, 59L, 70L, 31L, 24L, 55L, 40L, 43L, 25L,
> 65L, 12L, 67L, 21L, 7L, 27L, 49L, 72L, 54L, 41L, 23L, 34L, 5L,
> 64L, 35L, 18L, 71L, 11L, 24L, 19L, 38L, 14L, 4L, 56L, 5L, 54L,
> 34L, 64L, 55L, 33L, 69L, 71L, 52L, 61L, 48L, 23L, 43L, 41L, 20L,
> 39L, 11L, 63L, 36L, 22L, 9L, 25L, 27L, 51L, 53L, 37L, 57L, 13L,
> 18L, 64L, 22L, 53L, 16L, 5L, 28L, 60L, 31L, 11L, 29L, 45L, 59L,
> 72L, 49L, 67L, 13L, 20L, 3L, 42L, 44L, 69L, 33L, 38L, 15L, 70L,
> 35L, 48L, 26L, 56L, 19L, 39L, 43L, 40L, 14L, 2L, 68L, 51L, 12L,
> 47L, 10L, 55L, 23L, 4L, 71L, 41L, 50L, 7L, 24L, 61L, 27L, 54L,
> 46L, 58L, 37L, 66L, 57L, 1L, 36L, 32L, 18L, 62L, 9L, 30L, 21L,
> 6L, 52L, 8L, 65L, 17L, 25L, 63L, 34L, 65L, 22L, 56L, 9L, 7L,
> 11L, 31L, 4L, 63L, 29L, 61L, 54L, 12L, 62L, 59L, 5L, 23L, 53L,
> 36L, 24L, 35L, 66L, 49L, 72L, 18L, 70L, 32L, 43L, 20L, 45L, 34L,
> 46L, 28L, 6L, 44L, 71L, 39L, 13L, 27L, 1L, 58L, 30L, 68L, 17L,
> 33L, 26L, 57L, 15L, 21L, 52L, 48L, 42L, 16L, 40L, 38L, 8L, 69L,
> 2L, 51L, 67L, 55L, 64L, 47L, 60L, 19L, 41L, 50L, 3L, 14L, 25L,
> 10L, 37L, 6L, 15L, 45L, 49L, 8L, 17L, 50L, 16L, 58L, 72L, 26L,
> 60L, 7L, 32L, 1L, 46L, 66L, 68L, 62L, 47L, 35L, 70L, 10L, 31L,
> 65L, 2L, 3L, 21L, 12L, 30L, 40L, 28L, 59L, 42L, 67L, 44L, 29L
> ), rep_num = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L), lp = c(NA, 41.64, 38.8, 44.45, 40.54,
> 38.54, 41.94, 39.6, 37.39, 40.95, 38.45, 43.47, 41.66, 40.91,
> 42.68, 43.12, 38.22, 40.95, 46.24, 42.95, 38.95, 39.88, 40.57,
> 40.13, 38.57, 45.45, 40.78, 43.52, 39.75, 39.93, 40.35, 37.6,
> 37.7, 43.05, 43.32, 41.31, 39.03, 42.5, 43.18, 41.32, 39.58,
> 40.62, 39.64, 39.85, 38.75, 40.18, 41.44, 40.5, 40.87, 40.75,
> 38.37, 40.26, 35.11, 40.89, 41.67, 38.87, 37.32, 35.85, 38.25,
> 42.21, 43.15, 38.69, 38.8, 38.77, 37.98, 39.8, 33.37, 40.09,
> 42.87, 44.07, 43.78, 42.47, 42.8, 41.1, 41.17, 44.07, 44.24,
> 43.16, 46.12, 42.64, 43.92, 42.16, 43.69, 42.89, 42.63, 43.58,
> 44.4, 43.17, 43.3, 42.5, 42.6, 44.05, 44.63, 43.09, 45.17, 45.17,
> 42.78, 42.38, 45.76, 42.16, 44.22, 43.67, 40.45, 41.95, 42.49,
> 41.98, 45.15, 46.01, 38.51, 43.08, 45.19, 44.53, 43.14, 39.93,
> 46.84, 43.59, 41.68, 43.7, 44.63, 44.02, 42.07, 43.88, 43.2,
> 46.2, 40.84, 39.14, 43.89, 42.58, 41.53, 45.32, 39.56, 43.77,
> 45.45, 45.16, 43.4, 40.08, 42.27, 43.74, 43.77, 41.31, 41.38,
> 45.01, 45.76, 40.4, 48.39, 46.27, 15.44, 44.76, 47.45, 44.87,
> 46.8, 41.83, 43.03, 43.96, 42.51, 45.06, 45.55, 44.9, 42.47,
> 44.9, 45, 44.77, 43.79, 44.26, 44.57, 44.4, 43.42, 42.31, 47.18,
> 43.83, 45.72, 44.83, 44.96, 40.28, 42.85, 41.23, 45.23, 47.09,
> 43.61, 42.69, 46.27, 45.16, 44.14, 43.34, 45.97, 43.81, 43.01,
> 39.82, 45.91, 45.97, 43.61, 45.12, 46.37, 42.67, 42.47, 45.86,
> 44.19, 44.46, 42.64, 43.95, 44.93, 40.33, 42.75, 39.92, 44.17,
> 44.49, 41.51, 42.43, 44.14, 40.5, 41.29, 44.89, 37.98, 39.02,
> 39.62, 42.13, 39.03, 44.16, 39.15, 41.49, 42.63, 40.11, 39.97,
> 42.85, 35.98, 39.45, 40.99, 42.44, 42.11, 37.36, 40.63, 40.69,
> 43.57, 39.04, 39.3, 42.19, 36.88, 40.39, 37.78, 38.6, 40.2, 40.98,
> 36.58, 43.59, 42.49, 39.96, 39.84, 40.43, 38.94, 42.72, 39.43,
> 42.13, 40.36, 40.58, 40.01, 42.17, 42.17, 41.35, 43.27, 40.15,
> 39.76, 40.94, 40.87, 42.32, 41.81, 41.97, 43.72, 41.32, 40.83,
> 37.64, 41.03, 38.98, 40.61, 41.17, 41.96, 40.07, 38.48, 42.85,
> 38.68, 39.09, 42.16, 38.14, 37.99, 41.06, 37.4, 41.88, 39.35,
> 39.73, 38.38, 41.34, 40.67, 40.89, 39.28, 37.59, 39.3, 39.72,
> 40.79, 39.42, 34.5, 37.61, 37.76, 40.24, 41.17, 41.24, 42.14,
> 42.53, 39.71, 39.44, 40.19, 42.51, 40.15, 36.26, 37.48, 40.43,
> 37.5, 42.11, 41.44, 40.29, 39.75, 37.58, 41.25, 40.39, 41.63,
> 42.18, 43.71, 38.69, 43.47, 41.98, 35.51, 42.22, 38.51, 40.17,
> 39.4, 39.54, 41.7, 40.93, 40.01, 35.97, 41.44, 41.89, 40.9, 39.95,
> 40.69, 43.18, 37.03, 39.91, 36.75, 42.75, 41.61, 42.6, 38.55,
> 42.84, 38.41, 42.55, 41.44, 41.03, 40, 41.35, 42.18, 42.66, 40.06,
> 42.48, 41.33, 41.92, 40.88, 40.99, 42.78, 38.26, 40.81, 39.95,
> 37.89, 40.81, 38.33, 39.33, 39.67, 40.12, 41.75, 39.81, 41.92,
> 44.59, 37.8, 40.54, 40.49, 41.82, 42.13, 39.93, 39.94, 42.54,
> 41.74, 43.06, 41.72, 41.27, 39.42, 42.07, 40.06, 42.1, 38.91,
> 39.28, 42.94, 39.94, 41.86, 38.56, 38.15, 41.47, 42.34, 38.14,
> 40.19, 40.2, 43.01, 42.35, 40.89, 41.44, 42.89, 44.49, 39.1,
> 38.42, 37.44, 43.28, 37.96, 40.56, 41.53, 41.59, 41.45, 42.55,
> 40.6, 44.04, 39.31, 41.08, 37.14, 40.03, 41.49, 40.82, 38.47,
> 43.43, 38.82, 40.4, 41.09, 42.26, 41.85, 41.13, 37.27, 41.97,
> 45.24, 43.35, 40.53, 43.14, 39.71, 42.77, 42.2, 42.13, 41.91,
> 42.59, 41.99, 42.11, 40.43, 40.02, 43.53, 43.01, 39.55, 43.2,
> 39.95, 42.51, 42, 42.26, 42.39, 42.64, 39.9, 41.89, 43.05, 41.02,
> 41.95, 39.92, 43.42, 44.01, 42.6, 40.84, 42.86, 44.31, 42.24,
> 41.25, 42.38, 44.99, 41.24, 43.97, 41.12, 37.88, 41.53, 41.56,
> 39.18, 40.83, 42.96, 41.92, 43.86, 42.48, 42.65, 43.3, 41.99,
> 42.51, 40.65, 42.77, 34.71, 40.97, 38.15, 39.76, 36.74, 37.95,
> 39.17, 38.22, 39.31, 43.57, 37.21, 39.35, 42.37, 42.01, 42.39,
> 43.21, 36.91, 36.69, 41.07, 41.91, 34.63, 40.61, 36.23, 38.12,
> 40.76, 39.14, 41.81, 39.14, 41.04, 37.32, 40.83, 40.81, 38.29,
> 39.61, 40.96, 40.71, 40.47, 38.64, 39.76, 38.19, 39.05, 38.04,
> 41.14, 38.35, 42.3, 34.44, 40.93, 39.3, 41.44, 38.3, 42.74, 36.66,
> 40.02, 36.62, 40.48, 41.72, 41.23, 41.81, 42.07, 40.22, 37.83,
> 36.54, 37.77, 40.49, 38.65, 43.2, 43.32, 40.67, 41.95, 36.11,
> 39.28, 42.38, 40.35, 40.3, 43.48, 40.55, 40.54, 44.03, 41.94,
> 37.97, 41.98, 41.53, 38.19, 38.66, 41.18, 41.95, 42.53, 38.7,
> 44.55, 42.39, 41.55, 38.46, 42.27, 42.19, 41.95, 41.81, 39.81,
> 38.12, 42.94, 42.99, 38.85, 41.26, 43.13, 44.21, 38.54, 44.02,
> 43.46, 41.64, 42.06, 42.11, 42.34, 41.86, 37.91, 40.89, 40.5,
> 39.54, 37.87, 40.86, 41.36, 41.77, 42.03, 39.15, 40.04, 44.11,
> 41.34, 42.97, 38.42, 37.28, 41.04, 41.48, 38.82, 41.94, 37.95,
> 40.9, 40.39, 40.31, 43.19, 41.22, 41.49, 41.25, 40.07, 36.7,
> 39.97, 39.99, 41.7, 37.09, 42.58, 43.01, 37.7, 41.81, 39.99,
> 42.95, 43.19, 42.69, 41.5, 40.64, 43.24, 41.14, 41.21, 41.29,
> 41.43, 44.21, 38.52, 42.54, 40.54, 42.49, 43.2, 38.12, 40.08,
> 39.02, 41.45, 42.33, 41.11, 38.93, 41.63, 44.22, 41.41, 39.08,
> 40.9, 41.1, 43.88, 40.96, 46.75, 47.54, 40.35, 41.97, 44.94,
> 44.91, 44.66, 44.5, 44.4, 46.4, 47.97, 46.05, 45.57, 42.83, 41.48,
> 47.48, 45.43, 41.98, 43.14, 45.6, 44.78, 45.45, 45.69, 44.82,
> 44.24, 41.14, 43.14, 46.61, 43.92, 43.56, 43.68, 45.37, 45.15,
> 40.76, 43.78, 44.67, 41.36, 41.4, 40.97, 41.87, 39.83, 43.8,
> 48.36, 44.28, 43.29, 44.69, 43.26, 43.35, 44.34, 45.08, 42.26,
> 39.7, 42.4, 44.03, 43.22, 42.71, 45.89, 44.89, 44.81, 42.59,
> 40.85, 43.82, 44.85, 47.47, 43.64, 42.65, 45.67, 43.24, 42.33,
> 40.61, 38.3, 39.84, 41.08, 42.33, 44.44, 40.85, 39.58, 42.55,
> 41.75, 39.44, 41.79, 39.31, 41.34, 42.76, 40.79, 37.31, 42.85,
> 42.88, 42.01, 44.63, 38.85, 41.13, 40.43, 41.34, 43.14, 40.58,
> 42.21, 38.94, 44.88, 42.33, 42.61, 41.88, 41.13, 41.83, 42.8)), .Names =
> c("field",
> "set", "ent_num", "rep_num", "lp"), class = "data.frame", row.names = c(NA,
> -787L))
>
> # session info
>
> R version 3.4.1 (2017-06-30)
>
> Platform: i386-w64-mingw32/i386 (32-bit)
>
> Running under: Windows 7 x64 (build 7601) Service Pack 1
>
>
>
> Matrix products: default
>
>
>
> locale:
>
> [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
> States.1252
>
> [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C
>
> [5] LC_TIME=English_United States.1252
>
>
>
> attached base packages:
>
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
>
>
> other attached packages:
>
> [1] bindrcpp_0.2 tidyr_0.6.3  dplyr_0.7.4
>
>
>
> loaded via a namespace (and not attached):
>
>  [1] compiler_3.4.1   magrittr_1.5     assertthat_0.2.0 R6_2.2.2
>  tools_3.4.1
>
>  [6] glue_1.1.1       tibble_1.3.3     Rcpp_0.12.11     stringi_1.1.5
> pkgconfig_2.0.1
>
> [11] rlang_0.1.2      bindr_0.1
>
> This email and any attachments were sent from a Monsanto email account and
> may contain confidential and/or privileged information. If you are not the
> intended recipient, please contact the sender and delete this email and any
> attachments immediately. Any unauthorized use, including disclosing,
> printing, storing, copying or distributing this email, is prohibited. All
> emails and attachments sent to or from Monsanto email accounts may be
> subject to monitoring, reading, and archiving by Monsanto, including its
> affiliates and subsidiaries, as permitted by applicable law. Thank you.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
> UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
>
>
> This email and any attachments were sent from a Monsanto email account and
> may contain confidential and/or privileged information. If you are not the
> intended recipient, please contact the sender and delete this email and any
> attachments immediately. Any unauthorized use, including disclosing,
> printing, storing, copying or distributing this email, is prohibited. All
> emails and attachments sent to or from Monsanto email accounts may be
> subject to monitoring, reading, and archiving by Monsanto, including its
> affiliates and subsidiaries, as permitted by applicable law. Thank you.
>
>
>
>
>
> This email and any attachments were sent from a Monsanto email account and
> may contain confidential and/or privileged information. If you are not the
> intended recipient, please contact the sender and delete this email and any
> attachments immediately. Any unauthorized use, including disclosing,
> printing, storing, copying or distributing this email, is prohibited. All
> emails and attachments sent to or from Monsanto email accounts may be
> subject to monitoring, reading, and archiving by Monsanto, including its
> affiliates and subsidiaries, as permitted by applicable law. Thank you.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From chocold12 at gmail.com  Thu Dec 14 20:00:10 2017
From: chocold12 at gmail.com (lily li)
Date: Thu, 14 Dec 2017 12:00:10 -0700
Subject: [R] Errors in reading in txt files
In-Reply-To: <C883D7B3-521E-4AA8-ABFB-14DBE5C483C1@xs4all.nl>
References: <CAN5afy_Sewg6z1P+n-u9qHk29+1UjaddH7ZKGbSPn9APVXwyiQ@mail.gmail.com>
 <C883D7B3-521E-4AA8-ABFB-14DBE5C483C1@xs4all.nl>
Message-ID: <CAN5afy-=XF-5nNa1AMw+qo87zHArOeC9-2rWp7DDPGtkQr+DUg@mail.gmail.com>

Thanks, Berend. I thought R can recognize the space automatically, such as
na.strings="", or sep=' '.

On Thu, Dec 14, 2017 at 11:58 AM, Berend Hasselman <bhh at xs4all.nl> wrote:

>
> > On 14 Dec 2017, at 19:36, lily li <chocold12 at gmail.com> wrote:
> >
> > Hi R users,
> >
> > I have a question about reading from text files. The file has the
> structure
> > below:
> >
> > Time                            Column1   Column2
> > 01.01.2001-12:00:00
>
> This line does not contain 3 elements; only one.
> You'll have to fix that line. Delete it, prepend it with a comment
> character of add enough columns.
>
>
> Berend
>
> > 01.01.2001-24:00:00        12             11
> > 01.02.2001-12:00:00        13             10
> > 01.02.2001-24:00:00        11             12
> > 01.03.2001-12:00:00        15             11
> > 01.03.2001-24:00:00        16             10
> > ...
> >
> > I just use the simple script to open it: df = read.table('DATAM',
> head=T).
> >
> > But it has the error and thus cannot read the file:
> > Error in scan(file = file, what = what, sep = sep, quote = quote, dec =
> > dec,  :
> >  line 1 did not have 3 elements
> >
> > How to read it with three fixed columns, and how to read the time format
> in
> > the first column correctly? Thanks for your help.
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From istazahn at gmail.com  Thu Dec 14 20:01:10 2017
From: istazahn at gmail.com (Ista Zahn)
Date: Thu, 14 Dec 2017 14:01:10 -0500
Subject: [R] Errors in reading in txt files
In-Reply-To: <C883D7B3-521E-4AA8-ABFB-14DBE5C483C1@xs4all.nl>
References: <CAN5afy_Sewg6z1P+n-u9qHk29+1UjaddH7ZKGbSPn9APVXwyiQ@mail.gmail.com>
 <C883D7B3-521E-4AA8-ABFB-14DBE5C483C1@xs4all.nl>
Message-ID: <CA+vqiLGjP=6QeJ1=n2=SzM0kZMU37mSY_tKVsiucW_kDH1ma=g@mail.gmail.com>

On Thu, Dec 14, 2017 at 1:58 PM, Berend Hasselman <bhh at xs4all.nl> wrote:
>
>> On 14 Dec 2017, at 19:36, lily li <chocold12 at gmail.com> wrote:
>>
>> Hi R users,
>>
>> I have a question about reading from text files. The file has the structure
>> below:
>>
>> Time                            Column1   Column2
>> 01.01.2001-12:00:00
>
> This line does not contain 3 elements; only one.
> You'll have to fix that line. Delete it, prepend it with a comment character of add enough columns.

I definitely don't recommend that. Instead, read

?read.table

to learn about the "fill" and "header" arguments.

df = read.table("DATAM", header = TRUE, fill = TRUE)

will probably work.

Best,
Ista


>
>
> Berend
>
>> 01.01.2001-24:00:00        12             11
>> 01.02.2001-12:00:00        13             10
>> 01.02.2001-24:00:00        11             12
>> 01.03.2001-12:00:00        15             11
>> 01.03.2001-24:00:00        16             10
>> ...
>>
>> I just use the simple script to open it: df = read.table('DATAM', head=T).
>>
>> But it has the error and thus cannot read the file:
>> Error in scan(file = file, what = what, sep = sep, quote = quote, dec =
>> dec,  :
>>  line 1 did not have 3 elements
>>
>> How to read it with three fixed columns, and how to read the time format in
>> the first column correctly? Thanks for your help.
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bhh at xs4all.nl  Thu Dec 14 21:16:13 2017
From: bhh at xs4all.nl (Berend Hasselman)
Date: Thu, 14 Dec 2017 21:16:13 +0100
Subject: [R] Errors in reading in txt files
In-Reply-To: <CA+vqiLGjP=6QeJ1=n2=SzM0kZMU37mSY_tKVsiucW_kDH1ma=g@mail.gmail.com>
References: <CAN5afy_Sewg6z1P+n-u9qHk29+1UjaddH7ZKGbSPn9APVXwyiQ@mail.gmail.com>
 <C883D7B3-521E-4AA8-ABFB-14DBE5C483C1@xs4all.nl>
 <CA+vqiLGjP=6QeJ1=n2=SzM0kZMU37mSY_tKVsiucW_kDH1ma=g@mail.gmail.com>
Message-ID: <BBF9105F-6DF7-4FD8-92F3-6616F08B8066@xs4all.nl>


> On 14 Dec 2017, at 20:01, Ista Zahn <istazahn at gmail.com> wrote:
> 
> On Thu, Dec 14, 2017 at 1:58 PM, Berend Hasselman <bhh at xs4all.nl> wrote:
>> 
>>> On 14 Dec 2017, at 19:36, lily li <chocold12 at gmail.com> wrote:
>>> 
>>> Hi R users,
>>> 
>>> I have a question about reading from text files. The file has the structure
>>> below:
>>> 
>>> Time                            Column1   Column2
>>> 01.01.2001-12:00:00
>> 
>> This line does not contain 3 elements; only one.
>> You'll have to fix that line. Delete it, prepend it with a comment character of add enough columns.
> 
> I definitely don't recommend that. Instead, read
> 
> ?read.table
> 
> to learn about the "fill" and "header" arguments.
> 
> df = read.table("DATAM", header = TRUE, fill = TRUE)
> 
> will probably work.
> 

Yes. I agree. It's much better.
I should have experimented some more.

Berend

> Best,
> Ista
> 
> 
>> 
>> 
>> Berend
>> 
>>> 01.01.2001-24:00:00        12             11
>>> 01.02.2001-12:00:00        13             10
>>> 01.02.2001-24:00:00        11             12
>>> 01.03.2001-12:00:00        15             11
>>> 01.03.2001-24:00:00        16             10
>>> ...
>>> 
>>> I just use the simple script to open it: df = read.table('DATAM', head=T).
>>> 
>>> But it has the error and thus cannot read the file:
>>> Error in scan(file = file, what = what, sep = sep, quote = quote, dec =
>>> dec,  :
>>> line 1 did not have 3 elements
>>> 
>>> How to read it with three fixed columns, and how to read the time format in
>>> the first column correctly? Thanks for your help.
>>> 
>>>      [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From macqueen1 at llnl.gov  Thu Dec 14 21:31:34 2017
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Thu, 14 Dec 2017 20:31:34 +0000
Subject: [R] Errors in reading in txt files
In-Reply-To: <CA+vqiLGjP=6QeJ1=n2=SzM0kZMU37mSY_tKVsiucW_kDH1ma=g@mail.gmail.com>
References: <CAN5afy_Sewg6z1P+n-u9qHk29+1UjaddH7ZKGbSPn9APVXwyiQ@mail.gmail.com>
 <C883D7B3-521E-4AA8-ABFB-14DBE5C483C1@xs4all.nl>
 <CA+vqiLGjP=6QeJ1=n2=SzM0kZMU37mSY_tKVsiucW_kDH1ma=g@mail.gmail.com>
Message-ID: <32112127-9B34-4AD0-BDA8-420640D67184@llnl.gov>

In addition to which, I would recommend

df <- read.table("DATAM", header = TRUE, fill = TRUE, stringsAsFactors=FALSE)

and then converting the Time column to POSIXct date-time values using
  as.POSIXct()
specifying the format using formatting codes found in
  ?strptime
because the times are not in the POSIXct default format.


This example might indicate the idea:

> as.POSIXct('2012-10-12 13:14')
[1] "2012-10-12 13:14:00 PDT"
> class(as.POSIXct('2012-10-12 13:14'))
[1] "POSIXct" "POSIXt" 

-Don

--
Don MacQueen
Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062
Lab cell 925-724-7509
 
 

On 12/14/17, 11:01 AM, "R-help on behalf of Ista Zahn" <r-help-bounces at r-project.org on behalf of istazahn at gmail.com> wrote:

    On Thu, Dec 14, 2017 at 1:58 PM, Berend Hasselman <bhh at xs4all.nl> wrote:
    >
    >> On 14 Dec 2017, at 19:36, lily li <chocold12 at gmail.com> wrote:
    >>
    >> Hi R users,
    >>
    >> I have a question about reading from text files. The file has the structure
    >> below:
    >>
    >> Time                            Column1   Column2
    >> 01.01.2001-12:00:00
    >
    > This line does not contain 3 elements; only one.
    > You'll have to fix that line. Delete it, prepend it with a comment character of add enough columns.
    
    I definitely don't recommend that. Instead, read
    
    ?read.table
    
    to learn about the "fill" and "header" arguments.
    
    df = read.table("DATAM", header = TRUE, fill = TRUE)
    
    will probably work.
    
    Best,
    Ista
    
    
    >
    >
    > Berend
    >
    >> 01.01.2001-24:00:00        12             11
    >> 01.02.2001-12:00:00        13             10
    >> 01.02.2001-24:00:00        11             12
    >> 01.03.2001-12:00:00        15             11
    >> 01.03.2001-24:00:00        16             10
    >> ...
    >>
    >> I just use the simple script to open it: df = read.table('DATAM', head=T).
    >>
    >> But it has the error and thus cannot read the file:
    >> Error in scan(file = file, what = what, sep = sep, quote = quote, dec =
    >> dec,  :
    >>  line 1 did not have 3 elements
    >>
    >> How to read it with three fixed columns, and how to read the time format in
    >> the first column correctly? Thanks for your help.
    >>
    >>       [[alternative HTML version deleted]]
    >>
    >> ______________________________________________
    >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    >> and provide commented, minimal, self-contained, reproducible code.
    >
    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.
    
    ______________________________________________
    R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    https://stat.ethz.ch/mailman/listinfo/r-help
    PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    and provide commented, minimal, self-contained, reproducible code.
    


From milujisb at gmail.com  Thu Dec 14 23:57:10 2017
From: milujisb at gmail.com (Miluji Sb)
Date: Thu, 14 Dec 2017 23:57:10 +0100
Subject: [R] GAM Poisson
Message-ID: <CAMLwc7Na59in7aVRxf6oBuk4YepsjaBsaUOdwi3VazZKcnL7Sw@mail.gmail.com>

Dear all,

I apologize as this may not be a strictly R question. I am running GAM
models using the mgcv package.

I was wondering if the interpretation of the smooth splines of the 'x'
variable is the same in the following two cases:

# Linear probability model
m1 <- gam(count ~ factor(city) + factor(year) + s(x),
data=data,na.action=na.omit)

# Poisson
m2 <- gam(count ~ factor(city) + factor(year) + s(x),data=data,
family=poisson,na.action=na.omit)

Thank you!

Sincerely,

Milu

	[[alternative HTML version deleted]]


From lasse at lassekliemann.de  Thu Dec 14 23:26:54 2017
From: lasse at lassekliemann.de (Lasse Kliemann)
Date: Thu, 14 Dec 2017 23:26:54 +0100
Subject: [R] delta and sd parameters for power.t.test
Message-ID: <87r2rxndxt.fsf@lassekliemann.de>

What is the rationale behind having both the delta and sd parameters for
the power.t.test function? For the relevant noncentrality parameter, we
only need the ratio delta/sd. If my effect size is given as Cohen's d,
then I only got that ratio and not sd.

As far as I see, in such a case, I can specify delta=d and leave sd at
its default value 1. Is this correct or am I missing something?

Thanks.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 832 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20171214/be724ff1/attachment.sig>

From dwinsemius at comcast.net  Fri Dec 15 02:46:12 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 14 Dec 2017 17:46:12 -0800
Subject: [R] delta and sd parameters for power.t.test
In-Reply-To: <87r2rxndxt.fsf@lassekliemann.de>
References: <87r2rxndxt.fsf@lassekliemann.de>
Message-ID: <A7F875F3-5B73-4BDE-9485-8C87888A2404@comcast.net>


> On Dec 14, 2017, at 2:26 PM, Lasse Kliemann <lasse at lassekliemann.de> wrote:
> 
> What is the rationale behind having both the delta and sd parameters for
> the power.t.test function?

One is the standard deviation of the hypothesized data (or pooled sd in the case of two sample) under the "alternative" and one is the mean of that data (or equivalently the differences if this is a two-sample test).

> For the relevant noncentrality parameter, we
> only need the ratio delta/sd. If my effect size is given as Cohen's d,
> then I only got that ratio and not sd.
> 

If the data has been "standardized", then Cohen's d could be given to the function as the value for delta since the sd default is 1.


> As far as I see, in such a case, I can specify delta=d and leave sd at
> its default value 1. Is this correct or am I missing something?
> 

Actually it sounds as though we are missing something. The power.t.test function makes no mention of Cohen's d or effect size. Is this question in response to a homework assignment about which we have not been informed?

Further questions should have some actual R code to make this an on-topic discussion for Rhelp.


> Thanks.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From lasse at lassekliemann.de  Fri Dec 15 10:37:48 2017
From: lasse at lassekliemann.de (Lasse Kliemann)
Date: Fri, 15 Dec 2017 10:37:48 +0100
Subject: [R] delta and sd parameters for power.t.test [solved]
In-Reply-To: <87r2rxndxt.fsf@lassekliemann.de>
References: <87r2rxndxt.fsf@lassekliemann.de>
Message-ID: <87o9n0nxg3.fsf@lassekliemann.de>

Lasse Kliemann <lasse at lassekliemann.de> writes:

> What is the rationale behind having both the delta and sd parameters for
> the power.t.test function? For the relevant noncentrality parameter, we
> only need the ratio delta/sd. If my effect size is given as Cohen's d,
> then I only got that ratio and not sd.
>
> As far as I see, in such a case, I can specify delta=d and leave sd at
> its default value 1.

Looking at the code in src/library/stats/R/power.R has clarified this
for me. The only spots were a given value of sd is actually used is (1)
in the computation of the ncp, where we have sqrt(n/tsample) * delta/sd,
so only the ratio delta/sd counts; (2) when delta is to be computed,
then sd is used for the interval given to uniroot, namely sd * c(1e-7,
1e+7), which I guess is a heuristic to cover all the plausible
values. So, yes, Cohen's d can be passed as delta=d when sd=1 is
maintained. Maybe a comment on this in the documentation would be
helpful. (For example, the pwr package
https://CRAN.R-project.org/package=pwr already mentions Cohen's d, but
that package is non-standard and unfortunately not even contained in the
Debian repositories. So it can make sense to stick with the standard
power.t.test as far as possible.)
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 832 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20171215/5852c06b/attachment.sig>

From esawiek at gmail.com  Fri Dec 15 15:32:38 2017
From: esawiek at gmail.com (Ek Esawi)
Date: Fri, 15 Dec 2017 09:32:38 -0500
Subject: [R] match and new columns
In-Reply-To: <CAJOiR6Zq05JLpx60xFWpbzByFOhuh-K7-ZkGcu=-BF9sRzt6aQ@mail.gmail.com>
References: <CAJOiR6Zq05JLpx60xFWpbzByFOhuh-K7-ZkGcu=-BF9sRzt6aQ@mail.gmail.com>
Message-ID: <CA+ZkTxvGs=VvGo8Svcb0DXsZXqKUP-Tb+0BLdKLy40k1uVQt5A@mail.gmail.com>

Hi Val?

Here is something similar to what Bill suggested.

wainb <- which(tdat$A %in% tdat$B)
tdat[c(length(tdat$D)-1,length(tdat$D)),c("D","E")] <-
tdat[wainb,c("B","C")]

HTH

EK

On Wed, Dec 13, 2017 at 4:36 PM, Val <valkremk at gmail.com> wrote:

> Hi all,
>
> I have a data frame
> tdat <- read.table(textConnection("A B C Y
> A12 B03 C04 0.70
> A23 B05 C06 0.05
> A14 B06 C07 1.20
> A25 A23 A12 3.51
> A16 A25 A14 2,16"),header = TRUE)
>
> I want match tdat$B with tdat$A and populate the  column   values of tdat$A
> ( col A and Col B) in the newly created columns (col D and col  E).  please
> find my attempt and the desired output below
>
> Desired output
> A B C Y  D E
> A12 B03 C04 0.70  0  0
> A23 B05 C06 0.05  0  0
> A14 B06 C07 1.20  0  0
> A25 A23 A12 3.51 B05 C06
> A16 A25 A14 2,16 A23 A12
>
> my attempt,
>
> tdat$D <- 0
> tdat$E <- 0
>
> if(tdat$B %in% tdat$A)
>   {
>   tdat$D <- tdat$A[tdat$B]
>   tdat$E <- tdat$A[tdat$C]
> }
>  but did not work.
>
> Thank you in advance
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From akshay_e4 at hotmail.com  Fri Dec 15 13:45:51 2017
From: akshay_e4 at hotmail.com (akshay kulkarni)
Date: Fri, 15 Dec 2017 12:45:51 +0000
Subject: [R] something weird has happened....!!!!!!!!!!
Message-ID: <SL2P216MB0091C22C4374F19A455B368EC80B0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear Members,



Today something weird has happened on my R console. I have attached two screenshots of the same vector in my R console but they differ.


Also one of my function returns negative values, even after double checking the code, which should return only positive values..


Whats wrong..? Reinstall R?


Thanks for help....


AKSHAY M KULKARNI


-------------- next part --------------
A non-text attachment was scrubbed...
Name: rscreenshot1.png
Type: image/png
Size: 252096 bytes
Desc: rscreenshot1.png
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20171215/83a04284/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Rscreenshot2.png
Type: image/png
Size: 260644 bytes
Desc: Rscreenshot2.png
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20171215/83a04284/attachment-0001.png>

From murdoch.duncan at gmail.com  Fri Dec 15 16:11:30 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 15 Dec 2017 10:11:30 -0500
Subject: [R] something weird has happened....!!!!!!!!!!
In-Reply-To: <SL2P216MB0091C22C4374F19A455B368EC80B0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB0091C22C4374F19A455B368EC80B0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
Message-ID: <4318a6af-3e75-e6da-80a0-78806856dd9d@gmail.com>

On 15/12/2017 7:45 AM, akshay kulkarni wrote:
> dear Members,
> 
> 
> 
> Today something weird has happened on my R console. I have attached two screenshots of the same vector in my R console but they differ.

Those are function calls, you aren't just printing the same vector 
twice, you're producing two vectors.

But in what way do they differ?  I looked at 3 entries, and they were 
identical.

Duncan Murdoch
> 
> 
> Also one of my function returns negative values, even after double checking the code, which should return only positive values..
> 
> 
> Whats wrong..? Reinstall R?
> 
> 
> Thanks for help....
> 
> 
> AKSHAY M KULKARNI
> 
> 
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mashranga at yahoo.com  Fri Dec 15 16:11:37 2017
From: mashranga at yahoo.com (Mohammad Tanvir Ahamed)
Date: Fri, 15 Dec 2017 15:11:37 +0000 (UTC)
Subject: [R] something weird has happened....!!!!!!!!!!
In-Reply-To: <SL2P216MB0091C22C4374F19A455B368EC80B0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB0091C22C4374F19A455B368EC80B0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
Message-ID: <1691079011.299412.1513350697087@mail.yahoo.com>

Dear Akshay,
Where is the problem !!Two data set seems identical? except one of them show 10 column and other showing 11 column on r console .??
Regards.............Tanvir Ahamed    Stockholm, Sweden     |  mashranga at yahoo.com  

    On Friday, December 15, 2017, 3:49:03 PM GMT+1, akshay kulkarni <akshay_e4 at hotmail.com> wrote:  
 
 dear Members,



Today something weird has happened on my R console. I have attached two screenshots of the same vector in my R console but they differ.


Also one of my function returns negative values, even after double checking the code, which should return only positive values..


Whats wrong..? Reinstall R?


Thanks for help....


AKSHAY M KULKARNI

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.  
	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Fri Dec 15 17:39:41 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 15 Dec 2017 08:39:41 -0800
Subject: [R] something weird has happened....!!!!!!!!!!
In-Reply-To: <4318a6af-3e75-e6da-80a0-78806856dd9d@gmail.com>
References: <SL2P216MB0091C22C4374F19A455B368EC80B0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
 <4318a6af-3e75-e6da-80a0-78806856dd9d@gmail.com>
Message-ID: <6DF51275-F66A-4825-B373-2FD987C5C640@dcn.davis.ca.us>

Entry 250 is different between the two. However, I (we?) have no idea what that function is so it might be using randomness as part of its calculation.
-- 
Sent from my phone. Please excuse my brevity.

On December 15, 2017 7:11:30 AM PST, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>On 15/12/2017 7:45 AM, akshay kulkarni wrote:
>> dear Members,
>> 
>> 
>> 
>> Today something weird has happened on my R console. I have attached
>two screenshots of the same vector in my R console but they differ.
>
>Those are function calls, you aren't just printing the same vector 
>twice, you're producing two vectors.
>
>But in what way do they differ?  I looked at 3 entries, and they were 
>identical.
>
>Duncan Murdoch
>> 
>> 
>> Also one of my function returns negative values, even after double
>checking the code, which should return only positive values..
>> 
>> 
>> Whats wrong..? Reinstall R?
>> 
>> 
>> Thanks for help....
>> 
>> 
>> AKSHAY M KULKARNI
>> 
>> 
>> 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From macqueen1 at llnl.gov  Fri Dec 15 17:52:48 2017
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Fri, 15 Dec 2017 16:52:48 +0000
Subject: [R] something weird has happened....!!!!!!!!!!
Message-ID: <0905050D-7C49-48A2-AC3E-243E80F9E954@llnl.gov>

You could try this and see what you get:

  unique(  yguii(ZEEL.NS, "o") - yguii(ZEEL.NS, "o")  )

or maybe

  table(  yguii(ZEEL.NS, "o") - yguii(ZEEL.NS, "o")  )

You showed two sets of output from the expression
   yguii(ZEEL.NS, "o")

Were they done one right after the other? Or could ZEEL.NS have changed in between?

The function could be using a variable that's only defined in the global environment (presumably by mistake, but it's an easy mistake to make), and its value changed in between function calls.

Reinstalling R would be a waste of time.

-Don

--
Don MacQueen
Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062
Lab cell 925-724-7509
 
 

On 12/15/17, 4:45 AM, "R-help on behalf of akshay kulkarni" <r-help-bounces at r-project.org on behalf of akshay_e4 at hotmail.com> wrote:

    dear Members,
    
    
    
    Today something weird has happened on my R console. I have attached two screenshots of the same vector in my R console but they differ.
    
    
    Also one of my function returns negative values, even after double checking the code, which should return only positive values..
    
    
    Whats wrong..? Reinstall R?
    
    
    Thanks for help....
    
    
    AKSHAY M KULKARNI
    
    
    


From wdunlap at tibco.com  Fri Dec 15 17:58:44 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 15 Dec 2017 08:58:44 -0800
Subject: [R] something weird has happened....!!!!!!!!!!
In-Reply-To: <0905050D-7C49-48A2-AC3E-243E80F9E954@llnl.gov>
References: <0905050D-7C49-48A2-AC3E-243E80F9E954@llnl.gov>
Message-ID: <CAF8bMca1CvMJ8KRY8nDZMF2vHkrruuzteqBM1NVrGFx+zMR-Ew@mail.gmail.com>

You can see if your function uses R's random number generator with
the following function.

isRandom <- function(expr) {
    randomSeedBefore <- get0(".Random.seed")
    force(expr)
    !identical(randomSeedBefore, get0(".Random.seed"))
}
isRandom(1:10)
#[1] FALSE
isRandom(runif(3)>.4)
#[1] TRUE

It gives a false negative if a function restores .Random.seed after making
some random numbers, but that is fairly uncommon.  It may give misleading
results if you use functions like parLapply.



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Fri, Dec 15, 2017 at 8:52 AM, MacQueen, Don <macqueen1 at llnl.gov> wrote:

> You could try this and see what you get:
>
>   unique(  yguii(ZEEL.NS, "o") - yguii(ZEEL.NS, "o")  )
>
> or maybe
>
>   table(  yguii(ZEEL.NS, "o") - yguii(ZEEL.NS, "o")  )
>
> You showed two sets of output from the expression
>    yguii(ZEEL.NS, "o")
>
> Were they done one right after the other? Or could ZEEL.NS have changed in
> between?
>
> The function could be using a variable that's only defined in the global
> environment (presumably by mistake, but it's an easy mistake to make), and
> its value changed in between function calls.
>
> Reinstalling R would be a waste of time.
>
> -Don
>
> --
> Don MacQueen
> Lawrence Livermore National Laboratory
> 7000 East Ave., L-627
> Livermore, CA 94550
> 925-423-1062
> Lab cell 925-724-7509
>
>
>
> On 12/15/17, 4:45 AM, "R-help on behalf of akshay kulkarni" <
> r-help-bounces at r-project.org on behalf of akshay_e4 at hotmail.com> wrote:
>
>     dear Members,
>
>
>
>     Today something weird has happened on my R console. I have attached
> two screenshots of the same vector in my R console but they differ.
>
>
>     Also one of my function returns negative values, even after double
> checking the code, which should return only positive values..
>
>
>     Whats wrong..? Reinstall R?
>
>
>     Thanks for help....
>
>
>     AKSHAY M KULKARNI
>
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From chocold12 at gmail.com  Fri Dec 15 18:21:54 2017
From: chocold12 at gmail.com (lily li)
Date: Fri, 15 Dec 2017 10:21:54 -0700
Subject: [R] Errors in reading in txt files
In-Reply-To: <32112127-9B34-4AD0-BDA8-420640D67184@llnl.gov>
References: <CAN5afy_Sewg6z1P+n-u9qHk29+1UjaddH7ZKGbSPn9APVXwyiQ@mail.gmail.com>
 <C883D7B3-521E-4AA8-ABFB-14DBE5C483C1@xs4all.nl>
 <CA+vqiLGjP=6QeJ1=n2=SzM0kZMU37mSY_tKVsiucW_kDH1ma=g@mail.gmail.com>
 <32112127-9B34-4AD0-BDA8-420640D67184@llnl.gov>
Message-ID: <CAN5afy9bNdMeS3OOpf0kdCWvvEG8D9XPtGY_AQuJpqZ50QoWPw@mail.gmail.com>

I use the method, df$Time = as.POSIXct(df$Time), but it has the warning
message:
Error in as.POSIXlt.character(x, tz, ...) :
  character string is not in a standard unambiguous format

On Thu, Dec 14, 2017 at 1:31 PM, MacQueen, Don <macqueen1 at llnl.gov> wrote:

> In addition to which, I would recommend
>
> df <- read.table("DATAM", header = TRUE, fill = TRUE,
> stringsAsFactors=FALSE)
>
> and then converting the Time column to POSIXct date-time values using
>   as.POSIXct()
> specifying the format using formatting codes found in
>   ?strptime
> because the times are not in the POSIXct default format.
>
>
> This example might indicate the idea:
>
> > as.POSIXct('2012-10-12 13:14')
> [1] "2012-10-12 13:14:00 PDT"
> > class(as.POSIXct('2012-10-12 13:14'))
> [1] "POSIXct" "POSIXt"
>
> -Don
>
> --
> Don MacQueen
> Lawrence Livermore National Laboratory
> 7000 East Ave., L-627
> Livermore, CA 94550
> 925-423-1062
> Lab cell 925-724-7509
>
>
>
> On 12/14/17, 11:01 AM, "R-help on behalf of Ista Zahn" <
> r-help-bounces at r-project.org on behalf of istazahn at gmail.com> wrote:
>
>     On Thu, Dec 14, 2017 at 1:58 PM, Berend Hasselman <bhh at xs4all.nl>
> wrote:
>     >
>     >> On 14 Dec 2017, at 19:36, lily li <chocold12 at gmail.com> wrote:
>     >>
>     >> Hi R users,
>     >>
>     >> I have a question about reading from text files. The file has the
> structure
>     >> below:
>     >>
>     >> Time                            Column1   Column2
>     >> 01.01.2001-12:00:00
>     >
>     > This line does not contain 3 elements; only one.
>     > You'll have to fix that line. Delete it, prepend it with a comment
> character of add enough columns.
>
>     I definitely don't recommend that. Instead, read
>
>     ?read.table
>
>     to learn about the "fill" and "header" arguments.
>
>     df = read.table("DATAM", header = TRUE, fill = TRUE)
>
>     will probably work.
>
>     Best,
>     Ista
>
>
>     >
>     >
>     > Berend
>     >
>     >> 01.01.2001-24:00:00        12             11
>     >> 01.02.2001-12:00:00        13             10
>     >> 01.02.2001-24:00:00        11             12
>     >> 01.03.2001-12:00:00        15             11
>     >> 01.03.2001-24:00:00        16             10
>     >> ...
>     >>
>     >> I just use the simple script to open it: df = read.table('DATAM',
> head=T).
>     >>
>     >> But it has the error and thus cannot read the file:
>     >> Error in scan(file = file, what = what, sep = sep, quote = quote,
> dec =
>     >> dec,  :
>     >>  line 1 did not have 3 elements
>     >>
>     >> How to read it with three fixed columns, and how to read the time
> format in
>     >> the first column correctly? Thanks for your help.
>     >>
>     >>       [[alternative HTML version deleted]]
>     >>
>     >> ______________________________________________
>     >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>     >> https://stat.ethz.ch/mailman/listinfo/r-help
>     >> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
>     >> and provide commented, minimal, self-contained, reproducible code.
>     >
>     > ______________________________________________
>     > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>     > https://stat.ethz.ch/mailman/listinfo/r-help
>     > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
>     > and provide commented, minimal, self-contained, reproducible code.
>
>     ______________________________________________
>     R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
>     and provide commented, minimal, self-contained, reproducible code.
>
>
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Dec 15 18:23:10 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 15 Dec 2017 09:23:10 -0800
Subject: [R] something weird has happened....!!!!!!!!!!
In-Reply-To: <SL2P216MB0091C22C4374F19A455B368EC80B0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB0091C22C4374F19A455B368EC80B0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
Message-ID: <35F583AC-174E-456D-947C-09EEBFCE1749@comcast.net>


> On Dec 15, 2017, at 4:45 AM, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
> 
> dear Members,
> 
> 
> 
> Today something weird has happened on my R console. I have attached two screenshots of the same vector in my R console but they differ.
> 
> 
> Also one of my function returns negative values, even after double checking the code, which should return only positive values..
> 
> 
> Whats wrong..? Reinstall R?
> 
> 
> Thanks for help....

We have no way of knowing what the function `yguii` might be doing. Apparently it has some sort of "random" internal process. If you want to see whether this is really a deterministic process (a pseudo-random process), then you should use `set.seed` to establish a reproducible state in the RNG prior to your function call.
> 
> 
> AKSHAY M KULKARNI
> 
> 
> <rscreenshot1.png><Rscreenshot2.png>______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From jdnewmil at dcn.davis.ca.us  Fri Dec 15 18:24:27 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 15 Dec 2017 09:24:27 -0800
Subject: [R] something weird has happened....!!!!!!!!!!
In-Reply-To: <SL2P216MB009105C1B511E03315E5C283C80B0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB0091C22C4374F19A455B368EC80B0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
 <4318a6af-3e75-e6da-80a0-78806856dd9d@gmail.com>,
 <6DF51275-F66A-4825-B373-2FD987C5C640@dcn.davis.ca.us>
 <SL2P216MB009105C1B511E03315E5C283C80B0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
Message-ID: <14C4D610-D450-49B0-891B-4B84A317569C@dcn.davis.ca.us>

Be sure to keep the mailing list in the loop by using reply-all or equivalent in your email program. 

You can use the set.seed function with any fixed value before you call functions that use randomness to create a reproducible output. 
-- 
Sent from my phone. Please excuse my brevity.

On December 15, 2017 9:07:24 AM PST, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>dear Jeff,
>
>                 I think you have hit the bulls eye.
>
>I am using MICE to populate NA values(imputation) in the source data.
>Everytime I call the function MICE populates different values instead
>of the NA's , right? Is this the randomness you are speaking about?
>
>
>I will test the function again tomorrow. Hope you are right....!!!!!!
>
>
>Thanks a lot......!!!!!!!!
>
>
>yours
>
>AKSHAY M KULKARNI
>
>________________________________
>From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>Sent: Friday, December 15, 2017 10:09 PM
>To: r-help at r-project.org; Duncan Murdoch; akshay kulkarni;
>r-help at r-project.org
>Subject: Re: [R] something weird has happened....!!!!!!!!!!
>
>Entry 250 is different between the two. However, I (we?) have no idea
>what that function is so it might be using randomness as part of its
>calculation.
>--
>Sent from my phone. Please excuse my brevity.
>
>On December 15, 2017 7:11:30 AM PST, Duncan Murdoch
><murdoch.duncan at gmail.com> wrote:
>>On 15/12/2017 7:45 AM, akshay kulkarni wrote:
>>> dear Members,
>>>
>>>
>>>
>>> Today something weird has happened on my R console. I have attached
>>two screenshots of the same vector in my R console but they differ.
>>
>>Those are function calls, you aren't just printing the same vector
>>twice, you're producing two vectors.
>>
>>But in what way do they differ?  I looked at 3 entries, and they were
>>identical.
>>
>>Duncan Murdoch
>>>
>>>
>>> Also one of my function returns negative values, even after double
>>checking the code, which should return only positive values..
>>>
>>>
>>> Whats wrong..? Reinstall R?
>>>
>>>
>>> Thanks for help....
>>>
>>>
>>> AKSHAY M KULKARNI
>>>
>>>
>>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>R-help -- Main R Mailing List: Primary help - Homepage -
>SfS<https://stat.ethz.ch/mailman/listinfo/r-help>
>stat.ethz.ch
>The main R mailing list, for announcements about the development of R
>and the availability of new code, questions and answers about problems
>and solutions using R ...
>
>
>
>>> PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>R-help -- Main R Mailing List: Primary help - Homepage -
>SfS<https://stat.ethz.ch/mailman/listinfo/r-help>
>stat.ethz.ch
>The main R mailing list, for announcements about the development of R
>and the availability of new code, questions and answers about problems
>and solutions using R ...
>
>
>
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Fri Dec 15 18:26:18 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 15 Dec 2017 09:26:18 -0800
Subject: [R] Errors in reading in txt files
In-Reply-To: <CAN5afy9bNdMeS3OOpf0kdCWvvEG8D9XPtGY_AQuJpqZ50QoWPw@mail.gmail.com>
References: <CAN5afy_Sewg6z1P+n-u9qHk29+1UjaddH7ZKGbSPn9APVXwyiQ@mail.gmail.com>
 <C883D7B3-521E-4AA8-ABFB-14DBE5C483C1@xs4all.nl>
 <CA+vqiLGjP=6QeJ1=n2=SzM0kZMU37mSY_tKVsiucW_kDH1ma=g@mail.gmail.com>
 <32112127-9B34-4AD0-BDA8-420640D67184@llnl.gov>
 <CAN5afy9bNdMeS3OOpf0kdCWvvEG8D9XPtGY_AQuJpqZ50QoWPw@mail.gmail.com>
Message-ID: <772269C8-44C5-4BA7-86DB-3ED4C56D56B8@comcast.net>


> On Dec 15, 2017, at 9:21 AM, lily li <chocold12 at gmail.com> wrote:
> 
> I use the method, df$Time = as.POSIXct(df$Time), but it has the warning
> message:
> Error in as.POSIXlt.character(x, tz, ...) :
>  character string is not in a standard unambiguous format

That's because your date-time data is not in "%Y-%m-%d %H:%M" format. Read:

 ?strptime

-- 
David.
> 
> On Thu, Dec 14, 2017 at 1:31 PM, MacQueen, Don <macqueen1 at llnl.gov> wrote:
> 
>> In addition to which, I would recommend
>> 
>> df <- read.table("DATAM", header = TRUE, fill = TRUE,
>> stringsAsFactors=FALSE)
>> 
>> and then converting the Time column to POSIXct date-time values using
>>  as.POSIXct()
>> specifying the format using formatting codes found in
>>  ?strptime
>> because the times are not in the POSIXct default format.
>> 
>> 
>> This example might indicate the idea:
>> 
>>> as.POSIXct('2012-10-12 13:14')
>> [1] "2012-10-12 13:14:00 PDT"
>>> class(as.POSIXct('2012-10-12 13:14'))
>> [1] "POSIXct" "POSIXt"
>> 
>> -Don
>> 
>> --
>> Don MacQueen
>> Lawrence Livermore National Laboratory
>> 7000 East Ave., L-627
>> Livermore, CA 94550
>> 925-423-1062
>> Lab cell 925-724-7509
>> 
>> 
>> 
>> On 12/14/17, 11:01 AM, "R-help on behalf of Ista Zahn" <
>> r-help-bounces at r-project.org on behalf of istazahn at gmail.com> wrote:
>> 
>>    On Thu, Dec 14, 2017 at 1:58 PM, Berend Hasselman <bhh at xs4all.nl>
>> wrote:
>>> 
>>>> On 14 Dec 2017, at 19:36, lily li <chocold12 at gmail.com> wrote:
>>>> 
>>>> Hi R users,
>>>> 
>>>> I have a question about reading from text files. The file has the
>> structure
>>>> below:
>>>> 
>>>> Time                            Column1   Column2
>>>> 01.01.2001-12:00:00
>>> 
>>> This line does not contain 3 elements; only one.
>>> You'll have to fix that line. Delete it, prepend it with a comment
>> character of add enough columns.
>> 
>>    I definitely don't recommend that. Instead, read
>> 
>>    ?read.table
>> 
>>    to learn about the "fill" and "header" arguments.
>> 
>>    df = read.table("DATAM", header = TRUE, fill = TRUE)
>> 
>>    will probably work.
>> 
>>    Best,
>>    Ista
>> 
>> 
>>> 
>>> 
>>> Berend
>>> 
>>>> 01.01.2001-24:00:00        12             11
>>>> 01.02.2001-12:00:00        13             10
>>>> 01.02.2001-24:00:00        11             12
>>>> 01.03.2001-12:00:00        15             11
>>>> 01.03.2001-24:00:00        16             10
>>>> ...
>>>> 
>>>> I just use the simple script to open it: df = read.table('DATAM',
>> head=T).
>>>> 
>>>> But it has the error and thus cannot read the file:
>>>> Error in scan(file = file, what = what, sep = sep, quote = quote,
>> dec =
>>>> dec,  :
>>>> line 1 did not have 3 elements
>>>> 
>>>> How to read it with three fixed columns, and how to read the time
>> format in
>>>> the first column correctly? Thanks for your help.
>>>> 
>>>>      [[alternative HTML version deleted]]
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>>    ______________________________________________
>>    R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>    https://stat.ethz.ch/mailman/listinfo/r-help
>>    PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>>    and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From jdnewmil at dcn.davis.ca.us  Fri Dec 15 19:15:54 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 15 Dec 2017 10:15:54 -0800
Subject: [R] Errors in reading in txt files
In-Reply-To: <CAN5afy9bNdMeS3OOpf0kdCWvvEG8D9XPtGY_AQuJpqZ50QoWPw@mail.gmail.com>
References: <CAN5afy_Sewg6z1P+n-u9qHk29+1UjaddH7ZKGbSPn9APVXwyiQ@mail.gmail.com>
 <C883D7B3-521E-4AA8-ABFB-14DBE5C483C1@xs4all.nl>
 <CA+vqiLGjP=6QeJ1=n2=SzM0kZMU37mSY_tKVsiucW_kDH1ma=g@mail.gmail.com>
 <32112127-9B34-4AD0-BDA8-420640D67184@llnl.gov>
 <CAN5afy9bNdMeS3OOpf0kdCWvvEG8D9XPtGY_AQuJpqZ50QoWPw@mail.gmail.com>
Message-ID: <616FD656-C989-4F82-AD05-24E4420E3DFB@dcn.davis.ca.us>

Your times are formatted as

01.01.2001-24:00:00

but the default format is

2001-01-01 24:00:00

so you need to specify a format argument with as.POSIXct. Read about format strings in ?strptime.
-- 
Sent from my phone. Please excuse my brevity.

On December 15, 2017 9:21:54 AM PST, lily li <chocold12 at gmail.com> wrote:
>I use the method, df$Time = as.POSIXct(df$Time), but it has the warning
>message:
>Error in as.POSIXlt.character(x, tz, ...) :
>  character string is not in a standard unambiguous format
>
>On Thu, Dec 14, 2017 at 1:31 PM, MacQueen, Don <macqueen1 at llnl.gov>
>wrote:
>
>> In addition to which, I would recommend
>>
>> df <- read.table("DATAM", header = TRUE, fill = TRUE,
>> stringsAsFactors=FALSE)
>>
>> and then converting the Time column to POSIXct date-time values using
>>   as.POSIXct()
>> specifying the format using formatting codes found in
>>   ?strptime
>> because the times are not in the POSIXct default format.
>>
>>
>> This example might indicate the idea:
>>
>> > as.POSIXct('2012-10-12 13:14')
>> [1] "2012-10-12 13:14:00 PDT"
>> > class(as.POSIXct('2012-10-12 13:14'))
>> [1] "POSIXct" "POSIXt"
>>
>> -Don
>>
>> --
>> Don MacQueen
>> Lawrence Livermore National Laboratory
>> 7000 East Ave., L-627
>> Livermore, CA 94550
>> 925-423-1062
>> Lab cell 925-724-7509
>>
>>
>>
>> On 12/14/17, 11:01 AM, "R-help on behalf of Ista Zahn" <
>> r-help-bounces at r-project.org on behalf of istazahn at gmail.com> wrote:
>>
>>     On Thu, Dec 14, 2017 at 1:58 PM, Berend Hasselman <bhh at xs4all.nl>
>> wrote:
>>     >
>>     >> On 14 Dec 2017, at 19:36, lily li <chocold12 at gmail.com> wrote:
>>     >>
>>     >> Hi R users,
>>     >>
>>     >> I have a question about reading from text files. The file has
>the
>> structure
>>     >> below:
>>     >>
>>     >> Time                            Column1   Column2
>>     >> 01.01.2001-12:00:00
>>     >
>>     > This line does not contain 3 elements; only one.
>>     > You'll have to fix that line. Delete it, prepend it with a
>comment
>> character of add enough columns.
>>
>>     I definitely don't recommend that. Instead, read
>>
>>     ?read.table
>>
>>     to learn about the "fill" and "header" arguments.
>>
>>     df = read.table("DATAM", header = TRUE, fill = TRUE)
>>
>>     will probably work.
>>
>>     Best,
>>     Ista
>>
>>
>>     >
>>     >
>>     > Berend
>>     >
>>     >> 01.01.2001-24:00:00        12             11
>>     >> 01.02.2001-12:00:00        13             10
>>     >> 01.02.2001-24:00:00        11             12
>>     >> 01.03.2001-12:00:00        15             11
>>     >> 01.03.2001-24:00:00        16             10
>>     >> ...
>>     >>
>>     >> I just use the simple script to open it: df =
>read.table('DATAM',
>> head=T).
>>     >>
>>     >> But it has the error and thus cannot read the file:
>>     >> Error in scan(file = file, what = what, sep = sep, quote =
>quote,
>> dec =
>>     >> dec,  :
>>     >>  line 1 did not have 3 elements
>>     >>
>>     >> How to read it with three fixed columns, and how to read the
>time
>> format in
>>     >> the first column correctly? Thanks for your help.
>>     >>
>>     >>       [[alternative HTML version deleted]]
>>     >>
>>     >> ______________________________________________
>>     >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>see
>>     >> https://stat.ethz.ch/mailman/listinfo/r-help
>>     >> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>>     >> and provide commented, minimal, self-contained, reproducible
>code.
>>     >
>>     > ______________________________________________
>>     > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>see
>>     > https://stat.ethz.ch/mailman/listinfo/r-help
>>     > PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>>     > and provide commented, minimal, self-contained, reproducible
>code.
>>
>>     ______________________________________________
>>     R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>     https://stat.ethz.ch/mailman/listinfo/r-help
>>     PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>>     and provide commented, minimal, self-contained, reproducible
>code.
>>
>>
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From r.turner at auckland.ac.nz  Fri Dec 15 22:30:35 2017
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sat, 16 Dec 2017 10:30:35 +1300
Subject: [R] [FORGED]  something weird has happened....!!!!!!!!!!
In-Reply-To: <SL2P216MB0091C22C4374F19A455B368EC80B0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB0091C22C4374F19A455B368EC80B0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
Message-ID: <95f0990b-e229-614b-6727-597da2c8d55f@auckland.ac.nz>

On 16/12/17 01:45, akshay kulkarni wrote:
> dear Members,
> 
> 
> 
> Today something weird has happened on my R console. I have attached two screenshots of the same vector in my R console but they differ.
> 
> 
> Also one of my function returns negative values, even after double checking the code, which should return only positive values..
> 
> 
> Whats wrong..? 

How on earth could anyone possibly tell on the basis of the (opaque) 
pseudo-information that you have given us?

> Reinstall R?

It is possible, but *highly unlikely* that this is what you should do.

As I said, it's impossible to suggest what you really should do unless 
you provide some genuine information.

Few members of this list are telepathic.  I certainly am not.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From rmh at temple.edu  Sat Dec 16 01:40:37 2017
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri, 15 Dec 2017 19:40:37 -0500
Subject: [R] t.trellis doesn't invert
Message-ID: <CAGx1TMDiE0e5Lkja98YnjEhyQOPLPNJMjcDZYZwcuy2=LRQdTg@mail.gmail.com>

> version
               _
platform       x86_64-w64-mingw32
arch           x86_64
os             mingw32
system         x86_64, mingw32
status         Patched
major          3
minor          4.3
year           2017
month          12
day            12
svn rev        73903
language       R
version.string R version 3.4.3 Patched (2017-12-12 r73903)
nickname       Kite-Eating Tree
> tt <- xyplot(y ~ x | a*b, data=data.frame(x=1:6, y=7:12, a=c(1:3,1:3), b=c(1,1,1,2,2,2)))
> tt
> t(tt)
> t(t(tt))
Error in update.trellis(x, perm.cond = rev(x$perm.cond)) :
  Invalid value of 'perm.cond'
>


compare to numeric matrix

> tmp <- cbind(1:3, 4:6)
> tmp
     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]    3    6
> t(tmp)
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    4    5    6
> t(t(tmp))
     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]    3    6
>


From akshay_e4 at hotmail.com  Sat Dec 16 08:39:02 2017
From: akshay_e4 at hotmail.com (akshay kulkarni)
Date: Sat, 16 Dec 2017 07:39:02 +0000
Subject: [R] help needed on MICE....
Message-ID: <SL2P216MB0091F3C18D7F481FB2B77039C8080@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear members,



I have written functions f1 and f2, which is used in a larger function p. f1 and f2 calls on a data set which is acted on by the mice function. when f1 and f2 calls on mice, the source data set gets filled in by different values for the same position which was earlier populated by NA's. This perverts the return object of p, which acts on the source data populated by different values for the same NA position.


Is there any other way to fill the NA position, in the source data, by the same value by the mice function, irrespective by the number of calls by mice? In other words, the NA value has to be filled by the same value whether in the first call or the second call or any other call.


Thanks a lot


Yours sincerely,

AKSHAY M KULKARNI


	[[alternative HTML version deleted]]


From emorway at usgs.gov  Sat Dec 16 14:18:35 2017
From: emorway at usgs.gov (Morway, Eric)
Date: Sat, 16 Dec 2017 05:18:35 -0800
Subject: [R] Finding center of mass in a hydrologic time series
Message-ID: <CAPoqHzqmjtAJ1cv+c_orE-i_A__XPf_iriT15O+vUjd95E1n_Q@mail.gmail.com>

The small bit of script below is an example of what I'm attempting to do -
find the day on which the 'center of mass' occurs.  In case that is the
wrong term, I'd like to know the day that essentially cuts the area under
the curve in to two equal parts:

set.seed(4004)
Date <- seq(as.Date('2000-09-01'), as.Date('2000-09-30'), by='day')
hyd <- ((100*(sin(seq(0.5,4.5,length.out=30))+10) +
seq(45,1,length.out=30)) + rnorm(30)*8) - 800

# View the example curve
plot(Date, hyd, las=1)

# By trial-and-error, the day on which the center of mass occurs is the
11th day:
# Add up the area under the curve for the first 11 days and compare
# with the last 19 days:

sum(hyd[1:11])
# 3546.364
sum(hyd[12:30])
# 3947.553

# Add up the area under the curve for the first 12 days and compare
# with the last 18 days:

sum(hyd[1:12])
# 3875.753
sum(hyd[13:30])
# 3618.164

By day 12, the halfway point has already been passed, so the answer that
would be returned would be:

Date[11]
# "2000-09-11"

For the larger problem, it'd be handy if the proposed function could
process a multi-year time series (a runoff hydrograph) and return the day
of the center of mass for each year in the time series.

I appreciate any pointers...Eric

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Sat Dec 16 14:32:42 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Sat, 16 Dec 2017 15:32:42 +0200
Subject: [R] Finding center of mass in a hydrologic time series
In-Reply-To: <CAPoqHzqmjtAJ1cv+c_orE-i_A__XPf_iriT15O+vUjd95E1n_Q@mail.gmail.com>
References: <CAPoqHzqmjtAJ1cv+c_orE-i_A__XPf_iriT15O+vUjd95E1n_Q@mail.gmail.com>
Message-ID: <CAGgJW77VGwaY3CbkPe0-eNXoAH+Jd52vMjNzf6oKTdUw+T6bzQ@mail.gmail.com>

Hi Eric,
How about

match( TRUE, cumsum(hyd/sum(hyd)) > .5 ) - 1

HTH,
Eric


On Sat, Dec 16, 2017 at 3:18 PM, Morway, Eric <emorway at usgs.gov> wrote:

> The small bit of script below is an example of what I'm attempting to do -
> find the day on which the 'center of mass' occurs.  In case that is the
> wrong term, I'd like to know the day that essentially cuts the area under
> the curve in to two equal parts:
>
> set.seed(4004)
> Date <- seq(as.Date('2000-09-01'), as.Date('2000-09-30'), by='day')
> hyd <- ((100*(sin(seq(0.5,4.5,length.out=30))+10) +
> seq(45,1,length.out=30)) + rnorm(30)*8) - 800
>
> # View the example curve
> plot(Date, hyd, las=1)
>
> # By trial-and-error, the day on which the center of mass occurs is the
> 11th day:
> # Add up the area under the curve for the first 11 days and compare
> # with the last 19 days:
>
> sum(hyd[1:11])
> # 3546.364
> sum(hyd[12:30])
> # 3947.553
>
> # Add up the area under the curve for the first 12 days and compare
> # with the last 18 days:
>
> sum(hyd[1:12])
> # 3875.753
> sum(hyd[13:30])
> # 3618.164
>
> By day 12, the halfway point has already been passed, so the answer that
> would be returned would be:
>
> Date[11]
> # "2000-09-11"
>
> For the larger problem, it'd be handy if the proposed function could
> process a multi-year time series (a runoff hydrograph) and return the day
> of the center of mass for each year in the time series.
>
> I appreciate any pointers...Eric
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From erinm.hodgess at gmail.com  Sat Dec 16 15:00:38 2017
From: erinm.hodgess at gmail.com (Erin Hodgess)
Date: Sat, 16 Dec 2017 08:00:38 -0600
Subject: [R] Generating help files for a function
Message-ID: <CACxE24kF6jD9Ea3Pc-b92OC65CQfCmUyyFDD_X1ytUzZd+05vA@mail.gmail.com>

Hello everyone!

I'm in the process of writing a package, and I'm using the lovely "R
Package" book as a guideline.

However, in the midst of my work,  I discovered that I had omitted a
function and am now putting in it the package.  Not a problem.  But the
problem is the help file.  What is the best way to generate a help file
"after the fact" like that, please?

Thank you in advance.  Hope everyone is enjoying various holidays.

Sincerely,
Erin


-- 
Erin Hodgess
Associate Professor
Department of Mathematical and Statistics
University of Houston - Downtown
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From btupper at bigelow.org  Sat Dec 16 15:36:55 2017
From: btupper at bigelow.org (Ben Tupper)
Date: Sat, 16 Dec 2017 09:36:55 -0500
Subject: [R] Generating help files for a function
In-Reply-To: <CACxE24kF6jD9Ea3Pc-b92OC65CQfCmUyyFDD_X1ytUzZd+05vA@mail.gmail.com>
References: <CACxE24kF6jD9Ea3Pc-b92OC65CQfCmUyyFDD_X1ytUzZd+05vA@mail.gmail.com>
Message-ID: <4E5EE605-F279-4B95-82BF-619951F1E6F7@bigelow.org>

Hi,

If you are using roxygen-style function documentation then why not use devtools::document()?  

Ben

  

> On Dec 16, 2017, at 9:00 AM, Erin Hodgess <erinm.hodgess at gmail.com> wrote:
> 
> Hello everyone!
> 
> I'm in the process of writing a package, and I'm using the lovely "R
> Package" book as a guideline.
> 
> However, in the midst of my work,  I discovered that I had omitted a
> function and am now putting in it the package.  Not a problem.  But the
> problem is the help file.  What is the best way to generate a help file
> "after the fact" like that, please?
> 
> Thank you in advance.  Hope everyone is enjoying various holidays.
> 
> Sincerely,
> Erin
> 
> 
> -- 
> Erin Hodgess
> Associate Professor
> Department of Mathematical and Statistics
> University of Houston - Downtown
> mailto: erinm.hodgess at gmail.com
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Ecocast Reports: http://seascapemodeling.org/ecocast.html
Tick Reports: https://report.bigelow.org/tick/
Jellyfish Reports: https://jellyfish.bigelow.org/jellyfish/




	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sat Dec 16 16:08:40 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 16 Dec 2017 07:08:40 -0800
Subject: [R] Generating help files for a function
In-Reply-To: <4E5EE605-F279-4B95-82BF-619951F1E6F7@bigelow.org>
References: <CACxE24kF6jD9Ea3Pc-b92OC65CQfCmUyyFDD_X1ytUzZd+05vA@mail.gmail.com>
 <4E5EE605-F279-4B95-82BF-619951F1E6F7@bigelow.org>
Message-ID: <CAGxFJbTjX5GTuGtf8K3EPjVTJBVzZ9KP23tQn55_t-yLVffaeg@mail.gmail.com>

... and please note for the future and in case Ben's reply does not suffice
that such queries should generally go to the r-package-devel mailing list.


Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Sat, Dec 16, 2017 at 6:36 AM, Ben Tupper <btupper at bigelow.org> wrote:

> Hi,
>
> If you are using roxygen-style function documentation then why not use
> devtools::document()?
>
> Ben
>
>
>
> > On Dec 16, 2017, at 9:00 AM, Erin Hodgess <erinm.hodgess at gmail.com>
> wrote:
> >
> > Hello everyone!
> >
> > I'm in the process of writing a package, and I'm using the lovely "R
> > Package" book as a guideline.
> >
> > However, in the midst of my work,  I discovered that I had omitted a
> > function and am now putting in it the package.  Not a problem.  But the
> > problem is the help file.  What is the best way to generate a help file
> > "after the fact" like that, please?
> >
> > Thank you in advance.  Hope everyone is enjoying various holidays.
> >
> > Sincerely,
> > Erin
> >
> >
> > --
> > Erin Hodgess
> > Associate Professor
> > Department of Mathematical and Statistics
> > University of Houston - Downtown
> > mailto: erinm.hodgess at gmail.com
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> Ben Tupper
> Bigelow Laboratory for Ocean Sciences
> 60 Bigelow Drive, P.O. Box 380
> East Boothbay, Maine 04544
> http://www.bigelow.org
>
> Ecocast Reports: http://seascapemodeling.org/ecocast.html
> Tick Reports: https://report.bigelow.org/tick/
> Jellyfish Reports: https://jellyfish.bigelow.org/jellyfish/
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Sat Dec 16 17:32:41 2017
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Sat, 16 Dec 2017 17:32:41 +0100
Subject: [R] Finding center of mass in a hydrologic time series
In-Reply-To: <CAGgJW77VGwaY3CbkPe0-eNXoAH+Jd52vMjNzf6oKTdUw+T6bzQ@mail.gmail.com>
References: <CAPoqHzqmjtAJ1cv+c_orE-i_A__XPf_iriT15O+vUjd95E1n_Q@mail.gmail.com>
 <CAGgJW77VGwaY3CbkPe0-eNXoAH+Jd52vMjNzf6oKTdUw+T6bzQ@mail.gmail.com>
Message-ID: <CAJuCY5zAt3LRzXfea9bpjQsv4Jj66pPo98rcL5Z4TsHmKiPsbg@mail.gmail.com>

Slightly faster: sum(cumsum(hyd) <= .5 * sum(hyd))

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
AND FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Kliniekstraat 25, B-1070 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no
more than asking him to perform a post-mortem examination: he may be
able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does
not ensure that a reasonable answer can be extracted from a given body
of data. ~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////


Van 14 tot en met 19 december 2017 verhuizen we uit onze vestiging in
Brussel naar het Herman Teirlinckgebouw op de site Thurn & Taxis.
Vanaf dan ben je welkom op het nieuwe adres: Havenlaan 88 bus 73, 1000 Brussel.

///////////////////////////////////////////////////////////////////////////////////////////



2017-12-16 14:32 GMT+01:00 Eric Berger <ericjberger at gmail.com>:
> Hi Eric,
> How about
>
> match( TRUE, cumsum(hyd/sum(hyd)) > .5 ) - 1
>
> HTH,
> Eric
>
>
> On Sat, Dec 16, 2017 at 3:18 PM, Morway, Eric <emorway at usgs.gov> wrote:
>
>> The small bit of script below is an example of what I'm attempting to do -
>> find the day on which the 'center of mass' occurs.  In case that is the
>> wrong term, I'd like to know the day that essentially cuts the area under
>> the curve in to two equal parts:
>>
>> set.seed(4004)
>> Date <- seq(as.Date('2000-09-01'), as.Date('2000-09-30'), by='day')
>> hyd <- ((100*(sin(seq(0.5,4.5,length.out=30))+10) +
>> seq(45,1,length.out=30)) + rnorm(30)*8) - 800
>>
>> # View the example curve
>> plot(Date, hyd, las=1)
>>
>> # By trial-and-error, the day on which the center of mass occurs is the
>> 11th day:
>> # Add up the area under the curve for the first 11 days and compare
>> # with the last 19 days:
>>
>> sum(hyd[1:11])
>> # 3546.364
>> sum(hyd[12:30])
>> # 3947.553
>>
>> # Add up the area under the curve for the first 12 days and compare
>> # with the last 18 days:
>>
>> sum(hyd[1:12])
>> # 3875.753
>> sum(hyd[13:30])
>> # 3618.164
>>
>> By day 12, the halfway point has already been passed, so the answer that
>> would be returned would be:
>>
>> Date[11]
>> # "2000-09-11"
>>
>> For the larger problem, it'd be handy if the proposed function could
>> process a multi-year time series (a runoff hydrograph) and return the day
>> of the center of mass for each year in the time series.
>>
>> I appreciate any pointers...Eric
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Berwin.Turlach at gmail.com  Sat Dec 16 16:15:27 2017
From: Berwin.Turlach at gmail.com (Berwin A Turlach)
Date: Sat, 16 Dec 2017 23:15:27 +0800
Subject: [R] Generating help files for a function
In-Reply-To: <CACxE24kF6jD9Ea3Pc-b92OC65CQfCmUyyFDD_X1ytUzZd+05vA@mail.gmail.com>
References: <CACxE24kF6jD9Ea3Pc-b92OC65CQfCmUyyFDD_X1ytUzZd+05vA@mail.gmail.com>
Message-ID: <20171216231527.5e330049@absentia>

G'day Erin,

On Sat, 16 Dec 2017 08:00:38 -0600
Erin Hodgess <erinm.hodgess at gmail.com> wrote:

> I'm in the process of writing a package, and I'm using the lovely "R
> Package" book as a guideline.
> 
> However, in the midst of my work,  I discovered that I had omitted a
> function and am now putting in it the package.  Not a problem.  But
> the problem is the help file.  What is the best way to generate a
> help file "after the fact" like that, please?

It depends on how you decided to write the documentation.  If you
follow the "R Package" guidelines and use roxygen2, just add the
comments for the documentation at the beginning of the file and follow
the procedure outline in "R Packages" book.

If you are writing the documentation separate, more like the "Writing R
Extensions" manual, then (1) start R, (2) source the file in which the
function is so that is is in your workspace, (3) say "prompt(foo)" if
the function's name is foo and (4) copy the resulting foo.Rd into
the /man directory of your package.

Cheers,

	Berwin


From murdoch.duncan at gmail.com  Sat Dec 16 18:08:13 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 16 Dec 2017 12:08:13 -0500
Subject: [R] Generating help files for a function
In-Reply-To: <20171216231527.5e330049@absentia>
References: <CACxE24kF6jD9Ea3Pc-b92OC65CQfCmUyyFDD_X1ytUzZd+05vA@mail.gmail.com>
 <20171216231527.5e330049@absentia>
Message-ID: <ff35f2b6-59db-719b-9b5a-7710a49624df@gmail.com>

On 16/12/2017 10:15 AM, Berwin A Turlach wrote:
> G'day Erin,
> 
> On Sat, 16 Dec 2017 08:00:38 -0600
> Erin Hodgess <erinm.hodgess at gmail.com> wrote:
> 
>> I'm in the process of writing a package, and I'm using the lovely "R
>> Package" book as a guideline.
>>
>> However, in the midst of my work,  I discovered that I had omitted a
>> function and am now putting in it the package.  Not a problem.  But
>> the problem is the help file.  What is the best way to generate a
>> help file "after the fact" like that, please?
> 
> It depends on how you decided to write the documentation.  If you
> follow the "R Package" guidelines and use roxygen2, just add the
> comments for the documentation at the beginning of the file and follow
> the procedure outline in "R Packages" book.
> 
> If you are writing the documentation separate, more like the "Writing R
> Extensions" manual, then (1) start R, (2) source the file in which the
> function is so that is is in your workspace, (3) say "prompt(foo)" if
> the function's name is foo and (4) copy the resulting foo.Rd into
> the /man directory of your package.

I'm in the latter camp, but my workflow is slightly different from Berwin's:

After writing a new function and putting it into the package:

1. install the package, and attach it (using library() or require()).
2. use setwd() to change to the man directory.
3. use prompt() to create the skeleton help page.

After that, you edit that new .Rd file, and build the package again.

The first two steps are particularly easy in RStudio:  for 1, just click 
"Install and restart" in the build pane, and for 2, navigate in the file 
pane to the man directory, and choose "Set as working directory" from 
the "More" tab.

Duncan Murdoch


From es at enricoschumann.net  Sat Dec 16 18:27:21 2017
From: es at enricoschumann.net (Enrico Schumann)
Date: Sat, 16 Dec 2017 18:27:21 +0100
Subject: [R] Generating help files for a function
In-Reply-To: <CACxE24kF6jD9Ea3Pc-b92OC65CQfCmUyyFDD_X1ytUzZd+05vA@mail.gmail.com>
 (Erin Hodgess's message of "Sat, 16 Dec 2017 08:00:38 -0600")
References: <CACxE24kF6jD9Ea3Pc-b92OC65CQfCmUyyFDD_X1ytUzZd+05vA@mail.gmail.com>
Message-ID: <87r2rulh1i.fsf@enricoschumann.net>

On Sat, 16 Dec 2017, Erin Hodgess writes:

> Hello everyone!
>
> I'm in the process of writing a package, and I'm using the lovely "R
> Package" book as a guideline.
>
> However, in the midst of my work,  I discovered that I had omitted a
> function and am now putting in it the package.  Not a problem.  But the
> problem is the help file.  What is the best way to generate a help file
> "after the fact" like that, please?
>
> Thank you in advance.  Hope everyone is enjoying various holidays.
>
> Sincerely,
> Erin

see ?prompt 

-- 
Enrico Schumann
Lucerne, Switzerland
http://enricoschumann.net


From abouelmakarim1962 at gmail.com  Sat Dec 16 20:44:30 2017
From: abouelmakarim1962 at gmail.com (AbouEl-Makarim Aboueissa)
Date: Sat, 16 Dec 2017 14:44:30 -0500
Subject: [R] Auto Data in the ISLR Package
Message-ID: <CAE9stmcaTUQ5P-NLD1RU5it1W=8YW7DwmUdz+HYYigm+bGgSqw@mail.gmail.com>

Dear All:

I would like to create a subset data set *with only* all Ford and all
Toyota cars from the Auto data set  in ISLR R Package.  Thank you very much
in advance.

Please use the following code to see how is the data look like.


install.packages("ISLR")
library(ISLR)
data(Auto)
head(Auto)


with many thanks
abou
______________________


*AbouEl-Makarim Aboueissa, PhD*

*Professor of Statistics*

*Department of Mathematics and Statistics*
*University of Southern Maine*

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sat Dec 16 21:28:05 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 16 Dec 2017 12:28:05 -0800
Subject: [R] Auto Data in the ISLR Package
In-Reply-To: <CAE9stmcaTUQ5P-NLD1RU5it1W=8YW7DwmUdz+HYYigm+bGgSqw@mail.gmail.com>
References: <CAE9stmcaTUQ5P-NLD1RU5it1W=8YW7DwmUdz+HYYigm+bGgSqw@mail.gmail.com>
Message-ID: <CAGxFJbS20DZ9ynbQ6eabA1hj7JspkDvXDnwZqzmSMD5+HsZeCw@mail.gmail.com>

I did not care to load the packages -- small reproducible examples are
preferable, as the posting guide suggests.

But, if I have understood correctly:

See, e.g. ?subset

Alternatively, you can read up on indexing data frames in any good basic R
tutorial.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Sat, Dec 16, 2017 at 11:44 AM, AbouEl-Makarim Aboueissa <
abouelmakarim1962 at gmail.com> wrote:

> Dear All:
>
> I would like to create a subset data set *with only* all Ford and all
> Toyota cars from the Auto data set  in ISLR R Package.  Thank you very much
> in advance.
>
> Please use the following code to see how is the data look like.
>
>
> install.packages("ISLR")
> library(ISLR)
> data(Auto)
> head(Auto)
>
>
> with many thanks
> abou
> ______________________
>
>
> *AbouEl-Makarim Aboueissa, PhD*
>
> *Professor of Statistics*
>
> *Department of Mathematics and Statistics*
> *University of Southern Maine*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Sun Dec 17 09:10:19 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Sun, 17 Dec 2017 10:10:19 +0200
Subject: [R] Auto Data in the ISLR Package
In-Reply-To: <CAGxFJbS20DZ9ynbQ6eabA1hj7JspkDvXDnwZqzmSMD5+HsZeCw@mail.gmail.com>
References: <CAE9stmcaTUQ5P-NLD1RU5it1W=8YW7DwmUdz+HYYigm+bGgSqw@mail.gmail.com>
 <CAGxFJbS20DZ9ynbQ6eabA1hj7JspkDvXDnwZqzmSMD5+HsZeCw@mail.gmail.com>
Message-ID: <CAGgJW770GJyh_d_TYR_0Kr7azreV-9BsVsROd74pBZECrO=LEg@mail.gmail.com>

myAuto <- Auto[ grep("ford|toyota",Auto$name),]



On Sat, Dec 16, 2017 at 10:28 PM, Bert Gunter <bgunter.4567 at gmail.com>
wrote:

> I did not care to load the packages -- small reproducible examples are
> preferable, as the posting guide suggests.
>
> But, if I have understood correctly:
>
> See, e.g. ?subset
>
> Alternatively, you can read up on indexing data frames in any good basic R
> tutorial.
>
> Cheers,
> Bert
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
> On Sat, Dec 16, 2017 at 11:44 AM, AbouEl-Makarim Aboueissa <
> abouelmakarim1962 at gmail.com> wrote:
>
> > Dear All:
> >
> > I would like to create a subset data set *with only* all Ford and all
> > Toyota cars from the Auto data set  in ISLR R Package.  Thank you very
> much
> > in advance.
> >
> > Please use the following code to see how is the data look like.
> >
> >
> > install.packages("ISLR")
> > library(ISLR)
> > data(Auto)
> > head(Auto)
> >
> >
> > with many thanks
> > abou
> > ______________________
> >
> >
> > *AbouEl-Makarim Aboueissa, PhD*
> >
> > *Professor of Statistics*
> >
> > *Department of Mathematics and Statistics*
> > *University of Southern Maine*
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> > posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Sun Dec 17 10:00:40 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Sun, 17 Dec 2017 10:00:40 +0100
Subject: [R] Auto Data in the ISLR Package
In-Reply-To: <CAGgJW770GJyh_d_TYR_0Kr7azreV-9BsVsROd74pBZECrO=LEg@mail.gmail.com>
References: <CAE9stmcaTUQ5P-NLD1RU5it1W=8YW7DwmUdz+HYYigm+bGgSqw@mail.gmail.com>
 <CAGxFJbS20DZ9ynbQ6eabA1hj7JspkDvXDnwZqzmSMD5+HsZeCw@mail.gmail.com>
 <CAGgJW770GJyh_d_TYR_0Kr7azreV-9BsVsROd74pBZECrO=LEg@mail.gmail.com>
Message-ID: <7F30A2EF-6A43-48EA-8F5B-BE66C1A08449@gmail.com>

That probably works in this case, but it would cause grief if another car make had "ford" somewhere inside its name e.g. "bedford". Safer general practice is 

Auto[Auto$name %in% c("ford", "toyota"),]

or similar using subset().

-pd

> On 17 Dec 2017, at 09:10 , Eric Berger <ericjberger at gmail.com> wrote:
> 
> myAuto <- Auto[ grep("ford|toyota",Auto$name),]
> 
> 
> 
> On Sat, Dec 16, 2017 at 10:28 PM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
> 
>> I did not care to load the packages -- small reproducible examples are
>> preferable, as the posting guide suggests.
>> 
>> But, if I have understood correctly:
>> 
>> See, e.g. ?subset
>> 
>> Alternatively, you can read up on indexing data frames in any good basic R
>> tutorial.
>> 
>> Cheers,
>> Bert
>> 
>> Bert Gunter
>> 
>> "The trouble with having an open mind is that people keep coming along and
>> sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>> 
>> On Sat, Dec 16, 2017 at 11:44 AM, AbouEl-Makarim Aboueissa <
>> abouelmakarim1962 at gmail.com> wrote:
>> 
>>> Dear All:
>>> 
>>> I would like to create a subset data set *with only* all Ford and all
>>> Toyota cars from the Auto data set  in ISLR R Package.  Thank you very
>> much
>>> in advance.
>>> 
>>> Please use the following code to see how is the data look like.
>>> 
>>> 
>>> install.packages("ISLR")
>>> library(ISLR)
>>> data(Auto)
>>> head(Auto)
>>> 
>>> 
>>> with many thanks
>>> abou
>>> ______________________
>>> 
>>> 
>>> *AbouEl-Makarim Aboueissa, PhD*
>>> 
>>> *Professor of Statistics*
>>> 
>>> *Department of Mathematics and Statistics*
>>> *University of Southern Maine*
>>> 
>>>        [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/
>>> posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>>        [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From ericjberger at gmail.com  Sun Dec 17 10:23:17 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Sun, 17 Dec 2017 11:23:17 +0200
Subject: [R] Auto Data in the ISLR Package
In-Reply-To: <7F30A2EF-6A43-48EA-8F5B-BE66C1A08449@gmail.com>
References: <CAE9stmcaTUQ5P-NLD1RU5it1W=8YW7DwmUdz+HYYigm+bGgSqw@mail.gmail.com>
 <CAGxFJbS20DZ9ynbQ6eabA1hj7JspkDvXDnwZqzmSMD5+HsZeCw@mail.gmail.com>
 <CAGgJW770GJyh_d_TYR_0Kr7azreV-9BsVsROd74pBZECrO=LEg@mail.gmail.com>
 <7F30A2EF-6A43-48EA-8F5B-BE66C1A08449@gmail.com>
Message-ID: <CAGgJW760Vy9z8ihn8r63x8=q5eJKsUSKd7aNsf6y_JDL2sHAmw@mail.gmail.com>

Hi Peter,
I looked at the Auto data frame and tested before I sent my reply. The
entries in the "name" column are actually models, such as

> head(Auto$name)
[1] chevrolet chevelle malibu buick skylark 320         plymouth satellite
      amc rebel sst
[5] ford torino               ford galaxie 500

What you are suggesting won't work. I agree with your "bedford" example as
a problem, but given the size of the result set in this case (~73 rows)
it's easy to eyeball the results and see if they're ok.

Regards,
Eric


On Sun, Dec 17, 2017 at 11:00 AM, peter dalgaard <pdalgd at gmail.com> wrote:

> That probably works in this case, but it would cause grief if another car
> make had "ford" somewhere inside its name e.g. "bedford". Safer general
> practice is
>
> Auto[Auto$name %in% c("ford", "toyota"),]
>
> or similar using subset().
>
> -pd
>
> > On 17 Dec 2017, at 09:10 , Eric Berger <ericjberger at gmail.com> wrote:
> >
> > myAuto <- Auto[ grep("ford|toyota",Auto$name),]
> >
> >
> >
> > On Sat, Dec 16, 2017 at 10:28 PM, Bert Gunter <bgunter.4567 at gmail.com>
> > wrote:
> >
> >> I did not care to load the packages -- small reproducible examples are
> >> preferable, as the posting guide suggests.
> >>
> >> But, if I have understood correctly:
> >>
> >> See, e.g. ?subset
> >>
> >> Alternatively, you can read up on indexing data frames in any good
> basic R
> >> tutorial.
> >>
> >> Cheers,
> >> Bert
> >>
> >> Bert Gunter
> >>
> >> "The trouble with having an open mind is that people keep coming along
> and
> >> sticking things into it."
> >> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >>
> >> On Sat, Dec 16, 2017 at 11:44 AM, AbouEl-Makarim Aboueissa <
> >> abouelmakarim1962 at gmail.com> wrote:
> >>
> >>> Dear All:
> >>>
> >>> I would like to create a subset data set *with only* all Ford and all
> >>> Toyota cars from the Auto data set  in ISLR R Package.  Thank you very
> >> much
> >>> in advance.
> >>>
> >>> Please use the following code to see how is the data look like.
> >>>
> >>>
> >>> install.packages("ISLR")
> >>> library(ISLR)
> >>> data(Auto)
> >>> head(Auto)
> >>>
> >>>
> >>> with many thanks
> >>> abou
> >>> ______________________
> >>>
> >>>
> >>> *AbouEl-Makarim Aboueissa, PhD*
> >>>
> >>> *Professor of Statistics*
> >>>
> >>> *Department of Mathematics and Statistics*
> >>> *University of Southern Maine*
> >>>
> >>>        [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/
> >>> posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >>        [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/
> >> posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From abouelmakarim1962 at gmail.com  Sun Dec 17 10:58:55 2017
From: abouelmakarim1962 at gmail.com (AbouEl-Makarim Aboueissa)
Date: Sun, 17 Dec 2017 04:58:55 -0500
Subject: [R] Auto Data in the ISLR Package
In-Reply-To: <CAGgJW770GJyh_d_TYR_0Kr7azreV-9BsVsROd74pBZECrO=LEg@mail.gmail.com>
References: <CAE9stmcaTUQ5P-NLD1RU5it1W=8YW7DwmUdz+HYYigm+bGgSqw@mail.gmail.com>
 <CAGxFJbS20DZ9ynbQ6eabA1hj7JspkDvXDnwZqzmSMD5+HsZeCw@mail.gmail.com>
 <CAGgJW770GJyh_d_TYR_0Kr7azreV-9BsVsROd74pBZECrO=LEg@mail.gmail.com>
Message-ID: <CAE9stmcSHt9-JGa-idz6aE2Jos03MX2RT_AHgmnD5pZ-gxB5Mg@mail.gmail.com>

Dear Eric:

Thank you very much. It works nicely.

*Just one more thing;* how to create a new variable (say, *Make*) with *Make
= Ford* for the ford brand and *Make = T**oyota* for the toyota brand.

Once again thank you all.

abou

______________________


*AbouEl-Makarim Aboueissa, PhD*

*Professor of Statistics*

*Department of Mathematics and Statistics*
*University of Southern Maine*


On Sun, Dec 17, 2017 at 3:10 AM, Eric Berger <ericjberger at gmail.com> wrote:

> myAuto <- Auto[ grep("ford|toyota",Auto$name),]
>
>
>
> On Sat, Dec 16, 2017 at 10:28 PM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
>
>> I did not care to load the packages -- small reproducible examples are
>> preferable, as the posting guide suggests.
>>
>> But, if I have understood correctly:
>>
>> See, e.g. ?subset
>>
>> Alternatively, you can read up on indexing data frames in any good basic R
>> tutorial.
>>
>> Cheers,
>> Bert
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along and
>> sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>> On Sat, Dec 16, 2017 at 11:44 AM, AbouEl-Makarim Aboueissa <
>> abouelmakarim1962 at gmail.com> wrote:
>>
>> > Dear All:
>> >
>> > I would like to create a subset data set *with only* all Ford and all
>> > Toyota cars from the Auto data set  in ISLR R Package.  Thank you very
>> much
>> > in advance.
>> >
>> > Please use the following code to see how is the data look like.
>> >
>> >
>> > install.packages("ISLR")
>> > library(ISLR)
>> > data(Auto)
>> > head(Auto)
>> >
>> >
>> > with many thanks
>> > abou
>> > ______________________
>> >
>> >
>> > *AbouEl-Makarim Aboueissa, PhD*
>> >
>> > *Professor of Statistics*
>> >
>> > *Department of Mathematics and Statistics*
>> > *University of Southern Maine*
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/
>> > posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Sun Dec 17 11:25:42 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Sun, 17 Dec 2017 12:25:42 +0200
Subject: [R] Auto Data in the ISLR Package
In-Reply-To: <CAE9stmcSHt9-JGa-idz6aE2Jos03MX2RT_AHgmnD5pZ-gxB5Mg@mail.gmail.com>
References: <CAE9stmcaTUQ5P-NLD1RU5it1W=8YW7DwmUdz+HYYigm+bGgSqw@mail.gmail.com>
 <CAGxFJbS20DZ9ynbQ6eabA1hj7JspkDvXDnwZqzmSMD5+HsZeCw@mail.gmail.com>
 <CAGgJW770GJyh_d_TYR_0Kr7azreV-9BsVsROd74pBZECrO=LEg@mail.gmail.com>
 <CAE9stmcSHt9-JGa-idz6aE2Jos03MX2RT_AHgmnD5pZ-gxB5Mg@mail.gmail.com>
Message-ID: <CAGgJW75vB6UrNVfyx6pQb9guwQZNxaV63=q-EOjinw_KUG95AQ@mail.gmail.com>

myAuto <- Auto[ grep("ford|toyota",Auto$name),]
myAuto$Make <- NA
myAuto$Make[grep("ford",myAuto$name)] <- "Ford"
myAuto$Make[grep("toyota",myAuto$name)] <- "Toyota"

Regards,
Eric


On Sun, Dec 17, 2017 at 11:58 AM, AbouEl-Makarim Aboueissa <
abouelmakarim1962 at gmail.com> wrote:

> Dear Eric:
>
> Thank you very much. It works nicely.
>
> *Just one more thing;* how to create a new variable (say, *Make*) with *Make
> = Ford* for the ford brand and *Make = T**oyota* for the toyota brand.
>
> Once again thank you all.
>
> abou
>
> ______________________
>
>
> *AbouEl-Makarim Aboueissa, PhD*
>
> *Professor of Statistics*
>
> *Department of Mathematics and Statistics*
> *University of Southern Maine*
>
>
> On Sun, Dec 17, 2017 at 3:10 AM, Eric Berger <ericjberger at gmail.com>
> wrote:
>
>> myAuto <- Auto[ grep("ford|toyota",Auto$name),]
>>
>>
>>
>> On Sat, Dec 16, 2017 at 10:28 PM, Bert Gunter <bgunter.4567 at gmail.com>
>> wrote:
>>
>>> I did not care to load the packages -- small reproducible examples are
>>> preferable, as the posting guide suggests.
>>>
>>> But, if I have understood correctly:
>>>
>>> See, e.g. ?subset
>>>
>>> Alternatively, you can read up on indexing data frames in any good basic
>>> R
>>> tutorial.
>>>
>>> Cheers,
>>> Bert
>>>
>>> Bert Gunter
>>>
>>> "The trouble with having an open mind is that people keep coming along
>>> and
>>> sticking things into it."
>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>>
>>> On Sat, Dec 16, 2017 at 11:44 AM, AbouEl-Makarim Aboueissa <
>>> abouelmakarim1962 at gmail.com> wrote:
>>>
>>> > Dear All:
>>> >
>>> > I would like to create a subset data set *with only* all Ford and all
>>> > Toyota cars from the Auto data set  in ISLR R Package.  Thank you very
>>> much
>>> > in advance.
>>> >
>>> > Please use the following code to see how is the data look like.
>>> >
>>> >
>>> > install.packages("ISLR")
>>> > library(ISLR)
>>> > data(Auto)
>>> > head(Auto)
>>> >
>>> >
>>> > with many thanks
>>> > abou
>>> > ______________________
>>> >
>>> >
>>> > *AbouEl-Makarim Aboueissa, PhD*
>>> >
>>> > *Professor of Statistics*
>>> >
>>> > *Department of Mathematics and Statistics*
>>> > *University of Southern Maine*
>>> >
>>> >         [[alternative HTML version deleted]]
>>> >
>>> > ______________________________________________
>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>> > PLEASE do read the posting guide http://www.R-project.org/
>>> > posting-guide.html
>>> > and provide commented, minimal, self-contained, reproducible code.
>>> >
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>> ng-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From abouelmakarim1962 at gmail.com  Sun Dec 17 13:12:30 2017
From: abouelmakarim1962 at gmail.com (AbouEl-Makarim Aboueissa)
Date: Sun, 17 Dec 2017 07:12:30 -0500
Subject: [R] Auto Data in the ISLR Package
In-Reply-To: <CAGgJW75vB6UrNVfyx6pQb9guwQZNxaV63=q-EOjinw_KUG95AQ@mail.gmail.com>
References: <CAE9stmcaTUQ5P-NLD1RU5it1W=8YW7DwmUdz+HYYigm+bGgSqw@mail.gmail.com>
 <CAGxFJbS20DZ9ynbQ6eabA1hj7JspkDvXDnwZqzmSMD5+HsZeCw@mail.gmail.com>
 <CAGgJW770GJyh_d_TYR_0Kr7azreV-9BsVsROd74pBZECrO=LEg@mail.gmail.com>
 <CAE9stmcSHt9-JGa-idz6aE2Jos03MX2RT_AHgmnD5pZ-gxB5Mg@mail.gmail.com>
 <CAGgJW75vB6UrNVfyx6pQb9guwQZNxaV63=q-EOjinw_KUG95AQ@mail.gmail.com>
Message-ID: <CAE9stmdpbcJ9CrJEp3An01p+N4fT3GbW5kxfVh_bf-PZ-Qdu8Q@mail.gmail.com>

Dear Eric:

Many Thanks

abou

______________________


*AbouEl-Makarim Aboueissa, PhD*

*Professor of Statistics*

*Department of Mathematics and Statistics*
*University of Southern Maine*


On Sun, Dec 17, 2017 at 5:25 AM, Eric Berger <ericjberger at gmail.com> wrote:

> myAuto <- Auto[ grep("ford|toyota",Auto$name),]
> myAuto$Make <- NA
> myAuto$Make[grep("ford",myAuto$name)] <- "Ford"
> myAuto$Make[grep("toyota",myAuto$name)] <- "Toyota"
>
> Regards,
> Eric
>
>
> On Sun, Dec 17, 2017 at 11:58 AM, AbouEl-Makarim Aboueissa <
> abouelmakarim1962 at gmail.com> wrote:
>
>> Dear Eric:
>>
>> Thank you very much. It works nicely.
>>
>> *Just one more thing;* how to create a new variable (say, *Make*) with *Make
>> = Ford* for the ford brand and *Make = T**oyota* for the toyota brand.
>>
>> Once again thank you all.
>>
>> abou
>>
>> ______________________
>>
>>
>> *AbouEl-Makarim Aboueissa, PhD*
>>
>> *Professor of Statistics*
>>
>> *Department of Mathematics and Statistics*
>> *University of Southern Maine*
>>
>>
>> On Sun, Dec 17, 2017 at 3:10 AM, Eric Berger <ericjberger at gmail.com>
>> wrote:
>>
>>> myAuto <- Auto[ grep("ford|toyota",Auto$name),]
>>>
>>>
>>>
>>> On Sat, Dec 16, 2017 at 10:28 PM, Bert Gunter <bgunter.4567 at gmail.com>
>>> wrote:
>>>
>>>> I did not care to load the packages -- small reproducible examples are
>>>> preferable, as the posting guide suggests.
>>>>
>>>> But, if I have understood correctly:
>>>>
>>>> See, e.g. ?subset
>>>>
>>>> Alternatively, you can read up on indexing data frames in any good
>>>> basic R
>>>> tutorial.
>>>>
>>>> Cheers,
>>>> Bert
>>>>
>>>> Bert Gunter
>>>>
>>>> "The trouble with having an open mind is that people keep coming along
>>>> and
>>>> sticking things into it."
>>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>>>
>>>> On Sat, Dec 16, 2017 at 11:44 AM, AbouEl-Makarim Aboueissa <
>>>> abouelmakarim1962 at gmail.com> wrote:
>>>>
>>>> > Dear All:
>>>> >
>>>> > I would like to create a subset data set *with only* all Ford and all
>>>> > Toyota cars from the Auto data set  in ISLR R Package.  Thank you
>>>> very much
>>>> > in advance.
>>>> >
>>>> > Please use the following code to see how is the data look like.
>>>> >
>>>> >
>>>> > install.packages("ISLR")
>>>> > library(ISLR)
>>>> > data(Auto)
>>>> > head(Auto)
>>>> >
>>>> >
>>>> > with many thanks
>>>> > abou
>>>> > ______________________
>>>> >
>>>> >
>>>> > *AbouEl-Makarim Aboueissa, PhD*
>>>> >
>>>> > *Professor of Statistics*
>>>> >
>>>> > *Department of Mathematics and Statistics*
>>>> > *University of Southern Maine*
>>>> >
>>>> >         [[alternative HTML version deleted]]
>>>> >
>>>> > ______________________________________________
>>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>>> > PLEASE do read the posting guide http://www.R-project.org/
>>>> > posting-guide.html
>>>> > and provide commented, minimal, self-contained, reproducible code.
>>>> >
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>>> ng-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>>
>>
>

	[[alternative HTML version deleted]]


From profjcnash at gmail.com  Sun Dec 17 15:08:13 2017
From: profjcnash at gmail.com (J C Nash)
Date: Sun, 17 Dec 2017 09:08:13 -0500
Subject: [R] fortune candidate
Message-ID: <97e56103-384d-0382-0a24-b0e9658a6f1d@gmail.com>

>From Dirk E. on the R-devel list

> Many things a developer / power-user would do are very difficult on
> Windows. It is one of the charms of the platform. On the other hand you do
> get a few solitaire games so I guess everybody is happy.


JN


From Achim.Zeileis at uibk.ac.at  Sun Dec 17 21:56:07 2017
From: Achim.Zeileis at uibk.ac.at (Achim Zeileis)
Date: Sun, 17 Dec 2017 21:56:07 +0100 (CET)
Subject: [R] fortune candidate
In-Reply-To: <97e56103-384d-0382-0a24-b0e9658a6f1d@gmail.com>
References: <97e56103-384d-0382-0a24-b0e9658a6f1d@gmail.com>
Message-ID: <alpine.DEB.2.21.1712172155440.15857@paninaro>

On Sun, 17 Dec 2017, J C Nash wrote:

> From Dirk E. on the R-devel list
>
>> Many things a developer / power-user would do are very difficult on
>> Windows. It is one of the charms of the platform. On the other hand you do
>> get a few solitaire games so I guess everybody is happy.

Thanks, added to the devel version of "fortunes" on R-Forge.
Z

>
> JN
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From emorway at usgs.gov  Mon Dec 18 15:47:29 2017
From: emorway at usgs.gov (Morway, Eric)
Date: Mon, 18 Dec 2017 06:47:29 -0800
Subject: [R] Finding center of mass in a hydrologic time series
In-Reply-To: <CAGgJW77VGwaY3CbkPe0-eNXoAH+Jd52vMjNzf6oKTdUw+T6bzQ@mail.gmail.com>
References: <CAPoqHzqmjtAJ1cv+c_orE-i_A__XPf_iriT15O+vUjd95E1n_Q@mail.gmail.com>
 <CAGgJW77VGwaY3CbkPe0-eNXoAH+Jd52vMjNzf6oKTdUw+T6bzQ@mail.gmail.com>
Message-ID: <CAPoqHzprc1fwFPxb=7kgeEDC2_UWaR8h9TJjNZcvu9YYBmuZnw@mail.gmail.com>

Eric B's response provided just the kind of quick & simple solution I was
hoping for (appears as the function com below).  However, I once again
failed to take advantage of the power of R and have reverted back to using
a for loop for the next step of the processing.  The example below (which
requires the library EGRET for pulling an example dataset) works, but
probably can be replaced with some version of the apply functionality?  So
far, I've been unable to figure out how to enter the arguments to the apply
function.  The idea is this: for each unique water year (variable 'wyrs' in
example below) in a 27 year continuous time series of daily values, find
the date of the 'center of mass', and build a vector of those dates.
Thanks, -Eric M

library(EGRET)

StartDate <- "1990-10-01"
EndDate <- "2017-09-30"
siteNumber <- "10310000"
QParameterCd <- "00060"

Daily <- readNWISDaily(siteNumber, QParameterCd, StartDate, EndDate)

# Define 'center of mass' function
com <- function(x) {
    match(TRUE, cumsum(x/sum(x)) > 0.5) - 1
}


wyrs <- unique(Daily$waterYear)
for(i in (1:length(wyrs))){
    OneYr <- Daily[Daily$waterYear==wyrs[i], ]
    mid <- com(OneYr$Q)
    if(i==1){
        midPts <- as.Date(OneYr$Date[mid])
    } else {
        midPts <- c(midPts, as.Date(OneYr$Date[mid]))
    }
}



Eric Morway
Research Hydrologist
Nevada Water Science Center
U.S. Geological Survey
2730 N. Deer Run Rd.
Carson City, NV 89701
(775) 887-7668
*orcid*:  0000-0002-8553-6140 <http://orcid.org/0000-0002-8553-6140>



On Sat, Dec 16, 2017 at 5:32 AM, Eric Berger <ericjberger at gmail.com> wrote:

> Hi Eric,
> How about
>
> match( TRUE, cumsum(hyd/sum(hyd)) > .5 ) - 1
>
> HTH,
> Eric
>
>
> On Sat, Dec 16, 2017 at 3:18 PM, Morway, Eric <emorway at usgs.gov> wrote:
>
>> The small bit of script below is an example of what I'm attempting to do -
>> find the day on which the 'center of mass' occurs.  In case that is the
>> wrong term, I'd like to know the day that essentially cuts the area under
>> the curve in to two equal parts:
>>
>> set.seed(4004)
>> Date <- seq(as.Date('2000-09-01'), as.Date('2000-09-30'), by='day')
>> hyd <- ((100*(sin(seq(0.5,4.5,length.out=30))+10) +
>> seq(45,1,length.out=30)) + rnorm(30)*8) - 800
>>
>> # View the example curve
>> plot(Date, hyd, las=1)
>>
>> # By trial-and-error, the day on which the center of mass occurs is the
>> 11th day:
>> # Add up the area under the curve for the first 11 days and compare
>> # with the last 19 days:
>>
>> sum(hyd[1:11])
>> # 3546.364
>> sum(hyd[12:30])
>> # 3947.553
>>
>> # Add up the area under the curve for the first 12 days and compare
>> # with the last 18 days:
>>
>> sum(hyd[1:12])
>> # 3875.753
>> sum(hyd[13:30])
>> # 3618.164
>>
>> By day 12, the halfway point has already been passed, so the answer that
>> would be returned would be:
>>
>> Date[11]
>> # "2000-09-11"
>>
>> For the larger problem, it'd be handy if the proposed function could
>> process a multi-year time series (a runoff hydrograph) and return the day
>> of the center of mass for each year in the time series.
>>
>> I appreciate any pointers...Eric
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From hatemzaharna at yahoo.com  Mon Dec 18 12:21:24 2017
From: hatemzaharna at yahoo.com (HATMZAHARNA)
Date: Mon, 18 Dec 2017 13:21:24 +0200
Subject: [R] chi-square distribution table
Message-ID: <c5d216f6-989d-6436-a587-6fe22bb54582@yahoo.com>

Please could you tell me how to make code to outpot chi-square 
distribution table?

Please help


From hatemzaharna at yahoo.com  Mon Dec 18 12:28:04 2017
From: hatemzaharna at yahoo.com (HATMZAHARNA)
Date: Mon, 18 Dec 2017 13:28:04 +0200
Subject: [R] chi-square distribution table
Message-ID: <ed4bff51-273c-de01-d6b1-72eeed412978@yahoo.com>

Please could you tell me how to make code to outpot chi-square 
distribution table?

Please help


From hatemzaharna at yahoo.com  Mon Dec 18 15:41:22 2017
From: hatemzaharna at yahoo.com (HATMZAHARNA)
Date: Mon, 18 Dec 2017 16:41:22 +0200
Subject: [R] chi-square distribution table
Message-ID: <570cfba8-e6cd-7fe9-20fc-d8335d15a99b@yahoo.com>

Please could you tell me how to make code to make chi-square 
distribution table?

Please help


From ericjberger at gmail.com  Mon Dec 18 16:42:38 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Mon, 18 Dec 2017 17:42:38 +0200
Subject: [R] Finding center of mass in a hydrologic time series
In-Reply-To: <CAPoqHzprc1fwFPxb=7kgeEDC2_UWaR8h9TJjNZcvu9YYBmuZnw@mail.gmail.com>
References: <CAPoqHzqmjtAJ1cv+c_orE-i_A__XPf_iriT15O+vUjd95E1n_Q@mail.gmail.com>
 <CAGgJW77VGwaY3CbkPe0-eNXoAH+Jd52vMjNzf6oKTdUw+T6bzQ@mail.gmail.com>
 <CAPoqHzprc1fwFPxb=7kgeEDC2_UWaR8h9TJjNZcvu9YYBmuZnw@mail.gmail.com>
Message-ID: <CAGgJW75J4a5vAmmSb54U9SouQTUAUqG15DX4R-KYndSLBzaNHw@mail.gmail.com>

Hi Eric,
the following works for me.

HTH,
Eric

library(EGRET)

StartDate <- "1990-10-01"
EndDate <- "2017-09-30"
siteNumber <- "10310000"
QParameterCd <- "00060"

Daily <- readNWISDaily(siteNumber, QParameterCd, StartDate, EndDate)

# Define 'center of mass' function
com <- function(x) {
  match(TRUE, cumsum(x/sum(x)) > 0.5) - 1
}
wyrs <- unique(Daily$waterYear)
x <- as.Date(sapply( wyrs, function(yr) { Df <-
Daily[Daily$waterYear==yr,];  Df$Date[com(Df$Q)] } ), "1970-01-01")



On Mon, Dec 18, 2017 at 4:47 PM, Morway, Eric <emorway at usgs.gov> wrote:

> Eric B's response provided just the kind of quick & simple solution I was
> hoping for (appears as the function com below).  However, I once again
> failed to take advantage of the power of R and have reverted back to using
> a for loop for the next step of the processing.  The example below (which
> requires the library EGRET for pulling an example dataset) works, but
> probably can be replaced with some version of the apply functionality?  So
> far, I've been unable to figure out how to enter the arguments to the apply
> function.  The idea is this: for each unique water year (variable 'wyrs' in
> example below) in a 27 year continuous time series of daily values, find
> the date of the 'center of mass', and build a vector of those dates.
> Thanks, -Eric M
>
> library(EGRET)
>
> StartDate <- "1990-10-01"
> EndDate <- "2017-09-30"
> siteNumber <- "10310000"
> QParameterCd <- "00060"
>
> Daily <- readNWISDaily(siteNumber, QParameterCd, StartDate, EndDate)
>
> # Define 'center of mass' function
> com <- function(x) {
>     match(TRUE, cumsum(x/sum(x)) > 0.5) - 1
> }
>
>
> wyrs <- unique(Daily$waterYear)
> for(i in (1:length(wyrs))){
>     OneYr <- Daily[Daily$waterYear==wyrs[i], ]
>     mid <- com(OneYr$Q)
>     if(i==1){
>         midPts <- as.Date(OneYr$Date[mid])
>     } else {
>         midPts <- c(midPts, as.Date(OneYr$Date[mid]))
>     }
> }
>
>
>
> Eric Morway
> Research Hydrologist
> Nevada Water Science Center
> U.S. Geological Survey
> 2730 N. Deer Run Rd.
> <https://maps.google.com/?q=2730+N.+Deer+Run+Rd.Carson+City,+NV+89701+(775&entry=gmail&source=g>
> Carson City, NV 89701
> <https://maps.google.com/?q=2730+N.+Deer+Run+Rd.Carson+City,+NV+89701+(775&entry=gmail&source=g>
> (775
> <https://maps.google.com/?q=2730+N.+Deer+Run+Rd.Carson+City,+NV+89701+(775&entry=gmail&source=g>)
> 887-7668
> *orcid*:  0000-0002-8553-6140 <http://orcid.org/0000-0002-8553-6140>
>
>
>
> On Sat, Dec 16, 2017 at 5:32 AM, Eric Berger <ericjberger at gmail.com>
> wrote:
>
>> Hi Eric,
>> How about
>>
>> match( TRUE, cumsum(hyd/sum(hyd)) > .5 ) - 1
>>
>> HTH,
>> Eric
>>
>>
>> On Sat, Dec 16, 2017 at 3:18 PM, Morway, Eric <emorway at usgs.gov> wrote:
>>
>>> The small bit of script below is an example of what I'm attempting to do
>>> -
>>> find the day on which the 'center of mass' occurs.  In case that is the
>>> wrong term, I'd like to know the day that essentially cuts the area under
>>> the curve in to two equal parts:
>>>
>>> set.seed(4004)
>>> Date <- seq(as.Date('2000-09-01'), as.Date('2000-09-30'), by='day')
>>> hyd <- ((100*(sin(seq(0.5,4.5,length.out=30))+10) +
>>> seq(45,1,length.out=30)) + rnorm(30)*8) - 800
>>>
>>> # View the example curve
>>> plot(Date, hyd, las=1)
>>>
>>> # By trial-and-error, the day on which the center of mass occurs is the
>>> 11th day:
>>> # Add up the area under the curve for the first 11 days and compare
>>> # with the last 19 days:
>>>
>>> sum(hyd[1:11])
>>> # 3546.364
>>> sum(hyd[12:30])
>>> # 3947.553
>>>
>>> # Add up the area under the curve for the first 12 days and compare
>>> # with the last 18 days:
>>>
>>> sum(hyd[1:12])
>>> # 3875.753
>>> sum(hyd[13:30])
>>> # 3618.164
>>>
>>> By day 12, the halfway point has already been passed, so the answer that
>>> would be returned would be:
>>>
>>> Date[11]
>>> # "2000-09-11"
>>>
>>> For the larger problem, it'd be handy if the proposed function could
>>> process a multi-year time series (a runoff hydrograph) and return the day
>>> of the center of mass for each year in the time series.
>>>
>>> I appreciate any pointers...Eric
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>> ng-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Mon Dec 18 17:07:14 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 18 Dec 2017 08:07:14 -0800
Subject: [R] Finding center of mass in a hydrologic time series
In-Reply-To: <CAGgJW75J4a5vAmmSb54U9SouQTUAUqG15DX4R-KYndSLBzaNHw@mail.gmail.com>
References: <CAPoqHzqmjtAJ1cv+c_orE-i_A__XPf_iriT15O+vUjd95E1n_Q@mail.gmail.com>
 <CAGgJW77VGwaY3CbkPe0-eNXoAH+Jd52vMjNzf6oKTdUw+T6bzQ@mail.gmail.com>
 <CAPoqHzprc1fwFPxb=7kgeEDC2_UWaR8h9TJjNZcvu9YYBmuZnw@mail.gmail.com>
 <CAGgJW75J4a5vAmmSb54U9SouQTUAUqG15DX4R-KYndSLBzaNHw@mail.gmail.com>
Message-ID: <CAGxFJbQM8vofXDHCyYuGUi72YNHtnj9CQqxt37sJvNxfrMuG_w@mail.gmail.com>

... and for the record:

The apply() family of functions -- here sapply() -- are *not* vectorized.
This means that one should not expect them to be necessarily more efficient
than expicit for() loops. Their advantage for many is clarity of code.

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Mon, Dec 18, 2017 at 7:42 AM, Eric Berger <ericjberger at gmail.com> wrote:

> Hi Eric,
> the following works for me.
>
> HTH,
> Eric
>
> library(EGRET)
>
> StartDate <- "1990-10-01"
> EndDate <- "2017-09-30"
> siteNumber <- "10310000"
> QParameterCd <- "00060"
>
> Daily <- readNWISDaily(siteNumber, QParameterCd, StartDate, EndDate)
>
> # Define 'center of mass' function
> com <- function(x) {
>   match(TRUE, cumsum(x/sum(x)) > 0.5) - 1
> }
> wyrs <- unique(Daily$waterYear)
> x <- as.Date(sapply( wyrs, function(yr) { Df <-
> Daily[Daily$waterYear==yr,];  Df$Date[com(Df$Q)] } ), "1970-01-01")
>
>
>
> On Mon, Dec 18, 2017 at 4:47 PM, Morway, Eric <emorway at usgs.gov> wrote:
>
> > Eric B's response provided just the kind of quick & simple solution I was
> > hoping for (appears as the function com below).  However, I once again
> > failed to take advantage of the power of R and have reverted back to
> using
> > a for loop for the next step of the processing.  The example below (which
> > requires the library EGRET for pulling an example dataset) works, but
> > probably can be replaced with some version of the apply functionality?
> So
> > far, I've been unable to figure out how to enter the arguments to the
> apply
> > function.  The idea is this: for each unique water year (variable 'wyrs'
> in
> > example below) in a 27 year continuous time series of daily values, find
> > the date of the 'center of mass', and build a vector of those dates.
> > Thanks, -Eric M
> >
> > library(EGRET)
> >
> > StartDate <- "1990-10-01"
> > EndDate <- "2017-09-30"
> > siteNumber <- "10310000"
> > QParameterCd <- "00060"
> >
> > Daily <- readNWISDaily(siteNumber, QParameterCd, StartDate, EndDate)
> >
> > # Define 'center of mass' function
> > com <- function(x) {
> >     match(TRUE, cumsum(x/sum(x)) > 0.5) - 1
> > }
> >
> >
> > wyrs <- unique(Daily$waterYear)
> > for(i in (1:length(wyrs))){
> >     OneYr <- Daily[Daily$waterYear==wyrs[i], ]
> >     mid <- com(OneYr$Q)
> >     if(i==1){
> >         midPts <- as.Date(OneYr$Date[mid])
> >     } else {
> >         midPts <- c(midPts, as.Date(OneYr$Date[mid]))
> >     }
> > }
> >
> >
> >
> > Eric Morway
> > Research Hydrologist
> > Nevada Water Science Center
> > U.S. Geological Survey
> > 2730 N. Deer Run Rd.
> > <https://maps.google.com/?q=2730+N.+Deer+Run+Rd.Carson+
> City,+NV+89701+(775&entry=gmail&source=g>
> > Carson City, NV 89701
> > <https://maps.google.com/?q=2730+N.+Deer+Run+Rd.Carson+
> City,+NV+89701+(775&entry=gmail&source=g>
> > (775
> > <https://maps.google.com/?q=2730+N.+Deer+Run+Rd.Carson+
> City,+NV+89701+(775&entry=gmail&source=g>)
> > 887-7668
> > *orcid*:  0000-0002-8553-6140 <http://orcid.org/0000-0002-8553-6140>
> >
> >
> >
> > On Sat, Dec 16, 2017 at 5:32 AM, Eric Berger <ericjberger at gmail.com>
> > wrote:
> >
> >> Hi Eric,
> >> How about
> >>
> >> match( TRUE, cumsum(hyd/sum(hyd)) > .5 ) - 1
> >>
> >> HTH,
> >> Eric
> >>
> >>
> >> On Sat, Dec 16, 2017 at 3:18 PM, Morway, Eric <emorway at usgs.gov> wrote:
> >>
> >>> The small bit of script below is an example of what I'm attempting to
> do
> >>> -
> >>> find the day on which the 'center of mass' occurs.  In case that is the
> >>> wrong term, I'd like to know the day that essentially cuts the area
> under
> >>> the curve in to two equal parts:
> >>>
> >>> set.seed(4004)
> >>> Date <- seq(as.Date('2000-09-01'), as.Date('2000-09-30'), by='day')
> >>> hyd <- ((100*(sin(seq(0.5,4.5,length.out=30))+10) +
> >>> seq(45,1,length.out=30)) + rnorm(30)*8) - 800
> >>>
> >>> # View the example curve
> >>> plot(Date, hyd, las=1)
> >>>
> >>> # By trial-and-error, the day on which the center of mass occurs is the
> >>> 11th day:
> >>> # Add up the area under the curve for the first 11 days and compare
> >>> # with the last 19 days:
> >>>
> >>> sum(hyd[1:11])
> >>> # 3546.364
> >>> sum(hyd[12:30])
> >>> # 3947.553
> >>>
> >>> # Add up the area under the curve for the first 12 days and compare
> >>> # with the last 18 days:
> >>>
> >>> sum(hyd[1:12])
> >>> # 3875.753
> >>> sum(hyd[13:30])
> >>> # 3618.164
> >>>
> >>> By day 12, the halfway point has already been passed, so the answer
> that
> >>> would be returned would be:
> >>>
> >>> Date[11]
> >>> # "2000-09-11"
> >>>
> >>> For the larger problem, it'd be handy if the proposed function could
> >>> process a multi-year time series (a runoff hydrograph) and return the
> day
> >>> of the center of mass for each year in the time series.
> >>>
> >>> I appreciate any pointers...Eric
> >>>
> >>>         [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posti
> >>> ng-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >>
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Mon Dec 18 18:12:46 2017
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Mon, 18 Dec 2017 17:12:46 +0000
Subject: [R] chi-square distribution table
In-Reply-To: <570cfba8-e6cd-7fe9-20fc-d8335d15a99b@yahoo.com>
References: <570cfba8-e6cd-7fe9-20fc-d8335d15a99b@yahoo.com>
Message-ID: <be767762-fc23-e72b-c848-ddc761ad99ad@dewey.myzen.co.uk>

You might get better answers if you show us what you attempted and why 
it did not do what you had wished for.

On 18/12/2017 14:41, HATMZAHARNA via R-help wrote:
> Please could you tell me how to make code to make chi-square 
> distribution table?
> 
> Please help
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From axbergtimothy at gmail.com  Tue Dec 19 04:22:14 2017
From: axbergtimothy at gmail.com (Timothy Axberg)
Date: Mon, 18 Dec 2017 21:22:14 -0600
Subject: [R] Partial differential equation
Message-ID: <CAGFL1OgqLrbgFH91+478ks_uRUSiF3rbU-S4POZgke0TfxnjKQ@mail.gmail.com>

Hello, I am having troubles with heat conduction problem. Below is the
given information. Should I move forward with this problem like any other
1-D PDE?

?T/?t = a* (?^2T/?x^2)



I.C. For t = 0 and 0 ? x ? 10, T = 0 ?C

B.C. For x = 0 cm and all t , T = 100?C

For x = 10 cm and all t , T = 0 ?C

The thermal diffusivity is a = 2.0 cm^2 /s.


I also added what I have for code in R. Any help will be appreciated. Thanks

- Timothy

From marc_grt at yahoo.fr  Tue Dec 19 10:39:58 2017
From: marc_grt at yahoo.fr (Marc Girondot)
Date: Tue, 19 Dec 2017 10:39:58 +0100
Subject: [R] chi-square distribution table
In-Reply-To: <570cfba8-e6cd-7fe9-20fc-d8335d15a99b@yahoo.com>
References: <570cfba8-e6cd-7fe9-20fc-d8335d15a99b@yahoo.com>
Message-ID: <1f73964e-4c3e-eb44-305a-70ca0a819af8@yahoo.fr>

Le 18/12/2017 ? 15:41, HATMZAHARNA via R-help a ?crit?:
> Please could you tell me how to make code to make chi-square 
> distribution table?
>
> Please help
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
To help you to begin to do a code that we could enhance, here are some 
functions that will be usefull for you:

 > qchisq(p=1-0.05, df=1)
[1] 3.841459

Then, using a range of df, from example 1 to 10:

 > qchisq(p=1-0.05, df=1:10)
 ?[1]? 3.841459? 5.991465? 7.814728? 9.487729 11.070498 12.591587 
14.067140 15.507313 16.918978 18.307038

To help to present a nice table:

 > X <- qchisq(p=1-0.05, df=1)
 > format(X, digits = 4+log10(X), width = 10)
[1] "???? 3.841"

Or use the xtable package

Sincerely

Marc


From thpe at simecol.de  Tue Dec 19 22:06:17 2017
From: thpe at simecol.de (Thomas Petzoldt)
Date: Tue, 19 Dec 2017 22:06:17 +0100
Subject: [R] Partial differential equation
In-Reply-To: <CAGFL1OgqLrbgFH91+478ks_uRUSiF3rbU-S4POZgke0TfxnjKQ@mail.gmail.com>
References: <CAGFL1OgqLrbgFH91+478ks_uRUSiF3rbU-S4POZgke0TfxnjKQ@mail.gmail.com>
Message-ID: <ebdffd25-f728-2ae6-c2c4-58a06ac14e99@simecol.de>

On 19.12.2017 04:22, Timothy Axberg wrote:
> Hello, I am having troubles with heat conduction problem. Below is the
> given information. Should I move forward with this problem like any other
> 1-D PDE?

Yes. You may use your favorite search engine and search for:

"heat transfer" "deSolve" R

or have a look at:

http://desolve.r-forge.r-project.org

> 
> ?T/?t = a* (?^2T/?x^2)
> 
> 
> 
> I.C. For t = 0 and 0 ? x ? 10, T = 0 ?C
> 
> B.C. For x = 0 cm and all t , T = 100?C
> 
> For x = 10 cm and all t , T = 0 ?C
> 
> The thermal diffusivity is a = 2.0 cm^2 /s.
> 
> 
> I also added what I have for code in R. Any help will be appreciated. Thanks

There was no code appended.

> - Timothy

Hope it helps,

Thomas


From axbergtimothy at gmail.com  Tue Dec 19 23:13:03 2017
From: axbergtimothy at gmail.com (Timothy Axberg)
Date: Tue, 19 Dec 2017 16:13:03 -0600
Subject: [R] Nonlinear regression
Message-ID: <CAGFL1Og+qcfeZ8krXYWiMBAM9L3BbNDFPbQH0EGXsd-=7FbVFg@mail.gmail.com>

Hello, I am working on a small data set and trying to find values of Qmax
and Kl for the equation

qe = (Qmax * Kl * ce) / (1 + ?l* ce)

I found my Qmax and Kl through the linear model butam now trying to find
them through the "nls" function however an error comes up that I am
unfamiliar with. I have the file loaded for reference. Any help is
appreciated. Thank you.

-Timothy

From jdnewmil at dcn.davis.ca.us  Wed Dec 20 00:49:12 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 19 Dec 2017 15:49:12 -0800
Subject: [R] Nonlinear regression
In-Reply-To: <CAGFL1Og+qcfeZ8krXYWiMBAM9L3BbNDFPbQH0EGXsd-=7FbVFg@mail.gmail.com>
References: <CAGFL1Og+qcfeZ8krXYWiMBAM9L3BbNDFPbQH0EGXsd-=7FbVFg@mail.gmail.com>
Message-ID: <DCCD1AEF-1135-489D-B1FC-71A2C7701045@dcn.davis.ca.us>

Your text has unreadable characters in it. Use plain text. 

Also, your attachments are not coming through. Read the Posting Guide about attachments... best results are usually obtained by inserting the R code in the body of your email and not attaching anything. 
-- 
Sent from my phone. Please excuse my brevity.

On December 19, 2017 2:13:03 PM PST, Timothy Axberg <axbergtimothy at gmail.com> wrote:
>Hello, I am working on a small data set and trying to find values of
>Qmax
>and Kl for the equation
>
>qe = (Qmax * Kl * ce) / (1 + ?l* ce)
>
>I found my Qmax and Kl through the linear model butam now trying to
>find
>them through the "nls" function however an error comes up that I am
>unfamiliar with. I have the file loaded for reference. Any help is
>appreciated. Thank you.
>
>-Timothy
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Wed Dec 20 01:13:47 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 19 Dec 2017 16:13:47 -0800
Subject: [R] Nonlinear regression
In-Reply-To: <CAGFL1Og3rVWHxCR12QYwuMvZgN1v8AhoNCncJ1xAwcRVyAHj3A@mail.gmail.com>
References: <CAGFL1Og+qcfeZ8krXYWiMBAM9L3BbNDFPbQH0EGXsd-=7FbVFg@mail.gmail.com>
 <DCCD1AEF-1135-489D-B1FC-71A2C7701045@dcn.davis.ca.us>
 <CAGFL1Og3rVWHxCR12QYwuMvZgN1v8AhoNCncJ1xAwcRVyAHj3A@mail.gmail.com>
Message-ID: <270FE250-369B-4EBD-9EF6-5D5F17C87725@dcn.davis.ca.us>

You also need to reply-all so the mailing list stays in the loop. 
-- 
Sent from my phone. Please excuse my brevity.

On December 19, 2017 4:00:29 PM PST, Timothy Axberg <axbergtimothy at gmail.com> wrote:
>Sorry about that. Here is the code typed directly on the email.
>
>qe = (Qmax * Kl * ce) / (1 + Kl * ce)
>
>##The data
>ce <- c(15.17, 42.15, 69.12, 237.7, 419.77)
>qe <- c(17.65, 30.07, 65.36, 81.7, 90.2)
>
>##The linearized data
>celin <- 1/ce
>qelin <- 1/qe
>
>plot(ce, qe, xlim = xlim, ylim = ylim)
>
>##The linear model
>fit1 <- lm(qelin ~ celin)
>intercept1 <- fit1$coefficients[1]
>slope1 <- fit1$coeffecients[2]
>summary(fit1)
>
>Qmax <- 1/intercept1
>Kl <- .735011*Qmax
>
>xlim <- range(ce, celin)
>ylim <- range(qe, qelin)
>
>abline(lm(qelin ~ celin))
>
>c <- seq(min(ce), max(ce))
>q <- (Qmax*Kl*c)/(1+(Kl*c))
>
>lines(c, q)
>
>
>##My attempt at the nonlinear regression
>fit2 <- nls(qe ~ ((Qmax*Kl*ce)/(1+(Kl*ce))), start = list(Qmax = Qmax,
>Kl =
>Kl))
>results <- fit2$m$getpars()
>Qmax2 <- fit2$m$getpars()[1]
>Kl2 <- fit2$m$getpars()[2]
>summary(fit2)
>
>-Timothy


From emorway at usgs.gov  Wed Dec 20 01:32:23 2017
From: emorway at usgs.gov (Morway, Eric)
Date: Tue, 19 Dec 2017 16:32:23 -0800
Subject: [R] outlining (highlighting) pixels in ggplot2
Message-ID: <CAPoqHzpsSO60oe4PSG4XM9EerhT6t--HZKmoH+7Y34LoBhc6Tw@mail.gmail.com>

Using the small reproducible example below, I'd like to know if one can
somehow use the matrix "sig" (defined below) to add a black outline (with
lwd=2) to all pixels with a corresponding value of 1 in the matrix 'sig'?
So for example, in the ggplot2 plot below, the pixel located at [1,3] would
be outlined by a black square since the value at sig[1,3] == 1.  This is my
first foray into ggplot2, and so far the googles hasn't helped me determine
if this is possible.  Thanks, Eric

PS, my attempt to stretch the color scale such that (-1 * zmin) = zmax =
max(abs(m1)) has failed (i.e., ..., autoscale = FALSE, zmin = -1 * zmax1,
zmax = zmax1), any pointers on this would greatly appreciated as well.

# Import packages
library(ggplot2)
library(RColorBrewer)
library(reshape2)

m1 <- matrix(c(
-0.0024, -0.0031, -0.0021, -0.0034, -0.0060, -1.00e-02, -8.47e-03, -0.0117,
-0.0075, -0.0043, -0.0026, -0.0021,
-0.0015, -0.0076, -0.0032, -0.0105, -0.0107, -2.73e-02, -3.37e-02, -0.0282,
-0.0149, -0.0070, -0.0046, -0.0039,
-0.0121, -0.0155, -0.0203, -0.0290, -0.0330, -3.19e-02, -1.74e-02, -0.0103,
-0.0084, -0.0180, -0.0162, -0.0136,
-0.0073, -0.0053, -0.0050, -0.0058, -0.0060, -4.38e-03, -2.21e-03, -0.0012,
-0.0026, -0.0026, -0.0034, -0.0073,
-0.0027, -0.0031, -0.0054, -0.0069, -0.0071, -6.28e-03, -2.88e-03, -0.0014,
-0.0031, -0.0037, -0.0030, -0.0027,
-0.0261, -0.0223, -0.0216, -0.0293, -0.0327, -3.17e-02, -1.77e-02, -0.0084,
-0.0059, -0.0060, -0.0120, -0.0157,
 0.0045,  0.0006, -0.0031, -0.0058, -0.0093, -9.20e-03, -6.76e-03,
-0.0033,  0.0002,  0.0045,  0.0080,  0.0084,
-0.0021, -0.0018, -0.0020, -0.0046, -0.0080, -2.73e-03,  7.43e-04,  0.0004,
-0.0010, -0.0017, -0.0022, -0.0024,
-0.0345, -0.0294, -0.0212, -0.0194, -0.0192, -2.25e-02, -2.05e-02, -0.0163,
-0.0179, -0.0213, -0.0275, -0.0304,
-0.0034, -0.0038, -0.0040, -0.0045, -0.0059, -1.89e-03,  6.99e-05, -0.0050,
-0.0114, -0.0112, -0.0087, -0.0064,
-0.0051, -0.0061, -0.0052, -0.0035,  0.0012, -7.41e-06, -3.43e-03, -0.0055,
-0.0020,  0.0016, -0.0024, -0.0069,
-0.0061, -0.0068, -0.0089, -0.0107, -0.0104, -7.65e-03,  2.43e-03,  0.0008,
-0.0006, -0.0014, -0.0021, -0.0057,
 0.0381,  0.0149, -0.0074, -0.0302, -0.0550, -6.40e-02, -5.28e-02, -0.0326,
-0.0114,  0.0121,  0.0367,  0.0501,
-0.0075, -0.0096, -0.0123, -0.0200, -0.0288, -2.65e-02, -2.08e-02, -0.0176,
-0.0146, -0.0067, -0.0038, -0.0029,
-0.0154, -0.0162, -0.0252, -0.0299, -0.0350, -3.40e-02, -2.51e-02, -0.0172,
-0.0139, -0.0091, -0.0119, -0.0156),
  nrow = 15, ncol = 12, byrow=TRUE,
  dimnames = list(rev(c("TH1", "IN1", "IN3", "GL1", "LH1", "ED9", "TC1",
"TC2", "TC3", "UT1", "UT3", "UT5", "GC1", "BC1", "WC1")),
                              c(format(seq(as.Date('2000-10-01'),
as.Date('2001-09-30'), by='month'), "%b"))))

# palette definition
palette <- colorRampPalette(c("darkblue", "blue", "white", "red",
"darkred"))

# find max stretch value
zmax1 = max(abs(m1))

m1.melted <- melt(m1)
names(m1.melted) <- c('Site','Month', 'Concentration')

# Set up an example matrix with binary code for which results (pixels) are
significant
set.seed(4004)
sig <- matrix(round(abs(rnorm(15*12)/3)), nrow = 15, ncol = 12)

ggplot(m1.melted, aes(x = Month, y = Site, fill = Concentration), autoscale
= FALSE, zmin = -1 * zmax1, zmax = zmax1) +
  geom_tile() +
  coord_equal() +
  scale_fill_gradient2(low = "darkred",
                       mid = "white",
                       high = "darkblue",
                       midpoint = 0)

	[[alternative HTML version deleted]]


From axbergtimothy at gmail.com  Wed Dec 20 01:28:00 2017
From: axbergtimothy at gmail.com (Timothy Axberg)
Date: Tue, 19 Dec 2017 18:28:00 -0600
Subject: [R] Nonlinear regression
In-Reply-To: <270FE250-369B-4EBD-9EF6-5D5F17C87725@dcn.davis.ca.us>
References: <CAGFL1Og+qcfeZ8krXYWiMBAM9L3BbNDFPbQH0EGXsd-=7FbVFg@mail.gmail.com>
 <DCCD1AEF-1135-489D-B1FC-71A2C7701045@dcn.davis.ca.us>
 <CAGFL1Og3rVWHxCR12QYwuMvZgN1v8AhoNCncJ1xAwcRVyAHj3A@mail.gmail.com>
 <270FE250-369B-4EBD-9EF6-5D5F17C87725@dcn.davis.ca.us>
Message-ID: <CAGFL1OgH2rJpqJ_SaneJYE3BMNCO+vgK0m1NCP4nZVHWN+Q=8Q@mail.gmail.com>

Should I repost the question with reply-all?

On Tue, Dec 19, 2017 at 6:13 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> You also need to reply-all so the mailing list stays in the loop.
> --
> Sent from my phone. Please excuse my brevity.
>
> On December 19, 2017 4:00:29 PM PST, Timothy Axberg <
> axbergtimothy at gmail.com> wrote:
> >Sorry about that. Here is the code typed directly on the email.
> >
> >qe = (Qmax * Kl * ce) / (1 + Kl * ce)
> >
> >##The data
> >ce <- c(15.17, 42.15, 69.12, 237.7, 419.77)
> >qe <- c(17.65, 30.07, 65.36, 81.7, 90.2)
> >
> >##The linearized data
> >celin <- 1/ce
> >qelin <- 1/qe
> >
> >plot(ce, qe, xlim = xlim, ylim = ylim)
> >
> >##The linear model
> >fit1 <- lm(qelin ~ celin)
> >intercept1 <- fit1$coefficients[1]
> >slope1 <- fit1$coeffecients[2]
> >summary(fit1)
> >
> >Qmax <- 1/intercept1
> >Kl <- .735011*Qmax
> >
> >xlim <- range(ce, celin)
> >ylim <- range(qe, qelin)
> >
> >abline(lm(qelin ~ celin))
> >
> >c <- seq(min(ce), max(ce))
> >q <- (Qmax*Kl*c)/(1+(Kl*c))
> >
> >lines(c, q)
> >
> >
> >##My attempt at the nonlinear regression
> >fit2 <- nls(qe ~ ((Qmax*Kl*ce)/(1+(Kl*ce))), start = list(Qmax = Qmax,
> >Kl =
> >Kl))
> >results <- fit2$m$getpars()
> >Qmax2 <- fit2$m$getpars()[1]
> >Kl2 <- fit2$m$getpars()[2]
> >summary(fit2)
> >
> >-Timothy
>

	[[alternative HTML version deleted]]


From berwin.turlach at gmail.com  Wed Dec 20 05:29:10 2017
From: berwin.turlach at gmail.com (Berwin A Turlach)
Date: Wed, 20 Dec 2017 12:29:10 +0800
Subject: [R] Nonlinear regression
In-Reply-To: <CAGFL1OgH2rJpqJ_SaneJYE3BMNCO+vgK0m1NCP4nZVHWN+Q=8Q@mail.gmail.com>
References: <CAGFL1Og+qcfeZ8krXYWiMBAM9L3BbNDFPbQH0EGXsd-=7FbVFg@mail.gmail.com>
 <DCCD1AEF-1135-489D-B1FC-71A2C7701045@dcn.davis.ca.us>
 <CAGFL1Og3rVWHxCR12QYwuMvZgN1v8AhoNCncJ1xAwcRVyAHj3A@mail.gmail.com>
 <270FE250-369B-4EBD-9EF6-5D5F17C87725@dcn.davis.ca.us>
 <CAGFL1OgH2rJpqJ_SaneJYE3BMNCO+vgK0m1NCP4nZVHWN+Q=8Q@mail.gmail.com>
Message-ID: <20171220122910.24137a37@ECM-DTC-716.uniwa.uwa.edu.au>

G'day Timothy,

On Tue, 19 Dec 2017 18:28:00 -0600
Timothy Axberg <axbergtimothy at gmail.com> wrote:

> Should I repost the question with reply-all?

Nope, we got all from Jeff's post. :)

> On Tue, Dec 19, 2017 at 6:13 PM, Jeff Newmiller
> <jdnewmil at dcn.davis.ca.us> wrote:
> 
> > You also need to reply-all so the mailing list stays in the loop.
> > --
> > Sent from my phone. Please excuse my brevity.
> >
> > On December 19, 2017 4:00:29 PM PST, Timothy Axberg <  
> > axbergtimothy at gmail.com> wrote:
> > >Sorry about that. Here is the code typed directly on the email.
> > >
> > >qe = (Qmax * Kl * ce) / (1 + Kl * ce)
[...]
> > >##The linearized data
> > >celin <- 1/ce
> > >qelin <- 1/qe

Plotting qelin against celin, I can see why you call this the
linearized data.  But fitting a linear model to these data obviously
does not give you good starting values for nls().

Given your model equation, I would linearize the model to:
 
   qe = Qmax*KL * ce + KL * ce*qe

and fit a non-intercept linear model to predict qe by ce and
ce*qe.  From this model I would then determine starting values for
nls().  It seems to work with your data set:

R> ce <- c(15.17, 42.15, 69.12, 237.7, 419.77)
R> qe <- c(17.65, 30.07, 65.36, 81.7, 90.2)
R> fit2 <- lm(qe ~ ce + I(ce*qe) - 1)
R> summary(fit2)
R> Kl <- - coef(fit2)[2]
R> Qmax <- coef(fit2)[1]/Kl
R> plot(ce, qe)
R> c <- seq(min(ce), max(ce))
R> q <- (Qmax*Kl*c)/(1+(Kl*c))
R> lines(c, q)
R> fit2 <- nls(qe ~ ((Qmax*Kl*ce)/(1+(Kl*ce))), start = list(Qmax = Qmax,Kl =Kl)) 
R> summary(fit2)

Formula: qe ~ ((Qmax * Kl * ce)/(1 + (Kl * ce)))

Parameters:
      Estimate Std. Error t value Pr(>|t|)   
Qmax 106.42602   12.82808   8.296  0.00367 **
Kl     0.01456    0.00543   2.681  0.07496 . 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 9.249 on 3 degrees of freedom

Number of iterations to convergence: 2 
Achieved convergence tolerance: 6.355e-06

HTH.

Cheers,

	Berwin


From E.Vettorazzi at uke.de  Wed Dec 20 15:33:13 2017
From: E.Vettorazzi at uke.de (Eik Vettorazzi)
Date: Wed, 20 Dec 2017 15:33:13 +0100
Subject: [R] outlining (highlighting) pixels in ggplot2
In-Reply-To: <CAPoqHzpsSO60oe4PSG4XM9EerhT6t--HZKmoH+7Y34LoBhc6Tw@mail.gmail.com>
References: <CAPoqHzpsSO60oe4PSG4XM9EerhT6t--HZKmoH+7Y34LoBhc6Tw@mail.gmail.com>
Message-ID: <d0121783-7b13-c5e7-d2d1-6763db1b26e2@uke.de>

Hi Eric,
you can use an annotate-layer, eg

ind<-which(sig>0,arr.ind = T)
ggplot(m1.melted, aes(x = Month, y = Site, fill = Concentration), autoscale
       = FALSE, zmin = -1 * zmax1, zmax = zmax1) +
  geom_tile() +
  coord_equal() +
  scale_fill_gradient2(low = "darkred",
                       mid = "white",
                       high = "darkblue",
                       midpoint = 0)
+annotate("rect",ymin=ind[,"row"]-.5,ymax=.5+ind[,"row"],
xmin=-.5+ind[,"col"],xmax=.5+ind[,"col"],colour="red", size=.5,
linetype=1, fill=NA)


Cheers

Am 20.12.2017 um 01:32 schrieb Morway, Eric:
> library(ggplot2)
> library(RColorBrewer)
> library(reshape2)
> 
> m1 <- matrix(c(
> -0.0024, -0.0031, -0.0021, -0.0034, -0.0060, -1.00e-02, -8.47e-03, -0.0117,
> -0.0075, -0.0043, -0.0026, -0.0021,
> -0.0015, -0.0076, -0.0032, -0.0105, -0.0107, -2.73e-02, -3.37e-02, -0.0282,
> -0.0149, -0.0070, -0.0046, -0.0039,
> -0.0121, -0.0155, -0.0203, -0.0290, -0.0330, -3.19e-02, -1.74e-02, -0.0103,
> -0.0084, -0.0180, -0.0162, -0.0136,
> -0.0073, -0.0053, -0.0050, -0.0058, -0.0060, -4.38e-03, -2.21e-03, -0.0012,
> -0.0026, -0.0026, -0.0034, -0.0073,
> -0.0027, -0.0031, -0.0054, -0.0069, -0.0071, -6.28e-03, -2.88e-03, -0.0014,
> -0.0031, -0.0037, -0.0030, -0.0027,
> -0.0261, -0.0223, -0.0216, -0.0293, -0.0327, -3.17e-02, -1.77e-02, -0.0084,
> -0.0059, -0.0060, -0.0120, -0.0157,
>  0.0045,  0.0006, -0.0031, -0.0058, -0.0093, -9.20e-03, -6.76e-03,
> -0.0033,  0.0002,  0.0045,  0.0080,  0.0084,
> -0.0021, -0.0018, -0.0020, -0.0046, -0.0080, -2.73e-03,  7.43e-04,  0.0004,
> -0.0010, -0.0017, -0.0022, -0.0024,
> -0.0345, -0.0294, -0.0212, -0.0194, -0.0192, -2.25e-02, -2.05e-02, -0.0163,
> -0.0179, -0.0213, -0.0275, -0.0304,
> -0.0034, -0.0038, -0.0040, -0.0045, -0.0059, -1.89e-03,  6.99e-05, -0.0050,
> -0.0114, -0.0112, -0.0087, -0.0064,
> -0.0051, -0.0061, -0.0052, -0.0035,  0.0012, -7.41e-06, -3.43e-03, -0.0055,
> -0.0020,  0.0016, -0.0024, -0.0069,
> -0.0061, -0.0068, -0.0089, -0.0107, -0.0104, -7.65e-03,  2.43e-03,  0.0008,
> -0.0006, -0.0014, -0.0021, -0.0057,
>  0.0381,  0.0149, -0.0074, -0.0302, -0.0550, -6.40e-02, -5.28e-02, -0.0326,
> -0.0114,  0.0121,  0.0367,  0.0501,
> -0.0075, -0.0096, -0.0123, -0.0200, -0.0288, -2.65e-02, -2.08e-02, -0.0176,
> -0.0146, -0.0067, -0.0038, -0.0029,
> -0.0154, -0.0162, -0.0252, -0.0299, -0.0350, -3.40e-02, -2.51e-02, -0.0172,
> -0.0139, -0.0091, -0.0119, -0.0156),
>   nrow = 15, ncol = 12, byrow=TRUE,
>   dimnames = list(rev(c("TH1", "IN1", "IN3", "GL1", "LH1", "ED9", "TC1",
> "TC2", "TC3", "UT1", "UT3", "UT5", "GC1", "BC1", "WC1")),
>                               c(format(seq(as.Date('2000-10-01'),
> as.Date('2001-09-30'), by='month'), "%b"))))
> 
> # palette definition
> palette <- colorRampPalette(c("darkblue", "blue", "white", "red",
> "darkred"))
> 
> # find max stretch value
> zmax1 = max(abs(m1))
> 
> m1.melted <- melt(m1)
> names(m1.melted) <- c('Site','Month', 'Concentration')
> 
> # Set up an example matrix with binary code for which results (pixels) are
> significant
> set.seed(4004)
> sig <- matrix(round(abs(rnorm(15*12)/3)), nrow = 15, ncol = 12)
> 
> ggplot(m1.melted, aes(x = Month, y = Site, fill = Concentration), autoscale
> = FALSE, zmin = -1 * zmax1, zmax = zmax1) +
>   geom_tile() +
>   coord_equal() +
>   scale_fill_gradient2(low = "darkred",
>                        mid = "white",
>                        high = "darkblue",
>                        midpoint = 0)

-- 
Eik Vettorazzi

Department of Medical Biometry and Epidemiology
University Medical Center Hamburg-Eppendorf

Martinistrasse 52
building W 34
20246 Hamburg

Phone: +49 (0) 40 7410 - 58243
Fax:   +49 (0) 40 7410 - 57790
Web: www.uke.de/imbe
--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Martina Saurin (komm.)
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING

From christian at echoffmann.ch  Wed Dec 20 14:26:25 2017
From: christian at echoffmann.ch (Christian)
Date: Wed, 20 Dec 2017 14:26:25 +0100
Subject: [R] [ESS] R window: Text is read only (: Another round :)
In-Reply-To: <dc42e4e2-fd83-112e-b205-a76ce62b22e0@echoffmann.ch>
References: <55e68135-5902-98ac-58ba-21a84b03e9c8@echoffmann.ch>
 <877ewkulqb.fsf@galago> <dc42e4e2-fd83-112e-b205-a76ce62b22e0@echoffmann.ch>
Message-ID: <2aadb796-5888-6c00-a791-0a342effbb36@echoffmann.ch>

Dear All

Working along in R (Emacs {Aquamacs}, ESS ) usually I am getting a long 
R buffer with many rows. To reduce its length, I used to select 
backwards and delete those unnecessary rows. Now I cannot do that, only 
C-k allows single lines to be deleted. the mode line says 'Text is 
read-only *R*>'. Both R and ESS are up to date.

Aquamacs 3.3  GNU Emacs 25.1.1 (x86_64-apple-darwin14.1.0, NS 
appkit-1344.72 Version 10.10.2 (Build 14C109))

R version 3.4.3 (2017-11-30)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS High Sierra 10.13.2

ess: 0171204.1404

This behaviour exists at the very start of R where I cannot evendo 
anything in R.

The *ESS* buffer says

(inf-ess 3.0): prog=R, start-args=--no-readline  , echoes=t
Making Process...Buf *R*, :Proc R, :Prog R
  :Args= --no-readline
Start File=nil
(inferior-ess: waiting for process to start (before hook)
(inferior-ess 3): waiting for process after hook

What additional information could be helpful to stop this strange behaviour?

  TIA? C.


-- 
Christian Hoffmann
Rigiblickstrasse 15b
CH-8915 Hausen am Albis
Switzerland
Telefon +41-(0)44-7640853


From Stephen.Bond at cibc.com  Wed Dec 20 15:51:02 2017
From: Stephen.Bond at cibc.com (Bond, Stephen)
Date: Wed, 20 Dec 2017 14:51:02 +0000
Subject: [R] offset with a factor
Message-ID: <624EC9773CAB044ABA65327271BED9B62183E17B@CBMCC-X10-MB06.ad.cibc.com>

Knowledgeable useRs,

Please, advise how to use offset with a factor. I estimate monthly effects from a much bigger data set as monthly effects seem to be stable, and other variables are estimated from a small, but recent data set as there is variation in those non-seasonal coefficients.
How can I use the seasonality estimates from the big data set as an offset provided to the small data set. I know an offset is supposed to be quantitative, but this is such a practical and sensible scenario, I feel compelled. Assume I have 11 coefs estimated with contr.sum.

Thanks everybody

Stephen


	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Wed Dec 20 17:22:30 2017
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Wed, 20 Dec 2017 16:22:30 +0000
Subject: [R] offset with a factor
In-Reply-To: <624EC9773CAB044ABA65327271BED9B62183E17B@CBMCC-X10-MB06.ad.cibc.com>
References: <624EC9773CAB044ABA65327271BED9B62183E17B@CBMCC-X10-MB06.ad.cibc.com>
Message-ID: <8e28e6e9-7ddf-b282-43fd-a78d0a14ba60@dewey.myzen.co.uk>

The documentation for ?offset states that you can have more than one offset.

Michael

On 20/12/2017 14:51, Bond, Stephen wrote:
> Knowledgeable useRs,
> 
> Please, advise how to use offset with a factor. I estimate monthly effects from a much bigger data set as monthly effects seem to be stable, and other variables are estimated from a small, but recent data set as there is variation in those non-seasonal coefficients.
> How can I use the seasonality estimates from the big data set as an offset provided to the small data set. I know an offset is supposed to be quantitative, but this is such a practical and sensible scenario, I feel compelled. Assume I have 11 coefs estimated with contr.sum.
> 
> Thanks everybody
> 
> Stephen
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From dwinsemius at comcast.net  Wed Dec 20 17:52:19 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 20 Dec 2017 08:52:19 -0800
Subject: [R] offset with a factor
In-Reply-To: <624EC9773CAB044ABA65327271BED9B62183E17B@CBMCC-X10-MB06.ad.cibc.com>
References: <624EC9773CAB044ABA65327271BED9B62183E17B@CBMCC-X10-MB06.ad.cibc.com>
Message-ID: <0F2B713A-EE62-4240-B8F5-C8C4F8C43D83@comcast.net>


> On Dec 20, 2017, at 6:51 AM, Bond, Stephen <Stephen.Bond at cibc.com> wrote:
> 
> Knowledgeable useRs,
> 
> Please, advise how to use offset with a factor. I estimate monthly effects from a much bigger data set as monthly effects seem to be stable, and other variables are estimated from a small, but recent data set as there is variation in those non-seasonal coefficients.
> How can I use the seasonality estimates from the big data set as an offset provided to the small data set. I know an offset is supposed to be quantitative, but this is such a practical and sensible scenario, I feel compelled. Assume I have 11 coefs estimated with contr.sum.

Why not create a variable that specifies the relevant coefficient for each of the various levels of the factor (and 0 for the reference level? Then pass that variable to the offset argument.

-- 
David.
> 
> Thanks everybody
> 
> Stephen
> 
> 
> 	[[alternative HTML version deleted]]

Do note: Rhelp is a plain-text mailing list.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From raheleh.amirkhah at gmail.com  Wed Dec 20 16:55:52 2017
From: raheleh.amirkhah at gmail.com (Rahele Amirkhah)
Date: Wed, 20 Dec 2017 15:55:52 +0000 (UTC)
Subject: [R] problem in installing "simpleaffy"
References: <1736796449.1886859.1513785352183.ref@mail.yahoo.com>
Message-ID: <1736796449.1886859.1513785352183@mail.yahoo.com>

Dear Madam/ Sir,
I am using?R version 3.4.2.?I want to analyse microarray data. when I want to install "simpleaffy" package I get this error "package ?simpleaffy? is not available (for R version 3.4.2)". I have the same problem with?R version 3.3.2.
Could you please help me to solve it?
I am working with RStudio 0.99.903.exe. I also have problem in getting the new release of RStudio. Could you please tell me how I can get the last release??
Many thanks in advance.Best regards,Raheleh Amirkhah
	[[alternative HTML version deleted]]


From emorway at usgs.gov  Wed Dec 20 18:21:13 2017
From: emorway at usgs.gov (Morway, Eric)
Date: Wed, 20 Dec 2017 09:21:13 -0800
Subject: [R] outlining (highlighting) pixels in ggplot2
In-Reply-To: <d0121783-7b13-c5e7-d2d1-6763db1b26e2@uke.de>
References: <CAPoqHzpsSO60oe4PSG4XM9EerhT6t--HZKmoH+7Y34LoBhc6Tw@mail.gmail.com>
 <d0121783-7b13-c5e7-d2d1-6763db1b26e2@uke.de>
Message-ID: <CAPoqHzqwCXQyYNW_5XLA20K=EKK7kajE-n+zAsYAmxpUb7kr8g@mail.gmail.com>

I apprecaite the guidance Eik, that works great!  I'm also wondering if you
have any pointers for how I might stretch the color scale so that the max
and min values are the same?  Right now, the min is -0.064 and the max is
something closer to 0.04.  As you can see in what I sent, I tried adding:

zmax1 = max(abs(m1))

ggplot(..., autoscale = FALSE, zmin = -1 * zmax1, zmax = zmax1) + ...

to ggplot, but I'm either using the wrong arguments or have not added them
to the correct spot.  I haven't been able to find the fix for this and
would very much appreciate any further guidance you can offer for this last
fix.

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Dec 20 21:00:01 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 20 Dec 2017 12:00:01 -0800
Subject: [R] problem in installing "simpleaffy"
In-Reply-To: <1736796449.1886859.1513785352183@mail.yahoo.com>
References: <1736796449.1886859.1513785352183.ref@mail.yahoo.com>
 <1736796449.1886859.1513785352183@mail.yahoo.com>
Message-ID: <848AB3CB-C754-457F-82A9-B3C99B9FCF2C@comcast.net>


> On Dec 20, 2017, at 7:55 AM, Rahele Amirkhah <raheleh.amirkhah at gmail.com> wrote:
> 
> Dear Madam/ Sir,
> I am using R version 3.4.2. I want to analyse microarray data. when I want to install "simpleaffy" package I get this error "package ?simpleaffy? is not available (for R version 3.4.2)". I have the same problem with R version 3.3.2.

That's because it's not a CRAN package. It's a Bioc package.


> Could you please help me to solve it?
> I am working with RStudio 0.99.903.exe. I also have problem in getting the new release of RStudio. Could you please tell me how I can get the last release? 

That's off-topic for this mailing list. Ask at RStudio

> Many thanks in advance.Best regards,Raheleh Amirkhah
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From E.Vettorazzi at uke.de  Wed Dec 20 21:15:38 2017
From: E.Vettorazzi at uke.de (Eik Vettorazzi)
Date: Wed, 20 Dec 2017 21:15:38 +0100
Subject: [R] outlining (highlighting) pixels in ggplot2
In-Reply-To: <CAPoqHzqwCXQyYNW_5XLA20K=EKK7kajE-n+zAsYAmxpUb7kr8g@mail.gmail.com>
References: <CAPoqHzpsSO60oe4PSG4XM9EerhT6t--HZKmoH+7Y34LoBhc6Tw@mail.gmail.com>
 <d0121783-7b13-c5e7-d2d1-6763db1b26e2@uke.de>
 <CAPoqHzqwCXQyYNW_5XLA20K=EKK7kajE-n+zAsYAmxpUb7kr8g@mail.gmail.com>
Message-ID: <493e22f9-4c82-d4d2-3c26-37e0f966e1ee@uke.de>

just add

limits=c(-zmax1,zmax1)

to the scale_fill_gradient2-call

hth


Am 20.12.2017 um 18:21 schrieb Morway, Eric:
> I apprecaite the guidance Eik, that works great!? I'm also wondering if
> you have any pointers for how I might stretch the color scale so that
> the max and min values are the same?? Right now, the min is -0.064 and
> the max is something closer to 0.04.? As you can see in what I sent, I
> tried adding:
> 
> zmax1 = max(abs(m1))
> 
> ggplot(..., autoscale = FALSE, zmin = -1 * zmax1, zmax = zmax1) + ...
> 
> to ggplot, but I'm either using the wrong arguments or have not added
> them to the correct spot.? I haven't been able to find the fix for this
> and would very much appreciate any further guidance you can offer for
> this last fix.??

-- 
Eik Vettorazzi

Universit?tsklinikum Hamburg-Eppendorf
Institut f?r Medizinische Biometrie und Epidemiologie

Martinistra?e 52
Geb?ude W 34
20246 Hamburg

Telefon: +49 (0) 40 7410 - 58243
Fax:     +49 (0) 40 7410 - 57790

Web: www.uke.de/imbe


--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Martina Saurin (komm.)
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING

From alkauffm at fastmail.fm  Wed Dec 20 22:09:38 2017
From: alkauffm at fastmail.fm (Albrecht Kauffmann)
Date: Wed, 20 Dec 2017 22:09:38 +0100
Subject: [R] [ESS] R window: Text is read only (: Another round :)
In-Reply-To: <2aadb796-5888-6c00-a791-0a342effbb36@echoffmann.ch>
References: <55e68135-5902-98ac-58ba-21a84b03e9c8@echoffmann.ch>
 <877ewkulqb.fsf@galago>
 <dc42e4e2-fd83-112e-b205-a76ce62b22e0@echoffmann.ch>
 <2aadb796-5888-6c00-a791-0a342effbb36@echoffmann.ch>
Message-ID: <1513804178.1202123.1211669272.36B54BF0@webmail.messagingengine.com>

Dear Christian,

to leave the read-only state, did you try C-x C-q in the mode line?

Cheers, Albrecht

-- 
  Albrecht Kauffmann
  alkauffm at fastmail.fm

Am Mi, 20. Dez 2017, um 14:26, schrieb Christian:
> Dear All
> 
> Working along in R (Emacs {Aquamacs}, ESS ) usually I am getting a long 
> R buffer with many rows. To reduce its length, I used to select 
> backwards and delete those unnecessary rows. Now I cannot do that, only 
> C-k allows single lines to be deleted. the mode line says 'Text is 
> read-only *R*>'. Both R and ESS are up to date.
> 
> Aquamacs 3.3  GNU Emacs 25.1.1 (x86_64-apple-darwin14.1.0, NS 
> appkit-1344.72 Version 10.10.2 (Build 14C109))
> 
> R version 3.4.3 (2017-11-30)
> Platform: x86_64-apple-darwin15.6.0 (64-bit)
> Running under: macOS High Sierra 10.13.2
> 
> ess: 0171204.1404
> 
> This behaviour exists at the very start of R where I cannot evendo 
> anything in R.
> 
> The *ESS* buffer says
> 
> (inf-ess 3.0): prog=R, start-args=--no-readline  , echoes=t
> Making Process...Buf *R*, :Proc R, :Prog R
>   :Args= --no-readline
> Start File=nil
> (inferior-ess: waiting for process to start (before hook)
> (inferior-ess 3): waiting for process after hook
> 
> What additional information could be helpful to stop this strange behaviour?
> 
>   TIA? C.
> 
> 
> -- 
> Christian Hoffmann
> Rigiblickstrasse 15b
> CH-8915 Hausen am Albis
> Switzerland
> Telefon +41-(0)44-7640853
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From axbergtimothy at gmail.com  Wed Dec 20 21:04:18 2017
From: axbergtimothy at gmail.com (Timothy Axberg)
Date: Wed, 20 Dec 2017 14:04:18 -0600
Subject: [R] Nonlinear regression
In-Reply-To: <20171220122910.24137a37@ECM-DTC-716.uniwa.uwa.edu.au>
References: <CAGFL1Og+qcfeZ8krXYWiMBAM9L3BbNDFPbQH0EGXsd-=7FbVFg@mail.gmail.com>
 <DCCD1AEF-1135-489D-B1FC-71A2C7701045@dcn.davis.ca.us>
 <CAGFL1Og3rVWHxCR12QYwuMvZgN1v8AhoNCncJ1xAwcRVyAHj3A@mail.gmail.com>
 <270FE250-369B-4EBD-9EF6-5D5F17C87725@dcn.davis.ca.us>
 <CAGFL1OgH2rJpqJ_SaneJYE3BMNCO+vgK0m1NCP4nZVHWN+Q=8Q@mail.gmail.com>
 <20171220122910.24137a37@ECM-DTC-716.uniwa.uwa.edu.au>
Message-ID: <CAGFL1Oi93bFbdUeUrW6GY+F_oZNAWnkLUSjXQuzwGGoRFavY7A@mail.gmail.com>

Thank you! I didn't realize I wasn't linearizing it correctly. This was
very helpful.

-Timothy

On Tue, Dec 19, 2017 at 10:29 PM, Berwin A Turlach <berwin.turlach at gmail.com
> wrote:

> G'day Timothy,
>
> On Tue, 19 Dec 2017 18:28:00 -0600
> Timothy Axberg <axbergtimothy at gmail.com> wrote:
>
> > Should I repost the question with reply-all?
>
> Nope, we got all from Jeff's post. :)
>
> > On Tue, Dec 19, 2017 at 6:13 PM, Jeff Newmiller
> > <jdnewmil at dcn.davis.ca.us> wrote:
> >
> > > You also need to reply-all so the mailing list stays in the loop.
> > > --
> > > Sent from my phone. Please excuse my brevity.
> > >
> > > On December 19, 2017 4:00:29 PM PST, Timothy Axberg <
> > > axbergtimothy at gmail.com> wrote:
> > > >Sorry about that. Here is the code typed directly on the email.
> > > >
> > > >qe = (Qmax * Kl * ce) / (1 + Kl * ce)
> [...]
> > > >##The linearized data
> > > >celin <- 1/ce
> > > >qelin <- 1/qe
>
> Plotting qelin against celin, I can see why you call this the
> linearized data.  But fitting a linear model to these data obviously
> does not give you good starting values for nls().
>
> Given your model equation, I would linearize the model to:
>
>    qe = Qmax*KL * ce + KL * ce*qe
>
> and fit a non-intercept linear model to predict qe by ce and
> ce*qe.  From this model I would then determine starting values for
> nls().  It seems to work with your data set:
>
> R> ce <- c(15.17, 42.15, 69.12, 237.7, 419.77)
> R> qe <- c(17.65, 30.07, 65.36, 81.7, 90.2)
> R> fit2 <- lm(qe ~ ce + I(ce*qe) - 1)
> R> summary(fit2)
> R> Kl <- - coef(fit2)[2]
> R> Qmax <- coef(fit2)[1]/Kl
> R> plot(ce, qe)
> R> c <- seq(min(ce), max(ce))
> R> q <- (Qmax*Kl*c)/(1+(Kl*c))
> R> lines(c, q)
> R> fit2 <- nls(qe ~ ((Qmax*Kl*ce)/(1+(Kl*ce))), start = list(Qmax =
> Qmax,Kl =Kl))
> R> summary(fit2)
>
> Formula: qe ~ ((Qmax * Kl * ce)/(1 + (Kl * ce)))
>
> Parameters:
>       Estimate Std. Error t value Pr(>|t|)
> Qmax 106.42602   12.82808   8.296  0.00367 **
> Kl     0.01456    0.00543   2.681  0.07496 .
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Residual standard error: 9.249 on 3 degrees of freedom
>
> Number of iterations to convergence: 2
> Achieved convergence tolerance: 6.355e-06
>
> HTH.
>
> Cheers,
>
>         Berwin
>
>

	[[alternative HTML version deleted]]


From lorenzo.isella at gmail.com  Thu Dec 21 11:29:31 2017
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Thu, 21 Dec 2017 11:29:31 +0100
Subject: [R] Fitting Beta Distribution
Message-ID: <CAE-ioUzadAHvm1aWjY0F-ucq6ZMcTqyfPUjE3ittURFnux3w2w@mail.gmail.com>

Dear All,
I need to fit a custom probability density (based on the symmetric beta
distribution B(shape, shape), where the two parameters shape1 and shape2
are identical) to my data.
The trouble is that I experience some problems also when dealing with the
plain vanilla symmetric beta distribution.
Please consider the code at the end of the email.
In the code, dbeta1 is the density of the beta distribution for
shape1=shape2=shape.
In the code, dbeta2 is the same quantity written explicitly, without the
normalization factor (which should not matter at all if we talk about
maximizing a quantity).

I then generate some random numbers according to Beta(0.2, 0.2) and I try
to estimate the shape parameter using

1) fitdistr from MASS
2) mle from stats4

Results: generally speaking I have non-sense estimates of the shape
parameter when I use dbeta2 instead of dbeta1 and I do not understand why.
On top of that, mle crashes with dbeta2 and often I have numerical problems
depending on how I seed the x sequence of random numbers.

I must be misunderstanding something, so any suggestion is appreciated.
Cheers

Lorenzo

#########################################################################

library(MASS)
library(stats4)


dbeta1 <- function(x, shape, ...)
       dbeta(x, shape, shape, ...)


dbeta2 <- function(x, shape){


    res <- x^(shape-1)*(1-x)^(shape-1)

    return(res)

}



LL1 <- function(shape){


    R <- dbeta1(x, shape)


    res <- -sum(log(R))

    return(res)


}




LL2 <- function(shape){


    R <- dbeta2(x, shape)


    res <- -sum(log(R))

    return(res)


}




set.seed(124)

x <-rbeta(1000, 0.2, 0.2)


fit_dbeta1 <- fitdistr( x , dbeta1, start=list(shape=0.5) , method="Brent",
lower=c(0), upper=c(1))

print("estimate of shape from fit_dbeta1 is")
print(fit_dbeta1$estimate)




fit_dbeta2 <- fitdistr( x , dbeta2, start=list(shape=0.5) , method="Brent",
lower=c(0), upper=c(1))

print("estimate of shape from fit_dbeta2 is")
print(fit_dbeta2$estimate)



fit_LL1 <- mle(LL1, start=list(shape=0.5))

print("estimate of from fit_LL1")
print(summary(fit_LL1))

## this does not work

fit_LL2 <- mle(LL2, start=list(shape=0.5))

	[[alternative HTML version deleted]]


From lorenzo.isella at gmail.com  Thu Dec 21 12:12:57 2017
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Thu, 21 Dec 2017 12:12:57 +0100
Subject: [R] Fitting Beta Distribution
Message-ID: <CAE-ioUy_EN_MJ65ZOuWQVNF9wO-NmDp56TvuaB+9fQsN3CNXVw@mail.gmail.com>

I answer my own question: I had overlooked the fact that the normalization
factor is also a function of the parameters I want to optimise, hence I
should write


dbeta2 <- function(x, shape){


    res <- x^(shape-1)*(1-x)^(shape-1)/beta(shape, shape)

    return(res)

}

after which the results are consistent.

---------- Forwarded message ----------
From: Lorenzo Isella <lorenzo.isella at yopmail.com>
Date: 21 December 2017 at 11:29
Subject: Fitting Beta Distribution
To: "r-help at r-project.org" <r-help at r-project.org>


Dear All,
I need to fit a custom probability density (based on the symmetric beta
distribution B(shape, shape), where the two parameters shape1 and shape2
are identical) to my data.
The trouble is that I experience some problems also when dealing with the
plain vanilla symmetric beta distribution.
Please consider the code at the end of the email.
In the code, dbeta1 is the density of the beta distribution for
shape1=shape2=shape.
In the code, dbeta2 is the same quantity written explicitly, without the
normalization factor (which should not matter at all if we talk about
maximizing a quantity).

I then generate some random numbers according to Beta(0.2, 0.2) and I try
to estimate the shape parameter using

1) fitdistr from MASS
2) mle from stats4

Results: generally speaking I have non-sense estimates of the shape
parameter when I use dbeta2 instead of dbeta1 and I do not understand why.
On top of that, mle crashes with dbeta2 and often I have numerical problems
depending on how I seed the x sequence of random numbers.

I must be misunderstanding something, so any suggestion is appreciated.
Cheers

Lorenzo

#########################################################################

library(MASS)
library(stats4)


dbeta1 <- function(x, shape, ...)
       dbeta(x, shape, shape, ...)


dbeta2 <- function(x, shape){


    res <- x^(shape-1)*(1-x)^(shape-1)

    return(res)

}



LL1 <- function(shape){


    R <- dbeta1(x, shape)


    res <- -sum(log(R))

    return(res)


}




LL2 <- function(shape){


    R <- dbeta2(x, shape)


    res <- -sum(log(R))

    return(res)


}




set.seed(124)

x <-rbeta(1000, 0.2, 0.2)


fit_dbeta1 <- fitdistr( x , dbeta1, start=list(shape=0.5) , method="Brent",
lower=c(0), upper=c(1))

print("estimate of shape from fit_dbeta1 is")
print(fit_dbeta1$estimate)




fit_dbeta2 <- fitdistr( x , dbeta2, start=list(shape=0.5) , method="Brent",
lower=c(0), upper=c(1))

print("estimate of shape from fit_dbeta2 is")
print(fit_dbeta2$estimate)



fit_LL1 <- mle(LL1, start=list(shape=0.5))

print("estimate of from fit_LL1")
print(summary(fit_LL1))

## this does not work

fit_LL2 <- mle(LL2, start=list(shape=0.5))

	[[alternative HTML version deleted]]


From klebyn at yahoo.com.br  Thu Dec 21 12:40:16 2017
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Thu, 21 Dec 2017 09:40:16 -0200
Subject: [R] develop.raw error ( adimpro )
Message-ID: <ed2f61e7-cf9d-6f74-1f7f-df89fe3801f1@yahoo.com.br>

Hello all,
I'm trying to use the adimpro package to read RAW files (image). Make 
readins is OK!
> r <-  read.raw( '20171218_210956.dng', type='RAW', compress=FALSE )
> summary( r )
> ### cut the many lines...
Filter pattern: GR/BG
> extract.info( r )
[1] "GR/B"
> 
To next, develop the raw file using the "develop.raw" function.
But the function returns the error:
>  dr <- develop.raw( r )
Error in dim(mat) <- c(4, 3) :
 ? dims [product 12] do not match the length of object [0]

My suspicion is that the develop.raw function fails to get the correct 
Bayer's? pattern.
Within the develop.raw function there is a code (below) that will always 
return the pattern as a variable of NULL value

> bayer <- switch(extract.info( r ), RGGB =  1, GRBG = 2, BGGR = 3, GBRG = 4)
> bayer
NULL
> 
Is there anything to do to get around the situation?

Thank you so much
Cleber Borges

ps.: below more details


##############################
 > r <- read.raw( '20171218_210956.dng', type='RAW', compress=FALSE )
 > summary( r )
 ???????? Image file: 20171218_210956.dng
 ??? Image dimension: 3024 4032
 ??????? Color space: RAW
 ??????? Color depth: 16bit
 ?? Gamma correction: FALSE? Type: None
 ??????? White point: D65
 ????????????? Range: 0 65535

EXIF-Information:

Filename: 20171218_210956.dng
Timestamp: Mon Dec 18 21:09:56 2017
Camera: Samsung SM-G935F
DNG Version: 1.4.0.0
ISO speed: 200
Shutter: 1/24.0 sec
Aperture: f/1.7
Focal length: 4.2 mm
Embedded ICC profile: no
Number of raw images: 1
Thumb size:?? 504 x 376
Full size:?? 4032 x 3024
Image size:? 4032 x 3024
Output size: 3024 x 4032
Raw colors: 3
Filter pattern: GR/BG
Daylight multipliers: 2.022219 0.916446 1.101952
Camera multipliers: 1.765517 1.000000 1.675941 0.000000
 > extract.info( r )
[1] "GR/B"
 >
 > dr <- develop.raw( r )
Error in dim(mat) <- c(4, 3) :
 ? dims [product 12] do not match the length of object [0]
 > ### my suspection
 > bayer <- switch(extract.info( r ), RGGB = 1, GRBG = 2, BGGR = 3, GBRG 
= 4)
 > bayer
NULL

 > sessionInfo()
R version 3.4.2 (2017-09-28)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

Matrix products: default

locale:
[1] LC_COLLATE=Portuguese_Brazil.1252 LC_CTYPE=Portuguese_Brazil.1252
[3] LC_MONETARY=Portuguese_Brazil.1252 LC_NUMERIC=C
[5] LC_TIME=Portuguese_Brazil.1252

attached base packages:
[1] grid????? stats???? graphics? grDevices utils datasets? methods
[8] base

other attached packages:
[1] fields_9.0?????? maps_3.2.0?????? spam_2.1-1 dotCall64_0.9-5
[5] adimpro_0.8.2??? awsMethods_1.0-4

loaded via a namespace (and not attached):
[1] compiler_3.4.2 tools_3.4.2
 >



---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Dec 21 16:07:46 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 21 Dec 2017 07:07:46 -0800
Subject: [R] develop.raw error ( adimpro )
In-Reply-To: <ed2f61e7-cf9d-6f74-1f7f-df89fe3801f1@yahoo.com.br>
References: <ed2f61e7-cf9d-6f74-1f7f-df89fe3801f1@yahoo.com.br>
Message-ID: <CAGxFJbS-74BVWmPW0JnGWB8jkoYx9q_D+0ZOLUgCA=23MQ9CAQ@mail.gmail.com>

If you do not receive a satisfactory reply here, contacting the package
maintainer for such specific issues may be your next option. (S)he can be
found by maintainer("adimpro") .

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Thu, Dec 21, 2017 at 3:40 AM, Cleber N.Borges via R-help <
r-help at r-project.org> wrote:

> Hello all,
> I'm trying to use the adimpro package to read RAW files (image). Make
> readins is OK!
> > r <-  read.raw( '20171218_210956.dng', type='RAW', compress=FALSE )
> > summary( r )
> > ### cut the many lines...
> Filter pattern: GR/BG
> > extract.info( r )
> [1] "GR/B"
> >
> To next, develop the raw file using the "develop.raw" function.
> But the function returns the error:
> >  dr <- develop.raw( r )
> Error in dim(mat) <- c(4, 3) :
>    dims [product 12] do not match the length of object [0]
>
> My suspicion is that the develop.raw function fails to get the correct
> Bayer's  pattern.
> Within the develop.raw function there is a code (below) that will always
> return the pattern as a variable of NULL value
>
> > bayer <- switch(extract.info( r ), RGGB =  1, GRBG = 2, BGGR = 3, GBRG
> = 4)
> > bayer
> NULL
> >
> Is there anything to do to get around the situation?
>
> Thank you so much
> Cleber Borges
>
> ps.: below more details
>
>
> ##############################
>  > r <- read.raw( '20171218_210956.dng', type='RAW', compress=FALSE )
>  > summary( r )
>           Image file: 20171218_210956.dng
>      Image dimension: 3024 4032
>          Color space: RAW
>          Color depth: 16bit
>     Gamma correction: FALSE  Type: None
>          White point: D65
>                Range: 0 65535
>
> EXIF-Information:
>
> Filename: 20171218_210956.dng
> Timestamp: Mon Dec 18 21:09:56 2017
> Camera: Samsung SM-G935F
> DNG Version: 1.4.0.0
> ISO speed: 200
> Shutter: 1/24.0 sec
> Aperture: f/1.7
> Focal length: 4.2 mm
> Embedded ICC profile: no
> Number of raw images: 1
> Thumb size:   504 x 376
> Full size:   4032 x 3024
> Image size:  4032 x 3024
> Output size: 3024 x 4032
> Raw colors: 3
> Filter pattern: GR/BG
> Daylight multipliers: 2.022219 0.916446 1.101952
> Camera multipliers: 1.765517 1.000000 1.675941 0.000000
>  > extract.info( r )
> [1] "GR/B"
>  >
>  > dr <- develop.raw( r )
> Error in dim(mat) <- c(4, 3) :
>    dims [product 12] do not match the length of object [0]
>  > ### my suspection
>  > bayer <- switch(extract.info( r ), RGGB = 1, GRBG = 2, BGGR = 3, GBRG
> = 4)
>  > bayer
> NULL
>
>  > sessionInfo()
> R version 3.4.2 (2017-09-28)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 7 x64 (build 7601) Service Pack 1
>
> Matrix products: default
>
> locale:
> [1] LC_COLLATE=Portuguese_Brazil.1252 LC_CTYPE=Portuguese_Brazil.1252
> [3] LC_MONETARY=Portuguese_Brazil.1252 LC_NUMERIC=C
> [5] LC_TIME=Portuguese_Brazil.1252
>
> attached base packages:
> [1] grid      stats     graphics  grDevices utils datasets  methods
> [8] base
>
> other attached packages:
> [1] fields_9.0       maps_3.2.0       spam_2.1-1 dotCall64_0.9-5
> [5] adimpro_0.8.2    awsMethods_1.0-4
>
> loaded via a namespace (and not attached):
> [1] compiler_3.4.2 tools_3.4.2
>  >
>
>
>
> ---
> Este email foi escaneado pelo Avast antiv?rus.
> https://www.avast.com/antivirus
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jwijffels at bnosac.be  Tue Dec 19 22:20:18 2017
From: jwijffels at bnosac.be (Jan Wijffels)
Date: Tue, 19 Dec 2017 22:20:18 +0100
Subject: [R] [R-pkgs] release of version 0.2 of the textrank package
Message-ID: <CAJ9GNamGbHhLUkxLUPYFw8Jr889KFHPa9gkzn1rWfd9wnwH3ug@mail.gmail.com>

Hello R users,

I'm pleased to announce the release of version 0.2 of the textrank package
on CRAN: https://CRAN.R-project.org/package=textrank

*The package is a natural language processing package which allows one to
summarize text by finding*
*- relevant sentences*
*- relevant keywords*

This is done by constructing a sentence network which finds how sentences
are related to one another (word overlap). On that network Google Pagerank
is used in order to find relevant sentences.

In a similar way 'textrank' can also be used to extract keywords. How? A
word network is constructed by looking if words are following one another.
On top of that network the 'Pagerank' algorithm is applied to extract
relevant words. Relevant words which are following one another are next
pasted together to get keywords.

The package has a vignette at
https://cran.r-project.org/web/packages/textrank/vignettes/textrank.html
and it also plays nicely with the udpipe package ?
https://CRAN.R-project.org/package=udpipe which is good for parts-of-speech
tagging, lemmatisation, dependency parsing and general NLP processing.

?all the best,
Jan


Jan Wijffels
Statistician
www.bnosac.be

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages

From Sam.Albers at gov.bc.ca  Wed Dec 20 01:38:12 2017
From: Sam.Albers at gov.bc.ca (Albers, Sam ENV:EX)
Date: Wed, 20 Dec 2017 00:38:12 +0000
Subject: [R] [R-pkgs] New package: tidyhydat
Message-ID: <2d81a93ee9bf4d5daa4e5eb3ec9c3886@E3PMBX15.idir.BCGOV>

Hello all,

tidyhydat - https://CRAN.R-project.org/package=tidyhydat - was accepted on CRAN recently. tidyhydat is a new R package that provides functions in R to access Canadian hydrometric data sources and then tidy that data.  We have tried to simplify the process of importing data so that users can focus on modelling and visualizing hydrometric data. 

Historical data is contained within HYDAT, the Canadian national Water Data Archive, which is published quarterly by the Government of Canada's Department of Environment and Climate Change. Data in this archive range from 1850 to 2017. tidyhydat also provides functions to access real-time data over the web. This package would be of interest to anyone who has need for Canadian hydrometric data in R. 

This package was peer-reviewed by the rOpenSci open review process and is now part of that suite of packages.

A short vignette outlining tidyhydat's usage can be found here: https://cran.r-project.org/web/packages/tidyhydat/vignettes/tidyhydat_an_introduction.html

The project page can be found here: https://github.com/ropensci/tidyhydat

Best,

Sam

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From spnrpa at gmail.com  Thu Dec 21 02:37:49 2017
From: spnrpa at gmail.com (Spence Aiello)
Date: Wed, 20 Dec 2017 17:37:49 -0800
Subject: [R] [R-pkgs] A New R Kernel for Jupyter
Message-ID: <CAL2dDUswKZKK=8hngdyDbgxgweidd-Hb6NL-MGCF8PLP7X_dqQ@mail.gmail.com>

Hello R Community,

'JuniperKernel' is a new package that provides a 'Jupyter' kernel
implementation. This package is the first C++ 'Jupyter' kernel to exist in
CRAN. Its architecture differs from incumbent R kernels in novel ways that
allow for streaming I/O and full support of 'Jupyter' extensions.

I would be grateful for any feedback!

Happy Holidays,
Spencer

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From kw.stat at gmail.com  Thu Dec 21 20:44:48 2017
From: kw.stat at gmail.com (Kevin Wright)
Date: Thu, 21 Dec 2017 13:44:48 -0600
Subject: [R] [R-pkgs] New package: nipals
Message-ID: <CAKFxdiRho4tcE8z4_SMsjx8DCiVAajyCfN1o2GyBc2n0LL1CKA@mail.gmail.com>

I would like to announce the availability of the 'nipals' package on CRAN,
https://cran.r-project.org/web/packages/nipals/.

The nipals package tries to do just one thing really well...find principal
components of a matrix using Nonlinear Partial Least Squares. Missing
values are allowed, the principal components are orthogonal, and the code
has been heavily optimized.

Details of the package can be found in a pair of vignettes:

Comparing NIPALS functions in R
https://cran.r-project.org/web/packages/nipals/vignettes/nipals_comparisons.html

NIPALS optimization notes
https://cran.r-project.org/web/packages/nipals/vignettes/nipals_optimization.html


Please report any issues through the Github page:
https://github.com/kwstat/nipals

Kevin Wright

-- 
Kevin Wright

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From pdalgd at gmail.com  Fri Dec 22 18:00:12 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 22 Dec 2017 18:00:12 +0100
Subject: [R] Fitting Beta Distribution
In-Reply-To: <CAE-ioUzadAHvm1aWjY0F-ucq6ZMcTqyfPUjE3ittURFnux3w2w@mail.gmail.com>
References: <CAE-ioUzadAHvm1aWjY0F-ucq6ZMcTqyfPUjE3ittURFnux3w2w@mail.gmail.com>
Message-ID: <AF4B9A5C-069F-47DA-8722-6AB3BB295D26@gmail.com>

Stop right there and rethink! The normalization factor depends on the parameter that you are maximizing over.

-pd

> On 21 Dec 2017, at 11:29 , Lorenzo Isella <lorenzo.isella at gmail.com> wrote:
> 
> In the code, dbeta1 is the density of the beta distribution for
> shape1=shape2=shape.
> In the code, dbeta2 is the same quantity written explicitly, without the
> normalization factor (which should not matter at all if we talk about
> maximizing a quantity).
> 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From roy.mendelssohn at noaa.gov  Sat Dec 23 00:35:23 2017
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Fri, 22 Dec 2017 15:35:23 -0800
Subject: [R] Development versions of xtractomatic and rerddapXtracto
Message-ID: <CC10B2D3-46A3-481A-A3E5-894743E83487@noaa.gov>

If you are a user of the R package "xtractomatic",  I have a new development version available,  as well as a test version of the package "rerddapXtracto".  The  biggest changes are functions that can take the output of any of the data download functions and quickly map the data.   These functions use the package "plotdap". Also, a lot of the code in the development version of "xtractomatic" has been cleaned up and simplified.   If you have used "xtractomatic" before,  be certain to read the vignette  (https://rmendels.github.io/Usingxtractomatic_Dev.nb.html) as there have been some changes,  most notably in the order that arguments are passed in the function calls, what is optional in the call, and the two new plotting routines.  These changes were  made to support the idea that "data" should be the first argument  in the function calls,  and also to make the xtractomatic functions very close to the calls in  "rerddapXtracto".  This is important because this will likely be the last version of "xtractomatic", with future development being put on "rerddapXtracto".

"rerddapXtracto" has the same functionality as "xtractomatic" but will work with any gridded dataset on any ERDDAP server,  by making use of the R package "rerddap", while "xtractomatic" only works with certain datasets on the ERD ERDDAP server.  The advantage of the "xtractomatic" approach is we chose what we thought were the most useful datasets (out of over 1000 datasets), and information about those datasets are built into the program.  But that means the "xtractomatic" package can not be used to access a large number of datasets.  "rerddapXtracto" is much more general,  but  the user must know the ERDDAP server they want to access, which dataset, and must first obtain information about that dataset by using the function rerddap::info().  "rerddapXtracto" also now has simple mapping of the data using the package "plotdap".  The vignette for "rerddapXtracto" is at  https://rmendels.github.io/UsingrerddapXtracto.nb.html.

I have not submitted these packages to CRAN because at the moment they depend  either on packages or versions of packages that are only on Github,  not CRAN.  While I know there are ways around this when submitting to CRAN,  I feel that this defeats a lot of the purpose of CRAN.  CRAN checks consistency of package with other packages,  with development versions of R,  and also notifies developers when packages they use are changed, and provides an uniform installation of compatible packages.  These packages will be submitted to CRAN when the packages they depend on are available.  

The best information on installation is the vignettes.  The quick start version,  the development version of "xtractomatic" can be installed using:

devtools::install_github("rmendels/xtractomatic",  ref = "development")

and "rerddapXtracto" can be installed using:

devtools::install_github("rmendels/rerddapXtracto")

Several warnings about the installations:

1.  Both packages use the package "plotdap" for graphics.  This package at the moment is available only on Github,  at:

https://github.com/ropensci/plotdap

2. "plotdap" itself depends on a fairly large number of packages,  In some testing,  people sometimes had to get a more recent,  non-CRAN version of other packages to have correct functionality.  If you run into problems let me know. 

3. "rerddapXtracto" depends on the package "rerddap",  but not on the CRAN version.  There was some changes in functionality in the Github version, in particular the handling of caches,  as well as some code changes.  To use "rerddapXtracto" you must install the Github version of "rerddap" available from https://github.com/ropensci/rerddap.

Basically the initial installation of either package may not go cleanly,  if there are problems let me know, it may take several attempts to get all the dependencies correct.  As noted earlier,  this is one of the benefits provided by CRAN,  and why I do not want to put these packages on CRAN until all of their dependencies are there also.


-Roy

 


**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new street address***
110 McAllister Way
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From Paul.Egeler at spectrumhealth.org  Fri Dec 22 17:23:42 2017
From: Paul.Egeler at spectrumhealth.org (Paul.Egeler at spectrumhealth.org)
Date: Fri, 22 Dec 2017 16:23:42 +0000
Subject: [R] [R-pkgs] Introducing samplesizeCMH: Power and Sample Size
 Calculation for the Cochran-Mantel-Haenszel Test
Message-ID: <4E3720A8CDD9F941943008CD0BAEF5DA0FC89740@DVMSMBS03.Spectrum-Health.org>

Dear R users,

I am happy to announce the new package `samplesizeCMH` is now available on CRAN <https://cran.r-project.org/package=samplesizeCMH>.

The `samplesizeCMH` package will compute power and sample size for the Cochran-Mantel-Haenszel test for stratified 2x2 tables. There are also several helper functions for working with 2x2 tables, such as converting a proportion into an odds estimate and converting two row/column proportions into an odds ratio estimate.

Please see package vignettes for more information <https://cran.r-project.org/web/packages/samplesizeCMH/vignettes/>. 

Your feedback is welcome. Please feel free to submit issues and suggestions on the package issues page <https://github.com/pegeler/samplesizeCMH/issues>.

Best regards,

Paul Egeler, M.S., GStat
Biostatistician, Sr.
Spectrum Health Offices of Research Administration

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From Victordongy at hotmail.com  Mon Dec 25 10:53:09 2017
From: Victordongy at hotmail.com (Ye Dong)
Date: Mon, 25 Dec 2017 09:53:09 +0000
Subject: [R]   package : plm : pgmm question
Message-ID: <PN1PR01MB07844FF8F4A6748E3DE477B0A1010@PN1PR01MB0784.INDPRD01.PROD.OUTLOOK.COM>

Dear Sir,



I am using the package pgmm you build in panel regression. However, I found that when T is 10, N=30, the error would show as following:



system is computationally singular: reciprocal condition number

But the similar code works well on Stata, so I wonder  how I can optimize the algorithm, for example , the inverse matrix optimization ? And I have checked my data as well, no multicollinearity problem exists. Another problem is that although I have some NA in the panel data, the panel dataframe is still recognized as balanced model. But with plm, the dataframe would be recognized unbalanced.

Thanks and Best regards,

Ye Dong


	[[alternative HTML version deleted]]


From ahmedatia80 at gmail.com  Mon Dec 25 19:21:14 2017
From: ahmedatia80 at gmail.com (Ahmed Attia)
Date: Mon, 25 Dec 2017 16:21:14 -0200
Subject: [R] LSD-test
Message-ID: <CAG6S0O=QHzEqpKScYbg8gFS2Z35J_GrGsr0Hrb7uhtW1vVOCtg@mail.gmail.com>

LSD-test produces error for this code;

code <- as.factor(Rotationdata_R$`Rot/code`)  #factor in the main
Nitrogen <- as.factor(Rotationdata_R$Nitrogen) #factor in the sub
Rep <- as.factor(Rotationdata_R$REP) #blocks
Year <- as.factor(Rotationdata_R$YEAR)  #years

model <- aov(Rotationdata_R$`GY(Mg/ha)`~Rep+code*as.factor(Nitrogen)+Error(Rep/Year/code),data=Rotationdata_R)
summary(model)

LSD.test(y=model,trt="code",console=TRUE)

the error;

Error in as.data.frame.default(x[[i]], optional = TRUE,
stringsAsFactors = stringsAsFactors) :
  cannot coerce class "c("aovlist", "listof")" to a data.frame

What is wrong with this.



Ahmed Attia, Ph.D.
Agronomist & Soil Scientist


From dwinsemius at comcast.net  Mon Dec 25 22:38:35 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 25 Dec 2017 13:38:35 -0800
Subject: [R] LSD-test
In-Reply-To: <CAG6S0O=QHzEqpKScYbg8gFS2Z35J_GrGsr0Hrb7uhtW1vVOCtg@mail.gmail.com>
References: <CAG6S0O=QHzEqpKScYbg8gFS2Z35J_GrGsr0Hrb7uhtW1vVOCtg@mail.gmail.com>
Message-ID: <4315CBDC-CA62-442A-A282-A12AB048D441@comcast.net>


> On Dec 25, 2017, at 10:21 AM, Ahmed Attia <ahmedatia80 at gmail.com> wrote:
> 
> LSD.test

?LSD.test
No documentation for ?LSD.test? in specified packages and libraries:
you could try ???LSD.test?

-- 
David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From ahmedatia80 at gmail.com  Mon Dec 25 23:09:22 2017
From: ahmedatia80 at gmail.com (Ahmed Attia)
Date: Mon, 25 Dec 2017 20:09:22 -0200
Subject: [R] LSD-test
In-Reply-To: <4315CBDC-CA62-442A-A282-A12AB048D441@comcast.net>
References: <CAG6S0O=QHzEqpKScYbg8gFS2Z35J_GrGsr0Hrb7uhtW1vVOCtg@mail.gmail.com>
 <4315CBDC-CA62-442A-A282-A12AB048D441@comcast.net>
Message-ID: <CAG6S0O=_1tk02eYYcf8nXrM11x8sgXm_XR0Cb3Pa2=-Uvr-4nA@mail.gmail.com>

The model should be class aov or lm and my model class is aovlist.
tried tidy from broom library but did not work. To make it class aov,
I had to remove the error term;

model <- aov(Rotationdata_R$`GY(Mg/ha)`~Rep+code*as.factor(Nitrogen),data=Rotationdata_R)
Ahmed Attia, Ph.D.
Agronomist & Soil Scientist






On Mon, Dec 25, 2017 at 7:38 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>
>> On Dec 25, 2017, at 10:21 AM, Ahmed Attia <ahmedatia80 at gmail.com> wrote:
>>
>> LSD.test
>
> ?LSD.test
> No documentation for ?LSD.test? in specified packages and libraries:
> you could try ???LSD.test?
>
> --
> David Winsemius
> Alameda, CA, USA
>
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
>
>
>
>
>


From dwinsemius at comcast.net  Mon Dec 25 23:33:50 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 25 Dec 2017 14:33:50 -0800
Subject: [R] LSD-test
In-Reply-To: <CAG6S0O=_1tk02eYYcf8nXrM11x8sgXm_XR0Cb3Pa2=-Uvr-4nA@mail.gmail.com>
References: <CAG6S0O=QHzEqpKScYbg8gFS2Z35J_GrGsr0Hrb7uhtW1vVOCtg@mail.gmail.com>
 <4315CBDC-CA62-442A-A282-A12AB048D441@comcast.net>
 <CAG6S0O=_1tk02eYYcf8nXrM11x8sgXm_XR0Cb3Pa2=-Uvr-4nA@mail.gmail.com>
Message-ID: <C38CEBF9-5C66-4FD7-8FB3-741ADCE5F98C@comcast.net>


> On Dec 25, 2017, at 2:09 PM, Ahmed Attia <ahmedatia80 at gmail.com> wrote:
> 
> The model should be class aov or lm and my model class is aovlist.
> tried tidy from broom library but did not work. To make it class aov,
> I had to remove the error term;
> 
> model <- aov(Rotationdata_R$`GY(Mg/ha)`~Rep+code*as.factor(Nitrogen),data=Rotationdata_R)

You seemed to have missed my point that LSD.test is not in the packages loaded by default.


> Ahmed Attia, Ph.D.
> Agronomist & Soil Scientist
> 
> 
> 
> 
> 
> 
> On Mon, Dec 25, 2017 at 7:38 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>> 
>>> On Dec 25, 2017, at 10:21 AM, Ahmed Attia <ahmedatia80 at gmail.com> wrote:
>>> 
>>> LSD.test
>> 
>> ?LSD.test
>> No documentation for ?LSD.test? in specified packages and libraries:
>> you could try ???LSD.test?
>> 
>> --
>> David Winsemius
>> Alameda, CA, USA
>> 
>> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
>> 
>> 
>> 
>> 
>> 

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From krcabrer at une.net.co  Mon Dec 25 23:37:21 2017
From: krcabrer at une.net.co (Kenneth Roy Cabrera Torres)
Date: Mon, 25 Dec 2017 17:37:21 -0500
Subject: [R] LSD-test
In-Reply-To: <C38CEBF9-5C66-4FD7-8FB3-741ADCE5F98C@comcast.net>
References: <CAG6S0O=QHzEqpKScYbg8gFS2Z35J_GrGsr0Hrb7uhtW1vVOCtg@mail.gmail.com>
 <4315CBDC-CA62-442A-A282-A12AB048D441@comcast.net>
 <CAG6S0O=_1tk02eYYcf8nXrM11x8sgXm_XR0Cb3Pa2=-Uvr-4nA@mail.gmail.com>
 <C38CEBF9-5C66-4FD7-8FB3-741ADCE5F98C@comcast.net>
Message-ID: <24dd185c-bbf7-887d-28b3-96dd784a0c5b@une.net.co>

Check the agricolae package.


library(agricolae)


El 25/12/17 a las 17:33, David Winsemius escribi?:
>> On Dec 25, 2017, at 2:09 PM, Ahmed Attia <ahmedatia80 at gmail.com> wrote:
>>
>> The model should be class aov or lm and my model class is aovlist.
>> tried tidy from broom library but did not work. To make it class aov,
>> I had to remove the error term;
>>
>> model <- aov(Rotationdata_R$`GY(Mg/ha)`~Rep+code*as.factor(Nitrogen),data=Rotationdata_R)
> You seemed to have missed my point that LSD.test is not in the packages loaded by default.
>
>
>> Ahmed Attia, Ph.D.
>> Agronomist & Soil Scientist
>>
>>
>>
>>
>>
>>
>> On Mon, Dec 25, 2017 at 7:38 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>>> On Dec 25, 2017, at 10:21 AM, Ahmed Attia <ahmedatia80 at gmail.com> wrote:
>>>>
>>>> LSD.test
>>> ?LSD.test
>>> No documentation for ?LSD.test? in specified packages and libraries:
>>> you could try ???LSD.test?
>>>
>>> --
>>> David Winsemius
>>> Alameda, CA, USA
>>>
>>> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
>>>
>>>
>>>
>>>
>>>
> David Winsemius
> Alameda, CA, USA
>
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tring at gvdnet.dk  Mon Dec 25 19:12:20 2017
From: tring at gvdnet.dk (Troels Ring)
Date: Mon, 25 Dec 2017 19:12:20 +0100
Subject: [R] plot representation of calculated value known to be 7.4
Message-ID: <7cf66763-8ce1-b571-cbe7-8bf808cc6a75@gvdnet.dk>

Dear friends - merry Christmas and thanks a lot for much help during the 
year!

In the example below I fail to understand how the calculated value pH is 
represented in a simple plot - also included. The calculations are 
useful in practice and likely to be right in principle but I cannot see 
how this occurs: why a calculated value of 7.4 known as numeric is not 
simply plotted as such. It happened on Windows both 7 and 10 with R 
version 3.4.1.

All best wishes

Troels

ff <- function(H,SID,ATOT,ka)? H + SID - kw/H - ka*ATOT/(H+ka)
 ?ka <- 1e-7
 ?kw <- 1e-14
 ?ATOT <- seq(0,0.3,length=100)*1e-3

 ?for (i in 1:length(ATOT))? {
 ?SID[i] <- uniroot(ff,c(-1,1),tol=.Machine$double.eps,maxiter=100000,ka=ka,
ATOT=ATOT[i], H = 10^-7.4)$root}
ATOT

#confirm pH 0 7.4

H <- c()
 ?for (i in 1:length(ATOT))? {
 ?H[i] <- 
uniroot(ff,c(1e-19,1),tol=.Machine$double.eps,maxiter=100000,ka=ka,
ATOT=ATOT[i], SID = SID[i])$root}

(pH <- -log10(H))
plot(pH)
str(pH)
# num [1:100] 7.4 7.4 7.4 7.4 7.4 ...
z <- rep(7.4,length(ATOT))
all.equal(z,pH)
#TRUE
points(z,col="red")


From tring at gvdnet.dk  Mon Dec 25 20:00:09 2017
From: tring at gvdnet.dk (Troels Ring)
Date: Mon, 25 Dec 2017 20:00:09 +0100
Subject: [R] plot representation of calculated value known to be 7.4
In-Reply-To: <7cf66763-8ce1-b571-cbe7-8bf808cc6a75@gvdnet.dk>
References: <7cf66763-8ce1-b571-cbe7-8bf808cc6a75@gvdnet.dk>
Message-ID: <7d849cbd-cea4-66ad-0b68-80affcc5e701@gvdnet.dk>

Dear friends - copy paste missed

SID <- c() before the first loop - sorry

BW Troels


Den 25-12-2017 kl. 19:12 skrev Troels Ring:
>
> Dear friends - merry Christmas and thanks a lot for much help during 
> the year!
>
> In the example below I fail to understand how the calculated value pH 
> is represented in a simple plot - also included. The calculations are 
> useful in practice and likely to be right in principle but I cannot 
> see how this occurs: why a calculated value of 7.4 known as numeric is 
> not simply plotted as such. It happened on Windows both 7 and 10 with 
> R version 3.4.1.
>
> All best wishes
>
> Troels
>
> ff <- function(H,SID,ATOT,ka)? H + SID - kw/H - ka*ATOT/(H+ka)
> ?ka <- 1e-7
> ?kw <- 1e-14
> ?ATOT <- seq(0,0.3,length=100)*1e-3
>
> ?for (i in 1:length(ATOT))? {
> ?SID[i] <- 
> uniroot(ff,c(-1,1),tol=.Machine$double.eps,maxiter=100000,ka=ka,
> ATOT=ATOT[i], H = 10^-7.4)$root}
> ATOT
>
> #confirm pH 0 7.4
>
> H <- c()
> ?for (i in 1:length(ATOT))? {
> ?H[i] <- 
> uniroot(ff,c(1e-19,1),tol=.Machine$double.eps,maxiter=100000,ka=ka,
> ATOT=ATOT[i], SID = SID[i])$root}
>
> (pH <- -log10(H))
> plot(pH)
> str(pH)
> # num [1:100] 7.4 7.4 7.4 7.4 7.4 ...
> z <- rep(7.4,length(ATOT))
> all.equal(z,pH)
> #TRUE
> points(z,col="red")
>


From bgunter.4567 at gmail.com  Tue Dec 26 01:03:58 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 25 Dec 2017 16:03:58 -0800
Subject: [R] plot representation of calculated value known to be 7.4
In-Reply-To: <7d849cbd-cea4-66ad-0b68-80affcc5e701@gvdnet.dk>
References: <7cf66763-8ce1-b571-cbe7-8bf808cc6a75@gvdnet.dk>
 <7d849cbd-cea4-66ad-0b68-80affcc5e701@gvdnet.dk>
Message-ID: <CAGxFJbRgdHKGnUztUqkNknsodE1_YvCUzHxoY0ynsmpONEz1uw@mail.gmail.com>

Note that ?all.equal clearly says that it tests for **approximate equality
only** with tolerance "close to 1.5 e-8.

So..

> all.equal(z,pH, tol = 1e-15)
[1] "Mean relative difference: 6.732527e-11"

and

> print(pH, digits =15)
## output omitted

Shows you what's going on.

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Mon, Dec 25, 2017 at 11:00 AM, Troels Ring <tring at gvdnet.dk> wrote:

> Dear friends - copy paste missed
>
> SID <- c() before the first loop - sorry
>
> BW Troels
>
>
>
> Den 25-12-2017 kl. 19:12 skrev Troels Ring:
>
>>
>> Dear friends - merry Christmas and thanks a lot for much help during the
>> year!
>>
>> In the example below I fail to understand how the calculated value pH is
>> represented in a simple plot - also included. The calculations are useful
>> in practice and likely to be right in principle but I cannot see how this
>> occurs: why a calculated value of 7.4 known as numeric is not simply
>> plotted as such. It happened on Windows both 7 and 10 with R version 3.4.1.
>>
>> All best wishes
>>
>> Troels
>>
>> ff <- function(H,SID,ATOT,ka)  H + SID - kw/H - ka*ATOT/(H+ka)
>>  ka <- 1e-7
>>  kw <- 1e-14
>>  ATOT <- seq(0,0.3,length=100)*1e-3
>>
>>  for (i in 1:length(ATOT))  {
>>  SID[i] <- uniroot(ff,c(-1,1),tol=.Machine$double.eps,maxiter=100000,
>> ka=ka,
>> ATOT=ATOT[i], H = 10^-7.4)$root}
>> ATOT
>>
>> #confirm pH 0 7.4
>>
>> H <- c()
>>  for (i in 1:length(ATOT))  {
>>  H[i] <- uniroot(ff,c(1e-19,1),tol=.Machine$double.eps,maxiter=100000
>> ,ka=ka,
>> ATOT=ATOT[i], SID = SID[i])$root}
>>
>> (pH <- -log10(H))
>> plot(pH)
>> str(pH)
>> # num [1:100] 7.4 7.4 7.4 7.4 7.4 ...
>> z <- rep(7.4,length(ATOT))
>> all.equal(z,pH)
>> #TRUE
>> points(z,col="red")
>>
>>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From aglowka at stanford.edu  Tue Dec 26 03:36:55 2017
From: aglowka at stanford.edu (=?UTF-8?Q?Aleksander_G=c5=82=c3=b3wka?=)
Date: Mon, 25 Dec 2017 18:36:55 -0800
Subject: [R] identifying convergence or non-convergence of mixed-effects
 regression model in lme4 from model output
Message-ID: <3bddd6b3-a09d-d90b-4a0e-dddb6b0e11c1@stanford.edu>

Hi R community!

I've fitted three mixed-effects regression models to a thousand 
bootstrap samples (case-resampling regression) using the lme4 package in 
a custom-built for-loop. The only output I saved were the inferential 
statistics for my fixed and random effects. I did not save any output 
related to the performance to the machine learning algorithm used to fit 
the models (REML=FALSE). After running all the simulations, I got about 
two dozen messages of this kind:

25: In checkConv(attr(opt, "derivs"), opt$par, ctrl = 
control$checkConv,? ... :
 ? Model failed to converge with max|grad| = 4.49732 (tol = 0.002, 
component 1)
26: In checkConv(attr(opt, "derivs"), opt$par, ctrl = 
control$checkConv,? ... :
 ? Model is nearly unidentifiable: large eigenvalue ratio
 ?- Rescale variables?

Since I only get the error messages after all the computations have been 
executed, looking at these error messages is not helpful, because, as 
you can see, they don't allow me to identify which bootstrap sample the 
non-converged model was fit to they are referring to. Since I don't 
think I can recover this information from the already run simulations, I 
need to modify my information extractor code to include convergence 
information and rerun the simulations.

My question is the following: which attribute in the summary of a 
mixed-effects model in lme4 allows me to check if the model has 
converged or not? What value would the parameter corresponding to that 
attribute have to have in order for me to conclude the model has not 
converged?

Here are my current extractor functions for fixed and random effects.

lmer.data.extract = function(lmer.mod, name=deparse(substitute(lmer.mod))){

 ? #extract predictor names & create data frame, attach other cols to 
new data frame
 ? mod.data = data.frame(summary(lmer.mod)$coefficients[,1])
 ? names(mod.data) = "estimate"
 ? mod.data$std.error = as.numeric(summary(lmer.mod)$coefficients[,2]) 
#std errors
 ? mod.data$df = as.numeric(summary(lmer.mod)$coefficients[,3]) #degrees 
of freedom
 ? mod.data$t.val = as.numeric(summary(lmer.mod)$coefficients[,4]) #t-values
 ? mod.data$p.val = as.numeric(summary(lmer.mod)$coefficients[,5]) 
#p-values

 ? #extract AIC, BIC, logLik, deviance df.resid
 ? mod.data$AIC = as.numeric(summary(lmer.mod)$AIC[1])
 ? mod.data$BIC = as.numeric(summary(lmer.mod)$AICtab[2][1])
 ? mod.data$logLik = as.numeric(summary(lmer.mod)$AICtab[3][1])
 ? mod.data$deviance = as.numeric(summary(lmer.mod)$AICtab[4][1])
 ? mod.data$df.resid = as.numeric(summary(lmer.mod)$AICtab[5][1])

 ? #add number of datapoints
 ? mod.data$N = as.numeric(summary(incr.best.m)$devcomp$dims[1])

 ? #add model name
 ? mod.data$model = name

 ? return(mod.data)

}

lmer.ranef.data.extract = function(lmer.mod, 
name=deparse(substitute(lmer.mod))){

 ? #extract random effect variance, standard error, correlations between 
slope and intercept
 ? mod.data.ef = as.data.frame(VarCorr(lmer.mod))

 ? mod.data.ef$n.subj = as.numeric(summary(lmer.mod)$ngrps[1]) #number 
of subjects
 ? mod.data.ef$n.item = as.numeric(summary(lmer.mod)$ngrps[2]) #number 
of items

 ? #add number of datapoints
 ? mod.data.ef$N = as.numeric(summary(incr.best.m)$devcomp$dims[1])

 ? #add model name
 ? mod.data.ef$model = name

 ? return(mod.data.ef)

}

I'm also including the structure of an example model that did converge 
(but I can I tell from the output?).

List of 18
 ?$ methTitle?? : chr "Linear mixed model fit by maximum likelihood? 
\nt-tests use? Satterthwaite approximations to degrees of freedom"
 ?$ objClass??? : atomic [1:1] lmerMod
 ? ..- attr(*, "package")= chr "lme4"
 ?$ devcomp???? :List of 2
 ? ..$ cmp : Named num [1:10] 176.85 59.09 95.43 3.84 99.27 ...
 ? .. ..- attr(*, "names")= chr [1:10] "ldL2" "ldRX2" "wrss" "ussq" ...
 ? ..$ dims: Named int [1:12] 1742 1742 10 1732 94 4 1 2 0 0 ...
 ? .. ..- attr(*, "names")= chr [1:12] "N" "n" "p" "nmp" ...
 ?$ isLmer????? : logi TRUE
 ?$ useScale??? : logi TRUE
 ?$ logLik????? :Class 'logLik' : -65 (df=15)
 ?$ family????? : NULL
 ?$ link??????? : NULL
 ?$ ngrps?????? : Named num [1:2] 36 29
 ? ..- attr(*, "names")= chr [1:2] "subj" "item"
 ?$ coefficients: num [1:10, 1:5] 7.00546 0.04234 -0.00258 0.09094 
-0.00804 ...
 ? ..- attr(*, "dimnames")=List of 2
 ? .. ..$ : chr [1:10] "(Intercept)" "FreqABCD.log.std" 
"LogitABCD.neg.log.std" "MIABCD.neg.log.std" ...
 ? .. ..$ : chr [1:5] "Estimate" "Std. Error" "df" "t value" ...
 ?$ sigma?????? : num 0.239
 ?$ vcov??????? :Formal class 'dpoMatrix' [package "Matrix"] with 5 slots
 ? .. ..@ x?????? : num [1:100] 1.15e-03 3.40e-05 -5.12e-05 2.18e-05 
3.65e-06 ...
 ? .. ..@ Dim???? : int [1:2] 10 10
 ? .. ..@ Dimnames:List of 2
 ? .. .. ..$ : chr [1:10] "(Intercept)" "FreqABCD.log.std" 
"LogitABCD.neg.log.std" "MIABCD.neg.log.std" ...
 ? .. .. ..$ : chr [1:10] "(Intercept)" "FreqABCD.log.std" 
"LogitABCD.neg.log.std" "MIABCD.neg.log.std" ...
 ? .. ..@ uplo??? : chr "U"
 ? .. ..@ factors :List of 1
 ? .. .. ..$ correlation:Formal class 'corMatrix' [package "Matrix"] 
with 6 slots
 ? .. .. .. .. ..@ sd????? : num [1:10] 0.0339 0.0519 0.013 0.0439 
0.0068 ...
 ? .. .. .. .. ..@ x?????? : num [1:100] 1 0.0194 -0.1162 0.0147 0.0158 ...
 ? .. .. .. .. ..@ Dim???? : int [1:2] 10 10
 ? .. .. .. .. ..@ Dimnames:List of 2
 ? .. .. .. .. .. ..$ : chr [1:10] "(Intercept)" "FreqABCD.log.std" 
"LogitABCD.neg.log.std" "MIABCD.neg.log.std" ...
 ? .. .. .. .. .. ..$ : chr [1:10] "(Intercept)" "FreqABCD.log.std" 
"LogitABCD.neg.log.std" "MIABCD.neg.log.std" ...
 ? .. .. .. .. ..@ uplo??? : chr "U"
 ? .. .. .. .. ..@ factors :List of 1
 ? .. .. .. .. .. ..$ Cholesky:Formal class 'Cholesky' [package 
"Matrix"] with 5 slots
 ? .. .. .. .. .. .. .. ..@ x?????? : num [1:100] 1 0 0 0 0 0 0 0 0 0 ...
 ? .. .. .. .. .. .. .. ..@ Dim???? : int [1:2] 10 10
 ? .. .. .. .. .. .. .. ..@ Dimnames:List of 2
 ? .. .. .. .. .. .. .. .. ..$ : NULL
 ? .. .. .. .. .. .. .. .. ..$ : NULL
 ? .. .. .. .. .. .. .. ..@ uplo??? : chr "U"
 ? .. .. .. .. .. .. .. ..@ diag??? : chr "N"
 ?$ varcor????? :List of 2
 ? ..$ subj: num [1, 1] 0.0273
 ? .. ..- attr(*, "dimnames")=List of 2
 ? .. .. ..$ : chr "(Intercept)"
 ? .. .. ..$ : chr "(Intercept)"
 ? .. ..- attr(*, "stddev")= Named num 0.165
 ? .. .. ..- attr(*, "names")= chr "(Intercept)"
 ? .. ..- attr(*, "correlation")= num [1, 1] 1
 ? .. .. ..- attr(*, "dimnames")=List of 2
 ? .. .. .. ..$ : chr "(Intercept)"
 ? .. .. .. ..$ : chr "(Intercept)"
 ? ..$ item: num [1:2, 1:2] 0.00417 0.000484 0.000484 0.00289
 ? .. ..- attr(*, "dimnames")=List of 2
 ? .. .. ..$ : chr [1:2] "(Intercept)" "FreqABCD.log.std"
 ? .. .. ..$ : chr [1:2] "(Intercept)" "FreqABCD.log.std"
 ? .. ..- attr(*, "stddev")= Named num [1:2] 0.0646 0.0538
 ? .. .. ..- attr(*, "names")= chr [1:2] "(Intercept)" "FreqABCD.log.std"
 ? .. ..- attr(*, "correlation")= num [1:2, 1:2] 1 0.139 0.139 1
 ? .. .. ..- attr(*, "dimnames")=List of 2
 ? .. .. .. ..$ : chr [1:2] "(Intercept)" "FreqABCD.log.std"
 ? .. .. .. ..$ : chr [1:2] "(Intercept)" "FreqABCD.log.std"
 ? ..- attr(*, "sc")= num 0.239
 ? ..- attr(*, "useSc")= logi TRUE
 ? ..- attr(*, "class")= chr "VarCorr.merMod"
 ?$ AICtab????? : Named num [1:5] 159.7 241.6 -64.8 129.7 1727
 ? ..- attr(*, "names")= chr [1:5] "AIC" "BIC" "logLik" "deviance" ...
 ?$ call??????? : language lme4::lmer(formula = RT.log ~ 
FreqABCD.log.std + LogitABCD.neg.log.std + MIABCD.neg.log.std + 
AS.data$freq.sub.PC1 +????? AS.data$freq.sub.PC2 + AS.data$freq.sub.PC3 
+ AS.data$freq.sub.PC4 + block + nletter.std + (1 | subj) +? ...
 ?$ residuals?? : Named num [1:1742] 0.713 0.498 -0.361 -0.101 2.594 ...
 ? ..- attr(*, "names")= chr [1:1742] "1" "2" "3" "4" ...
 ?$ fitMsgs???? : chr(0)
 ?$ optinfo???? :List of 7
 ? ..$ optimizer: chr "bobyqa"
 ? ..$ control? :List of 1
 ? .. ..$ iprint: int 0
 ? ..$ derivs?? :List of 2
 ? .. ..$ gradient: num [1:4] 9.81e-06 -5.34e-06 -1.60e-05 7.06e-05
 ? .. ..$ Hessian : num [1:4, 1:4] 245.9 28.5 3.3 -13.7 28.5 ...
 ? ..$ conv???? :List of 2
 ? .. ..$ opt : int 0
 ? .. ..$ lme4: list()
 ? ..$ feval??? : int 107
 ? ..$ warnings : list()
 ? ..$ val????? : num [1:4] 0.6919 0.2705 0.0314 0.223
 ?- attr(*, "class")= chr "summary.merMod"

I'd appreciate any advice you may have!

Thank you,

Aleksander G??wka
PhD Candidate
Department of Linguistics
Stanford University
**

	[[alternative HTML version deleted]]


From tring at gvdnet.dk  Tue Dec 26 08:49:50 2017
From: tring at gvdnet.dk (Troels Ring)
Date: Tue, 26 Dec 2017 08:49:50 +0100
Subject: [R] plot representation of calculated value known to be 7.4
In-Reply-To: <7d849cbd-cea4-66ad-0b68-80affcc5e701@gvdnet.dk>
References: <7cf66763-8ce1-b571-cbe7-8bf808cc6a75@gvdnet.dk>
 <7d849cbd-cea4-66ad-0b68-80affcc5e701@gvdnet.dk>
Message-ID: <8199b100-c693-0895-5f8e-aa83815e2842@gvdnet.dk>

Thanks a lot - formatting the ordinate as ylim=c(4,10) before plotting 
pH also removed the problem, and options(digits=10) confirmed that pH 
was not all exactly 7.4 - as I knew. Still I wonder just why R chooses 
to plot(ATOT,pH) as shown with repeated "7.4" instead of some more 
detailed representation. Thanks a gain and happy New Year!

Troels


Den 26-12-2017 kl. 01:03 skrev Bert Gunter:
> Note that ?all.equal clearly says that it tests for **approximate 
> equality only** with tolerance "close to 1.5 e-8.
>
> So..
>
> > all.equal(z,pH, tol = 1e-15)
> [1] "Mean relative difference: 6.732527e-11"
>
> and
>
> > print(pH, digits =15)
> ## output omitted
>
> Shows you what's going on.
>
> Cheers,
> Bert
>
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along 
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
> On Mon, Dec 25, 2017 at 11:00 AM, Troels Ring <tring at gvdnet.dk 
> <mailto:tring at gvdnet.dk>> wrote:
>
>     Dear friends - copy paste missed
>
>     SID <- c() before the first loop - sorry
>
>     BW Troels
>
>
>
>     Den 25-12-2017 kl. 19:12 skrev Troels Ring:
>
>
>         Dear friends - merry Christmas and thanks a lot for much help
>         during the year!
>
>         In the example below I fail to understand how the calculated
>         value pH is represented in a simple plot - also included. The
>         calculations are useful in practice and likely to be right in
>         principle but I cannot see how this occurs: why a calculated
>         value of 7.4 known as numeric is not simply plotted as such.
>         It happened on Windows both 7 and 10 with R version 3.4.1.
>
>         All best wishes
>
>         Troels
>
>         ff <- function(H,SID,ATOT,ka)? H + SID - kw/H - ka*ATOT/(H+ka)
>         ?ka <- 1e-7
>         ?kw <- 1e-14
>         ?ATOT <- seq(0,0.3,length=100)*1e-3
>
>         ?for (i in 1:length(ATOT))? {
>         ?SID[i] <-
>         uniroot(ff,c(-1,1),tol=.Machine$double.eps,maxiter=100000,ka=ka,
>         ATOT=ATOT[i], H = 10^-7.4)$root}
>         ATOT
>
>         #confirm pH 0 7.4
>
>         H <- c()
>         ?for (i in 1:length(ATOT))? {
>         ?H[i] <-
>         uniroot(ff,c(1e-19,1),tol=.Machine$double.eps,maxiter=100000,ka=ka,
>         ATOT=ATOT[i], SID = SID[i])$root}
>
>         (pH <- -log10(H))
>         plot(pH)
>         str(pH)
>         # num [1:100] 7.4 7.4 7.4 7.4 7.4 ...
>         z <- rep(7.4,length(ATOT))
>         all.equal(z,pH)
>         #TRUE
>         points(z,col="red")
>
>
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>     and provide commented, minimal, self-contained, reproducible code.
>
>


	[[alternative HTML version deleted]]


From karagullemre at gmail.com  Tue Dec 26 09:36:01 2017
From: karagullemre at gmail.com (=?utf-8?Q?Emre_Karag=C3=BClle?=)
Date: Tue, 26 Dec 2017 11:36:01 +0300
Subject: [R] Time Series with Neural Networks
Message-ID: <5a4209e6.429a500a.5852b.942c@mx.google.com>

Hi,
I am would like to ask few questions. 
I am trying to forecast  hourly electricity prices by 24 hours ahead.
I have hourly data starting from 2015*12*18 to 2017-10-24
and I have defined the data as time series as written in the code below.

Then I am trying do neural network with 23 non-seasonal dummies and 1 seasonal dummy.
But I don?t know whether training set is enough.( Guess it is 50 hours in here?)

The problem is that I couldn?t 24 for output here. How can I make such forecast?
 And my MASE score (6.95 in the Test set) is not good. Could be related to shortness of training set?

The Code:

library(zoo)
library(readxl)
setwd("C:/Users/emrek/Dropbox/2017-2018 Master Thesis/DATA")
epias <- read_excel("eski.epias.xlsx")


nPTF <- epias$`PTF (TL/MWh)`
nSMF<- epias$`SMF(TL/MWh)`
nC<- epias$`TT(MWh)`
nEAK<- epias$`EAK-Toplam (MWh)`
nTP<- epias$`Toplam (MWh)`

times     <- seq(from=as.POSIXct("2015-12-18 00:00:00"), to=as.POSIXct("2017-10-24 23:00:00"), by="hour")
mydata <- rnorm(length(times))

PTF <- zoo(nPTF, order.by=times )
SMF <- zoo(nSMF, order.by=times )
C <- zoo(nC, order.by=times )
EAK <- zoo(nEAK, order.by=times )
TP<- zoo(nTP, order.by=times )
SH <- (EAK-TP)

epias <- cbind(PTF,C,SH)
View(epias)

#neural networks
library(forecast)
set.seed(201)
epias.nn <- nnetar(PTF, repeats = 50, p=23, P=1, size =12)
summary(epias.nn$model[[1]])

epias.pred <- forecast(epias.nn, h= 24)
accuracy(epias.pred, 24)

plot(PTF, ylim=c(0,500) , ylab=  , xlab= , bty="l", xaxt="n", xlim=c(as.POSIXct("2017-10-20 00:00:00"),as.POSIXct("2017-10-25 23:00:00")) , lty=1 )

lines(epias.pred$fitted,lwd = 2,col="blue")


Best Regards,
--
Emre


	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Dec 26 16:35:39 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 26 Dec 2017 07:35:39 -0800
Subject: [R] plot representation of calculated value known to be 7.4
In-Reply-To: <8199b100-c693-0895-5f8e-aa83815e2842@gvdnet.dk>
References: <7cf66763-8ce1-b571-cbe7-8bf808cc6a75@gvdnet.dk>
 <7d849cbd-cea4-66ad-0b68-80affcc5e701@gvdnet.dk>
 <8199b100-c693-0895-5f8e-aa83815e2842@gvdnet.dk>
Message-ID: <CAGxFJbSgSyYYzwT2i0CEXDzsKNORUO4hOtiiKg_oBUvxqin1YQ@mail.gmail.com>

Inline.

-- Bert

> Thanks a lot - formatting the ordinate as ylim=c(4,10) before plotting pH also removed the problem, and options(digits=10) confirmed that pH was not all exactly 7.4 - as I knew. Still I wonder just why R chooses to plot(ATOT,pH) as shown with repeated "7.4" instead of some more detailed representation. Thanks a gain and happy New Year!
--------------------
There is no need to wonder if you just follow the docs in detail:

Your first stop should be ?plot.default. If you read carefully through
it, you will find in the examples:

##--- Log-Log Plot  with  custom axes

In this example, you will find the use of the axis() function to
create the custom axes. So this suggests, to me anyway, that ?axis
should be the next port of call.

?axis reveals that axis() has a "labels" argument. The details section
of the docs say:

"If labels is not specified, the numeric values supplied or calculated
for at are converted to character strings as if they were a numeric
vector printed by print.default(digits = 7)."

So -- aha! -- that looks like the explanation you sought. And, indeed,
checking ?print.default, it says in "Details":

"The same number of decimal places is used throughout a vector. This
means that digits specifies the minimum number of significant digits
to be used, and that at least one entry will be encoded with that
minimum number. However, if all the encoded elements then have
trailing zeroes, the number of decimal places is reduced until at
least one element has a non-zero final digit."

So, yes, it's a bit of a "Long Day's Journey...," but if you expend
the effort to follow it all through carefully, especially for R's base
functionality (it gets much more chancy with user-contributed
packages, of course) it almost aways **is** all there.

Cheers,
Bert





>
> Troels
>
>
> Den 26-12-2017 kl. 01:03 skrev Bert Gunter:
>
> Note that ?all.equal clearly says that it tests for **approximate equality only** with tolerance "close to 1.5 e-8.
>
> So..
>
> > all.equal(z,pH, tol = 1e-15)
> [1] "Mean relative difference: 6.732527e-11"
>
> and
>
> > print(pH, digits =15)
> ## output omitted
>
> Shows you what's going on.
>
> Cheers,
> Bert
>
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
> On Mon, Dec 25, 2017 at 11:00 AM, Troels Ring <tring at gvdnet.dk> wrote:
>>
>> Dear friends - copy paste missed
>>
>> SID <- c() before the first loop - sorry
>>
>> BW Troels
>>
>>
>>
>> Den 25-12-2017 kl. 19:12 skrev Troels Ring:
>>>
>>>
>>> Dear friends - merry Christmas and thanks a lot for much help during the year!
>>>
>>> In the example below I fail to understand how the calculated value pH is represented in a simple plot - also included. The calculations are useful in practice and likely to be right in principle but I cannot see how this occurs: why a calculated value of 7.4 known as numeric is not simply plotted as such. It happened on Windows both 7 and 10 with R version 3.4.1.
>>>
>>> All best wishes
>>>
>>> Troels
>>>
>>> ff <- function(H,SID,ATOT,ka)  H + SID - kw/H - ka*ATOT/(H+ka)
>>>  ka <- 1e-7
>>>  kw <- 1e-14
>>>  ATOT <- seq(0,0.3,length=100)*1e-3
>>>
>>>  for (i in 1:length(ATOT))  {
>>>  SID[i] <- uniroot(ff,c(-1,1),tol=.Machine$double.eps,maxiter=100000,ka=ka,
>>> ATOT=ATOT[i], H = 10^-7.4)$root}
>>> ATOT
>>>
>>> #confirm pH 0 7.4
>>>
>>> H <- c()
>>>  for (i in 1:length(ATOT))  {
>>>  H[i] <- uniroot(ff,c(1e-19,1),tol=.Machine$double.eps,maxiter=100000,ka=ka,
>>> ATOT=ATOT[i], SID = SID[i])$root}
>>>
>>> (pH <- -log10(H))
>>> plot(pH)
>>> str(pH)
>>> # num [1:100] 7.4 7.4 7.4 7.4 7.4 ...
>>> z <- rep(7.4,length(ATOT))
>>> all.equal(z,pH)
>>> #TRUE
>>> points(z,col="red")
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From paulbernal07 at gmail.com  Tue Dec 26 17:27:52 2017
From: paulbernal07 at gmail.com (Paul Bernal)
Date: Tue, 26 Dec 2017 11:27:52 -0500
Subject: [R] Error when trying to install package rggobi
Message-ID: <CAMOcQfO1UScj_6NDxREpcG7oXmpLTt3Skcxe_e777id07qdLug@mail.gmail.com>

Dear friends,

I am currently using R version 3.4.2 (64-bit for Windows) and when trying
to download package rggobi an error pops up with the following message:

"The program can?t start because libxml2-2.dll is missing from your
computer. Try reinstalling the program to fix this problem"

Is this suggesting me to reinstall R or is there a way to get the
libxml2-2.dll somewhere to solve this issue?

Best regards,

Paul

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Tue Dec 26 18:13:32 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 26 Dec 2017 09:13:32 -0800
Subject: [R] Error when trying to install package rggobi
In-Reply-To: <CAMOcQfO1UScj_6NDxREpcG7oXmpLTt3Skcxe_e777id07qdLug@mail.gmail.com>
References: <CAMOcQfO1UScj_6NDxREpcG7oXmpLTt3Skcxe_e777id07qdLug@mail.gmail.com>
Message-ID: <A4EE1427-14B9-442C-BCA3-240098FEA438@dcn.davis.ca.us>

Have you read [1]? Specifically the SystemRequirements and URL fields? If you have installed GGobi previously, perhaps you need to do it again?

[1] https://cran.r-project.org/web/packages/rggobi/index.html
-- 
Sent from my phone. Please excuse my brevity.

On December 26, 2017 8:27:52 AM PST, Paul Bernal <paulbernal07 at gmail.com> wrote:
>Dear friends,
>
>I am currently using R version 3.4.2 (64-bit for Windows) and when
>trying
>to download package rggobi an error pops up with the following message:
>
>"The program can?t start because libxml2-2.dll is missing from your
>computer. Try reinstalling the program to fix this problem"
>
>Is this suggesting me to reinstall R or is there a way to get the
>libxml2-2.dll somewhere to solve this issue?
>
>Best regards,
>
>Paul
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From tring at gvdnet.dk  Tue Dec 26 18:31:10 2017
From: tring at gvdnet.dk (Troels Ring)
Date: Tue, 26 Dec 2017 18:31:10 +0100
Subject: [R] plot representation of calculated value known to be 7.4
In-Reply-To: <8199b100-c693-0895-5f8e-aa83815e2842@gvdnet.dk>
References: <7cf66763-8ce1-b571-cbe7-8bf808cc6a75@gvdnet.dk>
 <7d849cbd-cea4-66ad-0b68-80affcc5e701@gvdnet.dk>
 <8199b100-c693-0895-5f8e-aa83815e2842@gvdnet.dk>
Message-ID: <4bcab132-24b9-5cce-9d41-f730d9cae01c@gvdnet.dk>

Thanks a lot -- yes it is amazing how much coherent detail has been put 
into this and kept integrated :-)

Best wishes

Troels



Den 26-12-2017 kl. 16:35 skrev Bert Gunter:
> Inline.
>
> -- Bert
>
>> Thanks a lot - formatting the ordinate as ylim=c(4,10) before plotting pH also removed the problem, and options(digits=10) confirmed that pH was not all exactly 7.4 - as I knew. Still I wonder just why R chooses to plot(ATOT,pH) as shown with repeated "7.4" instead of some more detailed representation. Thanks a gain and happy New Year!
> --------------------
> There is no need to wonder if you just follow the docs in detail:
>
> Your first stop should be ?plot.default. If you read carefully through
> it, you will find in the examples:
>
> ##--- Log-Log Plot  with  custom axes
>
> In this example, you will find the use of the axis() function to
> create the custom axes. So this suggests, to me anyway, that ?axis
> should be the next port of call.
>
> ?axis reveals that axis() has a "labels" argument. The details section
> of the docs say:
>
> "If labels is not specified, the numeric values supplied or calculated
> for at are converted to character strings as if they were a numeric
> vector printed by print.default(digits = 7)."
>
> So -- aha! -- that looks like the explanation you sought. And, indeed,
> checking ?print.default, it says in "Details":
>
> "The same number of decimal places is used throughout a vector. This
> means that digits specifies the minimum number of significant digits
> to be used, and that at least one entry will be encoded with that
> minimum number. However, if all the encoded elements then have
> trailing zeroes, the number of decimal places is reduced until at
> least one element has a non-zero final digit."
>
> So, yes, it's a bit of a "Long Day's Journey...," but if you expend
> the effort to follow it all through carefully, especially for R's base
> functionality (it gets much more chancy with user-contributed
> packages, of course) it almost aways **is** all there.
>
> Cheers,
> Bert
>
>
>
>
>
>> Troels
>>
>>
>> Den 26-12-2017 kl. 01:03 skrev Bert Gunter:
>>
>> Note that ?all.equal clearly says that it tests for **approximate equality only** with tolerance "close to 1.5 e-8.
>>
>> So..
>>
>>> all.equal(z,pH, tol = 1e-15)
>> [1] "Mean relative difference: 6.732527e-11"
>>
>> and
>>
>>> print(pH, digits =15)
>> ## output omitted
>>
>> Shows you what's going on.
>>
>> Cheers,
>> Bert
>>
>>
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>> On Mon, Dec 25, 2017 at 11:00 AM, Troels Ring <tring at gvdnet.dk> wrote:
>>> Dear friends - copy paste missed
>>>
>>> SID <- c() before the first loop - sorry
>>>
>>> BW Troels
>>>
>>>
>>>
>>> Den 25-12-2017 kl. 19:12 skrev Troels Ring:
>>>>
>>>> Dear friends - merry Christmas and thanks a lot for much help during the year!
>>>>
>>>> In the example below I fail to understand how the calculated value pH is represented in a simple plot - also included. The calculations are useful in practice and likely to be right in principle but I cannot see how this occurs: why a calculated value of 7.4 known as numeric is not simply plotted as such. It happened on Windows both 7 and 10 with R version 3.4.1.
>>>>
>>>> All best wishes
>>>>
>>>> Troels
>>>>
>>>> ff <- function(H,SID,ATOT,ka)  H + SID - kw/H - ka*ATOT/(H+ka)
>>>>   ka <- 1e-7
>>>>   kw <- 1e-14
>>>>   ATOT <- seq(0,0.3,length=100)*1e-3
>>>>
>>>>   for (i in 1:length(ATOT))  {
>>>>   SID[i] <- uniroot(ff,c(-1,1),tol=.Machine$double.eps,maxiter=100000,ka=ka,
>>>> ATOT=ATOT[i], H = 10^-7.4)$root}
>>>> ATOT
>>>>
>>>> #confirm pH 0 7.4
>>>>
>>>> H <- c()
>>>>   for (i in 1:length(ATOT))  {
>>>>   H[i] <- uniroot(ff,c(1e-19,1),tol=.Machine$double.eps,maxiter=100000,ka=ka,
>>>> ATOT=ATOT[i], SID = SID[i])$root}
>>>>
>>>> (pH <- -log10(H))
>>>> plot(pH)
>>>> str(pH)
>>>> # num [1:100] 7.4 7.4 7.4 7.4 7.4 ...
>>>> z <- rep(7.4,length(ATOT))
>>>> all.equal(z,pH)
>>>> #TRUE
>>>> points(z,col="red")
>>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From csrabak at bol.com.br  Tue Dec 26 18:54:14 2017
From: csrabak at bol.com.br (csrabak at bol.com.br)
Date: Tue, 26 Dec 2017 15:54:14 -0200
Subject: [R] Unexpected behaviour of windowsFonts() when Rdevga is edited
Message-ID: <5a428cc6da7a2_46d13fdc6a7f11343484e@a4-winter19.mail>

Hi folks,
?

Running R in a Windows machine:
?
> sessionInfo()
R version 3.4.2 (2017-09-28)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 8.1 x64 (build 9600)
?
Matrix products: default
?
locale:
[1] LC_COLLATE=Portuguese_Brazil.1252? LC_CTYPE=Portuguese_Brazil.1252? ?
[3] LC_MONETARY=Portuguese_Brazil.1252 LC_NUMERIC=C? ? ? ? ? ? ? ? ? ? ??
[5] LC_TIME=Portuguese_Brazil.1252? ??
?
attached base packages:
[1] stats? ? ?graphics? grDevices utils? ? ?datasets? methods? ?base? ? ?
?
loaded via a namespace (and not attached):
[1] compiler_3.4.2? nlme_3.1-131? ? grid_3.4.2? ? ? lattice_0.20-35
[5] fortunes_1.5-4?
?
Where Rdevega was edited as folows:
?
# The system-wide copy is in rwxxxx/etc.
# A user copy can be installed in `R_USER'.
?
# Format is?
# [TT] face:style
# where style is plain, bold, italic or bold&italic.
# If 'TT' is present, only TrueType/OpenType fonts are searched.
# Remarks:
#? ?(a) Windows graphics engine can only rotate TrueType fonts;
#? ?(b) Only the first 32 fonts will be used.
?
<Rdevga>
TT Verdana : plain
TT Verdana : bold
TT Verdana : italic
TT Verdana : bold&italic
?
# Please, don't change the following definition. The plot math engine
# assumes that font5 contains greek letters and math symbols
TT Symbol: plain
?
TT Times New Roman : plain
TT Times New Roman : bold
TT Times New Roman : italic
TT Times New Roman : bold&italic
?
TT Courier New: plain
TT Courier New: bold
TT Courier New: italic
TT Courier New: bold&italic
?
TT Century Gothic : plain
TT Century Gothic : bold
TT Century Gothic : italic
TT Century Gothic : bold&italic
?
TT Matisse ITC: plain
TT Wingdings: plain
</Rdevga>
?
?
When I check the font family:
?
> windowsFonts()$sans
[1] "TT Arial"
?
I find unexpected that the family name is still "TT Arial" although the font when ploting or using text() for annotation in graphics employs the font selected in the Rdevga file (Verdana in this case).
?
I searched all documentation and the Net and the answer to this puzzle eluded me, so if not a bug, I find this very unexpected!
?
TIA
?
--
Cesar Rabak


From yzan at volny.cz  Wed Dec 27 12:48:13 2017
From: yzan at volny.cz (Jan Motl)
Date: Wed, 27 Dec 2017 12:48:13 +0100
Subject: [R] Numerical stability in chisq.test
Message-ID: <CF803020-685A-4DAB-9F78-1508E639AAA3@volny.cz>

The chisq.test contains following code:
	STATISTIC <- sum(sort((x - E)^2/E, decreasing = TRUE))

However, based on book Accuracy and stability of numerical algorithms <http://ftp.demec.ufpr.br/CFD/bibliografia/Higham_2002_Accuracy%20and%20Stability%20of%20Numerical%20Algorithms.pdf> Table 4.1 on page 89, it is better to sort the data in increasing order than in decreasing order, when the data are non-negative.

A demonstrative example:
x = matrix(c(rep(1.1, 10000)), 10^16, nrow = 10001, ncol = 1)    # We have a vector with 10000*1.1 and 1*10^16
c(sum(sort(x, decreasing = TRUE)), sum(sort(x, decreasing = FALSE)))
The result:
	10000000000010996 10000000000011000
When we sort the data in the increasing order, we get the correct result. If we sort the data in the decreasing order, we get a result that is off by 4.

Shouldn't the sort be in the increasing order rather than in the decreasing order?

Best regards,
 Jan Motl


PS: This post is based on discussion on https://stackoverflow.com/questions/47847295/why-does-chisq-test-sort-data-in-descending-order-before-summation <https://stackoverflow.com/questions/47847295/why-does-chisq-test-sort-data-in-descending-order-before-summation>.




	[[alternative HTML version deleted]]


From yvb1975 at gmail.com  Wed Dec 27 18:26:21 2017
From: yvb1975 at gmail.com (Yury Bobr)
Date: Wed, 27 Dec 2017 20:26:21 +0300
Subject: [R] Error in dimnames in R
Message-ID: <CABO8i0+jv-1Ay6nRk=gd-T2F5yFfFZUa7NXtE-aHoP+J+yEKMA@mail.gmail.com>

Could anyone help me with some little problem? When I plot the frontier I
get the following message: *"Error in dimnames(x) <- dn : length of
'dimnames' [1] not equal to array extent"*(see below for detail). How could
I solve this. Thanks a lot.

##---------------------------- Portfolio construction &
Optimisation------------------------

#Assets: LUTAX, PFODX,BRGAX,GFAFX,NMSAX,EGINX,IPOYX,SCWFX,FGLDX,PAGEX

#Getting monthly returns of the assets
library(quantmod)
library(tseries)
library(timeSeries)

LUTAX <- monthlyReturn((getSymbols("LUTAX",auto.assign=FALSE)[,4]),type =
"arithmetic")
colnames(LUTAX) <- c("LUTAX")
PFODX <- monthlyReturn((getSymbols("PFODX",auto.assign=FALSE)[,4]),type =
"arithmetic")
colnames(PFODX) <- c("PFODX")
BRGAX <- monthlyReturn((getSymbols("BRGAX",auto.assign=FALSE)[,4]),type =
"arithmetic")
colnames(BRGAX) <- c("BRGAX")
GFAFX <- monthlyReturn((getSymbols("GFAFX",auto.assign=FALSE)[,4]),type =
"arithmetic")
colnames(GFAFX) <- c("GFAFX")
NMSAX <- monthlyReturn((getSymbols("NMSAX",auto.assign=FALSE)[,4]),type =
"arithmetic")
colnames(NMSAX) <- c("NMSAX")
EGINX <- monthlyReturn((getSymbols("EGINX",auto.assign=FALSE)[,4]),type =
"arithmetic")
colnames(EGINX) <- c("EGINX")
IPOYX <- monthlyReturn((getSymbols("IPOYX",auto.assign=FALSE)[,4]),type =
"arithmetic")
colnames(IPOYX) <- c("IPOYX")
SCWFX <- monthlyReturn((getSymbols("SCWFX",auto.assign=FALSE)[,4]),type =
"arithmetic")
colnames(SCWFX) <- c("SCWFX")
FGLDX <- monthlyReturn((getSymbols("FGLDX",auto.assign=FALSE)[,4]),type =
"arithmetic")
colnames(FGLDX) <- c("FGLDX")
PAGEX <- monthlyReturn((getSymbols("PAGEX",auto.assign=FALSE)[,4]),type =
"arithmetic")
colnames(PAGEX) <- c("PAGEX")

# Merging returns of the assets (excluding NA's)
portfolio_returns <- merge(LUTAX,
PFODX,BRGAX,GFAFX,NMSAX,EGINX,IPOYX,SCWFX,FGLDX,PAGEX,all=F)
data <- as.timeSeries(portfolio_returns)

#Optimisation portfolio
library(fPortfolio)

spec <- portfolioSpec()
setNFrontierPoints <- 25
setSolver(spec) <- "solveRquadprog"
constraints <-
c("minW[1:1]=0.12","maxW[1:1]=0.18","minW[2:2]=0.12","maxW[2:2]=0.18",

"minW[3:3]=0.10","maxW[3:3]=0.15","minW[4:4]=0.08","maxW[4:4]=0.12",

"minW[5:5]=0.08","maxW[5:5]=0.12","minW[6:6]=0.05","maxW[6:6]=0.10",

"minW[7:7]=0.05","maxW[7:7]=0.10","minW[8:8]=0.08","maxW[8:8]=0.12",

"minW[9:9]=0.05","maxW[9:9]=0.10","minW[10:10]=0.08","maxW[10:10]=0.12",
                 "minsumW[c(1:1,2:2)]=0.27","maxsumW[c(1:1,2:2)]=0.33",

"minsumW[c(3:3,4:4,6:6,10:10)]=0.37","maxsumW[c(3:3,4:4,6:6,10:10)]=0.43",

"minsumW[c(5:5,7:7,8:8,9:9)]=0.27","maxsumW[c(5:5,7:7,8:8,9:9)]=0.33",
                 "maxsumW[c(1:1,2:2,3:3,4:4,5:5,6:6,7:7,8:8,9:9,10:10)]=1")

portfolioConstraints(data,spec,constraints)

frontier<- portfolioFrontier(data,spec,constraints)
print(frontier)

tailoredFrontierPlot( frontier)

After running the last command above I get the following message:* "Error
in dimnames(x) <- dn : length of 'dimnames' [1] not equal to array extent"*



<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
???
???????. www.avast.ru
<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
<#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

	[[alternative HTML version deleted]]


From yadavneog at gmail.com  Wed Dec 27 20:05:10 2017
From: yadavneog at gmail.com (yadav neog)
Date: Thu, 28 Dec 2017 00:35:10 +0530
Subject: [R] require help
Message-ID: <CACdLcRzJOstpfnHOUoMUTH18=-p6MBrFQzSjyYHMBUK+2M3=kA@mail.gmail.com>

Respected sir,
hoping that you are well.sir, i am trying to run Tado-Yamamoto causality
test with my data. I have three variables. but in running wal.test in R, I
have faced problems (especially in 'terms' arguments). my results have
shown as...

Error in L %*% V : non-conformable arguments

-- kindly help me in solving this issue. I have also attached my codes
and data to this email.


Yadawananda Neog
Research Scholar
Department of Economics
Banaras Hindu University
Mob. 9838545073

2 Attachments



-- 
Yadawananda Neog
Research Scholar
Department of Economics
Banaras Hindu University
Mob. 9838545073

From bgunter.4567 at gmail.com  Wed Dec 27 22:30:57 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 27 Dec 2017 13:30:57 -0800
Subject: [R] identifying convergence or non-convergence of mixed-effects
 regression model in lme4 from model output
In-Reply-To: <3bddd6b3-a09d-d90b-4a0e-dddb6b0e11c1@stanford.edu>
References: <3bddd6b3-a09d-d90b-4a0e-dddb6b0e11c1@stanford.edu>
Message-ID: <CAGxFJbTyg3FWSjAj=fRoZRJTup_o-dv6KZDO5q7Z=8dFxHbBwA@mail.gmail.com>

This is more likely to get a helpful response if you post on the
r-sig-mixed-models list rather than r-help.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Dec 25, 2017 at 6:36 PM, Aleksander G??wka <aglowka at stanford.edu> wrote:
> Hi R community!
>
> I've fitted three mixed-effects regression models to a thousand
> bootstrap samples (case-resampling regression) using the lme4 package in
> a custom-built for-loop. The only output I saved were the inferential
> statistics for my fixed and random effects. I did not save any output
> related to the performance to the machine learning algorithm used to fit
> the models (REML=FALSE). After running all the simulations, I got about
> two dozen messages of this kind:
>
> 25: In checkConv(attr(opt, "derivs"), opt$par, ctrl =
> control$checkConv,  ... :
>    Model failed to converge with max|grad| = 4.49732 (tol = 0.002,
> component 1)
> 26: In checkConv(attr(opt, "derivs"), opt$par, ctrl =
> control$checkConv,  ... :
>    Model is nearly unidentifiable: large eigenvalue ratio
>   - Rescale variables?
>
> Since I only get the error messages after all the computations have been
> executed, looking at these error messages is not helpful, because, as
> you can see, they don't allow me to identify which bootstrap sample the
> non-converged model was fit to they are referring to. Since I don't
> think I can recover this information from the already run simulations, I
> need to modify my information extractor code to include convergence
> information and rerun the simulations.
>
> My question is the following: which attribute in the summary of a
> mixed-effects model in lme4 allows me to check if the model has
> converged or not? What value would the parameter corresponding to that
> attribute have to have in order for me to conclude the model has not
> converged?
>
> Here are my current extractor functions for fixed and random effects.
>
> lmer.data.extract = function(lmer.mod, name=deparse(substitute(lmer.mod))){
>
>    #extract predictor names & create data frame, attach other cols to
> new data frame
>    mod.data = data.frame(summary(lmer.mod)$coefficients[,1])
>    names(mod.data) = "estimate"
>    mod.data$std.error = as.numeric(summary(lmer.mod)$coefficients[,2])
> #std errors
>    mod.data$df = as.numeric(summary(lmer.mod)$coefficients[,3]) #degrees
> of freedom
>    mod.data$t.val = as.numeric(summary(lmer.mod)$coefficients[,4]) #t-values
>    mod.data$p.val = as.numeric(summary(lmer.mod)$coefficients[,5])
> #p-values
>
>    #extract AIC, BIC, logLik, deviance df.resid
>    mod.data$AIC = as.numeric(summary(lmer.mod)$AIC[1])
>    mod.data$BIC = as.numeric(summary(lmer.mod)$AICtab[2][1])
>    mod.data$logLik = as.numeric(summary(lmer.mod)$AICtab[3][1])
>    mod.data$deviance = as.numeric(summary(lmer.mod)$AICtab[4][1])
>    mod.data$df.resid = as.numeric(summary(lmer.mod)$AICtab[5][1])
>
>    #add number of datapoints
>    mod.data$N = as.numeric(summary(incr.best.m)$devcomp$dims[1])
>
>    #add model name
>    mod.data$model = name
>
>    return(mod.data)
>
> }
>
> lmer.ranef.data.extract = function(lmer.mod,
> name=deparse(substitute(lmer.mod))){
>
>    #extract random effect variance, standard error, correlations between
> slope and intercept
>    mod.data.ef = as.data.frame(VarCorr(lmer.mod))
>
>    mod.data.ef$n.subj = as.numeric(summary(lmer.mod)$ngrps[1]) #number
> of subjects
>    mod.data.ef$n.item = as.numeric(summary(lmer.mod)$ngrps[2]) #number
> of items
>
>    #add number of datapoints
>    mod.data.ef$N = as.numeric(summary(incr.best.m)$devcomp$dims[1])
>
>    #add model name
>    mod.data.ef$model = name
>
>    return(mod.data.ef)
>
> }
>
> I'm also including the structure of an example model that did converge
> (but I can I tell from the output?).
>
> List of 18
>   $ methTitle   : chr "Linear mixed model fit by maximum likelihood
> \nt-tests use  Satterthwaite approximations to degrees of freedom"
>   $ objClass    : atomic [1:1] lmerMod
>    ..- attr(*, "package")= chr "lme4"
>   $ devcomp     :List of 2
>    ..$ cmp : Named num [1:10] 176.85 59.09 95.43 3.84 99.27 ...
>    .. ..- attr(*, "names")= chr [1:10] "ldL2" "ldRX2" "wrss" "ussq" ...
>    ..$ dims: Named int [1:12] 1742 1742 10 1732 94 4 1 2 0 0 ...
>    .. ..- attr(*, "names")= chr [1:12] "N" "n" "p" "nmp" ...
>   $ isLmer      : logi TRUE
>   $ useScale    : logi TRUE
>   $ logLik      :Class 'logLik' : -65 (df=15)
>   $ family      : NULL
>   $ link        : NULL
>   $ ngrps       : Named num [1:2] 36 29
>    ..- attr(*, "names")= chr [1:2] "subj" "item"
>   $ coefficients: num [1:10, 1:5] 7.00546 0.04234 -0.00258 0.09094
> -0.00804 ...
>    ..- attr(*, "dimnames")=List of 2
>    .. ..$ : chr [1:10] "(Intercept)" "FreqABCD.log.std"
> "LogitABCD.neg.log.std" "MIABCD.neg.log.std" ...
>    .. ..$ : chr [1:5] "Estimate" "Std. Error" "df" "t value" ...
>   $ sigma       : num 0.239
>   $ vcov        :Formal class 'dpoMatrix' [package "Matrix"] with 5 slots
>    .. ..@ x       : num [1:100] 1.15e-03 3.40e-05 -5.12e-05 2.18e-05
> 3.65e-06 ...
>    .. ..@ Dim     : int [1:2] 10 10
>    .. ..@ Dimnames:List of 2
>    .. .. ..$ : chr [1:10] "(Intercept)" "FreqABCD.log.std"
> "LogitABCD.neg.log.std" "MIABCD.neg.log.std" ...
>    .. .. ..$ : chr [1:10] "(Intercept)" "FreqABCD.log.std"
> "LogitABCD.neg.log.std" "MIABCD.neg.log.std" ...
>    .. ..@ uplo    : chr "U"
>    .. ..@ factors :List of 1
>    .. .. ..$ correlation:Formal class 'corMatrix' [package "Matrix"]
> with 6 slots
>    .. .. .. .. ..@ sd      : num [1:10] 0.0339 0.0519 0.013 0.0439
> 0.0068 ...
>    .. .. .. .. ..@ x       : num [1:100] 1 0.0194 -0.1162 0.0147 0.0158 ...
>    .. .. .. .. ..@ Dim     : int [1:2] 10 10
>    .. .. .. .. ..@ Dimnames:List of 2
>    .. .. .. .. .. ..$ : chr [1:10] "(Intercept)" "FreqABCD.log.std"
> "LogitABCD.neg.log.std" "MIABCD.neg.log.std" ...
>    .. .. .. .. .. ..$ : chr [1:10] "(Intercept)" "FreqABCD.log.std"
> "LogitABCD.neg.log.std" "MIABCD.neg.log.std" ...
>    .. .. .. .. ..@ uplo    : chr "U"
>    .. .. .. .. ..@ factors :List of 1
>    .. .. .. .. .. ..$ Cholesky:Formal class 'Cholesky' [package
> "Matrix"] with 5 slots
>    .. .. .. .. .. .. .. ..@ x       : num [1:100] 1 0 0 0 0 0 0 0 0 0 ...
>    .. .. .. .. .. .. .. ..@ Dim     : int [1:2] 10 10
>    .. .. .. .. .. .. .. ..@ Dimnames:List of 2
>    .. .. .. .. .. .. .. .. ..$ : NULL
>    .. .. .. .. .. .. .. .. ..$ : NULL
>    .. .. .. .. .. .. .. ..@ uplo    : chr "U"
>    .. .. .. .. .. .. .. ..@ diag    : chr "N"
>   $ varcor      :List of 2
>    ..$ subj: num [1, 1] 0.0273
>    .. ..- attr(*, "dimnames")=List of 2
>    .. .. ..$ : chr "(Intercept)"
>    .. .. ..$ : chr "(Intercept)"
>    .. ..- attr(*, "stddev")= Named num 0.165
>    .. .. ..- attr(*, "names")= chr "(Intercept)"
>    .. ..- attr(*, "correlation")= num [1, 1] 1
>    .. .. ..- attr(*, "dimnames")=List of 2
>    .. .. .. ..$ : chr "(Intercept)"
>    .. .. .. ..$ : chr "(Intercept)"
>    ..$ item: num [1:2, 1:2] 0.00417 0.000484 0.000484 0.00289
>    .. ..- attr(*, "dimnames")=List of 2
>    .. .. ..$ : chr [1:2] "(Intercept)" "FreqABCD.log.std"
>    .. .. ..$ : chr [1:2] "(Intercept)" "FreqABCD.log.std"
>    .. ..- attr(*, "stddev")= Named num [1:2] 0.0646 0.0538
>    .. .. ..- attr(*, "names")= chr [1:2] "(Intercept)" "FreqABCD.log.std"
>    .. ..- attr(*, "correlation")= num [1:2, 1:2] 1 0.139 0.139 1
>    .. .. ..- attr(*, "dimnames")=List of 2
>    .. .. .. ..$ : chr [1:2] "(Intercept)" "FreqABCD.log.std"
>    .. .. .. ..$ : chr [1:2] "(Intercept)" "FreqABCD.log.std"
>    ..- attr(*, "sc")= num 0.239
>    ..- attr(*, "useSc")= logi TRUE
>    ..- attr(*, "class")= chr "VarCorr.merMod"
>   $ AICtab      : Named num [1:5] 159.7 241.6 -64.8 129.7 1727
>    ..- attr(*, "names")= chr [1:5] "AIC" "BIC" "logLik" "deviance" ...
>   $ call        : language lme4::lmer(formula = RT.log ~
> FreqABCD.log.std + LogitABCD.neg.log.std + MIABCD.neg.log.std +
> AS.data$freq.sub.PC1 +      AS.data$freq.sub.PC2 + AS.data$freq.sub.PC3
> + AS.data$freq.sub.PC4 + block + nletter.std + (1 | subj) +  ...
>   $ residuals   : Named num [1:1742] 0.713 0.498 -0.361 -0.101 2.594 ...
>    ..- attr(*, "names")= chr [1:1742] "1" "2" "3" "4" ...
>   $ fitMsgs     : chr(0)
>   $ optinfo     :List of 7
>    ..$ optimizer: chr "bobyqa"
>    ..$ control  :List of 1
>    .. ..$ iprint: int 0
>    ..$ derivs   :List of 2
>    .. ..$ gradient: num [1:4] 9.81e-06 -5.34e-06 -1.60e-05 7.06e-05
>    .. ..$ Hessian : num [1:4, 1:4] 245.9 28.5 3.3 -13.7 28.5 ...
>    ..$ conv     :List of 2
>    .. ..$ opt : int 0
>    .. ..$ lme4: list()
>    ..$ feval    : int 107
>    ..$ warnings : list()
>    ..$ val      : num [1:4] 0.6919 0.2705 0.0314 0.223
>   - attr(*, "class")= chr "summary.merMod"
>
> I'd appreciate any advice you may have!
>
> Thank you,
>
> Aleksander G??wka
> PhD Candidate
> Department of Linguistics
> Stanford University
> **
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rmsharp at me.com  Wed Dec 27 22:43:45 2017
From: rmsharp at me.com (R. Mark Sharp)
Date: Wed, 27 Dec 2017 16:43:45 -0500
Subject: [R] require help
In-Reply-To: <CACdLcRzJOstpfnHOUoMUTH18=-p6MBrFQzSjyYHMBUK+2M3=kA@mail.gmail.com>
References: <CACdLcRzJOstpfnHOUoMUTH18=-p6MBrFQzSjyYHMBUK+2M3=kA@mail.gmail.com>
Message-ID: <A5D9FDEE-7D38-4E34-810C-D8281C867926@me.com>

Yadav,

We need some information that is missing in order to help you.


PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

Mark
R. Mark Sharp, Ph.D.
Data Scientist and Biomedical Statistical Consultant
7526 Meadow Green St.
San Antonio, TX 78251
mobile: 210-218-2868
rmsharp at me.com










> On Dec 27, 2017, at 2:05 PM, yadav neog <yadavneog at gmail.com> wrote:
> 
> Respected sir,
> hoping that you are well.sir, i am trying to run Tado-Yamamoto causality
> test with my data. I have three variables. but in running wal.test in R, I
> have faced problems (especially in 'terms' arguments). my results have
> shown as...
> 
> Error in L %*% V : non-conformable arguments
> 
> -- kindly help me in solving this issue. I have also attached my codes
> and data to this email.
> 
> 
> Yadawananda Neog
> Research Scholar
> Department of Economics
> Banaras Hindu University
> Mob. 9838545073
> 
> 2 Attachments
> 
> 
> 
> -- 
> Yadawananda Neog
> Research Scholar
> Department of Economics
> Banaras Hindu University
> Mob. 9838545073
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Thu Dec 28 08:10:46 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 28 Dec 2017 07:10:46 +0000
Subject: [R] Error in dimnames in R
In-Reply-To: <CABO8i0+jv-1Ay6nRk=gd-T2F5yFfFZUa7NXtE-aHoP+J+yEKMA@mail.gmail.com>
References: <CABO8i0+jv-1Ay6nRk=gd-T2F5yFfFZUa7NXtE-aHoP+J+yEKMA@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF880155FD1A2A@SRVEXCHCM301.precheza.cz>

Hi

Without knowing much about the functions you use I would suggest few hints.

Do not use HTML format, your message could be scrambled.
Does frontier conform to data expected by function tailoredFrontierPlot?
Your best option could be to contact directly the maintainer. You could get his address by

maintainer("fPortfolio")

Instead of your whole code you should post the minimum reproducible example, with data preferably copied from

dput(frontier)

directly to your mail.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Yury Bobr
> Sent: Wednesday, December 27, 2017 6:26 PM
> To: r-help at r-project.org
> Subject: [R] Error in dimnames in R
>
> Could anyone help me with some little problem? When I plot the frontier I get
> the following message: *"Error in dimnames(x) <- dn : length of 'dimnames' [1]
> not equal to array extent"*(see below for detail). How could I solve this. Thanks
> a lot.
>
> ##---------------------------- Portfolio construction &
> Optimisation------------------------
>
> #Assets: LUTAX,
> PFODX,BRGAX,GFAFX,NMSAX,EGINX,IPOYX,SCWFX,FGLDX,PAGEX
>
> #Getting monthly returns of the assets
> library(quantmod)
> library(tseries)
> library(timeSeries)
>
> LUTAX <- monthlyReturn((getSymbols("LUTAX",auto.assign=FALSE)[,4]),type =
> "arithmetic")
> colnames(LUTAX) <- c("LUTAX")
> PFODX <- monthlyReturn((getSymbols("PFODX",auto.assign=FALSE)[,4]),type =
> "arithmetic")
> colnames(PFODX) <- c("PFODX")
> BRGAX <- monthlyReturn((getSymbols("BRGAX",auto.assign=FALSE)[,4]),type =
> "arithmetic")
> colnames(BRGAX) <- c("BRGAX")
> GFAFX <- monthlyReturn((getSymbols("GFAFX",auto.assign=FALSE)[,4]),type =
> "arithmetic")
> colnames(GFAFX) <- c("GFAFX")
> NMSAX <- monthlyReturn((getSymbols("NMSAX",auto.assign=FALSE)[,4]),type =
> "arithmetic")
> colnames(NMSAX) <- c("NMSAX")
> EGINX <- monthlyReturn((getSymbols("EGINX",auto.assign=FALSE)[,4]),type =
> "arithmetic")
> colnames(EGINX) <- c("EGINX")
> IPOYX <- monthlyReturn((getSymbols("IPOYX",auto.assign=FALSE)[,4]),type =
> "arithmetic")
> colnames(IPOYX) <- c("IPOYX")
> SCWFX <- monthlyReturn((getSymbols("SCWFX",auto.assign=FALSE)[,4]),type =
> "arithmetic")
> colnames(SCWFX) <- c("SCWFX")
> FGLDX <- monthlyReturn((getSymbols("FGLDX",auto.assign=FALSE)[,4]),type =
> "arithmetic")
> colnames(FGLDX) <- c("FGLDX")
> PAGEX <- monthlyReturn((getSymbols("PAGEX",auto.assign=FALSE)[,4]),type =
> "arithmetic")
> colnames(PAGEX) <- c("PAGEX")
>
> # Merging returns of the assets (excluding NA's) portfolio_returns <-
> merge(LUTAX,
> PFODX,BRGAX,GFAFX,NMSAX,EGINX,IPOYX,SCWFX,FGLDX,PAGEX,all=F)
> data <- as.timeSeries(portfolio_returns)
>
> #Optimisation portfolio
> library(fPortfolio)
>
> spec <- portfolioSpec()
> setNFrontierPoints <- 25
> setSolver(spec) <- "solveRquadprog"
> constraints <-
> c("minW[1:1]=0.12","maxW[1:1]=0.18","minW[2:2]=0.12","maxW[2:2]=0.18",
>
> "minW[3:3]=0.10","maxW[3:3]=0.15","minW[4:4]=0.08","maxW[4:4]=0.12",
>
> "minW[5:5]=0.08","maxW[5:5]=0.12","minW[6:6]=0.05","maxW[6:6]=0.10",
>
> "minW[7:7]=0.05","maxW[7:7]=0.10","minW[8:8]=0.08","maxW[8:8]=0.12",
>
> "minW[9:9]=0.05","maxW[9:9]=0.10","minW[10:10]=0.08","maxW[10:10]=0.1
> 2",
>                  "minsumW[c(1:1,2:2)]=0.27","maxsumW[c(1:1,2:2)]=0.33",
>
> "minsumW[c(3:3,4:4,6:6,10:10)]=0.37","maxsumW[c(3:3,4:4,6:6,10:10)]=0.43"
> ,
>
> "minsumW[c(5:5,7:7,8:8,9:9)]=0.27","maxsumW[c(5:5,7:7,8:8,9:9)]=0.33",
>                  "maxsumW[c(1:1,2:2,3:3,4:4,5:5,6:6,7:7,8:8,9:9,10:10)]=1")
>
> portfolioConstraints(data,spec,constraints)
>
> frontier<- portfolioFrontier(data,spec,constraints)
> print(frontier)
>
> tailoredFrontierPlot( frontier)
>
> After running the last command above I get the following message:* "Error in
> dimnames(x) <- dn : length of 'dimnames' [1] not equal to array extent"*
>
>
>
> <https://www.avast.com/sig-
> email?utm_medium=email&utm_source=link&utm_campaign=sig-
> email&utm_content=webmail>
> ???
> ???????. www.avast.ru
> <https://www.avast.com/sig-
> email?utm_medium=email&utm_source=link&utm_campaign=sig-
> email&utm_content=webmail>
> <#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From petr.pikal at precheza.cz  Thu Dec 28 08:16:35 2017
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 28 Dec 2017 07:16:35 +0000
Subject: [R] require help
In-Reply-To: <CACdLcRzJOstpfnHOUoMUTH18=-p6MBrFQzSjyYHMBUK+2M3=kA@mail.gmail.com>
References: <CACdLcRzJOstpfnHOUoMUTH18=-p6MBrFQzSjyYHMBUK+2M3=kA@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF880155FD1A3D@SRVEXCHCM301.precheza.cz>

Hi

Did you mean wald.test? If not, what is wal.test?

Instead of attaching anything (which is usually stripped from the mail), you should copy your code directly to your mail and preferably together with your  data.

See ?dput

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of yadav neog
> Sent: Wednesday, December 27, 2017 8:05 PM
> To: dnewmil at dcn.davis.ca.us; R mailing list <r-help at r-project.org>
> Subject: [R] require help
>
> Respected sir,
> hoping that you are well.sir, i am trying to run Tado-Yamamoto causality test
> with my data. I have three variables. but in running wal.test in R, I have faced
> problems (especially in 'terms' arguments). my results have shown as...
>
> Error in L %*% V : non-conformable arguments
>
> -- kindly help me in solving this issue. I have also attached my codes and data to
> this email.
>
>
> Yadawananda Neog
> Research Scholar
> Department of Economics
> Banaras Hindu University
> Mob. 9838545073
>
> 2 Attachments
>
>
>
> --
> Yadawananda Neog
> Research Scholar
> Department of Economics
> Banaras Hindu University
> Mob. 9838545073
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From jorgefernandosaraiva at gmail.com  Thu Dec 28 16:52:47 2017
From: jorgefernandosaraiva at gmail.com (Jorge Fernando Saraiva de Menezes)
Date: Thu, 28 Dec 2017 17:52:47 +0200
Subject: [R] Why aov() with Error() gives three strata?
Message-ID: <CALnMTgEmPB7+XNos31LvCx0J1UmqF4ffhHZDhS4eq+uQUD=Bfg@mail.gmail.com>

Dear list users,

I am trying to learn Repeated measures ANOVA using the aov() interface, but
I'm struggling to understand its output.

According to tutorials on the web, formula for a repeated measures design
is:

aov(Y ~ IV+ Error(SUBJECT/IV) )

This formula does work but it returns three strata (Error:SUBJECT, Error:
SUBJECT:IV, Error: Within), when I would expect two strata (Within and
Between subjects). I've seems some tutorials  show the exactly same setup,
but returning only the two first strata.

Is it possible to have two or three strata depending on the data?
If there is always three strata, how this would fit the interpretation of
between vs within effects?

Below a reproducible example that gives three strata:

data(beavers)
data=data.frame(id =
rep(c("beaver1","beaver2"),c(nrow(beaver1),nrow(beaver2))),rbind(beaver1,beaver2))
data$activ=factor(data$activ)
#balance dataset to have 6 samples for every combination of beaver and
activity.
balanced = split(data,interaction(data$id,data$activ))
sizes = sapply(balanced,nrow)
selected = lapply(sizes,sample.int,6)
balanced = mapply(function(x,y) {x[y,]}, balanced,selected,SIMPLIFY=F)
balanced = do.call(rbind,balanced)
aov(temp~activ+Error(id/activ),data=balanced)

Thanks,
Jorge

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Dec 28 19:04:33 2017
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 28 Dec 2017 10:04:33 -0800
Subject: [R] Why aov() with Error() gives three strata?
In-Reply-To: <CALnMTgEmPB7+XNos31LvCx0J1UmqF4ffhHZDhS4eq+uQUD=Bfg@mail.gmail.com>
References: <CALnMTgEmPB7+XNos31LvCx0J1UmqF4ffhHZDhS4eq+uQUD=Bfg@mail.gmail.com>
Message-ID: <CAGxFJbQ=61qq2B4+qBkoHrSXkZ-8PSzA0AfJeF59BUQbp+iwzg@mail.gmail.com>

Jorge:

FYI, *generally speaking,* queries that are mostly statistical in
nature, such as yours, are off topic here -- this list is about R
programming help, not statistical help. Having said that, you still
may get a useful response here -- the r-help/statistics intersection
*is* nonempty. However, if not, 2.5 suggestions:

1. Try posting to r-sig-mixed-models instead. Repeated measures are a
type of mixed/multilevel model and you may receive some useful
suggestions there, including alternative R approaches to fitting such
model (e.g. using lme() or lmer() ).

2. Alternatively, try posting to a statistics site like stats.stackexchange.com.

2.5. Or, if you can, the best idea might be to sit down with a local
statistics expert.

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Dec 28, 2017 at 7:52 AM, Jorge Fernando Saraiva de Menezes
<jorgefernandosaraiva at gmail.com> wrote:
> Dear list users,
>
> I am trying to learn Repeated measures ANOVA using the aov() interface, but
> I'm struggling to understand its output.
>
> According to tutorials on the web, formula for a repeated measures design
> is:
>
> aov(Y ~ IV+ Error(SUBJECT/IV) )
>
> This formula does work but it returns three strata (Error:SUBJECT, Error:
> SUBJECT:IV, Error: Within), when I would expect two strata (Within and
> Between subjects). I've seems some tutorials  show the exactly same setup,
> but returning only the two first strata.
>
> Is it possible to have two or three strata depending on the data?
> If there is always three strata, how this would fit the interpretation of
> between vs within effects?
>
> Below a reproducible example that gives three strata:
>
> data(beavers)
> data=data.frame(id =
> rep(c("beaver1","beaver2"),c(nrow(beaver1),nrow(beaver2))),rbind(beaver1,beaver2))
> data$activ=factor(data$activ)
> #balance dataset to have 6 samples for every combination of beaver and
> activity.
> balanced = split(data,interaction(data$id,data$activ))
> sizes = sapply(balanced,nrow)
> selected = lapply(sizes,sample.int,6)
> balanced = mapply(function(x,y) {x[y,]}, balanced,selected,SIMPLIFY=F)
> balanced = do.call(rbind,balanced)
> aov(temp~activ+Error(id/activ),data=balanced)
>
> Thanks,
> Jorge
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ramesh.yapalparvi at icloud.com  Thu Dec 28 18:13:08 2017
From: ramesh.yapalparvi at icloud.com (Ramesh YAPALPARVI)
Date: Thu, 28 Dec 2017 12:13:08 -0500
Subject: [R] Help with dates
Message-ID: <49CCE5B4-BE17-4113-AF9B-29E9B65515BB@icloud.com>

Hi all,

I?m struggling to get the dates in proper format.
I have dates as factors and is in the form 02/27/34( 34 means 1934). If I use

as.Date with format %d%m%y it gets converted to 2034-02-27. I tried changing the origin in the as.Date command but nothing worked. Any help is appreciated.

Thanks,
Ramesh

From jorgefernandosaraiva at gmail.com  Thu Dec 28 19:36:20 2017
From: jorgefernandosaraiva at gmail.com (Jorge Fernando Saraiva de Menezes)
Date: Thu, 28 Dec 2017 20:36:20 +0200
Subject: [R] Why aov() with Error() gives three strata?
In-Reply-To: <CAGxFJbQ=61qq2B4+qBkoHrSXkZ-8PSzA0AfJeF59BUQbp+iwzg@mail.gmail.com>
References: <CALnMTgEmPB7+XNos31LvCx0J1UmqF4ffhHZDhS4eq+uQUD=Bfg@mail.gmail.com>
 <CAGxFJbQ=61qq2B4+qBkoHrSXkZ-8PSzA0AfJeF59BUQbp+iwzg@mail.gmail.com>
Message-ID: <CALnMTgEQemvLWWYAfe6cSZrKr4jHdKnWhNukOKwd+4_kss_Rvg@mail.gmail.com>

Bert, thanks for the reply but I feel that my question is less about
statistics and more about R interface. Specifically, because the output of
R seems different than other programs (systat, for example, gives a between
and a within table instead of a three level one).

I am familiar with the connection between mixed models and repeated
measures,and how mixed models are essentially replacing the aov models due
to their greater flexibility. But I feel that despite understanding a
little of the logic behind the mixed models that aov error terms seem
completely different to me than lmer randoms.

 I will post in those support lists you pass to me, if nothing comes from
here. However I had little luck in the stats exchange when I tried there.

About a local expert, I am once more in a corner. there are many people in
my department who excel in statistics. But I none use R, drastically
reducing their ability to explain to me the output of aov.

Em 28 de dez de 2017 20:04, "Bert Gunter" <bgunter.4567 at gmail.com> escreveu:

> Jorge:
>
> FYI, *generally speaking,* queries that are mostly statistical in
> nature, such as yours, are off topic here -- this list is about R
> programming help, not statistical help. Having said that, you still
> may get a useful response here -- the r-help/statistics intersection
> *is* nonempty. However, if not, 2.5 suggestions:
>
> 1. Try posting to r-sig-mixed-models instead. Repeated measures are a
> type of mixed/multilevel model and you may receive some useful
> suggestions there, including alternative R approaches to fitting such
> model (e.g. using lme() or lmer() ).
>
> 2. Alternatively, try posting to a statistics site like
> stats.stackexchange.com.
>
> 2.5. Or, if you can, the best idea might be to sit down with a local
> statistics expert.
>
> Cheers,
> Bert
>
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Thu, Dec 28, 2017 at 7:52 AM, Jorge Fernando Saraiva de Menezes
> <jorgefernandosaraiva at gmail.com> wrote:
> > Dear list users,
> >
> > I am trying to learn Repeated measures ANOVA using the aov() interface,
> but
> > I'm struggling to understand its output.
> >
> > According to tutorials on the web, formula for a repeated measures design
> > is:
> >
> > aov(Y ~ IV+ Error(SUBJECT/IV) )
> >
> > This formula does work but it returns three strata (Error:SUBJECT, Error:
> > SUBJECT:IV, Error: Within), when I would expect two strata (Within and
> > Between subjects). I've seems some tutorials  show the exactly same
> setup,
> > but returning only the two first strata.
> >
> > Is it possible to have two or three strata depending on the data?
> > If there is always three strata, how this would fit the interpretation of
> > between vs within effects?
> >
> > Below a reproducible example that gives three strata:
> >
> > data(beavers)
> > data=data.frame(id =
> > rep(c("beaver1","beaver2"),c(nrow(beaver1),nrow(beaver2))),
> rbind(beaver1,beaver2))
> > data$activ=factor(data$activ)
> > #balance dataset to have 6 samples for every combination of beaver and
> > activity.
> > balanced = split(data,interaction(data$id,data$activ))
> > sizes = sapply(balanced,nrow)
> > selected = lapply(sizes,sample.int,6)
> > balanced = mapply(function(x,y) {x[y,]}, balanced,selected,SIMPLIFY=F)
> > balanced = do.call(rbind,balanced)
> > aov(temp~activ+Error(id/activ),data=balanced)
> >
> > Thanks,
> > Jorge
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jacob-simmering at uiowa.edu  Thu Dec 28 21:54:57 2017
From: jacob-simmering at uiowa.edu (Simmering, Jacob E)
Date: Thu, 28 Dec 2017 20:54:57 +0000
Subject: [R] Help with dates
In-Reply-To: <49CCE5B4-BE17-4113-AF9B-29E9B65515BB@icloud.com>
References: <49CCE5B4-BE17-4113-AF9B-29E9B65515BB@icloud.com>
Message-ID: <1FEFB22E-E4D0-4ADD-BB2F-7E9FD75CFEC9@uiowa.edu>

Your dates have an incomplete year information with 34. R assumes that 00-68 are 2000 to 2068 and 69 to 99 are 1969 to 1999. See ?strptime and the details for %y. 

You can either append ?19? to the start of your year variable to make it completely express the year or check if the date is in the future (assuming all dates should be in the past) and subtract 100 years from the date. 


> On Dec 28, 2017, at 11:13 AM, Ramesh YAPALPARVI <ramesh.yapalparvi at icloud.com> wrote:
> 
> Hi all,
> 
> I?m struggling to get the dates in proper format.
> I have dates as factors and is in the form 02/27/34( 34 means 1934). If I use
> 
> as.Date with format %d%m%y it gets converted to 2034-02-27. I tried changing the origin in the as.Date command but nothing worked. Any help is appreciated.
> 
> Thanks,
> Ramesh
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rsherry8 at comcast.net  Fri Dec 29 01:02:57 2017
From: rsherry8 at comcast.net (rsherry8)
Date: Thu, 28 Dec 2017 19:02:57 -0500
Subject: [R] RQuantLib
Message-ID: <5A458631.6070409@comcast.net>


I have recently installed R on my new computer. I also want to install 
the package RQuantLib. So I run the following command and get the 
following output:

 >  install.packages("RQuantLib")
Installing package into ?C:/Users/rsher/Documents/R/win-library/3.2?
(as ?lib? is unspecified)
--- Please select a CRAN mirror for use in this session ---
Warning message:
package ?RQuantLib? is not available (for R version 3.2.4 Revised)

The package did not install. Am I doing something wrong. Is the package 
going to be updated for the latest version of R?

Thanks,
Bob


From pablo.ortiz at yale.edu  Fri Dec 29 01:03:10 2017
From: pablo.ortiz at yale.edu (PABLO ORTIZ PINEDA)
Date: Thu, 28 Dec 2017 19:03:10 -0500
Subject: [R] Help with script
Message-ID: <ddd8a2e1-b64a-1308-60a1-3199bdc01918@yale.edu>

Hello there. Happy new year for everyone!

I need help with a table. This table contains 300 rows and 192 columns. 
Being the first column the ID of my samples that can have several 
observations.

I need to generate e NEW table that contains a single ID with the sum of 
the observations by columns:
For example:

Example
ID?? A??? B? ? C? ? D? ? E? ? F? ? G.... 191 columns
a1?? 0??? 0??? 0? ? 1??? 1? ? 2??? 0...
a2?? 0??? 1??? 0? ? 1??? 2 ?? 2??? 1...
a2?? 0??? 1??? 1? ? 2??? 0? ? 2??? 1...
a3?? 0??? 1??? 1? ? 1??? 1 ?? 1??? 1....
...300rows
In this case I want to make a new table in which there is only 1 ID and 
the values of each columns A...G are added. I
n the example the new table would have only 3 IDs. a1, a2 and 3 and a2 
has the values added by column:
a2?? 0?? 2?? 1?? 3?? 2?? 4?? 2..

Thank you so much and have a wonderful year!.

-- 
Pablo A. Ortiz-Pineda (Ph.D.)
Molecular Biology & Bioinformatics
Yale University. School of Medicine.
Pediatrics Department.
New Haven, CT 06510


	[[alternative HTML version deleted]]


From josh.m.ulrich at gmail.com  Fri Dec 29 04:28:00 2017
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Thu, 28 Dec 2017 21:28:00 -0600
Subject: [R] RQuantLib
In-Reply-To: <5A458631.6070409@comcast.net>
References: <5A458631.6070409@comcast.net>
Message-ID: <CAPPM_gRcw9UKaN+vCuB3gZkGwCZA=juBBAsx3QyCPC3CYVe+Vw@mail.gmail.com>

On Thu, Dec 28, 2017 at 6:02 PM, rsherry8 <rsherry8 at comcast.net> wrote:
>
> I have recently installed R on my new computer. I also want to install the
> package RQuantLib. So I run the following command and get the following
> output:
>
>>  install.packages("RQuantLib")
> Installing package into ?C:/Users/rsher/Documents/R/win-library/3.2?
> (as ?lib? is unspecified)
> --- Please select a CRAN mirror for use in this session ---
> Warning message:
> package ?RQuantLib? is not available (for R version 3.2.4 Revised)
>
> The package did not install. Am I doing something wrong. Is the package
> going to be updated for the latest version of R?
>
Windows binary packages are only built for the most current (major)
version of R.  You need to upgrade to at least R-3.4.0, or you will
have to install RQuantLib (and therefore QuantLib itself) from source.

> Thanks,
> Bob
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com
R/Finance 2018 | www.rinfinance.com


From ericjberger at gmail.com  Fri Dec 29 06:08:43 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Fri, 29 Dec 2017 07:08:43 +0200
Subject: [R] Help with script
In-Reply-To: <ddd8a2e1-b64a-1308-60a1-3199bdc01918@yale.edu>
References: <ddd8a2e1-b64a-1308-60a1-3199bdc01918@yale.edu>
Message-ID: <CAGgJW75fxfXZEM=HvVxoJvVozNskHPpaXW-yC_CkCA4gX=-Msg@mail.gmail.com>

Hi Pablo,
There are probably many ways to do this in R. This suggestion uses dplyr.
The solution is actually only one line (see the line starting with dat2).
The first section simply creates the example data.

library(dplyr)
# 1. set up the example data
m <- matrix( c(0,0,0,0,0,1,1,1,0,0,1,1,1,1,2,1,1,2,0,1,2,2,2,1,0,1,1,1),
nrow=4)
dat <- as.data.frame(m)
dat$ID <- c("a1","a2","a2","a3")
dat <- dat[,c(8,1:7)]
colnames(dat) <- c("ID",LETTERS[1:7])

#2. group the data by ID, summing the columns in each group
dat2 <- group_by(dat,ID) %>% summarise_all( sum )

#3. show the results
dat2

# # A tibble: 3 x 8
#      ID     A     B     C     D     E     F     G
#   <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
# 1    a1     0     0     0     1     1     2     0
# 2    a2     0     2     1     3     2     4     2
# 3    a3     0     1     1     1     1     1     1

HTH,
Eric


On Fri, Dec 29, 2017 at 2:03 AM, PABLO ORTIZ PINEDA <pablo.ortiz at yale.edu>
wrote:

> Hello there. Happy new year for everyone!
>
> I need help with a table. This table contains 300 rows and 192 columns.
> Being the first column the ID of my samples that can have several
> observations.
>
> I need to generate e NEW table that contains a single ID with the sum of
> the observations by columns:
> For example:
>
> Example
> ID   A    B    C    D    E    F    G.... 191 columns
> a1   0    0    0    1    1    2    0...
> a2   0    1    0    1    2    2    1...
> a2   0    1    1    2    0    2    1...
> a3   0    1    1    1    1    1    1....
> ...300rows
> In this case I want to make a new table in which there is only 1 ID and
> the values of each columns A...G are added. I
> n the example the new table would have only 3 IDs. a1, a2 and 3 and a2
> has the values added by column:
> a2   0   2   1   3   2   4   2..
>
> Thank you so much and have a wonderful year!.
>
> --
> Pablo A. Ortiz-Pineda (Ph.D.)
> Molecular Biology & Bioinformatics
> Yale University. School of Medicine.
> Pediatrics Department.
> New Haven, CT 06510
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From marc_grt at yahoo.fr  Fri Dec 29 06:29:00 2017
From: marc_grt at yahoo.fr (Marc Girondot)
Date: Fri, 29 Dec 2017 06:29:00 +0100
Subject: [R] Help with dates
In-Reply-To: <49CCE5B4-BE17-4113-AF9B-29E9B65515BB@icloud.com>
References: <49CCE5B4-BE17-4113-AF9B-29E9B65515BB@icloud.com>
Message-ID: <ba7e1ed4-a67b-35f2-1507-12222af25980@yahoo.fr>

Le 28/12/2017 ? 18:13, Ramesh YAPALPARVI a ?crit?:
> Hi all,
>
> I?m struggling to get the dates in proper format.
> I have dates as factors and is in the form 02/27/34( 34 means 1934). If I use

Try this

x <- "02/27/34"
x2 <- paste0(substr(x, 1, 6), "19", substr(x, 7, 8))
as.Date(x2, format="%m/%d/%Y")
[1] "1934-02-27"

or

x2 <- gsub("(../../)(..)", "\\119\\2", x)

Marc

> as.Date with format %d%m%y it gets converted to 2034-02-27. I tried changing the origin in the as.Date command but nothing worked. Any help is appreciated.
>
> Thanks,
> Ramesh
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
__________________________________________________________
Marc Girondot, Pr

Laboratoire Ecologie, Syst?matique et Evolution
Equipe de Conservation des Populations et des Communaut?s
CNRS, AgroParisTech et Universit? Paris-Sud 11 , UMR 8079
B?timent 362
91405 Orsay Cedex, France

Tel:  33 1 (0)1.69.15.72.30   Fax: 33 1 (0)1.69.15.73.53
e-mail: marc.girondot at u-psud.fr
Web: http://www.ese.u-psud.fr/epc/conservation/Marc.html
Skype: girondot


From ruipbarradas at sapo.pt  Fri Dec 29 06:01:29 2017
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Fri, 29 Dec 2017 05:01:29 +0000
Subject: [R] Help with script
In-Reply-To: <ddd8a2e1-b64a-1308-60a1-3199bdc01918@yale.edu>
References: <ddd8a2e1-b64a-1308-60a1-3199bdc01918@yale.edu>
Message-ID: <f09a3a5d-bbed-14d1-3294-8f92bd1146a0@sapo.pt>

Hello,

Just use ?aggregate.

Example <- read.table(text = "
ID   A    B    C    D    E    F    G
a1   0    0    0    1    1    2    0
a2   0    1    0    1    2    2    1
a2   0    1    1    2    0    2    1
a3   0    1    1    1    1    1    1
", header = TRUE)

aggregate(. ~ ID, Example , sum)


Happy holidays,

Rui Barradas

On 12/29/2017 12:03 AM, PABLO ORTIZ PINEDA wrote:
> Hello there. Happy new year for everyone!
> 
> I need help with a table. This table contains 300 rows and 192 columns.
> Being the first column the ID of my samples that can have several
> observations.
> 
> I need to generate e NEW table that contains a single ID with the sum of
> the observations by columns:
> For example:
> 
> Example
> ID?? A??? B? ? C? ? D? ? E? ? F? ? G.... 191 columns
> a1?? 0??? 0??? 0? ? 1??? 1? ? 2??? 0...
> a2?? 0??? 1??? 0? ? 1??? 2 ?? 2??? 1...
> a2?? 0??? 1??? 1? ? 2??? 0? ? 2??? 1...
> a3?? 0??? 1??? 1? ? 1??? 1 ?? 1??? 1....
> ...300rows
> In this case I want to make a new table in which there is only 1 ID and
> the values of each columns A...G are added. I
> n the example the new table would have only 3 IDs. a1, a2 and 3 and a2
> has the values added by column:
> a2?? 0?? 2?? 1?? 3?? 2?? 4?? 2..
> 
> Thank you so much and have a wonderful year!.
>


From pdalgd at gmail.com  Fri Dec 29 08:41:33 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 29 Dec 2017 08:41:33 +0100
Subject: [R] Why aov() with Error() gives three strata?
In-Reply-To: <CALnMTgEQemvLWWYAfe6cSZrKr4jHdKnWhNukOKwd+4_kss_Rvg@mail.gmail.com>
References: <CALnMTgEmPB7+XNos31LvCx0J1UmqF4ffhHZDhS4eq+uQUD=Bfg@mail.gmail.com>
 <CAGxFJbQ=61qq2B4+qBkoHrSXkZ-8PSzA0AfJeF59BUQbp+iwzg@mail.gmail.com>
 <CALnMTgEQemvLWWYAfe6cSZrKr4jHdKnWhNukOKwd+4_kss_Rvg@mail.gmail.com>
Message-ID: <1DB5E61E-C7C2-4D36-873C-8A96E0C87681@gmail.com>

At any rate: 

Error(SUBJECT/IV)

specifies two random effects: SUBJECT and SUBJECT:IV. This is most easily understood if you conceptually arrange your data in a SUBJECT x IV table: One effect is a set of random errors added to each row, the other is a set of effects added to each cell. 

If you have more than one observation within each cell, then you need a third set of errors to account for differences within cells and this is labeled "Within" variation. With one observation per cell, this stratum disappears (as far as I recall, haven't checked). 

Actually, this oversimplifies a little: What actually happens is that data gets split into 

1: row means
2: differences between cells within rows
3: differences between observations within cells

and if the stratum variances are decreasing, then this can be interpreted using random effects as above, with variances of each component proportional to the successive differences. (All assuming that you have a balanced data layout, otherwise aov() is just the wrong tool.)

-pd

> On 28 Dec 2017, at 19:36 , Jorge Fernando Saraiva de Menezes <jorgefernandosaraiva at gmail.com> wrote:
> 
> Bert, thanks for the reply but I feel that my question is less about
> statistics and more about R interface. Specifically, because the output of
> R seems different than other programs (systat, for example, gives a between
> and a within table instead of a three level one).
> 
> I am familiar with the connection between mixed models and repeated
> measures,and how mixed models are essentially replacing the aov models due
> to their greater flexibility. But I feel that despite understanding a
> little of the logic behind the mixed models that aov error terms seem
> completely different to me than lmer randoms.
> 
> I will post in those support lists you pass to me, if nothing comes from
> here. However I had little luck in the stats exchange when I tried there.
> 
> About a local expert, I am once more in a corner. there are many people in
> my department who excel in statistics. But I none use R, drastically
> reducing their ability to explain to me the output of aov.
> 
> Em 28 de dez de 2017 20:04, "Bert Gunter" <bgunter.4567 at gmail.com> escreveu:
> 
>> Jorge:
>> 
>> FYI, *generally speaking,* queries that are mostly statistical in
>> nature, such as yours, are off topic here -- this list is about R
>> programming help, not statistical help. Having said that, you still
>> may get a useful response here -- the r-help/statistics intersection
>> *is* nonempty. However, if not, 2.5 suggestions:
>> 
>> 1. Try posting to r-sig-mixed-models instead. Repeated measures are a
>> type of mixed/multilevel model and you may receive some useful
>> suggestions there, including alternative R approaches to fitting such
>> model (e.g. using lme() or lmer() ).
>> 
>> 2. Alternatively, try posting to a statistics site like
>> stats.stackexchange.com.
>> 
>> 2.5. Or, if you can, the best idea might be to sit down with a local
>> statistics expert.
>> 
>> Cheers,
>> Bert
>> 
>> 
>> 
>> Bert Gunter
>> 
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>> 
>> 
>> On Thu, Dec 28, 2017 at 7:52 AM, Jorge Fernando Saraiva de Menezes
>> <jorgefernandosaraiva at gmail.com> wrote:
>>> Dear list users,
>>> 
>>> I am trying to learn Repeated measures ANOVA using the aov() interface,
>> but
>>> I'm struggling to understand its output.
>>> 
>>> According to tutorials on the web, formula for a repeated measures design
>>> is:
>>> 
>>> aov(Y ~ IV+ Error(SUBJECT/IV) )
>>> 
>>> This formula does work but it returns three strata (Error:SUBJECT, Error:
>>> SUBJECT:IV, Error: Within), when I would expect two strata (Within and
>>> Between subjects). I've seems some tutorials  show the exactly same
>> setup,
>>> but returning only the two first strata.
>>> 
>>> Is it possible to have two or three strata depending on the data?
>>> If there is always three strata, how this would fit the interpretation of
>>> between vs within effects?
>>> 
>>> Below a reproducible example that gives three strata:
>>> 
>>> data(beavers)
>>> data=data.frame(id =
>>> rep(c("beaver1","beaver2"),c(nrow(beaver1),nrow(beaver2))),
>> rbind(beaver1,beaver2))
>>> data$activ=factor(data$activ)
>>> #balance dataset to have 6 samples for every combination of beaver and
>>> activity.
>>> balanced = split(data,interaction(data$id,data$activ))
>>> sizes = sapply(balanced,nrow)
>>> selected = lapply(sizes,sample.int,6)
>>> balanced = mapply(function(x,y) {x[y,]}, balanced,selected,SIMPLIFY=F)
>>> balanced = do.call(rbind,balanced)
>>> aov(temp~activ+Error(id/activ),data=balanced)
>>> 
>>> Thanks,
>>> Jorge
>>> 
>>>        [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pdalgd at gmail.com  Fri Dec 29 09:13:22 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 29 Dec 2017 09:13:22 +0100
Subject: [R] Help with dates
In-Reply-To: <1FEFB22E-E4D0-4ADD-BB2F-7E9FD75CFEC9@uiowa.edu>
References: <49CCE5B4-BE17-4113-AF9B-29E9B65515BB@icloud.com>
 <1FEFB22E-E4D0-4ADD-BB2F-7E9FD75CFEC9@uiowa.edu>
Message-ID: <76913F19-95BD-4C80-8148-98ED0B8E5E5F@gmail.com>

Mostly agree, except that I would suggest hardcoding the notion of "in the future", so that you don't get surprises when someone reruns your code 20 years from now.

-pd

> On 28 Dec 2017, at 21:54 , Simmering, Jacob E <jacob-simmering at uiowa.edu> wrote:
> 
> Your dates have an incomplete year information with 34. R assumes that 00-68 are 2000 to 2068 and 69 to 99 are 1969 to 1999. See ?strptime and the details for %y. 
> 
> You can either append ?19? to the start of your year variable to make it completely express the year or check if the date is in the future (assuming all dates should be in the past) and subtract 100 years from the date. 
> 
> 
>> On Dec 28, 2017, at 11:13 AM, Ramesh YAPALPARVI <ramesh.yapalparvi at icloud.com> wrote:
>> 
>> Hi all,
>> 
>> I?m struggling to get the dates in proper format.
>> I have dates as factors and is in the form 02/27/34( 34 means 1934). If I use
>> 
>> as.Date with format %d%m%y it gets converted to 2034-02-27. I tried changing the origin in the as.Date command but nothing worked. Any help is appreciated.
>> 
>> Thanks,
>> Ramesh
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From topijush at gmail.com  Fri Dec 29 08:29:26 2017
From: topijush at gmail.com (Pijush Das)
Date: Fri, 29 Dec 2017 12:59:26 +0530
Subject: [R] Facing problem in installing the package named "methyAnalysis"
Message-ID: <CAGa91zgSV9qyOsa-7pGX0jNHFtd_BU2q5dpOiWh1pDAJo_74FA@mail.gmail.com>

Dear Sir,




I have been using R for a long time. But recently I have faced a problem
when installing the Bioconductor package named "methyAnalysis". Firstly it
was require to update my older R (R version 3.4.3 (2017-11-30)) in to newer
version. That time I have also updated the RStudio software.

After that when I have tried to install the package named "methyAnalysis".
It shows some error given below.

No methods found in package ?IRanges? for requests: ?%in%?,
?elementLengths?, ?elementMetadata?, ?ifelse?, ?queryHits?, ?Rle?,
?subjectHits?, ?t? when loading ?bumphunter?
Error: package or namespace load failed for ?methyAnalysis?:
 objects ?.__T__split:base?, ?split? are not exported by 'namespace:IRanges'
In addition: Warning message:
replacing previous import ?BiocGenerics::image? by ?graphics::image? when
loading ?methylumi?

I also try to install the package after downloading the source package from
Bioconductor but the method is useless.

Please help me to install the package named "methyAnalysis".

Thanking you



regards
Pijush

	[[alternative HTML version deleted]]


From abouelmakarim1962 at gmail.com  Fri Dec 29 11:45:25 2017
From: abouelmakarim1962 at gmail.com (AbouEl-Makarim Aboueissa)
Date: Fri, 29 Dec 2017 05:45:25 -0500
Subject: [R] Draw Overlapping Circles with shaded tracks
Message-ID: <CAE9stmfEke0yLe4w+crBWF7uCFfvseLqCrJYOxxu1BsNnch+2w@mail.gmail.com>

Dear All:


I am wondering if there is a way in R to draw these two circles with shaded
tracks in both circles using R, and make both circles uncovered. I am
trying to make it in MS words, but I could not. Your help will be highly
appreciated.


In my previous post I added the image of the two circles, but the post
never published. I just thought to resent the post again without the image.

with many thanks
abou
______________________


*AbouEl-Makarim Aboueissa, PhD*

*Professor of Statistics*

*Department of Mathematics and Statistics*
*University of Southern Maine*

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Fri Dec 29 12:50:55 2017
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Fri, 29 Dec 2017 11:50:55 +0000
Subject: [R] Facing problem in installing the package named
 "methyAnalysis"
In-Reply-To: <CAGa91zgSV9qyOsa-7pGX0jNHFtd_BU2q5dpOiWh1pDAJo_74FA@mail.gmail.com>
References: <CAGa91zgSV9qyOsa-7pGX0jNHFtd_BU2q5dpOiWh1pDAJo_74FA@mail.gmail.com>
Message-ID: <ee91fcdb-cb59-0da6-d629-94540fcc878e@dewey.myzen.co.uk>

Dear Pijush

You might do better to ask on the Bioconductor list as IRanges does not 
seem to be on CRAN so I deduce it is a Bioconductor package too.

Michael

On 29/12/2017 07:29, Pijush Das wrote:
> Dear Sir,
> 
> 
> 
> 
> I have been using R for a long time. But recently I have faced a problem
> when installing the Bioconductor package named "methyAnalysis". Firstly it
> was require to update my older R (R version 3.4.3 (2017-11-30)) in to newer
> version. That time I have also updated the RStudio software.
> 
> After that when I have tried to install the package named "methyAnalysis".
> It shows some error given below.
> 
> No methods found in package ?IRanges? for requests: ?%in%?,
> ?elementLengths?, ?elementMetadata?, ?ifelse?, ?queryHits?, ?Rle?,
> ?subjectHits?, ?t? when loading ?bumphunter?
> Error: package or namespace load failed for ?methyAnalysis?:
>   objects ?.__T__split:base?, ?split? are not exported by 'namespace:IRanges'
> In addition: Warning message:
> replacing previous import ?BiocGenerics::image? by ?graphics::image? when
> loading ?methylumi?
> 
> I also try to install the package after downloading the source package from
> Bioconductor but the method is useless.
> 
> Please help me to install the package named "methyAnalysis".
> 
> Thanking you
> 
> 
> 
> regards
> Pijush
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From lucam1968 at gmail.com  Fri Dec 29 15:31:44 2017
From: lucam1968 at gmail.com (Luca Meyer)
Date: Fri, 29 Dec 2017 15:31:44 +0100
Subject: [R] Writing text files out of a dataset
Message-ID: <CABQyo84nPBPYVxzzu9pNT5+ZhYOuypM+=49xEE9ZzWPfqCKczg@mail.gmail.com>

Hello,

I am trying to run the following syntax for all cases within the dataframe
"data"

d1 <- data[1,c("material")]
fileConn<-file("TESTI/d1.txt")
writeLines(d1, fileConn)
close(fileConn)

I am trying to use the for function:

 for (i in 1:nrow(data)){
  d[i] <- data[i,c("material")]
  fileConn<-file("TESTI/d[i].txt")
  writeLines(d[i], fileConn)
  close(fileConn)
}

but I get the error:

Object "d" not found

Any suggestion on how I can solve the above?

Thanks,

Luca

	[[alternative HTML version deleted]]


From ericjberger at gmail.com  Fri Dec 29 16:13:37 2017
From: ericjberger at gmail.com (Eric Berger)
Date: Fri, 29 Dec 2017 17:13:37 +0200
Subject: [R] Writing text files out of a dataset
In-Reply-To: <CABQyo84nPBPYVxzzu9pNT5+ZhYOuypM+=49xEE9ZzWPfqCKczg@mail.gmail.com>
References: <CABQyo84nPBPYVxzzu9pNT5+ZhYOuypM+=49xEE9ZzWPfqCKczg@mail.gmail.com>
Message-ID: <CAGgJW77N2fbt09+o4oEzJAMLuibV6Reog0hKNnjP3q0LpazU+w@mail.gmail.com>

You have an error with the filename in the loop.
Try replacing the relevant line wtih
fileConn<-file(sprintf("TESTI/%d.txt",i))

HTH,
Eric


On Fri, Dec 29, 2017 at 4:31 PM, Luca Meyer <lucam1968 at gmail.com> wrote:

> Hello,
>
> I am trying to run the following syntax for all cases within the dataframe
> "data"
>
> d1 <- data[1,c("material")]
> fileConn<-file("TESTI/d1.txt")
> writeLines(d1, fileConn)
> close(fileConn)
>
> I am trying to use the for function:
>
>  for (i in 1:nrow(data)){
>   d[i] <- data[i,c("material")]
>   fileConn<-file("TESTI/d[i].txt")
>   writeLines(d[i], fileConn)
>   close(fileConn)
> }
>
> but I get the error:
>
> Object "d" not found
>
> Any suggestion on how I can solve the above?
>
> Thanks,
>
> Luca
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From topijush at gmail.com  Fri Dec 29 13:00:06 2017
From: topijush at gmail.com (Pijush Das)
Date: Fri, 29 Dec 2017 17:30:06 +0530
Subject: [R] Facing problem in installing the package named
	"methyAnalysis"
In-Reply-To: <ee91fcdb-cb59-0da6-d629-94540fcc878e@dewey.myzen.co.uk>
References: <CAGa91zgSV9qyOsa-7pGX0jNHFtd_BU2q5dpOiWh1pDAJo_74FA@mail.gmail.com>
 <ee91fcdb-cb59-0da6-d629-94540fcc878e@dewey.myzen.co.uk>
Message-ID: <CAGa91zjtV3JMnLpNdwdxTaXb4kcy1yCePJMwRZuj5KW+yuwDYQ@mail.gmail.com>

Thank you Michael Dewey.
Can you please send me the email id for Bioconductor.




regards
Pijush

On Fri, Dec 29, 2017 at 5:20 PM, Michael Dewey <lists at dewey.myzen.co.uk>
wrote:

> Dear Pijush
>
> You might do better to ask on the Bioconductor list as IRanges does not
> seem to be on CRAN so I deduce it is a Bioconductor package too.
>
> Michael
>
>
> On 29/12/2017 07:29, Pijush Das wrote:
>
>> Dear Sir,
>>
>>
>>
>>
>> I have been using R for a long time. But recently I have faced a problem
>> when installing the Bioconductor package named "methyAnalysis". Firstly it
>> was require to update my older R (R version 3.4.3 (2017-11-30)) in to
>> newer
>> version. That time I have also updated the RStudio software.
>>
>> After that when I have tried to install the package named "methyAnalysis".
>> It shows some error given below.
>>
>> No methods found in package ?IRanges? for requests: ?%in%?,
>> ?elementLengths?, ?elementMetadata?, ?ifelse?, ?queryHits?, ?Rle?,
>> ?subjectHits?, ?t? when loading ?bumphunter?
>> Error: package or namespace load failed for ?methyAnalysis?:
>>   objects ?.__T__split:base?, ?split? are not exported by
>> 'namespace:IRanges'
>> In addition: Warning message:
>> replacing previous import ?BiocGenerics::image? by ?graphics::image? when
>> loading ?methylumi?
>>
>> I also try to install the package after downloading the source package
>> from
>> Bioconductor but the method is useless.
>>
>> Please help me to install the package named "methyAnalysis".
>>
>> Thanking you
>>
>>
>>
>> regards
>> Pijush
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
>

	[[alternative HTML version deleted]]


From jorgefernandosaraiva at gmail.com  Fri Dec 29 14:53:16 2017
From: jorgefernandosaraiva at gmail.com (Jorge Fernando Saraiva de Menezes)
Date: Fri, 29 Dec 2017 15:53:16 +0200
Subject: [R] Why aov() with Error() gives three strata?
In-Reply-To: <1DB5E61E-C7C2-4D36-873C-8A96E0C87681@gmail.com>
References: <CALnMTgEmPB7+XNos31LvCx0J1UmqF4ffhHZDhS4eq+uQUD=Bfg@mail.gmail.com>
 <CAGxFJbQ=61qq2B4+qBkoHrSXkZ-8PSzA0AfJeF59BUQbp+iwzg@mail.gmail.com>
 <CALnMTgEQemvLWWYAfe6cSZrKr4jHdKnWhNukOKwd+4_kss_Rvg@mail.gmail.com>
 <1DB5E61E-C7C2-4D36-873C-8A96E0C87681@gmail.com>
Message-ID: <CALnMTgET-c3m6C6zfq_wutxBCF-P+KVrr1MJqu0hLPCvOrdLBw@mail.gmail.com>

Thank you Peter, I now understand it. Indeed, when there is only one
replicate per combination of IV and SUBJECT there is only two strata. The
reason this behavior is not observed in other statistical tools is because
they usually do not allow more than one measure per combination of period
and subject.

Cheers,
Jorge






2017-12-29 9:41 GMT+02:00 peter dalgaard <pdalgd at gmail.com>:

> At any rate:
>
> Error(SUBJECT/IV)
>
> specifies two random effects: SUBJECT and SUBJECT:IV. This is most easily
> understood if you conceptually arrange your data in a SUBJECT x IV table:
> One effect is a set of random errors added to each row, the other is a set
> of effects added to each cell.
>
> If you have more than one observation within each cell, then you need a
> third set of errors to account for differences within cells and this is
> labeled "Within" variation. With one observation per cell, this stratum
> disappears (as far as I recall, haven't checked).
>
> Actually, this oversimplifies a little: What actually happens is that data
> gets split into
>
> 1: row means
> 2: differences between cells within rows
> 3: differences between observations within cells
>
> and if the stratum variances are decreasing, then this can be interpreted
> using random effects as above, with variances of each component
> proportional to the successive differences. (All assuming that you have a
> balanced data layout, otherwise aov() is just the wrong tool.)
>
> -pd
>
> > On 28 Dec 2017, at 19:36 , Jorge Fernando Saraiva de Menezes <
> jorgefernandosaraiva at gmail.com> wrote:
> >
> > Bert, thanks for the reply but I feel that my question is less about
> > statistics and more about R interface. Specifically, because the output
> of
> > R seems different than other programs (systat, for example, gives a
> between
> > and a within table instead of a three level one).
> >
> > I am familiar with the connection between mixed models and repeated
> > measures,and how mixed models are essentially replacing the aov models
> due
> > to their greater flexibility. But I feel that despite understanding a
> > little of the logic behind the mixed models that aov error terms seem
> > completely different to me than lmer randoms.
> >
> > I will post in those support lists you pass to me, if nothing comes from
> > here. However I had little luck in the stats exchange when I tried there.
> >
> > About a local expert, I am once more in a corner. there are many people
> in
> > my department who excel in statistics. But I none use R, drastically
> > reducing their ability to explain to me the output of aov.
> >
> > Em 28 de dez de 2017 20:04, "Bert Gunter" <bgunter.4567 at gmail.com>
> escreveu:
> >
> >> Jorge:
> >>
> >> FYI, *generally speaking,* queries that are mostly statistical in
> >> nature, such as yours, are off topic here -- this list is about R
> >> programming help, not statistical help. Having said that, you still
> >> may get a useful response here -- the r-help/statistics intersection
> >> *is* nonempty. However, if not, 2.5 suggestions:
> >>
> >> 1. Try posting to r-sig-mixed-models instead. Repeated measures are a
> >> type of mixed/multilevel model and you may receive some useful
> >> suggestions there, including alternative R approaches to fitting such
> >> model (e.g. using lme() or lmer() ).
> >>
> >> 2. Alternatively, try posting to a statistics site like
> >> stats.stackexchange.com.
> >>
> >> 2.5. Or, if you can, the best idea might be to sit down with a local
> >> statistics expert.
> >>
> >> Cheers,
> >> Bert
> >>
> >>
> >>
> >> Bert Gunter
> >>
> >> "The trouble with having an open mind is that people keep coming along
> >> and sticking things into it."
> >> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >>
> >>
> >> On Thu, Dec 28, 2017 at 7:52 AM, Jorge Fernando Saraiva de Menezes
> >> <jorgefernandosaraiva at gmail.com> wrote:
> >>> Dear list users,
> >>>
> >>> I am trying to learn Repeated measures ANOVA using the aov() interface,
> >> but
> >>> I'm struggling to understand its output.
> >>>
> >>> According to tutorials on the web, formula for a repeated measures
> design
> >>> is:
> >>>
> >>> aov(Y ~ IV+ Error(SUBJECT/IV) )
> >>>
> >>> This formula does work but it returns three strata (Error:SUBJECT,
> Error:
> >>> SUBJECT:IV, Error: Within), when I would expect two strata (Within and
> >>> Between subjects). I've seems some tutorials  show the exactly same
> >> setup,
> >>> but returning only the two first strata.
> >>>
> >>> Is it possible to have two or three strata depending on the data?
> >>> If there is always three strata, how this would fit the interpretation
> of
> >>> between vs within effects?
> >>>
> >>> Below a reproducible example that gives three strata:
> >>>
> >>> data(beavers)
> >>> data=data.frame(id =
> >>> rep(c("beaver1","beaver2"),c(nrow(beaver1),nrow(beaver2))),
> >> rbind(beaver1,beaver2))
> >>> data$activ=factor(data$activ)
> >>> #balance dataset to have 6 samples for every combination of beaver and
> >>> activity.
> >>> balanced = split(data,interaction(data$id,data$activ))
> >>> sizes = sapply(balanced,nrow)
> >>> selected = lapply(sizes,sample.int,6)
> >>> balanced = mapply(function(x,y) {x[y,]}, balanced,selected,SIMPLIFY=F)
> >>> balanced = do.call(rbind,balanced)
> >>> aov(temp~activ+Error(id/activ),data=balanced)
> >>>
> >>> Thanks,
> >>> Jorge
> >>>
> >>>        [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/
> >> posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From rbaer at atsu.edu  Fri Dec 29 18:05:47 2017
From: rbaer at atsu.edu (Robert Baer)
Date: Fri, 29 Dec 2017 11:05:47 -0600
Subject: [R] Facing problem in installing the package named
 "methyAnalysis"
In-Reply-To: <CAGa91zjtV3JMnLpNdwdxTaXb4kcy1yCePJMwRZuj5KW+yuwDYQ@mail.gmail.com>
References: <CAGa91zgSV9qyOsa-7pGX0jNHFtd_BU2q5dpOiWh1pDAJo_74FA@mail.gmail.com>
 <ee91fcdb-cb59-0da6-d629-94540fcc878e@dewey.myzen.co.uk>
 <CAGa91zjtV3JMnLpNdwdxTaXb4kcy1yCePJMwRZuj5KW+yuwDYQ@mail.gmail.com>
Message-ID: <2eac2213-da0d-0177-c355-f2a1fb5b3e82@atsu.edu>

Bioconductor help is here:

https://www.bioconductor.org/help/



On 12/29/2017 6:00 AM, Pijush Das wrote:
> Thank you Michael Dewey.
> Can you please send me the email id for Bioconductor.
>
>
>
>
> regards
> Pijush
>
> On Fri, Dec 29, 2017 at 5:20 PM, Michael Dewey <lists at dewey.myzen.co.uk>
> wrote:
>
>> Dear Pijush
>>
>> You might do better to ask on the Bioconductor list as IRanges does not
>> seem to be on CRAN so I deduce it is a Bioconductor package too.
>>
>> Michael
>>
>>
>> On 29/12/2017 07:29, Pijush Das wrote:
>>
>>> Dear Sir,
>>>
>>>
>>>
>>>
>>> I have been using R for a long time. But recently I have faced a problem
>>> when installing the Bioconductor package named "methyAnalysis". Firstly it
>>> was require to update my older R (R version 3.4.3 (2017-11-30)) in to
>>> newer
>>> version. That time I have also updated the RStudio software.
>>>
>>> After that when I have tried to install the package named "methyAnalysis".
>>> It shows some error given below.
>>>
>>> No methods found in package ?IRanges? for requests: ?%in%?,
>>> ?elementLengths?, ?elementMetadata?, ?ifelse?, ?queryHits?, ?Rle?,
>>> ?subjectHits?, ?t? when loading ?bumphunter?
>>> Error: package or namespace load failed for ?methyAnalysis?:
>>>    objects ?.__T__split:base?, ?split? are not exported by
>>> 'namespace:IRanges'
>>> In addition: Warning message:
>>> replacing previous import ?BiocGenerics::image? by ?graphics::image? when
>>> loading ?methylumi?
>>>
>>> I also try to install the package after downloading the source package
>>> from
>>> Bioconductor but the method is useless.
>>>
>>> Please help me to install the package named "methyAnalysis".
>>>
>>> Thanking you
>>>
>>>
>>>
>>> regards
>>> Pijush
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>> ng-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>> --
>> Michael
>> http://www.dewey.myzen.co.uk/home.html
>>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From martin.morgan at roswellpark.org  Fri Dec 29 17:58:27 2017
From: martin.morgan at roswellpark.org (Martin Morgan)
Date: Fri, 29 Dec 2017 11:58:27 -0500
Subject: [R] Facing problem in installing the package named
 "methyAnalysis"
In-Reply-To: <CAGa91zjtV3JMnLpNdwdxTaXb4kcy1yCePJMwRZuj5KW+yuwDYQ@mail.gmail.com>
References: <CAGa91zgSV9qyOsa-7pGX0jNHFtd_BU2q5dpOiWh1pDAJo_74FA@mail.gmail.com>
 <ee91fcdb-cb59-0da6-d629-94540fcc878e@dewey.myzen.co.uk>
 <CAGa91zjtV3JMnLpNdwdxTaXb4kcy1yCePJMwRZuj5KW+yuwDYQ@mail.gmail.com>
Message-ID: <53374c51-428c-45b4-2647-1af62bf8bb4c@roswellpark.org>

On 12/29/2017 07:00 AM, Pijush Das wrote:
> Thank you Michael Dewey.
> Can you please send me the email id for Bioconductor.

https://support.bioconductor.org

Make sure you are using packages from a consistent version of Bioconductor

   source("https://bioconductor.org/biocLite.R")
   BiocInstaller::biocValid()

Martin

> 
> 
> 
> 
> regards
> Pijush
> 
> On Fri, Dec 29, 2017 at 5:20 PM, Michael Dewey <lists at dewey.myzen.co.uk>
> wrote:
> 
>> Dear Pijush
>>
>> You might do better to ask on the Bioconductor list as IRanges does not
>> seem to be on CRAN so I deduce it is a Bioconductor package too.
>>
>> Michael
>>
>>
>> On 29/12/2017 07:29, Pijush Das wrote:
>>
>>> Dear Sir,
>>>
>>>
>>>
>>>
>>> I have been using R for a long time. But recently I have faced a problem
>>> when installing the Bioconductor package named "methyAnalysis". Firstly it
>>> was require to update my older R (R version 3.4.3 (2017-11-30)) in to
>>> newer
>>> version. That time I have also updated the RStudio software.
>>>
>>> After that when I have tried to install the package named "methyAnalysis".
>>> It shows some error given below.
>>>
>>> No methods found in package ?IRanges? for requests: ?%in%?,
>>> ?elementLengths?, ?elementMetadata?, ?ifelse?, ?queryHits?, ?Rle?,
>>> ?subjectHits?, ?t? when loading ?bumphunter?
>>> Error: package or namespace load failed for ?methyAnalysis?:
>>>    objects ?.__T__split:base?, ?split? are not exported by
>>> 'namespace:IRanges'
>>> In addition: Warning message:
>>> replacing previous import ?BiocGenerics::image? by ?graphics::image? when
>>> loading ?methylumi?
>>>
>>> I also try to install the package after downloading the source package
>>> from
>>> Bioconductor but the method is useless.
>>>
>>> Please help me to install the package named "methyAnalysis".
>>>
>>> Thanking you
>>>
>>>
>>>
>>> regards
>>> Pijush
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>> ng-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>> --
>> Michael
>> http://www.dewey.myzen.co.uk/home.html
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


This email message may contain legally privileged and/or...{{dropped:2}}


From ruipbarradas at sapo.pt  Fri Dec 29 18:49:20 2017
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Fri, 29 Dec 2017 17:49:20 +0000
Subject: [R] Writing text files out of a dataset
In-Reply-To: <CABQyo84nPBPYVxzzu9pNT5+ZhYOuypM+=49xEE9ZzWPfqCKczg@mail.gmail.com>
References: <CABQyo84nPBPYVxzzu9pNT5+ZhYOuypM+=49xEE9ZzWPfqCKczg@mail.gmail.com>
Message-ID: <7e009ee6-91f3-64ec-c304-6d0252adcb8b@sapo.pt>

Hello,

You have to create the vector 'd' outside the loop before using it.

d <- numeric(nrow(data))

Only then comes the loop.

Hope this helps,

Rui Barradas

On 12/29/2017 2:31 PM, Luca Meyer wrote:
> Hello,
> 
> I am trying to run the following syntax for all cases within the dataframe
> "data"
> 
> d1 <- data[1,c("material")]
> fileConn<-file("TESTI/d1.txt")
> writeLines(d1, fileConn)
> close(fileConn)
> 
> I am trying to use the for function:
> 
>   for (i in 1:nrow(data)){
>    d[i] <- data[i,c("material")]
>    fileConn<-file("TESTI/d[i].txt")
>    writeLines(d[i], fileConn)
>    close(fileConn)
> }
> 
> but I get the error:
> 
> Object "d" not found
> 
> Any suggestion on how I can solve the above?
> 
> Thanks,
> 
> Luca
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jdnewmil at dcn.davis.ca.us  Fri Dec 29 20:00:09 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 29 Dec 2017 11:00:09 -0800
Subject: [R] Writing text files out of a dataset
In-Reply-To: <7e009ee6-91f3-64ec-c304-6d0252adcb8b@sapo.pt>
References: <CABQyo84nPBPYVxzzu9pNT5+ZhYOuypM+=49xEE9ZzWPfqCKczg@mail.gmail.com>
 <7e009ee6-91f3-64ec-c304-6d0252adcb8b@sapo.pt>
Message-ID: <C64EC15D-7875-4EAD-9321-58C7C7C6F53F@dcn.davis.ca.us>

This presumes that 'data[["material"]]' is numeric.

It is unnecessary to put the output data into a separate vector d (it is already in a vector that is part of the 'data' data frame).  I would just overwrite `d` (or `d1`) each time through the loop:

 for (i in seq_along( data[["material"]] ) ){
    d <- data[i,c("material")]
    fileConn <- file(paste0("TESTI/d", i, ".txt"))
    writeLines(d, fileConn)
    close(fileConn)
}

or

for ( i in seq_along( data[["material"]] ) ){
   writeLines( data[i,c("material")], file=paste0("TESTI/d", i, ".txt" ) )
}
-- 
Sent from my phone. Please excuse my brevity.

On December 29, 2017 9:49:20 AM PST, Rui Barradas <ruipbarradas at sapo.pt> wrote:
>Hello,
>
>You have to create the vector 'd' outside the loop before using it.
>
>d <- numeric(nrow(data))
>
>Only then comes the loop.
>
>Hope this helps,
>
>Rui Barradas
>
>On 12/29/2017 2:31 PM, Luca Meyer wrote:
>> Hello,
>> 
>> I am trying to run the following syntax for all cases within the
>dataframe
>> "data"
>> 
>> d1 <- data[1,c("material")]
>> fileConn<-file("TESTI/d1.txt")
>> writeLines(d1, fileConn)
>> close(fileConn)
>> 
>> I am trying to use the for function:
>> 
>>   for (i in 1:nrow(data)){
>>    d[i] <- data[i,c("material")]
>>    fileConn<-file("TESTI/d[i].txt")
>>    writeLines(d[i], fileConn)
>>    close(fileConn)
>> }
>> 
>> but I get the error:
>> 
>> Object "d" not found
>> 
>> Any suggestion on how I can solve the above?
>> 
>> Thanks,
>> 
>> Luca
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From rsherry8 at comcast.net  Fri Dec 29 19:31:15 2017
From: rsherry8 at comcast.net (rsherry8)
Date: Fri, 29 Dec 2017 13:31:15 -0500
Subject: [R] RQuantLib
In-Reply-To: <CAPPM_gRcw9UKaN+vCuB3gZkGwCZA=juBBAsx3QyCPC3CYVe+Vw@mail.gmail.com>
References: <5A458631.6070409@comcast.net>
 <CAPPM_gRcw9UKaN+vCuB3gZkGwCZA=juBBAsx3QyCPC3CYVe+Vw@mail.gmail.com>
Message-ID: <5A4689F3.8000503@comcast.net>

Joshua,

Thanks for the response. When you said at least version 3.4.0, I 
upgraded to 3.4.2 which I believe is the current version. Now, I 
attempted to install the package RQuantLib but it did not work. Here is 
what I got:

 > install.packages("RQuantLib")
Installing package into ?C:/Users/rsher/Documents/R/win-library/3.4?
(as ?lib? is unspecified)
Warning message:
package ?RQuantLib? is not available (for R version 3.4.2)


Please help.
Thanks,
Bob Sherry

On 12/28/2017 10:28 PM, Joshua Ulrich wrote:
> On Thu, Dec 28, 2017 at 6:02 PM, rsherry8 <rsherry8 at comcast.net> wrote:
>> I have recently installed R on my new computer. I also want to install the
>> package RQuantLib. So I run the following command and get the following
>> output:
>>
>>>   install.packages("RQuantLib")
>> Installing package into ?C:/Users/rsher/Documents/R/win-library/3.2?
>> (as ?lib? is unspecified)
>> --- Please select a CRAN mirror for use in this session ---
>> Warning message:
>> package ?RQuantLib? is not available (for R version 3.2.4 Revised)
>>
>> The package did not install. Am I doing something wrong. Is the package
>> going to be updated for the latest version of R?
>>
> Windows binary packages are only built for the most current (major)
> version of R.  You need to upgrade to at least R-3.4.0, or you will
> have to install RQuantLib (and therefore QuantLib itself) from source.
>
>> Thanks,
>> Bob
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>


From esawiek at gmail.com  Fri Dec 29 21:18:45 2017
From: esawiek at gmail.com (Ek Esawi)
Date: Fri, 29 Dec 2017 15:18:45 -0500
Subject: [R] Help with script
In-Reply-To: <ddd8a2e1-b64a-1308-60a1-3199bdc01918@yale.edu>
References: <ddd8a2e1-b64a-1308-60a1-3199bdc01918@yale.edu>
Message-ID: <CA+ZkTxuRf6JhH4FcHLbvWk9deWGoSGVuG4jMBr75eOQcJ=Ze_Q@mail.gmail.com>

 Hi Pablo,

Rui's suggestion is probably the best and easiest. In addition to
Eric's, i think the apply functions will work well here too.

Best of luck,
EK

On Thu, Dec 28, 2017 at 7:03 PM, PABLO ORTIZ PINEDA
<pablo.ortiz at yale.edu> wrote:
> Hello there. Happy new year for everyone!
>
> I need help with a table. This table contains 300 rows and 192 columns.
> Being the first column the ID of my samples that can have several
> observations.
>
> I need to generate e NEW table that contains a single ID with the sum of
> the observations by columns:
> For example:
>
> Example
> ID   A    B    C    D    E    F    G.... 191 columns
> a1   0    0    0    1    1    2    0...
> a2   0    1    0    1    2    2    1...
> a2   0    1    1    2    0    2    1...
> a3   0    1    1    1    1    1    1....
> ...300rows
> In this case I want to make a new table in which there is only 1 ID and
> the values of each columns A...G are added. I
> n the example the new table would have only 3 IDs. a1, a2 and 3 and a2
> has the values added by column:
> a2   0   2   1   3   2   4   2..
>
> Thank you so much and have a wonderful year!.
>
> --
> Pablo A. Ortiz-Pineda (Ph.D.)
> Molecular Biology & Bioinformatics
> Yale University. School of Medicine.
> Pediatrics Department.
> New Haven, CT 06510
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.tu-dortmund.de  Fri Dec 29 22:52:07 2017
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Fri, 29 Dec 2017 22:52:07 +0100
Subject: [R] package : plm : pgmm question
In-Reply-To: <PN1PR01MB07844FF8F4A6748E3DE477B0A1010@PN1PR01MB0784.INDPRD01.PROD.OUTLOOK.COM>
References: <PN1PR01MB07844FF8F4A6748E3DE477B0A1010@PN1PR01MB0784.INDPRD01.PROD.OUTLOOK.COM>
Message-ID: <98035b92-e0a9-65aa-f6c3-5123f1121345@statistik.tu-dortmund.de>

Please talk to the package maintainer whi may not be listen on R-help.

Best,
Uwe Ligges


On 25.12.2017 10:53, Ye Dong wrote:
> Dear Sir,
> 
> 
> 
> I am using the package pgmm you build in panel regression. However, I found that when T is 10, N=30, the error would show as following:
> 
> 
> 
> system is computationally singular: reciprocal condition number
> 
> But the similar code works well on Stata, so I wonder  how I can optimize the algorithm, for example , the inverse matrix optimization ? And I have checked my data as well, no multicollinearity problem exists. Another problem is that although I have some NA in the panel data, the panel dataframe is still recognized as balanced model. But with plm, the dataframe would be recognized unbalanced.
> 
> Thanks and Best regards,
> 
> Ye Dong
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From drjimlemon at gmail.com  Sat Dec 30 00:06:19 2017
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sat, 30 Dec 2017 10:06:19 +1100
Subject: [R] Draw Overlapping Circles with shaded tracks
In-Reply-To: <CAE9stmfEke0yLe4w+crBWF7uCFfvseLqCrJYOxxu1BsNnch+2w@mail.gmail.com>
References: <CAE9stmfEke0yLe4w+crBWF7uCFfvseLqCrJYOxxu1BsNnch+2w@mail.gmail.com>
Message-ID: <CA+8X3fWsJeoTWAVx569oDQ3m-CODn=gDFRVQjmzzNZsJODoHVQ@mail.gmail.com>

Hi Abou,
Without an illustration it's hard to work out what you want. here is a
simple example of two circles using semi-transparency. Is this any
help?

pdf("circles.pdf")
plot(0:10,type="n",axes=FALSE,xlab="",ylab="")
draw.circle(4,5,radius=3,border="#ff0000aa",lwd=10)
draw.circle(6,5,radius=3,border="#0000ffaa",lwd=10)
dev.off()

Jim

On Fri, Dec 29, 2017 at 9:45 PM, AbouEl-Makarim Aboueissa
<abouelmakarim1962 at gmail.com> wrote:
> Dear All:
>
>
> I am wondering if there is a way in R to draw these two circles with shaded
> tracks in both circles using R, and make both circles uncovered. I am
> trying to make it in MS words, but I could not. Your help will be highly
> appreciated.
>
>
> In my previous post I added the image of the two circles, but the post
> never published. I just thought to resent the post again without the image.
>
> with many thanks
> abou
> ______________________
>
>
> *AbouEl-Makarim Aboueissa, PhD*
>
> *Professor of Statistics*
>
> *Department of Mathematics and Statistics*
> *University of Southern Maine*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From orvaquim at gmail.com  Sat Dec 30 08:24:28 2017
From: orvaquim at gmail.com (Orvalho Augusto)
Date: Fri, 29 Dec 2017 23:24:28 -0800
Subject: [R] RQuantLib
In-Reply-To: <5A4689F3.8000503@comcast.net>
References: <5A458631.6070409@comcast.net>
 <CAPPM_gRcw9UKaN+vCuB3gZkGwCZA=juBBAsx3QyCPC3CYVe+Vw@mail.gmail.com>
 <5A4689F3.8000503@comcast.net>
Message-ID: <CAF4WX-c5PM4_oReOfOE_5HDC6JeykDW8_4zRA1x0mGmKxeqqEg@mail.gmail.com>

Hi Bob,

I don't know what is the cause of your trouble but try this:
1. Download the zip of package.

2. And install it from local zip files. This you find on the Packages menu.

Hope it helps
OA


On Fri, Dec 29, 2017 at 10:31 AM, rsherry8 <rsherry8 at comcast.net> wrote:

> Joshua,
>
> Thanks for the response. When you said at least version 3.4.0, I upgraded
> to 3.4.2 which I believe is the current version. Now, I attempted to
> install the package RQuantLib but it did not work. Here is what I got:
>
> > install.packages("RQuantLib")
> Installing package into ?C:/Users/rsher/Documents/R/win-library/3.4?
> (as ?lib? is unspecified)
> Warning message:
> package ?RQuantLib? is not available (for R version 3.4.2)
>
>
> Please help.
> Thanks,
> Bob Sherry
>
>
> On 12/28/2017 10:28 PM, Joshua Ulrich wrote:
>
>> On Thu, Dec 28, 2017 at 6:02 PM, rsherry8 <rsherry8 at comcast.net> wrote:
>>
>>> I have recently installed R on my new computer. I also want to install
>>> the
>>> package RQuantLib. So I run the following command and get the following
>>> output:
>>>
>>>   install.packages("RQuantLib")
>>>>
>>> Installing package into ?C:/Users/rsher/Documents/R/win-library/3.2?
>>> (as ?lib? is unspecified)
>>> --- Please select a CRAN mirror for use in this session ---
>>> Warning message:
>>> package ?RQuantLib? is not available (for R version 3.2.4 Revised)
>>>
>>> The package did not install. Am I doing something wrong. Is the package
>>> going to be updated for the latest version of R?
>>>
>>> Windows binary packages are only built for the most current (major)
>> version of R.  You need to upgrade to at least R-3.4.0, or you will
>> have to install RQuantLib (and therefore QuantLib itself) from source.
>>
>> Thanks,
>>> Bob
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>> ng-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Sat Dec 30 14:25:15 2017
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Sat, 30 Dec 2017 13:25:15 +0000
Subject: [R] RQuantLib
In-Reply-To: <5A4689F3.8000503@comcast.net>
References: <5A458631.6070409@comcast.net>
 <CAPPM_gRcw9UKaN+vCuB3gZkGwCZA=juBBAsx3QyCPC3CYVe+Vw@mail.gmail.com>
 <5A4689F3.8000503@comcast.net>
Message-ID: <b5324217-8160-2736-45c9-0bb6da4c215d@dewey.myzen.co.uk>

Dear Bob

In fact the current release is 3.4.3 I can think of no reason why that 
should matter here but it might be worth trying to upgrade to it.

Michael

On 29/12/2017 18:31, rsherry8 wrote:
> Joshua,
> 
> Thanks for the response. When you said at least version 3.4.0, I 
> upgraded to 3.4.2 which I believe is the current version. Now, I 
> attempted to install the package RQuantLib but it did not work. Here is 
> what I got:
> 
>  > install.packages("RQuantLib")
> Installing package into ?C:/Users/rsher/Documents/R/win-library/3.4?
> (as ?lib? is unspecified)
> Warning message:
> package ?RQuantLib? is not available (for R version 3.4.2)
> 
> 
> Please help.
> Thanks,
> Bob Sherry
> 
> On 12/28/2017 10:28 PM, Joshua Ulrich wrote:
>> On Thu, Dec 28, 2017 at 6:02 PM, rsherry8 <rsherry8 at comcast.net> wrote:
>>> I have recently installed R on my new computer. I also want to 
>>> install the
>>> package RQuantLib. So I run the following command and get the following
>>> output:
>>>
>>>> ? install.packages("RQuantLib")
>>> Installing package into ?C:/Users/rsher/Documents/R/win-library/3.2?
>>> (as ?lib? is unspecified)
>>> --- Please select a CRAN mirror for use in this session ---
>>> Warning message:
>>> package ?RQuantLib? is not available (for R version 3.2.4 Revised)
>>>
>>> The package did not install. Am I doing something wrong. Is the package
>>> going to be updated for the latest version of R?
>>>
>> Windows binary packages are only built for the most current (major)
>> version of R.? You need to upgrade to at least R-3.4.0, or you will
>> have to install RQuantLib (and therefore QuantLib itself) from source.
>>
>>> Thanks,
>>> Bob
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide 
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From jdnewmil at dcn.davis.ca.us  Sat Dec 30 14:48:22 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 30 Dec 2017 05:48:22 -0800
Subject: [R] RQuantLib
In-Reply-To: <b5324217-8160-2736-45c9-0bb6da4c215d@dewey.myzen.co.uk>
References: <5A458631.6070409@comcast.net>
 <CAPPM_gRcw9UKaN+vCuB3gZkGwCZA=juBBAsx3QyCPC3CYVe+Vw@mail.gmail.com>
 <5A4689F3.8000503@comcast.net>
 <b5324217-8160-2736-45c9-0bb6da4c215d@dewey.myzen.co.uk>
Message-ID: <531E7250-AD0F-42D1-AF67-DF2E3C489276@dcn.davis.ca.us>

Sometimes that message appears when using a CRAN mirror that is not up-to-date or has communication problems. Might also try another mirror.
-- 
Sent from my phone. Please excuse my brevity.

On December 30, 2017 5:25:15 AM PST, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>Dear Bob
>
>In fact the current release is 3.4.3 I can think of no reason why that 
>should matter here but it might be worth trying to upgrade to it.
>
>Michael
>
>On 29/12/2017 18:31, rsherry8 wrote:
>> Joshua,
>> 
>> Thanks for the response. When you said at least version 3.4.0, I 
>> upgraded to 3.4.2 which I believe is the current version. Now, I 
>> attempted to install the package RQuantLib but it did not work. Here
>is 
>> what I got:
>> 
>>  > install.packages("RQuantLib")
>> Installing package into ?C:/Users/rsher/Documents/R/win-library/3.4?
>> (as ?lib? is unspecified)
>> Warning message:
>> package ?RQuantLib? is not available (for R version 3.4.2)
>> 
>> 
>> Please help.
>> Thanks,
>> Bob Sherry
>> 
>> On 12/28/2017 10:28 PM, Joshua Ulrich wrote:
>>> On Thu, Dec 28, 2017 at 6:02 PM, rsherry8 <rsherry8 at comcast.net>
>wrote:
>>>> I have recently installed R on my new computer. I also want to 
>>>> install the
>>>> package RQuantLib. So I run the following command and get the
>following
>>>> output:
>>>>
>>>>> ? install.packages("RQuantLib")
>>>> Installing package into
>?C:/Users/rsher/Documents/R/win-library/3.2?
>>>> (as ?lib? is unspecified)
>>>> --- Please select a CRAN mirror for use in this session ---
>>>> Warning message:
>>>> package ?RQuantLib? is not available (for R version 3.2.4 Revised)
>>>>
>>>> The package did not install. Am I doing something wrong. Is the
>package
>>>> going to be updated for the latest version of R?
>>>>
>>> Windows binary packages are only built for the most current (major)
>>> version of R.? You need to upgrade to at least R-3.4.0, or you will
>>> have to install RQuantLib (and therefore QuantLib itself) from
>source.
>>>
>>>> Thanks,
>>>> Bob
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide 
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>-- 
>Michael
>http://www.dewey.myzen.co.uk/home.html
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From rsherry8 at comcast.net  Sat Dec 30 16:54:01 2017
From: rsherry8 at comcast.net (rsherry8)
Date: Sat, 30 Dec 2017 10:54:01 -0500
Subject: [R] RQuantLib
In-Reply-To: <CAF4WX-c5PM4_oReOfOE_5HDC6JeykDW8_4zRA1x0mGmKxeqqEg@mail.gmail.com>
References: <5A458631.6070409@comcast.net>
 <CAPPM_gRcw9UKaN+vCuB3gZkGwCZA=juBBAsx3QyCPC3CYVe+Vw@mail.gmail.com>
 <5A4689F3.8000503@comcast.net>
 <CAF4WX-c5PM4_oReOfOE_5HDC6JeykDW8_4zRA1x0mGmKxeqqEg@mail.gmail.com>
Message-ID: <5A47B699.4030007@comcast.net>

OA,

Thanks for the response. I downloaded the file RQuantLib_0.4.4.tar.gz 
into a directory c:\r.zip on my Windows machine. I then ran the 
following command and I received the following output:

 > install.packages("RQuantLib", lib="/r.zip/")
Warning message:
package ?RQuantLib? is not available (for R version 3.4.3)

I did not unpack the .gz file. Should I have?

Please comment.
Bob Sherry

On 12/30/2017 2:24 AM, Orvalho Augusto wrote:
> Hi Bob,
>
> I don't know what is the cause of your trouble but try this:
> 1. Download the zip of package.
>
> 2. And install it from local zip files. This you find on the Packages 
> menu.
>
> Hope it helps
> OA
>
>
> On Fri, Dec 29, 2017 at 10:31 AM, rsherry8 <rsherry8 at comcast.net 
> <mailto:rsherry8 at comcast.net>> wrote:
>
>     Joshua,
>
>     Thanks for the response. When you said at least version 3.4.0, I
>     upgraded to 3.4.2 which I believe is the current version. Now, I
>     attempted to install the package RQuantLib but it did not work.
>     Here is what I got:
>
>     > install.packages("RQuantLib")
>     Installing package into ?C:/Users/rsher/Documents/R/win-library/3.4?
>     (as ?lib? is unspecified)
>     Warning message:
>     package ?RQuantLib? is not available (for R version 3.4.2)
>
>
>     Please help.
>     Thanks,
>     Bob Sherry
>
>
>     On 12/28/2017 10:28 PM, Joshua Ulrich wrote:
>
>         On Thu, Dec 28, 2017 at 6:02 PM, rsherry8
>         <rsherry8 at comcast.net <mailto:rsherry8 at comcast.net>> wrote:
>
>             I have recently installed R on my new computer. I also
>             want to install the
>             package RQuantLib. So I run the following command and get
>             the following
>             output:
>
>                   install.packages("RQuantLib")
>
>             Installing package into
>             ?C:/Users/rsher/Documents/R/win-library/3.2?
>             (as ?lib? is unspecified)
>             --- Please select a CRAN mirror for use in this session ---
>             Warning message:
>             package ?RQuantLib? is not available (for R version 3.2.4
>             Revised)
>
>             The package did not install. Am I doing something wrong.
>             Is the package
>             going to be updated for the latest version of R?
>
>         Windows binary packages are only built for the most current
>         (major)
>         version of R.  You need to upgrade to at least R-3.4.0, or you
>         will
>         have to install RQuantLib (and therefore QuantLib itself) from
>         source.
>
>             Thanks,
>             Bob
>
>             ______________________________________________
>             R-help at r-project.org <mailto:R-help at r-project.org> mailing
>             list -- To UNSUBSCRIBE and more, see
>             https://stat.ethz.ch/mailman/listinfo/r-help
>             <https://stat.ethz.ch/mailman/listinfo/r-help>
>             PLEASE do read the posting guide
>             http://www.R-project.org/posting-guide.html
>             <http://www.R-project.org/posting-guide.html>
>             and provide commented, minimal, self-contained,
>             reproducible code.
>
>
>
>
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>     and provide commented, minimal, self-contained, reproducible code.
>
>


	[[alternative HTML version deleted]]


From rsherry8 at comcast.net  Sat Dec 30 17:08:55 2017
From: rsherry8 at comcast.net (rsherry8)
Date: Sat, 30 Dec 2017 11:08:55 -0500
Subject: [R] RQuantLib
In-Reply-To: <531E7250-AD0F-42D1-AF67-DF2E3C489276@dcn.davis.ca.us>
References: <5A458631.6070409@comcast.net>
 <CAPPM_gRcw9UKaN+vCuB3gZkGwCZA=juBBAsx3QyCPC3CYVe+Vw@mail.gmail.com>
 <5A4689F3.8000503@comcast.net>
 <b5324217-8160-2736-45c9-0bb6da4c215d@dewey.myzen.co.uk>
 <531E7250-AD0F-42D1-AF67-DF2E3C489276@dcn.davis.ca.us>
Message-ID: <5A47BA17.7050003@comcast.net>

Jeff,

I tired another mirror but that did not work. I tired the following two 
commands:

 > ap <- available.packages()
 > View(ap)

The output of the View command showed numerous packages available but it 
did not show RQuantLib.

Bob Sherry

On 12/30/2017 8:48 AM, Jeff Newmiller wrote:
> Sometimes that message appears when using a CRAN mirror that is not up-to-date or has communication problems. Might also try another mirror.
> -- Sent from my phone. Please excuse my brevity. On December 30, 2017 
> 5:25:15 AM PST, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>> >Dear Bob
>> >
>> >In fact the current release is 3.4.3 I can think of no reason why that
>> >should matter here but it might be worth trying to upgrade to it.
>> >
>> >Michael
>> >
>> >On 29/12/2017 18:31, rsherry8 wrote:
>>> >>Joshua,
>>> >>
>>> >>Thanks for the response. When you said at least version 3.4.0, I
>>> >>upgraded to 3.4.2 which I believe is the current version. Now, I
>>> >>attempted to install the package RQuantLib but it did not work. Here
>> >is
>>> >>what I got:
>>> >>
>>> >>  > install.packages("RQuantLib")
>>> >>Installing package into ?C:/Users/rsher/Documents/R/win-library/3.4?
>>> >>(as ?lib? is unspecified)
>>> >>Warning message:
>>> >>package ?RQuantLib? is not available (for R version 3.4.2)
>>> >>
>>> >>
>>> >>Please help.
>>> >>Thanks,
>>> >>Bob Sherry
>>> >>
>>> >>On 12/28/2017 10:28 PM, Joshua Ulrich wrote:
>>>> >>>On Thu, Dec 28, 2017 at 6:02 PM, rsherry8<rsherry8 at comcast.net>
>> >wrote:
>>>>> >>>>I have recently installed R on my new computer. I also want to
>>>>> >>>>install the
>>>>> >>>>package RQuantLib. So I run the following command and get the
>> >following
>>>>> >>>>output:
>>>>> >>>>
>>>>>> >>>>>   install.packages("RQuantLib")
>>>>> >>>>Installing package into
>> >?C:/Users/rsher/Documents/R/win-library/3.2?
>>>>> >>>>(as ?lib? is unspecified)
>>>>> >>>>--- Please select a CRAN mirror for use in this session ---
>>>>> >>>>Warning message:
>>>>> >>>>package ?RQuantLib? is not available (for R version 3.2.4 Revised)
>>>>> >>>>
>>>>> >>>>The package did not install. Am I doing something wrong. Is the
>> >package
>>>>> >>>>going to be updated for the latest version of R?
>>>>> >>>>
>>>> >>>Windows binary packages are only built for the most current (major)
>>>> >>>version of R.  You need to upgrade to at least R-3.4.0, or you will
>>>> >>>have to install RQuantLib (and therefore QuantLib itself) from
>> >source.
>>>> >>>
>>>>> >>>>Thanks,
>>>>> >>>>Bob
>>>>> >>>>
>>>>> >>>>______________________________________________
>>>>> >>>>R-help at r-project.org  mailing list -- To UNSUBSCRIBE and more, see
>>>>> >>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> >>>>PLEASE do read the posting guide
>>>>> >>>>http://www.R-project.org/posting-guide.html
>>>>> >>>>and provide commented, minimal, self-contained, reproducible code.
>>>> >>>
>>>> >>>
>>> >>
>>> >>______________________________________________
>>> >>R-help at r-project.org  mailing list -- To UNSUBSCRIBE and more, see
>>> >>https://stat.ethz.ch/mailman/listinfo/r-help
>>> >>PLEASE do read the posting guide
>>> >>http://www.R-project.org/posting-guide.html
>>> >>and provide commented, minimal, self-contained, reproducible code.
>> >
>> >-- 
>> >Michael
>> >http://www.dewey.myzen.co.uk/home.html
>> >
>> >______________________________________________
>> >R-help at r-project.org  mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Sat Dec 30 17:20:54 2017
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 30 Dec 2017 08:20:54 -0800
Subject: [R] RQuantLib
In-Reply-To: <5A47BA17.7050003@comcast.net>
References: <5A458631.6070409@comcast.net>
 <CAPPM_gRcw9UKaN+vCuB3gZkGwCZA=juBBAsx3QyCPC3CYVe+Vw@mail.gmail.com>
 <5A4689F3.8000503@comcast.net>
 <b5324217-8160-2736-45c9-0bb6da4c215d@dewey.myzen.co.uk>
 <531E7250-AD0F-42D1-AF67-DF2E3C489276@dcn.davis.ca.us>
 <5A47BA17.7050003@comcast.net>
Message-ID: <CFB7D8ED-16B2-4245-BD72-851CE4518B9B@dcn.davis.ca.us>

I am not a user of this package, but I think you should read [1], particularly the README and system requirements. You may need to learn about compiling from source (.tar.gz) if you cannot find a recent precompiled package file (.zip).

[1] https://cran.r-project.org/web/packages/RQuantLib/index.html
-- 
Sent from my phone. Please excuse my brevity.

On December 30, 2017 8:08:55 AM PST, rsherry8 <rsherry8 at comcast.net> wrote:
>Jeff,
>
>I tired another mirror but that did not work. I tired the following two
>
>commands:
>
> > ap <- available.packages()
> > View(ap)
>
>The output of the View command showed numerous packages available but
>it 
>did not show RQuantLib.
>
>Bob Sherry
>
>On 12/30/2017 8:48 AM, Jeff Newmiller wrote:
>> Sometimes that message appears when using a CRAN mirror that is not
>up-to-date or has communication problems. Might also try another
>mirror.
>> -- Sent from my phone. Please excuse my brevity. On December 30, 2017
>
>> 5:25:15 AM PST, Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>>> >Dear Bob
>>> >
>>> >In fact the current release is 3.4.3 I can think of no reason why
>that
>>> >should matter here but it might be worth trying to upgrade to it.
>>> >
>>> >Michael
>>> >
>>> >On 29/12/2017 18:31, rsherry8 wrote:
>>>> >>Joshua,
>>>> >>
>>>> >>Thanks for the response. When you said at least version 3.4.0, I
>>>> >>upgraded to 3.4.2 which I believe is the current version. Now, I
>>>> >>attempted to install the package RQuantLib but it did not work.
>Here
>>> >is
>>>> >>what I got:
>>>> >>
>>>> >>  > install.packages("RQuantLib")
>>>> >>Installing package into
>?C:/Users/rsher/Documents/R/win-library/3.4?
>>>> >>(as ?lib? is unspecified)
>>>> >>Warning message:
>>>> >>package ?RQuantLib? is not available (for R version 3.4.2)
>>>> >>
>>>> >>
>>>> >>Please help.
>>>> >>Thanks,
>>>> >>Bob Sherry
>>>> >>
>>>> >>On 12/28/2017 10:28 PM, Joshua Ulrich wrote:
>>>>> >>>On Thu, Dec 28, 2017 at 6:02 PM, rsherry8<rsherry8 at comcast.net>
>>> >wrote:
>>>>>> >>>>I have recently installed R on my new computer. I also want
>to
>>>>>> >>>>install the
>>>>>> >>>>package RQuantLib. So I run the following command and get the
>>> >following
>>>>>> >>>>output:
>>>>>> >>>>
>>>>>>> >>>>>   install.packages("RQuantLib")
>>>>>> >>>>Installing package into
>>> >?C:/Users/rsher/Documents/R/win-library/3.2?
>>>>>> >>>>(as ?lib? is unspecified)
>>>>>> >>>>--- Please select a CRAN mirror for use in this session ---
>>>>>> >>>>Warning message:
>>>>>> >>>>package ?RQuantLib? is not available (for R version 3.2.4
>Revised)
>>>>>> >>>>
>>>>>> >>>>The package did not install. Am I doing something wrong. Is
>the
>>> >package
>>>>>> >>>>going to be updated for the latest version of R?
>>>>>> >>>>
>>>>> >>>Windows binary packages are only built for the most current
>(major)
>>>>> >>>version of R.  You need to upgrade to at least R-3.4.0, or you
>will
>>>>> >>>have to install RQuantLib (and therefore QuantLib itself) from
>>> >source.
>>>>> >>>
>>>>>> >>>>Thanks,
>>>>>> >>>>Bob
>>>>>> >>>>
>>>>>> >>>>______________________________________________
>>>>>> >>>>R-help at r-project.org  mailing list -- To UNSUBSCRIBE and
>more, see
>>>>>> >>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> >>>>PLEASE do read the posting guide
>>>>>> >>>>http://www.R-project.org/posting-guide.html
>>>>>> >>>>and provide commented, minimal, self-contained, reproducible
>code.
>>>>> >>>
>>>>> >>>
>>>> >>
>>>> >>______________________________________________
>>>> >>R-help at r-project.org  mailing list -- To UNSUBSCRIBE and more,
>see
>>>> >>https://stat.ethz.ch/mailman/listinfo/r-help
>>>> >>PLEASE do read the posting guide
>>>> >>http://www.R-project.org/posting-guide.html
>>>> >>and provide commented, minimal, self-contained, reproducible
>code.
>>> >
>>> >-- 
>>> >Michael
>>> >http://www.dewey.myzen.co.uk/home.html
>>> >
>>> >______________________________________________
>>> >R-help at r-project.org  mailing list -- To UNSUBSCRIBE and more, see
>>> >https://stat.ethz.ch/mailman/listinfo/r-help
>>> >PLEASE do read the posting guide
>>> >http://www.R-project.org/posting-guide.html
>>> >and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Sat Dec 30 18:44:13 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 30 Dec 2017 09:44:13 -0800
Subject: [R] RQuantLib
In-Reply-To: <5A47B699.4030007@comcast.net>
References: <5A458631.6070409@comcast.net>
 <CAPPM_gRcw9UKaN+vCuB3gZkGwCZA=juBBAsx3QyCPC3CYVe+Vw@mail.gmail.com>
 <5A4689F3.8000503@comcast.net>
 <CAF4WX-c5PM4_oReOfOE_5HDC6JeykDW8_4zRA1x0mGmKxeqqEg@mail.gmail.com>
 <5A47B699.4030007@comcast.net>
Message-ID: <C22113AC-8B73-45DA-9C54-45397F7B5F50@comcast.net>


> On Dec 30, 2017, at 7:54 AM, rsherry8 <rsherry8 at comcast.net> wrote:
> 
> OA,
> 
> Thanks for the response. I downloaded the file RQuantLib_0.4.4.tar.gz 
> into a directory c:\r.zip on my Windows machine. I then ran the 
> following command and I received the following output:
> 
>> install.packages("RQuantLib", lib="/r.zip/")
> Warning message:
> package ?RQuantLib? is not available (for R version 3.4.3)

The install.packages command should include repo=NULL when installing from local binary package, and it also should include type="source when the package is not binary.
> 
> I did not unpack the .gz file. Should I have?
> 
> Please comment.
> Bob Sherry
> 
> On 12/30/2017 2:24 AM, Orvalho Augusto wrote:
>> Hi Bob,
>> 
>> I don't know what is the cause of your trouble but try this:
>> 1. Download the zip of package.
>> 
>> 2. And install it from local zip files. This you find on the Packages 
>> menu.
>> 
>> Hope it helps
>> OA
>> 
>> 
>> On Fri, Dec 29, 2017 at 10:31 AM, rsherry8 <rsherry8 at comcast.net 
>> <mailto:rsherry8 at comcast.net>> wrote:
>> 
>>    Joshua,
>> 
>>    Thanks for the response. When you said at least version 3.4.0, I
>>    upgraded to 3.4.2 which I believe is the current version. Now, I
>>    attempted to install the package RQuantLib but it did not work.
>>    Here is what I got:
>> 
>>> install.packages("RQuantLib")
>>    Installing package into ?C:/Users/rsher/Documents/R/win-library/3.4?
>>    (as ?lib? is unspecified)
>>    Warning message:
>>    package ?RQuantLib? is not available (for R version 3.4.2)
>> 
>> 
>>    Please help.
>>    Thanks,
>>    Bob Sherry
>> 
>> 
>>    On 12/28/2017 10:28 PM, Joshua Ulrich wrote:
>> 
>>        On Thu, Dec 28, 2017 at 6:02 PM, rsherry8
>>        <rsherry8 at comcast.net <mailto:rsherry8 at comcast.net>> wrote:
>> 
>>            I have recently installed R on my new computer. I also
>>            want to install the
>>            package RQuantLib. So I run the following command and get
>>            the following
>>            output:
>> 
>>                  install.packages("RQuantLib")
>> 
>>            Installing package into
>>            ?C:/Users/rsher/Documents/R/win-library/3.2?
>>            (as ?lib? is unspecified)
>>            --- Please select a CRAN mirror for use in this session ---
>>            Warning message:
>>            package ?RQuantLib? is not available (for R version 3.2.4
>>            Revised)
>> 
>>            The package did not install. Am I doing something wrong.
>>            Is the package
>>            going to be updated for the latest version of R?
>> 
>>        Windows binary packages are only built for the most current
>>        (major)
>>        version of R.  You need to upgrade to at least R-3.4.0, or you
>>        will
>>        have to install RQuantLib (and therefore QuantLib itself) from
>>        source.
>> 
>>            Thanks,
>>            Bob
>> 
>>            ______________________________________________
>>            R-help at r-project.org <mailto:R-help at r-project.org> mailing
>>            list -- To UNSUBSCRIBE and more, see
>>            https://stat.ethz.ch/mailman/listinfo/r-help
>>            <https://stat.ethz.ch/mailman/listinfo/r-help>
>>            PLEASE do read the posting guide
>>            http://www.R-project.org/posting-guide.html
>>            <http://www.R-project.org/posting-guide.html>
>>            and provide commented, minimal, self-contained,
>>            reproducible code.
>> 
>> 
>> 
>> 
>>    ______________________________________________
>>    R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>>    To UNSUBSCRIBE and more, see
>>    https://stat.ethz.ch/mailman/listinfo/r-help
>>    <https://stat.ethz.ch/mailman/listinfo/r-help>
>>    PLEASE do read the posting guide
>>    http://www.R-project.org/posting-guide.html
>>    <http://www.R-project.org/posting-guide.html>
>>    and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From dwinsemius at comcast.net  Sat Dec 30 18:46:22 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 30 Dec 2017 09:46:22 -0800
Subject: [R] RQuantLib
In-Reply-To: <C22113AC-8B73-45DA-9C54-45397F7B5F50@comcast.net>
References: <5A458631.6070409@comcast.net>
 <CAPPM_gRcw9UKaN+vCuB3gZkGwCZA=juBBAsx3QyCPC3CYVe+Vw@mail.gmail.com>
 <5A4689F3.8000503@comcast.net>
 <CAF4WX-c5PM4_oReOfOE_5HDC6JeykDW8_4zRA1x0mGmKxeqqEg@mail.gmail.com>
 <5A47B699.4030007@comcast.net>
 <C22113AC-8B73-45DA-9C54-45397F7B5F50@comcast.net>
Message-ID: <E8A4736E-CAF8-44F4-A255-449B512DDFB7@comcast.net>


> On Dec 30, 2017, at 9:44 AM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Dec 30, 2017, at 7:54 AM, rsherry8 <rsherry8 at comcast.net> wrote:
>> 
>> OA,
>> 
>> Thanks for the response. I downloaded the file RQuantLib_0.4.4.tar.gz 
>> into a directory c:\r.zip on my Windows machine. I then ran the 
>> following command and I received the following output:
>> 
>>> install.packages("RQuantLib", lib="/r.zip/")
>> Warning message:
>> package ?RQuantLib? is not available (for R version 3.4.3)
> 
> The install.packages command should include repo=NULL when installing from local binary package, and it also should include type="source when the package is not binary.

Furthermore, the system requirements in the DESCRIPTION file are:


SystemRequirements:	QuantLib library (>= 1.8.0) from http://quantlib.org, Boost library from http://www.boost.org


>> 
>> I did not unpack the .gz file. Should I have?
>> 
>> Please comment.
>> Bob Sherry
>> 
>> On 12/30/2017 2:24 AM, Orvalho Augusto wrote:
>>> Hi Bob,
>>> 
>>> I don't know what is the cause of your trouble but try this:
>>> 1. Download the zip of package.
>>> 
>>> 2. And install it from local zip files. This you find on the Packages 
>>> menu.
>>> 
>>> Hope it helps
>>> OA
>>> 
>>> 
>>> On Fri, Dec 29, 2017 at 10:31 AM, rsherry8 <rsherry8 at comcast.net 
>>> <mailto:rsherry8 at comcast.net>> wrote:
>>> 
>>>   Joshua,
>>> 
>>>   Thanks for the response. When you said at least version 3.4.0, I
>>>   upgraded to 3.4.2 which I believe is the current version. Now, I
>>>   attempted to install the package RQuantLib but it did not work.
>>>   Here is what I got:
>>> 
>>>> install.packages("RQuantLib")
>>>   Installing package into ?C:/Users/rsher/Documents/R/win-library/3.4?
>>>   (as ?lib? is unspecified)
>>>   Warning message:
>>>   package ?RQuantLib? is not available (for R version 3.4.2)
>>> 
>>> 
>>>   Please help.
>>>   Thanks,
>>>   Bob Sherry
>>> 
>>> 
>>>   On 12/28/2017 10:28 PM, Joshua Ulrich wrote:
>>> 
>>>       On Thu, Dec 28, 2017 at 6:02 PM, rsherry8
>>>       <rsherry8 at comcast.net <mailto:rsherry8 at comcast.net>> wrote:
>>> 
>>>           I have recently installed R on my new computer. I also
>>>           want to install the
>>>           package RQuantLib. So I run the following command and get
>>>           the following
>>>           output:
>>> 
>>>                 install.packages("RQuantLib")
>>> 
>>>           Installing package into
>>>           ?C:/Users/rsher/Documents/R/win-library/3.2?
>>>           (as ?lib? is unspecified)
>>>           --- Please select a CRAN mirror for use in this session ---
>>>           Warning message:
>>>           package ?RQuantLib? is not available (for R version 3.2.4
>>>           Revised)
>>> 
>>>           The package did not install. Am I doing something wrong.
>>>           Is the package
>>>           going to be updated for the latest version of R?
>>> 
>>>       Windows binary packages are only built for the most current
>>>       (major)
>>>       version of R.  You need to upgrade to at least R-3.4.0, or you
>>>       will
>>>       have to install RQuantLib (and therefore QuantLib itself) from
>>>       source.
>>> 
>>>           Thanks,
>>>           Bob
>>> 
>>>           ______________________________________________
>>>           R-help at r-project.org <mailto:R-help at r-project.org> mailing
>>>           list -- To UNSUBSCRIBE and more, see
>>>           https://stat.ethz.ch/mailman/listinfo/r-help
>>>           <https://stat.ethz.ch/mailman/listinfo/r-help>
>>>           PLEASE do read the posting guide
>>>           http://www.R-project.org/posting-guide.html
>>>           <http://www.R-project.org/posting-guide.html>
>>>           and provide commented, minimal, self-contained,
>>>           reproducible code.
>>> 
>>> 
>>> 
>>> 
>>>   ______________________________________________
>>>   R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>>>   To UNSUBSCRIBE and more, see
>>>   https://stat.ethz.ch/mailman/listinfo/r-help
>>>   <https://stat.ethz.ch/mailman/listinfo/r-help>
>>>   PLEASE do read the posting guide
>>>   http://www.R-project.org/posting-guide.html
>>>   <http://www.R-project.org/posting-guide.html>
>>>   and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> 
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From dwinsemius at comcast.net  Sat Dec 30 18:49:18 2017
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 30 Dec 2017 09:49:18 -0800
Subject: [R] RQuantLib
In-Reply-To: <E8A4736E-CAF8-44F4-A255-449B512DDFB7@comcast.net>
References: <5A458631.6070409@comcast.net>
 <CAPPM_gRcw9UKaN+vCuB3gZkGwCZA=juBBAsx3QyCPC3CYVe+Vw@mail.gmail.com>
 <5A4689F3.8000503@comcast.net>
 <CAF4WX-c5PM4_oReOfOE_5HDC6JeykDW8_4zRA1x0mGmKxeqqEg@mail.gmail.com>
 <5A47B699.4030007@comcast.net>
 <C22113AC-8B73-45DA-9C54-45397F7B5F50@comcast.net>
 <E8A4736E-CAF8-44F4-A255-449B512DDFB7@comcast.net>
Message-ID: <5863FFC2-B5E1-492F-BB62-B086E41CE4E1@comcast.net>


> On Dec 30, 2017, at 9:46 AM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Dec 30, 2017, at 9:44 AM, David Winsemius <dwinsemius at comcast.net> wrote:
>> 
>> 
>>> On Dec 30, 2017, at 7:54 AM, rsherry8 <rsherry8 at comcast.net> wrote:
>>> 
>>> OA,
>>> 
>>> Thanks for the response. I downloaded the file RQuantLib_0.4.4.tar.gz 
>>> into a directory c:\r.zip on my Windows machine. I then ran the 
>>> following command and I received the following output:
>>> 
>>>> install.packages("RQuantLib", lib="/r.zip/")
>>> Warning message:
>>> package ?RQuantLib? is not available (for R version 3.4.3)
>> 
>> The install.packages command should include repo=NULL when installing from local binary package, and it also should include type="source"

Added the missing dbl-quote.

>>  when the package is not binary.
> 
> Furthermore, the system requirements in the DESCRIPTION file are:
> 
> 
> SystemRequirements:	QuantLib library (>= 1.8.0) from http://quantlib.org, Boost library from http://www.boost.org

And finally (perhaps):

The binaries at CRAN for windows are still at 0.4.2 so you might consider install that version from source:

RQuantLib_0.4.2.tar.gz at https://cran.r-project.org/src/contrib/Archive/RQuantLib/RQuantLib_0.4.2.tar.gz


> 
> 
>>> 
>>> I did not unpack the .gz file. Should I have?
>>> 
>>> Please comment.
>>> Bob Sherry
>>> 
>>> On 12/30/2017 2:24 AM, Orvalho Augusto wrote:
>>>> Hi Bob,
>>>> 
>>>> I don't know what is the cause of your trouble but try this:
>>>> 1. Download the zip of package.
>>>> 
>>>> 2. And install it from local zip files. This you find on the Packages 
>>>> menu.
>>>> 
>>>> Hope it helps
>>>> OA
>>>> 
>>>> 
>>>> On Fri, Dec 29, 2017 at 10:31 AM, rsherry8 <rsherry8 at comcast.net 
>>>> <mailto:rsherry8 at comcast.net>> wrote:
>>>> 
>>>>  Joshua,
>>>> 
>>>>  Thanks for the response. When you said at least version 3.4.0, I
>>>>  upgraded to 3.4.2 which I believe is the current version. Now, I
>>>>  attempted to install the package RQuantLib but it did not work.
>>>>  Here is what I got:
>>>> 
>>>>> install.packages("RQuantLib")
>>>>  Installing package into ?C:/Users/rsher/Documents/R/win-library/3.4?
>>>>  (as ?lib? is unspecified)
>>>>  Warning message:
>>>>  package ?RQuantLib? is not available (for R version 3.4.2)
>>>> 
>>>> 
>>>>  Please help.
>>>>  Thanks,
>>>>  Bob Sherry
>>>> 
>>>> 
>>>>  On 12/28/2017 10:28 PM, Joshua Ulrich wrote:
>>>> 
>>>>      On Thu, Dec 28, 2017 at 6:02 PM, rsherry8
>>>>      <rsherry8 at comcast.net <mailto:rsherry8 at comcast.net>> wrote:
>>>> 
>>>>          I have recently installed R on my new computer. I also
>>>>          want to install the
>>>>          package RQuantLib. So I run the following command and get
>>>>          the following
>>>>          output:
>>>> 
>>>>                install.packages("RQuantLib")
>>>> 
>>>>          Installing package into
>>>>          ?C:/Users/rsher/Documents/R/win-library/3.2?
>>>>          (as ?lib? is unspecified)
>>>>          --- Please select a CRAN mirror for use in this session ---
>>>>          Warning message:
>>>>          package ?RQuantLib? is not available (for R version 3.2.4
>>>>          Revised)
>>>> 
>>>>          The package did not install. Am I doing something wrong.
>>>>          Is the package
>>>>          going to be updated for the latest version of R?
>>>> 
>>>>      Windows binary packages are only built for the most current
>>>>      (major)
>>>>      version of R.  You need to upgrade to at least R-3.4.0, or you
>>>>      will
>>>>      have to install RQuantLib (and therefore QuantLib itself) from
>>>>      source.
>>>> 
>>>>          Thanks,
>>>>          Bob
>>>> 
>>>>          ______________________________________________
>>>>          R-help at r-project.org <mailto:R-help at r-project.org> mailing
>>>>          list -- To UNSUBSCRIBE and more, see
>>>>          https://stat.ethz.ch/mailman/listinfo/r-help
>>>>          <https://stat.ethz.ch/mailman/listinfo/r-help>
>>>>          PLEASE do read the posting guide
>>>>          http://www.R-project.org/posting-guide.html
>>>>          <http://www.R-project.org/posting-guide.html>
>>>>          and provide commented, minimal, self-contained,
>>>>          reproducible code.
>>>> 
>>>> 
>>>> 
>>>> 
>>>>  ______________________________________________
>>>>  R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>>>>  To UNSUBSCRIBE and more, see
>>>>  https://stat.ethz.ch/mailman/listinfo/r-help
>>>>  <https://stat.ethz.ch/mailman/listinfo/r-help>
>>>>  PLEASE do read the posting guide
>>>>  http://www.R-project.org/posting-guide.html
>>>>  <http://www.R-project.org/posting-guide.html>
>>>>  and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> 
>>> 
>>> 
>>> 	[[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> David Winsemius
>> Alameda, CA, USA
>> 
>> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From rahulsharm9 at gmail.com  Sat Dec 30 19:34:26 2017
From: rahulsharm9 at gmail.com (Rahul Sharm)
Date: Sat, 30 Dec 2017 10:34:26 -0800
Subject: [R] 10 Minute Time Window Aggregate By Multiple Grouping Variables
Message-ID: <CABL1_MaKrVXwbtyox74A6me81SRqgqUUB6kstnzuiaw10o+gaA@mail.gmail.com>

Hello All,
I have 1M rows of time stamped information about delivery trucks and their
trip related information from simulation .Detailed info the column names
and attributes for clarity


   1. id: alphanumeric factor/character
   2. datetime.of.trip.start: POSIXct yyyy-mm-dd hh:mm:ss
   3. datetime.of.trip.end: POSIXct yyyy-mm-dd hh:mm:ss
   4. trip.distance: numeric, miles
   5. trip.fuel.consumed: numeric, gallons

Close to 2500 trucks , simulated for a year with varying trip behavior.
This is what I am trying to accomplish 1. First, I want to create 10 minute
intervals from a chosen start date and end date from simulation, ex:
2011-03-30 00:00:00 to 2012-04-01 00:00:00 { approximately 8760 hours *
six, 10 minute blocks per hour = looking at 52K unique timestamps }. If it
works, will look into 15 minute, hourly and so on. * This will be
representative of a start.time.index * I want to recreate the same time
window with same frequency but for a column named end.time.index with time
increment of 10 minutes 2. Go to the raw data, inspect the "
datetime.of.trip.start " and "datetime.of.trip.end", get the time span,
match it with the derived 10 min time time intervals (start and end),
equally distribute the columns 4,5 (the numeric variables) among the 10
minute indices.
Single row example

   - datetime.of.trip.start: 2017-01-01 00:00:00
   - datetime.of.end.start: 2017-01-01 01:00:00
   - trip.distance = 60 miles
   - trip.fuel.consumed = 6 gallons



Interpretation of raw data, on January 1, 2017 from 00 am/midnight to 1 am,
the delivery truck traveled 60 miles using 6 gallons of fuel 3. I want to
be able to do this over entire raw data, get the aggregate sum of the
variables of interest (distance and fuel for example) by the 10.min start
and end time intervals across all ids as long as there is the 10min
start/end time overlap derived from the raw data 4. Repeat the same with
some grouping criteria from DateTime or suitable index, example day of week
or by truckid. From index 1 (00:00:00 to  00:10:00 ) to index 6(00:00:50 to
01:00:00), 60 miles and 6 gallons are equally distributed.

So far, I have created the 10 min HH:MM ids and exploring the foverlaps in
data.table, period.apply from xts, converting the time indices into
numerics and explore ddply %>% mutate %>% group by options. I am trying to
get a more streamlined version that would work as a time series object
which would help me get the descriptive statistics (aggregate by grouping
criteria for sums and means) and do some plotting and time series analysis
(RMA, smoothing, mixed regression models). I got stuck and totally lost
when working with zoo and xts, I was creating the index and working on
period.apply but was not sure how to work around with 2 POSIXct column
variables (start and end). I have the desired index and the cleaned data, I
am looking for a more elegant data.table solution to complete the 10min
aggregate.

Thanks!

 # simple illustration of the daa

id<-c("A1","A1","A1","A1","A1","A1")

start.time.index=as.POSIXct (c("2017-01-01 00:00:00","2017-01-01
00:10:00","2017-01-01 00:20:00","2017-01-01 00:30:00","2017-01-01
00:40:00","2017-01-01 00:50:00"), format="%Y-%m-%d %H:%M:%S")


end.time.index=as.POSIXct (c("2017-01-01 00:10:00","2017-01-01
00:20:00","2017-01-01 00:30:00","2017-01-01 00:40:00","2017-01-01
00:50:00","2017-01-01 01:00:00"), format="%Y-%m-%d %H:%M:%S")

trip.miles=c(10,10,10,10,10,10)
trip.fuel=c(1,1,1,1,1,1) # total of 6 gallons in 1 hour equally
divided into 6 bins of 10 minute window.

desired.data<-c(id,start.time.index,end.time.index,trip.miles,end.time.index)

	[[alternative HTML version deleted]]


From rsherry8 at comcast.net  Sat Dec 30 23:21:11 2017
From: rsherry8 at comcast.net (rsherry8)
Date: Sat, 30 Dec 2017 17:21:11 -0500
Subject: [R] RQuantLib
In-Reply-To: <C22113AC-8B73-45DA-9C54-45397F7B5F50@comcast.net>
References: <5A458631.6070409@comcast.net>
 <CAPPM_gRcw9UKaN+vCuB3gZkGwCZA=juBBAsx3QyCPC3CYVe+Vw@mail.gmail.com>
 <5A4689F3.8000503@comcast.net>
 <CAF4WX-c5PM4_oReOfOE_5HDC6JeykDW8_4zRA1x0mGmKxeqqEg@mail.gmail.com>
 <5A47B699.4030007@comcast.net>
 <C22113AC-8B73-45DA-9C54-45397F7B5F50@comcast.net>
Message-ID: <5A481157.6060202@comcast.net>

Based upon comments I received on the R-Help list, I ran the following 
commands and got the following output:

 >  drat::addRepo("ghrr")
 >  install.packages("RQuantLib", type="binary")
Installing package into ?C:/Users/rsher/Documents/R/win-library/3.4?
(as ?lib? is unspecified)
--- Please select a CRAN mirror for use in this session ---
Warning: unable to access index for repository 
https://ghrr.github.io/drat/bin/windows/contrib/3.4:
   cannot open URL 
'https://ghrr.github.io/drat/bin/windows/contrib/3.4/PACKAGES'
also installing the dependency ?Rcpp?

trying URL 'https://cran.uib.no/bin/windows/contrib/3.4/Rcpp_0.12.14.zip'
Content type 'application/zip' length 4358936 bytes (4.2 MB)
downloaded 4.2 MB

trying URL 'https://cran.uib.no/bin/windows/contrib/3.4/RQuantLib_0.4.2.zip'
Content type 'application/zip' length 7259503 bytes (6.9 MB)
downloaded 6.9 MB

package ?Rcpp? successfully unpacked and MD5 sums checked
package ?RQuantLib? successfully unpacked and MD5 sums checked

The downloaded binary packages are in
C:\Users\rsher\AppData\Local\Temp\Rtmp0OoZCW\downloaded_packages

This time, I used the mirror in Europe rather than a mirror in the 
United States and it worked. I am wondering if that matters. I want to 
thank everybody for their help.

Bob Sherry


On 12/30/2017 12:44 PM, David Winsemius wrote:
>> On Dec 30, 2017, at 7:54 AM, rsherry8 <rsherry8 at comcast.net> wrote:
>>
>> OA,
>>
>> Thanks for the response. I downloaded the file RQuantLib_0.4.4.tar.gz
>> into a directory c:\r.zip on my Windows machine. I then ran the
>> following command and I received the following output:
>>
>>> install.packages("RQuantLib", lib="/r.zip/")
>> Warning message:
>> package ?RQuantLib? is not available (for R version 3.4.3)
> The install.packages command should include repo=NULL when installing from local binary package, and it also should include type="source when the package is not binary.
>> I did not unpack the .gz file. Should I have?
>>
>> Please comment.
>> Bob Sherry
>>
>> On 12/30/2017 2:24 AM, Orvalho Augusto wrote:
>>> Hi Bob,
>>>
>>> I don't know what is the cause of your trouble but try this:
>>> 1. Download the zip of package.
>>>
>>> 2. And install it from local zip files. This you find on the Packages
>>> menu.
>>>
>>> Hope it helps
>>> OA
>>>
>>>
>>> On Fri, Dec 29, 2017 at 10:31 AM, rsherry8 <rsherry8 at comcast.net
>>> <mailto:rsherry8 at comcast.net>> wrote:
>>>
>>>     Joshua,
>>>
>>>     Thanks for the response. When you said at least version 3.4.0, I
>>>     upgraded to 3.4.2 which I believe is the current version. Now, I
>>>     attempted to install the package RQuantLib but it did not work.
>>>     Here is what I got:
>>>
>>>> install.packages("RQuantLib")
>>>     Installing package into ?C:/Users/rsher/Documents/R/win-library/3.4?
>>>     (as ?lib? is unspecified)
>>>     Warning message:
>>>     package ?RQuantLib? is not available (for R version 3.4.2)
>>>
>>>
>>>     Please help.
>>>     Thanks,
>>>     Bob Sherry
>>>
>>>
>>>     On 12/28/2017 10:28 PM, Joshua Ulrich wrote:
>>>
>>>         On Thu, Dec 28, 2017 at 6:02 PM, rsherry8
>>>         <rsherry8 at comcast.net <mailto:rsherry8 at comcast.net>> wrote:
>>>
>>>             I have recently installed R on my new computer. I also
>>>             want to install the
>>>             package RQuantLib. So I run the following command and get
>>>             the following
>>>             output:
>>>
>>>                   install.packages("RQuantLib")
>>>
>>>             Installing package into
>>>             ?C:/Users/rsher/Documents/R/win-library/3.2?
>>>             (as ?lib? is unspecified)
>>>             --- Please select a CRAN mirror for use in this session ---
>>>             Warning message:
>>>             package ?RQuantLib? is not available (for R version 3.2.4
>>>             Revised)
>>>
>>>             The package did not install. Am I doing something wrong.
>>>             Is the package
>>>             going to be updated for the latest version of R?
>>>
>>>         Windows binary packages are only built for the most current
>>>         (major)
>>>         version of R.  You need to upgrade to at least R-3.4.0, or you
>>>         will
>>>         have to install RQuantLib (and therefore QuantLib itself) from
>>>         source.
>>>
>>>             Thanks,
>>>             Bob
>>>
>>>             ______________________________________________
>>>             R-help at r-project.org <mailto:R-help at r-project.org> mailing
>>>             list -- To UNSUBSCRIBE and more, see
>>>             https://stat.ethz.ch/mailman/listinfo/r-help
>>>             <https://stat.ethz.ch/mailman/listinfo/r-help>
>>>             PLEASE do read the posting guide
>>>             http://www.R-project.org/posting-guide.html
>>>             <http://www.R-project.org/posting-guide.html>
>>>             and provide commented, minimal, self-contained,
>>>             reproducible code.
>>>
>>>
>>>
>>>
>>>     ______________________________________________
>>>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>>>     To UNSUBSCRIBE and more, see
>>>     https://stat.ethz.ch/mailman/listinfo/r-help
>>>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>>>     PLEASE do read the posting guide
>>>     http://www.R-project.org/posting-guide.html
>>>     <http://www.R-project.org/posting-guide.html>
>>>     and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> David Winsemius
> Alameda, CA, USA
>
> 'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law
>
>
>
>
>
>


From ashimkapoor at gmail.com  Sun Dec 31 13:39:14 2017
From: ashimkapoor at gmail.com (Ashim Kapoor)
Date: Sun, 31 Dec 2017 18:09:14 +0530
Subject: [R] clustering levels using Tukey HSD in a one way anova
Message-ID: <CAC8=1eqD=Shtf=8+yLNhm-Wv9QN_6Lh8GhwJp=jVySUg=95TTg@mail.gmail.com>

Dear all,

I am doing a one way between subjects anova in an unbalanced data set.
Suppose we have "a" levels of the one factor. I want to merge the not so
significantly different  levels into the same cluster.

Can I do a Tukey Kramer HSD and then use the following algorithm:

For i in 2 : "a"
     For j in 1 : i-1
            if mean of level i is not significantly different to the mean
of level j,then put i and j in the same cluster. After the first time mean
of level i is not different to the mean of level j , just goto the next i ,
no need to compare with remaining j's.

Alternately,

I do not do Tukey Kramer HSD.  I only run the above algorithm. At each
iteration of the inner loop compute the contrast : mean of level i  = mean
j. At the first match I come out of the inner loop. To control for the (at
most)  1+ 2 + ... + (n-1) comparisons we can use bonferroni/scheffe / some
other technique.

Since this is a statistics query I have posted on stackexchange.  I have
not received a reply so I am posting my query here. Can some one please
answer my query here or on stackexchange?

The link to the query on stackexchange is:

https://stats.stackexchange.com/questions/320930/one-way-
anova-clustering-levels-using-tukey-kramer-hsd

Best Regards,
Ashim

	[[alternative HTML version deleted]]


From jrkrideau at yahoo.ca  Sun Dec 31 14:10:59 2017
From: jrkrideau at yahoo.ca (John Kane)
Date: Sun, 31 Dec 2017 13:10:59 +0000 (UTC)
Subject: [R] Draw Overlapping Circles with shaded tracks
In-Reply-To: <CA+8X3fWsJeoTWAVx569oDQ3m-CODn=gDFRVQjmzzNZsJODoHVQ@mail.gmail.com>
References: <CAE9stmfEke0yLe4w+crBWF7uCFfvseLqCrJYOxxu1BsNnch+2w@mail.gmail.com>
 <CA+8X3fWsJeoTWAVx569oDQ3m-CODn=gDFRVQjmzzNZsJODoHVQ@mail.gmail.com>
Message-ID: <1651743274.6758450.1514725859996@mail.yahoo.com>

That code nees the plotrix package:
library(plotrix)
pdf("circles.pdf")
plot(0:10,type="n",axes=FALSE,xlab="",ylab="")
draw.circle(4,5,radius=3,border="#ff0000aa",lwd=10)
draw.circle(6,5,radius=3,border="#0000ffaa",lwd=10)
dev.off()


 

    On Friday, December 29, 2017, 6:06:32 PM EST, Jim Lemon <drjimlemon at gmail.com> wrote:  
 
 Hi Abou,
Without an illustration it's hard to work out what you want. here is a
simple example of two circles using semi-transparency. Is this any
help?

pdf("circles.pdf")
plot(0:10,type="n",axes=FALSE,xlab="",ylab="")
draw.circle(4,5,radius=3,border="#ff0000aa",lwd=10)
draw.circle(6,5,radius=3,border="#0000ffaa",lwd=10)
dev.off()

Jim

On Fri, Dec 29, 2017 at 9:45 PM, AbouEl-Makarim Aboueissa
<abouelmakarim1962 at gmail.com> wrote:
> Dear All:
>
>
> I am wondering if there is a way in R to draw these two circles with shaded
> tracks in both circles using R, and make both circles uncovered. I am
> trying to make it in MS words, but I could not. Your help will be highly
> appreciated.
>
>
> In my previous post I added the image of the two circles, but the post
> never published. I just thought to resent the post again without the image.
>
> with many thanks
> abou
> ______________________
>
>
> *AbouEl-Makarim Aboueissa, PhD*
>
> *Professor of Statistics*
>
> *Department of Mathematics and Statistics*
> *University of Southern Maine*
>
>? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
  
	[[alternative HTML version deleted]]


From marc_grt at yahoo.fr  Sun Dec 31 14:40:49 2017
From: marc_grt at yahoo.fr (Marc Girondot)
Date: Sun, 31 Dec 2017 14:40:49 +0100
Subject: [R] Draw Overlapping Circles with shaded tracks
In-Reply-To: <1651743274.6758450.1514725859996@mail.yahoo.com>
References: <CAE9stmfEke0yLe4w+crBWF7uCFfvseLqCrJYOxxu1BsNnch+2w@mail.gmail.com>
 <CA+8X3fWsJeoTWAVx569oDQ3m-CODn=gDFRVQjmzzNZsJODoHVQ@mail.gmail.com>
 <1651743274.6758450.1514725859996@mail.yahoo.com>
Message-ID: <cc6657ea-3733-d7aa-abc2-74d2e0a160a3@yahoo.fr>

Another solution:

library("HelpersMG") plot(0:10,type="n",axes=FALSE,xlab="",ylab="", 
asp=1) ellipse(center.x = 3, center.y = 5, radius.x = 5, radius.y = 5, 
lwd=10, col=NA, border=rgb(red = 1, green = 0, blue=0, alpha = 0.5)) 
ellipse(center.x = 8, center.y = 5, radius.x = 5, radius.y = 5, lwd=10, 
col=NA, border=rgb(red = 0, green = 1, blue=0, alpha = 0.5))


(Without the graphic example, it is difficult to know what tit was 
supposed to do !)

Marc

Le 31/12/2017 ? 14:10, John Kane via R-help a ?crit?:
> That code nees the plotrix package:
> library(plotrix)
> pdf("circles.pdf")
> plot(0:10,type="n",axes=FALSE,xlab="",ylab="")
> draw.circle(4,5,radius=3,border="#ff0000aa",lwd=10)
> draw.circle(6,5,radius=3,border="#0000ffaa",lwd=10)
> dev.off()
>
>
>   
>
>      On Friday, December 29, 2017, 6:06:32 PM EST, Jim Lemon <drjimlemon at gmail.com> wrote:
>   
>   Hi Abou,
> Without an illustration it's hard to work out what you want. here is a
> simple example of two circles using semi-transparency. Is this any
> help?
>
> pdf("circles.pdf")
> plot(0:10,type="n",axes=FALSE,xlab="",ylab="")
> draw.circle(4,5,radius=3,border="#ff0000aa",lwd=10)
> draw.circle(6,5,radius=3,border="#0000ffaa",lwd=10)
> dev.off()
>
> Jim
>
> On Fri, Dec 29, 2017 at 9:45 PM, AbouEl-Makarim Aboueissa
> <abouelmakarim1962 at gmail.com> wrote:
>> Dear All:
>>
>>
>> I am wondering if there is a way in R to draw these two circles with shaded
>> tracks in both circles using R, and make both circles uncovered. I am
>> trying to make it in MS words, but I could not. Your help will be highly
>> appreciated.
>>
>>
>> In my previous post I added the image of the two circles, but the post
>> never published. I just thought to resent the post again without the image.
>>
>> with many thanks
>> abou
>> ______________________
>>
>>
>> *AbouEl-Makarim Aboueissa, PhD*
>>
>> *Professor of Statistics*
>>
>> *Department of Mathematics and Statistics*
>> *University of Southern Maine*
>>
>>  ? ? ? ? [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>    
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



	[[alternative HTML version deleted]]


From marc_grt at yahoo.fr  Sun Dec 31 15:03:58 2017
From: marc_grt at yahoo.fr (Marc Girondot)
Date: Sun, 31 Dec 2017 15:03:58 +0100
Subject: [R] Order of methods for optimx
Message-ID: <e3ac59c2-1179-33bb-94e9-5ec21e8e834e@yahoo.fr>

Dear R-er,

For a non-linear optimisation, I used optim() with BFGS method but it 
stopped regularly before to reach a true mimimum. It was not a problem 
with limit of iterations, just a local minimum. I was able sometimes to 
reach better minimum using several rounds of optim().

Then I moved to optimx() to do the different optim rounds automatically 
using "Nelder-Mead" and "BFGS" methods

I find a huge time difference using system.time() based on the order of 
these both methods:

 > snb # "Nelder-Mead" and "BFGS"
 ??? user?? system? elapsed
1021.656??? 0.200 1021.695
 > sbn # "BFGS" and "Nelder-Mead"
 ??? user?? system? elapsed
3140.096??? 0.384 3139.728

But optimx() with "Nelder-Mead" and "BFGS" stops in local minimum:

 > o_mu2p1$value
[1] 2297.557

whereas "BFGS" and "Nelder-Mead" stops in better model (not sure if it 
is a global minimum, but it is better):

 > o_mu2p1$value
[1] 2297.305

####################

My questions are:

Are my findings common and documented (order of methods giving non 
consistant results) or are they specific to the model I try to fit ?

If it is known problem, where I can find the best advice about order of 
methods ?

####################

To reproduce what I said:

install.packages("http://www.ese.u-psud.fr/epc/conservation/CRAN/HelpersMG.tar.gz", 
repos=NULL, type="source")
install.packages("http://www.ese.u-psud.fr/epc/conservation/CRAN/phenology.tar.gz", 
repos=NULL, type="source")

library("phenology")
library("car")
library("optimx")
library("numDeriv")

ECFOCF_2002 <- TableECFOCF(MarineTurtles_2002)
snb <- system.time(
 ? o_mu2p1_nb <- fitCF(par = structure(c(0.244863062899293,
 ?????????????????????????????????? 43.5578630363601,
 ?????????????????????????????????? 4.94764539004454,
 ?????????????????????????????????? 129.606771856887,
 ?????????????????????????????????? -0.323749593604171,
 ?????????????????????????????????? 1.37326091618759),
 ???????????????????????????????? .Names = c("mu1", "size1", "mu2",
 ??????????????????????????????????????????? "size2", "p", "OTN")),
 ???????????????? method = c("Nelder-Mead", "BFGS"),
 ???????????????? data=ECFOCF_2002, hessian = TRUE)
)
sbn <- system.time(
 ? o_mu2p1_bn <- fitCF(par = structure(c(0.244863062899293,
 ???????????????????????????????????? 43.5578630363601,
 ???????????????????????????????????? 4.94764539004454,
 ???????????????????????????????????? 129.606771856887,
 ???????????????????????????????????? -0.323749593604171,
 ???????????????????????????????????? 1.37326091618759),
 ?????????????????????????????????? .Names = c("mu1", "size1", "mu2",
 ?????????????????????????????????????????????? "size2", "p", "OTN")),
 ?????????????????? method = c("BFGS", "Nelder-Mead"),
 ?????????????????? data=ECFOCF_2002, hessian = TRUE)
)

snb
o_mu2p1_nb$value

sbn
o_mu2p1_bn$value


From abouelmakarim1962 at gmail.com  Sun Dec 31 15:13:15 2017
From: abouelmakarim1962 at gmail.com (AbouEl-Makarim Aboueissa)
Date: Sun, 31 Dec 2017 09:13:15 -0500
Subject: [R] Draw Overlapping Circles with shaded tracks
In-Reply-To: <cc6657ea-3733-d7aa-abc2-74d2e0a160a3@yahoo.fr>
References: <CAE9stmfEke0yLe4w+crBWF7uCFfvseLqCrJYOxxu1BsNnch+2w@mail.gmail.com>
 <CA+8X3fWsJeoTWAVx569oDQ3m-CODn=gDFRVQjmzzNZsJODoHVQ@mail.gmail.com>
 <1651743274.6758450.1514725859996@mail.yahoo.com>
 <cc6657ea-3733-d7aa-abc2-74d2e0a160a3@yahoo.fr>
Message-ID: <CAE9stmd-Qx2tFb+vSoJ5aWpAS_VyF++BWZDPfuuzb_hQqnd-vw@mail.gmail.com>

Dear All:

Thank you very much for all of you.

I just have one more thing. Is there a way to fill the borders with small
dots, may be different sizes.

I tried to do it, but it looks ugly.

Here what I tried:




library(plotrix)


plot(0:10, 0:10, type="n",axes=FALSE,xlab="",ylab="")   #### 0:5,

draw.circle(4,5,radius=3,border="#ff0000aa", lwd=75)

draw.circle(4,5,radius=2.50,border="red",lty=3,lwd=3)
draw.circle(4,5,radius=2.55,border="red",lty=3,lwd=3)
draw.circle(4,5,radius=2.60,border="red",lty=3,lwd=3)
draw.circle(4,5,radius=2.65,border="red",lty=3,lwd=3)
draw.circle(4,5,radius=2.70,border="red",lty=3,lwd=3)

draw.circle(4,5,radius=2.75,border="red",lty=3,lwd=3)
draw.circle(4,5,radius=2.80,border="red",lty=3,lwd=3)
draw.circle(4,5,radius=2.85,border="red",lty=3,lwd=3)
draw.circle(4,5,radius=2.90,border="red",lty=3,lwd=3)
draw.circle(4,5,radius=2.95,border="red",lty=3,lwd=3)
draw.circle(4,5,radius=3.0,border="red",lty=3,lwd=3)

draw.circle(4,5,radius=3.05,border="red",lty=3,lwd=3)
draw.circle(4,5,radius=3.10,border="red",lty=3,lwd=3)
draw.circle(4,5,radius=3.15,border="red",lty=3,lwd=3)
draw.circle(4,5,radius=3.20,border="red",lty=3,lwd=3)
draw.circle(4,5,radius=3.25,border="red",lty=3,lwd=3)


draw.circle(4,5,radius=3.30,border="red",lty=3,lwd=3)
draw.circle(4,5,radius=3.35,border="red",lty=3,lwd=3)
draw.circle(4,5,radius=3.40,border="red",lty=3,lwd=3)
draw.circle(4,5,radius=3.45,border="red",lty=3,lwd=3)
draw.circle(4,5,radius=3.50,border="red",lty=3,lwd=3)



draw.circle(7.5,5,radius=3,border="#0000ffaa",lwd=75)


draw.circle(7.5,5,radius=2.50,border="blue",lty=3,lwd=3)
draw.circle(7.5,5,radius=2.55,border="blue",lty=3,lwd=3)
draw.circle(7.5,5,radius=2.60,border="blue",lty=3,lwd=3)
draw.circle(7.5,5,radius=2.65,border="blue",lty=3,lwd=3)
draw.circle(7.5,5,radius=2.70,border="blue",lty=3,lwd=3)

draw.circle(7.5,5,radius=2.75,border="blue",lty=3,lwd=3)
draw.circle(7.5,5,radius=2.80,border="blue",lty=3,lwd=3)
draw.circle(7.5,5,radius=2.85,border="blue",lty=3,lwd=3)
draw.circle(7.5,5,radius=2.90,border="blue",lty=3,lwd=3)
draw.circle(7.5,5,radius=2.95,border="blue",lty=3,lwd=3)
draw.circle(7.5,5,radius=3.0,border="blue",lty=3,lwd=3)

draw.circle(7.5,5,radius=3.05,border="blue",lty=3,lwd=3)
draw.circle(7.5,5,radius=3.10,border="blue",lty=3,lwd=3)
draw.circle(7.5,5,radius=3.15,border="blue",lty=3,lwd=3)
draw.circle(7.5,5,radius=3.20,border="blue",lty=3,lwd=3)
draw.circle(7.5,5,radius=3.25,border="blue",lty=3,lwd=3)


draw.circle(7.5,5,radius=3.30,border="blue",lty=3,lwd=3)
draw.circle(7.5,5,radius=3.35,border="blue",lty=3,lwd=3)
draw.circle(7.5,5,radius=3.40,border="blue",lty=3,lwd=3)
draw.circle(7.5,5,radius=3.45,border="blue",lty=3,lwd=3)
draw.circle(7.5,5,radius=3.50,border="blue",lty=3,lwd=3)



Once again thank you very much

abou

______________________


*AbouEl-Makarim Aboueissa, PhD*

*Professor of Statistics*

*Department of Mathematics and Statistics*
*University of Southern Maine*


On Sun, Dec 31, 2017 at 8:40 AM, Marc Girondot via R-help <
r-help at r-project.org> wrote:

> Another solution:
>
> library("HelpersMG") plot(0:10,type="n",axes=FALSE,xlab="",ylab="",
> asp=1) ellipse(center.x = 3, center.y = 5, radius.x = 5, radius.y = 5,
> lwd=10, col=NA, border=rgb(red = 1, green = 0, blue=0, alpha = 0.5))
> ellipse(center.x = 8, center.y = 5, radius.x = 5, radius.y = 5, lwd=10,
> col=NA, border=rgb(red = 0, green = 1, blue=0, alpha = 0.5))
>
>
> (Without the graphic example, it is difficult to know what tit was
> supposed to do !)
>
> Marc
>
> Le 31/12/2017 ? 14:10, John Kane via R-help a ?crit :
> > That code nees the plotrix package:
> > library(plotrix)
> > pdf("circles.pdf")
> > plot(0:10,type="n",axes=FALSE,xlab="",ylab="")
> > draw.circle(4,5,radius=3,border="#ff0000aa",lwd=10)
> > draw.circle(6,5,radius=3,border="#0000ffaa",lwd=10)
> > dev.off()
> >
> >
> >
> >
> >      On Friday, December 29, 2017, 6:06:32 PM EST, Jim Lemon <
> drjimlemon at gmail.com> wrote:
> >
> >   Hi Abou,
> > Without an illustration it's hard to work out what you want. here is a
> > simple example of two circles using semi-transparency. Is this any
> > help?
> >
> > pdf("circles.pdf")
> > plot(0:10,type="n",axes=FALSE,xlab="",ylab="")
> > draw.circle(4,5,radius=3,border="#ff0000aa",lwd=10)
> > draw.circle(6,5,radius=3,border="#0000ffaa",lwd=10)
> > dev.off()
> >
> > Jim
> >
> > On Fri, Dec 29, 2017 at 9:45 PM, AbouEl-Makarim Aboueissa
> > <abouelmakarim1962 at gmail.com> wrote:
> >> Dear All:
> >>
> >>
> >> I am wondering if there is a way in R to draw these two circles with
> shaded
> >> tracks in both circles using R, and make both circles uncovered. I am
> >> trying to make it in MS words, but I could not. Your help will be highly
> >> appreciated.
> >>
> >>
> >> In my previous post I added the image of the two circles, but the post
> >> never published. I just thought to resent the post again without the
> image.
> >>
> >> with many thanks
> >> abou
> >> ______________________
> >>
> >>
> >> *AbouEl-Makarim Aboueissa, PhD*
> >>
> >> *Professor of Statistics*
> >>
> >> *Department of Mathematics and Statistics*
> >> *University of Southern Maine*
> >>
> >>          [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From profjcnash at gmail.com  Sun Dec 31 18:05:38 2017
From: profjcnash at gmail.com (J C Nash)
Date: Sun, 31 Dec 2017 12:05:38 -0500
Subject: [R] Order of methods for optimx
In-Reply-To: <e3ac59c2-1179-33bb-94e9-5ec21e8e834e@yahoo.fr>
References: <e3ac59c2-1179-33bb-94e9-5ec21e8e834e@yahoo.fr>
Message-ID: <eefd872c-93aa-fec7-807b-ae74acad1efd@gmail.com>

The results are very sensitive in some cases to configuration (tolerances, etc.) and problem.

Are you using the "follow-on" option? That will definitely be order dependent.

optimx is currently under review by Ravi Varadhan and I. Updating optimx proved very difficult
because of interactions between different options, so I introduced optimr, with version optimrx
on R-forge to have more solvers. When solvers were "down" on CRAN the codes would sometimes give
trouble, but we think we can check for this now. Also we want to leave optimx present because a
number of people are using it, so that function will be part of the updated package which will
be called optimx, and optimr/rx will be deprecated to reduce the package count.

There is still work to be done, though we think it is now more or less fixed, and I can send
a tarball if desired. We'd welcome feedback. The new structure uses a single function optimr()
that can call any one of nearly 2 dozen solvers, and function opm() will do what optimx does for
multiple methods. opm() simply loops through optimr() for each method. A big change is that
optimr() uses the optim() syntax INCLUDING parscale for all methods. The "follow-on"
polyalgorithm and multiple starts are in separate functions to avoid the interaction that was
giving me maintenance issues with the optimx function (and which could give strange behaviour
with optimx() now, so multiple features at once is likely suspect).

If you have gradients, I recommend using Rvmmin (callable from optimr()) or a method that can
use gradients. If you have bounds, do use a method with bounds capability.

No gradient, then BOBYQA from minqa (also callable from optimx() or optimr()) can often be
very efficient, but is quirky and occasionally gives rubbish in my experience.

Cheers, JN

On 2017-12-31 09:03 AM, Marc Girondot via R-help wrote:
> Dear R-er,
> 
> For a non-linear optimisation, I used optim() with BFGS method but it stopped regularly before to reach a true mimimum.
> It was not a problem with limit of iterations, just a local minimum. I was able sometimes to reach better minimum using
> several rounds of optim().
> 
> Then I moved to optimx() to do the different optim rounds automatically using "Nelder-Mead" and "BFGS" methods
> 
> I find a huge time difference using system.time() based on the order of these both methods:
> 
>> snb # "Nelder-Mead" and "BFGS"
> ??? user?? system? elapsed
> 1021.656??? 0.200 1021.695
>> sbn # "BFGS" and "Nelder-Mead"
> ??? user?? system? elapsed
> 3140.096??? 0.384 3139.728
> 
> But optimx() with "Nelder-Mead" and "BFGS" stops in local minimum:
> 
>> o_mu2p1$value
> [1] 2297.557
> 
> whereas "BFGS" and "Nelder-Mead" stops in better model (not sure if it is a global minimum, but it is better):
> 
>> o_mu2p1$value
> [1] 2297.305
> 
> ####################
> 
> My questions are:
> 
> Are my findings common and documented (order of methods giving non consistant results) or are they specific to the model
> I try to fit ?
> 
> If it is known problem, where I can find the best advice about order of methods ?
> 
> ####################
> 
> To reproduce what I said:
> 
> install.packages("http://www.ese.u-psud.fr/epc/conservation/CRAN/HelpersMG.tar.gz", repos=NULL, type="source")
> install.packages("http://www.ese.u-psud.fr/epc/conservation/CRAN/phenology.tar.gz", repos=NULL, type="source")
> 
> library("phenology")
> library("car")
> library("optimx")
> library("numDeriv")
> 
> ECFOCF_2002 <- TableECFOCF(MarineTurtles_2002)
> snb <- system.time(
> ? o_mu2p1_nb <- fitCF(par = structure(c(0.244863062899293,
> ?????????????????????????????????? 43.5578630363601,
> ?????????????????????????????????? 4.94764539004454,
> ?????????????????????????????????? 129.606771856887,
> ?????????????????????????????????? -0.323749593604171,
> ?????????????????????????????????? 1.37326091618759),
> ???????????????????????????????? .Names = c("mu1", "size1", "mu2",
> ??????????????????????????????????????????? "size2", "p", "OTN")),
> ???????????????? method = c("Nelder-Mead", "BFGS"),
> ???????????????? data=ECFOCF_2002, hessian = TRUE)
> )
> sbn <- system.time(
> ? o_mu2p1_bn <- fitCF(par = structure(c(0.244863062899293,
> ???????????????????????????????????? 43.5578630363601,
> ???????????????????????????????????? 4.94764539004454,
> ???????????????????????????????????? 129.606771856887,
> ???????????????????????????????????? -0.323749593604171,
> ???????????????????????????????????? 1.37326091618759),
> ?????????????????????????????????? .Names = c("mu1", "size1", "mu2",
> ?????????????????????????????????????????????? "size2", "p", "OTN")),
> ?????????????????? method = c("BFGS", "Nelder-Mead"),
> ?????????????????? data=ECFOCF_2002, hessian = TRUE)
> )
> 
> snb
> o_mu2p1_nb$value
> 
> sbn
> o_mu2p1_bn$value
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From andrewjmarx at gmail.com  Sun Dec 31 18:55:49 2017
From: andrewjmarx at gmail.com (Andrew Marx)
Date: Sun, 31 Dec 2017 12:55:49 -0500
Subject: [R] Perform mantel test on subset of distance matrix
Message-ID: <CAHWk_TmucRCp67vxJrdkR02e-jegM8gaCpgu=eSCJVf=M_XujA@mail.gmail.com>

I'm trying to perform a mantel test that ignores specific pairs in my
distance matrices. The reasoning is that some geographic distances
below a certain threshold suffer from spatial autocorrelation, or
perhaps ecological relationships become less relevant that stochastic
processes above a certain threshold.

The problem is that I can't find a way to do it. If I replace values
in either or both of the distance matrices with NA, mantel.rtest (ade4
package) gives the following error: Error in if (any(distmat < tol))
warning("Zero distance(s)") : missing value where TRUE/FALSE needed

Here's a trivial example that tries to exclude elements of the first
matrix that equal 11:

library(ade4)
a <- matrix(data = 1:36, nrow = 6)
b <- matrix(data = 1:36, nrow = 6)
a[a==11] <- NA
mantel.rtest(as.dist(a), as.dist(b))

Is there a way to do this, either with this package or another?


