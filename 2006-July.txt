From raphael.fraser at gmail.com  Sat Jul  1 00:00:00 2006
From: raphael.fraser at gmail.com (Raphael Fraser)
Date: Fri, 30 Jun 2006 17:00:00 -0500
Subject: [R] Fwd: time series patterns
In-Reply-To: <BAY106-F9EBDB789D3ADA346BDB6BBB7D0@phx.gbl>
References: <509bb6a90606301258t49153c36vb199dc6e2c8a855a@mail.gmail.com>
	<BAY106-F9EBDB789D3ADA346BDB6BBB7D0@phx.gbl>
Message-ID: <509bb6a90606301500o14c7213ay8cd36d5855a423b6@mail.gmail.com>

---------- Forwarded message ----------
From: Alexander Nervedi <alexnerdy at hotmail.com>
Date: Jun 30, 2006 4:56 PM
Subject: time series patterns
To: raphael.fraser at gmail.com


Hi all.

I have a factor variable distributed over time. I am looking for an elegant
way to code duration of a state. Suppose,

>rainfall.shocks <- factor(sample(c(1,2,3), size = 15, replace = TRUE, prob
>= unit.p),
+                  label = c("Drought", "Normal", "High"))
>
>rainfall.shocks
[1] Normal  High    High    Drought Normal  Normal  High    Normal  Drought
[10] Normal  Drought Normal  Normal  Normal  Normal


So capture the duration of say drought, I'd need a variable that is able to
keep track of rainfall.shocks as well as its past values. I was wondering if
there is any obvious way to do this. the Drought variable in this case would
have values

0 0 0 1 0 0 0 0 1 0 1 0 0 0 0

many thanks for the suggestions you are likely to make.

Alexander Nervedi

_________________________________________________________________
Express yourself instantly with MSN Messenger! Download today - it's FREE!
http://messenger.msn.click-url.com/go/onm00200471ave/direct/01/


From Jean-Francois.Paillotin at imag.fr  Sat Jul  1 00:13:22 2006
From: Jean-Francois.Paillotin at imag.fr (J-Francois Paillotin)
Date: Sat, 1 Jul 2006 00:13:22 +0200 (CEST)
Subject: [R] "{Spam?} {Virus?} Message could not be delivered"
Message-ID: <200606302213.k5UMDMTQ015396@rhin-bis.imag.fr>

I am out of the office and will be back on Monday 31 July.
 
Your mail will not be forwarded automatically.
Your mail regarding "{Spam?} {Virus?} Message could not be delivered" will be read when I return

In urgent cases please contact Mr Kholdoun Torki

mailto:Kholdoun.Torki at imag.fr


From alexnerdy at hotmail.com  Sat Jul  1 00:38:28 2006
From: alexnerdy at hotmail.com (Alexander Nervedi)
Date: Fri, 30 Jun 2006 22:38:28 +0000
Subject: [R] weird error message
In-Reply-To: <BAY106-F27A5055DB1C027AE20CCF3BB7D0@phx.gbl>
Message-ID: <BAY106-F300BD4FAED8A1A4AD35C48BB7D0@phx.gbl>

Hi!

In the example below why is x[[1]] == 0.2237724 false?

Alexander Nervedi


>x <- runif(10)
>x[[1]]
[1] 0.2237724

>x
[1] 0.2237724 0.2678944 0.9375811 0.5963889 0.6180519 0.6449580 0.7308510
[8] 0.7347386 0.4837286 0.1416100

>x[[1]] == 0.2237724
FALSE






>From: "Alexander Nervedi" <alexnerdy at hotmail.com>
>To: r-help at stat.math.ethz.ch
>Subject: Re: [R] Creating Vectors
>Date: Fri, 30 Jun 2006 21:57:35 +0000
>
>Hi all.
>
>I have a factor variable distributed over time. I am looking for an elegant
>way to code duration of a state. Suppose,
>
> >rainfall.shocks <- factor(sample(c(1,2,3), size = 15, replace = TRUE, 
>prob
> >= unit.p),
>+                  label = c("Drought", "Normal", "High"))
> >
> >rainfall.shocks
>[1] Normal  High    High    Drought Normal  Normal  High    Normal  Drought
>[10] Normal  Drought Normal  Normal  Normal  Normal
>
>
>So capture the duration of say drought, I'd need a variable that is able to
>keep track of rainfall.shocks as well as its past values. I was wondering 
>if
>there is any obvious way to do this. the Drought variable in this case 
>would
>have values
>
>0 0 0 1 0 0 0 0 1 0 1 0 0 0 0
>
>many thanks for the suggestions you are likely to make.
>
>Alexander Nervedi
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


From jholtman at gmail.com  Sat Jul  1 01:01:04 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 30 Jun 2006 19:01:04 -0400
Subject: [R] Creating Vectors
In-Reply-To: <509bb6a90606301258t49153c36vb199dc6e2c8a855a@mail.gmail.com>
References: <509bb6a90606301258t49153c36vb199dc6e2c8a855a@mail.gmail.com>
Message-ID: <644e1f320606301601p2e0ab562j493958bb65d522c1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060630/d59c4940/attachment.pl 

From ggrothendieck at gmail.com  Sat Jul  1 01:20:28 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 30 Jun 2006 19:20:28 -0400
Subject: [R] weird error message
In-Reply-To: <BAY106-F300BD4FAED8A1A4AD35C48BB7D0@phx.gbl>
References: <BAY106-F27A5055DB1C027AE20CCF3BB7D0@phx.gbl>
	<BAY106-F300BD4FAED8A1A4AD35C48BB7D0@phx.gbl>
Message-ID: <971536df0606301620h7d2e882at3c0f150bdffee996@mail.gmail.com>

This is FAQ 7.31:

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f

Also please do not piggy back on other threads since it makes the
archives less useful.


On 6/30/06, Alexander Nervedi <alexnerdy at hotmail.com> wrote:
> Hi!
>
> In the example below why is x[[1]] == 0.2237724 false?
>
> Alexander Nervedi
>
>
> >x <- runif(10)
> >x[[1]]
> [1] 0.2237724
>
> >x
> [1] 0.2237724 0.2678944 0.9375811 0.5963889 0.6180519 0.6449580 0.7308510
> [8] 0.7347386 0.4837286 0.1416100
>
> >x[[1]] == 0.2237724
> FALSE
>
>
>
>
>
>
> >From: "Alexander Nervedi" <alexnerdy at hotmail.com>
> >To: r-help at stat.math.ethz.ch
> >Subject: Re: [R] Creating Vectors
> >Date: Fri, 30 Jun 2006 21:57:35 +0000
> >
> >Hi all.
> >
> >I have a factor variable distributed over time. I am looking for an elegant
> >way to code duration of a state. Suppose,
> >
> > >rainfall.shocks <- factor(sample(c(1,2,3), size = 15, replace = TRUE,
> >prob
> > >= unit.p),
> >+                  label = c("Drought", "Normal", "High"))
> > >
> > >rainfall.shocks
> >[1] Normal  High    High    Drought Normal  Normal  High    Normal  Drought
> >[10] Normal  Drought Normal  Normal  Normal  Normal
> >
> >
> >So capture the duration of say drought, I'd need a variable that is able to
> >keep track of rainfall.shocks as well as its past values. I was wondering
> >if
> >there is any obvious way to do this. the Drought variable in this case
> >would
> >have values
> >
> >0 0 0 1 0 0 0 0 1 0 1 0 0 0 0
> >
> >many thanks for the suggestions you are likely to make.
> >
> >Alexander Nervedi
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> >http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From markleeds at verizon.net  Sat Jul  1 02:50:08 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Fri, 30 Jun 2006 19:50:08 -0500 (CDT)
Subject: [R] postscript file too large : maybe an R question
Message-ID: <4505466.1091491151715009027.JavaMail.root@vms070.mailsrvcs.net>

i created a postscipt file in R and then i downloaded a free version
of ghostview to view it. unfortunately, i get the message

fata error : dynamic memory exhausted
when i try to view it.

when i do a dir on windows xp, the file size is 149,034,475
and i know there about 17,000 graphs. is there
a way of possibly viewing this size postscript file in R itself ?

                                   Thanks


From zhliur at yahoo.com  Sat Jul  1 06:11:31 2006
From: zhliur at yahoo.com (yyan liu)
Date: Fri, 30 Jun 2006 21:11:31 -0700 (PDT)
Subject: [R] polynomial expansion in R
Message-ID: <20060701041131.50212.qmail@web35803.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060630/082bc0d0/attachment.pl 

From david.shin at pearson.com  Sat Jul  1 07:15:21 2006
From: david.shin at pearson.com (Shin, David)
Date: Sat, 1 Jul 2006 00:15:21 -0500 
Subject: [R] generate bi-variate normal data
Message-ID: <6F3CF8F7047E374CAF4DCED3ED14576E0D1BB3B8@iowacexch4.ic.ncs.com>

Dear all,

I would like to generate bi-variate normal data given that the first column
of the data is known. for example:
I first generate a set of data using the command, 
x <- rmvnorm(10, c(0, 0), matrix(c(1, 0, 0, 1), 2))

then I would like to sum up the two columns of x:
x.sum <- apply(x, 1, sum)

now with x.sum I would like to generate another column of data, say y, that
makes cbind(x.sum, y) follow a bi-variate normal distribution with mean =
c(0, 0) and sigma = matrix(c(1, 0, 0, 1),2)

I will appreciate for all insights.

David s.

**************************************************************************** 
This email may contain confidential material.\ If you were n...{{dropped}}


From markleeds at verizon.net  Sat Jul  1 07:27:09 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Sat, 01 Jul 2006 00:27:09 -0500 (CDT)
Subject: [R] generate bi-variate normal data
Message-ID: <15932953.1053511151731629179.JavaMail.root@vms062.mailsrvcs.net>

>From: "Shin, David" <david.shin at pearson.com>
>Date: Sat Jul 01 00:15:21 CDT 2006
>To: "'r-help at stat.math.ethz.ch'" <r-help at stat.math.ethz.ch>
>Subject: [R] generate bi-variate normal data

it's an interesting question. someone else
on this list can answer more explicitly but
i think you have to use the result for the multivariate
normal distribution ( bivariate case ) where , if the
joint is normal , then the conditional is normal also
with parameters a function of the 2 means and the elements of
the covariance matrix. the result in any decent mathematical statistics such as  casella berger. so, given the one column, generate the other column conditionally using the formula  and then the joint dist will be bivariate normal.




>Dear all,
>
>I would like to generate bi-variate normal data given that the first column
>of the data is known. for example:
>I first generate a set of data using the command, 
>x <- rmvnorm(10, c(0, 0), matrix(c(1, 0, 0, 1), 2))
>
>then I would like to sum up the two columns of x:
>x.sum <- apply(x, 1, sum)
>
>now with x.sum I would like to generate another column of data, say y, that
>makes cbind(x.sum, y) follow a bi-variate normal distribution with mean =
>c(0, 0) and sigma = matrix(c(1, 0, 0, 1),2)
>
>I will appreciate for all insights.
>
>David s.
>
>**************************************************************************** 
>This email may contain confidential material.\ If you were n...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From epistat at gmail.com  Sat Jul  1 08:50:35 2006
From: epistat at gmail.com (zhijie zhang)
Date: Sat, 1 Jul 2006 14:50:35 +0800
Subject: [R] general linear model and generalized linear model
Message-ID: <2fc17e30606302350y6136bcacw442d5cb18042091f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060701/18184439/attachment.pl 

From ajayshah at mayin.org  Sat Jul  1 08:54:57 2006
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Sat, 1 Jul 2006 12:24:57 +0530
Subject: [R] SUMMARY: making contour plots using (x,y,z) data
Message-ID: <20060701065457.GA7713@lubyanka.local>

Folks,

A few days ago, I had asked a question on this mailing list about
making a contour plot where a function z(x,y) is evaluated on a grid
of (x,y) points, and the data structure at hand is a simple table of
(x,y,z) points. As usual, R has wonderful resources (and subtle
complexity) in doing this, and the gurus of the list showed me the
way. Here's a complete working example. One might stumble on contour()
but lattice::contourplot() fits this task better since the former
requires a certain unusual data representation, while
lattice::contourplot() wants a more natural data representation.

            -ans.

# Setup an interesting data matrix of (x,y,z) points:
points <- structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0.998, 0.124, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0.998, 0.71, 0.068, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0.998, 0.898, 0.396, 0.058, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.998, 0.97, 0.726, 0.268, 0.056, 0.006, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.996, 0.88, 0.546, 0.208, 0.054, 0.012, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.998, 0.964, 0.776, 0.418, 0.18, 0.054, 0.014, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.998, 0.906, 0.664, 0.342, 0.166, 0.056, 0.018, 0.006, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.986, 0.862, 0.568, 0.29, 0.15, 0.056, 0.022, 0.008, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.954, 0.778, 0.494, 0.26, 0.148, 0.056, 0.024, 0.012, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.906, 0.712, 0.43, 0.242, 0.144, 0.058, 0.028, 0.012, 0.006, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.878, 0.642, 0.38, 0.222, 0.142, 0.066, 0.034, 0.014, 0.008, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0, 0.846, 0.586, 0.348, 0.208, 0.136, 0.068, 0.034, 0.016, 0.012, 0.006, 0.004, 0.002, 0, 0, 0, 0, 0, 0, 0, 0.8, 0.538, 0.318, 0.204, 0.136, 0.07, 0.046, 0.024, 0.012, 0.008, 0.004, 0.002, 0.002, 0, 0, 0, 0, 0, 0, 0.762, 0.496, 0.294, 0.2, 0.138, 0.072, 0.05, 0.024, 0.014, 0.012, 0.006, 0.004, 0.002, 0.002, 0, 0, 0, 0, 0, 0.704, 0.472, 0.286, 0.198, 0.138, 0.074, 0.054, 0.028, 0.016, 0.012, 0.008, 0.006, 0.004, 0.002, 0.002, 0, 0, 0, 0, 0.668, 0.438, 0.276, 0.196, 0.138, 0.078, 0.054, 0.032, 0.024, 0.014, 0.012, 0.008, 0.004, 0.004, 0.002, 0.002, 0, 0, 0, 0.634, 0.412, 0.27, 0.194, 0.14, 0.086, 0.056, 0.032, 0.024, 0.016, 0.012, 0.01, 0.006, 0.004, 0.004, 0.002, 0.002, 0, 0, 0.604, 0.388, 0.26, 0.19, 0.144, 0.088, 0.058, 0.048, 0.026, 0.022, 0.014, 0.012, 0.008, 0.006, 0.004, 0.004, 0.002, 0.002, 0, 0.586, 0.376, 0.256, 0.19, 0.146, 0.094, 0.062, 0.052, 0.028, 0.024, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004, 0.004, 0.002, 0.002, 0.566, 0.364, 0.254, 0.192, 0.148, 0.098, 0.064, 0.054, 0.032, 0.024, 0.022, 0.014, 0.012, 0.012, 0.008, 0.004, 0.004, 0.004, 0.002), .Dim = c(399, 3), .Dimnames = list(NULL, c("x", "y", "z")))

# Understand this object --
summary(points)
  # x is a grid from 0 to 1
  # y is a grid from 20 to 200
  # z is the interesting object which will be the 3rd dimension.

# Solution using contourplot() from package 'lattice'
library(lattice)
d3 <- data.frame(points)
contourplot(z ~ x+y, data=d3)
## or nicer
contourplot(z ~ x+y, data=d3, cuts=20, region = TRUE)
## or using logit - transformed z values:
contourplot(qlogis(z) ~ x+y, data=d3, pretty=TRUE, region = TRUE)

# An equally interesting alternative is levelplot()
levelplot(z ~ x+y, pretty=TRUE, contour=TRUE, data=d3)

# There is a contour() function in R. Even though it sounds obvious
# for the purpose, it is a bit hard to use. 
# contour() wants 3 inputs: vectors of x and y values, and a matrix of
# z values, where the x values correspond to the rows of z, and the y
# values to the columns.  A collection of points like `points' above
# needs to be turned into such a grid. It might sound odd, but contour()
# image() and persp() have used this kind of input for the longest time.
# 
# For irregular data, there's an interp function in the akima package
# that can do this in general.
#
# The `points' object that I have above - a list of (x,y,z) points -
# fits directly into the mentality of lattice::contourplot() 

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From tobias.verbeke at telenet.be  Sat Jul  1 10:58:58 2006
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Sat, 01 Jul 2006 08:58:58 +0000
Subject: [R] general linear model and generalized linear model
Message-ID: <W44031295462681151744338@hoboe1bl6.telenet-ops.be>

>----- Oorspronkelijk bericht -----
>Van: zhijie zhang [mailto:epistat at gmail.com]
>Verzonden: zaterdag, juli 1, 2006 08:50 AM
>Aan: r-help at stat.math.ethz.ch
>Onderwerp: [R] general linear model and generalized linear model
>
>Dear friends,
>  I searched the R site and found a lot of results on general linear model
>and generalized linear model , and i was confused by them. Here, I only want
>to get some concise answers on the following questions and i'll study it by
>your hints:
> 1. Which function(package) could be used to fit the general linear model ?

Function lm from the stats package (which comes with R).
See ?lm

>2. Which function(package) could be used to fit the generalized linear model
>?

Function glm from the stats package.
See ?glm

Chapter 11 of `An Introduction to R' (which comes with R and 
is available on CRAN) is devoted to statistical models in R
and has sections on linear models and generalized linear models.
Cf. http://cran.r-project.org/manuals.html

>3. How to tell them which variables in my dataset are categorical variables
>that will be used as dummy variables?

You should (only) make sure these variables are so-called factors 
(which is the way R represents categorical variables). 
Chapter 4 of `An Introduction to R' is entirely devoted to factors.

HTH,
Tobias

>Thanks very much!
>
>
>-- 
>Kind Regards,
>Zhi Jie,Zhang ,PHD
>Department of Epidemiology
>School of Public Health
>Fudan University
>Tel:86-21-54237149
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>


From sw283 at maths.bath.ac.uk  Sat Jul  1 15:47:52 2006
From: sw283 at maths.bath.ac.uk (Simon Wood)
Date: Sat, 1 Jul 2006 14:47:52 +0100 (BST)
Subject: [R] getting the smoother matrix from smooth.spline
In-Reply-To: <971536df0606300914n6510c24cm3df3d38ffe574ced@mail.gmail.com>
References: <20060624184137.81438.qmail@web31201.mail.mud.yahoo.com> 
	<Pine.LNX.4.64.0606301707010.9510@osiris.maths.bath.ac.uk>
	<971536df0606300914n6510c24cm3df3d38ffe574ced@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0607011436450.1770@archer.maths.bath.ac.uk>

maybe not directly, as it's returning the hat/influence/smoother matrix 
rather than the model/design matrix itself... however a similar trick 
which manipulated the `fit$coef' component of a single spline fit and then 
predicted from this at the x values would be one way of extracting the 
model matrix.

> Perhaps this could be developed into a spline smooth method
> for model.matrix and included in R.
>
> On 6/30/06, Simon Wood <sw283 at maths.bath.ac.uk> wrote:
>> smooth.matrix = function(x, df){
>>  n = length(x);
>>  A = matrix(0, n, n);
>>  for(i in 1:n){
>>        y = rep(0, n); y[i]=1;
>>        yi = predict(smooth.spline(x, y, df=df),x)$y;
>>        A[,i]= yi;
>> }
>>  (A+t(A))/2;
>> }
>> 
>> 
>> >- Simon Wood, Mathematical Sciences, University of Bath, Bath BA2 7AY
>> >-             +44 (0)1225 386603         www.maths.bath.ac.uk/~sw283/
>> 
>> 
>> On Sat, 24 Jun 2006, Gregory Gentlemen wrote:
>> 
>> > Can anyone tell me the trick for obtaining the smoother matrix from 
>> smooth.spline when there are non-unique values for x. I have the following 
>> code but, of course, it only works when all values of x are unique.
>> >
>> >  ## get the smoother matrix (x having unique values
>> > smooth.matrix = function(x, df){
>> > n = length(x);
>> > A = matrix(0, n, n);
>> > for(i in 1:n){
>> >       y = rep(0, n); y[i]=1;
>> >       yi = smooth.spline(x, y, df=df)$y;
>> >       A[,i]= yi;
>> > }
>> > (A+t(A))/2;
>> > }
>> >
>> >
>> >  Thanks for any assistance,
>> >  Gregory
>> >
>> >
>> > ---------------------------------
>> >
>> > ---------------------------------
>> > Get a sneak peak at messages with a handy reading pane.
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> >
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> 
>


From xiaosu at mail.ucf.edu  Sat Jul  1 16:50:12 2006
From: xiaosu at mail.ucf.edu (Xiaogang Su)
Date: Sat, 01 Jul 2006 10:50:12 -0400
Subject: [R] Start Model for POLYCLASS
Message-ID: <s4a65377.050@mail.ucf.edu>

Dear all, 

I have a question on how to set up the starting model in POLYCLASS and
make sure the terms in the starting model retained in the final
POLYCLASS model. 

In the function POLYMARS, this can be done using the STARTMODEL option.
See below for example, I started with model 
y= b0 + b1*X1 + b2*X2 + b3*X4 + b4*X5 + b5*X2*X5 + e

> m00 <- matrix(c(
     1,  NA, 0, NA, 1,     
     2,  NA, 0, NA, 1,     
     4,  NA, 0, NA, 1,     
     5,  NA, 0, NA, 1,
     2,  NA, 5, NA, 1),nrow = 5, ncol=5, byrow=TRUE);

> m2 <- polymars(response=PID2$y, predictors=PID2[,1:7], 
startmodel=m00) 
> summary(m2)  

But I could not figure out how this works for POLYCLASS. There is an
option FIT in POLYCLASS, which needs to be a POLYCLASS object though. 

Any suggestion or information is greatly appreciated. 

Sincerely,
Xiaogang Su


================================
Xiaogang Su,  Assistant Professor
Department of Statistics and Actuarial Science
University of Central Florida
Orlando, FL 32816
(407) 823-2940 [O]
xiaosu at mail.ucf.edu
http://pegasus.cc.ucf.edu/~xsu/


From cberry at tajo.ucsd.edu  Sat Jul  1 17:47:13 2006
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Sat, 1 Jul 2006 08:47:13 -0700
Subject: [R] Way to convert data frame to matrix
Message-ID: <Pine.LNX.4.64.0607010843180.25589@tajo.ucsd.edu>


I think this will do what you want:

dframe <- read.table("your.text.file", <other args as needed> )

mat <- tapply(dframe[,3],dframe[,1:2],c)



On Fri, 30 Jun 2006, Wade Wall wrote:

>  I have a text file that I have imported into R.  It contains 3 columns and
> 316940 rows.  The first column is vegetation plot ID, the second species
> names and the third is a cover value (numeric).  I imported using the
> read.table function.
>
> My problem is this.  I need to reformat the information as a matrix, with
> the first column becoming the row labels and the second the column labels
> and the cover values as the matrix cell data.  However, since the
> read.tablefunction imported the data as an indexed data frame, I can't
> use the columns
> as vectors.  Is there a way around this, to convert the data frame as 3
> separate vectors?  I have been looking all over for a function, and my
> programming skills are not great.
>
> Thanks in advance
>
> 	[[alternative HTML version deleted]]
>
>
>
>    [ Part 3.23: "Included Message" ]
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0717


From maechler at stat.math.ethz.ch  Sat Jul  1 17:50:11 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 1 Jul 2006 17:50:11 +0200
Subject: [R] polynomial expansion in R
In-Reply-To: <20060701041131.50212.qmail@web35803.mail.mud.yahoo.com>
References: <20060701041131.50212.qmail@web35803.mail.mud.yahoo.com>
Message-ID: <17574.39347.670280.803537@stat.math.ethz.ch>

>>>>> "yyan" == yyan liu <zhliur at yahoo.com>
>>>>>     on Fri, 30 Jun 2006 21:11:31 -0700 (PDT) writes:

    yyan> Hi: I have two vectors of data, x and y and I want to
    yyan> get the "polynomial" expansion of (x+y)^p with any
    yyan> integer power p in R. Suppose p=2, then I want a
    yyan> matrix of five vectors, namely, x y x^2 y^2 x*y. The
    yyan> coefficient of the polynomial is not needed. I can
    yyan> write it manully if p is small. But I want it in the
    yyan> case of p=10 or even bigger, is there any function in
    yyan> R can do that automatically for me with certain choice
    yyan> of p?  Thx a lot!
 

 polym(x,y, degree = p, raw = TRUE)

will probably be pretty close to what you want.

Martin Maechler, ETH Zurich


From patrick.giraudoux at univ-fcomte.fr  Sat Jul  1 18:57:10 2006
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sat, 01 Jul 2006 18:57:10 +0200
Subject: [R] nlme: correlation structure in gls and zero distance
Message-ID: <44A6A966.5060206@univ-fcomte.fr>

Dear listers,

I am trying to model the distribution of  fox density over years  in the 
Doubs department. Measurements have been taken on 470 plots in March 
each year and georeferenced. Average density is supposed to be different 
each year.

In a first approach, I would like to use a general model of this type, 
taking spatial correlation into account:

gls(IKAfox~an,correlation=corExp(2071,form=~x+y,nugget=1.22),data=renliev)

but I get

 > 
gls(IKAfox~an,correlation=corExp(2071,form=~x+y,nugget=1.22),data=renliev)
Error in getCovariate.corSpatial(object, data = data) :
        Cannot have zero distances in "corSpatial"

I understand that the 470 geographical coordinates are repeated three 
times (measurement are taken each of the three years at the same place) 
which obviously cannot be handled there.

Does anybody know a way to work around that except jittering slightly 
the geographical coordinates?

Thanks in advance,

Patrick


From ivowel at gmail.com  Sat Jul  1 20:23:25 2006
From: ivowel at gmail.com (ivo welch)
Date: Sat, 1 Jul 2006 14:23:25 -0400
Subject: [R] curiosity question: new graphics vs. old graphics subsystem
Message-ID: <50d1c22d0607011123x1fc06845oceb6cbaf008ee00@mail.gmail.com>

I just read paul murrell's new book, R graphics.

now, I have always used the traditional graphics system.  apparently,
the new (trellis?) system is an entirely separate graphics system.
after reading the book, I cannot figure out what the intrinsic
capability advantage of the old graphics system is that cannot be
replicated in the trellis system.

if the new system's capabilities are practically a subset of the old
system, why don't we design a compatibility layer so that we can just
have one graphics subsystem, instead?  it seems weird that newbies
learn the standard system first, and then, instead of building on it
with more complex functions, are told to forget about things and start
with something new.  I do like the simplicity of learning of the old
system, but this would be the same if it were to come through a
compatibility layer, too.  and then it would be easy to build learning
on it.

but maybe I have it all wrong.  maybe there is something unique about
the old system that the new system cannot do.  curious:  what is it?

I also found the naming of the new system confusing.  there is
trellis, there is lattice, there is grid.  how exactly should the new
system be called?  paul calls the old system "traditional."  the new
one seems to rear its head in different forms.

some other opinions (which follow the old rule that everyone has one):

* if we had one graphics subsystem, paul's book, and for this matter
any explanation of the R graphics system, would become more
parsimonious.

* R is, IMHO, the premier "programmed graphics" package today.  I may
be complaining, but I also recognize that it is great.  so, please
consider this to be only a suggestion.

* we have a pixmap image function.  we should also have a pdf
includegraphics function, which can import an existing graphics image.
 if a device (X11) is incapable of displaying it, we should just
display a rectangle of the bounding box.  this would open up even more
avenues to the ability of R to create graphics.

regards,

/ivo


From binabina at bellsouth.net  Sun Jul  2 02:05:06 2006
From: binabina at bellsouth.net (zubin)
Date: Sat, 01 Jul 2006 20:05:06 -0400
Subject: [R] Data Manipulations - Group By equivalent
In-Reply-To: <449DA9F1.90700@bellsouth.net>
References: <446D1C5F.9060002@bellsouth.net> <4498072C.5010600@bellsouth.net>
	<449DA9F1.90700@bellsouth.net>
Message-ID: <44A70DB2.9080106@bellsouth.net>

Hello, a beginner R user - boy i wish there was a book on just data 
manipulations for SAS users learning R (equivalent to the SAS DATA 
STEP)..  Okay, my question: 

I have a panel data set, hotel data occupancy by month for 12 months, 
1000 hotels.  I have a field labeled 'year' and want to consolidate the 
monthly records using an average into 1000 occupancy numbers - just a 
simple average of the 12 months by hotel.  In SQL this operation is 
pretty easy, a group by query (group by hotel where year = 2005, avg 
occupancy) - how is this done in R? (in R language not SQL).  Thx!

-zubin


From f.harrell at vanderbilt.edu  Sun Jul  2 02:43:45 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 01 Jul 2006 19:43:45 -0500
Subject: [R] Data Manipulations - Group By equivalent
In-Reply-To: <44A70DB2.9080106@bellsouth.net>
References: <446D1C5F.9060002@bellsouth.net>
	<4498072C.5010600@bellsouth.net>	<449DA9F1.90700@bellsouth.net>
	<44A70DB2.9080106@bellsouth.net>
Message-ID: <44A716C1.1040705@vanderbilt.edu>

zubin wrote:
> Hello, a beginner R user - boy i wish there was a book on just data 
> manipulations for SAS users learning R (equivalent to the SAS DATA 
> STEP)..  Okay, my question: 
> 
> I have a panel data set, hotel data occupancy by month for 12 months, 
> 1000 hotels.  I have a field labeled 'year' and want to consolidate the 
> monthly records using an average into 1000 occupancy numbers - just a 
> simple average of the 12 months by hotel.  In SQL this operation is 
> pretty easy, a group by query (group by hotel where year = 2005, avg 
> occupancy) - how is this done in R? (in R language not SQL).  Thx!
> 
> -zubin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

This is in the most basic R documentation.  Please read it.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From liuwensui at gmail.com  Sun Jul  2 04:39:08 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Sat, 1 Jul 2006 22:39:08 -0400
Subject: [R] Data Manipulations - Group By equivalent
In-Reply-To: <44A70DB2.9080106@bellsouth.net>
References: <446D1C5F.9060002@bellsouth.net> <4498072C.5010600@bellsouth.net>
	<449DA9F1.90700@bellsouth.net> <44A70DB2.9080106@bellsouth.net>
Message-ID: <1115a2b00607011939v26a29f90saf450431b9b2d008@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060701/a030822f/attachment.pl 

From epistat at gmail.com  Sun Jul  2 06:39:17 2006
From: epistat at gmail.com (zhijie zhang)
Date: Sun, 2 Jul 2006 12:39:17 +0800
Subject: [R] replace values?
Message-ID: <2fc17e30607012139g1168236an26b7cd2fc93e4ea9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060702/26e38d9e/attachment.pl 

From jgreenberg at arc.nasa.gov  Sun Jul  2 07:34:25 2006
From: jgreenberg at arc.nasa.gov (Jonathan Greenberg)
Date: Sat, 1 Jul 2006 22:34:25 -0700
Subject: [R] Optional variables in function?
Message-ID: <001701c69d99$2e7b56b0$c101a8c0@QWARD>

I'm a bit new to writing R functions and I was wondering what the "best
practice" for having optional variables in a function is, and how to test
for optional and non-optional variables?  e.g. if I have the following
function:

helpme <- function(a,b,c) {


}

In this example, I want c to be an optional variable, but a and b to be
required.  How do I:
1) test to see if the user has inputted c
2) break out of the function of the user has NOT inputted a or b.

Thanks!

--j

--

Jonathan A. Greenberg, PhD
NRC Research Associate
NASA Ames Research Center
MS 242-4
Moffett Field, CA 94035-1000
Phone: 415-794-5043
AIM: jgrn3007
MSN: jgrn3007 at hotmail.com


From thomaspreuth at web.de  Sun Jul  2 09:57:02 2006
From: thomaspreuth at web.de (Thomas Preuth)
Date: Sun, 02 Jul 2006 09:57:02 +0200
Subject: [R] problems with simple statistical procedures
Message-ID: <44A77C4E.3090902@web.de>

Hello,

I use an imported dataframe and want to extract the mean value for one 
column.
after typing "mean (rae.df$VOL_DEP)" I receive
"[1] NA
Warning message:
Argument ist weder numerisch noch boolesch: gebe NA zur?ck in: 
mean.default("rae.df$POINT_Y_CH") "

But when i look into the dataframe the column is characterized as numeric.

Sorry for bothering but as a complete newbie I just cannot halp myself.

Greetings,
thomas


From otoomet at ut.ee  Sun Jul  2 12:00:10 2006
From: otoomet at ut.ee (Ott Toomet)
Date: Sun, 2 Jul 2006 13:00:10 +0300
Subject: [R] workaround for numeric problems
Message-ID: <200607021000.k62A0Ag4016446@hugo.obs.ee>

Dear R-people,

I have to compute 

C - -(pnorm(B)*dnorm(B)*B + dnorm(B)^2)/pnorm(B)^2

This expression seems to be converging to -1 if B approaches to -Inf
(although I am unable to prove it).  R has no problems until B equals
around -28 or less, where both numerator and denominator go to 0 and
you get NaN. A simple workaround I did was

C <- ifelse(B > -25,
           -(pnorm(B)*dnorm(B)*B + dnorm(B)^2)/pnorm(B)^2,
            -1)

It works well for me (32bit intel/linux platform).  But what about
other processors/platforms/compilator options?  Are there any better
ways for finding out at which values the numerical problems start?
Can one derive something from .Machine$double.eps (but what about the
precison of dnorm and other analytic functions)?

Thanks in advance,
Ott


From baron at psych.upenn.edu  Sun Jul  2 12:12:43 2006
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sun, 2 Jul 2006 06:12:43 -0400
Subject: [R] replace values?
In-Reply-To: <2fc17e30607012139g1168236an26b7cd2fc93e4ea9@mail.gmail.com>
References: <2fc17e30607012139g1168236an26b7cd2fc93e4ea9@mail.gmail.com>
Message-ID: <20060702101243.GC2770@psych.upenn.edu>

On 07/02/06 12:39, zhijie zhang wrote:
> Dear friends,
>   i have a dataset like this:
> x y z
> 1 2 3
> 2 3 1
> 3 2 1
> 1 1 3
> 2 1 2
> 3 2 3
> 2 1 1
> I want to replace x with the following values:1<-a,2<-b,3<-c,4<-d;
>              replace y with the following values:1<-b,2<-a,3<-c,4<-d;
>              replace z with the following values:1<-d,2<-c,3<-b,4<-a;

Here's one way.  Call your dataset M, and assume it is a
data.frame.  This method of replacement works best when you are
replacing consecutive integers, as you are.  Note that X[1] is
"a", X[2] is "b" and so on.

X <- c("a","b","c","d")
Y <- c("b","a","c","d")
Z <- c("d","c","b","a")
M$x <- X[M$x]
M$y <- Y[M$y]
M$z <- Z[M$z]

> Finally,select two subsets:
> 1. if x='a';
> 2.x='a' and y='a';

M[M$x=="a",]
M[M$x=="a" & M$y=="a",]

The subsets will be rows.  I'm not sure that's what you mean.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron


From mb.atelier at web.de  Sun Jul  2 12:54:43 2006
From: mb.atelier at web.de (Matthias Braeunig)
Date: Sun, 02 Jul 2006 12:54:43 +0200
Subject: [R] replace values?
In-Reply-To: <2fc17e30607012139g1168236an26b7cd2fc93e4ea9@mail.gmail.com>
References: <2fc17e30607012139g1168236an26b7cd2fc93e4ea9@mail.gmail.com>
Message-ID: <44A7A5F3.5020000@web.de>

# reproducing your example
xx<-"x y z
+ 1 2 3
+ 2 3 1
+ 3 2 1
+ 1 1 3
+ 2 1 2
+ 3 2 3
+ 2 1 1"

# you did not tell us the class of your data, assuming data.frame
df<-read.table(textConnection(xx),header=T,colClasses="factor")

# a clean way to do what you want is using factors with ?levels
# (note that data has already been read as factor)
levels(df$x)<-c("a","b","c","d")
levels(df$y)<-c("b","a","c","d")
levels(df$z)<-c("d","c","b","a")

subset(df,x=="a")
  x y z
1 a a b
4 a b b
subset(df,x=="a"&y=="a")
  x y z
1 a a b


HTH, m


zhijie zhang wrote:
> Dear friends,
>   i have a dataset like this:
> x y z
> 1 2 3
> 2 3 1
> 3 2 1
> 1 1 3
> 2 1 2
> 3 2 3
> 2 1 1
> I want to replace x with the following values:1<-a,2<-b,3<-c,4<-d;
>              replace y with the following values:1<-b,2<-a,3<-c,4<-d;
>              replace z with the following values:1<-d,2<-c,3<-b,4<-a;
> Finally,select two subsets:
> 1. if x='a';
> 2.x='a' and y='a';
>  thanks very much!


From Dimitris.Rizopoulos at med.kuleuven.be  Sun Jul  2 12:59:10 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Sun, 02 Jul 2006 12:59:10 +0200
Subject: [R] workaround for numeric problems
In-Reply-To: <200607021000.k62A0Ag4016446@hugo.obs.ee>
References: <200607021000.k62A0Ag4016446@hugo.obs.ee>
Message-ID: <1151837950.44a7a6fee30c5@webmail1.kuleuven.be>

I'd compute this in the log-scale (taking also advantage of the 'log' 
and 'log.p' arguments of dnorm() and pnorm(), respectively), and then 
transform back, e.g.,

fn1 <- function(B){
    -(pnorm(B) * dnorm(B) * B + dnorm(B)^2)/pnorm(B)^2
}

fn2 <- function(B){
    p1 <- dnorm(B, log = TRUE) + log(-B) - pnorm(B, log.p = TRUE)
    p2 <- 2 * (dnorm(B, log = TRUE) - pnorm(B, log.p = TRUE))
    exp(p1) - exp(p2)
}

fn1(c(-15, -25, -35, -55, -105))
fn2(c(-15, -25, -35, -55, -105))


I hope it helps.

Best,
Dimitris

---- 
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting Ott Toomet <otoomet at ut.ee>:

> Dear R-people,
> 
> I have to compute 
> 
> C - -(pnorm(B)*dnorm(B)*B + dnorm(B)^2)/pnorm(B)^2
> 
> This expression seems to be converging to -1 if B approaches to -Inf
> (although I am unable to prove it).  R has no problems until B
> equals
> around -28 or less, where both numerator and denominator go to 0 and
> you get NaN. A simple workaround I did was
> 
> C <- ifelse(B > -25,
>            -(pnorm(B)*dnorm(B)*B + dnorm(B)^2)/pnorm(B)^2,
>             -1)
> 
> It works well for me (32bit intel/linux platform).  But what about
> other processors/platforms/compilator options?  Are there any better
> ways for finding out at which values the numerical problems start?
> Can one derive something from .Machine$double.eps (but what about
> the
> precison of dnorm and other analytic functions)?
> 
> Thanks in advance,
> Ott
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From jholtman at gmail.com  Sun Jul  2 13:31:24 2006
From: jholtman at gmail.com (jim holtman)
Date: Sun, 2 Jul 2006 07:31:24 -0400
Subject: [R] Optional variables in function?
In-Reply-To: <001701c69d99$2e7b56b0$c101a8c0@QWARD>
References: <001701c69d99$2e7b56b0$c101a8c0@QWARD>
Message-ID: <644e1f320607020431t6181b7e1u7d5b08df0b9d01ac@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060702/29600c76/attachment.pl 

From epistat at gmail.com  Sun Jul  2 14:33:20 2006
From: epistat at gmail.com (zhijie zhang)
Date: Sun, 2 Jul 2006 20:33:20 +0800
Subject: [R] how to recode in my dataset?
Message-ID: <2fc17e30607020533m4449e647o273dbe1090b90f56@mail.gmail.com>

Dear Rusers,
 My question is about "recode variables". First, i'd like to say
something about the idea of recoding:
 My dataset have three variables:type,soiltem and airtem,which means
grass type, soil temperature and air temperature. As we all known, the
change of air temperature is greater than soil temperature,so the
values in those two different temperaturemay represent different
range.
 My recoding is to recode soiltem with 0.2 intervals, and airtem with
0.5 intervals, that is:
In soiltem:0~0.2<-0.1,  0.2~0.4<-0.3, 0.4`0.6<-0.5,...etc;
In airtem:0~0.5<-0.25,  0.5~1<-0.75, 1`1.5<-1.25,...etc;
My example like this:
type<-c(1, 1, 2, 3,4,1,1,4,3,2)
soiltem<-c(19.2,18.6,19.5,19.8,19.6,20.6,19.1,18.7,22.4,21.6)
airtem<-c(19.9,20.5,21.6,25.6,22.6,21.3,23.7,21.5,24.7,24.4)
mydata<-data.frame(type,soiltem,airtem) #copy the above four arguments
to generate the dataset

mydata
   type soiltem airtem
1     1    19.2   19.9
2     1    18.6   20.5
3     2    19.5   21.6
4     3    19.8   25.6
5     4    19.6   22.6
6     1    20.6   21.3
7     1    19.1   23.7
8     4    18.7   21.5
9     3    22.4   24.7
10    2    21.6   24.4

Thanks very much!
-- 
Kind Regards,
Zhi Jie,Zhang ,PHD
Department of Epidemiology
School of Public Health
Fudan University
Tel:86-21-54237149


From ThadenJohnJ at uams.edu  Sun Jul  2 14:38:21 2006
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Sun, 2 Jul 2006 07:38:21 -0500
Subject: [R] sparse matrix tools
In-Reply-To: <mailman.7.1151834402.22992.r-help@stat.math.ethz.ch>
Message-ID: <0C6BF3FC506F664F90C8BA3E0160462D04A75840@EXCHANGE3.ad.uams.edu>

Dear R-Help list:
    I'm using the Matrix library to operate on 600 X ~5000 element
unsymmetrical sparse arrays. So far, so good, but if I find I need more
speed or functionality, how hard would it be to utilize other sparse
matrix toolsets from within R, say MUMPS, PARDISO or UMFPACK, that do
not have explicit R interfaces?  More information on these is available
here
   www.cise.ufl.edu/research/sparse/umfpack/ 
   www.computational.unibas.ch/cs/scicomp/software/pardiso
   www.enseeiht.fr/lima/apo/MUMPS/ 
and in these reviews
   ftp://ftp.numerical.rl.ac.uk/pub/reports/ghsNAGIR20051r1.pdf 
   http://www.cise.ufl.edu/research/sparse/codes/ 
neither of which reviewed the R Matrix package, unfortunately.
Thanks,   
- John Thaden, Ph.D., U. Arkansas for Med. Sci., Little Rock.

Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From ligges at statistik.uni-dortmund.de  Sun Jul  2 14:57:50 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 02 Jul 2006 14:57:50 +0200
Subject: [R] problems with simple statistical procedures
In-Reply-To: <44A77C4E.3090902@web.de>
References: <44A77C4E.3090902@web.de>
Message-ID: <44A7C2CE.5050707@statistik.uni-dortmund.de>

Thomas Preuth wrote:
> Hello,
> 
> I use an imported dataframe and want to extract the mean value for one 
> column.
> after typing "mean (rae.df$VOL_DEP)" I receive
> "[1] NA
> Warning message:
> Argument ist weder numerisch noch boolesch: gebe NA zur?ck in: 
> mean.default("rae.df$POINT_Y_CH") "

Well,
   rae.df$VOL_DEP != "rae.df$POINT_Y_CH"

I think this is really strange. Are you sure this is the exact call and 
its output? If so, please tell us the output of
   str(rae.df)

Uwe Ligges




> But when i look into the dataframe the column is characterized as numeric.
> 
> Sorry for bothering but as a complete newbie I just cannot halp myself.
> 
> Greetings,
> thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ligges at statistik.uni-dortmund.de  Sun Jul  2 15:15:09 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 02 Jul 2006 15:15:09 +0200
Subject: [R] send output to printer
In-Reply-To: <44A57507.1030901@web.de>
References: <44A57507.1030901@web.de>
Message-ID: <44A7C6DD.9080500@statistik.uni-dortmund.de>

Matthias Braeunig wrote:
> It has to be a simple thing, but I could not figure it out:
> 
> How do I send the text output from object x to the printer?
> As a shell user I would expect a pipe to the printer... "|kprinter" or
> "|lpr -Pmyprinter" somehow. And yes, I'm on Linux.

I think capture.output() helps to send stuff to a connection.

Uwe Ligges


> Thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From patrick.giraudoux at univ-fcomte.fr  Sun Jul  2 15:36:23 2006
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sun, 02 Jul 2006 15:36:23 +0200
Subject: [R] nlme: correlation structure in gls and zero distance
In-Reply-To: <44A6C578.4010409@cropdesign.com>
References: <44A6A966.5060206@univ-fcomte.fr> <44A6C578.4010409@cropdesign.com>
Message-ID: <44A7CBD7.1060303@univ-fcomte.fr>



Joris De Wolf a ?crit :
> Have you tried to define 'an' as a group? Like in
>
> gls(IKAfox~an,correlation=corExp(2071,form=~x+y|an,nugget=1.22),data=renliev) 
>
>
> A small data set might help to explain the problem.
>
> Joris
Thanks. Seems to work with a small artificial data set:


an<-as.factor(rep(2001:2004,each=10))
x<-rep(rnorm(10),times=4)
y<-rep(rnorm(10),times=4)
IKA<-rpois(40,2)
site<-as.factor(rep(letters[1:10],times=4))


library(nlme)

mod1<-gls(IKA~an-1,correlation=corExp(form=~x+y))

 >Error in getCovariate.corSpatial(object, data = data) :
        Cannot have zero distances in "corSpatial"


mod2<-gls(IKA~an-1,correlation=corExp(form=~x+y|an))

 > mod2
Generalized least squares fit by REML
  Model: IKA ~ an - 1
  Data: NULL
  Log-restricted-likelihood: -73.63998

Coefficients:
  an2001   an2002   an2003   an2004
1.987611 2.454520 2.429907 2.761011

Correlation Structure: Exponential spatial correlation
 Formula: ~x + y | an
 Parameter estimate(s):
    range
0.4304012
Degrees of freedom: 40 total; 36 residual
Residual standard error: 1.746205





>
> Joris
>
> Patrick Giraudoux wrote:
>> Dear listers,
>>
>> I am trying to model the distribution of  fox density over years  in 
>> the Doubs department. Measurements have been taken on 470 plots in 
>> March each year and georeferenced. Average density is supposed to be 
>> different each year.
>>
>> In a first approach, I would like to use a general model of this 
>> type, taking spatial correlation into account:
>>
>> gls(IKAfox~an,correlation=corExp(2071,form=~x+y,nugget=1.22),data=renliev) 
>>
>>
>> but I get
>>
>>  > 
>> gls(IKAfox~an,correlation=corExp(2071,form=~x+y,nugget=1.22),data=renliev) 
>>
>> Error in getCovariate.corSpatial(object, data = data) :
>>         Cannot have zero distances in "corSpatial"
>>
>> I understand that the 470 geographical coordinates are repeated three 
>> times (measurement are taken each of the three years at the same 
>> place) which obviously cannot be handled there.
>>
>> Does anybody know a way to work around that except jittering slightly 
>> the geographical coordinates?
>>
>> Thanks in advance,
>>
>> Patrick
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
> confidentiality notice:
> The information contained in this e-mail is confidential a...{{dropped}}


From jh1030 at columbia.edu  Sun Jul  2 16:12:25 2006
From: jh1030 at columbia.edu (JENNIFER HILL)
Date: Sun, 2 Jul 2006 10:12:25 -0400 (EDT)
Subject: [R] large dataset!
Message-ID: <200607021412.k62ECPDT008467@papaya.cc.columbia.edu>


Hi, I need to analyze data that has 3.5 million observations and
about 60 variables and I was planning on using R to do this but
I can't even seem to read in the data.  It just freezes and ties
up the whole system -- and this is on a Linux box purchased about
6 months ago on a dual-processor PC that was pretty much the top
of the line.  I've tried expanding R the memory limits but it 
doesn't help.  I'll be hugely disappointed if I can't use R b/c
I need to do build tailor-made models (multilevel and other 
complexities).   My fall-back is the SPlus big data package but
I'd rather avoid if anyone can provide a solution....

Thanks!!!!

Jennifer Hill


From m_nica at hotmail.com  Sun Jul  2 16:19:02 2006
From: m_nica at hotmail.com (Mihai Nica)
Date: Sun, 2 Jul 2006 09:19:02 -0500
Subject: [R] curiosity question: new graphics vs. old graphics subsystem
Message-ID: <BAY105-W44A6C55A153B2A64666B5F8730@phx.gbl>

Well, as a newbee, I believe your idea is great. However, the R Core team is, in my humble opinion, way too stretched (for a free software development team) to do this. A complementary development team (similar to, say, the Tinn-R team) might be able to address this issue. I wish I would have the skills to contribute :-) Just my 2c.

The least of learning is done in the classrooms.
  - Thomas Merton

----------------------------------------
> Date: Sun, 2 Jul 2006 09:34:39 -0400
> From: ivowel at gmail.com
> To: m_nica at hotmail.com
> Subject: Re: [R] curiosity question: new graphics vs. old graphics subsystem
> 
> hi mihai:  it is more likely that the developers will take this more
> seriously if you echo my concern on r-help itself.  regards, /iaw
> 
> On 7/1/06, Mihai Nica <m_nica at hotmail.com> wrote:
> >
> > Wow, this is what I would say if I knew how to say it :-)!!!! For newbees
> > (such as myself) or those who lack programming expertise (and, why not, for
> > those not interested in programming) this approach would be great.
> > mihai
> >
> > ________________________________
> > Express yourself instantly with Windows Live Messenger

_________________________________________________________________
Express yourself: design your homepage the way you want it with Live.com.


From Dimitris.Rizopoulos at med.kuleuven.be  Sun Jul  2 20:02:52 2006
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Sun, 02 Jul 2006 20:02:52 +0200
Subject: [R] how to recode in my dataset?
In-Reply-To: <2fc17e30607020533m4449e647o273dbe1090b90f56@mail.gmail.com>
References: <2fc17e30607020533m4449e647o273dbe1090b90f56@mail.gmail.com>
Message-ID: <1151863372.44a80a4c51cfa@webmail1.kuleuven.be>

probably ?cut() is what you're looking for, e.g., something like:

ind <- cut(mydata$soiltem, seq(0, 60, 0.2), labels = FALSE)
seq(0.1, 60, 0.2)[ind]


I hope it helps.

Best,
Dimitris

---- 
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting zhijie zhang <epistat at gmail.com>:

> Dear Rusers,
>  My question is about "recode variables". First, i'd like to say
> something about the idea of recoding:
>  My dataset have three variables:type,soiltem and airtem,which means
> grass type, soil temperature and air temperature. As we all known,
> the
> change of air temperature is greater than soil temperature,so the
> values in those two different temperaturemay represent different
> range.
>  My recoding is to recode soiltem with 0.2 intervals, and airtem
> with
> 0.5 intervals, that is:
> In soiltem:0~0.2<-0.1,  0.2~0.4<-0.3, 0.4`0.6<-0.5,...etc;
> In airtem:0~0.5<-0.25,  0.5~1<-0.75, 1`1.5<-1.25,...etc;
> My example like this:
> type<-c(1, 1, 2, 3,4,1,1,4,3,2)
> soiltem<-c(19.2,18.6,19.5,19.8,19.6,20.6,19.1,18.7,22.4,21.6)
> airtem<-c(19.9,20.5,21.6,25.6,22.6,21.3,23.7,21.5,24.7,24.4)
> mydata<-data.frame(type,soiltem,airtem) #copy the above four
> arguments
> to generate the dataset
> 
> mydata
>    type soiltem airtem
> 1     1    19.2   19.9
> 2     1    18.6   20.5
> 3     2    19.5   21.6
> 4     3    19.8   25.6
> 5     4    19.6   22.6
> 6     1    20.6   21.3
> 7     1    19.1   23.7
> 8     4    18.7   21.5
> 9     3    22.4   24.7
> 10    2    21.6   24.4
> 
> Thanks very much!
> -- 
> Kind Regards,
> Zhi Jie,Zhang ,PHD
> Department of Epidemiology
> School of Public Health
> Fudan University
> Tel:86-21-54237149
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From A.Robinson at ms.unimelb.edu.au  Sun Jul  2 23:05:35 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Mon, 3 Jul 2006 07:05:35 +1000
Subject: [R] large dataset!
In-Reply-To: <200607021412.k62ECPDT008467@papaya.cc.columbia.edu>
References: <200607021412.k62ECPDT008467@papaya.cc.columbia.edu>
Message-ID: <20060702210535.GB758@ms.unimelb.edu.au>

Jennifer,

it sounds like that's too much data for R to hold in your computer's
RAM. You should give serious consideration as to whether you need all
those data for the models that you're fitting, and if so, whether you
need to do them all at once.  If not, think about pre-processing
steps, using e.g. SQL command, to pull out the data that you need. For
example, if the data are spatial, then think about analyzing them by
patches.  

Good luck,

Andrew

On Sun, Jul 02, 2006 at 10:12:25AM -0400, JENNIFER HILL wrote:
> 
> Hi, I need to analyze data that has 3.5 million observations and
> about 60 variables and I was planning on using R to do this but
> I can't even seem to read in the data.  It just freezes and ties
> up the whole system -- and this is on a Linux box purchased about
> 6 months ago on a dual-processor PC that was pretty much the top
> of the line.  I've tried expanding R the memory limits but it 
> doesn't help.  I'll be hugely disappointed if I can't use R b/c
> I need to do build tailor-made models (multilevel and other 
> complexities).   My fall-back is the SPlus big data package but
> I'd rather avoid if anyone can provide a solution....
> 
> Thanks!!!!
> 
> Jennifer Hill
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From epistat at gmail.com  Mon Jul  3 05:11:32 2006
From: epistat at gmail.com (zhijie zhang)
Date: Mon, 3 Jul 2006 11:11:32 +0800
Subject: [R] how to get the studentized residuals in lm()
Message-ID: <2fc17e30607022011h4ba3d6b4v26617726e51ac89a@mail.gmail.com>

Dear friends,
 In s-plus, lm()  generates the the studentized residuals
automatically for us, and In R, it seems don't have the results: After
i fitted lm(), i use attibutes() to see the objects and didn't find
studentized residuals .
 How to get the the studentized residuals in lm(),have i missed something?
thanks very much!

-- 
Kind Regards,
Zhi Jie,Zhang ,PHD
Department of Epidemiology
School of Public Health
Fudan University
Tel:86-21-54237149


From jjonphl at gmail.com  Mon Jul  3 05:40:51 2006
From: jjonphl at gmail.com (miguel manese)
Date: Mon, 3 Jul 2006 11:40:51 +0800
Subject: [R] large dataset!
In-Reply-To: <20060702210535.GB758@ms.unimelb.edu.au>
References: <200607021412.k62ECPDT008467@papaya.cc.columbia.edu>
	<20060702210535.GB758@ms.unimelb.edu.au>
Message-ID: <d35b9da60607022040i75672985offb8091736ad173c@mail.gmail.com>

Hello Jennifer,

I'm writing a package SQLiteDF for Google SOC2006, under the
supervision of Prof. Bates & Prof. Riley. Basically, it stores data
frame into sqlite databases (i.e. in a file) and aims to be
transparently accessible to R using the same operators for ordinary
data frames.

Right now, it's quite usable (the "indexers" are working, and some
other generic methods), and only for linux (I should have the windows
package any time soon though). I would love to hear about your
requirements so as to test my package.

Cheers,
M. Manese

On 7/3/06, Andrew Robinson <A.Robinson at ms.unimelb.edu.au> wrote:
> Jennifer,
>
<snipped>


From nbderby at gmail.com  Mon Jul  3 06:10:53 2006
From: nbderby at gmail.com (Nathaniel Derby)
Date: Sun, 2 Jul 2006 21:10:53 -0700
Subject: [R] panel ordering in nlme and augPred plots
Message-ID: <7ab3bfe50607022110v1fefe29p82b4de3e76f63f92@mail.gmail.com>

Hi,

I'm new at this, I'm very confused, and I think I'm missing something
important here.  In our pet example we have this:

> fm <- lme(Orthodont)
> plot(Orthodont)
> plot(augPred(fm, level = 0:1))

which gives us a trellis plot with the females above the males,
starting with "F03", "F04", "F11", "F06", etc.  I thought the point of
this was to create an ordering where the females are ordered ("F01",
"F02", "F03", etc -- followed by the males being ordered).  However,
the solution given ...

> fm <- lme(Orthodont)
> plot(Orthodont)
> plot(augPred(fm1, level = 0:1), skip = rep(c(F,T), c(16, 2)))

... doesn't solve it -- although it does do all the females before
starting on the males.  That is, it starts with "F02", "F08", "F03",
... which isn't in order either.

Running Petr's code also gave output which wasn't ordered by the subjects.

Could someone please explain to me how to order the panels of the
trellis plot by the subjects?


thanks,

Nandor


From ronggui.huang at gmail.com  Mon Jul  3 06:29:42 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Mon, 3 Jul 2006 12:29:42 +0800
Subject: [R] how to get the studentized residuals in lm()
In-Reply-To: <2fc17e30607022011h4ba3d6b4v26617726e51ac89a@mail.gmail.com>
References: <2fc17e30607022011h4ba3d6b4v26617726e51ac89a@mail.gmail.com>
Message-ID: <38b9f0350607022129r1c026efehb5fd223f2e641e77@mail.gmail.com>

> help.search("studentized")

You will see:
studres(MASS)           Extract Studentized Residuals from a Linear Model



2006/7/3, zhijie zhang <epistat at gmail.com>:
> Dear friends,
>  In s-plus, lm()  generates the the studentized residuals
> automatically for us, and In R, it seems don't have the results: After
> i fitted lm(), i use attibutes() to see the objects and didn't find
> studentized residuals .
>  How to get the the studentized residuals in lm(),have i missed something?
> thanks very much!
>
> --
> Kind Regards,
> Zhi Jie,Zhang ,PHD
> Department of Epidemiology
> School of Public Health
> Fudan University
> Tel:86-21-54237149
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
??????
Department of Sociology
Fudan University


From ronggui.huang at gmail.com  Mon Jul  3 06:33:25 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Mon, 3 Jul 2006 12:33:25 +0800
Subject: [R] how to recode in my dataset?
In-Reply-To: <2fc17e30607020533m4449e647o273dbe1090b90f56@mail.gmail.com>
References: <2fc17e30607020533m4449e647o273dbe1090b90f56@mail.gmail.com>
Message-ID: <38b9f0350607022133m5760d3c1l72e22cea7ab9a842@mail.gmail.com>

I always use "recode" function (in the car packages) to recode
variables.That works well and I like that function.

2006/7/2, zhijie zhang <epistat at gmail.com>:
> Dear Rusers,
>  My question is about "recode variables". First, i'd like to say
> something about the idea of recoding:
>  My dataset have three variables:type,soiltem and airtem,which means
> grass type, soil temperature and air temperature. As we all known, the
> change of air temperature is greater than soil temperature,so the
> values in those two different temperaturemay represent different
> range.
>  My recoding is to recode soiltem with 0.2 intervals, and airtem with
> 0.5 intervals, that is:
> In soiltem:0~0.2<-0.1,  0.2~0.4<-0.3, 0.4`0.6<-0.5,...etc;
> In airtem:0~0.5<-0.25,  0.5~1<-0.75, 1`1.5<-1.25,...etc;
> My example like this:
> type<-c(1, 1, 2, 3,4,1,1,4,3,2)
> soiltem<-c(19.2,18.6,19.5,19.8,19.6,20.6,19.1,18.7,22.4,21.6)
> airtem<-c(19.9,20.5,21.6,25.6,22.6,21.3,23.7,21.5,24.7,24.4)
> mydata<-data.frame(type,soiltem,airtem) #copy the above four arguments
> to generate the dataset
>
> mydata
>    type soiltem airtem
> 1     1    19.2   19.9
> 2     1    18.6   20.5
> 3     2    19.5   21.6
> 4     3    19.8   25.6
> 5     4    19.6   22.6
> 6     1    20.6   21.3
> 7     1    19.1   23.7
> 8     4    18.7   21.5
> 9     3    22.4   24.7
> 10    2    21.6   24.4
>
> Thanks very much!
> --
> Kind Regards,
> Zhi Jie,Zhang ,PHD
> Department of Epidemiology
> School of Public Health
> Fudan University
> Tel:86-21-54237149
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
??????
Department of Sociology
Fudan University


From ronggui.huang at gmail.com  Mon Jul  3 06:59:27 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Mon, 3 Jul 2006 12:59:27 +0800
Subject: [R] Data Manipulations - Group By equivalent
In-Reply-To: <1115a2b00607011939v26a29f90saf450431b9b2d008@mail.gmail.com>
References: <446D1C5F.9060002@bellsouth.net> <4498072C.5010600@bellsouth.net>
	<449DA9F1.90700@bellsouth.net> <44A70DB2.9080106@bellsouth.net>
	<1115a2b00607011939v26a29f90saf450431b9b2d008@mail.gmail.com>
Message-ID: <38b9f0350607022159o1357f4f1nd75c5781c023828f@mail.gmail.com>

use doBy package will be more easy.

# GENERATE A TREATMENT GROUP #
group<-as.factor(paste("treatment", rep(1:2, 4), sep = '_'));
# CREATE A SERIES OF RANDOM VALUES #
x<-rnorm(length(group));
# CREATE A DATA FRAME TO COMBINE THE ABOVE TWO #
data<-data.frame(group, x);
library(doBy)
summ2<-summaryBy(x~group,data=data,FUN=c(mean,sum),na.rm=T,prefix=c("mean","sum"))
combine2<-merge(data,summ)

Ronggui


2006/7/2, Wensui Liu <liuwensui at gmail.com>:
> Zubin,
>
> I bet you are working for intercontinental hotels and think you probably are
> not the real Zubin there. right? ^_^. If you have chance, could you please
> say hi to him for me?
>
> Here is a piece of R code I copy from my blog side by side with SAS. You
> might need to tweak it a little to get what you need.
>
>  CALCULATE GROUP SUMMARY IN R
> ##################################################
> # HOW TO CALCULATE GROUP SUMMARY IN R #
> # DATE : DEC-13, 2005 #
> ##################################################
> # EQUIVALENT SAS CODE: #
> # #
> # DATA DATA; #
> # DO I = 1 TO 2; #
> # DO J = 1 TO 4; #
> # GROUP = 'TREATMENT_'||PUT(I, 1.); #
> # X = RANNOR(1); #
> # OUTPUT; #
> # END; #
> # END; #
> # KEEP GROUP X; #
> # RUN; #
> # #
> # PROC SQL; #
> # CREATE TABLE COMBINE AS #
> # SELECT *, MEAN(X) AS MEAN_X, SUM(X) AS SUM_X #
> # FROM DATA #
> # GROUP BY GROUP; #
> # QUIT; #
> ##################################################
>
>
> # GENERATE A TREATMENT GROUP #
> group<-as.factor(paste("treatment", rep(1:2, 4), sep = '_'));
>
> # CREATE A SERIES OF RANDOM VALUES #
> x<-rnorm(length(group));
>
> # CREATE A DATA FRAME TO COMBINE THE ABOVE TWO #
> data<-data.frame(group, x);
>
> # CALCULATE SUMMARY FOR X #
> x.mean<-tapply(data$x, data$group, mean, na.rm = T);
> x.sum<-tapply(data$x, data$group, sum, na.rm = T);
>
> # CREATE A DATA FRAME TO COMBINE SUMMARIES #
> summ<-data.frame(x.mean, x.sum, group = names(x.mean));
>
> # COMBINE DATA AND SUMMARIES TOGETHER #
> combine<-merge(data, summ, by = "group");
>
>
> On 7/1/06, zubin <binabina at bellsouth.net> wrote:
> >
> > Hello, a beginner R user - boy i wish there was a book on just data
> > manipulations for SAS users learning R (equivalent to the SAS DATA
> > STEP)..  Okay, my question:
> >
> > I have a panel data set, hotel data occupancy by month for 12 months,
> > 1000 hotels.  I have a field labeled 'year' and want to consolidate the
> > monthly records using an average into 1000 occupancy numbers - just a
> > simple average of the 12 months by hotel.  In SQL this operation is
> > pretty easy, a group by query (group by hotel where year = 2005, avg
> > occupancy) - how is this done in R? (in R language not SQL).  Thx!
> >
> > -zubin
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
>
>
> --
> WenSui Liu
> (http://spaces.msn.com/statcompute/blog)
> Senior Decision Support Analyst
> Health Policy and Clinical Effectiveness
> Cincinnati Children Hospital Medical Center
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
??????
Department of Sociology
Fudan University


From priti.desai at kalyptorisk.com  Mon Jul  3 07:26:45 2006
From: priti.desai at kalyptorisk.com (priti desai)
Date: Mon, 3 Jul 2006 10:56:45 +0530
Subject: [R] Query :   Chi Square goodness of fit test
Message-ID: <2AB7346A3227A74BB97F9A0D79E3E65A01BFE1@mailserver.kalyptorisk.com>

If we have the data base of frauds given below


 no. of  frauds = variable

variable
<-c(4,1,6,9,9,10,2,4,8,2,3,0,1,2,3,1,3,4,5,4,4,4,9,5,4,3,11,8,12,3,10,0,
7)

pmf  <- dpois(i, lambda, log = FALSE)  # prob. mass function of variable

How to apply chi-square goodness of fit to test, Sample coming from
Poisson distribution.
How to calculate observed frequencies & expected frequencies, after that
how to calculate chi 2 test and interpret the result  



The formula which I have used & answer which I am getting is as follows,


chisq.test(variable, p=pmf, simulate.p.value =FALSE, correct = FALSE)



        Chi-squared test for given probabilities

data:  No_of_Frouds 
X-squared = 1.043111e+15, df = 32, p-value < 2.2e-16

Warning message:
Chi-squared approximation may be incorrect in: chisq.test(No_of_Frouds,
p = pmf, simulate.p.value = FALSE, correct = FALSE) 
 


But the answer is not correct. 

Please suggest me the correct variable, calculations & formula in R.

 Awaiting your positive reply.
 
  Regards,
  Priti.


From msleonidl at mscc.huji.ac.il  Mon Jul  3 08:51:41 2006
From: msleonidl at mscc.huji.ac.il (Landsman Leonid)
Date: Mon, 3 Jul 2006 08:51:41 +0200
Subject: [R] Problem with try()
Message-ID: <007701c69e6d$242b7710$cc00020a@Home3>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060703/53628de5/attachment.pl 

From Johan.VanKerckhoven at econ.kuleuven.be  Mon Jul  3 08:52:44 2006
From: Johan.VanKerckhoven at econ.kuleuven.be (Van Kerckhoven, Johan)
Date: Mon, 3 Jul 2006 08:52:44 +0200
Subject: [R] Questions concerning function 'svm' in e1071 package
Message-ID: <F9E424F22569F3469181E1BA1EE24CED82F3F7@ECONSRVEX5.econ.kuleuven.ac.be>

Greetings everyone,

I have the following problem (illustrating R-code at bottom of mail):
Given a training sample with binary outcomes (-1/+1), I train a linear
Support Vector Machine to separate them. Afterwards, I compute the
weight vector w in the usual way, and obtain the fitted values as
w'x + b > 0  ==>  yfitted = 1, otherwise -1.

However, upon verifying with the 'predict' method, the outcomes do not
match up as they should. I've already tried to find information
concerning this issue on the R-help board, but to no avail. Can any of
you point me in the right direction?

Signed,

Johan Van Kerckhoven
ORSTAT and University Center of Statistics
Katholieke Universiteit Leuven

----------------------------------------------------------------------

#initialization of the problem

rm(list=ls())

library(e1071)

set.seed(2)

n = 50
d = 4
p = 0.5

x = matrix(rnorm(n*d), ncol=d)

mushift = c(1, -1, rep(0, d-2))

y = runif(n) > p
y = factor(2*y - 1)

x = x - outer(rep(1, n), mushift)
x[y == 1, ] = x[y == 1] + 2*outer(rep(1, sum(y == 1)), mushift)

svclass = svm(x, y, scale=FALSE, kernel="linear")

#Computation of the weight vector

w = t(svclass$coefs) %*% svclass$SV
if (y[1] == -1) {
   w = -w
}

#Derivation of predicted class lavels

#Using method in documentation
yfit = (x %*% t(w) + svclass$rho) > 0
yfit = factor(2*yfit - 1)

#Extracting them directly from the model
yfit2 = svclass$fitted

#Display where predictions differ from each other
yfit != yfit2

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From h.wickham at gmail.com  Mon Jul  3 09:25:20 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 3 Jul 2006 09:25:20 +0200
Subject: [R] [R-pkgs] ggplot: a new system for drawing graphics in R
Message-ID: <f8e6ff050607030025k77d4075bv7faf5b68d54b56b3@mail.gmail.com>

ggplot provides a new system for drawing graphics in R, based on the
Grammar of Graphics. It combines the advantages of both base and
lattice graphics: conditioning and shared axes are handled
automatically, and you can still build up a plot step by step from
multiple data sources. It also implements a more sophisticated
multidimensional conditioning system and a consistent interface to map
data to visual attributes.  ggplot (along with reshape) received the
John Chambers Award for Statistical Computing.

ggplot is available now from CRAN (install.packages("ggplot")) and
more information is available at my website (http://had.co.nz/ggplot)
including copies of talks, examples, and a guide showing how to
convert your existing lattice code.

To get started I recommend you look at:

 * the introductory vignette: vignette("introduction", "ggplot")
 * help for the quick plotting command: ?qplot
 * help for the full plotting commands: ?ggplot

I want to provide great documentation, so if there is anything you
think I am missing, please let me know.

Regards,

Hadley

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From epistat at gmail.com  Mon Jul  3 09:37:55 2006
From: epistat at gmail.com (zhijie zhang)
Date: Mon, 3 Jul 2006 15:37:55 +0800
Subject: [R] could i change the ouput style on summary?
Message-ID: <2fc17e30607030037o33244010ob14187c92fbdda1e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060703/7241ce3e/attachment.pl 

From maechler at stat.math.ethz.ch  Mon Jul  3 09:44:38 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 3 Jul 2006 09:44:38 +0200
Subject: [R] sparse matrix tools
In-Reply-To: <0C6BF3FC506F664F90C8BA3E0160462D04A75840@EXCHANGE3.ad.uams.edu>
References: <mailman.7.1151834402.22992.r-help@stat.math.ethz.ch>
	<0C6BF3FC506F664F90C8BA3E0160462D04A75840@EXCHANGE3.ad.uams.edu>
Message-ID: <17576.51942.906278.393541@stat.math.ethz.ch>

>>>>> "JJTh" == Thaden, John J <ThadenJohnJ at uams.edu>
>>>>>     on Sun, 2 Jul 2006 07:38:21 -0500 writes:

    JJTh> Dear R-Help list: I'm using the Matrix library to
    JJTh> operate on 600 X ~5000 element unsymmetrical sparse
    JJTh> arrays. So far, so good, but if I find I need more
    JJTh> speed or functionality, 

Can you be more specific?  
Functionality:
   For asymmetric matrices, in our view, the most important gap to
   fill is the  LU  decomposition and hence solve() features.

Speed: Are you sure the time your R code spends is spent in 
       functions from "Matrix"?  {Did you use 'Rprof()' ?}
       If yes, which ones?


    JJTh> how hard would it be to
    JJTh> utilize other sparse matrix toolsets from within R,
    JJTh> say MUMPS, PARDISO or UMFPACK, that do not have
    JJTh> explicit R interfaces?  More information on these is
    JJTh> available here

    JJTh> www.cise.ufl.edu/research/sparse/umfpack/
    JJTh> www.computational.unibas.ch/cs/scicomp/software/pardiso
    JJTh> www.enseeiht.fr/lima/apo/MUMPS/ 

>From these, only the first one is open source.
Unfortunately, the PARDISO people seem to believe in money
making with scientific software -- a particular shame for since
they only work at most an hour away from me.
MUMPS is said to be "public domain", but then you only get it
after filling out a web form and only for a specific hardware.  Also,
it is about massively parallel computation, very interesting for
sparse matrices, but AFAIK not yet in our main focus.

UMFPACK is different, and even ships with the Matrix
package, because we have planned to interface to it, but haven't
yet got to that.  What parts of UMFPACK functionality would you
be interested in ?

    JJTh> and in these reviews

    JJTh> ftp://ftp.numerical.rl.ac.uk/pub/reports/ghsNAGIR20051r1.pdf
    JJTh> http://www.cise.ufl.edu/research/sparse/codes/ 

    JJTh> neither of which reviewed the R Matrix package, unfortunately.

    JJTh> Thanks, - John Thaden, Ph.D., U. Arkansas for
    JJTh> Med. Sci., Little Rock.


Regards,
Martin Maechler,  ETH Zurich


From chrysopa at gmail.com  Sun Jul  2 23:33:23 2006
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Sun, 2 Jul 2006 18:33:23 -0300
Subject: [R] Calculation of lags
Message-ID: <200607021833.23866.chrysopa@gmail.com>

Hi,

If I have the follow situation:

A dependent variable (i.e. number of insects) that is affected by an 
independent variable (i.e. rain). The problem is that the measure of rain 
affect the population in other moment. So there exit a lag between the rain 
and the number of insects. Exist in R any tool to find what is this lag? 

Explain better.

Suppose that I have a linear relationship between rain and insects. More rain 
make more insects. If I try to model the rain and insects in the same moment 
I cant see this relationship because the rain today affect the number of 
insects in the future. Thus, I need to model the present rain with the number 
of insect in the future. But when is this future? 1 day after, 2 days after, 
etc.

How the best way to calculate this lag?

Thanks
Ronaldo
-- 
	Todos chegamos um dia como a agua e nos vamos como o vento
		-- Graham Greene
--
> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8190 | chrysopa em gmail.com
| ICQ#: 5692561 | LinuxUser#: 205366


From spencer.graves at pdf.com  Sat Jul  1 06:42:55 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 30 Jun 2006 21:42:55 -0700
Subject: [R] lme convergence
In-Reply-To: <009901c69c13$54616ae0$0540210a@www.domain>
References: <002001c69ad3$38073050$7c94100a@win.ad.jhu.edu>	<44A48790.9030002@pdf.com>
	<009901c69c13$54616ae0$0540210a@www.domain>
Message-ID: <44A5FD4F.7080607@pdf.com>

Hi, Dimitris:

	  The change you suggested sounds constructive.  Unfortunately, it did 
NOT solve the problem, at least for the modification of the example from 
the 'lme' help page I tested.

	  However, one other similar change (and adding 'nlme:::' to the calls 
for functions hidden in an nlme namespace, identified partly with 
'traceback()') produced a version of 'lme.formula' that actually 
returned an answer for my test case:

 > fm1 <- lme(distance ~ age, data = Orthodont,
+            control=lmeControl(msMaxIter=1,
+              returnObject=TRUE))
Warning message:
nlminb problem, convergence error code = 1 ; message = iteration limit 
reached without convergence (9) in: lme.formula(distance ~ age, data = 
Orthodont, control = lmeControl(msMaxIter = 1,

	  Below please find (1) the 2 changes and (2) a complete version of 
'lme.formula' that worked for this example.

	  Thanks for your contribution.
	  Spencer Graves
### change 1 ##################
     if (!needUpdate(lmeSt)) {
       if(optRes$convergence){
         msg <- paste(controlvals$opt,
             "problem, convergence error code =",
             optRes$convergence, "; message =",
                    optRes$message)
         {
           if(!controlvals$returnObject)
             stop(msg)
           else
             warning(msg)
         }
         break
       }
     }
### change 2 #################
     if (numIter > controlvals$maxIter) {
       msg <- paste("Maximum number of iterations",
          "(lmeControl(maxIter)) reached without convergence.")
       if (controlvals$returnObject) {
         warning(msg)
         break
       } else {
         stop(msg)
       }
     }
### lme.formula revised ###########
lme.formula <-
   function(fixed,
	   data = sys.frame(sys.parent()),
	   random = pdSymm( eval( as.call( fixed[ -2 ] ) ) ),
	   correlation = NULL,
	   weights = NULL,
	   subset,
	   method = c("REML", "ML"),
	   na.action = na.fail,
	   control = list(),
            contrasts = NULL,
            keep.data = TRUE)
{
   Call <- match.call()
   miss.data <- missing(data) || !is.data.frame(data)

   ## control parameters
   controlvals <- lmeControl()
   if (!missing(control)) {
     if(!is.null(control$nlmStepMax) && control$nlmStepMax < 0) {
       warning("Negative control$nlmStepMax - using default value")
       control$nlmStepMax <- NULL
     }
     controlvals[names(control)] <- control
   }

   ##
   ## checking arguments
   ##
   if (!inherits(fixed, "formula") || length(fixed) != 3) {
     stop("\nFixed-effects model must be a formula of the form \"resp ~ 
pred\"")
   }
   method <- match.arg(method)
   REML <- method == "REML"
   reSt <- reStruct(random, REML = REML, data = NULL)
   groups <- getGroupsFormula(reSt)
   if (is.null(groups)) {
     if (inherits(data, "groupedData")) {
       groups <- getGroupsFormula(data)
       namGrp <- rev(names(getGroupsFormula(data, asList = TRUE)))
       Q <- length(namGrp)
       if (length(reSt) != Q) { # may need to repeat reSt
	if (length(reSt) != 1) {
	  stop("Incompatible lengths for \"random\" and grouping factors")
	}
         randL <- vector("list", Q)
         names(randL) <- rev(namGrp)
         for(i in 1:Q) randL[[i]] <- random
         randL <- as.list(randL)
	reSt <- reStruct(randL, REML = REML, data = NULL)
       } else {
	names(reSt) <- namGrp
       }
     } else {
       ## will assume single group
       groups <- ~ 1
       names(reSt) <- "1"
     }
   }
   ## check if corStruct is present and assign groups to its formula,
   ## if necessary
   if (!is.null(correlation)) {
     if(!is.null(corGrpsForm <- getGroupsFormula(correlation, asList = 
TRUE))) {
       corGrpsForm <- unlist(lapply(corGrpsForm,
                                    function(el) deparse(el[[2]])))
       corQ <- length(corGrpsForm)
       lmeGrpsForm <- unlist(lapply(splitFormula(groups),
                         function(el) deparse(el[[2]])))
       lmeQ <- length(lmeGrpsForm)
       if (corQ <= lmeQ) {
         if (any(corGrpsForm != lmeGrpsForm[1:corQ])) {
           stop(paste("Incompatible formulas for groups in \"random\"",
                      "and \"correlation\""))
         }
         if (corQ < lmeQ) {
           warning(paste("Cannot use smaller level of grouping for",
                         "\"correlation\" than for \"random\". Replacing",
                         "the former with the latter."))
           attr(correlation, "formula") <-
             eval(parse(text = paste("~",
 
deparse(getCovariateFormula(formula(correlation))[[2]]),
                          "|", deparse(groups[[2]]))))
         }
       } else {
         if (any(lmeGrpsForm != corGrpsForm[1:lmeQ])) {
           stop(paste("Incompatible formulas for groups in \"random\"",
                      "and \"correlation\""))
         }
       }
     } else {
       ## using the same grouping as in random
       attr(correlation, "formula") <-
         eval(parse(text = paste("~",
		     deparse(getCovariateFormula(formula(correlation))[[2]]),
		     "|", deparse(groups[[2]]))))
       corQ <- lmeQ <- 1
     }
     } else {
     corQ <- lmeQ <- 1
   }
   ## create an lme structure containing the random effects model and 
plug-ins
   lmeSt <- lmeStruct(reStruct = reSt, corStruct = correlation,
		     varStruct = varFunc(weights))

   ## extract a data frame with enough information to evaluate
   ## fixed, groups, reStruct, corStruct, and varStruct
   mfArgs <- list(formula = asOneFormula(formula(lmeSt), fixed, groups),
		 data = data, na.action = na.action)
   if (!missing(subset)) {
     mfArgs[["subset"]] <- asOneSidedFormula(Call[["subset"]])[[2]]
   }
   mfArgs$drop.unused.levels <- TRUE
   dataMix <- do.call("model.frame", mfArgs)
   origOrder <- row.names(dataMix)	# preserve the original order
   for(i in names(contrasts))            # handle contrasts statement
       contrasts(dataMix[[i]]) = contrasts[[i]]
   ## sort the model.frame by groups and get the matrices and parameters
   ## used in the estimation procedures
   grps <- getGroups(dataMix, groups)
   ## ordering data by groups
   if (inherits(grps, "factor")) {	# single level
     ord <- order(grps)	#"order" treats a single named argument peculiarly
     grps <- data.frame(grps)
     row.names(grps) <- origOrder
     names(grps) <- as.character(deparse((groups[[2]])))
   } else {
     ord <- do.call("order", grps)
     ## making group levels unique
     for(i in 2:ncol(grps)) {
       grps[, i] <-
         as.factor(paste(as.character(grps[, i-1]), as.character(grps[,i]),
                         sep = "/"))
       NULL
     }
   }
   if (corQ > lmeQ) {
     ## may have to reorder by the correlation groups
     ord <- do.call("order", getGroups(dataMix,
                                  getGroupsFormula(correlation)))
   }
   grps <- grps[ord, , drop = FALSE]
   dataMix <- dataMix[ord, ,drop = FALSE]
   revOrder <- match(origOrder, row.names(dataMix)) # putting in orig. order

   ## obtaining basic model matrices
   N <- nrow(grps)
   Z <- model.matrix(reSt, dataMix)
   ncols <- attr(Z, "ncols")
   Names(lmeSt$reStruct) <- attr(Z, "nams")
   ## keeping the contrasts for later use in predict
   contr <- attr(Z, "contr")
   X <- model.frame(fixed, dataMix)
   Terms <- attr(X, "terms")
   auxContr <- lapply(X, function(el)
		     if (inherits(el, "factor") &&
                          length(levels(el)) > 1) contrasts(el))
   contr <- c(contr, auxContr[is.na(match(names(auxContr), names(contr)))])
   contr <- contr[!unlist(lapply(contr, is.null))]
   X <- model.matrix(fixed, data=X)
   y <- eval(fixed[[2]], dataMix)
   ncols <- c(ncols, dim(X)[2], 1)
   Q <- ncol(grps)
   ## creating the condensed linear model
   attr(lmeSt, "conLin") <-
     list(Xy = array(c(Z, X, y), c(N, sum(ncols)),
	     list(row.names(dataMix), c(colnames(Z), colnames(X),
					deparse(fixed[[2]])))),
	 dims = nlme:::MEdims(grps, ncols), logLik = 0)
   ## checking if enough observations per group to estimate ranef
   tmpDims <- attr(lmeSt, "conLin")$dims
   if (max(tmpDims$ZXlen[[1]]) < tmpDims$qvec[1]) {
     warning(paste("Fewer observations than random effects in all level",
                   Q,"groups"))
   }
   ## degrees of freedom for testing fixed effects
   fixDF <- nlme:::getFixDF(X, grps, attr(lmeSt, "conLin")$dims$ngrps,
                     terms = Terms)
   ## initialization
   lmeSt <- Initialize(lmeSt, dataMix, grps, control = controlvals)
   parMap <- attr(lmeSt, "pmap")
   ## Checking possibility of single decomposition
   if (length(lmeSt) == 1)  {	# reStruct only, can do one decomposition
     ## need to save conLin for calculating fitted values and residuals
     oldConLin <- attr(lmeSt, "conLin")
     decomp <- TRUE
     attr(lmeSt, "conLin") <- nlme:::MEdecomp(attr(lmeSt, "conLin"))
   } else decomp <- FALSE
   ##
   ## getting the linear mixed effects fit object,
   ## possibly iterating for variance functions
   ##
   numIter <- 0
   repeat {
     oldPars <- coef(lmeSt)
     optRes <- if (controlvals$opt == "nlminb") {
         nlminb(c(coef(lmeSt)),
                function(lmePars) -logLik(lmeSt, lmePars),
                control = list(iter.max = controlvals$msMaxIter,
                trace = controlvals$msVerbose))
     } else {
         optim(c(coef(lmeSt)),
               function(lmePars) -logLik(lmeSt, lmePars),
               control = list(trace = controlvals$msVerbose,
               maxit = controlvals$msMaxIter,
               reltol = if(numIter == 0) controlvals$msTol
               else 100*.Machine$double.eps),
               method = controlvals$optimMethod)
     }
     numIter0 <- NULL
     coef(lmeSt) <- optRes$par
     attr(lmeSt, "lmeFit") <- nlme:::MEestimate(lmeSt, grps)
     ## checking if any updating is needed
     if (!needUpdate(lmeSt)) {
       if(optRes$convergence){
         msg <- paste(controlvals$opt,
             "problem, convergence error code =",
             optRes$convergence, "; message =",
                    optRes$message)
         {
           if(!controlvals$returnObject)
             stop(msg)
           else
             warning(msg)
         }
         break
       }
     }
     ## updating the fit information
     numIter <- numIter + 1
     lmeSt <- update(lmeSt, dataMix)
     ## calculating the convergence criterion
     aConv <- coef(lmeSt)
     conv <- abs((oldPars - aConv)/ifelse(aConv == 0, 1, aConv))
     aConv <- NULL
     for(i in names(lmeSt)) {
       if (any(parMap[,i])) {
	aConv <- c(aConv, max(conv[parMap[,i]]))
	names(aConv)[length(aConv)] <- i
       }
     }
     if (max(aConv) <= controlvals$tolerance) {
       break
     }
     if (numIter > controlvals$maxIter) {
       msg <- paste("Maximum number of iterations",
          "(lmeControl(maxIter)) reached without convergence.")
       if (controlvals$returnObject) {
         warning(msg)
         break
       } else {
         stop(msg)
       }
     }
   }

   ## wrapping up
   lmeFit <- attr(lmeSt, "lmeFit")
   names(lmeFit$beta) <- namBeta <- colnames(X)
   attr(fixDF, "varFixFact") <- varFix <- lmeFit$sigma * lmeFit$varFix
   varFix <- crossprod(varFix)
   dimnames(varFix) <- list(namBeta, namBeta)
   ##
   ## fitted.values and residuals (in original order)
   ##
   Fitted <- fitted(lmeSt, level = 0:Q,
		   conLin = if (decomp) {
		     oldConLin
		   } else {
		     attr(lmeSt, "conLin")
		   })[revOrder, , drop = FALSE]
   Resid <- y[revOrder] - Fitted
   attr(Resid, "std") <- lmeFit$sigma/(varWeights(lmeSt)[revOrder])
   ## putting groups back in original order
   grps <- grps[revOrder, , drop = FALSE]
   ## making random effects estimates consistently ordered
#  for(i in names(lmeSt$reStruct)) {
#    lmeFit$b[[i]] <- lmeFit$b[[i]][unique(as.character(grps[, i])),, 
drop = F]
#    NULL
#  }
   ## inverting back reStruct
   lmeSt$reStruct <- solve(lmeSt$reStruct)
   ## saving part of dims
   dims <- attr(lmeSt, "conLin")$dims[c("N", "Q", "qvec", "ngrps", "ncol")]
   ## getting the approximate var-cov of the parameters
   if (controlvals$apVar) {
     apVar <- nlme:::lmeApVar(lmeSt, lmeFit$sigma,
		      .relStep = controlvals[[".relStep"]],
                       minAbsPar = controlvals[["minAbsParApVar"]],
		      natural = controlvals[["natural"]])
   } else {
     apVar <- "Approximate variance-covariance matrix not available"
   }
   ## getting rid of condensed linear model and fit
   attr(lmeSt, "conLin") <- NULL
   attr(lmeSt, "lmeFit") <- NULL
   ##
   ## creating the  lme object
   ##
   estOut <- list(modelStruct = lmeSt,
		 dims = dims,
		 contrasts = contr,
		 coefficients = list(
		     fixed = lmeFit$beta,
		     random = lmeFit$b),
		 varFix = varFix,
		 sigma = lmeFit$sigma,
		 apVar = apVar,
		 logLik = lmeFit$logLik,
		 numIter = if (needUpdate(lmeSt)) numIter
		   else numIter0,
		 groups = grps,
		 call = Call,
                  terms = Terms,
		 method = method,
		 fitted = Fitted,
		 residuals = Resid,
                  fixDF = fixDF)
   if (keep.data && !miss.data) estOut$data <- data
   if (inherits(data, "groupedData")) {
     ## saving labels and units for plots
     attr(estOut, "units") <- attr(data, "units")
     attr(estOut, "labels") <- attr(data, "labels")
   }
   class(estOut) <- "lme"
   estOut
}

###################################
Dimitris Rizopoulos wrote:
> I think the following part of lme.formula
> 
> if (numIter > controlvals$maxIter) {
>     stop("Maximum number of iterations reached without convergence.")
> }
> 
> 
> should be something like
> 
> 
> if (numIter > controlvals$maxIter) {
>     if (controlvals$returnObject) {
>         warning("Maximum number of iterations reached without 
> convergence.")
>         break
>     } else {
>         stop("Maximum number of iterations reached without 
> convergence.")
>     }
> }
> 
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- 
> From: "Spencer Graves" <spencer.graves at pdf.com>
> To: "Ravi Varadhan" <rvaradhan at jhmi.edu>
> Cc: "'Pryseley Assam'" <assampryseley at yahoo.com>; "'R-Users'" 
> <R-help at stat.math.ethz.ch>; "Douglas Bates" <bates at stat.wisc.edu>
> Sent: Friday, June 30, 2006 4:08 AM
> Subject: Re: [R] lme convergence
> 
> 
>>   Does anyone know how to obtain the 'returnObject' from an 'lme' 
>> run
>> that fails to converge?  An argument of this name is described on 
>> the
>> 'lmeControl' help page as, "a logical value indicating whether the
>> fitted object should be returned when the maximum number of 
>> iterations
>> is reached without convergence of the algorithm. Default is 
>> 'FALSE'."
>>
>>   Unfortunately, I've so far been unable to get it to work, as
>> witnessed by the following modification of an example from the 
>> '?lme'
>> help page:
>>
>>> library(nlme)
>>> fm1 <- lme(distance ~ age, data = Orthodont,
>> +            control=lmeControl(msMaxIter=1))
>> Error in lme.formula(distance ~ age, data = Orthodont, control =
>> lmeControl(msMaxIter = 1)) :
>> iteration limit reached without convergence (9)
>>> fm1
>> Error: object "fm1" not found
>>> fm1 <- lme(distance ~ age, data = Orthodont,
>> +            control=lmeControl(msMaxIter=1,
>> +              returnObject=TRUE))
>> Error in lme.formula(distance ~ age, data = Orthodont, control =
>> lmeControl(msMaxIter = 1,  :
>> iteration limit reached without convergence (9)
>>> fm1
>> Error: object "fm1" not found
>>
>>   I might be able to fix the problem myself, working through the 
>> 'lme'
>> code line by line, e.g., using 'debug'.  However, I'm not ready to 
>> do
>> that just now.
>>
>>   Best Wishes,
>>   Spencer Graves
>>
>> Ravi Varadhan wrote:
>>> Use "try" to capture error messages without breaking the loop.
>>> ?try
>>>
>>> --------------------------------------------------------------------------
>>> Ravi Varadhan, Ph.D.
>>> Assistant Professor,  The Center on Aging and Health
>>> Division of Geriatric Medicine and Gerontology
>>> Johns Hopkins University
>>> Ph: (410) 502-2619
>>> Fax: (410) 614-9625
>>> Email:  rvaradhan at jhmi.edu
>>> Webpage: 
>>> http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
>>> --------------------------------------------------------------------------
>>>
>>>> -----Original Message-----
>>>> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
>>>> bounces at stat.math.ethz.ch] On Behalf Of Pryseley Assam
>>>> Sent: Wednesday, June 28, 2006 12:18 PM
>>>> To: R-Users
>>>> Subject: [R] lme convergence
>>>>
>>>> Dear R-Users,
>>>>
>>>>   Is it possible to get the covariance matrix from an lme model 
>>>> that did
>>>> not converge ?
>>>>
>>>>   I am doing a simulation which entails fitting linear mixed 
>>>> models, using
>>>> a "for loop".
>>>>   Within each loop, i generate a new data set and analyze it using 
>>>> a mixed
>>>> model.  The loop stops When the "lme function" does not converge 
>>>> for a
>>>> simulated dataset. I want to inquire if there is a method to 
>>>> suppress the
>>>> error message from the lme function, or better still, a way of 
>>>> going about
>>>> this issue of the loop ending once the lme function does not 
>>>> converge.
>>>>
>>>>   Thanks in advance,
>>>>   Pryseley
>>>>
>>>>
>>>> ---------------------------------
>>>>
>>>>
>>>> [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide! 
>>>> http://www.R-project.org/posting-
>>>> guide.html
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From Anupam.Tyagi at rediffmail.com  Mon Jul  3 09:53:46 2006
From: Anupam.Tyagi at rediffmail.com (Anupam Tyagi)
Date: Mon, 3 Jul 2006 07:53:46 +0000 (UTC)
Subject: [R] large dataset!
References: <200607021412.k62ECPDT008467@papaya.cc.columbia.edu>
Message-ID: <loom.20060703T095308-145@post.gmane.org>

JENNIFER HILL <jh1030 <at> columbia.edu> writes:

> 
> 
> Hi, I need to analyze data that has 3.5 million observations and
> about 60 variables and I was planning on using R to do this but
> I can't even seem to read in the data.  It just freezes and ties
> up the whole system -- and this is on a Linux box purchased about
> 6 months ago on a dual-processor PC that was pretty much the top
> of the line.  I've tried expanding R the memory limits but it 
> doesn't help.  I'll be hugely disappointed if I can't use R b/c
> I need to do build tailor-made models (multilevel and other 
> complexities).   My fall-back is the SPlus big data package but
> I'd rather avoid if anyone can provide a solution....
> 
> Thanks!!!!
> 
> Jennifer Hill
> 
Dear Jennifer, you may want to look at the R newsletters. A few years ago it had
an article on using DBMS with R, like MySQL, Oracle, etc. This is a frequently
asked question: There are also some posts over the past few years that may be
helpful. I have successfully read large database into MySQL, and accessed it
from R---it was larger than your database. I hope that helps. Anupam Tyagi.


From A.Robinson at ms.unimelb.edu.au  Sun Jul  2 23:24:29 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Mon, 3 Jul 2006 07:24:29 +1000
Subject: [R] [R-pkgs] Rassist - Student-friendly package
Message-ID: <20060702212429.GC758@ms.unimelb.edu.au>

The Rassist package has been loaded to CRAN.  This package is designed
to make R easier for new users, by providing extra checks and
feedback. 

Presently the package functionality includes:

* offers an alternative help facility, eg(.), with examples first,
  with additional examples included.  eg() offers a start help menu,
  and eg(.) incorporates help.search(.) automatically.  It also
  locates the relevant package if not loaded.

* Some mathematical functions validate input received.  They give
  feedback on units used, and when in error, prompt the user to
  correct the input.

* plot(.) offers feedback and cautions about input format.  

* deskcheck incorporates debug(), in straight forward testing.

* source(.), under development, in error produces a directory listing.

We believe that much remains to be done, but we wanted to solicit
community feedback from interested parties. Please pass on suggestions
for other functions, or better ways to achieve the same goals, to me.

Cheers

Andrew

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From TobiasBr at Taquanta.com  Mon Jul  3 10:28:58 2006
From: TobiasBr at Taquanta.com (Brandt, T. (Tobias))
Date: Mon, 3 Jul 2006 10:28:58 +0200 
Subject: [R] rownames, colnames, and date and time
Message-ID: <A77412E534FCD248A93A81F37CC75B7A033F8D8B@waxbill.africa.nedcor.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060703/94d4649b/attachment.pl 

From Matthias.Kohl at stamats.de  Mon Jul  3 08:06:37 2006
From: Matthias.Kohl at stamats.de (Matthias Kohl)
Date: Mon, 3 Jul 2006 08:06:37 +0200 (MEST)
Subject: [R] median of gamma distribution
Message-ID: <200607030606.k6366bcm029725@post.webmailer.de>

Hi,

to compute the median (or expectation, var, sd, IQR, mad, ...) you can also use package "distrEx".
library(distrEx)
(G <- Gammad())
median(G)

Matthias


----- original Nachricht --------

Betreff: Re: [R] median of gamma distribution
Gesendet: Fri, 30. Jun 2006
Von: Ted.Harding at nessie.mcc.ac.uk

> On 30-Jun-06 Philip He wrote:
> > Doese anyone know a R function to find the median of a gamma
> > distribution?
> 
> qgamma will do it. Test:
> 
> > -log(0.5)
> [1] 0.6931472
> > qgamma(0.5,1)
> [1] 0.6931472
> 
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 30-Jun-06                                       Time: 16:53:16
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 

--- original Nachricht Ende ----


--
Dr. rer. nat. Matthias Kohl
Matthias.Kohl at stamats.de
www.stamats.de


From jholtman at gmail.com  Mon Jul  3 13:12:23 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 3 Jul 2006 07:12:23 -0400
Subject: [R] could i change the ouput style on summary?
In-Reply-To: <2fc17e30607030037o33244010ob14187c92fbdda1e@mail.gmail.com>
References: <2fc17e30607030037o33244010ob14187c92fbdda1e@mail.gmail.com>
Message-ID: <644e1f320607030412j1198c000yadc1877225dea810@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060703/2264f068/attachment.pl 

From M.P.M.Boks at umcutrecht.nl  Mon Jul  3 13:17:24 2006
From: M.P.M.Boks at umcutrecht.nl (Boks, M.P.M.)
Date: Mon, 3 Jul 2006 13:17:24 +0200
Subject: [R] help a newbie with a loop
Message-ID: <2AD792FE1A79F046B908C364C29622F201250D80@EX05.ds.umcutrecht.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060703/13d6bf54/attachment.pl 

From statwi01 at staff.hu-berlin.de  Mon Jul  3 13:21:02 2006
From: statwi01 at staff.hu-berlin.de (statwi01)
Date: Mon, 03 Jul 2006 13:21:02 +0200
Subject: [R] Determine file access modifiers at file creation
Message-ID: <44A8FD9E.8020403@staff.hu-berlin.de>

Hi,

is in R a command or option to a command which allows to set the file 
access modifiers for newly created files under Linux ? Currently a new 
file (e.g. with cat) is created with "rw-r--r--" and I would like to 
have "rw-rw-r". And I do not want to use umask.

Thanks in advance

  Sigbert


From jsorkin at grecc.umaryland.edu  Mon Jul  3 13:39:20 2006
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Mon, 03 Jul 2006 07:39:20 -0400
Subject: [R] macro facility in R
Message-ID: <s4a8c9c7.085@MEDICINE.umaryland.edu>

R 2.2 on windows XP
I have a dataset with multiple columns. Some of the columns represent
independent variables, some represent dependent variables. I would like
to run the same analyses on a fixed set of independent variables,
changing only the dependent variable, e.g.
y1-y2=x1+x2+x3
y3-y4=x1+x2+x3
y5-y6=x1+x2+x3, etc.
I know I can write a function to perform the analyses, however in order
to make the analyses easy to do, I really need a macro scripting
language that will allow preprocessing of the function and substitution
of macro variables with parameters passed to the macro, i.e. I need
someting akin to the macro facility in SAS. Is there a macro facility
that I can use with R? I have tried help.search("macro") and did not
have any success.
Thanks,
John  

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC,
University of Maryland School of Medicine Claude D. Pepper OAIC,
University of Maryland Clinical Nutrition Research Unit, and
Baltimore VA Center Stroke of Excellence

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119
jsorkin at grecc.umaryland.edu


From florian.koller at gfk.com  Mon Jul  3 13:40:33 2006
From: florian.koller at gfk.com (florian.koller at gfk.com)
Date: Mon, 3 Jul 2006 13:40:33 +0200
Subject: [R] legend with filled boxes AND lines
Message-ID: <OFC8EAFB28.9603DB51-ONC12571A0.00368D2D-C12571A0.0040214F@gfk.de>


Dear all,

Is there a straightforward way to create a legend box that has both filled
boxes and lines?
So far I have built around this problem by creating two legends (with bty =
"n") and manually drawing a box around both (but this is cumbersome,
because I have to check upon the y coordinates of the legends every time).

If I do something like > legend( ...,c("X1","X2", "mean"), fill = c("red",
"blue", 0), lty = (0,0,2)) < , I cannot get rid of the unfilled box or
change the color of the fill box border (from its default color "black"),
and I end up with two filled boxes and an empty, black-lined box plus the
line as a legend for the third argument "mean". This trick therefore only
works if I define "black" as the bg color for the complete legend box
(because it masks the empty  box from the fill argument). So, if there is a
command to modify the color of the fill box border line (not the legend box
border line), this would help me, too (still not ideal, though...).

Thanks,
Florian



______________________
Florian Koller
GfK Fernsehforschung GmbH
Research Consulting & Development
Nordwestring 101
D-90319 N?rnberg
Fon  +49 (0)911 395-3554
Fax  +49 (0)911 395-4130
www.gfk.de / www.gfk.com



_________________________

Diese E-Mail (ggf. nebst Anhang) enth?lt vertrauliche und/oder rechtlich
gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat sind, oder
diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte sofort den
Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren sowie die
unbefugte Weitergabe dieser Mail ist nicht gestattet.

This e-mail (and any attachment/s) contains confidential and/or privileged
information. If you are not the intended recipient (or have received this
e-mail in error) please notify the sender immediately and destroy this
e-mail. Any unauthorised copying, disclosure or distribution of the
material in this e-mail is strictly forbidden.


From ggrothendieck at gmail.com  Mon Jul  3 13:44:12 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 3 Jul 2006 07:44:12 -0400
Subject: [R] rownames, colnames, and date and time
In-Reply-To: <A77412E534FCD248A93A81F37CC75B7A033F8D8B@waxbill.africa.nedcor.net>
References: <A77412E534FCD248A93A81F37CC75B7A033F8D8B@waxbill.africa.nedcor.net>
Message-ID: <971536df0607030444p1606cbd3ob6a7390e5d9f1777@mail.gmail.com>

Note that if you are really trying to represent time series using
rownames as the time variable then you might consider using
the zoo or its packages (or ts class if they are regular) instead.

library(zoo)

mymat<-matrix(1:4,nrow=2,ncol=2)
mydates<-as.POSIXct(c("2001-1-24","2005-12-25"))
z <- zoo(mymat, mydates)
z


On 7/3/06, Brandt, T. (Tobias) <TobiasBr at taquanta.com> wrote:
> Hi all
>
> I was wondering whether there has ever been an update on the rownames and
> colnames behaviour as described by Eric below?
>
> I still get the same behaviour, exactly as described by Eric, on my WinXP
> installation of R-2.3.0.  I also posted a message to r-help on Friday but
> looking through the online archives it seems to have not made it to the
> list.  I would agree with Eric that a consistent use of the typecast would
> be a reasonable solution.
>
> Any comments?
>
> Tobias Brandt
> Quantitative Analyst
> Taquanta Asset Managers
> Nedbank Clock Tower
> Victoria & Alfred Waterfront, Cape Town 8001
> Tel : +27 (0) 21 416 6602
> Fax : +27 (0) 21 416 9945
> Email : TobiasBr at Taquanta.com <mailto:TobiasBr at Taquanta.com>
>
>
> >-----Original Message-----
> >From: r-help-bounces at stat.math.ethz.ch
> >[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Erich Neuwirth
> >Sent: 21 March 2006 01:31 PM
> >To: R-help at stat.math.ethz.ch
> >Cc: Christian Prinoth
> >Subject: [R] rownames, colnames, and date and time
> >
> >I noticed something surprising (in R 2.2.1 on WinXP) According
> >to the documentation, rownames and colnames are character vectors.
> >Assigning a vector of class POSIXct or POSIXlt as rownames or
> >colnames therefore is not strictly according to the rules.
> >In some cases, R performs a reasonable typecast, but in some
> >other cases where the same typecast also would be possible, it
> >does not.
> >
> >Assigning a vector of class POSIXct to the rownames or names
> >of a dataframe creates a reasonable string representation of
> >the dates (and possibly times).
> >Assigning such a vector to the rownames or colnames of a
> >matrix produces rownames or colnames consisting of the integer
> >representation of the date-time value.
> >Trying to assign a vector of class POSIXlt in all cases
> >(dataframes and matrices, rownames, colnames, names) produces an error.
> >
> >Demonstration code is given below.
> >
> >This is somewhat inconsistent.
> >Perhaps a reasonable solution could be that the typecast used
> >for POSIXct and dataframes is used in all the other cases also.
> >
> >Code:
> >
> >mymat<-matrix(1:4,nrow=2,ncol=2)
> >mydf<-data.frame(mymat)
> >mydates<-as.POSIXct(c("2001-1-24","2005-12-25"))
> >
> >rownames(mydf)<-mydates
> >names(mydf)<-mydates
> >rownames(mymat)<-mydates
> >colnames(mymat)<-mydates
> >
> >print(deparse(mydates))
> >print(deparse(rownames(mydf)))
> >print(deparse(names(mydf)))
> >print(deparse(rownames(mymat)))
> >print(deparse(colnames(mymat)))
> >
> >mydates1<-as.POSIXlt(mydates)
> >
> ># the following lines will not work and
> ># produce errors
> >
> >rownames(mydf)<-mydates1
> >names(mydf)<-mydates1
> >rownames(mymat)<-mydates1
> >colnames(mymat)<-mydates1
> >
> >
> >--
> >Erich Neuwirth
> >Institute for Scientific Computing and
> >Didactic Center for Computer Science
> >University of Vienna
> >phone: +43-1-4277-39464  fax: +43-1-4277-39459
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> >http://www.R-project.org/posting-guide.html
> >
>
> ********************
> Nedbank Limited Reg No 1951/000009/06. The following link displays the names of the Nedbank Board of Directors and Company Secretary. [ http://www.nedbank.co.za/terms/DirectorsNedbank.htm ]
> This email is confidential and is intended for the addressee only. The following link will take you to Nedbank's legal notice. [ http://www.nedbank.co.za/terms/EmailDisclaimer.htm ]
> ********************
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jholtman at gmail.com  Mon Jul  3 13:46:05 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 3 Jul 2006 07:46:05 -0400
Subject: [R] Determine file access modifiers at file creation
In-Reply-To: <44A8FD9E.8020403@staff.hu-berlin.de>
References: <44A8FD9E.8020403@staff.hu-berlin.de>
Message-ID: <644e1f320607030446u5fb942e2ldded84b357955f5c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060703/4755f860/attachment.pl 

From ggrothendieck at gmail.com  Mon Jul  3 13:52:23 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 3 Jul 2006 07:52:23 -0400
Subject: [R] Calculation of lags
In-Reply-To: <200607021833.23866.chrysopa@gmail.com>
References: <200607021833.23866.chrysopa@gmail.com>
Message-ID: <971536df0607030452k38a3c0ay89dadd01d9c4d0f2@mail.gmail.com>

Represent your series as a ts object and see ?lag
Also lag.plot may or may not be of use to  you.


On 7/2/06, Ronaldo Reis-Jr. <chrysopa at gmail.com> wrote:
> Hi,
>
> If I have the follow situation:
>
> A dependent variable (i.e. number of insects) that is affected by an
> independent variable (i.e. rain). The problem is that the measure of rain
> affect the population in other moment. So there exit a lag between the rain
> and the number of insects. Exist in R any tool to find what is this lag?
>
> Explain better.
>
> Suppose that I have a linear relationship between rain and insects. More rain
> make more insects. If I try to model the rain and insects in the same moment
> I cant see this relationship because the rain today affect the number of
> insects in the future. Thus, I need to model the present rain with the number
> of insect in the future. But when is this future? 1 day after, 2 days after,
> etc.
>
> How the best way to calculate this lag?
>
> Thanks
> Ronaldo
> --
>        Todos chegamos um dia como a agua e nos vamos como o vento
>                -- Graham Greene
> --
> > Prof. Ronaldo Reis J?nior
> |  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
> | : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
> | `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
> |   `- Fone: (38) 3229-8190 | chrysopa at gmail.com
> | ICQ#: 5692561 | LinuxUser#: 205366
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jholtman at gmail.com  Mon Jul  3 14:05:45 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 3 Jul 2006 08:05:45 -0400
Subject: [R] help a newbie with a loop
In-Reply-To: <2AD792FE1A79F046B908C364C29622F201250D80@EX05.ds.umcutrecht.nl>
References: <2AD792FE1A79F046B908C364C29622F201250D80@EX05.ds.umcutrecht.nl>
Message-ID: <644e1f320607030505o52d55d9byea2ff173d035b060@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060703/7eb81e2e/attachment.pl 

From statwi01 at staff.hu-berlin.de  Mon Jul  3 14:11:58 2006
From: statwi01 at staff.hu-berlin.de (statwi01)
Date: Mon, 03 Jul 2006 14:11:58 +0200
Subject: [R] Determine file access modifiers at file creation
In-Reply-To: <644e1f320607030446u5fb942e2ldded84b357955f5c@mail.gmail.com>
References: <44A8FD9E.8020403@staff.hu-berlin.de>
	<644e1f320607030446u5fb942e2ldded84b357955f5c@mail.gmail.com>
Message-ID: <44A9098E.6070300@staff.hu-berlin.de>

Hi,
> Try using 'system':
>
> system("chmod 664 filename")
>   
That's my plan, if I can not do it in R directly.

Thanks Sigbert


From p.dalgaard at biostat.ku.dk  Mon Jul  3 14:13:46 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Jul 2006 14:13:46 +0200
Subject: [R] macro facility in R
In-Reply-To: <s4a8c9c7.085@MEDICINE.umaryland.edu>
References: <s4a8c9c7.085@MEDICINE.umaryland.edu>
Message-ID: <x2mzbqga2t.fsf@viggo.kubism.ku.dk>

"John Sorkin" <jsorkin at grecc.umaryland.edu> writes:

> R 2.2 on windows XP

(that's a bit old...)

> I have a dataset with multiple columns. Some of the columns represent
> independent variables, some represent dependent variables. I would like
> to run the same analyses on a fixed set of independent variables,
> changing only the dependent variable, e.g.
> y1-y2=x1+x2+x3
> y3-y4=x1+x2+x3
> y5-y6=x1+x2+x3, etc.
> I know I can write a function to perform the analyses, however in order
> to make the analyses easy to do, I really need a macro scripting
> language that will allow preprocessing of the function and substitution
> of macro variables with parameters passed to the macro, i.e. I need
> someting akin to the macro facility in SAS. Is there a macro facility
> that I can use with R? I have tried help.search("macro") and did not
> have any success.
> Thanks,
> John  

Some people have indicated that they might want to try their hand and
write a macro facility for R, at some time in the future. (There are
some parts of the R internals that would benefit from such a facility
too).

Meanwhile, update() is your friend.

mdl <- lm(y1-y2 ~ x1+x2+x3,....)
summary(mdl)
summary(update(mdl, y3 - y4 ~ .))
 ...etc...


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From murdoch at stats.uwo.ca  Mon Jul  3 14:15:03 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 03 Jul 2006 08:15:03 -0400
Subject: [R] macro facility in R
In-Reply-To: <s4a8c9c7.085@MEDICINE.umaryland.edu>
References: <s4a8c9c7.085@MEDICINE.umaryland.edu>
Message-ID: <44A90A47.8030607@stats.uwo.ca>

On 7/3/2006 7:39 AM, John Sorkin wrote:
> R 2.2 on windows XP
> I have a dataset with multiple columns. Some of the columns represent
> independent variables, some represent dependent variables. I would like
> to run the same analyses on a fixed set of independent variables,
> changing only the dependent variable, e.g.
> y1-y2=x1+x2+x3
> y3-y4=x1+x2+x3
> y5-y6=x1+x2+x3, etc.
> I know I can write a function to perform the analyses, however in order
> to make the analyses easy to do, I really need a macro scripting
> language that will allow preprocessing of the function and substitution
> of macro variables with parameters passed to the macro, i.e. I need
> someting akin to the macro facility in SAS. Is there a macro facility
> that I can use with R? I have tried help.search("macro") and did not
> have any success.

I think the substitute function in R does most of what you would want 
from a macro facility.

For example,

 > analyze <- function(dependent, data) {
+   formula <- as.formula(substitute(dep ~ x1 + x2 + x3,
+                         list(dep=substitute(dependent))))
+   lm(formula=formula, data=data)
+ }
 >
 > dat <- data.frame(y = rnorm(10)+1:10, x1=rnorm(10)+1:10, 
x2=rnorm(10), x3=rnorm(10))
 >
 > analyze(y, dat)

Call:
lm(formula = formula, data = data)

Coefficients:
(Intercept)           x1           x2           x3
      1.1256       0.8248       0.1671      -0.1907


I used substitute twice:  the inner call gets the unevaluated expression 
that was passed as "dependent"; the outer one puts that in place of the 
"dep" variable.

Duncan Murdoch


From ccleland at optonline.net  Mon Jul  3 14:16:40 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 03 Jul 2006 08:16:40 -0400
Subject: [R] macro facility in R
In-Reply-To: <s4a8c9c7.085@MEDICINE.umaryland.edu>
References: <s4a8c9c7.085@MEDICINE.umaryland.edu>
Message-ID: <44A90AA8.9090007@optonline.net>

John Sorkin wrote:
> R 2.2 on windows XP
> I have a dataset with multiple columns. Some of the columns represent
> independent variables, some represent dependent variables. I would like
> to run the same analyses on a fixed set of independent variables,
> changing only the dependent variable, e.g.
> y1-y2=x1+x2+x3
> y3-y4=x1+x2+x3
> y5-y6=x1+x2+x3, etc.
> I know I can write a function to perform the analyses, however in order
> to make the analyses easy to do, I really need a macro scripting
> language that will allow preprocessing of the function and substitution
> of macro variables with parameters passed to the macro, i.e. I need
> someting akin to the macro facility in SAS. Is there a macro facility
> that I can use with R? I have tried help.search("macro") and did not
> have any success.

Also try RSiteSearch("macro", restrict="functions").  And have a look at 
Thomas Lumley's column in the following issue of Rnews:

http://cran.r-project.org/doc/Rnews/Rnews_2001-3.pdf

> Thanks,
> John  
> 
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC,
> University of Maryland School of Medicine Claude D. Pepper OAIC,
> University of Maryland Clinical Nutrition Research Unit, and
> Baltimore VA Center Stroke of Excellence
> 
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> 
> 410-605-7119
> jsorkin at grecc.umaryland.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ThadenJohnJ at uams.edu  Mon Jul  3 14:22:31 2006
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Mon, 3 Jul 2006 07:22:31 -0500
Subject: [R] sparse matrix tools
In-Reply-To: <17576.51942.906278.393541@stat.math.ethz.ch>
Message-ID: <0C6BF3FC506F664F90C8BA3E0160462D04A75848@EXCHANGE3.ad.uams.edu>

So far I've used only 'new("dgC.Matrix",...)' and
'new("dgT.Matrix",...)'!  I did not mean to malign
Matrix speed/functionality -- I've not tested these
yet -- nor do I quite know yet what function(s) I
needto perform on my matrices! My questions were  
hypothetical.  Thanks for the additional information
on packages I mentioned. As I become able to reply 
regarding my needs for (Matrix or UMFPACK) functions,
I shall reply offlist.  Thanks and best regards, 
-John Thaden (operating R v.2.3.0 within Windows XP)

> MM = Martin Maechler [maechler at stat.math.ethz.ch]  
> replied on Monday, July 03, 2006 2:45 AM:

>>>>> "JJTh" == Thaden, John J <ThadenJohnJ at uams.edu>
>>>>>     on Sun, 2 Jul 2006 07:38:21 -0500 writes:

    JJTh> Dear R-Help list: I'm using the Matrix library to
    JJTh> operate on 600 X ~5000 element unsymmetrical sparse
    JJTh> arrays. So far, so good, but if I find I need more
    JJTh> speed or functionality, 

MM> Can you be more specific?  
MM> Functionality:
MM>    For asymmetric matrices, in our view, the most 
MM>    important gap to fill is the LU  decomposition
MM>    and hence solve() features.
MM> 
MM> Speed: Are you sure the time your R code spends is
MM>    spent in functions from "Matrix"?  {Did you use
MM> 'Rprof()' ?}? If yes, which ones?

    JJTh> how hard would it be to
    JJTh> utilize other sparse matrix toolsets from within R,
    JJTh> say MUMPS, PARDISO or UMFPACK, that do not have
    JJTh> explicit R interfaces?  More information on these is
    JJTh> available here

    JJTh> www.cise.ufl.edu/research/sparse/umfpack/
    JJTh> www.computational.unibas.ch/cs/scicomp/software/pardiso
    JJTh> www.enseeiht.fr/lima/apo/MUMPS/ 

>From these, only the first one is open source.
Unfortunately, the PARDISO people seem to believe in money
making with scientific software -- a particular shame for since
they only work at most an hour away from me.
MUMPS is said to be "public domain", but then you only get it
after filling out a web form and only for a specific hardware.  Also,
it is about massively parallel computation, very interesting for
sparse matrices, but AFAIK not yet in our main focus.

UMFPACK is different, and even ships with the Matrix
package, because we have planned to interface to it, but haven't
yet got to that.  What parts of UMFPACK functionality would you
be interested in ?

    JJTh> and in these reviews

    JJTh> ftp://ftp.numerical.rl.ac.uk/pub/reports/ghsNAGIR20051r1.pdf
    JJTh> http://www.cise.ufl.edu/research/sparse/codes/ 

    JJTh> neither of which reviewed the R Matrix package, unfortunately.

    JJTh> Thanks, - John Thaden, Ph.D., U. Arkansas for
    JJTh> Med. Sci., Little Rock.


Regards,
Martin Maechler,  ETH Zurich

Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From statwi01 at staff.hu-berlin.de  Mon Jul  3 14:33:51 2006
From: statwi01 at staff.hu-berlin.de (statwi01)
Date: Mon, 03 Jul 2006 14:33:51 +0200
Subject: [R] Workspace size
In-Reply-To: <644e1f320607030446u5fb942e2ldded84b357955f5c@mail.gmail.com>
References: <44A8FD9E.8020403@staff.hu-berlin.de>
	<644e1f320607030446u5fb942e2ldded84b357955f5c@mail.gmail.com>
Message-ID: <44A90EAF.8070804@staff.hu-berlin.de>

Hi,

Can I determine the approximate size of a workspace on the harddisk 
before saving it via "sys.save.image(name)" ?

Thanks in advance

  Sigbert


From ggrothendieck at gmail.com  Mon Jul  3 15:01:07 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 3 Jul 2006 09:01:07 -0400
Subject: [R] macro facility in R
In-Reply-To: <s4a8c9c7.085@MEDICINE.umaryland.edu>
References: <s4a8c9c7.085@MEDICINE.umaryland.edu>
Message-ID: <971536df0607030601h7b97cbbcq5272d57755ba0ff2@mail.gmail.com>

Try this:

# test data
set.seed(1)
mat <- matrix(rnorm(900), nc = 9)
colnames(mat) <- letters[1:9]
DF <- as.data.frame(mat)

# run lm's and display coefs
for(i in seq(1, 5, 2)) {
	dat <- cbind(z = DF[,i] - DF[,i+1], DF[7:9])
	Coef <- coef(lm(z ~., dat))
	cat("y: DF[,", i, "] - DF[,", i+1, "], coef: ", Coef, "\n")
}




On 7/3/06, John Sorkin <jsorkin at grecc.umaryland.edu> wrote:
> R 2.2 on windows XP
> I have a dataset with multiple columns. Some of the columns represent
> independent variables, some represent dependent variables. I would like
> to run the same analyses on a fixed set of independent variables,
> changing only the dependent variable, e.g.
> y1-y2=x1+x2+x3
> y3-y4=x1+x2+x3
> y5-y6=x1+x2+x3, etc.
> I know I can write a function to perform the analyses, however in order
> to make the analyses easy to do, I really need a macro scripting
> language that will allow preprocessing of the function and substitution
> of macro variables with parameters passed to the macro, i.e. I need
> someting akin to the macro facility in SAS. Is there a macro facility
> that I can use with R? I have tried help.search("macro") and did not
> have any success.
> Thanks,
> John
>
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC,
> University of Maryland School of Medicine Claude D. Pepper OAIC,
> University of Maryland Clinical Nutrition Research Unit, and
> Baltimore VA Center Stroke of Excellence
>
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>
> 410-605-7119
> jsorkin at grecc.umaryland.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From sleepingwell at gmail.com  Mon Jul  3 15:10:55 2006
From: sleepingwell at gmail.com (Simon Knapp)
Date: Mon, 3 Jul 2006 23:10:55 +1000
Subject: [R]  help a newbie with a loop
In-Reply-To: <2AD792FE1A79F046B908C364C29622F201250D80@EX05.ds.umcutrecht.nl>
References: <2AD792FE1A79F046B908C364C29622F201250D80@EX05.ds.umcutrecht.nl>
Message-ID: <4f8ec3aa0607030610q78bb41cl3485fa769885f57c@mail.gmail.com>

On 7/3/06, Boks, M.P.M. <M.P.M.Boks at umcutrecht.nl> wrote:
>
>
> Hi,
>
> I am new in R and stumbled on a problem my (more experienced) friends
> can not help with with. Why isnt this code working?
>
> The function is working, also with the loop and the graph appears,
>
> only when I build another loop around it  (for different values of p) ,
> R stays in a loop?
>
> Can't it take more then 2 loops in one program?

You can have as many as you like (as far as I know)

>
>
> powerb<-function(x,sp2,a,b,b1,m)
> {   sx<-(sum(x^2)-(sum(x)^2)/length(x))/length(x)
>    n0<-ceiling((((qnorm(1-(a/2))+qnorm(1-b))/b1)^2)*(((m+1)/m)*sp2/sx))
>    repeat
>    {
> n1<-ceiling((((qt(1-(a/2),n0-4)+qt(1-b,n0-4))/b1)^2)*(((m+1)/m)*sp2/sx))
>        if(n0==n1) break
>        n0<-n1
>    }
>    return(c(sx,n1))
> }
>
> x<-rnorm(1000,0,1)
> x<-x[order(x)]
>
> res<-matrix(0,1000,2)
>
>
> #use the function and plot  for different values of ind and p
> for ( p in c(0.05,0.10,0.15,0.20,0.25,0.30,0.40,0.50))
> {  risk<-p*(2-p)
> nonrisk<-(1-p)^2
> m<-nonrisk/risk
>
> for (ind in 1:500)
> {res[ind,]<-powerb(x[c(1:(500-ind),(500+ind):1000)],4,0.05,0.20,0.1,m)}
>
> plot(res[,1],res[,2],type="p",ylab="n",xlab="var(x)",main="b=0.1,power=0
> .80,alpha=0.05,dominant met p=0.25")}
>

I modified your function as follows:

powerb<-function(x,sp2,a,b,b1,m){
    sx<-(sum(x^2)-(sum(x)^2)/length(x))/length(x)
    n0<-ceiling((((qnorm(1-(a/2))+qnorm(1-b))/b1)^2)*(((m+1)/m)*sp2/sx))
    repeat{
        n1<-ceiling((((qt(1-(a/2),n0-4)+qt(1-b,n0-4))/b1)^2)*(((m+1)/m)*sp2/sx))
        if(n0==n1) break else cat("p = ", p, ", ind = ", ind, ", n0 =
", n0, ", n1 = ", n1, "\n", sep='')
        n0<-n1
    }
    return(c(sx,n1))
}

and got, for example, the following output:

....
p = 0.2, ind = 278, n0 = 2288, n1 = 2289
p = 0.2, ind = 278, n0 = 2289, n1 = 2288
p = 0.2, ind = 278, n0 = 2288, n1 = 2289
p = 0.2, ind = 278, n0 = 2289, n1 = 2288
p = 0.2, ind = 278, n0 = 2288, n1 = 2289
p = 0.2, ind = 278, n0 = 2289, n1 = 2288
p = 0.2, ind = 278, n0 = 2288, n1 = 2289
p = 0.2, ind = 278, n0 = 2289, n1 = 2288
...


so your in an infinite loop. Maybe you can check the difference
between n0 and n1 instead, or perhaps better, do something like:

powerb<-function(x,sp2,a,b,b1,m){
    sx<-(sum(x^2)-(sum(x)^2)/length(x))/length(x)
    n0<-ceiling((((qnorm(1-(a/2))+qnorm(1-b))/b1)^2)*(((m+1)/m)*sp2/sx))
    n0.1 <- n1.1 <- Inf
    repeat{
        n1<-ceiling((((qt(1-(a/2),n0-4)+qt(1-b,n0-4))/b1)^2)*(((m+1)/m)*sp2/sx))
        if(n0==n1) break
        if(n0.1==n1 && n1.1==n0) break
        n0.1 <- n0
        n1.1 <- n1
        n0<-n1
    }
    return(c(sx,n1))
}


From dimitris.rizopoulos at med.kuleuven.be  Mon Jul  3 15:16:57 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 3 Jul 2006 15:16:57 +0200
Subject: [R] macro facility in R
References: <s4a8c9c7.085@MEDICINE.umaryland.edu>
	<971536df0607030601h7b97cbbcq5272d57755ba0ff2@mail.gmail.com>
Message-ID: <020e01c69ea2$f5d84600$0540210a@www.domain>


----- Original Message ----- 
From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
To: "John Sorkin" <jsorkin at grecc.umaryland.edu>
Cc: <r-help at stat.math.ethz.ch>
Sent: Monday, July 03, 2006 3:01 PM
Subject: Re: [R] macro facility in R


> Try this:
>
> # test data
> set.seed(1)
> mat <- matrix(rnorm(900), nc = 9)
> colnames(mat) <- letters[1:9]
> DF <- as.data.frame(mat)
>
> # run lm's and display coefs
> for(i in seq(1, 5, 2)) {
> dat <- cbind(z = DF[,i] - DF[,i+1], DF[7:9])
> Coef <- coef(lm(z ~., dat))
> cat("y: DF[,", i, "] - DF[,", i+1, "], coef: ", Coef, "\n")
> }
>

In case of lm(), this could also be written as:

y <- mat[, c(1, 3, 5)] - mat[, c(2, 4, 6)]
x <- mat[, 7:9]
lm(y ~ x)


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


>
>
> On 7/3/06, John Sorkin <jsorkin at grecc.umaryland.edu> wrote:
>> R 2.2 on windows XP
>> I have a dataset with multiple columns. Some of the columns 
>> represent
>> independent variables, some represent dependent variables. I would 
>> like
>> to run the same analyses on a fixed set of independent variables,
>> changing only the dependent variable, e.g.
>> y1-y2=x1+x2+x3
>> y3-y4=x1+x2+x3
>> y5-y6=x1+x2+x3, etc.
>> I know I can write a function to perform the analyses, however in 
>> order
>> to make the analyses easy to do, I really need a macro scripting
>> language that will allow preprocessing of the function and 
>> substitution
>> of macro variables with parameters passed to the macro, i.e. I need
>> someting akin to the macro facility in SAS. Is there a macro 
>> facility
>> that I can use with R? I have tried help.search("macro") and did 
>> not
>> have any success.
>> Thanks,
>> John
>>
>> John Sorkin M.D., Ph.D.
>> Chief, Biostatistics and Informatics
>> Baltimore VA Medical Center GRECC,
>> University of Maryland School of Medicine Claude D. Pepper OAIC,
>> University of Maryland Clinical Nutrition Research Unit, and
>> Baltimore VA Center Stroke of Excellence
>>
>> University of Maryland School of Medicine
>> Division of Gerontology
>> Baltimore VA Medical Center
>> 10 North Greene Street
>> GRECC (BT/18/GR)
>> Baltimore, MD 21201-1524
>>
>> 410-605-7119
>> jsorkin at grecc.umaryland.edu
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From j.van_den_hoff at fz-rossendorf.de  Mon Jul  3 15:36:18 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Mon, 03 Jul 2006 15:36:18 +0200
Subject: [R] Optional variables in function?
In-Reply-To: <644e1f320607020431t6181b7e1u7d5b08df0b9d01ac@mail.gmail.com>
References: <001701c69d99$2e7b56b0$c101a8c0@QWARD>
	<644e1f320607020431t6181b7e1u7d5b08df0b9d01ac@mail.gmail.com>
Message-ID: <44A91D52.1020501@fz-rossendorf.de>

jim holtman wrote:
> ?missing
> 
> On 7/2/06, Jonathan Greenberg <jgreenberg at arc.nasa.gov> wrote:
>> I'm a bit new to writing R functions and I was wondering what the "best
>> practice" for having optional variables in a function is, and how to test
>> for optional and non-optional variables?  e.g. if I have the following
>> function:
>>
>> helpme <- function(a,b,c) {
>>
>>
>> }
>>
>> In this example, I want c to be an optional variable, but a and b to be
>> required.  How do I:
>> 1) test to see if the user has inputted c
>> 2) break out of the function of the user has NOT inputted a or b.

if(missing(c))
    stop("need c")

or

if(missing(c))
    return()

depending on your problem it might be better to provide sensible default 
values for a,b,c in the function definition

helpme<-function(a=A, b=B, c=C) {...}

instead of enforcing that every argument is specified?

joerg


>>
>> Thanks!
>>
>> --j
>>
>> --
>>
>> Jonathan A. Greenberg, PhD
>> NRC Research Associate
>> NASA Ames Research Center
>> MS 242-4
>> Moffett Field, CA 94035-1000
>> Phone: 415-794-5043
>> AIM: jgrn3007
>> MSN: jgrn3007 at hotmail.com
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
> 
> 
>


From Mike.Prager at noaa.gov  Mon Jul  3 15:49:51 2006
From: Mike.Prager at noaa.gov (Michael Prager)
Date: Mon, 03 Jul 2006 09:49:51 -0400
Subject: [R] macro facility in R
In-Reply-To: <s4a8c9c7.085@MEDICINE.umaryland.edu>
References: <s4a8c9c7.085@MEDICINE.umaryland.edu>
Message-ID: <44A9207F.1070703@noaa.gov>

Here is one user's perspective on this, with no pretense of being 
definitive.  In the S (R) world, the expression "computing on the 
language" is used to encompass what I would call the tasks of macro 
programming.  This involves uses of various S (R) expressions that 
convert between names of objects and the object themselves; pasting 
together expressions out of fixed and variable text; and executing 
them.  I for one have been able to do everything I could do in the SAS 
macro language, but it has taken trial and error.

To get started, you might consult section 3.5 of S Programming by 
Venables and Ripley.

MHP

John Sorkin wrote on 7/3/2006 7:39 AM:
> R 2.2 on windows XP
> I have a dataset with multiple columns. Some of the columns represent
> independent variables, some represent dependent variables. I would like
> to run the same analyses on a fixed set of independent variables,
> changing only the dependent variable, e.g.
> y1-y2=x1+x2+x3
> y3-y4=x1+x2+x3
> y5-y6=x1+x2+x3, etc.
> I know I can write a function to perform the analyses, however in order
> to make the analyses easy to do, I really need a macro scripting
> language that will allow preprocessing of the function and substitution
> of macro variables with parameters passed to the macro, i.e. I need
> someting akin to the macro facility in SAS. Is there a macro facility
> that I can use with R? I have tried help.search("macro") and did not
> have any success.
> Thanks,
> John  
>
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC,
> University of Maryland School of Medicine Claude D. Pepper OAIC,
> University of Maryland Clinical Nutrition Research Unit, and
> Baltimore VA Center Stroke of Excellence
>
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>
> 410-605-7119
> jsorkin at grecc.umaryland.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>   

-- 
Michael H. Prager, Ph.D.
Population Dynamics Team
NOAA Center for Coastal Habitat and Fisheries Research
NMFS Southeast Fisheries Science Center
Beaufort, North Carolina  28516  USA
http://shrimp.ccfhrb.noaa.gov/~mprager/


From katrin.braesel at imise.uni-leipzig.de  Mon Jul  3 15:46:28 2006
From: katrin.braesel at imise.uni-leipzig.de (Katrin Braesel)
Date: Mon, 03 Jul 2006 15:46:28 +0200
Subject: [R] problem with --vanilla in R
Message-ID: <44A91FB4.7090007@imise.uni-leipzig.de>

Hallo,

I'm using the R Version 2.3.0 (2006-04-24) on Suse Linux 10.1.
With an older R and Linux version I could write a R-function into a file
 and execute it with the command:
R --vanilla < script.r

In the file script.r was code like this:
postscript(file="results.eps")
x<-2
y<-3
plot(x,y)
dev.off()

It worked fine, but now nothing happens. In the manual page this option
is still mentioned.
I don't know what to do, to make it working again.

Thanks for your help.
With kind regards,
Katrin


From ligges at statistik.uni-dortmund.de  Mon Jul  3 16:13:12 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 03 Jul 2006 16:13:12 +0200
Subject: [R] problem with --vanilla in R
In-Reply-To: <44A91FB4.7090007@imise.uni-leipzig.de>
References: <44A91FB4.7090007@imise.uni-leipzig.de>
Message-ID: <44A925F8.3010200@statistik.uni-dortmund.de>

Katrin Braesel wrote:
> Hallo,
> 
> I'm using the R Version 2.3.0 (2006-04-24) on Suse Linux 10.1.
> With an older R and Linux version I could write a R-function into a file
>  and execute it with the command:
> R --vanilla < script.r
> 
> In the file script.r was code like this:
> postscript(file="results.eps")
> x<-2
> y<-3
> plot(x,y)
> dev.off()
> 
> It worked fine, but now nothing happens. In the manual page this option
> is still mentioned.
> I don't know what to do, to make it working again.


Works for me with R-2.3.1 using an appropriate shell.

Maybe you misspelled the filename or don't have write access? What 
happens after R --vanilla < script.r

BTW: I'd rather use R CMD BATCH, and I'd call the script script.R...

Uwe Ligges


> Thanks for your help.
> With kind regards,
> Katrin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From epistat at gmail.com  Mon Jul  3 16:15:50 2006
From: epistat at gmail.com (zhijie zhang)
Date: Mon, 3 Jul 2006 22:15:50 +0800
Subject: [R] do i set the correct argument?
Message-ID: <2fc17e30607030715sfd89e16he176e64173d174cd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060703/289b83da/attachment.pl 

From rdporto1 at terra.com.br  Mon Jul  3 17:46:52 2006
From: rdporto1 at terra.com.br (rdporto1)
Date: Mon,  3 Jul 2006 12:46:52 -0300
Subject: [R] large dataset!
Message-ID: <J1U3U4$C38EE3DEF464E3D5E84F1CDE365D4BAC@terra.com.br>

Jennifer,

we had a little discussion about this topic
last May when I had a similar problem.
It is archived at

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/76401.html

You can follow the thread to see the various 
arguments and solutions. I tried to summarize
the possible suggested approachs at

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/76583.html

HTH,

Rogerio Porto.

---------- Cabe?alho original -----------

De: r-help-bounces at stat.math.ethz.ch
Para: r-help at stat.math.ethz.ch
C?pia: 
Data: Sun, 2 Jul 2006 10:12:25 -0400 (EDT)
Assunto: [R] large dataset!

> 
> Hi, I need to analyze data that has 3.5 million observations and
> about 60 variables and I was planning on using R to do this but
> I can't even seem to read in the data.  It just freezes and ties
> up the whole system -- and this is on a Linux box purchased about
> 6 months ago on a dual-processor PC that was pretty much the top
> of the line.  I've tried expanding R the memory limits but it 
> doesn't help.  I'll be hugely disappointed if I can't use R b/c
> I need to do build tailor-made models (multilevel and other 
> complexities).   My fall-back is the SPlus big data package but
> I'd rather avoid if anyone can provide a solution....
> 
> Thanks!!!!
> 
> Jennifer Hill
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From maechler at stat.math.ethz.ch  Mon Jul  3 18:13:08 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 3 Jul 2006 18:13:08 +0200
Subject: [R] how to get the studentized residuals in lm()
In-Reply-To: <38b9f0350607022129r1c026efehb5fd223f2e641e77@mail.gmail.com>
References: <2fc17e30607022011h4ba3d6b4v26617726e51ac89a@mail.gmail.com>
	<38b9f0350607022129r1c026efehb5fd223f2e641e77@mail.gmail.com>
Message-ID: <17577.16916.339106.642781@stat.math.ethz.ch>

>>>>> "ronggui" == ronggui  <ronggui.huang at gmail.com>
>>>>>     on Mon, 3 Jul 2006 12:29:42 +0800 writes:

    >> help.search("studentized")
    ronggui> You will see:
    ronggui> studres(MASS)           Extract Studentized Residuals from a Linear Model

Yes.   But you don't even need another package.  Thanks to 
John Fox, there's
     rstudent()
     rstandard()
etc, even in the 'stats' package, with methods for "lm" and for 
"glm" objects.

And now, in R-devel (2.4.0-to-be),
   help.search("studentized")
will show the 'influence.measures' help page which contains rstudent().

Martin Maechler, ETH Zurich



    ronggui> 2006/7/3, zhijie zhang <epistat at gmail.com>:
    >> Dear friends,
    >> In s-plus, lm()  generates the the studentized residuals
    >> automatically for us, and In R, it seems don't have the results: After
    >> i fitted lm(), i use attibutes() to see the objects and didn't find
    >> studentized residuals .
    >> How to get the the studentized residuals in lm(),have i missed something?
    >> thanks very much!
    >> 
    >> --
    >> Kind Regards,
    >> Zhi Jie,Zhang ,PHD
    >> Department of Epidemiology
    >> School of Public Health
    >> Fudan University
    >> Tel:86-21-54237149
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
    >> 


    ronggui> -- 
    ronggui> $A;FHY9s(B
    ronggui> Department of Sociology
    ronggui> Fudan University

    ronggui> ______________________________________________
    ronggui> R-help at stat.math.ethz.ch mailing list
    ronggui> https://stat.ethz.ch/mailman/listinfo/r-help
    ronggui> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From tlumley at u.washington.edu  Mon Jul  3 18:32:19 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 3 Jul 2006 09:32:19 -0700 (PDT)
Subject: [R] macro facility in R
In-Reply-To: <s4a8c9c7.085@MEDICINE.umaryland.edu>
References: <s4a8c9c7.085@MEDICINE.umaryland.edu>
Message-ID: <Pine.LNX.4.64.0607030927380.21683@homer22.u.washington.edu>

On Mon, 3 Jul 2006, John Sorkin wrote:
> R 2.2 on windows XP
> I have a dataset with multiple columns. Some of the columns represent
> independent variables, some represent dependent variables. I would like
> to run the same analyses on a fixed set of independent variables,
> changing only the dependent variable, e.g.
> y1-y2=x1+x2+x3
> y3-y4=x1+x2+x3
> y5-y6=x1+x2+x3, etc.

Other people have mentioned my defmacro() function in R-news, however when 
I have to do this sort of thing I usually use substitute or bquote

For example

for(i in 1+(1:5)*2){
   A<-as.name(paste("y",i,sep=""))
   B<-as.name(paste("y",i+1,sep="")
   eval(bquote(lm(I(.(A)-.(B))~x1+x2+x3,data=dat)))
}


 	-thomas


From maechler at stat.math.ethz.ch  Mon Jul  3 18:41:54 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 3 Jul 2006 18:41:54 +0200
Subject: [R] legend with filled boxes AND lines
In-Reply-To: <OFC8EAFB28.9603DB51-ONC12571A0.00368D2D-C12571A0.0040214F@gfk.de>
References: <OFC8EAFB28.9603DB51-ONC12571A0.00368D2D-C12571A0.0040214F@gfk.de>
Message-ID: <17577.18642.535438.551431@stat.math.ethz.ch>

Did you try  legend(......, lty=..., fill=...,  merge = TRUE) ?

In an example I just tried, this allowed to give filled boxes
*and* lines.

Please give a reproducible example of what you did -- maybe by
modifying one of the many  example(legend)  examples.

Martin Maechler, ETH Zurich

>>>>> "florian" == florian koller <florian.koller at gfk.com>
>>>>>     on Mon, 3 Jul 2006 13:40:33 +0200 writes:

    florian> Dear all,

    florian> Is there a straightforward way to create a legend
    florian> box that has both filled boxes and lines?  So far I
    florian> have built around this problem by creating two
    florian> legends (with bty = "n") and manually drawing a box
    florian> around both (but this is cumbersome, because I have
    florian> to check upon the y coordinates of the legends
    florian> every time).

    florian> If I do something like > legend( ...,c("X1","X2",
    florian> "mean"), fill = c("red", "blue", 0), lty = (0,0,2))
    florian> < , I cannot get rid of the unfilled box or change
    florian> the color of the fill box border (from its default
    florian> color "black"), and I end up with two filled boxes
    florian> and an empty, black-lined box plus the line as a
    florian> legend for the third argument "mean". This trick
    florian> therefore only works if I define "black" as the bg
    florian> color for the complete legend box (because it masks
    florian> the empty box from the fill argument). So, if there
    florian> is a command to modify the color of the fill box
    florian> border line (not the legend box border line), this
    florian> would help me, too (still not ideal, though...).

    florian> Thanks,
    florian> Florian



    florian> ______________________
    florian> Florian Koller
    florian> GfK Fernsehforschung GmbH
    florian> Research Consulting & Development
    florian> Nordwestring 101
    florian> D-90319 N?rnberg
    florian> Fon  +49 (0)911 395-3554
    florian> Fax  +49 (0)911 395-4130
    florian> www.gfk.de / www.gfk.com



    florian> _________________________

    florian> Diese E-Mail (ggf. nebst Anhang) enth?lt vertrauliche und/oder rechtlich
    florian> gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat sind, oder
    florian> diese E-Mail irrt?mlich erhalten haben, informieren Sie bitte sofort den
    florian> Absender und vernichten Sie diese Mail. Das unerlaubte Kopieren sowie die
    florian> unbefugte Weitergabe dieser Mail ist nicht gestattet.

    florian> This e-mail (and any attachment/s) contains confidential and/or privileged
    florian> information. If you are not the intended recipient (or have received this
    florian> e-mail in error) please notify the sender immediately and destroy this
    florian> e-mail. Any unauthorised copying, disclosure or distribution of the
    florian> material in this e-mail is strictly forbidden.

    florian> ______________________________________________
    florian> R-help at stat.math.ethz.ch mailing list
    florian> https://stat.ethz.ch/mailman/listinfo/r-help
    florian> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From spencer.graves at pdf.com  Mon Jul  3 18:56:18 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 03 Jul 2006 09:56:18 -0700
Subject: [R] help with coxme
In-Reply-To: <web-212457757@cgatepro-4.mail.virginia.edu>
References: <web-212457757@cgatepro-4.mail.virginia.edu>
Message-ID: <44A94C32.9060709@pdf.com>

	  When I tried it, I seemed to get inconsistent results, the first of 
which was irreproducible.  The first error message said that 'rats2' was 
not a data.frame.  I don't know what I did to get that, but when I tried 
it again, I got the following:

Error in max(kindex) : object "kindex" not found
 > traceback()
3: max(kindex)
2: max(kindex)
1: coxme(Surv(time, status) ~ rx + x2, data = rats2, random = ~(1 +
        x2) | litter)

	  This sounds to me like a logic error in 'coxme';  I am therefore 
cc-ing the maintainer listed in help(package='kinship').

	  If it were my problem, I might list coxme and make a local copy of 
it.  It's easy to find the offending 'max(kindex)'.  It's harder to 
figure out what kindex should be in this context.  To attempt that, I 
might request debug(coxme), then try again the 'coxme' call that 
produced the error message and trace through it line by line until I 
figured it out.

	  Hope this helps.
	  Spencer Graves

Lei Liu wrote:
> Hi there,
> 
> I have a question on fitting data by coxme. In particular I want to fit a 
> random intercept and random slope cox model. Using the rats dataset as an 
> example, I generated another covariate x2 and want to specify a random slope 
> for x2. Here is my code:
> 
> x2=matrix(rep(runif(50), 3), 50, 3)
> x2=as.vector(t(x2))
> 
> rats2=cbind(rats, x2)
> 
> But when I used the coxme function as follows, it gave an error message. 
> What is the right way to do it? Thanks a lot!
> 
>   coxme(Surv(time, status) ~ rx+x2, data=rats2, random=~ (1+x2)|litter )
> 
> 
> Lei Liu
> Assistant Professor
> Division of Biostatistics and Epidemiology
> Dept. of Public Health Sciences
> School of Medicine
> University of Virginia
> 
> 3181 Hospital West Complex
> Charlottesville, VA 22908-0717
> 
> 1-434-982-3364 (o)
> 1-434-806-8086 (c)
> 
> liulei at virginia.edu
> ll9f at virginia.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From florian.koller at gfk.com  Mon Jul  3 19:23:14 2006
From: florian.koller at gfk.com (florian.koller at gfk.com)
Date: Mon, 3 Jul 2006 19:23:14 +0200
Subject: [R] Antwort: Re:  legend with filled boxes AND lines
In-Reply-To: <17577.18642.535438.551431@stat.math.ethz.ch>
Message-ID: <OF9835CE1E.B1FD6A88-ONC12571A0.005EA7DF-C12571A0.005F80E2@gfk.de>

Hi Martin,

I know about the merge command, but I want a line without the box in the
legend. Ideally I would need some argument that tells the fill subcommand
not only to suppress the box color (see example below), but also to
suppress the frame of the fill box. Alternatively if someone could tell me
how to modify the color of the frame, this would help, too, because I could
simply set it to the bg command.


x1 <- rnorm(100)
x2 <- rnorm(100, 2)
hist(x1, main = "", col = "orange",ylab = "density", xlab = "x", freq = F,
density = 55,
     xlim = c(-2, 5), ylim = c(0, 0.5))
par(new = T)
hist(x2, main = "", col = "green", ylab = "", xlab = "",axes = F,
xlim = c(-2, 5), ylim = c(0, 0.5), density = 45, freq = F)
abline(v = mean(x1), col = "orange", lty = 2, lwd = 2.5)
abline(v = mean(x2), col = "green", lty = 2, lwd = 2.5)
legend(3, 0.45, legend = c("x1", "x2", "mean(x1)", "mean(x2)"),
       col = c("orange", "green"), fill=c("orange","green", 0, 0),
       lty = c(0, 0, 2, 2), merge = T)


Thank you,
Florian Koller


Martin Maechler <maechler at stat.math.ethz.ch> schrieb am 03/07/2006
18:41:54:

> Did you try  legend(......, lty=..., fill=...,  merge = TRUE) ?
>
> In an example I just tried, this allowed to give filled boxes
> *and* lines.
>
> Please give a reproducible example of what you did -- maybe by
> modifying one of the many  example(legend)  examples.
>
> Martin Maechler, ETH Zurich
>
> >>>>> "florian" == florian koller <florian.koller at gfk.com>
> >>>>>     on Mon, 3 Jul 2006 13:40:33 +0200 writes:
>
>     florian> Dear all,
>
>     florian> Is there a straightforward way to create a legend
>     florian> box that has both filled boxes and lines?  So far I
>     florian> have built around this problem by creating two
>     florian> legends (with bty = "n") and manually drawing a box
>     florian> around both (but this is cumbersome, because I have
>     florian> to check upon the y coordinates of the legends
>     florian> every time).
>
>     florian> If I do something like > legend( ...,c("X1","X2",
>     florian> "mean"), fill = c("red", "blue", 0), lty = (0,0,2))
>     florian> < , I cannot get rid of the unfilled box or change
>     florian> the color of the fill box border (from its default
>     florian> color "black"), and I end up with two filled boxes
>     florian> and an empty, black-lined box plus the line as a
>     florian> legend for the third argument "mean". This trick
>     florian> therefore only works if I define "black" as the bg
>     florian> color for the complete legend box (because it masks
>     florian> the empty box from the fill argument). So, if there
>     florian> is a command to modify the color of the fill box
>     florian> border line (not the legend box border line), this
>     florian> would help me, too (still not ideal, though...).
>
>     florian> Thanks,
>     florian> Florian
>
>
>
>     florian> ______________________
>     florian> Florian Koller
>     florian> GfK Fernsehforschung GmbH
>     florian> Research Consulting & Development
>     florian> Nordwestring 101
>     florian> D-90319 N?rnberg
>     florian> Fon  +49 (0)911 395-3554
>     florian> Fax  +49 (0)911 395-4130
>     florian> www.gfk.de / www.gfk.com
>
>
>
>     florian> _________________________
>
>     florian> Diese E-Mail (ggf. nebst Anhang) enth?lt vertrauliche
> und/oder rechtlich
>     florian> gesch?tzte Informationen. Wenn Sie nicht der richtige
> Adressat sind, oder
>     florian> diese E-Mail irrt?mlich erhalten haben, informieren Sie
> bitte sofort den
>     florian> Absender und vernichten Sie diese Mail. Das unerlaubte
> Kopieren sowie die
>     florian> unbefugte Weitergabe dieser Mail ist nicht gestattet.
>
>     florian> This e-mail (and any attachment/s) contains
> confidential and/or privileged
>     florian> information. If you are not the intended recipient (or
> have received this
>     florian> e-mail in error) please notify the sender immediately
> and destroy this
>     florian> e-mail. Any unauthorised copying, disclosure or
> distribution of the
>     florian> material in this e-mail is strictly forbidden.
>
>     florian> ______________________________________________
>     florian> R-help at stat.math.ethz.ch mailing list
>     florian> https://stat.ethz.ch/mailman/listinfo/r-help
>     florian> PLEASE do read the posting guide! http://www.R-project.
> org/posting-guide.html


From pj.bell at yahoo.co.uk  Mon Jul  3 20:26:03 2006
From: pj.bell at yahoo.co.uk (Piet Bell)
Date: Mon, 3 Jul 2006 19:26:03 +0100 (BST)
Subject: [R] gamm
Message-ID: <20060703182603.87603.qmail@web27613.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060703/29f93ef4/attachment.pl 

From spencer.graves at pdf.com  Mon Jul  3 20:31:06 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 03 Jul 2006 11:31:06 -0700
Subject: [R] lme and SAS Proc mixed
In-Reply-To: <200606301931.PAA24757@arrakis.vcu.edu>
References: <200606301931.PAA24757@arrakis.vcu.edu>
Message-ID: <44A9626A.7070002@pdf.com>

	  Your example is entirely too complicated for me to parse in the time 
available, but I have a few questions that I hope might help:

	  First, have you examined str(fit.lme) plus all the other help pages 
listed under "See Also" in the "lme" help page, especially "lmeObject"? 
  With luck, this may answer your questions.

	  Second, are all your random effects nested, or are some crossed?  If 
the model is all standard nesting with no complicated inhomogeneity of 
variance or correlation structure, you may be working too hard:  The 
'nlme' package is designed to make standard nested mixed-model analysis 
as easily as Doug Bates could make it with a general tool in the S 
language paradigm.  Functions like 'pdBlocked' are relatively 
inefficient attempts to adapt the 'nlme' paradigm to support crossed 
random effects, etc.  If you have both crossed and nested random 
effects, you might try Bates' current project, 'lmer' associated with 
the 'lme4' package.  Unfortunately, that package currently has fewer 
helper functions for things like confidence intervals.  (It's Bates' 
leading edge development effort.)

	  Third, can generate a much simpler, self-contained problem that 
exhibits the same features as your more complicated example?  If your 
email had contained a few lines of R code that someone like me could 
copy and paste into R and get what you got, it would increase your 
chances of getting a more informative reply quicker.  Without it, I'm 
left to guessing.

	  Fourth, if you want to know, "which is more correct", have you 
considered Monte Carlo?  The nlme package includes a function 
simulate.lme, which might help you.  The lme4 package includes mcmcsamp.

	  Hope this helps.
	  Spencer Graves

Kellie J. Archer, Ph.D. wrote:
> I am trying to use lme to fit a mixed effects model to get the same
> results as when using the following SAS code:
> 
> proc mixed;
> class refseqid probeid probeno end;
> model expression=end logpgc / ddfm=satterth;
> random probeno probeid / subject=refseqid type=cs;
> lsmeans end / diff cl; run;
> 
> There are 3 genes (refseqid) which is the large grouping factor, with
> 2 probeids nested within each refseqid, and 16 probenos nested within
> each of the probeids.
> 
> I have specified in the SAS Proc Mixed procedure that the
> variance-covariance structure is to be compound symmetric. Therefore,
> the variance-covariance matrix is a block diagonal matrix of the form,
> 
> V_1  0   0
> 0   V_2  0
> 0    0   V3
> 
> where each V_i represents a RefSeqID. Moreover, for each V_i the
> structure within the block is 
> 
> v_{11}   v{12}
> v_{21}   v{22}
> 
> where v_{11} and v_{22} are different probeids nested within the
> refseqid, and so are correlated. The structure of
> both v_{11} and v_{22} are compound symmetric, and v_{12} and v{21}
> contain a constant for all elements of the matrix.
> 
> I have tried to reproduce this using lme, but it is unclear from the
> documentation (and Pinheiro & Bates text) how the pdBlocked and
> compound symmetric structure can be combined.
> 
> fit.lme<-lme(expression~End+logpgc,random=list(RefSeqID=pdBlocked(list
> (~1,~ProbeID-1),pdClass="pdSymm")),data=dataset,correlation=corCompSym
> m(form=~1|RefSeqID/ProbeID/ProbeNo))
> 
> 
> The point estimates are essentially the same comparing R and SAS for
> the fixed effects, but the 95% confidence intervals are much shorter
> using lme(). In order to find the difference in the algorithms used by
> SAS and R I tried to extract the variance-covariance matrix to look at
> its structure. I used the getVarCov() command, but it tells me that
> this function is not available for nested structures. Is there another
> way to extract the variance-covariance structure for nested models?
> Does anyone know how I could get the var-cov structure above using
> lme?
> 
> 
> Kellie J. Archer, Ph.D.
> Assistant Professor, Department of Biostatistics
> Fellow, Center for the Study of Biological Complexity
> Virginia Commonwealth University
> 1101 East Marshall St., B1-066
> Richmond, VA 23298-0032
> phone: (804) 827-2039
> fax: (804) 828-8900
> e-mail: kjarcher at vcu.edu
> website: www.people.vcu.edu/~kjarcher
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From jporzak at gmail.com  Mon Jul  3 21:52:32 2006
From: jporzak at gmail.com (Jim Porzak)
Date: Mon, 3 Jul 2006 12:52:32 -0700
Subject: [R]  xlab, ylab in balloonplot(tab)?
Message-ID: <2a9c000c0607031252x5036e569k71a86b396b71e8e3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060703/42aa16a8/attachment.pl 

From markleeds at verizon.net  Mon Jul  3 22:37:58 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Mon, 03 Jul 2006 15:37:58 -0500 (CDT)
Subject: [R] difficult data manipulation question
Message-ID: <1432347.5012871151959078685.JavaMail.root@vms168.mailsrvcs.net>


hi everyone :

suppose i have a matrix in which some column names are identical so,
for example, TEMP

  "AAA", "BBB", "CCC", "DDD","AAA", "BBB"
    0      2      1     2      0      0
    2      3      7     6      0      1
    1.5    4      9     9      6      0
    1.0    6      10    11     3      3


I didn't even check  yet whether identical column names are allowed
in a matrix but i hope they are.

assuming that they are, then i would like to be able to take the matrix and  make a new matrix with the following requirements.

1) whenever there is a unique column name, just take that column for the new matrix

2) whenever the column name is not unique, take the one
that has the most non zero elements ? ( in the case of
ties, i don't care which one is picked ).

so, in this case, the resulting matrix would just be the first 4 columns.

i realize ( or atleast i think ) that 
sum( TEMP[(TEMP[,columnname] !=0) ,columnname) will give me the
number of non elements in a column with the name columnmame
but how to use this deal with the non uniqueness to solve my particular problem is beyond me. plus, i think the command will
bomb because columnname will not always be unique ? 
Thanks for any help. I realize this is not a trivial problem so I really appreciate it.

                                          Mark


From ggrothendieck at gmail.com  Mon Jul  3 22:58:14 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 3 Jul 2006 16:58:14 -0400
Subject: [R] difficult data manipulation question
In-Reply-To: <1432347.5012871151959078685.JavaMail.root@vms168.mailsrvcs.net>
References: <1432347.5012871151959078685.JavaMail.root@vms168.mailsrvcs.net>
Message-ID: <971536df0607031358u35b537cfyb04df06d8c8c0b12@mail.gmail.com>

Try this:

# test data
# read in header separately so R does not make column names unique
Lines <- "AAA BBB CCC DDD AAA BBB
   0      2      1     2      0      0
   2      3      7     6      0      1
   1.5    4      9     9      6      0
   1.0    6      10    11     3      3
"
DF <- read.table(textConnection(Lines), skip = 1)
names(DF) <- scan(textConnection(Lines), what = "", nlines = 1)

f <- function(x) x[which.max(colSums(DF[x]!=0))]
tapply(seq(DF), names(DF), f)

On 7/3/06, markleeds at verizon.net <markleeds at verizon.net> wrote:
>
> hi everyone :
>
> suppose i have a matrix in which some column names are identical so,
> for example, TEMP
>
>  "AAA", "BBB", "CCC", "DDD","AAA", "BBB"
>    0      2      1     2      0      0
>    2      3      7     6      0      1
>    1.5    4      9     9      6      0
>    1.0    6      10    11     3      3
>
>
> I didn't even check  yet whether identical column names are allowed
> in a matrix but i hope they are.
>
> assuming that they are, then i would like to be able to take the matrix and  make a new matrix with the following requirements.
>
> 1) whenever there is a unique column name, just take that column for the new matrix
>
> 2) whenever the column name is not unique, take the one
> that has the most non zero elements ? ( in the case of
> ties, i don't care which one is picked ).
>
> so, in this case, the resulting matrix would just be the first 4 columns.
>
> i realize ( or atleast i think ) that
> sum( TEMP[(TEMP[,columnname] !=0) ,columnname) will give me the
> number of non elements in a column with the name columnmame
> but how to use this deal with the non uniqueness to solve my particular problem is beyond me. plus, i think the command will
> bomb because columnname will not always be unique ?
> Thanks for any help. I realize this is not a trivial problem so I really appreciate it.
>
>                                          Mark
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From bruno.giordano at music.mcgill.ca  Mon Jul  3 23:56:58 2006
From: bruno.giordano at music.mcgill.ca (Bruno L. Giordano)
Date: Mon, 03 Jul 2006 17:56:58 -0400
Subject: [R]  analogue of group option of SAS MIXED/random in R
Message-ID: <004501c69eeb$a12cd190$6400a8c0@brungio>

Dear list,

I am trying to use lme to build the analogue of the following SAS MIXED
random specification:

random int+Variable1+Variable2 /subject = Subject group=Condition type=vc;

which gives a Condition-blocked heterogeneity in the random effects
variance-covariance matrix.

Needless to say, I have a hard time in specifying Condition-specific
heterogeneities in the variance-covariance parameters.

I initially tried the following commands (without Condition-heterogeneity in
the random effects):

G.Data<-groupedData(Response~1|Subject,data=In.Data)
Fit1<-lme(Response~1+Variable1+Variable2*Condition,random=pdDiag(~1+Variable1+Variable2),method="REML",data=G.Data)

but have no idea about where to go from here (note that I don't want to nest 
Subject in Condition).

Thanks!!

    Bruno


From pj.bell at yahoo.co.uk  Tue Jul  4 00:05:08 2006
From: pj.bell at yahoo.co.uk (Piet Bell)
Date: Mon, 3 Jul 2006 23:05:08 +0100 (BST)
Subject: [R] gamm and binomial data
Message-ID: <20060703220508.51207.qmail@web27606.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060703/0c3c6ba1/attachment.pl 

From jporzak at gmail.com  Tue Jul  4 01:05:24 2006
From: jporzak at gmail.com (Jim Porzak)
Date: Mon, 3 Jul 2006 16:05:24 -0700
Subject: [R] xlab, ylab in balloonplot(tab)?
In-Reply-To: <2a9c000c0607031252x5036e569k71a86b396b71e8e3@mail.gmail.com>
References: <2a9c000c0607031252x5036e569k71a86b396b71e8e3@mail.gmail.com>
Message-ID: <2a9c000c0607031605j2dc62dd0ne16891fe70165563@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060703/6090a3a2/attachment.pl 

From jholtman at gmail.com  Tue Jul  4 03:59:43 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 3 Jul 2006 21:59:43 -0400
Subject: [R] difficult data manipulation question
In-Reply-To: <971536df0607031358u35b537cfyb04df06d8c8c0b12@mail.gmail.com>
References: <1432347.5012871151959078685.JavaMail.root@vms168.mailsrvcs.net>
	<971536df0607031358u35b537cfyb04df06d8c8c0b12@mail.gmail.com>
Message-ID: <644e1f320607031859o696652c8vb01d2dd229bd275d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060703/25443955/attachment.pl 

From p.murrell at auckland.ac.nz  Tue Jul  4 04:37:41 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 04 Jul 2006 14:37:41 +1200
Subject: [R] curiosity question: new graphics vs. old graphics subsystem
In-Reply-To: <50d1c22d0607011123x1fc06845oceb6cbaf008ee00@mail.gmail.com>
References: <50d1c22d0607011123x1fc06845oceb6cbaf008ee00@mail.gmail.com>
Message-ID: <44A9D475.1070407@stat.auckland.ac.nz>

Hi


ivo welch wrote:
> I just read paul murrell's new book, R graphics.
> 
> now, I have always used the traditional graphics system.  apparently,
> the new (trellis?) system is an entirely separate graphics system.
> after reading the book, I cannot figure out what the intrinsic
> capability advantage of the old graphics system is that cannot be
> replicated in the trellis system.
> 
> if the new system's capabilities are practically a subset of the old
> system, why don't we design a compatibility layer so that we can just
> have one graphics subsystem, instead?  it seems weird that newbies
> learn the standard system first, and then, instead of building on it
> with more complex functions, are told to forget about things and start
> with something new.  I do like the simplicity of learning of the old
> system, but this would be the same if it were to come through a
> compatibility layer, too.  and then it would be easy to build learning
> on it.
> 
> but maybe I have it all wrong.  maybe there is something unique about
> the old system that the new system cannot do.  curious:  what is it?


The main problem is that traditional graphics thinks everything is a
plot.  This is good because it makes things like this possible ...

plot(1)
text(1, 1, "whoop-dee do")

... the important bit is that the 'text()' call has a reliable meaning:
it adds text to the current plot.  The downside is that if what you want
to draw is not a plot, you have to fight the system to do it.

The grid system thinks everything is a picture.  This is good because if
you want to draw something other than a plot, you can.  It is bad
because if I do something like ...

grid.text("la-dee da")

... that is not necessarily going to be added to a plot (unless I first
make sure that I am in an appropriate viewport).

There are other issues such as the fact that grid is generally more
memory hungry and slightly slower.

A "compatibility layer" has been discussed and it is perhaps possible,
BUT it would require reimplementing the traditional graphics system on
top of grid, which would be a pretty icky task.  Contributions always
welcome :)


> I also found the naming of the new system confusing.  there is
> trellis, there is lattice, there is grid.  how exactly should the new
> system be called?  paul calls the old system "traditional."  the new
> one seems to rear its head in different forms.


Reread Section 1.2 :)  There are two basic graphical systems
("traditional" and grid) and numerous graphical packages built on top of
each.  Lots of packages build on top of the traditional system.  Several
packages now exist on top of 'grid' (notably 'lattice', 'vcd', 'ggplot',
and 'hexbin').


> some other opinions (which follow the old rule that everyone has one):
> 
> * if we had one graphics subsystem, paul's book, and for this matter
> any explanation of the R graphics system, would become more
> parsimonious.
> 
> * R is, IMHO, the premier "programmed graphics" package today.  I may
> be complaining, but I also recognize that it is great.  so, please
> consider this to be only a suggestion.
> 
> * we have a pixmap image function.  we should also have a pdf
> includegraphics function, which can import an existing graphics image.
>  if a device (X11) is incapable of displaying it, we should just
> display a rectangle of the bounding box.  this would open up even more
> avenues to the ability of R to create graphics.


There is now a 'grImport' package on CRAN for this sort of thing (also
see http://www.r-project.org/useR-2006/Slides/Murrell.pdf).

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From p.murrell at auckland.ac.nz  Tue Jul  4 04:39:56 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 04 Jul 2006 14:39:56 +1200
Subject: [R] postscript file too large : maybe an R question
In-Reply-To: <4505466.1091491151715009027.JavaMail.root@vms070.mailsrvcs.net>
References: <4505466.1091491151715009027.JavaMail.root@vms070.mailsrvcs.net>
Message-ID: <44A9D4FC.60705@stat.auckland.ac.nz>

Hi


markleeds at verizon.net wrote:
> i created a postscipt file in R and then i downloaded a free version
> of ghostview to view it. unfortunately, i get the message
> 
> fata error : dynamic memory exhausted
> when i try to view it.
> 
> when i do a dir on windows xp, the file size is 149,034,475
> and i know there about 17,000 graphs. is there
> a way of possibly viewing this size postscript file in R itself ?


This postscript file presumably has more than one page(?).  Take a look
at the 'onefile' argument in '?postscript'.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From pbarata at infolink.com.br  Tue Jul  4 05:24:17 2006
From: pbarata at infolink.com.br (Paulo Barata)
Date: Tue, 04 Jul 2006 00:24:17 -0300
Subject: [R] parameter las (function par / graphics) on right axis
In-Reply-To: <mailman.9.1151488803.12054.r-help@stat.math.ethz.ch>
References: <mailman.9.1151488803.12054.r-help@stat.math.ethz.ch>
Message-ID: <44A9DF61.5020007@infolink.com.br>


Dear R users,

When trying to produce a graph with a scale on the right axis,
I came across a possible problem with graph making in R.

When using plot or barplot, when the parameter las (used
by the graphics function par) is set to 1 (axis labels always
horizontal), labels on the right axis are cropped when they
have more than two digits. It seems that the graphics window
does not allow for the extra space needed when scale labels
are horizontal and have more than two digits. Resizing the
graphics window does not make the graphs to show correctly.

Should outer regions of the graph, or figure regions, or figure
margins be somehow redefined when las = 1?

I use R 2.3.1, running under Windows XP. My monitor is a Samsung
SyncMaster 794MB+, resolution is 1024 x 768 pixels. (The problem
also occurs at the lower resolution 800 x 600 pixels).

Some simple examples - barplot:

# right scale shows correctly, labels are vertical
y <- c(100,100,100,150,150,150,150)
barplot(y, ylim=c(0,200), axes=FALSE)
box()
axis(side=4)

# the rightmost digit of (horizontal) scale labels is cropped
y <- c(100,100,100,150,150,150,150)
barplot(y, ylim=c(0,200), axes=FALSE)
box()
axis(side=4,las=1)

Some simple examples - plot:

# right scale shows correctly, labels are vertical
x <- runif(25,0,2000)
y <- runif(25,0,2000)
plot(x,y,axes=FALSE)
box()
axis(side=4)

# the two rightmost digits of (horizontal) scale labels are cropped
x <- runif(25,0,2000)
y <- runif(25,0,2000)
plot(x,y,axes=FALSE)
box()
axis(side=4,las=1)

Regards,

Paulo Barata

-------------------------------------------------------------------
Paulo Barata
Fundacao Oswaldo Cruz
Rua Leopoldo Bulhoes 1480 - 8A
21041-210  Rio de Janeiro - RJ
Brasil
Fax: 55-21-2232-9218
E-mail: pbarata at infolink.com.br


From wwguocn at gmail.com  Tue Jul  4 05:38:31 2006
From: wwguocn at gmail.com (Guo Wei-Wei)
Date: Tue, 4 Jul 2006 11:38:31 +0800
Subject: [R] Problems on testing moderating effect (or interactive effect).
Message-ID: <d3677d7d0607032038y1d2f977ciae2554eaa4f2f7f@mail.gmail.com>

Hi everyone,

I want to do test on moderating effect. I have three factors, A, B,
and C. A has influence on B, and C moderating the influence.  The
relationship looks like this:

A -----> B
     ^
     |
    C

A, B, and C are all scale variables. I think I can test the moderating
effect by adding a interactive variable between A and C. But I'm not
sure how to do.

Is there a default way to do it in package sem?

I'm also thinking about create a interaction variable of A and C, but
I don't know how to it. A has n (n = 27) items and p (p = 288) cases
and C has m (m = 16) iterms and p (p = 288) cases.

Does anyone have any suggestion? Thanks in advance.


From bruno.giordano at music.mcgill.ca  Tue Jul  4 07:30:03 2006
From: bruno.giordano at music.mcgill.ca (Bruno L. Giordano)
Date: Tue, 04 Jul 2006 01:30:03 -0400
Subject: [R] analogue of group option of SAS MIXED/random in R
References: <004501c69eeb$a12cd190$6400a8c0@brungio>
Message-ID: <008b01c69f2a$e88a5320$6400a8c0@brungio>

Well,
just in case somebody is interested, the following R code gives the same
estimates as the SAS code below:

######R code###########
G.Data<-groupedData(Response~1|Subject,data=In.Data)
G.Data$Condition<-as.ordered(G.Data$Condition)
G.Data$Const<-rep(1,length(Variable1))
tmp<-pdDiag(~Condition:Const+Condition:Variable1+Condition:Variable2-1)
Fit1<-lme(Response~1+Variable1+Variable2*Condition,
random=tmp,
method="REML",data=G.Data)


########SAS code########
proc mixed data=InData;class Subject Condition;
model Response=Variable1 Variable2 Condition Variable2*Condition;
random int Variable1 Variable2/subject = Subject group=Condition type=vc;
run;



    Bruno



----- Original Message ----- 
From: "Bruno L. Giordano" <bruno.giordano at music.mcgill.ca>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, July 03, 2006 5:56 PM
Subject: [R] analogue of group option of SAS MIXED/random in R


> Dear list,
>
> I am trying to use lme to build the analogue of the following SAS MIXED
> random specification:
>
> random int+Variable1+Variable2 /subject = Subject group=Condition type=vc;
>
> which gives a Condition-blocked heterogeneity in the random effects
> variance-covariance matrix.
>
> Needless to say, I have a hard time in specifying Condition-specific
> heterogeneities in the variance-covariance parameters.
>
> I initially tried the following commands (without Condition-heterogeneity
> in
> the random effects):
>
> G.Data<-groupedData(Response~1|Subject,data=In.Data)
> Fit1<-lme(Response~1+Variable1+Variable2*Condition,random=pdDiag(~1+Variable1+Variable2),method="REML",data=G.Data)
>
> but have no idea about where to go from here (note that I don't want to
> nest
> Subject in Condition).
>
> Thanks!!
>
>    Bruno
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Bruno L. Giordano, PhD
CIRMMT
Schulich School of Music, McGill University
555 Sherbrooke Street West
Montr?al, QC H3A 1E3
Canada
http://www.music.mcgill.ca/~bruno/


From spluque at gmail.com  Tue Jul  4 07:38:46 2006
From: spluque at gmail.com (Sebastian Luque)
Date: Tue, 04 Jul 2006 00:38:46 -0500
Subject: [R] randomization test for a two-way ANOVA
Message-ID: <873bdi2al5.fsf@arctocephalus.homelinux.org>

Hi,

I've looked into ways of implementing this procedure, i.e. repeating the
two-way ANOVA many times, scrambling the order of cases across the
treatments, to produce a distribution of F ratios for each effect.  This
seemed a job for the 'boot' package.  However, I'm not sure I'm doing an
actual randomization test, as opposed to a bootstrap here.  This is how
I've coded the test (using the poisons data):


---<---------------cut here---------------start-------------->---
require(boot)

"boot.lm" <- function(data, i)
{
    mod <- lm(time ~ treat * poison, data=data[i, ])
    anova(mod)["F value"][[1]][-4]      # the F ratios for each effect
}

poisons.boot <- boot(poisons, boot.lm, R=1000, sim="permutation")
---<---------------cut here---------------end---------------->---


Is this the right way to ask for a randomization test using 'boot'?
Thanks in advance.


Cheers,

-- 
Seb


From aironyiu at yahoo.com.hk  Mon Jul  3 10:41:31 2006
From: aironyiu at yahoo.com.hk (Airon Yiu)
Date: Mon, 3 Jul 2006 16:41:31 +0800 (CST)
Subject: [R] Harmonic Regression in R
Message-ID: <20060703084131.34811.qmail@web53811.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060703/7af7ff62/attachment.pl 

From rashmi6380 at yahoo.co.uk  Tue Jul  4 08:22:54 2006
From: rashmi6380 at yahoo.co.uk (Rashmi Pant)
Date: Tue, 4 Jul 2006 07:22:54 +0100 (BST)
Subject: [R] IMPORTING FILE FROM EXCEL
Message-ID: <20060704062254.89668.qmail@web26015.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060704/b8cdc80e/attachment.pl 

From ligges at statistik.uni-dortmund.de  Tue Jul  4 08:38:55 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 04 Jul 2006 08:38:55 +0200
Subject: [R] IMPORTING FILE FROM EXCEL
In-Reply-To: <20060704062254.89668.qmail@web26015.mail.ukl.yahoo.com>
References: <20060704062254.89668.qmail@web26015.mail.ukl.yahoo.com>
Message-ID: <44AA0CFF.4060207@statistik.uni-dortmund.de>

Rashmi Pant wrote:
> Hi 
>   I am a beginner with R.
>   I have been trying to import a tab delimited excel file but i get the following error message

Tab delimited is not Excel.

>   > file.show('C:\Documents and Settings\stats\Desktop\SUMI\plasma2.txt')

See R for Windows FAQ "R can't find my file, but I know it is there!"

Uwe Ligges



> Warning message:
> file.show(): file 'C:Documents and SettingsstatsDesktopSUMIplasma2.txt' does not exist 
>  I have understood the programming part but i cannot go ahead unless i have imported the file. I have consulted the R-help archive without success.
>   Any help will be appreciated
>   Thanks in advance
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ligges at statistik.uni-dortmund.de  Tue Jul  4 08:43:40 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 04 Jul 2006 08:43:40 +0200
Subject: [R] xlab, ylab in balloonplot(tab)?
In-Reply-To: <2a9c000c0607031605j2dc62dd0ne16891fe70165563@mail.gmail.com>
References: <2a9c000c0607031252x5036e569k71a86b396b71e8e3@mail.gmail.com>
	<2a9c000c0607031605j2dc62dd0ne16891fe70165563@mail.gmail.com>
Message-ID: <44AA0E1C.4070202@statistik.uni-dortmund.de>

Jim Porzak wrote:
> Dear ListRs,
> 
> I'm not understanding something.
> 
> I'm trying to add xlab & ylab to a balloon plot of a table object. From
> gplots docs I thought following should work:
> 
> require(gplots)
> # From balloonplot example:
>      # Create an example using table
>      xnames <- sample( letters[1:3], 50, replace=2)
>      ynames <- sample( 1:5, 50, replace=2)
> 
>      tab <- table(xnames, ynames)
> 
>      balloonplot(tab)
> 
> # Try xlab, ylab:
> balloonplot(tab, xlab = "MyX", ylab = "MyY")
> 
> 
> But second plot is no different from first.
> 
> R.version.string: Version 2.3.1 (2006-06-01)
> gplots version: 2.3.0
> on WinXP SP1
> 


Please send feature requests and bug reports of contributed packages to 
the package maintainer (CCing) rather than to R-help.

Uwe Ligges


From ligges at statistik.uni-dortmund.de  Tue Jul  4 08:46:01 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 04 Jul 2006 08:46:01 +0200
Subject: [R] parameter las (function par / graphics) on right axis
In-Reply-To: <44A9DF61.5020007@infolink.com.br>
References: <mailman.9.1151488803.12054.r-help@stat.math.ethz.ch>
	<44A9DF61.5020007@infolink.com.br>
Message-ID: <44AA0EA9.3050900@statistik.uni-dortmund.de>

Paulo Barata wrote:
> Dear R users,
> 
> When trying to produce a graph with a scale on the right axis,
> I came across a possible problem with graph making in R.
> 
> When using plot or barplot, when the parameter las (used
> by the graphics function par) is set to 1 (axis labels always
> horizontal), labels on the right axis are cropped when they
> have more than two digits. It seems that the graphics window
> does not allow for the extra space needed when scale labels
> are horizontal and have more than two digits. Resizing the
> graphics window does not make the graphs to show correctly.
> 
> Should outer regions of the graph, or figure regions, or figure
> margins be somehow redefined when las = 1?


Yes, you can specify margins with par("las" = ....). See ?par for details.

Example:
  y <- c(100,100,100,150,150,150,150)
  par(mar = c(1,1,1,4) + 0.1)
  barplot(y, ylim=c(0,200), axes=FALSE)
  box()
  axis(side=4,las=1)


Uwe Ligges



> I use R 2.3.1, running under Windows XP. My monitor is a Samsung
> SyncMaster 794MB+, resolution is 1024 x 768 pixels. (The problem
> also occurs at the lower resolution 800 x 600 pixels).
> 
> Some simple examples - barplot:
> 
> # right scale shows correctly, labels are vertical
> y <- c(100,100,100,150,150,150,150)
> barplot(y, ylim=c(0,200), axes=FALSE)
> box()
> axis(side=4)
> 
> # the rightmost digit of (horizontal) scale labels is cropped
> y <- c(100,100,100,150,150,150,150)
> barplot(y, ylim=c(0,200), axes=FALSE)
> box()
> axis(side=4,las=1)
> 
> Some simple examples - plot:
> 
> # right scale shows correctly, labels are vertical
> x <- runif(25,0,2000)
> y <- runif(25,0,2000)
> plot(x,y,axes=FALSE)
> box()
> axis(side=4)
> 
> # the two rightmost digits of (horizontal) scale labels are cropped
> x <- runif(25,0,2000)
> y <- runif(25,0,2000)
> plot(x,y,axes=FALSE)
> box()
> axis(side=4,las=1)
> 
> Regards,
> 
> Paulo Barata
> 
> -------------------------------------------------------------------
> Paulo Barata
> Fundacao Oswaldo Cruz
> Rua Leopoldo Bulhoes 1480 - 8A
> 21041-210  Rio de Janeiro - RJ
> Brasil
> Fax: 55-21-2232-9218
> E-mail: pbarata at infolink.com.br
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From jacques.veslot at good.ibl.fr  Tue Jul  4 09:25:56 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Tue, 04 Jul 2006 09:25:56 +0200
Subject: [R] IMPORTING FILE FROM EXCEL
In-Reply-To: <20060704062254.89668.qmail@web26015.mail.ukl.yahoo.com>
References: <20060704062254.89668.qmail@web26015.mail.ukl.yahoo.com>
Message-ID: <44AA1804.60806@good.ibl.fr>

if you don't need to import several dataset, you can directly change current directory from the file 
menu and move to your working directory instead of substuting all backslashes for slashes in import 
function argument.
you can also copy your dataset in excel and do :
 > data1 <- read.delim("clipboard")
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Rashmi Pant a ?crit :
> Hi 
>   I am a beginner with R.
>   I have been trying to import a tab delimited excel file but i get the following error message
>   > file.show('C:\Documents and Settings\stats\Desktop\SUMI\plasma2.txt')
> Warning message:
> file.show(): file 'C:Documents and SettingsstatsDesktopSUMIplasma2.txt' does not exist 
>  I have understood the programming part but i cannot go ahead unless i have imported the file. I have consulted the R-help archive without success.
>   Any help will be appreciated
>   Thanks in advance
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From epistat at gmail.com  Tue Jul  4 09:56:17 2006
From: epistat at gmail.com (zhijie zhang)
Date: Tue, 4 Jul 2006 15:56:17 +0800
Subject: [R] random sampling problems?
Message-ID: <2fc17e30607040056l1e211c93s4dbfed206d148c70@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060704/a172cf1f/attachment.pl 

From epistat at gmail.com  Tue Jul  4 09:57:41 2006
From: epistat at gmail.com (zhijie zhang)
Date: Tue, 4 Jul 2006 15:57:41 +0800
Subject: [R] how do we sample in spatial statistics?
Message-ID: <2fc17e30607040057i42a15f27o69913f2b04011b38@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060704/e5d14a90/attachment.pl 

From jacques.veslot at good.ibl.fr  Tue Jul  4 10:22:15 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Tue, 04 Jul 2006 10:22:15 +0200
Subject: [R] random sampling problems?
In-Reply-To: <2fc17e30607040056l1e211c93s4dbfed206d148c70@mail.gmail.com>
References: <2fc17e30607040056l1e211c93s4dbfed206d148c70@mail.gmail.com>
Message-ID: <44AA2537.1050705@good.ibl.fr>

with replacement or not ?

without replacement:
data1 <- cbind(id=1:9, expand.grid(x=1:3,y=1:3))
merge(data1, sapply(data1[,c("x","y")], sample, 3), all.y=T)

why not:
data1[sample(data1$id, 3),]
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


zhijie zhang a ?crit :
> Dear friends,
>  suppose my dataset is the following data:
> 
> id<-1:9
> x<-c(1,2,3,1,2,3,1,2,3)
> y<-c(1,1,1,2,2,2,3,3,3)
> data<-data.frame(id,x,y)
> 
>      id  x   y
> 1  1   1   1
> 2  2   2   1
> 3  3   3   1
> 4  4   1   2
> 5  5   2   2
> 6  6   3   2
> 7  7   1   3
> 8  8   2   3
> 9  9   3   3
> i want to do sampling like this:say the sample size is 3.
>  First: random sampling from x;
> Next ,random sampling from y ;and combing sampled x and sampled y;
> Finally, output the samples: id x and y.
> I think i could call it two-dimension sampling.
> Thanks very much!
> 
> 
>


From Ted.Harding at nessie.mcc.ac.uk  Tue Jul  4 10:30:04 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 04 Jul 2006 09:30:04 +0100 (BST)
Subject: [R] random sampling problems?
In-Reply-To: <2fc17e30607040056l1e211c93s4dbfed206d148c70@mail.gmail.com>
Message-ID: <XFMail.060704093004.Ted.Harding@nessie.mcc.ac.uk>

On 04-Jul-06 zhijie zhang wrote:
> Dear friends,
>  suppose my dataset is the following data:
> 
> id<-1:9
> x<-c(1,2,3,1,2,3,1,2,3)
> y<-c(1,1,1,2,2,2,3,3,3)
> data<-data.frame(id,x,y)
> 
>      id  x   y
> 1  1   1   1
> 2  2   2   1
> 3  3   3   1
> 4  4   1   2
> 5  5   2   2
> 6  6   3   2
> 7  7   1   3
> 8  8   2   3
> 9  9   3   3
> i want to do sampling like this:say the sample size is 3.
>  First: random sampling from x;
> Next ,random sampling from y ;and combing sampled x and sampled y;
> Finally, output the samples: id x and y.
> I think i could call it two-dimension sampling.
> Thanks very much!

I'm not quite sure what you are asking, but perhaps you can confirm
that it corresponds to the following, which is what you seem to mean
according to your wording.

You want an output which consists of the following:

  a random sample of 3 from the list of values of x
  an independent random sample of 3 from the list of values of y
  the list of the id values for the sampled values of x
  the list of the id values for the sampled values of y

  where the sampling is without replacement (default for "sample")

If that is the case, then

  idx<-sample(data$id,3)
  idx
# [1] 6 8 9
  idy<-sample(data$id,3)
  idy
# [1] 7 2 3
  sampx<-data$x[idx]
  sampx
# [1] 3 2 3
  sampy<-data$y[idy]
  sampy
# [1] 3 1 1
  samples<-data.frame(sampx,sampy,idx,idy)
  samples
#   sampx sampy idx idy
# 1     3     3   6   7
# 2     2     1   8   2
# 3     3     1   9   3

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 04-Jul-06                                       Time: 09:30:00
------------------------------ XFMail ------------------------------


From lindners at umich.edu  Tue Jul  4 10:52:38 2006
From: lindners at umich.edu (Stephan Lindner)
Date: Tue, 4 Jul 2006 04:52:38 -0400
Subject: [R] using weights in lrm
Message-ID: <20060704085238.GC22924@umich.edu>

Dear all,


just a quick question regarding weights in logistic regression. I do 



results <- lrm(y.js ~
                h.hhsize             
               + h.death1              
               + h.ill1                  
               + h.ljob1              
               + h.fin1 
               + h.div1 
               + h.fail1 
               + h.sex
               + h.ch.1      
               + h.ch.5      
               + h.ch.12     
               + h.ch.13     
               + h.popgroup
               + y.school.now
               ,x=T,y=T, data=d.caps1y, weights=weightsd, normwt=TRUE
                )


The regression works (in the sense that the results are not way off
the one w/o wighting the sample), but I get the following warning messages:

Warning messages:
1: number of items to replace is not a multiple of replacement length 
2: currently weights are ignored in model validation and bootstrapping lrm fits in: lrm(y.js ~ h.hhsize + h.death1 + h.ill1 + h.ljob1 + h.fin1 +  


Perhaps someone can help me clearifying the warning messages -- thanks
a lot in advance !


Cheers,

Stephan

	


-- 
-----------------------
Stephan Lindner, Dipl.Vw.
Doctoral Student in Economics
1512 Gilbert Ct., V-17
Ann Arbor, Michigan 48105
U.S.A.
Tel.: 001-734-272-2437
E-Mail: lindners at umich.edu

"If I have seen further it is by standing on the shoulders of giants."
-- Isaac Newton


From vincent at 7d4.com  Tue Jul  4 11:43:19 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Tue, 04 Jul 2006 11:43:19 +0200
Subject: [R] removing for loop
Message-ID: <44AA3837.7050504@7d4.com>

Dear Rusers,

Trying to reduce my for loops addiction,
could somebody tell me if there are ways to simplify
(and perhaps accelerate ?) the following line

for (i in 1:N) for (j in 1:N) m[i,j] = b[i]-b[j];

(where m is a NxN matrix and b a vector of length N)

Thanks for any hint.


From p.dalgaard at biostat.ku.dk  Tue Jul  4 11:49:14 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Jul 2006 11:49:14 +0200
Subject: [R] removing for loop
In-Reply-To: <44AA3837.7050504@7d4.com>
References: <44AA3837.7050504@7d4.com>
Message-ID: <x2lkr9r97p.fsf@viggo.kubism.ku.dk>

vincent at 7d4.com writes:

> Dear Rusers,
> 
> Trying to reduce my for loops addiction,
> could somebody tell me if there are ways to simplify
> (and perhaps accelerate ?) the following line
> 
> for (i in 1:N) for (j in 1:N) m[i,j] = b[i]-b[j];
> 
> (where m is a NxN matrix and b a vector of length N)
> 
> Thanks for any hint.

m <- outer(b, b, "-") 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From r_econometrics at yahoo.co.in  Tue Jul  4 11:49:38 2006
From: r_econometrics at yahoo.co.in (Sumanta Basak)
Date: Tue, 4 Jul 2006 10:49:38 +0100 (BST)
Subject: [R] Column Selection
Message-ID: <20060704094938.59974.qmail@web7608.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060704/26ec2ad9/attachment.pl 

From ligges at statistik.uni-dortmund.de  Tue Jul  4 11:52:49 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 04 Jul 2006 11:52:49 +0200
Subject: [R] removing for loop
In-Reply-To: <44AA3837.7050504@7d4.com>
References: <44AA3837.7050504@7d4.com>
Message-ID: <44AA3A71.4000806@statistik.uni-dortmund.de>

vincent at 7d4.com wrote:
> Dear Rusers,
> 
> Trying to reduce my for loops addiction,
> could somebody tell me if there are ways to simplify
> (and perhaps accelerate ?) the following line
> 
> for (i in 1:N) for (j in 1:N) m[i,j] = b[i]-b[j];


outer(b, b, "-")

Uwe Ligges


> (where m is a NxN matrix and b a vector of length N)
> 
> Thanks for any hint.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From dimitris.rizopoulos at med.kuleuven.be  Tue Jul  4 11:56:49 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 4 Jul 2006 11:56:49 +0200
Subject: [R] removing for loop
References: <44AA3837.7050504@7d4.com>
Message-ID: <006901c69f50$2b27da70$0540210a@www.domain>

try this:

m <- outer(b, b, "-")


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: <vincent at 7d4.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, July 04, 2006 11:43 AM
Subject: [R] removing for loop


> Dear Rusers,
>
> Trying to reduce my for loops addiction,
> could somebody tell me if there are ways to simplify
> (and perhaps accelerate ?) the following line
>
> for (i in 1:N) for (j in 1:N) m[i,j] = b[i]-b[j];
>
> (where m is a NxN matrix and b a vector of length N)
>
> Thanks for any hint.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From stecalza at tiscali.it  Tue Jul  4 11:58:46 2006
From: stecalza at tiscali.it (Stefano Calza)
Date: Tue, 4 Jul 2006 11:58:46 +0200
Subject: [R] removing for loop
In-Reply-To: <44AA3837.7050504@7d4.com>
References: <44AA3837.7050504@7d4.com>
Message-ID: <20060704095846.GB3986@med.unibs.it>

I guess outer(b,b,"-") gives what you want

HIH,
Stefano


On Tue, Jul 04, 2006 at 11:43:19AM +0200, vincent a 7d4.com wrote:
<vincent>Dear Rusers,
<vincent>
<vincent>Trying to reduce my for loops addiction,
<vincent>could somebody tell me if there are ways to simplify
<vincent>(and perhaps accelerate ?) the following line
<vincent>
<vincent>for (i in 1:N) for (j in 1:N) m[i,j] = b[i]-b[j];
<vincent>
<vincent>(where m is a NxN matrix and b a vector of length N)
<vincent>
<vincent>Thanks for any hint.
<vincent>
<vincent>______________________________________________
<vincent>R-help a stat.math.ethz.ch mailing list
<vincent>https://stat.ethz.ch/mailman/listinfo/r-help
<vincent>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From vincent at 7d4.com  Tue Jul  4 12:08:11 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Tue, 04 Jul 2006 12:08:11 +0200
Subject: [R] removing for loop
In-Reply-To: <44AA3837.7050504@7d4.com>
References: <44AA3837.7050504@7d4.com>
Message-ID: <44AA3E0B.6030602@7d4.com>

Thank you very much to everybody
for your answer.


From maechler at stat.math.ethz.ch  Tue Jul  4 12:23:17 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 4 Jul 2006 12:23:17 +0200
Subject: [R] read.table() features {was "difficult data .."}
In-Reply-To: <971536df0607031358u35b537cfyb04df06d8c8c0b12@mail.gmail.com>
References: <1432347.5012871151959078685.JavaMail.root@vms168.mailsrvcs.net>
	<971536df0607031358u35b537cfyb04df06d8c8c0b12@mail.gmail.com>
Message-ID: <17578.16789.903874.664299@stat.math.ethz.ch>

>>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at gmail.com>
>>>>>     on Mon, 3 Jul 2006 16:58:14 -0400 writes:

    Gabor> Try this:

    Gabor> # test data
    Gabor> # read in header separately so R does not make column names unique

    Gabor> Lines <- "AAA BBB CCC DDD AAA BBB
    Gabor> 0      2      1     2      0      0
    Gabor> 2      3      7     6      0      1
    Gabor> 1.5    4      9     9      6      0
    Gabor> 1.0    6      10    11     3      3
    Gabor> "
    Gabor> DF <- read.table(textConnection(Lines), skip = 1)
    Gabor> names(DF) <- scan(textConnection(Lines), what = "", nlines = 1)

Hmm, this is unnecessarily slightly complicated.
Instead, rather make use of read.table()'s capabilities, by

   DF <- read.table(textConnection(Lines), check.names=FALSE, header=TRUE)
   ##                                      ^^^^^^^^^^^^^^^^^

Martin Maechler, ETH Zurich


From lanre.okusanya at gmail.com  Tue Jul  4 12:24:40 2006
From: lanre.okusanya at gmail.com (Lanre Okusanya)
Date: Tue, 4 Jul 2006 06:24:40 -0400
Subject: [R] IMPORTING FILE FROM EXCEL
In-Reply-To: <44AA1804.60806@good.ibl.fr>
References: <20060704062254.89668.qmail@web26015.mail.ukl.yahoo.com>
	<44AA1804.60806@good.ibl.fr>
Message-ID: <6e25bb420607040324j4df6615exf48befbc8a02f010@mail.gmail.com>

try

read.table('C:\\Documents and Settings\\stats\\Desktop\\SUMI\\plasma2.txt')
for file.show, it is the same except note the slashes.

On 7/4/06, Jacques VESLOT <jacques.veslot at good.ibl.fr> wrote:
> if you don't need to import several dataset, you can directly change current
> directory from the file
> menu and move to your working directory instead of substuting all
> backslashes for slashes in import
> function argument.
> you can also copy your dataset in excel and do :
>  > data1 <- read.delim("clipboard")
> -------------------------------------------------------------------
> Jacques VESLOT
>
> CNRS UMR 8090
> I.B.L (2?me ?tage)
> 1 rue du Professeur Calmette
> B.P. 245
> 59019 Lille Cedex
>
> Tel : 33 (0)3.20.87.10.44
> Fax : 33 (0)3.20.87.10.31
>
> http://www-good.ibl.fr
> -------------------------------------------------------------------
>
>
> Rashmi Pant a ?crit :
> > Hi
> >   I am a beginner with R.
> >   I have been trying to import a tab delimited excel file but i get the
> following error message
> >   > file.show('C:\Documents and Settings\stats\Desktop\SUMI\plasma2.txt')
> > Warning message:
> > file.show(): file 'C:Documents and SettingsstatsDesktopSUMIplasma2.txt'
> does not exist
> >  I have understood the programming part but i cannot go ahead unless i
> have imported the file. I have consulted the R-help archive without success.
> >   Any help will be appreciated
> >   Thanks in advance
> >
> >  		
> > ---------------------------------
> >
> > 	[[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


From murdoch at stats.uwo.ca  Tue Jul  4 12:28:51 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 04 Jul 2006 06:28:51 -0400
Subject: [R] IMPORTING FILE FROM EXCEL
In-Reply-To: <20060704062254.89668.qmail@web26015.mail.ukl.yahoo.com>
References: <20060704062254.89668.qmail@web26015.mail.ukl.yahoo.com>
Message-ID: <44AA42E3.2070402@stats.uwo.ca>

On 7/4/2006 2:22 AM, Rashmi Pant wrote:
> Hi 
>   I am a beginner with R.
>   I have been trying to import a tab delimited excel file but i get the following error message
>   > file.show('C:\Documents and Settings\stats\Desktop\SUMI\plasma2.txt')

R treats backslashes ("\") as escape characters.  It's easier just to 
use forward slashes, i.e. c:/Documents..., and easiest of all is to use 
the function file.choose() which pops up a file selection dialog.

Duncan Murdoch

> Warning message:
> file.show(): file 'C:Documents and SettingsstatsDesktopSUMIplasma2.txt' does not exist 
>  I have understood the programming part but i cannot go ahead unless i have imported the file. I have consulted the R-help archive without success.
>   Any help will be appreciated
>   Thanks in advance
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From baron at psych.upenn.edu  Tue Jul  4 13:09:06 2006
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 4 Jul 2006 07:09:06 -0400
Subject: [R] Problems on testing moderating effect (or interactive
	effect).
In-Reply-To: <d3677d7d0607032038y1d2f977ciae2554eaa4f2f7f@mail.gmail.com>
References: <d3677d7d0607032038y1d2f977ciae2554eaa4f2f7f@mail.gmail.com>
Message-ID: <20060704110906.GA23027@psych.upenn.edu>

On 07/04/06 11:38, Guo Wei-Wei wrote:
> Hi everyone,
> 
> I want to do test on moderating effect. I have three factors, A, B,
> and C. A has influence on B, and C moderating the influence.  The
> relationship looks like this:
> 
> A -----> B
>      ^
>      |
>     C
> 
> A, B, and C are all scale variables. I think I can test the moderating
> effect by adding a interactive variable between A and C. But I'm not
> sure how to do.
> 
> Is there a default way to do it in package sem?
> 
> I'm also thinking about create a interaction variable of A and C, but
> I don't know how to it. A has n (n = 27) items and p (p = 288) cases
> and C has m (m = 16) iterms and p (p = 288) cases.

Moderation is usually tested with an interaction.  You would use
lm() not sem.  For example,

summary(lm(B ~ A*C))

which will report the main effects of A and C as well as their
interaction.  (Of course, main effects may be meaningless if
there is an interaction.)  See the help page for formula.

So far I'm assuming that you are interested in individual
differences (cases).  So A, B, and C would be the means of each
case.  If, for example, A is actually a matrix in which each row
is a case, you would use something like rowMeans(A), etc., for
each variable, so you could say

summary(lm(rowMeans(B) ~ rowMeans(A)*rowMeans(C)))

(or else compute each of these first).

However, you may be interested in moderation WITHIN cases, across 
items.

If you look up moderation on Google, you find

http://davidakenny.net/cm/moderation.htm

which cites

Judd, C. M., Kenny, D. A., & McClelland, G. H. (2001). Estimating 
and testing mediation and moderation in within-participant
designs. Psychological Methods, 6, 115-134. 

I have not read this article, but other articles by the same
authors are both clear and well reasoned.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
Editor: Judgment and Decision Making (http://journal.sjdm.org)


From wwguocn at gmail.com  Tue Jul  4 13:51:02 2006
From: wwguocn at gmail.com (Guo Wei-Wei)
Date: Tue, 4 Jul 2006 19:51:02 +0800
Subject: [R] Problems on testing moderating effect (or interactive
	effect).
In-Reply-To: <20060704110906.GA23027@psych.upenn.edu>
References: <d3677d7d0607032038y1d2f977ciae2554eaa4f2f7f@mail.gmail.com>
	<20060704110906.GA23027@psych.upenn.edu>
Message-ID: <d3677d7d0607040451j3c6a9753ma120c2452113b613@mail.gmail.com>

Thank you Jonathan,

Can I use the variance-covariance matrix as the input data? Just like
what SEM does. My mentor told me to avoid sperate the operation into
two step, that is to get the factors' means first and then to test the
relationships. I'm used to use sem package. I'm not familiar with
lm(). I trid summary(lm(B ~ A*C)) and failed to get any result.

Can sem deal with mediation? And could you tell me the command of
generating a interaction item of A (nxp) and C (mxp)?

And you give a nice reference. Thank you very much!

2006/7/4, Jonathan Baron <baron at psych.upenn.edu>:
> On 07/04/06 11:38, Guo Wei-Wei wrote:
> > Hi everyone,
> >
> > I want to do test on moderating effect. I have three factors, A, B,
> > and C. A has influence on B, and C moderating the influence.  The
> > relationship looks like this:
> >
> > A -----> B
> >      ^
> >      |
> >     C
> >
> > A, B, and C are all scale variables. I think I can test the moderating
> > effect by adding a interactive variable between A and C. But I'm not
> > sure how to do.
> >
> > Is there a default way to do it in package sem?
> >
> > I'm also thinking about create a interaction variable of A and C, but
> > I don't know how to it. A has n (n = 27) items and p (p = 288) cases
> > and C has m (m = 16) iterms and p (p = 288) cases.
>
> Moderation is usually tested with an interaction.  You would use
> lm() not sem.  For example,
>
> summary(lm(B ~ A*C))
>
> which will report the main effects of A and C as well as their
> interaction.  (Of course, main effects may be meaningless if
> there is an interaction.)  See the help page for formula.
>
> So far I'm assuming that you are interested in individual
> differences (cases).  So A, B, and C would be the means of each
> case.  If, for example, A is actually a matrix in which each row
> is a case, you would use something like rowMeans(A), etc., for
> each variable, so you could say
>
> summary(lm(rowMeans(B) ~ rowMeans(A)*rowMeans(C)))
>
> (or else compute each of these first).
>
> However, you may be interested in moderation WITHIN cases, across
> items.
>
> If you look up moderation on Google, you find
>
> http://davidakenny.net/cm/moderation.htm
>
> which cites
>
> Judd, C. M., Kenny, D. A., & McClelland, G. H. (2001). Estimating
> and testing mediation and moderation in within-participant
> designs. Psychological Methods, 6, 115-134.
>
> I have not read this article, but other articles by the same
> authors are both clear and well reasoned.
>
> --
> Jonathan Baron, Professor of Psychology, University of Pennsylvania
> Home page: http://www.sas.upenn.edu/~baron
> Editor: Judgment and Decision Making (http://journal.sjdm.org)
>


From jsorkin at grecc.umaryland.edu  Tue Jul  4 14:47:35 2006
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 04 Jul 2006 08:47:35 -0400
Subject: [R] IMPORTING FILE FROM EXCEL
Message-ID: <s4aa2b43.043@MEDICINE.umaryland.edu>

Rashmi,
I suggest you store the file as a comma separated file (.CSV). You can
import a .csv file using the read.csv function
data<-read.csv("d:\\myfile.csvl")
I suggest you look at the help file for read.csv, i.e.
help(read.csv)
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC,
University of Maryland School of Medicine Claude D. Pepper OAIC,
University of Maryland Clinical Nutrition Research Unit, and
Baltimore VA Center Stroke of Excellence

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119
jsorkin at grecc.umaryland.edu

>>> Rashmi Pant <rashmi6380 at yahoo.co.uk> 07/04/06 2:22 AM >>>
Hi 
  I am a beginner with R.
  I have been trying to import a tab delimited excel file but i get the
following error message
  > file.show('C:\Documents and
Settings\stats\Desktop\SUMI\plasma2.txt')
Warning message:
file.show(): file 'C:Documents and SettingsstatsDesktopSUMIplasma2.txt'
does not exist 
 I have understood the programming part but i cannot go ahead unless i
have imported the file. I have consulted the R-help archive without
success.
  Any help will be appreciated
  Thanks in advance

 		
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From klemens.vierlinger at arcs.ac.at  Tue Jul  4 14:55:01 2006
From: klemens.vierlinger at arcs.ac.at (Klemens Vierlinger)
Date: Tue, 04 Jul 2006 14:55:01 +0200
Subject: [R] add plot to heatmap
Message-ID: <44AA6525.6090604@arcs.ac.at>

Dear useRs,

I was wondering if anyone could direct me to a way how I can add plots 
of covariates to a heatmap. I found heatmap_plus in the package 
Heatplus, which almost does the job but allows only one interval-scaled 
covariate to be plotted (is there a reason for this which I have 
overlooked?).

any help would be much appreciated!
thanks
Klemens


From f.harrell at vanderbilt.edu  Tue Jul  4 14:59:31 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 04 Jul 2006 07:59:31 -0500
Subject: [R] using weights in lrm
In-Reply-To: <20060704085238.GC22924@umich.edu>
References: <20060704085238.GC22924@umich.edu>
Message-ID: <44AA6633.2020507@vanderbilt.edu>

Stephan Lindner wrote:
> Dear all,
> 
> 
> just a quick question regarding weights in logistic regression. I do 
> 
> 
> 
> results <- lrm(y.js ~
>                 h.hhsize             
>                + h.death1              
>                + h.ill1                  
>                + h.ljob1              
>                + h.fin1 
>                + h.div1 
>                + h.fail1 
>                + h.sex
>                + h.ch.1      
>                + h.ch.5      
>                + h.ch.12     
>                + h.ch.13     
>                + h.popgroup
>                + y.school.now
>                ,x=T,y=T, data=d.caps1y, weights=weightsd, normwt=TRUE
>                 )
> 
> 
> The regression works (in the sense that the results are not way off
> the one w/o wighting the sample), but I get the following warning messages:
> 
> Warning messages:
> 1: number of items to replace is not a multiple of replacement length 
> 2: currently weights are ignored in model validation and bootstrapping lrm fits in: lrm(y.js ~ h.hhsize + h.death1 + h.ill1 + h.ljob1 + h.fin1 +  
> 
> 
> Perhaps someone can help me clearifying the warning messages -- thanks
> a lot in advance !

I think the second warning is clear.  Regarding the first, make sure 
that the weights vector has length equal to the number of rows in 
d.capsly.  Sometimes you have to subset weights.  If that's not the 
problem, try to create a minimal failing example and we'll work on it.

Frank Harrell

> 
> 
> Cheers,
> 
> Stephan
> 
> 	
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From p.dalgaard at biostat.ku.dk  Tue Jul  4 15:03:42 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Jul 2006 15:03:42 +0200
Subject: [R] IMPORTING FILE FROM EXCEL
In-Reply-To: <s4aa2b43.043@MEDICINE.umaryland.edu>
References: <s4aa2b43.043@MEDICINE.umaryland.edu>
Message-ID: <x2bqs5r07l.fsf@viggo.kubism.ku.dk>

"John Sorkin" <jsorkin at grecc.umaryland.edu> writes:

> Rashmi,
> I suggest you store the file as a comma separated file (.CSV). You can
> import a .csv file using the read.csv function
> data<-read.csv("d:\\myfile.csvl")
> I suggest you look at the help file for read.csv, i.e.
> help(read.csv)

Sorry, but this doesn't make sense. read.delim() reads tab-separated
files just as easily as read.csv() reads comma-separated ones (and in
both cases, there is a "2" variant for locales in which comma is used
as decimals separator as in 0,02 EUR).

As others have pointed out, the real issue was how to specify a
filename, and the use of the backslash escape. And Duncan M. pointed
towards file.choose() as a user-friendly helper on Windows.

> John
> 
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC,
> University of Maryland School of Medicine Claude D. Pepper OAIC,
> University of Maryland Clinical Nutrition Research Unit, and
> Baltimore VA Center Stroke of Excellence
> 
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> 
> 410-605-7119
> jsorkin at grecc.umaryland.edu
> 
> >>> Rashmi Pant <rashmi6380 at yahoo.co.uk> 07/04/06 2:22 AM >>>
> Hi 
>   I am a beginner with R.
>   I have been trying to import a tab delimited excel file but i get the
> following error message
>   > file.show('C:\Documents and
> Settings\stats\Desktop\SUMI\plasma2.txt')
> Warning message:
> file.show(): file 'C:Documents and SettingsstatsDesktopSUMIplasma2.txt'
> does not exist 
>  I have understood the programming part but i cannot go ahead unless i
> have imported the file. I have consulted the R-help archive without
> success.
>   Any help will be appreciated
>   Thanks in advance
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ivowel at gmail.com  Tue Jul  4 15:05:01 2006
From: ivowel at gmail.com (ivo welch)
Date: Tue, 4 Jul 2006 09:05:01 -0400
Subject: [R] curiosity question: new graphics vs. old graphics subsystem
In-Reply-To: <44A9D475.1070407@stat.auckland.ac.nz>
References: <50d1c22d0607011123x1fc06845oceb6cbaf008ee00@mail.gmail.com>
	<44A9D475.1070407@stat.auckland.ac.nz>
Message-ID: <50d1c22d0607040605l24475d41s3d57943c294799ed@mail.gmail.com>

hi paul:  thank you.  great explanation.

* what you describe seems like a fairly modest advantage of the
traditional system.  I have never used the new system, but this part
of the translation should be simple: if a plot() is called, set the
current viewport.  emulation done.  (if the user has issued a plot()
and then screws around with the viewport, R could complain and tell
the user that she is now on her own.)  I am probably way too naive
here.  there must be a lot trickier stuff.

* R regularly makes small changes that break old R code in order to
improve R.  This is good.  IMHO, emulating the old graphics system
would not have to be perfect one-to-one.

* when new graphics features are added, there must be duplication of
effort to get it into both graphics systems.  would it not be easier
in the long-run to have just one system to maintain?

* grImport and your slides look nice.  alas, I am searching for
grImport in your book, and it ain't there.  ;-).  also, pdf should be
as important as postscript to include these days.

* speed and memory size for graphics are less of an issue now than it
used to be.

sheesh, this almost sounds ungrateful.  don't mean to be.  thanks to
the R team for having put this graphics together.  it really is the
best thing out there.  this discrepancy in graphics systems is really
nothing more than a minor nuisance.

best,

/iaw


From baron at psych.upenn.edu  Tue Jul  4 15:15:33 2006
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 4 Jul 2006 09:15:33 -0400
Subject: [R] Problems on testing moderating effect (or interactive
	effect).
In-Reply-To: <d3677d7d0607040451j3c6a9753ma120c2452113b613@mail.gmail.com>
References: <d3677d7d0607032038y1d2f977ciae2554eaa4f2f7f@mail.gmail.com>
	<20060704110906.GA23027@psych.upenn.edu>
	<d3677d7d0607040451j3c6a9753ma120c2452113b613@mail.gmail.com>
Message-ID: <20060704131533.GA29438@psych.upenn.edu>

On 07/04/06 19:51, Guo Wei-Wei wrote:
> Can I use the variance-covariance matrix as the input data? Just like
> what SEM does. My mentor told me to avoid sperate the operation into
> two step, that is to get the factors' means first and then to test the
> relationships. I'm used to use sem package. I'm not familiar with
> lm(). I trid summary(lm(B ~ A*C)) and failed to get any result.
> 
> Can sem deal with mediation? And could you tell me the command of
> generating a interaction item of A (nxp) and C (mxp)?

I don't use sem, but I don't see why you need it for this.  This
is a simple regression problem, so far as I can tell.

I think you need to do some reading of the R documentation.  What
are A, B, and C?  They should be vectors.  That was the point of
my comment about rowMeans.

You seem to be guessing and relying on authority instead of
trying to understand.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron


From Jan.Davidsen at nfh.uit.no  Tue Jul  4 15:31:05 2006
From: Jan.Davidsen at nfh.uit.no (Jan Grimsrud Davidsen)
Date: Tue, 4 Jul 2006 15:31:05 +0200
Subject: [R] change a specific time into seconds since midnight
Message-ID: <A32B73A3D3273442B16562E7D4B2E6EF5C85E7@eden5.ad.uit.no>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060704/81dee887/attachment.pl 

From ggrothendieck at gmail.com  Tue Jul  4 15:47:00 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 4 Jul 2006 09:47:00 -0400
Subject: [R] change a specific time into seconds since midnight
In-Reply-To: <A32B73A3D3273442B16562E7D4B2E6EF5C85E7@eden5.ad.uit.no>
References: <A32B73A3D3273442B16562E7D4B2E6EF5C85E7@eden5.ad.uit.no>
Message-ID: <971536df0607040647q5df0681au6de7be4a443be4b2@mail.gmail.com>

Try this:

library(chron)
24 * 60 * 60 * times("10:49:48")


On 7/4/06, Jan Grimsrud Davidsen <Jan.Davidsen at nfh.uit.no> wrote:
> Does anyone know a code which can transfer a column with time into seconds since midnight?
>
> Example: (10:49:48) --> 38988
>
> Thanks :-)
>
>
>
> Jan Grimsrud Davidsen
>
> --------------------------------------------------------------------------------------
>
> PhD student
>
> Norwegian College of Fishery Science
> University of Troms?
>
>
>
>
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>


From lindners at umich.edu  Tue Jul  4 15:51:37 2006
From: lindners at umich.edu (Stephan Lindner)
Date: Tue, 4 Jul 2006 09:51:37 -0400
Subject: [R] using weights in lrm
In-Reply-To: <44AA6633.2020507@vanderbilt.edu>
References: <20060704085238.GC22924@umich.edu>
	<44AA6633.2020507@vanderbilt.edu>
Message-ID: <20060704135137.GA25114@umich.edu>


Dear all,

here's my own answer to the first warning message -- the warning
message comes from handling missing values, which is specified as
na.delte as default in lrm.

Cheers,

Stephan




# Consider a toy data frame:


> d.temp
   y.js h.hhsize h.work.frac h.age  h.sex h.popgroup weightsd cluster
1    No        3   0.3333333    20 female   Coloured 47.80062    1001
2    No        5   0.6000000    18 female   Coloured 47.80062    1001
3   Yes        4   0.7500000    18 female      White 47.80062    1001
4   Yes        6   0.5000000    21 female   Coloured 49.71264    1002
5    No        6   0.5000000    15 female   Coloured 49.71264    1002
6    No        3   0.6666667    20 female      White 49.71264    1002
7    No        3   0.3333333    21 female      White 49.71264    1002
8   Yes        6   0.6666667    19 female      White 49.71264    1002
9    No        6   0.6666667    16   male      White 49.71264    1002
10   No        3   0.3333333    16   male   Coloured 49.71264    1002
11   No        5   0.4000000    15   male   Coloured 42.85572    1003
12   No        6   0.6666667    18   male      White 42.85572    1003
13   No        4   0.2500000    17   male      White 45.88860    1004
14   No        3   0.3333333    15 female   Coloured 45.88860    1004
15   No        4   0.5000000    19 female      White 45.88860    1004
16  Yes        4   0.5000000    16 female      White 45.88860    1004
17  Yes        6   0.3333333    21 female   Coloured 45.88860    1004
18   No        3   0.6666667    15 female      White 46.03022    1005
19  Yes        5   0.4000000    20 female      White 46.03022    1005
20   No        5   1.0000000    19 female      White 46.03022    1005


# The dependent variable has no missing values. Then, lrm works fine. 

results <- robcov(ols.results <- lrm(y.js ~
                                     + h.hhsize             
                                     + h.work.frac           
                                     + factor(h.age)        
                                     + h.sex
                                     + h.popgroup           
                                     
                                    ,data=d.temp,x=T,y=T
                                    ,weights=weightsd, normwt=TRUE),
                                    d.temp$cluster)


# Now change the first observation to a missing value:

d.temp$y.js[1] <- NA

# and do the same again produces the warning:


results <- robcov(ols.results <- lrm(y.js ~
                                     + h.hhsize             
                                     + h.work.frac           
                                     + factor(h.age)        
                                     + h.sex
                                     + h.popgroup           
                                     
                                    ,data=d.temp,x=T,y=T
                                    ,weights=weightsd, normwt=TRUE),
                                    d.temp$cluster)



# But specifying na.action="exclude" resolves it.

results <- robcov(ols.results <- lrm(y.js ~
                                     + h.hhsize             
                                     + h.work.frac           
                                     + factor(h.age)        
                                     + h.sex
                                     + h.popgroup           
                                     
                                    ,data=d.temp,x=T,y=T, na.action="na.exclude"
                                    ,weights=weightsd, normwt=TRUE),
                                    d.temp$cluster)


# ------------------------------------------- #


On Tue, Jul 04, 2006 at 07:59:31AM -0500, Frank E Harrell Jr wrote:
> Stephan Lindner wrote:
> >Dear all,
> >
> >
> >just a quick question regarding weights in logistic regression. I do 
> >
> >
> >
> >results <- lrm(y.js ~
> >                h.hhsize             
> >               + h.death1              
> >               + h.ill1                  
> >               + h.ljob1              
> >               + h.fin1 
> >               + h.div1 
> >               + h.fail1 
> >               + h.sex
> >               + h.ch.1      
> >               + h.ch.5      
> >               + h.ch.12     
> >               + h.ch.13     
> >               + h.popgroup
> >               + y.school.now
> >               ,x=T,y=T, data=d.caps1y, weights=weightsd, normwt=TRUE
> >                )
> >
> >
> >The regression works (in the sense that the results are not way off
> >the one w/o wighting the sample), but I get the following warning messages:
> >
> >Warning messages:
> >1: number of items to replace is not a multiple of replacement length 
> >2: currently weights are ignored in model validation and bootstrapping lrm 
> >fits in: lrm(y.js ~ h.hhsize + h.death1 + h.ill1 + h.ljob1 + h.fin1 +  
> >
> >Perhaps someone can help me clearifying the warning messages -- thanks
> >a lot in advance !
> 
> I think the second warning is clear.  Regarding the first, make sure 
> that the weights vector has length equal to the number of rows in 
> d.capsly.  Sometimes you have to subset weights.  If that's not the 
> problem, try to create a minimal failing example and we'll work on it.
> 
> Frank Harrell
> 
> >
> >
> >Cheers,
> >
> >Stephan
> >
> >	
> >
> >
> 
> 
> -- 
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                      Department of Biostatistics   Vanderbilt University
> 
> 

-- 
-----------------------
Stephan Lindner, Dipl.Vw.
Doctoral Student in Economics
1512 Gilbert Ct., V-17
Ann Arbor, Michigan 48105
U.S.A.
Tel.: 001-734-272-2437
E-Mail: lindners at umich.edu

"If I have seen further it is by standing on the shoulders of giants."
-- Isaac Newton


From jholtman at gmail.com  Tue Jul  4 16:09:58 2006
From: jholtman at gmail.com (jim holtman)
Date: Tue, 4 Jul 2006 10:09:58 -0400
Subject: [R] Column Selection
In-Reply-To: <20060704094938.59974.qmail@web7608.mail.in.yahoo.com>
References: <20060704094938.59974.qmail@web7608.mail.in.yahoo.com>
Message-ID: <644e1f320607040709v46bbbc85kf9627b914e62bc92@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060704/1ce6cc82/attachment.pl 

From jc at or.psychology.dal.ca  Tue Jul  4 16:20:57 2006
From: jc at or.psychology.dal.ca (John Christie)
Date: Tue, 4 Jul 2006 11:20:57 -0300
Subject: [R] lmer print outs without T
Message-ID: <FBD652D4-D333-419E-9D7D-9B92C3DBD27C@or.psychology.dal.ca>

Hi,
	I have been having a tedious issue with lmer models with lots of  
factors and lots of levels.  In order to get the basic information at  
the beginning of the print out I also have to generate these enormous  
tables as well.  Is there a method command to leave off all of the  
effects and correlations?  Or, do I have to go to string commands?


From epistat at gmail.com  Tue Jul  4 16:44:38 2006
From: epistat at gmail.com (zhijie zhang)
Date: Tue, 4 Jul 2006 22:44:38 +0800
Subject: [R] who can explain the difference between the R and SAS on the
	results of GLM
Message-ID: <2fc17e30607040744u61cb4f8bo7bb2bf8ec23e2f32@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060704/19348c52/attachment.pl 

From markleeds at verizon.net  Tue Jul  4 17:03:14 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Tue, 04 Jul 2006 10:03:14 -0500 (CDT)
Subject: [R] [Fwd: formatting using the write statement]
Message-ID: <7176716.3400741152025394370.JavaMail.root@vms170.mailsrvcs.net>


>I have a series of write statements because
>i am writing  to a file
>where the characters strings are the column names of a dataframe
>and the numbers are the elements in a particular row.
>So, a file might look like
>
>AAA  2.1
>BB  3.1
>AHLZ 0.2
>
>and it would be named "rowname".mls.
>
>so, each time i get to a new row, i create a new file and write to it.
>
>the code is below and it works fine. my only question is
>there a way to tell the write statement that i want the data
>to be written in a fixed format ( same number of spaces
>betweewn character strings and numbers ) rather than written as it
>is above ? i tried sep = " ", instead of ncolumns butt hat
>just makde things worse. thanks.
>
>for (rownum in 1 (1:nrow(sortedForecastData)) {
>
>outfile=paste(forecsastfiledir,rownames(sortedForecastData[rownum],".mls",sep="")
>
>for colnum in (1:ncol(sortedForecastData) {
> 
>if ( colnum == 1 ) {
>
>write(c(colnames(sortedForecastData[colnum],sortedForecastData[rownum,colnum],file=outfile,ncolumns=2,append=FALSE) 
>
>} else {
>
>write(c(colnames(sortedForecastData)[colnum],sortedForecastData[rownum,colnum],file=outfile,ncolumns=2,append=TRUE )
>
>}
>}


From f.harrell at vanderbilt.edu  Tue Jul  4 17:08:07 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 04 Jul 2006 10:08:07 -0500
Subject: [R] using weights in lrm
In-Reply-To: <20060704135137.GA25114@umich.edu>
References: <20060704085238.GC22924@umich.edu>
	<44AA6633.2020507@vanderbilt.edu>
	<20060704135137.GA25114@umich.edu>
Message-ID: <44AA8457.7020009@vanderbilt.edu>

Stephan Lindner wrote:
> Dear all,
> 
> here's my own answer to the first warning message -- the warning
> message comes from handling missing values, which is specified as
> na.delte as default in lrm.
> 
> Cheers,
> 
> Stephan

You're right, and I found the bug.  Until we update the Design package 
please put this command at the top of you script to get the corrected 
version of lrm:

source('http://biostat.mc.vanderbilt.edu/cgi-bin/cvsweb.cgi/~checkout~/Design/R/lrm.s?rev=1.4;content-type=text%2Fplain')

Thanks

Frank

> 
> 
> 
> 
> # Consider a toy data frame:
> 
> 
>> d.temp
>    y.js h.hhsize h.work.frac h.age  h.sex h.popgroup weightsd cluster
> 1    No        3   0.3333333    20 female   Coloured 47.80062    1001
> 2    No        5   0.6000000    18 female   Coloured 47.80062    1001
> 3   Yes        4   0.7500000    18 female      White 47.80062    1001
> 4   Yes        6   0.5000000    21 female   Coloured 49.71264    1002
> 5    No        6   0.5000000    15 female   Coloured 49.71264    1002
> 6    No        3   0.6666667    20 female      White 49.71264    1002
> 7    No        3   0.3333333    21 female      White 49.71264    1002
> 8   Yes        6   0.6666667    19 female      White 49.71264    1002
> 9    No        6   0.6666667    16   male      White 49.71264    1002
> 10   No        3   0.3333333    16   male   Coloured 49.71264    1002
> 11   No        5   0.4000000    15   male   Coloured 42.85572    1003
> 12   No        6   0.6666667    18   male      White 42.85572    1003
> 13   No        4   0.2500000    17   male      White 45.88860    1004
> 14   No        3   0.3333333    15 female   Coloured 45.88860    1004
> 15   No        4   0.5000000    19 female      White 45.88860    1004
> 16  Yes        4   0.5000000    16 female      White 45.88860    1004
> 17  Yes        6   0.3333333    21 female   Coloured 45.88860    1004
> 18   No        3   0.6666667    15 female      White 46.03022    1005
> 19  Yes        5   0.4000000    20 female      White 46.03022    1005
> 20   No        5   1.0000000    19 female      White 46.03022    1005
> 
> 
> # The dependent variable has no missing values. Then, lrm works fine. 
> 
> results <- robcov(ols.results <- lrm(y.js ~
>                                      + h.hhsize             
>                                      + h.work.frac           
>                                      + factor(h.age)        
>                                      + h.sex
>                                      + h.popgroup           
>                                      
>                                     ,data=d.temp,x=T,y=T
>                                     ,weights=weightsd, normwt=TRUE),
>                                     d.temp$cluster)
> 
> 
> # Now change the first observation to a missing value:
> 
> d.temp$y.js[1] <- NA
> 
> # and do the same again produces the warning:
> 
> 
> results <- robcov(ols.results <- lrm(y.js ~
>                                      + h.hhsize             
>                                      + h.work.frac           
>                                      + factor(h.age)        
>                                      + h.sex
>                                      + h.popgroup           
>                                      
>                                     ,data=d.temp,x=T,y=T
>                                     ,weights=weightsd, normwt=TRUE),
>                                     d.temp$cluster)
> 
> 
> 
> # But specifying na.action="exclude" resolves it.
> 
> results <- robcov(ols.results <- lrm(y.js ~
>                                      + h.hhsize             
>                                      + h.work.frac           
>                                      + factor(h.age)        
>                                      + h.sex
>                                      + h.popgroup           
>                                      
>                                     ,data=d.temp,x=T,y=T, na.action="na.exclude"
>                                     ,weights=weightsd, normwt=TRUE),
>                                     d.temp$cluster)
> 
> 
> # ------------------------------------------- #
> 
> 
> On Tue, Jul 04, 2006 at 07:59:31AM -0500, Frank E Harrell Jr wrote:
>> Stephan Lindner wrote:
>>> Dear all,
>>>
>>>
>>> just a quick question regarding weights in logistic regression. I do 
>>>
>>>
>>>
>>> results <- lrm(y.js ~
>>>                h.hhsize             
>>>               + h.death1              
>>>               + h.ill1                  
>>>               + h.ljob1              
>>>               + h.fin1 
>>>               + h.div1 
>>>               + h.fail1 
>>>               + h.sex
>>>               + h.ch.1      
>>>               + h.ch.5      
>>>               + h.ch.12     
>>>               + h.ch.13     
>>>               + h.popgroup
>>>               + y.school.now
>>>               ,x=T,y=T, data=d.caps1y, weights=weightsd, normwt=TRUE
>>>                )
>>>
>>>
>>> The regression works (in the sense that the results are not way off
>>> the one w/o wighting the sample), but I get the following warning messages:
>>>
>>> Warning messages:
>>> 1: number of items to replace is not a multiple of replacement length 
>>> 2: currently weights are ignored in model validation and bootstrapping lrm 
>>> fits in: lrm(y.js ~ h.hhsize + h.death1 + h.ill1 + h.ljob1 + h.fin1 +  
>>>
>>> Perhaps someone can help me clearifying the warning messages -- thanks
>>> a lot in advance !
>> I think the second warning is clear.  Regarding the first, make sure 
>> that the weights vector has length equal to the number of rows in 
>> d.capsly.  Sometimes you have to subset weights.  If that's not the 
>> problem, try to create a minimal failing example and we'll work on it.
>>
>> Frank Harrell
>>
>>>
>>> Cheers,
>>>
>>> Stephan
>>>
>>> 	
>>>
>>>
>>
>> -- 
>> Frank E Harrell Jr   Professor and Chair           School of Medicine
>>                      Department of Biostatistics   Vanderbilt University
>>
>>
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From Clement.Viel at polytech-lille.net  Tue Jul  4 17:11:30 2006
From: Clement.Viel at polytech-lille.net (=?ISO-8859-1?Q?Cl=E9ment_Viel?=)
Date: Tue, 4 Jul 2006 16:11:30 +0100
Subject: [R] Problems when computing the 1rst derivative of mixtures of
	densities
Message-ID: <67a23b050607040811s49075148h4034be1350a3a337@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060704/b8b909d3/attachment.pl 

From jholtman at gmail.com  Tue Jul  4 17:26:44 2006
From: jholtman at gmail.com (jim holtman)
Date: Tue, 4 Jul 2006 11:26:44 -0400
Subject: [R] [Fwd: formatting using the write statement]
In-Reply-To: <7176716.3400741152025394370.JavaMail.root@vms170.mailsrvcs.net>
References: <7176716.3400741152025394370.JavaMail.root@vms170.mailsrvcs.net>
Message-ID: <644e1f320607040826j3a2adee9haf60115feed8774e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060704/2450013a/attachment.pl 

From ggrothendieck at gmail.com  Tue Jul  4 17:42:14 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 4 Jul 2006 11:42:14 -0400
Subject: [R] coloring individual points in lattice xyplot
Message-ID: <971536df0607040842v69c39f8bl7b13bae45c0b0fe1@mail.gmail.com>

If I wish to color groups in xyplot I can do this:

   library(lattice)
   x <- 1:10
   y <- cbind(x, x+1)
   xyplot(y ~ rep(x,2), group = col(y), col = 1:2)

How do I color different points differently within a group.

For example, I want to produce this plot (except that I only
want to have two groups, not 11):

   xyplot(y ~ rep(x,2), group = c(rep(1, 10), 2:11), col = 1:11)

I am thinking of something like this (although
this does not work, its just to get the idea across):

   xyplot(y ~ rep(x,2), group = col(y), col = list(1, 2:11))

where, in general, I have a list with one component per group
whose elements are scalars to color the whole group or
vectors one color per point in the group.  I don't know
ahead of time what the list is.

I am looking for a general approach to this within the lattice
xyplot plot framework; the above is just an example.


From celso.barros at gmail.com  Tue Jul  4 18:14:24 2006
From: celso.barros at gmail.com (Celso Barros)
Date: Tue, 4 Jul 2006 13:14:24 -0300
Subject: [R] Robust standard errors in logistic regression
Message-ID: <f6b7dfdc0607040914u254a4906qf70bed0e2355ea8f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060704/375cdfb8/attachment.pl 

From f.harrell at vanderbilt.edu  Tue Jul  4 18:23:34 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 04 Jul 2006 11:23:34 -0500
Subject: [R] Robust standard errors in logistic regression
In-Reply-To: <f6b7dfdc0607040914u254a4906qf70bed0e2355ea8f@mail.gmail.com>
References: <f6b7dfdc0607040914u254a4906qf70bed0e2355ea8f@mail.gmail.com>
Message-ID: <44AA9606.2030701@vanderbilt.edu>

Celso Barros wrote:
> I am trying to get robust standard errors in a logistic regression. Is there
> any way to do it, either in car or in MASS?
> 
> Thanks for the help,
> 
>                                           Celso

One way to do it is to install the Hmisc and Design packages then

f <- lrm(y ~ rcs(age,5)*sex+race, x=TRUE, y=TRUE)
g <- robcov(f)   # replaces variance-covariance matrix with sandwich 
estimator; can also adjust for intra-cluster correlations
h <- bootcov(f)  # bootstrap covariance matrix, also allows clusters

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From tuechler at gmx.at  Tue Jul  4 18:33:15 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Tue, 04 Jul 2006 17:33:15 +0100
Subject: [R] unique deletes names - intended?
Message-ID: <3.0.6.32.20060704173315.00aca4f8@pop.gmx.net>

Dear All,

as shown in the example, unique() deletes names of vector elements. 
Is this intended?
Of course, one can use indexing by !duplicated() instead.

Greetings,
Heinz

## unique deletes names
v1 <- c(a=1, b=2, c=3, e=2, a=4)
unique(v1) # names deleted

v1[!duplicated(v1)] # names preserved


platform       i386-pc-mingw32                          
arch           i386                                     
os             mingw32                                  
system         i386, mingw32                            
status         Patched                                  
major          2                                        
minor          3.1                                      
year           2006                                     
month          07                                       
day            01                                       
svn rev        38471                                    
language       R                                        
version.string Version 2.3.1 Patched (2006-07-01 r38471)


From maud.pousset at noos.fr  Tue Jul  4 18:37:34 2006
From: maud.pousset at noos.fr (Pousset)
Date: Tue, 4 Jul 2006 18:37:34 +0200
Subject: [R] Latent Class Analysis
Message-ID: <20060704163644.6D6445A0B2@zeus.cochin.inserm.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060704/557cdb27/attachment.pl 

From Achim.Zeileis at wu-wien.ac.at  Tue Jul  4 18:54:17 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 4 Jul 2006 18:54:17 +0200
Subject: [R] Robust standard errors in logistic regression
In-Reply-To: <f6b7dfdc0607040914u254a4906qf70bed0e2355ea8f@mail.gmail.com>
References: <f6b7dfdc0607040914u254a4906qf70bed0e2355ea8f@mail.gmail.com>
Message-ID: <20060704185417.58355140.Achim.Zeileis@wu-wien.ac.at>

On Tue, 4 Jul 2006 13:14:24 -0300 Celso Barros wrote:

> I am trying to get robust standard errors in a logistic regression.
> Is there any way to do it, either in car or in MASS?

Package sandwich offers various types of sandwich estimators that can
also be applied to objects of class "glm", in particular sandwich()
which computes the standard Eicker-Huber-White estimate.

These robust covariance matrices can be plugged into various inference
functions such as linear.hypothesis() in car, or coeftest() and
waldtest() in lmtest.

See the man pages and package vignettes for examples.
Z

> Thanks for the help,
> 
>                                           Celso
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


From deepayan.sarkar at gmail.com  Tue Jul  4 20:24:36 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 4 Jul 2006 13:24:36 -0500
Subject: [R] coloring individual points in lattice xyplot
In-Reply-To: <971536df0607040842v69c39f8bl7b13bae45c0b0fe1@mail.gmail.com>
References: <971536df0607040842v69c39f8bl7b13bae45c0b0fe1@mail.gmail.com>
Message-ID: <eb555e660607041124g3df6e08fvee0506b09736df68@mail.gmail.com>

On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> If I wish to color groups in xyplot I can do this:
>
>    library(lattice)
>    x <- 1:10
>    y <- cbind(x, x+1)
>    xyplot(y ~ rep(x,2), group = col(y), col = 1:2)
>
> How do I color different points differently within a group.
>
> For example, I want to produce this plot (except that I only
> want to have two groups, not 11):
>
>    xyplot(y ~ rep(x,2), group = c(rep(1, 10), 2:11), col = 1:11)
>
> I am thinking of something like this (although
> this does not work, its just to get the idea across):
>
>    xyplot(y ~ rep(x,2), group = col(y), col = list(1, 2:11))
>
> where, in general, I have a list with one component per group
> whose elements are scalars to color the whole group or
> vectors one color per point in the group.  I don't know
> ahead of time what the list is.
>
> I am looking for a general approach to this within the lattice
> xyplot plot framework; the above is just an example.

The general approach is to write your own panel function. For a
possible template, look at the functions panel.superpose and
panel.superpose.2 and how they handle the 'type' argument.

Deepayan


From aaron.solomon.adelman at gmail.com  Tue Jul  4 20:26:07 2006
From: aaron.solomon.adelman at gmail.com (Aaron Solomon Adelman)
Date: Tue, 4 Jul 2006 14:26:07 -0400
Subject: [R] Exporting tables to RTF?
Message-ID: <51D07869-879D-476C-AA21-6CE30FC5DEB2@gmail.com>

Greetings.

Yesterday I managed to jury-rig a system to get a table out of R and  
import it into Nisus Writer Express (a word processor for Mac OS X).   
What I came up with is brittle, since it depends on an AppleScript  
script which has to be in a specific location and on no one touching  
the computer, since the AppleScript remotely controls Nisus Writer  
Express rather blindly and will make it do something wrong if  
everything is not exactly where it expects it to be.

There has to be a more robust way of doing this.

Ideally, I'd like to have R translate one of its tables into a table  
in an RTF file and then have Nisus Writer Express open the file.  Has  
anyone written any code to do the RTF file-writing part?  (I didn't  
find anything in CRAN.  The second part I can do easily enough.)

Alternatively, I could try to get rid of the external AppleScript and  
control Nisus Writer Express directly from R.  However, to do that, I  
need to use system() to pass commands to the shell to pass commands  
to AppleScript to pass commands to Nisus Writer Express.  I haven't  
gotten this to work because I haven't managed to figure out how to  
formulate the escape codes.  My AppleScript commands have to be in  
the form of:

tell application "Nisus Writer Express" to <do something or other>

Can anyone tell me how to escape such statements so I can pass them  
to osascript (a shell program that runs AppleScript) and (by  
extension) pass them to system()?

Thanks in advance for any help anyone can provide.

Aaron

-----
Aaron Solomon (ben Saul Joseph) Adelman
E-mail:  Aaron.Solomon.Adelman at gmail.com, adelmaas at musc.edu
Web-sites:  http://weirdthingoftheday.blogspot.com/ , http:// 
people.musc.edu/~adelmaas/
AOL Instant Messenger & Yahoo! Messenger:  Hiergargo
ICQ:  258691118
Jabber:  Aaron.Solomon.Adelman at gmail.com


From ggrothendieck at gmail.com  Tue Jul  4 20:31:49 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 4 Jul 2006 14:31:49 -0400
Subject: [R] coloring individual points in lattice xyplot
In-Reply-To: <eb555e660607041124g3df6e08fvee0506b09736df68@mail.gmail.com>
References: <971536df0607040842v69c39f8bl7b13bae45c0b0fe1@mail.gmail.com>
	<eb555e660607041124g3df6e08fvee0506b09736df68@mail.gmail.com>
Message-ID: <971536df0607041131g3321c410kbda7f191c3883605@mail.gmail.com>

On 7/4/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > If I wish to color groups in xyplot I can do this:
> >
> >    library(lattice)
> >    x <- 1:10
> >    y <- cbind(x, x+1)
> >    xyplot(y ~ rep(x,2), group = col(y), col = 1:2)
> >
> > How do I color different points differently within a group.
> >
> > For example, I want to produce this plot (except that I only
> > want to have two groups, not 11):
> >
> >    xyplot(y ~ rep(x,2), group = c(rep(1, 10), 2:11), col = 1:11)
> >
> > I am thinking of something like this (although
> > this does not work, its just to get the idea across):
> >
> >    xyplot(y ~ rep(x,2), group = col(y), col = list(1, 2:11))
> >
> > where, in general, I have a list with one component per group
> > whose elements are scalars to color the whole group or
> > vectors one color per point in the group.  I don't know
> > ahead of time what the list is.
> >
> > I am looking for a general approach to this within the lattice
> > xyplot plot framework; the above is just an example.
>
> The general approach is to write your own panel function. For a
> possible template, look at the functions panel.superpose and
> panel.superpose.2 and how they handle the 'type' argument.
>
> Deepayan
>

There is no example in ?panel.superpose.  Do you think you
could provide an example for the situation in my post?

I have done quite a bit of RSiteSearch'ing and googling prior to
posting and all the examples I found had colors that depended
on the group, none addressed the situation in my post -- i.e.
coloring individual points within groups.


From p.murrell at auckland.ac.nz  Tue Jul  4 22:12:48 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 05 Jul 2006 08:12:48 +1200
Subject: [R] curiosity question: new graphics vs. old graphics subsystem
In-Reply-To: <50d1c22d0607040605l24475d41s3d57943c294799ed@mail.gmail.com>
References: <50d1c22d0607011123x1fc06845oceb6cbaf008ee00@mail.gmail.com>	
	<44A9D475.1070407@stat.auckland.ac.nz>
	<50d1c22d0607040605l24475d41s3d57943c294799ed@mail.gmail.com>
Message-ID: <44AACBC0.5010300@stat.auckland.ac.nz>

Hi


ivo welch wrote:
> hi paul:  thank you.  great explanation.
> 
> * what you describe seems like a fairly modest advantage of the
> traditional system.  I have never used the new system, but this part
> of the translation should be simple: if a plot() is called, set the
> current viewport.  emulation done.  (if the user has issued a plot()
> and then screws around with the viewport, R could complain and tell
> the user that she is now on her own.)  I am probably way too naive
> here.  there must be a lot trickier stuff.


There is the package 'gridBase' which provides some support for this
"naive" approach.


> * R regularly makes small changes that break old R code in order to
> improve R.  This is good.  IMHO, emulating the old graphics system
> would not have to be perfect one-to-one.


But the person maintaining the emulation code would have to support more
than just YHO :)


> * when new graphics features are added, there must be duplication of
> effort to get it into both graphics systems.  would it not be easier
> in the long-run to have just one system to maintain?


Yes!


> * grImport and your slides look nice.  alas, I am searching for
> grImport in your book, and it ain't there.  ;-).  also, pdf should be
> as important as postscript to include these days.


The book preceded work on grImport.

grImport imports PostScript because that is possible without writing a
PostScript interpreter (it uses ghostscript).  Many tools exist to
convert from other vector formats to PostScript (e.g., pdf2ps
[ghostscript], pdftops [xpdf]).


> * speed and memory size for graphics are less of an issue now than it
> used to be.


Right.  Otherwise grid would not exist.  But some datasets and
visualization problems are getting larger at an even greater rate.


> sheesh, this almost sounds ungrateful.  don't mean to be.  thanks to
> the R team for having put this graphics together.  it really is the
> best thing out there.  this discrepancy in graphics systems is really
> nothing more than a minor nuisance.


Suggestions are welcome, as long as they are not accompanied by a
deadline :)  You are of course correct.  It would be much better if
there were only one graphics system.  The only sensible solution would
be an emulation layer, but it needs to become enough of an itch for
someone that they have to scratch it.  At this stage it just seems to be
a bit of a rash;  unsightly, but the cure is worse than the disease.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From mharbur at umn.edu  Tue Jul  4 22:21:46 2006
From: mharbur at umn.edu (Matthew Harbur)
Date: Tue, 4 Jul 2006 15:21:46 -0500
Subject: [R] summary.formula table questions
Message-ID: <000001c69fa7$7a2bbf40$6501a8c0@UOFMSWDE0.local>

Question 1) I am using the summary.formula function, from the Hmisc,
package, to construct a table of the mean establishment rates of multiple
plant ecotypes.  I do not want the count (N) columns included in the table.
Is there a way to suppress that statistic? 

Question 2) The table will have multiple columns, each representing a
different location. Is there a way to include Tukey's HSD at the bottom of
each column?

Thank you.

Matt

Matthew M. Harbur
University of Minnesota Southwest Research and Outreach Center
23669 130th Street
Lamberton, MN  56152
(507) 752-5091 (office)
(507) 752-5097 (fax)


From wwguocn at gmail.com  Wed Jul  5 03:12:15 2006
From: wwguocn at gmail.com (Guo Wei-Wei)
Date: Wed, 5 Jul 2006 09:12:15 +0800
Subject: [R] Problems on testing moderating effect (or interactive
	effect).
In-Reply-To: <20060704131533.GA29438@psych.upenn.edu>
References: <d3677d7d0607032038y1d2f977ciae2554eaa4f2f7f@mail.gmail.com>
	<20060704110906.GA23027@psych.upenn.edu>
	<d3677d7d0607040451j3c6a9753ma120c2452113b613@mail.gmail.com>
	<20060704131533.GA29438@psych.upenn.edu>
Message-ID: <d3677d7d0607041812u2bfe6d62r42e252efebe7963d@mail.gmail.com>

Thank you Jonathan.
I do need to read R documentation. The extra problem is time ....
Thank you for your reply.

2006/7/4, Jonathan Baron <baron at psych.upenn.edu>:
> On 07/04/06 19:51, Guo Wei-Wei wrote:
> > Can I use the variance-covariance matrix as the input data? Just like
> > what SEM does. My mentor told me to avoid sperate the operation into
> > two step, that is to get the factors' means first and then to test the
> > relationships. I'm used to use sem package. I'm not familiar with
> > lm(). I trid summary(lm(B ~ A*C)) and failed to get any result.
> >
> > Can sem deal with mediation? And could you tell me the command of
> > generating a interaction item of A (nxp) and C (mxp)?
>
> I don't use sem, but I don't see why you need it for this.  This
> is a simple regression problem, so far as I can tell.
>
> I think you need to do some reading of the R documentation.  What
> are A, B, and C?  They should be vectors.  That was the point of
> my comment about rowMeans.
>
> You seem to be guessing and relying on authority instead of
> trying to understand.
>
> --
> Jonathan Baron, Professor of Psychology, University of Pennsylvania
> Home page: http://www.sas.upenn.edu/~baron
>
>


From spencer.graves at pdf.com  Wed Jul  5 03:17:25 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 04 Jul 2006 18:17:25 -0700
Subject: [R] Fwd: time series patterns
In-Reply-To: <509bb6a90606301500o14c7213ay8cd36d5855a423b6@mail.gmail.com>
References: <509bb6a90606301258t49153c36vb199dc6e2c8a855a@mail.gmail.com>	<BAY106-F9EBDB789D3ADA346BDB6BBB7D0@phx.gbl>
	<509bb6a90606301500o14c7213ay8cd36d5855a423b6@mail.gmail.com>
Message-ID: <44AB1325.5060603@pdf.com>

	  I how you code something like this depends on what you plan to do 
with it.  If it were my problem, I might want to try some hidden Markov 
models.  RSiteSearch("hidden markov", "functions") produced 29 hits for 
me just now.  You might try some of the packages mentioned there.  They 
might suggest supplemental coding and / or plotting procedures that 
might provide insight into your application.

	  Hope this helps.
	  Spencer Graves
p.s.  The 'repeated' package and others by J.K. Lindsey are not 
available from CRAN.  A year ago, I downloaded material from his 
personal web site, which was the first hit on a Google search for "J K 
Lindsey".  I also remember seeing it on CRAN or r-project.org.  Today, 
the closest I can come is "jlindsey at ulg.ac.be" (T?l : 	+32 4 3662964). 
If you want to try any of his R packages and you don't have any better 
results with a web search than I got just now, I suggest send him an 
email.

Raphael Fraser wrote:
> ---------- Forwarded message ----------
> From: Alexander Nervedi <alexnerdy at hotmail.com>
> Date: Jun 30, 2006 4:56 PM
> Subject: time series patterns
> To: raphael.fraser at gmail.com
> 
> 
> Hi all.
> 
> I have a factor variable distributed over time. I am looking for an elegant
> way to code duration of a state. Suppose,
> 
>> rainfall.shocks <- factor(sample(c(1,2,3), size = 15, replace = TRUE, prob
>> = unit.p),
> +                  label = c("Drought", "Normal", "High"))
>> rainfall.shocks
> [1] Normal  High    High    Drought Normal  Normal  High    Normal  Drought
> [10] Normal  Drought Normal  Normal  Normal  Normal
> 
> 
> So capture the duration of say drought, I'd need a variable that is able to
> keep track of rainfall.shocks as well as its past values. I was wondering if
> there is any obvious way to do this. the Drought variable in this case would
> have values
> 
> 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0
> 
> many thanks for the suggestions you are likely to make.
> 
> Alexander Nervedi
> 
> _________________________________________________________________
> Express yourself instantly with MSN Messenger! Download today - it's FREE!
> http://messenger.msn.click-url.com/go/onm00200471ave/direct/01/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ggrothendieck at gmail.com  Wed Jul  5 04:29:44 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 4 Jul 2006 22:29:44 -0400
Subject: [R] coloring individual points in lattice xyplot
In-Reply-To: <971536df0607041131g3321c410kbda7f191c3883605@mail.gmail.com>
References: <971536df0607040842v69c39f8bl7b13bae45c0b0fe1@mail.gmail.com>
	<eb555e660607041124g3df6e08fvee0506b09736df68@mail.gmail.com>
	<971536df0607041131g3321c410kbda7f191c3883605@mail.gmail.com>
Message-ID: <971536df0607041929s3c6b9ccei204ea42ed1004f61@mail.gmail.com>

I can get the types to work or the colors but not both:

# this gets the types right but not the colors
library(lattice)
x <- 1:10
y <- cbind(y1 = x, y2 = x+1)
cols <- c(rep(1,10), 1:10)
xyplot(y ~ c(x,x), groups = col(y), type = c("o", "p"),
   panel = function(x, y, subscripts, groups, ...)
      panel.superpose.2(x, y, subscripts, groups, col = cols[subscripts], ...)
)

			
# this gets the colors right but not the types
library(lattice)
x <- 1:10
y <- cbind(y1 = x, y2 = x+1)
cols <- c(rep(1,10), 1:10)
xyplot(y ~ c(x,x), groups = col(y), type = c("o", "p"),
   panel = function(x, y, subscripts, groups, ...)
      panel.xyplot(x, y, col = cols[subscripts], ...)
)


On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 7/4/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> > On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > If I wish to color groups in xyplot I can do this:
> > >
> > >    library(lattice)
> > >    x <- 1:10
> > >    y <- cbind(x, x+1)
> > >    xyplot(y ~ rep(x,2), group = col(y), col = 1:2)
> > >
> > > How do I color different points differently within a group.
> > >
> > > For example, I want to produce this plot (except that I only
> > > want to have two groups, not 11):
> > >
> > >    xyplot(y ~ rep(x,2), group = c(rep(1, 10), 2:11), col = 1:11)
> > >
> > > I am thinking of something like this (although
> > > this does not work, its just to get the idea across):
> > >
> > >    xyplot(y ~ rep(x,2), group = col(y), col = list(1, 2:11))
> > >
> > > where, in general, I have a list with one component per group
> > > whose elements are scalars to color the whole group or
> > > vectors one color per point in the group.  I don't know
> > > ahead of time what the list is.
> > >
> > > I am looking for a general approach to this within the lattice
> > > xyplot plot framework; the above is just an example.
> >
> > The general approach is to write your own panel function. For a
> > possible template, look at the functions panel.superpose and
> > panel.superpose.2 and how they handle the 'type' argument.
> >
> > Deepayan
> >
>
> There is no example in ?panel.superpose.  Do you think you
> could provide an example for the situation in my post?
>
> I have done quite a bit of RSiteSearch'ing and googling prior to
> posting and all the examples I found had colors that depended
> on the group, none addressed the situation in my post -- i.e.
> coloring individual points within groups.
>


From deepayan.sarkar at gmail.com  Wed Jul  5 05:05:40 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 4 Jul 2006 22:05:40 -0500
Subject: [R] coloring individual points in lattice xyplot
In-Reply-To: <971536df0607041929s3c6b9ccei204ea42ed1004f61@mail.gmail.com>
References: <971536df0607040842v69c39f8bl7b13bae45c0b0fe1@mail.gmail.com>
	<eb555e660607041124g3df6e08fvee0506b09736df68@mail.gmail.com>
	<971536df0607041131g3321c410kbda7f191c3883605@mail.gmail.com>
	<971536df0607041929s3c6b9ccei204ea42ed1004f61@mail.gmail.com>
Message-ID: <eb555e660607042005i5c760480xa2b63738164e3611@mail.gmail.com>

On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> I can get the types to work or the colors but not both:

Sorry if I wasn't clear, but I didn't mean that you could use
panel.superpose[.2] to do what you wanted. I only meant that you could
use it as a template that may help you to write your own panel
function. What you want is not possible with tools available in
lattice.

Deepayan

> # this gets the types right but not the colors
> library(lattice)
> x <- 1:10
> y <- cbind(y1 = x, y2 = x+1)
> cols <- c(rep(1,10), 1:10)
> xyplot(y ~ c(x,x), groups = col(y), type = c("o", "p"),
>    panel = function(x, y, subscripts, groups, ...)
>       panel.superpose.2(x, y, subscripts, groups, col = cols[subscripts], ...)
> )
>
>
> # this gets the colors right but not the types
> library(lattice)
> x <- 1:10
> y <- cbind(y1 = x, y2 = x+1)
> cols <- c(rep(1,10), 1:10)
> xyplot(y ~ c(x,x), groups = col(y), type = c("o", "p"),
>    panel = function(x, y, subscripts, groups, ...)
>       panel.xyplot(x, y, col = cols[subscripts], ...)
> )
>
>
> On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > On 7/4/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> > > On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > If I wish to color groups in xyplot I can do this:
> > > >
> > > >    library(lattice)
> > > >    x <- 1:10
> > > >    y <- cbind(x, x+1)
> > > >    xyplot(y ~ rep(x,2), group = col(y), col = 1:2)
> > > >
> > > > How do I color different points differently within a group.
> > > >
> > > > For example, I want to produce this plot (except that I only
> > > > want to have two groups, not 11):
> > > >
> > > >    xyplot(y ~ rep(x,2), group = c(rep(1, 10), 2:11), col = 1:11)
> > > >
> > > > I am thinking of something like this (although
> > > > this does not work, its just to get the idea across):
> > > >
> > > >    xyplot(y ~ rep(x,2), group = col(y), col = list(1, 2:11))
> > > >
> > > > where, in general, I have a list with one component per group
> > > > whose elements are scalars to color the whole group or
> > > > vectors one color per point in the group.  I don't know
> > > > ahead of time what the list is.
> > > >
> > > > I am looking for a general approach to this within the lattice
> > > > xyplot plot framework; the above is just an example.
> > >
> > > The general approach is to write your own panel function. For a
> > > possible template, look at the functions panel.superpose and
> > > panel.superpose.2 and how they handle the 'type' argument.
> > >
> > > Deepayan
> > >
> >
> > There is no example in ?panel.superpose.  Do you think you
> > could provide an example for the situation in my post?
> >
> > I have done quite a bit of RSiteSearch'ing and googling prior to
> > posting and all the examples I found had colors that depended
> > on the group, none addressed the situation in my post -- i.e.
> > coloring individual points within groups.
> >
>


-- 
http://www.stat.wisc.edu/~deepayan/


From f.harrell at vanderbilt.edu  Wed Jul  5 05:46:14 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 04 Jul 2006 22:46:14 -0500
Subject: [R] summary.formula table questions
In-Reply-To: <000001c69fa7$7a2bbf40$6501a8c0@UOFMSWDE0.local>
References: <000001c69fa7$7a2bbf40$6501a8c0@UOFMSWDE0.local>
Message-ID: <44AB3606.3080806@vanderbilt.edu>

Matthew Harbur wrote:
> Question 1) I am using the summary.formula function, from the Hmisc,
> package, to construct a table of the mean establishment rates of multiple
> plant ecotypes.  I do not want the count (N) columns included in the table.
> Is there a way to suppress that statistic? 
> 
> Question 2) The table will have multiple columns, each representing a
> different location. Is there a way to include Tukey's HSD at the bottom of
> each column?
> 
> Thank you.
> 
> Matt

Matt,

For 1) where method='reverse' or 'cross' you can do
s <- summary( ... )
print(s, prn=FALSE)

For method='response' the latex.summary.formula.response method has a 
prn argument and it would not be too hard to add one to 
print.summary.formula.response.

Your best bet for 2) is probably to use summary.formula with 
method='reverse' and to write a new 'test' function for continuous 
response variables that includes some kind of across-column summary.

Frank

> 
> Matthew M. Harbur
> University of Minnesota Southwest Research and Outreach Center
> 23669 130th Street
> Lamberton, MN  56152
> (507) 752-5091 (office)
> (507) 752-5097 (fax)

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From ggrothendieck at gmail.com  Wed Jul  5 06:05:08 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 5 Jul 2006 00:05:08 -0400
Subject: [R] coloring individual points in lattice xyplot
In-Reply-To: <eb555e660607042005i5c760480xa2b63738164e3611@mail.gmail.com>
References: <971536df0607040842v69c39f8bl7b13bae45c0b0fe1@mail.gmail.com>
	<eb555e660607041124g3df6e08fvee0506b09736df68@mail.gmail.com>
	<971536df0607041131g3321c410kbda7f191c3883605@mail.gmail.com>
	<971536df0607041929s3c6b9ccei204ea42ed1004f61@mail.gmail.com>
	<eb555e660607042005i5c760480xa2b63738164e3611@mail.gmail.com>
Message-ID: <971536df0607042105v1853b7f7y88bd7b774fa4ff53@mail.gmail.com>

OK.  It looks like I need to go to the lower level llines and lpoints to
do this.  I wrote a panel routine, mypanel, and it seems to work (see
below); however, currently it assumes types and cols are in the global
environment or at least somewhere where they will be found.

1. Is it somehow possible to stick these into some structures set up by
lattice already and then retrieve them from lattice from within mypanel?
2. Any other improvements to the example below?

mypanel <- function(x, y, subscripts, groups, ...) {
      for(g in 1:nlevels(groups)) {
         idx <- g == groups
         xx <- x[idx]; yy <- y[idx]; ccols <- cols[subscripts][idx]
         if (any(idx)) {
            switch(types[g],
               p = lpoints(xx, yy, col = ccols),
               l = llines(xx, yy, col = ccols),
               o = { lpoints(xx, yy, col = ccols)
                        llines(xx, yy, col = ccols) })
         }
       }
    }

x <- 1:10
y <- cbind(y1 = x, y2 = x+1)
cols <- c(rep(1,10), 1:10)
types <- c("o", "p")
xyplot(y ~ c(x,x), groups = factor(col(y)), type = types, panel = mypanel)



On 7/4/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > I can get the types to work or the colors but not both:
>
> Sorry if I wasn't clear, but I didn't mean that you could use
> panel.superpose[.2] to do what you wanted. I only meant that you could
> use it as a template that may help you to write your own panel
> function. What you want is not possible with tools available in
> lattice.
>
> Deepayan
>
> > # this gets the types right but not the colors
> > library(lattice)
> > x <- 1:10
> > y <- cbind(y1 = x, y2 = x+1)
> > cols <- c(rep(1,10), 1:10)
> > xyplot(y ~ c(x,x), groups = col(y), type = c("o", "p"),
> >    panel = function(x, y, subscripts, groups, ...)
> >       panel.superpose.2(x, y, subscripts, groups, col = cols[subscripts], ...)
> > )
> >
> >
> > # this gets the colors right but not the types
> > library(lattice)
> > x <- 1:10
> > y <- cbind(y1 = x, y2 = x+1)
> > cols <- c(rep(1,10), 1:10)
> > xyplot(y ~ c(x,x), groups = col(y), type = c("o", "p"),
> >    panel = function(x, y, subscripts, groups, ...)
> >       panel.xyplot(x, y, col = cols[subscripts], ...)
> > )
> >
> >
> > On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > On 7/4/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> > > > On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > > If I wish to color groups in xyplot I can do this:
> > > > >
> > > > >    library(lattice)
> > > > >    x <- 1:10
> > > > >    y <- cbind(x, x+1)
> > > > >    xyplot(y ~ rep(x,2), group = col(y), col = 1:2)
> > > > >
> > > > > How do I color different points differently within a group.
> > > > >
> > > > > For example, I want to produce this plot (except that I only
> > > > > want to have two groups, not 11):
> > > > >
> > > > >    xyplot(y ~ rep(x,2), group = c(rep(1, 10), 2:11), col = 1:11)
> > > > >
> > > > > I am thinking of something like this (although
> > > > > this does not work, its just to get the idea across):
> > > > >
> > > > >    xyplot(y ~ rep(x,2), group = col(y), col = list(1, 2:11))
> > > > >
> > > > > where, in general, I have a list with one component per group
> > > > > whose elements are scalars to color the whole group or
> > > > > vectors one color per point in the group.  I don't know
> > > > > ahead of time what the list is.
> > > > >
> > > > > I am looking for a general approach to this within the lattice
> > > > > xyplot plot framework; the above is just an example.
> > > >
> > > > The general approach is to write your own panel function. For a
> > > > possible template, look at the functions panel.superpose and
> > > > panel.superpose.2 and how they handle the 'type' argument.
> > > >
> > > > Deepayan
> > > >
> > >
> > > There is no example in ?panel.superpose.  Do you think you
> > > could provide an example for the situation in my post?
> > >
> > > I have done quite a bit of RSiteSearch'ing and googling prior to
> > > posting and all the examples I found had colors that depended
> > > on the group, none addressed the situation in my post -- i.e.
> > > coloring individual points within groups.
> > >
> >
>
>
> --
> http://www.stat.wisc.edu/~deepayan/
>


From dimitrijoe at ipea.gov.br  Wed Jul  5 06:46:38 2006
From: dimitrijoe at ipea.gov.br (Dimitri Szerman)
Date: Wed, 5 Jul 2006 01:46:38 -0300
Subject: [R] creating a data frame from a list
Message-ID: <b6150c70607042146r76992bfbo56d9ffb53273f81a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060705/5279c4ad/attachment.pl 

From ggrothendieck at gmail.com  Wed Jul  5 07:18:37 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 5 Jul 2006 01:18:37 -0400
Subject: [R] creating a data frame from a list
In-Reply-To: <b6150c70607042146r76992bfbo56d9ffb53273f81a@mail.gmail.com>
References: <b6150c70607042146r76992bfbo56d9ffb53273f81a@mail.gmail.com>
Message-ID: <971536df0607042218x23effdccxfce0771f749a78a2@mail.gmail.com>

Create a zoo object from each component of lst
using the names as the "times".   Then merge
them together using merge.zoo, set the rownames
from the times and extract out the data:

library(zoo)
z <- do.call(merge, lapply(lst, function(x) zoo(x, names(x))))
rownames(z) <- time(z)
coredata(z)


On 7/5/06, Dimitri Szerman <dimitrijoe at ipea.gov.br> wrote:
> Dear all,
>
> I have a list with three (named) numeric vectors:
>
> > lst = list(a=c(A=1,B=8) , b=c(A=2,B=3,C=0), c=c(B=2,D=0) )
> > lst
> $a
> A B
> 1 8
>
> $b
> A B C
> 2 3 0
>
> $c
> B D
> 2 0
>
>
> Now, I'd love to use this list to create the following data frame:
>
> > dtf = data.frame(a=c(A=1,B=8,C=NA,D=NA),
> +                  b=c(A=2,B=3,C=0,D=NA),
> +                  c=c(A=NA,B=2,C=NA,D=0) )
>
> > dtf
>    a    b     c
> A   1   2  NA
> B   8   3     2
> C NA   0  NA
> D NA NA    0
>
> That is, I wish to "merge" the three vectors in the list into a data frame
> by their "(row)"names. Any suggestions? Thanks in advance for youR-Help.
>
> Dimitri
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From sell_mirage_ne at hotmail.com  Wed Jul  5 07:19:13 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Wed, 05 Jul 2006 00:19:13 -0500
Subject: [R] yaxis in plot(density(...))
Message-ID: <BAY110-F3CDAFC7A659CAECA74C50C7760@phx.gbl>

Dear R-users

Could you explain what is the unit of yaxis in plot(density(...))?

It says "density" but it is unclear to me what density is.

Thanks

Taka,


From celso.barros at gmail.com  Wed Jul  5 09:09:17 2006
From: celso.barros at gmail.com (Celso Barros)
Date: Wed, 5 Jul 2006 04:09:17 -0300
Subject: [R] p-values
Message-ID: <f6b7dfdc0607050009y7e4992bese565972d21d0a501@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060705/683f2acf/attachment.pl 

From maechler at stat.math.ethz.ch  Wed Jul  5 09:38:32 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 5 Jul 2006 09:38:32 +0200
Subject: [R] yaxis in plot(density(...))
In-Reply-To: <BAY110-F3CDAFC7A659CAECA74C50C7760@phx.gbl>
References: <BAY110-F3CDAFC7A659CAECA74C50C7760@phx.gbl>
Message-ID: <17579.27768.853866.517531@stat.math.ethz.ch>

>>>>> "Taka" == Taka Matzmoto <sell_mirage_ne at hotmail.com>
>>>>>     on Wed, 05 Jul 2006 00:19:13 -0500 writes:

    Taka> Dear R-users
    Taka> Could you explain what is the unit of yaxis in plot(density(...))?

    Taka> It says "density" but it is unclear to me what density is.

This is really not a question about R, but about first-level
probability and statistics:

The unit of a density {estimated or not}  
    f(x)  is   1 / (unit of x),
since one of the major properties of a density is  \int f(x) dx == 1 
and the latter is ``dimension free'' (as others would say).

E.g. the  unit in  plot(density(faithful$eruptions))  is
      1 / min  aka  '[min ^{-1}]'  (min = minutes of eruption time)

Martin Maechler,  ETH Zurich


From celso.barros at gmail.com  Wed Jul  5 09:50:29 2006
From: celso.barros at gmail.com (Celso Barros)
Date: Wed, 5 Jul 2006 04:50:29 -0300
Subject: [R] Robust standard errors in logistic regression
In-Reply-To: <20060704185417.58355140.Achim.Zeileis@wu-wien.ac.at>
References: <f6b7dfdc0607040914u254a4906qf70bed0e2355ea8f@mail.gmail.com>
	<20060704185417.58355140.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <f6b7dfdc0607050050w386e2cbao2d1a1a301c648ae1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060705/244f65f1/attachment.pl 

From blindglobe at gmail.com  Wed Jul  5 10:14:58 2006
From: blindglobe at gmail.com (A.J. Rossini)
Date: Wed, 5 Jul 2006 10:14:58 +0200
Subject: [R] Editors which have strong/solid support for SWeave?
Message-ID: <1abe3fa90607050114p1d77c792p714282dd39fe05b6@mail.gmail.com>

Greetings!

I have a few colleagues who like the idea of Sweave, but have failed
to become enlightened monks of the One True Editor
(http://www.dina.dk/~abraham/religion/)

Are there any other Microsoft-centric editors or IDEs which have solid
support for writing SWeave documents (dual R / LaTeX enhancements
similar to ESS's support)?  Has anyone tried the folding editors which
support Noweb?

(the alternative would be brainwashing, but that is generally frowned upon ;-).

best,
-tony

blindglobe at gmail.com
Muttenz, Switzerland.
"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).


From maechler at stat.math.ethz.ch  Wed Jul  5 10:25:59 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 5 Jul 2006 10:25:59 +0200
Subject: [R] Robust standard errors in logistic regression
In-Reply-To: <f6b7dfdc0607050050w386e2cbao2d1a1a301c648ae1@mail.gmail.com>
References: <f6b7dfdc0607040914u254a4906qf70bed0e2355ea8f@mail.gmail.com>
	<20060704185417.58355140.Achim.Zeileis@wu-wien.ac.at>
	<f6b7dfdc0607050050w386e2cbao2d1a1a301c648ae1@mail.gmail.com>
Message-ID: <17579.30615.839209.966789@stat.math.ethz.ch>

>>>>> "Celso" == Celso Barros <celso.barros at gmail.com>
>>>>>     on Wed, 5 Jul 2006 04:50:29 -0300 writes:

 [...............]


    Celso> By the way, I was wondering if there is a way to use rlm (from MASS)
    Celso> to estimate robust standard errors for logistic regression? 

rlm stands for 'robust lm'.  What you need here is  'robust glm'.

I've already replied to a similar message by you,
mentioning the (relatively) new package "robustbase".
After installing it, you can
use
	robustbase::glmrob()

[or just glmrob(), after attaching the package by "library(robustbase)"]
and its summary function does provide you with robust standard
errors (and even P-values which you seem to like particularly ;-).

Martin Maechler, ETH Zurich


From groemping at tfh-berlin.de  Wed Jul  5 10:17:27 2006
From: groemping at tfh-berlin.de (=?ISO-8859-1?Q?Ulrike_Gr=F6mping?=)
Date: Wed, 5 Jul 2006 10:17:27 +0200
Subject: [R] [R-pkgs] relaimpo: CRAN package for relative importance in
	linear	regression
Message-ID: <20060630103932.M37854@tfh-berlin.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060705/51983db1/attachment.pl 
-------------- next part --------------
_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages

From ripley at stats.ox.ac.uk  Wed Jul  5 11:02:00 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Jul 2006 10:02:00 +0100 (BST)
Subject: [R] incomplete final line found by readLines on ...
In-Reply-To: <BAY110-F12679687B95B5EFC29AAE8C77D0@phx.gbl>
References: <BAY110-F12679687B95B5EFC29AAE8C77D0@phx.gbl>
Message-ID: <Pine.LNX.4.64.0607050958590.29423@gannet.stats.ox.ac.uk>

readLines in R-devel has argument warn=TRUE, so you can easily change the 
behaviour if using that version of R.

On Fri, 30 Jun 2006, Taka Matzmoto wrote:

> Dear R-users
>
> I need to read some text files produced by some other software.
> I used readLines (with n = -1 ) command and then tried to find some numbers
> I liked to extract or some line numbers I like to know.
>
> The problem is that there is no empty line at the end of the text files.

The problem is actually that the final `line' does not end in an EOL mark 
(LF or CRLF as appropriate), which often indicates a broken text file.

> R gave me this warning below;
>
> In addition: Warning message:
> incomplete final line found by readLines on 'output.txt'
>
> I know this doesn't affect anything and just warning messages. Is there any
> way to prevent this warning message.
>
> Thanks
>
> Taka
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Jul  5 11:12:48 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Jul 2006 10:12:48 +0100 (BST)
Subject: [R] Problem with try()
In-Reply-To: <007701c69e6d$242b7710$cc00020a@Home3>
References: <007701c69e6d$242b7710$cc00020a@Home3>
Message-ID: <Pine.LNX.4.64.0607051010370.29423@gannet.stats.ox.ac.uk>

The problem here seems to be that the C code you are using does not return 
to R.  If so, it is nothing to do with try(), and you need to modify the 
way you are calling gsl to use a different error handler.

BTW, this seems much more appropriate to R-devel (see the posting guide).

On Mon, 3 Jul 2006, Landsman Leonid wrote:

> Dear R-experts,
> I am running a large simulation exercise where the enough complicated integration is required.
>
> The integral is computed within a C-function called Denom by use of function qags from the gsl library.
>
> Here is a piece of R-code:
>
>        denom<-try(.C("Denom",as.double(x),as.integer(n), as.integer(p),
>        as.double(param), as.double(delta),res=as.double(results)))
>        denomres=if (class(denom)=="try-error") NA else denom$res
>
>
> Sometimes, it happens that the integration process fails with the follwoing error message
>
> gsl: qags.c:553: ERROR: bad integrand behavior found in the integration interval
> Default GSL error handler invoked.
>
>
> and the whole simulation job is destroyed. My question is: why try() does not work and how to fix this problem?
>
> Much thanks,
>
> Leonid Landsman.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ccleland at optonline.net  Wed Jul  5 11:43:39 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 05 Jul 2006 05:43:39 -0400
Subject: [R] Editors which have strong/solid support for SWeave?
In-Reply-To: <1abe3fa90607050114p1d77c792p714282dd39fe05b6@mail.gmail.com>
References: <1abe3fa90607050114p1d77c792p714282dd39fe05b6@mail.gmail.com>
Message-ID: <44AB89CB.5000609@optonline.net>

A.J. Rossini wrote:
> Greetings!
> 
> I have a few colleagues who like the idea of Sweave, but have failed
> to become enlightened monks of the One True Editor
> (http://www.dina.dk/~abraham/religion/)
> 
> Are there any other Microsoft-centric editors or IDEs which have solid
> support for writing SWeave documents (dual R / LaTeX enhancements
> similar to ESS's support)?  Has anyone tried the folding editors which
> support Noweb?

Tony:
   I don't know what you mean by a folding editor or Microsoft-centric, 
but I am using R, WinEdt, and MikTeX on WinXP.  I have been using Sweave 
with this setup for several months and have been happy with it.  Thanks 
to Uwe Ligges, the RWinEdt package provides R enhancements to WinEdt, 
and WinEdt is configured to work with MikTeX by default (can be 
configured to work with other LaTeX systems).
   The editor itself is very intuitive - all you really need to know to 
get started is how to write *.Rnw files (via Sweave documentation and 
the many examples on can find).  I can recommend this setup for anyone 
working on Windows.

http://www.winedt.com/

http://cran.r-project.org/src/contrib/Descriptions/RWinEdt.html

hope this helps,

Chuck

> (the alternative would be brainwashing, but that is generally frowned upon ;-).
> 
> best,
> -tony
> 
> blindglobe at gmail.com
> Muttenz, Switzerland.
> "Commit early,commit often, and commit in a repository from which we can easily
> roll-back your mistakes" (AJR, 4Jan05).
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ripley at stats.ox.ac.uk  Wed Jul  5 12:04:28 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Jul 2006 11:04:28 +0100 (BST)
Subject: [R] unique deletes names - intended?
In-Reply-To: <3.0.6.32.20060704173315.00aca4f8@pop.gmx.net>
References: <3.0.6.32.20060704173315.00aca4f8@pop.gmx.net>
Message-ID: <Pine.LNX.4.64.0607051034290.29423@gannet.stats.ox.ac.uk>

On Tue, 4 Jul 2006, Heinz Tuechler wrote:

> Dear All,
>
> as shown in the example, unique() deletes names of vector elements.
> Is this intended?

Yes.  Think of the vector as a set: it is supposed to immaterial which of 
the duplicated elements is retained.

The help page says

      An object of the same type of 'x'. but if an element is equal to
      one with a smaller index, it is removed.

so it is starting with a new object, not 'x'.  However, the array method 
works differently, so the documentation needs clarification.

> Of course, one can use indexing by !duplicated() instead.

Be careful, as you might get a method for [ and that might not do want you 
intended (e.g. for a time series).


> Greetings,
> Heinz
>
> ## unique deletes names
> v1 <- c(a=1, b=2, c=3, e=2, a=4)
> unique(v1) # names deleted
>
> v1[!duplicated(v1)] # names preserved
>
>
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status         Patched
> major          2
> minor          3.1
> year           2006
> month          07
> day            01
> svn rev        38471
> language       R
> version.string Version 2.3.1 Patched (2006-07-01 r38471)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From blindglobe at gmail.com  Wed Jul  5 12:07:12 2006
From: blindglobe at gmail.com (A.J. Rossini)
Date: Wed, 5 Jul 2006 12:07:12 +0200
Subject: [R] Editors which have strong/solid support for SWeave?
In-Reply-To: <44AB89CB.5000609@optonline.net>
References: <1abe3fa90607050114p1d77c792p714282dd39fe05b6@mail.gmail.com>
	<44AB89CB.5000609@optonline.net>
Message-ID: <1abe3fa90607050307p20b9ba8ct615cf8fb698816c7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060705/21df3c2a/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Jul  5 12:11:22 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Jul 2006 11:11:22 +0100 (BST)
Subject: [R] summary.rlm (was p-values)
In-Reply-To: <f6b7dfdc0607050009y7e4992bese565972d21d0a501@mail.gmail.com>
References: <f6b7dfdc0607050009y7e4992bese565972d21d0a501@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0607051106390.7034@gannet.stats.ox.ac.uk>

On Wed, 5 Jul 2006, Celso Barros wrote:

> Dear All,
>
> When I run rlm to obtain robust standard errors, my output does not include
> p-values. Is there any reason p-values should not be used in this case? Is
> there an argument I could use in rlm so that the output does
> include p-values?

First you would have to derive a reliable theory for the tests you want 
p-values for.  The t ratios are approximately normally distributed, but to 
quote realistic p-values you would need much more accurate distribtution 
theory.

For summary.lm we can use Student's t distribution and say the results are 
exact only under normality.  There is no analogue for summary.rlm.  Note 
that S does not quote p-values for summmary.glm for similar reasons: R 
does but I do not consider it to be a good idea.

>
> Thanks in advance,
>
> Celso
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Jul  5 12:25:40 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Jul 2006 11:25:40 +0100 (BST)
Subject: [R] Workspace size
In-Reply-To: <44A90EAF.8070804@staff.hu-berlin.de>
References: <44A8FD9E.8020403@staff.hu-berlin.de>
	<644e1f320607030446u5fb942e2ldded84b357955f5c@mail.gmail.com>
	<44A90EAF.8070804@staff.hu-berlin.de>
Message-ID: <Pine.LNX.4.64.0607051121440.7824@gannet.stats.ox.ac.uk>

On Mon, 3 Jul 2006, statwi01 wrote:

> Can I determine the approximate size of a workspace on the harddisk
> before saving it via "sys.save.image(name)" ?

No.  (Also, it depends on the options in force: saved images can be ASCII 
or binary or compressed or not, and it is impossible to predict how
well compression will work: it can be very effective.)

And please note what the help page says:

      Internal functions in the base package, which are only
      user-visible because of the special nature of the base namespace.

      'sys.save.image' is a system function that is called by 'q()' and
      its GUI analogs; 'sys.load.image' is called by the startup code.
      These functions should not be called directly and are subject to
      change.

      'sys.save.image' closes all connections first, to ensure that it
      is able to open a connection to save the image.  This is
      appropriate when called from 'q()' and allies, but reinforces the
      warning that it should not be called directly.

so please don't use it.

> Thanks in advance
>
>  Sigbert

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From m.reuter at ucl.ac.uk  Wed Jul  5 12:30:25 2006
From: m.reuter at ucl.ac.uk (Max Reuter)
Date: Wed, 5 Jul 2006 11:30:25 +0100
Subject: [R] R 2.3.1 on OSX: scrolling command line
Message-ID: <p06002000c0d13d5ea2e8@[128.40.82.145]>

Dear all,

I am running R in the terminal (X11) of Mac OS. Recently, I upgraded 
my OS from 10.3 to 10.4 and installed version  2.3.1 of R. Since 
then, R has the annoying behaviour of scrolling long command lines to 
the left, rather than breaking the line and showing the entire 
command on several lines. Starting R with '--no-readline' fixes the 
problem, but at the price of losing command line history and working 
arrow keys.

Is there a way to way to define what R does to long command lines?

Many thanks for your help, Max
-- 



___________________________________________________________
Max Reuter

Departement of Biology
University College London
The Galton Laboratory
4 Stephenson Way
London NW1 2HE, UK

Phone: +44-20-76795095			 Fax: +44-20-76795052


From ripley at stats.ox.ac.uk  Wed Jul  5 12:58:00 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Jul 2006 11:58:00 +0100 (BST)
Subject: [R] R 2.3.1 on OSX: scrolling command line
In-Reply-To: <p06002000c0d13d5ea2e8@[128.40.82.145]>
References: <p06002000c0d13d5ea2e8@[128.40.82.145]>
Message-ID: <Pine.LNX.4.64.0607051146550.8359@gannet.stats.ox.ac.uk>

On Wed, 5 Jul 2006, Max Reuter wrote:

> Dear all,
>
> I am running R in the terminal (X11) of Mac OS. Recently, I upgraded
> my OS from 10.3 to 10.4 and installed version  2.3.1 of R. Since
> then, R has the annoying behaviour of scrolling long command lines to
> the left, rather than breaking the line and showing the entire
> command on several lines. Starting R with '--no-readline' fixes the
> problem, but at the price of losing command line history and working
> arrow keys.
>
> Is there a way to way to define what R does to long command lines?

It isn't R!  This is a function of readline and the terminal, not of R 
which has not yet seen the line.

What is likely is that your R install (how did you install it?) is built 
against a readline clone (I believe there is one from NetBSD? in MacOS 
10.4) rather than GNU readline, and the fix is likely to be to install GNU 
readline and then rebuild R from the sources.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jim at bitwrit.com.au  Thu Jul  6 03:03:46 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Wed, 05 Jul 2006 21:03:46 -0400
Subject: [R] could i change the ouput style on summary?
Message-ID: <44AC6172.2070107@bitwrit.com.au>

Hi Zhi Jie,

I had a look at the summary function, and a minor extension seems to do
what you want.

is.numeric.factor<-function(x) {
   if(is.factor(x)) {
    if(any(is.na(as.numeric(levels(x))))) return(FALSE)
    return(TRUE)
   }
   return(FALSE)
}

summary.table<-function(x) {
  oldwarn<-options("warn")
  options(warn=-1)
  varnames<-names(x)
  num.index<-which(sapply(x,is.numeric))
  fac.index<-which(sapply(x,is.numeric.factor))
  cat("Summary table for",deparse(substitute(x)),"\n")
  cat("             Min.  1st Qu.   Median     Mean  3rd Qu.
Max.\n")
  if(length(num.index)) {
   for(i in 1:length(num.index))

cat(formatC(c(varnames[num.index[i]],as.numeric(summary(x[[num.index[i]]]))),width=8),"\n")
  }
  if(length(fac.index)) {
   for(i in 1:length(fac.index))

cat(formatC(c(varnames[fac.index[i]],as.numeric(summary(x[[fac.index[i]]]))),width=8),"\n")
  }
  options(warn=oldwarn$warn)
}

However, I got interested in the problem in an unoccupied moment and 
wrote a set of functions named "describe" that may be an even more 
suitable solution to your problem. Let me know if you would like to try 
them.

Jim


From ligges at statistik.uni-dortmund.de  Wed Jul  5 13:49:15 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 05 Jul 2006 13:49:15 +0200
Subject: [R] Editors which have strong/solid support for SWeave?
In-Reply-To: <1abe3fa90607050307p20b9ba8ct615cf8fb698816c7@mail.gmail.com>
References: <1abe3fa90607050114p1d77c792p714282dd39fe05b6@mail.gmail.com>	<44AB89CB.5000609@optonline.net>
	<1abe3fa90607050307p20b9ba8ct615cf8fb698816c7@mail.gmail.com>
Message-ID: <44ABA73B.1080402@statistik.uni-dortmund.de>

A.J. Rossini wrote:
> What I'm wondering about is can you have both the LaTeX and R "advantages"
> that WinEDT provides available at the same time?   (does it handle the modes
> in a context-sensitive way within the same document, or do you have to use
> one or the other, or do you just have to be careful?)


No, not at the same time. I planned to add such a feature for the last 
two years or so, but there were always topics with higher priority on my 
ToDo list.

Anyway, these days Tinn-R seems to be the "Windows" way to go. It 
already supports R, LaTeX and SWeave.

Best,
Uwe Ligges

> 
> On 7/5/06, Chuck Cleland <ccleland at optonline.net> wrote:
>> A.J. Rossini wrote:
>>> Greetings!
>>>
>>> I have a few colleagues who like the idea of Sweave, but have failed
>>> to become enlightened monks of the One True Editor
>>> (http://www.dina.dk/~abraham/religion/)
>>>
>>> Are there any other Microsoft-centric editors or IDEs which have solid
>>> support for writing SWeave documents (dual R / LaTeX enhancements
>>> similar to ESS's support)?  Has anyone tried the folding editors which
>>> support Noweb?
>> Tony:
>>   I don't know what you mean by a folding editor or Microsoft-centric,
>> but I am using R, WinEdt, and MikTeX on WinXP.  I have been using Sweave
>> with this setup for several months and have been happy with it.  Thanks
>> to Uwe Ligges, the RWinEdt package provides R enhancements to WinEdt,
>> and WinEdt is configured to work with MikTeX by default (can be
>> configured to work with other LaTeX systems).
>>   The editor itself is very intuitive - all you really need to know to
>> get started is how to write *.Rnw files (via Sweave documentation and
>> the many examples on can find).  I can recommend this setup for anyone
>> working on Windows.
>>
>> http://www.winedt.com/
>>
>> http://cran.r-project.org/src/contrib/Descriptions/RWinEdt.html
>>
>> hope this helps,
>>
>> Chuck
>>
>>> (the alternative would be brainwashing, but that is generally frowned
>> upon ;-).
>>> best,
>>> -tony
>>>
>>> blindglobe at gmail.com
>>> Muttenz, Switzerland.
>>> "Commit early,commit often, and commit in a repository from which we can
>> easily
>>> roll-back your mistakes" (AJR, 4Jan05).
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>> --
>> Chuck Cleland, Ph.D.
>> NDRI, Inc.
>> 71 West 23rd Street, 8th floor
>> New York, NY 10010
>> tel: (212) 845-4495 (Tu, Th)
>> tel: (732) 512-0171 (M, W, F)
>> fax: (917) 438-0894
>>
> 
> 
>


From ccleland at optonline.net  Wed Jul  5 13:52:11 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 05 Jul 2006 07:52:11 -0400
Subject: [R] Editors which have strong/solid support for SWeave?
In-Reply-To: <1abe3fa90607050307p20b9ba8ct615cf8fb698816c7@mail.gmail.com>
References: <1abe3fa90607050114p1d77c792p714282dd39fe05b6@mail.gmail.com>
	<44AB89CB.5000609@optonline.net>
	<1abe3fa90607050307p20b9ba8ct615cf8fb698816c7@mail.gmail.com>
Message-ID: <44ABA7EB.8060807@optonline.net>

A.J. Rossini wrote:
> What I'm wondering about is can you have both the LaTeX and R "advantages"
> that WinEDT provides available at the same time?   (does it handle the modes
> in a context-sensitive way within the same document, or do you have to use
> one or the other, or do you just have to be careful?)

   As far as I know, you have to use one mode or the other.  I write the 
*.Rnw file in R mode (via RWinEdt).  I then Sweave the *.Rnw file and 
open the *.tex file it creates in LaTeX mode.  There is typically 
nothing I do with the *.tex file other than turning it into PDF (and I 
do this within WinEdt in LaTeX mode).  If anyone has a different 
approach they think is superior, I would like to hear about it too.

> On 7/5/06, Chuck Cleland <ccleland at optonline.net> wrote:
>> A.J. Rossini wrote:
>>> Greetings!
>>>
>>> I have a few colleagues who like the idea of Sweave, but have failed
>>> to become enlightened monks of the One True Editor
>>> (http://www.dina.dk/~abraham/religion/)
>>>
>>> Are there any other Microsoft-centric editors or IDEs which have solid
>>> support for writing SWeave documents (dual R / LaTeX enhancements
>>> similar to ESS's support)?  Has anyone tried the folding editors which
>>> support Noweb?
>> Tony:
>>   I don't know what you mean by a folding editor or Microsoft-centric,
>> but I am using R, WinEdt, and MikTeX on WinXP.  I have been using Sweave
>> with this setup for several months and have been happy with it.  Thanks
>> to Uwe Ligges, the RWinEdt package provides R enhancements to WinEdt,
>> and WinEdt is configured to work with MikTeX by default (can be
>> configured to work with other LaTeX systems).
>>   The editor itself is very intuitive - all you really need to know to
>> get started is how to write *.Rnw files (via Sweave documentation and
>> the many examples on can find).  I can recommend this setup for anyone
>> working on Windows.
>>
>> http://www.winedt.com/
>>
>> http://cran.r-project.org/src/contrib/Descriptions/RWinEdt.html
>>
>> hope this helps,
>>
>> Chuck
>>
>>> (the alternative would be brainwashing, but that is generally frowned
>> upon ;-).
>>> best,
>>> -tony
>>>
>>> blindglobe at gmail.com
>>> Muttenz, Switzerland.
>>> "Commit early,commit often, and commit in a repository from which we can
>> easily
>>> roll-back your mistakes" (AJR, 4Jan05).
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>> --
>> Chuck Cleland, Ph.D.
>> NDRI, Inc.
>> 71 West 23rd Street, 8th floor
>> New York, NY 10010
>> tel: (212) 845-4495 (Tu, Th)
>> tel: (732) 512-0171 (M, W, F)
>> fax: (917) 438-0894
>>
> 
> 
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From rmh at temple.edu  Wed Jul  5 14:15:07 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed,  5 Jul 2006 08:15:07 -0400 (EDT)
Subject: [R] R 2.3.1 on OSX: scrolling command line
Message-ID: <20060705081507.BDR57674@po-d.temple.edu>

Try
   M-x toggle-truncate-lines
If that solves the problem, you can add it to your site-start.el

Rich


From blindglobe at gmail.com  Wed Jul  5 14:23:09 2006
From: blindglobe at gmail.com (A.J. Rossini)
Date: Wed, 5 Jul 2006 14:23:09 +0200
Subject: [R] Editors which have strong/solid support for SWeave?
In-Reply-To: <44ABA7EB.8060807@optonline.net>
References: <1abe3fa90607050114p1d77c792p714282dd39fe05b6@mail.gmail.com>
	<44AB89CB.5000609@optonline.net>
	<1abe3fa90607050307p20b9ba8ct615cf8fb698816c7@mail.gmail.com>
	<44ABA7EB.8060807@optonline.net>
Message-ID: <1abe3fa90607050523y7cf211d9qcbac4ac6c95b3bef@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060705/c89effad/attachment.pl 

From blindglobe at gmail.com  Wed Jul  5 14:24:23 2006
From: blindglobe at gmail.com (A.J. Rossini)
Date: Wed, 5 Jul 2006 14:24:23 +0200
Subject: [R] Editors which have strong/solid support for SWeave?
In-Reply-To: <44ABA73B.1080402@statistik.uni-dortmund.de>
References: <1abe3fa90607050114p1d77c792p714282dd39fe05b6@mail.gmail.com>
	<44AB89CB.5000609@optonline.net>
	<1abe3fa90607050307p20b9ba8ct615cf8fb698816c7@mail.gmail.com>
	<44ABA73B.1080402@statistik.uni-dortmund.de>
Message-ID: <1abe3fa90607050524j794a8b46y41496ef7e94b41d4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060705/2d46dfa5/attachment.pl 

From tuechler at gmx.at  Wed Jul  5 14:31:19 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Wed, 05 Jul 2006 13:31:19 +0100
Subject: [R] unique deletes names - intended?
In-Reply-To: <Pine.LNX.4.64.0607051034290.29423@gannet.stats.ox.ac.uk>
References: <3.0.6.32.20060704173315.00aca4f8@pop.gmx.net>
	<3.0.6.32.20060704173315.00aca4f8@pop.gmx.net>
Message-ID: <3.0.6.32.20060705133119.00ad5ff0@pop.gmx.net>

Thank you for your clarification.

At 11:04 05.07.2006 +0100, Prof Brian Ripley wrote:
>On Tue, 4 Jul 2006, Heinz Tuechler wrote:
>
>> Dear All,
>>
>> as shown in the example, unique() deletes names of vector elements.
>> Is this intended?
>
>Yes.  Think of the vector as a set: it is supposed to immaterial which of 
>the duplicated elements is retained.
>
>The help page says
>
>      An object of the same type of 'x'. but if an element is equal to
>      one with a smaller index, it is removed.
>
>so it is starting with a new object, not 'x'.  However, the array method 
>works differently, so the documentation needs clarification.
>
>> Of course, one can use indexing by !duplicated() instead.
>
>Be careful, as you might get a method for [ and that might not do want you 
>intended (e.g. for a time series).
>
>
>> Greetings,
>> Heinz
>>
>> ## unique deletes names
>> v1 <- c(a=1, b=2, c=3, e=2, a=4)
>> unique(v1) # names deleted
>>
>> v1[!duplicated(v1)] # names preserved
>>
>>
>> platform       i386-pc-mingw32
>> arch           i386
>> os             mingw32
>> system         i386, mingw32
>> status         Patched
>> major          2
>> minor          3.1
>> year           2006
>> month          07
>> day            01
>> svn rev        38471
>> language       R
>> version.string Version 2.3.1 Patched (2006-07-01 r38471)
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>>
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>


From maechler at stat.math.ethz.ch  Wed Jul  5 14:51:02 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 5 Jul 2006 14:51:02 +0200
Subject: [R] p-values for robust regression
In-Reply-To: <f6b7dfdc0607050009y7e4992bese565972d21d0a501@mail.gmail.com>
References: <f6b7dfdc0607050009y7e4992bese565972d21d0a501@mail.gmail.com>
Message-ID: <17579.46518.812020.585297@stat.math.ethz.ch>

  [Oops! Written 6 hours ago, the following was accidentally not sent.]

>>>>> "Celso" == Celso Barros <celso.barros at gmail.com>
>>>>>     on Wed, 5 Jul 2006 04:09:17 -0300 writes:

    Celso> When I run rlm to obtain robust standard errors, my output does not include
    Celso> p-values. Is there any reason p-values should not be used in this case? 

yes (see also below).

    Celso> Is there an argument I could use in rlm so that the output does
    Celso> include p-values?
no.

What are the reasons?

How to properly do hypothesis testing in the context of robust
regression has partly been an open research problem.  Whereas
or has been solved in Elvezio Ronchetti's PhD thesis (1982)
by tau-tests, see chapter 7 of  Hampel, Rousseeuw, Ronchetti,
Stahel (1986), these are not (directly) related to standard
errors, and t-tests with some degrees of freedom.
Hence they are not so intuitively explainable, and not entirely
trivial to implement.  Probably this is one of reasons, why they
(tau-tests) haven't been programmed for MASS (the book and the R package).

Recent research, namely,
     Croux, C., Dhaene, G. and Hoorelbeke, D. (2003) _Robust standard
     errors for robust estimators_, Discussion Papers Series 03.16,
     K.U. Leuven, CES.
has been made use of by Matias Salibian-Barrera's roblm()
function now available as  lmrob() from package 'robustbase'.
There,  mod <- lmrob(........);  summary( mod ) 
does provide you with P-values.
But we still recommend *not* to ``believe in the P-values''
blindly, but rather base your data analysis on serious analysis
of residuals and other model checking.


From jhz22 at medschl.cam.ac.uk  Wed Jul  5 14:51:02 2006
From: jhz22 at medschl.cam.ac.uk (Jing Hua Zhao)
Date: Wed, 5 Jul 2006 13:51:02 +0100
Subject: [R] help with coxme
Message-ID: <6E9326598B435940A0090E6B20E051BF0D9992@me-mail1.medlan.cam.ac.uk>

Many thanks. I am actually able to check for this with S-PLUS 7 for
Linux and obtained the same error message concerning kindex in coxme. It
is thus worthwhile to give a note to the S-PLUS author before looking
into the details. Best, JH

-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com] 
Sent: 03 July 2006 17:56
To: Lei Liu
Cc: r-help at stat.math.ethz.ch; Jing hua Zhao
Subject: Re: [R] help with coxme

	  When I tried it, I seemed to get inconsistent results, the
first of 
which was irreproducible.  The first error message said that 'rats2' was

not a data.frame.  I don't know what I did to get that, but when I tried

it again, I got the following:

Error in max(kindex) : object "kindex" not found
 > traceback()
3: max(kindex)
2: max(kindex)
1: coxme(Surv(time, status) ~ rx + x2, data = rats2, random = ~(1 +
        x2) | litter)

	  This sounds to me like a logic error in 'coxme';  I am
therefore 
cc-ing the maintainer listed in help(package='kinship').

	  If it were my problem, I might list coxme and make a local
copy of 
it.  It's easy to find the offending 'max(kindex)'.  It's harder to 
figure out what kindex should be in this context.  To attempt that, I 
might request debug(coxme), then try again the 'coxme' call that 
produced the error message and trace through it line by line until I 
figured it out.

	  Hope this helps.
	  Spencer Graves

Lei Liu wrote:
> Hi there,
> 
> I have a question on fitting data by coxme. In particular I want to
fit a 
> random intercept and random slope cox model. Using the rats dataset as
an 
> example, I generated another covariate x2 and want to specify a random
slope 
> for x2. Here is my code:
> 
> x2=matrix(rep(runif(50), 3), 50, 3)
> x2=as.vector(t(x2))
> 
> rats2=cbind(rats, x2)
> 
> But when I used the coxme function as follows, it gave an error
message. 
> What is the right way to do it? Thanks a lot!
> 
>   coxme(Surv(time, status) ~ rx+x2, data=rats2, random=~ (1+x2)|litter
)
> 
> 
> Lei Liu
> Assistant Professor
> Division of Biostatistics and Epidemiology
> Dept. of Public Health Sciences
> School of Medicine
> University of Virginia
> 
> 3181 Hospital West Complex
> Charlottesville, VA 22908-0717
> 
> 1-434-982-3364 (o)
> 1-434-806-8086 (c)
> 
> liulei at virginia.edu
> ll9f at virginia.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From f.harrell at vanderbilt.edu  Wed Jul  5 15:01:06 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 05 Jul 2006 08:01:06 -0500
Subject: [R] Robust standard errors in logistic regression
In-Reply-To: <f6b7dfdc0607050050w386e2cbao2d1a1a301c648ae1@mail.gmail.com>
References: <f6b7dfdc0607040914u254a4906qf70bed0e2355ea8f@mail.gmail.com>	<20060704185417.58355140.Achim.Zeileis@wu-wien.ac.at>
	<f6b7dfdc0607050050w386e2cbao2d1a1a301c648ae1@mail.gmail.com>
Message-ID: <44ABB812.7080700@vanderbilt.edu>

Celso Barros wrote:
>      Dear Frank and Achim,
> 
>      Thanks for the help. But I must be doing something wrong. I tried to do
> as you suggested:

You didn't do everything I suggested.  Add x=TRUE, y=TRUE after the 
formula given to lrm.  The output for g will answer your other needs.
Frank

> 
>> B11<-lrm(HIGH93~HIEDYRS)
>> g<-robcov(B11)
> 
>        But I got the following message:
> 
> 
> Error in residuals.lrm(fit, type = if (method == "huber") "score" else
> "hscore") :
>         you did not specify y=T in the fit
> 
>        By the way, I was wondering if there is a way to use rlm (from MASS)
> to estimate robust standard errors for logistic regression? I am more
> familiar with rlm than with packages such as sandwich.
> 
>       rlm has the big advantage of having a very friendly output, similar to
> the familiar lm output (for instance, it has a clearly located "standard
> errors" column), and (obviously, due to my poor understanding) I find other
> outputs a little bit confusing.
> 
>         Thanks in advance,
> 
>                                         Celso
> 
> 
> On 7/4/06, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
>> On Tue, 4 Jul 2006 13:14:24 -0300 Celso Barros wrote:
>>
>>> I am trying to get robust standard errors in a logistic regression.
>>> Is there any way to do it, either in car or in MASS?
>> Package sandwich offers various types of sandwich estimators that can
>> also be applied to objects of class "glm", in particular sandwich()
>> which computes the standard Eicker-Huber-White estimate.
>>
>> These robust covariance matrices can be plugged into various inference
>> functions such as linear.hypothesis() in car, or coeftest() and
>> waldtest() in lmtest.
>>
>> See the man pages and package vignettes for examples.
>> Z
>>
>>> Thanks for the help,
>>>
>>>                                           Celso


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From m.reuter at ucl.ac.uk  Wed Jul  5 15:00:14 2006
From: m.reuter at ucl.ac.uk (Max Reuter)
Date: Wed, 5 Jul 2006 14:00:14 +0100
Subject: [R] R 2.3.1 on OSX: scrolling command line
In-Reply-To: <Pine.LNX.4.64.0607051146550.8359@gannet.stats.ox.ac.uk>
References: <p06002000c0d13d5ea2e8@[128.40.82.145]>
	<Pine.LNX.4.64.0607051146550.8359@gannet.stats.ox.ac.uk>
Message-ID: <p06002003c0d16787848a@[128.40.82.145]>

>It isn't R!  This is a function of readline and the terminal, not of 
>R which has not yet seen the line.
>
>What is likely is that your R install (how did you install it?) is 
>built against a readline clone (I believe there is one from NetBSD? 
>in MacOS 10.4) rather than GNU readline, and the fix is likely to be 
>to install GNU readline and then rebuild R from the sources.

This is indeed the solution! Eeverything is back to normal now, after 
I installed GNU readline (4.3) through fink and rebuilt R from source.

Many thanks!
-- 



___________________________________________________________
Max Reuter
				        
Departement of Biology
University College London
The Galton Laboratory
4 Stephenson Way
London NW1 2HE, UK

Phone: +44-20-76795095			 Fax: +44-20-76795052


From close2ceo at yahoo.com  Wed Jul  5 15:23:11 2006
From: close2ceo at yahoo.com (Xiaodong Jin)
Date: Wed, 5 Jul 2006 06:23:11 -0700 (PDT)
Subject: [R] auto load package splines
Message-ID: <20060705132311.76917.qmail@web31210.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060705/9d8ad219/attachment.pl 

From ligges at statistik.uni-dortmund.de  Wed Jul  5 15:37:54 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 05 Jul 2006 15:37:54 +0200
Subject: [R] auto load package splines
In-Reply-To: <20060705132311.76917.qmail@web31210.mail.mud.yahoo.com>
References: <20060705132311.76917.qmail@web31210.mail.mud.yahoo.com>
Message-ID: <44ABC0B2.5020508@statistik.uni-dortmund.de>

Xiaodong Jin wrote:
> May I ask how to include following procedures into R script (such as first.r) such that it will do an automatical call next time after I open R?
>    
>   Packages -> Load Packages -> splines


See ?Startup

Uwe Ligges

>   Thanks,
>   Shelton
> 
>  			
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From murdoch at stats.uwo.ca  Wed Jul  5 15:36:38 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 05 Jul 2006 09:36:38 -0400
Subject: [R] auto load package splines
In-Reply-To: <20060705132311.76917.qmail@web31210.mail.mud.yahoo.com>
References: <20060705132311.76917.qmail@web31210.mail.mud.yahoo.com>
Message-ID: <44ABC066.9040006@stats.uwo.ca>

On 7/5/2006 9:23 AM, Xiaodong Jin wrote:
> May I ask how to include following procedures into R script (such as first.r) such that it will do an automatical call next time after I open R?
>    
>   Packages -> Load Packages -> splines

You can do this through your .Rprofile file, or site-wide through the 
RHOME/etc/Rprofile file.  There's an example of the latter in the 
?Startup man page.   It can also be done by defining the environment 
variable

R_DEFAULT_PACKAGES=splines

but then *only* the packages you list will be attached (i.e. splines in 
the example above); you're probably better off working with .Rprofile.

Duncan Murdoch


From rdiaz at cnio.es  Wed Jul  5 15:55:02 2006
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Wed, 5 Jul 2006 15:55:02 +0200
Subject: [R] Editors which have strong/solid support for SWeave?
In-Reply-To: <1abe3fa90607050114p1d77c792p714282dd39fe05b6@mail.gmail.com>
References: <1abe3fa90607050114p1d77c792p714282dd39fe05b6@mail.gmail.com>
Message-ID: <200607051555.02875.rdiaz@cnio.es>

On Wednesday 05 July 2006 10:14, A.J. Rossini wrote:
> Greetings!
>
> I have a few colleagues who like the idea of Sweave, but have failed
> to become enlightened monks of the One True Editor
> (http://www.dina.dk/~abraham/religion/)
>
> Are there any other Microsoft-centric editors or IDEs which have solid
> support for writing SWeave documents (dual R / LaTeX enhancements
> similar to ESS's support)?  Has anyone tried the folding editors which
> support Noweb?


Dear Tony,


I often use Leo (http://webpages.charter.net/edreamleo/front.html) which is 
like a literate editor on steroids (folding + outlining, noweb and cweb 
support, and a _lot_ more), and I use it for all complex/long Rnw documents, 
including interacting with R ...

...but I "cheat", because the editing itself (of the "nodes" or folds), 
including submitting code to R from the R chunks, I do in emacs (with ESS).

Leo is available for Linux, Win, Mac and is written in Python.

R.


>
> (the alternative would be brainwashing, but that is generally frowned upon
> ;-).
>
> best,
> -tony
>
> blindglobe at gmail.com
> Muttenz, Switzerland.
> "Commit early,commit often, and commit in a repository from which we can
> easily roll-back your mistakes" (AJR, 4Jan05).
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Ram?n D?az-Uriarte
Bioinformatics 
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}


From dvumani at hotmail.com  Wed Jul  5 15:55:31 2006
From: dvumani at hotmail.com (Vumani Dlamini)
Date: Wed, 05 Jul 2006 13:55:31 +0000
Subject: [R] i suspect that there a memory leak in "vmmin"?
Message-ID: <BAY110-F6B6708C724FA83CADB26FA3760@phx.gbl>

Dear listers,
Am currently using MCMC approaches to estimate some parameters of my model. 
One parameter has to be updated using a tuned gamma distribution. So at each 
iteration I estimate the mean and variance of the density of the gamma 
approximation using "vmmin" (i also supply the gradient argument). For 
moderate replications the procedure works, but if I increase them R crashes. 
If instead of the tuned gamma density i use the adaptive rejection sampling 
procedure by Gilks and Wild (downloaded from their website) the procedure 
returns the results for any number of replications, thus I suspected that 
there was a memory leak in "vmmin". I am not an expert in "C"  and am only a 
windows user. Have tried some of the debugging tools available to us windows 
users: gdb, mpatrol, duma without success. Am not sure whether this is a 
bug.

Version 2.3.1 (2006-06-01)
Windows XP

Regards, Vumani



Rgui.exe caused an Access Violation at location 100dec2f in module R.dll 
Reading from location 3ff7c27d.

Registers:
eax=000063ac ebx=01e773a0 ecx=3ff7c27a edx=00000002 esi=01e254b4 
edi=00000002
eip=100dec2f esp=00e0ecb0 ebp=00e0ed78 iopl=0         nv up ei pl nz na pe 
nc
cs=001b  ss=0023  ds=0023  es=0023  fs=003b  gs=0000             
efl=00000202

Call stack:
100DEC2F  R.dll:100DEC2F  SET_TAG
100E0527  R.dll:100E0527  Rf_allocVector
100E0D37  R.dll:100E0D37  R_alloc
100F2ACA  R.dll:100F2ACA  vmmin
023F5E96  gem.dll:023F5E96  weibullMH
10088C33  R.dll:10088C33  do_dotcall
100B8827  R.dll:100B8827  Rf_eval
100BA4D2  R.dll:100BA4D2  do_set
100B86CB  R.dll:100B86CB  Rf_eval
100BA5D5  R.dll:100BA5D5  do_begin
100B86CB  R.dll:100B86CB  Rf_eval
100BB8EB  R.dll:100BB8EB  Rf_applyClosure
100B85F8  R.dll:100B85F8  Rf_eval
100BA4D2  R.dll:100BA4D2  do_set
100B86CB  R.dll:100B86CB  Rf_eval
100DB51C  R.dll:100DB51C  Rf_ReplIteration
100DBAA6  R.dll:100DBAA6  run_Rmainloop
004013CF  Rgui.exe:004013CF
00401316  Rgui.exe:00401316
00401518  Rgui.exe:00401518
00401236  Rgui.exe:00401236
00401288  Rgui.exe:00401288
7C816D4F  kernel32.dll:7C816D4F  RegisterWaitForInputIdle


From bibiko at eva.mpg.de  Wed Jul  5 15:55:47 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Wed, 5 Jul 2006 15:55:47 +0200
Subject: [R] R 2.3.1 on Mac OSX:  apply a function to 'dist()' ?
Message-ID: <57D4B2C7-9F97-421A-BDF2-832F4959471F@eva.mpg.de>


Dear all,

maybe there is someone who has an hint for me.

I have to calculate a distance matrix by using my own function to get  
the distance between two rows of my matrix.

The normal way to do this is to use two 'for loops' like

for(i in 1:nrow)
   for(j in i:nrow)
     _my_function(matrix[i,],matrix[j,])

OK. This works, but unfortunately I have a very huge matrix and it  
takes hours even on a cpu-cluster.

My question is:

Is there any chance to increase the speed of doing this?
My first idea was to apply my function to 'dist()' like 'outer 
(x,y,"fun")'. But up to know there is no implementation for that(?)  
and I don't know whether it would increase the speed.
On the other hand it would be possible to write a C-routine for that,  
but I have to use several function to calculate the distance and some  
other people who want to use this algorithm aren't familiar with C.

I would be pleased if there are any hints!

Many thanks in advance!

Cheers,

Hans


From blindglobe at gmail.com  Wed Jul  5 16:05:20 2006
From: blindglobe at gmail.com (A.J. Rossini)
Date: Wed, 5 Jul 2006 16:05:20 +0200
Subject: [R] Editors which have strong/solid support for SWeave?
In-Reply-To: <200607051555.02875.rdiaz@cnio.es>
References: <1abe3fa90607050114p1d77c792p714282dd39fe05b6@mail.gmail.com>
	<200607051555.02875.rdiaz@cnio.es>
Message-ID: <1abe3fa90607050705r11fd0b70we52dea14e0168459@mail.gmail.com>

On 7/5/06, Ramon Diaz-Uriarte <rdiaz at cnio.es> wrote:
> On Wednesday 05 July 2006 10:14, A.J. Rossini wrote:
> > Greetings!
> >
> > I have a few colleagues who like the idea of Sweave, but have failed
> > to become enlightened monks of the One True Editor
> > (http://www.dina.dk/~abraham/religion/)
> >
> > Are there any other Microsoft-centric editors or IDEs which have solid
> > support for writing SWeave documents (dual R / LaTeX enhancements
> > similar to ESS's support)?  Has anyone tried the folding editors which
> > support Noweb?
>
>
> Dear Tony,
>
>
> I often use Leo (http://webpages.charter.net/edreamleo/front.html) which is
> like a literate editor on steroids (folding + outlining, noweb and cweb
> support, and a _lot_ more), and I use it for all complex/long Rnw documents,
> including interacting with R ...
>
> ...but I "cheat", because the editing itself (of the "nodes" or folds),
> including submitting code to R from the R chunks, I do in emacs (with ESS).
>
> Leo is available for Linux, Win, Mac and is written in Python.
>

I've used Leo a few years ago, and liked it (but not enough to
convert).  I'll have to try it again.  Thanks!


best,
-tony

blindglobe at gmail.com
Muttenz, Switzerland.
"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).


From sachinj.2006 at yahoo.com  Wed Jul  5 16:25:01 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Wed, 5 Jul 2006 07:25:01 -0700 (PDT)
Subject: [R] Run-Sequence Plot
Message-ID: <20060705142501.65434.qmail@web37610.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060705/e893c958/attachment.pl 

From aaron.solomon.adelman at gmail.com  Wed Jul  5 16:25:09 2006
From: aaron.solomon.adelman at gmail.com (Aaron Solomon Adelman)
Date: Wed, 5 Jul 2006 10:25:09 -0400
Subject: [R] Exporting tables to RTF?
In-Reply-To: <mailman.7.1152093603.28450.r-help@stat.math.ethz.ch>
References: <mailman.7.1152093603.28450.r-help@stat.math.ethz.ch>
Message-ID: <3BBE99D7-8730-4D2F-BE38-ECCBCC86A680@gmail.com>

Greetings.

I'd like to thank everyone who responded to my original request.  I  
got something up and working very quickly.  I'm including the  
resulting function below in case anyone else wants to try something  
like this in the future so they can find it in the archives.

Aaron

-----

# nice.table
# This is a function which takes a table x, creates an RTF file  
containing the table, and opens it in Nisus Writer Express.
# Dependencies:  The R package xtable, textutil (command-line utility  
in Mac OS X 10.4)
# Also required:  Nisus Writer Express

# Arguments:
# x, caption, align, digits, display:  As in xtable.
# file:  The name of the output RTF file.  If not provided, then the  
caption is used for the name of the file, and the file is created on  
the Desktop.  If no caption is provided either, then the file will  
have the mysterious name "x.rtf".
# NA.string:  As in print.xtable.

# Bugs:
# Spaces in filenames don't come through properly.  A lot of this  
function is R passing commands to the shell.  If you want spaces in  
the file name, you must prefix them with "\\" to get them through the  
R interpreter and the  shell.

nice.table <- function(x, caption=NULL, align=NULL, digits=NULL,  
display=NULL, file="", NA.string="N/A")
{	library(xtable)

	xx <- xtable(x, caption=caption, align=align, digits=digits,  
display=display)

	x_file_location <- "~/Desktop/"
	if(is.null(caption))
	{	x_file_name <- "x"
	} else
	{	x_file_name <- gsub(" ", "_", caption, fixed=TRUE)
		x_file_name <- gsub(":", "-", x_file_name)
		x_file_name <- gsub("/", "_", x_file_name)
	}
	html_x_path <- paste(x_file_location, x_file_name, ".html", sep="")
	print.xtable(xx, type="html", file=html_x_path,  
caption.placement="top", NA.string=NA.string)

	if(file == "") file <- paste(x_file_location, x_file_name, ".rtf",  
sep="")
	incantation <- paste("textutil -convert rtf -output ", file, " ",  
html_x_path, sep="")
	system(incantation)
	incantation <- paste("rm ", html_x_path, sep="")
	system(incantation)

	incantation <- paste("open -b com.nisus.NisusWriter ", file, sep="")
	system(incantation)
}


-----
Aaron Solomon (ben Saul Joseph) Adelman
E-mail:  Aaron.Solomon.Adelman at gmail.com, adelmaas at musc.edu
Web-sites:  http://weirdthingoftheday.blogspot.com/ , http:// 
people.musc.edu/~adelmaas/
AOL Instant Messenger & Yahoo! Messenger:  Hiergargo
ICQ:  258691118
Jabber:  Aaron.Solomon.Adelman at gmail.com


From ggrothendieck at gmail.com  Wed Jul  5 16:35:49 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 5 Jul 2006 10:35:49 -0400
Subject: [R] Exporting tables to RTF?
In-Reply-To: <3BBE99D7-8730-4D2F-BE38-ECCBCC86A680@gmail.com>
References: <mailman.7.1152093603.28450.r-help@stat.math.ethz.ch>
	<3BBE99D7-8730-4D2F-BE38-ECCBCC86A680@gmail.com>
Message-ID: <971536df0607050735l3db837bds872d70392ea2e3d5@mail.gmail.com>

You could reduce the if statement to just this (untested):

   x_file_name <- if (is.null(caption)) "x" else chartr(" :/", "_-_", file_name)

On 7/5/06, Aaron Solomon Adelman <aaron.solomon.adelman at gmail.com> wrote:
> Greetings.
>
> I'd like to thank everyone who responded to my original request.  I
> got something up and working very quickly.  I'm including the
> resulting function below in case anyone else wants to try something
> like this in the future so they can find it in the archives.
>
> Aaron
>
> -----
>
> # nice.table
> # This is a function which takes a table x, creates an RTF file
> containing the table, and opens it in Nisus Writer Express.
> # Dependencies:  The R package xtable, textutil (command-line utility
> in Mac OS X 10.4)
> # Also required:  Nisus Writer Express
>
> # Arguments:
> # x, caption, align, digits, display:  As in xtable.
> # file:  The name of the output RTF file.  If not provided, then the
> caption is used for the name of the file, and the file is created on
> the Desktop.  If no caption is provided either, then the file will
> have the mysterious name "x.rtf".
> # NA.string:  As in print.xtable.
>
> # Bugs:
> # Spaces in filenames don't come through properly.  A lot of this
> function is R passing commands to the shell.  If you want spaces in
> the file name, you must prefix them with "\\" to get them through the
> R interpreter and the  shell.
>
> nice.table <- function(x, caption=NULL, align=NULL, digits=NULL,
> display=NULL, file="", NA.string="N/A")
> {       library(xtable)
>
>        xx <- xtable(x, caption=caption, align=align, digits=digits,
> display=display)
>
>        x_file_location <- "~/Desktop/"
>        if(is.null(caption))
>        {       x_file_name <- "x"
>        } else
>        {       x_file_name <- gsub(" ", "_", caption, fixed=TRUE)
>                x_file_name <- gsub(":", "-", x_file_name)
>                x_file_name <- gsub("/", "_", x_file_name)
>        }
>        html_x_path <- paste(x_file_location, x_file_name, ".html", sep="")
>        print.xtable(xx, type="html", file=html_x_path,
> caption.placement="top", NA.string=NA.string)
>
>        if(file == "") file <- paste(x_file_location, x_file_name, ".rtf",
> sep="")
>        incantation <- paste("textutil -convert rtf -output ", file, " ",
> html_x_path, sep="")
>        system(incantation)
>        incantation <- paste("rm ", html_x_path, sep="")
>        system(incantation)
>
>        incantation <- paste("open -b com.nisus.NisusWriter ", file, sep="")
>        system(incantation)
> }
>
>
> -----
> Aaron Solomon (ben Saul Joseph) Adelman
> E-mail:  Aaron.Solomon.Adelman at gmail.com, adelmaas at musc.edu
> Web-sites:  http://weirdthingoftheday.blogspot.com/ , http://
> people.musc.edu/~adelmaas/
> AOL Instant Messenger & Yahoo! Messenger:  Hiergargo
> ICQ:  258691118
> Jabber:  Aaron.Solomon.Adelman at gmail.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From david.shin at pearson.com  Wed Jul  5 16:59:28 2006
From: david.shin at pearson.com (Shin, David)
Date: Wed, 5 Jul 2006 09:59:28 -0500 
Subject: [R] generate bi-variate normal data
Message-ID: <6F3CF8F7047E374CAF4DCED3ED14576E0D1BB3BA@iowacexch4.ic.ncs.com>

Mark,

Thanks. I am still waiting for someone to help me with this matter. I will
appreciate for any insight from you or the others.

Thanks.

David


-----Original Message-----
From: markleeds at verizon.net [mailto:markleeds at verizon.net] 
Sent: Saturday, July 01, 2006 12:27 AM
To: Shin, David
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] generate bi-variate normal data

>From: "Shin, David" <david.shin at pearson.com>
>Date: Sat Jul 01 00:15:21 CDT 2006
>To: "'r-help at stat.math.ethz.ch'" <r-help at stat.math.ethz.ch>
>Subject: [R] generate bi-variate normal data

it's an interesting question. someone else
on this list can answer more explicitly but
i think you have to use the result for the multivariate
normal distribution ( bivariate case ) where , if the
joint is normal , then the conditional is normal also
with parameters a function of the 2 means and the elements of
the covariance matrix. the result in any decent mathematical statistics such
as  casella berger. so, given the one column, generate the other column
conditionally using the formula  and then the joint dist will be bivariate
normal.




>Dear all,
>
>I would like to generate bi-variate normal data given that the first column
>of the data is known. for example:
>I first generate a set of data using the command, 
>x <- rmvnorm(10, c(0, 0), matrix(c(1, 0, 0, 1), 2))
>
>then I would like to sum up the two columns of x:
>x.sum <- apply(x, 1, sum)
>
>now with x.sum I would like to generate another column of data, say y, that
>makes cbind(x.sum, y) follow a bi-variate normal distribution with mean =
>c(0, 0) and sigma = matrix(c(1, 0, 0, 1),2)
>
>I will appreciate for all insights.
>
>David s.
>
>***************************************************************************
* 
>This email may contain confidential material.\ If you were n...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

**************************************************************************** 
This email may contain confidential material.\ If you were n...{{dropped}}


From therneau at mayo.edu  Wed Jul  5 17:11:23 2006
From: therneau at mayo.edu (Terry Therneau)
Date: Wed, 5 Jul 2006 10:11:23 -0500 (CDT)
Subject: [R] Problem with coxme
Message-ID: <200607051511.k65FBND01159@blindpig.mayo.edu>


------------- Begin Forwarded Message -------------

Date: Wed, 5 Jul 2006 09:09:14 -0500 (CDT)
From: Terry Therneau <therneau at mayo.edu>
Subject: RE: Problem with coxme
To: jhz22 at medschl.cam.ac.uk
Cc: R-help at stat.mat.ethz.ch, liulei at virginia.edu, spencer.graves at pdf.com
Content-MD5: BXKVsHtW/1I9mIUqrXBU0g==

  The original question involved a strange error message from coxme when
trying to fit a random slopes model:

     coxme(Surv(time, status) ~ rx+x2, data=rats2, random=~ (1+x2)|litter)
     
 Synopsis
   	a. coxme does not yet support random slopes models
  	b. it should recognize the request and print a sensible error, instead
 it just dies further down in the routine after it has gotton thoroughly
 confused.
        c. the actual error message is interesting, as an example of just how
 odd the error message from a confused routine can be; but has no other
 use.
 
   Thanks to Jing Zhao who forwarded the message on to me, as I prefer Splus
(blasphemy!) and don't monitor the r-help list,  and also to Spencer
Graves and others who tried to further identify the bug.


		Terry Therneau
		Mayo Clinic
		(author of coxme)

PS		 
  Random slopes are on my "to do" list, and in fact most of the infrastructure
for them is already built into the underlying C code.  Unfortunately it's
been on the list for 3 years now and may stay there until a data set crosses
my own desk that requires it (or there is sufficient clamor from without).
In my defense, I was first sidetracked by trying to understand and support
pdMat structures, something I've since abandoned all hope of.  They are great
examples of object-oriented programming: elegant, short, code resuse, and 
incomprehensible (see www.netfunny.com/rhf/jokes/98/May/stroustrup.html).


------------- End Forwarded Message -------------


From peterdlauren at yahoo.com  Wed Jul  5 17:16:05 2006
From: peterdlauren at yahoo.com (Peter Lauren)
Date: Wed, 5 Jul 2006 08:16:05 -0700 (PDT)
Subject: [R] Colinearity Function in R
Message-ID: <20060705151605.96144.qmail@web30301.mail.mud.yahoo.com>

Is there a colinearty function implemented in R?  I
have tried help.search("colinearity") and
help.search("collinearity") and have searched for
"colinearity" and "collinearity" on
http://www.rpad.org/Rpad/Rpad-refcard.pdf but with no
success.

Many thanks in advance,
Peter Lauren.


From bibiko at eva.mpg.de  Wed Jul  5 17:37:15 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Wed, 5 Jul 2006 17:37:15 +0200
Subject: [R] R 2.3.1 on Mac OSX: apply a function to 'dist()' ?
Message-ID: <4A1B1F55-9B0F-4C3F-AE80-56CA96C12F2E@eva.mpg.de>


Dear Sarah,

many thanks for the hint.

Unfortunately my distance metric is more complex. For me it is not  
only important to know whether there is a change between rows but  
also to know where (which column) the change occurs and furthermore  
to know if there is a change in e.g. column 5 to look at column 144  
etc. in order to be able to calculate a kind of weighted distance  
matrix.
I doubt that it would be possible to decompose my metric into some of  
the common manipulations(?)

Albeit I had a look at the package ecodist (many thanks for that, now  
I can solve an other problem ;) ). Based on this mechanism of  
decomposing I will adopt this idea to prepare some subfunctions  
written in C which could be combined to solve a specific problem.  
Doing so other users can modify these a bit easier (I hope).

But nevertheless I still have a speed problem!

Best regards,

Hans


From sundar.dorai-raj at pdf.com  Wed Jul  5 17:52:58 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 05 Jul 2006 10:52:58 -0500
Subject: [R] Colinearity Function in R
In-Reply-To: <20060705151605.96144.qmail@web30301.mail.mud.yahoo.com>
References: <20060705151605.96144.qmail@web30301.mail.mud.yahoo.com>
Message-ID: <44ABE05A.8020600@pdf.com>



Peter Lauren wrote:
> Is there a colinearty function implemented in R?  I
> have tried help.search("colinearity") and
> help.search("collinearity") and have searched for
> "colinearity" and "collinearity" on
> http://www.rpad.org/Rpad/Rpad-refcard.pdf but with no
> success.
> 
> Many thanks in advance,
> Peter Lauren.
> 


Forgot one:

RSiteSearch("collinearity")

which returns (among others that may not be so helpful)

http://finzi.psych.upenn.edu/R/library/perturb/html/00Index.html

HTH,

--sundar


From tlumley at u.washington.edu  Wed Jul  5 17:56:40 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 5 Jul 2006 08:56:40 -0700 (PDT)
Subject: [R] Robust standard errors in logistic regression
In-Reply-To: <17579.30615.839209.966789@stat.math.ethz.ch>
References: <f6b7dfdc0607040914u254a4906qf70bed0e2355ea8f@mail.gmail.com>
	<20060704185417.58355140.Achim.Zeileis@wu-wien.ac.at>
	<f6b7dfdc0607050050w386e2cbao2d1a1a301c648ae1@mail.gmail.com>
	<17579.30615.839209.966789@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0607050837230.32591@homer22.u.washington.edu>

On Wed, 5 Jul 2006, Martin Maechler wrote:
>>>>>> "Celso" == Celso Barros <celso.barros at gmail.com>
>>>>>>     on Wed, 5 Jul 2006 04:50:29 -0300 writes:
>
> [...............]
>    Celso> By the way, I was wondering if there is a way to use rlm (from MASS)
>    Celso> to estimate robust standard errors for logistic regression?
>
> rlm stands for 'robust lm'.  What you need here is  'robust glm'.
>
> I've already replied to a similar message by you,
> mentioning the (relatively) new package "robustbase".
> After installing it, you can
> use
> 	robustbase::glmrob()

We have a clash of terminology here.  The "robust standard errors" that 
"sandwich" and "robcov" give are almost completely unrelated to glmrob(). 
My guess is that Celso wants glmrob(), but I don't know for sure.

The Huber/White sandwich variance estimator for parameters in an ordinary 
generalized linear model gives an estimate of the variance that is 
consistent if the systematic part of the model is correctly specified and 
conservative otherwise.  It is a computationally cheap linear 
approximation to the bootstrap.  These variance estimators seem to usually 
be called "model-robust", though I prefer Nils Hjort's suggestion of 
"model-agnostic", which avoids confusion with "robust statistics". This is 
what sandwich and robcov() do.

glmrob() and rlm() give robust estimation of regression parameters. That 
is, if the data come from a model that is close to the exponential family 
model underlying glm, the estimates will be close to the parameters from 
that exponential family model.  This is a more common statistical sense of 
the term "robust".


I think the confusion has been increased by the fact that earlier S 
implementations of robust regression didn't provide standard errors, 
whereas rlm() and glmrob() do. This was partly a quality-of-implementation 
issue and partly because of theoretical difficulties with, eg, lms().


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From sundar.dorai-raj at pdf.com  Wed Jul  5 18:01:31 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 05 Jul 2006 11:01:31 -0500
Subject: [R] i suspect that there a memory leak in "vmmin"?
In-Reply-To: <BAY110-F6B6708C724FA83CADB26FA3760@phx.gbl>
References: <BAY110-F6B6708C724FA83CADB26FA3760@phx.gbl>
Message-ID: <44ABE25B.7080506@pdf.com>



Vumani Dlamini wrote:
> Dear listers,
> Am currently using MCMC approaches to estimate some parameters of my model. 
> One parameter has to be updated using a tuned gamma distribution. So at each 
> iteration I estimate the mean and variance of the density of the gamma 
> approximation using "vmmin" (i also supply the gradient argument). For 
> moderate replications the procedure works, but if I increase them R crashes. 
> If instead of the tuned gamma density i use the adaptive rejection sampling 
> procedure by Gilks and Wild (downloaded from their website) the procedure 
> returns the results for any number of replications, thus I suspected that 
> there was a memory leak in "vmmin". I am not an expert in "C"  and am only a 
> windows user. Have tried some of the debugging tools available to us windows 
> users: gdb, mpatrol, duma without success. Am not sure whether this is a 
> bug.
> 
> Version 2.3.1 (2006-06-01)
> Windows XP
> 
> Regards, Vumani
> 
> 
> 
> Rgui.exe caused an Access Violation at location 100dec2f in module R.dll 
> Reading from location 3ff7c27d.
> 
> Registers:
> eax=000063ac ebx=01e773a0 ecx=3ff7c27a edx=00000002 esi=01e254b4 
> edi=00000002
> eip=100dec2f esp=00e0ecb0 ebp=00e0ed78 iopl=0         nv up ei pl nz na pe 
> nc
> cs=001b  ss=0023  ds=0023  es=0023  fs=003b  gs=0000             
> efl=00000202
> 
> Call stack:
> 100DEC2F  R.dll:100DEC2F  SET_TAG
> 100E0527  R.dll:100E0527  Rf_allocVector
> 100E0D37  R.dll:100E0D37  R_alloc
> 100F2ACA  R.dll:100F2ACA  vmmin
> 023F5E96  gem.dll:023F5E96  weibullMH
> 10088C33  R.dll:10088C33  do_dotcall
> 100B8827  R.dll:100B8827  Rf_eval
> 100BA4D2  R.dll:100BA4D2  do_set
> 100B86CB  R.dll:100B86CB  Rf_eval
> 100BA5D5  R.dll:100BA5D5  do_begin
> 100B86CB  R.dll:100B86CB  Rf_eval
> 100BB8EB  R.dll:100BB8EB  Rf_applyClosure
> 100B85F8  R.dll:100B85F8  Rf_eval
> 100BA4D2  R.dll:100BA4D2  do_set
> 100B86CB  R.dll:100B86CB  Rf_eval
> 100DB51C  R.dll:100DB51C  Rf_ReplIteration
> 100DBAA6  R.dll:100DBAA6  run_Rmainloop
> 004013CF  Rgui.exe:004013CF
> 00401316  Rgui.exe:00401316
> 00401518  Rgui.exe:00401518
> 00401236  Rgui.exe:00401236
> 00401288  Rgui.exe:00401288
> 7C816D4F  kernel32.dll:7C816D4F  RegisterWaitForInputIdle
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

It's not clear to my why you suspect "vmmin" (aka BFGS) rather than how 
you are using said function? Can you provide a re-producible example as 
the posting guide asks?

Also, are you using "optim" in R? Or are you calling vmmin from C-code? 
Again, an example would clear up many of the questions you have asked.

--sundar


From deepayan.sarkar at gmail.com  Wed Jul  5 18:16:16 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 5 Jul 2006 11:16:16 -0500
Subject: [R] coloring individual points in lattice xyplot
In-Reply-To: <971536df0607042105v1853b7f7y88bd7b774fa4ff53@mail.gmail.com>
References: <971536df0607040842v69c39f8bl7b13bae45c0b0fe1@mail.gmail.com>
	<eb555e660607041124g3df6e08fvee0506b09736df68@mail.gmail.com>
	<971536df0607041131g3321c410kbda7f191c3883605@mail.gmail.com>
	<971536df0607041929s3c6b9ccei204ea42ed1004f61@mail.gmail.com>
	<eb555e660607042005i5c760480xa2b63738164e3611@mail.gmail.com>
	<971536df0607042105v1853b7f7y88bd7b774fa4ff53@mail.gmail.com>
Message-ID: <eb555e660607050916u4a92347o752be78559be848f@mail.gmail.com>

On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> OK.  It looks like I need to go to the lower level llines and lpoints to
> do this.  I wrote a panel routine, mypanel, and it seems to work (see
> below); however, currently it assumes types and cols are in the global
> environment or at least somewhere where they will be found.
>
> 1. Is it somehow possible to stick these into some structures set up by
> lattice already and then retrieve them from lattice from within mypanel?

Not sure what you mean by that, but I would do something like the
following (this allows 'col' to be a list as in your original post).
Note that arguments to the panel function can be supplied directly to
the high level function (that's why graphical parameters can be
supplied to xyplot in the first place).

mypanel <-
    function(x, y, subscripts, groups,
             col = 1,
             type = "p",
             ...)
{
    col <- rep(as.list(col), length = nlevels(groups))
    type <- rep(as.list(type), length = nlevels(groups))
    for(g in 1:nlevels(groups)) {
         idx <- g == groups[subscripts]
         xx <- x[idx]; yy <- y[idx];
         panel.xyplot(x[idx], y[idx],
                      col = col[[g]],
                      type = type[[g]],
                      ...)
    }
}

xyplot(y ~ c(x,x), groups = factor(col(y)),
       panel = mypanel,
       type = c("o", "p"),
       col = list("black", 1:10))


> 2. Any other improvements to the example below?
>
> mypanel <- function(x, y, subscripts, groups, ...) {
>       for(g in 1:nlevels(groups)) {
>          idx <- g == groups

This won't work for more than one panel.

>          xx <- x[idx]; yy <- y[idx]; ccols <- cols[subscripts][idx]
>          if (any(idx)) {
>             switch(types[g],
>                p = lpoints(xx, yy, col = ccols),
>                l = llines(xx, yy, col = ccols),
>                o = { lpoints(xx, yy, col = ccols)
>                         llines(xx, yy, col = ccols) })
>          }
>        }
>     }
>
> x <- 1:10
> y <- cbind(y1 = x, y2 = x+1)
> cols <- c(rep(1,10), 1:10)
> types <- c("o", "p")
> xyplot(y ~ c(x,x), groups = factor(col(y)), type = types, panel = mypanel)
>
>
>
> On 7/4/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> > On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > I can get the types to work or the colors but not both:
> >
> > Sorry if I wasn't clear, but I didn't mean that you could use
> > panel.superpose[.2] to do what you wanted. I only meant that you could
> > use it as a template that may help you to write your own panel
> > function. What you want is not possible with tools available in
> > lattice.
> >
> > Deepayan
> >
> > > # this gets the types right but not the colors
> > > library(lattice)
> > > x <- 1:10
> > > y <- cbind(y1 = x, y2 = x+1)
> > > cols <- c(rep(1,10), 1:10)
> > > xyplot(y ~ c(x,x), groups = col(y), type = c("o", "p"),
> > >    panel = function(x, y, subscripts, groups, ...)
> > >       panel.superpose.2(x, y, subscripts, groups, col = cols[subscripts], ...)
> > > )
> > >
> > >
> > > # this gets the colors right but not the types
> > > library(lattice)
> > > x <- 1:10
> > > y <- cbind(y1 = x, y2 = x+1)
> > > cols <- c(rep(1,10), 1:10)
> > > xyplot(y ~ c(x,x), groups = col(y), type = c("o", "p"),
> > >    panel = function(x, y, subscripts, groups, ...)
> > >       panel.xyplot(x, y, col = cols[subscripts], ...)
> > > )
> > >
> > >
> > > On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > On 7/4/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> > > > > On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > > > If I wish to color groups in xyplot I can do this:
> > > > > >
> > > > > >    library(lattice)
> > > > > >    x <- 1:10
> > > > > >    y <- cbind(x, x+1)
> > > > > >    xyplot(y ~ rep(x,2), group = col(y), col = 1:2)
> > > > > >
> > > > > > How do I color different points differently within a group.
> > > > > >
> > > > > > For example, I want to produce this plot (except that I only
> > > > > > want to have two groups, not 11):
> > > > > >
> > > > > >    xyplot(y ~ rep(x,2), group = c(rep(1, 10), 2:11), col = 1:11)
> > > > > >
> > > > > > I am thinking of something like this (although
> > > > > > this does not work, its just to get the idea across):
> > > > > >
> > > > > >    xyplot(y ~ rep(x,2), group = col(y), col = list(1, 2:11))
> > > > > >
> > > > > > where, in general, I have a list with one component per group
> > > > > > whose elements are scalars to color the whole group or
> > > > > > vectors one color per point in the group.  I don't know
> > > > > > ahead of time what the list is.
> > > > > >
> > > > > > I am looking for a general approach to this within the lattice
> > > > > > xyplot plot framework; the above is just an example.
> > > > >
> > > > > The general approach is to write your own panel function. For a
> > > > > possible template, look at the functions panel.superpose and
> > > > > panel.superpose.2 and how they handle the 'type' argument.
> > > > >
> > > > > Deepayan
> > > > >
> > > >
> > > > There is no example in ?panel.superpose.  Do you think you
> > > > could provide an example for the situation in my post?
> > > >
> > > > I have done quite a bit of RSiteSearch'ing and googling prior to
> > > > posting and all the examples I found had colors that depended
> > > > on the group, none addressed the situation in my post -- i.e.
> > > > coloring individual points within groups.
> > > >
> > >
> >
> >
> > --
> > http://www.stat.wisc.edu/~deepayan/
> >
>


-- 
http://www.stat.wisc.edu/~deepayan/


From rdiaz at cnio.es  Wed Jul  5 18:42:42 2006
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Wed, 5 Jul 2006 18:42:42 +0200
Subject: [R] Editors which have strong/solid support for SWeave?
In-Reply-To: <1abe3fa90607050705r11fd0b70we52dea14e0168459@mail.gmail.com>
References: <1abe3fa90607050114p1d77c792p714282dd39fe05b6@mail.gmail.com>
	<200607051555.02875.rdiaz@cnio.es>
	<1abe3fa90607050705r11fd0b70we52dea14e0168459@mail.gmail.com>
Message-ID: <200607051842.42590.rdiaz@cnio.es>

On Wednesday 05 July 2006 16:05, A.J. Rossini wrote:
> On 7/5/06, Ramon Diaz-Uriarte <rdiaz at cnio.es> wrote:
> > On Wednesday 05 July 2006 10:14, A.J. Rossini wrote:
> > > Greetings!
> > >
> > > I have a few colleagues who like the idea of Sweave, but have failed
> > > to become enlightened monks of the One True Editor
> > > (http://www.dina.dk/~abraham/religion/)
> > >
> > > Are there any other Microsoft-centric editors or IDEs which have solid
> > > support for writing SWeave documents (dual R / LaTeX enhancements
> > > similar to ESS's support)?  Has anyone tried the folding editors which
> > > support Noweb?
> >
> > Dear Tony,
> >
> >
> > I often use Leo (http://webpages.charter.net/edreamleo/front.html) which
> > is like a literate editor on steroids (folding + outlining, noweb and
> > cweb support, and a _lot_ more), and I use it for all complex/long Rnw
> > documents, including interacting with R ...
> >
> > ...but I "cheat", because the editing itself (of the "nodes" or folds),
> > including submitting code to R from the R chunks, I do in emacs (with
> > ESS).
> >
> > Leo is available for Linux, Win, Mac and is written in Python.
>
> I've used Leo a few years ago, and liked it (but not enough to
> convert).  I'll have to try it again.  Thanks!


>From my Leo's usage patterns I think I'm still praying at the emacs church. I 
guess my soul is saved (for now).


But I find Leo great, and I always wish I could use it more. Making it 
understand R syntax for syntax highlighting seems to be relatively easy, more 
so with the recent changes in Leo's code 
(http://webpages.charter.net/edreamleo/coloring.html), and at least one other 
R user who also frequents R-help, Ed Borasky, is interested in these issues 
(http://sourceforge.net/forum/forum.php?thread_id=1524935&forum_id=10226).


I think what would be a real blast is to have Leo understand R (and LaTeX), 
more or less the way leo understands Python. For instance, when one imports a 
Python file it gets broken down ("outlined") by function, method, etc. This 
seems doable (e.g., http://sourceforge.net/forum/message.php?msg_id=3614539), 
but I haven't yet had time to look at it. And then, Leo also offers a general 
way (which I think is still only fully exploited with Python files) for 
autocompletion, etc, (though this seems to be a harder problem).


Just my random ramblings.

Best,

R.







>
>
> best,
> -tony
>
> blindglobe at gmail.com
> Muttenz, Switzerland.
> "Commit early,commit often, and commit in a repository from which we can
> easily roll-back your mistakes" (AJR, 4Jan05).

-- 
Ram??n D??az-Uriarte
Bioinformatics 
Centro Nacional de Investigaciones Oncol??gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern??ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}


From andy_liaw at merck.com  Wed Jul  5 18:50:28 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 5 Jul 2006 12:50:28 -0400
Subject: [R] generate bi-variate normal data
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA028847E1@usctmx1106.merck.com>

From: Shin, David
> 
> Dear all,
> 
> I would like to generate bi-variate normal data given that 
> the first column of the data is known. for example:
> I first generate a set of data using the command, x <- 
> rmvnorm(10, c(0, 0), matrix(c(1, 0, 0, 1), 2))
> 
> then I would like to sum up the two columns of x:
> x.sum <- apply(x, 1, sum)
> 
> now with x.sum I would like to generate another column of 
> data, say y, that makes cbind(x.sum, y) follow a bi-variate 
> normal distribution with mean = c(0, 0) and sigma = 
> matrix(c(1, 0, 0, 1),2)

x.sum as you described would be distributed as normal with mean=0 and
variance=2 (so you might as well just use x.sum <- rnorm(10, 0, sqrt(2))),
so I don't see how you can get to the second step where you want x.sum to
have variance=1.  Also, since the covariances are 0, you could just generate
the columns separately using rnorm() and cbind() them together.

It might be helpful for you to get some basic understanding of math stat.  I
only say that because most likely there are other steps to whatever task you
are doing (people are unlikely to be generating random numbers just for
kicks), and there's no telling what other things you are doing
inefficiently, or even erroneously.

Andy

 
> I will appreciate for all insights.
> 
> David s.
> 
> **************************************************************
> **************
> This email may contain confidential material.\ If you were...{{dropped}}


From CodyH at BaylorHealth.edu  Wed Jul  5 18:54:14 2006
From: CodyH at BaylorHealth.edu (Hamilton, Cody)
Date: Wed, 5 Jul 2006 11:54:14 -0500
Subject: [R] Invoking SAS in order to use sas.get
Message-ID: <E52E20F6B4A2F548B16BB259DD5168CF02FB67FD@BHDAEXCH11.bhcs.pvt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060705/e29a6b98/attachment.pl 

From luo_weijun at yahoo.com  Wed Jul  5 19:00:04 2006
From: luo_weijun at yahoo.com (Luo Weijun)
Date: Wed, 5 Jul 2006 10:00:04 -0700 (PDT)
Subject: [R] install RMySQL  under Mac OS X 10.4.7
Message-ID: <20060705170004.99325.qmail@web35505.mail.mud.yahoo.com>

Hello All,

I tried to install RMySQL package, but the error
messages says there is no such package, even though I
did see RMySQL is there in the contributed package
list in all mirror sites of CRAN I tried. Not sure
what is the problem.

> mysql.home <- '/usr/local/mysql' 
>
Sys.putenv('PKG_CPPFLAGS'=paste('-I',file.path(mysql.home,'include'),sep=''))

>
Sys.putenv('PKG_LIBS'=paste('-L',file.path(mysql.home,'lib'),'
-lmysqlclient',sep='')) 
> install.packages('RMySQL', repos =
"http://www.biometrics.mtu.edu/CRAN/",dependencies =T)
dependency ''RMySQL'' is not available
> install.packages('RMySQL', repos =
"http://www.biometrics.mtu.edu/CRAN/")                
Warning in download.packages(pkgs, destdir = tmpd,
available = available,  : 
         no package 'RMySQL' at the repositories

I tried download the package manually under unix, and
install it from within R, and no luck too. 

cd ~/download/R
curl -O
http://www.biometrics.mtu.edu/CRAN/src/contrib/RMySQL_0.5-7.tar.gz

> install.packages( 'RMySQL_0.5-7.tar.gz', repos =
NULL)
Error in gzfile(file, "r") : unable to open connection
In addition: Warning message:
cannot open compressed file
'RMySQL_0.5-7.tar.gz/DESCRIPTION' 
>

> sessionInfo()
Version 2.3.0 (2006-04-24) 
powerpc-apple-darwin8.6.0 

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices"
"utils"     "datasets" 
[7] "base"     


could anybody tell me what happens here, and how to
fix the problem? Thank you so much!
Weijun


From david.shin at pearson.com  Wed Jul  5 19:14:23 2006
From: david.shin at pearson.com (Shin, David)
Date: Wed, 5 Jul 2006 12:14:23 -0500 
Subject: [R] generate bi-variate normal data
Message-ID: <6F3CF8F7047E374CAF4DCED3ED14576E0D1BB3C2@iowacexch4.ic.ncs.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060705/409e494d/attachment.pl 

From matias at stat.ubc.ca  Wed Jul  5 19:19:10 2006
From: matias at stat.ubc.ca (Matias Salibian-Barrera)
Date: Wed, 05 Jul 2006 10:19:10 -0700
Subject: [R] [RsR]  p-values for robust regression
In-Reply-To: <17579.46518.812020.585297@stat.math.ethz.ch>
References: <f6b7dfdc0607050009y7e4992bese565972d21d0a501@mail.gmail.com>
	<17579.46518.812020.585297@stat.math.ethz.ch>
Message-ID: <44ABF48E.4090307@stat.ubc.ca>


Dear Celso,

I would only add a few comments to Martin's explanation below:

- tau-tests were primarily meant for general nested hypotheses, whereas 
for a hypothesis of the form "beta_j = 0" for a single index "j" one can 
also use (as it's done for glm estimators, say) *approximate* p-values 
based on a normal approximation to the distribution of the ratio 
"estimator / standard error" -- these are the p-values that 
"summary.lmrob" currently reports, based on the *robust standard errors* 
of Croux et al. 2003 (full reference in Martin's e-mail below) that 
remain valid even when the data may contain asymmetric outliers;

- the asymptotic distribution of tau-tests is known for symmetrically 
distributed errors and, furthermore, it involves a weighted sum of 
independent chi-squared distributions, with weights depending on the 
eigenvalues of the (asymptotic) covariance matrix of the explanatory 
variables. Not surprisingly, their p-values are rather difficult to 
calculate in practice (although approximations do exist: see Alfio 
Marazzi's ROBETH and S-PLUS's robust libraries);

- for nested linear hypotheses, the tests in Markatou and Hettmansperger 
(1990, "Robust bounded-influence tests in linear models", JASA, 85, 
187-190) provide an alternative to the tau-tests with the "usual" 
asymptotic chi-squared distribution, although this asymptotic 
approximation is also known to hold for symmetrically distributed 
errors, and moreover, seems to be rather sensitive to the presence of 
outliers (see my paper in JSPI, 2005, 128, 241-257), while the Robust 
Bootstrap performs quite well in estimating the p-values for these 
robust tests.


Summarizing:

- the standard errors and p-values for individual hypotheses of the form 
"beta_j=0" reported by summary.lmrob (in robustbase) are (robust) 
asymptotic approximations, which should be interpreted and used accordingly;
- if you're interested in nested linear hypotheses, there are some 
proposals in the literature to obtain robust p-values for robust tests 
although they have not been implemented in robustbase yet (hopefully 
they will be in the near future).

Best,

Matias

--
______________________________________________________________
Matias Salibian-Barrera - Department of Statistics
University of British Columbia - matias at stat.ubc.ca
Phone: (604) 822-3410 - Fax: (604) 822-6960


Martin Maechler wrote:
>   [Oops! Written 6 hours ago, the following was accidentally not sent.]
> 
>>>>>> "Celso" == Celso Barros <celso.barros at gmail.com>
>>>>>>     on Wed, 5 Jul 2006 04:09:17 -0300 writes:
> 
>     Celso> When I run rlm to obtain robust standard errors, my output does not include
>     Celso> p-values. Is there any reason p-values should not be used in this case? 
> 
> yes (see also below).
> 
>     Celso> Is there an argument I could use in rlm so that the output does
>     Celso> include p-values?
> no.
> 
> What are the reasons?
> 
> How to properly do hypothesis testing in the context of robust
> regression has partly been an open research problem.  Whereas
> or has been solved in Elvezio Ronchetti's PhD thesis (1982)
> by tau-tests, see chapter 7 of  Hampel, Rousseeuw, Ronchetti,
> Stahel (1986), these are not (directly) related to standard
> errors, and t-tests with some degrees of freedom.
> Hence they are not so intuitively explainable, and not entirely
> trivial to implement.  Probably this is one of reasons, why they
> (tau-tests) haven't been programmed for MASS (the book and the R package).
> 
> Recent research, namely,
>      Croux, C., Dhaene, G. and Hoorelbeke, D. (2003) _Robust standard
>      errors for robust estimators_, Discussion Papers Series 03.16,
>      K.U. Leuven, CES.
> has been made use of by Matias Salibian-Barrera's roblm()
> function now available as  lmrob() from package 'robustbase'.
> There,  mod <- lmrob(........);  summary( mod ) 
> does provide you with P-values.
> But we still recommend *not* to ``believe in the P-values''
> blindly, but rather base your data analysis on serious analysis
> of residuals and other model checking.
> 
> _______________________________________________
> R-SIG-Robust at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-robust
> 
> 


--


From ripley at stats.ox.ac.uk  Wed Jul  5 19:25:13 2006
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Wed, 5 Jul 2006 18:25:13 +0100 (BST)
Subject: [R] install RMySQL  under Mac OS X 10.4.7
In-Reply-To: <20060705170004.99325.qmail@web35505.mail.mud.yahoo.com>
Message-ID: <Pine.GSO.4.31.0607051822390.26715-100000@markov.stats>

If this is R as installed from CRAN, it is by default looking for a binary
and not a source package.

Please study the `R Installation and Administration' manual.

On Wed, 5 Jul 2006, Luo Weijun wrote:

> Hello All,
>
> I tried to install RMySQL package, but the error
> messages says there is no such package, even though I
> did see RMySQL is there in the contributed package
> list in all mirror sites of CRAN I tried. Not sure
> what is the problem.
>
> > mysql.home <- '/usr/local/mysql'
> >
> Sys.putenv('PKG_CPPFLAGS'=paste('-I',file.path(mysql.home,'include'),sep=''))
>
> >
> Sys.putenv('PKG_LIBS'=paste('-L',file.path(mysql.home,'lib'),'
> -lmysqlclient',sep=''))
> > install.packages('RMySQL', repos =
> "http://www.biometrics.mtu.edu/CRAN/",dependencies =T)
> dependency ''RMySQL'' is not available
> > install.packages('RMySQL', repos =
> "http://www.biometrics.mtu.edu/CRAN/")
> Warning in download.packages(pkgs, destdir = tmpd,
> available = available,  :
>          no package 'RMySQL' at the repositories
>
> I tried download the package manually under unix, and
> install it from within R, and no luck too.
>
> cd ~/download/R
> curl -O
> http://www.biometrics.mtu.edu/CRAN/src/contrib/RMySQL_0.5-7.tar.gz

And where did you run R from?  I think you needed to give a full path here
....

> > install.packages( 'RMySQL_0.5-7.tar.gz', repos =
> NULL)
> Error in gzfile(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open compressed file
> 'RMySQL_0.5-7.tar.gz/DESCRIPTION'
> >
>
> > sessionInfo()
> Version 2.3.0 (2006-04-24)
> powerpc-apple-darwin8.6.0
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices"
> "utils"     "datasets"
> [7] "base"
>
>
> could anybody tell me what happens here, and how to
> fix the problem? Thank you so much!
> Weijun
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Charles.Annis at StatisticalEngineering.com  Wed Jul  5 19:28:22 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Wed, 5 Jul 2006 13:28:22 -0400
Subject: [R] tcl/tk with R
Message-ID: <03b301c6a058$6a9b32c0$6600a8c0@DD4XFW31>

Greetings:

I would like to use tcl/tk with R, and have read "A Primer on the R-Tcl/Tk
Package" by Peter Dalgaard in Rnews, Volume 1/3, September 2001.

Are there more recent do-it-yourself instructions available?

I have been unsuccessful with the example in the tcltk2 package due to a
syntax error.

I think that I have isolated the problem to this code snippet:

for (i in 1:length(Themes)) {
tkadd(themeMenu, "command", label = Themes[i], command = eval(parse(text =
paste("function() tk2theme(\"", Themes[sep = "")))) }

And the problem seems to be here:

paste("function() tk2theme(\"", Themes[sep = ""))

Error: syntax error in "paste("function() tk2theme(\"", Themes[sep = "")"


Further R seems not to like ' \" '

A closer look at the paste command suggests that it is incomplete and should
also contain something like

Themes[i], sep=""


I am using R Version 2.3.1 (2006-06-01) on a DELL WindowsXP system with 2
gig RAM.


Many thanks for your counsel.


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:? 614-455-3265
http://www.StatisticalEngineering.com
?


From ligges at statistik.uni-dortmund.de  Wed Jul  5 19:35:26 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 05 Jul 2006 19:35:26 +0200
Subject: [R] Invoking SAS in order to use sas.get
In-Reply-To: <E52E20F6B4A2F548B16BB259DD5168CF02FB67FD@BHDAEXCH11.bhcs.pvt>
References: <E52E20F6B4A2F548B16BB259DD5168CF02FB67FD@BHDAEXCH11.bhcs.pvt>
Message-ID: <44ABF85E.3080607@statistik.uni-dortmund.de>

Hamilton, Cody wrote:
> I am trying to use the sas.get function to bring a SAS dataset into R.
> After calling the sas.get function I got an error message stating that
> 'sas' is not a recognized internal or external command, etc.  I am
> guessing that I need to specify the internal command to invoke SAS in
> the sasprog option for the sas.get function, but I don't know how to
> determine what that command might be.  Any thoughts would be
> appreciated.
> 
>

What is sas.get()?
If you are talking about read.ssd() in package "foreign", you have to 
specify the path to the sas executable. If you are on Windows (your mail 
header suggests, but you have not told it), this might begin with 
"c:/Program files/........." unless you have sas in your PATH already.

Uwe Ligges



> 
> Cody Hamilton, Ph.D
> 
> Institute for Health Care Research and Improvement
> 
> Baylor Health Care System
> 
> (214) 265-3618
> 
> 
> 
> 
> 
> This e-mail, facsimile, or letter and any files or attachments transmitted with it contains information that is confidential and privileged. This information is intended only for the use of the individual(s) and entity(ies) to whom it is addressed. If you are the intended recipient, further disclosures are prohibited without proper authorization. If you are not the intended recipient, any disclosure, copying, printing, or use of this information is strictly prohibited and possibly a violation of federal or state law and regulations. If you have received this information in error, please notify Baylor Health Care System immediately at 1-866-402-1661 or via e-mail at privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries, and affiliates hereby claim all applicable privileges related to this information.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From CodyH at BaylorHealth.edu  Wed Jul  5 19:35:40 2006
From: CodyH at BaylorHealth.edu (Hamilton, Cody)
Date: Wed, 5 Jul 2006 12:35:40 -0500
Subject: [R] Invoking SAS in order to use sas.get
In-Reply-To: <44ABF85E.3080607@statistik.uni-dortmund.de>
Message-ID: <E52E20F6B4A2F548B16BB259DD5168CF02FB67FF@BHDAEXCH11.bhcs.pvt>


Sas.get is a function available from the Hmisc library.  I am using
Windows (sorry for not mentioning that).

-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
Sent: Wednesday, July 05, 2006 12:35 PM
To: Hamilton, Cody
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Invoking SAS in order to use sas.get

Hamilton, Cody wrote:
> I am trying to use the sas.get function to bring a SAS dataset into R.
> After calling the sas.get function I got an error message stating that
> 'sas' is not a recognized internal or external command, etc.  I am
> guessing that I need to specify the internal command to invoke SAS in
> the sasprog option for the sas.get function, but I don't know how to
> determine what that command might be.  Any thoughts would be
> appreciated.
>
>

What is sas.get()?
If you are talking about read.ssd() in package "foreign", you have to
specify the path to the sas executable. If you are on Windows (your mail

header suggests, but you have not told it), this might begin with
"c:/Program files/........." unless you have sas in your PATH already.

Uwe Ligges



>
> Cody Hamilton, Ph.D
>
> Institute for Health Care Research and Improvement
>
> Baylor Health Care System
>
> (214) 265-3618
>
>
>
>
>
> This e-mail, facsimile, or letter and any files or attachments
transmitted with it contains information that is confidential and
privileged. This information is intended only for the use of the
individual(s) and entity(ies) to whom it is addressed. If you are the
intended recipient, further disclosures are prohibited without proper
authorization. If you are not the intended recipient, any disclosure,
copying, printing, or use of this information is strictly prohibited and
possibly a violation of federal or state law and regulations. If you
have received this information in error, please notify Baylor Health
Care System immediately at 1-866-402-1661 or via e-mail at
privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries,
and affiliates hereby claim all applicable privileges related to this
information.
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


This e-mail, facsimile, or letter and any files or attachments transmitted with it contains information that is confidential and privileged. This information is intended only for the use of the individual(s) and entity(ies) to whom it is addressed. If you are the intended recipient, further disclosures are prohibited without proper authorization. If you are not the intended recipient, any disclosure, copying, printing, or use of this information is strictly prohibited and possibly a violation of federal or state law and regulations. If you have received this information in error, please notify Baylor Health Care System immediately at 1-866-402-1661 or via e-mail at privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries, and affiliates hereby claim all applicable privileges related to this information.


From andy_liaw at merck.com  Wed Jul  5 19:53:12 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 5 Jul 2006 13:53:12 -0400
Subject: [R] generate bi-variate normal data
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02884806@usctmx1106.merck.com>

If (X, Y) ~ N(mx, my, sx^2, sy^2, r)  (r being the correlation, not
covariance), then 

Y|X ~ N(my + r * (sy / sx) * (X - mx), sy^2 * (1 - r^2))

You can use this to work out what you need to do.

Best,
Andy


________________________________

	From: Shin, David [mailto:david.shin at pearson.com] 
	Sent: Wednesday, July 05, 2006 1:14 PM
	To: Liaw, Andy; 'r-help at stat.math.ethz.ch'
	Subject: RE: [R] generate bi-variate normal data [Broadcast]
	
	

	Thanks for Andy's comment and help. I should have used a better
example for my question. Below is my exact question and I will appreciate a
lot for any insights.

	I generate a bi-variate normal distribution with mean = c(0, 0.2)
and variance covariance matrix = matrix(c(1, .025, .025, .0025), nrow = 2):

	> x <- rmvnorm(10, c(0, 0.2), matrix(c(1, .025, .025, .0025), nrow =
2))

	> x

	            [,1]      [,2]

	 [1,]  0.1595351 0.1715898

	 [2,] -0.5177577 0.1839222

	 [3,] -0.8794011 0.1896593

	 [4,]  1.0584185 0.2208470

	 [5,]  0.1960055 0.2199169

	 [6,]  0.6450406 0.1773001

	 [7,] -2.2160986 0.1810803

	 [8,]  0.2131569 0.1223121

	 [9,] -0.3598349 0.2402232

	[10,] -0.3905455 0.1787059

	> x.sum <- apply(x,1,sum)

	> x.sum

	 [1]  0.3311249 -0.3338355 -0.6897418  1.2792655  0.4159225
0.8223407 -2.0350183  0.3354690 -0.1196116 -0.2118395

	if I call x[,1] as theta.year1 and x[,2] as growth.year1 then the
mean of x.sum is 0+0.2 = -.2 and the standard deviation of x.sum is
sqrt(1+.0025+2*.5*1*.05) = 1.0259

	<<...OLE_Obj...>> 

	assume the correlation is again 0.5, I would like to generate
another bi-variate normal distribution with the fixed first column that
equals to x.sum. If the mean and SD of the second column is 0.2 and 0.05,
respectively, the mean of this bi-variate normal distribution is c(0.2, 0.2)
and the variance-covariance matrix is: matrix(c(1.0259^2, 0.5*1.0259*0.05,
0.5*1.0259*0.05, 0.0025), 2)

	Thanks again for helping me.

	David






		-----Original Message-----
	From: Liaw, Andy [mailto:andy_liaw at merck.com]
	Sent: Wednesday, July 05, 2006 11:50 AM
	To: Shin, David; 'r-help at stat.math.ethz.ch'
	Subject: RE: [R] generate bi-variate normal data

	From: Shin, David

	> 

	> Dear all,

	> 

	> I would like to generate bi-variate normal data given that 

	> the first column of the data is known. for example:

	> I first generate a set of data using the command, x <- 

	> rmvnorm(10, c(0, 0), matrix(c(1, 0, 0, 1), 2))

	> 

	> then I would like to sum up the two columns of x:

	> x.sum <- apply(x, 1, sum)

	> 

	> now with x.sum I would like to generate another column of 

	> data, say y, that makes cbind(x.sum, y) follow a bi-variate 

	> normal distribution with mean = c(0, 0) and sigma = 

	> matrix(c(1, 0, 0, 1),2)

	x.sum as you described would be distributed as normal with mean=0
and

	variance=2 (so you might as well just use x.sum <- rnorm(10, 0,
sqrt(2))),

	so I don't see how you can get to the second step where you want
x.sum to

	have variance=1.  Also, since the covariances are 0, you could just
generate

	the columns separately using rnorm() and cbind() them together.

	It might be helpful for you to get some basic understanding of math
stat.  I

	only say that because most likely there are other steps to whatever
task you

	are doing (people are unlikely to be generating random numbers just
for

	kicks), and there's no telling what other things you are doing

	inefficiently, or even erroneously.

	Andy

	 

	> I will appreciate for all insights.

	> 

	> David s.

	> 

	> **************************************************************

	> **************

	> This email may contain confidential material.\ If you were 

	> n...{{dropped}}

	> 

	> ______________________________________________

	> R-help at stat.math.ethz.ch mailing list

	> https://stat.ethz.ch/mailman/listinfo/r-help

	> PLEASE do read the posting guide! 

	> http://www.R-project.org/posting-guide.html

	> 

	> 


	
----------------------------------------------------------------------------
--

	Notice:  This e-mail message, together with any attachments,
contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse
Station, New Jersey, USA 08889), and/or its affiliates (which may be known
outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in
Japan, as Banyu) that may be confidential, proprietary copyrighted and/or
legally privileged. It is intended solely for the use of the individual or
entity named on this message.  If you are not the intended recipient, and
have received this message in error, please notify us immediately by reply
e-mail and then delete it from your system.

	
----------------------------------------------------------------------------
--

	
****************************************************************************


	This email may contain confidential 
	material. If you were not an intended recipient, 
	Please notify the sender and delete all copies. 
	We may monitor email to and from our network.


From ligges at statistik.uni-dortmund.de  Wed Jul  5 20:07:02 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 05 Jul 2006 20:07:02 +0200
Subject: [R] Invoking SAS in order to use sas.get
In-Reply-To: <E52E20F6B4A2F548B16BB259DD5168CF02FB67FF@BHDAEXCH11.bhcs.pvt>
References: <E52E20F6B4A2F548B16BB259DD5168CF02FB67FF@BHDAEXCH11.bhcs.pvt>
Message-ID: <44ABFFC6.8090704@statistik.uni-dortmund.de>

Hamilton, Cody wrote:
> Sas.get is a function available from the Hmisc library.  I am using
> Windows (sorry for not mentioning that).


Same as mentioned in my former mail also applies for sas.get() in Hmisc.

Uwe Ligges


> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> 
> Sent: Wednesday, July 05, 2006 12:35 PM
> To: Hamilton, Cody
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Invoking SAS in order to use sas.get
> 
> Hamilton, Cody wrote:
>> I am trying to use the sas.get function to bring a SAS dataset into R.
>> After calling the sas.get function I got an error message stating that
>> 'sas' is not a recognized internal or external command, etc.  I am
>> guessing that I need to specify the internal command to invoke SAS in
>> the sasprog option for the sas.get function, but I don't know how to
>> determine what that command might be.  Any thoughts would be
>> appreciated.
>>
> 
> 
> What is sas.get()?
> If you are talking about read.ssd() in package "foreign", you have to
> 
> specify the path to the sas executable. If you are on Windows (your mail
> 
> header suggests, but you have not told it), this might begin with
> 
> "c:/Program files/........." unless you have sas in your PATH already.
> 
> Uwe Ligges
> 
> 
> 
> 
>> Cody Hamilton, Ph.D
>>
> 
>> Institute for Health Care Research and Improvement
>>
> 
>> Baylor Health Care System
>>
> 
>> (214) 265-3618
>>
> 
> 
> 
> 
> 
>> This e-mail, facsimile, or letter and any files or attachments
> transmitted with it contains information that is confidential and
> privileged. This information is intended only for the use of the
> individual(s) and entity(ies) to whom it is addressed. If you are the
> intended recipient, further disclosures are prohibited without proper
> authorization. If you are not the intended recipient, any disclosure,
> copying, printing, or use of this information is strictly prohibited and
> possibly a violation of federal or state law and regulations. If you
> have received this information in error, please notify Baylor Health
> Care System immediately at 1-866-402-1661 or via e-mail at
> privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries,
> and affiliates hereby claim all applicable privileges related to this
> information.
>> 	[[alternative HTML version deleted]]
>>
> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> This e-mail, facsimile, or letter and any files or attachments transmitted with it contains information that is confidential and privileged. This information is intended only for the use of the individual(s) and entity(ies) to whom it is addressed. If you are the intended recipient, further disclosures are prohibited without proper authorization. If you are not the intended recipient, any disclosure, copying, printing, or use of this information is strictly prohibited and possibly a violation of federal or state law and regulations. If you have received this information in error, please notify Baylor Health Care System immediately at 1-866-402-1661 or via e-mail at privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries, and affiliates hereby claim all applicable privileges related to this information.


From uofiowa at gmail.com  Wed Jul  5 20:06:40 2006
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 5 Jul 2006 14:06:40 -0400
Subject: [R] fMultivar rollMax question
Message-ID: <3f87cc6d0607051106x15fd945eo158c2a27aa267ee8@mail.gmail.com>

I am using fMultivar under R 2.2.1 on a Debian linux box. Could
someone, please, explain to me why there are two trailing NAs in the
last statement in the code beow?


> library(fMultivar)
> x <- 1:20
> rollMax(x, n=3)
 [1]  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
> rollMax(x, n=2)
 [1]  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
> rollMax(x, n=1)
 [1]  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 NA NA
>


From CodyH at BaylorHealth.edu  Wed Jul  5 20:18:28 2006
From: CodyH at BaylorHealth.edu (Hamilton, Cody)
Date: Wed, 5 Jul 2006 13:18:28 -0500
Subject: [R] Invoking SAS in order to use sas.get
In-Reply-To: <44ABFFC6.8090704@statistik.uni-dortmund.de>
Message-ID: <E52E20F6B4A2F548B16BB259DD5168CF02FB6800@BHDAEXCH11.bhcs.pvt>


Thanks for your response.  I specified the path to the SAS executable
'C:/Program Files/.../sas.exe', using the sasprog option in sas.get()
and got a similar error message: C:/Program is not recognized as an
internal or external command.  I would like to apologize in advance if I
am making a very foolish mistake here . . .

-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
Sent: Wednesday, July 05, 2006 13:7 PM
To: Hamilton, Cody
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Invoking SAS in order to use sas.get

Hamilton, Cody wrote:
> Sas.get is a function available from the Hmisc library.  I am using
> Windows (sorry for not mentioning that).


Same as mentioned in my former mail also applies for sas.get() in Hmisc.

Uwe Ligges


> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
>
> Sent: Wednesday, July 05, 2006 12:35 PM
> To: Hamilton, Cody
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Invoking SAS in order to use sas.get
>
> Hamilton, Cody wrote:
>> I am trying to use the sas.get function to bring a SAS dataset into
R.
>> After calling the sas.get function I got an error message stating
that
>> 'sas' is not a recognized internal or external command, etc.  I am
>> guessing that I need to specify the internal command to invoke SAS in
>> the sasprog option for the sas.get function, but I don't know how to
>> determine what that command might be.  Any thoughts would be
>> appreciated.
>>
>
>
> What is sas.get()?
> If you are talking about read.ssd() in package "foreign", you have to
>
> specify the path to the sas executable. If you are on Windows (your
mail
>
> header suggests, but you have not told it), this might begin with
>
> "c:/Program files/........." unless you have sas in your PATH already.
>
> Uwe Ligges
>
>
>
>
>> Cody Hamilton, Ph.D
>>
>
>> Institute for Health Care Research and Improvement
>>
>
>> Baylor Health Care System
>>
>
>> (214) 265-3618
>>
>
>
>
>
>
>> This e-mail, facsimile, or letter and any files or attachments
> transmitted with it contains information that is confidential and
> privileged. This information is intended only for the use of the
> individual(s) and entity(ies) to whom it is addressed. If you are the
> intended recipient, further disclosures are prohibited without proper
> authorization. If you are not the intended recipient, any disclosure,
> copying, printing, or use of this information is strictly prohibited
and
> possibly a violation of federal or state law and regulations. If you
> have received this information in error, please notify Baylor Health
> Care System immediately at 1-866-402-1661 or via e-mail at
> privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries,
> and affiliates hereby claim all applicable privileges related to this
> information.
>> 	[[alternative HTML version deleted]]
>>
>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>
> This e-mail, facsimile, or letter and any files or attachments
transmitted with it contains information that is confidential and
privileged. This information is intended only for the use of the
individual(s) and entity(ies) to whom it is addressed. If you are the
intended recipient, further disclosures are prohibited without proper
authorization. If you are not the intended recipient, any disclosure,
copying, printing, or use of this information is strictly prohibited and
possibly a violation of federal or state law and regulations. If you
have received this information in error, please notify Baylor Health
Care System immediately at 1-866-402-1661 or via e-mail at
privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries,
and affiliates hereby claim all applicable privileges related to this
information.


This e-mail, facsimile, or letter and any files or attachments transmitted with it contains information that is confidential and privileged. This information is intended only for the use of the individual(s) and entity(ies) to whom it is addressed. If you are the intended recipient, further disclosures are prohibited without proper authorization. If you are not the intended recipient, any disclosure, copying, printing, or use of this information is strictly prohibited and possibly a violation of federal or state law and regulations. If you have received this information in error, please notify Baylor Health Care System immediately at 1-866-402-1661 or via e-mail at privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries, and affiliates hereby claim all applicable privileges related to this information.


From f.harrell at vanderbilt.edu  Wed Jul  5 20:24:10 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 05 Jul 2006 13:24:10 -0500
Subject: [R] Invoking SAS in order to use sas.get
In-Reply-To: <E52E20F6B4A2F548B16BB259DD5168CF02FB67FF@BHDAEXCH11.bhcs.pvt>
References: <E52E20F6B4A2F548B16BB259DD5168CF02FB67FF@BHDAEXCH11.bhcs.pvt>
Message-ID: <44AC03CA.8060108@vanderbilt.edu>

Hamilton, Cody wrote:
> Sas.get is a function available from the Hmisc library.  I am using
> Windows (sorry for not mentioning that).

Search for the file sas.exe to find its full path name.  Put that path 
in your Windows system path.  Or specify it in one of the sas.get 
arguments.  The first way is recommended though.

Frank

> 
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> Sent: Wednesday, July 05, 2006 12:35 PM
> To: Hamilton, Cody
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Invoking SAS in order to use sas.get
> 
> Hamilton, Cody wrote:
> 
>>I am trying to use the sas.get function to bring a SAS dataset into R.
>>After calling the sas.get function I got an error message stating that
>>'sas' is not a recognized internal or external command, etc.  I am
>>guessing that I need to specify the internal command to invoke SAS in
>>the sasprog option for the sas.get function, but I don't know how to
>>determine what that command might be.  Any thoughts would be
>>appreciated.
>>
>>
> 
> 
> What is sas.get()?
> If you are talking about read.ssd() in package "foreign", you have to
> specify the path to the sas executable. If you are on Windows (your mail
> 
> header suggests, but you have not told it), this might begin with
> "c:/Program files/........." unless you have sas in your PATH already.
> 
> Uwe Ligges
> 
> 
> 
> 
>>Cody Hamilton, Ph.D
>>
>>Institute for Health Care Research and Improvement
>>
>>Baylor Health Care System
>>
>>(214) 265-3618
>>
>>
>>
>>
>>
>>This e-mail, facsimile, or letter and any files or attachments
> 
> transmitted with it contains information that is confidential and
> privileged. This information is intended only for the use of the
> individual(s) and entity(ies) to whom it is addressed. If you are the
> intended recipient, further disclosures are prohibited without proper
> authorization. If you are not the intended recipient, any disclosure,
> copying, printing, or use of this information is strictly prohibited and
> possibly a violation of federal or state law and regulations. If you
> have received this information in error, please notify Baylor Health
> Care System immediately at 1-866-402-1661 or via e-mail at
> privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries,
> and affiliates hereby claim all applicable privileges related to this
> information.
> 
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
> 
> http://www.R-project.org/posting-guide.html
> 
> 
> This e-mail, facsimile, or letter and any files or attachments transmitted with it contains information that is confidential and privileged. This information is intended only for the use of the individual(s) and entity(ies) to whom it is addressed. If you are the intended recipient, further disclosures are prohibited without proper authorization. If you are not the intended recipient, any disclosure, copying, printing, or use of this information is strictly prohibited and possibly a violation of federal or state law and regulations. If you have received this information in error, please notify Baylor Health Care System immediately at 1-866-402-1661 or via e-mail at privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries, and affiliates hereby claim all applicable privileges related to this information.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From f.harrell at vanderbilt.edu  Wed Jul  5 20:27:55 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 05 Jul 2006 13:27:55 -0500
Subject: [R] Invoking SAS in order to use sas.get
In-Reply-To: <E52E20F6B4A2F548B16BB259DD5168CF02FB6800@BHDAEXCH11.bhcs.pvt>
References: <E52E20F6B4A2F548B16BB259DD5168CF02FB6800@BHDAEXCH11.bhcs.pvt>
Message-ID: <44AC04AB.9030908@vanderbilt.edu>

Hamilton, Cody wrote:
> Thanks for your response.  I specified the path to the SAS executable
> 'C:/Program Files/.../sas.exe', using the sasprog option in sas.get()
> and got a similar error message: C:/Program is not recognized as an
> internal or external command.  I would like to apologize in advance if I
> am making a very foolish mistake here . . .

Please read the posting guide, in particular include all the code in 
question.  Best just to fix your system path.

Frank

> 
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> Sent: Wednesday, July 05, 2006 13:7 PM
> To: Hamilton, Cody
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Invoking SAS in order to use sas.get
> 
> Hamilton, Cody wrote:
> 
>>Sas.get is a function available from the Hmisc library.  I am using
>>Windows (sorry for not mentioning that).
> 
> 
> 
> Same as mentioned in my former mail also applies for sas.get() in Hmisc.
> 
> Uwe Ligges
> 
> 
> 
>>-----Original Message-----
>>From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
>>
>>Sent: Wednesday, July 05, 2006 12:35 PM
>>To: Hamilton, Cody
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] Invoking SAS in order to use sas.get
>>
>>Hamilton, Cody wrote:
>>
>>>I am trying to use the sas.get function to bring a SAS dataset into
> 
> R.
> 
>>>After calling the sas.get function I got an error message stating
> 
> that
> 
>>>'sas' is not a recognized internal or external command, etc.  I am
>>>guessing that I need to specify the internal command to invoke SAS in
>>>the sasprog option for the sas.get function, but I don't know how to
>>>determine what that command might be.  Any thoughts would be
>>>appreciated.
>>>
>>
>>
>>What is sas.get()?
>>If you are talking about read.ssd() in package "foreign", you have to
>>
>>specify the path to the sas executable. If you are on Windows (your
> 
> mail
> 
>>header suggests, but you have not told it), this might begin with
>>
>>"c:/Program files/........." unless you have sas in your PATH already.
>>
>>Uwe Ligges
>>
>>
>>
>>
>>
>>>Cody Hamilton, Ph.D
>>>
>>
>>>Institute for Health Care Research and Improvement
>>>
>>
>>>Baylor Health Care System
>>>
>>
>>>(214) 265-3618
>>>
>>
>>
>>
>>
>>
>>>This e-mail, facsimile, or letter and any files or attachments
>>
>>transmitted with it contains information that is confidential and
>>privileged. This information is intended only for the use of the
>>individual(s) and entity(ies) to whom it is addressed. If you are the
>>intended recipient, further disclosures are prohibited without proper
>>authorization. If you are not the intended recipient, any disclosure,
>>copying, printing, or use of this information is strictly prohibited
> 
> and
> 
>>possibly a violation of federal or state law and regulations. If you
>>have received this information in error, please notify Baylor Health
>>Care System immediately at 1-866-402-1661 or via e-mail at
>>privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries,
>>and affiliates hereby claim all applicable privileges related to this
>>information.
>>
>>>	[[alternative HTML version deleted]]
>>>
>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>
>>http://www.R-project.org/posting-guide.html
>>
>>

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From andy_liaw at merck.com  Wed Jul  5 20:27:31 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 5 Jul 2006 14:27:31 -0400
Subject: [R] Invoking SAS in order to use sas.get
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02884820@usctmx1106.merck.com>

I'm speculating, but you might need to substitute "Progra~1" for "Program
Files" in the path.

Andy 

From: Hamilton, Cody
> 
> Thanks for your response.  I specified the path to the SAS 
> executable 'C:/Program Files/.../sas.exe', using the sasprog 
> option in sas.get() and got a similar error message: 
> C:/Program is not recognized as an internal or external 
> command.  I would like to apologize in advance if I am making 
> a very foolish mistake here . . .
> 
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> Sent: Wednesday, July 05, 2006 13:7 PM
> To: Hamilton, Cody
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Invoking SAS in order to use sas.get
> 
> Hamilton, Cody wrote:
> > Sas.get is a function available from the Hmisc library.  I am using 
> > Windows (sorry for not mentioning that).
> 
> 
> Same as mentioned in my former mail also applies for 
> sas.get() in Hmisc.
> 
> Uwe Ligges
> 
> 
> > -----Original Message-----
> > From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> >
> > Sent: Wednesday, July 05, 2006 12:35 PM
> > To: Hamilton, Cody
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Invoking SAS in order to use sas.get
> >
> > Hamilton, Cody wrote:
> >> I am trying to use the sas.get function to bring a SAS dataset into
> R.
> >> After calling the sas.get function I got an error message stating
> that
> >> 'sas' is not a recognized internal or external command, etc.  I am 
> >> guessing that I need to specify the internal command to 
> invoke SAS in 
> >> the sasprog option for the sas.get function, but I don't 
> know how to 
> >> determine what that command might be.  Any thoughts would be 
> >> appreciated.
> >>
> >
> >
> > What is sas.get()?
> > If you are talking about read.ssd() in package "foreign", 
> you have to
> >
> > specify the path to the sas executable. If you are on Windows (your
> mail
> >
> > header suggests, but you have not told it), this might begin with
> >
> > "c:/Program files/........." unless you have sas in your 
> PATH already.
> >
> > Uwe Ligges
> >
> >
> >
> >
> >> Cody Hamilton, Ph.D
> >>
> >
> >> Institute for Health Care Research and Improvement
> >>
> >
> >> Baylor Health Care System
> >>
> >
> >> (214) 265-3618
> >>
> >
> >
> >
> >
> >
> >> This e-mail, facsimile, or letter and any files or attachments
> > transmitted with it contains information that is confidential and 
> > privileged. This information is intended only for the use of the
> > individual(s) and entity(ies) to whom it is addressed. If 
> you are the 
> > intended recipient, further disclosures are prohibited 
> without proper 
> > authorization. If you are not the intended recipient, any 
> disclosure, 
> > copying, printing, or use of this information is strictly prohibited
> and
> > possibly a violation of federal or state law and 
> regulations. If you 
> > have received this information in error, please notify 
> Baylor Health 
> > Care System immediately at 1-866-402-1661 or via e-mail at 
> > privacy at baylorhealth.edu. Baylor Health Care System, its 
> subsidiaries, 
> > and affiliates hereby claim all applicable privileges 
> related to this 
> > information.
> >> 	[[alternative HTML version deleted]]
> >>
> >
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list 
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> > This e-mail, facsimile, or letter and any files or attachments
> transmitted with it contains information that is confidential 
> and privileged. This information is intended only for the use of the
> individual(s) and entity(ies) to whom it is addressed. If you 
> are the intended recipient, further disclosures are 
> prohibited without proper authorization. If you are not the 
> intended recipient, any disclosure, copying, printing, or use 
> of this information is strictly prohibited and possibly a 
> violation of federal or state law and regulations. If you 
> have received this information in error, please notify Baylor 
> Health Care System immediately at 1-866-402-1661 or via 
> e-mail at privacy at baylorhealth.edu. Baylor Health Care 
> System, its subsidiaries, and affiliates hereby claim all 
> applicable privileges related to this information.
> 
> 
> This e-mail, facsimile, or letter and any files or 
> attachments transmitted with it contains information that is 
> confidential and privileged. This information is intended 
> only for the use of the individual(s) and entity(ies) to whom 
> it is addressed. If you are the intended recipient, further 
> disclosures are prohibited without proper authorization. If 
> you are not the intended recipient, any disclosure, copying, 
> printing, or use of this information is strictly prohibited 
> and possibly a violation of federal or state law and 
> regulations. If you have received this information in error, 
> please notify Baylor Health Care System immediately at 
> 1-866-402-1661 or via e-mail at privacy at baylorhealth.edu. 
> Baylor Health Care System, its subsidiaries, and affiliates 
> hereby claim all applicable privileges related to this information.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From NordlDJ at dshs.wa.gov  Wed Jul  5 20:36:51 2006
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS))
Date: Wed, 5 Jul 2006 11:36:51 -0700
Subject: [R] Invoking SAS in order to use sas.get
Message-ID: <592E8923DB6EA348BE8E33FCAADEFFFC13EED90B@dshs-exch2.dshs.wa.lcl>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Hamilton, Cody
> Sent: Wednesday, July 05, 2006 11:18 AM
> To: Uwe Ligges
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Invoking SAS in order to use sas.get
> 
> 
> Thanks for your response.  I specified the path to the SAS executable
> 'C:/Program Files/.../sas.exe', using the sasprog option in sas.get()
> and got a similar error message: C:/Program is not recognized as an
> internal or external command.  I would like to apologize in advance if I
> am making a very foolish mistake here . . .
> 
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> Sent: Wednesday, July 05, 2006 13:7 PM
> To: Hamilton, Cody
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Invoking SAS in order to use sas.get
> 
> Hamilton, Cody wrote:
> > Sas.get is a function available from the Hmisc library.  I am using
> > Windows (sorry for not mentioning that).
> 
> 
> Same as mentioned in my former mail also applies for sas.get() in Hmisc.
> 
> Uwe Ligges
> 
> 
> > -----Original Message-----
> > From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> >
> > Sent: Wednesday, July 05, 2006 12:35 PM
> > To: Hamilton, Cody
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Invoking SAS in order to use sas.get
> >
> > Hamilton, Cody wrote:
> >> I am trying to use the sas.get function to bring a SAS dataset into
> R.
> >> After calling the sas.get function I got an error message stating
> that
> >> 'sas' is not a recognized internal or external command, etc.  I am
> >> guessing that I need to specify the internal command to invoke SAS in
> >> the sasprog option for the sas.get function, but I don't know how to
> >> determine what that command might be.  Any thoughts would be
> >> appreciated.
> >>
> >
> >
> > What is sas.get()?
> > If you are talking about read.ssd() in package "foreign", you have to
> >
> > specify the path to the sas executable. If you are on Windows (your
> mail
> >
> > header suggests, but you have not told it), this might begin with
> >
> > "c:/Program files/........." unless you have sas in your PATH already.
> >
> > Uwe Ligges
> >
> >
> >
> >
> >> Cody Hamilton, Ph.D
> >>
> >
> >> Institute for Health Care Research and Improvement
> >>
> >
> >> Baylor Health Care System
> >>
> >
> >> (214) 265-3618
> >>
> >
> >
> >
> >
> >
> >> This e-mail, facsimile, or letter and any files or attachments
> > transmitted with it contains information that is confidential and
> > privileged. This information is intended only for the use of the
> > individual(s) and entity(ies) to whom it is addressed. If you are the
> > intended recipient, further disclosures are prohibited without proper
> > authorization. If you are not the intended recipient, any disclosure,
> > copying, printing, or use of this information is strictly prohibited
> and
> > possibly a violation of federal or state law and regulations. If you
> > have received this information in error, please notify Baylor Health
> > Care System immediately at 1-866-402-1661 or via e-mail at
> > privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries,
> > and affiliates hereby claim all applicable privileges related to this
> > information.
> >> 	[[alternative HTML version deleted]]
> >>
> >
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> > This e-mail, facsimile, or letter and any files or attachments
> transmitted with it contains information that is confidential and
> privileged. This information is intended only for the use of the
> individual(s) and entity(ies) to whom it is addressed. If you are the
> intended recipient, further disclosures are prohibited without proper
> authorization. If you are not the intended recipient, any disclosure,
> copying, printing, or use of this information is strictly prohibited and
> possibly a violation of federal or state law and regulations. If you
> have received this information in error, please notify Baylor Health
> Care System immediately at 1-866-402-1661 or via e-mail at
> privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries,
> and affiliates hereby claim all applicable privileges related to this
> information.
> 
> 
> This e-mail, facsimile, or letter and any files or attachments transmitted
> with it contains information that is confidential and privileged. This
> information is intended only for the use of the individual(s) and
> entity(ies) to whom it is addressed. If you are the intended recipient,
> further disclosures are prohibited without proper authorization. If you
> are not the intended recipient, any disclosure, copying, printing, or use
> of this information is strictly prohibited and possibly a violation of
> federal or state law and regulations. If you have received this
> information in error, please notify Baylor Health Care System immediately
> at 1-866-402-1661 or via e-mail at privacy at baylorhealth.edu. Baylor Health
> Care System, its subsidiaries, and affiliates hereby claim all applicable
> privileges related to this information.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html

Because your path has a space in it, i.e. "c:\Program Files\...", Windows
requires quotes around the path\filename.  So, you need to include
double-quotes as part of your path specification.  Some thing like:

'"C:/Program Files/.../sas.exe"'

Hope this helps,

Dan


Daniel J. Nordlund
Research and Data Analysis
Washington State Department of Social and Health Services
Olympia, WA  98504-5204


From CodyH at BaylorHealth.edu  Wed Jul  5 21:03:48 2006
From: CodyH at BaylorHealth.edu (Hamilton, Cody)
Date: Wed, 5 Jul 2006 14:03:48 -0500
Subject: [R] Invoking SAS in order to use sas.get
In-Reply-To: <592E8923DB6EA348BE8E33FCAADEFFFC13EED90B@dshs-exch2.dshs.wa.lcl>
Message-ID: <E52E20F6B4A2F548B16BB259DD5168CF02FB6803@BHDAEXCH11.bhcs.pvt>


Dan,

You are right - that was my problem.  Thank you so much for your help!

I had submitted the code

trial<-sas.get(library='Y:/Hamilton/Data',member='cody',
		sasprog='C:/Program Files/SAS Institute/SAS/V9/sas.exe')

instead of the correct code (with the double quotes)

trial<-sas.get(library='Y:/Hamilton/Data',member='cody',
		sasprog='"C:/Program Files/SAS
Institute/SAS/V9/sas.exe"')

Thank you to Uwe, Douglas, Andy, and Frank for their help as well.  (And
I will be sure to include the entire code next time.)

-Cody


-----Original Message-----
From: Nordlund, Dan (DSHS) [mailto:NordlDJ at dshs.wa.gov]
Sent: Wednesday, July 05, 2006 13:37 PM
To: Hamilton, Cody; Uwe Ligges
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] Invoking SAS in order to use sas.get

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Hamilton, Cody
> Sent: Wednesday, July 05, 2006 11:18 AM
> To: Uwe Ligges
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Invoking SAS in order to use sas.get
>
>
> Thanks for your response.  I specified the path to the SAS executable
> 'C:/Program Files/.../sas.exe', using the sasprog option in sas.get()
> and got a similar error message: C:/Program is not recognized as an
> internal or external command.  I would like to apologize in advance if
I
> am making a very foolish mistake here . . .
>
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> Sent: Wednesday, July 05, 2006 13:7 PM
> To: Hamilton, Cody
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Invoking SAS in order to use sas.get
>
> Hamilton, Cody wrote:
> > Sas.get is a function available from the Hmisc library.  I am using
> > Windows (sorry for not mentioning that).
>
>
> Same as mentioned in my former mail also applies for sas.get() in
Hmisc.
>
> Uwe Ligges
>
>
> > -----Original Message-----
> > From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> >
> > Sent: Wednesday, July 05, 2006 12:35 PM
> > To: Hamilton, Cody
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Invoking SAS in order to use sas.get
> >
> > Hamilton, Cody wrote:
> >> I am trying to use the sas.get function to bring a SAS dataset into
> R.
> >> After calling the sas.get function I got an error message stating
> that
> >> 'sas' is not a recognized internal or external command, etc.  I am
> >> guessing that I need to specify the internal command to invoke SAS
in
> >> the sasprog option for the sas.get function, but I don't know how
to
> >> determine what that command might be.  Any thoughts would be
> >> appreciated.
> >>
> >
> >
> > What is sas.get()?
> > If you are talking about read.ssd() in package "foreign", you have
to
> >
> > specify the path to the sas executable. If you are on Windows (your
> mail
> >
> > header suggests, but you have not told it), this might begin with
> >
> > "c:/Program files/........." unless you have sas in your PATH
already.
> >
> > Uwe Ligges
> >
> >
> >
> >
> >> Cody Hamilton, Ph.D
> >>
> >
> >> Institute for Health Care Research and Improvement
> >>
> >
> >> Baylor Health Care System
> >>
> >
> >> (214) 265-3618
> >>
> >
> >
> >
> >
> >
> >> This e-mail, facsimile, or letter and any files or attachments
> > transmitted with it contains information that is confidential and
> > privileged. This information is intended only for the use of the
> > individual(s) and entity(ies) to whom it is addressed. If you are
the
> > intended recipient, further disclosures are prohibited without
proper
> > authorization. If you are not the intended recipient, any
disclosure,
> > copying, printing, or use of this information is strictly prohibited
> and
> > possibly a violation of federal or state law and regulations. If you
> > have received this information in error, please notify Baylor Health
> > Care System immediately at 1-866-402-1661 or via e-mail at
> > privacy at baylorhealth.edu. Baylor Health Care System, its
subsidiaries,
> > and affiliates hereby claim all applicable privileges related to
this
> > information.
> >> 	[[alternative HTML version deleted]]
> >>
> >
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> > This e-mail, facsimile, or letter and any files or attachments
> transmitted with it contains information that is confidential and
> privileged. This information is intended only for the use of the
> individual(s) and entity(ies) to whom it is addressed. If you are the
> intended recipient, further disclosures are prohibited without proper
> authorization. If you are not the intended recipient, any disclosure,
> copying, printing, or use of this information is strictly prohibited
and
> possibly a violation of federal or state law and regulations. If you
> have received this information in error, please notify Baylor Health
> Care System immediately at 1-866-402-1661 or via e-mail at
> privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries,
> and affiliates hereby claim all applicable privileges related to this
> information.
>
>
> This e-mail, facsimile, or letter and any files or attachments
transmitted
> with it contains information that is confidential and privileged. This
> information is intended only for the use of the individual(s) and
> entity(ies) to whom it is addressed. If you are the intended
recipient,
> further disclosures are prohibited without proper authorization. If
you
> are not the intended recipient, any disclosure, copying, printing, or
use
> of this information is strictly prohibited and possibly a violation of
> federal or state law and regulations. If you have received this
> information in error, please notify Baylor Health Care System
immediately
> at 1-866-402-1661 or via e-mail at privacy at baylorhealth.edu. Baylor
Health
> Care System, its subsidiaries, and affiliates hereby claim all
applicable
> privileges related to this information.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html

Because your path has a space in it, i.e. "c:\Program Files\...",
Windows
requires quotes around the path\filename.  So, you need to include
double-quotes as part of your path specification.  Some thing like:

'"C:/Program Files/.../sas.exe"'

Hope this helps,

Dan


Daniel J. Nordlund
Research and Data Analysis
Washington State Department of Social and Health Services
Olympia, WA  98504-5204



This e-mail, facsimile, or letter and any files or attachments transmitted with it contains information that is confidential and privileged. This information is intended only for the use of the individual(s) and entity(ies) to whom it is addressed. If you are the intended recipient, further disclosures are prohibited without proper authorization. If you are not the intended recipient, any disclosure, copying, printing, or use of this information is strictly prohibited and possibly a violation of federal or state law and regulations. If you have received this information in error, please notify Baylor Health Care System immediately at 1-866-402-1661 or via e-mail at privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries, and affiliates hereby claim all applicable privileges related to this information.


From ggrothendieck at gmail.com  Wed Jul  5 21:56:52 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 5 Jul 2006 15:56:52 -0400
Subject: [R] coloring individual points in lattice xyplot
In-Reply-To: <eb555e660607050916u4a92347o752be78559be848f@mail.gmail.com>
References: <971536df0607040842v69c39f8bl7b13bae45c0b0fe1@mail.gmail.com>
	<eb555e660607041124g3df6e08fvee0506b09736df68@mail.gmail.com>
	<971536df0607041131g3321c410kbda7f191c3883605@mail.gmail.com>
	<971536df0607041929s3c6b9ccei204ea42ed1004f61@mail.gmail.com>
	<eb555e660607042005i5c760480xa2b63738164e3611@mail.gmail.com>
	<971536df0607042105v1853b7f7y88bd7b774fa4ff53@mail.gmail.com>
	<eb555e660607050916u4a92347o752be78559be848f@mail.gmail.com>
Message-ID: <971536df0607051256r628678b0saa7f63a319b95dce@mail.gmail.com>

On 7/5/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > OK.  It looks like I need to go to the lower level llines and lpoints to
> > do this.  I wrote a panel routine, mypanel, and it seems to work (see
> > below); however, currently it assumes types and cols are in the global
> > environment or at least somewhere where they will be found.
> >
> > 1. Is it somehow possible to stick these into some structures set up by
> > lattice already and then retrieve them from lattice from within mypanel?
>
> Not sure what you mean by that, but I would do something like the
> following (this allows 'col' to be a list as in your original post).
> Note that arguments to the panel function can be supplied directly to
> the high level function (that's why graphical parameters can be
> supplied to xyplot in the first place).
>
> mypanel <-
>    function(x, y, subscripts, groups,
>             col = 1,
>             type = "p",
>             ...)
> {
>    col <- rep(as.list(col), length = nlevels(groups))
>    type <- rep(as.list(type), length = nlevels(groups))
>    for(g in 1:nlevels(groups)) {
>         idx <- g == groups[subscripts]
>         xx <- x[idx]; yy <- y[idx];
>         panel.xyplot(x[idx], y[idx],
>                      col = col[[g]],
>                      type = type[[g]],
>                      ...)
>    }
> }
>
> xyplot(y ~ c(x,x), groups = factor(col(y)),
>       panel = mypanel,
>       type = c("o", "p"),
>       col = list("black", 1:10))


Thanks!!! This is quite an improvement.

>
>
> > 2. Any other improvements to the example below?
> >
> > mypanel <- function(x, y, subscripts, groups, ...) {
> >       for(g in 1:nlevels(groups)) {
> >          idx <- g == groups
>
> This won't work for more than one panel.

Could you explain this?  Under what situation does it not work?
The following, for example, appears to work:

xyplot(y ~ c(x,x) | col(y), groups = factor(col(y)),
      panel = mypanel,
      type = c("o", "p"),
      col = list("black", 1:10),
      layout = 1:2)




>
> >          xx <- x[idx]; yy <- y[idx]; ccols <- cols[subscripts][idx]
> >          if (any(idx)) {
> >             switch(types[g],
> >                p = lpoints(xx, yy, col = ccols),
> >                l = llines(xx, yy, col = ccols),
> >                o = { lpoints(xx, yy, col = ccols)
> >                         llines(xx, yy, col = ccols) })
> >          }
> >        }
> >     }
> >
> > x <- 1:10
> > y <- cbind(y1 = x, y2 = x+1)
> > cols <- c(rep(1,10), 1:10)
> > types <- c("o", "p")
> > xyplot(y ~ c(x,x), groups = factor(col(y)), type = types, panel = mypanel)
> >
> >
> >
> > On 7/4/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> > > On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > I can get the types to work or the colors but not both:
> > >
> > > Sorry if I wasn't clear, but I didn't mean that you could use
> > > panel.superpose[.2] to do what you wanted. I only meant that you could
> > > use it as a template that may help you to write your own panel
> > > function. What you want is not possible with tools available in
> > > lattice.
> > >
> > > Deepayan
> > >
> > > > # this gets the types right but not the colors
> > > > library(lattice)
> > > > x <- 1:10
> > > > y <- cbind(y1 = x, y2 = x+1)
> > > > cols <- c(rep(1,10), 1:10)
> > > > xyplot(y ~ c(x,x), groups = col(y), type = c("o", "p"),
> > > >    panel = function(x, y, subscripts, groups, ...)
> > > >       panel.superpose.2(x, y, subscripts, groups, col = cols[subscripts], ...)
> > > > )
> > > >
> > > >
> > > > # this gets the colors right but not the types
> > > > library(lattice)
> > > > x <- 1:10
> > > > y <- cbind(y1 = x, y2 = x+1)
> > > > cols <- c(rep(1,10), 1:10)
> > > > xyplot(y ~ c(x,x), groups = col(y), type = c("o", "p"),
> > > >    panel = function(x, y, subscripts, groups, ...)
> > > >       panel.xyplot(x, y, col = cols[subscripts], ...)
> > > > )
> > > >
> > > >
> > > > On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > > On 7/4/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> > > > > > On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > > > > If I wish to color groups in xyplot I can do this:
> > > > > > >
> > > > > > >    library(lattice)
> > > > > > >    x <- 1:10
> > > > > > >    y <- cbind(x, x+1)
> > > > > > >    xyplot(y ~ rep(x,2), group = col(y), col = 1:2)
> > > > > > >
> > > > > > > How do I color different points differently within a group.
> > > > > > >
> > > > > > > For example, I want to produce this plot (except that I only
> > > > > > > want to have two groups, not 11):
> > > > > > >
> > > > > > >    xyplot(y ~ rep(x,2), group = c(rep(1, 10), 2:11), col = 1:11)
> > > > > > >
> > > > > > > I am thinking of something like this (although
> > > > > > > this does not work, its just to get the idea across):
> > > > > > >
> > > > > > >    xyplot(y ~ rep(x,2), group = col(y), col = list(1, 2:11))
> > > > > > >
> > > > > > > where, in general, I have a list with one component per group
> > > > > > > whose elements are scalars to color the whole group or
> > > > > > > vectors one color per point in the group.  I don't know
> > > > > > > ahead of time what the list is.
> > > > > > >
> > > > > > > I am looking for a general approach to this within the lattice
> > > > > > > xyplot plot framework; the above is just an example.
> > > > > >
> > > > > > The general approach is to write your own panel function. For a
> > > > > > possible template, look at the functions panel.superpose and
> > > > > > panel.superpose.2 and how they handle the 'type' argument.
> > > > > >
> > > > > > Deepayan
> > > > > >
> > > > >
> > > > > There is no example in ?panel.superpose.  Do you think you
> > > > > could provide an example for the situation in my post?
> > > > >
> > > > > I have done quite a bit of RSiteSearch'ing and googling prior to
> > > > > posting and all the examples I found had colors that depended
> > > > > on the group, none addressed the situation in my post -- i.e.
> > > > > coloring individual points within groups.
> > > > >
> > > >
> > >
> > >
> > > --
> > > http://www.stat.wisc.edu/~deepayan/
> > >
> >
>
>
> --
> http://www.stat.wisc.edu/~deepayan/
>


From ggrothendieck at gmail.com  Wed Jul  5 21:59:08 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 5 Jul 2006 15:59:08 -0400
Subject: [R] coloring individual points in lattice xyplot
In-Reply-To: <971536df0607051256r628678b0saa7f63a319b95dce@mail.gmail.com>
References: <971536df0607040842v69c39f8bl7b13bae45c0b0fe1@mail.gmail.com>
	<eb555e660607041124g3df6e08fvee0506b09736df68@mail.gmail.com>
	<971536df0607041131g3321c410kbda7f191c3883605@mail.gmail.com>
	<971536df0607041929s3c6b9ccei204ea42ed1004f61@mail.gmail.com>
	<eb555e660607042005i5c760480xa2b63738164e3611@mail.gmail.com>
	<971536df0607042105v1853b7f7y88bd7b774fa4ff53@mail.gmail.com>
	<eb555e660607050916u4a92347o752be78559be848f@mail.gmail.com>
	<971536df0607051256r628678b0saa7f63a319b95dce@mail.gmail.com>
Message-ID: <971536df0607051259p4b5fb7feg48fc7c14d2abe78d@mail.gmail.com>

On 7/5/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 7/5/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> > On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > OK.  It looks like I need to go to the lower level llines and lpoints to
> > > do this.  I wrote a panel routine, mypanel, and it seems to work (see
> > > below); however, currently it assumes types and cols are in the global
> > > environment or at least somewhere where they will be found.
> > >
> > > 1. Is it somehow possible to stick these into some structures set up by
> > > lattice already and then retrieve them from lattice from within mypanel?
> >
> > Not sure what you mean by that, but I would do something like the
> > following (this allows 'col' to be a list as in your original post).
> > Note that arguments to the panel function can be supplied directly to
> > the high level function (that's why graphical parameters can be
> > supplied to xyplot in the first place).
> >
> > mypanel <-
> >    function(x, y, subscripts, groups,
> >             col = 1,
> >             type = "p",
> >             ...)
> > {
> >    col <- rep(as.list(col), length = nlevels(groups))
> >    type <- rep(as.list(type), length = nlevels(groups))
> >    for(g in 1:nlevels(groups)) {
> >         idx <- g == groups[subscripts]
> >         xx <- x[idx]; yy <- y[idx];
> >         panel.xyplot(x[idx], y[idx],
> >                      col = col[[g]],
> >                      type = type[[g]],
> >                      ...)
> >    }
> > }
> >
> > xyplot(y ~ c(x,x), groups = factor(col(y)),
> >       panel = mypanel,
> >       type = c("o", "p"),
> >       col = list("black", 1:10))
>
>
> Thanks!!! This is quite an improvement.
>
> >
> >
> > > 2. Any other improvements to the example below?
> > >
> > > mypanel <- function(x, y, subscripts, groups, ...) {
> > >       for(g in 1:nlevels(groups)) {
> > >          idx <- g == groups
> >
> > This won't work for more than one panel.
>
> Could you explain this?  Under what situation does it not work?
> The following, for example, appears to work:
>
> xyplot(y ~ c(x,x) | col(y), groups = factor(col(y)),
>      panel = mypanel,
>      type = c("o", "p"),
>      col = list("black", 1:10),
>      layout = 1:2)

Just to clarify mypanel refers to your version of mypanel.
Were you saying that your version still does not work in certain
situations with multiple panels or only mine?

>
>
>
>
> >
> > >          xx <- x[idx]; yy <- y[idx]; ccols <- cols[subscripts][idx]
> > >          if (any(idx)) {
> > >             switch(types[g],
> > >                p = lpoints(xx, yy, col = ccols),
> > >                l = llines(xx, yy, col = ccols),
> > >                o = { lpoints(xx, yy, col = ccols)
> > >                         llines(xx, yy, col = ccols) })
> > >          }
> > >        }
> > >     }
> > >
> > > x <- 1:10
> > > y <- cbind(y1 = x, y2 = x+1)
> > > cols <- c(rep(1,10), 1:10)
> > > types <- c("o", "p")
> > > xyplot(y ~ c(x,x), groups = factor(col(y)), type = types, panel = mypanel)
> > >
> > >
> > >
> > > On 7/4/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> > > > On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > > I can get the types to work or the colors but not both:
> > > >
> > > > Sorry if I wasn't clear, but I didn't mean that you could use
> > > > panel.superpose[.2] to do what you wanted. I only meant that you could
> > > > use it as a template that may help you to write your own panel
> > > > function. What you want is not possible with tools available in
> > > > lattice.
> > > >
> > > > Deepayan
> > > >
> > > > > # this gets the types right but not the colors
> > > > > library(lattice)
> > > > > x <- 1:10
> > > > > y <- cbind(y1 = x, y2 = x+1)
> > > > > cols <- c(rep(1,10), 1:10)
> > > > > xyplot(y ~ c(x,x), groups = col(y), type = c("o", "p"),
> > > > >    panel = function(x, y, subscripts, groups, ...)
> > > > >       panel.superpose.2(x, y, subscripts, groups, col = cols[subscripts], ...)
> > > > > )
> > > > >
> > > > >
> > > > > # this gets the colors right but not the types
> > > > > library(lattice)
> > > > > x <- 1:10
> > > > > y <- cbind(y1 = x, y2 = x+1)
> > > > > cols <- c(rep(1,10), 1:10)
> > > > > xyplot(y ~ c(x,x), groups = col(y), type = c("o", "p"),
> > > > >    panel = function(x, y, subscripts, groups, ...)
> > > > >       panel.xyplot(x, y, col = cols[subscripts], ...)
> > > > > )
> > > > >
> > > > >
> > > > > On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > > > On 7/4/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> > > > > > > On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > > > > > If I wish to color groups in xyplot I can do this:
> > > > > > > >
> > > > > > > >    library(lattice)
> > > > > > > >    x <- 1:10
> > > > > > > >    y <- cbind(x, x+1)
> > > > > > > >    xyplot(y ~ rep(x,2), group = col(y), col = 1:2)
> > > > > > > >
> > > > > > > > How do I color different points differently within a group.
> > > > > > > >
> > > > > > > > For example, I want to produce this plot (except that I only
> > > > > > > > want to have two groups, not 11):
> > > > > > > >
> > > > > > > >    xyplot(y ~ rep(x,2), group = c(rep(1, 10), 2:11), col = 1:11)
> > > > > > > >
> > > > > > > > I am thinking of something like this (although
> > > > > > > > this does not work, its just to get the idea across):
> > > > > > > >
> > > > > > > >    xyplot(y ~ rep(x,2), group = col(y), col = list(1, 2:11))
> > > > > > > >
> > > > > > > > where, in general, I have a list with one component per group
> > > > > > > > whose elements are scalars to color the whole group or
> > > > > > > > vectors one color per point in the group.  I don't know
> > > > > > > > ahead of time what the list is.
> > > > > > > >
> > > > > > > > I am looking for a general approach to this within the lattice
> > > > > > > > xyplot plot framework; the above is just an example.
> > > > > > >
> > > > > > > The general approach is to write your own panel function. For a
> > > > > > > possible template, look at the functions panel.superpose and
> > > > > > > panel.superpose.2 and how they handle the 'type' argument.
> > > > > > >
> > > > > > > Deepayan
> > > > > > >
> > > > > >
> > > > > > There is no example in ?panel.superpose.  Do you think you
> > > > > > could provide an example for the situation in my post?
> > > > > >
> > > > > > I have done quite a bit of RSiteSearch'ing and googling prior to
> > > > > > posting and all the examples I found had colors that depended
> > > > > > on the group, none addressed the situation in my post -- i.e.
> > > > > > coloring individual points within groups.
> > > > > >
> > > > >
> > > >
> > > >
> > > > --
> > > > http://www.stat.wisc.edu/~deepayan/
> > > >
> > >
> >
> >
> > --
> > http://www.stat.wisc.edu/~deepayan/
> >
>


From deepayan.sarkar at gmail.com  Wed Jul  5 22:37:46 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 5 Jul 2006 15:37:46 -0500
Subject: [R] coloring individual points in lattice xyplot
In-Reply-To: <971536df0607051259p4b5fb7feg48fc7c14d2abe78d@mail.gmail.com>
References: <971536df0607040842v69c39f8bl7b13bae45c0b0fe1@mail.gmail.com>
	<eb555e660607041124g3df6e08fvee0506b09736df68@mail.gmail.com>
	<971536df0607041131g3321c410kbda7f191c3883605@mail.gmail.com>
	<971536df0607041929s3c6b9ccei204ea42ed1004f61@mail.gmail.com>
	<eb555e660607042005i5c760480xa2b63738164e3611@mail.gmail.com>
	<971536df0607042105v1853b7f7y88bd7b774fa4ff53@mail.gmail.com>
	<eb555e660607050916u4a92347o752be78559be848f@mail.gmail.com>
	<971536df0607051256r628678b0saa7f63a319b95dce@mail.gmail.com>
	<971536df0607051259p4b5fb7feg48fc7c14d2abe78d@mail.gmail.com>
Message-ID: <eb555e660607051337t12f8ff1fn40b510cfdcb4d8b3@mail.gmail.com>

On 7/5/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 7/5/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > On 7/5/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> > > On 7/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > OK.  It looks like I need to go to the lower level llines and lpoints to
> > > > do this.  I wrote a panel routine, mypanel, and it seems to work (see
> > > > below); however, currently it assumes types and cols are in the global
> > > > environment or at least somewhere where they will be found.
> > > >
> > > > 1. Is it somehow possible to stick these into some structures set up by
> > > > lattice already and then retrieve them from lattice from within mypanel?
> > >
> > > Not sure what you mean by that, but I would do something like the
> > > following (this allows 'col' to be a list as in your original post).
> > > Note that arguments to the panel function can be supplied directly to
> > > the high level function (that's why graphical parameters can be
> > > supplied to xyplot in the first place).
> > >
> > > mypanel <-
> > >    function(x, y, subscripts, groups,
> > >             col = 1,
> > >             type = "p",
> > >             ...)
> > > {
> > >    col <- rep(as.list(col), length = nlevels(groups))
> > >    type <- rep(as.list(type), length = nlevels(groups))
> > >    for(g in 1:nlevels(groups)) {
> > >         idx <- g == groups[subscripts]
> > >         xx <- x[idx]; yy <- y[idx];
> > >         panel.xyplot(x[idx], y[idx],
> > >                      col = col[[g]],
> > >                      type = type[[g]],
> > >                      ...)
> > >    }
> > > }
> > >
> > > xyplot(y ~ c(x,x), groups = factor(col(y)),
> > >       panel = mypanel,
> > >       type = c("o", "p"),
> > >       col = list("black", 1:10))
> >
> >
> > Thanks!!! This is quite an improvement.
> >
> > >
> > >
> > > > 2. Any other improvements to the example below?
> > > >
> > > > mypanel <- function(x, y, subscripts, groups, ...) {
> > > >       for(g in 1:nlevels(groups)) {
> > > >          idx <- g == groups
> > >
> > > This won't work for more than one panel.
> >
> > Could you explain this?  Under what situation does it not work?
> > The following, for example, appears to work:
> >
> > xyplot(y ~ c(x,x) | col(y), groups = factor(col(y)),
> >      panel = mypanel,
> >      type = c("o", "p"),
> >      col = list("black", 1:10),
> >      layout = 1:2)
>
> Just to clarify mypanel refers to your version of mypanel.
> Were you saying that your version still does not work in certain
> situations with multiple panels or only mine?

I meant your version. You need the [subscripts] part, which your
version didn't have (mine did). This is because unlike the 'x' and 'y'
variables, 'groups' doesn't get subsetted before being passed on to
the panel function. In fact, apart from the fact that it is evaluated
in 'data', 'groups' is in principle just like any other `non-standard'
argument, in that it gets passed on to the panel function unchanged.

-Deepayan


From bates at stat.wisc.edu  Wed Jul  5 22:44:13 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 5 Jul 2006 15:44:13 -0500
Subject: [R] lmer print outs without T
In-Reply-To: <FBD652D4-D333-419E-9D7D-9B92C3DBD27C@or.psychology.dal.ca>
References: <FBD652D4-D333-419E-9D7D-9B92C3DBD27C@or.psychology.dal.ca>
Message-ID: <40e66e0b0607051344y5fab089cuc8c40bced48e58a6@mail.gmail.com>

On 7/4/06, John Christie <jc at or.psychology.dal.ca> wrote:
> Hi,
>         I have been having a tedious issue with lmer models with lots of
> factors and lots of levels.  In order to get the basic information at
> the beginning of the print out I also have to generate these enormous
> tables as well.  Is there a method command to leave off all of the
> effects and correlations?  Or, do I have to go to string commands?

Depending on exactly which parts of the output you want it may be
easiest to obtain them from the result of summary applied to your
fitted model.  Use str to determine the structure of the object then
extract the slots that you want.

Warning: This is a somewhat risky practice in that we don't guarantee
that the classes and slot names will remain constant.  If there is an
extractor function that gives the result that you want it is much
safer to use that than to reach in to a structure and extract
individual slots or components.  However, in this case I don't think
such an extractor exists.

As an example

> (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
Linear mixed-effects model fit by REML
Formula: Reaction ~ Days + (Days | Subject)
	  Data: sleepstudy
      AIC      BIC    logLik MLdeviance REMLdeviance
 1753.628 1769.593 -871.8141   1751.986     1743.628
Random effects:
 Groups   Name        Variance Std.Dev. Corr
 Subject  (Intercept) 612.090  24.7405
          Days         35.072   5.9221  0.066
 Residual             654.941  25.5918
number of obs: 180, groups: Subject, 18

Fixed effects:
            Estimate Std. Error t value
(Intercept) 251.4051     6.8246  36.838
Days         10.4673     1.5458   6.771

Correlation of Fixed Effects:
     (Intr)
Days -0.138
> so <- summary(fm1)
> showClass(class(so))

Slots:

Name:         isG  methTitle     logLik      ngrps      sigma      coefs
Class:    logical  character     logLik    integer    numeric     matrix

Name:        vcov      REmat     AICtab      flist         Zt          X
Class:  dpoMatrix     matrix data.frame       list  dgCMatrix     matrix

Name:           y        wts     wrkres     method   useScale     family
Class:    numeric    numeric    numeric  character    logical     family

Name:        call     cnames         nc         Gp        XtX        ZtZ
Class:       call       list    integer    integer  dpoMatrix  dsCMatrix

Name:         ZtX        Zty        Xty      Omega          L        RZX
Class:  dgeMatrix    numeric    numeric       list  dCHMsuper  dgeMatrix

Name:         RXX        rZy        rXy    devComp   deviance      fixef
Class:  dtrMatrix    numeric    numeric    numeric    numeric    numeric

Name:       ranef     RZXinv       bVar   gradComp     status     assign
Class:    numeric  dgeMatrix       list       list    logical    integer

Name:       frame      terms
Class: data.frame      terms

Extends:
Class "summary.mer", directly
Class "lmer", directly
Class "mer", by class "summary.mer"
Class "mer", by class "lmer"
> so at AICtab
      AIC      BIC    logLik MLdeviance REMLdeviance
 1753.628 1769.593 -871.8141   1751.986     1743.628
> print(so at REmat, quote = FALSE)
 Groups   Name        Variance Std.Dev. Corr
 Subject  (Intercept) 612.090  24.7405
          Days         35.072   5.9221  0.066
 Residual             654.941  25.5918


From macq at llnl.gov  Wed Jul  5 22:55:15 2006
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 5 Jul 2006 13:55:15 -0700
Subject: [R] install RMySQL  under Mac OS X 10.4.7
In-Reply-To: <20060705170004.99325.qmail@web35505.mail.mud.yahoo.com>
References: <20060705170004.99325.qmail@web35505.mail.mud.yahoo.com>
Message-ID: <p06230909c0d1d67f539f@[128.115.153.6]>

Try

    install.packages('RMySQL', repos 
="http://www.biometrics.mtu.edu/CRAN/", dependencies=TRUE , 
type='source')

Check the help for install.packages() for an explanation of the 
'type' argument, and some Mac-specific information about it.

-Don

At 10:00 AM -0700 7/5/06, Luo Weijun wrote:
>Hello All,
>
>I tried to install RMySQL package, but the error
>messages says there is no such package, even though I
>did see RMySQL is there in the contributed package
>list in all mirror sites of CRAN I tried. Not sure
>what is the problem.
>
>>  mysql.home <- '/usr/local/mysql'
>>
>Sys.putenv('PKG_CPPFLAGS'=paste('-I',file.path(mysql.home,'include'),sep=''))
>
>>
>Sys.putenv('PKG_LIBS'=paste('-L',file.path(mysql.home,'lib'),'
>-lmysqlclient',sep=''))
>  > install.packages('RMySQL', repos =
>"http://www.biometrics.mtu.edu/CRAN/",dependencies =T)
>dependency ''RMySQL'' is not available
>  > install.packages('RMySQL', repos =
>"http://www.biometrics.mtu.edu/CRAN/")               
>Warning in download.packages(pkgs, destdir = tmpd,
>available = available,  :
>          no package 'RMySQL' at the repositories
>
>I tried download the package manually under unix, and
>install it from within R, and no luck too.
>
>cd ~/download/R
>curl -O
>http://www.biometrics.mtu.edu/CRAN/src/contrib/RMySQL_0.5-7.tar.gz
>
>>  install.packages( 'RMySQL_0.5-7.tar.gz', repos =
>NULL)
>Error in gzfile(file, "r") : unable to open connection
>In addition: Warning message:
>cannot open compressed file
>'RMySQL_0.5-7.tar.gz/DESCRIPTION'
>>
>
>>  sessionInfo()
>Version 2.3.0 (2006-04-24)
>powerpc-apple-darwin8.6.0
>
>attached base packages:
>[1] "methods"   "stats"     "graphics"  "grDevices"
>"utils"     "datasets"
>[7] "base"    
>
>
>could anybody tell me what happens here, and how to
>fix the problem? Thank you so much!
>Weijun
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA


From p.dalgaard at biostat.ku.dk  Wed Jul  5 23:30:26 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Jul 2006 23:30:26 +0200
Subject: [R] tcl/tk with R
In-Reply-To: <03b301c6a058$6a9b32c0$6600a8c0@DD4XFW31>
References: <03b301c6a058$6a9b32c0$6600a8c0@DD4XFW31>
Message-ID: <x2u05v4u4t.fsf@turmalin.kubism.ku.dk>

"Charles Annis, P.E." <Charles.Annis at statisticalengineering.com> writes:

> Greetings:
> 
> I would like to use tcl/tk with R, and have read "A Primer on the R-Tcl/Tk
> Package" by Peter Dalgaard in Rnews, Volume 1/3, September 2001.
> 
> Are there more recent do-it-yourself instructions available?

There's a later Rnews update, but that too could do with a revision.

@Article{pd:tcltk-update,
  author =       {Peter Dalgaard},
  title =        {Changes to the {R-Tcl/Tk} package},
  journal =      {R News},
  year =         {2002},
  volume =       2,
  number =       3,
  pages =        {25-27}
}

At this point, I think that the best introductory document is the web
pages by James Wettenhalls web pages at

http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From markleeds at verizon.net  Wed Jul  5 23:33:24 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Wed, 05 Jul 2006 16:33:24 -0500 (CDT)
Subject: [R] apologes if you already saw this :efficiency question
Message-ID: <5951469.1017771152135206338.JavaMail.root@vms171.mailsrvcs.net>

hi everyone : i'm not sure if my previous mail about
this got sent. i was typing and
erroneosuyl hit a button and lost what i was typing.

anyway, i have the code below ( it works ) in which i run through the rows of a dataframe, taking out the first two
fields which are characters strings ( with some extra spacing so
i yuse gsub) and appending these character strings to a list so that i can build one big list.

there are 17,000 rows so i was hoping there might be a ( even just slightly. it doesn't have to be incresible improvement ) more efficient way to do this. I also think that remember someone saying that using the c command to make something bigger is not a good idea.

the code is below. thanks.

              for paircounter in 1:nrow(tempdata) {

                 firsstock<-gsub(" ","",tempdata[paircounter,1]
                 secondstock<-gsub(" ","",tempdata[paircounter,2]

                  if ( paircounter == 1 ) {
                      stocklist<-c(firststock,secondstock)
                   } else {
                       stocklist<(stocklist,firststock,secondstock)
                   }
                 }


From xenamaa at yahoo.com  Wed Jul  5 18:39:25 2006
From: xenamaa at yahoo.com (zana adeb)
Date: Wed, 5 Jul 2006 09:39:25 -0700 (PDT)
Subject: [R] question
Message-ID: <20060705163925.96381.qmail@web53108.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060705/97ea359f/attachment.pl 

From Thomas.Petzoldt at TU-Dresden.de  Wed Jul  5 17:44:35 2006
From: Thomas.Petzoldt at TU-Dresden.de (Thomas Petzoldt)
Date: Wed, 05 Jul 2006 17:44:35 +0200
Subject: [R] [R-pkgs] package simecol uploaded to CRAN
Message-ID: <44ABDE63.6090204@TU-Dresden.de>

Dear useRs,

a new and completely re-worked version of the "simecol" package

   SIMulatiion of ECOLogical (and other) dynamic systems

is now available on CRAN. Compared to the S3 based 0.2-x versions an
improved object model using S4 classes was employed. Please ask me for
assistance in case of incompatibility.

Comments are welcome, Thomas Petzoldt

----------------------------------------------------------------------

Abstract:
=========

The simecol package is intended to give users (scientists and students)
and interactive environment to implement, distribute, simulate and
document ecological and other dynamic models without the need to write
long simulation programs. An object oriented approach based on the S4
class system is proposed, which should provide a consistent but still
flexible and extensible way to implement dynamic models.

Models of different type are provided as data and in source code (see
directory "examples") which are intended as a starting point to write
and distribute your own simObj models.

The package is supplemented with several utility functions (e.g.
seedfill or neighbours), which can be used independently from simObj
objects.

More Info:
==========

? simecol-package: general description of the package
? parms: functions to get or set model parameters etc.


Examples:
=========

file.show(system.file("examples/lv.R", package = "simecol"))
file.show(system.file("examples/conway.R", package = "simecol"))
file.show(system.file("examples/upca.R", package = "simecol"))

Homepage:
=========
http://www.simecol.de with examples, useR-2006 slides and poster.



--
Dr. Thomas Petzoldt
Technische Universitaet Dresden
Institut fuer Hydrobiologie        thomas.petzoldt at tu-dresden.de
01062 Dresden                      http://tu-dresden.de/hydrobiologie/
GERMANY

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From ggrothendieck at gmail.com  Thu Jul  6 02:07:14 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 5 Jul 2006 20:07:14 -0400
Subject: [R] tcl/tk with R
In-Reply-To: <x2u05v4u4t.fsf@turmalin.kubism.ku.dk>
References: <03b301c6a058$6a9b32c0$6600a8c0@DD4XFW31>
	<x2u05v4u4t.fsf@turmalin.kubism.ku.dk>
Message-ID: <971536df0607051707t1917ebbay26e07e80b906d56c@mail.gmail.com>

Also note the last link on that page which gives sources of other info.

On 05 Jul 2006 23:30:26 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> "Charles Annis, P.E." <Charles.Annis at statisticalengineering.com> writes:
>
> > Greetings:
> >
> > I would like to use tcl/tk with R, and have read "A Primer on the R-Tcl/Tk
> > Package" by Peter Dalgaard in Rnews, Volume 1/3, September 2001.
> >
> > Are there more recent do-it-yourself instructions available?
>
> There's a later Rnews update, but that too could do with a revision.
>
> @Article{pd:tcltk-update,
>  author =       {Peter Dalgaard},
>  title =        {Changes to the {R-Tcl/Tk} package},
>  journal =      {R News},
>  year =         {2002},
>  volume =       2,
>  number =       3,
>  pages =        {25-27}
> }
>
> At this point, I think that the best introductory document is the web
> pages by James Wettenhalls web pages at
>
> http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/
>
> --
>   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From Soren.Hojsgaard at agrsci.dk  Thu Jul  6 02:21:43 2006
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Thu, 6 Jul 2006 02:21:43 +0200
Subject: [R] Rgraphviz: How to control the colours of edges in a graph
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC03878365@DJFPOST01.djf.agrsci.dk>

Using Rgraphviz, I draw the undirected graph with vertices A,B,C and D and edges A:B, B:C, C:D, D:A, A:C. I want the vertices A and B to be red and C and D to be blue. The problem is the following: I want the edges A:B and B:C to be green and the edges C:D and C:A to be yellow, while the edge A:C can have the default colour black. I assume that I have to specify this using the edgeAttrs-argument in the plot function, but I can't figure out how. Can anyone help?
Thanks.
S?ren
 
Code for drawing the graph:
 
   library(Rgraphviz)
   V <- c("A","B","C","D")
   E <- list(c("A","B"),c("B","C"),c("C","D"),c("D","A"),c("A","C"))
   Eidx <- lapply(E, match, V)
   edL <- vector("list", length=length(V))
   names(edL) <- V
   for (i in 1:length(Eidx)){
     tmp <- Eidx[[i]]
     print(tmp)
     edL[[tmp[1]]]$edges <- c(edL[[tmp[1]]]$edges, tmp[2])
     edL[[tmp[2]]]$edges <- c(edL[[tmp[2]]]$edges, tmp[1])
   }
   G <- new("graphNEL", nodes=V, edgeL=edL)
   nAttrs <- list()
   nAttrs$fillcolor <- c("red","red","blue","blue")
   names(nAttrs$fillcolor) <- V 
   plot(G, "neato",nodeAttrs=nAttrs)


From rduval at gmail.com  Thu Jul  6 03:11:46 2006
From: rduval at gmail.com (Robert Duval)
Date: Wed, 5 Jul 2006 21:11:46 -0400
Subject: [R] Robust standard errors in logistic regression
In-Reply-To: <Pine.LNX.4.64.0607050837230.32591@homer22.u.washington.edu>
References: <f6b7dfdc0607040914u254a4906qf70bed0e2355ea8f@mail.gmail.com>
	<20060704185417.58355140.Achim.Zeileis@wu-wien.ac.at>
	<f6b7dfdc0607050050w386e2cbao2d1a1a301c648ae1@mail.gmail.com>
	<17579.30615.839209.966789@stat.math.ethz.ch>
	<Pine.LNX.4.64.0607050837230.32591@homer22.u.washington.edu>
Message-ID: <2b6e342f0607051811l89c8412pc6004b1d6320d651@mail.gmail.com>

This discussion leads to another point which is more subtle, but more
important...

You can always get Huber-White (a.k.a robust) estimators of the
standard errors even in non-linear models like the logistic
regression. However, if you beleive your errors do not satisfy the
standard assumptions of the model, then you should not be running that
model as this might lead to biased parameter estimates.

For instance, in the linear regression model you have consistent
parameter estimates independently of whethere the errors are
heteroskedastic or not. However, in the case of non-linear models it
is usually the case that heteroskedasticity will lead to biased
parameter estimates (unless you fix it explicitly somehow).

Stata is famous for providing Huber-White std. errors in most of their
regression estimates, whether linear or non-linear. But this is
nonsensical in the non-linear models since in these cases you would be
consistently estimating the standard errors of inconsistent
parameters.

This point and potential solutions to this problem is nicely discussed
in Wooldrige's Econometric Analysis of Cross Section and Panel Data.






On 7/5/06, Thomas Lumley <tlumley at u.washington.edu> wrote:
> On Wed, 5 Jul 2006, Martin Maechler wrote:
> >>>>>> "Celso" == Celso Barros <celso.barros at gmail.com>
> >>>>>>     on Wed, 5 Jul 2006 04:50:29 -0300 writes:
> >
> > [...............]
> >    Celso> By the way, I was wondering if there is a way to use rlm (from MASS)
> >    Celso> to estimate robust standard errors for logistic regression?
> >
> > rlm stands for 'robust lm'.  What you need here is  'robust glm'.
> >
> > I've already replied to a similar message by you,
> > mentioning the (relatively) new package "robustbase".
> > After installing it, you can
> > use
> >       robustbase::glmrob()
>
> We have a clash of terminology here.  The "robust standard errors" that
> "sandwich" and "robcov" give are almost completely unrelated to glmrob().
> My guess is that Celso wants glmrob(), but I don't know for sure.
>
> The Huber/White sandwich variance estimator for parameters in an ordinary
> generalized linear model gives an estimate of the variance that is
> consistent if the systematic part of the model is correctly specified and
> conservative otherwise.  It is a computationally cheap linear
> approximation to the bootstrap.  These variance estimators seem to usually
> be called "model-robust", though I prefer Nils Hjort's suggestion of
> "model-agnostic", which avoids confusion with "robust statistics". This is
> what sandwich and robcov() do.
>
> glmrob() and rlm() give robust estimation of regression parameters. That
> is, if the data come from a model that is close to the exponential family
> model underlying glm, the estimates will be close to the parameters from
> that exponential family model.  This is a more common statistical sense of
> the term "robust".
>
>
> I think the confusion has been increased by the fact that earlier S
> implementations of robust regression didn't provide standard errors,
> whereas rlm() and glmrob() do. This was partly a quality-of-implementation
> issue and partly because of theoretical difficulties with, eg, lms().
>
>
>         -thomas
>
> Thomas Lumley                   Assoc. Professor, Biostatistics
> tlumley at u.washington.edu        University of Washington, Seattle
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Thu Jul  6 03:33:20 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 5 Jul 2006 21:33:20 -0400
Subject: [R] Rgraphviz: How to control the colours of edges in a graph
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC03878365@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC03878365@DJFPOST01.djf.agrsci.dk>
Message-ID: <971536df0607051833qb2f2a0ub64c0fa0d483eab2@mail.gmail.com>

Try this:

eAttrs <- list(color = c("A~B" = "green", "B~C" = "green", "C~D" = "yellow",
   "A~C" = "yellow"))
plot(G, "neato", nodeAttrs = nAttrs, edgeAttrs = eAttrs)

More info via:
   vignette("Rgraphviz")


On 7/5/06, S?ren H?jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> Using Rgraphviz, I draw the undirected graph with vertices A,B,C and D and edges A:B, B:C, C:D, D:A, A:C. I want the vertices A and B to be red and C and D to be blue. The problem is the following: I want the edges A:B and B:C to be green and the edges C:D and C:A to be yellow, while the edge A:C can have the default colour black. I assume that I have to specify this using the edgeAttrs-argument in the plot function, but I can't figure out how. Can anyone help?
> Thanks.
> S?ren
>
> Code for drawing the graph:
>
>   library(Rgraphviz)
>   V <- c("A","B","C","D")
>   E <- list(c("A","B"),c("B","C"),c("C","D"),c("D","A"),c("A","C"))
>   Eidx <- lapply(E, match, V)
>   edL <- vector("list", length=length(V))
>   names(edL) <- V
>   for (i in 1:length(Eidx)){
>     tmp <- Eidx[[i]]
>     print(tmp)
>     edL[[tmp[1]]]$edges <- c(edL[[tmp[1]]]$edges, tmp[2])
>     edL[[tmp[2]]]$edges <- c(edL[[tmp[2]]]$edges, tmp[1])
>   }
>   G <- new("graphNEL", nodes=V, edgeL=edL)
>   nAttrs <- list()
>   nAttrs$fillcolor <- c("red","red","blue","blue")
>   names(nAttrs$fillcolor) <- V
>   plot(G, "neato",nodeAttrs=nAttrs)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From Sun.Jia at fujixerox.co.jp  Thu Jul  6 03:36:44 2006
From: Sun.Jia at fujixerox.co.jp (Sun Jia)
Date: Thu, 6 Jul 2006 10:36:44 +0900
Subject: [R] PLS method
Message-ID: <001c01c6a09c$a3627980$bf89f981@leipc>

dear all,

I am a new comer to R and statistic.  Now I have a little confuse about the
package pls.

I have to use 5 components to form a model. There are strong relationship
between some of the components, which leads to the changes of the sign of
each coeficeince, of course this is unwanted when using the normal
regression way. So I choose the way of PLS, which is good at solve this kind
of problem.

In my work,

q is the response and w,c,d,r,o are the 5 components.

         w     c       d      r           o        q

1  219.580 0.880 102.742 12.988 0.9380 11

2  245.806 0.900  97.798 11.764 1.0080 12

3  219.850 0.910  93.764  5.608 1.1006 16

4  226.904 0.842 110.080 14.614 0.8398  7

5  250.792 0.868 108.212 14.714 0.8990 10

6  225.264 0.930  96.748  6.906 1.1784 16

7  229.562 0.856 103.204 12.900 0.8730 12

8  239.560 0.880 101.036 11.766 0.9452 12

9  199.008 0.920  91.338  3.918 1.1234 17

10 220.458 0.910  88.322  9.868 1.0746 13

11 201.228 0.910  89.202 10.328 1.0514 14

12 199.160 0.920  90.126  2.088 1.1326 15

13 135.540 0.786 121.506 19.140 0.6934  2

14 296.272 0.864 130.896 22.614 0.9104  6

15 190.766 0.840 108.050  7.336 0.8210  8

I have used the following sentence.

b.pls<-mvr(q~w+c+d+r+o,data=b,method="simpls")

> coef.mvr(b.pls)

, , 5 comps



            q

w  0.01993749

c 12.42713250

d -0.12050551

r -0.20287088

o  9.63670488



 I have found that the sign of each component still cannot be explained by
the reality. For instance, the sign of w should be negative rather than
positive.

> b.pls<-mvr(q~c+d+r+o,data=b,method="simpls")

> coef.mvr(b.pls)

, , 4 comps



            q

c 76.39196611

d -0.06512864

r -0.18272329

o -3.02212146





When I delete one of the components, the w, I found the coefficients of the
rest ones also changes the sign, the component of o.

As far as I concerned, this kind of situation should only happened when use
the normal regression rather than PLS regression.

Is there any wrong with my understanding?

 Why does this problem happen?



I am appreciated for your help and answer.


From epistat at gmail.com  Thu Jul  6 03:49:46 2006
From: epistat at gmail.com (zhijie zhang)
Date: Thu, 6 Jul 2006 09:49:46 +0800
Subject: [R] questions on data management
Message-ID: <2fc17e30607051849g2ec28d7bx59220d47f50149fe@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060706/e040b258/attachment.pl 

From jholtman at gmail.com  Thu Jul  6 04:49:33 2006
From: jholtman at gmail.com (jim holtman)
Date: Wed, 5 Jul 2006 22:49:33 -0400
Subject: [R] apologes if you already saw this :efficiency question
In-Reply-To: <5951469.1017771152135206338.JavaMail.root@vms171.mailsrvcs.net>
References: <5951469.1017771152135206338.JavaMail.root@vms171.mailsrvcs.net>
Message-ID: <644e1f320607051949p2d4cc16t657aab24ada5031e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060705/528860c6/attachment.pl 

From ThadenJohnJ at uams.edu  Thu Jul  6 06:53:02 2006
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Wed, 5 Jul 2006 23:53:02 -0500
Subject: [R] Warning while subsetting using Matrix package
In-Reply-To: <mailman.5.1151920802.10328.r-help@stat.math.ethz.ch>
Message-ID: <0C6BF3FC506F664F90C8BA3E0160462D04A7586F@EXCHANGE3.ad.uams.edu>

Hello,
  Could someone please explain the following warning while subsetting in
the Matrix package?
Thanks,
John Thaden, PhD
U. Arkansas for Med. Sci.
Little Rock AR USA

> # In the Matrix package...
> library("Matrix")
> # ...I had previously created a sparse matrix in triplet form:
> str(x)
Formal class 'dgTMatrix' [package "Matrix"] with 6 slots
  ..@ i       : int [1:923636] 1 2 3 4 5 6 7 8 9 10 ...
  ..@ j       : int [1:923636] 1 1 1 1 1 1 1 1 1 1 ...
  ..@ Dim     : int [1:2] 600 4482
  ..@ Dimnames:List of 2
  .. ..$ : chr [1:601] "50" "51" "52" "53" ...
  .. ..$ : chr [1:4482] "1" "2" "3" "4" ...
  ..@ x       : num [1:923636] 50.2 51.2 52.2 53.2 54.2 ...
  ..@ factors : list()
>
> # While subsetting x, I was surprised to get this warning: 
> y<-x[1:300,]
Warning message:
number of items to replace is not a multiple of replacement length
> # I did not get a warning, however, if I were more explicit...
> y <- x[1:300,1:4482]
>
> # Of course with a standard matrix, neither form gives a warning:
> (z <- matrix(1:12,4))
     [,1] [,2] [,3]
[1,]    1    5    9
[2,]    2    6   10
[3,]    3    7   11
[4,]    4    8   12
> z[1:2,]
[1,]    1    5    9
[2,]    2    6   10
> z[1:2,1:4]
[1,]    1    5    9
[2,]    2    6   10
> # I wonder what is going on?

Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From ThadenJohnJ at uams.edu  Thu Jul  6 07:02:10 2006
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Thu, 6 Jul 2006 00:02:10 -0500
Subject: [R] Warning while subsetting using Matrix package
Message-ID: <0C6BF3FC506F664F90C8BA3E0160462D04A75870@EXCHANGE3.ad.uams.edu>

Sorry, I omitted background information:
  R version: 2.3.0
  OS: Windows XP
  CPU:  Pentium III, 
  RAM:  768 MB

Also, what command-line memory settings might prevent R from crashing
while using the Matrix package to convert my 600 X 4482 dgTMatrix to the
dgCMatrix class or to an expanded Matrix, via the as() function? I can
do this with half of the matrix, 300 x 4482.

Thanks,
John T

Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From markleeds at verizon.net  Thu Jul  6 07:52:53 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 06 Jul 2006 00:52:53 -0500 (CDT)
Subject: [R] apologes if you already saw this :efficiency question
Message-ID: <29085745.839911152165173574.JavaMail.root@vms070.mailsrvcs.net>

>From: jim holtman <jholtman at gmail.com>
>Date: Wed Jul 05 21:49:33 CDT 2006
>To: "markleeds at verizon.net" <markleeds at verizon.net>
>Cc: r-help at stat.math.ethz.ch
>Subject: Re: [R] apologes if you already saw this :efficiency question


jim : i don't want to take advantage of your kindness and
generosity but when you have time, could you think about
the following.

remember the function gabor gave me to pick out the column
of a dataframe ( for the same named columns ) that hadt
the most non zero elements. 

it was tapply(seq(DF),names(Df),f) where f was

function(x) x[which.max(colSums(Df[x]!=0)]

I was hoping that it wouldn't be so difficult to change
the criteria to the following.

rather than pick out the column with the maximum # of nonzero elements, I want to take the average of the same named columns but
don't include zero valued elements that are in any rows. So, the resultant matrix would be the unique names and the columns would
be averages of the samed named columns but if a column had a zero
in one of it s rows, then that zero wouldn't be included in the average. Basically, this is because in this case,
zero doesn't really mean 0. it means leave it out because it's not involved.

i'm sorry to bother you and it's not urgwnt and i won't
start bothering you all the time. i am very aware of (
not in the R sense but in other ways ) how generosity can
get taken advantage of so that's the las tthing I want to do.
Thanks a lot. also, sometimes examples help, so
, if you need one, i can definitely make one up. actually,
i will make one up and send you
in the next email. i want to send this because if
i write too long an email my email dies and i lose it.


                                             Mark






















>Is this what you want to do??> x <- data.frame(a=paste(letters[1:10], 1:10), 
>+ b=paste(letters[11:20], 1:10), c=paste(LETTERS[1:10], 1:10))
>> x
>????? a??? b??? c
>1?? a 1? k 1? A 1
>2?? b 2? l 2? B 2
>3?? c 3? m 3? C 3
>4?? d 4? n 4? D 4
>5?? e 5? o 5? E 5
>6?? f 6? p 6? F 6
>7?? g 7? q 7? G 7
>8?? h 8? r 8? H 8
>9?? i 9? s 9? I 9
>10 j 10 t 10 J 10
>> (y <- as.vector(t(x[,1:2])))
>?[1] "a 1"? "k 1"? "b 2"? "l 2"? "c 3"? "m 3"? "d 4"? "n 4"? "e 5"? "o 5"? "f 6"? "p 6"? "g 7"? "q 7" 
>[15] "h 8"? "r 8"? "i 9"? "s 9"? "j 10" "t 10"
>> gsub(" ", "", y)
>?[1] "a1"? "k1"? "b2"? "l2"? "c3"? "m3"? "d4"? "n4"? "e5"? "o5"? "f6"? "p6"? "g7"? "q7"? "h8"? "r8" 
>[17] "i9"? "s9"? "j10" "t10"
>> 
>
>
>?On 7/5/06, markleeds at verizon.net <markleeds at verizon.net> wrote:hi everyone : i'm not sure if my previous mail about
>this got sent. i was typing and
>erroneosuyl hit a button and lost what i was typing.
>
>anyway, i have the code below ( it works ) in which i run through the rows of a dataframe, taking out the first two
>fields which are characters strings ( with some extra spacing so
>i yuse gsub) and appending these character strings to a list so that i can build one big list.
>
>there are 17,000 rows so i was hoping there might be a ( even just slightly. it doesn't have to be incresible improvement ) more efficient way to do this. I also think that remember someone saying that using the c command to make something bigger is not a good idea.
>
>the code is below. thanks.
>
>???????????? for paircounter in 1:nrow(tempdata) {
>
>????????????????firsstock<-gsub(" ","",tempdata[paircounter,1]
>????????????????secondstock<-gsub(" ","",tempdata[paircounter,2]
>
>???????????????? if ( paircounter == 1 ) {
>???????????????????? stocklist<-c(firststock,secondstock)
>??????????????????} else {
>??????????????????????stocklist<(stocklist,firststock,secondstock)
>??????????????????}
>????????????????}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>-- 
>Jim Holtman
>Cincinnati, OH
>+1 513 646 9390 (Cell)
>+1 513 247 0281 (Home)
>
>What is the problem you are trying to solve?


From jacques.veslot at good.ibl.fr  Thu Jul  6 09:19:15 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Thu, 06 Jul 2006 09:19:15 +0200
Subject: [R] questions on data management
In-Reply-To: <2fc17e30607051849g2ec28d7bx59220d47f50149fe@mail.gmail.com>
References: <2fc17e30607051849g2ec28d7bx59220d47f50149fe@mail.gmail.com>
Message-ID: <44ACB973.5030406@good.ibl.fr>

merge(mn[sample(1:nrow(mn), 3, rep=F),], xy, by.x=c("m","n"), by.y=c("x","y"))
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------

zhijie zhang a ?crit :
> Dear friends,
>  suppose i have two datasets: A  and B
> A:
> id<-1:6
> x<-c(1,2,3,4,5,6)
> y<-c(2,4,6,8,3,2)
> xy<-data.frame(id,x,y)
> B
>  m<-c(1,1,3,3,5,5)
> n<-c(2,2,6,6,3,3)
> mn<-data.frame(m,n)
> Now, i want to perfomr two tasks:
> 1. get a subset of B,no duplicate values,:
> C:
> m n
> 1 2
> 3 6
> 5 3
> 
> 2.Extract the values in A on the conditions that x=m and y=n
> the results should be:
>  id x y
> 1 1 2
> 3 3 6
> 5 5 3
> Thanks very much!
> 
> 
> 
>


From maechler at stat.math.ethz.ch  Thu Jul  6 09:32:31 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 6 Jul 2006 09:32:31 +0200
Subject: [R] Warning while subsetting using Matrix package
In-Reply-To: <0C6BF3FC506F664F90C8BA3E0160462D04A75870@EXCHANGE3.ad.uams.edu>
References: <0C6BF3FC506F664F90C8BA3E0160462D04A75870@EXCHANGE3.ad.uams.edu>
Message-ID: <17580.48271.592693.686921@stat.math.ethz.ch>

>>>>> "JohnT" == Thaden, John J <ThadenJohnJ at uams.edu>
>>>>>     on Thu, 6 Jul 2006 00:02:10 -0500 writes:

  JohnT> > # ...I had previously created a sparse matrix in triplet form:
  JohnT> > str(x)
  JohnT> Formal class 'dgTMatrix' [package "Matrix"] with 6 slots
  JohnT>   ..@ i       : int [1:923636] 1 2 3 4 5 6 7 8 9 10 ...
  JohnT>   ..@ j       : int [1:923636] 1 1 1 1 1 1 1 1 1 1 ...
  JohnT>   ..@ Dim     : int [1:2] 600 4482
  JohnT>   ..@ Dimnames:List of 2
  JohnT>   .. ..$ : chr [1:601] "50" "51" "52" "53" ...
  JohnT>   .. ..$ : chr [1:4482] "1" "2" "3" "4" ...
  JohnT>   ..@ x       : num [1:923636] 50.2 51.2 52.2 53.2 54.2 ...
  JohnT>   ..@ factors : list()
  JohnT> >
  JohnT> > # While subsetting x, I was surprised to get this warning: 
  JohnT> > y<-x[1:300,]
  JohnT> Warning message:
  JohnT> number of items to replace is not a multiple of replacement length

and later

    JohnT> Sorry, I omitted background information:
    JohnT> R version: 2.3.0
    JohnT> OS: Windows XP
    JohnT> CPU:  Pentium III, 
    JohnT> RAM:  768 MB

You omitted the most pertinent information: The version of
'Matrix' you are using.
The latest released version of Matrix does *not* show the
behavior you mentioned.
{So I have now spent 20 minutes just because you did not update 'Matrix'..}

    JohnT> Also, what command-line memory settings might prevent R from crashing
    JohnT> while using the Matrix package to convert my 600 X 4482 dgTMatrix to the
    JohnT> dgCMatrix class or to an expanded Matrix, via the as() function? I can
    JohnT> do this with half of the matrix, 300 x 4482.

It's hard to believe that you get a "crash" when coercing to
'dgC' -- but of course this really depends how much memory you
have already goggled up by other large objects in your R
workspace, or by other applications running at the same time in
Windows.  Coercing to a full matrix will of course require 
8 * 601 * 4482 = 21549456 extra bytes just for the numbers.
That's only 21.5 Megabytes, so I wonder..

I have never seen R crashes from using 'Matrix', but then I
work with an operating system, not with M$ Windows.  
Maybe you meant you got an error message "... memory allocation .."?

    JohnT> Thanks,
    JohnT> John T


From dieter.menne at menne-biomed.de  Thu Jul  6 10:17:53 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 6 Jul 2006 08:17:53 +0000 (UTC)
Subject: [R] Editors which have strong/solid support for SWeave?
References: <1abe3fa90607050114p1d77c792p714282dd39fe05b6@mail.gmail.com>	<44AB89CB.5000609@optonline.net>
	<1abe3fa90607050307p20b9ba8ct615cf8fb698816c7@mail.gmail.com>
	<44ABA73B.1080402@statistik.uni-dortmund.de>
Message-ID: <loom.20060706T101159-196@post.gmane.org>

Uwe Ligges <ligges <at> statistik.uni-dortmund.de> writes:

> Anyway, these days Tinn-R seems to be the "Windows" way to go. It 
> already supports R, LaTeX and SWeave.

A somewhat underused feature in Tinn-R is background color. I use a very light
(5%) gray for the <<Rcode >>= sections in rnw-file, giving a separation between
R and tex without beeing to distracting.

Dieter


From rdiaz at cnio.es  Thu Jul  6 10:19:39 2006
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Thu, 6 Jul 2006 10:19:39 +0200
Subject: [R] Colinearity Function in R
In-Reply-To: <20060705151605.96144.qmail@web30301.mail.mud.yahoo.com>
References: <20060705151605.96144.qmail@web30301.mail.mud.yahoo.com>
Message-ID: <200607061019.39587.rdiaz@cnio.es>

Dear Peter,

I especially like the VIF (and GVIF) functions in package car, by John Fox. 
(I'm assuming you are dealing with [generalized] linear models).

HTH,

R.


On Wednesday 05 July 2006 17:16, Peter Lauren wrote:
> Is there a colinearty function implemented in R?  I
> have tried help.search("colinearity") and
> help.search("collinearity") and have searched for
> "colinearity" and "collinearity" on
> http://www.rpad.org/Rpad/Rpad-refcard.pdf but with no
> success.
>
> Many thanks in advance,
> Peter Lauren.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Ram?n D?az-Uriarte
Bioinformatics 
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}


From vag_pan at yahoo.gr  Thu Jul  6 10:40:14 2006
From: vag_pan at yahoo.gr (Vangelis Pan)
Date: Thu, 6 Jul 2006 09:40:14 +0100 (BST)
Subject: [R] About Arrangements sets
Message-ID: <20060706084014.93813.qmail@web26710.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060706/9b656fe0/attachment.pl 

From tlu004 at student.uib.no  Thu Jul  6 12:04:54 2006
From: tlu004 at student.uib.no (Torleif Markussen Lunde)
Date: Thu, 06 Jul 2006 12:04:54 +0200
Subject: [R] Usage of repeat,while or loop. Documetation?
Message-ID: <44ACE046.6030406@student.uib.no>

Hi
I want to run the loop described in the example below, but cannot find any 
good documentation on how to do it. I wold be very happy if someone could 
help me on this, as it would save me for a lot of work.

Kindest regards
Torleif
University of Bergen/Norway

#Sample 1

m<-0

#35-40
#want to repeat this untill m=300
m<-m+1
if (length(type[maling==m&type==1&length>=35&length<=39])>0)
a<-length(type[maling==m&type==1&length>=35&length<=39])  else a<-0

if (length(type[maling==m&type==2&length>=35&length<=39])>0)
b<-length(type[maling==m&type==2&length>=35&length<=39]) else b<-0

if (length(type[maling==m&type==3&length>=35&length<=39])>0)
c<-length(type[maling==m&type==3&length>=35&length<=39]) else c<-0

if (length(type[maling==m&type==4&length>=35&length<=39])>0)
d<-length(type[maling==m&type==4&length>=35&length<=39]) else d<-0

if (length(type[maling==m&type==5&length>=35&length<=39])>0)
e<-length(type[maling==m&type==5&length>=35&length<=39]) else e<-0

a35<-(a+b)/(a+b+c+d+e); a35<-format(a35,digits=4)
l35<-length(length[maling==m&type==1&length>=35&length<=39])

.
.
.
.
.

#65+
if (length(type[maling==m&type==1&length>=65])>0)
a<-length(type[maling==m&type==1&length>=65])  else a<-0

if (length(type[maling==m&type==2&length>=65])>0)
b<-length(type[maling==m&type==2&length>=65]) else b<-0

if (length(type[maling==m&type==3&length>=65])>0)
c<-length(type[maling==m&type==3&length>=65]) else c<-0


if (length(type[maling==m&type==4&length>=65])>0)
d<-length(type[maling==m&type==4&length>=65]) else d<-0

if (length(type[maling==m&type==5&length>=65])>0)
e<-length(type[maling==m&type==5&length>=65]) else e<-0

a65<-(a+b)/(a+b+c+d+e); a65<-format(a65,digits=4)
l65<-length(length[maling==m&type==1&length>=65])

la1<-latitude[maling==m]; la1<-format(la1[1],digits=9)
lo1<-longitude[maling==m]; lo1<-format(lo1[1],digits=9)
year1<-year[maling==m]; year1<-format(year1[1],digits=4)
mnd1<-mnd[maling==m]; mnd1<-format(mnd1[1],digits=2)
dag1<-dag[maling==m]; dag1<-format(dag1[1],digits=2)
omr1<-omr[maling==m]; omr1<-format(omr1[1],digits=2)
dist1<-dist[maling==m]; dist1<-format(dist1[1],digits=2)
start1<-start[maling==m]; start1<-format(start1[1],digits=4)
stopp1<-stopp[maling==m]; stopp1<-format(stopp1[1],digits=4)
homr1<-homr[maling==m]; homr1<-format(homr1[1],digits=2)
skip1<-skip[maling==m]; skip1<-skip1[1]

######################
#Then i want to update the kt1 name to change from kt1 to kt2 to kt3...
#kt300. The goal is to create a table with
#kt1
#kt2
.
.
#kt300
#by using rbind. Suggestions on how to update the table as the loop is
#running are welcome.
######################

kt1<-c(la1,lo1,year1,mnd1,dag1,omr1,dist1,start1,stopp1,homr1,a30,a35,a40,a45,a50,a55,a60,a65,l30,l35,l40,l45,l50,l55,l60,l65)


From ivan.kalafatic at gmail.com  Thu Jul  6 13:19:29 2006
From: ivan.kalafatic at gmail.com (Ivan Kalafatic)
Date: Thu, 6 Jul 2006 12:19:29 +0100
Subject: [R] Problem with garchFit function in fSeries
Message-ID: <6dbf89a50607060419k2a0892bdtd2264694e78a2278@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060706/c165afdb/attachment.pl 

From jacques.veslot at good.ibl.fr  Thu Jul  6 13:26:49 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Thu, 06 Jul 2006 13:26:49 +0200
Subject: [R] questions on data management
In-Reply-To: <2fc17e30607051849g2ec28d7bx59220d47f50149fe@mail.gmail.com>
References: <2fc17e30607051849g2ec28d7bx59220d47f50149fe@mail.gmail.com>
Message-ID: <44ACF379.6050103@good.ibl.fr>


mnu <- unique(mn[order(mn$m,mn$n),])
mnu$p <- table(paste(mn$m, mn$n))

merge(mnu[sample(1:nrow(mnu), size=3, prob=mnu$p, replace=F), c("m","n")], xy, by.x=c("m","n"), 
by.y=c("x","y"))

-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


zhijie zhang a ?crit :
> Dear friends,
>  suppose i have two datasets: A  and B
> A:
> id<-1:6
> x<-c(1,2,3,4,5,6)
> y<-c(2,4,6,8,3,2)
> xy<-data.frame(id,x,y)
> B
>  m<-c(1,1,3,3,5,5)
> n<-c(2,2,6,6,3,3)
> mn<-data.frame(m,n)
> Now, i want to perfomr two tasks:
> 1. get a subset of B,no duplicate values,:
> C:
> m n
> 1 2
> 3 6
> 5 3
> 
> 2.Extract the values in A on the conditions that x=m and y=n
> the results should be:
>  id x y
> 1 1 2
> 3 3 6
> 5 5 3
> Thanks very much!
> 
> 
> 
>


From andrea.meyer at unibas.ch  Thu Jul  6 13:34:15 2006
From: andrea.meyer at unibas.ch (Andrea Meyer)
Date: Thu, 06 Jul 2006 13:34:15 +0200
Subject: [R] Software for Epidmiological, Longitudinal Data
Message-ID: <44ACF537.9000805@unibas.ch>

Hello

We are a team working on a prospective psychological study. The study 
design is based on assessing data of three generations of humans over a 
long time period, wherein epidemiological as well as biological data 
will be assessed. Sample sizes will range from about 100 to several 
thousand depending on the research question.
 
Currently we are looking for an apropriate statistical package. Here are 
some features that the software should have:
- strong in the analysis of epidemiological and longitudinal data
- platform independent (should run under different operating systems 
like Windows, Mac OS, Unix)
- Ease of use for non-statistic-professionals (i.e. userfriendly GUI)
- High acceptance by scientific journals, by the FDA
- Importance relative to other packages with respect to the number of 
users, the number of publications in which the software is used, the 
market share etc.  (including the recent development of these indices!) 
 
As we had some problems in finding information concerning these items we 
would like to ask you where we might find it (if at all) and why R is 
presumably the best competitor and why?
 
Thanks in advance for any suggestions concerning this!

-- 

Dr. Andrea Hans Meyer
University of Basel
Institute of Psychology
Missionsstrasse 60/62
CH-4055 Basel
Switzerland
Phone: ++41 61 267 06 55
Fax:     ++41 61 267 06 59


From srini_iyyer_bio at yahoo.com  Thu Jul  6 14:18:47 2006
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Thu, 6 Jul 2006 05:18:47 -0700 (PDT)
Subject: [R] Comparing two matrices
Message-ID: <20060706121847.67018.qmail@web38101.mail.mud.yahoo.com>

hi:

I have matrix with dimensions(200 X 20,000). I have
another file, a tab-delim file where first column
variables are row names and second column variables
are column names. 


For instance:

> tmat
  Apple Orange Mango Grape Star
A     0      0     0     0    0
O     0      0     0     0    0
M     0      0     0     0    0
G     0      0     0     0    0
S     0      0     0     0    0



> tb # tab- delim file. 
      V1 V2
1  Apple  S
2  Apple  A
3  Apple  O
4 Orange  A
5 Orange  O
6 Orange  S
7  Mango  M
8  Mango  A
9  Mango  S


I have to read each line of the 'tb' (tab delim file),
take the first variable, check if matches any rowname
of the matrix. Take the second variable of the row in
and check if it matches any column name.  If so,  put
1 else leave it. 


The following is a small piece of code that, I felt is
a solutions. However, since my original matrix and
tab-delim file is very very huge, I am not sure if it
is really doing the correct thing. Could any one
please help me if I am doing this correct. 



> for(i in 1:length(tb[,1])){
+  r = tb[i,1]
+  c = as.character(tb[i,2])
+  tmat[rownames(tmat)==c,colnames(tmat)==r] <-1
+ }



> tmat
  Apple Orange Mango Grape Star
A     1      1     1     0    0
O     1      1     0     0    0
M     0      0     1     0    0
G     0      0     0     0    0
S     1      1     1     0    0



Thanks.


From roolio4news at gmail.com  Thu Jul  6 14:30:53 2006
From: roolio4news at gmail.com (Julien Laugel)
Date: Thu, 6 Jul 2006 12:30:53 +0000
Subject: [R] use of apply in a data frame on a row by row basis
Message-ID: <a0b6f2b80607060530q6609be1enbb63ed0fbeb9c5d6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060706/e1d87939/attachment.pl 

From andy_liaw at merck.com  Thu Jul  6 14:36:23 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 6 Jul 2006 08:36:23 -0400
Subject: [R] Comparing two matrices  [Broadcast]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA028848AF@usctmx1106.merck.com>

It might be a bit faster to do matrix indexing:

R> tbm <- as.matrix(tb) # turn it into a character matrix
R> tmat[cbind(match(tbm[,2], rownames(tmat)), match(tbm[,1],
colnames(tmat)))] <- 1
> tmat
  Apple Orange Mango Grape Star
A     1      1     1     0    0
O     1      1     0     0    0
M     0      0     1     0    0
G     0      0     0     0    0
S     1      1     1     0    0

HTH,
Andy
 

From: Srinivas Iyyer
> 
> hi:
> 
> I have matrix with dimensions(200 X 20,000). I have another 
> file, a tab-delim file where first column variables are row 
> names and second column variables are column names. 
> 
> 
> For instance:
> 
> > tmat
>   Apple Orange Mango Grape Star
> A     0      0     0     0    0
> O     0      0     0     0    0
> M     0      0     0     0    0
> G     0      0     0     0    0
> S     0      0     0     0    0
> 
> 
> 
> > tb # tab- delim file. 
>       V1 V2
> 1  Apple  S
> 2  Apple  A
> 3  Apple  O
> 4 Orange  A
> 5 Orange  O
> 6 Orange  S
> 7  Mango  M
> 8  Mango  A
> 9  Mango  S
> 
> 
> I have to read each line of the 'tb' (tab delim file), take 
> the first variable, check if matches any rowname of the 
> matrix. Take the second variable of the row in and check if 
> it matches any column name.  If so,  put
> 1 else leave it. 
> 
> 
> The following is a small piece of code that, I felt is a 
> solutions. However, since my original matrix and tab-delim 
> file is very very huge, I am not sure if it is really doing 
> the correct thing. Could any one please help me if I am doing 
> this correct. 
> 
> 
> 
> > for(i in 1:length(tb[,1])){
> +  r = tb[i,1]
> +  c = as.character(tb[i,2])
> +  tmat[rownames(tmat)==c,colnames(tmat)==r] <-1 }
> 
> 
> 
> > tmat
>   Apple Orange Mango Grape Star
> A     1      1     1     0    0
> O     1      1     0     0    0
> M     0      0     1     0    0
> G     0      0     0     0    0
> S     1      1     1     0    0
> 
> 
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From p.dalgaard at biostat.ku.dk  Thu Jul  6 14:37:05 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Jul 2006 14:37:05 +0200
Subject: [R] Comparing two matrices
In-Reply-To: <20060706121847.67018.qmail@web38101.mail.mud.yahoo.com>
References: <20060706121847.67018.qmail@web38101.mail.mud.yahoo.com>
Message-ID: <x2bqs26ham.fsf@viggo.kubism.ku.dk>

Srinivas Iyyer <srini_iyyer_bio at yahoo.com> writes:

> hi:
> 
> I have matrix with dimensions(200 X 20,000). I have
> another file, a tab-delim file where first column
> variables are row names and second column variables
> are column names. 
> 
> 
> For instance:
> 
> > tmat
>   Apple Orange Mango Grape Star
> A     0      0     0     0    0
> O     0      0     0     0    0
> M     0      0     0     0    0
> G     0      0     0     0    0
> S     0      0     0     0    0
> 
> 
> 
> > tb # tab- delim file. 
>       V1 V2
> 1  Apple  S
> 2  Apple  A
> 3  Apple  O
> 4 Orange  A
> 5 Orange  O
> 6 Orange  S
> 7  Mango  M
> 8  Mango  A
> 9  Mango  S
> 
> 
> I have to read each line of the 'tb' (tab delim file),
> take the first variable, check if matches any rowname
> of the matrix. Take the second variable of the row in
> and check if it matches any column name.  If so,  put
> 1 else leave it. 
> 
> 
> The following is a small piece of code that, I felt is
> a solutions. However, since my original matrix and
> tab-delim file is very very huge, I am not sure if it
> is really doing the correct thing. Could any one
> please help me if I am doing this correct. 
> 
> 
> 
> > for(i in 1:length(tb[,1])){
> +  r = tb[i,1]
> +  c = as.character(tb[i,2])
> +  tmat[rownames(tmat)==c,colnames(tmat)==r] <-1
> + }

There are much faster ways. Try (untested)

n1 <- match(tb$V1, rownames(tmat))
n2 <- match(tb$V2, colnames(tmat))
m <- unique(cbind(n1,n2)[complete.cases(n1,n2),])
tmat[m] <- 1

The unique() part may or may not be beneficial. 



-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From dimitris.rizopoulos at med.kuleuven.be  Thu Jul  6 14:43:42 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 6 Jul 2006 14:43:42 +0200
Subject: [R] Comparing two matrices
References: <20060706121847.67018.qmail@web38101.mail.mud.yahoo.com>
Message-ID: <004501c6a0f9$d02abb00$0540210a@www.domain>

try something like:

mat1 <- matrix(0, 26, 100, dimnames = list(letters, 1:100))
mat2 <- cbind(sample(letters, 10), sample(100, 10))
###########
mat1[cbind(match(mat2[, 1], rownames(mat1)), match(mat2[, 2], 
colnames(mat1)))] <- 1


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Srinivas Iyyer" <srini_iyyer_bio at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, July 06, 2006 2:18 PM
Subject: [R] Comparing two matrices


> hi:
>
> I have matrix with dimensions(200 X 20,000). I have
> another file, a tab-delim file where first column
> variables are row names and second column variables
> are column names.
>
>
> For instance:
>
>> tmat
>  Apple Orange Mango Grape Star
> A     0      0     0     0    0
> O     0      0     0     0    0
> M     0      0     0     0    0
> G     0      0     0     0    0
> S     0      0     0     0    0
>
>
>
>> tb # tab- delim file.
>      V1 V2
> 1  Apple  S
> 2  Apple  A
> 3  Apple  O
> 4 Orange  A
> 5 Orange  O
> 6 Orange  S
> 7  Mango  M
> 8  Mango  A
> 9  Mango  S
>
>
> I have to read each line of the 'tb' (tab delim file),
> take the first variable, check if matches any rowname
> of the matrix. Take the second variable of the row in
> and check if it matches any column name.  If so,  put
> 1 else leave it.
>
>
> The following is a small piece of code that, I felt is
> a solutions. However, since my original matrix and
> tab-delim file is very very huge, I am not sure if it
> is really doing the correct thing. Could any one
> please help me if I am doing this correct.
>
>
>
>> for(i in 1:length(tb[,1])){
> +  r = tb[i,1]
> +  c = as.character(tb[i,2])
> +  tmat[rownames(tmat)==c,colnames(tmat)==r] <-1
> + }
>
>
>
>> tmat
>  Apple Orange Mango Grape Star
> A     1      1     1     0    0
> O     1      1     0     0    0
> M     0      0     1     0    0
> G     0      0     0     0    0
> S     1      1     1     0    0
>
>
>
> Thanks.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From murdoch at stats.uwo.ca  Thu Jul  6 14:46:51 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 06 Jul 2006 08:46:51 -0400
Subject: [R] Comparing two matrices
In-Reply-To: <20060706121847.67018.qmail@web38101.mail.mud.yahoo.com>
References: <20060706121847.67018.qmail@web38101.mail.mud.yahoo.com>
Message-ID: <44AD063B.7060906@stats.uwo.ca>

On 7/6/2006 8:18 AM, Srinivas Iyyer wrote:
> hi:
> 
> I have matrix with dimensions(200 X 20,000). I have
> another file, a tab-delim file where first column
> variables are row names and second column variables
> are column names. 
> 
> 
> For instance:
> 
>> tmat
>   Apple Orange Mango Grape Star
> A     0      0     0     0    0
> O     0      0     0     0    0
> M     0      0     0     0    0
> G     0      0     0     0    0
> S     0      0     0     0    0
> 
> 
> 
>> tb # tab- delim file. 
>       V1 V2
> 1  Apple  S
> 2  Apple  A
> 3  Apple  O
> 4 Orange  A
> 5 Orange  O
> 6 Orange  S
> 7  Mango  M
> 8  Mango  A
> 9  Mango  S
> 
> 
> I have to read each line of the 'tb' (tab delim file),
> take the first variable, check if matches any rowname
> of the matrix. Take the second variable of the row in
> and check if it matches any column name.  If so,  put
> 1 else leave it. 
> 
> 
> The following is a small piece of code that, I felt is
> a solutions. However, since my original matrix and
> tab-delim file is very very huge, I am not sure if it
> is really doing the correct thing. Could any one
> please help me if I am doing this correct. 
> 
> 
> 
>> for(i in 1:length(tb[,1])){
> +  r = tb[i,1]
> +  c = as.character(tb[i,2])
> +  tmat[rownames(tmat)==c,colnames(tmat)==r] <-1
> + }

I think that works, but it's not as fast as some other ways of doing the 
same thing.  For example, table(tb) will give you a table of the counts 
of each pair of entries in tb.  pmin(table(tb), 1) will set the maximum 
count to 1.

An advantage of this approach is that it will show you if there are any 
entries in tb that aren't in your tmat (typos, etc.).  A disadvantage is 
that if there are any missing categories (e.g. G, Grape, Star in your 
sample) they won't show up at all, and you may need some manipulations 
to get things to look exactly the way you asked.  For example,

 > pmin(table(tb))
         V2
V1       A M O S
   Apple  1 0 1 1
   Mango  1 1 0 1
   Orange 1 0 1 1
 > pmin(table(tb[,2:1]))
    V1
V2  Apple Mango Orange
   A     1     1      1
   M     0     1      0
   O     1     0      1
   S     1     1      1


Duncan Murdoch



> 
> 
> 
>> tmat
>   Apple Orange Mango Grape Star
> A     1      1     1     0    0
> O     1      1     0     0    0
> M     0      0     1     0    0
> G     0      0     0     0    0
> S     1      1     1     0    0
> 
> 
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ggrothendieck at gmail.com  Thu Jul  6 14:54:48 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 6 Jul 2006 08:54:48 -0400
Subject: [R] use of apply in a data frame on a row by row basis
In-Reply-To: <a0b6f2b80607060530q6609be1enbb63ed0fbeb9c5d6@mail.gmail.com>
References: <a0b6f2b80607060530q6609be1enbb63ed0fbeb9c5d6@mail.gmail.com>
Message-ID: <971536df0607060554l318f3babm9cb42bdf03f305d9@mail.gmail.com>

On 7/6/06, Julien Laugel <roolio4news at gmail.com> wrote:
> Hello all,
>
> I'm trying to use the apply function on a data frame,
> by applying a function that takes a one row data.frame as argument .
>
> Here's the example :
> myfun = function(x) paste(x$f1 , x$f2)
> df = data.frame(f1 = c(1,4,10),f2 = "hello")
>
> apply(df,1,myfun) ==> Does not work (I get "character(0)" )

Try one of these:

sapply(split(df, rownames(df)), myfun)
sapply(1:nrow(df), function(i) paste(df$f1[i], df$f2[i]))
sapply(1:nrow(df), function(i) paste(df[i,1], df[i,2]))

>
> Though : myfun(df[1,]) works,
> and myfun(df) works as well.
>
> So if myfun(df) works, that's fine!
> BUT, if the output of the function a bit more complex, it does not work :
> In this case I'm using a timeSeries object (from Rmetrics)
>
> library(fMultivar)
> timerange = timeSequence(from = "2001-04-11", length.out = 3,by = "weeks",
> format = "%Y-%m-%d", FinCenter = "GMT")
> myfun2 = function(x) timeSeries(rnorm(3),timerange)
>
> In this case, myfun2(df) returns only the result of the 1st row!
> BUT
> apply(df,1,myfun2) in this case, does work
>
> But I haven't used the "$" notation to access one field
> Now if I try (and that is the cas that bugs me currently) :
>
> myfun3 = function(x) timeSeries(rep(x$f1,3),timerange)
>
> myfun3(df)
> Error in "rownames<-"(`*tmp*`, value = c("2001-04-11 02:00:00", "2001-04-18
> 02:00:00", :
> length of 'dimnames' [1] is not equal to range of the array
> (sorry for the translation : I'm using a french version of R)
> Though
> > myfun3(df[1,])
> TS.1
> 2001-04-11 02:00:00 1
> 2001-04-18 02:00:00 1
> 2001-04-25 02:00:00 1
> works well
> BUT :
> apply(df,1,myfun3)
> yields :
>
> Error in array(x, c(length(x), 1), if (!is.null(names(x))) list(names(x), :
> trying to change an attribute in NULL
>
> So as a result I'm verry annoyed, and I don't see the logic behind.
> How can I do to make a apply-like function on the latter function? (myfun3)
>
> Many thanks in advance
>
> Regards,
>
> Roolio
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From sekemp at glam.ac.uk  Thu Jul  6 14:58:07 2006
From: sekemp at glam.ac.uk (Kemp S E (Comp))
Date: Thu, 6 Jul 2006 13:58:07 +0100
Subject: [R] package download numbers
Message-ID: <0BA7EE4D4646E0409D458D347C508B7801280497@MAILSERV1.uni.glam.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060706/ae715619/attachment.pl 

From f.harrell at vanderbilt.edu  Thu Jul  6 15:06:25 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 06 Jul 2006 08:06:25 -0500
Subject: [R] Colinearity Function in R
In-Reply-To: <200607061019.39587.rdiaz@cnio.es>
References: <20060705151605.96144.qmail@web30301.mail.mud.yahoo.com>
	<200607061019.39587.rdiaz@cnio.es>
Message-ID: <44AD0AD1.3050107@vanderbilt.edu>

Ramon Diaz-Uriarte wrote:
> Dear Peter,
> 
> I especially like the VIF (and GVIF) functions in package car, by John Fox. 
> (I'm assuming you are dealing with [generalized] linear models).
> 
> HTH,
> 
> R.

And even though ignoring Y prevents you from seeing the whole picture, 
redundancy analysis of X can help you do data reduction in an unbiased 
way to build reliable models.  The transcan and varclus functions in the 
Hmisc package help in this way.  transcan will show you the maximum R^2 
that can be obtained in predicting each predictor from all others, using 
a spline generalized additive model.  varclus draws a variable 
clustering dendogram.

Frank Harrell

> 
> 
> On Wednesday 05 July 2006 17:16, Peter Lauren wrote:
>> Is there a colinearty function implemented in R?  I
>> have tried help.search("colinearity") and
>> help.search("collinearity") and have searched for
>> "colinearity" and "collinearity" on
>> http://www.rpad.org/Rpad/Rpad-refcard.pdf but with no
>> success.
>>
>> Many thanks in advance,
>> Peter Lauren.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From andy_liaw at merck.com  Thu Jul  6 15:21:06 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 6 Jul 2006 09:21:06 -0400
Subject: [R] use of apply in a data frame on a row by row basis
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA028848D2@usctmx1106.merck.com>

?apply has the answer, in particular, in the "Detail" section:

  If X is not an array but has a dimension attribute, apply attempts
  to coerce it to an array via as.matrix if it is two-dimensional 
  (e.g., data frames) or via as.array. 

Thus the function being apply()'ed  should not be expecting a 1-row
data frame.  Instead, it should be expecting a vector.

Andy 

From: Julien Laugel
> 
> Hello all,
> 
> I'm trying to use the apply function on a data frame, by 
> applying a function that takes a one row data.frame as argument .
> 
> Here's the example :
> myfun = function(x) paste(x$f1 , x$f2)
> df = data.frame(f1 = c(1,4,10),f2 = "hello")
> 
> apply(df,1,myfun) ==> Does not work (I get "character(0)" )
> 
> Though : myfun(df[1,]) works,
> and myfun(df) works as well.
> 
> So if myfun(df) works, that's fine!
> BUT, if the output of the function a bit more complex, it 
> does not work :
> In this case I'm using a timeSeries object (from Rmetrics)
> 
> library(fMultivar)
> timerange = timeSequence(from = "2001-04-11", length.out = 
> 3,by = "weeks", format = "%Y-%m-%d", FinCenter = "GMT")
> myfun2 = function(x) timeSeries(rnorm(3),timerange)
> 
> In this case, myfun2(df) returns only the result of the 1st row!
> BUT
> apply(df,1,myfun2) in this case, does work
> 
> But I haven't used the "$" notation to access one field Now 
> if I try (and that is the cas that bugs me currently) :
> 
> myfun3 = function(x) timeSeries(rep(x$f1,3),timerange)
> 
> myfun3(df)
> Error in "rownames<-"(`*tmp*`, value = c("2001-04-11 
> 02:00:00", "2001-04-18 02:00:00", :
> length of 'dimnames' [1] is not equal to range of the array 
> (sorry for the translation : I'm using a french version of R) Though
> > myfun3(df[1,])
> TS.1
> 2001-04-11 02:00:00 1
> 2001-04-18 02:00:00 1
> 2001-04-25 02:00:00 1
> works well
> BUT :
> apply(df,1,myfun3)
> yields :
> 
> Error in array(x, c(length(x), 1), if (!is.null(names(x))) 
> list(names(x), :
> trying to change an attribute in NULL
> 
> So as a result I'm verry annoyed, and I don't see the logic behind.
> How can I do to make a apply-like function on the latter 
> function? (myfun3)
> 
> Many thanks in advance
> 
> Regards,
> 
> Roolio
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From murdoch at stats.uwo.ca  Thu Jul  6 15:36:39 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 06 Jul 2006 09:36:39 -0400
Subject: [R] package download numbers
In-Reply-To: <0BA7EE4D4646E0409D458D347C508B7801280497@MAILSERV1.uni.glam.ac.uk>
References: <0BA7EE4D4646E0409D458D347C508B7801280497@MAILSERV1.uni.glam.ac.uk>
Message-ID: <44AD11E7.7000706@stats.uwo.ca>

On 7/6/2006 8:58 AM, Kemp S E (Comp) wrote:
> Hi,
>  
> I have a contributed package on CRAN (GammaTest) and was wondering whether it is possible to find out how many times it has been downloaded. I would like to include this information in my Ph.D. thesis.

I think the answer is no.  CRAN maintains logs; I'm not sure if they're 
public or not.  But there are dozens of mirrors of CRAN, and downloaders 
are encouraged to get packages from one of them.  CRAN has no way of 
knowing if I download something from one of the mirrors.

There's also the issue of caches:  if you ask to download a package, 
your ISP may not make a request for it, they may just deliver it out of 
their cache.

So it would be possible to get a drastic underestimate of the number of 
downloads, but it is essentially impossible to get an accurate estimate.

This is why we encourage people who publish papers where the work used R 
or R packages to cite them in their bibliography.  Web of Science 
(http://isiknowledge.com/WOS) will track citations in the open 
literature; this is one way the authors can get academic recognition for 
their work.  Use the citation() function to see how the author would 
like to be cited, or a default format if the author hasn't specified.

Duncan Murdoch


From R.Birnie at leeds.ac.uk  Thu Jul  6 15:52:38 2006
From: R.Birnie at leeds.ac.uk (Richard Birnie)
Date: Thu, 6 Jul 2006 14:52:38 +0100
Subject: [R] pvclust Error:NA/NaN/Inf in foreign function call (arg 11)
Message-ID: <C6AC304539F43C4196253FD94B7960F307AF8B@HERMES2.ds.leeds.ac.uk>

Hi all,

I'm new to R and I'm struggling to decipher an error message. Briefly, I am trying to use the pvclust package to do hierarchical clustering of some CGH data. The data is from the Progenetix CGH database. It is arranged as a table where each column is a single case and each row is a single chromosome band. The value in each cell is either 0, 1, 2, or -1. Corresponding to no change, gain, high level amplification, or loss respectively Based on the documentation for the pvclust package I came up with this code.

>ProgenetixCRC.all.pvclust <- pvclust(ProgenetixCRC.all, method.dist="cor", method.hclust="average",use.cor="pairwise.complete.obs",nboot=1000)

this results in the following error

Bootstrap (r = 0.5)... Error in hclust(distance, method = method.hclust) : 
	NA/NaN/Inf in foreign function call (arg 11)
In addition: Warning message:
the standard deviation is zero in: cor(x, y, na.method, method == "kendall"

I'm not really clear on what this means. I have searched the mailing list archive and as best I can tell this is caused by missing values in the dataset. This is confusing because my dataset does not have any missing values. The only thing I can think of is that the function is treating 0 as a missing value. Is this the case and if so what is the best way to handle this? 

I also found something about using 'traceback()' to trace failed function calls, so I tried that, the output means little to me but here it is.

>traceback()
4: hclust(distance, method = method.hclust)
3: FUN(X[[1]], ...)
2: lapply(r, boot.hclust, data = data, object.hclust = data.hclust, 
       nboot = nboot, method.dist = method.dist, use.cor = use.cor, 
       method.hclust = method.hclust, store = store)
1: pvclust(ProgenetixCRC.all, method.dist = "cor", method.hclust = "average", 
       use.cor = "pairwise.complete.obs", nboot = 1000)

and my session info
> sessionInfo()
R version 2.2.1, 2005-12-20, i686-redhat-linux-gnu 

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[7] "base"     

other attached packages:
pvclust 
"1.1-0" 

Can someone please advise how to handle this. If any more info is needed just tell me what commands.
regards,
Richard
Dr Richard Birnie
Scientific Officer
Section of Pathology and Tumour Biology
Welcome Brenner Building, LIMM
St James University Hospital
Beckett St, Leeds, LS9 7TF
Tel:0113 3438624
e-mail: r.birnie at leeds.ac.uk


From markleeds at verizon.net  Thu Jul  6 15:59:18 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 06 Jul 2006 08:59:18 -0500 (CDT)
Subject: [R] tapply question
Message-ID: <27583569.8425541152194358865.JavaMail.root@vms172.mailsrvcs.net>

I think I understand tapply but i still
can't figure out how to do the following.

I have a dataframe where some of the column names are the same
and i want to make a new dataframe where columns
that have the same name are averaged by row.

so, if the data frame, DF, was 

AAA    BBB      CCC   AAA DDD
1       0        7     11  13
2        0       8     12  14
3        0       6      0  15

then the resulting data frame would be exactly the same except
that the AAA column would be 

6   comes from  (11 + 1)/2
7    comes from  (12 + 2)/2
3   stays 3 because the element in the other AAA is zero
so i don't want to average that one. it shoulsd just stay 3.

So, I do 

DF[DF == 0]<-NA
rowaverage<-function(x) x[rowMeans(forecastDf[x],na.rm=TRUE)
revisedDF<-tapply(seq(DF),names(DF),rowmeans)

there are two problems with this :

1) i need to go through the rows of the same name, not the columns
so i don't think seq(DF) is right because that goes through 
the columns but i want to go through rows.

2) BBB will come back with ALL NA's ( since
it was unique and there was nothing else to average ( and I don't know how to transform that BB column to all zero's.

thanks and i'm sorry for so many questions. i'm getting bettter with this stuff and my questions will decrease soon.

my guess is that i no longer should be using tapply ?
and should be using some other version of apply.
thanks
                                         mark


From jacques.veslot at good.ibl.fr  Thu Jul  6 16:10:33 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Thu, 06 Jul 2006 16:10:33 +0200
Subject: [R] tapply question
In-Reply-To: <27583569.8425541152194358865.JavaMail.root@vms172.mailsrvcs.net>
References: <27583569.8425541152194358865.JavaMail.root@vms172.mailsrvcs.net>
Message-ID: <44AD19D9.6090604@good.ibl.fr>

i think you can't have column with the same names.

 > data.frame(AAA=1:3, AAA=4:6)
   AAA AAA.1
1   1     4
2   2     5
3   3     6

but you could subset the data frame by names using substring():

sapply(unique(substring(names(data1), 1, 3)), function(x)
	rowMeans(data1[, substring(names(data1), 1, 3) == x])


-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


markleeds at verizon.net a ?crit :
> I think I understand tapply but i still
> can't figure out how to do the following.
> 
> I have a dataframe where some of the column names are the same
> and i want to make a new dataframe where columns
> that have the same name are averaged by row.
> 
> so, if the data frame, DF, was 
> 
> AAA    BBB      CCC   AAA DDD
> 1       0        7     11  13
> 2        0       8     12  14
> 3        0       6      0  15
> 
> then the resulting data frame would be exactly the same except
> that the AAA column would be 
> 
> 6   comes from  (11 + 1)/2
> 7    comes from  (12 + 2)/2
> 3   stays 3 because the element in the other AAA is zero
> so i don't want to average that one. it shoulsd just stay 3.
> 
> So, I do 
> 
> DF[DF == 0]<-NA
> rowaverage<-function(x) x[rowMeans(forecastDf[x],na.rm=TRUE)
> revisedDF<-tapply(seq(DF),names(DF),rowmeans)
> 
> there are two problems with this :
> 
> 1) i need to go through the rows of the same name, not the columns
> so i don't think seq(DF) is right because that goes through 
> the columns but i want to go through rows.
> 
> 2) BBB will come back with ALL NA's ( since
> it was unique and there was nothing else to average ( and I don't know how to transform that BB column to all zero's.
> 
> thanks and i'm sorry for so many questions. i'm getting bettter with this stuff and my questions will decrease soon.
> 
> my guess is that i no longer should be using tapply ?
> and should be using some other version of apply.
> thanks
>                                          mark
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jholtman at gmail.com  Thu Jul  6 16:16:36 2006
From: jholtman at gmail.com (jim holtman)
Date: Thu, 6 Jul 2006 10:16:36 -0400
Subject: [R] tapply question
In-Reply-To: <27583569.8425541152194358865.JavaMail.root@vms172.mailsrvcs.net>
References: <27583569.8425541152194358865.JavaMail.root@vms172.mailsrvcs.net>
Message-ID: <644e1f320607060716j655a16absb4310850eccc9057@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060706/363a4989/attachment.pl 

From sachinj.2006 at yahoo.com  Thu Jul  6 16:16:47 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Thu, 6 Jul 2006 07:16:47 -0700 (PDT)
Subject: [R] KPSS test
Message-ID: <20060706141647.87886.qmail@web37612.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060706/c1cf1dd7/attachment.pl 

From stp02fm at sheffield.ac.uk  Thu Jul  6 16:16:53 2006
From: stp02fm at sheffield.ac.uk (F Monadjemi)
Date: Thu,  6 Jul 2006 15:16:53 +0100
Subject: [R] Warning message
Message-ID: <1152195413.44ad1b55af97a@webmail.shef.ac.uk>


Dear reader,

I am trying to simulate 1000 data from nonlinear model in order to be able to
do
mixed effect analysis. If the program works but give you following warning
message, what should I do? Can I still accept the result, which is about the
precision of model parameter estimation? 


FALSE CONVERGENCE. in: ms( ~  - logLik(nlmeSt, nlmePars), start = list(nlmePars
=
	c(coef(nlmeSt))), control =  ....
SINGULAR CONVERGENCE. in: ms( ~  - logLik(nlmeSt, nlmePars), start =
list(nlmePars =
	c(coef(nlmeSt))), control =  ....


Thanks

Farinaz


From christine.krahenbuhl at integragen.com  Thu Jul  6 16:28:08 2006
From: christine.krahenbuhl at integragen.com (Christine Krahenbuhl)
Date: Thu, 06 Jul 2006 16:28:08 +0200
Subject: [R] problem with rdocdir option when installing R2.3.1
Message-ID: <44AD1DF8.80805@integragen.com>

Hi,

In the "R documentation and Administration" manual it is said that "The 
configure or make variables |rdocdir| and |rsharedir| can be used to 
install the system-independent doc and share directories to somewhere 
other than |libdir|".

Then I've installed R v2.3.1 on a solaris platform (SunOS 5.9)  with the 
following commands :
 > ./configure --prefix=/somewhere/stats  rdocdir=/elsewhere/html
 > make
 > make install

It works well since documentation files are located in in 
/elsewhere/html but when I use the html browser to compulse the 
documentation, the links doesn't work.
In fact, all HREF balise are like "<a href="../../library/package/...">" 
but the library directory is not installed to "../../library" it is 
installed to "/somewhere/stats/R-2.3.1/library".

Is there a parameter that I've missed ?

Thank you for your help

Christine


From ripley at stats.ox.ac.uk  Thu Jul  6 16:54:22 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 6 Jul 2006 15:54:22 +0100 (BST)
Subject: [R] problem with rdocdir option when installing R2.3.1
In-Reply-To: <44AD1DF8.80805@integragen.com>
References: <44AD1DF8.80805@integragen.com>
Message-ID: <Pine.LNX.4.64.0607061549540.7570@gannet.stats.ox.ac.uk>

On Thu, 6 Jul 2006, Christine Krahenbuhl wrote:

> Hi,
>
> In the "R documentation and Administration" manual it is said that "The
> configure or make variables |rdocdir| and |rsharedir| can be used to
> install the system-independent doc and share directories to somewhere
> other than |libdir|".

And indeed it does so.

> Then I've installed R v2.3.1 on a solaris platform (SunOS 5.9)  with the
> following commands :
> > ./configure --prefix=/somewhere/stats  rdocdir=/elsewhere/html
> > make
> > make install
>
> It works well since documentation files are located in in
> /elsewhere/html but when I use the html browser to compulse the

What does `compulse' mean, and what exactly did you do?

> documentation, the links doesn't work.
> In fact, all HREF balise are like "<a href="../../library/package/...">"
> but the library directory is not installed to "../../library" it is
> installed to "/somewhere/stats/R-2.3.1/library".
>
> Is there a parameter that I've missed ?

You are supposed to use help.start() to view the HTML help pages: it makes 
the requisite links to a session-specific directory.  (This is unrelated 
to your subject line: you use help.start() however you installed R.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Andie_Baker at bio-rad.com  Thu Jul  6 17:32:15 2006
From: Andie_Baker at bio-rad.com (Andie Baker)
Date: Thu, 6 Jul 2006 08:32:15 -0700
Subject: [R] biostatistics help list
Message-ID: <OF5E15E1CE.51F45D5D-ON882571A3.00550F28-882571A3.005559D8@bio-rad.com>

Hello,

I am searching for a good general help list in biostatistics and came 
across your site.  Can you recommend a list?  Thanks so much!

Andie

Andie Baker PhD
Biostatistician II
Bio-Rad Diagnostics Group
14620 NE N Woodinville Way
Woodinville, WA 98072
Telephone: (425) 498-1647
andie_baker at bio-rad.com


From ashelton at albany.edu  Thu Jul  6 18:19:38 2006
From: ashelton at albany.edu (Anne P Shelton)
Date: Thu, 6 Jul 2006 12:19:38 -0400
Subject: [R] read.xport issues
Message-ID: <9D95C2906FCCE04F836ECA17C4CE09210268C647@UAEXCH.univ.albany.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060706/3e2c7c9d/attachment.pl 

From pieterprovoost at gmail.com  Thu Jul  6 18:29:30 2006
From: pieterprovoost at gmail.com (Pieter Provoost)
Date: Thu, 06 Jul 2006 18:29:30 +0200
Subject: [R] custom tick labels on image
Message-ID: <44AD3A6A.1000507@gmail.com>

Hi,

I'm trying to visualise a matrix using image(), and I would like to add 
the row and column names as tick labels. I tried adding an axis, but the 
problem is that this axis goes from 0 to 1 which results in labels being 
added starting at the far right end of the axis.

image(t(data), col = terrain.colors(20), axes=FALSE)
axis(3, at = 1:length(colnames(data)), labels=colnames(data))

How can I do this better?
Cheers
Pieter


From ruser1000 at yahoo.com  Thu Jul  6 11:37:22 2006
From: ruser1000 at yahoo.com (ruser ruser)
Date: Thu, 6 Jul 2006 02:37:22 -0700 (PDT)
Subject: [R] as.data.frame question
Message-ID: <20060706093722.44288.qmail@web55912.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060706/d14220b2/attachment.pl 

From ripley at stats.ox.ac.uk  Thu Jul  6 18:50:29 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 6 Jul 2006 17:50:29 +0100 (BST)
Subject: [R] custom tick labels on image
In-Reply-To: <44AD3A6A.1000507@gmail.com>
References: <44AD3A6A.1000507@gmail.com>
Message-ID: <Pine.LNX.4.64.0607061745570.12172@gannet.stats.ox.ac.uk>

On Thu, 6 Jul 2006, Pieter Provoost wrote:

> Hi,
>
> I'm trying to visualise a matrix using image(), and I would like to add
> the row and column names as tick labels. I tried adding an axis, but the
> problem is that this axis goes from 0 to 1 which results in labels being
> added starting at the far right end of the axis.
>
> image(t(data), col = terrain.colors(20), axes=FALSE)
> axis(3, at = 1:length(colnames(data)), labels=colnames(data))
>
> How can I do this better?

It is probably easiest to supply x and y when calling image, as in the 
final example on the help page.  (And BTW, ncol() gives the number of 
columns more readily than length(colnames()).)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From andy_liaw at merck.com  Thu Jul  6 19:00:45 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 6 Jul 2006 13:00:45 -0400
Subject: [R] as.data.frame question
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02884972@usctmx1106.merck.com>

You shouldn't need the as.data.frame(), as sqlFetch() already returns
a data.frame.  You could try adding the as.is argument to sqlFetch()
(see how it's specified in ?read.table), or else convert them
afterward.

Andy 

From: ruser ruser
>  
>    
>   I'm using the RODBC library to read in an Excel file using 
> the odbcConnectExcel function. I can read the Excel data 
> prefectly, however all my character variables get converted 
> to factors when using
>   my.data <- as.data.frame(sqlFetch(channel, "Sheet1"))
>    
>   I've tried using
>   my.data <- as.data.frame(I(sqlFetch(channel, "Sheet1")))
>    
>   but this creates just one column of data. Is there an 
> efficient way to read in the character variables from my 
> Excel data as characters rather than converting them to factors.
>    
>   Any help is much appreciated.
>    
>   Regards,
>    
>   Harry
>    
> 
>  		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From voiklis at gmail.com  Thu Jul  6 19:18:54 2006
From: voiklis at gmail.com (JOHN VOIKLIS)
Date: Thu, 6 Jul 2006 13:18:54 -0400
Subject: [R] problem installing package on ppc
Message-ID: <a253ef4c0607061018r6a5adf6aq4bbb98965c15e931@mail.gmail.com>

Hello,

I am running R Version 2.3.0 (2006-04-24) for the
powerpc-apple-darwin8.6.0 platform. I recently intalled the Rstem
package from the omegahat repository
(http://www.omegahat.org/R/bin/macosx/powerpc/contrib/2.3/), but, when
I tried to load the library, R throws the following error:

Error: package 'Rstem' is not installed for 'arch=ppc'

I have checked with the author and one other Rstem user; their ppc
binaries work just fine. The problem seems to be on my end.

Does anyone have an idea what might be going wrong?

Thank you for any help you can offer,

John


From pieterprovoost at gmail.com  Thu Jul  6 19:20:25 2006
From: pieterprovoost at gmail.com (Pieter Provoost)
Date: Thu, 06 Jul 2006 19:20:25 +0200
Subject: [R] custom tick labels on image
In-Reply-To: <Pine.LNX.4.64.0607061745570.12172@gannet.stats.ox.ac.uk>
References: <44AD3A6A.1000507@gmail.com>
	<Pine.LNX.4.64.0607061745570.12172@gannet.stats.ox.ac.uk>
Message-ID: <44AD4659.3030709@gmail.com>

Prof Brian Ripley schreef:
> On Thu, 6 Jul 2006, Pieter Provoost wrote:
>
>> Hi,
>>
>> I'm trying to visualise a matrix using image(), and I would like to add
>> the row and column names as tick labels. I tried adding an axis, but the
>> problem is that this axis goes from 0 to 1 which results in labels being
>> added starting at the far right end of the axis.
>>
>> image(t(data), col = terrain.colors(20), axes=FALSE)
>> axis(3, at = 1:length(colnames(data)), labels=colnames(data))
>>
>> How can I do this better?
>
> It is probably easiest to supply x and y when calling image, as in the 
> final example on the help page.  (And BTW, ncol() gives the number of 
> columns more readily than length(colnames()).)
>

Thanks, this worked fine. One additional question: how can I rotate the 
labels so they all can be displayed? Srt doesn't seem to work.

x <- (1:nrow(data))
y <- (1:ncol(data))
image(y, x, t(data), col = heat.colors(20), 
axes=FALSE,xlab="",ylab="",srt=45)
axis(3, at = 1:ncol(data), labels=colnames(data),srt=45,tick=FALSE)
axis(2, at = 1:nrow(data), labels=rownames(data),srt=45,tick=FALSE)


From ThadenJohnJ at uams.edu  Thu Jul  6 19:29:42 2006
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Thu, 6 Jul 2006 12:29:42 -0500
Subject: [R] R crash with 'library(Matrix);
 as(x, "dgCMatrix")' [was: Warning while subsetting...]
In-Reply-To: <17580.48271.592693.686921@stat.math.ethz.ch>
Message-ID: <0C6BF3FC506F664F90C8BA3E0160462D04A75873@EXCHANGE3.ad.uams.edu>

Martin Maechler replied to my query "Warning while subsetting...":

MartinM> >>>>> "JohnT" == Thaden, John J <ThadenJohnJ at uams.edu>
MartinM> >>>>>     on Thu, 6 Jul 2006 00:02:10 -0500 writes:

    JohnT> ...
    JohnT> > # While subsetting x, I was surprised to get this warning: 
    JohnT> > y<-x[1:300,]
    JohnT> Warning message:
    JohnT> number of items to replace is not a multiple of replacement
length

  MartinM> and later

    JohnT> Sorry, I omitted background information:
    JohnT> R version: 2.3.0
    JohnT> OS: Windows XP
    JohnT> CPU:  Pentium III, 
    JohnT> RAM:  768 MB

  MartinM> You omitted the most pertinent information: The 
  MartinM> version of 'Matrix' you are using.
  MartinM> The latest released version of Matrix does
  MartinM> *not* show the behavior you mentioned. {So I have 
  MartinM> now spent 20 minutes just because you did not 
  MartinM> update 'Matrix'..}

The Matrix package was version 0.995-10, now is 0.995-11. 
The R base was version 2.3.0, now is 2.3.1. 
Subsetting 'y <- x[1:300,]' now works. Please accept my apology.

    JohnT> Also, what command-line memory settings might prevent
    JohnT> R from crashing while using the Matrix package to 
    JohnT> convert my 600 X 4482 dgTMatrix to the dgCMatrix class
    JohnT> or to an expanded Matrix, via the as() function? I can
    JohnT> do this with half of the matrix, 300 x 4482.

  MartinM> It's hard to believe that you get a "crash" 
  MartinM> when coercing to 'dgC' -- but of course this 
  MartinM> really depends how much memory you have already
  MartinM> goggled up by other large objects in your R
  MartinM> workspace, or by other applications running at
  MartinM> the same time in Windows.  Coercing to a full 
  MartinM> matrix will of course require 8 * 601 * 4482 = 
  MartinM> 21549456 extra bytes just for the numbers.
  MartinM> That's only 21.5 Megabytes, so I wonder..
  MartinM>
  MartinM> I have never seen R crashes from using 'Matrix', 
  MartinM> but then I work with an operating system, not 
  MartinM> with M$ Windows. 
  MartinM>
  MartinM> Maybe you meant you got an error message 
  MartinM> "... memory allocation .."?

Testing again, I closed all applications; disabled antivirus; 
opened RGui; removed all R objects but 'x' (a 600x4482 dgTMatrix); 
opened WinXP's 'Task Manager'; saw only "Rgui" under 
'Applications'; saw processes using a total of 287 MB of memory
under 'Processes'; closed 'Task Manager'; and typed R commands:

> # Steps leading to an R crash...
> ls()
[1] "x"
> str(x)
Formal class 'dgTMatrix' [package "Matrix"] with 6 slots
  ..@ i       : int [1:923636] 1 2 3 4 5 6 7 8 9 10 ...
  ..@ j       : int [1:923636] 1 1 1 1 1 1 1 1 1 1 ...
  ..@ Dim     : int [1:2] 600 4482
  ..@ Dimnames:List of 2
  .. ..$ : chr [1:601] "50" "51" "52" "53" ...
  .. ..$ : chr [1:4482] "1" "2" "3" "4" ...
  ..@ x       : num [1:923636] 50.2 51.2 52.2 53.2 54.2 ...
  ..@ factors : list()
> gc()
          used (Mb) gc trigger (Mb) max used (Mb)
Ncells  183529  5.0     407500 10.9   350000  9.4
Vcells 1928101 14.8    2286173 17.5  1928652 14.8
> library(Matrix)
Loading required package: lattice
> gc()
          used (Mb) gc trigger (Mb) max used (Mb)
Ncells  627772 16.8    1073225 28.7  1073225 28.7
Vcells 2165773 16.6    3345184 25.6  2332013 17.8
> search()
 [1] ".GlobalEnv" "package:Matrix" "package:lattice"
 [4] "package:methods" "package:stats" "package:graphics"  
 [7] "package:grDevices" "package:utils" "package:datasets"
 [10] "Autoloads"  "package:base"     
> #Now the line that causes crashes...
> y <- as(x,"dgCMatrix")

After ~10 seconds, R blinks off and a WinXP dialog appears: 

  R for Windows GUI front-end has encountered 
  a problem and needs to close.  We are sorry
  for the inconvenience....Error signature:
  AppName: rgui.exe  AppVer: 2.31.38247.0  
  ModName: matrix.dll Offset: 0000ff31....
  Report error?

Sincerely,
John Thaden

Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From roolio4news at gmail.com  Thu Jul  6 19:42:38 2006
From: roolio4news at gmail.com (Roolio)
Date: Thu, 6 Jul 2006 17:42:38 +0000
Subject: [R] which data structure for a set of time series ?
Message-ID: <a0b6f2b80607061042x54e00653o21b7190557af3eb4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060706/bdbfed4c/attachment.pl 

From ripley at stats.ox.ac.uk  Thu Jul  6 19:55:08 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 6 Jul 2006 18:55:08 +0100 (BST)
Subject: [R] custom tick labels on image
In-Reply-To: <44AD4659.3030709@gmail.com>
References: <44AD3A6A.1000507@gmail.com>
	<Pine.LNX.4.64.0607061745570.12172@gannet.stats.ox.ac.uk>
	<44AD4659.3030709@gmail.com>
Message-ID: <Pine.LNX.4.64.0607061853200.15918@gannet.stats.ox.ac.uk>

On Thu, 6 Jul 2006, Pieter Provoost wrote:

> Prof Brian Ripley schreef:
>> On Thu, 6 Jul 2006, Pieter Provoost wrote:
>> 
>>> Hi,
>>> 
>>> I'm trying to visualise a matrix using image(), and I would like to add
>>> the row and column names as tick labels. I tried adding an axis, but the
>>> problem is that this axis goes from 0 to 1 which results in labels being
>>> added starting at the far right end of the axis.
>>> 
>>> image(t(data), col = terrain.colors(20), axes=FALSE)
>>> axis(3, at = 1:length(colnames(data)), labels=colnames(data))
>>> 
>>> How can I do this better?
>> 
>> It is probably easiest to supply x and y when calling image, as in the 
>> final example on the help page.  (And BTW, ncol() gives the number of 
>> columns more readily than length(colnames()).)
>> 
>
> Thanks, this worked fine. One additional question: how can I rotate the 
> labels so they all can be displayed? Srt doesn't seem to work.

?axis says

      ...: other graphical parameters may also be passed as arguments to
           this function, particularly, 'cex.axis', 'col.axis' and
           'font.axis' for axis annotation, 'mgp' and 'xaxp' or 'yaxp'
           for positioning, 'tck' or 'tcl' for tick mark length and
           direction, 'las' for vertical/horizontal label orientation,
           or 'fg' instead of 'col', see 'par' on these.

so you need to use las and accept 0/90/etc.


>
> x <- (1:nrow(data))
> y <- (1:ncol(data))
> image(y, x, t(data), col = heat.colors(20), 
> axes=FALSE,xlab="",ylab="",srt=45)
> axis(3, at = 1:ncol(data), labels=colnames(data),srt=45,tick=FALSE)
> axis(2, at = 1:nrow(data), labels=rownames(data),srt=45,tick=FALSE)
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From markleeds at verizon.net  Thu Jul  6 20:17:17 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 06 Jul 2006 13:17:17 -0500 (CDT)
Subject: [R] [Fwd: as.data.frame question]
Message-ID: <7684597.790411152209837220.JavaMail.root@vms076.mailsrvcs.net>

>From: markleeds at verizon.net
>Date: Thu Jul 06 13:16:42 CDT 2006
>To: markleeds at verizon.net
>Subject: as.data.frame question

>hi all : as a result of an lapply command,
>i get the following output.
>
>
>$AAA
>     000106   000107  000108
>        5.5     6.5    3.0
>
>$BBB
>     000106   000107   000108
>        4         5     6
>
>$CCC 
>      000106   000107  000108
>        7        8       9
>
>does someone know how to turn this output into a dataframe
>( or matrix, that's fine also ) like the following :
>
>                 AAA   BBB   CCC
>
>      000106      5.5   4     7
>      000107      6.5   5     8 
>      000108      3.0   6     9
>
>i've tried sooooo many things with no luck. thanks a lot.


From andy_liaw at merck.com  Thu Jul  6 20:30:23 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 6 Jul 2006 14:30:23 -0400
Subject: [R] [Fwd: as.data.frame question]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA0288499F@usctmx1106.merck.com>

What's wrong with this?

> lst <- list(A=c(a=1, b=2, c=3), B=c(a=3, b=2, c=1), C=c(a=2, b=3, c=1))
> lst
$A
a b c 
1 2 3 

$B
a b c 
3 2 1 

$C
a b c 
2 3 1 

> as.data.frame(lst)
  A B C
a 1 3 2
b 2 2 3
c 3 1 1

Andy 

From: markleeds at verizon.net
> 
> >hi all : as a result of an lapply command, i get the 
> following output.
> >
> >
> >$AAA
> >     000106   000107  000108
> >        5.5     6.5    3.0
> >
> >$BBB
> >     000106   000107   000108
> >        4         5     6
> >
> >$CCC 
> >      000106   000107  000108
> >        7        8       9
> >
> >does someone know how to turn this output into a dataframe ( 
> or matrix, 
> >that's fine also ) like the following :
> >
> >                 AAA   BBB   CCC
> >
> >      000106      5.5   4     7
> >      000107      6.5   5     8 
> >      000108      3.0   6     9
> >
> >i've tried sooooo many things with no luck. thanks a lot.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From hpbenton at scripps.edu  Thu Jul  6 20:47:36 2006
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Thu, 6 Jul 2006 11:47:36 -0700
Subject: [R] RMySQL Suse 10.0 installing problems
Message-ID: <002c01c6a12c$a745a4b0$0202a8c0@hpb>

Thanks again everyone,
	I have a problem trying to install RMySQL on Suse 10.0. I have used
both the install.packages() and the 'R CMD INSTALL RMySQL..." neither
worked. I get the error message telling me that it cannot find mysql. So I
have set the flags using 

export PKG_CPPFLAGS="-I</usr/bin/>"
export PKG_LIBS="-L</usr/lib/mysql>"

After that I run R CMD INSTALL RMySQL... it starts to check and it cannot
find mysql_init, mysql.h then it exits after gcc(ing) with an error code of
make:*** [RS-DBI.o] Error 1

I also tried the other way that the help suggested of putting the dir in the
R CMD INSTALL command but that didn't work either giving me the same help
menu.

I am on opensuse 10.0 x86, and yes I su(ed). 

Cheers,

Research Technician
Mass Spectrometry
  o The
 /
o Scripps 
 \
  o Research
 /
o Institute


From markleeds at verizon.net  Thu Jul  6 21:10:54 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 06 Jul 2006 14:10:54 -0500 (CDT)
Subject: [R] [Fwd: as.data.frame question]
Message-ID: <20214556.806971152213054752.JavaMail.root@vms076.mailsrvcs.net>

>From: "Liaw, Andy" <andy_liaw at merck.com>
>Date: Thu Jul 06 13:30:23 CDT 2006
>To: markleeds at verizon.net, r-help at stat.math.ethz.ch
>Subject: RE: [R] [Fwd: as.data.frame question]

my apologies ( especially to jim holtman ).
i was doing the do.call( cbind,dataframe) thing but not setting the output to anything !!! that's why i wasn't getting what i was supposed to get. uuuugh. i'm really sorry and i wopn't
ask anything the rest of the day.



>What's wrong with this?
>
>> lst <- list(A=c(a=1, b=2, c=3), B=c(a=3, b=2, c=1), C=c(a=2, b=3, c=1))
>> lst
>$A
>a b c 
>1 2 3 
>
>$B
>a b c 
>3 2 1 
>
>$C
>a b c 
>2 3 1 
>
>> as.data.frame(lst)
>  A B C
>a 1 3 2
>b 2 2 3
>c 3 1 1
>
>Andy 
>
>From: markleeds at verizon.net
>> 
>> >hi all : as a result of an lapply command, i get the 
>> following output.
>> >
>> >
>> >$AAA
>> >     000106   000107  000108
>> >        5.5     6.5    3.0
>> >
>> >$BBB
>> >     000106   000107   000108
>> >        4         5     6
>> >
>> >$CCC 
>> >      000106   000107  000108
>> >        7        8       9
>> >
>> >does someone know how to turn this output into a dataframe ( 
>> or matrix, 
>> >that's fine also ) like the following :
>> >
>> >                 AAA   BBB   CCC
>> >
>> >      000106      5.5   4     7
>> >      000107      6.5   5     8 
>> >      000108      3.0   6     9
>> >
>> >i've tried sooooo many things with no luck. thanks a lot.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> 
>> 
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments...{{dropped}}


From sachinj.2006 at yahoo.com  Thu Jul  6 21:17:25 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Thu, 6 Jul 2006 12:17:25 -0700 (PDT)
Subject: [R] KPSS test
Message-ID: <20060706191725.22685.qmail@web37606.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060706/d3e72bc0/attachment.pl 

From markleeds at verizon.net  Thu Jul  6 21:39:29 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 06 Jul 2006 14:39:29 -0500 (CDT)
Subject: [R] KPSS test
Message-ID: <4460460.815871152214769218.JavaMail.root@vms076.mailsrvcs.net>

>From:  <markleeds at verizon.net>
>Date: Thu Jul 06 14:17:25 CDT 2006
>To: Sachin J <sachinj.2006 at yahoo.com>
>Subject: Re: [R] KPSS test

sachin : i think your interpretations are right given the data
but kpss is quite a different test than the usual tests
because it assumes that the null is stationarity while dickey fuller  ( DF )  and phillips perron ( PP ) ) assume that the null is a unit root. therefore, you should check whetheer
the conclusions you get from kpss are consistent with what you would get from DF or PP. the results often are not consistent.

also, DF depends on what terms ( trend, constant ) 
you used in your estimation of the model. i'm not sure if kpss 
does also. people generally report Dickey fuller results but they
are a little biased towards acepting  unit root ( lower
power )  so maybe that's why
you are using KPSS ? Eric Zivot has a nice explanation
of a lot of the of the stationarity tests in his S+Finmetrics 
book.

testing for cyclical variation is pretty complex because
that's basically the same as testing for seasonality.
check ord's or ender's book for relatively simple ways of doing that.












>
>>From: Sachin J <sachinj.2006 at yahoo.com>
>>Date: Thu Jul 06 14:17:25 CDT 2006
>>To: R-help at stat.math.ethz.ch
>>Subject: [R] KPSS test
>
>>Hi,
>>   
>>  Am I interpreting the results properly? Are my conclusions correct?
>>   
>>  > KPSS.test(df)
>>    ---- ----
>>  KPSS test
>>  ---- ----
>>    Null hypotheses: Level stationarity and stationarity around a linear trend.
>>  Alternative hypothesis: Unit root.
>>----
>>  Statistic for the null hypothesis of 
>>   level stationarity: 1.089 
>>      Critical values:
>>    0.10  0.05 0.025  0.01
>> 0.347 0.463 0.574 0.739
>>----
>>  Statistic for the null hypothesis of 
>>   trend stationarity: 0.13 
>>      Critical values:
>>    0.10  0.05 0.025  0.01
>> 0.119 0.146 0.176 0.216
>>----
>>  Lag truncation parameter: 1 
>>  
>>CONCLUSION: Reject Ho at 0.05 sig level - Level Stationary
>>     Fail to reject Ho at 0.05 sig level - Trend Stationary 
>>  
>>> kpss.test(df,null = c("Trend"))
>>          KPSS Test for Trend Stationarity
>>  data:  tsdata[, 6] 
>>KPSS Trend = 0.1298, Truncation lag parameter = 1, p-value = 0.07999
>>   
>>  CONCLUSION: Fail to reject Ho - Trend Stationary as p-value < sig. level (0.05)
>>  
>>> kpss.test(df,null = c("Level"))
>>          KPSS Test for Level Stationarity
>>  data:  tsdata[, 6] 
>>KPSS Level = 1.0891, Truncation lag parameter = 1, p-value = 0.01
>>  Warning message:
>>p-value smaller than printed p-value in: kpss.test(tsdata[, 6], null = c("Level")) 
>>   
>>  CONCLUSION: Reject Ho - Level Stationary as p-value > sig. level (0.05)
>>   
>>  Following is my data set
>>   
>>  structure(c(11.08, 7.08, 7.08, 6.08, 6.08, 6.08, 23.08, 32.08, 
>>8.08, 11.08, 6.08, 13.08, 13.83, 16.83, 19.83, 8.83, 20.83, 17.83, 
>>9.83, 20.83, 10.83, 12.83, 15.83, 11.83), .Tsp = c(2004, 2005.91666666667, 
>>12), class = "ts")
>>
>>  Also how do I test this time series for cyclical varitions? 
>>   
>>  Thanks in advance.
>>   
>>  Sachin
>>
>> 		
>>---------------------------------
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From sachinj.2006 at yahoo.com  Thu Jul  6 21:42:53 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Thu, 6 Jul 2006 12:42:53 -0700 (PDT)
Subject: [R] Access values in kpssstat-class
Message-ID: <20060706194253.73277.qmail@web37608.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060706/99a568fd/attachment.pl 

From sachinj.2006 at yahoo.com  Thu Jul  6 21:48:46 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Thu, 6 Jul 2006 12:48:46 -0700 (PDT)
Subject: [R] KPSS test
In-Reply-To: <4460460.815871152214769218.JavaMail.root@vms076.mailsrvcs.net>
Message-ID: <20060706194846.69029.qmail@web37613.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060706/a27a9254/attachment.pl 

From gangchen at mail.nih.gov  Thu Jul  6 22:50:53 2006
From: gangchen at mail.nih.gov (Gang Chen)
Date: Thu, 6 Jul 2006 16:50:53 -0400
Subject: [R] Package sem
In-Reply-To: <44A7CBD7.1060303@univ-fcomte.fr>
References: <44A6A966.5060206@univ-fcomte.fr> <44A6C578.4010409@cropdesign.com>
	<44A7CBD7.1060303@univ-fcomte.fr>
Message-ID: <DF8E3374-16E7-48A9-8A85-330E49FB8E15@mail.nih.gov>

Hi,

I am trying to run some path analysis with Dr. Fox's sem package. The  
number N in sem(ram, S, N) is supposed to be the total number of  
observations, right? However, in my situation the effective number of  
degrees of freedom for each observed variable is estimated by some  
auto-regression process, thus each variable bears a different DF. Is  
there a way I could run such a path analysis with a vector of DF's in  
sem?

Thanks,
Gang


From dimitrijoe at ipea.gov.br  Thu Jul  6 23:22:27 2006
From: dimitrijoe at ipea.gov.br (Dimitri Szerman)
Date: Thu, 6 Jul 2006 18:22:27 -0300
Subject: [R] nonparametric estimation of the density with right censored data
Message-ID: <b6150c70607061422i6454a394n3437c51624d9a55b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060706/92a32fa3/attachment.pl 

From qli at math.wustl.edu  Thu Jul  6 23:41:03 2006
From: qli at math.wustl.edu (qli at math.wustl.edu)
Date: Thu, 6 Jul 2006 16:41:03 -0500 (CDT)
Subject: [R] a question about glm( )
Message-ID: <33223.128.252.148.177.1152222063.squirrel@www.math.wustl.edu>

Hi,

I am working on an example about generalized linear model in a paper using
glm( ). The code is quite simple and straightforward, but the result is
rediculous. The true parameter is c(4, -6), but the result is c(2.264774,
-3.457114) Can anybody tell me the reason for this? Thanks a lot!!!

Here is the code:


g=function(t){exp(t)/(1+exp(t))}	#the given link function


n = 100	 # sample size
beta.true = c(4,-6)	#the true parameter

#----------------------------------------- the given x
x = rep(0,n)

for(i in 1:n)

	{if (i<=80)

		x[i]=0.90-0.0025*i

	 else

		x[i]=0.70-0.035*(i-80)

	}

x = cbind(1,x)

#----------------------------------------- to generate y

meany = g(x%*%beta.true)


y = rep(0,100)
for(i in 1:n)

	{ # simulate the data from a binomial distribution

		y[i] = rbinom(1,1,meany[i])
	}


#------------------------------------------ to do the Quasi-likelihood
beta.old = glm (y~x[,2],family=binomial())$coef


From gregor.gorjanc at gmail.com  Thu Jul  6 23:42:28 2006
From: gregor.gorjanc at gmail.com (Gregor Gorjanc)
Date: Thu, 06 Jul 2006 23:42:28 +0200
Subject: [R] Efficiency of subsetting with x[someCode] or test<-someCode and
	x[test]
Message-ID: <44AD83C4.6000205@bfro.uni-lj.si>

Hello!

Does anyone have any knowledge on efficiency of subsetting via

x[someCode]

or

ind <- someCode
x[ind]

In first approach I find it sometime cumbersome if someFunc() is
something complex i.e. long code, but it is short otherwise. I wonder if
assigning result to some indicator object is less efficient since
"copies" result is first stored and then reused. Is there any general
way with this that one should go for?

Thanks!

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From p.dalgaard at biostat.ku.dk  Thu Jul  6 23:53:09 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Jul 2006 23:53:09 +0200
Subject: [R] a question about glm( )
In-Reply-To: <33223.128.252.148.177.1152222063.squirrel@www.math.wustl.edu>
References: <33223.128.252.148.177.1152222063.squirrel@www.math.wustl.edu>
Message-ID: <x2ejwyienu.fsf@turmalin.kubism.ku.dk>

qli at math.wustl.edu writes:

> Hi,
> 
> I am working on an example about generalized linear model in a paper using
> glm( ). The code is quite simple and straightforward, but the result is
> rediculous. The true parameter is c(4, -6), but the result is c(2.264774,
> -3.457114) Can anybody tell me the reason for this? Thanks a lot!!!

What's ridiculous about that? With a sample size of 100, the
estimation variation is going to be substantial. I get

> beta.old
(Intercept)      x[, 2]
   3.096393   -4.845186
> confint(glm (y~x[,2],family=binomial()))
Waiting for profiling to be done...
                2.5 %    97.5 %
(Intercept)  1.251333  5.574944
x[, 2]      -8.093080 -2.370165

and c(4, -6) is well within the confidence limits.

 
> Here is the code:
> 
> 
> g=function(t){exp(t)/(1+exp(t))}	#the given link function
> 
> 
> n = 100	 # sample size
> beta.true = c(4,-6)	#the true parameter
> 
> #----------------------------------------- the given x
> x = rep(0,n)
> 
> for(i in 1:n)
> 
> 	{if (i<=80)
> 
> 		x[i]=0.90-0.0025*i
> 
> 	 else
> 
> 		x[i]=0.70-0.035*(i-80)
> 
> 	}
> 
> x = cbind(1,x)
> 
> #----------------------------------------- to generate y
> 
> meany = g(x%*%beta.true)
> 
> 
> y = rep(0,100)
> for(i in 1:n)
> 
> 	{ # simulate the data from a binomial distribution
> 
> 		y[i] = rbinom(1,1,meany[i])
> 	}
> 
> 
> #------------------------------------------ to do the Quasi-likelihood
> beta.old = glm (y~x[,2],family=binomial())$coef
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From s-walker at ti.com  Thu Jul  6 23:58:21 2006
From: s-walker at ti.com (Walker, Sam)
Date: Thu, 6 Jul 2006 16:58:21 -0500
Subject: [R] engineering notation format
Message-ID: <1F5C95F1B887EF42B14F31AEF36AD22F05A0D03E@dlee04.ent.ti.com>

Hi,

How can I format numbers to engineering notation?

That is, like scientific but where the exponent is always a multiple of
three.

Some examples:

1635 000 000      => 1.635E9
  163 500 000     => 163.5E6
0.000 000 000 135 => 135E-9

I've tried format and formatC but I couldn't get them to work they I
want.

Any help is greatly appreciated.


Thanks,
Sam


From Soren.Hojsgaard at agrsci.dk  Fri Jul  7 01:42:35 2006
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Fri, 7 Jul 2006 01:42:35 +0200
Subject: [R] Rgraphviz: Setting the edge width
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC03878366@DJFPOST01.djf.agrsci.dk>

I create an undirected graph with Rgraphviz (see code below). I would like to make the edges thicker. Can anyone help on this??
Regards
S?ren
 
 
   V <- c("A","B","C","D")
   E <- list(c("A","B"),c("B","C"),c("C","D"),c("D","A"),c("A","C"))
   Eidx <- lapply(E, match, V)
   edL <- vector("list", length=length(V))
   names(edL) <- V
   for (i in 1:length(Eidx)){
     tmp <- Eidx[[i]]
     print(tmp)
     edL[[tmp[1]]]$edges <- c(edL[[tmp[1]]]$edges, tmp[2])
     edL[[tmp[2]]]$edges <- c(edL[[tmp[2]]]$edges, tmp[1])
   }
   G <- new("graphNEL", nodes=V, edgeL=edL)
   nAttrs <- list()
   nAttrs$fillcolor <- c("red","red","blue","blue")
   names(nAttrs$fillcolor) <- V 
   eAttrs <- list(color = c("A~B" = "green", "B~C" = "green", "C~D" = "yellow",
      "A~C" = "yellow"))
   plot(G, "neato", nodeAttrs = nAttrs, edgeAttrs = eAttrs)


From kbeath at efs.mq.edu.au  Fri Jul  7 02:15:20 2006
From: kbeath at efs.mq.edu.au (Ken Beath)
Date: Fri, 7 Jul 2006 10:15:20 +1000
Subject: [R] densityplot and panel.groups
Message-ID: <169A0785-AFEB-4CF3-ADF4-AAB242E4CF1D@efs.mq.edu.au>

I am trying to plot multiple densityplots on each panel, and using  
panel.groups to do some additional plotting (not included in the  
example) as in this example.

library(lattice)

thedata <- data.frame(x=rnorm(1200),class=rep(1:3,each=200,times=2),
	group=rep(1:2,each=100,times=6))

densityplot(~x | class,groups=group,
	xlab='x',
	panel.groups = function(x,subscripts,groups,panel.number,...) {
		panel.densityplot(x=x,subscripts=subscripts,groups=groups,...)
# other stuff here
	},
	plot.points=FALSE,
	layout=c(3,1),
	data=thedata)

But get an "Error in panel.superpose(x, darg = darg, plot.points =  
plot.points, ref = FALSE,  : formal argument "panel.groups" matched  
by multiple actual arguments" using R 2.3.1 (MacOS X 10.4.7), I think  
this worked with a previous version of R.


From markleeds at verizon.net  Fri Jul  7 03:58:55 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Thu, 06 Jul 2006 20:58:55 -0500 (CDT)
Subject: [R] computational speed question
Message-ID: <1852497.321391152237535490.JavaMail.root@vms061.mailsrvcs.net>

I have a 250 row by 20,000 column dataframe called temp and
I do

rowaverage<-function(x) rowmeans(temp[x],na.rm=TRUE )
averages<-tapply(seq(temp),names(temp),rowaverage)
averages<-do.call('cbind',averages)

, is it okay that it's been running for 4 hours or
does this mean that something went wrong. I am on windows
XP and i did ctrl alt delete and it seems like the process
is running as far as i can tell. I have 4 cpus
and the one is getting used at its max  and when
i do ctrl alt dlete task manager and click on processes
it says 699,412K under the mem usage column
but that number hasn't changed in a looong time.

when i click on performance, cpu usage says 25 % which make sense.
and pf usage sys 970 MB. physical memory total is 8387312,
available 6868916 and system cache is 3007044 but these numbers
move around slightly.

kernel memory total is 108072
paged 81344
nonpaged 26744

as far as hardware, i'm pretty clueless. all
i know is that i have 4 cpus ( actually 2 cpus but
somehow each cpu is 2 cpus whatever that means. but only one of the 4 gets used st anyone time unless i run multiple instances of R ), and 9 gig of RAM. I don't know what kind of a chip i have but i know the computer is from Dell.

it's okay if it takes this long but i was just wondering
if there is a way to check if things have stopped or
somehow frozen ?

when i do a tail on the .Rout file ( I am running the program
using R CMD BATCH ), it's just sitting at the same spot
where this computation would be done so I can't tell much from that.
Thanks.


From deepayan.sarkar at gmail.com  Fri Jul  7 04:20:24 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 6 Jul 2006 21:20:24 -0500
Subject: [R] densityplot and panel.groups
In-Reply-To: <169A0785-AFEB-4CF3-ADF4-AAB242E4CF1D@efs.mq.edu.au>
References: <169A0785-AFEB-4CF3-ADF4-AAB242E4CF1D@efs.mq.edu.au>
Message-ID: <eb555e660607061920w9015c5bjcaaf032fa861655@mail.gmail.com>

On 7/6/06, Ken Beath <kbeath at efs.mq.edu.au> wrote:
> I am trying to plot multiple densityplots on each panel, and using
> panel.groups to do some additional plotting (not included in the
> example) as in this example.
>
> library(lattice)
>
> thedata <- data.frame(x=rnorm(1200),class=rep(1:3,each=200,times=2),
>         group=rep(1:2,each=100,times=6))
>
> densityplot(~x | class,groups=group,
>         xlab='x',
>         panel.groups = function(x,subscripts,groups,panel.number,...) {
>                 panel.densityplot(x=x,subscripts=subscripts,groups=groups,...)
> # other stuff here
>         },
>         plot.points=FALSE,
>         layout=c(3,1),
>         data=thedata)
>
> But get an "Error in panel.superpose(x, darg = darg, plot.points =
> plot.points, ref = FALSE,  : formal argument "panel.groups" matched
> by multiple actual arguments" using R 2.3.1 (MacOS X 10.4.7), I think
> this worked with a previous version of R.

Among the changes in lattice from a previous version is the following:

 o instead of ignoring it as before, 'panel.xyplot' and
   'panel.densityplot' now deal with a 'groups' argument appropriately
   by calling 'panel.superpose'.  Consequently, the default of 'panel'
   in 'xyplot' etc does not need to be conditional on 'groups'.

What you are seeing is a subtle side effect: panel.densityplot, which
is the default panel function in your call, does not allow a
'panel.groups' argument. You should additionally specify 'panel =
panel.superpose'.

Deepayan


From jholtman at gmail.com  Fri Jul  7 04:59:11 2006
From: jholtman at gmail.com (jim holtman)
Date: Thu, 6 Jul 2006 22:59:11 -0400
Subject: [R] computational speed question
In-Reply-To: <1852497.321391152237535490.JavaMail.root@vms061.mailsrvcs.net>
References: <1852497.321391152237535490.JavaMail.root@vms061.mailsrvcs.net>
Message-ID: <644e1f320607061959r2959c1cdw72f569ea7762c3bb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060706/5992dc0e/attachment.pl 

From flash.johny at gmail.com  Fri Jul  7 08:24:58 2006
From: flash.johny at gmail.com (flash johny)
Date: Fri, 7 Jul 2006 11:54:58 +0530
Subject: [R] Query regarding modelling using R
Message-ID: <b66b37410607062324o5a7e195bt4fb99265eb5f742@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060707/22b55e1f/attachment.pl 

From bi-info at home.nl  Wed Jul  5 15:11:56 2006
From: bi-info at home.nl (Bi-Info (http://members.home.nl/bi-info))
Date: Wed, 05 Jul 2006 15:11:56 +0200
Subject: [R] Crosstabs
Message-ID: <44ABBA9C.7050108@home.nl>

Dear Users,

I'm a complete novice to R.

I need to do a crosstabs in R, but my data is almost completely 
alphanumeric (with some variables scaled). The Table routine does not 
seem to accept alphanumeric data. What should I do? Do I need to recode 
it? How should I do that?

Thanks in advance,

Wilfred


From ripley at stats.ox.ac.uk  Fri Jul  7 08:48:04 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 7 Jul 2006 07:48:04 +0100 (BST)
Subject: [R] RMySQL Suse 10.0 installing problems
In-Reply-To: <002c01c6a12c$a745a4b0$0202a8c0@hpb>
References: <002c01c6a12c$a745a4b0$0202a8c0@hpb>
Message-ID: <Pine.LNX.4.64.0607070743490.21367@gannet.stats.ox.ac.uk>

On Thu, 6 Jul 2006, H. Paul Benton wrote:

> Thanks again everyone,
> 	I have a problem trying to install RMySQL on Suse 10.0. I have used
> both the install.packages() and the 'R CMD INSTALL RMySQL..." neither
> worked. I get the error message telling me that it cannot find mysql. So I
> have set the flags using
>
> export PKG_CPPFLAGS="-I</usr/bin/>"
> export PKG_LIBS="-L</usr/lib/mysql>"

Those cannot be right: you don't want <> and -I/usr/bin is not a plausible 
include path.  RPM-based systems normally need

export PKG_CPPFLAGS=-I/usr/include/mysql
export PKG_LIBS=-I/usr/lib/mysql

and if you have /usr/lib/mysql, try

/usr/lib/mysql/mysql_config -cflags

to be sure about the first.

>
> After that I run R CMD INSTALL RMySQL... it starts to check and it cannot
> find mysql_init, mysql.h then it exits after gcc(ing) with an error code of
> make:*** [RS-DBI.o] Error 1
>
> I also tried the other way that the help suggested of putting the dir in the
> R CMD INSTALL command but that didn't work either giving me the same help
> menu.
>
> I am on opensuse 10.0 x86, and yes I su(ed).
>
> Cheers,
>
> Research Technician
> Mass Spectrometry
>  o The
> /
> o Scripps
> \
>  o Research
> /
> o Institute
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Jul  7 08:51:48 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 7 Jul 2006 07:51:48 +0100 (BST)
Subject: [R] engineering notation format
In-Reply-To: <1F5C95F1B887EF42B14F31AEF36AD22F05A0D03E@dlee04.ent.ti.com>
References: <1F5C95F1B887EF42B14F31AEF36AD22F05A0D03E@dlee04.ent.ti.com>
Message-ID: <Pine.LNX.4.64.0607070749300.21367@gannet.stats.ox.ac.uk>

Both format and formatC (and sprintf) use C facilities for scientific 
format.  As far as I know C has no facilities for your desired format, so 
you would need to write your own C code and interface to R.

On Thu, 6 Jul 2006, Walker, Sam wrote:

> Hi,
>
> How can I format numbers to engineering notation?
>
> That is, like scientific but where the exponent is always a multiple of
> three.
>
> Some examples:
>
> 1635 000 000      => 1.635E9
>  163 500 000     => 163.5E6
> 0.000 000 000 135 => 135E-9
>
> I've tried format and formatC but I couldn't get them to work they I
> want.
>
> Any help is greatly appreciated.
>
>
> Thanks,
> Sam
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Bernhard_Pfaff at fra.invesco.com  Fri Jul  7 09:26:17 2006
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Fri, 7 Jul 2006 08:26:17 +0100
Subject: [R] KPSS test
In-Reply-To: <20060706194846.69029.qmail@web37613.mail.mud.yahoo.com>
Message-ID: <E4A9111DA23BA048B9A46686BF727CF4039173@DEFRAXMB01.corp.amvescap.net>

Hello Sachin,

a sequential testing procedure is described in the useR! book:

  @Book{,
    title = {Analysis of Integrated and Cointegrated Time Series with R},
    author = {B. Pfaff},
    publisher = {Springer},
    edition = {First},
    address = {New York},
    year = {2006},
    note = {ISBN 0-387-27960-1},
  }

Best,
Bernhard


Dr. Bernhard Pfaff
Global Structured Products Group
(Europe)

Invesco Asset Management Deutschland GmbH
Bleichstrasse 60-62
D-60313 Frankfurt am Main

Tel: +49(0)69 29807 230
Fax: +49(0)69 29807 178
Email: bernhard_pfaff at fra.invesco.com  

>-----Urspr?ngliche Nachricht-----
>Von: r-help-bounces at stat.math.ethz.ch 
>[mailto:r-help-bounces at stat.math.ethz.ch] Im Auftrag von Sachin J
>Gesendet: Donnerstag, 6. Juli 2006 21:49
>An: markleeds at verizon.net
>Cc: r-help at stat.math.ethz.ch
>Betreff: Re: [R] KPSS test
>
>Hi Mark,
>   
>  Thanx for the help. I will verify my results with PP and DF 
>test. Also as suggested I will take a look at the references 
>pointed out. One small doubt: How do I decide what terms ( 
>trend, constant, seasonality ) to include while using these 
>stationarity tests. Any references would be of great help. 
>   
>  Thanx,
>  Sachin
>   
>  
>
>markleeds at verizon.net wrote:
>  >From: 
>>Date: Thu Jul 06 14:17:25 CDT 2006
>>To: Sachin J 
>>Subject: Re: [R] KPSS test
>
>sachin : i think your interpretations are right given the data
>but kpss is quite a different test than the usual tests
>because it assumes that the null is stationarity while dickey 
>fuller ( DF ) and phillips perron ( PP ) ) assume that the 
>null is a unit root. therefore, you should check whetheer
>the conclusions you get from kpss are consistent with what you 
>would get from DF or PP. the results often are not consistent.
>
>also, DF depends on what terms ( trend, constant ) 
>you used in your estimation of the model. i'm not sure if kpss 
>does also. people generally report Dickey fuller results but they
>are a little biased towards acepting unit root ( lower
>power ) so maybe that's why
>you are using KPSS ? Eric Zivot has a nice explanation
>of a lot of the of the stationarity tests in his S+Finmetrics 
>book.
>
>testing for cyclical variation is pretty complex because
>that's basically the same as testing for seasonality.
>check ord's or ender's book for relatively simple ways of doing that.
>
>
>
>
>
>
>
>
>
>
>
>
>>
>>>From: Sachin J 
>>>Date: Thu Jul 06 14:17:25 CDT 2006
>>>To: R-help at stat.math.ethz.ch
>>>Subject: [R] KPSS test
>>
>>>Hi,
>>> 
>>> Am I interpreting the results properly? Are my conclusions correct?
>>> 
>>> > KPSS.test(df)
>>> ---- ----
>>> KPSS test
>>> ---- ----
>>> Null hypotheses: Level stationarity and stationarity around 
>a linear trend.
>>> Alternative hypothesis: Unit root.
>>>----
>>> Statistic for the null hypothesis of 
>>> level stationarity: 1.089 
>>> Critical values:
>>> 0.10 0.05 0.025 0.01
>>> 0.347 0.463 0.574 0.739
>>>----
>>> Statistic for the null hypothesis of 
>>> trend stationarity: 0.13 
>>> Critical values:
>>> 0.10 0.05 0.025 0.01
>>> 0.119 0.146 0.176 0.216
>>>----
>>> Lag truncation parameter: 1 
>>> 
>>>CONCLUSION: Reject Ho at 0.05 sig level - Level Stationary
>>> Fail to reject Ho at 0.05 sig level - Trend Stationary 
>>> 
>>>> kpss.test(df,null = c("Trend"))
>>> KPSS Test for Trend Stationarity
>>> data: tsdata[, 6] 
>>>KPSS Trend = 0.1298, Truncation lag parameter = 1, p-value = 0.07999
>>> 
>>> CONCLUSION: Fail to reject Ho - Trend Stationary as p-value 
>< sig. level (0.05)
>>> 
>>>> kpss.test(df,null = c("Level"))
>>> KPSS Test for Level Stationarity
>>> data: tsdata[, 6] 
>>>KPSS Level = 1.0891, Truncation lag parameter = 1, p-value = 0.01
>>> Warning message:
>>>p-value smaller than printed p-value in: kpss.test(tsdata[, 
>6], null = c("Level")) 
>>> 
>>> CONCLUSION: Reject Ho - Level Stationary as p-value > sig. 
>level (0.05)
>>> 
>>> Following is my data set
>>> 
>>> structure(c(11.08, 7.08, 7.08, 6.08, 6.08, 6.08, 23.08, 32.08, 
>>>8.08, 11.08, 6.08, 13.08, 13.83, 16.83, 19.83, 8.83, 20.83, 17.83, 
>>>9.83, 20.83, 10.83, 12.83, 15.83, 11.83), .Tsp = c(2004, 
>2005.91666666667, 
>>>12), class = "ts")
>>>
>>> Also how do I test this time series for cyclical varitions? 
>>> 
>>> Thanks in advance.
>>> 
>>> Sachin
>>>
>>> 
>>>---------------------------------
>>>
>>> [[alternative HTML version deleted]]
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
>
>
>
> 		
>---------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
>
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}


From wuming.gong at gmail.com  Fri Jul  7 09:32:10 2006
From: wuming.gong at gmail.com (Wuming Gong)
Date: Fri, 7 Jul 2006 15:32:10 +0800
Subject: [R] Polynomial kernel in SVM in e1071 package
Message-ID: <b428d06d0607070032w1a968d79v4129e9aaa0e2b06b@mail.gmail.com>

Dear list,

In some places (for example,
http://en.wikipedia.org/wiki/Support_vector_machine) , the polynomail
kernel in SVM is written as (u'*v + 1)^d, while in the document of
svm() in e1071 package, the polynomial kernel is written as
(gamma*u'*v + coef0)^d.  I am a little confused here:

When doing parameter optimization (grid search or so) for polynomial
kernel, does it need to tune four parameters, gamma, coef0, C and
degree, or just two of them, C and degree (and fixing gamma to 1 and
coef0 = 1)?

Thanks,

Wuming


From maechler at stat.math.ethz.ch  Fri Jul  7 10:34:07 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 7 Jul 2006 10:34:07 +0200
Subject: [R] R crash with 'library(Matrix); as(x, "dgCMatrix")' ...
In-Reply-To: <0C6BF3FC506F664F90C8BA3E0160462D04A75873@EXCHANGE3.ad.uams.edu>
References: <17580.48271.592693.686921@stat.math.ethz.ch>
	<0C6BF3FC506F664F90C8BA3E0160462D04A75873@EXCHANGE3.ad.uams.edu>
Message-ID: <17582.7295.224479.859384@stat.math.ethz.ch>

>>>>> "JohnT" == Thaden, John J <ThadenJohnJ at uams.edu>
>>>>>     on Thu, 6 Jul 2006 12:29:42 -0500 writes:

    JohnT> Martin Maechler replied to my query "Warning while subsetting...":
    MartinM> >>>>> "JohnT" == Thaden, John J <ThadenJohnJ at uams.edu>
    MartinM> >>>>>     on Thu, 6 Jul 2006 00:02:10 -0500 writes:

    JohnT> ...
    JohnT> > # While subsetting x, I was surprised to get this warning: 
    JohnT> > y<-x[1:300,]
    JohnT> Warning message:
    JohnT> number of items to replace is not a multiple of
    JohnT> replacement length

    MartinM> and later

    JohnT> Sorry, I omitted background information:
    JohnT> R version: 2.3.0
    JohnT> OS: Windows XP
    JohnT> CPU:  Pentium III, 
    JohnT> RAM:  768 MB

    MartinM> You omitted the most pertinent information: The 
    MartinM> version of 'Matrix' you are using.
    MartinM> The latest released version of Matrix does
    MartinM> *not* show the behavior you mentioned. {So I have 
    MartinM> now spent 20 minutes just because you did not 
    MartinM> update 'Matrix'..}

    JohnT> The Matrix package was version 0.995-10, now is 0.995-11. 
    JohnT> The R base was version 2.3.0, now is 2.3.1. 
    JohnT> Subsetting 'y <- x[1:300,]' now works. Please accept my apology.

    JohnT> Also, what command-line memory settings might prevent
    JohnT> R from crashing while using the Matrix package to 
    JohnT> convert my 600 X 4482 dgTMatrix to the dgCMatrix class
    JohnT> or to an expanded Matrix, via the as() function? I can
    JohnT> do this with half of the matrix, 300 x 4482.

    MartinM> It's hard to believe that you get a "crash" 
    MartinM> when coercing to 'dgC' -- but of course this 
    MartinM> really depends how much memory you have already
    MartinM> goggled up by other large objects in your R
    MartinM> workspace, or by other applications running at
    MartinM> the same time in Windows.  Coercing to a full 
    MartinM> matrix will of course require 8 * 601 * 4482 = 
    MartinM> 21549456 extra bytes just for the numbers.
    MartinM> That's only 21.5 Megabytes, so I wonder..
    MartinM> 
    MartinM> I have never seen R crashes from using 'Matrix', 

 (actually that's not even true; at some point in time we had a
  bug in 'Matrix' which lead to spurious segmentation faults)

    MartinM> but then I work with an operating system, not 
    MartinM> with M$ Windows. 
    MartinM> 
    MartinM> Maybe you meant you got an error message 
    MartinM> "... memory allocation .."?

    JohnT> Testing again, I closed all applications; disabled antivirus; 
    JohnT> opened RGui; removed all R objects but 'x' (a 600x4482 dgTMatrix); 
    JohnT> opened WinXP's 'Task Manager'; saw only "Rgui" under 
    JohnT> 'Applications'; saw processes using a total of 287 MB of memory
    JohnT> under 'Processes'; closed 'Task Manager'; and typed R commands:

    >> # Steps leading to an R crash...
    >> ls()
    JohnT> [1] "x"
    >> str(x)
    JohnT> Formal class 'dgTMatrix' [package "Matrix"] with 6 slots
    JohnT> ..@ i       : int [1:923636] 1 2 3 4 5 6 7 8 9 10 ...
    JohnT> ..@ j       : int [1:923636] 1 1 1 1 1 1 1 1 1 1 ...
    JohnT> ..@ Dim     : int [1:2] 600 4482
    JohnT> ..@ Dimnames:List of 2
    JohnT> .. ..$ : chr [1:601] "50" "51" "52" "53" ...
    JohnT> .. ..$ : chr [1:4482] "1" "2" "3" "4" ...
    JohnT> ..@ x       : num [1:923636] 50.2 51.2 52.2 53.2 54.2 ...
    JohnT> ..@ factors : list()
    >> gc()
    JohnT> used (Mb) gc trigger (Mb) max used (Mb)
    JohnT> Ncells  183529  5.0     407500 10.9   350000  9.4
    JohnT> Vcells 1928101 14.8    2286173 17.5  1928652 14.8
    >> library(Matrix)
    JohnT> Loading required package: lattice
    >> gc()
    JohnT> used (Mb) gc trigger (Mb) max used (Mb)
    JohnT> Ncells  627772 16.8    1073225 28.7  1073225 28.7
    JohnT> Vcells 2165773 16.6    3345184 25.6  2332013 17.8
    >> search()
    JohnT> [1] ".GlobalEnv" "package:Matrix" "package:lattice"
    JohnT> [4] "package:methods" "package:stats" "package:graphics"  
    JohnT> [7] "package:grDevices" "package:utils" "package:datasets"
    JohnT> [10] "Autoloads"  "package:base"     
    >> #Now the line that causes crashes...
    >> y <- as(x,"dgCMatrix")

    JohnT> After ~10 seconds, R blinks off and a WinXP dialog appears: 

    JohnT> R for Windows GUI front-end has encountered 
    JohnT> a problem and needs to close.  We are sorry
    JohnT> for the inconvenience....Error signature:
    JohnT> AppName: rgui.exe  AppVer: 2.31.38247.0  
    JohnT> ModName: matrix.dll Offset: 0000ff31....
    JohnT> Report error?

Thanks a lot, John, for the more detailed report.
I do wonder how it happens, since the memory allocation is not
really big.   E.g., I can easily solve ``your'' (well, a
simulated version of it) problem on a machine with only 512 MB
RAM:

  library("Matrix")

  ## MM: construct a matrix *as* John's :
  d <- as.integer(c(600,4482))
  n0 <- 923636
  set.seed(1)
  M <- new("dgTMatrix", Dim = d,
	   i = sort(sample(0:(d[1]-1), size = n0, replace = TRUE)),
	   j = sample(0:(d[2]-1), size = n0, replace = TRUE),
	   x = round(rnorm(n0, m = 50, sd = 10), 1))
  dimnames(M) <- list(paste("r", 1:d[1], sep=''),
		      paste("C", 1:d[2], sep=''))
  str(M)

  M1.10 <- M[1:10,] # gave warning in earlier versions of 'Matrix'

  ## on 'nanny' which has just 512 MB  (with other processes active, etc):
  gc()
  ##           used (Mb) gc trigger (Mb) max used (Mb)
  ## Ncells  642690 17.2    1073225 28.7  1073225 28.7
  ## Vcells 3136547 24.0    8305047 63.4  7988501 61.0

  mC <- as(M, "dgCMatrix")
  ##           ---------
  gc()
  ##           used (Mb) gc trigger (Mb) max used (Mb)
  ## Ncells  642721 17.2    1073225 28.7  1073225 28.7
  ## Vcells 4311327 32.9    8305047 63.4  7988501 61.0

  ## well, this will need a bit more memory, but should still work:
  mm <- as(M, "matrix")
  ##           -------
  gc()
  ##-           used (Mb) gc trigger (Mb) max used (Mb)
  ##- Ncells  642725 17.2    1073225 28.7  1073225 28.7
  ##- Vcells 7000528 53.5    8438708 64.4  7988501 61.0


I see in the CHANGES file for {R for Windows}

>> R 2.3.1 patched
>> ===============
>> 
>>  [.........................]
>> 
>> R could crash when very low on memory. (PR#8981)

So, maybe you can try to even run "R 2.3.1 patched" for Windows,
which you can get from here,
      http://cran.us.r-project.org/bin/windows/base/rpatched.html
and see if your crashes go away ?

Regards,
Martin


From epistat at gmail.com  Fri Jul  7 11:14:59 2006
From: epistat at gmail.com (zhijie zhang)
Date: Fri, 7 Jul 2006 17:14:59 +0800
Subject: [R] how to name a variable?
Message-ID: <2fc17e30607070214i708af939u689a09f6ab5947e2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060707/e0cb9a7f/attachment.pl 

From bibiko at eva.mpg.de  Fri Jul  7 11:27:07 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Fri, 7 Jul 2006 11:27:07 +0200
Subject: [R] engineering notation format
In-Reply-To: <Pine.LNX.4.64.0607070749300.21367@gannet.stats.ox.ac.uk>
References: <1F5C95F1B887EF42B14F31AEF36AD22F05A0D03E@dlee04.ent.ti.com>
	<Pine.LNX.4.64.0607070749300.21367@gannet.stats.ox.ac.uk>
Message-ID: <ED9F2979-F6C7-4987-8D6D-3C7D88DEAADD@eva.mpg.de>

Hi,

try this:

formatEng <- function(x) {
   s<-as.numeric(strsplit(format(x, scientific=T),"e")[[1]])
   return(paste(s[1]*10^(s[2]%%3),as.integer(s[2]-(s[2]%%3)),sep="e"))
}

>>
>> Some examples:
>>
>> 1635 000 000      => 1.635E9
>>  163 500 000     => 163.5E6
>> 0.000 000 000 135 != 135E-9
>> 0.000 000 000 135 => 125E-12 ?


Hans

**********************************************************
Hans-Joerg Bibiko
Max Planck Institute for Evolutionary Anthropology
Department of Linguistics
Deutscher Platz 6     phone:   +49 (0) 341 3550 341
D-04103 Leipzig       fax:     +49 (0) 341 3550 333
Germany               e-mail:  bibiko at eva.mpg.de


From ronggui.huang at gmail.com  Fri Jul  7 11:32:49 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Fri, 7 Jul 2006 17:32:49 +0800
Subject: [R] how to name a variable?
In-Reply-To: <2fc17e30607070214i708af939u689a09f6ab5947e2@mail.gmail.com>
References: <2fc17e30607070214i708af939u689a09f6ab5947e2@mail.gmail.com>
Message-ID: <38b9f0350607070232o5ac49811ra28ef1e2c88b4db0@mail.gmail.com>

> s<-data.frame(var.name=seq(1,6,by=2))
> s
  var.name
1        1
2        3
3        5


2006/7/7, zhijie zhang <epistat at gmail.com>:
> Dear friends,
>  The "s" in the following argument don't have a variable name, how should i
> give it a name?
>  > s<-data.frame(seq(1,6,by=2))
> > s
>   seq.1..6..by...2.
> 1                 1
> 2                 3
> 3                 5
>
> thanks very much!
>
> --
> Kind Regards,
> Zhi Jie,Zhang ,PHD
> Department of Epidemiology
> School of Public Health
> Fudan University
> Tel:86-21-54237149
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
??????
Department of Sociology
Fudan University


From spencer.graves at pdf.com  Fri Jul  7 11:49:05 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 07 Jul 2006 02:49:05 -0700
Subject: [R] Start Model for POLYCLASS
In-Reply-To: <s4a65377.050@mail.ucf.edu>
References: <s4a65377.050@mail.ucf.edu>
Message-ID: <44AE2E11.7020003@pdf.com>

	  I have not seen a reply, so I will offer a few suggestions, even 
though I've never used the 'polspline' package.  I scanned several of 
the help pages and looked in "~\library\polspline' where R is installed 
on my hard drive and found no further documentation, and I found nothing 
new from RSiteSearch("polyclass").  Googling for 'polyclass' led me to 
"http://bear.fhcrc.org/~clk/soft.html", the home page for Charles 
Kooperberg, the author and maintainer.  If it were my problem, I might 
try writing him directly at the email address given in 
help(package="polyclass");  I'm including him as a "cc" on this reply.

	  I spent time looking at this, because 'polyclass', 'polymars' and 
'polspline' seem potentially related related to one of my secondary 
interests.  Unfortunately, I couldn't find sufficient documentation to 
allow me to proceed with the time I felt I could afford to invest in 
this right now.

	  If it were my problem, before I wrote another email about this, I'd 
first list the function 'polyclass', make a local copy, then work 
through an example line by line using 'debug(polyclass)'.  This 'debug' 
facility is remarkably powerful and easy to use.  I've solved many 
problems like this using 'debug' in this way.

	  If that failed to provide the necessary enlightenment, I'd submit 
another post including a self-contained example based on a modification 
of the 'iris' example featured in the 'polyclass' help page.  Your 
example is NOT self contained, so I would NOT use that.  Using the 
'iris' example would make it much easier to explain clearly what you 
want.  It also makes it much easier for someone like me to experiment 
with alternatives and describe what I did in terms of a tested example.

	  Hope this helps.
p.s.  I suggest you also review the posting guide! 
"www.R-project.org/posting-guide.html".

Xiaogang Su wrote:
> Dear all, 
> 
> I have a question on how to set up the starting model in POLYCLASS and
> make sure the terms in the starting model retained in the final
> POLYCLASS model. 
> 
> In the function POLYMARS, this can be done using the STARTMODEL option.
> See below for example, I started with model 
> y= b0 + b1*X1 + b2*X2 + b3*X4 + b4*X5 + b5*X2*X5 + e
> 
>> m00 <- matrix(c(
>      1,  NA, 0, NA, 1,     
>      2,  NA, 0, NA, 1,     
>      4,  NA, 0, NA, 1,     
>      5,  NA, 0, NA, 1,
>      2,  NA, 5, NA, 1),nrow = 5, ncol=5, byrow=TRUE);
> 
>> m2 <- polymars(response=PID2$y, predictors=PID2[,1:7], 
> startmodel=m00) 
>> summary(m2)  
> 
> But I could not figure out how this works for POLYCLASS. There is an
> option FIT in POLYCLASS, which needs to be a POLYCLASS object though. 
> 
> Any suggestion or information is greatly appreciated. 
> 
> Sincerely,
> Xiaogang Su
> 
> 
> ================================
> Xiaogang Su,  Assistant Professor
> Department of Statistics and Actuarial Science
> University of Central Florida
> Orlando, FL 32816
> (407) 823-2940 [O]
> xiaosu at mail.ucf.edu
> http://pegasus.cc.ucf.edu/~xsu/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From dieter.menne at menne-biomed.de  Fri Jul  7 12:01:21 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 7 Jul 2006 10:01:21 +0000 (UTC)
Subject: [R] Query regarding modelling using R
References: <b66b37410607062324o5a7e195bt4fb99265eb5f742@mail.gmail.com>
Message-ID: <loom.20060707T120006-902@post.gmane.org>

flash johny <flash.johny <at> gmail.com> writes:

> 
> Hi,
> I am working on project to build a model for analysing Mortgage-backed
> securities.
> I am doing this as a part of my summer project.

Try

http://cran.r-project.org/src/contrib/Views/

Dieter


From ligges at statistik.uni-dortmund.de  Fri Jul  7 12:15:10 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 07 Jul 2006 12:15:10 +0200
Subject: [R] Crosstabs
In-Reply-To: <44ABBA9C.7050108@home.nl>
References: <44ABBA9C.7050108@home.nl>
Message-ID: <44AE342E.7010106@statistik.uni-dortmund.de>

Bi-Info (http://members.home.nl/bi-info) wrote:
> Dear Users,
> 
> I'm a complete novice to R.
> 
> I need to do a crosstabs in R, but my data is almost completely 
> alphanumeric (with some variables scaled). The Table routine does not 
> seem to accept alphanumeric data. What should I do? Do I need to recode 
> it? How should I do that?


Works for me:

  table(c("a", "a", "b"), c("a", "c", "c"))

Uwe Ligges


> 
> Thanks in advance,
> 
> Wilfred
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ligges at statistik.uni-dortmund.de  Fri Jul  7 13:00:53 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 07 Jul 2006 13:00:53 +0200
Subject: [R] Polynomial kernel in SVM in e1071 package
In-Reply-To: <b428d06d0607070032w1a968d79v4129e9aaa0e2b06b@mail.gmail.com>
References: <b428d06d0607070032w1a968d79v4129e9aaa0e2b06b@mail.gmail.com>
Message-ID: <44AE3EE5.2070505@statistik.uni-dortmund.de>

Wuming Gong wrote:
> Dear list,
> 
> In some places (for example,
> http://en.wikipedia.org/wiki/Support_vector_machine) , the polynomail
> kernel in SVM is written as (u'*v + 1)^d, while in the document of
> svm() in e1071 package, the polynomial kernel is written as
> (gamma*u'*v + coef0)^d.  I am a little confused here:
> 
> When doing parameter optimization (grid search or so) for polynomial
> kernel, does it need to tune four parameters, gamma, coef0, C and
> degree, or just two of them, C and degree (and fixing gamma to 1 and
> coef0 = 1)?

All of them, unless you know reasonable values, but that's very improbable.

Uwe Ligges

> 
> Thanks,
> 
> Wuming
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From celso.barros at gmail.com  Fri Jul  7 13:13:10 2006
From: celso.barros at gmail.com (Celso Barros)
Date: Fri, 7 Jul 2006 08:13:10 -0300
Subject: [R] Diverging results with SPSS
Message-ID: <f6b7dfdc0607070413n60b84a53t560e30768f0804c0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060707/8d08ba86/attachment.pl 

From p.dalgaard at biostat.ku.dk  Fri Jul  7 13:28:57 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Jul 2006 13:28:57 +0200
Subject: [R] Diverging results with SPSS
In-Reply-To: <f6b7dfdc0607070413n60b84a53t560e30768f0804c0@mail.gmail.com>
References: <f6b7dfdc0607070413n60b84a53t560e30768f0804c0@mail.gmail.com>
Message-ID: <x2ejwxu006.fsf@viggo.kubism.ku.dk>

"Celso Barros" <celso.barros at gmail.com> writes:

> Dear List,
> 
> I apologize in advance if this is silly. I tried to replicate an analysis I
> did previously in SPSS using R, and was surprised to find different results.
> 
> 
> So my question is: shouldn't the following SPSS syntax
> 
> 
> REGRESSION
> 
> DEPENDENT INC89
> 
> /METHOD=ENTER hiedyrs experien SE93rec.
> 
> Yeld the same results of the following R command
> 
> modelB<-lm(INC89~HIEDYRS+EXPERIEN+SE93REC)

Possibly, if all variables are quantitative.

What are the differences that you see?
 
> 
> I assume the is some difference in some default options.Or maybe it was a
> problem when the data was imported. After using the read.spss in the foreign
> package, I got the following warning message:
> 
> Unrecognized record type 7, subtype 16 encountered in system file

You might check whether your variables have the same summary
statistics in both systems.
 
> Thanks in advance for the help and apologies for the trouble,
> 
> 
> Celso
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From fkoller at isr.umich.edu  Fri Jul  7 13:47:08 2006
From: fkoller at isr.umich.edu (Florian Koller)
Date: Fri, 7 Jul 2006 07:47:08 -0400
Subject: [R] Diverging results with SPSS
References: <f6b7dfdc0607070413n60b84a53t560e30768f0804c0@mail.gmail.com>
Message-ID: <942E05ED295BE1489D46F6301DACE3A30175AC54@isr-mail1.ad.isr.umich.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060707/e09ee8ff/attachment.pl 

From francoisromain at free.fr  Fri Jul  7 14:52:40 2006
From: francoisromain at free.fr (Romain Francois)
Date: Fri, 07 Jul 2006 14:52:40 +0200
Subject: [R] R-sig-foo searching,
 was : High breakdown/efficiency statistics -- was RE: Rosner's	test
In-Reply-To: <17563.43410.601845.498856@stat.math.ethz.ch>
References: <17562.23802.793188.600782@stat.math.ethz.ch>	<001701c6961b$2452fe20$0e7e11ac@gne.windows.gene.com>
	<17563.43410.601845.498856@stat.math.ethz.ch>
Message-ID: <44AE5918.9090103@free.fr>

Le 23.06.2006 10:42, Martin Maechler a ?crit :
> {BTW: Did you know that to *search* mailing list archives of
>       such R-SIG-foo mailing lists, you can use google very
>       efficiently by prepending the mailing list name and 'site:stat.ethz.ch'?
>       e.g., use google search on
>       "R-SIG-robust site:stat.ethz.ch lmrob"
> }
>   
Great idea, with a little bonus for the gnome mini-commander lovers (the 
other folks can just ignore this), a new macro :

expression :  |^rs-(.*):(.*)$|
command : gnome-open "http://www.google.fr/search?q=R-SIG-\1 \2 
site:stat.ethz.ch"

Now you can type
  rs-robust:lmrob
on your gnome mini commander applet

Cheers,

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
mixmod 1.7 is released : http://www-math.univ-fcomte.fr/mixmod/index.php
+---------------------------------------------------------------+
| Romain FRANCOIS - http://francoisromain.free.fr               |
| Doctorant INRIA Futurs / EDF                                  |
+---------------------------------------------------------------+


From close2ceo at yahoo.com  Fri Jul  7 15:18:17 2006
From: close2ceo at yahoo.com (Xiaodong Jin)
Date: Fri, 7 Jul 2006 06:18:17 -0700 (PDT)
Subject: [R] convert ms() to optim()
Message-ID: <20060707131817.83116.qmail@web31206.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060707/af12073a/attachment.pl 

From xenamaa at yahoo.com  Fri Jul  7 16:06:43 2006
From: xenamaa at yahoo.com (zana adeb)
Date: Fri, 7 Jul 2006 07:06:43 -0700 (PDT)
Subject: [R]  Calling R from Java
Message-ID: <20060707140643.36239.qmail@web53113.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060707/6d87c512/attachment.pl 

From s-walker at ti.com  Fri Jul  7 16:17:00 2006
From: s-walker at ti.com (Walker, Sam)
Date: Fri, 7 Jul 2006 09:17:00 -0500
Subject: [R] engineering notation format
Message-ID: <1F5C95F1B887EF42B14F31AEF36AD22F05A0D39B@dlee04.ent.ti.com>

Clever...

Good observation on the last example, mistake on my part.

Thanks for all the suggestions.

Sam

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hans-Joerg Bibiko
Sent: Friday, July 07, 2006 4:27 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] engineering notation format

Hi,

try this:

formatEng <- function(x) {
   s<-as.numeric(strsplit(format(x, scientific=T),"e")[[1]])
   return(paste(s[1]*10^(s[2]%%3),as.integer(s[2]-(s[2]%%3)),sep="e"))
}

>>
>> Some examples:
>>
>> 1635 000 000      => 1.635E9
>>  163 500 000     => 163.5E6
>> 0.000 000 000 135 != 135E-9
>> 0.000 000 000 135 => 125E-12 ? 

Hans

**********************************************************
Hans-Joerg Bibiko
Max Planck Institute for Evolutionary Anthropology
Department of Linguistics
Deutscher Platz 6     phone:   +49 (0) 341 3550 341
D-04103 Leipzig       fax:     +49 (0) 341 3550 333
Germany               e-mail:  bibiko at eva.mpg.de

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From vsdimitrov at yahoo.com  Fri Jul  7 17:00:35 2006
From: vsdimitrov at yahoo.com (Valentin Dimitrov)
Date: Fri, 7 Jul 2006 08:00:35 -0700 (PDT)
Subject: [R] parametric proportional hazard regression
Message-ID: <20060707150035.79550.qmail@web30812.mail.mud.yahoo.com>

Dear all,

I am trying to find a suitable R-function for
parametric proportional hazard regressions. The
package survival contains the coxph() function which
performs a Cox regression which leaves the base hazard
unspecified, i.e. it is a semi-parametric method. The
package Design contains the function pphsm() which is
good for parametric proportional hazard regressions
when the underlying base distribution is "weibull" or
"exponential". But what if I need a parametric
proportional hazard model with the other "usual"
distributions like '"gaussian"', '"logistic"',
'"lognormal"'and '"loglogistic"?

Thanks a lot for your support!

Valentin Dimitrov
Statistics and Econometrics
University of Saarland


From wade.wall at gmail.com  Fri Jul  7 17:20:21 2006
From: wade.wall at gmail.com (Wade Wall)
Date: Fri, 7 Jul 2006 11:20:21 -0400
Subject: [R] replace values in data frame
Message-ID: <e23082be0607070820l595a0db7g896a71182ccbe4a6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060707/5762de83/attachment.pl 

From spluque at gmail.com  Fri Jul  7 17:33:17 2006
From: spluque at gmail.com (Sebastian Luque)
Date: Fri, 07 Jul 2006 10:33:17 -0500
Subject: [R] dotplot (lattice) with panel.segments and groups
Message-ID: <8764i9whtu.fsf@arctocephalus.homelinux.org>

Hi,

The following produces almost exactly what I needed.  The problems are
that the 'panel.dotplot' call (commented) generates the error 'Error in
NextMethod("[") : argument "subscripts" is missing, with no default'.  The
other problem is that the colors alternate between the levels of the 'site'
variable, rather than 'year'.


barley$yield2 <- with(barley, yield + 5)

dotplot(site ~ yield | variety, data=barley, groups=year,
        yield2=barley$yield2, col=c("gray", "black"),
        panel=function(x, y, subscripts, yield2, ...) {
            ## panel.dotplot(x, y, ...)
            panel.segments(x, as.numeric(y),
                           yield2[subscripts], as.numeric(y), ...)
        })

R> sessionInfo()
Version 2.3.1 (2006-06-01) 
i486-pc-linux-gnu 

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[7] "base"     

other attached packages:
   chron  gmodels  lattice 
 "2.3-3" "2.12.0" "0.13-8"


Can somebody please suggest how to properly make that 'panel.dotplot' call
and set the 'col' argument?  Thanks in advance.


Cheers,

-- 
Seb


From mschwartz at mn.rr.com  Fri Jul  7 17:40:28 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 07 Jul 2006 10:40:28 -0500
Subject: [R] replace values in data frame
In-Reply-To: <e23082be0607070820l595a0db7g896a71182ccbe4a6@mail.gmail.com>
References: <e23082be0607070820l595a0db7g896a71182ccbe4a6@mail.gmail.com>
Message-ID: <1152286828.4667.4.camel@localhost.localdomain>

On Fri, 2006-07-07 at 11:20 -0400, Wade Wall wrote:
> Hi all,
> 
> I have a three columned list that I have imported into R.  The first column
> is a plot (ex. Plot1), the second is a species name (ex ACERRUB) and the
> third a numeric value.  I want to replace some of the second column names
> with other names (for example replace ACERRUB with ACERDRU).  The original
> and replacement values are in separate lists (not vectors), but I can't seem
> to find the right function to perform this.  The replace function seems to
> only want to work with numbers.
> 
> Any clues?
> 
> Wade

Without seeing the code you are using, we can only guess a syntax error
of some sort.  It works fine using the iris dataset:

> head(iris)
   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1           5.1         3.5          1.4         0.2  setosa
2           4.9         3.0          1.4         0.2  setosa
3           4.7         3.2          1.3         0.2  setosa
4           4.6         3.1          1.5         0.2  setosa
5           5.0         3.6          1.4         0.2  setosa
6           5.4         3.9          1.7         0.4  setosa
7           4.6         3.4          1.4         0.3  setosa
8           5.0         3.4          1.5         0.2  setosa
9           4.4         2.9          1.4         0.2  setosa
10          4.9         3.1          1.5         0.1  setosa

> iris$Species <- replace(iris$Species, iris$Species == "setosa",
                          "NewValue")

> head(iris)
   Sepal.Length Sepal.Width Petal.Length Petal.Width  Species
1           5.1         3.5          1.4         0.2 NewValue
2           4.9         3.0          1.4         0.2 NewValue
3           4.7         3.2          1.3         0.2 NewValue
4           4.6         3.1          1.5         0.2 NewValue
5           5.0         3.6          1.4         0.2 NewValue
6           5.4         3.9          1.7         0.4 NewValue
7           4.6         3.4          1.4         0.3 NewValue
8           5.0         3.4          1.5         0.2 NewValue
9           4.4         2.9          1.4         0.2 NewValue
10          4.9         3.1          1.5         0.1 NewValue


HTH,

Marc Schwartz


From andy_liaw at merck.com  Fri Jul  7 17:45:39 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 7 Jul 2006 11:45:39 -0400
Subject: [R] graphlets
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02884A71@usctmx1106.merck.com>

Naras presented something with similar functionalities using SVG a couple of
years ago, and since then the svgdevice package had been made available.  I
do not know SVG myself, but according to Naras, it's all possible.

Best,
Andy 

From: Terry Therneau
> 
>   I have an application where the Splus graphlets() package 
> would work very well, and would like to know if there is any 
> comparable work in R.  In response to a similar query on this 
> list about 1.5 years ago the Orca package was mentioned, but 
> it seems to be inactive.
>   For those who don't know, graphlets are an extended java 
> graphics device.  The identify function, for instance, allows 
> one to link both a label and a set of html links to each 
> point.  I am aware of the Sjava package in Omega, and am 
> exploring that, but would be very interested if there is 
> anything more.
> 	Terry Therneau
> 	Mayo Clinic
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From tlumley at u.washington.edu  Fri Jul  7 17:47:59 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 7 Jul 2006 08:47:59 -0700 (PDT)
Subject: [R] parametric proportional hazard regression
In-Reply-To: <20060707150035.79550.qmail@web30812.mail.mud.yahoo.com>
References: <20060707150035.79550.qmail@web30812.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0607070846180.9075@homer23.u.washington.edu>

On Fri, 7 Jul 2006, Valentin Dimitrov wrote:
> I am trying to find a suitable R-function for
> parametric proportional hazard regressions. The
> package survival contains the coxph() function which
> performs a Cox regression which leaves the base hazard
> unspecified, i.e. it is a semi-parametric method. The
> package Design contains the function pphsm() which is
> good for parametric proportional hazard regressions
> when the underlying base distribution is "weibull" or
> "exponential". But what if I need a parametric
> proportional hazard model with the other "usual"
> distributions like '"gaussian"', '"logistic"',
> '"lognormal"'and '"loglogistic"?

Those are not proportional hazards families of distributions. That is, if 
the distribution is gaussian for one value of the hazard ratio parameters 
it will not be gaussian for any other value.

You can get accelerated failure models with these distributions using 
survreg() in the survival package.


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From LuJ at edc.pitt.edu  Fri Jul  7 17:47:44 2006
From: LuJ at edc.pitt.edu (Lu, Jiang Jane)
Date: Fri, 7 Jul 2006 11:47:44 -0400
Subject: [R] How to change the type of segments ends?
Message-ID: <6E031567AB9AD24FA98F173673F0708F743537@boa.edc3.pitt.edu>

Hi,

I am trying to plot odds ratios and the corresponding confidence
intervals in horizontal segments. It would be ideal if the confidence
interval segment can be drawn with little vertical bars at both ends. I
have tried very hard to change the type of ends by using 'lend'
arguments, but cannot make it. I even tried 'arrows()', but still
failed. Following is the code I use:
====================================================
drug.or <- c(1.017,1.437,1.427,2.211)
drug.orl <- c(0.715,1.075,1.103,1.696)
drug.oru <- c(1.446,1.922,1.845,2.882)

yaxis <- seq(1,4,by=1)

plot(x=drug.or,y=yaxis,type='p',pch=17,xlim=c(0,3),axes=FALSE,
     xlab='Odds Ratio',ylab='',main='Reference Group: A only')
axis(1,at=seq(0,3,by=0.5),labels=paste(seq(0,3,by=0.5)))
axis(2,at=yaxis,las=2)


segments(x0=drug.orl,x1=drug.oru,y0=yaxis,y1=yaxis,col=4,lend=2)

# or try
#arrows(x0=drug.orl,x1=drug.oru,y0=yaxis,y1=yaxis,length=0.1,angle=0,cod
e=3,col=4,lend=2)

box()
=====================================================


Any comments or suggestions would be greatly appreciated.


Jane

University of Pittsburgh
Pittsburgh, PA 15261


From celso.barros at gmail.com  Fri Jul  7 17:52:01 2006
From: celso.barros at gmail.com (Celso Barros)
Date: Fri, 7 Jul 2006 12:52:01 -0300
Subject: [R] Diverging results with SPSS
In-Reply-To: <942E05ED295BE1489D46F6301DACE3A30175AC54@isr-mail1.ad.isr.umich.edu>
References: <f6b7dfdc0607070413n60b84a53t560e30768f0804c0@mail.gmail.com>
	<942E05ED295BE1489D46F6301DACE3A30175AC54@isr-mail1.ad.isr.umich.edu>
Message-ID: <f6b7dfdc0607070852p2bde9002m949cbfc25464669d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060707/dc207379/attachment.pl 

From CodyH at BaylorHealth.edu  Fri Jul  7 17:53:06 2006
From: CodyH at BaylorHealth.edu (Hamilton, Cody)
Date: Fri, 7 Jul 2006 10:53:06 -0500
Subject: [R] parametric proportional hazard regression
In-Reply-To: <20060707150035.79550.qmail@web30812.mail.mud.yahoo.com>
Message-ID: <E52E20F6B4A2F548B16BB259DD5168CF02FB681B@BHDAEXCH11.bhcs.pvt>


Valentin,

Have you tried survreg() in the Design library?

Regards,
   -Cody

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Valentin Dimitrov
Sent: Friday, July 07, 2006 10:1 AM
To: r-help at stat.math.ethz.ch
Subject: [R] parametric proportional hazard regression

Dear all,

I am trying to find a suitable R-function for
parametric proportional hazard regressions. The
package survival contains the coxph() function which
performs a Cox regression which leaves the base hazard
unspecified, i.e. it is a semi-parametric method. The
package Design contains the function pphsm() which is
good for parametric proportional hazard regressions
when the underlying base distribution is "weibull" or
"exponential". But what if I need a parametric
proportional hazard model with the other "usual"
distributions like '"gaussian"', '"logistic"',
'"lognormal"'and '"loglogistic"?

Thanks a lot for your support!

Valentin Dimitrov
Statistics and Econometrics
University of Saarland

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

This e-mail, facsimile, or letter and any files or attachments transmitted with it contains information that is confidential and privileged. This information is intended only for the use of the individual(s) and entity(ies) to whom it is addressed. If you are the intended recipient, further disclosures are prohibited without proper authorization. If you are not the intended recipient, any disclosure, copying, printing, or use of this information is strictly prohibited and possibly a violation of federal or state law and regulations. If you have received this information in error, please notify Baylor Health Care System immediately at 1-866-402-1661 or via e-mail at privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries, and affiliates hereby claim all applicable privileges related to this information.


From ahnmj at MIT.EDU  Fri Jul  7 17:53:30 2006
From: ahnmj at MIT.EDU (Minjeong Ahn)
Date: Fri, 07 Jul 2006 11:53:30 -0400
Subject: [R] Rpad Server Installation on Windows XP
Message-ID: <20060707115330.whtu3jo4kucko0gk@webmail.mit.edu>


 Hello,
 I have a question about Rpad Server Installation on Windows XP.
 Here is a guide
 http://www.rpad.org/Rpad/ServerNotes.html
 and I have been reading this more than ten times to figure this out, but
 still having trouble executing a Rpad website properly.

 Problem: for example, if I go to http://loclhost/Rpad/example1.rpad,
 I can see the whole website with the page loading sign staying on my
 screen (like forever)
 but if I push "calculate" button noting happens and the "page loading"
 sign goes away.

 Here, I think I have completed all steps needed,
 but I think I got something wrong with "Install the Statistice-
 R_perl_interface that come with Rpad in the serverversion directory." part.

 My Question: When I try installing serverversion files, I used
 -perl makefile.pl
 -nmake
 -nmake install

 in C:/Perl/bin directory. But I think the directory of perl might be
 some problem. I asked somebody out a week ago and he said I have to
 install perl on my path. Actually I don't quite know which directory
 he is talking about. I'm using xampp advanced version, so I should have
 perl installed and also I have active perl installed on my computer
 (c:/perl) What am I getting wrong?

 Question2: Do I have to install rterm too? If so, where?

 Any of your advice will be very appreciated.
 Thank you for your reading and have a great day everybody!

 best,
 Claire Ahn
 MIT


From mschwartz at mn.rr.com  Fri Jul  7 18:08:49 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 07 Jul 2006 11:08:49 -0500
Subject: [R] How to change the type of segments ends?
In-Reply-To: <6E031567AB9AD24FA98F173673F0708F743537@boa.edc3.pitt.edu>
References: <6E031567AB9AD24FA98F173673F0708F743537@boa.edc3.pitt.edu>
Message-ID: <1152288529.4667.9.camel@localhost.localdomain>

On Fri, 2006-07-07 at 11:47 -0400, Lu, Jiang Jane wrote:
> Hi,
> 
> I am trying to plot odds ratios and the corresponding confidence
> intervals in horizontal segments. It would be ideal if the confidence
> interval segment can be drawn with little vertical bars at both ends. I
> have tried very hard to change the type of ends by using 'lend'
> arguments, but cannot make it. I even tried 'arrows()', but still
> failed. Following is the code I use:
> ====================================================
> drug.or <- c(1.017,1.437,1.427,2.211)
> drug.orl <- c(0.715,1.075,1.103,1.696)
> drug.oru <- c(1.446,1.922,1.845,2.882)
> 
> yaxis <- seq(1,4,by=1)
> 
> plot(x=drug.or,y=yaxis,type='p',pch=17,xlim=c(0,3),axes=FALSE,
>      xlab='Odds Ratio',ylab='',main='Reference Group: A only')
> axis(1,at=seq(0,3,by=0.5),labels=paste(seq(0,3,by=0.5)))
> axis(2,at=yaxis,las=2)
> 
> 
> segments(x0=drug.orl,x1=drug.oru,y0=yaxis,y1=yaxis,col=4,lend=2)
> 
> # or try
> #arrows(x0=drug.orl,x1=drug.oru,y0=yaxis,y1=yaxis,length=0.1,angle=0,cod
> e=3,col=4,lend=2)
> 
> box()
> =====================================================

Try this using arrows():

drug.or <- c(1.017,1.437,1.427,2.211)
drug.orl <- c(0.715,1.075,1.103,1.696)
drug.oru <- c(1.446,1.922,1.845,2.882)

yaxis <- seq(1, 4, by=1)

plot(drug.or, yaxis ,type='p', pch=17, xlim=c(0,3), axes=FALSE,
     xlab='Odds Ratio', ylab='', main='Reference Group: A only')

axis(1, at=seq(0,3,by=0.5), labels=paste(seq(0,3,by=0.5)))
axis(2, at=yaxis, las=2)

arrows(drug.orl, yaxis, drug.oru, yaxis, angle=90,
       length=0.1, code=3, col=4, lend=2)


You need to specify an angle of 90 degrees to get the arrow heads to be
perpendicular to the primary line segment. An angle of 0 superimposes
the heads on the primary line segment, so they don't show.

HTH,

Marc Schwartz


From ripley at stats.ox.ac.uk  Fri Jul  7 18:10:45 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 7 Jul 2006 17:10:45 +0100 (BST)
Subject: [R] How to change the type of segments ends?
In-Reply-To: <6E031567AB9AD24FA98F173673F0708F743537@boa.edc3.pitt.edu>
References: <6E031567AB9AD24FA98F173673F0708F743537@boa.edc3.pitt.edu>
Message-ID: <Pine.LNX.4.64.0607071710120.12978@gannet.stats.ox.ac.uk>

You need angle=90. From the help page:

    angle: angle from the shaft of the arrow to the edge of the arrow
           head.


On Fri, 7 Jul 2006, Lu, Jiang Jane wrote:

> Hi,
>
> I am trying to plot odds ratios and the corresponding confidence
> intervals in horizontal segments. It would be ideal if the confidence
> interval segment can be drawn with little vertical bars at both ends. I
> have tried very hard to change the type of ends by using 'lend'
> arguments, but cannot make it. I even tried 'arrows()', but still
> failed. Following is the code I use:
> ====================================================
> drug.or <- c(1.017,1.437,1.427,2.211)
> drug.orl <- c(0.715,1.075,1.103,1.696)
> drug.oru <- c(1.446,1.922,1.845,2.882)
>
> yaxis <- seq(1,4,by=1)
>
> plot(x=drug.or,y=yaxis,type='p',pch=17,xlim=c(0,3),axes=FALSE,
>     xlab='Odds Ratio',ylab='',main='Reference Group: A only')
> axis(1,at=seq(0,3,by=0.5),labels=paste(seq(0,3,by=0.5)))
> axis(2,at=yaxis,las=2)
>
>
> segments(x0=drug.orl,x1=drug.oru,y0=yaxis,y1=yaxis,col=4,lend=2)
>
> # or try
> #arrows(x0=drug.orl,x1=drug.oru,y0=yaxis,y1=yaxis,length=0.1,angle=0,cod
> e=3,col=4,lend=2)
>
> box()
> =====================================================
>
>
> Any comments or suggestions would be greatly appreciated.
>
>
> Jane
>
> University of Pittsburgh
> Pittsburgh, PA 15261
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ccleland at optonline.net  Fri Jul  7 18:12:26 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 07 Jul 2006 12:12:26 -0400
Subject: [R] How to change the type of segments ends?
In-Reply-To: <6E031567AB9AD24FA98F173673F0708F743537@boa.edc3.pitt.edu>
References: <6E031567AB9AD24FA98F173673F0708F743537@boa.edc3.pitt.edu>
Message-ID: <44AE87EA.8010106@optonline.net>

Lu, Jiang Jane wrote:
> Hi,
> 
> I am trying to plot odds ratios and the corresponding confidence
> intervals in horizontal segments. It would be ideal if the confidence
> interval segment can be drawn with little vertical bars at both ends. I
> have tried very hard to change the type of ends by using 'lend'
> arguments, but cannot make it. I even tried 'arrows()', but still
> failed. Following is the code I use:
> ====================================================
> drug.or <- c(1.017,1.437,1.427,2.211)
> drug.orl <- c(0.715,1.075,1.103,1.696)
> drug.oru <- c(1.446,1.922,1.845,2.882)
> 
> yaxis <- seq(1,4,by=1)
> 
> plot(x=drug.or,y=yaxis,type='p',pch=17,xlim=c(0,3),axes=FALSE,
>      xlab='Odds Ratio',ylab='',main='Reference Group: A only')
> axis(1,at=seq(0,3,by=0.5),labels=paste(seq(0,3,by=0.5)))
> axis(2,at=yaxis,las=2)
> 
> 
> segments(x0=drug.orl,x1=drug.oru,y0=yaxis,y1=yaxis,col=4,lend=2)
> 
> # or try
> #arrows(x0=drug.orl,x1=drug.oru,y0=yaxis,y1=yaxis,length=0.1,angle=0,cod
> e=3,col=4,lend=2)

   The call to arrows() seems to do what you want if angle=90.  Also the 
lend argument seems unnecessary if you have angle=90.

> box()
> =====================================================
> 
> 
> Any comments or suggestions would be greatly appreciated.
> 
> 
> Jane
> 
> University of Pittsburgh
> Pittsburgh, PA 15261
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ggrothendieck at gmail.com  Fri Jul  7 18:15:27 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 7 Jul 2006 12:15:27 -0400
Subject: [R] dotplot (lattice) with panel.segments and groups
In-Reply-To: <8764i9whtu.fsf@arctocephalus.homelinux.org>
References: <8764i9whtu.fsf@arctocephalus.homelinux.org>
Message-ID: <971536df0607070915y63b8320eke7eaa69435fba01b@mail.gmail.com>

It seems to work if you handle the col= argument explicitly.
Suggest you double check this since I have not carefully done so:

barley$yield2 <- with(barley, yield + 5)

dotplot(site ~ yield | variety, data=barley, groups=year,
       yield2=barley$yield2, col=c("gray", "black"),
       panel=function(x, y, subscripts, groups, yield2, col, ...) {
           panel.dotplot(x, y, ...)
           panel.segments(x, as.numeric(y),
              yield2[subscripts], as.numeric(y),
		col = col[groups[subscripts]], ...)
       })



On 7/7/06, Sebastian Luque <spluque at gmail.com> wrote:
> Hi,
>
> The following produces almost exactly what I needed.  The problems are
> that the 'panel.dotplot' call (commented) generates the error 'Error in
> NextMethod("[") : argument "subscripts" is missing, with no default'.  The
> other problem is that the colors alternate between the levels of the 'site'
> variable, rather than 'year'.
>
>
> barley$yield2 <- with(barley, yield + 5)
>
> dotplot(site ~ yield | variety, data=barley, groups=year,
>        yield2=barley$yield2, col=c("gray", "black"),
>        panel=function(x, y, subscripts, yield2, ...) {
>            ## panel.dotplot(x, y, ...)
>            panel.segments(x, as.numeric(y),
>                           yield2[subscripts], as.numeric(y), ...)
>        })
>
> R> sessionInfo()
> Version 2.3.1 (2006-06-01)
> i486-pc-linux-gnu
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
>
> other attached packages:
>   chron  gmodels  lattice
>  "2.3-3" "2.12.0" "0.13-8"
>
>
> Can somebody please suggest how to properly make that 'panel.dotplot' call
> and set the 'col' argument?  Thanks in advance.
>
>
> Cheers,
>
> --
> Seb
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From gavin.simpson at ucl.ac.uk  Fri Jul  7 18:24:29 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 07 Jul 2006 17:24:29 +0100
Subject: [R] How to change the type of segments ends?
In-Reply-To: <6E031567AB9AD24FA98F173673F0708F743537@boa.edc3.pitt.edu>
References: <6E031567AB9AD24FA98F173673F0708F743537@boa.edc3.pitt.edu>
Message-ID: <1152289469.19815.8.camel@gsimpson.geog.ucl.ac.uk>

On Fri, 2006-07-07 at 11:47 -0400, Lu, Jiang Jane wrote:
> Hi,
> 
> I am trying to plot odds ratios and the corresponding confidence
> intervals in horizontal segments. It would be ideal if the confidence
> interval segment can be drawn with little vertical bars at both ends. I
> have tried very hard to change the type of ends by using 'lend'
> arguments, but cannot make it. I even tried 'arrows()', but still
> failed. Following is the code I use:
> ====================================================
> drug.or <- c(1.017,1.437,1.427,2.211)
> drug.orl <- c(0.715,1.075,1.103,1.696)
> drug.oru <- c(1.446,1.922,1.845,2.882)
> 
> yaxis <- seq(1,4,by=1)
> 
> plot(x=drug.or,y=yaxis,type='p',pch=17,xlim=c(0,3),axes=FALSE,
>      xlab='Odds Ratio',ylab='',main='Reference Group: A only')
> axis(1,at=seq(0,3,by=0.5),labels=paste(seq(0,3,by=0.5)))
> axis(2,at=yaxis,las=2)
> 
> 
> segments(x0=drug.orl,x1=drug.oru,y0=yaxis,y1=yaxis,col=4,lend=2)
> 
> # or try
> #arrows(x0=drug.orl,x1=drug.oru,y0=yaxis,y1=yaxis,length=0.1,angle=0,cod
> e=3,col=4,lend=2)
> 
> box()
> =====================================================
> 
> 
> Any comments or suggestions would be greatly appreciated.
> 
> 
> Jane

Hi Jane,

Thanks for the reproducible example. Why do you think lend will do what
you want? From ?par:

'lend' The line end style.  This can be specified as an integer or
          string:

          '0' and '"round"' mean rounded line caps [_default_];

          '1' and '"butt"' mean butt line caps;

          '2' and '"square"' mean square line caps.

          As from R 2.3.0 this can be specified inline.

This is talking about line endings - do you want hard or soft line ends
or type of joins in line segments? It does not mean the type of line
endings you might get in Adobe Illustrator or Inkscape vector drawing
packages.

You were close with arrows:

drug.or <- c(1.017,1.437,1.427,2.211)
drug.orl <- c(0.715,1.075,1.103,1.696)
drug.oru <- c(1.446,1.922,1.845,2.882)

yaxis <- seq(1,4,by=1)

plot(x = drug.or, y = yaxis, pch=17, xlim = c(0,3), axes=FALSE,
     xlab = 'Odds Ratio', ylab = '', main = 'Reference Group: A only')
labs <- seq(0, 3, by = 0.5)
axis(1, at = labs, labels = labs)
axis(2, at = yaxis, las = 2)
arrows(x0 = drug.orl, x1 = drug.oru, y0 = yaxis, y1 = yaxis, length=0.1,
code = 3, col = 4, angle = 90)

the angle argument is the key. See arrows? You don't need lend at all to
do what you want.

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
 London, UK. WC1E 6BT.         [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From LuJ at edc.pitt.edu  Fri Jul  7 18:37:58 2006
From: LuJ at edc.pitt.edu (Lu, Jiang Jane)
Date: Fri, 7 Jul 2006 12:37:58 -0400
Subject: [R] How to change the type of segments ends?
In-Reply-To: <1152289469.19815.8.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <6E031567AB9AD24FA98F173673F0708F743544@boa.edc3.pitt.edu>

Thank you all for such a prompt response. I misunderstood the meaning of
'lend' in par(). Now I have got the ideal confidence segments. I
appreciate all the specific explanations you have offered.

Sincerely,

Jane

-----Original Message-----
From: Gavin Simpson [mailto:gavin.simpson at ucl.ac.uk] 
Sent: Friday, July 07, 2006 12:24 PM
To: Lu, Jiang Jane
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] How to change the type of segments ends?

On Fri, 2006-07-07 at 11:47 -0400, Lu, Jiang Jane wrote:
> Hi,
> 
> I am trying to plot odds ratios and the corresponding confidence
> intervals in horizontal segments. It would be ideal if the confidence
> interval segment can be drawn with little vertical bars at both ends.
I
> have tried very hard to change the type of ends by using 'lend'
> arguments, but cannot make it. I even tried 'arrows()', but still
> failed. Following is the code I use:
> ====================================================
> drug.or <- c(1.017,1.437,1.427,2.211)
> drug.orl <- c(0.715,1.075,1.103,1.696)
> drug.oru <- c(1.446,1.922,1.845,2.882)
> 
> yaxis <- seq(1,4,by=1)
> 
> plot(x=drug.or,y=yaxis,type='p',pch=17,xlim=c(0,3),axes=FALSE,
>      xlab='Odds Ratio',ylab='',main='Reference Group: A only')
> axis(1,at=seq(0,3,by=0.5),labels=paste(seq(0,3,by=0.5)))
> axis(2,at=yaxis,las=2)
> 
> 
> segments(x0=drug.orl,x1=drug.oru,y0=yaxis,y1=yaxis,col=4,lend=2)
> 
> # or try
>
#arrows(x0=drug.orl,x1=drug.oru,y0=yaxis,y1=yaxis,length=0.1,angle=0,cod
> e=3,col=4,lend=2)
> 
> box()
> =====================================================
> 
> 
> Any comments or suggestions would be greatly appreciated.
> 
> 
> Jane

Hi Jane,

Thanks for the reproducible example. Why do you think lend will do what
you want? From ?par:

'lend' The line end style.  This can be specified as an integer or
          string:

          '0' and '"round"' mean rounded line caps [_default_];

          '1' and '"butt"' mean butt line caps;

          '2' and '"square"' mean square line caps.

          As from R 2.3.0 this can be specified inline.

This is talking about line endings - do you want hard or soft line ends
or type of joins in line segments? It does not mean the type of line
endings you might get in Adobe Illustrator or Inkscape vector drawing
packages.

You were close with arrows:

drug.or <- c(1.017,1.437,1.427,2.211)
drug.orl <- c(0.715,1.075,1.103,1.696)
drug.oru <- c(1.446,1.922,1.845,2.882)

yaxis <- seq(1,4,by=1)

plot(x = drug.or, y = yaxis, pch=17, xlim = c(0,3), axes=FALSE,
     xlab = 'Odds Ratio', ylab = '', main = 'Reference Group: A only')
labs <- seq(0, 3, by = 0.5)
axis(1, at = labs, labels = labs)
axis(2, at = yaxis, las = 2)
arrows(x0 = drug.orl, x1 = drug.oru, y0 = yaxis, y1 = yaxis, length=0.1,
code = 3, col = 4, angle = 90)

the angle argument is the key. See arrows? You don't need lend at all to
do what you want.

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
 London, UK. WC1E 6BT.         [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From wade.wall at gmail.com  Fri Jul  7 18:58:46 2006
From: wade.wall at gmail.com (Wade Wall)
Date: Fri, 7 Jul 2006 12:58:46 -0400
Subject: [R] replace values in data frame
In-Reply-To: <1152286828.4667.4.camel@localhost.localdomain>
References: <e23082be0607070820l595a0db7g896a71182ccbe4a6@mail.gmail.com>
	<1152286828.4667.4.camel@localhost.localdomain>
Message-ID: <e23082be0607070958q79f018cat50ef0779b51009f1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060707/a9545f95/attachment.pl 

From Horace.Tso at pgn.com  Fri Jul  7 19:06:08 2006
From: Horace.Tso at pgn.com (Horace Tso)
Date: Fri, 07 Jul 2006 10:06:08 -0700
Subject: [R] Converting data frame to zoo
Message-ID: <s4ae321a.033@pgn.com>

Dear list,

I know this is really basic question but I just couldn't get anything
to work. (I did a R site search with keywords "zoo" and "data frame" but
the server timed out on me.)

I have a time series which has the following (typical) format,

DATE               Open          High          Low          Close      
  Volume
01-JAN-2006     5.25           5.25          5.25          5.25        
  256
....

I read the data in from a csv file with read.csv() and it defaulted to
a data frame which I thought is fine. Now I want to convert it to zoo so
I did

x <- zoo(my.df)

which works just fine. But the Date column has been turned into a
factor. Is there a way to make it into a Date. I've tried,

x$Date <- as.Date(x$Date)  

but R complains that

Error in fromchar(x) : character string is not in a standard
unambiguous format

Thanks in advance.

Horace W. Tso






______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From mschwartz at mn.rr.com  Fri Jul  7 19:18:27 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 07 Jul 2006 12:18:27 -0500
Subject: [R] replace values in data frame
In-Reply-To: <e23082be0607070958q79f018cat50ef0779b51009f1@mail.gmail.com>
References: <e23082be0607070820l595a0db7g896a71182ccbe4a6@mail.gmail.com>
	<1152286828.4667.4.camel@localhost.localdomain>
	<e23082be0607070958q79f018cat50ef0779b51009f1@mail.gmail.com>
Message-ID: <1152292707.4667.36.camel@localhost.localdomain>

Thanks for re-posting onlist.  My offlist reply:

The 'list' argument, as per ?replace, needs to be a vector of one or
more indices into another vector, generally the source vector argument
'x', not the actual values that you want to replace. 'list' can either
be explicit integers as the indices, or the result of a logical
operation as I used in my example.

In addition, the objects 'list' and 'replace' in your code are not
vectors, but they are data frames. That is the default
type of object created when using read.table().

You can use str(list) and str(replace) to check this. The str() function
returns the structure of an object.

It is likely that you really want list$V1 and replace$V1 as the vectors
of interest here. V1 thru Vx are the default column names created when
using read.table() if no header row exists in the incoming file.

In addition, note that replace() is not vectorized. It will not handle
multiple search and replace arguments in a single call. See my example
using the iris dataset.

You would need to do each one separately. You can create a loop of one
type or another to cycle through multiple arguments if you wish, where
each cycle through the loop is a single call to replace() with a new set
of values as appropriate for the arguments.

Finally, you should generally not use R object or function names for
user created objects. Both 'list' and 'replace' are such objects. R will
generally be able to differentiate, but there is no guarantee and it
will help you avoid code debugging headaches.

HTH,

Marc Schwartz



On Fri, 2006-07-07 at 12:58 -0400, Wade Wall wrote:
> The format is like this.
> 
> Plot  species   Value
> 
> P1  ACERRUB   3
> P2  MAGNVIR    2
> P3  ARONARB   2
> etc.
> 
> imported using x<-read.table(file="filename.txt")
> 
> I want to replace a list of values in the 2nd column with another list. For
> example, I want to replace ARONARB with PHOTPYR.
> list<-read.table(file="originalnames.txt")
> replace<-read.table(file="replacementnames.txt")
> I tried to use replace in this manner:
> 
> newx<-replace(x,list,replace)
> however, I get the error message: error in replace, invalid subscript type.
> 
> I have tried transforming the above list and modifying the column names
> (column 2), but to no avail.  I hope this clarifies a little.  Sorry about
> that.

> 
> >
> > On Fri, 2006-07-07 at 11:20 -0400, Wade Wall wrote:
> > > Hi all,
> > >
> > > I have a three columned list that I have imported into R.  The first
> > column
> > > is a plot (ex. Plot1), the second is a species name (ex ACERRUB) and the
> > > third a numeric value.  I want to replace some of the second column
> > names
> > > with other names (for example replace ACERRUB with ACERDRU).  The
> > original
> > > and replacement values are in separate lists (not vectors), but I can't
> > seem
> > > to find the right function to perform this.  The replace function seems
> > to
> > > only want to work with numbers.
> > >
> > > Any clues?
> > >
> > > Wade
> >


From ggrothendieck at gmail.com  Fri Jul  7 19:22:19 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 7 Jul 2006 13:22:19 -0400
Subject: [R] Converting data frame to zoo
In-Reply-To: <s4ae321a.033@pgn.com>
References: <s4ae321a.033@pgn.com>
Message-ID: <971536df0607071022m5d9bb654x9f601f1431b703eb@mail.gmail.com>

Check out read.zoo in the zoo package.

On 7/7/06, Horace Tso <Horace.Tso at pgn.com> wrote:
> Dear list,
>
> I know this is really basic question but I just couldn't get anything
> to work. (I did a R site search with keywords "zoo" and "data frame" but
> the server timed out on me.)
>
> I have a time series which has the following (typical) format,
>
> DATE               Open          High          Low          Close
>  Volume
> 01-JAN-2006     5.25           5.25          5.25          5.25
>  256
> ....
>
> I read the data in from a csv file with read.csv() and it defaulted to
> a data frame which I thought is fine. Now I want to convert it to zoo so
> I did
>
> x <- zoo(my.df)
>
> which works just fine. But the Date column has been turned into a
> factor. Is there a way to make it into a Date. I've tried,
>
> x$Date <- as.Date(x$Date)
>
> but R complains that
>
> Error in fromchar(x) : character string is not in a standard
> unambiguous format
>
> Thanks in advance.
>
> Horace W. Tso
>
>
>
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From mschwartz at mn.rr.com  Fri Jul  7 19:35:45 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 07 Jul 2006 12:35:45 -0500
Subject: [R] replace values in data frame
In-Reply-To: <1152292707.4667.36.camel@localhost.localdomain>
References: <e23082be0607070820l595a0db7g896a71182ccbe4a6@mail.gmail.com>
	<1152286828.4667.4.camel@localhost.localdomain>
	<e23082be0607070958q79f018cat50ef0779b51009f1@mail.gmail.com>
	<1152292707.4667.36.camel@localhost.localdomain>
Message-ID: <1152293746.4667.47.camel@localhost.localdomain>

Wade,

Given that you appear to have multiple search and replace items to deal
with, here is a possible loop based "Global Search and Replace"
solution:

gsr <- function(Source, Search, Replace)
{
  if (length(Search) != length(Replace))
    stop("Search and Replace Must Have Equal Number of Items\n")

  Changed <- as.character(Source)

  for (i in 1:length(Search))
  {
    cat("Replacing: ", Search[i], " With: ", Replace[i], "\n")
    Changed <- replace(Changed, Changed == Search[i], Replace[i])
  }

  cat("\n")
  
  Changed
}


Source:  The source vector, which will be coerced to character

Search: The Search values to be matched in "Source" as a character
vector

Replace: The values that will replace those found in 'Search' on a
one-for-one basis.


This function returns a character vector. As with replace(), the result
must be assigned. The change is not done 'in place'. The function will
also output the search and replace values to the console during the
loop.


Again, using the iris dataset as an example:

> iris$Species
  [1] setosa     setosa     setosa     setosa     setosa     setosa    
  [7] setosa     setosa     setosa     setosa     setosa     setosa    
 [13] setosa     setosa     setosa     setosa     setosa     setosa    
 [19] setosa     setosa     setosa     setosa     setosa     setosa    
 [25] setosa     setosa     setosa     setosa     setosa     setosa    
 [31] setosa     setosa     setosa     setosa     setosa     setosa    
 [37] setosa     setosa     setosa     setosa     setosa     setosa    
 [43] setosa     setosa     setosa     setosa     setosa     setosa    
 [49] setosa     setosa     versicolor versicolor versicolor versicolor
 [55] versicolor versicolor versicolor versicolor versicolor versicolor
 [61] versicolor versicolor versicolor versicolor versicolor versicolor
 [67] versicolor versicolor versicolor versicolor versicolor versicolor
 [73] versicolor versicolor versicolor versicolor versicolor versicolor
 [79] versicolor versicolor versicolor versicolor versicolor versicolor
 [85] versicolor versicolor versicolor versicolor versicolor versicolor
 [91] versicolor versicolor versicolor versicolor versicolor versicolor
 [97] versicolor versicolor versicolor versicolor virginica  virginica 
[103] virginica  virginica  virginica  virginica  virginica  virginica 
[109] virginica  virginica  virginica  virginica  virginica  virginica 
[115] virginica  virginica  virginica  virginica  virginica  virginica 
[121] virginica  virginica  virginica  virginica  virginica  virginica 
[127] virginica  virginica  virginica  virginica  virginica  virginica 
[133] virginica  virginica  virginica  virginica  virginica  virginica 
[139] virginica  virginica  virginica  virginica  virginica  virginica 
[145] virginica  virginica  virginica  virginica  virginica  virginica 
Levels: setosa versicolor virginica


Note that iris$Species is a factor with specific levels. The gsr()
function above will coerce the Source argument to a character vector
internally and return a character vector:

> iris$Species <- gsr(iris$Species, 
                      c("setosa", "versicolor", "virginica"), 
                      c("s1", "v1", "v2"))
Replacing:  setosa  With:  s1 
Replacing:  versicolor  With:  v1 
Replacing:  virginica  With:  v2 

> iris$Species
  [1] "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1"
 [14] "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1"
 [27] "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1"
 [40] "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "s1" "v1" "v1"
 [53] "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1"
 [66] "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1"
 [79] "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1"
 [92] "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v1" "v2" "v2" "v2" "v2"
[105] "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2"
[118] "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2"
[131] "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2" "v2"
[144] "v2" "v2" "v2" "v2" "v2" "v2" "v2"


HTH,

Marc Schwartz


From roger.bos at gmail.com  Fri Jul  7 19:45:09 2006
From: roger.bos at gmail.com (roger bos)
Date: Fri, 7 Jul 2006 13:45:09 -0400
Subject: [R] Rpad Server Installation on Windows XP
In-Reply-To: <20060707115330.whtu3jo4kucko0gk@webmail.mit.edu>
References: <20060707115330.whtu3jo4kucko0gk@webmail.mit.edu>
Message-ID: <1db726800607071045k2823fd36vf896ccbcbd0bc765@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060707/ed10e86d/attachment.pl 

From markleeds at verizon.net  Fri Jul  7 19:57:05 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Fri, 07 Jul 2006 12:57:05 -0500 (CDT)
Subject: [R] attach and detach question
Message-ID: <2635140.970631152295025617.JavaMail.root@vms074.mailsrvcs.net>


I have a large R program that I am constantly running over and over again. At the beginning of this program, I create a hige matrix and a huge dataframe but these are constant. What I mean by constant is that, if I run the program over later, I really should just use the old matrix and dataframe ( if they exist ) that were created in a previous run so that the program doesn't have to spend time creating them. Unfortunately, I don't know how to do this so things take foreeer because every time i do a new run, i recreate these objects and the boss is getting a little annoyed. I know/think that I should use attach and detach commands  but 

1) i can't find an example somewhere of
just saving two objects rather than the whole session.
i've looked and looked and i can't find it.

2) if i am able to save these two objects, i was hoping that it
would be possible to write code inside my R program
that basically says, "if these objects already exist in
such and such database, then skip over this section of the code
that creates them".

if someone has some code or a page number or reference, this is fine because i used to do pretty much this in Splus so think i can figure it out if i can just find an example.  thanks a lot.


From Horace.Tso at pgn.com  Fri Jul  7 20:05:04 2006
From: Horace.Tso at pgn.com (Horace Tso)
Date: Fri, 07 Jul 2006 11:05:04 -0700
Subject: [R] Converting data frame to zoo
Message-ID: <s4ae3ff1.002@pgn.com>

Thanks Gabor. I figured out what went wrong. The culprit turns out to be
the headers in my data. read.zoo doesn't recognize column headers and
complains 

Error in read.zoo("C:\\.......\\table.csv",  : 
        index contains NAs

Or is there an option as in read.table(x, header=...) ? 

After the header line is removed it works fine.

H.



>>> "Gabor Grothendieck" <ggrothendieck at gmail.com> 7/7/2006 10:22 AM
>>>
Check out read.zoo in the zoo package.

On 7/7/06, Horace Tso <Horace.Tso at pgn.com> wrote:
> Dear list,
>
> I know this is really basic question but I just couldn't get
anything
> to work. (I did a R site search with keywords "zoo" and "data frame"
but
> the server timed out on me.)
>
> I have a time series which has the following (typical) format,
>
> DATE               Open          High          Low          Close
>  Volume
> 01-JAN-2006     5.25           5.25          5.25          5.25
>  256
> ....
>
> I read the data in from a csv file with read.csv() and it defaulted
to
> a data frame which I thought is fine. Now I want to convert it to zoo
so
> I did
>
> x <- zoo(my.df)
>
> which works just fine. But the Date column has been turned into a
> factor. Is there a way to make it into a Date. I've tried,
>
> x$Date <- as.Date(x$Date)
>
> but R complains that
>
> Error in fromchar(x) : character string is not in a standard
> unambiguous format
>
> Thanks in advance.
>
> Horace W. Tso
>
>
>
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html 
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html 
>


From ggrothendieck at gmail.com  Fri Jul  7 20:09:25 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 7 Jul 2006 14:09:25 -0400
Subject: [R] Converting data frame to zoo
In-Reply-To: <s4ae3ff1.001@pgn.com>
References: <s4ae3ff1.001@pgn.com>
Message-ID: <971536df0607071109w22d7886am9545b54e99df9d53@mail.gmail.com>

read.zoo supports all the arguments that read.table supports including
header=.  From the read.zoo help file:

 ...: further arguments passed to 'read.table'.

Regards.

On 7/7/06, Horace Tso <Horace.Tso at pgn.com> wrote:
> Thanks Gabor. I figured out what went wrong. The culprit turns out to be
> the headers in my data. read.zoo doesn't recognize column headers and
> complains
>
> Error in read.zoo("C:\\.......\\table.csv",  :
>        index contains NAs
>
> Or is there an option as in read.table(x, header=...) ?
>
> After the header line is removed it works fine.
>
> H.
>
>
>
> >>> "Gabor Grothendieck" <ggrothendieck at gmail.com> 7/7/2006 10:22 AM
> >>>
> Check out read.zoo in the zoo package.
>
> On 7/7/06, Horace Tso <Horace.Tso at pgn.com> wrote:
> > Dear list,
> >
> > I know this is really basic question but I just couldn't get
> anything
> > to work. (I did a R site search with keywords "zoo" and "data frame"
> but
> > the server timed out on me.)
> >
> > I have a time series which has the following (typical) format,
> >
> > DATE               Open          High          Low          Close
> >  Volume
> > 01-JAN-2006     5.25           5.25          5.25          5.25
> >  256
> > ....
> >
> > I read the data in from a csv file with read.csv() and it defaulted
> to
> > a data frame which I thought is fine. Now I want to convert it to zoo
> so
> > I did
> >
> > x <- zoo(my.df)
> >
> > which works just fine. But the Date column has been turned into a
> > factor. Is there a way to make it into a Date. I've tried,
> >
> > x$Date <- as.Date(x$Date)
> >
> > but R complains that
> >
> > Error in fromchar(x) : character string is not in a standard
> > unambiguous format
> >
> > Thanks in advance.
> >
> > Horace W. Tso
> >
> >
> >
> >
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>


From ggrothendieck at gmail.com  Fri Jul  7 20:11:35 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 7 Jul 2006 14:11:35 -0400
Subject: [R] attach and detach question
In-Reply-To: <2635140.970631152295025617.JavaMail.root@vms074.mailsrvcs.net>
References: <2635140.970631152295025617.JavaMail.root@vms074.mailsrvcs.net>
Message-ID: <971536df0607071111l2bc94040o66303ea7eff4448b@mail.gmail.com>

There is an example of saving just two objects in the example
section of ?save

To check whether an object of a given name exists see ?exists

On 7/7/06, markleeds at verizon.net <markleeds at verizon.net> wrote:
>
> I have a large R program that I am constantly running over and over again. At the beginning of this program, I create a hige matrix and a huge dataframe but these are constant. What I mean by constant is that, if I run the program over later, I really should just use the old matrix and dataframe ( if they exist ) that were created in a previous run so that the program doesn't have to spend time creating them. Unfortunately, I don't know how to do this so things take foreeer because every time i do a new run, i recreate these objects and the boss is getting a little annoyed. I know/think that I should use attach and detach commands  but
>
> 1) i can't find an example somewhere of
> just saving two objects rather than the whole session.
> i've looked and looked and i can't find it.
>
> 2) if i am able to save these two objects, i was hoping that it
> would be possible to write code inside my R program
> that basically says, "if these objects already exist in
> such and such database, then skip over this section of the code
> that creates them".
>
> if someone has some code or a page number or reference, this is fine because i used to do pretty much this in Splus so think i can figure it out if i can just find an example.  thanks a lot.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ccleland at optonline.net  Fri Jul  7 20:15:31 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 07 Jul 2006 14:15:31 -0400
Subject: [R] attach and detach question
In-Reply-To: <2635140.970631152295025617.JavaMail.root@vms074.mailsrvcs.net>
References: <2635140.970631152295025617.JavaMail.root@vms074.mailsrvcs.net>
Message-ID: <44AEA4C3.5050707@optonline.net>

markleeds at verizon.net wrote:
> I have a large R program that I am constantly running over and over again. At the beginning of this program, I create a hige matrix and a huge dataframe but these are constant. What I mean by constant is that, if I run the program over later, I really should just use the old matrix and dataframe ( if they exist ) that were created in a previous run so that the program doesn't have to spend time creating them. Unfortunately, I don't know how to do this so things take foreeer because every time i do a new run, i recreate these objects and the boss is getting a little annoyed. I know/think that I should use attach and detach commands  but 
> 
> 1) i can't find an example somewhere of
> just saving two objects rather than the whole session.
> i've looked and looked and i can't find it.
> 
> 2) if i am able to save these two objects, i was hoping that it
> would be possible to write code inside my R program
> that basically says, "if these objects already exist in
> such and such database, then skip over this section of the code
> that creates them".

   You probably want save() and load() rather than attach() and detach().

# Save only a couple of objects
save(huge.mat, huge.df, file="myfile.Rdata")

# Load objects only if they do not exist
if(!exists("huge.mat") & !exists ("huge.df")) load("myfile.Rdata") else
   {cat("\n", "Object(s) Already Exist", "\n")}

?save
?load
?exists

> if someone has some code or a page number or reference, this is fine because i used to do pretty much this in Splus so think i can figure it out if i can just find an example.  thanks a lot.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From jdrapp at gmail.com  Fri Jul  7 20:22:29 2006
From: jdrapp at gmail.com (justin rapp)
Date: Fri, 7 Jul 2006 14:22:29 -0400
Subject: [R] Levels and GLM
Message-ID: <af81db5a0607071122v6d4d1f0ahe61fd72aff27cb58@mail.gmail.com>

I am using the as.factor command to use with glm.  When I use the command

>maj <- as.factor(data.logistic$Majors)
>maj

I receive the following output:
  [1] M M N M M M M N N M M M N M M M M M M M M M M M N M N N M M N M
M N M M M M M
 [40] N M N M M N M M M N M N M N M N N N M N M M M M M M N M N M M M
M M N N M M M
 [79] M M M N N M M N M N M M M M M M M M M M M M M M M N M M M M M N
M M M M M N M
[118] M M M N M N N M M M M M M M M N M N M M M M M N M M M M N M M M
N N M M M N M
[157] M M M M M M M M M M M M M N M M N N M M N M M M M M M M M M M M
M M N M N M M
[196] M N M M M M M M M M N M M M M M M M M N M M M M M M M M M M M M
M M N M M N N
[235] M M M M M N M M M M M M N N M M N M M M M M M M M M M M M M M M
M N M M M M N
[274] N M M M M M M N M M M M M M M M M M N N M N M M M M M M M M M M
N M N N M M M
[313] M M M M M M M N M M M M M N M M M M M M M M M M M M M M M N M M
M M M M M N M
[352] M N M N M M N M M M M N M M M M M M M M M M N M M N N
Levels: M N

When I enter:

> logistic.glm <- glm(data.logistic$X100.Yard.Average ~ data.logistic$Overall + maj, family=binomial)
> logistic.glm

I receive the following output:

Call:  glm(formula = data.logistic$X100.Yard.Average ~
data.logistic$Overall +      maj, family = binomial)

Coefficients:
          (Intercept)  data.logistic$Overall                   majN
              2.38819               -0.02718               -0.18385

Degrees of Freedom: 377 Total (i.e. Null);  375 Residual
Null Deviance:	    514.5
Residual Deviance: 410.7 	AIC: 416.7

My question:  Why is there no output for majM?  Any help would be
greatly appreciated


From timmdanker at gmx.de  Thu Jul  6 21:09:27 2006
From: timmdanker at gmx.de (Timm Danker)
Date: Thu, 06 Jul 2006 21:09:27 +0200
Subject: [R] [R-pkgs] ggplot: a new system for drawing graphics in R
In-Reply-To: <f8e6ff050607030025k77d4075bv7faf5b68d54b56b3@mail.gmail.com>
References: <f8e6ff050607030025k77d4075bv7faf5b68d54b56b3@mail.gmail.com>
Message-ID: <e8jn58$c11$1@sea.gmane.org>

I think the ggplot package is extremely promising.
Parts of the dokumentation are very good alread, e.g I recently managed 
to write my first own grob function.
One thing I am missing in the dokumentation is a little more detail on 
how to modify the colours of a plot.
Specifically, if I have:

  xx<-"x y g bar
1 1 1 1
1 5 2 3
2 3 1 3
2 4 2 2"
df=read.table(textConnection(xx),header=T);df
  ggplot(df, aes=list(y=y, x=factor(x),bar=bar))->p
  ggbar(p, aes=list(fill=g,barcolour=g+1), avoid="dodge", sort=TRUE)

... I get a beatiful barplot, but would like to change the colors of the 
bars. I tried

?scfill

and did not understand too much of the help file.
I think some more examples would do it.

Timm

hadley wickham schrieb:
> ggplot provides a new system for drawing graphics in R, based on the
> Grammar of Graphics. It combines the advantages of both base and
> lattice graphics: conditioning and shared axes are handled
> automatically, and you can still build up a plot step by step from
> multiple data sources. It also implements a more sophisticated
> multidimensional conditioning system and a consistent interface to map
> data to visual attributes.  ggplot (along with reshape) received the
> John Chambers Award for Statistical Computing.
> 
> ggplot is available now from CRAN (install.packages("ggplot")) and
> more information is available at my website (http://had.co.nz/ggplot)
> including copies of talks, examples, and a guide showing how to
> convert your existing lattice code.
> 
> To get started I recommend you look at:
> 
>  * the introductory vignette: vignette("introduction", "ggplot")
>  * help for the quick plotting command: ?qplot
>  * help for the full plotting commands: ?ggplot
> 
> I want to provide great documentation, so if there is anything you
> think I am missing, please let me know.
> 
> Regards,
> 
> Hadley
> 
> _______________________________________________
> R-packages mailing list
> R-packages at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-packages
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From gavin.simpson at ucl.ac.uk  Fri Jul  7 20:26:35 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 07 Jul 2006 19:26:35 +0100
Subject: [R] attach and detach question
In-Reply-To: <2635140.970631152295025617.JavaMail.root@vms074.mailsrvcs.net>
References: <2635140.970631152295025617.JavaMail.root@vms074.mailsrvcs.net>
Message-ID: <1152296795.19815.18.camel@gsimpson.geog.ucl.ac.uk>

On Fri, 2006-07-07 at 12:57 -0500, markleeds at verizon.net wrote:
<snipped />
> 
> 1) i can't find an example somewhere of
> just saving two objects rather than the whole session.
> i've looked and looked and i can't find it.

?save

> 2) if i am able to save these two objects, i was hoping that it
> would be possible to write code inside my R program
> that basically says, "if these objects already exist in
> such and such database, then skip over this section of the code
> that creates them".
> 

Is this the kind of thing you mean? I used the FileExists function from
package RandomFields as this does the file checking - you could do
something similar with ?file.list and match the name but why reinvent
the wheel.

I flipped your situation 2 around, if you have some really big objects
then it would make sense to generate the objects, save them out to
files. From a new session, you are likely not to have objects, so
perhaps better to check the file exists, if so load it, then check the
right objects are there. Alter to what suits you best.

## create two objects to be saved
obj1 <- matrix(rnorm(1000), ncol = 10)
obj2 <- matrix(rnorm(1000), ncol = 10)
## save them
save(obj1, obj2, file = "tmp.file.RData")
## clean up
rm(list = ls())
ls()

## load the required package
require(RandomFields)

## load the saved objects
if(FileExists("tmp.file.RData")) {
  load("tmp.file.RData")
  if(exists("obj1") & exists("obj2")) {
    ## more code here
    summary(obj1)
    summary(obj2)
  } else
  stop("Required objects don't exist!")
} else
stop("Big objects not found, check file exists!")

## clean up
rm(list = ls())

## if file exists but not correct objects
if(FileExists("tmp.file.RData")) {
  load("tmp.file.RData")
  ## simulate different object names
  obj3 <- obj1
  obj4 <- obj2
  rm(obj1, obj2)
  if(exists("obj1") & exists("obj2")) {
    ## more code here
    summary(obj1)
    summary(obj2)
  } else
  stop("Required objects don't exist!")
} else
stop("Big objects not found, check file exists!")

## clean up
rm(list = ls())

## if file doesn't exists
unlink("tmp.file.RData")
if(FileExists("tmp.file.RData")) {
  load("tmp.file.RData")
  ## remove the objects
  obj3 <- obj1
  obj4 <- obj2
  rm(obj1, obj2)
  if(exists("obj1") & exists("obj2")) {
    ## more code here
    summary(obj1)
    summary(obj2)
  } else
  stop("Required objects don't exist!")
} else
stop("Big objects not found, check file exists!")

HTH,

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
 London, UK. WC1E 6BT.         [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From spencer.graves at pdf.com  Fri Jul  7 20:42:54 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 07 Jul 2006 11:42:54 -0700
Subject: [R] panel ordering in nlme and augPred plots
In-Reply-To: <7ab3bfe50607022110v1fefe29p82b4de3e76f63f92@mail.gmail.com>
References: <7ab3bfe50607022110v1fefe29p82b4de3e76f63f92@mail.gmail.com>
Message-ID: <44AEAB2E.9060006@pdf.com>

	  I'm not sufficiently familiar with 'trellis' / 'lattice' to provide 
an easy, complete answer to your question, but I can explain the current 
behavior and provide a hack to get you what you want.  With luck, 
someone else will suggest an improvement.

	  First, let's decompose your lovely, self-contained example in a more 
informative way:

fm <- lme(Orthodont)
ap <- augPred(fm, level = 0:1)
plot(ap, skip = rep(c(F,T), c(16, 2)))

	  Next, let's examine the structure of 'ap':

str(ap)
Classes augPred  and `data.frame':	2862 obs. of  4 variables:
  $ age     : num  8 10 12 14 8 10 12 14 8 10 ...
  $ .groups : Ord.factor w/ 27 levels "M16"<"M05"<"M02"<..: 15 15 15 15 
3 3 3 3 7 7 ...
   ..- attr(*, "label")= chr "Subject"
<snip ...>

	  The key here is that 'ap$.groups' is of class Ord.factor with levels 
as follows:

 > levels(ap$.groups)
  [1] "M16" "M05" "M02" "M11" "M07" "M08" "M03" "M12" "M13" "M14" "M09" 
"M15"
[13] "M06" "M04" "M01" "M10" "F10" "F09" "F06" "F01" "F05" "F07" "F02" "F08"
[25] "F03" "F04" "F11"
	
	  These names appear in the same order here that they appear in the 
trellis display, reading the display from left to right and bottom to 
top -- NOT top to bottom.

	  This also tells you one way to get around this:  Change the order of 
the levels.  We can do this as follows:

ap2 <- ap
ap2$.groups <- with(ap,
       ordered(as.character(.groups),
               sort(levels(.groups))))
plot(ap2, skip = rep(c(F,T), c(11, 1)))

	  Hope this helps.
	  Spencer Graves	

Nathaniel Derby wrote:
> Hi,
> 
> I'm new at this, I'm very confused, and I think I'm missing something
> important here.  In our pet example we have this:
> 
>> fm <- lme(Orthodont)
>> plot(Orthodont)
>> plot(augPred(fm, level = 0:1))
> 
> which gives us a trellis plot with the females above the males,
> starting with "F03", "F04", "F11", "F06", etc.  I thought the point of
> this was to create an ordering where the females are ordered ("F01",
> "F02", "F03", etc -- followed by the males being ordered).  However,
> the solution given ...
> 
>> fm <- lme(Orthodont)
>> plot(Orthodont)
>> plot(augPred(fm1, level = 0:1), skip = rep(c(F,T), c(16, 2)))
> 
> ... doesn't solve it -- although it does do all the females before
> starting on the males.  That is, it starts with "F02", "F08", "F03",
> ... which isn't in order either.
> 
> Running Petr's code also gave output which wasn't ordered by the subjects.
> 
> Could someone please explain to me how to order the panels of the
> trellis plot by the subjects?
> 
> 
> thanks,
> 
> Nandor
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From deepayan.sarkar at gmail.com  Fri Jul  7 20:53:10 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 7 Jul 2006 13:53:10 -0500
Subject: [R] dotplot (lattice) with panel.segments and groups
In-Reply-To: <8764i9whtu.fsf@arctocephalus.homelinux.org>
References: <8764i9whtu.fsf@arctocephalus.homelinux.org>
Message-ID: <eb555e660607071153k544bbb84s3ed9fdd7cf063a0b@mail.gmail.com>

On 7/7/06, Sebastian Luque <spluque at gmail.com> wrote:
> Hi,
>
> The following produces almost exactly what I needed.  The problems are
> that the 'panel.dotplot' call (commented) generates the error 'Error in
> NextMethod("[") : argument "subscripts" is missing, with no default'.

It's just as it says: panel.dotplot wants a 'subscripts' argument when
'groups' is not null, and you have forgotten to give it one.

> The
> other problem is that the colors alternate between the levels of the 'site'
> variable, rather than 'year'.

It's doing what it's being told to do. You probably want something like

dotplot(site ~ yield | variety, data=barley,
        groups=year, yield2=barley$yield2,
        col=c("gray", "black"),
        panel = panel.superpose,
        panel.groups = function(x, y, subscripts, yield2, col, ...) {
            panel.xyplot(x, y, col = col, ...)
            panel.segments(x, y, yield2[subscripts], y, col = col)
        })

Note that you don't want to use panel.dotplot here as it would draw a
reference line both times. If you do want a reference line, add that
in the panel function.

The explicit use of 'col' should not have been necessary, i.e., the
following should have worked:

dotplot(site ~ yield | variety, data=barley,
        groups=year, yield2=barley$yield2,
        col=c("gray", "black"),
        panel = panel.superpose,
        panel.groups = function(x, y, subscripts, yield2, ...) {
            panel.xyplot(x, y, col = ...)
            panel.segments(x, y, yield2[subscripts], y, ...)
        })

I believe this doesn't work because of a grid bug. I'll take that up
in a separate thread.

-Deepayan

>
>
> barley$yield2 <- with(barley, yield + 5)
>
> dotplot(site ~ yield | variety, data=barley, groups=year,
>         yield2=barley$yield2, col=c("gray", "black"),
>         panel=function(x, y, subscripts, yield2, ...) {
>             ## panel.dotplot(x, y, ...)
>             panel.segments(x, as.numeric(y),
>                            yield2[subscripts], as.numeric(y), ...)
>         })
>
> R> sessionInfo()
> Version 2.3.1 (2006-06-01)
> i486-pc-linux-gnu
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
>
> other attached packages:
>    chron  gmodels  lattice
>  "2.3-3" "2.12.0" "0.13-8"
>
>
> Can somebody please suggest how to properly make that 'panel.dotplot' call
> and set the 'col' argument?  Thanks in advance.


From ggrothendieck at gmail.com  Fri Jul  7 21:00:08 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 7 Jul 2006 15:00:08 -0400
Subject: [R] dotplot (lattice) with panel.segments and groups
In-Reply-To: <eb555e660607071153k544bbb84s3ed9fdd7cf063a0b@mail.gmail.com>
References: <8764i9whtu.fsf@arctocephalus.homelinux.org>
	<eb555e660607071153k544bbb84s3ed9fdd7cf063a0b@mail.gmail.com>
Message-ID: <971536df0607071200x3e31cd0cid98ec0d4aa64cb07@mail.gmail.com>

Could you explain what panel.groups= does and what the difference
is between panel.groups= and panel= ?  In ?xyplot it just says:

panel.groups: useful mostly for 'xyplot' and 'densityplot'. Applies
          when 'panel' is 'panel.superpose' (which happens by default
          in these cases if 'groups' is non-null)

which indicates when it might apply but not what it does.

Also can we assume from the above that the panel=panel.superpose
in your response could have been omitted?

Thanks.


On 7/7/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> On 7/7/06, Sebastian Luque <spluque at gmail.com> wrote:
> > Hi,
> >
> > The following produces almost exactly what I needed.  The problems are
> > that the 'panel.dotplot' call (commented) generates the error 'Error in
> > NextMethod("[") : argument "subscripts" is missing, with no default'.
>
> It's just as it says: panel.dotplot wants a 'subscripts' argument when
> 'groups' is not null, and you have forgotten to give it one.
>
> > The
> > other problem is that the colors alternate between the levels of the 'site'
> > variable, rather than 'year'.
>
> It's doing what it's being told to do. You probably want something like
>
> dotplot(site ~ yield | variety, data=barley,
>        groups=year, yield2=barley$yield2,
>        col=c("gray", "black"),
>        panel = panel.superpose,
>        panel.groups = function(x, y, subscripts, yield2, col, ...) {
>            panel.xyplot(x, y, col = col, ...)
>            panel.segments(x, y, yield2[subscripts], y, col = col)
>        })
>
> Note that you don't want to use panel.dotplot here as it would draw a
> reference line both times. If you do want a reference line, add that
> in the panel function.
>
> The explicit use of 'col' should not have been necessary, i.e., the
> following should have worked:
>
> dotplot(site ~ yield | variety, data=barley,
>        groups=year, yield2=barley$yield2,
>        col=c("gray", "black"),
>        panel = panel.superpose,
>        panel.groups = function(x, y, subscripts, yield2, ...) {
>            panel.xyplot(x, y, col = ...)
>            panel.segments(x, y, yield2[subscripts], y, ...)
>        })
>
> I believe this doesn't work because of a grid bug. I'll take that up
> in a separate thread.
>
> -Deepayan
>
> >
> >
> > barley$yield2 <- with(barley, yield + 5)
> >
> > dotplot(site ~ yield | variety, data=barley, groups=year,
> >         yield2=barley$yield2, col=c("gray", "black"),
> >         panel=function(x, y, subscripts, yield2, ...) {
> >             ## panel.dotplot(x, y, ...)
> >             panel.segments(x, as.numeric(y),
> >                            yield2[subscripts], as.numeric(y), ...)
> >         })
> >
> > R> sessionInfo()
> > Version 2.3.1 (2006-06-01)
> > i486-pc-linux-gnu
> >
> > attached base packages:
> > [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> > [7] "base"
> >
> > other attached packages:
> >    chron  gmodels  lattice
> >  "2.3-3" "2.12.0" "0.13-8"
> >
> >
> > Can somebody please suggest how to properly make that 'panel.dotplot' call
> > and set the 'col' argument?  Thanks in advance.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From e.rehak at t-online.de  Fri Jul  7 21:05:29 2006
From: e.rehak at t-online.de (Mark Hempelmann)
Date: Fri, 07 Jul 2006 21:05:29 +0200
Subject: [R] Multistage Sampling
Message-ID: <44AEB079.6000407@t-online.de>

Dear WizaRds, dear Thomas,

    First of all, I want to tell you how grateful I am for all your 
support. I wish I will be able to help others along one day the same way 
you do. Thank you so much. I am struggling with a multistage sampling 
design:

library(survey)
multi3  <- data.frame(cluster=c(1,1,1,1 ,2,2,2, 3,3), id=c(1,2,3,4, 
1,2,3, 1,2),
nl=c(4,4,4,4, 3,3,3, 2,2), Nl=c(100,100,100,100, 50,50,50, 75,75), 
M=rep(23,9),
y=c(23,33,77,25, 35,74,27, 37,72) )

dmulti3 <- svydesign(id=~cluster+id, fpc=~M+Nl, data=multi3)
svymean (~y, dmulti3)
    mean     SE
y 45.796 5.5483

svytotal(~y, dmulti3)
  total    SE
y 78999 13643

and I estimate the population total as N=M/m sum(Nl) = 
23/3*(100+50+75)=1725. With this, my variance estimator is:
y1<-mean(multi3$y[1:4]) # 39.5
y2<-mean(multi3$y[5:7]) # 45.33
y3<-mean(multi3$y[8:9]) # 54.5

yT1<-100*y1 # 3950 total cluster 1
yT2<-50*y2 # 2266.67 total cluster 2
yT3<-75*y3 # 4087.5 total cluster 3
ybarT<-1/3*sum(yT1,yT2,yT3) # 3434.722
s1 <- var(multi3$y[1:4]) # 643.67 var cluster 1
s2 <- var(multi3$y[5:7]) # 632.33 var cluster 2
s3 <- var(multi3$y[8:9]) # 612.5 var cluster 3

var.yT <- 23^2*( 20/23*1/6*sum( 
(yT1-ybarT)^2,(yT2-ybarT)^2,(yT3-ybarT)^2 ) +
1/69 * sum(100*96*s1, 50*47*s2, 75*73*s3) ) # 242 101 517

but
var.yT/1725^2 = 81.36157
SE = 9.02006,
but it should be SE=13643/1725=7.90899

Is this calculation correct? I remember svytotal using a different 
variance estimator compared to svymean, and that svytotal gives the 
unbiased estimation. To solve the problem, I went ahead and tried to 
calibrate the design object, telling Survey the population total N=1725:

dmulti3.cal <- calibrate(dmulti3, ~1, pop=1725)
svymean (~y, dmulti3.cal)
    mean     SE
y 45.796 5.5483

svytotal(~y, dmulti3.cal)
  total     SE
y 78999 9570.7

, which indeed gives me the computed svymean SE, but alas, I still don't 
know why my variance is so different. I think it might have sthg to do 
with a differently computed N and the fact that your estimator formula 
is a different one. Since I calculated the Taylor Series solution, i 
suppose there must be another approach? The calibration help page tells 
me to enter a list of population total vectors for each cluster, which 
would result in:

dmulti3.cal <- calibrate(dmulti3, ~1, pop=c(100,50,75))
Error in regcalibrate.survey.design2(design, formula, population, 
aggregate.stage = aggregate.stage,  :
Population and sample totals are not the same length.

I am very grateful for your help and wish you alle the best
Yours
mark


From markleeds at verizon.net  Fri Jul  7 21:17:40 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Fri, 07 Jul 2006 14:17:40 -0500 (CDT)
Subject: [R] Multistage Sampling
Message-ID: <5936270.1045281152299860944.JavaMail.root@vms074.mailsrvcs.net>

>From: Mark Hempelmann <e.rehak at t-online.de>
>Date: Fri Jul 07 14:05:29 CDT 2006
>To: r-help at stat.math.ethz.ch
>Subject: [R] Multistage Sampling

i also find it an truly amazing group also. the general kindness
and generosity of everyone is beyond belief.  it will be a long time coming but i hope i can help some day also. unfortunately,
i can't help you with your question either.

                                           mark

            

>Dear WizaRds, dear Thomas,
>
>    First of all, I want to tell you how grateful I am for all your 
>support. I wish I will be able to help others along one day the same way 
>you do. Thank you so much. I am struggling with a multistage sampling 
>design:
>
>library(survey)
>multi3  <- data.frame(cluster=c(1,1,1,1 ,2,2,2, 3,3), id=c(1,2,3,4, 
>1,2,3, 1,2),
>nl=c(4,4,4,4, 3,3,3, 2,2), Nl=c(100,100,100,100, 50,50,50, 75,75), 
>M=rep(23,9),
>y=c(23,33,77,25, 35,74,27, 37,72) )
>
>dmulti3 <- svydesign(id=~cluster+id, fpc=~M+Nl, data=multi3)
>svymean (~y, dmulti3)
>    mean     SE
>y 45.796 5.5483
>
>svytotal(~y, dmulti3)
>  total    SE
>y 78999 13643
>
>and I estimate the population total as N=M/m sum(Nl) = 
>23/3*(100+50+75)=1725. With this, my variance estimator is:
>y1<-mean(multi3$y[1:4]) # 39.5
>y2<-mean(multi3$y[5:7]) # 45.33
>y3<-mean(multi3$y[8:9]) # 54.5
>
>yT1<-100*y1 # 3950 total cluster 1
>yT2<-50*y2 # 2266.67 total cluster 2
>yT3<-75*y3 # 4087.5 total cluster 3
>ybarT<-1/3*sum(yT1,yT2,yT3) # 3434.722
>s1 <- var(multi3$y[1:4]) # 643.67 var cluster 1
>s2 <- var(multi3$y[5:7]) # 632.33 var cluster 2
>s3 <- var(multi3$y[8:9]) # 612.5 var cluster 3
>
>var.yT <- 23^2*( 20/23*1/6*sum( 
>(yT1-ybarT)^2,(yT2-ybarT)^2,(yT3-ybarT)^2 ) +
>1/69 * sum(100*96*s1, 50*47*s2, 75*73*s3) ) # 242 101 517
>
>but
>var.yT/1725^2 = 81.36157
>SE = 9.02006,
>but it should be SE=13643/1725=7.90899
>
>Is this calculation correct? I remember svytotal using a different 
>variance estimator compared to svymean, and that svytotal gives the 
>unbiased estimation. To solve the problem, I went ahead and tried to 
>calibrate the design object, telling Survey the population total N=1725:
>
>dmulti3.cal <- calibrate(dmulti3, ~1, pop=1725)
>svymean (~y, dmulti3.cal)
>    mean     SE
>y 45.796 5.5483
>
>svytotal(~y, dmulti3.cal)
>  total     SE
>y 78999 9570.7
>
>, which indeed gives me the computed svymean SE, but alas, I still don't 
>know why my variance is so different. I think it might have sthg to do 
>with a differently computed N and the fact that your estimator formula 
>is a different one. Since I calculated the Taylor Series solution, i 
>suppose there must be another approach? The calibration help page tells 
>me to enter a list of population total vectors for each cluster, which 
>would result in:
>
>dmulti3.cal <- calibrate(dmulti3, ~1, pop=c(100,50,75))
>Error in regcalibrate.survey.design2(design, formula, population, 
>aggregate.stage = aggregate.stage,  :
>Population and sample totals are not the same length.
>
>I am very grateful for your help and wish you alle the best
>Yours
>mark
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From deepayan.sarkar at gmail.com  Fri Jul  7 21:57:10 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 7 Jul 2006 14:57:10 -0500
Subject: [R] dotplot (lattice) with panel.segments and groups
In-Reply-To: <971536df0607071200x3e31cd0cid98ec0d4aa64cb07@mail.gmail.com>
References: <8764i9whtu.fsf@arctocephalus.homelinux.org>
	<eb555e660607071153k544bbb84s3ed9fdd7cf063a0b@mail.gmail.com>
	<971536df0607071200x3e31cd0cid98ec0d4aa64cb07@mail.gmail.com>
Message-ID: <eb555e660607071257q56bf1ae4nef95734ab50db390@mail.gmail.com>

On 7/7/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Could you explain what panel.groups= does and what the difference
> is between panel.groups= and panel= ?  In ?xyplot it just says:
>
> panel.groups: useful mostly for 'xyplot' and 'densityplot'. Applies
>           when 'panel' is 'panel.superpose' (which happens by default
>           in these cases if 'groups' is non-null)
>
> which indicates when it might apply but not what it does.

That's wrong (it used to be right - a good example of why \synopsis is
bad). Since lattice 0.13-x, panel.superpose is never the default panel
function. An updated version with improved documentation should be out
soon.

'panel.groups' is simply an argument to panel.superpose, and is
described in ?panel.superpose. Thus, it only makes sense as an
argument to xyplot/dotplot/whatever when the panel function is
panel.superpose, and not otherwise. The entry for the graphical
parameters in ?panel.superpose isn't as useful as it could be, I have
just updated it to read:

col, col.line, col.symbol, pch, cex, fill, font, fontface,
fontfamily, lty, lwd, alpha: graphical
          parameters, replicated to be as long as the number of
          groups.  These are eventually passed down to 'panel.groups',
          but as scalars rather than vectors.  When 'panel.groups' is
          called for the i-th level of 'groups', the corresponding
          element of each graphical parameter is passed to it.

Hope that makes things a bit clearer.

Deepayan


From Max.Kuhn at pfizer.com  Fri Jul  7 22:10:42 2006
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Fri, 7 Jul 2006 16:10:42 -0400
Subject: [R]  Levels and GLM
Message-ID: <71257D09F114DA4A8E134DEAC70F25D3058FC9CD@groamrexm03.amer.pfizer.com>

jdrapp,

By default, R fits full rank models. If you are coming from SAS, you're 
probably used to less than full rank model parameterizations. 

>From Section 11.1.1 of "An Introduction to R" at

http://cran.r-project.org/doc/manuals/R-intro.html#Contrasts

there is this:

 "What about a k-level factor A? The answer differs for unordered and 
 ordered factors. For unordered factors k - 1 columns are generated 
 for the indicators of the second, ..., kth levels of the factor. 
 (Thus the implicit parameterization is to contrast the response at 
 each level with that at the first.)"

So level "M" is the "reference cell". Assuming that 
data.logistic$Overall is continuous, the intercept is the estimate of 
the mean response when maj = "M" and data.logistic$Overall = 0. The 
estimate for majN is the difference between the reference cell
(estimated 
by the intercept) and the mean response when maj = "N" and 
data.logistic$Overall = 0.

You should check out ?model.matrix and ?contrasts.

Max


> I am using the as.factor command to use with glm.  When I use the
command
> 
> >maj <- as.factor(data.logistic$Majors)
> >maj
> 
> I receive the following output:
>   [1] M M N M M M M N N M M M N M M M M M M M M M M M N M N N M M N M
> M N M M M M M
>  [40] N M N M M N M M M N M N M N M N N N M N M M M M M M N M N M M M
> M M N N M M M
>  [79] M M M N N M M N M N M M M M M M M M M M M M M M M N M M M M M N
> M M M M M N M
> [118] M M M N M N N M M M M M M M M N M N M M M M M N M M M M N M M M
> N N M M M N M
> [157] M M M M M M M M M M M M M N M M N N M M N M M M M M M M M M M M
> M M N M N M M
> [196] M N M M M M M M M M N M M M M M M M M N M M M M M M M M M M M M
> M M N M M N N
> [235] M M M M M N M M M M M M N N M M N M M M M M M M M M M M M M M M
> M N M M M M N
> [274] N M M M M M M N M M M M M M M M M M N N M N M M M M M M M M M M
> N M N N M M M
> [313] M M M M M M M N M M M M M N M M M M M M M M M M M M M M M N M M
> M M M M M N M
> [352] M N M N M M N M M M M N M M M M M M M M M M N M M N N
> Levels: M N
> 
> When I enter:
> 
> > logistic.glm <- glm(data.logistic$X100.Yard.Average ~
data.logistic$Overall + maj, family=binomial)
> > logistic.glm
> 
> I receive the following output:
> 
> Call:  glm(formula = data.logistic$X100.Yard.Average ~
> data.logistic$Overall +      maj, family = binomial)
> 
> Coefficients:
>           (Intercept)  data.logistic$Overall                   majN
>               2.38819               -0.02718               -0.18385
> 
> Degrees of Freedom: 377 Total (i.e. Null);  375 Residual
> Null Deviance:	    514.5
> Residual Deviance: 410.7 	AIC: 416.7
> 
> My question:  Why is there no output for majM?  Any help would be
> greatly appreciated
----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From Max.Kuhn at pfizer.com  Fri Jul  7 22:17:06 2006
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Fri, 7 Jul 2006 16:17:06 -0400
Subject: [R] FW:  Levels and GLM
Message-ID: <71257D09F114DA4A8E134DEAC70F25D3058FC9DF@groamrexm03.amer.pfizer.com>

 
One correction... since you are fitting a logistic model, it is 
technically correct to say the "mean value of the linear predictor," 
instead of "mean response".

20 lashes for me.

Max

-----Original Message-----
From: Kuhn, Max 
Sent: Friday, July 07, 2006 4:11 PM
To: 'r-help at stat.math.ethz.ch'
Subject: [R] Levels and GLM

jdrapp,

By default, R fits full rank models. If you are coming from SAS, you're 
probably used to less than full rank model parameterizations. 

>From Section 11.1.1 of "An Introduction to R" at

http://cran.r-project.org/doc/manuals/R-intro.html#Contrasts

there is this:

 "What about a k-level factor A? The answer differs for unordered and 
 ordered factors. For unordered factors k - 1 columns are generated 
 for the indicators of the second, ..., kth levels of the factor. 
 (Thus the implicit parameterization is to contrast the response at 
 each level with that at the first.)"

So level "M" is the "reference cell". Assuming that 
data.logistic$Overall is continuous, the intercept is the estimate of 
the mean response when maj = "M" and data.logistic$Overall = 0. The 
estimate for majN is the difference between the reference cell
(estimated 
by the intercept) and the mean response when maj = "N" and 
data.logistic$Overall = 0.

You should check out ?model.matrix and ?contrasts.

Max


> I am using the as.factor command to use with glm.  When I use the
command
> 
> >maj <- as.factor(data.logistic$Majors)
> >maj
> 
> I receive the following output:
>   [1] M M N M M M M N N M M M N M M M M M M M M M M M N M N N M M N M
> M N M M M M M
>  [40] N M N M M N M M M N M N M N M N N N M N M M M M M M N M N M M M
> M M N N M M M
>  [79] M M M N N M M N M N M M M M M M M M M M M M M M M N M M M M M N
> M M M M M N M
> [118] M M M N M N N M M M M M M M M N M N M M M M M N M M M M N M M M
> N N M M M N M
> [157] M M M M M M M M M M M M M N M M N N M M N M M M M M M M M M M M
> M M N M N M M
> [196] M N M M M M M M M M N M M M M M M M M N M M M M M M M M M M M M
> M M N M M N N
> [235] M M M M M N M M M M M M N N M M N M M M M M M M M M M M M M M M
> M N M M M M N
> [274] N M M M M M M N M M M M M M M M M M N N M N M M M M M M M M M M
> N M N N M M M
> [313] M M M M M M M N M M M M M N M M M M M M M M M M M M M M M N M M
> M M M M M N M
> [352] M N M N M M N M M M M N M M M M M M M M M M N M M N N
> Levels: M N
> 
> When I enter:
> 
> > logistic.glm <- glm(data.logistic$X100.Yard.Average ~
data.logistic$Overall + maj, family=binomial)
> > logistic.glm
> 
> I receive the following output:
> 
> Call:  glm(formula = data.logistic$X100.Yard.Average ~
> data.logistic$Overall +      maj, family = binomial)
> 
> Coefficients:
>           (Intercept)  data.logistic$Overall                   majN
>               2.38819               -0.02718               -0.18385
> 
> Degrees of Freedom: 377 Total (i.e. Null);  375 Residual
> Null Deviance:	    514.5
> Residual Deviance: 410.7 	AIC: 416.7
> 
> My question:  Why is there no output for majM?  Any help would be
> greatly appreciated
----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From p.dalgaard at biostat.ku.dk  Fri Jul  7 23:23:01 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Jul 2006 23:23:01 +0200
Subject: [R] FW:  Levels and GLM
In-Reply-To: <71257D09F114DA4A8E134DEAC70F25D3058FC9DF@groamrexm03.amer.pfizer.com>
References: <71257D09F114DA4A8E134DEAC70F25D3058FC9DF@groamrexm03.amer.pfizer.com>
Message-ID: <x2zmflm7nu.fsf@turmalin.kubism.ku.dk>

"Kuhn, Max" <Max.Kuhn at pfizer.com> writes:

>  
> One correction... since you are fitting a logistic model, it is 
> technically correct to say the "mean value of the linear predictor," 
> instead of "mean response".
> 
> 20 lashes for me.

...plus five more for forgetting that the link is nonlinear and that
you can't meaningfully speak about the mean on the linear predictor
scale. "Value of linear predictor corresponding to the mean response"
is correct (I hope...)

> So level "M" is the "reference cell". Assuming that 
> data.logistic$Overall is continuous, the intercept is the estimate of 
> the mean response when maj = "M" and data.logistic$Overall = 0. The 
> estimate for majN is the difference between the reference cell

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ggrothendieck at gmail.com  Fri Jul  7 23:24:35 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 7 Jul 2006 17:24:35 -0400
Subject: [R] dotplot (lattice) with panel.segments and groups
In-Reply-To: <eb555e660607071257q56bf1ae4nef95734ab50db390@mail.gmail.com>
References: <8764i9whtu.fsf@arctocephalus.homelinux.org>
	<eb555e660607071153k544bbb84s3ed9fdd7cf063a0b@mail.gmail.com>
	<971536df0607071200x3e31cd0cid98ec0d4aa64cb07@mail.gmail.com>
	<eb555e660607071257q56bf1ae4nef95734ab50db390@mail.gmail.com>
Message-ID: <971536df0607071424l3fc58865odde13757cce9cba5@mail.gmail.com>

On 7/7/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> On 7/7/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Could you explain what panel.groups= does and what the difference
> > is between panel.groups= and panel= ?  In ?xyplot it just says:
> >
> > panel.groups: useful mostly for 'xyplot' and 'densityplot'. Applies
> >           when 'panel' is 'panel.superpose' (which happens by default
> >           in these cases if 'groups' is non-null)
> >
> > which indicates when it might apply but not what it does.
>
> That's wrong (it used to be right - a good example of why \synopsis is
> bad). Since lattice 0.13-x, panel.superpose is never the default panel
> function. An updated version with improved documentation should be out
> soon.
>
> 'panel.groups' is simply an argument to panel.superpose, and is
> described in ?panel.superpose. Thus, it only makes sense as an
> argument to xyplot/dotplot/whatever when the panel function is
> panel.superpose, and not otherwise. The entry for the graphical
> parameters in ?panel.superpose isn't as useful as it could be, I have
> just updated it to read:
>
> col, col.line, col.symbol, pch, cex, fill, font, fontface,
> fontfamily, lty, lwd, alpha: graphical
>          parameters, replicated to be as long as the number of
>          groups.  These are eventually passed down to 'panel.groups',
>          but as scalars rather than vectors.  When 'panel.groups' is
>          called for the i-th level of 'groups', the corresponding
>          element of each graphical parameter is passed to it.
>
> Hope that makes things a bit clearer.
>
> Deepayan
>

Thanks.  That does help.


From tlumley at u.washington.edu  Fri Jul  7 23:34:36 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 7 Jul 2006 14:34:36 -0700 (PDT)
Subject: [R] Multistage Sampling
In-Reply-To: <44AEB079.6000407@t-online.de>
References: <44AEB079.6000407@t-online.de>
Message-ID: <Pine.LNX.4.64.0607071401230.9075@homer23.u.washington.edu>

On Fri, 7 Jul 2006, Mark Hempelmann wrote:
> library(survey)
> multi3  <- data.frame(cluster=c(1,1,1,1 ,2,2,2, 3,3), id=c(1,2,3,4,
> 1,2,3, 1,2),
> nl=c(4,4,4,4, 3,3,3, 2,2), Nl=c(100,100,100,100, 50,50,50, 75,75),
> M=rep(23,9),
> y=c(23,33,77,25, 35,74,27, 37,72) )
>
> dmulti3 <- svydesign(id=~cluster+id, fpc=~M+Nl, data=multi3)
> svymean (~y, dmulti3)
>    mean     SE
> y 45.796 5.5483
>
> svytotal(~y, dmulti3)
>  total    SE
> y 78999 13643
>
> and I estimate the population total as N=M/m sum(Nl) =
> 23/3*(100+50+75)=1725. With this, my variance estimator is:
> y1<-mean(multi3$y[1:4]) # 39.5
> y2<-mean(multi3$y[5:7]) # 45.33
> y3<-mean(multi3$y[8:9]) # 54.5
>
> yT1<-100*y1 # 3950 total cluster 1
> yT2<-50*y2 # 2266.67 total cluster 2
> yT3<-75*y3 # 4087.5 total cluster 3
> ybarT<-1/3*sum(yT1,yT2,yT3) # 3434.722
> s1 <- var(multi3$y[1:4]) # 643.67 var cluster 1
> s2 <- var(multi3$y[5:7]) # 632.33 var cluster 2
> s3 <- var(multi3$y[8:9]) # 612.5 var cluster 3
>
> var.yT <- 23^2*( 20/23*1/6*sum(
> (yT1-ybarT)^2,(yT2-ybarT)^2,(yT3-ybarT)^2 ) +
> 1/69 * sum(100*96*s1, 50*47*s2, 75*73*s3) ) # 242 101 517

I don't have any of my reference books here today, but if you use
   var.yT <- 23^2*( 20/23*1/6*sum(
    (yT1-ybarT)^2,(yT2-ybarT)^2,(yT3-ybarT)^2 ) +
    1/69 * sum(100*96*s1/4, 50*47*s2/3, 75*73*s3/2) ) # 242 101 517
the results agrees with svytotal(), and with Stata, and with formulas in a 
couple of sets of lecture notes I found by Googling.


> but
> var.yT/1725^2 = 81.36157
> SE = 9.02006,
> but it should be SE=13643/1725=7.90899
>
> Is this calculation correct? I remember svytotal using a different
> variance estimator compared to svymean, and that svytotal gives the
> unbiased estimation.

This calculation is not correct for the mean, since it ignores the 
uncertainty in the estimated population total.  The correct standard error 
comes from treating the mean as a ratio of estimated total to estimated 
population size. In this case you have to do it that way since you don't 
know the population size, but R always does it this way. Because the 
estimated population size and total are correlated, taking into account 
the uncertainty in the denominator actually reduces the standard error.

The easiest way to reproduce the result that R gets is to do it the same 
way that R does: compute the standard error of the mean as the standard 
error of the total of a suitable set of estimating functions. If you 
define a new variable (y-45.796*1)/1725 and estimate the standard error of 
the total of this variable it will give:
> svytotal(~I((y-45.796)/1725),dmulti3) 
I((y - 45.796)/1725) 0.0002963 5.5482

which is what svymean() gives for the standard error of the mean of y. 
Using your formula for the variance of the total (with the corrections 
above) on this variable also gives
> sqrt(var.yT)
[1] 5.54824


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From spencer.graves at pdf.com  Sat Jul  8 02:10:35 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 07 Jul 2006 17:10:35 -0700
Subject: [R] Harmonic Regression in R
In-Reply-To: <20060703084131.34811.qmail@web53811.mail.yahoo.com>
References: <20060703084131.34811.qmail@web53811.mail.yahoo.com>
Message-ID: <44AEF7FB.4090101@pdf.com>

	  Since I haven't seen an answer to this, I'll offer a couple of 
comments.  I don't recall having heard the term 'harmonic regression' 
prior to seeing your email, but it sounded interesting, so I did some 
searching.  First, RSiteSearch("harminic regression") produced 7 hits, 
one of which discusses the 'cyclones' package, which may be what you want:

http://finzi.psych.upenn.edu/R/library/cyclones/html/00Index.html

	  Second, for the benefit of folks like me who aren't sure of the 
definition, it appears to be linear regression on sines and cosines of 
something like 'time'.  The following link describes how to do this 
using 'lm' in S-Plus or R:

http://www.math.jmu.edu/~tomitayx/math328/Ch6SlideD.pdf

	  Hope this helps.
	  Spencer Graves

Airon Yiu wrote:
> Dear all:
>    
>   Does anyone has harmonic regresssion analysis package written in R (to be used in Windows platform) ?
>    
>   Thanks
> 
>  _______________________________________
>  YM - ???u?T??
>  ?N???A?S???W???A?A???B?????i?H?d?U?T?????A?A???A?W?????N?????Y?????A???????????N?????C
>  http://messenger.yahoo.com.hk
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ggrothendieck at gmail.com  Sat Jul  8 02:36:45 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 7 Jul 2006 20:36:45 -0400
Subject: [R] Harmonic Regression in R
In-Reply-To: <44AEF7FB.4090101@pdf.com>
References: <20060703084131.34811.qmail@web53811.mail.yahoo.com>
	<44AEF7FB.4090101@pdf.com>
Message-ID: <971536df0607071736i6e9352f3l15d9ea3e9d60d95f@mail.gmail.com>

I have not seen it myself (I have the first edition which uses FORTRAN)
but I believe the second edition of Peter Bloomfield's book on Fourier
Analysis contains harmonic regression code in S-Plus and that may
work in R.

On 7/7/06, Spencer Graves <spencer.graves at pdf.com> wrote:
>          Since I haven't seen an answer to this, I'll offer a couple of
> comments.  I don't recall having heard the term 'harmonic regression'
> prior to seeing your email, but it sounded interesting, so I did some
> searching.  First, RSiteSearch("harminic regression") produced 7 hits,
> one of which discusses the 'cyclones' package, which may be what you want:
>
> http://finzi.psych.upenn.edu/R/library/cyclones/html/00Index.html
>
>          Second, for the benefit of folks like me who aren't sure of the
> definition, it appears to be linear regression on sines and cosines of
> something like 'time'.  The following link describes how to do this
> using 'lm' in S-Plus or R:
>
> http://www.math.jmu.edu/~tomitayx/math328/Ch6SlideD.pdf
>
>          Hope this helps.
>          Spencer Graves
>
> Airon Yiu wrote:
> > Dear all:
> >
> >   Does anyone has harmonic regresssion analysis package written in R (to be used in Windows platform) ?
> >
> >   Thanks
> >
> >  _______________________________________
> >  YM - ???u?T??
> >  ?N???A?S???W???A?A???B?????i?H?d?U?T?????A?A???A?W?????N?????Y?????A???????????N?????C
> >  http://messenger.yahoo.com.hk
> >       [[alternative HTML version deleted]]
> >
> >
> >
> > ------------------------------------------------------------------------
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From r_o_sanchez2003 at yahoo.com.ar  Sat Jul  8 02:39:13 2006
From: r_o_sanchez2003 at yahoo.com.ar (raul sanchez)
Date: Fri, 7 Jul 2006 21:39:13 -0300 (ART)
Subject: [R] KhmaladzeTest
Message-ID: <20060708003913.11112.qmail@web30610.mail.mud.yahoo.com>

Hello. I am a beginer in R and I can not implement the KhmaladzeTest in the following command. Please help me!!!!!!!!!!!
  PD: I attach thw results and the messages of the R program
   
  R : Copyright 2006, The R Foundation for Statistical Computing
Version 2.3.1 (2006-06-01)
ISBN 3-900051-07-0

R es un software libre y viene sin GARANTIA ALGUNA.
Usted puede redistribuirlo bajo ciertas circunstancias.
Escriba 'license()' o 'licence()' para detalles de distribucion.

R es un proyecto colaborativo con muchos contribuyentes.
Escriba 'contributors()' para obtener mas informacion y
'citation()' para saber como citar R o paquetes de R en publicaciones.

Escriba 'demo()' para demostraciones, 'help()' para el sistema on-line 
de ayuda,
o 'help.start()' para abrir el sistema de ayuda HTML con su navegador.
Escriba 'q()' para salir de R.

> utils:::menuInstallLocal()
package 'quantreg' successfully unpacked and MD5 sums checked
updating HTML package descriptions
> utils:::menuInstallLocal()
package 'foreign' successfully unpacked and MD5 sums checked
updating HTML package descriptions
> utils:::menuInstallLocal()
package 'Rcmdr' successfully unpacked and MD5 sums checked
updating HTML package descriptions
> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
+ if(nchar(pkg)) library(pkg, character.only=TRUE)})
> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
+ if(nchar(pkg)) library(pkg, character.only=TRUE)})
quantreg package loaded:  To cite see citation("quantreg")
> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
+ if(nchar(pkg)) library(pkg, character.only=TRUE)})
> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
+ if(nchar(pkg)) library(pkg, character.only=TRUE)})
Loading required package: tcltk
Loading Tcl/Tk interface ... done
--- Please select a CRAN mirror for use in this session ---
also installing the dependencies 'acepack', 'scatterplot3d', 'fBasics', 
'Hmisc', 'quadprog', 'oz', 'mlbench', 'randomForest', 'SparseM', 
'xtable', 'chron', 'fCalendar', 'its', 'tseries', 'DAAG', 'e1071', 'mvtnorm', 
'zoo', 'strucchange', 'sandwich', 'dynlm', 'leaps'

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/acepack_1.3-2.2.zip'
Content 
type 'application/zip' length 55667 bytes
URL abierta
downloaded 54Kb

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/scatterplot3d_0.3-24.zip'
Content 
type 'application/zip' length 540318 bytes
URL abierta
downloaded 527Kb

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/fBasics_221.10065.zip'
Content 
type 'application/zip' length 3327499 bytes
URL abierta
downloaded 3249Kb

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/Hmisc_3.0-12.zip'
Content 
type 'application/zip' length 1993038 bytes
URL abierta
downloaded 1946Kb

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/quadprog_1.4-8.zip'
Content 
type 'application/zip' length 38626 bytes
URL abierta
downloaded 37Kb

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/oz_1.0-13.zip'
Content 
type 'application/zip' length 39640 bytes
URL abierta
downloaded 38Kb

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/mlbench_1.1-1.zip'
Content 
type 'application/zip' length 1324913 bytes
URL abierta
downloaded 1293Kb

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/randomForest_4.5-16.zip'
Content 
type 'application/zip' length 209710 bytes
URL abierta
downloaded 204Kb

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/SparseM_0.68.zip'
Content 
type 'application/zip' length 728594 bytes
URL abierta
downloaded 711Kb

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/xtable_1.3-2.zip'
Content 
type 'application/zip' length 56703 bytes
URL abierta
downloaded 55Kb

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/chron_2.3-4.zip'
Content 
type 'application/zip' length 101287 bytes
URL abierta
downloaded 98Kb

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/fCalendar_221.10065.zip'
Content 
type 'application/zip' length 754551 bytes
URL abierta
downloaded 736Kb

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/its_1.1.1.zip'
Content 
type 'application/zip' length 194287 bytes
URL abierta
downloaded 189Kb

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/tseries_0.10-3.zip'
Content 
type 'application/zip' length 392748 bytes
URL abierta
downloaded 383Kb

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/DAAG_0.79.zip'
Content 
type 'application/zip' length 796328 bytes
URL abierta
downloaded 777Kb

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/e1071_1.5-13.zip'
Content 
type 'application/zip' length 627972 bytes
URL abierta

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/mvtnorm_0.7-2.zip'
Content 
type 'application/zip' length 214609 bytes
URL abierta

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/zoo_1.1-0.zip'
Content 
type 'application/zip' length 695387 bytes
URL abierta

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/strucchange_1.2-13.zip'
Content 
type 'application/zip' length 866668 bytes
URL abierta

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/sandwich_1.9-0.zip'
Content 
type 'application/zip' length 675607 bytes
URL abierta

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/dynlm_0.1-2.zip'
Content 
type 'application/zip' length 43966 bytes
URL abierta

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/leaps_2.7.zip'
Content 
type 'application/zip' length 73301 bytes
URL abierta

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/relimp_0.9-6.zip'
Content 
type 'application/zip' length 70677 bytes
URL abierta

probando la URL 
'http://cran.au.r-project.org/bin/windows/contrib/2.3/multcomp_0.4-8.zip'
Content 
type 'application/zip' length 310081 bytes
URL abierta



Versi?n del Rcmdr 1.1-7 
> Datos <- read.dta("C:/Mis documentos/r1-r1.dta", convert.dates=TRUE, 
convert.factors=TRUE, missing.type=TRUE, convert.underscore=TRUE, 
warn.missing.labels=TRUE)
Aviso en read.dta("C:/Mis documentos/r1-r1.dta", convert.dates = TRUE,  
: 
         'missing.type' only applicable to version 8 files
> fit1 <- rq(yyy ~ lll, tau = 0.5, data = Datos)
> fit1
Call:
rq(formula = yyy ~ lll, tau = 0.5, data = Datos)

Coefficients:
(Intercept)         lll 
   4.249606   -0.929411 

Degrees of freedom: 36 total; 34 residual
> summary(fit1)

Call: rq(formula = yyy ~ lll, tau = 0.5, data = Datos)

tau: [1] 0.5

Coefficients:
            coefficients lower bd upper bd
(Intercept)  4.24961      2.36774  4.95617
lll         -0.92941     -1.08243 -0.52124
> r1 <- resid(fit1)
> c1 <- coef(fit1)
> summary(fit1, se = "nid")

Call: rq(formula = yyy ~ lll, tau = 0.5, data = Datos)

tau: [1] 0.5

Coefficients:
            Value    Std. Error t value  Pr(>|t|)
(Intercept)  4.24961  1.05525    4.02710  0.00046
lll         -0.92941  0.22934   -4.05246  0.00043
> fit <- rqProcess( y.net ~ lgdp2 + fse2 + gedy2 + Iy2 + gcony2, 
+                 data = barro, taus = seq(.1,.9,by = .05))
Erro en inherits(x, "data.frame") : objeto "barro" no encontrado
> fit <- rqProcess(yyy ~ lll, data = Datos, taus = seq(.1,.9, by = 
.05))
taus: 0.1  Erro en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
        tau - h < 0:  error in summary.rq
> f <- rqProcess(yyy ~ lll, data = Datos, taus = seq(.1,.9, by = .05))
taus: 0.1  Erro en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
        tau - h < 0:  error in summary.rq
> fit1 <- rqProcess(yyy ~ lll, data = Datos, taus = seq(.1,.9, by = 
.05))
taus: 0.1  Erro en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
        tau - h < 0:  error in summary.rq
> T1 <- KhmaladzeTest(yyy ~ lll, taus = -1, nullH = "location")
Erro en eval(expr, envir, enclos) : objeto "yyy" no encontrado
> Datos <- read.dta("C:/Mis documentos/r1-r1.dta", convert.dates=TRUE, 
convert.factors=TRUE, missing.type=TRUE, convert.underscore=TRUE, 
warn.missing.labels=TRUE)
Aviso en read.dta("C:/Mis documentos/r1-r1.dta", convert.dates = TRUE,  
: 
         'missing.type' only applicable to version 8 files
> T1 <- KhmaladzeTest(yyy ~ lll, taus = -1, nullH = "location")
Erro en eval(expr, envir, enclos) : objeto "yyy" no encontrado
> T1 <- KhmaladzeTest( yyy ~ lll, taus = -1, nullH = "location")
Erro en eval(expr, envir, enclos) : objeto "yyy" no encontrado
> data(barro)
> invisible(edit(barro))
> rqProcess(yyy ~ lll, data = Datos, taus = -1, nullH = "location")
$taus
[1] 0.3161186 0.3543310 0.4007676 0.4150787 0.4268406 0.4542215 
0.5008570 0.5304535 0.5690997
[10] 0.6206522 0.6522563

$qtaus
[1] -0.037757189 -0.033227447 -0.032656681 -0.016152701 -0.015090004 
-0.014413058
[7] -0.013738239 -0.013471008 -0.012253429 -0.010675005 -0.009866834

$Vhat
          [,1]      [,2]      [,3]       [,4]       [,5]       [,6]       
[,7]       [,8]
[1,] 0.3064789 0.5104205 0.6209815 -0.0850221 -0.2053440 -0.1631580 
-0.1766553 -0.1768334
           [,9]      [,10]      [,11]
[1,] -0.1483747 -0.1191922 -0.1026903

$vhat
          [,1]      [,2]      [,3]       [,4]       [,5]       [,6]       
[,7]       [,8]
[1,] 0.3064789 0.5104205 0.6209815 -0.0850221 -0.2053440 -0.1631580 
-0.1766553 -0.1768334
           [,9]      [,10]      [,11]
[1,] -0.1483747 -0.1191922 -0.1026903

attr(,"class")
[1] "rqProcess"
> rqProcess(yyy ~ lll, data = Datos,taus = seq(.1,.9, by = .05), nullH 
= "location")
taus: 0.1  Erro en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
        tau - h < 0:  error in summary.rq
> KhmaladzeTest( yyy ~ lll, data = Datos, taus = seq(.1,.9, by = .05))
taus: 0.1  Erro en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
        tau - h < 0:  error in summary.rq
> T1 <- KhmaladzeTest(yyy ~ lll, taus = -1, nullH = "location")
Erro en eval(expr, envir, enclos) : objeto "yyy" no encontrado
> fit1 <- rqProcess(yyy ~ lll, data = Datos, taus = seq(.1,.9, by = 
.05))
taus: 0.1  Erro en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
        tau - h < 0:  error in summary.rq
> KhmaladzeTest( yyy ~ lll, data = Datos, taus = -1)
Erro en qr(X[1:p, ]) : NA/NaN/Inf en llamada a una funci?n externa (arg 
1)
> KhmaladzeTest( yyy ~ lll, data = Datos, taus = -1, nullH = 
"location")
Erro en qr(X[1:p, ]) : NA/NaN/Inf en llamada a una funci?n externa (arg 
1)
> data(barro)
> KhmaladzeTest( y.net ~ lgdp2 + fse2 + gedy2 + Iy2 + gcony2,
+ data = barro, taus = seq(.05,.95,by = .01))
taus: 0.05  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         1 non-positive fis
0.06  0.07  0.08  0.09  Aviso en summary.rq(rq(formula, data = data, 
tau = taus[i], method = "fn"),  : 
         1 non-positive fis
0.1  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], method 
= "fn"),  : 
         1 non-positive fis
0.11  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         2 non-positive fis
0.12  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         3 non-positive fis
0.13  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         3 non-positive fis
0.14  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         4 non-positive fis
0.15  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         3 non-positive fis
0.16  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         5 non-positive fis
0.17  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         4 non-positive fis
0.18  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         5 non-positive fis
0.19  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         4 non-positive fis
0.2  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], method 
= "fn"),  : 
         4 non-positive fis
0.21  0.22  0.23  0.24  0.25  Aviso en summary.rq(rq(formula, data = 
data, tau = taus[i], method = "fn"),  : 
         1 non-positive fis
0.26  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         1 non-positive fis
0.27  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         1 non-positive fis
0.28  0.29  0.3  0.31  0.32  0.33  Aviso en summary.rq(rq(formula, data 
= data, tau = taus[i], method = "fn"),  : 
         1 non-positive fis
0.34  0.35  0.36  0.37  0.38  0.39  0.4  0.41  0.42  0.43  0.44  0.45  
0.46  0.47  0.48  0.49  0.5  0.51  0.52  0.53  0.54  0.55  0.56  0.57  
Aviso en summary.rq(rq(formula, data = data, tau = taus[i], method = 
"fn"),  : 
         1 non-positive fis
0.58  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         2 non-positive fis
0.59  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         1 non-positive fis
0.6  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], method 
= "fn"),  : 
         1 non-positive fis
0.61  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         1 non-positive fis
0.62  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         1 non-positive fis
0.63  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         1 non-positive fis
0.64  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         1 non-positive fis
0.65  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         1 non-positive fis
0.66  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         1 non-positive fis
0.67  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         1 non-positive fis
0.68  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         1 non-positive fis
0.69  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         2 non-positive fis
0.7  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], method 
= "fn"),  : 
         2 non-positive fis
0.71  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         2 non-positive fis
0.72  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         1 non-positive fis
0.73  0.74  0.75  0.76  0.77  0.78  0.79  Aviso en 
summary.rq(rq(formula, data = data, tau = taus[i], method = "fn"),  : 
         2 non-positive fis
0.8  0.81  0.82  0.83  0.84  Aviso en summary.rq(rq(formula, data = 
data, tau = taus[i], method = "fn"),  : 
         2 non-positive fis
0.85  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         3 non-positive fis
0.86  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         3 non-positive fis
0.87  0.88  0.89  Aviso en summary.rq(rq(formula, data = data, tau = 
taus[i], method = "fn"),  : 
         1 non-positive fis
0.9  0.91  0.92  Aviso en summary.rq(rq(formula, data = data, tau = 
taus[i], method = "fn"),  : 
         2 non-positive fis
0.93  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         9 non-positive fis
0.94  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         13 non-positive fis
0.95  Aviso en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
         14 non-positive fis
$nullH
[1] "location"

$Tn
[1] 6.029988

$THn
    lgdp2      fse2     gedy2       Iy2    gcony2 
1.3198411 1.1505847 1.2480321 0.8466474 1.0036528 

attr(,"class")
[1] "KhmaladzeTest"
> data(Datos)
Aviso en data(Datos) : data set 'Datos' not found
> Datos <- read.dta("C:/Mis documentos/r1-r1.dta", convert.dates=TRUE, 
convert.factors=TRUE, missing.type=TRUE, convert.underscore=TRUE, 
warn.missing.labels=TRUE)
Aviso en read.dta("C:/Mis documentos/r1-r1.dta", convert.dates = TRUE,  
: 
         'missing.type' only applicable to version 8 files
> data(Datos)
Aviso en data(Datos) : data set 'Datos' not found
> Datos <- read.dta("C:/Mis documentos/r1-r1.dta", convert.dates=TRUE, 
convert.factors=TRUE, missing.type=TRUE, convert.underscore=TRUE, 
warn.missing.labels=TRUE)
Aviso en read.dta("C:/Mis documentos/r1-r1.dta", convert.dates = TRUE,  
: 
         'missing.type' only applicable to version 8 files
> KhmaladzeTest( yyy ~ lll, data = Datos, taus = -1, nullH = 
"location")
Erro en qr(X[1:p, ]) : NA/NaN/Inf en llamada a una funci?n externa (arg 
1)
> invisible(edit(Datos))
> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
+ if(nchar(pkg)) library(pkg, character.only=TRUE)})
> fit <- rqProcess( yyy ~ lll, data = Datos, taus = seq(.1,.9,by = 
.05))

> fff <- rqProcess( yyy ~ lll, data = Datos, taus = seq(.1,.9,by = 
.05))
taus: 0.1  Erro en summary.rq(rq(formula, data = data, tau = taus[i], 
method = "fn"),  : 
        tau - h < 0:  error in summary.rq
> fff <- rqProcess(yyy ~ lll, data = Datos, taus = -1, nullH = 
"location")
> khmaladze.test(fit, nullH = "location")
Error: no se pudo encontrar la funci?n "khmaladze.test"
> khmaladze.test(fff, nullH = "location")
Error: no se pudo encontrar la funci?n "khmaladze.test"
> Khmaladze.test(fff, nullH = "location")
Error: no se pudo encontrar la funci?n "Khmaladze.test"
> KhmaladzeTest(fff, nullH = "location")
Erro en terms.default(formula, data = data) : 
        no terms component
> KhmaladzeTest(fff,data = Datos)
Erro en terms.default(formula, data = data) : 
        no terms component
> KhmaladzeTest(fff, data = Datos)
Erro en terms.default(formula, data = data) : 
        no terms component
> KhmaladzeTest(fff, data = Datos, nullH = "location")
Erro en terms.default(formula, data = data) : 
        no terms component
> khmaladze.test(fit, nullH = "location")
Error: no se pudo encontrar la funci?n "khmaladze.test"
> KhmaladzeTest(fff, data = Datos, taus = -1, nullH = "location")
Erro en terms.default(formula, data = data) : 
        no terms component
> help.search("KhmaladzeTest")
> KhmaladzeTest( yyy  ~ lll, data = Datos, taus = -1, nullH = 
"location" ,  trim = c(0.05, 0.95)) 
Erro en qr(X[1:p, ]) : NA/NaN/Inf en llamada a una funci?n externa (arg 
1)
> KhmaladzeTest( fff, data = Datos, taus = -1, nullH = "location" ,  
trim = c(0.05, 0.95)) 
Erro en terms.default(formula, data = data) : 
        no terms component
> ls()
[1] "barro" "c1"    "Datos" "fff"   "fit1"  "GLM.1" "r1"   
> summary(fff)
      Length Class  Mode   
taus  11     -none- numeric
qtaus 11     -none- numeric
Vhat  11     -none- numeric
vhat  11     -none- numeric
> KhmaladzeTest(formula, data = NUL, taus = -1, nullH = "location"

+ > 
> KhmaladzeTest(fff, data = Datos, taus = -1, nullH = "location")
Erro en terms.default(formula, data = data) : 
        no terms component
> KhmaladzeTest(fff, data = Datos)
Erro en terms.default(formula, data = data) : 
        no terms component
> KhmaladzeTest( fff, data = Datos, taus = -1, nullH = "location" ,  
trim = c(0.05, 0.95))
Erro en terms.default(formula, data = data) : 
        no terms component
> KhmaladzeTest( fit1, data = Datos, taus = -1, nullH = "location" ,  
trim = c(0.05, 0.95))
Erro en qr(X[1:p, ]) : NA/NaN/Inf en llamada a una funci?n externa (arg 
1)
> fix(Datos)


 		
---------------------------------
 1GB gratis, Antivirus y Antispam
 Correo Yahoo!, el mejor correo web del mundo
 Abr? tu cuenta aqu?

From vsdimitrov at yahoo.com  Sat Jul  8 02:45:57 2006
From: vsdimitrov at yahoo.com (Valentin Dimitrov)
Date: Fri, 7 Jul 2006 17:45:57 -0700 (PDT)
Subject: [R] parametric proportional hazard regression
In-Reply-To: <Pine.LNX.4.64.0607070846180.9075@homer23.u.washington.edu>
Message-ID: <20060708004557.60360.qmail@web30803.mail.mud.yahoo.com>



> 
> Those are not proportional hazards families of
> distributions. That is, if 
> the distribution is gaussian for one value of the
> hazard ratio parameters 
> it will not be gaussian for any other value.
> 
> You can get accelerated failure models with these
> distributions using 
> survreg() in the survival package.
> 
> 
>  	-thomas


I do not need a accelerated failure model, but a
proportional hazard model with a f0= weibull,
exponential, loglogistic or lognormal baseline
distribution. The hazard function is
lambda(t)=exp(Xi*beta)*lambda0(t),
where lambda0 is the baseline hazard
lambda0(t)=f0(t)/(1-F0(t)) where f0 and F0 are the
baseline density and cumulative distribution
functions.
This is a proportional hazard model since the ratio
lambda(t|Xi)/lambda(t|Xj)=exp(Xi*beta)/exp(Xj*beta)
does not depend on t.

Valentin


From vsdimitrov at yahoo.com  Sat Jul  8 02:49:16 2006
From: vsdimitrov at yahoo.com (Valentin Dimitrov)
Date: Fri, 7 Jul 2006 17:49:16 -0700 (PDT)
Subject: [R] parametric proportional hazard regression
In-Reply-To: <E52E20F6B4A2F548B16BB259DD5168CF02FB681B@BHDAEXCH11.bhcs.pvt>
Message-ID: <20060708004916.61340.qmail@web30803.mail.mud.yahoo.com>

Cody,

I have tried the survreg() in the Design library, it
is analogous to survreg() in the survival library and
it seems to me it is designed only for accelerated
time models like the accelerated failure model (or
accelerated lifetime model) and not for proportional
hazard models. Correct me if I am wrong.

Valentin

--- "Hamilton, Cody" <CodyH at BaylorHealth.edu> wrote:

> 
> Valentin,
> 
> Have you tried survreg() in the Design library?
> 
> Regards,
>    -Cody
>


From spencer.graves at pdf.com  Sat Jul  8 03:35:57 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 07 Jul 2006 18:35:57 -0700
Subject: [R] Problems when computing the 1rst derivative of mixtures of
 densities
In-Reply-To: <67a23b050607040811s49075148h4034be1350a3a337@mail.gmail.com>
References: <67a23b050607040811s49075148h4034be1350a3a337@mail.gmail.com>
Message-ID: <44AF0BFD.8010504@pdf.com>

	  What problem are you trying to solve?  The mixture proportion you are 
trying to estimate violates the assumptions that provide a normal 
approximation to the distribution of maximum likelihood estimators. 
These situations even violate the assumptions for 2*log(likelihood 
ratio) being approximately chi-square.

	  If you want hypothesis tests or confidence intervals, Pinheiro and 
Bates (2000) recommended using 2*log(likelihood ratio) modified as 
suggested by Monte Carlo results.  More recently, Bates has incorporated 
an 'mcmcsamp' function in the 'lme4' and 'Matrix' libraries.

	  The best references I know on testing a parameter at a boundary are 
the following:

Pinheiro and Bates (2000) Mixed-Effects Models in S and S-PLUS 
(Springer, sec. 2.4)

Crainiceanu, Ruppert and Vogelsang (2003) ?Some properties of Likelihood 
Ratio tests in linear mixed models?

Crainiceanu, Ruppert, Claeskens, and Wand (2005) ?Exact Likelihood Ratio 
Tests for Penalized Splines?, Biometrika, 92(1)

	  Hope this helps.
	  Spencer Graves
p.s.  See also inline.

Cl?ment Viel wrote:
> Hi everybody,
> 
> I am currently working on mixtures of two densities ( f(xi,teta)=
> (1-teta)*f1(xi) + teta*f2(xi) ),
> particularly on the behavior of the variance for teta=0 (so sample only
> comes from the first distribution).
> To determine the maximum likelihood estimator I use the Newton-Rapdon
> Iteration. But when
> computing the first derivative I get a none linear function (with several
> asymptotes) which is completely
> absurd.

Why do you think this is absurd?  Nonlinear estimation can be very 
difficult.

	  Also, have you reviewed the capabilities in R for working with 
mixtures of distributions?  I just got 167 hits for 
RSiteSearch("mixtures") and 49 for RSiteSearch("mixture distributions", 
"functions").
> 
> This is my function to compute the first derivative:
> 
> phy=function(teta,vect1,vect2){
>     return( sum(( vect2 - vect1) / (( 1 - teta) * vect1 + teta * vect2)))
> }
> 
> note: vect1 and vect2 contains  values of the  two distributions computed
> from sample previously extracted.
> 
> Beside, vect2 - vect1 is constant and ( 1 - teta) * vect1 + teta * vect2) is
> linear and always defined so I am expecting
> a linear function for the first derivative. To my mind, it's likely comes
> from the operation of division but I don't understand
> why results are skewed.
> 
> Have you got any suggestions, please?
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From f.harrell at vanderbilt.edu  Sat Jul  8 04:21:48 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 07 Jul 2006 21:21:48 -0500
Subject: [R] parametric proportional hazard regression
In-Reply-To: <20060708004557.60360.qmail@web30803.mail.mud.yahoo.com>
References: <20060708004557.60360.qmail@web30803.mail.mud.yahoo.com>
Message-ID: <44AF16BC.4060304@vanderbilt.edu>

Valentin Dimitrov wrote:
> 
>> Those are not proportional hazards families of
>> distributions. That is, if 
>> the distribution is gaussian for one value of the
>> hazard ratio parameters 
>> it will not be gaussian for any other value.
>>
>> You can get accelerated failure models with these
>> distributions using 
>> survreg() in the survival package.
>>
>>
>>  	-thomas
> 
> 
> I do not need a accelerated failure model, but a
> proportional hazard model with a f0= weibull,
> exponential, loglogistic or lognormal baseline
> distribution. The hazard function is
> lambda(t)=exp(Xi*beta)*lambda0(t),
> where lambda0 is the baseline hazard
> lambda0(t)=f0(t)/(1-F0(t)) where f0 and F0 are the
> baseline density and cumulative distribution
> functions.
> This is a proportional hazard model since the ratio
> lambda(t|Xi)/lambda(t|Xj)=exp(Xi*beta)/exp(Xj*beta)
> does not depend on t.
> 
> Valentin

That will be a very strange model that I've never seen used before in 
survival analysis.  Interpretation of parameters other than the hazard 
ratio may be tricky.  Why do you need a nontraditional model such as this?

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From f.harrell at vanderbilt.edu  Sat Jul  8 04:22:36 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 07 Jul 2006 21:22:36 -0500
Subject: [R] parametric proportional hazard regression
In-Reply-To: <20060708004916.61340.qmail@web30803.mail.mud.yahoo.com>
References: <20060708004916.61340.qmail@web30803.mail.mud.yahoo.com>
Message-ID: <44AF16EC.1040901@vanderbilt.edu>

Valentin Dimitrov wrote:
> Cody,
> 
> I have tried the survreg() in the Design library, it
> is analogous to survreg() in the survival library and
> it seems to me it is designed only for accelerated
> time models like the accelerated failure model (or
> accelerated lifetime model) and not for proportional
> hazard models. Correct me if I am wrong.
> 
> Valentin

survreg does not exist in the Design package.  You may be thinking of 
psm which is a wrapper function for survreg.

> 
> --- "Hamilton, Cody" <CodyH at BaylorHealth.edu> wrote:
> 
>> Valentin,
>>
>> Have you tried survreg() in the Design library?
>>
>> Regards,
>>    -Cody
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From Regina_Rendas-Baum at brown.edu  Sat Jul  8 05:12:37 2006
From: Regina_Rendas-Baum at brown.edu (Rendas-Baum, Regina)
Date: Fri, 7 Jul 2006 23:12:37 -0400
Subject: [R] heteroskedastic ordered probit
Message-ID: <91C1AD5E37A96E42B8A200E334370A9C0495A211@MAIL2.AD.Brown.Edu>

Dear all,
 
is there existing code to fit a ordinal regression model with probit link where the variance can be modeled? For example, the polr function from the MASS package fits:
 
probitP(Y <= k | x) = zeta_k - eta

Instead of,
 
probitP(Y <= k | x) = (zeta_k - eta)/scale, where scale = exp(b*z) 
 
which is what I need.
 
I've seen these models go by the name of heteroskedastic ordered probit, location-scale glms, generalized nonlinear models, ... .  I know the package gnlm (Jim Lindsey) has a function to fit this model using the logit link, so I'm wondering if this is already done in R using probit instead.
 
Thanks.
 
Regina


From bgreen at dyson.brisnet.org.au  Sat Jul  8 11:14:38 2006
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Sat, 08 Jul 2006 19:14:38 +1000
Subject: [R] can rows of text be repeated using R to create a new datafile
Message-ID: <5.1.0.14.0.20060702162305.00c4e7d8@pop3.brisnet.org.au>

I am hoping for some advice as to whether the following task can be 
performed in R?

I have a spreadsheet with 325 rows x 3 columns. The data was collected from 
40 individuals who in most instances generated 8 bi-polar constructs  (the 
poles are separated by //). The first column is the construct number = 
1  to 325. The second is an id number =1 to 40; the third column is the 
data to be coded.  Because persons 1, 18 & 40 generated 9 rows of data, and 
person 5, 10 rows; rows =325 not 320.

What I want to do is repeat each row a set number of times - hopefully 4 
(the number of coders to code the data). Once repeated there would be 4 
conids x 1 , 4  x 2 etc 
. 4x 325.


Currently this is how the first 2 rows look:

conid	person	construct
1	1	offence against property  // offence against person
2	1	insight & cooperative with treatment //  lack insight

This is how I want them to look after editing.

conid	person	construct
1	1	offence against property  // offence against person
1	1	offence against property  // offence against person
1	1	offence against property  // offence against person
1	1	offence against property  // offence against person
2	1	insight & cooperative with treatment //  lack insight
2	1	insight & cooperative with treatment //  lack insight
2	1	insight & cooperative with treatment //  lack insight
2	1	insight & cooperative with treatment //  lack insight
2	1	insight & cooperative with treatment //  lack insight



Any assistance is much appreciated,

Bob


From ggrothendieck at gmail.com  Sat Jul  8 12:42:46 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 8 Jul 2006 06:42:46 -0400
Subject: [R] can rows of text be repeated using R to create a new
	datafile
In-Reply-To: <5.1.0.14.0.20060702162305.00c4e7d8@pop3.brisnet.org.au>
References: <5.1.0.14.0.20060702162305.00c4e7d8@pop3.brisnet.org.au>
Message-ID: <971536df0607080342p6940320ay1ab7ba115777aec9@mail.gmail.com>

Open Excel, select the data and press ctrl-C.
In R, type this line at the console to
read the clipboard into a data frame DF (or
see the section on Excel in the R Import/Export
manual for other ways):

  DF <- read.delim("clipboard", header = TRUE)

Now you can do this:

   DF[rep(1:nrow(DF), each = 4),]

On 7/8/06, Bob Green <bgreen at dyson.brisnet.org.au> wrote:
> I am hoping for some advice as to whether the following task can be
> performed in R?
>
> I have a spreadsheet with 325 rows x 3 columns. The data was collected from
> 40 individuals who in most instances generated 8 bi-polar constructs  (the
> poles are separated by //). The first column is the construct number =
> 1  to 325. The second is an id number =1 to 40; the third column is the
> data to be coded.  Because persons 1, 18 & 40 generated 9 rows of data, and
> person 5, 10 rows; rows =325 not 320.
>
> What I want to do is repeat each row a set number of times - hopefully 4
> (the number of coders to code the data). Once repeated there would be 4
> conids x 1 , 4  x 2 etc ?. 4x 325.
>
>
> Currently this is how the first 2 rows look:
>
> conid   person  construct
> 1       1       offence against property  // offence against person
> 2       1       insight & cooperative with treatment //  lack insight
>
> This is how I want them to look after editing.
>
> conid   person  construct
> 1       1       offence against property  // offence against person
> 1       1       offence against property  // offence against person
> 1       1       offence against property  // offence against person
> 1       1       offence against property  // offence against person
> 2       1       insight & cooperative with treatment //  lack insight
> 2       1       insight & cooperative with treatment //  lack insight
> 2       1       insight & cooperative with treatment //  lack insight
> 2       1       insight & cooperative with treatment //  lack insight
> 2       1       insight & cooperative with treatment //  lack insight
>
>
>
> Any assistance is much appreciated,
>
> Bob


From vsdimitrov at yahoo.com  Sat Jul  8 14:24:05 2006
From: vsdimitrov at yahoo.com (Valentin Dimitrov)
Date: Sat, 8 Jul 2006 05:24:05 -0700 (PDT)
Subject: [R] parametric proportional hazard regression
In-Reply-To: <44AF16BC.4060304@vanderbilt.edu>
Message-ID: <20060708122405.89318.qmail@web30804.mail.mud.yahoo.com>



--- Frank E Harrell Jr <f.harrell at vanderbilt.edu>
wrote:


> That will be a very strange model that I've never
> seen used before in 
> survival analysis.  Interpretation of parameters
> other than the hazard 
> ratio may be tricky.  Why do you need a
> nontraditional model such as this?
> 
> Frank
> 
> -- 
> Frank E Harrell Jr   Professor and Chair          
> School of Medicine
>                       Department of Biostatistics  
> Vanderbilt University
> 


I am new to the field of survival analysis and my
first  text read on that topic is Franses/Paap,
Quantitative Models in Marketing Research, Chapter 8.
There the kind of models I describe are the standard
proportional hazard models. But also in Internet I
find texts which say something about parametric PH
models, where the baseline hazard can take some
specific form! See for instance:
http://www.weibull.com/AccelTestWeb/proportional_hazards_model.htm
The second part reads "parametric PH models" and there
is an example with the Weibull baseline distribution.
The log-likelihood is derived, too. The same could be
done with the other distributions I mentioned
(lognormal, loglogistic, exponential). 
Anyway, if you say these models are untraditional,
then maybe there is also no R-function for them...
In that case I'll write a R-code myself to fit them
using ML.

Kind regards,
Valentin


From ggrothendieck at gmail.com  Sat Jul  8 14:50:44 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 8 Jul 2006 08:50:44 -0400
Subject: [R] parametric proportional hazard regression
In-Reply-To: <20060708122405.89318.qmail@web30804.mail.mud.yahoo.com>
References: <44AF16BC.4060304@vanderbilt.edu>
	<20060708122405.89318.qmail@web30804.mail.mud.yahoo.com>
Message-ID: <971536df0607080550s4f73e342u5b15fa2d06d2a789@mail.gmail.com>

There is some generalized F code (which would
include many other parameteric survival models as
submodels) at this page.  The R package seems
somewhat non-standard in terms of installation
and I have not tried it:
http://www.math.mun.ca/~ypeng/research/

On 7/8/06, Valentin Dimitrov <vsdimitrov at yahoo.com> wrote:
>
> I am new to the field of survival analysis and my
> first  text read on that topic is Franses/Paap,
> Quantitative Models in Marketing Research, Chapter 8.
> There the kind of models I describe are the standard
> proportional hazard models. But also in Internet I
> find texts which say something about parametric PH
> models, where the baseline hazard can take some
> specific form! See for instance:
> http://www.weibull.com/AccelTestWeb/proportional_hazards_model.htm
> The second part reads "parametric PH models" and there
> is an example with the Weibull baseline distribution.
> The log-likelihood is derived, too. The same could be
> done with the other distributions I mentioned
> (lognormal, loglogistic, exponential).
> Anyway, if you say these models are untraditional,
> then maybe there is also no R-function for them...
> In that case I'll write a R-code myself to fit them
> using ML.
>
> Kind regards,
> Valentin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From coforfe at gmail.com  Sat Jul  8 15:39:24 2006
From: coforfe at gmail.com (Carlos Ortega)
Date: Sat, 8 Jul 2006 15:39:24 +0200
Subject: [R] Availability of quadplot3d package (UseR!2006 Four Dimensional
	Barycentric Plots in 3D)
Message-ID: <7b18cd4d0607080639k429570fdn15712463de2507b7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060708/27938ff5/attachment.pl 

From roger at ysidro.econ.uiuc.edu  Sat Jul  8 16:38:18 2006
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Sat, 8 Jul 2006 09:38:18 -0500
Subject: [R] KhmaladzeTest
In-Reply-To: <20060708003913.11112.qmail@web30610.mail.mud.yahoo.com>
References: <20060708003913.11112.qmail@web30610.mail.mud.yahoo.com>
Message-ID: <7B2E26E4-9C4F-4420-9865-1765F4814B81@ysidro.econ.uiuc.edu>

Questions about packages should be directed to the package maintainers.
A more concise example of the difficulty, with accessible data would  
also be helpful.

url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820


On Jul 7, 2006, at 7:39 PM, raul sanchez wrote:

> Hello. I am a beginer in R and I can not implement the  
> KhmaladzeTest in the following command. Please help me!!!!!!!!!!!
>   PD: I attach thw results and the messages of the R program
>
>   R : Copyright 2006, The R Foundation for Statistical Computing
> Version 2.3.1 (2006-06-01)
> ISBN 3-900051-07-0
>
> R es un software libre y viene sin GARANTIA ALGUNA.
> Usted puede redistribuirlo bajo ciertas circunstancias.
> Escriba 'license()' o 'licence()' para detalles de distribucion.
>
> R es un proyecto colaborativo con muchos contribuyentes.
> Escriba 'contributors()' para obtener mas informacion y
> 'citation()' para saber como citar R o paquetes de R en publicaciones.
>
> Escriba 'demo()' para demostraciones, 'help()' para el sistema on-line
> de ayuda,
> o 'help.start()' para abrir el sistema de ayuda HTML con su navegador.
> Escriba 'q()' para salir de R.
>
>> utils:::menuInstallLocal()
> package 'quantreg' successfully unpacked and MD5 sums checked
> updating HTML package descriptions
>> utils:::menuInstallLocal()
> package 'foreign' successfully unpacked and MD5 sums checked
> updating HTML package descriptions
>> utils:::menuInstallLocal()
> package 'Rcmdr' successfully unpacked and MD5 sums checked
> updating HTML package descriptions
>> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
>> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
> quantreg package loaded:  To cite see citation("quantreg")
>> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
>> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
> Loading required package: tcltk
> Loading Tcl/Tk interface ... done
> --- Please select a CRAN mirror for use in this session ---
> also installing the dependencies 'acepack', 'scatterplot3d',  
> 'fBasics',
> 'Hmisc', 'quadprog', 'oz', 'mlbench', 'randomForest', 'SparseM',
> 'xtable', 'chron', 'fCalendar', 'its', 'tseries', 'DAAG', 'e1071',  
> 'mvtnorm',
> 'zoo', 'strucchange', 'sandwich', 'dynlm', 'leaps'
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/ 
> acepack_1.3-2.2.zip'
> Content
> type 'application/zip' length 55667 bytes
> URL abierta
> downloaded 54Kb
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/ 
> scatterplot3d_0.3-24.zip'
> Content
> type 'application/zip' length 540318 bytes
> URL abierta
> downloaded 527Kb
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/ 
> fBasics_221.10065.zip'
> Content
> type 'application/zip' length 3327499 bytes
> URL abierta
> downloaded 3249Kb
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/ 
> Hmisc_3.0-12.zip'
> Content
> type 'application/zip' length 1993038 bytes
> URL abierta
> downloaded 1946Kb
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/ 
> quadprog_1.4-8.zip'
> Content
> type 'application/zip' length 38626 bytes
> URL abierta
> downloaded 37Kb
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/oz_1.0-13.zip'
> Content
> type 'application/zip' length 39640 bytes
> URL abierta
> downloaded 38Kb
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/ 
> mlbench_1.1-1.zip'
> Content
> type 'application/zip' length 1324913 bytes
> URL abierta
> downloaded 1293Kb
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/ 
> randomForest_4.5-16.zip'
> Content
> type 'application/zip' length 209710 bytes
> URL abierta
> downloaded 204Kb
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/ 
> SparseM_0.68.zip'
> Content
> type 'application/zip' length 728594 bytes
> URL abierta
> downloaded 711Kb
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/ 
> xtable_1.3-2.zip'
> Content
> type 'application/zip' length 56703 bytes
> URL abierta
> downloaded 55Kb
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/chron_2.3-4.zip'
> Content
> type 'application/zip' length 101287 bytes
> URL abierta
> downloaded 98Kb
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/ 
> fCalendar_221.10065.zip'
> Content
> type 'application/zip' length 754551 bytes
> URL abierta
> downloaded 736Kb
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/its_1.1.1.zip'
> Content
> type 'application/zip' length 194287 bytes
> URL abierta
> downloaded 189Kb
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/ 
> tseries_0.10-3.zip'
> Content
> type 'application/zip' length 392748 bytes
> URL abierta
> downloaded 383Kb
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/DAAG_0.79.zip'
> Content
> type 'application/zip' length 796328 bytes
> URL abierta
> downloaded 777Kb
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/ 
> e1071_1.5-13.zip'
> Content
> type 'application/zip' length 627972 bytes
> URL abierta
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/ 
> mvtnorm_0.7-2.zip'
> Content
> type 'application/zip' length 214609 bytes
> URL abierta
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/zoo_1.1-0.zip'
> Content
> type 'application/zip' length 695387 bytes
> URL abierta
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/ 
> strucchange_1.2-13.zip'
> Content
> type 'application/zip' length 866668 bytes
> URL abierta
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/ 
> sandwich_1.9-0.zip'
> Content
> type 'application/zip' length 675607 bytes
> URL abierta
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/dynlm_0.1-2.zip'
> Content
> type 'application/zip' length 43966 bytes
> URL abierta
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/leaps_2.7.zip'
> Content
> type 'application/zip' length 73301 bytes
> URL abierta
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/ 
> relimp_0.9-6.zip'
> Content
> type 'application/zip' length 70677 bytes
> URL abierta
>
> probando la URL
> 'http://cran.au.r-project.org/bin/windows/contrib/2.3/ 
> multcomp_0.4-8.zip'
> Content
> type 'application/zip' length 310081 bytes
> URL abierta
>
>
>
> Versi?n del Rcmdr 1.1-7
>> Datos <- read.dta("C:/Mis documentos/r1-r1.dta", convert.dates=TRUE,
> convert.factors=TRUE, missing.type=TRUE, convert.underscore=TRUE,
> warn.missing.labels=TRUE)
> Aviso en read.dta("C:/Mis documentos/r1-r1.dta", convert.dates = TRUE,
> :
>          'missing.type' only applicable to version 8 files
>> fit1 <- rq(yyy ~ lll, tau = 0.5, data = Datos)
>> fit1
> Call:
> rq(formula = yyy ~ lll, tau = 0.5, data = Datos)
>
> Coefficients:
> (Intercept)         lll
>    4.249606   -0.929411
>
> Degrees of freedom: 36 total; 34 residual
>> summary(fit1)
>
> Call: rq(formula = yyy ~ lll, tau = 0.5, data = Datos)
>
> tau: [1] 0.5
>
> Coefficients:
>             coefficients lower bd upper bd
> (Intercept)  4.24961      2.36774  4.95617
> lll         -0.92941     -1.08243 -0.52124
>> r1 <- resid(fit1)
>> c1 <- coef(fit1)
>> summary(fit1, se = "nid")
>
> Call: rq(formula = yyy ~ lll, tau = 0.5, data = Datos)
>
> tau: [1] 0.5
>
> Coefficients:
>             Value    Std. Error t value  Pr(>|t|)
> (Intercept)  4.24961  1.05525    4.02710  0.00046
> lll         -0.92941  0.22934   -4.05246  0.00043
>> fit <- rqProcess( y.net ~ lgdp2 + fse2 + gedy2 + Iy2 + gcony2,
> +                 data = barro, taus = seq(.1,.9,by = .05))
> Erro en inherits(x, "data.frame") : objeto "barro" no encontrado
>> fit <- rqProcess(yyy ~ lll, data = Datos, taus = seq(.1,.9, by =
> .05))
> taus: 0.1  Erro en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>         tau - h < 0:  error in summary.rq
>> f <- rqProcess(yyy ~ lll, data = Datos, taus = seq(.1,.9, by = .05))
> taus: 0.1  Erro en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>         tau - h < 0:  error in summary.rq
>> fit1 <- rqProcess(yyy ~ lll, data = Datos, taus = seq(.1,.9, by =
> .05))
> taus: 0.1  Erro en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>         tau - h < 0:  error in summary.rq
>> T1 <- KhmaladzeTest(yyy ~ lll, taus = -1, nullH = "location")
> Erro en eval(expr, envir, enclos) : objeto "yyy" no encontrado
>> Datos <- read.dta("C:/Mis documentos/r1-r1.dta", convert.dates=TRUE,
> convert.factors=TRUE, missing.type=TRUE, convert.underscore=TRUE,
> warn.missing.labels=TRUE)
> Aviso en read.dta("C:/Mis documentos/r1-r1.dta", convert.dates = TRUE,
> :
>          'missing.type' only applicable to version 8 files
>> T1 <- KhmaladzeTest(yyy ~ lll, taus = -1, nullH = "location")
> Erro en eval(expr, envir, enclos) : objeto "yyy" no encontrado
>> T1 <- KhmaladzeTest( yyy ~ lll, taus = -1, nullH = "location")
> Erro en eval(expr, envir, enclos) : objeto "yyy" no encontrado
>> data(barro)
>> invisible(edit(barro))
>> rqProcess(yyy ~ lll, data = Datos, taus = -1, nullH = "location")
> $taus
> [1] 0.3161186 0.3543310 0.4007676 0.4150787 0.4268406 0.4542215
> 0.5008570 0.5304535 0.5690997
> [10] 0.6206522 0.6522563
>
> $qtaus
> [1] -0.037757189 -0.033227447 -0.032656681 -0.016152701 -0.015090004
> -0.014413058
> [7] -0.013738239 -0.013471008 -0.012253429 -0.010675005 -0.009866834
>
> $Vhat
>           [,1]      [,2]      [,3]       [,4]       [,5]       [,6]
> [,7]       [,8]
> [1,] 0.3064789 0.5104205 0.6209815 -0.0850221 -0.2053440 -0.1631580
> -0.1766553 -0.1768334
>            [,9]      [,10]      [,11]
> [1,] -0.1483747 -0.1191922 -0.1026903
>
> $vhat
>           [,1]      [,2]      [,3]       [,4]       [,5]       [,6]
> [,7]       [,8]
> [1,] 0.3064789 0.5104205 0.6209815 -0.0850221 -0.2053440 -0.1631580
> -0.1766553 -0.1768334
>            [,9]      [,10]      [,11]
> [1,] -0.1483747 -0.1191922 -0.1026903
>
> attr(,"class")
> [1] "rqProcess"
>> rqProcess(yyy ~ lll, data = Datos,taus = seq(.1,.9, by = .05), nullH
> = "location")
> taus: 0.1  Erro en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>         tau - h < 0:  error in summary.rq
>> KhmaladzeTest( yyy ~ lll, data = Datos, taus = seq(.1,.9, by = .05))
> taus: 0.1  Erro en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>         tau - h < 0:  error in summary.rq
>> T1 <- KhmaladzeTest(yyy ~ lll, taus = -1, nullH = "location")
> Erro en eval(expr, envir, enclos) : objeto "yyy" no encontrado
>> fit1 <- rqProcess(yyy ~ lll, data = Datos, taus = seq(.1,.9, by =
> .05))
> taus: 0.1  Erro en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>         tau - h < 0:  error in summary.rq
>> KhmaladzeTest( yyy ~ lll, data = Datos, taus = -1)
> Erro en qr(X[1:p, ]) : NA/NaN/Inf en llamada a una funci?n externa  
> (arg
> 1)
>> KhmaladzeTest( yyy ~ lll, data = Datos, taus = -1, nullH =
> "location")
> Erro en qr(X[1:p, ]) : NA/NaN/Inf en llamada a una funci?n externa  
> (arg
> 1)
>> data(barro)
>> KhmaladzeTest( y.net ~ lgdp2 + fse2 + gedy2 + Iy2 + gcony2,
> + data = barro, taus = seq(.05,.95,by = .01))
> taus: 0.05  Aviso en summary.rq(rq(formula, data = data, tau = taus 
> [i],
> method = "fn"),  :
>          1 non-positive fis
> 0.06  0.07  0.08  0.09  Aviso en summary.rq(rq(formula, data = data,
> tau = taus[i], method = "fn"),  :
>          1 non-positive fis
> 0.1  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],  
> method
> = "fn"),  :
>          1 non-positive fis
> 0.11  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          2 non-positive fis
> 0.12  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          3 non-positive fis
> 0.13  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          3 non-positive fis
> 0.14  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          4 non-positive fis
> 0.15  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          3 non-positive fis
> 0.16  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          5 non-positive fis
> 0.17  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          4 non-positive fis
> 0.18  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          5 non-positive fis
> 0.19  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          4 non-positive fis
> 0.2  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],  
> method
> = "fn"),  :
>          4 non-positive fis
> 0.21  0.22  0.23  0.24  0.25  Aviso en summary.rq(rq(formula, data =
> data, tau = taus[i], method = "fn"),  :
>          1 non-positive fis
> 0.26  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          1 non-positive fis
> 0.27  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          1 non-positive fis
> 0.28  0.29  0.3  0.31  0.32  0.33  Aviso en summary.rq(rq(formula,  
> data
> = data, tau = taus[i], method = "fn"),  :
>          1 non-positive fis
> 0.34  0.35  0.36  0.37  0.38  0.39  0.4  0.41  0.42  0.43  0.44  0.45
> 0.46  0.47  0.48  0.49  0.5  0.51  0.52  0.53  0.54  0.55  0.56  0.57
> Aviso en summary.rq(rq(formula, data = data, tau = taus[i], method =
> "fn"),  :
>          1 non-positive fis
> 0.58  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          2 non-positive fis
> 0.59  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          1 non-positive fis
> 0.6  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],  
> method
> = "fn"),  :
>          1 non-positive fis
> 0.61  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          1 non-positive fis
> 0.62  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          1 non-positive fis
> 0.63  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          1 non-positive fis
> 0.64  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          1 non-positive fis
> 0.65  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          1 non-positive fis
> 0.66  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          1 non-positive fis
> 0.67  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          1 non-positive fis
> 0.68  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          1 non-positive fis
> 0.69  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          2 non-positive fis
> 0.7  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],  
> method
> = "fn"),  :
>          2 non-positive fis
> 0.71  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          2 non-positive fis
> 0.72  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          1 non-positive fis
> 0.73  0.74  0.75  0.76  0.77  0.78  0.79  Aviso en
> summary.rq(rq(formula, data = data, tau = taus[i], method = "fn"),  :
>          2 non-positive fis
> 0.8  0.81  0.82  0.83  0.84  Aviso en summary.rq(rq(formula, data =
> data, tau = taus[i], method = "fn"),  :
>          2 non-positive fis
> 0.85  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          3 non-positive fis
> 0.86  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          3 non-positive fis
> 0.87  0.88  0.89  Aviso en summary.rq(rq(formula, data = data, tau =
> taus[i], method = "fn"),  :
>          1 non-positive fis
> 0.9  0.91  0.92  Aviso en summary.rq(rq(formula, data = data, tau =
> taus[i], method = "fn"),  :
>          2 non-positive fis
> 0.93  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          9 non-positive fis
> 0.94  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          13 non-positive fis
> 0.95  Aviso en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>          14 non-positive fis
> $nullH
> [1] "location"
>
> $Tn
> [1] 6.029988
>
> $THn
>     lgdp2      fse2     gedy2       Iy2    gcony2
> 1.3198411 1.1505847 1.2480321 0.8466474 1.0036528
>
> attr(,"class")
> [1] "KhmaladzeTest"
>> data(Datos)
> Aviso en data(Datos) : data set 'Datos' not found
>> Datos <- read.dta("C:/Mis documentos/r1-r1.dta", convert.dates=TRUE,
> convert.factors=TRUE, missing.type=TRUE, convert.underscore=TRUE,
> warn.missing.labels=TRUE)
> Aviso en read.dta("C:/Mis documentos/r1-r1.dta", convert.dates = TRUE,
> :
>          'missing.type' only applicable to version 8 files
>> data(Datos)
> Aviso en data(Datos) : data set 'Datos' not found
>> Datos <- read.dta("C:/Mis documentos/r1-r1.dta", convert.dates=TRUE,
> convert.factors=TRUE, missing.type=TRUE, convert.underscore=TRUE,
> warn.missing.labels=TRUE)
> Aviso en read.dta("C:/Mis documentos/r1-r1.dta", convert.dates = TRUE,
> :
>          'missing.type' only applicable to version 8 files
>> KhmaladzeTest( yyy ~ lll, data = Datos, taus = -1, nullH =
> "location")
> Erro en qr(X[1:p, ]) : NA/NaN/Inf en llamada a una funci?n externa  
> (arg
> 1)
>> invisible(edit(Datos))
>> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
>> fit <- rqProcess( yyy ~ lll, data = Datos, taus = seq(.1,.9,by =
> .05))
>
>> fff <- rqProcess( yyy ~ lll, data = Datos, taus = seq(.1,.9,by =
> .05))
> taus: 0.1  Erro en summary.rq(rq(formula, data = data, tau = taus[i],
> method = "fn"),  :
>         tau - h < 0:  error in summary.rq
>> fff <- rqProcess(yyy ~ lll, data = Datos, taus = -1, nullH =
> "location")
>> khmaladze.test(fit, nullH = "location")
> Error: no se pudo encontrar la funci?n "khmaladze.test"
>> khmaladze.test(fff, nullH = "location")
> Error: no se pudo encontrar la funci?n "khmaladze.test"
>> Khmaladze.test(fff, nullH = "location")
> Error: no se pudo encontrar la funci?n "Khmaladze.test"
>> KhmaladzeTest(fff, nullH = "location")
> Erro en terms.default(formula, data = data) :
>         no terms component
>> KhmaladzeTest(fff,data = Datos)
> Erro en terms.default(formula, data = data) :
>         no terms component
>> KhmaladzeTest(fff, data = Datos)
> Erro en terms.default(formula, data = data) :
>         no terms component
>> KhmaladzeTest(fff, data = Datos, nullH = "location")
> Erro en terms.default(formula, data = data) :
>         no terms component
>> khmaladze.test(fit, nullH = "location")
> Error: no se pudo encontrar la funci?n "khmaladze.test"
>> KhmaladzeTest(fff, data = Datos, taus = -1, nullH = "location")
> Erro en terms.default(formula, data = data) :
>         no terms component
>> help.search("KhmaladzeTest")
>> KhmaladzeTest( yyy  ~ lll, data = Datos, taus = -1, nullH =
> "location" ,  trim = c(0.05, 0.95))
> Erro en qr(X[1:p, ]) : NA/NaN/Inf en llamada a una funci?n externa  
> (arg
> 1)
>> KhmaladzeTest( fff, data = Datos, taus = -1, nullH = "location" ,
> trim = c(0.05, 0.95))
> Erro en terms.default(formula, data = data) :
>         no terms component
>> ls()
> [1] "barro" "c1"    "Datos" "fff"   "fit1"  "GLM.1" "r1"
>> summary(fff)
>       Length Class  Mode
> taus  11     -none- numeric
> qtaus 11     -none- numeric
> Vhat  11     -none- numeric
> vhat  11     -none- numeric
>> KhmaladzeTest(formula, data = NUL, taus = -1, nullH = "location"
>
> + >
>> KhmaladzeTest(fff, data = Datos, taus = -1, nullH = "location")
> Erro en terms.default(formula, data = data) :
>         no terms component
>> KhmaladzeTest(fff, data = Datos)
> Erro en terms.default(formula, data = data) :
>         no terms component
>> KhmaladzeTest( fff, data = Datos, taus = -1, nullH = "location" ,
> trim = c(0.05, 0.95))
> Erro en terms.default(formula, data = data) :
>         no terms component
>> KhmaladzeTest( fit1, data = Datos, taus = -1, nullH = "location" ,
> trim = c(0.05, 0.95))
> Erro en qr(X[1:p, ]) : NA/NaN/Inf en llamada a una funci?n externa  
> (arg
> 1)
>> fix(Datos)
>
>
>  		
> ---------------------------------
>  1GB gratis, Antivirus y Antispam
>  Correo Yahoo!, el mejor correo web del mundo
>  Abr? tu cuenta aqu?______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html


From merser at image.dk  Sat Jul  8 16:39:00 2006
From: merser at image.dk (=?iso-8859-1?Q?S=F8ren_Merser?=)
Date: Sat, 8 Jul 2006 16:39:00 +0200
Subject: [R] survfit, unused argument(s) (error ...)
Message-ID: <000301c6a29c$42c39670$6400a8c0@IBM>

Hi 

It seems that survfit() doesn't accept the argumnet 'error' as below  

>survfit(fit, error='greenwood')
Error in survfit.coxph(fit, error = "greenwood") : 
        unused argument(s) (error ...)

Isn't is allowed to do that for a coxph object?

Regards Soren

Windows XP, SP2
R 2.3.0


From jdrapp at gmail.com  Sat Jul  8 16:45:07 2006
From: jdrapp at gmail.com (justin rapp)
Date: Sat, 8 Jul 2006 10:45:07 -0400
Subject: [R] Non-Numeric Histograms
Message-ID: <af81db5a0607080745u2c23bc72n89802a2ff54a7743@mail.gmail.com>

I have a dataset with a variable that is non-numeric.  I was wondering
if there is an easy way in R to create a bar graph that will tell me
how many of each level of this non-numeric variable are in the overall
dataset.

Specifically, if my variable is LETTER, and I have A, B,C, D, E, F and
I want to find out how many of each letter there are, what is the
easiest way to do this in R and use the information to make a plot?

jdr


From ligges at statistik.uni-dortmund.de  Sat Jul  8 16:53:10 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 08 Jul 2006 16:53:10 +0200
Subject: [R] Non-Numeric Histograms
In-Reply-To: <af81db5a0607080745u2c23bc72n89802a2ff54a7743@mail.gmail.com>
References: <af81db5a0607080745u2c23bc72n89802a2ff54a7743@mail.gmail.com>
Message-ID: <44AFC6D6.4090500@statistik.uni-dortmund.de>

justin rapp wrote:
> I have a dataset with a variable that is non-numeric.  I was wondering
> if there is an easy way in R to create a bar graph that will tell me
> how many of each level of this non-numeric variable are in the overall
> dataset.
> 
> Specifically, if my variable is LETTER, and I have A, B,C, D, E, F and
> I want to find out how many of each letter there are, what is the
> easiest way to do this in R and use the information to make a plot?


  barplot(table(c("A", "A", "B")))

Uwe Ligges


> jdr
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ligges at statistik.uni-dortmund.de  Sat Jul  8 17:01:04 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 08 Jul 2006 17:01:04 +0200
Subject: [R] Availability of quadplot3d package (UseR!2006 Four
 Dimensional Barycentric Plots in 3D)
In-Reply-To: <7b18cd4d0607080639k429570fdn15712463de2507b7@mail.gmail.com>
References: <7b18cd4d0607080639k429570fdn15712463de2507b7@mail.gmail.com>
Message-ID: <44AFC8B0.7030600@statistik.uni-dortmund.de>

Carlos Ortega wrote:
> Dear List,
> 
> I have been unable fo find the package quadplot3d referred in the
> Abstract/Presentation "Four Dimensional Barycentric Plot in 3D" presented in
> the UserR!2006.
> 
> Does anyone know if it is available ? And if so, if it is ported to Windows
> ?


I think we should ask the author of that presentation, Geoffrey Matthews 
(CCing).

I'm also interested in adding a corresponding plot feature into package 
klaR, which currently only covers static "quadplots". Hence it would be 
nice to see the package quadplot3d on CRAN, or at least I'd like to see 
its functionality contributed to some other CRAN package such as misc3d.

Uwe Ligges
	


> Thanks in anticipation,
> Carlos Ortega
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From epistat at gmail.com  Sat Jul  8 17:12:23 2006
From: epistat at gmail.com (zhijie zhang)
Date: Sat, 8 Jul 2006 23:12:23 +0800
Subject: [R] which model (GLMs)is the best?
Message-ID: <2fc17e30607080812v11a6ea9g99726dd5cd090db2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060708/63ffc2ef/attachment.pl 

From Charles.Annis at StatisticalEngineering.com  Sat Jul  8 19:11:25 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Sat, 8 Jul 2006 13:11:25 -0400
Subject: [R] another tcl/tk query
Message-ID: <053801c6a2b1$8b892590$6600a8c0@DD4XFW31>

Greetings:

I wish to use a tcl/tk widget to ask for user-selected parameter values.  My
widget works ? it asks for and returns to my workspace the stuff I need.
Here is a snippet of my code:

###############################
OnOK <- function()
{
    LOG.X <<- as.logical(as.character(tclvalue(log.X.buttonValue)))
    LOG.Y <<- as.logical(as.character(tclvalue(log.Y.buttonValue)))
    natural.units.?.decision   <<-
as.double(as.character(tclvalue(?.decision)))
    natural.units.left.censor  <<-
as.double(as.character(tclvalue(left.censor)))
    natural.units.right.censor <<-
as.double(as.character(tclvalue(right.censor)))
    tkdestroy(t2)
}
###############################


My problem is this:  I would like to use the new input in the same routine
that created, used, and destroyed the widget.  I can?t seem to do that.  The
routine executes with what it has.  I must wait for the calling routine to
end before I can use the new info, which is correctly place in the
workspace, in subsequent R routines.

Is there a way I can use the updated values in the same routine that created
the widget?

Thanks for your advice ? and patience.


Charles Annis, P.E.

PS - I did read Prof. Ripley's post of Wed 8/31/2005
"Re: [R] tcl/tk return problem" but was unable to benefit from it.


Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:? 614-455-3265
http://www.StatisticalEngineering.com
?


From spencer.graves at pdf.com  Sat Jul  8 20:50:38 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 08 Jul 2006 11:50:38 -0700
Subject: [R] Fwd: time series patterns
In-Reply-To: <509bb6a90606301500o14c7213ay8cd36d5855a423b6@mail.gmail.com>
References: <509bb6a90606301258t49153c36vb199dc6e2c8a855a@mail.gmail.com>	<BAY106-F9EBDB789D3ADA346BDB6BBB7D0@phx.gbl>
	<509bb6a90606301500o14c7213ay8cd36d5855a423b6@mail.gmail.com>
Message-ID: <44AFFE7E.8010206@pdf.com>

	  For my previous email on this thread, I couldn't find Jim Lindsey's 
home page.  I just found it via www.r-project.org -> Search
-> "R Site Search" -> "Jim Lindsey's packages."  This led me to 
"http://popgen.unimaas.nl/~jlindsey/rcode.html".

	  ... just in case someone following this thread also has this problem.
	  Spencer Graves

######################
	  I how you code something like this depends on what you plan to do
with it.  If it were my problem, I might want to try some hidden Markov
models.  RSiteSearch("hidden markov", "functions") produced 29 hits for
me just now.  You might try some of the packages mentioned there.  They
might suggest supplemental coding and / or plotting procedures that
might provide insight into your application.

	  Hope this helps.
	  Spencer Graves
p.s.  The 'repeated' package and others by J.K. Lindsey are not
available from CRAN.  A year ago, I downloaded material from his
personal web site, which was the first hit on a Google search for "J K
Lindsey".  I also remember seeing it on CRAN or r-project.org.  Today,
the closest I can come is "jlindsey at ulg.ac.be" (T?l : 	+32 4 3662964).
If you want to try any of his R packages and you don't have any better
results with a web search than I got just now, I suggest send him an
email.

Raphael Fraser wrote:
> ---------- Forwarded message ----------
> From: Alexander Nervedi <alexnerdy at hotmail.com>
> Date: Jun 30, 2006 4:56 PM
> Subject: time series patterns
> To: raphael.fraser at gmail.com
> 
> 
> Hi all.
> 
> I have a factor variable distributed over time. I am looking for an elegant
> way to code duration of a state. Suppose,
> 
>> rainfall.shocks <- factor(sample(c(1,2,3), size = 15, replace = TRUE, prob
>> = unit.p),
> +                  label = c("Drought", "Normal", "High"))
>> rainfall.shocks
> [1] Normal  High    High    Drought Normal  Normal  High    Normal  Drought
> [10] Normal  Drought Normal  Normal  Normal  Normal
> 
> 
> So capture the duration of say drought, I'd need a variable that is able to
> keep track of rainfall.shocks as well as its past values. I was wondering if
> there is any obvious way to do this. the Drought variable in this case would
> have values
> 
> 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0
> 
> many thanks for the suggestions you are likely to make.
> 
> Alexander Nervedi
> 
> _________________________________________________________________
> Express yourself instantly with MSN Messenger! Download today - it's FREE!
> http://messenger.msn.click-url.com/go/onm00200471ave/direct/01/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ThadenJohnJ at uams.edu  Sat Jul  8 20:52:56 2006
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Sat, 8 Jul 2006 13:52:56 -0500
Subject: [R] SUMMARY: R crash with ''library(Matrix);
 as(x, "dgCMatrix")' [was: Warning while subsetting...]
In-Reply-To: <17580.48271.592693.686921@stat.math.ethz.ch>
Message-ID: <0C6BF3FC506F664F90C8BA3E0160462D04A7588E@EXCHANGE3.ad.uams.edu>

With thanks to Matrix package co-author Martin 
Maechler, I'm happy to report satisfactory closure 
of two recent threads I initiated about that package: 
   -  Warning while subsetting with Matrix
   -  R Crash with 'library(Matrix);as(x,"dgCMatrix")
   
In the first, I reported seeing a warning message after
selecting a subset of a matrix 'M' of class "dgTMatrix"
via the first but not the second of these commands:
 
  M10 <- M[1:10,]
  M10 <- M[1:10,1:10]

Acting on Martin's advice, I upgraded Matrix version 
0.995-10 to version 0.995-11.  This eliminated the
warning message.

In the second, I reported that class-conversion of 
a self-constructed "dgTMatrix" of dimension 600 X 4482
caused R to crash when done via either of the first two
but neither of the last two of these commands:

  Mc <- as(M, "dgCMatrix")
  Mm <- as(M, "matrix")
  Mc <- rbind(as(M[1:300,], "dgCMatrix"), as(M[301:600], "dgCMatrix")
  Mm <- rbind(as(M[1:300,], "matrix"), as(M[301:600], "matrix")

I surmised this was a memory issue but Martin pointed out 
the memory demand to hold even the expanded sparse matrix
is actually rather small (8 x 600 x 4482 = 21549456 bytes
= 21.5 Megabytes).
 
For obvious bandwidth reasons, I did not post my large 
matrix, so Martin created one of his own.  Neither he 
nor I could reproduce crashes with his matrix. Offlist,
I sent him my matrix. Upon class-conversion, it crashed 
his machine too.

Upon inspection, Martin discovered that my matrix did not
conform to the dgTMatrix class as defined in Matrix. In 
particular, my slots M at i and M at j were integer vectors 
beginning with ones, i.e., they respectively indexed the
first row and column of the matrix as "1", not as "0".  

However, correcting this did not stop crashes, until I also
realized that M at i had 601 (not 600) unique integer values, 
and corrected my Dims slot (M at Dims) to be c(601,4482).  This 
stopped the crashes.

Once created, Matrix objects behave like matrix objects in
the sense that "one-based" indexing is used to subset them
or re-assign values to them.  But for users who create 
them, especially using the new() function, it is helpful
to remind oneself that Matrix objects are created and 
stored using "zero-based" indexing.  This fact is
mentioned only obliquely in the current version's PDF file
and R-accessible documentation.  Hopefully that will change.

Again, my thanks to Martin. For me this experience confirms
The vigor of open-source, forum-supported projects.

-John Thaden, Ph.D.
Research Assistant Professor of Geriatrics
University of Arkansas for Medical Sciences
Little Rock, AR 72205
USA

Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From jdrapp at gmail.com  Sat Jul  8 21:05:12 2006
From: jdrapp at gmail.com (justin rapp)
Date: Sat, 8 Jul 2006 15:05:12 -0400
Subject: [R] Adding Lines to Plot
Message-ID: <af81db5a0607081205v1e841bdana6058e7e0a0c3aa0@mail.gmail.com>

This seems like a question that I should be able to answer on my own
but after looking at the documentation I cannot seem to find the
correct method.

How do I add lines to a bar plot that extend from the vertical axis?
For example, my vertical axis is numbered in increments of 10 and I
would like these to go across the whole graph.

Also, is there a way to have R label the value of each bar so that I
know the value of each factor that is being plotted?

Thanks in advance.

jdr


From spencer.graves at pdf.com  Sat Jul  8 21:12:30 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 08 Jul 2006 12:12:30 -0700
Subject: [R] PLS method
In-Reply-To: <001c01c6a09c$a3627980$bf89f981@leipc>
References: <001c01c6a09c$a3627980$bf89f981@leipc>
Message-ID: <44B0039E.8030402@pdf.com>

	  I believe that regression coefficients can change signs in partial 
least squares (pls) or in the related structural equation modeling (sem) 
for roughly the same reasons they can change sign in ordinary least 
squares (ols).  Both PLS and SEM essentially assume that the 
'independent' variables (X's) in the model are linear combinations of 
unobserved 'structural' variables plus noise.  Then the response 
variable(s) are linear combinations of these unobserved structural 
variables plus error.

	  I have used neither pls nor sem, so I can't go beyond this.  If it 
were my problem and I wanted to understand it better, I might cut the 
example down still further, e.g., to 4 observations with only 2 X's, and 
then try to program the entire thing in Excel using the 'solver'.  Or 
make local copies of functions like 'mvr' and use 'debug' to walk 
through the code line by line, looking carefully at what it does.

	  Before you do that, however, if you aren't clear on the similarities 
and differences between pls and sem, I suggest you explore that, e.g., 
using Google.  Just now, I found the following pls / sem comparison: 
"http://www2.gsu.edu/~mkteer/relmeth.html".

	  Hope this helps.
	  Spencer Graves

Sun Jia wrote:
> dear all,
> 
> I am a new comer to R and statistic.  Now I have a little confuse about the
> package pls.
> 
> I have to use 5 components to form a model. There are strong relationship
> between some of the components, which leads to the changes of the sign of
> each coeficeince, of course this is unwanted when using the normal
> regression way. So I choose the way of PLS, which is good at solve this kind
> of problem.
> 
> In my work,
> 
> q is the response and w,c,d,r,o are the 5 components.
> 
>          w     c       d      r           o        q
> 
> 1  219.580 0.880 102.742 12.988 0.9380 11
> 
> 2  245.806 0.900  97.798 11.764 1.0080 12
> 
> 3  219.850 0.910  93.764  5.608 1.1006 16
> 
> 4  226.904 0.842 110.080 14.614 0.8398  7
> 
> 5  250.792 0.868 108.212 14.714 0.8990 10
> 
> 6  225.264 0.930  96.748  6.906 1.1784 16
> 
> 7  229.562 0.856 103.204 12.900 0.8730 12
> 
> 8  239.560 0.880 101.036 11.766 0.9452 12
> 
> 9  199.008 0.920  91.338  3.918 1.1234 17
> 
> 10 220.458 0.910  88.322  9.868 1.0746 13
> 
> 11 201.228 0.910  89.202 10.328 1.0514 14
> 
> 12 199.160 0.920  90.126  2.088 1.1326 15
> 
> 13 135.540 0.786 121.506 19.140 0.6934  2
> 
> 14 296.272 0.864 130.896 22.614 0.9104  6
> 
> 15 190.766 0.840 108.050  7.336 0.8210  8
> 
> I have used the following sentence.
> 
> b.pls<-mvr(q~w+c+d+r+o,data=b,method="simpls")
> 
>> coef.mvr(b.pls)
> 
> , , 5 comps
> 
> 
> 
>             q
> 
> w  0.01993749
> 
> c 12.42713250
> 
> d -0.12050551
> 
> r -0.20287088
> 
> o  9.63670488
> 
> 
> 
>  I have found that the sign of each component still cannot be explained by
> the reality. For instance, the sign of w should be negative rather than
> positive.
> 
>> b.pls<-mvr(q~c+d+r+o,data=b,method="simpls")
> 
>> coef.mvr(b.pls)
> 
> , , 4 comps
> 
> 
> 
>             q
> 
> c 76.39196611
> 
> d -0.06512864
> 
> r -0.18272329
> 
> o -3.02212146
> 
> 
> 
> 
> 
> When I delete one of the components, the w, I found the coefficients of the
> rest ones also changes the sign, the component of o.
> 
> As far as I concerned, this kind of situation should only happened when use
> the normal regression rather than PLS regression.
> 
> Is there any wrong with my understanding?
> 
>  Why does this problem happen?
> 
> 
> 
> I am appreciated for your help and answer.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From jdrapp at gmail.com  Sat Jul  8 21:44:48 2006
From: jdrapp at gmail.com (justin rapp)
Date: Sat, 8 Jul 2006 15:44:48 -0400
Subject: [R] Summary Statistics for data.frame
Message-ID: <af81db5a0607081244o3ae7a65ah268c9215574b0ecc@mail.gmail.com>

I apologize for my constant questions but I am new to R and trying to
gain an appreciation for its capabilities.  The following task is easy
in Excel and I was hoping somebody could give me a quick explanation
for how it can be acheived in R so I can avoid having to switch
between the two applications.

How do I find the Summary Statistics in one Vector of the dataframe by
levels in another of the vectors.

For example, I have the following headings for my data.frame.
Conference
Year Drafted
Height
Weight
Ratio

I would like to see compute the mean Height, Weight, and Ratio as well
as their variances for each of the years under Year
Drafted(1980-2000).  What is the most efficient way of doing this?

Thank you.


From murdoch at stats.uwo.ca  Sat Jul  8 22:00:44 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 08 Jul 2006 16:00:44 -0400
Subject: [R] Summary Statistics for data.frame
In-Reply-To: <af81db5a0607081244o3ae7a65ah268c9215574b0ecc@mail.gmail.com>
References: <af81db5a0607081244o3ae7a65ah268c9215574b0ecc@mail.gmail.com>
Message-ID: <44B00EEC.2040304@stats.uwo.ca>

On 7/8/2006 3:44 PM, justin rapp wrote:
> I apologize for my constant questions but I am new to R and trying to
> gain an appreciation for its capabilities.  The following task is easy
> in Excel and I was hoping somebody could give me a quick explanation
> for how it can be acheived in R so I can avoid having to switch
> between the two applications.
> 
> How do I find the Summary Statistics in one Vector of the dataframe by
> levels in another of the vectors.
> 
> For example, I have the following headings for my data.frame.
> Conference
> Year Drafted
> Height
> Weight
> Ratio
> 
> I would like to see compute the mean Height, Weight, and Ratio as well
> as their variances for each of the years under Year
> Drafted(1980-2000).  What is the most efficient way of doing this?

I think the quickest is

by(mydf, mydf$Year, summary)

but this won't give you the variance.  You'll need your own little 
function to calculate mean and variance, e.g.

mysummary <- function(df) apply(df, 2,
                function(x) c(mean=mean(x), variance=var(x)))

by(mydf, mydf$Year, mysummary)

If you don't like the format of the output, you can play around with the 
mysummary function.  It will be applied to each subset of the 
data.frame, and the results will be put together into a list with one 
entry per level of mydf$Year.


Duncan


From jdrapp at gmail.com  Sat Jul  8 22:55:40 2006
From: jdrapp at gmail.com (justin rapp)
Date: Sat, 8 Jul 2006 16:55:40 -0400
Subject: [R] Summary Statistics for data.frame
In-Reply-To: <44B00EEC.2040304@stats.uwo.ca>
References: <af81db5a0607081244o3ae7a65ah268c9215574b0ecc@mail.gmail.com>
	<44B00EEC.2040304@stats.uwo.ca>
Message-ID: <af81db5a0607081355h68c98fcbsccfc84db523f5075@mail.gmail.com>

When I attempt to use the mysummary function, I obtain the following error:

Error in var(x) : missing observations in cov/cor

When I use:
by(data.logistic,data.logistic$Ydrafted,summary)

I receive no errors. I cut and pasted your mysummary function directly
into my r console.  Should I have made any adjustments to the code?

jdr

On 7/8/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 7/8/2006 3:44 PM, justin rapp wrote:
> > I apologize for my constant questions but I am new to R and trying to
> > gain an appreciation for its capabilities.  The following task is easy
> > in Excel and I was hoping somebody could give me a quick explanation
> > for how it can be acheived in R so I can avoid having to switch
> > between the two applications.
> >
> > How do I find the Summary Statistics in one Vector of the dataframe by
> > levels in another of the vectors.
> >
> > For example, I have the following headings for my data.frame.
> > Conference
> > Year Drafted
> > Height
> > Weight
> > Ratio
> >
> > I would like to see compute the mean Height, Weight, and Ratio as well
> > as their variances for each of the years under Year
> > Drafted(1980-2000).  What is the most efficient way of doing this?
>
> I think the quickest is
>
> by(mydf, mydf$Year, summary)
>
> but this won't give you the variance.  You'll need your own little
> function to calculate mean and variance, e.g.
>
> mysummary <- function(df) apply(df, 2,
>                 function(x) c(mean=mean(x), variance=var(x)))
>
> by(mydf, mydf$Year, mysummary)
>
> If you don't like the format of the output, you can play around with the
> mysummary function.  It will be applied to each subset of the
> data.frame, and the results will be put together into a list with one
> entry per level of mydf$Year.
>
>
> Duncan
>


From murdoch at stats.uwo.ca  Sat Jul  8 23:19:12 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 08 Jul 2006 17:19:12 -0400
Subject: [R] Summary Statistics for data.frame
In-Reply-To: <af81db5a0607081355h68c98fcbsccfc84db523f5075@mail.gmail.com>
References: <af81db5a0607081244o3ae7a65ah268c9215574b0ecc@mail.gmail.com>	
	<44B00EEC.2040304@stats.uwo.ca>
	<af81db5a0607081355h68c98fcbsccfc84db523f5075@mail.gmail.com>
Message-ID: <44B02150.9000005@stats.uwo.ca>

On 7/8/2006 4:55 PM, justin rapp wrote:
> When I attempt to use the mysummary function, I obtain the following error:
> 
> Error in var(x) : missing observations in cov/cor

var() gives that error if it sees NA values.  You can get it to remove 
them by using

var(x, na.rm = TRUE)

instead of var(x).  Whether that makes sense depends on the context of 
your problem.

Duncan Murdoch

> 
> When I use:
> by(data.logistic,data.logistic$Ydrafted,summary)
> 
> I receive no errors. I cut and pasted your mysummary function directly
> into my r console.  Should I have made any adjustments to the code?
> 
> jdr
> 
> On 7/8/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>> On 7/8/2006 3:44 PM, justin rapp wrote:
>>> I apologize for my constant questions but I am new to R and trying to
>>> gain an appreciation for its capabilities.  The following task is easy
>>> in Excel and I was hoping somebody could give me a quick explanation
>>> for how it can be acheived in R so I can avoid having to switch
>>> between the two applications.
>>>
>>> How do I find the Summary Statistics in one Vector of the dataframe by
>>> levels in another of the vectors.
>>>
>>> For example, I have the following headings for my data.frame.
>>> Conference
>>> Year Drafted
>>> Height
>>> Weight
>>> Ratio
>>>
>>> I would like to see compute the mean Height, Weight, and Ratio as well
>>> as their variances for each of the years under Year
>>> Drafted(1980-2000).  What is the most efficient way of doing this?
>> I think the quickest is
>>
>> by(mydf, mydf$Year, summary)
>>
>> but this won't give you the variance.  You'll need your own little
>> function to calculate mean and variance, e.g.
>>
>> mysummary <- function(df) apply(df, 2,
>>                 function(x) c(mean=mean(x), variance=var(x)))
>>
>> by(mydf, mydf$Year, mysummary)
>>
>> If you don't like the format of the output, you can play around with the
>> mysummary function.  It will be applied to each subset of the
>> data.frame, and the results will be put together into a list with one
>> entry per level of mydf$Year.
>>
>>
>> Duncan
>>


From klebyn at yahoo.com.br  Sat Jul  8 23:29:10 2006
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Sat, 8 Jul 2006 18:29:10 -0300 (ART)
Subject: [R] String mathematical function to  R-function
Message-ID: <20060708212910.23326.qmail@web30606.mail.mud.yahoo.com>



hello

I make a subroutine that give-me a (mathematical)
function in string format.
I would like transform this string into function ( R
function ).

thanks for any tips.
cleber


#e.g.
fun_String = "-100*x1 + 0*x2 + 100*x3"

fun <- function(x1,x2,x3){
return(
############

evaluation( fun_String )

############
)





True String mathematical function :-(  :-(

> nomes
[1] "8.49*x1*z1 + 6.13*x1*z2 + 6.4*x1*z3 + 6.9*x2*z1 +
4.54*x2*z2 + 3.99*x2*z3 + 19.31*x3*z1 + 12.49*x3*z2 +
3.86*x3*z3 + 5.25*x1*z1*z2 + -6.2*x1*z1*z3 +
9.07*x1*z2*z3 + 10.87*x2*z1*z2 + 9.78*x2*z1*z3 +
49.05*x2*z2*z3 + 4.56*x1*x2*z1 + -4.9*x1*x2*z2 +
4.01*x1*x2*z3 + -0.39*x3*z1*z2 + 14.34*x3*z1*z3 +
0.7*x3*z2*z3 + -2.84*x1*x3*z1 + 20.25*x1*x3*z2 +
6.44*x1*x3*z3 + -4.91*x2*x3*z1 + 5.45*x2*x3*z2 +
37.99*x2*x3*z3 + -22.24*x1*z1*z2*z3 +
-97.41*x2*z1*z2*z3 + -8.67*x1*x2*z1*z2 +
49.14*x1*x2*z1*z3 + 14.24*x1*x2*z2*z3 +
282.71*x3*z1*z2*z3 + 34.83*x1*x3*z1*z2 +
111.2*x1*x3*z1*z3 + 101.38*x1*x3*z2*z3 +
-6.93*x2*x3*z1*z2 + 90.16*x2*x3*z1*z3 +
-9.11*x2*x3*z2*z3 + 17.22*x1*x2*x3*z1 +
-29.42*x1*x2*x3*z2 + -19.87*x1*x2*x3*z3 +
-277.41*x1*x2*z1*z2*z3 + -482.82*x1*x3*z1*z2*z3 +
-688.23*x2*x3*z1*z2*z3 + -588.4*x1*x2*x3*z1*z2 +
-197.31*x1*x2*x3*z1*z3 + -722.58*x1*x2*x3*z2*z3 +
5536.59*x1*x2*x3*z1*z2*z3"
> 




		
_______________________________________________________ 

o discador agora!


From renaud.lancelot at gmail.com  Sun Jul  9 00:25:12 2006
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Sun, 9 Jul 2006 00:25:12 +0200
Subject: [R] String mathematical function to R-function
In-Reply-To: <20060708212910.23326.qmail@web30606.mail.mud.yahoo.com>
References: <20060708212910.23326.qmail@web30606.mail.mud.yahoo.com>
Message-ID: <c2ee56800607081525v2aec3e21veaaf1d4e7a539f35@mail.gmail.com>

> fun_String <- "-100*x1 + 0*x2 + 100*x3"
>
> fun <- function(x1, x2, x3){
+   eval(parse(text = fun_String))
+   }
>
> fun(4:0, -2:2, 0:4)
[1] -400 -200    0  200  400

Best,

Renaud

2006/7/8, Cleber N.Borges <klebyn at yahoo.com.br>:
>
>
> hello
>
> I make a subroutine that give-me a (mathematical)
> function in string format.
> I would like transform this string into function ( R
> function ).
>
> thanks for any tips.
> cleber
>
>
> #e.g.
> fun_String = "-100*x1 + 0*x2 + 100*x3"
>
> fun <- function(x1,x2,x3){
> return(
> ############
>
> evaluation( fun_String )
>
> ############
> )
>
>
>
>
>
> True String mathematical function :-(  :-(
>
> > nomes
> [1] "8.49*x1*z1 + 6.13*x1*z2 + 6.4*x1*z3 + 6.9*x2*z1 +
> 4.54*x2*z2 + 3.99*x2*z3 + 19.31*x3*z1 + 12.49*x3*z2 +
> 3.86*x3*z3 + 5.25*x1*z1*z2 + -6.2*x1*z1*z3 +
> 9.07*x1*z2*z3 + 10.87*x2*z1*z2 + 9.78*x2*z1*z3 +
> 49.05*x2*z2*z3 + 4.56*x1*x2*z1 + -4.9*x1*x2*z2 +
> 4.01*x1*x2*z3 + -0.39*x3*z1*z2 + 14.34*x3*z1*z3 +
> 0.7*x3*z2*z3 + -2.84*x1*x3*z1 + 20.25*x1*x3*z2 +
> 6.44*x1*x3*z3 + -4.91*x2*x3*z1 + 5.45*x2*x3*z2 +
> 37.99*x2*x3*z3 + -22.24*x1*z1*z2*z3 +
> -97.41*x2*z1*z2*z3 + -8.67*x1*x2*z1*z2 +
> 49.14*x1*x2*z1*z3 + 14.24*x1*x2*z2*z3 +
> 282.71*x3*z1*z2*z3 + 34.83*x1*x3*z1*z2 +
> 111.2*x1*x3*z1*z3 + 101.38*x1*x3*z2*z3 +
> -6.93*x2*x3*z1*z2 + 90.16*x2*x3*z1*z3 +
> -9.11*x2*x3*z2*z3 + 17.22*x1*x2*x3*z1 +
> -29.42*x1*x2*x3*z2 + -19.87*x1*x2*x3*z3 +
> -277.41*x1*x2*z1*z2*z3 + -482.82*x1*x3*z1*z2*z3 +
> -688.23*x2*x3*z1*z2*z3 + -588.4*x1*x2*x3*z1*z2 +
> -197.31*x1*x2*x3*z1*z3 + -722.58*x1*x2*x3*z2*z3 +
> 5536.59*x1*x2*x3*z1*z2*z3"
> >
>
>
>
>
>
> _______________________________________________________
>
> o discador agora!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
Renaud LANCELOT
D?partement Elevage et M?decine V?t?rinaire (EMVT) du CIRAD
Directeur adjoint charg? des affaires scientifiques

CIRAD, Animal Production and Veterinary Medicine Department
Deputy director for scientific affairs

Campus international de Baillarguet
TA 30 / B (B?t. B, Bur. 214)
34398 Montpellier Cedex 5 - France
T?l   +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 39 04
Fax   +33 (0)4 67 59 37 95


From mr.blacksheep at gmail.com  Sun Jul  9 00:40:00 2006
From: mr.blacksheep at gmail.com (Mike Nielsen)
Date: Sat, 8 Jul 2006 16:40:00 -0600
Subject: [R] Combining a list of similar dataframes into a single dataframe
Message-ID: <46a360560607081540l264ddd02xf4a4a99979fda4e@mail.gmail.com>

I would be very grateful to anyone who could point to the error of my
ways in the following.

I have a dataframe called net1, as such:

> str(net1)
`data.frame':    114192 obs. of  9 variables:
 $ server         : Factor w/ 122 levels "AB93-99","AMP93-1",..: 1 1 1
1 1 1 1 1 1 1 ...
 $ ts             :'POSIXct', format: chr  "2006-06-30 12:31:44"
"2006-06-30 12:31:44" "2006-06-30 12:31:44" "2006-06-30 12:31:44" ...
 $ instance       : Factor w/ 22 levels "1","2","Compaq Ethernet_Fast
Ethernet Adapter_Module",..: 4 4 4 4 4 4 4 4 4 4 ...
 $ instanceno     : Factor w/ 3 levels "1","2","3": 1 1 1 1 1 1 1 1 1 1 ...
 $ perftime       : num  3.16e+13 3.16e+13 3.16e+13 3.16e+13 3.16e+13 ...
 $ perffreq       : num  6.99e+08 6.99e+08 6.99e+08 6.99e+08 6.99e+08 ...
 $ perftime100nsec: num  1.28e+17 1.28e+17 1.28e+17 1.28e+17 1.28e+17 ...
 $ countername    : Factor w/ 4 levels "Bytes Received/sec",..: 1 3 2
4 1 3 2 4 1 3 ...
 $ countervalue   : num  6.08e+07 6.64e+07 5.58e+06 1.00e+08 6.09e+07 ...
>

What I am trying to do is subset this thing down by server, instance,
instanceno, countername and then apply a function to each subsetted
dataframe.  The function performs a calculation on countervalue,
essentially "collapsing" instanceno and instance down to a single
value.

Here is a snippet of my code:
t1 <- by(net1,
         list(
              net1$server,
              factor(as.character(net1$countername))),# get rid of
unused levels of countername for this server
         function(x){
           g <- by(x,
                   list(factor(as.character(x$instance)), # get rid of
unused levels of instance for this server
                   factor(as.character(x$instanceno))),   # same with instanceno

function(y){c(NA,mean(y$perffreq)*diff(y$countervalue)/diff(y$perftime))})
           data.frame(server=x$server,
                      ts=x$ts,
                      countername = x$countername,
                      countervalue =
apply(sapply(g[!sapply(g,is.null)],I),1,sum))
         })

So t1 then is a list of dataframes, each with an identical set of columns)

> str(t1[[1]])
`data.frame':	149 obs. of  4 variables:
 $ server      : Factor w/ 122 levels "AB93-99","AMP93-1",..: 1 1 1 1
1 1 1 1 1 1 ...
 $ ts          :'POSIXct', format: chr  "2006-06-30 12:31:44"
"2006-06-30 12:32:58" "2006-06-30 12:34:46" "2006-06-30 12:36:55" ...
 $ countername : Factor w/ 4 levels "Bytes Received/sec",..: 1 1 1 1 1
1 1 1 1 1 ...
 $ countervalue: num    NA  938  816 4213  906 ...

What I'd dearly love to do, without looping or lapply-ing through t1
and rbinding (too much data for this to finish quickly enough -- this
is about 10% of what I'm eventually going to have to manage), is
convert t1 to one big dataframe.

On the other hand, I admit that I may be going about this wrongly from
the start; perhaps there's a better approach?

Any pointers would be most gratefully received.

Many thanks!


-- 
Regards,

Mike Nielsen


From matthew.r.robinson at ed.ac.uk  Sun Jul  9 00:41:51 2006
From: matthew.r.robinson at ed.ac.uk (M R Robinson)
Date: Sat, 08 Jul 2006 23:41:51 +0100
Subject: [R] denominator degrees of freedom and F-values in nlme
Message-ID: <20060708234151.x04rc33tpcgc400s@www.sms.ed.ac.uk>

Hello,

I am struggling to understand how denominator degrees of freedom and 
subsequent significance testing based upon them works in nlme models.

I have a data set of 736 measurements (weight), taken within 3 
different age groups, on 497 individuals who fall into two 
morphological catagories (horn types).

My model is:  Y ~ weight + horn type / age group, random=~1|individual

I am modeling this using glmm.PQL function with family=neg.bin 
(negative binomial distribution, estimating theta based upon a glm 
without individual as a random effect). My data set will not be 
balanced, with varying numbers of measurements taken on different 
individuals and some individuals have no weight measures just a 
morphological type.

My output:
              denDF    numberdf
Intercept     495
weight        232       1
horn type     495       1
horn type:age 232       4

So my question is where do these denDF come from and how are they 
calculated? I wish to then test significane of these fixed effects and 
can get F-ratio's and P-values but are these appropriate?

Thank-you for your time.
Kind regards
Matthew

*********************************
Matt Robinson

Institute of Evolutionary Biology
Room 413, Ashworth Labs,
King's Buildings,
University of Edinburgh
EH9 3JT, UK

Tel: 0131 650 5990


From mr.blacksheep at gmail.com  Sun Jul  9 01:20:28 2006
From: mr.blacksheep at gmail.com (Mike Nielsen)
Date: Sat, 8 Jul 2006 17:20:28 -0600
Subject: [R] Combining a list of similar dataframes into a single
	dataframe
In-Reply-To: <46a360560607081540l264ddd02xf4a4a99979fda4e@mail.gmail.com>
References: <46a360560607081540l264ddd02xf4a4a99979fda4e@mail.gmail.com>
Message-ID: <46a360560607081620kd93fa88ya30111d0b11fa886@mail.gmail.com>

Well, this worked, and rather more quickly than I had expected.

Many thanks to the dogs, who told me the answer in return for walking
them and feeding them!

> jj <- eval(parse(text=paste(sep=" ","rbind(",paste(sep=" ","t1[[",1:length(t1),"]]",collapse=","),")")))
> str(jj)
`data.frame':	85644 obs. of  4 variables:
 $ server      : Factor w/ 122 levels "AB93-99","AMP93-1",..: 1 1 1 1
1 1 1 1 1 1 ...
 $ ts          :'POSIXct', format: chr  "2006-06-30 12:31:44"
"2006-06-30 12:32:58" "2006-06-30 12:34:46" "2006-06-30 12:36:55" ...
 $ countername : Factor w/ 4 levels "Bytes Received/sec",..: 1 1 1 1 1
1 1 1 1 1 ...
 $ countervalue: num    NA  938  816 4213  906 ...
>

On 7/8/06, Mike Nielsen <mr.blacksheep at gmail.com> wrote:
> I would be very grateful to anyone who could point to the error of my
> ways in the following.
>
> I have a dataframe called net1, as such:
>
> > str(net1)
> `data.frame':    114192 obs. of  9 variables:
>  $ server         : Factor w/ 122 levels "AB93-99","AMP93-1",..: 1 1 1
> 1 1 1 1 1 1 1 ...
>  $ ts             :'POSIXct', format: chr  "2006-06-30 12:31:44"
> "2006-06-30 12:31:44" "2006-06-30 12:31:44" "2006-06-30 12:31:44" ...
>  $ instance       : Factor w/ 22 levels "1","2","Compaq Ethernet_Fast
> Ethernet Adapter_Module",..: 4 4 4 4 4 4 4 4 4 4 ...
>  $ instanceno     : Factor w/ 3 levels "1","2","3": 1 1 1 1 1 1 1 1 1 1 ...
>  $ perftime       : num  3.16e+13 3.16e+13 3.16e+13 3.16e+13 3.16e+13 ...
>  $ perffreq       : num  6.99e+08 6.99e+08 6.99e+08 6.99e+08 6.99e+08 ...
>  $ perftime100nsec: num  1.28e+17 1.28e+17 1.28e+17 1.28e+17 1.28e+17 ...
>  $ countername    : Factor w/ 4 levels "Bytes Received/sec",..: 1 3 2
> 4 1 3 2 4 1 3 ...
>  $ countervalue   : num  6.08e+07 6.64e+07 5.58e+06 1.00e+08 6.09e+07 ...
> >
>
> What I am trying to do is subset this thing down by server, instance,
> instanceno, countername and then apply a function to each subsetted
> dataframe.  The function performs a calculation on countervalue,
> essentially "collapsing" instanceno and instance down to a single
> value.
>
> Here is a snippet of my code:
> t1 <- by(net1,
>          list(
>               net1$server,
>               factor(as.character(net1$countername))),# get rid of
> unused levels of countername for this server
>          function(x){
>            g <- by(x,
>                    list(factor(as.character(x$instance)), # get rid of
> unused levels of instance for this server
>                    factor(as.character(x$instanceno))),   # same with instanceno
>
> function(y){c(NA,mean(y$perffreq)*diff(y$countervalue)/diff(y$perftime))})
>            data.frame(server=x$server,
>                       ts=x$ts,
>                       countername = x$countername,
>                       countervalue =
> apply(sapply(g[!sapply(g,is.null)],I),1,sum))
>          })
>
> So t1 then is a list of dataframes, each with an identical set of columns)
>
> > str(t1[[1]])
> `data.frame':   149 obs. of  4 variables:
>  $ server      : Factor w/ 122 levels "AB93-99","AMP93-1",..: 1 1 1 1
> 1 1 1 1 1 1 ...
>  $ ts          :'POSIXct', format: chr  "2006-06-30 12:31:44"
> "2006-06-30 12:32:58" "2006-06-30 12:34:46" "2006-06-30 12:36:55" ...
>  $ countername : Factor w/ 4 levels "Bytes Received/sec",..: 1 1 1 1 1
> 1 1 1 1 1 ...
>  $ countervalue: num    NA  938  816 4213  906 ...
>
> What I'd dearly love to do, without looping or lapply-ing through t1
> and rbinding (too much data for this to finish quickly enough -- this
> is about 10% of what I'm eventually going to have to manage), is
> convert t1 to one big dataframe.
>
> On the other hand, I admit that I may be going about this wrongly from
> the start; perhaps there's a better approach?
>
> Any pointers would be most gratefully received.
>
> Many thanks!
>
>
> --
> Regards,
>
> Mike Nielsen
>


-- 
Regards,

Mike Nielsen


From markleeds at verizon.net  Sun Jul  9 01:39:22 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Sat, 08 Jul 2006 18:39:22 -0500 (CDT)
Subject: [R] Adding Lines to Plot
Message-ID: <865827.1642041152401962170.JavaMail.root@vms171.mailsrvcs.net>

>From: justin rapp <jdrapp at gmail.com>
>Date: Sat Jul 08 14:05:12 CDT 2006
>To: r-help at stat.math.ethz.ch
>Subject: [R] Adding Lines to Plot

i'm not definite on this and even
if it works , it;s not the most
efficient
but

abline(h=10)
abline(h=20)
abline(h=30)
etc

i'd be curious if that works on a bar plot
because i don't use bar plots. someone else will answer that knows
more than me. i just like to try to help out once in a while because
i get soooo much help form this list.









>This seems like a question that I should be able to answer on my own
>but after looking at the documentation I cannot seem to find the
>correct method.
>
>How do I add lines to a bar plot that extend from the vertical axis?
>For example, my vertical axis is numbered in increments of 10 and I
>would like these to go across the whole graph.
>
>Also, is there a way to have R label the value of each bar so that I
>know the value of each factor that is being plotted?
>
>Thanks in advance.
>
>jdr
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From andy_liaw at merck.com  Sun Jul  9 02:50:00 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 8 Jul 2006 20:50:00 -0400
Subject: [R] Combining a list of similar dataframes into a single data
 frame [Broadcast]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA01BD0E1B@usctmx1106.merck.com>

A couple of suggestions:
 
1. This screams out for do.call.  Try jj <- do.call("rbind", t1).
2. Use rowSums() instead of apply(..., 1, sum).
 
Andy

  _____  

From: r-help-bounces at stat.math.ethz.ch on behalf of Mike Nielsen
Sent: Sat 7/8/2006 7:20 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Combining a list of similar dataframes into a single
dataframe [Broadcast]



Well, this worked, and rather more quickly than I had expected. 

Many thanks to the dogs, who told me the answer in return for walking 
them and feeding them! 

> jj <- eval(parse(text=paste(sep=" ","rbind(",paste(sep="
","t1[[",1:length(t1),"]]",collapse=","),")"))) 
> str(jj) 
`data.frame':   85644 obs. of  4 variables: 
 $ server      : Factor w/ 122 levels "AB93-99","AMP93-1",..: 1 1 1 1 
1 1 1 1 1 1 ... 
 $ ts          :'POSIXct', format: chr  "2006-06-30 12:31:44" 
"2006-06-30 12:32:58" "2006-06-30 12:34:46" "2006-06-30 12:36:55" ... 
 $ countername : Factor w/ 4 levels "Bytes Received/sec",..: 1 1 1 1 1 
1 1 1 1 1 ... 
 $ countervalue: num    NA  938  816 4213  906 ... 
> 

On 7/8/06, Mike Nielsen <mr.blacksheep at gmail.com> wrote: 
> I would be very grateful to anyone who could point to the error of my 
> ways in the following. 
> 
> I have a dataframe called net1, as such: 
> 
> > str(net1) 
> `data.frame':    114192 obs. of  9 variables: 
>  $ server         : Factor w/ 122 levels "AB93-99","AMP93-1",..: 1 1 1 
> 1 1 1 1 1 1 1 ... 
>  $ ts             :'POSIXct', format: chr  "2006-06-30 12:31:44" 
> "2006-06-30 12:31:44" "2006-06-30 12:31:44" "2006-06-30 12:31:44" ... 
>  $ instance       : Factor w/ 22 levels "1","2","Compaq Ethernet_Fast 
> Ethernet Adapter_Module",..: 4 4 4 4 4 4 4 4 4 4 ... 
>  $ instanceno     : Factor w/ 3 levels "1","2","3": 1 1 1 1 1 1 1 1 1 1
... 
>  $ perftime       : num  3.16e+13 3.16e+13 3.16e+13 3.16e+13 3.16e+13 ... 
>  $ perffreq       : num  6.99e+08 6.99e+08 6.99e+08 6.99e+08 6.99e+08 ... 
>  $ perftime100nsec: num  1.28e+17 1.28e+17 1.28e+17 1.28e+17 1.28e+17 ... 
>  $ countername    : Factor w/ 4 levels "Bytes Received/sec",..: 1 3 2 
> 4 1 3 2 4 1 3 ... 
>  $ countervalue   : num  6.08e+07 6.64e+07 5.58e+06 1.00e+08 6.09e+07 ... 
> > 
> 
> What I am trying to do is subset this thing down by server, instance, 
> instanceno, countername and then apply a function to each subsetted 
> dataframe.  The function performs a calculation on countervalue, 
> essentially "collapsing" instanceno and instance down to a single 
> value. 
> 
> Here is a snippet of my code: 
> t1 <- by(net1, 
>          list( 
>               net1$server, 
>               factor(as.character(net1$countername))),# get rid of 
> unused levels of countername for this server 
>          function(x){ 
>            g <- by(x, 
>                    list(factor(as.character(x$instance)), # get rid of 
> unused levels of instance for this server 
>                    factor(as.character(x$instanceno))),   # same with
instanceno 
> 
> function(y){c(NA,mean(y$perffreq)*diff(y$countervalue)/diff(y$perftime))})

>            data.frame(server=x$server, 
>                       ts=x$ts, 
>                       countername = x$countername, 
>                       countervalue = 
> apply(sapply(g[!sapply(g,is.null)],I),1,sum)) 
>          }) 
> 
> So t1 then is a list of dataframes, each with an identical set of columns)

> 
> > str(t1[[1]]) 
> `data.frame':   149 obs. of  4 variables: 
>  $ server      : Factor w/ 122 levels "AB93-99","AMP93-1",..: 1 1 1 1 
> 1 1 1 1 1 1 ... 
>  $ ts          :'POSIXct', format: chr  "2006-06-30 12:31:44" 
> "2006-06-30 12:32:58" "2006-06-30 12:34:46" "2006-06-30 12:36:55" ... 
>  $ countername : Factor w/ 4 levels "Bytes Received/sec",..: 1 1 1 1 1 
> 1 1 1 1 1 ... 
>  $ countervalue: num    NA  938  816 4213  906 ... 
> 
> What I'd dearly love to do, without looping or lapply-ing through t1 
> and rbinding (too much data for this to finish quickly enough -- this 
> is about 10% of what I'm eventually going to have to manage), is 
> convert t1 to one big dataframe. 
> 
> On the other hand, I admit that I may be going about this wrongly from 
> the start; perhaps there's a better approach? 
> 
> Any pointers would be most gratefully received. 
> 
> Many thanks! 
> 
> 
> -- 
> Regards, 
> 
> Mike Nielsen 
> 


-- 
Regards, 

Mike Nielsen 

______________________________________________ 
R-help at stat.math.ethz.ch mailing list 
https://stat.ethz.ch/mailman/listinfo/r-help
<https://stat.ethz.ch/mailman/listinfo/r-help>  
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
<http://www.R-project.org/posting-guide.html>


From ff809 at ncf.ca  Sun Jul  9 02:53:25 2006
From: ff809 at ncf.ca (Brian Lunergan)
Date: Sat, 08 Jul 2006 20:53:25 -0400
Subject: [R] Hunting for snow...
Message-ID: <44B05385.40001@ncf.ca>

Evening folks:

I did an install.views of finance and econometrics and between the two of 
them R reported 'FracSim', 'RDCOMClient', 'snow', 'VGAM', and 'segmented' 
as missing dependencies. Now, I've managed to hunt down what appear to be 
current zip file copies of all of the packages but 'snow'. Is there a 
windows edition of the package out there someplace that will get along with 
R v2.3.1 and if so, how do I find it?

Please and thanks...

-- 
Brian Lunergan
Nepean, Ontario
Canada


---
avast! Antivirus: Outbound message clean.
Virus Database (VPS): 0627-3, 2006-07-07
Tested on: 2006-07-08 20:53:28
avast! is copyright (c) 2000-2006 ALWIL Software.
http://www.avast.com


From andy_liaw at merck.com  Sun Jul  9 03:13:33 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 8 Jul 2006 21:13:33 -0400
Subject: [R] String mathematical function to R-function
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA01BD0E1C@usctmx1106.merck.com>

Here's one long-winded way of going about it, using R's ability to
manipulate functions as first class objects:
 
> f.string
[1] "x1 + sqrt(3*x2)"
> p.string <- parse(text=f.string)
> f <- function() {}        # empty function
> v <- all.vars(p.string)  # all the variables in the expression
> a <- do.call("alist", as.list(rep(TRUE, length(v)))) # contruct arguments
> names(a) <- v
> formals(f) <- a
> body(f) <- p.string
> f
function (x1 = TRUE, x2 = TRUE) 
x1 + sqrt(3 * x2)
> f(3, 4)
[1] 6.464102

Andy

  _____  

From: r-help-bounces at stat.math.ethz.ch on behalf of Renaud Lancelot
Sent: Sat 7/8/2006 6:25 PM
To: Cleber N.Borges
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] String mathematical function to R-function [Broadcast]



> fun_String <- "-100*x1 + 0*x2 + 100*x3" 
> 
> fun <- function(x1, x2, x3){ 
+   eval(parse(text = fun_String)) 
+   } 
> 
> fun(4:0, -2:2, 0:4) 
[1] -400 -200    0  200  400 

Best, 

Renaud 

2006/7/8, Cleber N.Borges <klebyn at yahoo.com.br>: 
> 
> 
> hello 
> 
> I make a subroutine that give-me a (mathematical) 
> function in string format. 
> I would like transform this string into function ( R 
> function ). 
> 
> thanks for any tips. 
> cleber 
> 
> 
> #e.g. 
> fun_String = "-100*x1 + 0*x2 + 100*x3" 
> 
> fun <- function(x1,x2,x3){ 
> return( 
> ############ 
> 
> evaluation( fun_String ) 
> 
> ############ 
> ) 
> 
> 
> 
> 
> 
> True String mathematical function :-(  :-( 
> 
> > nomes 
> [1] "8.49*x1*z1 + 6.13*x1*z2 + 6.4*x1*z3 + 6.9*x2*z1 + 
> 4.54*x2*z2 + 3.99*x2*z3 + 19.31*x3*z1 + 12.49*x3*z2 + 
> 3.86*x3*z3 + 5.25*x1*z1*z2 + -6.2*x1*z1*z3 + 
> 9.07*x1*z2*z3 + 10.87*x2*z1*z2 + 9.78*x2*z1*z3 + 
> 49.05*x2*z2*z3 + 4.56*x1*x2*z1 + -4.9*x1*x2*z2 + 
> 4.01*x1*x2*z3 + -0.39*x3*z1*z2 + 14.34*x3*z1*z3 + 
> 0.7*x3*z2*z3 + -2.84*x1*x3*z1 + 20.25*x1*x3*z2 + 
> 6.44*x1*x3*z3 + -4.91*x2*x3*z1 + 5.45*x2*x3*z2 + 
> 37.99*x2*x3*z3 + -22.24*x1*z1*z2*z3 + 
> -97.41*x2*z1*z2*z3 + -8.67*x1*x2*z1*z2 + 
> 49.14*x1*x2*z1*z3 + 14.24*x1*x2*z2*z3 + 
> 282.71*x3*z1*z2*z3 + 34.83*x1*x3*z1*z2 + 
> 111.2*x1*x3*z1*z3 + 101.38*x1*x3*z2*z3 + 
> -6.93*x2*x3*z1*z2 + 90.16*x2*x3*z1*z3 + 
> -9.11*x2*x3*z2*z3 + 17.22*x1*x2*x3*z1 + 
> -29.42*x1*x2*x3*z2 + -19.87*x1*x2*x3*z3 + 
> -277.41*x1*x2*z1*z2*z3 + -482.82*x1*x3*z1*z2*z3 + 
> -688.23*x2*x3*z1*z2*z3 + -588.4*x1*x2*x3*z1*z2 + 
> -197.31*x1*x2*x3*z1*z3 + -722.58*x1*x2*x3*z2*z3 + 
> 5536.59*x1*x2*x3*z1*z2*z3" 
> > 
> 
> 
> 
> 
> 
> _______________________________________________________ 
> 
> o discador agora! 
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
<https://stat.ethz.ch/mailman/listinfo/r-help>  
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
<http://www.R-project.org/posting-guide.html>  
> 


-- 
Renaud LANCELOT 
D?partement Elevage et M?decine V?t?rinaire (EMVT) du CIRAD 
Directeur adjoint charg? des affaires scientifiques 

CIRAD, Animal Production and Veterinary Medicine Department 
Deputy director for scientific affairs 

Campus international de Baillarguet 
TA 30 / B (B?t. B, Bur. 214) 
34398 Montpellier Cedex 5 - France 
T?l   +33 (0)4 67 59 37 17 
Secr. +33 (0)4 67 59 39 04 
Fax   +33 (0)4 67 59 37 95 

______________________________________________ 
R-help at stat.math.ethz.ch mailing list 
https://stat.ethz.ch/mailman/listinfo/r-help
<https://stat.ethz.ch/mailman/listinfo/r-help>  
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
<http://www.R-project.org/posting-guide.html>


From markleeds at verizon.net  Sun Jul  9 04:05:53 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Sat, 08 Jul 2006 21:05:53 -0500 (CDT)
Subject: [R] last 2 questions about save and load
Message-ID: <7949974.1039971152410753293.JavaMail.root@vms062.mailsrvcs.net>

i played around and i get the hang of save and load. I just
have two final questions :

1) is loading the RandomFields package the only
way to check if a file ( created through the save function )
is out there or not ? Because, I generally won't know if one has
been created yet or not.

2) Suppose, you have the case where you
created and saved a file. When you do an ls()
without loading that file, it doesn't show up in the list
obviously. So, if you want to delete that file for whatever
reason, is it possible ? Thanks.

                                           Mark


From ThadenJohnJ at uams.edu  Sun Jul  9 05:45:57 2006
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Sat, 8 Jul 2006 22:45:57 -0500
Subject: [R] package:Matrix handling of data with identical indices
Message-ID: <0C6BF3FC506F664F90C8BA3E0160462D04A75890@EXCHANGE3.ad.uams.edu>

In the Matrix package v. 0.995-11 I see that the dgTMatrix
Class for compressed, sparse, triplet-form matrices handles
Identically indexed data instances by summing their values,
e.g., 

library(Matrix)
(Mt <- new("dgTMatrix", 
   i = as.integer(c(0,0,1,1,4)),
   j = as.integer(c(0,1,2,2,4)),
   x = as.double(1:5),
   Dim = as.integer(c(5,5))))
## 5 x 5 sparse Matrix of class "dgTMatrix"
## [1,] 1 2 . . .
## [2,] . . 7 . .    <--- 7 = 3 + 4.
## [3,] . . . . .
## [4,] . . . . .
## [5,] . . . . 5

# If instead I make a dgCMatrix-class matrix, the first
# instance is overwritten by the second, e.g.,

library(Matrix)
(Mc <- new("dgCMatrix", 
   i = as.integer(c(0,0,1,1,4)),
   p = as.integer(c(0,1,2,4,5)),
   x = as.double(1:5),
   Dim = as.integer(c(5,5))))
## 5 x 5 sparse Matrix of class "dgCMatrix"
##             
## [1,] 1 2 . .
## [2,] . . 4 .   <-- the datum '3' has been lost.
## [3,] . . . .
## [4,] . . . .
## [5,] . . . 5 

# If one arrives at the dgCMatrix via the dgTMatrix class,
# the summed value is of course preserved, e.g.,

(Mtc <- as(Mt, "dgCMatrix"))
## 5 x 5 sparse Matrix of class "dgCMatrix"
##               
## [1,] 1 2 . . .
## [2,] . . 7 . .
## [3,] . . . . .
## [4,] . . . . .
## [5,] . . . . 5

As there is nothing inherent in either compressed, sparse,
format that would prevent recognition and handling of
duplicated index pairs, I'm curious why the dgCMatrix
class doesn't also add x values in those instances?
I wonder also if others might benefit also by being able
to choose how these instances are handled, i.e.,
whether they are summed, averaged or overwritten?  

-John Thaden, Ph.D.
Research Assistant Professor of Geriatrics
University of Arkansas for Medical Sciences
Little Rock AR, USA


Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From jim at bitwrit.com.au  Sun Jul  9 20:12:42 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sun, 09 Jul 2006 14:12:42 -0400
Subject: [R] Non-Numeric Histograms
In-Reply-To: <af81db5a0607080745u2c23bc72n89802a2ff54a7743@mail.gmail.com>
References: <af81db5a0607080745u2c23bc72n89802a2ff54a7743@mail.gmail.com>
Message-ID: <44B1471A.3080407@bitwrit.com.au>

justin rapp wrote:
> I have a dataset with a variable that is non-numeric.  I was wondering
> if there is an easy way in R to create a bar graph that will tell me
> how many of each level of this non-numeric variable are in the overall
> dataset.
> 
> Specifically, if my variable is LETTER, and I have A, B,C, D, E, F and
> I want to find out how many of each letter there are, what is the
> easiest way to do this in R and use the information to make a plot?
> 
Hi Justin,

You might want to take a look at freq and plot.freq in the plotrix package.

JIm


From spluque at gmail.com  Sun Jul  9 06:31:55 2006
From: spluque at gmail.com (Sebastian Luque)
Date: Sat, 08 Jul 2006 23:31:55 -0500
Subject: [R] dotplot (lattice) with panel.segments and groups
References: <8764i9whtu.fsf@arctocephalus.homelinux.org>
	<eb555e660607071153k544bbb84s3ed9fdd7cf063a0b@mail.gmail.com>
	<971536df0607071200x3e31cd0cid98ec0d4aa64cb07@mail.gmail.com>
	<eb555e660607071257q56bf1ae4nef95734ab50db390@mail.gmail.com>
	<971536df0607071424l3fc58865odde13757cce9cba5@mail.gmail.com>
Message-ID: <87psgfcsas.fsf@arctocephalus.homelinux.org>

"Gabor Grothendieck" <ggrothendieck at gmail.com> wrote:

On 7/7/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:

> On 7/7/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>>> Could you explain what panel.groups= does and what the difference
>>> is between panel.groups= and panel= ?  In ?xyplot it just says:

>>> panel.groups: useful mostly for 'xyplot' and 'densityplot'. Applies
>>> when 'panel' is 'panel.superpose' (which happens by default
>>> in these cases if 'groups' is non-null)

>>> which indicates when it might apply but not what it does.

>> That's wrong (it used to be right - a good example of why \synopsis is
>> bad). Since lattice 0.13-x, panel.superpose is never the default panel
>> function. An updated version with improved documentation should be out
>> soon.

>> 'panel.groups' is simply an argument to panel.superpose, and is
>> described in ?panel.superpose. Thus, it only makes sense as an
>> argument to xyplot/dotplot/whatever when the panel function is
>> panel.superpose, and not otherwise. The entry for the graphical
>> parameters in ?panel.superpose isn't as useful as it could be, I have
>> just updated it to read:

>> col, col.line, col.symbol, pch, cex, fill, font, fontface,
>> fontfamily, lty, lwd, alpha: graphical
>> parameters, replicated to be as long as the number of
>> groups.  These are eventually passed down to 'panel.groups',
>> but as scalars rather than vectors.  When 'panel.groups' is
>> called for the i-th level of 'groups', the corresponding
>> element of each graphical parameter is passed to it.

>> Hope that makes things a bit clearer.


Thanks Gabor and Deepayan for your responses.


Cheers,

-- 
Seb


From ripley at stats.ox.ac.uk  Sun Jul  9 09:31:28 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 9 Jul 2006 08:31:28 +0100 (BST)
Subject: [R] Hunting for snow...
In-Reply-To: <44B05385.40001@ncf.ca>
References: <44B05385.40001@ncf.ca>
Message-ID: <Pine.LNX.4.64.0607090821100.27488@gannet.stats.ox.ac.uk>

On Sat, 8 Jul 2006, Brian Lunergan wrote:

> Evening folks:
>
> I did an install.views of finance and econometrics and between the two of
> them R reported 'FracSim', 'RDCOMClient', 'snow', 'VGAM', and 'segmented'
> as missing dependencies. Now, I've managed to hunt down what appear to be
> current zip file copies of all of the packages but 'snow'. Is there a
> windows edition of the package out there someplace that will get along with
> R v2.3.1 and if so, how do I find it?
>
> Please and thanks...

PLEASE read the README:

http://cran.r-project.org/bin/windows/contrib/2.3/@ReadMe

and see the check summary at

http://cran.r-project.org/bin/windows/contrib/checkSummaryWin.html

snow runs distributed R tasks across hosts: it has three modes and
prefers PVM and MPI if packages rpvm or Rmpi are available (which they are 
not for Windows).  As a result only some of its functionality can be used 
on Windows, and anyone who is going to get involved at that level of 
programming will very easily install the package from the sources.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Sun Jul  9 09:37:14 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 9 Jul 2006 08:37:14 +0100 (BST)
Subject: [R] last 2 questions about save and load
In-Reply-To: <7949974.1039971152410753293.JavaMail.root@vms062.mailsrvcs.net>
References: <7949974.1039971152410753293.JavaMail.root@vms062.mailsrvcs.net>
Message-ID: <Pine.LNX.4.64.0607090832030.27488@gannet.stats.ox.ac.uk>

On Sat, 8 Jul 2006, markleeds at verizon.net wrote:

> i played around and i get the hang of save and load. I just
> have two final questions :
>
> 1) is loading the RandomFields package the only
> way to check if a file ( created through the save function )
> is out there or not ? Because, I generally won't know if one has
> been created yet or not.

Of course not: See ?file.exists and ?file.info.  (RandomFields::FileExists 
itself depends on file.exists and has a different purpose.)

> 2) Suppose, you have the case where you
> created and saved a file. When you do an ls()
> without loading that file, it doesn't show up in the list
> obviously. So, if you want to delete that file for whatever
> reason, is it possible ? Thanks.

ls() returns a list of (some) R objects: it has nothing to do with files.
Now save()d files contain images of R objects, but the name of the file 
has nothing to do with the names of the object(s) it contains.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ff809 at ncf.ca  Sun Jul  9 13:39:31 2006
From: ff809 at ncf.ca (Brian Lunergan)
Date: Sun, 09 Jul 2006 07:39:31 -0400
Subject: [R] Hunting for snow...
In-Reply-To: <Pine.LNX.4.64.0607090821100.27488@gannet.stats.ox.ac.uk>
References: <44B05385.40001@ncf.ca>
	<Pine.LNX.4.64.0607090821100.27488@gannet.stats.ox.ac.uk>
Message-ID: <44B0EAF3.5050304@ncf.ca>

Prof Brian Ripley wrote:
> On Sat, 8 Jul 2006, Brian Lunergan wrote:
> 
>> Evening folks:
>>
>> I did an install.views of finance and econometrics and between the two of
>> them R reported 'FracSim', 'RDCOMClient', 'snow', 'VGAM', and 'segmented'
>> as missing dependencies. Now, I've managed to hunt down what appear to be
>> current zip file copies of all of the packages but 'snow'. Is there a
>> windows edition of the package out there someplace that will get along 
>> with
>> R v2.3.1 and if so, how do I find it?
>>
>> Please and thanks...
> 
> PLEASE read the README:
> 
> http://cran.r-project.org/bin/windows/contrib/2.3/@ReadMe
> 
> and see the check summary at
> 
> http://cran.r-project.org/bin/windows/contrib/checkSummaryWin.html
> 
> snow runs distributed R tasks across hosts: it has three modes and
> prefers PVM and MPI if packages rpvm or Rmpi are available (which they 
> are not for Windows).  As a result only some of its functionality can be 
> used on Windows, and anyone who is going to get involved at that level 
> of programming will very easily install the package from the sources.
> 
Let me see if I understand this correctly. Something in the finance task 
view would seem to consider snow a dependency; however, the package in 
question has dragged it along for the ride into Windows, crippling its 
usefulness.

Problem and a question. Having checked the file on the UofT CRAN mirror 
that lists dependencies for the available files snow only appears attached 
to three of them, none of which are listed as part the finance view or 
anything I already have on my system. What is R seeing these as a 
dependency for then? Perhaps the view maintainers can suggest something? If 
it is not available for windows and is only partially functional there can 
it be safely ignored as a dependency?

One other puzzle resulting from this. The check summary suggested problems 
with 'FracSim', 'RDCOMClient', 'VGAM', and 'segmented' as windows packages 
but yet the versions I found seemed to install without complaint onto my 
system. Three gave back the successful unpack msg. VGAM did not, but 
several of the demo scripts ran without complaint so I am assuming it is 
comfortable with v2.3.1 on a windows system. Later versions of the four 
than were tested in the check summary, perhaps?

-- 
Brian Lunergan
Nepean, Ontario
Canada


---
avast! Antivirus: Outbound message clean.
Virus Database (VPS): 0627-3, 2006-07-07
Tested on: 2006-07-09 07:39:36
avast! is copyright (c) 2000-2006 ALWIL Software.
http://www.avast.com


From xenamaa at yahoo.com  Sun Jul  9 14:06:17 2006
From: xenamaa at yahoo.com (zana adeb)
Date: Sun, 9 Jul 2006 05:06:17 -0700 (PDT)
Subject: [R] R
Message-ID: <20060709120617.96319.qmail@web53113.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060709/0252befe/attachment.pl 

From murdoch at stats.uwo.ca  Sun Jul  9 14:21:56 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 09 Jul 2006 08:21:56 -0400
Subject: [R] Hunting for snow...
In-Reply-To: <44B0EAF3.5050304@ncf.ca>
References: <44B05385.40001@ncf.ca>	<Pine.LNX.4.64.0607090821100.27488@gannet.stats.ox.ac.uk>
	<44B0EAF3.5050304@ncf.ca>
Message-ID: <44B0F4E4.90300@stats.uwo.ca>

On 7/9/2006 7:39 AM, Brian Lunergan wrote:
> Prof Brian Ripley wrote:
>> On Sat, 8 Jul 2006, Brian Lunergan wrote:
>>
>>> Evening folks:
>>>
>>> I did an install.views of finance and econometrics and between the two of
>>> them R reported 'FracSim', 'RDCOMClient', 'snow', 'VGAM', and 'segmented'
>>> as missing dependencies. Now, I've managed to hunt down what appear to be
>>> current zip file copies of all of the packages but 'snow'. Is there a
>>> windows edition of the package out there someplace that will get along 
>>> with
>>> R v2.3.1 and if so, how do I find it?
>>>
>>> Please and thanks...
>> PLEASE read the README:
>>
>> http://cran.r-project.org/bin/windows/contrib/2.3/@ReadMe
>>
>> and see the check summary at
>>
>> http://cran.r-project.org/bin/windows/contrib/checkSummaryWin.html
>>
>> snow runs distributed R tasks across hosts: it has three modes and
>> prefers PVM and MPI if packages rpvm or Rmpi are available (which they 
>> are not for Windows).  As a result only some of its functionality can be 
>> used on Windows, and anyone who is going to get involved at that level 
>> of programming will very easily install the package from the sources.
>>
> Let me see if I understand this correctly. Something in the finance task 
> view would seem to consider snow a dependency; however, the package in 
> question has dragged it along for the ride into Windows, crippling its 
> usefulness.
> 
> Problem and a question. Having checked the file on the UofT CRAN mirror 
> that lists dependencies for the available files snow only appears attached 
> to three of them, none of which are listed as part the finance view or 
> anything I already have on my system. What is R seeing these as a 
> dependency for then? Perhaps the view maintainers can suggest something? If 
> it is not available for windows and is only partially functional there can 
> it be safely ignored as a dependency?
> 
> One other puzzle resulting from this. The check summary suggested problems 
> with 'FracSim', 'RDCOMClient', 'VGAM', and 'segmented' as windows packages 
> but yet the versions I found seemed to install without complaint onto my 
> system. Three gave back the successful unpack msg. VGAM did not, but 
> several of the demo scripts ran without complaint so I am assuming it is 
> comfortable with v2.3.1 on a windows system. Later versions of the four 
> than were tested in the check summary, perhaps?

The check summary does more stringent tests than just installing the 
package.  It checks whether the examples will run, and, if the package 
author supplies some tests, checks that test results match expected results.

For example, in the case of FracSim, the error occurred when trying to 
run the fracsim.1d example.  Apparently the example gave no finite 
values so plot() couldn't set up a coordinate system.  (You can see this 
if you click on the "ERROR" link in the summary.)

Since this appears to be doing random number generation, it may just be 
that the seed was set to a particularly unlucky value.  But the code 
does assume that no such unlucky values exist...

Duncan Murdoch


From ligges at statistik.uni-dortmund.de  Sun Jul  9 14:24:40 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 09 Jul 2006 14:24:40 +0200
Subject: [R] Hunting for snow...
In-Reply-To: <44B0EAF3.5050304@ncf.ca>
References: <44B05385.40001@ncf.ca>	<Pine.LNX.4.64.0607090821100.27488@gannet.stats.ox.ac.uk>
	<44B0EAF3.5050304@ncf.ca>
Message-ID: <44B0F588.9070701@statistik.uni-dortmund.de>

Brian Lunergan wrote:
> Prof Brian Ripley wrote:
>> On Sat, 8 Jul 2006, Brian Lunergan wrote:
>>
>>> Evening folks:
>>>
>>> I did an install.views of finance and econometrics and between the two of
>>> them R reported 'FracSim', 'RDCOMClient', 'snow', 'VGAM', and 'segmented'
>>> as missing dependencies. Now, I've managed to hunt down what appear to be
>>> current zip file copies of all of the packages but 'snow'. Is there a
>>> windows edition of the package out there someplace that will get along 
>>> with
>>> R v2.3.1 and if so, how do I find it?
>>>
>>> Please and thanks...
>> PLEASE read the README:
>>
>> http://cran.r-project.org/bin/windows/contrib/2.3/@ReadMe
>>
>> and see the check summary at
>>
>> http://cran.r-project.org/bin/windows/contrib/checkSummaryWin.html
>>
>> snow runs distributed R tasks across hosts: it has three modes and
>> prefers PVM and MPI if packages rpvm or Rmpi are available (which they 
>> are not for Windows).  As a result only some of its functionality can be 
>> used on Windows, and anyone who is going to get involved at that level 
>> of programming will very easily install the package from the sources.
>>
> Let me see if I understand this correctly. Something in the finance task 
> view would seem to consider snow a dependency; however, the package in 
> question has dragged it along for the ride into Windows, crippling its 
> usefulness.
> 
> Problem and a question. Having checked the file on the UofT CRAN mirror 
> that lists dependencies for the available files snow only appears attached 
> to three of them, none of which are listed as part the finance view or 
> anything I already have on my system. What is R seeing these as a 
> dependency for then? Perhaps the view maintainers can suggest something? If 
> it is not available for windows and is only partially functional there can 
> it be safely ignored as a dependency?
> 
> One other puzzle resulting from this. The check summary suggested problems 
> with 'FracSim', 'RDCOMClient', 'VGAM', and 'segmented' as windows packages 
> but yet the versions I found seemed to install without complaint onto my 

Where did you find them? At least not in their current versions on the 
official repository on CRAN master for R-2.3.x, I hope.


> system. Three gave back the successful unpack msg. VGAM did not, but 
> several of the demo scripts ran without complaint so I am assuming it is 
> comfortable with v2.3.1 on a windows system. Later versions of the four 
> than were tested in the check summary, perhaps?

The check summary is recent. I can only imagine that you found earlier 
versions...

Uwe Ligges


From xenamaa at yahoo.com  Sun Jul  9 14:43:46 2006
From: xenamaa at yahoo.com (zana adeb)
Date: Sun, 9 Jul 2006 05:43:46 -0700 (PDT)
Subject: [R] help on executing JRClient examples
Message-ID: <20060709124346.90925.qmail@web53105.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060709/64ccd0c2/attachment.pl 

From ff809 at ncf.ca  Sun Jul  9 15:34:25 2006
From: ff809 at ncf.ca (Brian Lunergan)
Date: Sun, 09 Jul 2006 09:34:25 -0400
Subject: [R] Hunting for snow...
In-Reply-To: <44B0F588.9070701@statistik.uni-dortmund.de>
References: <44B05385.40001@ncf.ca>
	<Pine.LNX.4.64.0607090821100.27488@gannet.stats.ox.ac.uk>
	<44B0EAF3.5050304@ncf.ca> <44B0F588.9070701@statistik.uni-dortmund.de>
Message-ID: <44B105E1.5050607@ncf.ca>

Uwe Ligges wrote:
> Brian Lunergan wrote:
>> Prof Brian Ripley wrote:
>>> On Sat, 8 Jul 2006, Brian Lunergan wrote:
>>>
>>>> Evening folks:
>>>>
>>>> I did an install.views of finance and econometrics and between the 
>>>> two of
>>>> them R reported 'FracSim', 'RDCOMClient', 'snow', 'VGAM', and 
>>>> 'segmented'
>>>> as missing dependencies. Now, I've managed to hunt down what appear 
>>>> to be
>>>> current zip file copies of all of the packages but 'snow'. Is there a
>>>> windows edition of the package out there someplace that will get 
>>>> along with
>>>> R v2.3.1 and if so, how do I find it?
>>>>
>>>> Please and thanks...
>>> PLEASE read the README:
>>>
>>> http://cran.r-project.org/bin/windows/contrib/2.3/@ReadMe
>>>
>>> and see the check summary at
>>>
>>> http://cran.r-project.org/bin/windows/contrib/checkSummaryWin.html
>>>
>>> snow runs distributed R tasks across hosts: it has three modes and
>>> prefers PVM and MPI if packages rpvm or Rmpi are available (which 
>>> they are not for Windows).  As a result only some of its 
>>> functionality can be used on Windows, and anyone who is going to get 
>>> involved at that level of programming will very easily install the 
>>> package from the sources.
>>>
>> Let me see if I understand this correctly. Something in the finance 
>> task view would seem to consider snow a dependency; however, the 
>> package in question has dragged it along for the ride into Windows, 
>> crippling its usefulness.
>>
>> Problem and a question. Having checked the file on the UofT CRAN 
>> mirror that lists dependencies for the available files snow only 
>> appears attached to three of them, none of which are listed as part 
>> the finance view or anything I already have on my system. What is R 
>> seeing these as a dependency for then? Perhaps the view maintainers 
>> can suggest something? If it is not available for windows and is only 
>> partially functional there can it be safely ignored as a dependency?
>>
>> One other puzzle resulting from this. The check summary suggested 
>> problems with 'FracSim', 'RDCOMClient', 'VGAM', and 'segmented' as 
>> windows packages but yet the versions I found seemed to install 
>> without complaint onto my 
> 
> Where did you find them? At least not in their current versions on the 
> official repository on CRAN master for R-2.3.x, I hope.

FracSim_0.2.zip
http://cran.stat.ucla.edu/bin/windows/contrib/2.3/

RDCOMClient_0.91-0.zip
http://www.omegahat.org/R/bin/windows/contrib/R-2.3.0/

segmented_0.1-4.zip
http://cran.r-project.org/bin/windows/contrib/2.1/

VGAM_0.6-9.zip
http://www.stat.auckland.ac.nz/~yee/bin/windows/contrib/2.3/

These are the versions I found and the locations. Other than perhaps 
segmented all seem to be positioned as compatible with v2.3.x, but I own 
that I could be incorrect in my assessment of the situation.

>> system. Three gave back the successful unpack msg. VGAM did not, but 
>> several of the demo scripts ran without complaint so I am assuming it 
>> is comfortable with v2.3.1 on a windows system. Later versions of the 
>> four than were tested in the check summary, perhaps?
> 
> The check summary is recent. I can only imagine that you found earlier 
> versions...

-- 
Brian Lunergan
Nepean, Ontario
Canada


---
avast! Antivirus: Outbound message clean.
Virus Database (VPS): 0627-3, 2006-07-07
Tested on: 2006-07-09 09:34:29
avast! is copyright (c) 2000-2006 ALWIL Software.
http://www.avast.com


From ligges at statistik.uni-dortmund.de  Sun Jul  9 15:51:57 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 09 Jul 2006 15:51:57 +0200
Subject: [R] Hunting for snow...
In-Reply-To: <44B105E1.5050607@ncf.ca>
References: <44B05385.40001@ncf.ca>
	<Pine.LNX.4.64.0607090821100.27488@gannet.stats.ox.ac.uk>
	<44B0EAF3.5050304@ncf.ca>
	<44B0F588.9070701@statistik.uni-dortmund.de>
	<44B105E1.5050607@ncf.ca>
Message-ID: <44B109FD.8000206@statistik.uni-dortmund.de>

Brian Lunergan wrote:
> Uwe Ligges wrote:
>> Brian Lunergan wrote:
>>> Prof Brian Ripley wrote:
>>>> On Sat, 8 Jul 2006, Brian Lunergan wrote:
>>>>
>>>>> Evening folks:
>>>>>
>>>>> I did an install.views of finance and econometrics and between the 
>>>>> two of
>>>>> them R reported 'FracSim', 'RDCOMClient', 'snow', 'VGAM', and 
>>>>> 'segmented'
>>>>> as missing dependencies. Now, I've managed to hunt down what appear 
>>>>> to be
>>>>> current zip file copies of all of the packages but 'snow'. Is there a
>>>>> windows edition of the package out there someplace that will get 
>>>>> along with
>>>>> R v2.3.1 and if so, how do I find it?
>>>>>
>>>>> Please and thanks...
>>>> PLEASE read the README:
>>>>
>>>> http://cran.r-project.org/bin/windows/contrib/2.3/@ReadMe
>>>>
>>>> and see the check summary at
>>>>
>>>> http://cran.r-project.org/bin/windows/contrib/checkSummaryWin.html
>>>>
>>>> snow runs distributed R tasks across hosts: it has three modes and
>>>> prefers PVM and MPI if packages rpvm or Rmpi are available (which 
>>>> they are not for Windows).  As a result only some of its 
>>>> functionality can be used on Windows, and anyone who is going to get 
>>>> involved at that level of programming will very easily install the 
>>>> package from the sources.
>>>>
>>> Let me see if I understand this correctly. Something in the finance 
>>> task view would seem to consider snow a dependency; however, the 
>>> package in question has dragged it along for the ride into Windows, 
>>> crippling its usefulness.
>>>
>>> Problem and a question. Having checked the file on the UofT CRAN 
>>> mirror that lists dependencies for the available files snow only 
>>> appears attached to three of them, none of which are listed as part 
>>> the finance view or anything I already have on my system. What is R 
>>> seeing these as a dependency for then? Perhaps the view maintainers 
>>> can suggest something? If it is not available for windows and is only 
>>> partially functional there can it be safely ignored as a dependency?
>>>
>>> One other puzzle resulting from this. The check summary suggested 
>>> problems with 'FracSim', 'RDCOMClient', 'VGAM', and 'segmented' as 
>>> windows packages but yet the versions I found seemed to install 
>>> without complaint onto my 
>>
>> Where did you find them? At least not in their current versions on the 
>> official repository on CRAN master for R-2.3.x, I hope.
> 
> FracSim_0.2.zip
> http://cran.stat.ucla.edu/bin/windows/contrib/2.3/
> 
> RDCOMClient_0.91-0.zip
> http://www.omegahat.org/R/bin/windows/contrib/R-2.3.0/
> 
> segmented_0.1-4.zip
> http://cran.r-project.org/bin/windows/contrib/2.1/
> 
> VGAM_0.6-9.zip
> http://www.stat.auckland.ac.nz/~yee/bin/windows/contrib/2.3/

None of them is on CRAN master:

- RDCOMClient is on Omegahat
- segmented is CRAN but outdated for R-2.1.x
- VGAM is in a private repository
- FracSim: I cannot access
http://cran.stat.ucla.edu/bin/windows/contrib/2.3/
right now, but I guess it does not sync properly. At least, the package 
should not be there.

Uwe Ligges


> These are the versions I found and the locations. Other than perhaps 
> segmented all seem to be positioned as compatible with v2.3.x, but I own 
> that I could be incorrect in my assessment of the situation.
> 
>>> system. Three gave back the successful unpack msg. VGAM did not, but 
>>> several of the demo scripts ran without complaint so I am assuming it 
>>> is comfortable with v2.3.1 on a windows system. Later versions of the 
>>> four than were tested in the check summary, perhaps?
>>
>> The check summary is recent. I can only imagine that you found earlier 
>> versions...
>


From jdrapp at gmail.com  Sun Jul  9 16:24:50 2006
From: jdrapp at gmail.com (justin rapp)
Date: Sun, 9 Jul 2006 10:24:50 -0400
Subject: [R] KS Test Warning Message
Message-ID: <af81db5a0607090724g73cc988cn78836d3e3059cba5@mail.gmail.com>

All,

Happy World Cup and Wimbledon.   This morning finds me with the first
of my many daily questions.

I am running a ks.test on residuals obtained from a regression model.

I use this code:
> ks.test(Year5.lm$residuals,pnorm)

and obtain this output
	One-sample Kolmogorov-Smirnov test

data:  Year5.lm$residuals
D = 0.7196, p-value < 2.2e-16
alternative hypothesis: two.sided

Warning message:
cannot compute correct p-values with ties in: ks.test(Year5.lm$residuals, pnorm)

I am wondering if anybody can tell me what this error message means.

Also, could anybody clarify how I could have a regression model with a
high Rsquared, rouglhy .67, but with nonnormal residuals?  Does this
take away from the validity of my model?

jdr


From spencer.graves at pdf.com  Sun Jul  9 17:48:50 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 09 Jul 2006 08:48:50 -0700
Subject: [R] Problem with garchFit function in fSeries
In-Reply-To: <6dbf89a50607060419k2a0892bdtd2264694e78a2278@mail.gmail.com>
References: <6dbf89a50607060419k2a0892bdtd2264694e78a2278@mail.gmail.com>
Message-ID: <44B12562.3030308@pdf.com>

	  Without a self-contained example, it's difficult to say for sure. 
However, this looks to me like false convergence:  garchFit uses a 
nonlinear iteration to try to maximize the likelihood by minimizing 
(-log(likelihood)).  The matrix of second partial derivatives of 
(-log(likelihood)) provide an approximation to the covariance matrix of 
the parameter estimates.  If it stops before appropriate convergence, 
some of the diagonal elements of this matrix might be negative.  When I 
listed 'garchFit' just now, I found that the line before the one 
generating your warning was as follows:

	    fit$cvar = solve(fit$hessian)

This seems consistent with what I just said.

	  If you want to try to get around this, I suggest you start by 
focusing on the argument 'control' mentioned on the help page for 
'garchFit'.  If you follow that through the help pages for 'nlminb' and 
'optim', plus possibly 'demo/xmpDWChapter34.R' mentioned in the 
examples, I believe you will likely find a way to get around that.

	  First, however, I suggest you plot the data series and take a careful 
look at it, if you haven't already.

	  Also, you say it worked fine for you with only 1000 of the 1600 
observations.  Was that observations 1:1000 of this same series?  If 
yes, did you try also observations 601:1600?  I just wonder if the data 
in the last 600 observations behave rather differently from the first 
1000?

	  Hope this helps.
	  Spencer Graves
p.s.  If you'd like further assistance from the listserve, please submit 
another post.  Before you do, however, I suggest you read the posting 
guide! "www.R-project.org/posting-guide.html", especially the part about 
including a simple, self-contained example.  I would NOT recommend you 
include 1600 observations in an email like this.  However, with common R 
download commands, you might be able to describe in a couple of commands 
how to get the data, then how you subsetted it, etc.

Ivan Kalafatic wrote:
> I used garchFit function to fit 1600 observations of EURO/USD 2-day returns
> in GARCH(1,1) model.
> As part of the summary I got warning message:
> NaNs produced in: sqrt(diag(fit$cvar))
> 
> And didn't get any estimates for 3 params' std.error, t value or
> probability:
> 
> Error Analysis:
>         Estimate  Std. Error  t value Pr(>|t|)
> mu     -0.004827    0.020141   -0.240    0.811
> ar1     0.010311    0.026978    0.382    0.702
> omega   0.073813          NA       NA       NA
> alpha1  0.100000          NA       NA       NA
> beta1   0.800000          NA       NA       NA
> 
> When I reduced the sample size to 1000 observations output was ok.
> Can anyone help? What does this mean? What can be the cause of NA values?
> 
> Thank you,
> Ivan
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From bates at stat.wisc.edu  Sun Jul  9 18:06:00 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 9 Jul 2006 11:06:00 -0500
Subject: [R] package:Matrix handling of data with identical indices
In-Reply-To: <0C6BF3FC506F664F90C8BA3E0160462D04A75890@EXCHANGE3.ad.uams.edu>
References: <0C6BF3FC506F664F90C8BA3E0160462D04A75890@EXCHANGE3.ad.uams.edu>
Message-ID: <40e66e0b0607090906u21db4786qcab1b25a37cdcac2@mail.gmail.com>

The apparent contradiction here is again a case of inadequate
documentation and checking.

Because the order of the triplets in the triplet form of a sparse
matrix is essentially random it is permissible to have repeated
indices.  As you have seen, the interpretation of repeated indices is
that the value at any index is the sum of the values in the triplets
corresponding to that index.

 It is not permissible to have repeated indices in the compressed
form.  In the compressed form there is a well-defined ordering of the
indices, first by columns then by row within column and the row
indices must be increasing within columns.

Your matrix Mc should be flagged as invalid.  Martin and I should
discuss whether we want to add such a test to the validity method.  It
is not difficult to add the test but there will be a penalty in that
it will slow down all operations on such matrices and I'm not sure if
we want to pay that price to catch a rather infrequently occuring
problem.

There is some documentation of the internal representations of these
formats in the directory $R_LIB/Matrix/doc/UFSparse/.  The User Guides
in that directory are taken directly from the various sparse matrix
packages that Tim Davis at the University of Florida has written.  He
has a book that is scheduled for publication this  September

Tim Davis (2006), Direct Methods for Sparse Linear Systems, SIAM,
Philadelphia, PA

I hope we will be able to refer to that book for details of the
representation and algorithms.

On 7/8/06, Thaden, John J <ThadenJohnJ at uams.edu> wrote:
> In the Matrix package v. 0.995-11 I see that the dgTMatrix
> Class for compressed, sparse, triplet-form matrices handles
> Identically indexed data instances by summing their values,
> e.g.,
>
> library(Matrix)
> (Mt <- new("dgTMatrix",
>    i = as.integer(c(0,0,1,1,4)),
>    j = as.integer(c(0,1,2,2,4)),
>    x = as.double(1:5),
>    Dim = as.integer(c(5,5))))
> ## 5 x 5 sparse Matrix of class "dgTMatrix"
> ## [1,] 1 2 . . .
> ## [2,] . . 7 . .    <--- 7 = 3 + 4.
> ## [3,] . . . . .
> ## [4,] . . . . .
> ## [5,] . . . . 5
>
> # If instead I make a dgCMatrix-class matrix, the first
> # instance is overwritten by the second, e.g.,
>
> library(Matrix)
> (Mc <- new("dgCMatrix",
>    i = as.integer(c(0,0,1,1,4)),
>    p = as.integer(c(0,1,2,4,5)),
>    x = as.double(1:5),
>    Dim = as.integer(c(5,5))))
> ## 5 x 5 sparse Matrix of class "dgCMatrix"
> ##
> ## [1,] 1 2 . .
> ## [2,] . . 4 .   <-- the datum '3' has been lost.
> ## [3,] . . . .
> ## [4,] . . . .
> ## [5,] . . . 5
>
> # If one arrives at the dgCMatrix via the dgTMatrix class,
> # the summed value is of course preserved, e.g.,
>
> (Mtc <- as(Mt, "dgCMatrix"))
> ## 5 x 5 sparse Matrix of class "dgCMatrix"
> ##
> ## [1,] 1 2 . . .
> ## [2,] . . 7 . .
> ## [3,] . . . . .
> ## [4,] . . . . .
> ## [5,] . . . . 5
>
> As there is nothing inherent in either compressed, sparse,
> format that would prevent recognition and handling of
> duplicated index pairs, I'm curious why the dgCMatrix
> class doesn't also add x values in those instances?
> I wonder also if others might benefit also by being able
> to choose how these instances are handled, i.e.,
> whether they are summed, averaged or overwritten?
>
> -John Thaden, Ph.D.
> Research Assistant Professor of Geriatrics
> University of Arkansas for Medical Sciences
> Little Rock AR, USA
>
>
> Confidentiality Notice: This e-mail message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information.  Any unauthorized review, use, disclosure or distribution is prohibited.  If you are not the intended recipient, please contact the sender by reply e-mail and destroy all copies of the original message.
>
>


From bates at stat.wisc.edu  Sun Jul  9 18:19:41 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 9 Jul 2006 11:19:41 -0500
Subject: [R] denominator degrees of freedom and F-values in nlme
In-Reply-To: <20060708234151.x04rc33tpcgc400s@www.sms.ed.ac.uk>
References: <20060708234151.x04rc33tpcgc400s@www.sms.ed.ac.uk>
Message-ID: <40e66e0b0607090919v1163f693g648218d69a43c5fc@mail.gmail.com>

On 7/8/06, M R Robinson <matthew.r.robinson at ed.ac.uk> wrote:
> Hello,
>
> I am struggling to understand how denominator degrees of freedom and
> subsequent significance testing based upon them works in nlme models.
>
> I have a data set of 736 measurements (weight), taken within 3
> different age groups, on 497 individuals who fall into two
> morphological catagories (horn types).
>
> My model is:  Y ~ weight + horn type / age group, random=~1|individual
>
> I am modeling this using glmm.PQL function with family=neg.bin
> (negative binomial distribution, estimating theta based upon a glm
> without individual as a random effect). My data set will not be
> balanced, with varying numbers of measurements taken on different
> individuals and some individuals have no weight measures just a
> morphological type.
>
> My output:
>               denDF    numberdf
> Intercept     495
> weight        232       1
> horn type     495       1
> horn type:age 232       4
>
> So my question is where do these denDF come from and how are they
> calculated? I wish to then test significane of these fixed effects and
> can get F-ratio's and P-values but are these appropriate?

The algorithm for calculating those denominator degrees of freedom is
given in Chapter 2 of Pinheiro and Bates (2000), Mixed-effects Models
in S and S-PLUS, Springer.  It was designed to reproduce the results
of the BETWEENWITHIN option in SAS PROC MIXED.  On looking at that
algorithm recently I no longer feel that it is a good way of doing the
calculation but I don't have a better alternative at present.

Also, that algorithm and the use of the F test is suggested for linear
mixed models.  I'm not sure that it would apply "out of the box" to a
generalized liner mixed model, which is what you are fitting here.
However, for practical purposes you could assume a "worst case" of 232
denominator degrees of freedom for all terms because there is so
little difference between an F statistic with 232 denominator degrees
of freedom and one with  495 denominator degrees of freedom.

> Thank-you for your time.
> Kind regards
> Matthew
>
> *********************************
> Matt Robinson
>
> Institute of Evolutionary Biology
> Room 413, Ashworth Labs,
> King's Buildings,
> University of Edinburgh
> EH9 3JT, UK
>
> Tel: 0131 650 5990
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From Charles.Annis at StatisticalEngineering.com  Sun Jul  9 18:54:41 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Sun, 9 Jul 2006 12:54:41 -0400
Subject: [R] string problems with "\\" (Windows)
Message-ID: <05bd01c6a378$5f764fb0$6600a8c0@DD4XFW31>

Greetings, R-ians:

I'm using R 2.3.1 on WindowsXP.

I need to find the name of a file at the end of a sting that contains the
path + file, with the problematic "\\" as separators. 

The string looks something like this:

"C:\\Documents and Settings\\myName\\My Documents\\R
Projects\\Project1\\file.name.csv"

What I want is "file.name.csv"

Currently I use the name of the project as the splitter in

strsplit(string.name, split="Project1", extended = FALSE)

This works, of course, but I won't always be using folder Project1, so I
need a more universal way to find the name of a file at the end of a string
with "\\" separators.

Can I get there from here?  (I've looked through previous R-help listing of
similar problems but if it's there, I missed it.)

Thanks.


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:? 614-455-3265
http://www.StatisticalEngineering.com
?


From ggrothendieck at gmail.com  Sun Jul  9 19:03:00 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 9 Jul 2006 13:03:00 -0400
Subject: [R] string problems with "\\" (Windows)
In-Reply-To: <05bd01c6a378$5f764fb0$6600a8c0@DD4XFW31>
References: <05bd01c6a378$5f764fb0$6600a8c0@DD4XFW31>
Message-ID: <971536df0607091003w7095e515rcd332bc8d1f0238f@mail.gmail.com>

Try:

basename(string.name)


On 7/9/06, Charles Annis, P.E. <Charles.Annis at statisticalengineering.com> wrote:
> Greetings, R-ians:
>
> I'm using R 2.3.1 on WindowsXP.
>
> I need to find the name of a file at the end of a sting that contains the
> path + file, with the problematic "\\" as separators.
>
> The string looks something like this:
>
> "C:\\Documents and Settings\\myName\\My Documents\\R
> Projects\\Project1\\file.name.csv"
>
> What I want is "file.name.csv"
>
> Currently I use the name of the project as the splitter in
>
> strsplit(string.name, split="Project1", extended = FALSE)
>
> This works, of course, but I won't always be using folder Project1, so I
> need a more universal way to find the name of a file at the end of a string
> with "\\" separators.
>
> Can I get there from here?  (I've looked through previous R-help listing of
> similar problems but if it's there, I missed it.)
>
> Thanks.
>
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax: 614-455-3265
> http://www.StatisticalEngineering.com
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jholtman at gmail.com  Sun Jul  9 19:03:36 2006
From: jholtman at gmail.com (jim holtman)
Date: Sun, 9 Jul 2006 13:03:36 -0400
Subject: [R] string problems with "\\" (Windows)
In-Reply-To: <05bd01c6a378$5f764fb0$6600a8c0@DD4XFW31>
References: <05bd01c6a378$5f764fb0$6600a8c0@DD4XFW31>
Message-ID: <644e1f320607091003g79777da8va167261d81c9b527@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060709/c8327ad4/attachment.pl 

From murdoch at stats.uwo.ca  Sun Jul  9 19:08:28 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 09 Jul 2006 13:08:28 -0400
Subject: [R] string problems with "\\" (Windows)
In-Reply-To: <05bd01c6a378$5f764fb0$6600a8c0@DD4XFW31>
References: <05bd01c6a378$5f764fb0$6600a8c0@DD4XFW31>
Message-ID: <44B1380C.8080103@stats.uwo.ca>

basename() works.  If you're on a system that doesn't think \\ is a path 
separator, you could do something like

x <- "C:\\Documents and Settings\\myName\\My 
Documents\\RProjects\\Project1\\file.name.csv"
basename(gsub('\\\\','/',x))

Duncan Murdoch

Charles Annis, P.E. wrote:
> Greetings, R-ians:
>
> I'm using R 2.3.1 on WindowsXP.
>
> I need to find the name of a file at the end of a sting that contains the
> path + file, with the problematic "\\" as separators. 
>
> The string looks something like this:
>
> "C:\\Documents and Settings\\myName\\My Documents\\R
> Projects\\Project1\\file.name.csv"
>
> What I want is "file.name.csv"
>
> Currently I use the name of the project as the splitter in
>
> strsplit(string.name, split="Project1", extended = FALSE)
>
> This works, of course, but I won't always be using folder Project1, so I
> need a more universal way to find the name of a file at the end of a string
> with "\\" separators.
>
> Can I get there from here?  (I've looked through previous R-help listing of
> similar problems but if it's there, I missed it.)
>
> Thanks.
>
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>  
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From arnau.mir at uib.es  Sun Jul  9 19:21:47 2006
From: arnau.mir at uib.es (Arnau Mir)
Date: Sun, 9 Jul 2006 19:21:47 +0200 (CEST)
Subject: [R] distance in kmeans algorithm?
Message-ID: <62161.85.60.32.21.1152465707.squirrel@mnm.uib.es>

Hello.

Is it possible to choose the distance in the kmeans algorithm?

I have m vectors of n components and I want to cluster them using kmeans
algorithm but I want to use the Mahalanobis distance or another distance.

How can I do it in R?
If I use kmeans, I have no option to choose the distance.

Thanks in advance,

Arnau.


From roger at ysidro.econ.uiuc.edu  Sun Jul  9 19:31:16 2006
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Sun, 9 Jul 2006 12:31:16 -0500
Subject: [R] package:Matrix handling of data with identical indices
In-Reply-To: <40e66e0b0607090906u21db4786qcab1b25a37cdcac2@mail.gmail.com>
References: <0C6BF3FC506F664F90C8BA3E0160462D04A75890@EXCHANGE3.ad.uams.edu>
	<40e66e0b0607090906u21db4786qcab1b25a37cdcac2@mail.gmail.com>
Message-ID: <D91C1AE1-9352-4D87-8D29-210F9AED0060@ysidro.econ.uiuc.edu>

>

On 7/8/06, Thaden, John J <ThadenJohnJ at uams.edu> wrote:

> As there is nothing inherent in either compressed, sparse,
> format that would prevent recognition and handling of
> duplicated index pairs, I'm curious why the dgCMatrix
> class doesn't also add x values in those instances?

why not multiply them?  or take the larger one, or ...?  I would
interpret this as a case of user negligence -- there is no
"natural" default behavior for such cases.

On Jul 9, 2006, at 11:06 AM, Douglas Bates wrote:

> Your matrix Mc should be flagged as invalid.  Martin and I should
> discuss whether we want to add such a test to the validity method.  It
> is not difficult to add the test but there will be a penalty in that
> it will slow down all operations on such matrices and I'm not sure if
> we want to pay that price to catch a rather infrequently occuring
> problem.

Elaborating the validity procedure to flag such instances seems
to be well worth the  speed penalty in my view.  Of course,
anticipating every such misstep imposes a heavy burden
on developers and constitutes the real "cost" of more elaborate
validity checking.

[My 2cents based on experience with SparseM.]

url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820


From ripley at stats.ox.ac.uk  Sun Jul  9 19:44:12 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 9 Jul 2006 18:44:12 +0100 (BST)
Subject: [R] distance in kmeans algorithm?
In-Reply-To: <62161.85.60.32.21.1152465707.squirrel@mnm.uib.es>
References: <62161.85.60.32.21.1152465707.squirrel@mnm.uib.es>
Message-ID: <Pine.LNX.4.64.0607091836530.9564@gannet.stats.ox.ac.uk>

You do realize that Mahalanobis distance is just Euclidean distance on 
some linear transformation of the variables?  So all you need to do is to 
transform the data you pass to kmeans to 'sphere' the Mahalanobis 
distance.

The K means *algorithms* do depend on Euclidean distance (e.g. in choosing 
the cluster centres as the centroids), so your initial question makes 
little sense.  You can of course use the criterion with other distances, 
but you need to develop other algorithms to do so.

On Sun, 9 Jul 2006, Arnau Mir wrote:

> Hello.
>
> Is it possible to choose the distance in the kmeans algorithm?
>
> I have m vectors of n components and I want to cluster them using kmeans
> algorithm but I want to use the Mahalanobis distance or another distance.
>
> How can I do it in R?
> If I use kmeans, I have no option to choose the distance.
>
> Thanks in advance,
>
> Arnau.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ff809 at ncf.ca  Sun Jul  9 20:35:17 2006
From: ff809 at ncf.ca (Brian Lunergan)
Date: Sun, 09 Jul 2006 14:35:17 -0400
Subject: [R] Choice of repository and outdated vs unusable... was (Hunting
 for snow...)
In-Reply-To: <44B109FD.8000206@statistik.uni-dortmund.de>
References: <44B05385.40001@ncf.ca>
	<Pine.LNX.4.64.0607090821100.27488@gannet.stats.ox.ac.uk>
	<44B0EAF3.5050304@ncf.ca> <44B0F588.9070701@statistik.uni-dortmund.de>
	<44B105E1.5050607@ncf.ca> <44B109FD.8000206@statistik.uni-dortmund.de>
Message-ID: <44B14C65.8080707@ncf.ca>

Uwe Ligges wrote:
>>> Where did you find them? At least not in their current versions on 
>>> the official repository on CRAN master for R-2.3.x, I hope.
>>
>> FracSim_0.2.zip
>> http://cran.stat.ucla.edu/bin/windows/contrib/2.3/
>>
>> RDCOMClient_0.91-0.zip
>> http://www.omegahat.org/R/bin/windows/contrib/R-2.3.0/
>>
>> segmented_0.1-4.zip
>> http://cran.r-project.org/bin/windows/contrib/2.1/
>>
>> VGAM_0.6-9.zip
>> http://www.stat.auckland.ac.nz/~yee/bin/windows/contrib/2.3/
> 
> None of them is on CRAN master:
> 
> - RDCOMClient is on Omegahat
> - segmented is CRAN but outdated for R-2.1.x
> - VGAM is in a private repository
> - FracSim: I cannot access
> http://cran.stat.ucla.edu/bin/windows/contrib/2.3/
> right now, but I guess it does not sync properly. At least, the package 
> should not be there.
> 
> Uwe Ligges

Uwe:

So VGAM and RDCOMClient are on private repository sites. Is that so 
important? If a package is not on the CRAN master site are you supposed to 
avoid using it?

The FracSim home site indicated that 0.2 was the current version and a 
google search yielded the site I indicated, among others, as a repository 
of an apparent windows edition. That it's not on the master is more an 
issue between the maintainers of the master and the maintainers of the 
package, not the average end user IMHO. I found a source for what is 
apparently the current edition. What's wrong with that?

With regard to segmented, outdated does not mean unusable unless later 
editions of R or packages that call it as a dependency use features 
particular to later editions. If there is a newer windows edition of 
segmented then please point out a source for it.

Regards...
-- 
Brian Lunergan
Nepean, Ontario
Canada


---
avast! Antivirus: Outbound message clean.
Virus Database (VPS): 0627-3, 2006-07-07
Tested on: 2006-07-09 14:35:21
avast! is copyright (c) 2000-2006 ALWIL Software.
http://www.avast.com


From ThadenJohnJ at uams.edu  Sun Jul  9 20:53:48 2006
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Sun, 9 Jul 2006 13:53:48 -0500
Subject: [R] package:Matrix handling of data with identical indices
In-Reply-To: <D91C1AE1-9352-4D87-8D29-210F9AED0060@ysidro.econ.uiuc.edu>
Message-ID: <0C6BF3FC506F664F90C8BA3E0160462D04A75891@EXCHANGE3.ad.uams.edu>

On Sunday, July 09, 2006 12:31 PM, Roger Koenker = RK
<roger at ysidro.econ.uiuc.edu> wrote 

RK> On 7/8/06, Thaden, John J <ThadenJohnJ at uams.edu> wrote:

    JT> As there is nothing inherent in either compressed, sparse,
    JT> format that would prevent recognition and handling of
    JT> duplicated index pairs, I'm curious why the dgCMatrix
    JT> class doesn't also add x values in those instances?

RK> why not multiply them?  or take the larger one, 
RK> or ...?  I would interpret this as a case of user
RK> negligence -- there is no "natural" default behavior
RK> for such cases.

This user created example data to illustrate his question, but
of course he faces real data, analytical chemical in this case,
data that happen to come with an 8.4% occurrence of non-unique
index pairs, and also, quite literally, with a "natural" way 
to treat cases (the ~nature~ of the assay makes it correct to
sum them).  I can think of other natural data sets where 
averaging would be the "natural" behavior. So you are right 
that there is no "default" natural behavior, thus, my 
suggestion to leave that to user choice via function argument
or class slot, defaulted to summing.

Actually in this case there ~is~ one behavior superior to 
summing -- abstracting one of the data pair (that share indices)
into a second (very sparse) "overlay" matrix.  Perhaps it is
my negligence not to have done this instead querying the list :-)
I am doing it now.

Regards,
-John Thaden 

RK> On Jul 9, 2006, at 11:06 AM, Douglas Bates wrote:

  DB> Your matrix Mc should be flagged as invalid.  Martin and I should
  DB> discuss whether we want to add such a test to the validity method.
It
  DB> is not difficult to add the test but there will be a penalty in
that
  DB> it will slow down all operations on such matrices and I'm not sure
if
  DB> we want to pay that price to catch a rather infrequently occuring
  DB> problem.

RK> Elaborating the validity procedure to flag such instances seems
RK> to be well worth the  speed penalty in my view.  Of course,
RK> anticipating every such misstep imposes a heavy burden
RK> on developers and constitutes the real "cost" of more elaborate
RK> validity checking.
RK>
RK> [My 2cents based on experience with SparseM.]

Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From jdrapp at gmail.com  Sun Jul  9 21:32:17 2006
From: jdrapp at gmail.com (justin rapp)
Date: Sun, 9 Jul 2006 15:32:17 -0400
Subject: [R] Error Calculating Mean
Message-ID: <af81db5a0607091232t3592d331qbe29014ca70acda8@mail.gmail.com>

I have a vector containing players' weights.  When I enter


mode(data.linear$Weight)
"numeric" is returned.


When I type mean(data.linear$Weight)

NA is returned.

Any ideas as to why this may be the case?  I am trying to calculate
this ultimately so I can superimpose a normal density line over a
histogram containing the weights?


From mr.blacksheep at gmail.com  Sun Jul  9 21:45:22 2006
From: mr.blacksheep at gmail.com (Mike Nielsen)
Date: Sun, 9 Jul 2006 13:45:22 -0600
Subject: [R] Error Calculating Mean
In-Reply-To: <af81db5a0607091232t3592d331qbe29014ca70acda8@mail.gmail.com>
References: <af81db5a0607091232t3592d331qbe29014ca70acda8@mail.gmail.com>
Message-ID: <46a360560607091245w3ef06226n2fe42b6d8c53243f@mail.gmail.com>

I'd hazard a guess that data.linear$Weight may have a "Not Available"
data point (ie. missing data).

> f <- c(NA,rnorm(10))
> mode(f)
[1] "numeric"
> mean(f)
[1] NA

If you'd like to compute the mean anyway, you can use
> mean(f,na.rm=TRUE)
[1] 0.3433036


On 7/9/06, justin rapp <jdrapp at gmail.com> wrote:
> I have a vector containing players' weights.  When I enter
>
>
> mode(data.linear$Weight)
> "numeric" is returned.
>
>
> When I type mean(data.linear$Weight)
>
> NA is returned.
>
> Any ideas as to why this may be the case?  I am trying to calculate
> this ultimately so I can superimpose a normal density line over a
> histogram containing the weights?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
Regards,

Mike Nielsen


From ThadenJohnJ at uams.edu  Mon Jul 10 02:40:21 2006
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Sun, 9 Jul 2006 19:40:21 -0500
Subject: [R] package:Matrix handling of data with identical indices
In-Reply-To: <40e66e0b0607090906u21db4786qcab1b25a37cdcac2@mail.gmail.com>
Message-ID: <0C6BF3FC506F664F90C8BA3E0160462D04A75893@EXCHANGE3.ad.uams.edu>

Thanks. I see in the UMFPACK manual that the convention for csc matrices
for monotonic-increasing row indices is as you say.  

I notice UMFPACK flags errors liberally. What if Matrix or some other
package were made to interface with UMFPACK directly, to tap these and
other features?

-John Thaden

dmbates at gmail.com wrote:

...
...
DB> It is not permissible to have repeated indices in the compressed
form.  In the compressed form there is a well-defined ordering of the
indices, first by columns then by row within column and the row
indices must be increasing within columns.

DB> Your matrix Mc should be flagged as invalid.  Martin and I should
discuss whether we want to add such a test to the validity method.  It
is not difficult to add the test but there will be a penalty in that
it will slow down all operations on such matrices and I'm not sure if
we want to pay that price to catch a rather infrequently occuring
problem.

DB> There is some documentation of the internal representations of these
formats in the directory $R_LIB/Matrix/doc/UFSparse/.  The User Guides
in that directory are taken directly from the various sparse matrix
packages that Tim Davis at the University of Florida has written.  He
has a book that is scheduled for publication this  September

DB> Tim Davis (2006), Direct Methods for Sparse Linear Systems, SIAM,
Philadelphia, PA

DB> I hope we will be able to refer to that book for details of the
representation and algorithms.

On 7/8/06, Thaden, John J <ThadenJohnJ at uams.edu> wrote:
> In the Matrix package v. 0.995-11 I see that the dgTMatrix
> Class for compressed, sparse, triplet-form matrices handles
> Identically indexed data instances by summing their values,
> e.g.,
>
> library(Matrix)
> (Mt <- new("dgTMatrix",
>    i = as.integer(c(0,0,1,1,4)),
>    j = as.integer(c(0,1,2,2,4)),
>    x = as.double(1:5),
>    Dim = as.integer(c(5,5))))
> ## 5 x 5 sparse Matrix of class "dgTMatrix"
> ## [1,] 1 2 . . .
> ## [2,] . . 7 . .    <--- 7 = 3 + 4.
> ## [3,] . . . . .
> ## [4,] . . . . .
> ## [5,] . . . . 5
>
> # If instead I make a dgCMatrix-class matrix, the first
> # instance is overwritten by the second, e.g.,
>
> library(Matrix)
> (Mc <- new("dgCMatrix",
>    i = as.integer(c(0,0,1,1,4)),
>    p = as.integer(c(0,1,2,4,5)),
>    x = as.double(1:5),
>    Dim = as.integer(c(5,5))))
> ## 5 x 5 sparse Matrix of class "dgCMatrix"
> ##
> ## [1,] 1 2 . .
> ## [2,] . . 4 .   <-- the datum '3' has been lost.
> ## [3,] . . . .
> ## [4,] . . . .
> ## [5,] . . . 5
>
> # If one arrives at the dgCMatrix via the dgTMatrix class,
> # the summed value is of course preserved, e.g.,
>
> (Mtc <- as(Mt, "dgCMatrix"))
> ## 5 x 5 sparse Matrix of class "dgCMatrix"
> ##
> ## [1,] 1 2 . . .
> ## [2,] . . 7 . .
> ## [3,] . . . . .
> ## [4,] . . . . .
> ## [5,] . . . . 5
>
> As there is nothing inherent in either compressed, sparse,
> format that would prevent recognition and handling of
> duplicated index pairs, I'm curious why the dgCMatrix
> class doesn't also add x values in those instances?
> I wonder also if others might benefit also by being able
> to choose how these instances are handled, i.e.,
> whether they are summed, averaged or overwritten?
>
> -John Thaden, Ph.D.
> Research Assistant Professor of Geriatrics
> University of Arkansas for Medical Sciences
> Little Rock AR, USA
>
>
> Confidentiality Notice: This e-mail message, including any
attachments, is for the sole use of the intended recipient(s) and may
contain confidential and privileged information.  Any unauthorized
review, use, disclosure or distribution is prohibited.  If you are not
the intended recipient, please contact the sender by reply e-mail and
destroy all copies of the original message.
>
>

Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From spencer.graves at pdf.com  Mon Jul 10 04:03:00 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 09 Jul 2006 19:03:00 -0700
Subject: [R] R vs. other software?  (was:  Software for Epidmiological,
 Longitudinal Data)
In-Reply-To: <44ACF537.9000805@unibas.ch>
References: <44ACF537.9000805@unibas.ch>
Message-ID: <44B1B554.90105@pdf.com>

	  There have been numerous discussions on this listserve of the 
comparative advantages of R vs. SAS, for example, and various benchmark 
studies have been published.  If you haven't already tried Google, I 
encourage you to do so.


PRIMARY STRENGTHS OF R

	  From my perspective, R is the platform of choice for new statistical 
algorithm development for a rapidly increasing number of people.  The 
open source nature of R provides huge advantages for learning and for 
new algorithm development.  If I don't understand something, I can walk 
through the code line by line until I figure it out.  That's especially 
easy with the 'debug' facility in R.

	  Similarly, the ease with which I can experiment with a modification 
of an existing algorithm depends on the availability of source code for 
something similar.  With commercial software, that obstacle is often 
insurmountable.  With open source, this kind of experimentation is 
trivial for anyone who is not intimidated by command-driven software.


MARKET SHARE

	  The popularity of R can be judged in part from the fact that it is 
available from 58 mirrors in in 24 countries;  those were the numbers I 
got when I counted a couple of months ago.  At that time, the mirror I 
checked offered free instant access to 724 contributed packages beyond 
the base distribution, and this does not include much of the 
'Bioconductor', which specializes in microarray data.  There are 
occasional difference between mirrors, but those are temporary.

	  At one time, the market dominance of both SAS and IBM was sustained 
by the perception that 'nobody ever got fired' for using them.  Those 
days are history now for SAS as well as IBM, even with the Food and Drug 
Administration (FDA), where for many years, SAS was the de facto 
standard in many pharmaceutical companies for their applications for 
regulatory approval.

	  I have no data on the use of R in articles submitted to refereed 
journals, but I'm confident it is growing.

	  Increasingly, when I consider reading an article or buying a book, I 
look for the availability of a companion R package, because it can make 
a huge difference in how easily and thoroughly I can absorb the material.


SOFTWARE QUALITY
	
	  The core R distribution seems to be as solid as anything on the 
market today.  Contributed packages run from rock solid to highly buggy.


GRAPHICAL USER INTERFACE

	  R does not ship with a GUI, but several are available;  see 
"www.sciviews.org/_rgui".  I use XEmacs (ESS), mentioned on that web 
site, but it's primary a command line editor.  More GUI-type features 
are available from other open source software such as SciViews and JGR.
	  	

TECHNICAL SUPPORT

	  The level of support available from this listserve beats the level of 
technical support I've ever gotten for any commercial software.  Some 
people's questions never get answered, but that's true with technical 
support anywhere.  Other questions generate a feeding frenzy of replies. 
  This listserve has a posting guide 
(www.R-project.org/posting-guide.html);  I believe that posts that more 
closely match those guidelines are more likely to be greeted with 
multiple replies than silence.  R-help is on-line, 24-7.  If you really 
want an answer and don't get one in 24 hours, review the posting guide, 
and think about how you can make your question more clear, perhaps with 
a different subject line and / or a simple (or simpler) self-contained 
example.

	  The "R Site Search" is also great, and now there's an R Wiki.

	  For two other previous comments related to this, see the following:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/51244.html

https://stat.ethz.ch/pipermail/r-sig-finance/2004q4/000250.html

	  Hope this helps.
	  Spencer Graves

Andrea Meyer wrote:
> Hello
> 
> We are a team working on a prospective psychological study. The study 
> design is based on assessing data of three generations of humans over a 
> long time period, wherein epidemiological as well as biological data 
> will be assessed. Sample sizes will range from about 100 to several 
> thousand depending on the research question.
>  
> Currently we are looking for an apropriate statistical package. Here are 
> some features that the software should have:
> - strong in the analysis of epidemiological and longitudinal data
> - platform independent (should run under different operating systems 
> like Windows, Mac OS, Unix)
> - Ease of use for non-statistic-professionals (i.e. userfriendly GUI)
> - High acceptance by scientific journals, by the FDA
> - Importance relative to other packages with respect to the number of 
> users, the number of publications in which the software is used, the 
> market share etc.  (including the recent development of these indices!) 
>  
> As we had some problems in finding information concerning these items we 
> would like to ask you where we might find it (if at all) and why R is 
> presumably the best competitor and why?
>  
> Thanks in advance for any suggestions concerning this!
>


From dwm at wakatara.com  Mon Jul 10 05:01:10 2006
From: dwm at wakatara.com (Daryl Manning)
Date: Sun, 9 Jul 2006 20:01:10 -0700
Subject: [R] How to get R to ignore certain values when analyzing a column
	in a data table ?
Message-ID: <1EEE10B1-46D8-48EC-920C-A5655887ED3C@wakatara.com>

Apologies if this is in (one of the many) manuals somewhere... Trying  
to switch to R from other stats programs.

Basically, I have a large data table I've dumped from a DB, some of  
the values which are nulls '-' which I've converted to zeros. I've  
read it in using read.table fine.

I want R to ignore the zero values when graphing or doing various  
other calculations.

Is there a way to do this ?

I did try to use NA but kept getting errors that x must be numeric.

thanks in advance,
Daryl.


From sell_mirage_ne at hotmail.com  Mon Jul 10 05:53:19 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Sun, 09 Jul 2006 22:53:19 -0500
Subject: [R] problem in my code
Message-ID: <BAY110-F372FC2BEC639C1D10506F2C76B0@phx.gbl>

Dear R-users

I wrote a small program for assigning a membership

Here is my script

sample.size <- 60

x <- rnorm(sample.size, 0, 1)
y <- rnorm(sample.size, 0, 1)

x.mean <- mean(x)
y.mean <- mean(y)
membership <- numeric(sample.size)

for (i in 1:sample.size)
    {
        if ((x[i] < x.mean) && (y[i] < y.mean))
            {
                membership[i] <<- 1
            } else {
                        if ((x[i] > x.mean) && (y[i] < y.mean))
                            {
                                membership[i] <<- 2
                            } else {
                                        if ((x[i] > x.mean) && (y[i] > 
y.mean))
                                            {
                                                membership[i] <<- 3
                                            } else
                                                {
                                                    membership[i] <<- 4
                                                }
                                   }

                    }

    }
cbind(x,y,membership)

There is an error message
"Error: object "membership" not found"
I can't figure it out.

Any help or advice on improvement for this code will be appreciated.
I konw this code is not well written at all.

Thank you

Taka


From wataru at crayon.se.uec.ac.jp  Mon Jul 10 06:22:35 2006
From: wataru at crayon.se.uec.ac.jp (W. Yamamoto)
Date: Mon, 10 Jul 2006 13:22:35 +0900 (JST)
Subject: [R] problem in my code
In-Reply-To: <BAY110-F372FC2BEC639C1D10506F2C76B0@phx.gbl>
References: <BAY110-F372FC2BEC639C1D10506F2C76B0@phx.gbl>
Message-ID: <57476.130.153.146.67.1152505355.squirrel@lyra.lente.org>


Hi,

How about replacing all "<<-" with "<-"?  That error occured in
>                 membership[i] <<- 1
this line and this code stopped before
> cbind(x,y,membership)
this line.

Hope this may help you.
---
W. Yamamoto

> Dear R-users
>
> I wrote a small program for assigning a membership
>
> Here is my script
>
> sample.size <- 60
>
> x <- rnorm(sample.size, 0, 1)
> y <- rnorm(sample.size, 0, 1)
>
> x.mean <- mean(x)
> y.mean <- mean(y)
> membership <- numeric(sample.size)
>
> for (i in 1:sample.size)
>     {
>         if ((x[i] < x.mean) && (y[i] < y.mean))
>             {
>                 membership[i] <<- 1
>             } else {
>                         if ((x[i] > x.mean) && (y[i] < y.mean))
>                             {
>                                 membership[i] <<- 2
>                             } else {
>                                         if ((x[i] > x.mean) && (y[i] >
> y.mean))
>                                             {
>                                                 membership[i] <<- 3
>                                             } else
>                                                 {
>                                                     membership[i] <<- 4
>                                                 }
>                                    }
>
>                     }
>
>     }
> cbind(x,y,membership)
>
> There is an error message
> "Error: object "membership" not found"
> I can't figure it out.
>
> Any help or advice on improvement for this code will be appreciated.
> I konw this code is not well written at all.
>
> Thank you
>
> Taka
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>


From g0354502 at nccu.edu.tw  Mon Jul 10 06:26:04 2006
From: g0354502 at nccu.edu.tw (g0354502)
Date: Mon, 10 Jul 2006 12:26:04 +0800 (CST)
Subject: [R] about overdispersed poisson model
Message-ID: <1152505564.9731.g0354502@nccu.edu.tw>

Dear R users

  I have been looking for functions that can deal with overdispersed poisson
models. According to actuarial literature (England & Verall, Stochastic Claims 
Reserving in General Insurance , Institute of Actiuaries 2002) this can be handled through the
use of quasi likelihoods instead of normal likelihoods. However, we see them frequently
in this type of data, and we would like to be able to fit the model anyway.
If it is possible, would you please show me how to find the corresponding package and utilize them?

Best Regards,

            Chi Kai


From ronggui.huang at gmail.com  Mon Jul 10 06:42:26 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Mon, 10 Jul 2006 12:42:26 +0800
Subject: [R] about overdispersed poisson model
In-Reply-To: <1152505564.9731.g0354502@nccu.edu.tw>
References: <1152505564.9731.g0354502@nccu.edu.tw>
Message-ID: <38b9f0350607092142x6ebe7532qdb24dd6237fb2f4d@mail.gmail.com>

You can use glm to analysis this data ,with family=quasipoisson.
see ?glm and ?family



2006/7/10, g0354502 <g0354502 at nccu.edu.tw>:
> Dear R users
>
>   I have been looking for functions that can deal with overdispersed poisson
> models. According to actuarial literature (England & Verall, Stochastic Claims
> Reserving in General Insurance , Institute of Actiuaries 2002) this can be handled through the
> use of quasi likelihoods instead of normal likelihoods. However, we see them frequently
> in this type of data, and we would like to be able to fit the model anyway.
> If it is possible, would you please show me how to find the corresponding package and utilize them?
>
> Best Regards,
>
>             Chi Kai
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>


-- 
??????
Department of Sociology
Fudan University


From ggrothendieck at gmail.com  Mon Jul 10 06:55:08 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 10 Jul 2006 00:55:08 -0400
Subject: [R] problem in my code
In-Reply-To: <BAY110-F372FC2BEC639C1D10506F2C76B0@phx.gbl>
References: <BAY110-F372FC2BEC639C1D10506F2C76B0@phx.gbl>
Message-ID: <971536df0607092155y3e9174afy701600e6777a1bbb@mail.gmail.com>

The problem can be reduced to this:

x <- 1
x[1] <<- 2 # error

The following are ok:

x <- 1
x[1] <- 3

x <- 1
x <- 4

x <- 1
x <<- 5

Does anyone know why?  Is this a bug in <<- ?


On 7/9/06, Taka Matzmoto <sell_mirage_ne at hotmail.com> wrote:
> Dear R-users
>
> I wrote a small program for assigning a membership
>
> Here is my script
>
> sample.size <- 60
>
> x <- rnorm(sample.size, 0, 1)
> y <- rnorm(sample.size, 0, 1)
>
> x.mean <- mean(x)
> y.mean <- mean(y)
> membership <- numeric(sample.size)
>
> for (i in 1:sample.size)
>    {
>        if ((x[i] < x.mean) && (y[i] < y.mean))
>            {
>                membership[i] <<- 1
>            } else {
>                        if ((x[i] > x.mean) && (y[i] < y.mean))
>                            {
>                                membership[i] <<- 2
>                            } else {
>                                        if ((x[i] > x.mean) && (y[i] >
> y.mean))
>                                            {
>                                                membership[i] <<- 3
>                                            } else
>                                                {
>                                                    membership[i] <<- 4
>                                                }
>                                   }
>
>                    }
>
>    }
> cbind(x,y,membership)
>
> There is an error message
> "Error: object "membership" not found"
> I can't figure it out.
>
> Any help or advice on improvement for this code will be appreciated.
> I konw this code is not well written at all.
>
> Thank you
>
> Taka
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From wwguocn at gmail.com  Mon Jul 10 07:27:53 2006
From: wwguocn at gmail.com (Guo Wei-Wei)
Date: Mon, 10 Jul 2006 13:27:53 +0800
Subject: [R] A possible too old question on significant test of correlation
	matrix
Message-ID: <d3677d7d0607092227t5392a649l4d6e4e58a5e8ab99@mail.gmail.com>

Dear all,

I'm working on a data.frame named en.data, which has n cases and m columns.
I generate the correlation matrix of en.data by

> cor(en.data)

I find that there is no p-value on each correlation in the correlation
matrix. I searched in the R-help mail list and found some related
posts, but I didn't find direct way to solve the problem. Someone said
to use cor.test() or t.test(). The problem is that cor.test() and
t.test() can only apply on two vectors, not on a data.frame or a
matrix.

My solution is

for (i in 1:(ncol(en.data) -1)) {
   cor.test(en.data[,i], en.data[, i+1])
}

I think it is a stupid way. Is there a direct way to do so? After all,
it is a basic function to generate significant level of a correlation
in a correlation matrix.

Thank you in advance!
Wei-Wei


From celso.barros at gmail.com  Mon Jul 10 08:29:57 2006
From: celso.barros at gmail.com (Celso Barros)
Date: Mon, 10 Jul 2006 03:29:57 -0300
Subject: [R] weights in glmrob
Message-ID: <f6b7dfdc0607092329w6f93eaccq47baae32b6f8f4ae@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060710/f48508ef/attachment.pl 

From ripley at stats.ox.ac.uk  Mon Jul 10 08:35:15 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 10 Jul 2006 07:35:15 +0100 (BST)
Subject: [R] How to get R to ignore certain values when analyzing a
 column in a data table ?
In-Reply-To: <1EEE10B1-46D8-48EC-920C-A5655887ED3C@wakatara.com>
References: <1EEE10B1-46D8-48EC-920C-A5655887ED3C@wakatara.com>
Message-ID: <Pine.LNX.4.64.0607100733580.29747@gannet.stats.ox.ac.uk>

On Sun, 9 Jul 2006, Daryl Manning wrote:

> Apologies if this is in (one of the many) manuals somewhere... Trying
> to switch to R from other stats programs.
>
> Basically, I have a large data table I've dumped from a DB, some of
> the values which are nulls '-' which I've converted to zeros. I've
> read it in using read.table fine.
>
> I want R to ignore the zero values when graphing or doing various
> other calculations.
>
> Is there a way to do this ?
>
> I did try to use NA but kept getting errors that x must be numeric.

Did you use numeric NA?  That is the way to so this, so you will need to 
give a reproducible example (as the posting guide asks).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From buser at stat.math.ethz.ch  Mon Jul 10 09:35:24 2006
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 10 Jul 2006 09:35:24 +0200
Subject: [R] KS Test Warning Message
In-Reply-To: <af81db5a0607090724g73cc988cn78836d3e3059cba5@mail.gmail.com>
References: <af81db5a0607090724g73cc988cn78836d3e3059cba5@mail.gmail.com>
Message-ID: <17586.828.608134.245863@stat.math.ethz.ch>

Dear Justin

Ties means that you have identical values in
"Year5.lm$residuals". Please remark that you can have a large
R^2, but your residuals are not normally distributed. A large
R^2 shows a strong linear relationship, but that does not say
anything about the error distribution (see example below).

So to answer your question. Yes it can take away validity of
your model if the residuals are not normally distributed,
especially tests and confidence intervals for your parameters
are based on the normal assumption.
I'd recommend to verify model assumptions by graphical tools,
such as qqplot, Tukey-Anscombe Plot, ... 
Try:

plot(Year5.lm)

The power of KS-Test is quite small and graphical tools will
give you a hint about your true error distribution instead of
giving you only a p-value that "tells you" that the errors are
not normal.

set.seed(3)
x <- 1:100
## t-distributed errors
y <- x + rt(100,2)
## Strong linear relationship
plot(x,y)

## High R^2 due to strong linear relationship
summary(reg <- lm(y~x))
## The residuals are not normal distributed
qqnorm(resid(reg))
## Small power of KS-Test. Violation of model assumption is not detected
ks.test(resid(reg), "pnorm")

Best regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH Zurich	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


justin rapp writes:
 > All,
 > 
 > Happy World Cup and Wimbledon.   This morning finds me with the first
 > of my many daily questions.
 > 
 > I am running a ks.test on residuals obtained from a regression model.
 > 
 > I use this code:
 > > ks.test(Year5.lm$residuals,pnorm)
 > 
 > and obtain this output
 > 	One-sample Kolmogorov-Smirnov test
 > 
 > data:  Year5.lm$residuals
 > D = 0.7196, p-value < 2.2e-16
 > alternative hypothesis: two.sided
 > 
 > Warning message:
 > cannot compute correct p-values with ties in: ks.test(Year5.lm$residuals, pnorm)
 > 
 > I am wondering if anybody can tell me what this error message means.
 > 
 > Also, could anybody clarify how I could have a regression model with a
 > high Rsquared, rouglhy .67, but with nonnormal residuals?  Does this
 > take away from the validity of my model?
 > 
 > jdr
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From gavin.simpson at ucl.ac.uk  Mon Jul 10 09:39:05 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 10 Jul 2006 08:39:05 +0100
Subject: [R] A possible too old question on significant test
	of	correlation matrix
In-Reply-To: <d3677d7d0607092227t5392a649l4d6e4e58a5e8ab99@mail.gmail.com>
References: <d3677d7d0607092227t5392a649l4d6e4e58a5e8ab99@mail.gmail.com>
Message-ID: <1152517146.5665.9.camel@dhcppc2.my.nat.localnet>

On Mon, 2006-07-10 at 13:27 +0800, Guo Wei-Wei wrote:
> Dear all,
> 
> I'm working on a data.frame named en.data, which has n cases and m columns.
> I generate the correlation matrix of en.data by
> 
> > cor(en.data)
> 
> I find that there is no p-value on each correlation in the correlation
> matrix. I searched in the R-help mail list and found some related
> posts, but I didn't find direct way to solve the problem. Someone said
> to use cor.test() or t.test(). The problem is that cor.test() and
> t.test() can only apply on two vectors, not on a data.frame or a
> matrix.
> 
> My solution is
> 
> for (i in 1:(ncol(en.data) -1)) {
>    cor.test(en.data[,i], en.data[, i+1])
> }
> 
> I think it is a stupid way. Is there a direct way to do so? After all,
> it is a basic function to generate significant level of a correlation
> in a correlation matrix.
> 
> Thank you in advance!
> Wei-Wei

Hi,

Bill Venables posted a solution to this on the R-Help list in Jan 2000.
I made a minor modification to add a class to the result and wrote a
print method (which could probably do with some tidying but it works).

E.g.:

# paste in the functions below, then
data(iris)
corProb(iris[,1:4])

## prints
Correlations are shown below the diagonal
P-values are shown above the diagonal

             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length  1.0000       0.1519      0.0000       0.0000
Sepal.Width  -0.1176       1.0000      0.0000       0.0000
Petal.Length  0.8718      -0.4284      1.0000       0.0000
Petal.Width   0.8179      -0.3661      0.9629       1.0000

Is this what you want?

HTH

G

# correlation function
# based on post by Bill Venables on R-Help
# Date: Tue, 04 Jan 2000 15:05:39 +1000
# https://stat.ethz.ch/pipermail/r-help/2000-January/009758.html
# modified by G L Simpson, September 2003
# version 0.2: added print.cor.prob
#              added class statement to cor.prob
# version 0.1: original function of Bill Venables
corProb <- function(X, dfr = nrow(X) - 2) {
    R <- cor(X)
    above <- row(R) < col(R)
    r2 <- R[above]^2
    Fstat <- r2 * dfr / (1 - r2)
    R[above] <- 1 - pf(Fstat, 1, dfr)
    class(R) <- "corProb"
    R
}
print.corProb <- function(x, digits = getOption("digits"), quote = FALSE, na.print = "", 
    justify = "none", ...) {
    xx <- format(unclass(round(x, digits = 4)), digits = digits, justify = justify)
    if (any(ina <- is.na(x))) 
        xx[ina] <- na.print
    cat("\nCorrelations are shown below the diagonal\n")
    cat("P-values are shown above the diagonal\n\n")
    print(xx, quote = quote, ...)
    invisible(x)
}

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 *Note new Address and Fax and Telephone numbers from 10th April 2006*
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/cv/
WC1E 6BT                          [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From dimitris.rizopoulos at med.kuleuven.be  Mon Jul 10 09:57:43 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 10 Jul 2006 09:57:43 +0200
Subject: [R] A possible too old question on significant test of
	correlationmatrix
References: <d3677d7d0607092227t5392a649l4d6e4e58a5e8ab99@mail.gmail.com>
Message-ID: <009801c6a3f6$866f6f10$0540210a@www.domain>

you can use function rcor.test() from package 'ltm', e.g.,

help(rcor.test, package = "ltm")
###
library(ltm)

dat <- data.frame(matrix(rnorm(1000), 100, 10))

rcor.test(dat)
rcor.test(dat, method = "kendall")
rcor.test(dat, method = "spearman")


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Guo Wei-Wei" <wwguocn at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, July 10, 2006 7:27 AM
Subject: [R] A possible too old question on significant test of 
correlationmatrix


> Dear all,
>
> I'm working on a data.frame named en.data, which has n cases and m 
> columns.
> I generate the correlation matrix of en.data by
>
>> cor(en.data)
>
> I find that there is no p-value on each correlation in the 
> correlation
> matrix. I searched in the R-help mail list and found some related
> posts, but I didn't find direct way to solve the problem. Someone 
> said
> to use cor.test() or t.test(). The problem is that cor.test() and
> t.test() can only apply on two vectors, not on a data.frame or a
> matrix.
>
> My solution is
>
> for (i in 1:(ncol(en.data) -1)) {
>   cor.test(en.data[,i], en.data[, i+1])
> }
>
> I think it is a stupid way. Is there a direct way to do so? After 
> all,
> it is a basic function to generate significant level of a 
> correlation
> in a correlation matrix.
>
> Thank you in advance!
> Wei-Wei
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From Markus.Gesmann at lloyds.com  Mon Jul 10 10:14:14 2006
From: Markus.Gesmann at lloyds.com (Gesmann, Markus)
Date: Mon, 10 Jul 2006 09:14:14 +0100
Subject: [R] about overdispersed poisson model
Message-ID: <C3E3A3D81F4E0F438118DAA9722F12A90119635C@LNVCNTEXCH01.corp.lloydsnet>

Dear Chi Kai,

Three years ago there was a similar thread. 
At that time David Firth offered a solution for the quasipoission
problem with negative observations, see:
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/16143.html
I remember that his code gave you slightly different answers than the
example in England and Verrall's paper.

Kind Regards

Markus Gesmann
FPMA
Lloyd's Market Analysis
Lloyd's * One Lime Street * London * EC3M 7HA
Telephone +44 (0)20 7327 6472
Facsimile +44 (0)20 7327 5718
http://www.lloyds.com


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of g0354502
Sent: 10 July 2006 05:26
To: r-help at stat.math.ethz.ch
Subject: [R] about overdispersed poisson model


Dear R users

  I have been looking for functions that can deal with overdispersed
poisson
models. According to actuarial literature (England & Verall, Stochastic
Claims 
Reserving in General Insurance , Institute of Actiuaries 2002) this can
be handled through the
use of quasi likelihoods instead of normal likelihoods. However, we see
them frequently
in this type of data, and we would like to be able to fit the model
anyway.
If it is possible, would you please show me how to find the
corresponding package and utilize them?

Best Regards,

            Chi Kai

**********************************************************************
The information in this E-Mail and in any attachments is CON...{{dropped}}


From wwguocn at gmail.com  Mon Jul 10 10:22:22 2006
From: wwguocn at gmail.com (Guo Wei-Wei)
Date: Mon, 10 Jul 2006 16:22:22 +0800
Subject: [R] A possible too old question on significant test of
	correlation matrix
In-Reply-To: <1152517146.5665.9.camel@dhcppc2.my.nat.localnet>
References: <d3677d7d0607092227t5392a649l4d6e4e58a5e8ab99@mail.gmail.com>
	<1152517146.5665.9.camel@dhcppc2.my.nat.localnet>
Message-ID: <d3677d7d0607100122w458f7f79t2a9d339653bf4f09@mail.gmail.com>

Hi, Gavin, your program is excellent. Thank  you very much!

And I have two further questions.

1. Since it is very possible that the data contains missing value and
the program will failed against missing values, I have to delete all
the cases contained NA. Can it be done pairwisely?
2. Can the program show t values instead of p values?

Best regards,
Wei-Wei

2006/7/10, Gavin Simpson <gavin.simpson at ucl.ac.uk>:
> On Mon, 2006-07-10 at 13:27 +0800, Guo Wei-Wei wrote:
> > Dear all,
> >
> > I'm working on a data.frame named en.data, which has n cases and m columns.
> > I generate the correlation matrix of en.data by
> >
> > > cor(en.data)
> >
> > I find that there is no p-value on each correlation in the correlation
> > matrix. I searched in the R-help mail list and found some related
> > posts, but I didn't find direct way to solve the problem. Someone said
> > to use cor.test() or t.test(). The problem is that cor.test() and
> > t.test() can only apply on two vectors, not on a data.frame or a
> > matrix.
> >
> > My solution is
> >
> > for (i in 1:(ncol(en.data) -1)) {
> >    cor.test(en.data[,i], en.data[, i+1])
> > }
> >
> > I think it is a stupid way. Is there a direct way to do so? After all,
> > it is a basic function to generate significant level of a correlation
> > in a correlation matrix.
> >
> > Thank you in advance!
> > Wei-Wei
>
> Hi,
>
> Bill Venables posted a solution to this on the R-Help list in Jan 2000.
> I made a minor modification to add a class to the result and wrote a
> print method (which could probably do with some tidying but it works).
>
> E.g.:
>
> # paste in the functions below, then
> data(iris)
> corProb(iris[,1:4])
>
> ## prints
> Correlations are shown below the diagonal
> P-values are shown above the diagonal
>
>              Sepal.Length Sepal.Width Petal.Length Petal.Width
> Sepal.Length  1.0000       0.1519      0.0000       0.0000
> Sepal.Width  -0.1176       1.0000      0.0000       0.0000
> Petal.Length  0.8718      -0.4284      1.0000       0.0000
> Petal.Width   0.8179      -0.3661      0.9629       1.0000
>
> Is this what you want?
>
> HTH
>
> G
>
> # correlation function
> # based on post by Bill Venables on R-Help
> # Date: Tue, 04 Jan 2000 15:05:39 +1000
> # https://stat.ethz.ch/pipermail/r-help/2000-January/009758.html
> # modified by G L Simpson, September 2003
> # version 0.2: added print.cor.prob
> #              added class statement to cor.prob
> # version 0.1: original function of Bill Venables
> corProb <- function(X, dfr = nrow(X) - 2) {
>     R <- cor(X)
>     above <- row(R) < col(R)
>     r2 <- R[above]^2
>     Fstat <- r2 * dfr / (1 - r2)
>     R[above] <- 1 - pf(Fstat, 1, dfr)
>     class(R) <- "corProb"
>     R
> }
> print.corProb <- function(x, digits = getOption("digits"), quote = FALSE, na.print = "",
>     justify = "none", ...) {
>     xx <- format(unclass(round(x, digits = 4)), digits = digits, justify = justify)
>     if (any(ina <- is.na(x)))
>         xx[ina] <- na.print
>     cat("\nCorrelations are shown below the diagonal\n")
>     cat("P-values are shown above the diagonal\n\n")
>     print(xx, quote = quote, ...)
>     invisible(x)
> }
>
> --
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>  *Note new Address and Fax and Telephone numbers from 10th April 2006*
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> Gavin Simpson                     [t] +44 (0)20 7679 0522
> ECRC                              [f] +44 (0)20 7679 0565
> UCL Department of Geography
> Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
> Gower Street
> London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/cv/
> WC1E 6BT                          [w] http://www.ucl.ac.uk/~ucfagls/
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>
>


From Ted.Harding at nessie.mcc.ac.uk  Mon Jul 10 10:22:47 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 10 Jul 2006 09:22:47 +0100 (BST)
Subject: [R] R vs. other software?  (was:  Software for Epidmiologica
In-Reply-To: <44B1B554.90105@pdf.com>
Message-ID: <XFMail.060710092247.Ted.Harding@nessie.mcc.ac.uk>

On 10-Jul-06 Spencer Graves wrote:
> [...]

Excellent overview of many string reasons for R, Spencer! Many
thanks for this well-thought-out advocacy piece.

> [...]
> At one time, the market dominance of both SAS and IBM was
> sustained by the perception that 'nobody ever got fired'
> for using them. Those days are history now for SAS as well
> as IBM, [...]

Does that imply that, by now, some people *have* been fired
for using SAS  (implicitly, perhaps, because they should have
been using R)?

:)

Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 10-Jul-06                                       Time: 09:22:43
------------------------------ XFMail ------------------------------


From m.austwick at ucl.ac.uk  Mon Jul 10 10:43:51 2006
From: m.austwick at ucl.ac.uk (Dr Martin Austwick)
Date: Mon, 10 Jul 2006 09:43:51 +0100
Subject: [R] Sjava on Windows?
Message-ID: <44B21347.5050208@ucl.ac.uk>

Hello everyone

I am very much a newcomer to R and I need to call R processing routines 
from java and pass back the results to java. It seems sjava is the way 
to do this, but there seems to be a lot of complexity surrounding 
configuring R for this purpose. I need to do this under windows XP. Has 
anyone else had any experience with this and would they  be able to help 
a beginner to get this to work?

A colleague passed me this email address and suggested I contact it, 
apologies if this is not the usual etiquette.

Kind Regards

Martin Austwick.


From wwguocn at gmail.com  Mon Jul 10 10:49:03 2006
From: wwguocn at gmail.com (Guo Wei-Wei)
Date: Mon, 10 Jul 2006 16:49:03 +0800
Subject: [R] A possible too old question on significant test of
	correlationmatrix
In-Reply-To: <009801c6a3f6$866f6f10$0540210a@www.domain>
References: <d3677d7d0607092227t5392a649l4d6e4e58a5e8ab99@mail.gmail.com>
	<009801c6a3f6$866f6f10$0540210a@www.domain>
Message-ID: <d3677d7d0607100149o6a2c9a80i3f281477a95e59f7@mail.gmail.com>

Thank you, Dimitris.

The function rcor.test() is very nice. It can pass arguments to cor()
and solve my first problem of pairwised case deletion.

Best regards,
Wei-Wei


2006/7/10, Dimitris Rizopoulos <dimitris.rizopoulos at med.kuleuven.be>:
> you can use function rcor.test() from package 'ltm', e.g.,
>
> help(rcor.test, package = "ltm")
> ###
> library(ltm)
>
> dat <- data.frame(matrix(rnorm(1000), 100, 10))
>
> rcor.test(dat)
> rcor.test(dat, method = "kendall")
> rcor.test(dat, method = "spearman")
>
>
> I hope it helps.
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
>
>
>
> ----- Original Message -----
> From: "Guo Wei-Wei" <wwguocn at gmail.com>
> To: <r-help at stat.math.ethz.ch>
> Sent: Monday, July 10, 2006 7:27 AM
> Subject: [R] A possible too old question on significant test of
> correlationmatrix
>
>
> > Dear all,
> >
> > I'm working on a data.frame named en.data, which has n cases and m
> > columns.
> > I generate the correlation matrix of en.data by
> >
> >> cor(en.data)
> >
> > I find that there is no p-value on each correlation in the
> > correlation
> > matrix. I searched in the R-help mail list and found some related
> > posts, but I didn't find direct way to solve the problem. Someone
> > said
> > to use cor.test() or t.test(). The problem is that cor.test() and
> > t.test() can only apply on two vectors, not on a data.frame or a
> > matrix.
> >
> > My solution is
> >
> > for (i in 1:(ncol(en.data) -1)) {
> >   cor.test(en.data[,i], en.data[, i+1])
> > }
> >
> > I think it is a stupid way. Is there a direct way to do so? After
> > all,
> > it is a basic function to generate significant level of a
> > correlation
> > in a correlation matrix.
> >
> > Thank you in advance!
> > Wei-Wei
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
>


From ligges at statistik.uni-dortmund.de  Mon Jul 10 10:53:09 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 10 Jul 2006 10:53:09 +0200
Subject: [R] Choice of repository and outdated vs unusable... was
 (Hunting for snow...)
In-Reply-To: <44B14C65.8080707@ncf.ca>
References: <44B05385.40001@ncf.ca>
	<Pine.LNX.4.64.0607090821100.27488@gannet.stats.ox.ac.uk>
	<44B0EAF3.5050304@ncf.ca>
	<44B0F588.9070701@statistik.uni-dortmund.de>
	<44B105E1.5050607@ncf.ca>
	<44B109FD.8000206@statistik.uni-dortmund.de>
	<44B14C65.8080707@ncf.ca>
Message-ID: <44B21575.2040805@statistik.uni-dortmund.de>

Brian Lunergan wrote:
> Uwe Ligges wrote:
>>>> Where did you find them? At least not in their current versions on 
>>>> the official repository on CRAN master for R-2.3.x, I hope.
>>>
>>> FracSim_0.2.zip
>>> http://cran.stat.ucla.edu/bin/windows/contrib/2.3/
>>>
>>> RDCOMClient_0.91-0.zip
>>> http://www.omegahat.org/R/bin/windows/contrib/R-2.3.0/
>>>
>>> segmented_0.1-4.zip
>>> http://cran.r-project.org/bin/windows/contrib/2.1/
>>>
>>> VGAM_0.6-9.zip
>>> http://www.stat.auckland.ac.nz/~yee/bin/windows/contrib/2.3/
>>
>> None of them is on CRAN master:
>>
>> - RDCOMClient is on Omegahat
>> - segmented is CRAN but outdated for R-2.1.x
>> - VGAM is in a private repository
>> - FracSim: I cannot access
>> http://cran.stat.ucla.edu/bin/windows/contrib/2.3/
>> right now, but I guess it does not sync properly. At least, the 
>> package should not be there.
>>
>> Uwe Ligges
> 
> Uwe:
> 
> So VGAM and RDCOMClient are on private repository sites. Is that so 
> important? If a package is not on the CRAN master site are you supposed 
> to avoid using it?

Brian,

no, but in the mail before you tols us that the packages are not on the 
check list, and I gave the reason why ...


> The FracSim home site indicated that 0.2 was the current version and a 
> google search yielded the site I indicated, among others, as a 
> repository of an apparent windows edition. That it's not on the master 
> is more an issue between the maintainers of the master and the 
> maintainers of the package, not the average end user IMHO. I found a 

The reason is that it does not pass the checks, and that is only an 
issue of the maintainer of the package, not the CRAN maintainer. That 
you found the package is the issue of the maintainer of a CRAN mirror 
that is out of sync. It is an issue of the average end user to install a 
package from source if he does not get a binary.



> source for what is apparently the current edition. What's wrong with that?

Using the source is perfect.

I must have misunderstood that you have compiled from source. But then, 
be careful and run R CMD check in order to see whether the package works 
under your platform.

Best,
Uwe Ligges



> With regard to segmented, outdated does not mean unusable unless later 
> editions of R or packages that call it as a dependency use features 
> particular to later editions. If there is a newer windows edition of 
> segmented then please point out a source for it.
> 
> Regards...


From h.wickham at gmail.com  Mon Jul 10 11:02:16 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 10 Jul 2006 11:02:16 +0200
Subject: [R] Weighted histograms
Message-ID: <f8e6ff050607100202y3da0dd2cpadba1d66dd9ba13@mail.gmail.com>

Does anyone have any code for drawing weighted histograms (a la
Manet/Mondrian) in R?

Thanks,

Hadley


From r.hankin at noc.soton.ac.uk  Mon Jul 10 11:09:52 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 10 Jul 2006 10:09:52 +0100
Subject: [R] par(mfrow,mai) and multiple plot problem
Message-ID: <0672AC58-A667-44B7-AAD5-4BFBFBE2B873@soc.soton.ac.uk>

Hi

I'm having difficulty with a multiple plot.  What I want is 12
plots, all of which are the same size and shape, differing
only in colour.  Each one is a square, so there is an asp=1
in the plot call.  I'm working in an Sweave environment so I
am free to choose the height and width of the plot.

I want the columns to be labelled at  the head (here t=1,2,3)
and the rows to be labelled at the left hand side (here an animal),

I don't want axes.

I want the plots to occupy the majority of the area of the output
postscript.

I want the vertical distance between the plots to be the
same as the horizontal distance between the plots (not critical
but would be nice).

Can anyone help me with these requirements?

My best attempt is below, but it is no good because the
ylab argument that I'm trying to use for the row labelling
gets clipped.  Also, I can't get my horizontal spacing
to be equal to my vertical spacing (although the code
below isn't _too_ bad).





f <- function(...){
   jj <- expand.grid(1:10,1:10)
   colnames(jj) <- c("","")
   par(mai=rep(0,4)+0.2)
   plot(jj,pch=16,asp=1, axes=FALSE, ...)
}


postscript(file="~/f.ps",width=5,height=7)
par(mfrow=c(4,3))
f(main="t=1",ylab="fish")
f(main="t=2")
f(main="t=3")

f(ylab="dog")
f()
f()

f(ylab="slug")
f()
f()

f(ylab="pig")
f()
f()

dev.off()


--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From ripley at stats.ox.ac.uk  Mon Jul 10 12:20:19 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 10 Jul 2006 11:20:19 +0100 (BST)
Subject: [R] Sjava on Windows?
In-Reply-To: <44B21347.5050208@ucl.ac.uk>
References: <44B21347.5050208@ucl.ac.uk>
Message-ID: <Pine.LNX.4.64.0607101111340.18417@gannet.stats.ox.ac.uk>

(R)SJava is part of the Omegahat project, not the R project. In theory at 
least, Omegahat has its own mailing lists.

In any case, the R posting guide asks you to contact the maintainer of a 
contributed package before this list, so please do so now.

There are easier-to-setup alternatives to using R from Java: see
http://www.rosuda.org/software/ for example (some of which software is on 
CRAN).  (From your description Rserve may suffice: an alternative would be 
to use DCOM.)

On Mon, 10 Jul 2006, Dr Martin Austwick wrote:

> Hello everyone
>
> I am very much a newcomer to R and I need to call R processing routines
> from java and pass back the results to java. It seems sjava is the way
> to do this, but there seems to be a lot of complexity surrounding
> configuring R for this purpose. I need to do this under windows XP. Has
> anyone else had any experience with this and would they  be able to help
> a beginner to get this to work?
>
> A colleague passed me this email address and suggested I contact it,
> apologies if this is not the usual etiquette.
>
> Kind Regards
>
> Martin Austwick.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From r.hankin at noc.soton.ac.uk  Mon Jul 10 12:41:38 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 10 Jul 2006 11:41:38 +0100
Subject: [R] par(mfrow,mai) and multiple plot problem
In-Reply-To: <44B21D8D.9000007@cropdesign.com>
References: <0672AC58-A667-44B7-AAD5-4BFBFBE2B873@soc.soton.ac.uk>
	<44B21D8D.9000007@cropdesign.com>
Message-ID: <C395464F-D3E5-4C0B-AF8F-BE2B956036D3@soc.soton.ac.uk>

[apologies for possible multiple posting]

Hi Joris

great suggestion!  I never thought to use layout() in this way.


If I have

postscript(file="~/f.ps",width=5,height=8)

nf <- layout(matrix(
                     c(00,01,00,02,00,03,
                       04,05,00,06,00,07,
                       00,00,00,00,00,00,
                       08,09,00,10,00,11,
                       00,00,00,00,00,00,
                       12,13,00,14,00,15,
                       00,00,00,00,00,00,
                       16,17,00,18,00,19),8,6,byrow=TRUE), c(1,4,  
1,4, 1,4), c(1,4,1,4,1,4,1,4), TRUE)

layout.show(nf)
dev.off()

This is exactly what I want.

I hadn't appreciated that the empty rows and columns would be clear.

How best to put text in boxes 1,2,3 (that used to be main() and  
vertical text in
boxes 4,8,12,16 (that used to be ylab)?

best wishes

Robin



On 10 Jul 2006, at 10:27, Joris De Wolf wrote:

> Check
> ?layout
>
> It gives you more flexibility than
> par(mfrow=c(a,b))
>
> For instance, you can define your margins between the graphs as  
> cells in the layout without filling them by a plot.
>
> Joris
>
>
> Robin Hankin wrote:
>> Hi
>> I'm having difficulty with a multiple plot.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From R.Birnie at leeds.ac.uk  Mon Jul 10 12:56:13 2006
From: R.Birnie at leeds.ac.uk (Richard Birnie)
Date: Mon, 10 Jul 2006 11:56:13 +0100
Subject: [R] pvclust missing values problem
Message-ID: <C6AC304539F43C4196253FD94B7960F307AF8D@HERMES2.ds.leeds.ac.uk>

Hello all,

I posted a question to this list last week and received no response. I am unsure if this means no-one knows the answer or if I posed the question badly. I'm going to assume I posed the question badly and try again. I am new to R so it is quite likely it's a very naive question, however if there is something blindingly obvious that I am missing or if there is another resource I should consult that I haven't seen would someone be kind enough to point it out because it isn't obvious to me. Although my data is from biological experiments I think my problem is with R rather than the nature of the data, but I may be wrong.

I am attempting to use the pvclust package to do some hierarchical clustering on some CGH data I have downloaded from the Progenetix database (http://www.progenetix.de/~pgscripts/progenetix/Aboutprogenetix.html). The data is in tab delimited format, each column is a single sample each row is a chromosome band some example dummy data is shown below.

band     sample1  sample2  sample3 sample4
1p36_33     1       0        0       1
1p36_32     -1      0        -1       0
1p36_31     0       1        1       1
1p36_22    0        -1       -1      -1
etc.... where 0 = no change, 1 = gain, -1 = loss

I have read this file into R using:
> ProgenetixCRC.all.noXY <- read.table("/home/marraydb/Progenetix/Data/CRCall_noXY.txt", header=TRUE, sep="\t", row.names="band")

based on the pvclust documentation I came up with this:
>ProgenetixCRC.all.pvclust <- pvclust(ProgenetixCRC.all, method.dist="cor", method.hclust="average",use.cor="pairwise.complete.obs",nboot=1000)

this results in an error
Error in hclust(distance, method = method.hclust) :
        NA/NaN/Inf in foreign function call (arg 11)
Digging through the mailing list archives I've discovered this means that my dataset has missing values. This is very confusing because I have checked and there are no missing values. Running is.na() over the data matrix results in all false values which I take to mean none of the values are NA. I tried various options for the use.cor argument all with the same result. 

Since I originally posted this question I tried changing method.dist to euclidean, in this form the function executes without any errors. This is not to say the results actually mean anything of course. I am at a loss as to how to proceed any input from someone more experienced would be gratefully appreciated. If there is some reason why I should not be doing this analysis this way in the first place then I'd appreciate having that pointed out also. I've tried not to put excess information in here but if more is needed then let me know what and I'll post it.

I suspect the problem is me, however if it really is the case that no-one knows how to answer this then could anyone suggest another mailing list where I might get a better response. Would bioconductor be a better option for example?

Apologies for any offence caused by posting the same question but it's difficult for me to proceed until I get some kind of response, even if it is that this list is not the right place for this question.

Thanks for your patience,
Richard

Dr Richard Birnie
Scientific Officer
Section of Pathology and Tumour Biology
Welcome Brenner Building, LIMM
St James University Hospital
Beckett St, Leeds, LS9 7TF
Tel:0113 3438624
e-mail: r.birnie at leeds.ac.uk


From gavin.simpson at ucl.ac.uk  Mon Jul 10 13:48:13 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 10 Jul 2006 12:48:13 +0100
Subject: [R] A possible too old question on significant test
	of	correlation matrix
In-Reply-To: <d3677d7d0607100122w458f7f79t2a9d339653bf4f09@mail.gmail.com>
References: <d3677d7d0607092227t5392a649l4d6e4e58a5e8ab99@mail.gmail.com>
	<1152517146.5665.9.camel@dhcppc2.my.nat.localnet>
	<d3677d7d0607100122w458f7f79t2a9d339653bf4f09@mail.gmail.com>
Message-ID: <1152532093.21333.21.camel@gsimpson.geog.ucl.ac.uk>

On Mon, 2006-07-10 at 16:22 +0800, Guo Wei-Wei wrote:
> Hi, Gavin, your program is excellent. Thank  you very much!
> 
> And I have two further questions.
> 
> 1. Since it is very possible that the data contains missing value and
> the program will failed against missing values, I have to delete all
> the cases contained NA. Can it be done pairwisely?

Yes, with a modification to accept and pass on argument "use", e.g.:

data(iris)
## copy data
iris2 <- iris
## simulate some missing values in Sepal.Length
iris2[sample(1:nrow(iris2), 5), 1] <- NA

## corProb matrix with missing values
temp <- corProb(iris2[,1:4], use = "pairwise.complete.obs")

See ?cor for the options you can specify for "use". You'll need to paste
in the functions below for this to work.

> 2. Can the program show t values instead of p values?

Yes - this is R! The function Bill Venables wrote uses F-values, so I
looked at what cor.test was doing and modified the function to compute
either t or F values and to return them or their p-values.

temp <- corProb(iris[,1:4])
temp <- corProb(iris[,1:4], type = "t")
temp <- corProb(iris[,1:4], type = "t", pval = FALSE)

For t-values you can do the different test as in ?cor.test :

## with different tests
temp <- corProb(iris[,1:4], type = "t",
                alternative = "less")
temp <- corProb(iris[,1:4], type = "t",
                alternative = "greater")

Hopefully that does what you wanted.

G

## New functions.

corProb <- function(X, dfr = nrow(X) - 2,
                    use = c("all.obs", "complete.obs",
                      "pairwise.complete.obs"),
                    alternative = c("two.sided", "less",
                      "greater"),
                    type = c("F", "t"),
                    pval = TRUE) {
  USE <- match.arg(use)
  ALTERNATIVE <- match.arg(alternative)
  R <- cor(X, use = USE)
  above <- row(R) < col(R)
  r2 <- R[above]^2
  TYPE <- match.arg(type)
  if(TYPE == "t") {
    Tstat <- sqrt(dfr) * R[above]/sqrt(1 - r2)
    if(pval) {
      p <- pt(Tstat, dfr)
      R[above] <- switch(ALTERNATIVE, less = p,
                         greater = 1 - p, 
                         two.sided = 2 * min(p, 1 - p))
    }
    else
      R[above] <- Tstat
  }
  else {
    Fstat <- r2 * dfr / (1 - r2)
    if(pval)
      R[above] <- 1 - pf(Fstat, 1, dfr)
    else
      R[above] <- Fstat
  }
  class(R) <- "corProb"
  attr(R, "type") <- TYPE
  attr(R, "pval") <- pval
  attr(R, "hypoth") <- ALTERNATIVE
  R
}

print.corProb <- function(x, digits = getOption("digits"),
                          quote = FALSE, na.print = "",
                          justify = "none", ...) {
  xx <- format(unclass(round(x, digits = 4)), digits = digits,
               justify = justify)
  if (any(ina <- is.na(x)))
    xx[ina] <- na.print
  cat("\nCorrelations are shown below the diagonal\n")
  if(attr(x, "pval"))
     cat(paste("P-values of the ", attr(x, "type"),
               "-statistics are shown above the diagonal\n\n",
               sep = ""))
  else
     cat(paste(attr(x, "type"),
               "-values are shown above the diagonal\n\n",
               sep = ""))
  if(attr(x, "type") == "t") 
    hypoth <- switch(attr(x, "hypoth"),
                     less = "less than 0",
                     greater = "greater than 0",
                     two.sided = "not equal to 0")
  cat(paste("alternative hypothesis: true correlation is",
            hypoth, "\n\n"))
  print.default(xx, quote = quote, ...)
  invisible(x)
}

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
 London, UK. WC1E 6BT.         [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From alvaro.gutierrez at ufz.de  Mon Jul 10 13:52:39 2006
From: alvaro.gutierrez at ufz.de (Alvaro Gutierrez)
Date: Mon, 10 Jul 2006 13:52:39 +0200
Subject: [R] tree-ring response function in R?
Message-ID: <001d01c6a417$582aa270$6833418d@oesa.leipzig.ufz.de>

Hi,

I am looking forward if some of you could help me with this. I have been 
looking R-functions to conduct tree-ring analysis in R. I  know that exists 
a COFECHA version in R developed by C. Bigler. But I don't know if is there 
another collection of functions to do dendroclimatological analysis (such as 
response functions). Has someone developed such rutines in R ?,

thanks in advance,

Alvaro


From gavin.simpson at ucl.ac.uk  Mon Jul 10 14:03:00 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 10 Jul 2006 13:03:00 +0100
Subject: [R] pvclust missing values problem
In-Reply-To: <C6AC304539F43C4196253FD94B7960F307AF8D@HERMES2.ds.leeds.ac.uk>
References: <C6AC304539F43C4196253FD94B7960F307AF8D@HERMES2.ds.leeds.ac.uk>
Message-ID: <1152532980.21333.33.camel@gsimpson.geog.ucl.ac.uk>

On Mon, 2006-07-10 at 11:56 +0100, Richard Birnie wrote:
> Hello all,

Hi Richard,

Sorry, I know nothing about pvclust and have never used it, but here are
a couple of general suggestions/observations.

You are asked to contact the package maintainer *not* R-Help for
questions relating to contributed packages. I dare say only the
maintainer (or some kind soul with too much time on their hands to debug
the package) can solve your problem, if you can't work out how to do it
with R's debug tools. I have cc'd the maintainer on this reply.

When you get an error, it is useful to supply the output from
traceback() to see where the error actually occurred.

By the way, the error NA/NaN/Inf in foreign function call (arg 11)
doesn't necessarily mean you have missing values in your data set. Note
the NaN and Inf in that error message. It could just be that one of the
calculations resulted in NaN's or Inf's which hclust detected or caught
and issued the error. Without traceback(), this is pure speculation.

R version info, platform (OS) and version of pvclust are all useful
extra bits of info that you could have provided us or the maintainer as
well.

Hopefully you can sort your problem out with the maintainer.

Best,

G

> 
> I posted a question to this list last week and received no response. I
> am unsure if this means no-one knows the answer or if I posed the
> question badly. I'm going to assume I posed the question badly and try
> again. I am new to R so it is quite likely it's a very naive question,
> however if there is something blindingly obvious that I am missing or
> if there is another resource I should consult that I haven't seen
> would someone be kind enough to point it out because it isn't obvious
> to me. Although my data is from biological experiments I think my
> problem is with R rather than the nature of the data, but I may be
> wrong.
> 
> I am attempting to use the pvclust package to do some hierarchical
> clustering on some CGH data I have downloaded from the Progenetix
> database
> (http://www.progenetix.de/~pgscripts/progenetix/Aboutprogenetix.html).
> The data is in tab delimited format, each column is a single sample
> each row is a chromosome band some example dummy data is shown below.
> 
> band     sample1  sample2  sample3 sample4
> 1p36_33     1       0        0       1
> 1p36_32     -1      0        -1       0
> 1p36_31     0       1        1       1
> 1p36_22    0        -1       -1      -1
> etc.... where 0 = no change, 1 = gain, -1 = loss
> 
> I have read this file into R using:
> > ProgenetixCRC.all.noXY <-
> read.table("/home/marraydb/Progenetix/Data/CRCall_noXY.txt",
> header=TRUE, sep="\t", row.names="band")
> 
> based on the pvclust documentation I came up with this:
> >ProgenetixCRC.all.pvclust <- pvclust(ProgenetixCRC.all,
> method.dist="cor",
> method.hclust="average",use.cor="pairwise.complete.obs",nboot=1000)
> 
> this results in an error
> Error in hclust(distance, method = method.hclust) :
>         NA/NaN/Inf in foreign function call (arg 11)
> Digging through the mailing list archives I've discovered this means
> that my dataset has missing values. This is very confusing because I
> have checked and there are no missing values. Running is.na() over the
> data matrix results in all false values which I take to mean none of
> the values are NA. I tried various options for the use.cor argument
> all with the same result. 
> 
> Since I originally posted this question I tried changing method.dist
> to euclidean, in this form the function executes without any errors.
> This is not to say the results actually mean anything of course. I am
> at a loss as to how to proceed any input from someone more experienced
> would be gratefully appreciated. If there is some reason why I should
> not be doing this analysis this way in the first place then I'd
> appreciate having that pointed out also. I've tried not to put excess
> information in here but if more is needed then let me know what and
> I'll post it.
> 
> I suspect the problem is me, however if it really is the case that
> no-one knows how to answer this then could anyone suggest another
> mailing list where I might get a better response. Would bioconductor
> be a better option for example?
> 
> Apologies for any offence caused by posting the same question but it's
> difficult for me to proceed until I get some kind of response, even if
> it is that this list is not the right place for this question.
> 
> Thanks for your patience,
> Richard

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
 London, UK. WC1E 6BT.         [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From tuechler at gmx.at  Mon Jul 10 14:23:00 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Mon, 10 Jul 2006 13:23:00 +0100
Subject: [R] How to include NA's of a factor in  table?
Message-ID: <3.0.6.32.20060710132300.00ad9d80@pop.gmx.net>

Dear All,

Is there a better way to include NA's of a factor in the output of table()
than using as.character()?
Admittedly, I do not understand the help page for table concerning the
exclude argument applied to factors. I tried in different ways, but could
not get NA to be included in the table, if not using as.character() (see
example).

Greetings,
Heinz

## example
fcv <- factor(c('a', NA, 'c'))
table(fcv)                # shows a, c
table(fcv, exclude='a')   # shows c
table(fcv, exclude="")    # shows a, c
table(fcv, exclude=NULL)  # shows a, c
table(as.character(fcv), exclude=NULL) # shows a, c, NA

platform       i386-pc-mingw32                          
arch           i386                                     
os             mingw32                                  
system         i386, mingw32                            
status         Patched                                  
major          2                                        
minor          3.1                                      
year           2006                                     
month          07                                       
day            01                                       
svn rev        38471                                    
language       R                                        
version.string Version 2.3.1 Patched (2006-07-01 r38471)


From jacques.veslot at good.ibl.fr  Mon Jul 10 14:30:09 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Mon, 10 Jul 2006 14:30:09 +0200
Subject: [R] How to include NA's of a factor in  table?
In-Reply-To: <3.0.6.32.20060710132300.00ad9d80@pop.gmx.net>
References: <3.0.6.32.20060710132300.00ad9d80@pop.gmx.net>
Message-ID: <44B24851.9090701@good.ibl.fr>

fcv <- factor(c('a', NA, 'c'), exclude=NULL)
table(fcv, exclude=NULL)
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Heinz Tuechler a ?crit :
> Dear All,
> 
> Is there a better way to include NA's of a factor in the output of table()
> than using as.character()?
> Admittedly, I do not understand the help page for table concerning the
> exclude argument applied to factors. I tried in different ways, but could
> not get NA to be included in the table, if not using as.character() (see
> example).
> 
> Greetings,
> Heinz
> 
> ## example
> fcv <- factor(c('a', NA, 'c'))
> table(fcv)                # shows a, c
> table(fcv, exclude='a')   # shows c
> table(fcv, exclude="")    # shows a, c
> table(fcv, exclude=NULL)  # shows a, c
> table(as.character(fcv), exclude=NULL) # shows a, c, NA
> 
> platform       i386-pc-mingw32                          
> arch           i386                                     
> os             mingw32                                  
> system         i386, mingw32                            
> status         Patched                                  
> major          2                                        
> minor          3.1                                      
> year           2006                                     
> month          07                                       
> day            01                                       
> svn rev        38471                                    
> language       R                                        
> version.string Version 2.3.1 Patched (2006-07-01 r38471)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Mon Jul 10 14:34:13 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 10 Jul 2006 08:34:13 -0400
Subject: [R] How to include NA's of a factor in table?
In-Reply-To: <44B24851.9090701@good.ibl.fr>
References: <3.0.6.32.20060710132300.00ad9d80@pop.gmx.net>
	<44B24851.9090701@good.ibl.fr>
Message-ID: <971536df0607100534mf023bdaw1064a5e8922121f6@mail.gmail.com>

If we specify exclude=NULL in factor then it need not also be
specified in table:

fcv <- factor(c('a', NA, 'c'), exclude=NULL)
table(fcv)



On 7/10/06, Jacques VESLOT <jacques.veslot at good.ibl.fr> wrote:
> fcv <- factor(c('a', NA, 'c'), exclude=NULL)
> table(fcv, exclude=NULL)
> -------------------------------------------------------------------
> Jacques VESLOT
>
> CNRS UMR 8090
> I.B.L (2?me ?tage)
> 1 rue du Professeur Calmette
> B.P. 245
> 59019 Lille Cedex
>
> Tel : 33 (0)3.20.87.10.44
> Fax : 33 (0)3.20.87.10.31
>
> http://www-good.ibl.fr
> -------------------------------------------------------------------
>
>
> Heinz Tuechler a ?crit :
> > Dear All,
> >
> > Is there a better way to include NA's of a factor in the output of table()
> > than using as.character()?
> > Admittedly, I do not understand the help page for table concerning the
> > exclude argument applied to factors. I tried in different ways, but could
> > not get NA to be included in the table, if not using as.character() (see
> > example).
> >
> > Greetings,
> > Heinz
> >
> > ## example
> > fcv <- factor(c('a', NA, 'c'))
> > table(fcv)                # shows a, c
> > table(fcv, exclude='a')   # shows c
> > table(fcv, exclude="")    # shows a, c
> > table(fcv, exclude=NULL)  # shows a, c
> > table(as.character(fcv), exclude=NULL) # shows a, c, NA
> >
> > platform       i386-pc-mingw32
> > arch           i386
> > os             mingw32
> > system         i386, mingw32
> > status         Patched
> > major          2
> > minor          3.1
> > year           2006
> > month          07
> > day            01
> > svn rev        38471
> > language       R
> > version.string Version 2.3.1 Patched (2006-07-01 r38471)
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From R.Birnie at leeds.ac.uk  Mon Jul 10 14:48:48 2006
From: R.Birnie at leeds.ac.uk (Richard Birnie)
Date: Mon, 10 Jul 2006 13:48:48 +0100
Subject: [R] pvclust missing values problem
Message-ID: <C6AC304539F43C4196253FD94B7960F307AF90@HERMES2.ds.leeds.ac.uk>

Thanks for the responses,

Now I know why I got no response I'll know better in the future. Apologies all round for any inconvenience caused that was never my intention, just inexperience.

Richard

Dr Richard Birnie
Scientific Officer
Section of Pathology and Tumour Biology
Welcome Brenner Building, LIMM
St James University Hospital
Beckett St, Leeds, LS9 7TF
Tel:0113 3438624
e-mail: r.birnie at leeds.ac.uk


From timo.becker at oeaw.ac.at  Mon Jul 10 14:51:00 2006
From: timo.becker at oeaw.ac.at (Timo Becker)
Date: Mon, 10 Jul 2006 14:51:00 +0200
Subject: [R] distance in kmeans algorithm?
In-Reply-To: <mailman.8.1152525603.27272.r-help@stat.math.ethz.ch>
References: <mailman.8.1152525603.27272.r-help@stat.math.ethz.ch>
Message-ID: <44B24D34.60200@oeaw.ac.at>


>
>Hello.
>
>Is it possible to choose the distance in the kmeans algorithm?
>
>I have m vectors of n components and I want to cluster them using kmeans
>algorithm but I want to use the Mahalanobis distance or another distance.
>
>How can I do it in R?
>If I use kmeans, I have no option to choose the distance.
>
>Thanks in advance,
>
>Arnau.
>
>
>  
>
You can use Kmeans from the amap package with several distance measures.

# example for L1 and L2:
x <- matrix(c(0,0,0,1.5,1,-1), ncol=2, byrow=TRUE)
require(amap)
Kmeans(x, x[2:3,], method="manhattan")
Kmeans(x, x[2:3,], method="euclidean")

Cheers,
Timo

-- 
Timo Becker
Phonetics
Austrian Academy of Sciences
Acoustics Research Institute


From gavin.simpson at ucl.ac.uk  Mon Jul 10 14:54:27 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 10 Jul 2006 13:54:27 +0100
Subject: [R] A possible too old question on significant
	test	of	correlation matrix
In-Reply-To: <1152532093.21333.21.camel@gsimpson.geog.ucl.ac.uk>
References: <d3677d7d0607092227t5392a649l4d6e4e58a5e8ab99@mail.gmail.com>
	<1152517146.5665.9.camel@dhcppc2.my.nat.localnet>
	<d3677d7d0607100122w458f7f79t2a9d339653bf4f09@mail.gmail.com>
	<1152532093.21333.21.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <1152536067.21333.39.camel@gsimpson.geog.ucl.ac.uk>

On Mon, 2006-07-10 at 12:48 +0100, Gavin Simpson wrote:
> On Mon, 2006-07-10 at 16:22 +0800, Guo Wei-Wei wrote:
> > Hi, Gavin, your program is excellent. Thank  you very much!
> > 
> > And I have two further questions.
> > 
> > 1. Since it is very possible that the data contains missing value and
> > the program will failed against missing values, I have to delete all
> > the cases contained NA. Can it be done pairwisely?
> 
> Yes, with a modification to accept and pass on argument "use", e.g.:
> 
> data(iris)
> ## copy data
> iris2 <- iris
> ## simulate some missing values in Sepal.Length
> iris2[sample(1:nrow(iris2), 5), 1] <- NA
> 
> ## corProb matrix with missing values
> temp <- corProb(iris2[,1:4], use = "pairwise.complete.obs")
> 
> See ?cor for the options you can specify for "use". You'll need to paste
> in the functions below for this to work.
> 
> > 2. Can the program show t values instead of p values?
> 
> Yes - this is R! The function Bill Venables wrote uses F-values, so I
> looked at what cor.test was doing and modified the function to compute
> either t or F values and to return them or their p-values.

Oops, there was a simple error in the print method. Fixed below:

print.corProb <- function(x, digits = getOption("digits"),
                          quote = FALSE, na.print = "",
                          justify = "none", ...) {
  xx <- format(unclass(round(x, digits = 4)), digits = digits,
               justify = justify)
  if (any(ina <- is.na(x)))
    xx[ina] <- na.print
  cat("\nCorrelations are shown below the diagonal\n")
  if(attr(x, "pval"))
     cat(paste("P-values of the ", attr(x, "type"),
               "-statistics are shown above the diagonal\n\n",
               sep = ""))
  else
     cat(paste(attr(x, "type"),
               "-values are shown above the diagonal\n\n",
               sep = ""))
  if(attr(x, "type") == "t") {
    hypoth <- switch(attr(x, "hypoth"),
                     less = "less than 0",
                     greater = "greater than 0",
                     two.sided = "not equal to 0")
    cat(paste("alternative hypothesis: true correlation is",
              hypoth, "\n\n"))
  }
  print.default(xx, quote = quote, ...)
  invisible(x)
}

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
 London, UK. WC1E 6BT.         [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From jsorkin at grecc.umaryland.edu  Mon Jul 10 15:16:35 2006
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Mon, 10 Jul 2006 09:16:35 -0400
Subject: [R] How can I obtain the values of BIAS and STD. ERROR from
	a	bootstrap.
Message-ID: <s4b21b15.012@MEDICINE.umaryland.edu>

R 2.3.1
Windows XP

Question: How can I obtain the values of BIAS and STD. ERROR from a
bootstrap.

Background:
I am running a bootstrap:
result2<-boot(1:400,regSEvssample,R=5000)

and obtain the following results:

Bootstrap Statistics :
    original        bias    std. error
t1* 1.876602 -0.0001368616   0.1630380

I would like to get the values of ORIGINAL and BIAS. I can get the
value of ORIGINAL:

> result2$t0
[1] 1.876602

But I can't find a way to get the values of BIAS or  STD. ERROR. Can
someone suggest how I can obtain the values for these two statistics?

Thanks,
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC,
University of Maryland School of Medicine Claude D. Pepper OAIC,
University of Maryland Clinical Nutrition Research Unit, and
Baltimore VA Center Stroke of Excellence

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119
jsorkin at grecc.umaryland.edu


From ripley at stats.ox.ac.uk  Mon Jul 10 15:31:42 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 10 Jul 2006 14:31:42 +0100 (BST)
Subject: [R] How can I obtain the values of BIAS and STD. ERROR from a
 bootstrap.
In-Reply-To: <s4b21b15.012@MEDICINE.umaryland.edu>
References: <s4b21b15.012@MEDICINE.umaryland.edu>
Message-ID: <Pine.LNX.4.64.0607101430590.1548@gannet.stats.ox.ac.uk>

Look at the code of boot:::print.boot , which does the calculations.

On Mon, 10 Jul 2006, John Sorkin wrote:

> R 2.3.1
> Windows XP
>
> Question: How can I obtain the values of BIAS and STD. ERROR from a
> bootstrap.
>
> Background:
> I am running a bootstrap:
> result2<-boot(1:400,regSEvssample,R=5000)
>
> and obtain the following results:
>
> Bootstrap Statistics :
>    original        bias    std. error
> t1* 1.876602 -0.0001368616   0.1630380
>
> I would like to get the values of ORIGINAL and BIAS. I can get the
> value of ORIGINAL:
>
>> result2$t0
> [1] 1.876602
>
> But I can't find a way to get the values of BIAS or  STD. ERROR. Can
> someone suggest how I can obtain the values for these two statistics?
>
> Thanks,
> John
>
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC,
> University of Maryland School of Medicine Claude D. Pepper OAIC,
> University of Maryland Clinical Nutrition Research Unit, and
> Baltimore VA Center Stroke of Excellence
>
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>
> 410-605-7119
> jsorkin at grecc.umaryland.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From tuechler at gmx.at  Mon Jul 10 15:31:54 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Mon, 10 Jul 2006 14:31:54 +0100
Subject: [R] How to include NA's of a factor in table?
In-Reply-To: <971536df0607100534mf023bdaw1064a5e8922121f6@mail.gmail.com
 >
References: <44B24851.9090701@good.ibl.fr>
	<3.0.6.32.20060710132300.00ad9d80@pop.gmx.net>
	<44B24851.9090701@good.ibl.fr>
Message-ID: <3.0.6.32.20060710143154.00accb00@pop.gmx.net>

Thank you Jacques and Gabor!

Your solution does work and it led me to try also:

table(factor(fcv, exclude=NULL))  # shows a, c, NA 

Greetings,
Heinz



At 08:34 10.07.2006 -0400, Gabor Grothendieck wrote:
>If we specify exclude=NULL in factor then it need not also be
>specified in table:
>
>fcv <- factor(c('a', NA, 'c'), exclude=NULL)
>table(fcv)
>
>
>
>On 7/10/06, Jacques VESLOT <jacques.veslot at good.ibl.fr> wrote:
>> fcv <- factor(c('a', NA, 'c'), exclude=NULL)
>> table(fcv, exclude=NULL)
>> -------------------------------------------------------------------
>> Jacques VESLOT
>>
>> CNRS UMR 8090
>> I.B.L (2?me ?tage)
>> 1 rue du Professeur Calmette
>> B.P. 245
>> 59019 Lille Cedex
>>
>> Tel : 33 (0)3.20.87.10.44
>> Fax : 33 (0)3.20.87.10.31
>>
>> http://www-good.ibl.fr
>> -------------------------------------------------------------------
>>
>>
>> Heinz Tuechler a ?crit :
>> > Dear All,
>> >
>> > Is there a better way to include NA's of a factor in the output of
table()
>> > than using as.character()?
>> > Admittedly, I do not understand the help page for table concerning the
>> > exclude argument applied to factors. I tried in different ways, but could
>> > not get NA to be included in the table, if not using as.character() (see
>> > example).
>> >
>> > Greetings,
>> > Heinz
>> >
>> > ## example
>> > fcv <- factor(c('a', NA, 'c'))
>> > table(fcv)                # shows a, c
>> > table(fcv, exclude='a')   # shows c
>> > table(fcv, exclude="")    # shows a, c
>> > table(fcv, exclude=NULL)  # shows a, c
>> > table(as.character(fcv), exclude=NULL) # shows a, c, NA
>> >
>> > platform       i386-pc-mingw32
>> > arch           i386
>> > os             mingw32
>> > system         i386, mingw32
>> > status         Patched
>> > major          2
>> > minor          3.1
>> > year           2006
>> > month          07
>> > day            01
>> > svn rev        38471
>> > language       R
>> > version.string Version 2.3.1 Patched (2006-07-01 r38471)
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>> >
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>>
>
>


From priti.desai at kalyptorisk.com  Mon Jul 10 15:44:43 2006
From: priti.desai at kalyptorisk.com (priti desai)
Date: Mon, 10 Jul 2006 19:14:43 +0530
Subject: [R] Query:chi-squre test
Message-ID: <2AB7346A3227A74BB97F9A0D79E3E65A01C163@mailserver.kalyptorisk.com>

Hi,
  I have calculated chi-square goodness of fit test,Sample coming from
Poisson distribution.
please copy this script in R & run the script
The R script is as follows

########################## start
#########################################

No_of_Frauds<-
c(4,1,6,9,9,10,2,4,8,2,3,0,1,2,3,1,3,4,5,4,4,4,9,5,4,3,11,8,12,3,10,0,7)



lambda<- mean(No_of_Frauds)
 

# Chi-Squared Goodness of Fit Test

# Ho: The data follow a specified distribution Vs H1: Not Ho

# observed frequencies 

variable.cnts <- table(No_of_Frauds) 
variable.cnts

variable.cnts.prs <- dpois(as.numeric(names(variable.cnts)), lambda)
variable.cnts.prs

variable.cnts <- c(variable.cnts, 0) 
variable.cnts
variable.cnts.prs <- c(variable.cnts.prs, 1-sum(variable.cnts.prs))
variable.cnts.prs

tst <- chisq.test(variable.cnts, p=variable.cnts.prs)
Tst

######################### end ########################################


The result of R is as follows

Warning message:
Chi-squared approximation may be incorrect in: chisq.test(variable.cnts,
p = variable.cnts.prs) 
> tst

        Chi-squared test for given probabilities

data:  variable.cnts 
X-squared = 40.5614, df = 13, p-value = 0.0001122


But I have done calculations in Excel. I am getting different answer.

Observed  = 2,3,3,5,7,2,1,1,2,3,2,1,1,0
Expected=0.251005528,1.224602726,2.987288468,4.85811559,5.925428863,5.78
1782103,4.701348074,3.276697142,1.998288788,1.083247457,0.528493456,0.23
4400679,0.095299266,0.035764993


 Estimated Parameter  =4.878788

Chi square stat =  0.000113


My excel answer tally with the book which I have refer for excel.   
Please tell me the correct calculation in R.
And how to interprit the results in R.

Thanks.
Regards.
Priti.


From laurent.deniau at cern.ch  Mon Jul 10 15:54:02 2006
From: laurent.deniau at cern.ch (Laurent Deniau)
Date: Mon, 10 Jul 2006 15:54:02 +0200
Subject: [R] retrieving object name
Message-ID: <44B25BFA.2070706@cern.ch>

I have a data frame with named columns and I would like to know if it is 
possible to retrieve a column name once selected:

print(colnames(df))  # assumes to print "col1" "col2"
print.name(df$col1)  # would like to print "col1"
print.name(df$col2)  # would like to print "col2"

So what the print.name function should do?

My aim is not to print the column name but to select some settings from 
the column name withing the function (i.e. print.name), while this 
function is applied to several columns of the list/data.frame. Actually, 
I solved the problem by providing an extra parameter like:

print.name(df$col1, "col1")

but since I may have many of these columns/parameters combination, this 
is rather error prone and it would be much better if I could detect 
which columns of the data frame I am dealing with.

Thanks.

	ld.


From priti.desai at kalyptorisk.com  Mon Jul 10 16:00:45 2006
From: priti.desai at kalyptorisk.com (priti desai)
Date: Mon, 10 Jul 2006 19:30:45 +0530
Subject: [R] Query: chi square test
Message-ID: <2AB7346A3227A74BB97F9A0D79E3E65A01C164@mailserver.kalyptorisk.com>

Hi,
  I have calculated chi-square goodness of fit test,Sample coming from
Poisson distribution.
please copy this script in R & run the script The R script is as follows

########################## start
#########################################

No_of_Frauds<-
c(4,1,6,9,9,10,2,4,8,2,3,0,1,2,3,1,3,4,5,4,4,4,9,5,4,3,11,8,12,3,10,0,7)



lambda<- mean(No_of_Frauds)
 

# Chi-Squared Goodness of Fit Test

# Ho: The data follow a specified distribution Vs H1: Not Ho

# observed frequencies 

variable.cnts <- table(No_of_Frauds)
variable.cnts

variable.cnts.prs <- dpois(as.numeric(names(variable.cnts)), lambda)
variable.cnts.prs

variable.cnts <- c(variable.cnts, 0)
variable.cnts
variable.cnts.prs <- c(variable.cnts.prs, 1-sum(variable.cnts.prs))
variable.cnts.prs

tst <- chisq.test(variable.cnts, p=variable.cnts.prs) Tst

######################### end ########################################


The result of R is as follows

Warning message:
Chi-squared approximation may be incorrect in: chisq.test(variable.cnts,
p = variable.cnts.prs) 
> tst

        Chi-squared test for given probabilities

data:  variable.cnts
X-squared = 40.5614, df = 13, p-value = 0.0001122


But I have done calculations in Excel. I am getting different answer.

Observed  = 2,3,3,5,7,2,1,1,2,3,2,1,1,0
Expected=0.251005528,1.224602726,2.987288468,4.85811559,5.925428863,5.78
1782103,4.701348074,3.276697142,1.998288788,1.083247457,0.528493456,0.23
4400679,0.095299266,0.035764993


 Estimated Parameter  =4.878788

Chi square stat =  0.000113


My excel answer tally with the book which I have refer for excel.   
Please tell me the correct calculation in R.
And how to interprit the results in R.
Because actually data should fit for Poisson dist.

Thanks.
Regards.
Priti.


From tlumley at u.washington.edu  Mon Jul 10 16:12:54 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 10 Jul 2006 07:12:54 -0700 (PDT)
Subject: [R] parametric proportional hazard regression
In-Reply-To: <20060708004557.60360.qmail@web30803.mail.mud.yahoo.com>
References: <20060708004557.60360.qmail@web30803.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0607100708510.28265@homer24.u.washington.edu>

On Fri, 7 Jul 2006, Valentin Dimitrov wrote:
>
> I do not need a accelerated failure model, but a
> proportional hazard model with a f0= weibull,
> exponential, loglogistic or lognormal baseline
> distribution. The hazard function is
> lambda(t)=exp(Xi*beta)*lambda0(t),
> where lambda0 is the baseline hazard
> lambda0(t)=f0(t)/(1-F0(t)) where f0 and F0 are the
> baseline density and cumulative distribution
> functions.
> This is a proportional hazard model since the ratio
> lambda(t|Xi)/lambda(t|Xj)=exp(Xi*beta)/exp(Xj*beta)
> does not depend on t.
>

For a weibull (including exponential) model you can do this with survreg. 
For the other models you would have to maximize the likelihood directly. 
This will involve writing the likelihood directly in terms of the 
hazard and cumulative hazard, since a proportional hazards model that is 
gaussian at X=0 is not gaussian at any other X.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From ripley at stats.ox.ac.uk  Mon Jul 10 16:15:14 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 10 Jul 2006 15:15:14 +0100 (BST)
Subject: [R] retrieving object name
In-Reply-To: <44B25BFA.2070706@cern.ch>
References: <44B25BFA.2070706@cern.ch>
Message-ID: <Pine.LNX.4.64.0607101512160.9990@gannet.stats.ox.ac.uk>

On Mon, 10 Jul 2006, Laurent Deniau wrote:

> I have a data frame with named columns and I would like to know if it is
> possible to retrieve a column name once selected:

Not really.  df$col1 is a new object which does not know where it came 
from.

If you wanted to do this before selection, then

print.name <- function(df, col)
   struture(df[[col]], from=col)

would do the extraction and set an attribute to be consulted later.

> print(colnames(df))  # assumes to print "col1" "col2"
> print.name(df$col1)  # would like to print "col1"
> print.name(df$col2)  # would like to print "col2"
>
> So what the print.name function should do?
>
> My aim is not to print the column name but to select some settings from
> the column name withing the function (i.e. print.name), while this
> function is applied to several columns of the list/data.frame. Actually,
> I solved the problem by providing an extra parameter like:
>
> print.name(df$col1, "col1")
>
> but since I may have many of these columns/parameters combination, this
> is rather error prone and it would be much better if I could detect
> which columns of the data frame I am dealing with.
>
> Thanks.
>
> 	ld.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From tlumley at u.washington.edu  Mon Jul 10 16:17:35 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 10 Jul 2006 07:17:35 -0700 (PDT)
Subject: [R] survfit, unused argument(s) (error ...)
In-Reply-To: <000301c6a29c$42c39670$6400a8c0@IBM>
References: <000301c6a29c$42c39670$6400a8c0@IBM>
Message-ID: <Pine.LNX.4.64.0607100716590.28265@homer24.u.washington.edu>

On Sat, 8 Jul 2006, S?ren Merser wrote:

> Hi
>
> It seems that survfit() doesn't accept the argumnet 'error' as below
>
>> survfit(fit, error='greenwood')
> Error in survfit.coxph(fit, error = "greenwood") :
>        unused argument(s) (error ...)
>
> Isn't is allowed to do that for a coxph object?

Looking at the code it seems that you need vartype="greenwood", so the 
documentation is wrong.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

From p.dalgaard at biostat.ku.dk  Mon Jul 10 16:28:12 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Jul 2006 16:28:12 +0200
Subject: [R] Query:chi-squre test
In-Reply-To: <2AB7346A3227A74BB97F9A0D79E3E65A01C163@mailserver.kalyptorisk.com>
References: <2AB7346A3227A74BB97F9A0D79E3E65A01C163@mailserver.kalyptorisk.com>
Message-ID: <x2ac7hr0ub.fsf@turmalin.kubism.ku.dk>

"priti desai" <priti.desai at kalyptorisk.com> writes:

> Hi,
>   I have calculated chi-square goodness of fit test,Sample coming from
> Poisson distribution.
> please copy this script in R & run the script
> The R script is as follows
> 
> ########################## start
> #########################################
> 
> No_of_Frauds<-
> c(4,1,6,9,9,10,2,4,8,2,3,0,1,2,3,1,3,4,5,4,4,4,9,5,4,3,11,8,12,3,10,0,7)
> 
> 
> 
> lambda<- mean(No_of_Frauds)
>  
> 
> # Chi-Squared Goodness of Fit Test
> 
> # Ho: The data follow a specified distribution Vs H1: Not Ho
> 
> # observed frequencies 
> 
> variable.cnts <- table(No_of_Frauds) 
> variable.cnts
> 
> variable.cnts.prs <- dpois(as.numeric(names(variable.cnts)), lambda)
> variable.cnts.prs
> 
> variable.cnts <- c(variable.cnts, 0) 
> variable.cnts
> variable.cnts.prs <- c(variable.cnts.prs, 1-sum(variable.cnts.prs))
> variable.cnts.prs
> 
> tst <- chisq.test(variable.cnts, p=variable.cnts.prs)
> Tst
> 
> ######################### end ########################################
> 
> 
> The result of R is as follows
> 
> Warning message:
> Chi-squared approximation may be incorrect in: chisq.test(variable.cnts,
> p = variable.cnts.prs) 
> > tst
> 
>         Chi-squared test for given probabilities
> 
> data:  variable.cnts 
> X-squared = 40.5614, df = 13, p-value = 0.0001122
> 
> 
> But I have done calculations in Excel. I am getting different answer.
> 
> Observed  = 2,3,3,5,7,2,1,1,2,3,2,1,1,0
> Expected=0.251005528,1.224602726,2.987288468,4.85811559,5.925428863,5.78
> 1782103,4.701348074,3.276697142,1.998288788,1.083247457,0.528493456,0.23
> 4400679,0.095299266,0.035764993
> 
> 
>  Estimated Parameter  =4.878788
> 
> Chi square stat =  0.000113
> 
> 
> My excel answer tally with the book which I have refer for excel.   
> Please tell me the correct calculation in R.
> And how to interprit the results in R.

As far as I can see, the "Chi square stat" in Excel is essentially the
p-value in R. The slight difference appears to arise from Excel using
the point probability rather than the tail ditto in the last cell:

> O <- c(2,3,3,5,7,2,1,1,2,3,2,1,1,0)
> E <-  c(0.251005528,1.224602726,2.987288468,4.85811559,5.925428863,
+ 5.781782103,4.701348074,3.276697142,1.998288788,1.083247457,0.528493456,
+ 0.234400679,0.095299266,0.035764993)
> (O-E)^2/E
 [1] 1.218691e+01 2.573925e+00 5.409021e-05 4.143826e-03 1.948725e-01
 [6] 2.473610e+00 2.914053e+00 1.581883e+00 1.465377e-06 3.391598e+00
[11] 4.097178e+00 2.500600e+00 8.588560e+00 3.576499e-02
> sum((O-E)^2/E)
[1] 40.54315
> pchisq(sum((O-E)^2/E), 13,low=F)
[1] 0.0001129818
> E
 [1] 0.25100553 1.22460273 2.98728847 4.85811559 5.92542886 5.78178210
 [7] 4.70134807 3.27669714 1.99828879 1.08324746 0.52849346 0.23440068
[13] 0.09529927 0.03576499
> sum(E)
[1] 32.98176

Please don't assume that something is correct, just because it is
Excel output and some book mindlessly copied it...

The calculations are both wrong, because they ignore the fact that
lambda has been estimated from the data, and also because they deal
with very small expected cell counts. For a better test, you likely
need to simulate the distribution of the chi-square, or, as I'd be
inclined to do, go directly for the pretty obvious overdispersion:

> var(X)
[1] 11.17235
> var(X)/mean(X) # expected is ca. 1 in the Poisson distrib.
[1] 2.289984
> r <- replicate(100000,{X <- rpois(33, 4.87879); var(X)/mean(X)})
> sum(r > 2.289984)
[1] 5



-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ggrothendieck at gmail.com  Mon Jul 10 16:29:29 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 10 Jul 2006 10:29:29 -0400
Subject: [R] retrieving object name
In-Reply-To: <44B25BFA.2070706@cern.ch>
References: <44B25BFA.2070706@cern.ch>
Message-ID: <971536df0607100729ka8281c5gb6c546a09e5548ca@mail.gmail.com>

Use drop = FALSE.  For example using builtin data frame BOD we
can display the Time column with its heading:

BOD[, "Time", drop = FALSE]


On 7/10/06, Laurent Deniau <laurent.deniau at cern.ch> wrote:
> I have a data frame with named columns and I would like to know if it is
> possible to retrieve a column name once selected:
>
> print(colnames(df))  # assumes to print "col1" "col2"
> print.name(df$col1)  # would like to print "col1"
> print.name(df$col2)  # would like to print "col2"
>
> So what the print.name function should do?
>
> My aim is not to print the column name but to select some settings from
> the column name withing the function (i.e. print.name), while this
> function is applied to several columns of the list/data.frame. Actually,
> I solved the problem by providing an extra parameter like:
>
> print.name(df$col1, "col1")
>
> but since I may have many of these columns/parameters combination, this
> is rather error prone and it would be much better if I could detect
> which columns of the data frame I am dealing with.
>
> Thanks.
>
>        ld.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From tlumley at u.washington.edu  Mon Jul 10 16:37:07 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 10 Jul 2006 07:37:07 -0700 (PDT)
Subject: [R] problem in my code
In-Reply-To: <971536df0607092155y3e9174afy701600e6777a1bbb@mail.gmail.com>
References: <BAY110-F372FC2BEC639C1D10506F2C76B0@phx.gbl>
	<971536df0607092155y3e9174afy701600e6777a1bbb@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0607100727520.28265@homer24.u.washington.edu>

On Mon, 10 Jul 2006, Gabor Grothendieck wrote:

> The problem can be reduced to this:
>
> x <- 1
> x[1] <<- 2 # error
>
> The following are ok:
>
> x <- 1
> x[1] <- 3
>
> x <- 1
> x <- 4
>
> x <- 1
> x <<- 5
>
> Does anyone know why?  Is this a bug in <<- ?

No, it's a feature.  The fact that x<<-5 works is arguably a bug (though 
probably not worth fixing).

x[1] <<- 2 is equivalent (per section 3.4.4 of the language guide) to

`*tmp*` <- get("x", envir=parent.env(), inherits=TRUE)
`*tmp*`[1] <- 2
x <<- `*tmp*`

and the get() fails when you try to do this from the command line.  Since 
the point of superassignment is to assign in a lexical parent environment 
it makes no sense to do it directly at the command line.


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From maechler at stat.math.ethz.ch  Mon Jul 10 15:58:38 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 10 Jul 2006 15:58:38 +0200
Subject: [R] package:Matrix handling of data with identical indices
In-Reply-To: <D91C1AE1-9352-4D87-8D29-210F9AED0060@ysidro.econ.uiuc.edu>
References: <0C6BF3FC506F664F90C8BA3E0160462D04A75890@EXCHANGE3.ad.uams.edu>
	<40e66e0b0607090906u21db4786qcab1b25a37cdcac2@mail.gmail.com>
	<D91C1AE1-9352-4D87-8D29-210F9AED0060@ysidro.econ.uiuc.edu>
Message-ID: <17586.23822.474591.1357@stat.math.ethz.ch>

>>>>> "roger" == roger koenker <roger at ysidro.econ.uiuc.edu>
>>>>>     on Sun, 9 Jul 2006 12:31:16 -0500 writes:

    roger> On Jul 9, 2006, at 11:06 AM, Douglas Bates wrote:

    >> Your matrix Mc should be flagged as invalid.  Martin and
    >> I should discuss whether we want to add such a test to
    >> the validity method.  It is not difficult to add the test
    >> but there will be a penalty in that it will slow down all
    >> operations on such matrices and I'm not sure if we want
    >> to pay that price to catch a rather infrequently occuring
    >> problem.

    roger> Elaborating the validity procedure to flag such
    roger> instances seems to be well worth the speed penalty in
    roger> my view.  Of course, anticipating every such misstep
    roger> imposes a heavy burden on developers and constitutes
    roger> the real "cost" of more elaborate validity checking.

As I found, we already *have* a validate_dgCMatrix  in C code,
and adding an improved test for the validity of the 'p' slot,
solves ``all problems'' mentioned above --- without any
performance penalty.
Hence., in the upcoming next version of 'Matrix' (0.95-12),
John will get a proper error message immediately from
calling new(...) with the wrong 'p' (or 'Dim').

Martin

    roger> [My 2cents based on experience with SparseM.]

    roger> url: www.econ.uiuc.edu/~roger Roger Koenker email
    roger> rkoenker at uiuc.edu Department of Economics vox:
    roger> 217-333-4558 University of Illinois fax: 217-244-6678
    roger> Champaign, IL 61820


From laurent.deniau at cern.ch  Mon Jul 10 17:19:07 2006
From: laurent.deniau at cern.ch (Laurent Deniau)
Date: Mon, 10 Jul 2006 17:19:07 +0200
Subject: [R] retrieving object name
In-Reply-To: <Pine.LNX.4.64.0607101512160.9990@gannet.stats.ox.ac.uk>
References: <44B25BFA.2070706@cern.ch>
	<Pine.LNX.4.64.0607101512160.9990@gannet.stats.ox.ac.uk>
Message-ID: <44B26FEB.6010702@cern.ch>

Prof Brian Ripley wrote:
> On Mon, 10 Jul 2006, Laurent Deniau wrote:
> 
>> I have a data frame with named columns and I would like to know if it is
>> possible to retrieve a column name once selected:
> 
> 
> Not really.  df$col1 is a new object which does not know where it came 
> from.
> 
> If you wanted to do this before selection, then
> 
> print.name <- function(df, col)
>   struture(df[[col]], from=col)
> 
> would do the extraction and set an attribute to be consulted later.

This is an appropriate solution since I build the list myself (by 
splitting the data frame following some factors) but I do not call the 
function myself (e.g. print.name). Here is the final steps of the 
function which read and set the data.frame/list:

magnet.read.rt <- function (filename) {
   # ...

   # convert to list by magnet states
   MG <- split(MG,MG$magnet_state)

   # set state attribute for automatic settings
   for(col in names(MG))
     MG[[col]] <- structure(MG[[col]], state=col)

   invisible(MG)
}

and an example of its use:

.wc.scl <- function (MG) {
   c(CC=0.85, CM=1.0)[attr(MG,"state")]
}

wc.offset <- function (CM,CR) {
   scl <- .wc.scl(CM)
   #...
}

MB <- magnet.read.rt("magnet.dat")
CC.INJ <- wc.offset(MB$CC,MB$INJ)

Thanks.

a+, ld.


From hulubu at gmx.de  Mon Jul 10 18:43:03 2006
From: hulubu at gmx.de (hulubu at gmx.de)
Date: Mon, 10 Jul 2006 18:43:03 +0200
Subject: [R] 10^x instead 10EX on plot axes. How?
Message-ID: <20060710164303.213560@gmx.net>

Hi,

I'm drawing a very simple plot with both axes logarithmic (default base 10).
Example:
vec=c(1,10,100,1000,10000,100000,1000000,10000000)
plot(vec,vec,log="xy")

The axes on the plot now show the technical notation like 1E+3 but I would prefer to have it the notation 10 ^3 i.e. with the exponent here 3 superscript (raised).
Any help very much appreciated!

Best Regards 
 Tom
-- 


"Feel free" ? 10 GB Mailbox, 100 FreeSMS/Monat ...


From murdoch at stats.uwo.ca  Mon Jul 10 19:00:38 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 10 Jul 2006 13:00:38 -0400
Subject: [R] 10^x instead 10EX on plot axes. How?
In-Reply-To: <20060710164303.213560@gmx.net>
References: <20060710164303.213560@gmx.net>
Message-ID: <44B287B6.7030301@stats.uwo.ca>

On 7/10/2006 12:43 PM, hulubu at gmx.de wrote:
> Hi,
> 
> I'm drawing a very simple plot with both axes logarithmic (default base 10).
> Example:
> vec=c(1,10,100,1000,10000,100000,1000000,10000000)
> plot(vec,vec,log="xy")
> 
> The axes on the plot now show the technical notation like 1E+3 but I would prefer to have it the notation 10 ^3 i.e. with the exponent here 3 superscript (raised).
> Any help very much appreciated!

You can use the plotmath functions for axis labels.  For example,

 > vec=c(1,10,100,1000,10000,100000,1000000,10000000)
 > plot(vec,vec,log="xy", axes=F)
 > axis(1, at=10^c(0,2,4,6), labels=expression(1, 10^2, 10^4, 10^6))
 > axis(2, at=10^c(0,2,4,6), labels=expression(1, 10^2, 10^4, 10^6))
 > box()

Duncan Murdoch


From gunter.berton at gene.com  Mon Jul 10 19:03:14 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 10 Jul 2006 10:03:14 -0700
Subject: [R] 10^x instead 10EX on plot axes. How?
In-Reply-To: <20060710164303.213560@gmx.net>
Message-ID: <003e01c6a442$bc334030$711f210a@gne.windows.gene.com>

You can always draw the axes by hand.

?par with axes =FALSE
?axis
?plotmath for mathematical notation in R (for exponents)

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of hulubu at gmx.de
> Sent: Monday, July 10, 2006 9:43 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] 10^x instead 10EX on plot axes. How?
> 
> Hi,
> 
> I'm drawing a very simple plot with both axes logarithmic 
> (default base 10).
> Example:
> vec=c(1,10,100,1000,10000,100000,1000000,10000000)
> plot(vec,vec,log="xy")
> 
> The axes on the plot now show the technical notation like 
> 1E+3 but I would prefer to have it the notation 10 ^3 i.e. 
> with the exponent here 3 superscript (raised).
> Any help very much appreciated!
> 
> Best Regards 
>  Tom
> -- 
> 
> 
> "Feel free" - 10 GB Mailbox, 100 FreeSMS/Monat ...
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From cobleigh at gmail.com  Mon Jul 10 19:16:44 2006
From: cobleigh at gmail.com (Jamieson Cobleigh)
Date: Mon, 10 Jul 2006 13:16:44 -0400
Subject: [R] Setting the colors of lines in a trellis plot...
Message-ID: <7f50836c0607101016t472b64d2h95836fe8a10400ee@mail.gmail.com>

With some help from those with expertise on this list, I managed to
produce a plot using trellis that looked like I wanted it to look.
Now, I need to take the same plot and make the lines on it color, but
I want to specify the color for the lines myself.

I've managed to make the key use the colors I want.  I've managed to
make the symbols of the actual plot use the colors I want.  But I have
been unable to find the correct incantation to make the lines of the
actual plot use the colors I want.  Here's the relevant section of
code:

  mycolors <- c("black", "darkgreen", "red")

  mylines   <- Rows(superpose.line, 1:numlines);
  mylines$col <- mycolors

  mysymbols <- Rows(superpose.symbol, 1:numlines);
  mysymbols$pch <- c(15:18)[1:numlines]
  mysymbols$col <- mycolors

  print(xyplot(
    panel = panel.superpose,
    log10(states) ~ size,
    groups=category,
    data=data,
    type='b',
    lwd = 2,
    par.settings = list(superpose.symbol=mysymbols),
    ylim=c(y_min, y_max),
    scales = list(tck=c(1, 0), axs="r",

                  x=list(tick.number=(xmax - xmin + 1), at=xmin:xmax,
                  labels=xmin:xmax, cex=1.75),

                  y=list(axs="r", rot=c(90, 0), labels=y_labels,
                         at=y_at, cex=1.75
                  )
              ),
    key = list (
      text  = list(levels(data$category)),
      lines = list(type="b",
                   lty=mylines$lty,
                   pch=mysymbols$pch,
                   cex=rep(1.25, numlines),
                   col=mylines$col),
      x = .98,
      y = .25,
      corner=c(1,0)
    ),
    xlab = list(label="System Size", cex=2),
    ylab = list(label="States", cex=2),
  ))


Can anyone help me out with this?

Thanks!

Jamie


From ggrothendieck at gmail.com  Mon Jul 10 19:34:57 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 10 Jul 2006 13:34:57 -0400
Subject: [R] Setting the colors of lines in a trellis plot...
In-Reply-To: <7f50836c0607101016t472b64d2h95836fe8a10400ee@mail.gmail.com>
References: <7f50836c0607101016t472b64d2h95836fe8a10400ee@mail.gmail.com>
Message-ID: <971536df0607101034i357aa4a1l88bcef61bfe84d40@mail.gmail.com>

Use col= to specify colors, e.g.

    library(lattice)
    x <- 1:12
    xyplot(x ~ x, group = gl(3,4), col = 1:3, type = "l", auto.key = TRUE)

If this is not sufficiently close to your problem
1. cut your example down to a *minimal* size and
2. provide it as *self contained* and
3. *reproducible* code.

On 7/10/06, Jamieson Cobleigh <cobleigh at gmail.com> wrote:
> With some help from those with expertise on this list, I managed to
> produce a plot using trellis that looked like I wanted it to look.
> Now, I need to take the same plot and make the lines on it color, but
> I want to specify the color for the lines myself.
>
> I've managed to make the key use the colors I want.  I've managed to
> make the symbols of the actual plot use the colors I want.  But I have
> been unable to find the correct incantation to make the lines of the
> actual plot use the colors I want.  Here's the relevant section of
> code:
>
>  mycolors <- c("black", "darkgreen", "red")
>
>  mylines   <- Rows(superpose.line, 1:numlines);
>  mylines$col <- mycolors
>
>  mysymbols <- Rows(superpose.symbol, 1:numlines);
>  mysymbols$pch <- c(15:18)[1:numlines]
>  mysymbols$col <- mycolors
>
>  print(xyplot(
>    panel = panel.superpose,
>    log10(states) ~ size,
>    groups=category,
>    data=data,
>    type='b',
>    lwd = 2,
>    par.settings = list(superpose.symbol=mysymbols),
>    ylim=c(y_min, y_max),
>    scales = list(tck=c(1, 0), axs="r",
>
>                  x=list(tick.number=(xmax - xmin + 1), at=xmin:xmax,
>                  labels=xmin:xmax, cex=1.75),
>
>                  y=list(axs="r", rot=c(90, 0), labels=y_labels,
>                         at=y_at, cex=1.75
>                  )
>              ),
>    key = list (
>      text  = list(levels(data$category)),
>      lines = list(type="b",
>                   lty=mylines$lty,
>                   pch=mysymbols$pch,
>                   cex=rep(1.25, numlines),
>                   col=mylines$col),
>      x = .98,
>      y = .25,
>      corner=c(1,0)
>    ),
>    xlab = list(label="System Size", cex=2),
>    ylab = list(label="States", cex=2),
>  ))
>
>
> Can anyone help me out with this?
>
> Thanks!
>
> Jamie
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From Geoffrey.Matthews at wwu.edu  Mon Jul 10 19:31:25 2006
From: Geoffrey.Matthews at wwu.edu (Geoffrey Matthews)
Date: Mon, 10 Jul 2006 10:31:25 -0700
Subject: [R] Availability of quadplot3d package (UseR!2006 Four
	Dimensional Barycentric Plots in 3D)
References: <7b18cd4d0607080639k429570fdn15712463de2507b7@mail.gmail.com>
	<44AFC8B0.7030600@statistik.uni-dortmund.de>
Message-ID: <8910AF8FB6C6E84796358D71090AF7CF03EBFFF3@EVS1.univ.dir.wwu.edu>


Carlos and Uwe:

I apologize to all, but my server has crashed and I need to order
a new computer!  I'm at home now, but when I can get to my office
I'll try to send each of you a copy.

I will be submitting my packages to CRAN soon.  Uwe, perhaps the
quad3d stuff should just be incorporated into klaR?

Geoff

Geoffrey Matthews
geoffrey.matthews at wwu.edu




-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
Sent: Sat 7/8/2006 8:01 AM
To: Carlos Ortega
Cc: R-help at stat.math.ethz.ch; Geoffrey Matthews
Subject: Re: [R] Availability of quadplot3d package (UseR!2006 Four Dimensional Barycentric Plots in 3D)
 
Carlos Ortega wrote:
> Dear List,
> 
> I have been unable fo find the package quadplot3d referred in the
> Abstract/Presentation "Four Dimensional Barycentric Plot in 3D" presented in
> the UserR!2006.
> 
> Does anyone know if it is available ? And if so, if it is ported to Windows
> ?


I think we should ask the author of that presentation, Geoffrey Matthews 
(CCing).

I'm also interested in adding a corresponding plot feature into package 
klaR, which currently only covers static "quadplots". Hence it would be 
nice to see the package quadplot3d on CRAN, or at least I'd like to see 
its functionality contributed to some other CRAN package such as misc3d.

Uwe Ligges
	


> Thanks in anticipation,
> Carlos Ortega
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From cobleigh at gmail.com  Mon Jul 10 19:38:45 2006
From: cobleigh at gmail.com (Jamieson Cobleigh)
Date: Mon, 10 Jul 2006 13:38:45 -0400
Subject: [R] Setting the colors of lines in a trellis plot...
In-Reply-To: <971536df0607101034i357aa4a1l88bcef61bfe84d40@mail.gmail.com>
References: <7f50836c0607101016t472b64d2h95836fe8a10400ee@mail.gmail.com>
	<971536df0607101034i357aa4a1l88bcef61bfe84d40@mail.gmail.com>
Message-ID: <7f50836c0607101038u1387d4b7hb18dba56456792ec@mail.gmail.com>

The col command worked.

Thanks!

Jamie

On 7/10/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Use col= to specify colors, e.g.
>
>     library(lattice)
>     x <- 1:12
>     xyplot(x ~ x, group = gl(3,4), col = 1:3, type = "l", auto.key = TRUE)
>
> If this is not sufficiently close to your problem
> 1. cut your example down to a *minimal* size and
> 2. provide it as *self contained* and
> 3. *reproducible* code.
>
> On 7/10/06, Jamieson Cobleigh <cobleigh at gmail.com> wrote:
> > With some help from those with expertise on this list, I managed to
> > produce a plot using trellis that looked like I wanted it to look.
> > Now, I need to take the same plot and make the lines on it color, but
> > I want to specify the color for the lines myself.
> >
> > I've managed to make the key use the colors I want.  I've managed to
> > make the symbols of the actual plot use the colors I want.  But I have
> > been unable to find the correct incantation to make the lines of the
> > actual plot use the colors I want.  Here's the relevant section of
> > code:
> >
> >  mycolors <- c("black", "darkgreen", "red")
> >
> >  mylines   <- Rows(superpose.line, 1:numlines);
> >  mylines$col <- mycolors
> >
> >  mysymbols <- Rows(superpose.symbol, 1:numlines);
> >  mysymbols$pch <- c(15:18)[1:numlines]
> >  mysymbols$col <- mycolors
> >
> >  print(xyplot(
> >    panel = panel.superpose,
> >    log10(states) ~ size,
> >    groups=category,
> >    data=data,
> >    type='b',
> >    lwd = 2,
> >    par.settings = list(superpose.symbol=mysymbols),
> >    ylim=c(y_min, y_max),
> >    scales = list(tck=c(1, 0), axs="r",
> >
> >                  x=list(tick.number=(xmax - xmin + 1), at=xmin:xmax,
> >                  labels=xmin:xmax, cex=1.75),
> >
> >                  y=list(axs="r", rot=c(90, 0), labels=y_labels,
> >                         at=y_at, cex=1.75
> >                  )
> >              ),
> >    key = list (
> >      text  = list(levels(data$category)),
> >      lines = list(type="b",
> >                   lty=mylines$lty,
> >                   pch=mysymbols$pch,
> >                   cex=rep(1.25, numlines),
> >                   col=mylines$col),
> >      x = .98,
> >      y = .25,
> >      corner=c(1,0)
> >    ),
> >    xlab = list(label="System Size", cex=2),
> >    ylab = list(label="States", cex=2),
> >  ))
> >
> >
> > Can anyone help me out with this?
> >
> > Thanks!
> >
> > Jamie
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>


From Greg.Snow at intermountainmail.org  Mon Jul 10 19:52:54 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 10 Jul 2006 11:52:54 -0600
Subject: [R] another tcl/tk query
Message-ID: <07E228A5BE53C24CAD490193A7381BBB4B74E7@LP-EXCHVS07.CO.IHC.COM>

Try looking at the tkwait.window function, it may be what you need.  Create your tkwindow with all its components, then call tkwait.window on the toplevel window and the calling function will wait until that window goes away before continuing execution.

Hope this helps, 


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Charles Annis, P.E.
Sent: Saturday, July 08, 2006 11:11 AM
To: r-help at stat.math.ethz.ch
Subject: [R] another tcl/tk query

Greetings:

I wish to use a tcl/tk widget to ask for user-selected parameter values.  My widget works - it asks for and returns to my workspace the stuff I need.
Here is a snippet of my code:

###############################
OnOK <- function()
{
    LOG.X <<- as.logical(as.character(tclvalue(log.X.buttonValue)))
    LOG.Y <<- as.logical(as.character(tclvalue(log.Y.buttonValue)))
    natural.units.?.decision   <<-
as.double(as.character(tclvalue(?.decision)))
    natural.units.left.censor  <<-
as.double(as.character(tclvalue(left.censor)))
    natural.units.right.censor <<-
as.double(as.character(tclvalue(right.censor)))
    tkdestroy(t2)
}
###############################


My problem is this:  I would like to use the new input in the same routine that created, used, and destroyed the widget.  I can't seem to do that.  The routine executes with what it has.  I must wait for the calling routine to end before I can use the new info, which is correctly place in the workspace, in subsequent R routines.

Is there a way I can use the updated values in the same routine that created the widget?

Thanks for your advice - and patience.


Charles Annis, P.E.

PS - I did read Prof. Ripley's post of Wed 8/31/2005
"Re: [R] tcl/tk return problem" but was unable to benefit from it.


Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:? 614-455-3265
http://www.StatisticalEngineering.com
?

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From deepayan.sarkar at gmail.com  Mon Jul 10 20:27:49 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 10 Jul 2006 13:27:49 -0500
Subject: [R] Setting the colors of lines in a trellis plot...
In-Reply-To: <7f50836c0607101016t472b64d2h95836fe8a10400ee@mail.gmail.com>
References: <7f50836c0607101016t472b64d2h95836fe8a10400ee@mail.gmail.com>
Message-ID: <eb555e660607101127q268351b8r5381b72b14b7c53c@mail.gmail.com>

On 7/10/06, Jamieson Cobleigh <cobleigh at gmail.com> wrote:
> With some help from those with expertise on this list, I managed to
> produce a plot using trellis that looked like I wanted it to look.
> Now, I need to take the same plot and make the lines on it color, but
> I want to specify the color for the lines myself.
>
> I've managed to make the key use the colors I want.  I've managed to
> make the symbols of the actual plot use the colors I want.  But I have
> been unable to find the correct incantation to make the lines of the
> actual plot use the colors I want.  Here's the relevant section of
> code:
>
>   mycolors <- c("black", "darkgreen", "red")
>
>   mylines   <- Rows(superpose.line, 1:numlines);
>   mylines$col <- mycolors
>
>   mysymbols <- Rows(superpose.symbol, 1:numlines);
>   mysymbols$pch <- c(15:18)[1:numlines]
>   mysymbols$col <- mycolors
>
>   print(xyplot(
>     panel = panel.superpose,
>     log10(states) ~ size,
>     groups=category,
>     data=data,
>     type='b',
>     lwd = 2,
>     par.settings = list(superpose.symbol=mysymbols),

You have forgotten to add 'superpose.line=mylines'.

>     ylim=c(y_min, y_max),
>     scales = list(tck=c(1, 0), axs="r",
>
>                   x=list(tick.number=(xmax - xmin + 1), at=xmin:xmax,
>                   labels=xmin:xmax, cex=1.75),
>
>                   y=list(axs="r", rot=c(90, 0), labels=y_labels,
>                          at=y_at, cex=1.75
>                   )
>               ),
>     key = list (
>       text  = list(levels(data$category)),
>       lines = list(type="b",
>                    lty=mylines$lty,
>                    pch=mysymbols$pch,
>                    cex=rep(1.25, numlines),
>                    col=mylines$col),

You might consider using the 'auto.key' argument instead of 'key' (YMMV).

-Deepayan

>       x = .98,
>       y = .25,
>       corner=c(1,0)
>     ),
>     xlab = list(label="System Size", cex=2),
>     ylab = list(label="States", cex=2),
>   ))
>
>
> Can anyone help me out with this?
>
> Thanks!
>
> Jamie


From Greg.Snow at intermountainmail.org  Mon Jul 10 21:02:22 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 10 Jul 2006 13:02:22 -0600
Subject: [R] Adding Lines to Plot
Message-ID: <07E228A5BE53C24CAD490193A7381BBB4B7517@LP-EXCHVS07.CO.IHC.COM>

 There are a couple of different options that you can try (each
generalize a bit differently to other situations).

If you just want lines at the tick marks to go the entire width (and be
generated automatically to match the labels), then look at ?par and look
at 'tck', you can use this in the barplot call, or in a call to axis
after the fact.  If you want the bars on top then just issue another
call to barplot with add=T.

See abline and the h= option for drawing lines across the entire width.

See par and the 'usr' entry for how to find the user coordinates of the
edges of the plot (for adding lines by hand).

See the cnvrt.coords function in the TeachingDemos package for more
general conversions between coordinate systems (you can use this to draw
a line from half way across to the edge, or other aritrary locations,
probably overkill for your problem). 

The return value for barplot gives the center positions of the bars (the
heights match the data you input to barplot), see the first example in
the help for barplot.

Hope this helps,


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of justin rapp
Sent: Saturday, July 08, 2006 1:05 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Adding Lines to Plot

This seems like a question that I should be able to answer on my own but
after looking at the documentation I cannot seem to find the correct
method.

How do I add lines to a bar plot that extend from the vertical axis?
For example, my vertical axis is numbered in increments of 10 and I
would like these to go across the whole graph.

Also, is there a way to have R label the value of each bar so that I
know the value of each factor that is being plotted?

Thanks in advance.

jdr

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From jenifer at unt.edu  Mon Jul 10 21:24:12 2006
From: jenifer at unt.edu (Jenifer Larson-Hall)
Date: Mon, 10 Jul 2006 14:24:12 -0500
Subject: [R] Counting observations split by a factor when there are NAs
	in	the data
Message-ID: <s4b26330.008@GWIA.unt.edu>

I am a very novice R user, a social scientist (linguist) who is trying
to learn to use R after being very familiar with SPSS. Please be kind!

My concern:
I cannot figure out a way to get an accurate count of observations of
one column of data split by a factor when there are NAs in the data.

I know how to use commands like tapply and summaryBy to obtain other
summary statistics I am interested in, such as the following:
tapply(RLWTEST, list(STATUS), mean, na.rm=T)
summaryBy(RLWTEST~STATUS, data=lh.forgotten, FUN=c(mean, sd, min, max),
na.rm=T)

However, with tapply I know I cannot use length to get a count where
there are NAs. summaryBy appears to work the same way. I do know how to
get a count of the entire column using sum:
sum(!is.na(lh.forgotten$RLWTEST))

However, this does not give me a count split up by my factor (STATUS). I
have looked through Daalgard (2002) and Verzani (2005), and have
searched the help files, but with no luck.

Thank you in advance for your help. I love R and am interested in making
it more accessible to social scientist types like me. I know it can do
everything SPSS can and more, but sometimes the very simplest things
seem to be a lot harder in R.

Jenifer

Dr. Jenifer Larson-Hall
Assistant Professor of Linguistics
University of North Texas
(940)369-8950


From andy_liaw at merck.com  Mon Jul 10 21:30:45 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 10 Jul 2006 15:30:45 -0400
Subject: [R] Counting observations split by a factor when there are NA s
 in the data
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02884C7E@usctmx1106.merck.com>

Wouldn't something like table(status) give you want you want?  E.g.:

R> status <- factor(c("A", "B", "A", NA, "A", "B"))
R> table(status)
status
A B 
3 2 


Andy 

From: Jenifer Larson-Hall
> 
> I am a very novice R user, a social scientist (linguist) who 
> is trying to learn to use R after being very familiar with 
> SPSS. Please be kind!
> 
> My concern:
> I cannot figure out a way to get an accurate count of 
> observations of one column of data split by a factor when 
> there are NAs in the data.
> 
> I know how to use commands like tapply and summaryBy to 
> obtain other summary statistics I am interested in, such as 
> the following:
> tapply(RLWTEST, list(STATUS), mean, na.rm=T) 
> summaryBy(RLWTEST~STATUS, data=lh.forgotten, FUN=c(mean, sd, 
> min, max),
> na.rm=T)
> 
> However, with tapply I know I cannot use length to get a 
> count where there are NAs. summaryBy appears to work the same 
> way. I do know how to get a count of the entire column using sum:
> sum(!is.na(lh.forgotten$RLWTEST))
> 
> However, this does not give me a count split up by my factor 
> (STATUS). I have looked through Daalgard (2002) and Verzani 
> (2005), and have searched the help files, but with no luck.
> 
> Thank you in advance for your help. I love R and am 
> interested in making it more accessible to social scientist 
> types like me. I know it can do everything SPSS can and more, 
> but sometimes the very simplest things seem to be a lot harder in R.
> 
> Jenifer
> 
> Dr. Jenifer Larson-Hall
> Assistant Professor of Linguistics
> University of North Texas
> (940)369-8950
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From xiaosu at mail.ucf.edu  Mon Jul 10 21:41:30 2006
From: xiaosu at mail.ucf.edu (Xiaogang Su)
Date: Mon, 10 Jul 2006 15:41:30 -0400
Subject: [R] Start Model for POLYCLASS
Message-ID: <s4b27542.057@mail.ucf.edu>

Dear Dr. Graves, 

Thanks for your information and kindness! I really appreciate it.
According to my findings so far, POLYCLASS doesnot seem to allow for
variables being forced into the model. Even the commercialized MARS
doesnot have this function either. However, one ad hoc approach that
they suggested is to fit linear (or logistic) model and form rediduals
first and then run MARS with the residuals.  

Best regards,
-XG Su 


================================
Xiaogang Su,  Assistant Professor
Department of Statistics and Actuarial Science
University of Central Florida
Orlando, FL 32816
(407) 823-2940 [O]
xiaosu at mail.ucf.edu
http://pegasus.cc.ucf.edu/~xsu/


>>> Spencer Graves <spencer.graves at pdf.com> 7/7/2006 5:49:05 AM >>>
	  I have not seen a reply, so I will offer a few suggestions,
even 
though I've never used the 'polspline' package.  I scanned several of 
the help pages and looked in "~\library\polspline' where R is installed

on my hard drive and found no further documentation, and I found
nothing 
new from RSiteSearch("polyclass").  Googling for 'polyclass' led me to

"http://bear.fhcrc.org/~clk/soft.html", the home page for Charles 
Kooperberg, the author and maintainer.  If it were my problem, I might

try writing him directly at the email address given in 
help(package="polyclass");  I'm including him as a "cc" on this reply.

	  I spent time looking at this, because 'polyclass', 'polymars'
and 
'polspline' seem potentially related related to one of my secondary 
interests.  Unfortunately, I couldn't find sufficient documentation to

allow me to proceed with the time I felt I could afford to invest in 
this right now.

	  If it were my problem, before I wrote another email about
this, I'd 
first list the function 'polyclass', make a local copy, then work 
through an example line by line using 'debug(polyclass)'.  This 'debug'

facility is remarkably powerful and easy to use.  I've solved many 
problems like this using 'debug' in this way.

	  If that failed to provide the necessary enlightenment, I'd
submit 
another post including a self-contained example based on a modification

of the 'iris' example featured in the 'polyclass' help page.  Your 
example is NOT self contained, so I would NOT use that.  Using the 
'iris' example would make it much easier to explain clearly what you 
want.  It also makes it much easier for someone like me to experiment 
with alternatives and describe what I did in terms of a tested
example.

	  Hope this helps.
p.s.  I suggest you also review the posting guide! 
"www.R-project.org/posting-guide.html".

Xiaogang Su wrote:
> Dear all, 
> 
> I have a question on how to set up the starting model in POLYCLASS
and
> make sure the terms in the starting model retained in the final
> POLYCLASS model. 
> 
> In the function POLYMARS, this can be done using the STARTMODEL
option.
> See below for example, I started with model 
> y= b0 + b1*X1 + b2*X2 + b3*X4 + b4*X5 + b5*X2*X5 + e
> 
>> m00 <- matrix(c(
>      1,  NA, 0, NA, 1,     
>      2,  NA, 0, NA, 1,     
>      4,  NA, 0, NA, 1,     
>      5,  NA, 0, NA, 1,
>      2,  NA, 5, NA, 1),nrow = 5, ncol=5, byrow=TRUE);
> 
>> m2 <- polymars(response=PID2$y, predictors=PID2[,1:7], 
> startmodel=m00) 
>> summary(m2)  
> 
> But I could not figure out how this works for POLYCLASS. There is an
> option FIT in POLYCLASS, which needs to be a POLYCLASS object though.

> 
> Any suggestion or information is greatly appreciated. 
> 
> Sincerely,
> Xiaogang Su
> 
> 
> ================================
> Xiaogang Su,  Assistant Professor
> Department of Statistics and Actuarial Science
> University of Central Florida
> Orlando, FL 32816
> (407) 823-2940 [O]
> xiaosu at mail.ucf.edu 
> http://pegasus.cc.ucf.edu/~xsu/ 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From p.dalgaard at biostat.ku.dk  Mon Jul 10 21:47:06 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Jul 2006 21:47:06 +0200
Subject: [R] Counting observations split by a factor when there are NAs
	in	the data
In-Reply-To: <s4b26330.008@GWIA.unt.edu>
References: <s4b26330.008@GWIA.unt.edu>
Message-ID: <x2u05pz1hh.fsf@turmalin.kubism.ku.dk>

"Jenifer Larson-Hall" <jenifer at unt.edu> writes:

> I am a very novice R user, a social scientist (linguist) who is trying
> to learn to use R after being very familiar with SPSS. Please be kind!
> 
> My concern:
> I cannot figure out a way to get an accurate count of observations of
> one column of data split by a factor when there are NAs in the data.
> 
> I know how to use commands like tapply and summaryBy to obtain other
> summary statistics I am interested in, such as the following:
> tapply(RLWTEST, list(STATUS), mean, na.rm=T)
> summaryBy(RLWTEST~STATUS, data=lh.forgotten, FUN=c(mean, sd, min, max),
> na.rm=T)
> 
> However, with tapply I know I cannot use length to get a count where
> there are NAs. summaryBy appears to work the same way. I do know how to
> get a count of the entire column using sum:
> sum(!is.na(lh.forgotten$RLWTEST))
> 
> However, this does not give me a count split up by my factor (STATUS). I
> have looked through Daalgard (2002) and Verzani (2005), and have
                      ^^^^^^^^
                      Ahem!....

> searched the help files, but with no luck.

How about

with(lh.forgotten,
  tapply(!is.na(RLWTEST), STATUS, sum) 
)

or maybe just 

table(STATUS[!is.na(RLWTEST)]) 
 
> Thank you in advance for your help. I love R and am interested in making
> it more accessible to social scientist types like me. I know it can do
> everything SPSS can and more, but sometimes the very simplest things
> seem to be a lot harder in R.


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Ted.Harding at nessie.mcc.ac.uk  Mon Jul 10 21:52:50 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 10 Jul 2006 20:52:50 +0100 (BST)
Subject: [R] Source code for R graphics devices
Message-ID: <XFMail.060710205250.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

I'm trying to locate the source code for a (typical)
R graphics device, in order to study how it's done.

The underlying reason is that I'm thinking of trying
to create a graphics device for 'pic' (the diagram
drawing component of [g]troff).

I thought the xfig device would be a good place to
look, since the format of an xfig file is similar in
nature (though very different in detail) to 'pic' code.

When, in R, I type "xfig" to see the R code, I get the
line

  .Internal(XFig(file, old$paper, old$family, old$bg, old$fg,
      old$width, old$height, old$horizontal, old$pointsize, 
      old$onefile, old$pagecentre))

so I tried to locate the code for the Internal function "Xfig".

I can't seem to find it!

So where should I be looking?

Also -- a more general question on this topic -- presumably
any R graphics device is driven by a "stream" of raw graphics
data in some presumably device-independent format, which it
then translates. Where can I find this, and information on
its structure?

With thanks, and best wishes,
Ted.

PS Total newbie on this front!

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 10-Jul-06                                       Time: 20:52:46
------------------------------ XFMail ------------------------------


From robert-mcfadden at o2.pl  Mon Jul 10 22:00:26 2006
From: robert-mcfadden at o2.pl (Robert Mcfadden)
Date: Mon, 10 Jul 2006 22:00:26 +0200
Subject: [R] print color
Message-ID: <000001c6a45b$7cc2acb0$1191680a@robert>

B??dnie zakodowany tekst zosta? usuni?ty...
Plik: nie znany
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060710/8e0ea80d/attachment.pl 

From clk at fhcrc.org  Mon Jul 10 22:01:58 2006
From: clk at fhcrc.org (clk at fhcrc.org)
Date: Mon, 10 Jul 2006 13:01:58 -0700
Subject: [R] Start Model for POLYCLASS
In-Reply-To: <s4b27541.053@mail.ucf.edu>
References: <s4b27541.053@mail.ucf.edu>
Message-ID: <1152561718.44b2b23623ecc@webmail.fhcrc.org>

That's correct. The option is not there.
You can force it in a starting model - but it can still be deleted,

At some stage Insightful implemented a commercial set of polynomial
spline functions in S-Plus that does have that option. They got it
as far as a beta-library [quoting from my own homepage
http://bear.fhcrc.org/~clk]:
============================
 Doug Clarkson at Insightful Corp. has implemented commercial versions of hare,
heft, logspline and a variety of related methods, with lots of additional
features and options. See
http://www.insightful.com/downloads/libraries/default.asp. Look for S+BEST. 
============================
There are remaining bugs in this code.... Doug Clarkson is no longer
at Insightful, and the code seems to be pretty much abbandoned.

Charles

Quoting Xiaogang Su <xiaosu at mail.ucf.edu>:

> Dear Dr. Graves, 
> 
> Thanks for your information and kindness! I really appreciate it.
> According to my findings so far, POLYCLASS doesnot seem to allow for
> variables being forced into the model. Even the commercialized MARS
> doesnot have this function either. However, one ad hoc approach that
> they suggested is to fit linear (or logistic) model and form rediduals
> first and then run MARS with the residuals.  
> 
> Best regards,
> -XG Su 
> 
> 
> ================================
> Xiaogang Su,  Assistant Professor
> Department of Statistics and Actuarial Science
> University of Central Florida
> Orlando, FL 32816
> (407) 823-2940 [O]
> xiaosu at mail.ucf.edu
> http://pegasus.cc.ucf.edu/~xsu/
> 
> 
> >>> Spencer Graves <spencer.graves at pdf.com> 7/7/2006 5:49:05 AM >>>
> 	  I have not seen a reply, so I will offer a few suggestions,
> even 
> though I've never used the 'polspline' package.  I scanned several of 
> the help pages and looked in "~\library\polspline' where R is installed
> 
> on my hard drive and found no further documentation, and I found
> nothing 
> new from RSiteSearch("polyclass").  Googling for 'polyclass' led me to
> 
> "http://bear.fhcrc.org/~clk/soft.html", the home page for Charles 
> Kooperberg, the author and maintainer.  If it were my problem, I might
> 
> try writing him directly at the email address given in 
> help(package="polyclass");  I'm including him as a "cc" on this reply.
> 
> 	  I spent time looking at this, because 'polyclass', 'polymars'
> and 
> 'polspline' seem potentially related related to one of my secondary 
> interests.  Unfortunately, I couldn't find sufficient documentation to
> 
> allow me to proceed with the time I felt I could afford to invest in 
> this right now.
> 
> 	  If it were my problem, before I wrote another email about
> this, I'd 
> first list the function 'polyclass', make a local copy, then work 
> through an example line by line using 'debug(polyclass)'.  This 'debug'
> 
> facility is remarkably powerful and easy to use.  I've solved many 
> problems like this using 'debug' in this way.
> 
> 	  If that failed to provide the necessary enlightenment, I'd
> submit 
> another post including a self-contained example based on a modification
> 
> of the 'iris' example featured in the 'polyclass' help page.  Your 
> example is NOT self contained, so I would NOT use that.  Using the 
> 'iris' example would make it much easier to explain clearly what you 
> want.  It also makes it much easier for someone like me to experiment 
> with alternatives and describe what I did in terms of a tested
> example.
> 
> 	  Hope this helps.
> p.s.  I suggest you also review the posting guide! 
> "www.R-project.org/posting-guide.html".
> 
> Xiaogang Su wrote:
> > Dear all, 
> > 
> > I have a question on how to set up the starting model in POLYCLASS
> and
> > make sure the terms in the starting model retained in the final
> > POLYCLASS model. 
> > 
> > In the function POLYMARS, this can be done using the STARTMODEL
> option.
> > See below for example, I started with model 
> > y= b0 + b1*X1 + b2*X2 + b3*X4 + b4*X5 + b5*X2*X5 + e
> > 
> >> m00 <- matrix(c(
> >      1,  NA, 0, NA, 1,     
> >      2,  NA, 0, NA, 1,     
> >      4,  NA, 0, NA, 1,     
> >      5,  NA, 0, NA, 1,
> >      2,  NA, 5, NA, 1),nrow = 5, ncol=5, byrow=TRUE);
> > 
> >> m2 <- polymars(response=PID2$y, predictors=PID2[,1:7], 
> > startmodel=m00) 
> >> summary(m2)  
> > 
> > But I could not figure out how this works for POLYCLASS. There is an
> > option FIT in POLYCLASS, which needs to be a POLYCLASS object though.
> 
> > 
> > Any suggestion or information is greatly appreciated. 
> > 
> > Sincerely,
> > Xiaogang Su
> > 
> > 
> > ================================
> > Xiaogang Su,  Assistant Professor
> > Department of Statistics and Actuarial Science
> > University of Central Florida
> > Orlando, FL 32816
> > (407) 823-2940 [O]
> > xiaosu at mail.ucf.edu 
> > http://pegasus.cc.ucf.edu/~xsu/ 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


From kannaiah at bsd.uchicago.edu  Mon Jul 10 22:03:28 2006
From: kannaiah at bsd.uchicago.edu (kannaiah at bsd.uchicago.edu)
Date: Mon, 10 Jul 2006 15:03:28 -0500
Subject: [R] R newbie
Message-ID: <1152561808.44b2b290cb858@netmail.bsd.uchicago.edu>

Hello,

I am new to R and still feeling my way thru it. 

I am trying to plot the values from this file below on the X-axis of a plot. I
have attached the graph to the email...the one i am trying to recreate.

Exon	start		end
5'UTR	22540060	22540121
1	22540122	22540140
2	22540303	22540493
3	22541552	22541565
4	22542373	22542519
5	22544265	22544432
3'UTR	22544433	22544856


I would like to create small rectangles on the x-axis as colored boxes from
start position to end position of each exon...with the label showing 1, 2 etc
under each respective exon.

The 5' and 3' UTR would be colored different from the exons 1-5.


Any suggestions and ideas would greatly appreciated.

Thank you
Kiran


-------------------------------------------------
This email is intended only for the use of the individual or entity to which
it is addressed and may contain information that is privileged and
confidential.  If the reader of this email message is not the intended
recipient, you are hereby notified that any dissemination, distribution, or
copying of this communication is prohibited.  If you have received this email
in error, please notify the sender and destroy/delete all copies of the
transmittal.  Thank you.
-------------------------------------------------

From murdoch at stats.uwo.ca  Mon Jul 10 22:26:49 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 10 Jul 2006 16:26:49 -0400
Subject: [R] Source code for R graphics devices
In-Reply-To: <XFMail.060710205250.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060710205250.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <44B2B809.10501@stats.uwo.ca>

On 7/10/2006 3:52 PM, (Ted Harding) wrote:
> Hi Folks,
> 
> I'm trying to locate the source code for a (typical)
> R graphics device, in order to study how it's done.
> 
> The underlying reason is that I'm thinking of trying
> to create a graphics device for 'pic' (the diagram
> drawing component of [g]troff).
> 
> I thought the xfig device would be a good place to
> look, since the format of an xfig file is similar in
> nature (though very different in detail) to 'pic' code.
> 
> When, in R, I type "xfig" to see the R code, I get the
> line
> 
>   .Internal(XFig(file, old$paper, old$family, old$bg, old$fg,
>       old$width, old$height, old$horizontal, old$pointsize, 
>       old$onefile, old$pagecentre))
> 
> so I tried to locate the code for the Internal function "Xfig".
> 
> I can't seem to find it!
> 
> So where should I be looking?

The devices are in the grDevices package.  Look in the src directory 
there for the C code.  XFig is in the devPS.c file.

Duncan Murdoch

> 
> Also -- a more general question on this topic -- presumably
> any R graphics device is driven by a "stream" of raw graphics
> data in some presumably device-independent format, which it
> then translates. Where can I find this, and information on
> its structure?
> 
> With thanks, and best wishes,
> Ted.
> 
> PS Total newbie on this front!

Then it might be worth mentioning that this code isn't distributed with 
a binary version of R, you need a source tarball, and need to look in 
src/library/grDevices/src.

Duncan Murdoch
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 10-Jul-06                                       Time: 20:52:46
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From jfox at mcmaster.ca  Mon Jul 10 22:44:40 2006
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 10 Jul 2006 16:44:40 -0400
Subject: [R] Problem with package sem
In-Reply-To: <98CCA2C7-01FF-49D5-8767-58145AAF56EE@mail.nih.gov>
Message-ID: <web-131713545@cgpsrv2.cis.mcmaster.ca>

Dear Gang,

I'm not clear about the form of the model that you're trying to fit,
nor what the form of the likelihood would be for such as model, but the
short answer is that sem() requires that each covariance among observed
variables be for the same observations and hence for the same number
(N) of observations.

BTW, I didn't respond to your query to r-help because I was out of town
and unable to read email.

Regards,
 John

On Mon, 10 Jul 2006 11:36:06 -0400
 Gang Chen <gangchen at mail.nih.gov> wrote:
> Hi Dr. Fox,
> 
> I posted the following message to the R e-mail list, but didn't get
>  any response. I was wondering whether you could shed some light on
> my  problem.
> 
> Thank you very much.
> 
> Gang
> 
> ===
> 
> From: 	  p_connolly at ihug.co.nz
> Subject: 	Re: [R] Package sem
> Date: 	July 9, 2006 5:49:41 PM EDT
> To: 	  gangchen at mail.nih.gov
> 
> Hi,
> 
> I am trying to run some path analysis with Dr. Fox's sem package. The
> number N in sem(ram, S, N) is supposed to be the total number of
> observations, right? However, in my situation the effective number of
> degrees of freedom for each observed variable is estimated by some
> auto-regression process, thus each variable bears a different DF. Is
> there a way I could run such a path analysis with a vector of DF's in
> sem?
> 
> Thanks,
> Gang
> 

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/


From Ted.Harding at nessie.mcc.ac.uk  Mon Jul 10 22:49:17 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 10 Jul 2006 21:49:17 +0100 (BST)
Subject: [R] Source code for R graphics devices
In-Reply-To: <XFMail.060710205250.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.060710214917.Ted.Harding@nessie.mcc.ac.uk>

On 10-Jul-06 Ted Harding wrote:
> Hi Folks,
> 
> I'm trying to locate the source code for a (typical)
> R graphics device, in order to study how it's done.
> 
> [...]
> 
> When, in R, I type "xfig" to see the R code, I get the
> line
> 
>   .Internal(XFig(file, old$paper, old$family, old$bg, old$fg,
>       old$width, old$height, old$horizontal, old$pointsize, 
>       old$onefile, old$pagecentre))
> 
> so I tried to locate the code for the Internal function "Xfig".
> 
> I can't seem to find it!

Many thanks to Duncan and Chuck for help and advice. Which also
brought home to me that my problem was that I can't read!

Grepping for "Xfig" didn't work. Grepping for "XFig" would have
worked!

Thanks, and best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 10-Jul-06                                       Time: 21:49:15
------------------------------ XFMail ------------------------------


From br44114 at gmail.com  Mon Jul 10 22:53:26 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Mon, 10 Jul 2006 16:53:26 -0400
Subject: [R] print color
Message-ID: <8d5a36350607101353y66026ff2y3cfd7fab1ed29f25@mail.gmail.com>

One option is
library(R2HTML)
?HTML.cormat
The thing you're after is traffic highlighting (via CSS or HTML tags).
If HTML.cormat() doesn't do exactly what you want, modify the source
code. (By the way, I haven't used R2HTML so far so maybe there's a
more appropriate function.)


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robert Mcfadden
> Sent: Monday, July 10, 2006 4:00 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] print color
>
> Dear R Users,
>
> Is it possible to make R print the largest item in each row
> of a matrix X
> with red font? Example:
>
> 1        2        4        7
>
> 8        4        3        1
>
> ...............
>
> Therefore 7 and 8 should be in red color.
>
> I would appreciate any suggestion
>
> Robert McFadden
>
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


From jim at bitwrit.com.au  Tue Jul 11 13:26:14 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Tue, 11 Jul 2006 07:26:14 -0400
Subject: [R] 10^x instead 10EX on plot axes. How?
In-Reply-To: <20060710164303.213560@gmx.net>
References: <20060710164303.213560@gmx.net>
Message-ID: <44B38AD6.3050507@bitwrit.com.au>

hulubu at gmx.de wrote:
> Hi,
> 
> I'm drawing a very simple plot with both axes logarithmic (default base 10).
> Example:
> vec=c(1,10,100,1000,10000,100000,1000000,10000000)
> plot(vec,vec,log="xy")
> 
> The axes on the plot now show the technical notation like 1E+3 but I would prefer to have it the notation 10 ^3 i.e. with the exponent here 3 superscript (raised).
> Any help very much appreciated!
>
Hi Tom,

Have a look at axis.mult in the plotrix package.

Jim


From ivowel at gmail.com  Tue Jul 11 02:26:03 2006
From: ivowel at gmail.com (ivo welch)
Date: Mon, 10 Jul 2006 20:26:03 -0400
Subject: [R] move axis label text
Message-ID: <50d1c22d0607101726u7094a1d6r2dce39ab1e6a26ac@mail.gmail.com>

dear R wizards:  sorry, I am stumped.  what is the parameter to just
move the location of the x-label and y-lable (not the labels on the
ticks of the x-axis and y-axis)?  probably obvious, but not to me
right now... regards,  /iaw


From jholtman at gmail.com  Tue Jul 11 02:44:21 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 10 Jul 2006 20:44:21 -0400
Subject: [R] R newbie
In-Reply-To: <1152561808.44b2b290cb858@netmail.bsd.uchicago.edu>
References: <1152561808.44b2b290cb858@netmail.bsd.uchicago.edu>
Message-ID: <644e1f320607101744x3b8bc465qc995b95e4a529c03@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060710/893c119f/attachment.pl 

From zelickr at pdx.edu  Tue Jul 11 02:51:30 2006
From: zelickr at pdx.edu (Randy Zelick)
Date: Mon, 10 Jul 2006 17:51:30 -0700 (Pacific Daylight Time)
Subject: [R] file.choose but for folder/directory?
Message-ID: <Pine.WNT.4.64.0607101744001.3820@BIO-SB2-329-RZN.PSU.DS.PDX.EDU>


Hello there,

This question is relative to WindowsXP, using R 2.2.1:

I am looking for a function that allows a user to interactively choose a 
directory so I can use list.files to process all the files in that 
directory. I've looked at getwd, but this is not interactive. The 
functions file.choose and choose.files are the right idea, but these only 
permit selection of a file within a folder, not the folder itself.

Thanks for any and all suggestions,

=Randy=

R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201


From patrick at pdrechsler.de  Tue Jul 11 03:10:21 2006
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Tue, 11 Jul 2006 02:10:21 +0100
Subject: [R] test regression against given slope for reduced major axis
	regression (RMA)
Message-ID: <87lkr1osjm.fsf@pdrechsler.de>

Hi,

for testing if the slope of experimental data differs from a
given slope I'm using the function
"test_regression_against_slope" (see below).

I am now confronted with the problem that I have data which
requires a modelII regression (also called reduced major axes
regression (RMA) or geometric mean regression). For this I use
the function "modelII" (see below).

What would be a good way of adapting
"test_regression_against_slope" for use with RMA regression?

The question I am trying to answer is: "Does the slope acquired
from experimental data differ significantly from theoretical
predictions?"

Any feedback on this is highly appreciated. And if you need more
info do not hesitate to ask.

Kind Regards

Patrick



*test_regression_against_slope*

--8<------------------------schnipp------------------------->8---
test_regression_against_slope <- function(x,y,slope_2)
{
### TEST_REGRESSION_AGAINST_SLOPE tests a measured regression against a
### given regression.
###
### INPUT: 
###    
### x and y: raw data
### slope_2: the slope you would like to test against (ie 1/3)
###    
### OUTPUT:
###    
### pvalue: the P-value...
### upperlimit95 and lowerlimit95 give the 95 percent confidence
### intervall (two-tailed).
###
### see Sokal and Rohlf, p. 465/471
  
  n <- length(x)
  mydf <- n-2

  ## least square fit:
  x2 <- (x-mean(x))^2
  y2 <- (y-mean(y))^2

  ## regression (pedestrian solution):
  xy <- (x-mean(x))*(y-mean(y))
  slope1 <- sum(xy)/sum(x2)
  intercept_a <- mean(y) - slope1 * mean(x)

  ## model data y_hat:
  y_hat <- intercept_a + slope1 * x
  ## least squares of model data:
  y_hat2 <- (y - y_hat)^2

  s2yx <- sum(y_hat2) / (n-2)
  sb <- sqrt(s2yx/sum(x2))
  ts <- (slope1 - slope_2) / sb
  pvalue <-  2*(pt(abs(ts), df, lower.tail=FALSE))

  ## 0.95 for one-tailed 0.975 for two-tailed t-distribution with
  ## alpha<-5%:
  tval <- qt(.975, df=mydf)
  ts2 <- tval*sb

  lowerlimit95 <- slope1 - ts2
  upperlimit95 <- slope1 + ts2

  list(pvalue = pvalue,
       lowerlimit95 = lowerlimit95,
       upperlimit95 = upperlimit95)
}
--8<------------------------schnapp------------------------->8---




*modelII*

--8<------------------------schnipp------------------------->8---
modelII <- function(XjArray,YjArray){
###  ============================================================
###
### Purpose:
###
### Calculates MODEL II Regression paramaters. Also called "reduced
### major axis regression" (Prentice 1987) or "geometric mean
### regression" (Webb et al. 1981).
###
### Input:
###
### Two one dimensional arrays XjArray and YjArray containing the X
### and Y vectors.
###
### XjArray = [0 0.9 1.8 2.6 3.3 4.4 5.2 6.1 6.5 7.4]
### YjArray = [5.9 5.4 4.4 4.6 3.5 3.7 2.8 2.8 2.4 1.5]
###
### Output:
###
### A list with the following:
###
### sumXjYj As Double
### sumXj As Double, sumYj As Double
### sumXjSquared As Double, sumYjSquared As Double
### n As Long
### varXj, varYj
### output(7)
###
###  ============================================================

  sumXjYj <- 0
  sumXj <- 0
  sumYj <- 0
  n <- 0
  n <- length(XjArray)
  sumXjSquared <- 0
  sumYjSquared <- 0
  covariancexy <- 0
  
  for(i in 1:n){
    sumXjYj <- sumXjYj + XjArray[i] * YjArray[i]
    sumXj <- sumXj + XjArray[i]
    sumYj <- sumYj + YjArray[i]
    sumXjSquared <- sumXjSquared + XjArray[i]^2
    sumYjSquared <- sumYjSquared + YjArray[i]^2
  }
  
  ## Mean of X and Y vectors
  meanyj <- sumYj / n
  meanxj <- sumXj / n
  
  ## Create covariance
  for(i in 1:n){
    covariancexy <- covariancexy + ((XjArray[i] - meanxj) * (YjArray[i] - meanyj))
  }

  covariancexy <- covariancexy / n
  
  
  ## get variance of X and Y (SD)
  varXj <- (n * sumXjSquared-sumXj^2)/(n*(n - 1))
  varYj <- (n * sumYjSquared-sumYj^2)/(n*(n - 1))
  sdxij <- (sumXjSquared)-(sumXj^2/n)
  sdxik <- (sumYjSquared)-(sumYj^2/n)

  ## make beta    'sgn function to return sign with magnitude of 1
  betacoeff <- sign(covariancexy) * ((varYj^0.5) / (varXj^0.5))
  ##     'make intercept
  Intercept <- meanyj - meanxj * betacoeff
  
  ## Make R the pearson produce moment correlation coefficient
  if (varYj==0 | varXj==0){
    corrCoeff <- 0
  }else{
    corrCoeff <- (sumXjYj - ((sumXj * sumYj) / n)) / ((sdxij * sdxik)^0.5)
  }
  
  ## Make sample variances of betacoefficient and intercept
  variancebeta <- (varYj / varXj) * ((1 - (corrCoeff ^ 2)) / n)
  varianceintercept <- (varYj / n) * (1 - corrCoeff) * (2 + ((meanxj ^ 2) * ((1 + corrCoeff) / varXj)))
  sdbeta <- variancebeta^0.5
  sdintercept <- varianceintercept^0.5

  list(betacoeff=betacoeff, # <- Steigung
       Intercept=Intercept,
       sdbeta=sdbeta, # standard deviation
       sdintercept=sdintercept,
       meanxj=meanxj,
       meanyj=meanyj,
       corrCoeff=corrCoeff) # <- pearson correlation koeffizient ;
  
}
--8<------------------------schnapp------------------------->8---

-- 
Snoopy (on being house-trained with a rolled-up newspaper): 
It does tend however to give one a rather distorted view of the press!


From rmh at temple.edu  Tue Jul 11 03:31:59 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Mon, 10 Jul 2006 21:31:59 -0400 (EDT)
Subject: [R] file.choose but for folder/directory?
Message-ID: <20060710213159.BDY72378@po-d.temple.edu>

choose.dir

it is documented on the same page as choose.files


From murdoch at stats.uwo.ca  Tue Jul 11 03:39:45 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 10 Jul 2006 21:39:45 -0400
Subject: [R] file.choose but for folder/directory?
In-Reply-To: <Pine.WNT.4.64.0607101744001.3820@BIO-SB2-329-RZN.PSU.DS.PDX.EDU>
References: <Pine.WNT.4.64.0607101744001.3820@BIO-SB2-329-RZN.PSU.DS.PDX.EDU>
Message-ID: <44B30161.9080401@stats.uwo.ca>

On 7/10/2006 8:51 PM, Randy Zelick wrote:
> Hello there,
> 
> This question is relative to WindowsXP, using R 2.2.1:

Time to upgrade.  choose.dir (mentioned by Rich) was introduced in the 
next release.

Duncan Murdoch

> 
> I am looking for a function that allows a user to interactively choose a 
> directory so I can use list.files to process all the files in that 
> directory. I've looked at getwd, but this is not interactive. The 
> functions file.choose and choose.files are the right idea, but these only 
> permit selection of a file within a folder, not the folder itself.
> 
> Thanks for any and all suggestions,
> 
> =Randy=
> 
> R. Zelick				email: zelickr at pdx.edu
> Department of Biology			voice: 503-725-3086
> Portland State University		fax:   503-725-3888
> 
> mailing:
> P.O. Box 751
> Portland, OR 97207
> 
> shipping:
> 1719 SW 10th Ave, Room 246
> Portland, OR 97201
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ivowel at gmail.com  Tue Jul 11 03:57:17 2006
From: ivowel at gmail.com (ivo welch)
Date: Mon, 10 Jul 2006 21:57:17 -0400
Subject: [R] --no-save and --save toggle from inside R? + BATCH stderr
Message-ID: <50d1c22d0607101857g6bdbf172geb78378592d45872@mail.gmail.com>

dear R wizards:  is it possible to instruct R to save or no-save from
inside R?  or does this have to be given at invokation on the
command-line?    The same question applies to "--no-restore-data",
although this presumably would have to be decided in a .First()
function or something like it.

on a similar note, I would love a CMD BATCH invokation to output just
one line to stderr at the end of its run which tells me whether the
batch job ended prematurely because of an error.  I know this can be
undesirable if run in another program that refuses output (e.g., a web
browser, where this can cause havoc if issued before the
content-type:), so it would require that some switch like --silent
would suppress this one line summary/error.  This is not urgent---I
can use the R return code to write a wrapper for this facility, but
this might be nice default behavior.  My guess is that most of us
won't mind to see a flag in writing if something went badly wrong.

sincerely,

/iaw


From zelickr at pdx.edu  Tue Jul 11 03:57:38 2006
From: zelickr at pdx.edu (Randy Zelick)
Date: Mon, 10 Jul 2006 18:57:38 -0700 (Pacific Daylight Time)
Subject: [R] file.choose but for folder/directory -- THANKS!
In-Reply-To: <Pine.WNT.4.64.0607101744001.3820@BIO-SB2-329-RZN.PSU.DS.PDX.EDU>
References: <Pine.WNT.4.64.0607101744001.3820@BIO-SB2-329-RZN.PSU.DS.PDX.EDU>
Message-ID: <Pine.WNT.4.64.0607101855500.2144@BIO-SB2-329-RZN.PSU.DS.PDX.EDU>


Thanks to all who responded. I upgraded to 2.3.1 straight away, and 
choose.dir does exactly what I need.

Cheers,

=Randy=

R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201


From murdoch at stats.uwo.ca  Tue Jul 11 04:45:28 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 10 Jul 2006 22:45:28 -0400
Subject: [R] --no-save and --save toggle from inside R? + BATCH stderr
In-Reply-To: <50d1c22d0607101857g6bdbf172geb78378592d45872@mail.gmail.com>
References: <50d1c22d0607101857g6bdbf172geb78378592d45872@mail.gmail.com>
Message-ID: <44B310C8.2090008@stats.uwo.ca>

ivo welch wrote:
> dear R wizards:  is it possible to instruct R to save or no-save from
> inside R?  
You can specify the action when you call q() or quit(), but you can't 
change the default action.

> or does this have to be given at invokation on the
> command-line?    The same question applies to "--no-restore-data",
> although this presumably would have to be decided in a .First()
> function or something like it.
>   

This one can't currently be changed.

Duncan Murdoch
> on a similar note, I would love a CMD BATCH invokation to output just
> one line to stderr at the end of its run which tells me whether the
> batch job ended prematurely because of an error.  I know this can be
> undesirable if run in another program that refuses output (e.g., a web
> browser, where this can cause havoc if issued before the
> content-type:), so it would require that some switch like --silent
> would suppress this one line summary/error.  This is not urgent---I
> can use the R return code to write a wrapper for this facility, but
> this might be nice default behavior.  My guess is that most of us
> won't mind to see a flag in writing if something went badly wrong.
>
> sincerely,
>
> /iaw
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From gao at umd.edu  Tue Jul 11 04:57:04 2006
From: gao at umd.edu (Jing Gao)
Date: Mon, 10 Jul 2006 22:57:04 -0400
Subject: [R] problem with install gstat package
Message-ID: <76fa6312.d5f37b25.81b9800@po2.mail.umd.edu>

Hi there,
I tried to install gstat package to R in Linux. I follow the
instruction to use command " R CMD INSTALL -1 lib pkgs". But R
shows "syntax error in "R CMD"".
Can someone help me with the installation of gstat package?
Thank you!

Jing


From xiaosu at mail.ucf.edu  Tue Jul 11 08:19:13 2006
From: xiaosu at mail.ucf.edu (Xiaogang Su)
Date: Tue, 11 Jul 2006 02:19:13 -0400
Subject: [R] Start Model for POLYCLASS
Message-ID: <s4b30b34.068@mail.ucf.edu>

Dear Prof. Kooperberg,

Thanks a lot for your clarification! That is very helpful. I will check
out the polynomial
spline functions in S-Plus that you mentioned. 

Best regards,
-XG Su


================================
Xiaogang Su,  Assistant Professor
Department of Statistics and Actuarial Science
University of Central Florida
Orlando, FL 32816
(407) 823-2940 [O]
xiaosu at mail.ucf.edu
http://pegasus.cc.ucf.edu/~xsu/

>>> <clk at fhcrc.org> 07/10/06 4:01 PM >>>
That's correct. The option is not there.
You can force it in a starting model - but it can still be deleted,

At some stage Insightful implemented a commercial set of polynomial
spline functions in S-Plus that does have that option. They got it
as far as a beta-library [quoting from my own homepage
http://bear.fhcrc.org/~clk]:
============================
 Doug Clarkson at Insightful Corp. has implemented commercial versions
of hare,
heft, logspline and a variety of related methods, with lots of
additional
features and options. See
http://www.insightful.com/downloads/libraries/default.asp. Look for
S+BEST. 
============================
There are remaining bugs in this code.... Doug Clarkson is no longer
at Insightful, and the code seems to be pretty much abbandoned.

Charles

Quoting Xiaogang Su <xiaosu at mail.ucf.edu>:

> Dear Dr. Graves, 
> 
> Thanks for your information and kindness! I really appreciate it.
> According to my findings so far, POLYCLASS doesnot seem to allow for
> variables being forced into the model. Even the commercialized MARS
> doesnot have this function either. However, one ad hoc approach that
> they suggested is to fit linear (or logistic) model and form rediduals
> first and then run MARS with the residuals.  
> 
> Best regards,
> -XG Su 
> 
> 
> ================================
> Xiaogang Su,  Assistant Professor
> Department of Statistics and Actuarial Science
> University of Central Florida
> Orlando, FL 32816
> (407) 823-2940 [O]
> xiaosu at mail.ucf.edu
> http://pegasus.cc.ucf.edu/~xsu/
> 
> 
> >>> Spencer Graves <spencer.graves at pdf.com> 7/7/2006 5:49:05 AM >>>
> 	  I have not seen a reply, so I will offer a few suggestions,
> even 
> though I've never used the 'polspline' package.  I scanned several of 
> the help pages and looked in "~\library\polspline' where R is
installed
> 
> on my hard drive and found no further documentation, and I found
> nothing 
> new from RSiteSearch("polyclass").  Googling for 'polyclass' led me to
> 
> "http://bear.fhcrc.org/~clk/soft.html", the home page for Charles 
> Kooperberg, the author and maintainer.  If it were my problem, I might
> 
> try writing him directly at the email address given in 
> help(package="polyclass");  I'm including him as a "cc" on this reply.
> 
> 	  I spent time looking at this, because 'polyclass', 'polymars'
> and 
> 'polspline' seem potentially related related to one of my secondary 
> interests.  Unfortunately, I couldn't find sufficient documentation to
> 
> allow me to proceed with the time I felt I could afford to invest in 
> this right now.
> 
> 	  If it were my problem, before I wrote another email about
> this, I'd 
> first list the function 'polyclass', make a local copy, then work 
> through an example line by line using 'debug(polyclass)'.  This
'debug'
> 
> facility is remarkably powerful and easy to use.  I've solved many 
> problems like this using 'debug' in this way.
> 
> 	  If that failed to provide the necessary enlightenment, I'd
> submit 
> another post including a self-contained example based on a
modification
> 
> of the 'iris' example featured in the 'polyclass' help page.  Your 
> example is NOT self contained, so I would NOT use that.  Using the 
> 'iris' example would make it much easier to explain clearly what you 
> want.  It also makes it much easier for someone like me to experiment 
> with alternatives and describe what I did in terms of a tested
> example.
> 
> 	  Hope this helps.
> p.s.  I suggest you also review the posting guide! 
> "www.R-project.org/posting-guide.html".
> 
> Xiaogang Su wrote:
> > Dear all, 
> > 
> > I have a question on how to set up the starting model in POLYCLASS
> and
> > make sure the terms in the starting model retained in the final
> > POLYCLASS model. 
> > 
> > In the function POLYMARS, this can be done using the STARTMODEL
> option.
> > See below for example, I started with model 
> > y= b0 + b1*X1 + b2*X2 + b3*X4 + b4*X5 + b5*X2*X5 + e
> > 
> >> m00 <- matrix(c(
> >      1,  NA, 0, NA, 1,     
> >      2,  NA, 0, NA, 1,     
> >      4,  NA, 0, NA, 1,     
> >      5,  NA, 0, NA, 1,
> >      2,  NA, 5, NA, 1),nrow = 5, ncol=5, byrow=TRUE);
> > 
> >> m2 <- polymars(response=PID2$y, predictors=PID2[,1:7], 
> > startmodel=m00) 
> >> summary(m2)  
> > 
> > But I could not figure out how this works for POLYCLASS. There is an
> > option FIT in POLYCLASS, which needs to be a POLYCLASS object
though.
> 
> > 
> > Any suggestion or information is greatly appreciated. 
> > 
> > Sincerely,
> > Xiaogang Su
> > 
> > 
> > ================================
> > Xiaogang Su,  Assistant Professor
> > Department of Statistics and Actuarial Science
> > University of Central Florida
> > Orlando, FL 32816
> > (407) 823-2940 [O]
> > xiaosu at mail.ucf.edu 
> > http://pegasus.cc.ucf.edu/~xsu/ 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


From ripley at stats.ox.ac.uk  Tue Jul 11 08:21:42 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 11 Jul 2006 07:21:42 +0100 (BST)
Subject: [R] move axis label text
In-Reply-To: <50d1c22d0607101726u7094a1d6r2dce39ab1e6a26ac@mail.gmail.com>
References: <50d1c22d0607101726u7094a1d6r2dce39ab1e6a26ac@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0607110720190.25281@gannet.stats.ox.ac.uk>

On Mon, 10 Jul 2006, ivo welch wrote:

> dear R wizards:  sorry, I am stumped.  what is the parameter to just
> move the location of the x-label and y-lable (not the labels on the
> ticks of the x-axis and y-axis)?  probably obvious, but not to me
> right now... regards,  /iaw

They are all done by par(mgp):  ?title does say so.

     'mgp' The margin line (in 'mex' units) for the axis title, axis
          labels and axis line. The default is 'c(3, 1, 0)'.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Jul 11 08:25:00 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 11 Jul 2006 07:25:00 +0100 (BST)
Subject: [R] problem with install gstat package
In-Reply-To: <76fa6312.d5f37b25.81b9800@po2.mail.umd.edu>
References: <76fa6312.d5f37b25.81b9800@po2.mail.umd.edu>
Message-ID: <Pine.LNX.4.64.0607110723180.25281@gannet.stats.ox.ac.uk>

It is  -l (minus ell) not -1 (minus one)

Note that without actual cut-and-paste of the output, we cannot be sure if 
this is the problem: this is why the posting guide asks for such details.

On Mon, 10 Jul 2006, Jing Gao wrote:

> Hi there,
> I tried to install gstat package to R in Linux. I follow the
> instruction to use command " R CMD INSTALL -1 lib pkgs". But R
> shows "syntax error in "R CMD"".
> Can someone help me with the installation of gstat package?
> Thank you!
> 
> Jing

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Leandro_Magnusson at brown.edu  Tue Jul 11 08:27:44 2006
From: Leandro_Magnusson at brown.edu (Leandro Magnusson)
Date: Tue, 11 Jul 2006 03:27:44 -0300
Subject: [R] Tobit variance covariance matrix
Message-ID: <44B344E0.4010209@brown.edu>

Hi,

How can I recover the variance-covariance matrix of the tobit model from 
the variance-covariance of the survreg?
I first used to the survreg function and then I selected the variance 
matrix. However, the last parameter is log(scale) and not the variance 
of the standard deviation of the censored distribution as in the Tobit 
model.
tobit<- survreg(Surv(y, y > 0, type ='left')~ 0+ z + vh, dist = 'gaussian');
Om   <- tobit$var;

Thanks

Leandro


From r_econometrics at yahoo.co.in  Tue Jul 11 08:30:22 2006
From: r_econometrics at yahoo.co.in (Sumanta Basak)
Date: Tue, 11 Jul 2006 07:30:22 +0100 (BST)
Subject: [R] Plot Date
Message-ID: <20060711063022.37540.qmail@web7605.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060711/3a15111c/attachment.pl 

From r_econometrics at yahoo.co.in  Tue Jul 11 08:41:58 2006
From: r_econometrics at yahoo.co.in (Sumanta Basak)
Date: Tue, 11 Jul 2006 07:41:58 +0100 (BST)
Subject: [R] Other models of GARCH
Message-ID: <20060711064158.42390.qmail@web7608.mail.in.yahoo.com>




 				
---------------------------------
 Find out what India is talking about on  Yahoo! Answers India.
-------------- next part --------------
An embedded message was scrubbed...
From: unknown sender
Subject: no subject
Date: Fri, 12 May 2006 11:05:33 +0100 (BST)
Size: 1478
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060711/811b4c24/attachment.mht 

From vincent at 7d4.com  Tue Jul 11 08:52:44 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Tue, 11 Jul 2006 08:52:44 +0200
Subject: [R] Availability of quadplot3d package (UseR!2006
 Four	Dimensional Barycentric Plots in 3D)
In-Reply-To: <8910AF8FB6C6E84796358D71090AF7CF03EBFFF3@EVS1.univ.dir.wwu.edu>
References: <7b18cd4d0607080639k429570fdn15712463de2507b7@mail.gmail.com>	<44AFC8B0.7030600@statistik.uni-dortmund.de>
	<8910AF8FB6C6E84796358D71090AF7CF03EBFFF3@EVS1.univ.dir.wwu.edu>
Message-ID: <44B34ABC.1090206@7d4.com>

Geoffrey Matthews a ?crit :

> I will be submitting my packages to CRAN soon.  Uwe, perhaps the
> quad3d stuff should just be incorporated into klaR?

Hello Geoffrey,
I read your paper at
http://www.r-project.org/useR-2006/Presentations/
http://www.r-project.org/useR-2006/Slides/Matthews.pdf

I was also concerned by those vizualisations questions
and I just wanted to attest it's a very nice job.
Bravo and good continuation !


From celso.barros at gmail.com  Tue Jul 11 10:30:50 2006
From: celso.barros at gmail.com (Celso Barros)
Date: Tue, 11 Jul 2006 05:30:50 -0300
Subject: [R] DIfferent lengths
Message-ID: <f6b7dfdc0607110130x7e012efdw2e1f41fc50b12417@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060711/cff93c4a/attachment.pl 

From christian.ritter at shell.com  Tue Jul 11 11:12:26 2006
From: christian.ritter at shell.com (christian.ritter at shell.com)
Date: Tue, 11 Jul 2006 11:12:26 +0200
Subject: [R] Assistance with dll's to use with dyn.load
Message-ID: <156CDC8CCFD1894295D2907F16337A48014208C2@bru-s-006.europe.shell.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060711/744a5114/attachment.pl 

From robert-mcfadden at o2.pl  Tue Jul 11 11:19:31 2006
From: robert-mcfadden at o2.pl (Robert Mcfadden)
Date: Tue, 11 Jul 2006 11:19:31 +0200
Subject: [R] Second Partial Derivatives
Message-ID: <001601c6a4cb$1e290940$1191680a@robert>

B??dnie zakodowany tekst zosta? usuni?ty...
Plik: nie znany
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060711/cbf1fed0/attachment.pl 

From jennytimp at hotmail.com  Tue Jul 11 11:29:15 2006
From: jennytimp at hotmail.com (jenny tan)
Date: Tue, 11 Jul 2006 17:29:15 +0800
Subject: [R] Table of P values for Fisher's exact test
Message-ID: <BAY118-F7C2901D545BC8FD874403B9680@phx.gbl>

Hi,

I have a table of observed counts for various genetic markers. Instead of 
doing Fisher's exact test for each marker one at a time and recording the P 
value manually, is there a script to go through the whole list and generate 
the P value column automatically?

An example of my data:


Counts_CHB and Counts_AA are the observed counts for one allele.
2N_CHB and 2N_AA are the total number of alleles.

gene	Local_Pos	2N_CHB	2N_AA	Counts_CHB	Counts_AA     Exact_P
B2M	2475	90	46	0	5
B2M	3532	90	44	0	1
FCN2	2203	88	46	0	1
FCN2	3536	90	46	0	9
FCN2	4027	84	46	0	9
FCN2	4036	90	46	0	9
FCN2	4318	90	46	0	9
FCN2	4392	90	46	0	9
FCN2	9575	90	46	0	1


From celso.barros at gmail.com  Tue Jul 11 11:39:23 2006
From: celso.barros at gmail.com (Celso Barros)
Date: Tue, 11 Jul 2006 06:39:23 -0300
Subject: [R] weights in glmrob
Message-ID: <f6b7dfdc0607110239m5d2ab64fx3ef2871bc4d2c028@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060711/d1fe475a/attachment.pl 

From jacques.veslot at good.ibl.fr  Tue Jul 11 11:41:37 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Tue, 11 Jul 2006 11:41:37 +0200
Subject: [R] Table of P values for Fisher's exact test
In-Reply-To: <BAY118-F7C2901D545BC8FD874403B9680@phx.gbl>
References: <BAY118-F7C2901D545BC8FD874403B9680@phx.gbl>
Message-ID: <44B37251.2090606@good.ibl.fr>

apply(yourdata[, c("X2N_CHB","X2N_AA","Counts_CHB","Counts_AA")], 1, function(x) 
fisher.test(cbind(x[3:4], x[1:2]-x[3:4]))$p.value)
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


jenny tan a ?crit :
> Hi,
> 
> I have a table of observed counts for various genetic markers. Instead of 
> doing Fisher's exact test for each marker one at a time and recording the P 
> value manually, is there a script to go through the whole list and generate 
> the P value column automatically?
> 
> An example of my data:
> 
> 
> Counts_CHB and Counts_AA are the observed counts for one allele.
> 2N_CHB and 2N_AA are the total number of alleles.
> 
> gene	Local_Pos	2N_CHB	2N_AA	Counts_CHB	Counts_AA     Exact_P
> B2M	2475	90	46	0	5
> B2M	3532	90	44	0	1
> FCN2	2203	88	46	0	1
> FCN2	3536	90	46	0	9
> FCN2	4027	84	46	0	9
> FCN2	4036	90	46	0	9
> FCN2	4318	90	46	0	9
> FCN2	4392	90	46	0	9
> FCN2	9575	90	46	0	1
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From Sharon.Mazurel at ing.com  Tue Jul 11 11:44:47 2006
From: Sharon.Mazurel at ing.com (Sharon.Mazurel at ing.com)
Date: Tue, 11 Jul 2006 11:44:47 +0200
Subject: [R] Coxph function
Message-ID: <F8FD80E01ED2B04DAA5A7041BCF968380106103C@ing.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060711/7215ebcf/attachment.pl 

From Sharon.Mazurel at ing.com  Tue Jul 11 12:22:42 2006
From: Sharon.Mazurel at ing.com (Sharon.Mazurel at ing.com)
Date: Tue, 11 Jul 2006 12:22:42 +0200
Subject: [R] Coxph
Message-ID: <F8FD80E01ED2B04DAA5A7041BCF9683801061041@ing.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060711/6fe0ea66/attachment.pl 

From Rau at demogr.mpg.de  Tue Jul 11 12:44:05 2006
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Tue, 11 Jul 2006 12:44:05 +0200
Subject: [R] Second Partial Derivatives
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E6013DB512@HERMES.demogr.mpg.de>

Hi, 

> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robert Mcfadden
> 
> Does R have any build-in function which allow me to count 
> second partial
> derivatives numerically?

library(nlme)
?fdHess

I hope this is the direction you wanted to take.

Best,
Roland



----------
This mail has been sent through the MPI for Demographic Rese...{{dropped}}


From tuechler at gmx.at  Tue Jul 11 12:52:38 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Tue, 11 Jul 2006 11:52:38 +0100
Subject: [R] Coxph
In-Reply-To: <F8FD80E01ED2B04DAA5A7041BCF9683801061041@ing.com>
Message-ID: <3.0.6.32.20060711115238.00a5ee58@pop.gmx.net>

>From the help page of Surv:
"Although unusual, the event indicator can be omitted, in which case all
subjects are assumed to have an event."

That means, you can use coxph that way, _but_ it depends on your model.
Do you really want to model the time on study regardless of the kind of event?

Greetings,
Heinz T?chler

At 12:22 11.07.2006 +0200, Sharon.Mazurel at ing.com wrote:
>
>
>Dear all,
>
>
>
>My question is:
>
>In the Surv object you have two arguments, "time" and "event". I have
>two events, namely withdrawn and success. 
>I use no event or status argument in "Surv" because all my objects "die"
>in my data set.
>
>Does coxph function calculate the coefficients correctly when you put no
>"event" argument into the Surv object?
>
>Thus:
>
>Coxph(Surv(duration)~covariates,data=data) duration=duration of the deal
>
>Duration: is the time till one subject fails or succeed in my research.
>
>Can somebody help me?
>
>
>Best regards,
>
>
>Sharon Mazurel
>
>
>-----------------------------------------------------------------
>ATTENTION:
>The information in this electronic mail message is private and
>confidential, and only intended for the addressee. Should you
>receive this message by mistake, you are hereby notified that
>any disclosure, reproduction, distribution or use of this
>message is strictly prohibited. Please inform the sender by
>reply transmission and delete the message without copying or
>opening it.
>
>Messages and attachments are scanned for all viruses known.
>If this message contains password-protected attachments, the
>files have NOT been scanned for viruses by the ING mail domain.
>Always scan attachments before opening them.
>-----------------------------------------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>


From ggrothendieck at gmail.com  Tue Jul 11 12:53:54 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 11 Jul 2006 06:53:54 -0400
Subject: [R] Plot Date
In-Reply-To: <20060711063022.37540.qmail@web7605.mail.in.yahoo.com>
References: <20060711063022.37540.qmail@web7605.mail.in.yahoo.com>
Message-ID: <971536df0607110353j45d3064bo73bd4d6864ac3bbc@mail.gmail.com>

Try this.  We first generate some test data, then we plot the
data without the x axis.  We generate a subset of the times
consisting of a sequence of every 7 months and create an
axis with that.  Then we add smaller ticks every month.


library(zoo)

# test data
x <- seq(0, 10*365)
z <- zoo(x, as.Date("1990-01-01") + x)

plot(z, xaxt = "n")

tt <- seq(as.Date("1990-05-20"), time(z)[length(z)], by = "7 months")
axis(1, tt, format(tt, "%d/%m/%y"), cex.axis = 0.7, tcl = -0.6)

tt <- seq(as.Date("1990-01-20"), time(z)[length(z)], by = "months")
axis(1, tt, FALSE, tcl = -0.3)


On 7/11/06, Sumanta Basak <r_econometrics at yahoo.co.in> wrote:
> Hi R-Users,
>
> First of all i apologize if this is too simple for you. I want to plot an index of which i have daily data for 10 years. I want to plot specific dates in X axis, like 20/05/91     20/12/92     20/07/94   etc..... and the index value in Y axis. Please suggest me.
>
> Thanks,
> Sumanta.
>
>
> ---------------------------------
>  Find out what India is talking about on  Yahoo! Answers India.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From Sharon.Mazurel at ing.com  Tue Jul 11 12:59:27 2006
From: Sharon.Mazurel at ing.com (Sharon.Mazurel at ing.com)
Date: Tue, 11 Jul 2006 12:59:27 +0200
Subject: [R] Proportional Hazard Function and Competing risks
Message-ID: <F8FD80E01ED2B04DAA5A7041BCF9683801061043@ing.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060711/6cce82e9/attachment.pl 

From mcardeal at ufba.br  Tue Jul 11 13:20:21 2006
From: mcardeal at ufba.br (Mauricio Cardeal)
Date: Tue, 11 Jul 2006 08:20:21 -0300
Subject: [R] new object
Message-ID: <44B38975.9050901@ufba.br>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060711/1da40382/attachment.pl 

From ronggui.huang at gmail.com  Tue Jul 11 13:27:45 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Tue, 11 Jul 2006 19:27:45 +0800
Subject: [R] new object
In-Reply-To: <44B38975.9050901@ufba.br>
References: <44B38975.9050901@ufba.br>
Message-ID: <38b9f0350607110427n48154118obf4912727e9ca60c@mail.gmail.com>

> sum.out<-summary(fit)
> names(sum.out)
 [1] "surv"     "time"     "n.risk"   "n.event"  "conf.int" "std.err"
 [7] "lower"    "upper"    "strata"   "call"
> sum.out$n.risk
 [1] 11 10  8  7  5  4  2 12 10  8  6  5  4  3  2  1
> sum.out$n.event
 [1] 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1
> cbind(sum.out$n.risk,sum.out$n.event,sum.out$strata)
      [,1] [,2] [,3]
 [1,]   11    1    1
 [2,]   10    1    1
 [3,]    8    1    1
 [4,]    7    1    1
 [5,]    5    1    1
 [6,]    4    1    1
 [7,]    2    1    1
 [8,]   12    2    2
 [9,]   10    2    2
[10,]    8    1    2
[11,]    6    1    2
[12,]    5    1    2
[13,]    4    1    2
[14,]    3    1    2
[15,]    2    1    2
[16,]    1    1    2


2006/7/11, Mauricio Cardeal <mcardeal at ufba.br>:
> Hi !
>
> Please, how can I extract n.event and n.risk as a new object from
> example below ?  Thanks in advance.
> Mauricio
>
> require(survival)
> fit <- survfit(Surv(time, status) ~ x, data=aml)
>
> summary(fit)
>
> Call: survfit(formula = Surv(time, status) ~ x, data = aml)
>
>                 x=Maintained
>  time n.risk n.event survival std.err lower 95% CI upper 95% CI
>     9     11       1    0.909  0.0867       0.7541        1.000
>    13     10       1    0.818  0.1163       0.6192        1.000
>    18      8       1    0.716  0.1397       0.4884        1.000
>    23      7       1    0.614  0.1526       0.3769        0.999
>    31      5       1    0.491  0.1642       0.2549        0.946
>    34      4       1    0.368  0.1627       0.1549        0.875
>    48      2       1    0.184  0.1535       0.0359        0.944
>
>                 x=Nonmaintained
>  time n.risk n.event survival std.err lower 95% CI upper 95% CI
>     5     12       2   0.8333  0.1076       0.6470        1.000
>     8     10       2   0.6667  0.1361       0.4468        0.995
>    12      8       1   0.5833  0.1423       0.3616        0.941
>    23      6       1   0.4861  0.1481       0.2675        0.883
>    27      5       1   0.3889  0.1470       0.1854        0.816
>    30      4       1   0.2917  0.1387       0.1148        0.741
>    33      3       1   0.1944  0.1219       0.0569        0.664
>    43      2       1   0.0972  0.0919       0.0153        0.620
>    45      1       1   0.0000      NA           NA           NA
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
??????
Department of Sociology
Fudan University


From vincent at 7d4.com  Tue Jul 11 14:16:05 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Tue, 11 Jul 2006 14:16:05 +0200
Subject: [R] Misunderstanding with lines (or elsewhere)
Message-ID: <44B39685.80007@7d4.com>

Misunderstanding with lines(...) :
http://7d4.com/r/

I would like the y coordinate of the horizontal line to be 1/4,
and also the line to begin at x=0 and end at y=1.

I'm obviously missing something
... so if anybody could help.

Many thanks.


test = function()
{
bmp('test.bmp', width=100, height=100)
m = matrix(0, 2, 2)
par(new=T, fig = c(0,1,0,1), mai=c(0,0,0,0), mar=c(0,0,0,0), bty='n')
image(m)
lines(c(0,1), c(1/4,1/4))         # horizontal line
a = dev.off()
}


The result :
http://7d4.com/r/


From ripley at stats.ox.ac.uk  Tue Jul 11 14:25:11 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 11 Jul 2006 13:25:11 +0100 (BST)
Subject: [R] Misunderstanding with lines (or elsewhere)
In-Reply-To: <44B39685.80007@7d4.com>
References: <44B39685.80007@7d4.com>
Message-ID: <Pine.LNX.4.64.0607111322290.17674@gannet.stats.ox.ac.uk>

Think about the coordinate system you are using: if you don't set the 
margins to zero you will see what it is.

I suggest you supply x and y to image() to set the coordinate system to 
what you want it to be.

On Tue, 11 Jul 2006, vincent at 7d4.com wrote:

> Misunderstanding with lines(...) :
> http://7d4.com/r/
> 
> I would like the y coordinate of the horizontal line to be 1/4,
> and also the line to begin at x=0 and end at y=1.

And that is what you get.

> I'm obviously missing something
> ... so if anybody could help.
> 
> Many thanks.
> 
> 
> test = function()
> {
> bmp('test.bmp', width=100, height=100)
> m = matrix(0, 2, 2)
> par(new=T, fig = c(0,1,0,1), mai=c(0,0,0,0), mar=c(0,0,0,0), bty='n')
> image(m)
> lines(c(0,1), c(1/4,1/4))         # horizontal line
> a = dev.off()
> }
> 
> 
> The result :
> http://7d4.com/r/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jacques.veslot at good.ibl.fr  Tue Jul 11 14:28:11 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Tue, 11 Jul 2006 14:28:11 +0200
Subject: [R] new object
In-Reply-To: <44B38975.9050901@ufba.br>
References: <44B38975.9050901@ufba.br>
Message-ID: <44B3995B.6070301@good.ibl.fr>

 > names(summary(fit))
  [1] "surv"     "time"     "n.risk"   "n.event"  "conf.int" "std.err"
  [7] "lower"    "upper"    "strata"   "call"
 > summary(fit)$n.risk
  [1] 11 10  8  7  5  4  2 12 10  8  6  5  4  3  2  1

-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Mauricio Cardeal a ?crit :
> Hi !
> 
> Please, how can I extract n.event and n.risk as a new object from 
> example below ?  Thanks in advance.
> Mauricio
> 
> require(survival)
> fit <- survfit(Surv(time, status) ~ x, data=aml)
> 
> summary(fit)
> 
> Call: survfit(formula = Surv(time, status) ~ x, data = aml)
> 
>                 x=Maintained
>  time n.risk n.event survival std.err lower 95% CI upper 95% CI
>     9     11       1    0.909  0.0867       0.7541        1.000
>    13     10       1    0.818  0.1163       0.6192        1.000
>    18      8       1    0.716  0.1397       0.4884        1.000
>    23      7       1    0.614  0.1526       0.3769        0.999
>    31      5       1    0.491  0.1642       0.2549        0.946
>    34      4       1    0.368  0.1627       0.1549        0.875
>    48      2       1    0.184  0.1535       0.0359        0.944
> 
>                 x=Nonmaintained
>  time n.risk n.event survival std.err lower 95% CI upper 95% CI
>     5     12       2   0.8333  0.1076       0.6470        1.000
>     8     10       2   0.6667  0.1361       0.4468        0.995
>    12      8       1   0.5833  0.1423       0.3616        0.941
>    23      6       1   0.4861  0.1481       0.2675        0.883
>    27      5       1   0.3889  0.1470       0.1854        0.816
>    30      4       1   0.2917  0.1387       0.1148        0.741
>    33      3       1   0.1944  0.1219       0.0569        0.664
>    43      2       1   0.0972  0.0919       0.0153        0.620
>    45      1       1   0.0000      NA           NA           NA
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From edd at debian.org  Tue Jul 11 14:55:35 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 11 Jul 2006 07:55:35 -0500
Subject: [R] Other models of GARCH
In-Reply-To: <20060711064158.42390.qmail@web7608.mail.in.yahoo.com>
References: <20060711064158.42390.qmail@web7608.mail.in.yahoo.com>
Message-ID: <20060711125535.GA13708@eddelbuettel.com>

On Tue, Jul 11, 2006 at 07:41:58AM +0100, Sumanta Basak wrote: 
> Can you please tell me whether "R" can handle other type GARCH models
> like, FIGARCH, E-GARCH, JGR-GARCH etc? Any particular help for these 
> kind of models?

Yes R (no need to quote it) can: if you provide a (log-)likelihood function,
R can use its optimisers to find those parameter that maximise this
likelihood. 

In case you really wanted to ask if these are pre-programmed for you, then
the answer is no.

Hth, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From ivowel at gmail.com  Tue Jul 11 15:25:25 2006
From: ivowel at gmail.com (ivo welch)
Date: Tue, 11 Jul 2006 09:25:25 -0400
Subject: [R] --no-save and --save toggle from inside R? + BATCH stderr
In-Reply-To: <44B310C8.2090008@stats.uwo.ca>
References: <50d1c22d0607101857g6bdbf172geb78378592d45872@mail.gmail.com>
	<44B310C8.2090008@stats.uwo.ca>
Message-ID: <50d1c22d0607110625ydb08727i6573043b93e445d8@mail.gmail.com>

thank you.  exactly what I needed.

can I add it as a suggestion for the next R version to add
an option to quit that prints only one line to stderr (or stdout, but
the real one, not the batch output one) that gives a 1-line job
summary---"no error" (or nothing), "5 warnings", "5 warnings, fatal
error: blah".  except in rare circumstances, this is likely to be the
user-preferred behavior.

(taking a hand where I have gotten a finger---can I redefine a quit
function that can invoke the original quit() with an option?)

regards,

/iaw


From Zhijun.Liang at cern.ch  Tue Jul 11 15:30:43 2006
From: Zhijun.Liang at cern.ch (Zhijun Liang)
Date: Tue, 11 Jul 2006 15:30:43 +0200
Subject: [R] the problem of install Bugs package in R under linux environment
Message-ID: <4C5890AC4162B84F9661BEFE14762A0A013FC1D4@cernxchg04.cern.ch>

Dear All:
  Do you know how to install Bugs package in R under linux environment ?
  I try it ,but it failed to work,it seems that  Bugs package just available in windows.
  by the way ,when will the linux version of Bugs package come out ?
  Thank you in advance.
best regards
zhijun


From bruno.giordano at music.mcgill.ca  Tue Jul 11 15:31:29 2006
From: bruno.giordano at music.mcgill.ca (Bruno L. Giordano)
Date: Tue, 11 Jul 2006 09:31:29 -0400
Subject: [R]  non positive-definite G matrix in mixed models: bootstrap?
Message-ID: <000301c6a4ee$53cf63f0$6400a8c0@brungio>

Dear list,
In a mixed model I selected I find a non positive definite random effects 
variance-covariance matrix G, where some parameters are estimated close to 
zero, and related confidence intervals are incredibly large.

Since simplification of the random portion is not an option, for both 
interest in the parameters and significant increase in the model fit, I 
would like to collect "unbiased" random effects estimates.

I used bootstrap to this purpose, creating a linear model for each cluster 
and bootstraping the variance of the coefficients. Is this procedure 
reasonable? Would it be reasonable in this case to keep the marginal portion 
of the mixed model?
Note that in presence of positive-definite G matrix this bootstrap approach 
and the mixed effect model give highly similar estimates and that in the non 
positive-definite model the bootstrap and mixed model marginal-model 
estimates are highly similar as well.

Thank you
    Bruno


From vincent at 7d4.com  Tue Jul 11 15:39:58 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Tue, 11 Jul 2006 15:39:58 +0200
Subject: [R] Misunderstanding with lines (or elsewhere)
In-Reply-To: <Pine.LNX.4.64.0607111322290.17674@gannet.stats.ox.ac.uk>
References: <44B39685.80007@7d4.com>
	<Pine.LNX.4.64.0607111322290.17674@gannet.stats.ox.ac.uk>
Message-ID: <44B3AA2E.60504@7d4.com>

Prof Brian Ripley a ?crit :

> Think about the coordinate system you are using: if you don't set the 
> margins to zero you will see what it is.
> I suggest you supply x and y to image() to set the coordinate system to 
> what you want it to be.

Thank you for your answer.
If I have well understood, the right way to proceed
must be :

test = function()
{
m = matrix(0, 2, 2)
par(new=T, fig = c(0,1,0,1), mai=c(0,0,0,0), mar=c(0,0,0,0), bty='n')
x0 = c(0.25 , 0.75)
y0 = x0
image(x=x0, y=y0, z=m)
lines(c(0,1), c(1/4,1/4))         # horizontal line
}

which seems indeed to work.
Thanks.


From e.rehak at t-online.de  Tue Jul 11 15:40:53 2006
From: e.rehak at t-online.de (Mark Hempelmann)
Date: Tue, 11 Jul 2006 15:40:53 +0200
Subject: [R] Two Phase Sampling
Message-ID: <44B3AA65.7000900@t-online.de>

Dear WizaRds,

	I tried to construct a two-phase sampling design in Survey just the way 
I hoped understood in Vienna - I was wrong. I think I am too stupid to 
create the correct subset for phase 2. Phase1: Sample 1000 parts with 80 
defective. Phase2: Sample 100 parts out of these 1000 with  15 
defective. 0:ok, 1:defunct. The table below gives the conditional 
sampling values.

Please help me:

library(survey)
ss1 <- data.frame(id=1:1000, ph1.x=rep(c(1,0),c(10,990)),
subset=rep(c(1,0),c(100,900)), ph2.y=rep(c(1,0,NA),c(15,85,900)),
n1=rep(1000,1000), n2=rep(100,1000) )
table(ss1$ph1.y, ss1$ph2.x)

 >        Phase1.x
 >Phase2.y  0  1
 >       0 85  0
 >       1  5 10

p2 <- twophase(id=list(~id,~id), strata=list(NULL,NULL),
data=ss1, subset=~subset, fpc=list(~n1,~n2))
svymean (~ph2.y, design=p2s)

 >      mean SE
 >ph2.y 0.15  0

However, taking into consideration the 2nd sample, the estimator should be:

ph1.x.bar (phase1)=80/1000=0.08 and ph2.y.bar (phase2)=15/100=0.15 
defect boards, that means y.est=1.5*0.08=0.12 defect boards, since the 
RATIO ESTIMATOR equals 15/10=1.5 defect parts for the ratio of defect 
ph2/defect ph1.

What again did I do wrong? I am positive that the estimator is 12 
defective parts per 100 average, so how do I correctly construct the 
twophase design?

ps: I hope this is not sthg. undergraduates master eloquently...

Thank you so much for your help. I invite you to all the BBQ and beer 
there is in Europe!

Yours always
mark


From matmsh at yahoo.com  Mon Jul 10 20:45:32 2006
From: matmsh at yahoo.com (Shing Hing Man)
Date: Mon, 10 Jul 2006 19:45:32 +0100 (BST)
Subject: [R] Starting Rserve in Java using Runtime Failed
Message-ID: <20060710184532.88720.qmail@web52408.mail.yahoo.com>

I have tried using the following piece of Java code to

start Rserve on my Linux PC without success.


try {
String command = "R CMD Rserve ";
Process p = Runtime.getRuntime().exec(command);
returnCode = p.waitFor();
if (returnCode != 0) {
errorMessage = "Unexpected return code = " +
returnCode;
}

} catch (Exception e) {			
errorMessage = "Exception in running :" + e;
}


I did not get any non-zero return code or error
message. 
It works when I run 'R CMD Rserve' at the command
line.


My version of R is 2.2.1 and  I am using Rserve
v0.4-3.

Thanks in advance for any assistance!

Shing 

Home page :
  http://uk.geocities.com/matmsh/index.html


From mcardeal at ufba.br  Tue Jul 11 02:30:37 2006
From: mcardeal at ufba.br (Carlos Mauricio Cardeal Mendes)
Date: Mon, 10 Jul 2006 21:30:37 -0300
Subject: [R] help
Message-ID: <44B2F12D.80202@ufba.br>

Please, is there any R-list about survival package ?

Thanks
Mauricio


From ivowel at gmail.com  Tue Jul 11 16:01:43 2006
From: ivowel at gmail.com (ivo welch)
Date: Tue, 11 Jul 2006 10:01:43 -0400
Subject: [R] --no-save and --save toggle from inside R? + BATCH stderr
In-Reply-To: <50d1c22d0607110625ydb08727i6573043b93e445d8@mail.gmail.com>
References: <50d1c22d0607101857g6bdbf172geb78378592d45872@mail.gmail.com>
	<44B310C8.2090008@stats.uwo.ca>
	<50d1c22d0607110625ydb08727i6573043b93e445d8@mail.gmail.com>
Message-ID: <50d1c22d0607110701r29c2f0e9uff92f97ed92297fd@mail.gmail.com>

the following diff to the shell script invoking R prints an error
message if R terminates with an error code:

112c112,119
<       exec sh "${R_HOME}/bin/Rcmd" "${@}" ;;
---
>       sh "${R_HOME}/bin/Rcmd" "${@}"
>       rc="$?"
>       if [ "$rc" -ne 0 ]; then
>          shift ;
>          echo "${@}: Error Return Code: $rc"
>       fi
>       exit $?
>       ;;

regards,

/iaw


From murdoch at stats.uwo.ca  Tue Jul 11 16:18:43 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 11 Jul 2006 10:18:43 -0400
Subject: [R] --no-save and --save toggle from inside R? + BATCH stderr
In-Reply-To: <50d1c22d0607110701r29c2f0e9uff92f97ed92297fd@mail.gmail.com>
References: <50d1c22d0607101857g6bdbf172geb78378592d45872@mail.gmail.com>	
	<44B310C8.2090008@stats.uwo.ca>	
	<50d1c22d0607110625ydb08727i6573043b93e445d8@mail.gmail.com>
	<50d1c22d0607110701r29c2f0e9uff92f97ed92297fd@mail.gmail.com>
Message-ID: <44B3B343.5030307@stats.uwo.ca>

ivo welch wrote:
> the following diff to the shell script invoking R prints an error
> message if R terminates with an error code:
>
> 112c112,119
> <       exec sh "${R_HOME}/bin/Rcmd" "${@}" ;;
> ---
>   
>>       sh "${R_HOME}/bin/Rcmd" "${@}"
>>       rc="$?"
>>       if [ "$rc" -ne 0 ]; then
>>          shift ;
>>          echo "${@}: Error Return Code: $rc"
>>       fi
>>       exit $?
>>       ;;
>>     
This looks to me like a good suggestion, but I generally don't make 
modifications to platform-specific things on platforms I don't use, so 
I'd suggest posting this to R-devel or on the bug list as a wishlist 
item.  Otherwise it might get lost.


Duncan Murdoch


From rvaradhan at jhmi.edu  Tue Jul 11 16:23:00 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Tue, 11 Jul 2006 10:23:00 -0400
Subject: [R] Second Partial Derivatives
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E6013DB512@HERMES.demogr.mpg.de>
Message-ID: <001801c6a4f5$83b4ba00$7c94100a@win.ad.jhu.edu>

If you'd like to have accurate second derivatives, then check out the
"numDeriv" package, and in particular, the function "hessian".  The
derivatives are based on Richardson extrapolation, and can be evaluated to a
very high degree of accuracy.

Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
Webpage: http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html 
--------------------------------------------------------------------------
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Rau, Roland
> Sent: Tuesday, July 11, 2006 6:44 AM
> To: Robert Mcfadden; r-help at stat.math.ethz.ch
> Subject: Re: [R] Second Partial Derivatives
> 
> Hi,
> 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robert Mcfadden
> >
> > Does R have any build-in function which allow me to count
> > second partial
> > derivatives numerically?
> 
> library(nlme)
> ?fdHess
> 
> I hope this is the direction you wanted to take.
> 
> Best,
> Roland
> 
> 
> 
> ----------
> This mail has been sent through the MPI for Demographic Rese...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html


From HDoran at air.org  Tue Jul 11 16:23:46 2006
From: HDoran at air.org (Doran, Harold)
Date: Tue, 11 Jul 2006 10:23:46 -0400
Subject: [R] non positive-definite G matrix in mixed models: bootstrap?
Message-ID: <2323A6D37908A847A7C32F1E3662C80E1328B9@dc1ex01.air.org>

There is a paper by Rogosa and Saner which shows some equivalences in
what you are doing under certain conditions. They show similarities
between bootstrapping with linear models and how the estimates might be
similar to those obtained from a mixed model.

Rogosa, D. R., and Saner, H. M. (1995). Longitudinal data analysis
examples with random coefficient models. 
Journal of Educational and Behavioral Statistics, 20, 149-170. 

Harold


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bruno 
> L. Giordano
> Sent: Tuesday, July 11, 2006 9:31 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] non positive-definite G matrix in mixed models: 
> bootstrap?
> 
> Dear list,
> In a mixed model I selected I find a non positive definite 
> random effects variance-covariance matrix G, where some 
> parameters are estimated close to zero, and related 
> confidence intervals are incredibly large.
> 
> Since simplification of the random portion is not an option, 
> for both interest in the parameters and significant increase 
> in the model fit, I would like to collect "unbiased" random 
> effects estimates.
> 
> I used bootstrap to this purpose, creating a linear model for 
> each cluster and bootstraping the variance of the 
> coefficients. Is this procedure reasonable? Would it be 
> reasonable in this case to keep the marginal portion of the 
> mixed model?
> Note that in presence of positive-definite G matrix this 
> bootstrap approach and the mixed effect model give highly 
> similar estimates and that in the non positive-definite model 
> the bootstrap and mixed model marginal-model estimates are 
> highly similar as well.
> 
> Thank you
>     Bruno
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From xenamaa at yahoo.com  Tue Jul 11 16:32:09 2006
From: xenamaa at yahoo.com (zana adeb)
Date: Tue, 11 Jul 2006 07:32:09 -0700 (PDT)
Subject: [R] help on Rserve
Message-ID: <20060711143209.86715.qmail@web53107.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060711/bd0a650d/attachment.pl 

From tuechler at gmx.at  Tue Jul 11 16:31:58 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Tue, 11 Jul 2006 15:31:58 +0100
Subject: [R] Proportional Hazard Function and Competing risks
In-Reply-To: <F8FD80E01ED2B04DAA5A7041BCF9683801061043@ing.com>
Message-ID: <3.0.6.32.20060711153158.00ad5420@pop.gmx.net>

Maybe you find that thread helpful:

http://tolstoy.newcastle.edu.au/R/help/00b/1426.html

Heinz

At 12:59 11.07.2006 +0200, Sharon.Mazurel at ing.com wrote:
>
>
>How can I model coxph() in combination with competing risks
>
>i.e. I have two events and for event the object will leave the data set.
>So :
>
>Coxph(Surv(time,event)~....) the event is for all my objects 1.
>
>How can I model this?
>
>Sharon
>-----------------------------------------------------------------
>ATTENTION:
>The information in this electronic mail message is private and
>confidential, and only intended for the addressee. Should you
>receive this message by mistake, you are hereby notified that
>any disclosure, reproduction, distribution or use of this
>message is strictly prohibited. Please inform the sender by
>reply transmission and delete the message without copying or
>opening it.
>
>Messages and attachments are scanned for all viruses known.
>If this message contains password-protected attachments, the
>files have NOT been scanned for viruses by the ING mail domain.
>Always scan attachments before opening them.
>-----------------------------------------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>


From gavin.simpson at ucl.ac.uk  Tue Jul 11 16:34:11 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 11 Jul 2006 15:34:11 +0100
Subject: [R] Misunderstanding with lines (or elsewhere)
In-Reply-To: <44B3AA2E.60504@7d4.com>
References: <44B39685.80007@7d4.com>
	<Pine.LNX.4.64.0607111322290.17674@gannet.stats.ox.ac.uk>
	<44B3AA2E.60504@7d4.com>
Message-ID: <1152628451.1197.35.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2006-07-11 at 15:39 +0200, vincent at 7d4.com wrote:
> Prof Brian Ripley a ?crit :
> 
> > Think about the coordinate system you are using: if you don't set the 
> > margins to zero you will see what it is.
> > I suggest you supply x and y to image() to set the coordinate system to 
> > what you want it to be.
> 
> Thank you for your answer.
> If I have well understood, the right way to proceed
> must be :
> 
> test = function()
> {
> m = matrix(0, 2, 2)
> par(new=T, fig = c(0,1,0,1), mai=c(0,0,0,0), mar=c(0,0,0,0), bty='n')
> x0 = c(0.25 , 0.75)
> y0 = x0
> image(x=x0, y=y0, z=m)
> lines(c(0,1), c(1/4,1/4))         # horizontal line
> }
> 
> which seems indeed to work.
> Thanks.

You could swap the line:

lines(c(0,1), c(1/4, 1/4))

with:

abline(h = 0.25)

Does the same thing but is more readable IMHO.

Also, I get a warning with your code if no plot is open:

Warning message:
calling par(new=) with no plot

To stop this, drop the new = T (sic) in the call to par. Also, don't use
T when you mean TRUE.

So, in summary this seems to do the same thing as your function:

test = function()
{
m = matrix(0, 2, 2)
par(fig = c(0,1,0,1), mai=c(0,0,0,0), mar=c(0,0,0,0), bty='n')
x0 = c(0.25 , 0.75)
y0 = x0
image(x=x0, y=y0, z=m)
abline(h = 0.25)
}

HTH,

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
 London, UK. WC1E 6BT.         [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From ripley at stats.ox.ac.uk  Tue Jul 11 16:41:03 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 11 Jul 2006 15:41:03 +0100 (BST)
Subject: [R] --no-save and --save toggle from inside R? + BATCH stderr
In-Reply-To: <44B3B343.5030307@stats.uwo.ca>
References: <50d1c22d0607101857g6bdbf172geb78378592d45872@mail.gmail.com> 
	<44B310C8.2090008@stats.uwo.ca>
	<50d1c22d0607110625ydb08727i6573043b93e445d8@mail.gmail.com>
	<50d1c22d0607110701r29c2f0e9uff92f97ed92297fd@mail.gmail.com>
	<44B3B343.5030307@stats.uwo.ca>
Message-ID: <Pine.LNX.4.64.0607111531420.28013@gannet.stats.ox.ac.uk>

This sort of thing is contrary to the Unix spirit.  It gives the user no 
choice but to be told that there is a non-zero error code, whereas it is 
really easy for the user to wrap the command in his/her own script that 
reports the error code in whatever form is desired.

The Unix idea is to think about combining things from your toolkit, not
complicate the tools with endless options (and this would have to be made 
optional as I for one don't want it).

Assuming you are concerned with user errors and not those deep in R, you 
can use R's own error-handling system to set the error status anyway, and 
report as wanted.


On Tue, 11 Jul 2006, Duncan Murdoch wrote:

> ivo welch wrote:
> > the following diff to the shell script invoking R prints an error
> > message if R terminates with an error code:
> >
> > 112c112,119
> > <       exec sh "${R_HOME}/bin/Rcmd" "${@}" ;;
> > ---
> >   
> >>       sh "${R_HOME}/bin/Rcmd" "${@}"
> >>       rc="$?"
> >>       if [ "$rc" -ne 0 ]; then
> >>          shift ;
> >>          echo "${@}: Error Return Code: $rc"
> >>       fi
> >>       exit $?
> >>       ;;
> >>     
> This looks to me like a good suggestion, but I generally don't make 
> modifications to platform-specific things on platforms I don't use, so 
> I'd suggest posting this to R-devel or on the bug list as a wishlist 
> item.  Otherwise it might get lost.
> 
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From dominik.grathwohl at rdls.nestle.com  Tue Jul 11 16:52:15 2006
From: dominik.grathwohl at rdls.nestle.com (Grathwohl, Dominik, LAUSANNE,
	NRC-BAS)
Date: Tue, 11 Jul 2006 16:52:15 +0200
Subject: [R] Multiple tests on 2 way-ANOVA
Message-ID: <63E04C5ADEDACB4989972239CDABF05782BED1@chlsne01.nestle.com>

Dear r-helpers,

I have a question about multiple testing.
Here an example that puzzles me:
All matrixes and contrast vectors are presented in treatment contrasts.

1. example:
library(multcomp)
n<-60; sigma<-20
# n = sample size per group
# sigma standard deviation of the residuals

cov1 <- matrix(c(3/4,-1/2,-1/2,-1/2,1,0,-1/2,0,1), nrow = 3, ncol=3, byrow=TRUE, 
	dimnames = list(c("A", "B", "C"), c("C.1", "C.2", "C.3")))
# cov1 = variance covariance matrix of the beta coefficients of a 
# 2x2 factorial design (see Piantadosi 2005, p. 509)

cm1 <- matrix(c(0, 1, 0, 0, 0, 1), nrow = 2, ncol=3, byrow=TRUE, 
	dimnames = list(c("A", "B"), c("C.1", "C.2", "C.3")))
# cm1 = contrast matrix for main effects

v1 <- csimint(estpar=c(100, 6, 5), df=4*n-3, covm=cov1*sigma^2/n, cmatrix=cm1, conf.level=0.95)
summary(v1)

The adjusted p-values are almost the Bonferroni p-values.
If I understood right: You need not to adjust for multiple testing 
on main effects in a 2x2 factorial design 
assuming the absence of interaction. 
I do not think that there is a bug, 
I want to understand, why multcomp does adjust for multiple tests 
having all information about the design of the trial (variance covariance matrix)?
Or do I have to introduce somehow more information?

2. example:
And I have second question: How do I proper correct for multiple testing 
if I want to estimate in the presence of interaction the two average main effects.
Can some one point me to some literature where I can learn these things?
Here the example, 2x2 factorial with interaction, estimation of average main effects:

cov2 <- matrix(
c(1,-1,-1, 1,
 -1, 2, 1,-2,
 -1, 1, 2,-2,
  1,-2,-2, 4)
, nrow=4, ncol=4, byrow=TRUE)
cm2 <- matrix(c(0, 1, 0, 1/2, 0, 0, 1, 1/2), nrow = 2, ncol=4, byrow=TRUE, 
	dimnames = list(c("A", "B"), c("C.1", "C.2", "C.3", "C.4")))
v2 <- csimint(estpar=c(100, 6, 5, 2), df=4*n-4, covm=cov2*sigma^2/n, cmatrix=cm2, conf.level=0.95)
summary(v2)

I do not believe that this is the most efficient way for doing this, 
since I made already bad experience with the first example.

My R.version:

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    2.1            
year     2005           
month    12             
day      20             
svn rev  36812          
language R


From tlumley at u.washington.edu  Tue Jul 11 16:56:26 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 11 Jul 2006 07:56:26 -0700 (PDT)
Subject: [R] Proportional Hazard Function and Competing risks
In-Reply-To: <F8FD80E01ED2B04DAA5A7041BCF9683801061043@ing.com>
References: <F8FD80E01ED2B04DAA5A7041BCF9683801061043@ing.com>
Message-ID: <Pine.LNX.4.64.0607110753560.15956@homer23.u.washington.edu>

On Tue, 11 Jul 2006, Sharon.Mazurel at ing.com wrote:
> How can I model coxph() in combination with competing risks
>
> i.e. I have two events and for event the object will leave the data set.
> So :
>
> Coxph(Surv(time,event)~....) the event is for all my objects 1.
>
> How can I model this?

There is nothing built in. Some proposals for competing risks analysis 
involve Cox models and generalized linear models and so can be implemented 
in R.  It depends on what you want to do -- there isn't a standard 
solution because a lot of what people want to do with competing risks is 
provably impossible.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From jdrapp at gmail.com  Tue Jul 11 17:15:36 2006
From: jdrapp at gmail.com (justin rapp)
Date: Tue, 11 Jul 2006 11:15:36 -0400
Subject: [R] Error With TIFF of Plots
Message-ID: <af81db5a0607110815t29bdfab5v5e4bec3e9cf8ed4f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060711/8c311c06/attachment.pl 

From prabhusb at gmail.com  Tue Jul 11 17:18:30 2006
From: prabhusb at gmail.com (prabhu bhaga)
Date: Tue, 11 Jul 2006 10:18:30 -0500
Subject: [R] storing the estimates from lmer
Message-ID: <bb47cf700607110818m782a98c5le08396d2fca203a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060711/44a13d1a/attachment.pl 

From rvaradhan at jhmi.edu  Tue Jul 11 17:21:50 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Tue, 11 Jul 2006 11:21:50 -0400
Subject: [R] Proportional Hazard Function and Competing risks
In-Reply-To: <Pine.LNX.4.64.0607110753560.15956@homer23.u.washington.edu>
Message-ID: <000101c6a4fd$bb52e150$7c94100a@win.ad.jhu.edu>

Hi,

If you are interested in building regression models for the sub-distribution
functions (or cumulative incidence function), then you may want to look at
the "cmprsk" package by Gray.  It is based on the article: Fine and Gray
(JASA 1999).  There is also the approach based on multi-state models (see
the work of the Danish group led by Andersen) for modeling state
probabilities.

If you are interested only in the cause-specific hazard models, then you
don't need anything other than the standard Cox relative risk model or
something like that, where events from other causes are simply treated as
censoring.

If, however, you are interested in the net hazard or net probabilities, then
you enter the dangerous and highly controversial realm of the "classical"
competing risk problem.  You have to overcome a lot of conceptual,
philosophical, and statistical (e.g. identifiability) issues, before
attempting to model it using latent failure time models.  For a cogent
critique of this approach, see Prentice et al. (Biometrics 1978).  Crowder's
(2001) book presents a somewhat more pragmatic view of the classical
competing risks problem.

Hope this helps,
Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
Webpage: http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html 
--------------------------------------------------------------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Thomas Lumley
> Sent: Tuesday, July 11, 2006 10:56 AM
> To: Sharon.Mazurel at ing.com
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Proportional Hazard Function and Competing risks
> 
> On Tue, 11 Jul 2006, Sharon.Mazurel at ing.com wrote:
> > How can I model coxph() in combination with competing risks
> >
> > i.e. I have two events and for event the object will leave the data set.
> > So :
> >
> > Coxph(Surv(time,event)~....) the event is for all my objects 1.
> >
> > How can I model this?
> 
> There is nothing built in. Some proposals for competing risks analysis
> involve Cox models and generalized linear models and so can be implemented
> in R.  It depends on what you want to do -- there isn't a standard
> solution because a lot of what people want to do with competing risks is
> provably impossible.
> 
>  	-thomas
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html


From HDoran at air.org  Tue Jul 11 17:26:13 2006
From: HDoran at air.org (Doran, Harold)
Date: Tue, 11 Jul 2006 11:26:13 -0400
Subject: [R] storing the estimates from lmer
Message-ID: <2323A6D37908A847A7C32F1E3662C80E1328CF@dc1ex01.air.org>

You need the VarCorr function. I think you mean that lmer is in the
Matrix package.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of prabhu bhaga
> Sent: Tuesday, July 11, 2006 11:19 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] storing the estimates from lmer
> 
> Dear all,
> 
> I'm trying to store/extract  the mean& standard error of the 
> fixed effects parameter and the variance of the random 
> effects parameter from "lmer"
> procedure from mlmre4 package developed by bates n pinheiro. 
> while storing fixed effects parameter is straight forward, 
> the same is not true for storing the variance parameter of 
> the random effects. kindly help me
> 
> ~prabhu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


From wwguocn at gmail.com  Tue Jul 11 17:51:25 2006
From: wwguocn at gmail.com (Guo Wei-Wei)
Date: Tue, 11 Jul 2006 23:51:25 +0800
Subject: [R]  Question on partial effect
Message-ID: <d3677d7d0607110851u2b32c4f4pbf8230ef8e0be5f1@mail.gmail.com>

Dear all,

I don't know what's my question is called. I have a performance
variable A, such as sales. And I have another variable B, let's say
establish time of firm. I want to create the third variable that is
sales without the effect of establish time. Maybe it can be called
partial effect problem. I'm not sure.

Does anyone have any suggestion? Thank you in advance!

All the best,
Wei-Wei


From jgarcia at ija.csic.es  Tue Jul 11 17:56:51 2006
From: jgarcia at ija.csic.es (javier garcia-pintado)
Date: Tue, 11 Jul 2006 17:56:51 +0200
Subject: [R] tcltk: help pop-up
Message-ID: <44B3CA43.2090408@ija.csic.es>

Hi all;

Please, does anyone have a piece of R-tcltk code that includes a help
pop-up for any item and could send it as an example?

thanks and best regards,

Javier

-- 
Javier Garc?a-Pintado
Institute of Earth Sciences Jaume Almera (CSIC)
Lluis Sole Sabaris s/n, 08028 Barcelona
Phone: +34 934095410
Fax:   +34 934110012
e-mail:jgarcia at ija.csic.es 


From jgao at rhsmith.umd.edu  Tue Jul 11 18:00:49 2006
From: jgao at rhsmith.umd.edu (Jing Gao)
Date: Tue, 11 Jul 2006 12:00:49 -0400
Subject: [R] DIfferent lengths
Message-ID: <OFBD39C24E.D2E97677-ON852571A8.0057F77F-852571A8.0057F783@rhsmith.umd.edu>


From yschen at jhu.edu  Tue Jul 11 18:09:48 2006
From: yschen at jhu.edu (YIHSU CHEN)
Date: Tue, 11 Jul 2006 12:09:48 -0400
Subject: [R] problem of fixed-formated output using sprintf
Message-ID: <f447806a1634.44b3950c@johnshopkins.edu>

Dear R users:

I'm trying to generate a output file with fixed format using function "sprintf" in R. However, the execution time in R is very long even the toy data (smaller size df) seems to work fine. The syntax that I used is as follows:

df.fmt <- sprintf("%2s%2s%2.4f", df$v1, df$v2, df$v3)
write.table(df.fmt, output.name,...)  

The actual dataset is a df with the dimention of 67944 by 34. I'm wondering whether there is an elegant way of doing it. I would like output in a txt file. 

Many thanks.


Yihsu Chen
The Johns Hopkins University


From pierreclauss at yahoo.fr  Tue Jul 11 18:23:57 2006
From: pierreclauss at yahoo.fr (pierre clauss)
Date: Tue, 11 Jul 2006 16:23:57 +0000 (GMT)
Subject: [R] Date Format
Message-ID: <20060711162357.21469.qmail@web26309.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060711/9982b6c7/attachment.pl 

From gavin.simpson at ucl.ac.uk  Tue Jul 11 18:24:51 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 11 Jul 2006 17:24:51 +0100
Subject: [R] Question on partial effect
In-Reply-To: <d3677d7d0607110851u2b32c4f4pbf8230ef8e0be5f1@mail.gmail.com>
References: <d3677d7d0607110851u2b32c4f4pbf8230ef8e0be5f1@mail.gmail.com>
Message-ID: <1152635092.1197.61.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2006-07-11 at 23:51 +0800, Guo Wei-Wei wrote:
> Dear all,
> 
> I don't know what's my question is called. I have a performance
> variable A, such as sales. And I have another variable B, let's say
> establish time of firm. I want to create the third variable that is
> sales without the effect of establish time. Maybe it can be called
> partial effect problem. I'm not sure.
> 
> Does anyone have any suggestion? Thank you in advance!
> 
> All the best,
> Wei-Wei

Do you mean?

## dummy data
A <- rnorm(100)
B <- rnorm(100)
C <- resid(lm(A ~ B))

C now contains the residual variation in A after fitting B.

e.g. with some real data
?cars
data(cars) # not sure this is needed now, I forget
mod <- lm(dist ~ speed, data  = cars)
summary(mod)
partial <- resid(mod)

## check
mod2 <- lm(dist ~ partial, data = cars)
summary(mod2)
## from the two R^2 form mod1 and mod2 - partial contains dist minus
## the effects of speed
> 0.6511 + 0.3489
[1] 1

HTH

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
 London, UK. WC1E 6BT.         [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From wwguocn at gmail.com  Tue Jul 11 18:51:17 2006
From: wwguocn at gmail.com (Guo Wei-Wei)
Date: Wed, 12 Jul 2006 00:51:17 +0800
Subject: [R] Question on partial effect
In-Reply-To: <1152635092.1197.61.camel@gsimpson.geog.ucl.ac.uk>
References: <d3677d7d0607110851u2b32c4f4pbf8230ef8e0be5f1@mail.gmail.com>
	<1152635092.1197.61.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <d3677d7d0607110951r66351a1aga268d3b15a2f3e77@mail.gmail.com>

Thank you, Gavin. I think that might be what I need. But I'm a little
bit wandering what's the scale of resid(mod). Is it
scale(dist)/scale(speed), for example kilometer / (kilometer per
hour)? or something else?

Thank you very much!
Wei-Wei


2006/7/12, Gavin Simpson <gavin.simpson at ucl.ac.uk>:
> On Tue, 2006-07-11 at 23:51 +0800, Guo Wei-Wei wrote:
> > Dear all,
> >
> > I don't know what's my question is called. I have a performance
> > variable A, such as sales. And I have another variable B, let's say
> > establish time of firm. I want to create the third variable that is
> > sales without the effect of establish time. Maybe it can be called
> > partial effect problem. I'm not sure.
> >
> > Does anyone have any suggestion? Thank you in advance!
> >
> > All the best,
> > Wei-Wei
>
> Do you mean?
>
> ## dummy data
> A <- rnorm(100)
> B <- rnorm(100)
> C <- resid(lm(A ~ B))
>
> C now contains the residual variation in A after fitting B.
>
> e.g. with some real data
> ?cars
> data(cars) # not sure this is needed now, I forget
> mod <- lm(dist ~ speed, data  = cars)
> summary(mod)
> partial <- resid(mod)
>
> ## check
> mod2 <- lm(dist ~ partial, data = cars)
> summary(mod2)
> ## from the two R^2 form mod1 and mod2 - partial contains dist minus
> ## the effects of speed
> > 0.6511 + 0.3489
> [1] 1
>
> HTH
>
> G
> --
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>  Gavin Simpson                 [t] +44 (0)20 7679 0522
>  ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
>  Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
>  Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
>  London, UK. WC1E 6BT.         [w] http://www.ucl.ac.uk/~ucfagls/
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>
>


From ggrothendieck at gmail.com  Tue Jul 11 18:58:39 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 11 Jul 2006 12:58:39 -0400
Subject: [R] Date Format
In-Reply-To: <20060711162357.21469.qmail@web26309.mail.ukl.yahoo.com>
References: <20060711162357.21469.qmail@web26309.mail.ukl.yahoo.com>
Message-ID: <971536df0607110958x3c9596d7w19b2efb06cf25c26@mail.gmail.com>

Try this:

library(zoo)
as.Date(11328)

See the Help Desk article in R News 4/1 for more on dates.


On 7/11/06, pierre clauss <pierreclauss at yahoo.fr> wrote:
> Hi everybody,
> I need your precious help for, I think, a simple request, but I do not manage to solve this.
>
> When I use a "table" function with dates in the rows, the rows are coerced to number after the table function.
>
> So I need to transform the row names into date format. But I do not manage.
>
> Therefore, for an example, I manage to write this :
>
> datetest<-"06/01/2001"
> datetest<-as.Date(datetest,"%d/%m/%Y")
> datetest<-as.numeric(datetest)
>
> to get 11328.
>
> But I do not obtain the inverse tranformation :
>
> datetest<-as.Date(datetest,"%d/%m/%Y")
>
> How do we get this please ?
>
> Thanks a lot for your solution.
> Pierre.
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From bates at stat.wisc.edu  Tue Jul 11 19:11:03 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 11 Jul 2006 12:11:03 -0500
Subject: [R] storing the estimates from lmer
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E1328CF@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E1328CF@dc1ex01.air.org>
Message-ID: <40e66e0b0607111011i40758362me84aa2202100123e@mail.gmail.com>

On 7/11/06, Doran, Harold <HDoran at air.org> wrote:
> You need the VarCorr function. I think you mean that lmer is in the
> Matrix package.

Currently lmer is in the Matrix package.  The plan is to move it back
to the lme4 package when interpackage linking has been added to R -
perhaps as early as the release of R-2.4.0.  Because the lme4 package
automatically loads the Matrix package it is safest to act as if lmer
actually resided in the lme4 package and use

library(lme4)
fm1 <- lmer(...


>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of prabhu bhaga
> > Sent: Tuesday, July 11, 2006 11:19 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] storing the estimates from lmer
> >
> > Dear all,
> >
> > I'm trying to store/extract  the mean& standard error of the
> > fixed effects parameter and the variance of the random
> > effects parameter from "lmer"
> > procedure from mlmre4 package developed by bates n pinheiro.
> > while storing fixed effects parameter is straight forward,
> > the same is not true for storing the variance parameter of
> > the random effects. kindly help me
> >
> > ~prabhu
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jholtman at gmail.com  Tue Jul 11 19:30:28 2006
From: jholtman at gmail.com (jim holtman)
Date: Tue, 11 Jul 2006 13:30:28 -0400
Subject: [R] problem of fixed-formated output using sprintf
In-Reply-To: <f447806a1634.44b3950c@johnshopkins.edu>
References: <f447806a1634.44b3950c@johnshopkins.edu>
Message-ID: <644e1f320607111030m491d7f96h88f47345198e8650@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060711/30d7e712/attachment.pl 

From rdpeng at gmail.com  Tue Jul 11 19:39:50 2006
From: rdpeng at gmail.com (Roger D. Peng)
Date: Tue, 11 Jul 2006 13:39:50 -0400
Subject: [R] Date Format
In-Reply-To: <20060711162357.21469.qmail@web26309.mail.ukl.yahoo.com>
References: <20060711162357.21469.qmail@web26309.mail.ukl.yahoo.com>
Message-ID: <44B3E266.7050406@gmail.com>

Try

structure(11328, class = "Date")

or just

class(datetest) <- "Date"

-roger

pierre clauss wrote:
> Hi everybody,
> I need your precious help for, I think, a simple request, but I do not manage to solve this.
>  
> When I use a "table" function with dates in the rows, the rows are coerced to number after the table function.
>  
> So I need to transform the row names into date format. But I do not manage.
>  
> Therefore, for an example, I manage to write this :
>  
> datetest<-"06/01/2001"
> datetest<-as.Date(datetest,"%d/%m/%Y")
> datetest<-as.numeric(datetest)
>  
> to get 11328.
>  
> But I do not obtain the inverse tranformation :
>  
> datetest<-as.Date(datetest,"%d/%m/%Y")
>  
> How do we get this please ?
>  
> Thanks a lot for your solution.
> Pierre.
>  
>  
>  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From lalithaviswanath at yahoo.com  Tue Jul 11 19:41:08 2006
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Tue, 11 Jul 2006 10:41:08 -0700 (PDT)
Subject: [R] Query about getting averages across a certain parameter in a
	table
Message-ID: <20060711174108.18619.qmail@web34107.mail.mud.yahoo.com>

Hi
I have a table that goes 
data

cluster_ac  clockrate age class
7337         0.9       0.001  alpha_proteins
7888         0.1       0.78   beta proteins

etc

The class column can have 7-8 different unique values
While the clockrate and age columns are floats varying
from 0 to 1.

I wish to get the average clockrate across each of the
classes for this data.

I would appreciate your help regarding the aboe.

Thanks
Lalitha


From gavin.simpson at ucl.ac.uk  Tue Jul 11 19:47:17 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 11 Jul 2006 18:47:17 +0100
Subject: [R] Question on partial effect
In-Reply-To: <d3677d7d0607110951r66351a1aga268d3b15a2f3e77@mail.gmail.com>
References: <d3677d7d0607110851u2b32c4f4pbf8230ef8e0be5f1@mail.gmail.com>
	<1152635092.1197.61.camel@gsimpson.geog.ucl.ac.uk>
	<d3677d7d0607110951r66351a1aga268d3b15a2f3e77@mail.gmail.com>
Message-ID: <1152640037.1197.90.camel@gsimpson.geog.ucl.ac.uk>

On Wed, 2006-07-12 at 00:51 +0800, Guo Wei-Wei wrote:
> Thank you, Gavin. I think that might be what I need. But I'm a little
> bit wandering what's the scale of resid(mod). Is it
> scale(dist)/scale(speed), for example kilometer / (kilometer per
> hour)? or something else?
> 
> Thank you very much!
> Wei-Wei

The scale of dist - they are just the differences between observed dist
and fitted dist (based on speed). 

mod <- lm(dist ~ speed, data = cars)
resid(mod)

         1          2          3          4          5
  3.849460  11.849460  -5.947766  12.052234   2.119825
         6          7          8          9         10
 -7.812584  -3.744993   4.255007  12.255007  -8.677401
....

# visualise the residuals
plot(resid(mod) ~ dist, data = cars)
abline(h = 0, col = "grey")
## length of blue line represents the residual
lines(cars$dist, resid(mod), type = "h", col = "blue")

So you see that for the 1st residual it is 3.849 ft (the distances are
measured in feet, see ?cars)

Does this help?

G

> 
> 
> 2006/7/12, Gavin Simpson <gavin.simpson at ucl.ac.uk>:
> > On Tue, 2006-07-11 at 23:51 +0800, Guo Wei-Wei wrote:
> > > Dear all,
> > >
> > > I don't know what's my question is called. I have a performance
> > > variable A, such as sales. And I have another variable B, let's say
> > > establish time of firm. I want to create the third variable that is
> > > sales without the effect of establish time. Maybe it can be called
> > > partial effect problem. I'm not sure.
> > >
> > > Does anyone have any suggestion? Thank you in advance!
> > >
> > > All the best,
> > > Wei-Wei
> >
> > Do you mean?
> >
> > ## dummy data
> > A <- rnorm(100)
> > B <- rnorm(100)
> > C <- resid(lm(A ~ B))
> >
> > C now contains the residual variation in A after fitting B.
> >
> > e.g. with some real data
> > ?cars
> > data(cars) # not sure this is needed now, I forget
> > mod <- lm(dist ~ speed, data  = cars)
> > summary(mod)
> > partial <- resid(mod)
> >
> > ## check
> > mod2 <- lm(dist ~ partial, data = cars)
> > summary(mod2)
> > ## from the two R^2 form mod1 and mod2 - partial contains dist minus
> > ## the effects of speed
> > > 0.6511 + 0.3489
> > [1] 1
> >
> > HTH
> >
> > G
> > --
> > %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> >  Gavin Simpson                 [t] +44 (0)20 7679 0522
> >  ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
> >  Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
> >  Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
> >  London, UK. WC1E 6BT.         [w] http://www.ucl.ac.uk/~ucfagls/
> > %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
 London, UK. WC1E 6BT.         [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From ggrothendieck at gmail.com  Tue Jul 11 20:04:32 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 11 Jul 2006 14:04:32 -0400
Subject: [R] detach
Message-ID: <971536df0607111104j5cf964e9oa39d29d8e6ff8a3e@mail.gmail.com>

We try the following:

search()
as.Date(1)
zoo:::as.Date.numeric

under three circumstances:

1. on a fresh session
2. after issuing library(zoo) noting that as.Date.numeric is provided by zoo
3. after detaching zoo

as.Date(1) fails on #1 but succeeds in #2 and #3.  Should it not fail in #3
since zoo was detached?  Is this how its supposed to work?

(Note that entering zoo:::as.Date.numeric at the console
succeeds in displaying the function in all three cases.)

Here is the console session:

> R.version.string # XP
[1] "Version 2.3.1 Patched (2006-06-04 r38279)"
>
> search()
[1] ".GlobalEnv"        "package:methods"   "package:stats"
[4] "package:graphics"  "package:grDevices" "package:utils"
[7] "package:datasets"  "Autoloads"         "package:base"
> as.Date(1)
Error in as.Date.default(1) : do not know how to convert '1' to class "Date"
> zoo:::as.Date.numeric
function (x, ...)
structure(floor(x + 0.001), class = "Date")
<environment: namespace:zoo>
>
> library(zoo)
> search()
 [1] ".GlobalEnv"        "package:zoo"       "package:methods"
 [4] "package:stats"     "package:graphics"  "package:grDevices"
 [7] "package:utils"     "package:datasets"  "Autoloads"
[10] "package:base"
> as.Date(1)
[1] "1970-01-02"
> zoo:::as.Date.numeric
function (x, ...)
structure(floor(x + 0.001), class = "Date")
<environment: namespace:zoo>
>
> detach()
> search()
[1] ".GlobalEnv"        "package:methods"   "package:stats"
[4] "package:graphics"  "package:grDevices" "package:utils"
[7] "package:datasets"  "Autoloads"         "package:base"
> as.Date(1)
[1] "1970-01-02"
> zoo:::as.Date.numeric
function (x, ...)
structure(floor(x + 0.001), class = "Date")
<environment: namespace:zoo>
>


From jtokle at math.washington.edu  Tue Jul 11 20:51:01 2006
From: jtokle at math.washington.edu (Joshua Tokle)
Date: Tue, 11 Jul 2006 11:51:01 -0700 (PDT)
Subject: [R] R newbie: logical subsets
Message-ID: <Pine.LNX.4.64.0607111124540.27860@zeno1.math.washington.edu>

Hello!  I'm a newcomer to R hoping to replace some convoluted database 
code with an R script.  Unfortunately, I haven't been able to figure out 
how to implement the following logic.

Essentially, we have a database of transactions that are coded with a 
geographic locale and a type.  These are being loaded into a data.frame 
with named variables city, type, and price.  E.g., trans$city and all 
that.

We want to calculate mean prices by city and type, AFTER excluding 
outliers.  That is, we want to calculate the mean price in 3 steps:

1. calculate a mean and standard deviation by city and type over all 
transactions
2. create a subset of the original data frame, excluding transactions that 
differ from the relevant mean by more than 2 standard deviations
3. calculate a final mean by city and type based on this subset.

I'm stuck on step 2.  I would like to do something like the following:

fs <- list(factor(trans$city), factor(trans$type))
means <- tapply(trans$price, fs, mean)
stdevs <- tapply(trans$price, fs, sd)

filter <- abs(trans$price - means[trans$city, trans$type]) <
             2*stdevs[trans$city, trans$type]

sub <- subset(trans, filter)

The above code doesn't work.  What's the correct way to do this?

Thanks,
Josh


From ggrothendieck at gmail.com  Tue Jul 11 21:06:23 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 11 Jul 2006 15:06:23 -0400
Subject: [R] R newbie: logical subsets
In-Reply-To: <Pine.LNX.4.64.0607111124540.27860@zeno1.math.washington.edu>
References: <Pine.LNX.4.64.0607111124540.27860@zeno1.math.washington.edu>
Message-ID: <971536df0607111206r7346865ie313d0e6f7374059@mail.gmail.com>

Try this, using the built in anscombe data set:

anscombe[!rowSums(abs(scale(anscombe)) > 2),]



On 7/11/06, Joshua Tokle <jtokle at math.washington.edu> wrote:
> Hello!  I'm a newcomer to R hoping to replace some convoluted database
> code with an R script.  Unfortunately, I haven't been able to figure out
> how to implement the following logic.
>
> Essentially, we have a database of transactions that are coded with a
> geographic locale and a type.  These are being loaded into a data.frame
> with named variables city, type, and price.  E.g., trans$city and all
> that.
>
> We want to calculate mean prices by city and type, AFTER excluding
> outliers.  That is, we want to calculate the mean price in 3 steps:
>
> 1. calculate a mean and standard deviation by city and type over all
> transactions
> 2. create a subset of the original data frame, excluding transactions that
> differ from the relevant mean by more than 2 standard deviations
> 3. calculate a final mean by city and type based on this subset.
>
> I'm stuck on step 2.  I would like to do something like the following:
>
> fs <- list(factor(trans$city), factor(trans$type))
> means <- tapply(trans$price, fs, mean)
> stdevs <- tapply(trans$price, fs, sd)
>
> filter <- abs(trans$price - means[trans$city, trans$type]) <
>             2*stdevs[trans$city, trans$type]
>
> sub <- subset(trans, filter)
>
> The above code doesn't work.  What's the correct way to do this?
>
> Thanks,
> Josh
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From rogeriorosas at gmail.com  Tue Jul 11 21:11:53 2006
From: rogeriorosas at gmail.com (=?ISO-8859-1?Q?Rog=E9rio_Rosa_da_Silva?=)
Date: Tue, 11 Jul 2006 16:11:53 -0300
Subject: [R] script problem to obtain pairs of overlap values
Message-ID: <44B3F7F9.3020009@gmail.com>

Dear,

I wrote a code to estimate the overlap between two kernel distributions.
The script must estimates the overlap among each columns of data frame.
With S sampled species (columns) in my data frame, I want obtain
S(S-1)/2 pairs of overlap values between species.
However, the code is not well write at all (only an overlap value is
produced) and I can't find the solution.

To illustrate the calculations, I use the data frame "tdon" and the
value of the bandwidth "h", which was estimated in other part of script.

tdon <- data.frame (sp.1=c (5 ,9 ,NA ,5, 11) , sp.2=c (4, 2, 4, NA, 11,
),sp.3=c(5, 4, 2, 6, 13), sp.4=c(3 , 11, NA, 5, 3), sp.5=c(2 ,5 ,2, 9, 9))

> h

[1] 1.047 2.973 0.887 1.520 2.955



Here is the code:

for (i in 1:(nbcol-1)) # nbcol<-ncol(tdon)
    {tdon1<-tdon[,i]
    tdon11<- subset(tdon1,tdon1!="NA")
    fctk1<-function(x)
    {density (tdon11, bw=h[i], kernel="gaussian")$y}

for (j in (i+1):nbcol)
    {tdon2<-tdon[,j]
    tdon21<- subset(tdon2,tdon2!="NA")
    fctk2<-function(x)
    {density (tdon21, bw=h[j], kernel="gaussian")$y}

        diffctk<-function(x)
        {abs(fctk1(x)-fctk2(x))}

        intctk<- approxfun (diffctk(x), rule=2)
        int<- integrate(diffctk,-Inf,Inf)$value
             overlap<- 1 - 0.5* int
                }
                }

   
The use of "approxfun" to integrate the difference in the estimated
density values (my "diffctk" function) was suggested by Thomas Lumley,
but I'm not sure that I have found the solution or if this solution is correct for my problem.

I need that the "overlap" produce a vector with the length equal to 10, with
all pairs of overlap values.

Any help or advice on improvement for this code will be appreciated.

With kind regards,
   
    Rog?rio


From sell_mirage_ne at hotmail.com  Tue Jul 11 21:25:30 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Tue, 11 Jul 2006 14:25:30 -0500
Subject: [R] (no subject)
Message-ID: <BAY110-F3107FDA20019A01550748BC7680@phx.gbl>

Dear R-users

>prob <- c(0.5,0.4,0.3,0.1,0.0)
>cal <- prob * log(prob,base=2)
>cal
[1] -0.5000000 -0.5287712 -0.5210897 -0.3321928        NaN

Is there any way to change NaN to zero ?

Thank you

Taka


From BEN at SSANET.COM  Tue Jul 11 21:47:49 2006
From: BEN at SSANET.COM (Ben Fairbank)
Date: Tue, 11 Jul 2006 14:47:49 -0500
Subject: [R] (no subject)
Message-ID: <CA612484A337C6479EA341DF9EEE14AC051BFA35@hercules.ssainfo>

Well, it's a little inelegant, but since the amount of information is
the same in events that are certain to occur and certain not to occur,
you could add the line

prob[prob==0] <- 1

after you set up the prob vector, which would take care of the problem.


Ben Fairbank



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Taka Matzmoto
Sent: Tuesday, July 11, 2006 2:26 PM
To: r-help at stat.math.ethz.ch
Subject: [R] (no subject)

Dear R-users

>prob <- c(0.5,0.4,0.3,0.1,0.0)
>cal <- prob * log(prob,base=2)
>cal
[1] -0.5000000 -0.5287712 -0.5210897 -0.3321928        NaN

Is there any way to change NaN to zero ?

Thank you

Taka

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From elvis at xlsolutions-corp.com  Tue Jul 11 22:08:12 2006
From: elvis at xlsolutions-corp.com (elvis at xlsolutions-corp.com)
Date: Tue, 11 Jul 2006 13:08:12 -0700
Subject: [R] August R/Splus course @ 5 locations *** R/Splus Fundamentals
	and Programming Techniques
Message-ID: <20060711130811.9f08cc34deb45d78e54b3b5664e21546.832210808c.wbe@email.secureserver.net>


From sell_mirage_ne at hotmail.com  Tue Jul 11 22:21:31 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Tue, 11 Jul 2006 15:21:31 -0500
Subject: [R] 0* log(0)  should be zero but NaN
Message-ID: <BAY110-F8ECC2D2186B9DB48D8998C7680@phx.gbl>

Dear R-users

>prob <- c(0.5,0.4,0.3,0.1,0.0)
>cal <- prob * log(prob,base=2)
>cal
[1] -0.5000000 -0.5287712 -0.5210897 -0.3321928        NaN

Is there any way to change NaN to zero ?

I did come up with this by applying Ripley's relpy to my previous question

cal <-prob*log(pmax(prob,0.00000001),base=2)

Any suggestion ?

Thank you

Taka


From beperron at wustl.edu  Tue Jul 11 22:58:48 2006
From: beperron at wustl.edu (Brian Perron)
Date: Tue, 11 Jul 2006 15:58:48 -0500
Subject: [R] generating clustered data
Message-ID: <44B41108.9090702@wustl.edu>

Hello R folks,

Does anybody have code to share for generating (via simulation) 
clustered data?  The type of data I am looking to simulate would allow 
fitting of a multilevel model with random intercepts.   I looked at the 
mvtnorm package but am not quite sure how to create clusters.  (Can this 
be done by simply changing the seed?)  If somebody could point me where 
to look for the relevant code or perhaps send some sample code, that 
would be great.

Thanks,
Brian


From p_connolly at ihug.co.nz  Wed Jul 12 00:27:01 2006
From: p_connolly at ihug.co.nz (Patrick Connolly)
Date: Wed, 12 Jul 2006 10:27:01 +1200
Subject: [R] use of NULL environment is deprecated?
Message-ID: <20060711222701.GD12850@ihug.co.nz>

]> version
               _                         
platform       x86_64-unknown-linux-gnu  
arch           x86_64                    
os             linux-gnu                 
system         x86_64, linux-gnu         
status                                   
major          2                         
minor          3.1                       
year           2006                      
month          06                        
day            01                        
svn rev        38247                     
language       R                         
version.string Version 2.3.1 (2006-06-01)


I see in the NEWS file the line:
    o	Use of NULL as an environment is deprecated and gives a warning.

Which duly happens.  I get warnings like this:
> Warning message:
use of NULL environment is deprecated

My problem is that I don't know what is being referred to.  A little
birdie tells me that in later versions of R, those warnings will
become errors so I need to work out where they're coming from before I
can use later versions.

My question is: How does one work out which is being referred to by
such a message?  The traceback() function is useful when failure
occurs.  Is there an analagous way of looking into warnings?

TIA

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From murdoch at stats.uwo.ca  Wed Jul 12 00:41:11 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 11 Jul 2006 18:41:11 -0400
Subject: [R] use of NULL environment is deprecated?
In-Reply-To: <20060711222701.GD12850@ihug.co.nz>
References: <20060711222701.GD12850@ihug.co.nz>
Message-ID: <44B42907.6030404@stats.uwo.ca>

On 7/11/2006 6:27 PM, Patrick Connolly wrote:
> ]> version
>                _                         
> platform       x86_64-unknown-linux-gnu  
> arch           x86_64                    
> os             linux-gnu                 
> system         x86_64, linux-gnu         
> status                                   
> major          2                         
> minor          3.1                       
> year           2006                      
> month          06                        
> day            01                        
> svn rev        38247                     
> language       R                         
> version.string Version 2.3.1 (2006-06-01)
> 
> 
> I see in the NEWS file the line:
>     o	Use of NULL as an environment is deprecated and gives a warning.
> 
> Which duly happens.  I get warnings like this:
>> Warning message:
> use of NULL environment is deprecated
> 
> My problem is that I don't know what is being referred to.  A little
> birdie tells me that in later versions of R, those warnings will
> become errors so I need to work out where they're coming from before I
> can use later versions.
> 
> My question is: How does one work out which is being referred to by
> such a message?  The traceback() function is useful when failure
> occurs.  Is there an analagous way of looking into warnings?

options(warn=2) will convert warnings into errors, so traceback will work.

A common situation where I've seen that error is with binary saves from 
earlier versions of R being loaded into current versions.  For example, 
if you installed a package before, but didn't re-install it with 2.3.1, 
or if you are reloading a workspace saved in an earlier version.

Duncan Murdoch


From sundar.dorai-raj at pdf.com  Wed Jul 12 00:43:01 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 11 Jul 2006 15:43:01 -0700
Subject: [R] use of NULL environment is deprecated?
In-Reply-To: <20060711222701.GD12850@ihug.co.nz>
References: <20060711222701.GD12850@ihug.co.nz>
Message-ID: <44B42975.6020501@pdf.com>


Patrick Connolly wrote:
> ]> version
>                _                         
> platform       x86_64-unknown-linux-gnu  
> arch           x86_64                    
> os             linux-gnu                 
> system         x86_64, linux-gnu         
> status                                   
> major          2                         
> minor          3.1                       
> year           2006                      
> month          06                        
> day            01                        
> svn rev        38247                     
> language       R                         
> version.string Version 2.3.1 (2006-06-01)
> 
> 
> I see in the NEWS file the line:
>     o	Use of NULL as an environment is deprecated and gives a warning.
> 
> Which duly happens.  I get warnings like this:
> 
>>Warning message:
> 
> use of NULL environment is deprecated
> 
> My problem is that I don't know what is being referred to.  A little
> birdie tells me that in later versions of R, those warnings will
> become errors so I need to work out where they're coming from before I
> can use later versions.
> 
> My question is: How does one work out which is being referred to by
> such a message?  The traceback() function is useful when failure
> occurs.  Is there an analagous way of looking into warnings?
> 
> TIA
> 

Hi, Patrick,

This will happen when you load a package that was created for an earlier 
version of R. If this is your own package, recreate the package using 
R-2.3.1. If it's a package on CRAN, then update.packages() should also 
do the trick.

HTH,

--sundar


From jholtman at gmail.com  Wed Jul 12 00:58:19 2006
From: jholtman at gmail.com (jim holtman)
Date: Tue, 11 Jul 2006 18:58:19 -0400
Subject: [R] 0* log(0) should be zero but NaN
In-Reply-To: <BAY110-F8ECC2D2186B9DB48D8998C7680@phx.gbl>
References: <BAY110-F8ECC2D2186B9DB48D8998C7680@phx.gbl>
Message-ID: <644e1f320607111558g3652baafl67e4608eeee197e5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060711/ce0edd24/attachment.pl 

From p_connolly at ihug.co.nz  Wed Jul 12 00:59:11 2006
From: p_connolly at ihug.co.nz (Patrick Connolly)
Date: Wed, 12 Jul 2006 10:59:11 +1200
Subject: [R] use of NULL environment is deprecated?
In-Reply-To: <44B42907.6030404@stats.uwo.ca>
References: <20060711222701.GD12850@ihug.co.nz> <44B42907.6030404@stats.uwo.ca>
Message-ID: <20060711225911.GE12850@ihug.co.nz>

On Tue, 11-Jul-2006 at 06:41PM -0400, Duncan Murdoch wrote:

[....]

|> 
|> options(warn=2) will convert warnings into errors, so traceback will work.
|> 
|> A common situation where I've seen that error is with binary saves from 
|> earlier versions of R being loaded into current versions.  For example, 
|> if you installed a package before, but didn't re-install it with 2.3.1, 
|> or if you are reloading a workspace saved in an earlier version.

Thank you for the explanation.  In this case, it's the latter reason:
the workspace comes from a different computer as well as a different
version of R.

Will saving the objects anew overcome the problem?

(Thanks Sundar also.)

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From jholtman at gmail.com  Wed Jul 12 01:17:54 2006
From: jholtman at gmail.com (jim holtman)
Date: Tue, 11 Jul 2006 19:17:54 -0400
Subject: [R] Query about getting averages across a certain parameter in
	a table
In-Reply-To: <20060711174108.18619.qmail@web34107.mail.mud.yahoo.com>
References: <20060711174108.18619.qmail@web34107.mail.mud.yahoo.com>
Message-ID: <644e1f320607111617k520bc821m8c1ee656461bb5a9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060711/68a2b849/attachment.pl 

From CodyH at BaylorHealth.edu  Wed Jul 12 01:35:13 2006
From: CodyH at BaylorHealth.edu (Hamilton, Cody)
Date: Tue, 11 Jul 2006 18:35:13 -0500
Subject: [R] generating clustered data
In-Reply-To: <44B41108.9090702@wustl.edu>
Message-ID: <E52E20F6B4A2F548B16BB259DD5168CF02FB6840@BHDAEXCH11.bhcs.pvt>


Brian,

What about creating the covariance matrix with the help of the kronecker
product?  For instance, suppose your intercepts are ~ N(0,var1) and your
residual errors are ~ N(0,var2).  Suppose further that you want 10
clusters of 5 observations each.  I believe you can create the overall
covariance matrix with kronecker(diag(10),matrix(var1,5,5)) +
var2*diag(50).  This can then be fed as the variance to the mvtnorm
function.  Hope this helps.

Regards,
   -Cody

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Brian Perron
Sent: Tuesday, July 11, 2006 15:59 PM
To: r-help at stat.math.ethz.ch
Subject: [R] generating clustered data

Hello R folks,

Does anybody have code to share for generating (via simulation)
clustered data?  The type of data I am looking to simulate would allow
fitting of a multilevel model with random intercepts.   I looked at the
mvtnorm package but am not quite sure how to create clusters.  (Can this

be done by simply changing the seed?)  If somebody could point me where
to look for the relevant code or perhaps send some sample code, that
would be great.

Thanks,
Brian

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

This e-mail, facsimile, or letter and any files or attachmen...{{dropped}}


From andy_liaw at merck.com  Wed Jul 12 02:08:55 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 11 Jul 2006 20:08:55 -0400
Subject: [R] 0* log(0)  should be zero but NaN  [Broadcast]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02884E50@usctmx1106.merck.com>

Try:

R> (cal <- prob * log(ifelse(prob == 0, 1, prob), base=2))
[1] -0.5000000 -0.5287712 -0.5210897 -0.3321928  0.0000000

Andy 

From: Taka Matzmoto
> 
> Dear R-users
> 
> >prob <- c(0.5,0.4,0.3,0.1,0.0)
> >cal <- prob * log(prob,base=2)
> >cal
> [1] -0.5000000 -0.5287712 -0.5210897 -0.3321928        NaN
> 
> Is there any way to change NaN to zero ?
> 
> I did come up with this by applying Ripley's relpy to my 
> previous question
> 
> cal <-prob*log(pmax(prob,0.00000001),base=2)
> 
> Any suggestion ?
> 
> Thank you
> 
> Taka
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From murdoch at stats.uwo.ca  Wed Jul 12 02:16:18 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 11 Jul 2006 20:16:18 -0400
Subject: [R] use of NULL environment is deprecated?
In-Reply-To: <20060711225911.GE12850@ihug.co.nz>
References: <20060711222701.GD12850@ihug.co.nz> <44B42907.6030404@stats.uwo.ca>
	<20060711225911.GE12850@ihug.co.nz>
Message-ID: <44B43F52.4020907@stats.uwo.ca>

On 7/11/2006 6:59 PM, Patrick Connolly wrote:
> On Tue, 11-Jul-2006 at 06:41PM -0400, Duncan Murdoch wrote:
> 
> [....]
> 
> |> 
> |> options(warn=2) will convert warnings into errors, so traceback will work.
> |> 
> |> A common situation where I've seen that error is with binary saves from 
> |> earlier versions of R being loaded into current versions.  For example, 
> |> if you installed a package before, but didn't re-install it with 2.3.1, 
> |> or if you are reloading a workspace saved in an earlier version.
> 
> Thank you for the explanation.  In this case, it's the latter reason:
> the workspace comes from a different computer as well as a different
> version of R.
> 
> Will saving the objects anew overcome the problem?

Yes, it should, though there's a slight risk that someone might have 
meant NULL to mean an empty environment instead of the base environment. 
  This would be a bug with or without the change.

Duncan Murdoch


From wwguocn at gmail.com  Wed Jul 12 03:37:06 2006
From: wwguocn at gmail.com (Guo Wei-Wei)
Date: Wed, 12 Jul 2006 09:37:06 +0800
Subject: [R] Question on partial effect
In-Reply-To: <1152640037.1197.90.camel@gsimpson.geog.ucl.ac.uk>
References: <d3677d7d0607110851u2b32c4f4pbf8230ef8e0be5f1@mail.gmail.com>
	<1152635092.1197.61.camel@gsimpson.geog.ucl.ac.uk>
	<d3677d7d0607110951r66351a1aga268d3b15a2f3e77@mail.gmail.com>
	<1152640037.1197.90.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <d3677d7d0607111837h11694708tb677ac2f2b018dfd@mail.gmail.com>

Than you, Gavin. You helped me out a lot of problems.

Thank you very much!
Wei-Wei




2006/7/12, Gavin Simpson <gavin.simpson at ucl.ac.uk>:
> On Wed, 2006-07-12 at 00:51 +0800, Guo Wei-Wei wrote:
> > Thank you, Gavin. I think that might be what I need. But I'm a little
> > bit wandering what's the scale of resid(mod). Is it
> > scale(dist)/scale(speed), for example kilometer / (kilometer per
> > hour)? or something else?
> >
> > Thank you very much!
> > Wei-Wei
>
> The scale of dist - they are just the differences between observed dist
> and fitted dist (based on speed).
>
> mod <- lm(dist ~ speed, data = cars)
> resid(mod)
>
>          1          2          3          4          5
>   3.849460  11.849460  -5.947766  12.052234   2.119825
>          6          7          8          9         10
>  -7.812584  -3.744993   4.255007  12.255007  -8.677401
> ....
>
> # visualise the residuals
> plot(resid(mod) ~ dist, data = cars)
> abline(h = 0, col = "grey")
> ## length of blue line represents the residual
> lines(cars$dist, resid(mod), type = "h", col = "blue")
>
> So you see that for the 1st residual it is 3.849 ft (the distances are
> measured in feet, see ?cars)
>
> Does this help?
>
> G
>
> >
> >
> > 2006/7/12, Gavin Simpson <gavin.simpson at ucl.ac.uk>:
> > > On Tue, 2006-07-11 at 23:51 +0800, Guo Wei-Wei wrote:
> > > > Dear all,
> > > >
> > > > I don't know what's my question is called. I have a performance
> > > > variable A, such as sales. And I have another variable B, let's say
> > > > establish time of firm. I want to create the third variable that is
> > > > sales without the effect of establish time. Maybe it can be called
> > > > partial effect problem. I'm not sure.
> > > >
> > > > Does anyone have any suggestion? Thank you in advance!
> > > >
> > > > All the best,
> > > > Wei-Wei
> > >
> > > Do you mean?
> > >
> > > ## dummy data
> > > A <- rnorm(100)
> > > B <- rnorm(100)
> > > C <- resid(lm(A ~ B))
> > >
> > > C now contains the residual variation in A after fitting B.
> > >
> > > e.g. with some real data
> > > ?cars
> > > data(cars) # not sure this is needed now, I forget
> > > mod <- lm(dist ~ speed, data  = cars)
> > > summary(mod)
> > > partial <- resid(mod)
> > >
> > > ## check
> > > mod2 <- lm(dist ~ partial, data = cars)
> > > summary(mod2)
> > > ## from the two R^2 form mod1 and mod2 - partial contains dist minus
> > > ## the effects of speed
> > > > 0.6511 + 0.3489
> > > [1] 1
> > >
> > > HTH
> > >
> > > G
> > > --
> > > %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> > >  Gavin Simpson                 [t] +44 (0)20 7679 0522
> > >  ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
> > >  Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
> > >  Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
> > >  London, UK. WC1E 6BT.         [w] http://www.ucl.ac.uk/~ucfagls/
> > > %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> > >
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> --
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>  Gavin Simpson                 [t] +44 (0)20 7679 0522
>  ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
>  Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
>  Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
>  London, UK. WC1E 6BT.         [w] http://www.ucl.ac.uk/~ucfagls/
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>
>


From ggrothendieck at gmail.com  Wed Jul 12 03:52:49 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 11 Jul 2006 21:52:49 -0400
Subject: [R] 0* log(0) should be zero but NaN
In-Reply-To: <BAY110-F8ECC2D2186B9DB48D8998C7680@phx.gbl>
References: <BAY110-F8ECC2D2186B9DB48D8998C7680@phx.gbl>
Message-ID: <971536df0607111852p7c1700dco11c3462da33aee59@mail.gmail.com>

Try this:

p <- 0:10/10
p * log2(p + !p)

On 7/11/06, Taka Matzmoto <sell_mirage_ne at hotmail.com> wrote:
> Dear R-users
>
> >prob <- c(0.5,0.4,0.3,0.1,0.0)
> >cal <- prob * log(prob,base=2)
> >cal
> [1] -0.5000000 -0.5287712 -0.5210897 -0.3321928        NaN
>
> Is there any way to change NaN to zero ?
>
> I did come up with this by applying Ripley's relpy to my previous question
>
> cal <-prob*log(pmax(prob,0.00000001),base=2)
>
> Any suggestion ?
>
> Thank you
>
> Taka
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From gohidg at gmail.com  Wed Jul 12 05:08:18 2006
From: gohidg at gmail.com (Guohui Ding)
Date: Wed, 12 Jul 2006 11:08:18 +0800
Subject: [R] PDF version of Chinese translations of the manual "An
	Introduction to R"
Message-ID: <f04a1d1d0607112008l61b8c6edx8feec7f966905a68@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060712/4c01c2bd/attachment.pl 

From dimitrijoe at ipea.gov.br  Wed Jul 12 05:58:40 2006
From: dimitrijoe at ipea.gov.br (Dimitri Szerman)
Date: Wed, 12 Jul 2006 00:58:40 -0300
Subject: [R] help in vectorization
Message-ID: <b6150c70607112058h2bcec263i855c8fb1ca63ed24@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060712/a659ed6f/attachment.pl 

From ggrothendieck at gmail.com  Wed Jul 12 06:25:32 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 12 Jul 2006 00:25:32 -0400
Subject: [R] help in vectorization
In-Reply-To: <b6150c70607112058h2bcec263i855c8fb1ca63ed24@mail.gmail.com>
References: <b6150c70607112058h2bcec263i855c8fb1ca63ed24@mail.gmail.com>
Message-ID: <971536df0607112125l64896b3dk40d05310c286b0aa@mail.gmail.com>

Try this:

with(merge(dtf, dtf2), data.frame(y, m, inc = inc/def))

On 7/11/06, Dimitri Szerman <dimitrijoe at ipea.gov.br> wrote:
> Hi,
>
> I have two data frames. One is like
>
> > dtf = data.frame(y=c(rep(2002,4), rep(2003,5)),
> +                  m=c(9:12, 1:5),
> +                  def=c(.74,.75,.76,.78,.80,.82,.85,.85,.87))
>
> and the other
>
> dtf2 = data.frame(y=rep( c(2002,2003),20),
>                  m=c(trunc(runif(20,1,5)),trunc(runif(20,9,12))),
>                  inc=rnorm(40,mean=300,sd=150) )
>
> What I want is to divide dtf2$inc by dtf1$def by the "levels" of y and m
> (inc is income and def is the deflator). Any help in vectorizing this is
> welcome.
>
> Thank you,
> Dimitri


From gohidg at gmail.com  Wed Jul 12 06:55:52 2006
From: gohidg at gmail.com (Guohui Ding)
Date: Wed, 12 Jul 2006 12:55:52 +0800
Subject: [R] PDF version of Chinese translations of the manual "An
	Introduction to R"
Message-ID: <f04a1d1d0607112155y5bfca34epc460ddbc6693e1b6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060712/9a8aa32e/attachment.pl 

From macq at llnl.gov  Wed Jul 12 07:01:05 2006
From: macq at llnl.gov (Don MacQueen)
Date: Tue, 11 Jul 2006 22:01:05 -0700
Subject: [R] Date Format
In-Reply-To: <20060711162357.21469.qmail@web26309.mail.ukl.yahoo.com>
References: <20060711162357.21469.qmail@web26309.mail.ukl.yahoo.com>
Message-ID: <p06230902c0da3213061e@[192.168.2.2]>

If I understand your question correctly, then my suggestion is to try
   table( format(datetest), other.variable)
instead of
   table(datetest, other.variable)

-Don

At 4:23 PM +0000 7/11/06, pierre clauss wrote:
>Hi everybody,
>I need your precious help for, I think, a simple request, but I do 
>not manage to solve this.
>
>When I use a "table" function with dates in the rows, the rows are 
>coerced to number after the table function.
>
>So I need to transform the row names into date format. But I do not manage.
>
>Therefore, for an example, I manage to write this :
>
>datetest<-"06/01/2001"
>datetest<-as.Date(datetest,"%d/%m/%Y")
>datetest<-as.numeric(datetest)
>
>to get 11328.
>
>But I do not obtain the inverse tranformation :
>
>datetest<-as.Date(datetest,"%d/%m/%Y")
>
>How do we get this please ?
>
>Thanks a lot for your solution.
>Pierre.
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
---------------------------------
Don MacQueen
Lawrence Livermore National Laboratory
Livermore, CA, USA


From stephen.lane at exemail.com.au  Wed Jul 12 07:42:29 2006
From: stephen.lane at exemail.com.au (Steve Lane)
Date: Wed, 12 Jul 2006 15:42:29 +1000
Subject: [R] Rgnome in Ubuntu
Message-ID: <44B48BC5.5060704@exemail.com.au>

Hi
I am trying to set up Rgnome in Ubuntu (Dapper) and when I use the 
instructions:

/path/to/gnomeGUI/configure R_HOME=/path/to/R/installation
     make
     make install

I get an error message when I try to 'make' as follows:

/usr/local/lib64/R/etc/Makeconf:5: /make/vars.mk: No such file or directory
make: *** No rule to make target `/make/vars.mk'.  Stop.

Can anyone help me?
Steve


From ronggui.huang at gmail.com  Wed Jul 12 08:21:24 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Wed, 12 Jul 2006 14:21:24 +0800
Subject: [R] PDF version of Chinese translations of the manual "An
	Introduction to R"
In-Reply-To: <f04a1d1d0607112155y5bfca34epc460ddbc6693e1b6@mail.gmail.com>
References: <f04a1d1d0607112155y5bfca34epc460ddbc6693e1b6@mail.gmail.com>
Message-ID: <38b9f0350607112321s769f7f61nc48e1a309461bb24@mail.gmail.com>

That's a great news for all Chinese useR.

2006/7/12, Guohui Ding <gohidg at gmail.com>:
> Dear All,
>
>        I distributed the HTML style one year ago (
> http://www.biosino.org/pages/newhtm/r/schtml and
> http://www.biosino.org/pages/newhtm/r/tchtml ). Now I polished it, and
> rewrote it with Latex. You can download it from the URL:
> http://www.biosino.org/R/R-doc/  (
> http://www.biosino.org/R/R-doc/files/R-intro_cn.pdf).
>
>        Wish it will be useful for Chinese R users.
>
>        Any suggestion, comment and correction are welcome!
>
> --
> ADDRESS: Bioinformatics Center, Shanghai Institutes for Biological Sciences,
> Chinese Academy of Sciences
> 320 Yueyang Road, Shanghai 200031, P.R.China
> TELEPHONE: 86-21-54920086
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
??????
Department of Sociology
Fudan University


From mb.atelier at web.de  Wed Jul 12 08:41:56 2006
From: mb.atelier at web.de (Matthias Braeunig)
Date: Wed, 12 Jul 2006 08:41:56 +0200
Subject: [R] plot.ts panel function
Message-ID: <44B499B4.80709@web.de>

How can I set the ylim in individual panels of a "multiple" plot.ts?
The panels are all scaled to fully fit the series inside. But I need
specific scales, and colors ....

Here is an example:

plot.ts(cbind(1:10,1:10/10),ylim=c(0,3))	# ylim has no effect

Can someone more knowledgable please explain how to use the panel=
option correctly, thanks!

Cheers, m


From ottorini at mac.com  Wed Jul 12 11:05:20 2006
From: ottorini at mac.com (Jean-Marc Ottorini)
Date: Wed, 12 Jul 2006 11:05:20 +0200
Subject: [R] Access to conditioning values in "xyplot"
Message-ID: <248acdb525cd099e779835eb51f10cdf@mac.com>

Dear all,

   I have already searched unsuccessfully a lengthy R documentation, the 
archives of the list,  and the Internet for a solution to this 
problem... Could anybody, please, have a look at it?

The following  lattice graphics works well:


   residus <- xyplot (n ~ cg | di, data = myData,
                     scale = list(y = "free", x = "free"),
                     groups = bloc,
                     as.table = T,
                     xlab = "Cg",
                     ylab = "N / ha",
                     panel = function(x, y, subscripts, groups) {
                       panel.grid(h = -1, v = -1, col = "grey", lwd = 1, 
lty = 1)
                       ## --
                       ## panel.curve(expr, from, to, n = 101, 
curve.type = "l", ...)
                       ## --
                       panel.superpose(x, y, pch = c(1, 2), col = 
c("red","blue"), panel.groups = "panel.xyplot", subscripts, groups)
                     },
                     key = list( space = "top", transparent = TRUE, 
columns = 2,
                       points = list( pch = c(1, 2), col = c("red", 
"blue") ),
                       text = list( c("bloc 3", "bloc 4"))),
                     )


So far, so good, nothing tricky here...


Now I would like to uncomment this code to draw the line fitted to the 
points in each panel using panel.curve (expr = f(x, <di>, ...), where 
the function f is already known (i.e. has been fitted), depends on x, 
as needed, * but also on the value <di> for each panel of the 
conditioning variable di *, which I have no idea as how to access it...

Many thanks for your time...

Jean-Marc

  ----
Jean-Marc Ottorini               LERFoB, UMR INRA-ENGREF 1092
  email  ottorini at nancy.inra.fr          INRA - Centre de Nancy
  voice  +33-0383-394046                    F54280 - Champenoux
  fax    +33-0383-394034                                 France


  ----
Jean-Marc Ottorini               LERFoB, UMR INRA-ENGREF 1092
  email  ottorini at nancy.inra.fr          INRA - Centre de Nancy
  voice  +33-0383-394046                    F54280 - Champenoux
  fax    +33-0383-394034                                 France


From dieter.menne at menne-biomed.de  Wed Jul 12 11:30:24 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Wed, 12 Jul 2006 09:30:24 +0000 (UTC)
Subject: [R] plot.ts panel function
References: <44B499B4.80709@web.de>
Message-ID: <loom.20060712T112632-712@post.gmane.org>

Matthias Braeunig <mb.atelier <at> web.de> writes:

> 
> How can I set the ylim in individual panels of a "multiple" plot.ts?
> The panels are all scaled to fully fit the series inside. But I need
> specific scales, and colors ....
> 
> Here is an example:
> 
> plot.ts(cbind(1:10,1:10/10),ylim=c(0,3))	# ylim has no effect
> 
Personally, I tend to believe that this is an issue (being careful to avoid the
zoological word, which is reseved for core members) in plot.ts, where xlim and
ylim are not included in one of the plot.default calls. It works when you change
(around line 40 in plot.ts) to

  plot.default(x[, i], axes = FALSE, xlab = "",
      ylab = "", log = log, col = col, bg = bg, pch = pch,
       ann = ann, type = "n",xlim=xlim,ylim=ylim, ...)

But I am sure, that there were reasons to omit this, and I have not read the
documentation carefully.

Dieter


From ggrothendieck at gmail.com  Wed Jul 12 13:14:34 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 12 Jul 2006 07:14:34 -0400
Subject: [R] plot.ts panel function
In-Reply-To: <44B499B4.80709@web.de>
References: <44B499B4.80709@web.de>
Message-ID: <971536df0607120414h346c0595u8f9e3bfc5d0eadad@mail.gmail.com>

If you convert the ts object to a zoo object then it will
invoke plot.zoo in which case ylim= will work:

library(zoo)
x <- cbind(1:10,1:10/10)
plot(zoo(x), ylim = c(0,3))


On 7/12/06, Matthias Braeunig <mb.atelier at web.de> wrote:
> How can I set the ylim in individual panels of a "multiple" plot.ts?
> The panels are all scaled to fully fit the series inside. But I need
> specific scales, and colors ....
>
> Here is an example:
>
> plot.ts(cbind(1:10,1:10/10),ylim=c(0,3))        # ylim has no effect
>
> Can someone more knowledgable please explain how to use the panel=
> option correctly, thanks!
>
> Cheers, m
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From andrej.kastrin at siol.net  Wed Jul 12 13:25:48 2006
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Wed, 12 Jul 2006 13:25:48 +0200
Subject: [R] Discretize data.frame
Message-ID: <44B4DC3C.3050603@siol.net>

Dear useRs,

I use dics.ef function from dprep package to discretize continuous 
variable using intervals of equal frequencies. Dataset to be discretized 
include 4 continuous and 2 discrete variables in the following order:

Continuous Countinuous Countinuous Discrete Discrete Continuous

The problem emerge when I try to discretize the last continuos variable:

library(dprep)
dics.ef(my.dataframe, 1:6,5) ## all 6 variables in data.frame

I have no further idea why the function above don't work on the last 
variable in dataframe.

Thanks in advance for any further notes,

Andrej


From georg.otto at tuebingen.mpg.de  Wed Jul 12 13:24:47 2006
From: georg.otto at tuebingen.mpg.de (Georg Otto)
Date: Wed, 12 Jul 2006 13:24:47 +0200
Subject: [R] legend outside plotting area
Message-ID: <m1irm3kqv4.fsf@tuebingen.mpg.de>

Hi,

I would like to place a legend outside a plotting area. Could anybody
give me a hint how this is done?

Cheers,

Georg


From joris.dewolf at cropdesign.com  Wed Jul 12 13:32:17 2006
From: joris.dewolf at cropdesign.com (Joris De Wolf)
Date: Wed, 12 Jul 2006 13:32:17 +0200
Subject: [R] legend outside plotting area
In-Reply-To: <m1irm3kqv4.fsf@tuebingen.mpg.de>
References: <m1irm3kqv4.fsf@tuebingen.mpg.de>
Message-ID: <44B4DDC1.7030208@cropdesign.com>

see
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/68585.html

Georg Otto wrote:
> Hi,
> 
> I would like to place a legend outside a plotting area. Could anybody
> give me a hint how this is done?
> 
> Cheers,
> 
> Georg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


confidentiality notice:
The information contained in this e-mail is confidential and...{{dropped}}


From lobry at biomserv.univ-lyon1.fr  Wed Jul 12 13:45:10 2006
From: lobry at biomserv.univ-lyon1.fr (Jean lobry)
Date: Wed, 12 Jul 2006 13:45:10 +0200
Subject: [R] Are infix binary operators ** and ^ aliased?
In-Reply-To: <mailman.9.1152698403.20508.r-help@stat.math.ethz.ch>
References: <mailman.9.1152698403.20508.r-help@stat.math.ethz.ch>
Message-ID: <p06002005c0da8ecae384@[134.214.34.142]>

Dear R-help,

After making a typo (reminiscent of FORTRAN 77, I guess) I found the
following:

>  identical(all.equal(2^(-10:10), 2**(-10:10)), TRUE)
[1] TRUE

I have tried to find the documentation about the ** operator but I was
unsuccesful this way:

>  sessionInfo()
Version 2.3.1 (2006-06-01)
powerpc-apple-darwin8.6.0

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"
>  help("**")
No documentation for '**' in specified packages and libraries:
you could try 'help.search("**")'
>  help.search("\\*\\*")
No help files found with alias or concept or title matching '\*\*'
using regular expression matching.
>  RSiteSearch("**")
A search query has been submitted to http://search.r-project.org
The results page should open in your browser shortly
# --> Too many hits

Where can I find the documentation about the ** operator?

Thanks for any hint,

Jean Lobry
-- 
Jean R. Lobry            (lobry at biomserv.univ-lyon1.fr)
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
allo  : +33 472 43 12 87     fax    : +33 472 43 13 88
http://pbil.univ-lyon1.fr/members/lobry/


From ggrothendieck at gmail.com  Wed Jul 12 13:54:57 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 12 Jul 2006 07:54:57 -0400
Subject: [R] Are infix binary operators ** and ^ aliased?
In-Reply-To: <p06002005c0da8ecae384@134.214.34.142>
References: <mailman.9.1152698403.20508.r-help@stat.math.ethz.ch>
	<p06002005c0da8ecae384@134.214.34.142>
Message-ID: <971536df0607120454w728a211ajc4295d9ac2d2f1ed@mail.gmail.com>

It appears so.  If we define ^ for class "x" then ** seems to change
in the same way:

> "^.x" <- function(x,y) x+y
> y <- structure(3, class = "x")
> y ** 4
[1] 7
attr(,"class")
[1] "x"

On 7/12/06, Jean lobry <lobry at biomserv.univ-lyon1.fr> wrote:
> Dear R-help,
>
> After making a typo (reminiscent of FORTRAN 77, I guess) I found the
> following:
>
> >  identical(all.equal(2^(-10:10), 2**(-10:10)), TRUE)
> [1] TRUE
>
> I have tried to find the documentation about the ** operator but I was
> unsuccesful this way:
>
> >  sessionInfo()
> Version 2.3.1 (2006-06-01)
> powerpc-apple-darwin8.6.0
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
> >  help("**")
> No documentation for '**' in specified packages and libraries:
> you could try 'help.search("**")'
> >  help.search("\\*\\*")
> No help files found with alias or concept or title matching '\*\*'
> using regular expression matching.
> >  RSiteSearch("**")
> A search query has been submitted to http://search.r-project.org
> The results page should open in your browser shortly
> # --> Too many hits
>
> Where can I find the documentation about the ** operator?
>
> Thanks for any hint,
>
> Jean Lobry
> --
> Jean R. Lobry            (lobry at biomserv.univ-lyon1.fr)
> Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
> 43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
> allo  : +33 472 43 12 87     fax    : +33 472 43 13 88
> http://pbil.univ-lyon1.fr/members/lobry/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From p.dalgaard at biostat.ku.dk  Wed Jul 12 13:57:28 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Jul 2006 13:57:28 +0200
Subject: [R] Are infix binary operators ** and ^ aliased?
In-Reply-To: <p06002005c0da8ecae384@[134.214.34.142]>
References: <mailman.9.1152698403.20508.r-help@stat.math.ethz.ch>
	<p06002005c0da8ecae384@[134.214.34.142]>
Message-ID: <x24pxnc9xz.fsf@turmalin.kubism.ku.dk>

Jean lobry <lobry at biomserv.univ-lyon1.fr> writes:

> Dear R-help,
> 
> After making a typo (reminiscent of FORTRAN 77, I guess) I found the
> following:
> 
> >  identical(all.equal(2^(-10:10), 2**(-10:10)), TRUE)
> [1] TRUE
> 
> I have tried to find the documentation about the ** operator but I was
> unsuccesful this way:
> 
> >  sessionInfo()
> Version 2.3.1 (2006-06-01)
> powerpc-apple-darwin8.6.0
> 
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
> >  help("**")
> No documentation for '**' in specified packages and libraries:
> you could try 'help.search("**")'
> >  help.search("\\*\\*")
> No help files found with alias or concept or title matching '\*\*'
> using regular expression matching.
> >  RSiteSearch("**")
> A search query has been submitted to http://search.r-project.org
> The results page should open in your browser shortly
> # --> Too many hits
> 
> Where can I find the documentation about the ** operator?
> 
> Thanks for any hint,

It's a relic, which might be useful to have for someone, but which we
don't advertise:

> quote(3**2)
3^2


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From priti.desai at kalyptorisk.com  Wed Jul 12 13:26:29 2006
From: priti.desai at kalyptorisk.com (priti desai)
Date: Wed, 12 Jul 2006 16:56:29 +0530
Subject: [R] Query : Modification of graphs
Message-ID: <2AB7346A3227A74BB97F9A0D79E3E65A01C1DB@mailserver.kalyptorisk.com>

How to change the background in the plots
The e.g. for which I am working is as follows,
The R script is
###################    start    #######################################

amounts <-
c(21790,5669,34921.17,60152.47,47228.61,16566.13,8283,3980,5445,1000,125
0,0,1285,1222,5001,600,1950,11580,7520,7999,4850,3850,16900,6095,0,4246,
8556,8280,3020,7250,0,8807,0)

# histogram

hist(amounts)

# P-P plot(exponential dist)

Lambda        <- 1/mean(amounts)
N             <- length(amounts) 
e             <- c(1:N)
f             <- c((e-.5)/N)
Fx            <- c(1 - exp(-lambda*amounts))
g             <- sort(Fx)


plot (f, g, main ="P-P Plot",col=6)
abline (rq(g ~ f, tau = .5),col="blue")



# Q-Q plot

e             <- c(1:N)
f             <- c((e-.5)/N)
h             <- c(1-f)
i             <- -log(h,base=exp(1))
F1x           <- c((1/lambda)*i)

j             <- sort(amounts)
z12 <- sort(F1x)

plot (j,F1x,main = "Q-Q Plot",col=6)
abline (rq(F1x ~ j, tau = .5),col="red")

######################     end
#########################################

Please tell me how should I modify my graphs? 
i.e. change the background,  colors,    
how should I modify histogram, P-P plots ,Q-Q plots?

Please suggest me the correct modification, if any calculations &
formula in R.
 Awaiting your positive reply.
 
  Regards
  Priti.


From ggrothendieck at gmail.com  Wed Jul 12 14:10:36 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 12 Jul 2006 08:10:36 -0400
Subject: [R] detach
In-Reply-To: <971536df0607111104j5cf964e9oa39d29d8e6ff8a3e@mail.gmail.com>
References: <971536df0607111104j5cf964e9oa39d29d8e6ff8a3e@mail.gmail.com>
Message-ID: <971536df0607120510k85339bdu64f2f3699c25762c@mail.gmail.com>

Just wanted to add that entering this into a fresh session does not
help.  Note that as.Date.numeric is still defined after unloading the
namespace:

> as.Date(1)
Error in as.Date.default(1) : do not know how to convert '1' to class "Date"
> library(zoo)
> as.Date(1)
[1] "1970-01-02"
> unloadNamespace("zoo")
<environment: namespace:zoo>
> as.Date(1)
[1] "1970-01-02"

On 7/11/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> We try the following:
>
> search()
> as.Date(1)
> zoo:::as.Date.numeric
>
> under three circumstances:
>
> 1. on a fresh session
> 2. after issuing library(zoo) noting that as.Date.numeric is provided by zoo
> 3. after detaching zoo
>
> as.Date(1) fails on #1 but succeeds in #2 and #3.  Should it not fail in #3
> since zoo was detached?  Is this how its supposed to work?
>
> (Note that entering zoo:::as.Date.numeric at the console
> succeeds in displaying the function in all three cases.)
>
> Here is the console session:
>
> > R.version.string # XP
> [1] "Version 2.3.1 Patched (2006-06-04 r38279)"
> >
> > search()
> [1] ".GlobalEnv"        "package:methods"   "package:stats"
> [4] "package:graphics"  "package:grDevices" "package:utils"
> [7] "package:datasets"  "Autoloads"         "package:base"
> > as.Date(1)
> Error in as.Date.default(1) : do not know how to convert '1' to class "Date"
> > zoo:::as.Date.numeric
> function (x, ...)
> structure(floor(x + 0.001), class = "Date")
> <environment: namespace:zoo>
> >
> > library(zoo)
> > search()
>  [1] ".GlobalEnv"        "package:zoo"       "package:methods"
>  [4] "package:stats"     "package:graphics"  "package:grDevices"
>  [7] "package:utils"     "package:datasets"  "Autoloads"
> [10] "package:base"
> > as.Date(1)
> [1] "1970-01-02"
> > zoo:::as.Date.numeric
> function (x, ...)
> structure(floor(x + 0.001), class = "Date")
> <environment: namespace:zoo>
> >
> > detach()
> > search()
> [1] ".GlobalEnv"        "package:methods"   "package:stats"
> [4] "package:graphics"  "package:grDevices" "package:utils"
> [7] "package:datasets"  "Autoloads"         "package:base"
> > as.Date(1)
> [1] "1970-01-02"
> > zoo:::as.Date.numeric
> function (x, ...)
> structure(floor(x + 0.001), class = "Date")
> <environment: namespace:zoo>
> >
>


From D.Vonka at UvT.nl  Wed Jul 12 14:37:16 2006
From: D.Vonka at UvT.nl (David Vonka)
Date: Wed, 12 Jul 2006 14:37:16 +0200
Subject: [R] Is it possible to only read a subset by read.table ?
Message-ID: <44B4ECFC.1010801@UvT.nl>

Hello,

is it possible to do something like

DATA <- read.table(file="blabla.dat",subset=(sex=="male")),

i.e. make R read only a subset of a csv file ?
I think it would be useful in case of very big datasets,
but I can't find such a feature.

Thanks for an answer,
David Vonka


From ggrothendieck at gmail.com  Wed Jul 12 14:49:02 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 12 Jul 2006 08:49:02 -0400
Subject: [R] Is it possible to only read a subset by read.table ?
In-Reply-To: <44B4ECFC.1010801@UvT.nl>
References: <44B4ECFC.1010801@UvT.nl>
Message-ID: <971536df0607120549r2d5c5ff0q9d1fa83178fa40b9@mail.gmail.com>

You can use pipe with read.table as in:

http://tolstoy.newcastle.edu.au/R/help/06/02/20379.html

Also note skip= and nrows= arguments to read.table.

On 7/12/06, David Vonka <D.Vonka at uvt.nl> wrote:
> Hello,
>
> is it possible to do something like
>
> DATA <- read.table(file="blabla.dat",subset=(sex=="male")),
>
> i.e. make R read only a subset of a csv file ?
> I think it would be useful in case of very big datasets,
> but I can't find such a feature.
>
> Thanks for an answer,
> David Vonka
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From d.d.sutcliffe at durham.ac.uk  Wed Jul 12 15:05:45 2006
From: d.d.sutcliffe at durham.ac.uk (d d sutcliffe@durham ac uk)
Date: Wed, 12 Jul 2006 14:05:45 +0100
Subject: [R] ts command and stl function
Message-ID: <44B4F3A9.70303@durham.ac.uk>

Hi,

I have imported a csv file into R which contains one column (the rate er 
100,000 population of a disease, by month over 11 years) I coerced into 
a time series using the following function,

tstkr<-ts(tkr,deltat=1/12)

This seems to work fine, and when I check for the class of the object 
using class(tstkr) I get "ts" as the response.

When I try to use the stl function in stats I get the error message:

Error in stl(tstkr)only univariate series are allowed

I then tried this:

tstkr <- ts(c(tkr), deltat=1/12)

however this made no difference...I still get an error - does anybody 
know what is wrong?

Regards,

Daniel


From rdpeng at gmail.com  Wed Jul 12 15:06:51 2006
From: rdpeng at gmail.com (Roger Peng)
Date: Wed, 12 Jul 2006 09:06:51 -0400
Subject: [R] Is it possible to only read a subset by read.table ?
In-Reply-To: <44B4ECFC.1010801@UvT.nl>
References: <44B4ECFC.1010801@UvT.nl>
Message-ID: <66f3bd910607120606o4ccb5599x4307ce895b0b02b9@mail.gmail.com>

It's not so straightforward as that, but you could construct something
with readLines().

-roger

On 7/12/06, David Vonka <D.Vonka at uvt.nl> wrote:
> Hello,
>
> is it possible to do something like
>
> DATA <- read.table(file="blabla.dat",subset=(sex=="male")),
>
> i.e. make R read only a subset of a csv file ?
> I think it would be useful in case of very big datasets,
> but I can't find such a feature.
>
> Thanks for an answer,
> David Vonka
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From rdpeng at gmail.com  Wed Jul 12 15:14:03 2006
From: rdpeng at gmail.com (Roger Peng)
Date: Wed, 12 Jul 2006 09:14:03 -0400
Subject: [R] detach
In-Reply-To: <971536df0607111104j5cf964e9oa39d29d8e6ff8a3e@mail.gmail.com>
References: <971536df0607111104j5cf964e9oa39d29d8e6ff8a3e@mail.gmail.com>
Message-ID: <66f3bd910607120614q12ea1662ye8aed4974b7bbcc8@mail.gmail.com>

I believe (3) should always work as long as zoo has a namespace.  The
':::' operator will load the namespace of the package but will not
attach it to the search list.

-roger

On 7/11/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> We try the following:
>
> search()
> as.Date(1)
> zoo:::as.Date.numeric
>
> under three circumstances:
>
> 1. on a fresh session
> 2. after issuing library(zoo) noting that as.Date.numeric is provided by zoo
> 3. after detaching zoo
>
> as.Date(1) fails on #1 but succeeds in #2 and #3.  Should it not fail in #3
> since zoo was detached?  Is this how its supposed to work?
>
> (Note that entering zoo:::as.Date.numeric at the console
> succeeds in displaying the function in all three cases.)
>
> Here is the console session:
>
> > R.version.string # XP
> [1] "Version 2.3.1 Patched (2006-06-04 r38279)"
> >
> > search()
> [1] ".GlobalEnv"        "package:methods"   "package:stats"
> [4] "package:graphics"  "package:grDevices" "package:utils"
> [7] "package:datasets"  "Autoloads"         "package:base"
> > as.Date(1)
> Error in as.Date.default(1) : do not know how to convert '1' to class "Date"
> > zoo:::as.Date.numeric
> function (x, ...)
> structure(floor(x + 0.001), class = "Date")
> <environment: namespace:zoo>
> >
> > library(zoo)
> > search()
>  [1] ".GlobalEnv"        "package:zoo"       "package:methods"
>  [4] "package:stats"     "package:graphics"  "package:grDevices"
>  [7] "package:utils"     "package:datasets"  "Autoloads"
> [10] "package:base"
> > as.Date(1)
> [1] "1970-01-02"
> > zoo:::as.Date.numeric
> function (x, ...)
> structure(floor(x + 0.001), class = "Date")
> <environment: namespace:zoo>
> >
> > detach()
> > search()
> [1] ".GlobalEnv"        "package:methods"   "package:stats"
> [4] "package:graphics"  "package:grDevices" "package:utils"
> [7] "package:datasets"  "Autoloads"         "package:base"
> > as.Date(1)
> [1] "1970-01-02"
> > zoo:::as.Date.numeric
> function (x, ...)
> structure(floor(x + 0.001), class = "Date")
> <environment: namespace:zoo>
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From ggrothendieck at gmail.com  Wed Jul 12 15:21:26 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 12 Jul 2006 09:21:26 -0400
Subject: [R] detach
In-Reply-To: <66f3bd910607120614q12ea1662ye8aed4974b7bbcc8@mail.gmail.com>
References: <971536df0607111104j5cf964e9oa39d29d8e6ff8a3e@mail.gmail.com>
	<66f3bd910607120614q12ea1662ye8aed4974b7bbcc8@mail.gmail.com>
Message-ID: <971536df0607120621j42c311b7o18c585702e4022d3@mail.gmail.com>

Note that even this entered into a fresh session exhibits this
behavior.  There is no use of ::: here:

> as.Date(1)
Error in as.Date.default(1) : do not know how to convert '1' to class "Date"
> library(zoo)
> as.Date(1)
[1] "1970-01-02"
> unloadNamespace("zoo")
<environment: namespace:zoo>
> as.Date(1)
[1] "1970-01-02"
> R.version.string
[1] "Version 2.3.1 Patched (2006-06-04 r38279)"
> packageDescription("zoo")$Version
[1] "1.1-1"

On 7/12/06, Roger Peng <rdpeng at gmail.com> wrote:
> I believe (3) should always work as long as zoo has a namespace.  The
> ':::' operator will load the namespace of the package but will not
> attach it to the search list.
>
> -roger
>
> On 7/11/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > We try the following:
> >
> > search()
> > as.Date(1)
> > zoo:::as.Date.numeric
> >
> > under three circumstances:
> >
> > 1. on a fresh session
> > 2. after issuing library(zoo) noting that as.Date.numeric is provided by zoo
> > 3. after detaching zoo
> >
> > as.Date(1) fails on #1 but succeeds in #2 and #3.  Should it not fail in #3
> > since zoo was detached?  Is this how its supposed to work?
> >
> > (Note that entering zoo:::as.Date.numeric at the console
> > succeeds in displaying the function in all three cases.)
> >
> > Here is the console session:
> >
> > > R.version.string # XP
> > [1] "Version 2.3.1 Patched (2006-06-04 r38279)"
> > >
> > > search()
> > [1] ".GlobalEnv"        "package:methods"   "package:stats"
> > [4] "package:graphics"  "package:grDevices" "package:utils"
> > [7] "package:datasets"  "Autoloads"         "package:base"
> > > as.Date(1)
> > Error in as.Date.default(1) : do not know how to convert '1' to class "Date"
> > > zoo:::as.Date.numeric
> > function (x, ...)
> > structure(floor(x + 0.001), class = "Date")
> > <environment: namespace:zoo>
> > >
> > > library(zoo)
> > > search()
> >  [1] ".GlobalEnv"        "package:zoo"       "package:methods"
> >  [4] "package:stats"     "package:graphics"  "package:grDevices"
> >  [7] "package:utils"     "package:datasets"  "Autoloads"
> > [10] "package:base"
> > > as.Date(1)
> > [1] "1970-01-02"
> > > zoo:::as.Date.numeric
> > function (x, ...)
> > structure(floor(x + 0.001), class = "Date")
> > <environment: namespace:zoo>
> > >
> > > detach()
> > > search()
> > [1] ".GlobalEnv"        "package:methods"   "package:stats"
> > [4] "package:graphics"  "package:grDevices" "package:utils"
> > [7] "package:datasets"  "Autoloads"         "package:base"
> > > as.Date(1)
> > [1] "1970-01-02"
> > > zoo:::as.Date.numeric
> > function (x, ...)
> > structure(floor(x + 0.001), class = "Date")
> > <environment: namespace:zoo>
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
>
> --
> Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/
>


From msubianto at gmail.com  Wed Jul 12 15:28:43 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Wed, 12 Jul 2006 15:28:43 +0200
Subject: [R] Error install rgl package on linux
Message-ID: <c7c17cef0607120628m5ab294aer30fa8ef988f7b124@mail.gmail.com>

Dear all,

I tried to install rgl package on my linux machine fc5, I got an error.
Here I run R as user,
$ R

> options(repos=c(CRAN="http://cran.at.r-project.org/"))
> install.packages("rgl", lib="/home/subianto/local/lib/R/library/site-packages", dependencies=TRUE)
trying URL 'http://cran.at.r-project.org/src/contrib/rgl_0.67-2.tar.gz'
Content type 'application/x-tar' length 567592 bytes
opened URL
==================================================
downloaded 554Kb

* Installing *source* package 'rgl' ...
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking for X... libraries , headers
checking for libpng-config... yes
configure: using libpng-config
configure: using libpng dynamic linkage
configure: creating ./config.status
config.status: creating src/Makevars
** libs
g++ -I/usr/lib/R/include -I/usr/lib/R/include -I -DHAVE_PNG_H
-I/usr/include/libpng12 -I/usr/local/include   -fpic  -O2 -g -pipe
-Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
--param=ssp-buffer-size=4 -m32 -march=i386 -mtune=generic
-fasynchronous-unwind-tables -c api.cpp -o api.o
Disposable.hpp:13: warning: 'struct IDisposeListener' has virtual
functions but non-virtual destructor
types.h:77: warning: 'class DestroyHandler' has virtual functions but
non-virtual destructor
gui.hpp:56: warning: 'class gui::WindowImpl' has virtual functions but
non-virtual destructor
gui.hpp:90: warning: 'class gui::GUIFactory' has virtual functions but
non-virtual destructor
pixmap.h:39: warning: 'class PixmapFormat' has virtual functions but
non-virtual destructor

<snip>

Disposable.hpp:13: warning: 'struct IDisposeListener' has virtual
functions but non-virtual destructor
gui.hpp:56: warning: 'class gui::WindowImpl' has virtual functions but
non-virtual destructor
gui.hpp:90: warning: 'class gui::GUIFactory' has virtual functions but
non-virtual destructor
g++ -shared -L/usr/local/lib -o rgl.so api.o Background.o BBoxDeco.o
Color.o device.o devicemanager.o Disposable.o FaceSet.o fps.o geom.o
gl2ps.o glgui.o gui.o Light.o LineSet.o LineStripSet.o Material.o
math.o osxgui.o osxlib.o par3d.o pixmap.o PointSet.o PrimitiveSet.o
QuadSet.o RenderContext.o render.o rglview.o scene.o select.o Shape.o
SphereMesh.o SphereSet.o SpriteSet.o String.o Surface.o TextSet.o
Texture.o TriangleSet.o types.o Viewpoint.o win32gui.o win32lib.o
x11gui.o x11lib.o -L -lX11 -lXext -lGL -lGLU -L/usr/lib -lpng12
-L/usr/lib/R/lib -lR
/usr/bin/ld: cannot find -lXext
collect2: ld returned 1 exit status
make: *** [rgl.so] Error 1
chmod: cannot access
`/home/subianto/local/lib/R/library/site-packages/rgl/libs/*': No such
file or directory
ERROR: compilation failed for package 'rgl'
** Removing '/home/subianto/local/lib/R/library/site-packages/rgl'

The downloaded packages are in
        /tmp/RtmpH3o1qG/downloaded_packages
Warning messages:
1: installation of package 'rgl' had non-zero exit status in:
install.packages("rgl", lib =
"/home/subianto/local/lib/R/library/site-packages",
2: cannot create HTML package index in: tools:::unix.packages.html(.Library)
>

I think the error about missing devel-libraries on my fc5 but I do not
know which libraries are. I apologize for my poor linux knowledge and
expertise. Thank you very much.

Best, Muhammad Subianto


> version
               _
platform       i686-redhat-linux-gnu
arch           i686
os             linux-gnu
system         i686, linux-gnu
status
major          2
minor          3.1
year           2006
month          06
day            01
svn rev        38247
language       R
version.string Version 2.3.1 (2006-06-01)
>

$ uname -r
2.6.17-1.2145_FC5


From dnlsutcliffe at yahoo.co.uk  Wed Jul 12 15:53:25 2006
From: dnlsutcliffe at yahoo.co.uk (Daniel sutcliffe)
Date: Wed, 12 Jul 2006 14:53:25 +0100 (BST)
Subject: [R] ts and stl functions
Message-ID: <20060712135325.39789.qmail@web26905.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060712/918eaade/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Jul 12 15:54:07 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Jul 2006 14:54:07 +0100 (BST)
Subject: [R] Is it possible to only read a subset by read.table ?
In-Reply-To: <44B4ECFC.1010801@UvT.nl>
References: <44B4ECFC.1010801@UvT.nl>
Message-ID: <Pine.LNX.4.64.0607121441140.982@gannet.stats.ox.ac.uk>

On Wed, 12 Jul 2006, David Vonka wrote:

> Hello,
> 
> is it possible to do something like
> 
> DATA <- read.table(file="blabla.dat",subset=(sex=="male")),
> 
> i.e. make R read only a subset of a csv file ?
> I think it would be useful in case of very big datasets,
> but I can't find such a feature.

No.  It is possible to read only some columns: see colClasses.

It is not clear that the ability to skip based on something defined on 
other columns would actually be useful, especially as you can read 
datasets in blocks, process the blocks and combine the results.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From p.dalgaard at biostat.ku.dk  Wed Jul 12 15:57:53 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Jul 2006 15:57:53 +0200
Subject: [R] Error install rgl package on linux
In-Reply-To: <c7c17cef0607120628m5ab294aer30fa8ef988f7b124@mail.gmail.com>
References: <c7c17cef0607120628m5ab294aer30fa8ef988f7b124@mail.gmail.com>
Message-ID: <x2zmffapsu.fsf@turmalin.kubism.ku.dk>

"Muhammad Subianto" <msubianto at gmail.com> writes:


> QuadSet.o RenderContext.o render.o rglview.o scene.o select.o Shape.o
> SphereMesh.o SphereSet.o SpriteSet.o String.o Surface.o TextSet.o
> Texture.o TriangleSet.o types.o Viewpoint.o win32gui.o win32lib.o
> x11gui.o x11lib.o -L -lX11 -lXext -lGL -lGLU -L/usr/lib -lpng12
> -L/usr/lib/R/lib -lR
> /usr/bin/ld: cannot find -lXext
> collect2: ld returned 1 exit status
> make: *** [rgl.so] Error 1

...

> I think the error about missing devel-libraries on my fc5 but I do not
> know which libraries are. I apologize for my poor linux knowledge and
> expertise. Thank you very much.

I have

[pd at titmouse2 R]$ locate Xext
/usr/include/X11/extensions/Xext.h
/usr/include/X11/extensions/panoramiXext.h
/usr/lib/libXext.so
/usr/lib/libXext.so.6
/usr/lib/libXext.so.6.4.0
/usr/share/doc/libXext-1.0.0
.....


[pd at titmouse2 R]$ rpm -qf `locate Xext`
xorg-x11-proto-devel-7.0-6
xorg-x11-proto-devel-7.0-6
libXext-devel-1.0.0-3.2
libXext-1.0.0-3.2
....

In general, if you get an error about a missing -lfoo, installing RPMs
for libfoo and libfoo-devel is the first thing to try.



-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From sachinj.2006 at yahoo.com  Wed Jul 12 15:58:34 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Wed, 12 Jul 2006 06:58:34 -0700 (PDT)
Subject: [R] AICc vs AIC for model selection
Message-ID: <20060712135834.75532.qmail@web37611.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060712/47b3f206/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Jul 12 16:41:15 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Jul 2006 15:41:15 +0100 (BST)
Subject: [R] ts and stl functions
In-Reply-To: <20060712135325.39789.qmail@web26905.mail.ukl.yahoo.com>
References: <20060712135325.39789.qmail@web26905.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.64.0607121540110.6616@gannet.stats.ox.ac.uk>

On Wed, 12 Jul 2006, Daniel sutcliffe wrote:

> Hi, 
> 
> I have imported a csv file into R which contains one column (the rate er 100,000 population of a disease, by month over 11 years) I coerced into a time series using the following function, 
> 
> tstkr<-ts(tkr,deltat=1/12) 

That's likely making a ts out of a data frame and not the one column of 
the data frame.  What does dim(tkr) say?

> 
> This seems to work fine, and when I check for the class of the object using class(tstkr) I get "ts" as the response. 
> 
> When I try to use the stl function in stats I get the error message: 
> 
> Error in stl(tstkr)only univariate series are allowed 
> 
> I then tried this: 
> 
> tstkr <- ts(c(tkr), deltat=1/12) 
> 
> however this made no difference...I still get an error - does anybody know what is wrong? 

Try tkr[[1]]

> 
> Regards, 
> 
> Daniel 
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From xvyborny at fi.muni.cz  Wed Jul 12 16:46:36 2006
From: xvyborny at fi.muni.cz (Ondrej Vyborny)
Date: Wed, 12 Jul 2006 16:46:36 +0200
Subject: [R] DTW - dynamic time warping - and time series in R
Message-ID: <Pine.SGI.4.60.0607121628360.19268155@aisa.fi.muni.cz>

Hello,

can anybody tell me if there exists functions for DTW in R? I didn't find 
anything at CRAN's search page... Also any information about packages 
for time series preprocessing (for pattern matching) would be useful...

Thanks a lot,

ondra


From rstatistics at gmail.com  Wed Jul 12 16:54:31 2006
From: rstatistics at gmail.com (A.R. Criswell)
Date: Wed, 12 Jul 2006 21:54:31 +0700
Subject: [R] clear window on Mac OS X
Message-ID: <878be2c60607120754m3565deccl4835ef5346408e16@mail.gmail.com>

Hello All,

On Windows OS, one can clear previous results in the R window with
CNTL-L. Is there a similar command in R for Mac OS X?

Thanks


From msubianto at gmail.com  Wed Jul 12 16:57:18 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Wed, 12 Jul 2006 16:57:18 +0200
Subject: [R] Error install rgl package on linux
Message-ID: <c7c17cef0607120757u555e7950hf8a6fd0f0e4dc0ca@mail.gmail.com>

On this day 12/07/2006 15:57, Peter Dalgaard wrote:
> "Muhammad Subianto" <msubianto at gmail.com> writes:
>
>
>> QuadSet.o RenderContext.o render.o rglview.o scene.o select.o Shape.o
>> SphereMesh.o SphereSet.o SpriteSet.o String.o Surface.o TextSet.o
>> Texture.o TriangleSet.o types.o Viewpoint.o win32gui.o win32lib.o
>> x11gui.o x11lib.o -L -lX11 -lXext -lGL -lGLU -L/usr/lib -lpng12
>> -L/usr/lib/R/lib -lR
>> /usr/bin/ld: cannot find -lXext
>> collect2: ld returned 1 exit status
>> make: *** [rgl.so] Error 1
>
> ...
>
>> I think the error about missing devel-libraries on my fc5 but I do not
>> know which libraries are. I apologize for my poor linux knowledge and
>> expertise. Thank you very much.
>
> I have
>
> [pd at titmouse2 R]$ locate Xext
> /usr/include/X11/extensions/Xext.h
> /usr/include/X11/extensions/panoramiXext.h
> /usr/lib/libXext.so
> /usr/lib/libXext.so.6
> /usr/lib/libXext.so.6.4.0
> /usr/share/doc/libXext-1.0.0
> .....
>
>
> [pd at titmouse2 R]$ rpm -qf `locate Xext`
> xorg-x11-proto-devel-7.0-6
> xorg-x11-proto-devel-7.0-6
> libXext-devel-1.0.0-3.2
> libXext-1.0.0-3.2
> ....
>
> In general, if you get an error about a missing -lfoo, installing RPMs
> for libfoo and libfoo-devel is the first thing to try.
>
>
>

Thank you for giving me so much of your time to provide an explanation
of installing RPMs. After I installed "libXext-devel" and then
> options(repos=c(CRAN="http://cran.at.r-project.org/"))
> install.packages("rgl", lib="/home/subianto/local/lib/R/library/site-packages", dependencies=TRUE)

Yes, it works perfectly. Thanks a lot, to Peter Dalgaard.

Sincerely, Muhammad Subianto


From br44114 at gmail.com  Wed Jul 12 18:03:53 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Wed, 12 Jul 2006 12:03:53 -0400
Subject: [R] Is it possible to only read a subset by read.table ?
Message-ID: <8d5a36350607120903m1213f0e1q66597076051ba530@mail.gmail.com>

It's possible and straightforward (just don't use R). IMHO the GNU
Core Utilities
http://www.gnu.org/software/coreutils/
plus a few other tools such as sed, awk, grep etc are much more
appropriate than R for processing massive text files. (Get a good book
about UNIX shell scripting. On Windows you can use Services For Unix
or Cygwin.)

Also, here's an example that you could adapt to print the males from
your data set to a separate file, which you could then import in R.
#---print specific lines to another file---
suffix=_JAN06
for F in `ls *data*`
do
  echo $F
  sed -n -e '/2006-01-[0-9][0-9]/p' $F > ${F}${suffix}
done


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Vonka
> Sent: Wednesday, July 12, 2006 8:37 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Is it possible to only read a subset by read.table ?
>
> Hello,
>
> is it possible to do something like
>
> DATA <- read.table(file="blabla.dat",subset=(sex=="male")),
>
> i.e. make R read only a subset of a csv file ?
> I think it would be useful in case of very big datasets,
> but I can't find such a feature.
>
> Thanks for an answer,
> David Vonka
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


From Arne.Jol at Unilever.com  Wed Jul 12 18:41:42 2006
From: Arne.Jol at Unilever.com (Jol, Arne)
Date: Wed, 12 Jul 2006 17:41:42 +0100
Subject: [R] Keep value lables with data frame manipulation
Message-ID: <2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>

Dear R,

I import data from spss into a R data.frame. On this rawdata I do some
data processing (selection of observations, normalization, recoding of
variables etc..). The result is stored in a new data.frame, however, in
this new data.frame the value labels are lost.

Example of what I do in code:

# read raw data from spss
rawdata <- read.spss("./data/T50937.SAV",
	use.value.labels=FALSE,to.data.frame=TRUE)

# select the observations that we need
diarydata <- rawdata[rawdata$D22==2 | rawdata$D22==3 | rawdata$D22==17 |
rawdata$D22==18 | rawdata$D22==20 | rawdata$D22==22 |
 			rawdata$D22==24 | rawdata$D22==33,]

The result is that rawdata$D22 has value labels and that diarydata$D22
is numeric without value labels.

Question: How can I prevent this from happening?

Thanks in advance!
Groeten,
Arne


From Nicolas.Meyer at chru-strasbourg.fr  Wed Jul 12 18:48:54 2006
From: Nicolas.Meyer at chru-strasbourg.fr (Nicolas.Meyer at chru-strasbourg.fr)
Date: Wed, 12 Jul 2006 18:48:54 +0200
Subject: [R] Prediction interval of Y using BMA
Message-ID: <859726A76C47574081B445F2D0BD25FF28C4E4@PEGASUS.hus.chru-strasbourg.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060712/d7b5f2d3/attachment.pl 

From plummer at iarc.fr  Wed Jul 12 19:06:35 2006
From: plummer at iarc.fr (Martyn Plummer)
Date: Wed, 12 Jul 2006 19:06:35 +0200
Subject: [R] detach
In-Reply-To: <971536df0607120621j42c311b7o18c585702e4022d3@mail.gmail.com>
References: <971536df0607111104j5cf964e9oa39d29d8e6ff8a3e@mail.gmail.com>
	<66f3bd910607120614q12ea1662ye8aed4974b7bbcc8@mail.gmail.com>
	<971536df0607120621j42c311b7o18c585702e4022d3@mail.gmail.com>
Message-ID: <1152723995.5118.4.camel@seurat.iarc.fr>

My guess is that even though the zoo namespace is unloaded. The S3
method as.Date.numeric is still registered, and the name space will be
reloaded whenever it is dispatched.

> library(zoo)
> loadedNamespaces()
[1] "base"      "graphics"  "grDevices" "methods"   "stats"     "utils"
[7] "zoo"
> unloadNamespace("zoo")
<environment: namespace:zoo>
> loadedNamespaces() #Gone
[1] "base"      "graphics"  "grDevices" "methods"   "stats"     "utils"
> as.Date(1)
[1] "1970-01-02"
> loadedNamespaces() #Back again!
[1] "base"      "graphics"  "grDevices" "methods"   "stats"     "utils"
[7] "zoo"

So the question is whether there is a way to unregister an S3 method, so
that you can restore the method dispatching behaviour prior to the
package being loaded. I don't know the answer, but I'm guessing it's no.

On Wed, 2006-07-12 at 09:21 -0400, Gabor Grothendieck wrote:
> Note that even this entered into a fresh session exhibits this
> behavior.  There is no use of ::: here:
> 
> > as.Date(1)
> Error in as.Date.default(1) : do not know how to convert '1' to class "Date"
> > library(zoo)
> > as.Date(1)
> [1] "1970-01-02"
> > unloadNamespace("zoo")
> <environment: namespace:zoo>
> > as.Date(1)
> [1] "1970-01-02"
> > R.version.string
> [1] "Version 2.3.1 Patched (2006-06-04 r38279)"
> > packageDescription("zoo")$Version
> [1] "1.1-1"
> 
> On 7/12/06, Roger Peng <rdpeng at gmail.com> wrote:
> > I believe (3) should always work as long as zoo has a namespace.  The
> > ':::' operator will load the namespace of the package but will not
> > attach it to the search list.
> >
> > -roger
> >
> > On 7/11/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > We try the following:
> > >
> > > search()
> > > as.Date(1)
> > > zoo:::as.Date.numeric
> > >
> > > under three circumstances:
> > >
> > > 1. on a fresh session
> > > 2. after issuing library(zoo) noting that as.Date.numeric is provided by zoo
> > > 3. after detaching zoo
> > >
> > > as.Date(1) fails on #1 but succeeds in #2 and #3.  Should it not fail in #3
> > > since zoo was detached?  Is this how its supposed to work?
> > >
> > > (Note that entering zoo:::as.Date.numeric at the console
> > > succeeds in displaying the function in all three cases.)
> > >
> > > Here is the console session:
> > >
> > > > R.version.string # XP
> > > [1] "Version 2.3.1 Patched (2006-06-04 r38279)"
> > > >
> > > > search()
> > > [1] ".GlobalEnv"        "package:methods"   "package:stats"
> > > [4] "package:graphics"  "package:grDevices" "package:utils"
> > > [7] "package:datasets"  "Autoloads"         "package:base"
> > > > as.Date(1)
> > > Error in as.Date.default(1) : do not know how to convert '1' to class "Date"
> > > > zoo:::as.Date.numeric
> > > function (x, ...)
> > > structure(floor(x + 0.001), class = "Date")
> > > <environment: namespace:zoo>
> > > >
> > > > library(zoo)
> > > > search()
> > >  [1] ".GlobalEnv"        "package:zoo"       "package:methods"
> > >  [4] "package:stats"     "package:graphics"  "package:grDevices"
> > >  [7] "package:utils"     "package:datasets"  "Autoloads"
> > > [10] "package:base"
> > > > as.Date(1)
> > > [1] "1970-01-02"
> > > > zoo:::as.Date.numeric
> > > function (x, ...)
> > > structure(floor(x + 0.001), class = "Date")
> > > <environment: namespace:zoo>
> > > >
> > > > detach()
> > > > search()
> > > [1] ".GlobalEnv"        "package:methods"   "package:stats"
> > > [4] "package:graphics"  "package:grDevices" "package:utils"
> > > [7] "package:datasets"  "Autoloads"         "package:base"
> > > > as.Date(1)
> > > [1] "1970-01-02"
> > > > zoo:::as.Date.numeric
> > > function (x, ...)
> > > structure(floor(x + 0.001), class = "Date")
> > > <environment: namespace:zoo>
> > > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >
> >
> >
> > --
> > Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-----------------------------------------------------------------------
This message and its attachments are strictly confidential. ...{{dropped}}


From ripley at stats.ox.ac.uk  Wed Jul 12 19:24:36 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Jul 2006 18:24:36 +0100 (BST)
Subject: [R] detach
In-Reply-To: <1152723995.5118.4.camel@seurat.iarc.fr>
References: <971536df0607111104j5cf964e9oa39d29d8e6ff8a3e@mail.gmail.com>
	<66f3bd910607120614q12ea1662ye8aed4974b7bbcc8@mail.gmail.com>
	<971536df0607120621j42c311b7o18c585702e4022d3@mail.gmail.com>
	<1152723995.5118.4.camel@seurat.iarc.fr>
Message-ID: <Pine.LNX.4.64.0607121822370.11258@gannet.stats.ox.ac.uk>

On Wed, 12 Jul 2006, Martyn Plummer wrote:

> My guess is that even though the zoo namespace is unloaded. The S3
> method as.Date.numeric is still registered, and the name space will be
> reloaded whenever it is dispatched.
> > library(zoo)
> > loadedNamespaces()
> [1] "base"      "graphics"  "grDevices" "methods"   "stats"     "utils"
> [7] "zoo"
> > unloadNamespace("zoo")
> <environment: namespace:zoo>
> > loadedNamespaces() #Gone
> [1] "base"      "graphics"  "grDevices" "methods"   "stats"     "utils"
> > as.Date(1)
> [1] "1970-01-02"
> > loadedNamespaces() #Back again!
> [1] "base"      "graphics"  "grDevices" "methods"   "stats"     "utils"
> [7] "zoo"
> 
> So the question is whether there is a way to unregister an S3 method, so
> that you can restore the method dispatching behaviour prior to the
> package being loaded. I don't know the answer, but I'm guessing it's no.

It is no.  Namespaces don't work that way: one would have to keep a stack 
of past registered methods to be able to back out a registered method.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mschwartz at mn.rr.com  Wed Jul 12 20:14:22 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 12 Jul 2006 13:14:22 -0500
Subject: [R] Keep value lables with data frame manipulation
In-Reply-To: <2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>
References: <2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>
Message-ID: <1152728062.4662.25.camel@localhost.localdomain>

On Wed, 2006-07-12 at 17:41 +0100, Jol, Arne wrote:
> Dear R,
> 
> I import data from spss into a R data.frame. On this rawdata I do some
> data processing (selection of observations, normalization, recoding of
> variables etc..). The result is stored in a new data.frame, however, in
> this new data.frame the value labels are lost.
> 
> Example of what I do in code:
> 
> # read raw data from spss
> rawdata <- read.spss("./data/T50937.SAV",
> 	use.value.labels=FALSE,to.data.frame=TRUE)
> 
> # select the observations that we need
> diarydata <- rawdata[rawdata$D22==2 | rawdata$D22==3 | rawdata$D22==17 |
> rawdata$D22==18 | rawdata$D22==20 | rawdata$D22==22 |
>  			rawdata$D22==24 | rawdata$D22==33,]
> 
> The result is that rawdata$D22 has value labels and that diarydata$D22
> is numeric without value labels.
> 
> Question: How can I prevent this from happening?
> 
> Thanks in advance!
> Groeten,
> Arne

Two things:

1. With respect to your subsetting, your lengthy code can be replaced
with the following:

  diarydata <- subset(rawdata, D22 %in% c(2, 3, 17, 18, 20, 22, 24, 33))

See ?subset and ?"%in%" for more information.


2. With respect to keeping the label related attributes, the
'value.labels' attribute and the 'variable.labels' attribute will not by
default survive the use of "[".data.frame in R (see ?Extract
and ?"[.data.frame").

On the other hand, based upon my review of ?read.spss, the SPSS value
labels should be converted to the factor levels of the respective
columns when 'use.value.labels = TRUE' and these would survive a
subsetting.

If you want to consider a solution to the attribute subsetting issue,
you might want to review the following post by Gabor Grothendieck in
May, which provides a possible solution:

  https://stat.ethz.ch/pipermail/r-help/2006-May/106308.html

and this post by me, for an explanation of what is happening in Gabor's
solution:

  https://stat.ethz.ch/pipermail/r-help/2006-May/106351.html

HTH,

Marc Schwartz


From ggrothendieck at gmail.com  Wed Jul 12 20:48:10 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 12 Jul 2006 14:48:10 -0400
Subject: [R] detach
In-Reply-To: <Pine.LNX.4.64.0607121822370.11258@gannet.stats.ox.ac.uk>
References: <971536df0607111104j5cf964e9oa39d29d8e6ff8a3e@mail.gmail.com>
	<66f3bd910607120614q12ea1662ye8aed4974b7bbcc8@mail.gmail.com>
	<971536df0607120621j42c311b7o18c585702e4022d3@mail.gmail.com>
	<1152723995.5118.4.camel@seurat.iarc.fr>
	<Pine.LNX.4.64.0607121822370.11258@gannet.stats.ox.ac.uk>
Message-ID: <971536df0607121148t1e8ca475ic6c10b5286870310@mail.gmail.com>

Looks like one can do the following.  Don't know if it would cause
problems elsewhere.  Note that after removing as.Date.numeric
from the S3 Methods Table as.Date.numeric was no longer dispatched.

> as.Date(1)
Error in as.Date.default(1) : do not know how to convert '1' to class "Date"
> library(zoo)
> as.Date(1)
[1] "1970-01-02"
> unloadNamespace("zoo")
<environment: namespace:zoo>
> rm(as.Date.numeric, envir = .__S3MethodsTable__.)
> as.Date(1)
Error in as.Date.default(1) : do not know how to convert '1' to class "Date"

On 7/12/06, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Wed, 12 Jul 2006, Martyn Plummer wrote:
>
> > My guess is that even though the zoo namespace is unloaded. The S3
> > method as.Date.numeric is still registered, and the name space will be
> > reloaded whenever it is dispatched.
> > > library(zoo)
> > > loadedNamespaces()
> > [1] "base"      "graphics"  "grDevices" "methods"   "stats"     "utils"
> > [7] "zoo"
> > > unloadNamespace("zoo")
> > <environment: namespace:zoo>
> > > loadedNamespaces() #Gone
> > [1] "base"      "graphics"  "grDevices" "methods"   "stats"     "utils"
> > > as.Date(1)
> > [1] "1970-01-02"
> > > loadedNamespaces() #Back again!
> > [1] "base"      "graphics"  "grDevices" "methods"   "stats"     "utils"
> > [7] "zoo"
> >
> > So the question is whether there is a way to unregister an S3 method, so
> > that you can restore the method dispatching behaviour prior to the
> > package being loaded. I don't know the answer, but I'm guessing it's no.
>
> It is no.  Namespaces don't work that way: one would have to keep a stack
> of past registered methods to be able to back out a registered method.
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From Charles.Annis at StatisticalEngineering.com  Wed Jul 12 20:46:17 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Wed, 12 Jul 2006 14:46:17 -0400
Subject: [R] saving intermediate graphics
Message-ID: <009801c6a5e3$75dc02f0$6600a8c0@DD4XFW31>

Greetings, R-friends:

I'm using R 2.3.1 on a DELL box with 2 gig, running WindowsXP Pro.

I am making what is admittedly a very busy plot composed of qq plots with
superimposed density() plots.  I need to show everything and the result is
messy, but informative.

If I produce the basic qq plot, and then add incrementally to it, saving the
graphic at each step, the sequence does make the entire exercise
understandable.  (Honest!)

I can do this easily by clicking on the plot and saving it to PowerPoint as
a metafile.

I would like to automate the process.  I know how to produce win.metafiles,
but closing the file with dev.off() to produce the first graphic means that
I can't add to that and produce the subsequent graphic and file.

Is there a way to do this without generating the entire plot from the
beginning for each graphic?

Thanks


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:? 614-455-3265
http://www.StatisticalEngineering.com
?


From ripley at stats.ox.ac.uk  Wed Jul 12 21:01:24 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Jul 2006 20:01:24 +0100 (BST)
Subject: [R] saving intermediate graphics
In-Reply-To: <009801c6a5e3$75dc02f0$6600a8c0@DD4XFW31>
References: <009801c6a5e3$75dc02f0$6600a8c0@DD4XFW31>
Message-ID: <Pine.LNX.4.64.0607122000290.17904@gannet.stats.ox.ac.uk>

On Wed, 12 Jul 2006, Charles Annis, P.E. wrote:

> Greetings, R-friends:
> 
> I'm using R 2.3.1 on a DELL box with 2 gig, running WindowsXP Pro.
> 
> I am making what is admittedly a very busy plot composed of qq plots with
> superimposed density() plots.  I need to show everything and the result is
> messy, but informative.
> 
> If I produce the basic qq plot, and then add incrementally to it, saving the
> graphic at each step, the sequence does make the entire exercise
> understandable.  (Honest!)
> 
> I can do this easily by clicking on the plot and saving it to PowerPoint as
> a metafile.
> 
> I would like to automate the process.  I know how to produce win.metafiles,
> but closing the file with dev.off() to produce the first graphic means that
> I can't add to that and produce the subsequent graphic and file.
> 
> Is there a way to do this without generating the entire plot from the
> beginning for each graphic?

Have you tried savePlot()?  It should work incrementally.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Greg.Snow at intermountainmail.org  Wed Jul 12 21:20:02 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 12 Jul 2006 13:20:02 -0600
Subject: [R] R newbie: logical subsets
Message-ID: <07E228A5BE53C24CAD490193A7381BBB4B77D5@LP-EXCHVS07.CO.IHC.COM>

Gabor, your solution does not take into account the groups.  How about
something like:

iris2 <- iris
iris2$m <- ave(iris2$Sepal.Length, iris2$Species)
iris2$s <- ave(iris2$Sepal.Length, iris2$Species, FUN=sd)

iris2 <- transform(iris2, z= (Sepal.Length-m)/s)

iris2.2 <- subset(iris2, abs(z) < 2)

aggregate(iris2.2, list(iris2.2$Species), FUN=mean)



-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor
Grothendieck
Sent: Tuesday, July 11, 2006 1:06 PM
To: Joshua Tokle
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] R newbie: logical subsets

Try this, using the built in anscombe data set:

anscombe[!rowSums(abs(scale(anscombe)) > 2),]



On 7/11/06, Joshua Tokle <jtokle at math.washington.edu> wrote:
> Hello!  I'm a newcomer to R hoping to replace some convoluted database

> code with an R script.  Unfortunately, I haven't been able to figure 
> out how to implement the following logic.
>
> Essentially, we have a database of transactions that are coded with a 
> geographic locale and a type.  These are being loaded into a 
> data.frame with named variables city, type, and price.  E.g., 
> trans$city and all that.
>
> We want to calculate mean prices by city and type, AFTER excluding 
> outliers.  That is, we want to calculate the mean price in 3 steps:
>
> 1. calculate a mean and standard deviation by city and type over all 
> transactions 2. create a subset of the original data frame, excluding 
> transactions that differ from the relevant mean by more than 2 
> standard deviations 3. calculate a final mean by city and type based 
> on this subset.
>
> I'm stuck on step 2.  I would like to do something like the following:
>
> fs <- list(factor(trans$city), factor(trans$type)) means <- 
> tapply(trans$price, fs, mean) stdevs <- tapply(trans$price, fs, sd)
>
> filter <- abs(trans$price - means[trans$city, trans$type]) <
>             2*stdevs[trans$city, trans$type]
>
> sub <- subset(trans, filter)
>
> The above code doesn't work.  What's the correct way to do this?
>
> Thanks,
> Josh
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From Charles.Annis at StatisticalEngineering.com  Wed Jul 12 21:22:10 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Wed, 12 Jul 2006 15:22:10 -0400
Subject: [R] saving intermediate graphics
In-Reply-To: <Pine.LNX.4.64.0607122000290.17904@gannet.stats.ox.ac.uk>
Message-ID: <00a201c6a5e8$78eef600$6600a8c0@DD4XFW31>

Wow!  Exactly what I needed.

Thank you, Professor Ripley!

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof Brian Ripley
Sent: Wednesday, July 12, 2006 3:01 PM
To: Charles Annis, P.E.
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] saving intermediate graphics

On Wed, 12 Jul 2006, Charles Annis, P.E. wrote:

> Greetings, R-friends:
> 
> I'm using R 2.3.1 on a DELL box with 2 gig, running WindowsXP Pro.
> 
> I am making what is admittedly a very busy plot composed of qq plots with
> superimposed density() plots.  I need to show everything and the result is
> messy, but informative.
> 
> If I produce the basic qq plot, and then add incrementally to it, saving
the
> graphic at each step, the sequence does make the entire exercise
> understandable.  (Honest!)
> 
> I can do this easily by clicking on the plot and saving it to PowerPoint
as
> a metafile.
> 
> I would like to automate the process.  I know how to produce
win.metafiles,
> but closing the file with dev.off() to produce the first graphic means
that
> I can't add to that and produce the subsequent graphic and file.
> 
> Is there a way to do this without generating the entire plot from the
> beginning for each graphic?

Have you tried savePlot()?  It should work incrementally.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From Matthew.Findley at ch2m.com  Wed Jul 12 23:13:42 2006
From: Matthew.Findley at ch2m.com (Matthew.Findley at ch2m.com)
Date: Wed, 12 Jul 2006 15:13:42 -0600
Subject: [R] shapiro.test() output
Message-ID: <12E238925F448B48BC28E96479F125DE0138B8A6@CATSKILL.amr.ch2m.com>

R Users:

My question is probably more about elementary statistics than the
mechanics of using R, but I've been dabbling in R (version 2.2.0) and
used it recently  to test some data . 

I have a relatively small set of observations (n = 12) of arsenic
concentrations in background groundwater and wanted to test my
assumption of normality.  I used the Shapiro-Wilk test (by calling
shapiro.test() in R) and I'm not sure how to interpret the output.
Here's the input/output from the R console:

	>As = c(13, 17, 23, 9.5, 20, 15, 11, 17, 21, 14, 22, 13)
	>shapiro.test(As)

      	  Shapiro-Wilk normality test

	data:  As 
	W = 0.9513, p-value = 0.6555

How do I interpret this?  I understand, from poking around the internet,
that the higher the W statistic the "more normal" the data.

What is the null hypothesis - that the data is normally distributed?  

What does the p-value tell me?  65.55% chance of what - getting
W-statistic greater than or equal to 0.9513 (I picked this up from the
Dalgaard book, Introductory Statistics with R, but its not really
sinking in with respect to how it applies to a Shipiro Wilk test).? 

The method description - retrieved using ?shapiro.test() - is a bit
light on details.

Thanks much.

-------------------------------------
Matthew C. Findley, CPSSc
Environmental Scientist
 
CH2M HILL
mfindley at ch2m.com


From steve.miller at jhu.edu  Wed Jul 12 23:40:27 2006
From: steve.miller at jhu.edu (Steve Miller)
Date: Wed, 12 Jul 2006 16:40:27 -0500
Subject: [R] Is it possible to only read a subset by read.table ?
In-Reply-To: <8d5a36350607120903m1213f0e1q66597076051ba530@mail.gmail.com>
Message-ID: <200607122138.k6CLcsw9027535@hypatia.math.ethz.ch>

You could also use Perl/Python/Ruby to pipe the data to R, e.g. msci <-
read.table(pipe("python /steve/python/msciintl.py"),sep=",",header=T,
as.is=T)

This is a very reasonable way to exploit the data munging capabilities of
the agile languages. Of course, better still is to query the data into R
from a relational database.

Steve Miller

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
Sent: Wednesday, July 12, 2006 11:04 AM
To: D.Vonka at UvT.nl
Cc: r-help
Subject: Re: [R] Is it possible to only read a subset by read.table ?

It's possible and straightforward (just don't use R). IMHO the GNU
Core Utilities
http://www.gnu.org/software/coreutils/
plus a few other tools such as sed, awk, grep etc are much more
appropriate than R for processing massive text files. (Get a good book
about UNIX shell scripting. On Windows you can use Services For Unix
or Cygwin.)

Also, here's an example that you could adapt to print the males from
your data set to a separate file, which you could then import in R.
#---print specific lines to another file---
suffix=_JAN06
for F in `ls *data*`
do
  echo $F
  sed -n -e '/2006-01-[0-9][0-9]/p' $F > ${F}${suffix}
done


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Vonka
> Sent: Wednesday, July 12, 2006 8:37 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Is it possible to only read a subset by read.table ?
>
> Hello,
>
> is it possible to do something like
>
> DATA <- read.table(file="blabla.dat",subset=(sex=="male")),
>
> i.e. make R read only a subset of a csv file ?
> I think it would be useful in case of very big datasets,
> but I can't find such a feature.
>
> Thanks for an answer,
> David Vonka
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From p.dalgaard at biostat.ku.dk  Thu Jul 13 00:01:07 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Jul 2006 00:01:07 +0200
Subject: [R] shapiro.test() output
In-Reply-To: <12E238925F448B48BC28E96479F125DE0138B8A6@CATSKILL.amr.ch2m.com>
References: <12E238925F448B48BC28E96479F125DE0138B8A6@CATSKILL.amr.ch2m.com>
Message-ID: <x2k66iiiu4.fsf@turmalin.kubism.ku.dk>

<Matthew.Findley at ch2m.com> writes:

> R Users:
> 
> My question is probably more about elementary statistics than the
> mechanics of using R, but I've been dabbling in R (version 2.2.0) and
> used it recently  to test some data . 
> 
> I have a relatively small set of observations (n = 12) of arsenic
> concentrations in background groundwater and wanted to test my
> assumption of normality.  I used the Shapiro-Wilk test (by calling
> shapiro.test() in R) and I'm not sure how to interpret the output.
> Here's the input/output from the R console:
> 
> 	>As = c(13, 17, 23, 9.5, 20, 15, 11, 17, 21, 14, 22, 13)
> 	>shapiro.test(As)
> 
>       	  Shapiro-Wilk normality test
> 
> 	data:  As 
> 	W = 0.9513, p-value = 0.6555
> 
> How do I interpret this?  I understand, from poking around the internet,
> that the higher the W statistic the "more normal" the data.
> 
> What is the null hypothesis - that the data is normally distributed?  

Yup.

> What does the p-value tell me?  65.55% chance of what - getting
> W-statistic greater than or equal to 0.9513 (I picked this up from the
> Dalgaard book, Introductory Statistics with R, but its not really
> sinking in with respect to how it applies to a Shipiro Wilk test).? 

*Smaller* or equal - W=1.0 is the "perfect fit". The W statistic is
 pretty much the Pearson correlation applied to the curve drawn by
 qqnorm(). (The exact definition of what goes on the x axis differs
 slightly, I believe.) 

A low p-value would indicate that the W is too extreme to be explained
by chance variation - i.e. evidence against normal distribution.
In the present case you have no evidence against normal distribution
(beware that this is not evidence _for_ normality).

(Personally, I'm not too happy about these normality tests. They tend
to lack power in small samples and in large samples they often reject
distributions which  are perfectly adequate for normal-theory
analysis. Learning to evaluate a QQ plot seems a better idea.) 

 
> The method description - retrieved using ?shapiro.test() - is a bit
> light on details.

There are references therein, though...

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From sundar.dorai-raj at pdf.com  Thu Jul 13 00:28:49 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 12 Jul 2006 15:28:49 -0700
Subject: [R] RODBC, missing values, and Excel
Message-ID: <44B577A1.9080904@pdf.com>

Hi, all,

I'm trying to use RODBC to read data from Excel. However, I'm having 
trouble converting missing values to NA and rather perplexed by the 
output. Below illustrates my problem:

## DATA - copy to Excel and save as "tmp.xls"
## tmp.xls!Sheet1
x
0.11
0.11
na
na
na
0.11

## tmp.xls!Sheet2
x
0.11
0.11
na
na
na
na
0.11

## R Code
read.xls <- function(file, sheet = "Sheet1", ...) {
   require(RODBC)
   channel <- odbcConnectExcel(file)
   sheet <- sprintf("select * from `%s$`", sheet)
   x <- sqlQuery(channel, sheet, ...)
   odbcClose(channel)
   x
}

read.xls("./tmp.xls", "Sheet1", na.strings = "na")
## works as expected
#     x
#1 0.11
#2 0.11
#3   NA
#4   NA
#5   NA
#6 0.11

read.xls("./tmp.xls", "Sheet2", na.strings = "na")
## Huh? What happened?
#   x
#1 NA
#2 NA
#3 NA
#4 NA
#5 NA
#6 NA
#7 NA

 > sessionInfo()
Version 2.3.1 (2006-06-01)
i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
   RODBC
"1.1-7"


From simonlin at duke.edu  Thu Jul 13 01:36:45 2006
From: simonlin at duke.edu (Simon Lin)
Date: Wed, 12 Jul 2006 18:36:45 -0500
Subject: [R] adaptive knot placement for splines
Message-ID: <004101c6a60c$099054b0$c7987ca5@lurie.northwestern.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060712/b3da025e/attachment.pl 

From mwgrant2001 at yahoo.com  Thu Jul 13 01:51:21 2006
From: mwgrant2001 at yahoo.com (Michael Grant)
Date: Wed, 12 Jul 2006 16:51:21 -0700 (PDT)
Subject: [R] shapiro.test() output
In-Reply-To: <x2k66iiiu4.fsf@turmalin.kubism.ku.dk>
Message-ID: <20060712235121.19022.qmail@web52015.mail.yahoo.com>

Matthew,

You may find the following documents useful if your venture into environmental
statistics is serious. 

First, the 92 EPA Addendum on GW statistics--links at
http://www.epa.gov/correctiveaction/resource/guidance/sitechar/gwstats/gwstats.htm

The second is Helsel's book at the USGS

http://pubs.usgs.gov/twri/twri4a3/

Both documents have good discussions on normality tests for GW data including
probability plot correlation coefficients and variations in the (x) plotting
position--Blom, Cunane, etc.

Helsel is a good read 1.) his writing is so clear in his writing, 2.) he gets
into nonparametric approaches in so many areas of GW stats, and 3.) the
typography is nice--the book just a pleasant experience all around. Just be
advised this is only the beginning...

Oh, yes. It ain't safe to just dabble with environmental (contaminant)data--it
is too messy. Go whole hog or pass it up.

Best regards,
Michael Grant (works for the competition :O))

--- Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:

> <Matthew.Findley at ch2m.com> writes:
> 
> > R Users:
> > 
> > My question is probably more about elementary statistics than the
> > mechanics of using R, but I've been dabbling in R (version 2.2.0) and
> > used it recently  to test some data . 
> > 
> > I have a relatively small set of observations (n = 12) of arsenic
> > concentrations in background groundwater and wanted to test my
> > assumption of normality.  I used the Shapiro-Wilk test (by calling
> > shapiro.test() in R) and I'm not sure how to interpret the output.
> > Here's the input/output from the R console:
> > 
> > 	>As = c(13, 17, 23, 9.5, 20, 15, 11, 17, 21, 14, 22, 13)
> > 	>shapiro.test(As)
> > 
> >       	  Shapiro-Wilk normality test
> > 
> > 	data:  As 
> > 	W = 0.9513, p-value = 0.6555
> > 
> > How do I interpret this?  I understand, from poking around the internet,
> > that the higher the W statistic the "more normal" the data.
> > 
> > What is the null hypothesis - that the data is normally distributed?  
> 
> Yup.
> 
> > What does the p-value tell me?  65.55% chance of what - getting
> > W-statistic greater than or equal to 0.9513 (I picked this up from the
> > Dalgaard book, Introductory Statistics with R, but its not really
> > sinking in with respect to how it applies to a Shipiro Wilk test).? 
> 
> *Smaller* or equal - W=1.0 is the "perfect fit". The W statistic is
>  pretty much the Pearson correlation applied to the curve drawn by
>  qqnorm(). (The exact definition of what goes on the x axis differs
>  slightly, I believe.) 
> 
> A low p-value would indicate that the W is too extreme to be explained
> by chance variation - i.e. evidence against normal distribution.
> In the present case you have no evidence against normal distribution
> (beware that this is not evidence _for_ normality).
> 
> (Personally, I'm not too happy about these normality tests. They tend
> to lack power in small samples and in large samples they often reject
> distributions which  are perfectly adequate for normal-theory
> analysis. Learning to evaluate a QQ plot seems a better idea.) 
> 
>  
> > The method description - retrieved using ?shapiro.test() - is a bit
> > light on details.
> 
> There are references therein, though...
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From andy_liaw at merck.com  Thu Jul 13 02:56:02 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 12 Jul 2006 20:56:02 -0400
Subject: [R] adaptive knot placement for splines
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02884FD7@usctmx1106.merck.com>

Not TURBO, but BRUTO (bruto() in the mda package) proposed in the discussion
of that paper, or even MARS (mars(), also in mda).

Andy 

From: Simon Lin
> 
> Are there any R implementation of adaptive knot placement? No 
> need to be fancy. Something like stepwise selection by 
> Friedman and Silverman (1989)?
> 
> The implementation in polyspline is too specific for 
> lognormal distributions. 
> 
> Thanks!
> 
> Simon
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>


From ggrothendieck at gmail.com  Thu Jul 13 02:59:25 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 12 Jul 2006 20:59:25 -0400
Subject: [R] RODBC, missing values, and Excel
In-Reply-To: <44B577A1.9080904@pdf.com>
References: <44B577A1.9080904@pdf.com>
Message-ID: <971536df0607121759x56dc7c14k38f14e29696fe994@mail.gmail.com>

I also got a strange result too (I renamed it sdr.read.xls
to distinguish it from read.xls in gdata and noticed that a
space got into my na's somehow so I used "na " for my
na.strings:

> sdr.read.xls("/test.xls", "Sheet2", na.strings = "na ")
     x
1 <NA>
2 <NA>
3   na
4   na
5   na
6   na
7 <NA>

I had more success using read.xls in the gdata package.
Note that we need to install perl first if not already present:

> library(gdata)  # for read.xls
> read.xls("/test.xls", 2, na.strings = "na ")
     x
1 0.11
2 0.11
3   NA
4   NA
5   NA
6   NA
7 0.11

> R.version.string # XP
[1] "Version 2.3.1 Patched (2006-06-04 r38279)"
> packageDescription("gdata")$Version
[1] "2.1.2"
> packageDescription("RODBC")$Version
[1] "1.1-7"


On 7/12/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> Hi, all,
>
> I'm trying to use RODBC to read data from Excel. However, I'm having
> trouble converting missing values to NA and rather perplexed by the
> output. Below illustrates my problem:
>
> ## DATA - copy to Excel and save as "tmp.xls"
> ## tmp.xls!Sheet1
> x
> 0.11
> 0.11
> na
> na
> na
> 0.11
>
> ## tmp.xls!Sheet2
> x
> 0.11
> 0.11
> na
> na
> na
> na
> 0.11
>
> ## R Code
> read.xls <- function(file, sheet = "Sheet1", ...) {
>   require(RODBC)
>   channel <- odbcConnectExcel(file)
>   sheet <- sprintf("select * from `%s$`", sheet)
>   x <- sqlQuery(channel, sheet, ...)
>   odbcClose(channel)
>   x
> }
>
> read.xls("./tmp.xls", "Sheet1", na.strings = "na")
> ## works as expected
> #     x
> #1 0.11
> #2 0.11
> #3   NA
> #4   NA
> #5   NA
> #6 0.11
>
> read.xls("./tmp.xls", "Sheet2", na.strings = "na")
> ## Huh? What happened?
> #   x
> #1 NA
> #2 NA
> #3 NA
> #4 NA
> #5 NA
> #6 NA
> #7 NA
>
>  > sessionInfo()
> Version 2.3.1 (2006-06-01)
> i386-pc-mingw32
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
>
> other attached packages:
>   RODBC
> "1.1-7"
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jz7 at duke.edu  Thu Jul 13 03:28:26 2006
From: jz7 at duke.edu (jz7 at duke.edu)
Date: Wed, 12 Jul 2006 21:28:26 -0400 (EDT)
Subject: [R] question about cross-validation for mutivariate linear
	regression
Message-ID: <Pine.GSO.4.58.0607122121010.9724@godzilla.acpub.duke.edu>

Dear all,

I am trying to do a standard mutivariate linear regression for my training
set, and want to perform leave-one-out cross-validation to see how well
the model is. I found several CV in R, such as cv.lm in DAAG, cv.glm in
boot, and crossval in bootstrap. Which one should I use? And how can I get
the squared correlation coefficient (q^2) for LOO-CV? I have tried the
cv.lm in DAAG, but it seems that there is no option do give that output.

Thanks so much!


From sundar.dorai-raj at pdf.com  Thu Jul 13 03:52:07 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 12 Jul 2006 18:52:07 -0700
Subject: [R] RODBC, missing values, and Excel
In-Reply-To: <971536df0607121759x56dc7c14k38f14e29696fe994@mail.gmail.com>
References: <44B577A1.9080904@pdf.com>
	<971536df0607121759x56dc7c14k38f14e29696fe994@mail.gmail.com>
Message-ID: <44B5A747.60306@pdf.com>

Hi, Gabor,

Thanks for the reply. Perhaps Prof. Ripley will enlighten us as he is 
the RODBC maintainer.

Unfortunately, gdata::read.xls will not work for me (at least I don't 
think it will) because I need to refer to each worksheet by name and not 
by number. For example, I need extract data from "Sheet1" and not simply 
the first sheet.

Thanks,

--sundar

Gabor Grothendieck wrote:
> I also got a strange result too (I renamed it sdr.read.xls
> to distinguish it from read.xls in gdata and noticed that a
> space got into my na's somehow so I used "na " for my
> na.strings:
> 
> 
>>sdr.read.xls("/test.xls", "Sheet2", na.strings = "na ")
> 
>      x
> 1 <NA>
> 2 <NA>
> 3   na
> 4   na
> 5   na
> 6   na
> 7 <NA>
> 
> I had more success using read.xls in the gdata package.
> Note that we need to install perl first if not already present:
> 
> 
>>library(gdata)  # for read.xls
>>read.xls("/test.xls", 2, na.strings = "na ")
> 
>      x
> 1 0.11
> 2 0.11
> 3   NA
> 4   NA
> 5   NA
> 6   NA
> 7 0.11
> 
> 
>>R.version.string # XP
> 
> [1] "Version 2.3.1 Patched (2006-06-04 r38279)"
> 
>>packageDescription("gdata")$Version
> 
> [1] "2.1.2"
> 
>>packageDescription("RODBC")$Version
> 
> [1] "1.1-7"
> 
> 
> On 7/12/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> 
>>Hi, all,
>>
>>I'm trying to use RODBC to read data from Excel. However, I'm having
>>trouble converting missing values to NA and rather perplexed by the
>>output. Below illustrates my problem:
>>
>>## DATA - copy to Excel and save as "tmp.xls"
>>## tmp.xls!Sheet1
>>x
>>0.11
>>0.11
>>na
>>na
>>na
>>0.11
>>
>>## tmp.xls!Sheet2
>>x
>>0.11
>>0.11
>>na
>>na
>>na
>>na
>>0.11
>>
>>## R Code
>>read.xls <- function(file, sheet = "Sheet1", ...) {
>>  require(RODBC)
>>  channel <- odbcConnectExcel(file)
>>  sheet <- sprintf("select * from `%s$`", sheet)
>>  x <- sqlQuery(channel, sheet, ...)
>>  odbcClose(channel)
>>  x
>>}
>>
>>read.xls("./tmp.xls", "Sheet1", na.strings = "na")
>>## works as expected
>>#     x
>>#1 0.11
>>#2 0.11
>>#3   NA
>>#4   NA
>>#5   NA
>>#6 0.11
>>
>>read.xls("./tmp.xls", "Sheet2", na.strings = "na")
>>## Huh? What happened?
>>#   x
>>#1 NA
>>#2 NA
>>#3 NA
>>#4 NA
>>#5 NA
>>#6 NA
>>#7 NA
>>
>> > sessionInfo()
>>Version 2.3.1 (2006-06-01)
>>i386-pc-mingw32
>>
>>attached base packages:
>>[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
>>[7] "base"
>>
>>other attached packages:
>>  RODBC
>>"1.1-7"
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ggrothendieck at gmail.com  Thu Jul 13 04:24:12 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 12 Jul 2006 22:24:12 -0400
Subject: [R] RODBC, missing values, and Excel
In-Reply-To: <44B5A747.60306@pdf.com>
References: <44B577A1.9080904@pdf.com>
	<971536df0607121759x56dc7c14k38f14e29696fe994@mail.gmail.com>
	<44B5A747.60306@pdf.com>
Message-ID: <971536df0607121924m495802devbed554f664d270f6@mail.gmail.com>

Would it be good enough to just read all the sheets in?

The perl program can do that and although the read.xls R function does not
interface to that aspect of its functionality its not that difficult to access
it yourself.  Assume your excel file is in \test.xls .  Just
switch to that folder.  paste together a command to run the perl
program, run it, get a list of the file names it produced and read them in:

library(gdata)
setwd("/")
cmd <- paste("perl", system.file("perl/xls2csv.pl", package = "gdata"), "test")
system(cmd)
ff <- list.files(patt = "test_Sheet.*.csv")
sapply(ff, read.csv, na.strings = "na ", simplify = FALSE)


On 7/12/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> Hi, Gabor,
>
> Thanks for the reply. Perhaps Prof. Ripley will enlighten us as he is
> the RODBC maintainer.
>
> Unfortunately, gdata::read.xls will not work for me (at least I don't
> think it will) because I need to refer to each worksheet by name and not
> by number. For example, I need extract data from "Sheet1" and not simply
> the first sheet.
>
> Thanks,
>
> --sundar
>
> Gabor Grothendieck wrote:
> > I also got a strange result too (I renamed it sdr.read.xls
> > to distinguish it from read.xls in gdata and noticed that a
> > space got into my na's somehow so I used "na " for my
> > na.strings:
> >
> >
> >>sdr.read.xls("/test.xls", "Sheet2", na.strings = "na ")
> >
> >      x
> > 1 <NA>
> > 2 <NA>
> > 3   na
> > 4   na
> > 5   na
> > 6   na
> > 7 <NA>
> >
> > I had more success using read.xls in the gdata package.
> > Note that we need to install perl first if not already present:
> >
> >
> >>library(gdata)  # for read.xls
> >>read.xls("/test.xls", 2, na.strings = "na ")
> >
> >      x
> > 1 0.11
> > 2 0.11
> > 3   NA
> > 4   NA
> > 5   NA
> > 6   NA
> > 7 0.11
> >
> >
> >>R.version.string # XP
> >
> > [1] "Version 2.3.1 Patched (2006-06-04 r38279)"
> >
> >>packageDescription("gdata")$Version
> >
> > [1] "2.1.2"
> >
> >>packageDescription("RODBC")$Version
> >
> > [1] "1.1-7"
> >
> >
> > On 7/12/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> >
> >>Hi, all,
> >>
> >>I'm trying to use RODBC to read data from Excel. However, I'm having
> >>trouble converting missing values to NA and rather perplexed by the
> >>output. Below illustrates my problem:
> >>
> >>## DATA - copy to Excel and save as "tmp.xls"
> >>## tmp.xls!Sheet1
> >>x
> >>0.11
> >>0.11
> >>na
> >>na
> >>na
> >>0.11
> >>
> >>## tmp.xls!Sheet2
> >>x
> >>0.11
> >>0.11
> >>na
> >>na
> >>na
> >>na
> >>0.11
> >>
> >>## R Code
> >>read.xls <- function(file, sheet = "Sheet1", ...) {
> >>  require(RODBC)
> >>  channel <- odbcConnectExcel(file)
> >>  sheet <- sprintf("select * from `%s$`", sheet)
> >>  x <- sqlQuery(channel, sheet, ...)
> >>  odbcClose(channel)
> >>  x
> >>}
> >>
> >>read.xls("./tmp.xls", "Sheet1", na.strings = "na")
> >>## works as expected
> >>#     x
> >>#1 0.11
> >>#2 0.11
> >>#3   NA
> >>#4   NA
> >>#5   NA
> >>#6 0.11
> >>
> >>read.xls("./tmp.xls", "Sheet2", na.strings = "na")
> >>## Huh? What happened?
> >>#   x
> >>#1 NA
> >>#2 NA
> >>#3 NA
> >>#4 NA
> >>#5 NA
> >>#6 NA
> >>#7 NA
> >>
> >> > sessionInfo()
> >>Version 2.3.1 (2006-06-01)
> >>i386-pc-mingw32
> >>
> >>attached base packages:
> >>[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> >>[7] "base"
> >>
> >>other attached packages:
> >>  RODBC
> >>"1.1-7"
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Thu Jul 13 05:04:55 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 12 Jul 2006 23:04:55 -0400
Subject: [R] RODBC, missing values, and Excel
In-Reply-To: <971536df0607121924m495802devbed554f664d270f6@mail.gmail.com>
References: <44B577A1.9080904@pdf.com>
	<971536df0607121759x56dc7c14k38f14e29696fe994@mail.gmail.com>
	<44B5A747.60306@pdf.com>
	<971536df0607121924m495802devbed554f664d270f6@mail.gmail.com>
Message-ID: <971536df0607122004x4827e0f6pf4eb962cec10b5cd@mail.gmail.com>

In thinking about this some more I have a better idea.  Use rcom (or
RDCOMClient)
to get a list of the sheet names and then use that to determine which sheet you
need.  Then use read.xls to get it like this assuming that the Excel
file and path are C:\test.xls and that one of the sheets in that spreadsheet
is xyz.  In my version the na.strings had a space at the end so you may
need to change the na.strings= setting:

library(rcom)
xls <- "C:\\test.xls"
oxl <- comCreateObject('Excel.Application')
comSetProperty(oxl, "Visible", TRUE)  # this line optional
owb <- comGetProperty(oxl, "Workbooks")
ob <- comInvoke(owb, "Open", xls)
osheets <- comGetProperty(ob, "Worksheets")
n <- comGetProperty(osheets, "Count")
ithSheetName <- function(i)
	comGetProperty(comGetProperty(osheets, "Item", i), "Name")
sheetNames <- sapply(1:n, ithSheetName)
comInvoke(oxl, "Quit")

library(gdata)
read.xls(xls, match("xyz", sheetNames), na.strings = "na ")


On 7/12/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Would it be good enough to just read all the sheets in?
>
> The perl program can do that and although the read.xls R function does not
> interface to that aspect of its functionality its not that difficult to access
> it yourself.  Assume your excel file is in \test.xls .  Just
> switch to that folder.  paste together a command to run the perl
> program, run it, get a list of the file names it produced and read them in:
>
> library(gdata)
> setwd("/")
> cmd <- paste("perl", system.file("perl/xls2csv.pl", package = "gdata"), "test")
> system(cmd)
> ff <- list.files(patt = "test_Sheet.*.csv")
> sapply(ff, read.csv, na.strings = "na ", simplify = FALSE)
>
>
> On 7/12/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> > Hi, Gabor,
> >
> > Thanks for the reply. Perhaps Prof. Ripley will enlighten us as he is
> > the RODBC maintainer.
> >
> > Unfortunately, gdata::read.xls will not work for me (at least I don't
> > think it will) because I need to refer to each worksheet by name and not
> > by number. For example, I need extract data from "Sheet1" and not simply
> > the first sheet.
> >
> > Thanks,
> >
> > --sundar
> >
> > Gabor Grothendieck wrote:
> > > I also got a strange result too (I renamed it sdr.read.xls
> > > to distinguish it from read.xls in gdata and noticed that a
> > > space got into my na's somehow so I used "na " for my
> > > na.strings:
> > >
> > >
> > >>sdr.read.xls("/test.xls", "Sheet2", na.strings = "na ")
> > >
> > >      x
> > > 1 <NA>
> > > 2 <NA>
> > > 3   na
> > > 4   na
> > > 5   na
> > > 6   na
> > > 7 <NA>
> > >
> > > I had more success using read.xls in the gdata package.
> > > Note that we need to install perl first if not already present:
> > >
> > >
> > >>library(gdata)  # for read.xls
> > >>read.xls("/test.xls", 2, na.strings = "na ")
> > >
> > >      x
> > > 1 0.11
> > > 2 0.11
> > > 3   NA
> > > 4   NA
> > > 5   NA
> > > 6   NA
> > > 7 0.11
> > >
> > >
> > >>R.version.string # XP
> > >
> > > [1] "Version 2.3.1 Patched (2006-06-04 r38279)"
> > >
> > >>packageDescription("gdata")$Version
> > >
> > > [1] "2.1.2"
> > >
> > >>packageDescription("RODBC")$Version
> > >
> > > [1] "1.1-7"
> > >
> > >
> > > On 7/12/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> > >
> > >>Hi, all,
> > >>
> > >>I'm trying to use RODBC to read data from Excel. However, I'm having
> > >>trouble converting missing values to NA and rather perplexed by the
> > >>output. Below illustrates my problem:
> > >>
> > >>## DATA - copy to Excel and save as "tmp.xls"
> > >>## tmp.xls!Sheet1
> > >>x
> > >>0.11
> > >>0.11
> > >>na
> > >>na
> > >>na
> > >>0.11
> > >>
> > >>## tmp.xls!Sheet2
> > >>x
> > >>0.11
> > >>0.11
> > >>na
> > >>na
> > >>na
> > >>na
> > >>0.11
> > >>
> > >>## R Code
> > >>read.xls <- function(file, sheet = "Sheet1", ...) {
> > >>  require(RODBC)
> > >>  channel <- odbcConnectExcel(file)
> > >>  sheet <- sprintf("select * from `%s$`", sheet)
> > >>  x <- sqlQuery(channel, sheet, ...)
> > >>  odbcClose(channel)
> > >>  x
> > >>}
> > >>
> > >>read.xls("./tmp.xls", "Sheet1", na.strings = "na")
> > >>## works as expected
> > >>#     x
> > >>#1 0.11
> > >>#2 0.11
> > >>#3   NA
> > >>#4   NA
> > >>#5   NA
> > >>#6 0.11
> > >>
> > >>read.xls("./tmp.xls", "Sheet2", na.strings = "na")
> > >>## Huh? What happened?
> > >>#   x
> > >>#1 NA
> > >>#2 NA
> > >>#3 NA
> > >>#4 NA
> > >>#5 NA
> > >>#6 NA
> > >>#7 NA
> > >>
> > >> > sessionInfo()
> > >>Version 2.3.1 (2006-06-01)
> > >>i386-pc-mingw32
> > >>
> > >>attached base packages:
> > >>[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> > >>[7] "base"
> > >>
> > >>other attached packages:
> > >>  RODBC
> > >>"1.1-7"
> > >>
> > >>______________________________________________
> > >>R-help at stat.math.ethz.ch mailing list
> > >>https://stat.ethz.ch/mailman/listinfo/r-help
> > >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >>
> > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>


From matheric at u.washington.edu  Thu Jul 13 05:22:27 2006
From: matheric at u.washington.edu (Eric C. Jennings)
Date: Wed, 12 Jul 2006 20:22:27 -0700
Subject: [R] writing R output to a specific file type
Message-ID: <000301c6a62b$96669e70$113dd080@Victor1>

Is it possible to write R-output to a specific file type?

Specifically is it possible to write to a specific cell in an MS Excel 
spreadsheet?

Thanks,
Eric


From vincent at 7d4.com  Thu Jul 13 05:31:55 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Thu, 13 Jul 2006 05:31:55 +0200
Subject: [R] Misunderstanding with lines (or elsewhere)
In-Reply-To: <1152628451.1197.35.camel@gsimpson.geog.ucl.ac.uk>
References: <44B39685.80007@7d4.com>	
	<Pine.LNX.4.64.0607111322290.17674@gannet.stats.ox.ac.uk>	
	<44B3AA2E.60504@7d4.com>
	<1152628451.1197.35.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <44B5BEAB.7070604@7d4.com>

Thanks for the hints.


From ggrothendieck at gmail.com  Thu Jul 13 05:54:31 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 12 Jul 2006 23:54:31 -0400
Subject: [R] writing R output to a specific file type
In-Reply-To: <000301c6a62b$96669e70$113dd080@Victor1>
References: <000301c6a62b$96669e70$113dd080@Victor1>
Message-ID: <971536df0607122054r38c7d244icbafb873e2c2cee5@mail.gmail.com>

See:

http://tolstoy.newcastle.edu.au/R/help/05/07/8950.html

On 7/12/06, Eric C. Jennings <matheric at u.washington.edu> wrote:
> Is it possible to write R-output to a specific file type?
>
> Specifically is it possible to write to a specific cell in an MS Excel
> spreadsheet?
>
> Thanks,
> Eric
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From petr.pikal at precheza.cz  Thu Jul 13 08:00:28 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 13 Jul 2006 08:00:28 +0200
Subject: [R] How to get R to ignore certain values when analyzing a
	column	in a data table ?
In-Reply-To: <1EEE10B1-46D8-48EC-920C-A5655887ED3C@wakatara.com>
Message-ID: <44B5FD9C.2436.A4BF1C@localhost>

Hi

On 9 Jul 2006 at 20:01, Daryl Manning wrote:

To:             	r-help at stat.math.ethz.ch
From:           	Daryl Manning <dwm at wakatara.com>
Date sent:      	Sun, 9 Jul 2006 20:01:10 -0700
Subject:        	[R] How to get R to ignore certain values when analyzing a column
	in a data table ?

> Apologies if this is in (one of the many) manuals somewhere... Trying 
> to switch to R from other stats programs.
> 
> Basically, I have a large data table I've dumped from a DB, some of 
> the values which are nulls '-' which I've converted to zeros. I've 
> read it in using read.table fine.
> 
> I want R to ignore the zero values when graphing or doing various 
> other calculations.
> 
> Is there a way to do this ?
> 
> I did try to use NA but kept getting errors that x must be numeric.

What does str(your.data) says about your data? I presume it is 
factor.

datafr <- data.frame(a=rnorm(10), b=rnorm(10))
datafr[3,1]<-0

> datafr$a
 [1] -0.1645236 -0.2533617  0.0000000  0.5566632 -0.6887557 -
0.7074952  0.3645820  0.7685329 -0.1123462  0.8811077

> mean(datafr$a)
[1] 0.06444035

> datafr$a==0
 [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE

> datafr$a[datafr$a==0]
[1] 0

> datafr$a[datafr$a==0]<-NA

> mean(datafr$a)
[1] NA

> mean(datafr$a, na.rm=T)
[1] 0.07160039
>

works OK.

HTH
Petr


> 
> thanks in advance,
> Daryl.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Thu Jul 13 08:18:41 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 13 Jul 2006 08:18:41 +0200
Subject: [R] Query : Modification of graphs
In-Reply-To: <2AB7346A3227A74BB97F9A0D79E3E65A01C1DB@mailserver.kalyptorisk.com>
Message-ID: <44B601E1.16281.B57936@localhost>

Hi

On 12 Jul 2006 at 16:56, priti desai wrote:

Date sent:      	Wed, 12 Jul 2006 16:56:29 +0530
From:           	"priti desai" <priti.desai at kalyptorisk.com>
To:             	<r-help at stat.math.ethz.ch>
Subject:        	[R] Query : Modification of graphs

> How to change the background in the plots
> The e.g. for which I am working is as follows,
> The R script is
> ###################    start   
> #######################################
> 
> amounts <-
> c(21790,5669,34921.17,60152.47,47228.61,16566.13,8283,3980,5445,1000,1
> 25
> 0,0,1285,1222,5001,600,1950,11580,7520,7999,4850,3850,16900,6095,0,424
> 6, 8556,8280,3020,7250,0,8807,0)
> 
> # histogram
> 
> hist(amounts)
> 
> # P-P plot(exponential dist)
> 
> Lambda        <- 1/mean(amounts)
> N             <- length(amounts) 
> e             <- c(1:N)
> f             <- c((e-.5)/N)
> Fx            <- c(1 - exp(-lambda*amounts))
> g             <- sort(Fx)
> 
> 
> plot (f, g, main ="P-P Plot",col=6)
> abline (rq(g ~ f, tau = .5),col="blue")
> 
> 
> 
> # Q-Q plot
> 
> e             <- c(1:N)
> f             <- c((e-.5)/N)
> h             <- c(1-f)
> i             <- -log(h,base=exp(1))
> F1x           <- c((1/lambda)*i)
> 
> j             <- sort(amounts)
> z12 <- sort(F1x)
> 
> plot (j,F1x,main = "Q-Q Plot",col=6)
> abline (rq(F1x ~ j, tau = .5),col="red")
> 
> ######################     end
> #########################################
> 
> Please tell me how should I modify my graphs? 
> i.e. change the background,  colors,    
> how should I modify histogram, P-P plots ,Q-Q plots?

Did you read help pages about plot and hist?

see parameter col = and look also to par help page and bg parameter.

BTW when you will look at these you could try also look at qqplot.

HTH
Petr

> 
> Please suggest me the correct modification, if any calculations &
> formula in R.
>  Awaiting your positive reply.
> 
>   Regards
>   Priti.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Thu Jul 13 08:30:52 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 13 Jul 2006 08:30:52 +0200
Subject: [R] Usage of repeat,while or loop. Documetation?
In-Reply-To: <44ACE046.6030406@student.uib.no>
Message-ID: <44B604BC.13814.C0A1B8@localhost>

Hi

nobody has answered your question yet, so I try. There is no 
reproducible code in your mail. Besides you use names of bult-in 
functions for your data which gives it really messy lookup.

Such tasks can be solved by do.call or maybe  *apply but without more 
information, especially a structure of your data it is difficult to 
help you more.

Petr
  

On 6 Jul 2006 at 12:04, Torleif Markussen Lunde wrote:

Date sent:      	Thu, 06 Jul 2006 12:04:54 +0200
From:           	Torleif Markussen Lunde <tlu004 at student.uib.no>
Organization:   	UiB
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Usage of repeat,while or loop. Documetation?

> Hi
> I want to run the loop described in the example below, but cannot find
> any good documentation on how to do it. I wold be very happy if
> someone could help me on this, as it would save me for a lot of work.
> 
> Kindest regards
> Torleif
> University of Bergen/Norway
> 
> #Sample 1
> 
> m<-0
> 
> #35-40
> #want to repeat this untill m=300
> m<-m+1
> if (length(type[maling==m&type==1&length>=35&length<=39])>0)
> a<-length(type[maling==m&type==1&length>=35&length<=39])  else a<-0
> 
> if (length(type[maling==m&type==2&length>=35&length<=39])>0)
> b<-length(type[maling==m&type==2&length>=35&length<=39]) else b<-0
> 
> if (length(type[maling==m&type==3&length>=35&length<=39])>0)
> c<-length(type[maling==m&type==3&length>=35&length<=39]) else c<-0
> 
> if (length(type[maling==m&type==4&length>=35&length<=39])>0)
> d<-length(type[maling==m&type==4&length>=35&length<=39]) else d<-0
> 
> if (length(type[maling==m&type==5&length>=35&length<=39])>0)
> e<-length(type[maling==m&type==5&length>=35&length<=39]) else e<-0
> 
> a35<-(a+b)/(a+b+c+d+e); a35<-format(a35,digits=4)
> l35<-length(length[maling==m&type==1&length>=35&length<=39])
> 
> .
> .
> .
> .
> .
> 
> #65+
> if (length(type[maling==m&type==1&length>=65])>0)
> a<-length(type[maling==m&type==1&length>=65])  else a<-0
> 
> if (length(type[maling==m&type==2&length>=65])>0)
> b<-length(type[maling==m&type==2&length>=65]) else b<-0
> 
> if (length(type[maling==m&type==3&length>=65])>0)
> c<-length(type[maling==m&type==3&length>=65]) else c<-0
> 
> 
> if (length(type[maling==m&type==4&length>=65])>0)
> d<-length(type[maling==m&type==4&length>=65]) else d<-0
> 
> if (length(type[maling==m&type==5&length>=65])>0)
> e<-length(type[maling==m&type==5&length>=65]) else e<-0
> 
> a65<-(a+b)/(a+b+c+d+e); a65<-format(a65,digits=4)
> l65<-length(length[maling==m&type==1&length>=65])
> 
> la1<-latitude[maling==m]; la1<-format(la1[1],digits=9)
> lo1<-longitude[maling==m]; lo1<-format(lo1[1],digits=9)
> year1<-year[maling==m]; year1<-format(year1[1],digits=4)
> mnd1<-mnd[maling==m]; mnd1<-format(mnd1[1],digits=2)
> dag1<-dag[maling==m]; dag1<-format(dag1[1],digits=2)
> omr1<-omr[maling==m]; omr1<-format(omr1[1],digits=2)
> dist1<-dist[maling==m]; dist1<-format(dist1[1],digits=2)
> start1<-start[maling==m]; start1<-format(start1[1],digits=4)
> stopp1<-stopp[maling==m]; stopp1<-format(stopp1[1],digits=4)
> homr1<-homr[maling==m]; homr1<-format(homr1[1],digits=2)
> skip1<-skip[maling==m]; skip1<-skip1[1]
> 
> ######################
> #Then i want to update the kt1 name to change from kt1 to kt2 to
> #kt3... kt300. The goal is to create a table with kt1 kt2
> .
> .
> #kt300
> #by using rbind. Suggestions on how to update the table as the loop is
> #running are welcome.
> ######################
> 
> kt1<-c(la1,lo1,year1,mnd1,dag1,omr1,dist1,start1,stopp1,homr1,a30,a35,
> a40,a45,a50,a55,a60,a65,l30,l35,l40,l45,l50,l55,l60,l65)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From mchaudha at jhsph.edu  Thu Jul 13 08:43:08 2006
From: mchaudha at jhsph.edu (Ashraf Chaudhary)
Date: Thu, 13 Jul 2006 02:43:08 -0400
Subject: [R] Scalling/Centering the Data by an Index
Message-ID: <200607130643.k6D6hHsO025513@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060713/914da1e5/attachment.pl 

From blomsp at ozemail.com.au  Thu Jul 13 08:54:31 2006
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Thu, 13 Jul 2006 16:54:31 +1000
Subject: [R] Scalling/Centering the Data by an Index
In-Reply-To: <200607130643.k6D6hHsO025513@hypatia.math.ethz.ch>
References: <200607130643.k6D6hHsO025513@hypatia.math.ethz.ch>
Message-ID: <44B5EE27.7010409@ozemail.com.au>

If you want to centre the data, but not scale as well, turn off scale:

unlist(tapply(x, group, scale, scale=FALSE))

HTH,

Simon.

Ashraf Chaudhary wrote:
> Dear All:
> I would like to center the data in 'x' by 'group'. The following code scale
> the data and I have not been able to figure out how to change it so I get
> the centered data.
>  
> x <- c(1, 2, 3, 4, 5, 6, 7, 8)
> group <- c(1,1,1,2,2,2,2,2)
> unsplit(lapply(split(x,group),scale),group)
>
> I would appreciate your help.
>  
> Ashraf
> -----------------______________________________________
> M. Ashraf Chaudhary, Ph.D.
> Associate Scientist/Biostatistician
> Department of International Health
> Johns Hopkins Bloomberg School of Public Health
> 615 N. Wolfe St.  Room W5506
> Baltimore MD 21205-2179
>
> (410) 502-0741/fax: (410) 502-6733
>  <mailto:mchaudha at jhsph.edu> mchaudha at jhsph.edu
> Web:http://faculty.jhsph.edu/?F=Mohammad&L=Chaudhary
>  
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>   
-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C


From robert-mcfadden at o2.pl  Thu Jul 13 09:03:03 2006
From: robert-mcfadden at o2.pl (Robert Mcfadden)
Date: Thu, 13 Jul 2006 09:03:03 +0200
Subject: [R] shapiro.test() output
In-Reply-To: <12E238925F448B48BC28E96479F125DE0138B8A6@CATSKILL.amr.ch2m.com>
Message-ID: <000301c6a64a$622d3b40$1191680a@robert>



> -----Original Message-----
> From: r-help-bounces w stat.math.ethz.ch [mailto:r-help-
> bounces w stat.math.ethz.ch] On Behalf Of Matthew.Findley w ch2m.com
> Sent: Wednesday, July 12, 2006 11:14 PM
> To: r-help w stat.math.ethz.ch
> Subject: [R] shapiro.test() output
> 
> R Users:
> 
> My question is probably more about elementary statistics than the
> mechanics of using R, but I've been dabbling in R (version 2.2.0) and
> used it recently  to test some data .
> 
> I have a relatively small set of observations (n = 12) of arsenic
> concentrations in background groundwater and wanted to test my
> assumption of normality.  I used the Shapiro-Wilk test (by calling
> shapiro.test() in R) and I'm not sure how to interpret the output.
> Here's the input/output from the R console:
> 
> 	>As = c(13, 17, 23, 9.5, 20, 15, 11, 17, 21, 14, 22, 13)
> 	>shapiro.test(As)
> 
>       	  Shapiro-Wilk normality test
> 
> 	data:  As
> 	W = 0.9513, p-value = 0.6555
> 
> How do I interpret this?  I understand, from poking around the internet,
> that the higher the W statistic the "more normal" the data.
> 
> What is the null hypothesis - that the data is normally distributed?
> 
> What does the p-value tell me?  65.55% chance of what - getting
> W-statistic greater than or equal to 0.9513 (I picked this up from the
> Dalgaard book, Introductory Statistics with R, but its not really
> sinking in with respect to how it applies to a Shipiro Wilk test).?
> 
> The method description - retrieved using ?shapiro.test() - is a bit
> light on details.
> 
> Thanks much.

The null hypothesis: the data is normally distributed.
If p-value > \alpha (significance level) it means that there is no evidence
to reject null hypothesis. Otherwise you reject - your data is not normally
distributed.


From scruveil at genoscope.cns.fr  Thu Jul 13 09:53:32 2006
From: scruveil at genoscope.cns.fr (=?ISO-8859-1?Q?St=E9phane_Cruveiller?=)
Date: Thu, 13 Jul 2006 09:53:32 +0200
Subject: [R] simple question  about variables....
Message-ID: <44B5FBFC.6040604@genoscope.cns.fr>

Dear R users,

I have a simple question on variable manipulation.
Imagine I have an object "OBJ" that has "toto" as one of its variables.
I would like to understand why if I do

 > varname <- "toto"

 >OBJ$varname                             returns no results

whereas

 > OBJ[varname]                            returns the column entitled 
"toto"


Thanks for your help.

St?phane.


From jacques.veslot at good.ibl.fr  Thu Jul 13 10:09:47 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Thu, 13 Jul 2006 10:09:47 +0200
Subject: [R] simple question  about variables....
In-Reply-To: <44B5FBFC.6040604@genoscope.cns.fr>
References: <44B5FBFC.6040604@genoscope.cns.fr>
Message-ID: <44B5FFCB.50507@good.ibl.fr>

see ?"$"
'x$name' is equivalent to 'x[["name"]]'

so you need use :
eval(parse(text = paste("OBJ$", varname)))

-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


St?phane Cruveiller a ?crit :
> Dear R users,
> 
> I have a simple question on variable manipulation.
> Imagine I have an object "OBJ" that has "toto" as one of its variables.
> I would like to understand why if I do
> 
>  > varname <- "toto"
> 
>  >OBJ$varname                             returns no results
> 
> whereas
> 
>  > OBJ[varname]                            returns the column entitled 
> "toto"
> 
> 
> Thanks for your help.
> 
> St?phane.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From j.van_den_hoff at fz-rossendorf.de  Thu Jul 13 10:19:27 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Thu, 13 Jul 2006 10:19:27 +0200
Subject: [R] simple question  about variables....
In-Reply-To: <44B5FBFC.6040604@genoscope.cns.fr>
References: <44B5FBFC.6040604@genoscope.cns.fr>
Message-ID: <44B6020F.9020007@fz-rossendorf.de>

St?phane Cruveiller wrote:
> Dear R users,
> 
> I have a simple question on variable manipulation.
> Imagine I have an object "OBJ" that has "toto" as one of its variables.
> I would like to understand why if I do
> 
>  > varname <- "toto"
> 
>  >OBJ$varname                             returns no results
> 
> whereas
> 
>  > OBJ[varname]                            returns the column entitled 
> "toto"
> 
> 
> Thanks for your help.
> 
> St?phane.
> 

because if the value of `varname' is substituted in the expressions, in 
the first case that yields

OBJ$"toto" and in the second
OBJ["toto"]


the latter is valid, the former is not (you'd need `OBJ$toto' there), 
read ` ?"$" ':

"...Both '[[' and '$' select a single element of the list.  The main
      difference is that '$' does not allow computed indices, whereas
      '[[' does.  'x$name' is equivalent to 'x[["name"]]'..."


not, too, the difference between `[' (sublist) and `[[' (single element 
extraction)

joerg


From scruveil at genoscope.cns.fr  Thu Jul 13 10:19:16 2006
From: scruveil at genoscope.cns.fr (=?ISO-8859-1?Q?St=E9phane_Cruveiller?=)
Date: Thu, 13 Jul 2006 10:19:16 +0200
Subject: [R] simple question  about variables....
In-Reply-To: <44B5FFCB.50507@good.ibl.fr>
References: <44B5FBFC.6040604@genoscope.cns.fr> <44B5FFCB.50507@good.ibl.fr>
Message-ID: <44B60204.6040404@genoscope.cns.fr>

Thx for the tip....


St?phane.

Jacques VESLOT a ?crit :
> see ?"$"
> 'x$name' is equivalent to 'x[["name"]]'
>
> so you need use :
> eval(parse(text = paste("OBJ$", varname)))
>
> -------------------------------------------------------------------
> Jacques VESLOT
>
> CNRS UMR 8090
> I.B.L (2?me ?tage)
> 1 rue du Professeur Calmette
> B.P. 245
> 59019 Lille Cedex
>
> Tel : 33 (0)3.20.87.10.44
> Fax : 33 (0)3.20.87.10.31
>
> http://www-good.ibl.fr
> -------------------------------------------------------------------
>
>
> St?phane Cruveiller a ?crit :
>> Dear R users,
>>
>> I have a simple question on variable manipulation.
>> Imagine I have an object "OBJ" that has "toto" as one of its variables.
>> I would like to understand why if I do
>>
>>  > varname <- "toto"
>>
>>  >OBJ$varname                             returns no results
>>
>> whereas
>>
>>  > OBJ[varname]                            returns the column 
>> entitled "toto"
>>
>>
>> Thanks for your help.
>>
>> St?phane.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>


From jacques.veslot at good.ibl.fr  Thu Jul 13 10:32:19 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Thu, 13 Jul 2006 10:32:19 +0200
Subject: [R] simple question  about variables....
In-Reply-To: <44B6020F.9020007@fz-rossendorf.de>
References: <44B5FBFC.6040604@genoscope.cns.fr>
	<44B6020F.9020007@fz-rossendorf.de>
Message-ID: <44B60513.5030009@good.ibl.fr>

OBJ$"toto" works to...

 > b <- as.data.frame(matrix(1:4,2))
 > b
   V1 V2
1  1  3
2  2  4
 > b$"V1"
[1] 1 2

but varname is not evaluated in OBJ$varname.

-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Joerg van den Hoff a ?crit :
> St?phane Cruveiller wrote:
> 
>>Dear R users,
>>
>>I have a simple question on variable manipulation.
>>Imagine I have an object "OBJ" that has "toto" as one of its variables.
>>I would like to understand why if I do
>>
>> > varname <- "toto"
>>
>> >OBJ$varname                             returns no results
>>
>>whereas
>>
>> > OBJ[varname]                            returns the column entitled 
>>"toto"
>>
>>
>>Thanks for your help.
>>
>>St?phane.
>>
> 
> 
> because if the value of `varname' is substituted in the expressions, in 
> the first case that yields
> 
> OBJ$"toto" and in the second
> OBJ["toto"]
> 
> 
> the latter is valid, the former is not (you'd need `OBJ$toto' there), 
> read ` ?"$" ':
> 
> "...Both '[[' and '$' select a single element of the list.  The main
>       difference is that '$' does not allow computed indices, whereas
>       '[[' does.  'x$name' is equivalent to 'x[["name"]]'..."
> 
> 
> not, too, the difference between `[' (sublist) and `[[' (single element 
> extraction)
> 
> joerg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From rstatistics at gmail.com  Thu Jul 13 10:47:09 2006
From: rstatistics at gmail.com (A.R. Criswell)
Date: Thu, 13 Jul 2006 15:47:09 +0700
Subject: [R] Matrix binary for Mac OS X
Message-ID: <878be2c60607130147x629b163dt42ed22a65311b662@mail.gmail.com>

Hello all,

I can't seem to find package "Matrix" for Mac OS X. The R package
installer, when pointed to Bristol (UK) does not have Matrix amongst
its list.

Thank you


From tuechler at gmx.at  Thu Jul 13 10:59:39 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Thu, 13 Jul 2006 09:59:39 +0100
Subject: [R] Keep value lables with data frame manipulation
In-Reply-To: <1152728062.4662.25.camel@localhost.localdomain>
References: <2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>
	<2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>
Message-ID: <3.0.6.32.20060713095939.00ace1f8@pop.gmx.net>

At 13:14 12.07.2006 -0500, Marc Schwartz (via MN) wrote:
>On Wed, 2006-07-12 at 17:41 +0100, Jol, Arne wrote:
>> Dear R,
>> 
>> I import data from spss into a R data.frame. On this rawdata I do some
>> data processing (selection of observations, normalization, recoding of
>> variables etc..). The result is stored in a new data.frame, however, in
>> this new data.frame the value labels are lost.
>> 
>> Example of what I do in code:
>> 
>> # read raw data from spss
>> rawdata <- read.spss("./data/T50937.SAV",
>> 	use.value.labels=FALSE,to.data.frame=TRUE)
>> 
>> # select the observations that we need
>> diarydata <- rawdata[rawdata$D22==2 | rawdata$D22==3 | rawdata$D22==17 |
>> rawdata$D22==18 | rawdata$D22==20 | rawdata$D22==22 |
>>  			rawdata$D22==24 | rawdata$D22==33,]
>> 
>> The result is that rawdata$D22 has value labels and that diarydata$D22
>> is numeric without value labels.
>> 
>> Question: How can I prevent this from happening?
>> 
>> Thanks in advance!
>> Groeten,
>> Arne
>
>Two things:
>
>1. With respect to your subsetting, your lengthy code can be replaced
>with the following:
>
>  diarydata <- subset(rawdata, D22 %in% c(2, 3, 17, 18, 20, 22, 24, 33))
>
>See ?subset and ?"%in%" for more information.
>
>
>2. With respect to keeping the label related attributes, the
>'value.labels' attribute and the 'variable.labels' attribute will not by
>default survive the use of "[".data.frame in R (see ?Extract
>and ?"[.data.frame").
>
>On the other hand, based upon my review of ?read.spss, the SPSS value
>labels should be converted to the factor levels of the respective
>columns when 'use.value.labels = TRUE' and these would survive a
>subsetting.
>
>If you want to consider a solution to the attribute subsetting issue,
>you might want to review the following post by Gabor Grothendieck in
>May, which provides a possible solution:
>
>  https://stat.ethz.ch/pipermail/r-help/2006-May/106308.html
>
>and this post by me, for an explanation of what is happening in Gabor's
>solution:
>
>  https://stat.ethz.ch/pipermail/r-help/2006-May/106351.html
>
>HTH,
>
>Marc Schwartz
>
Hello Mark and Arne,

I worked on the suggestions of Gabor and Mark and programmed some functions
in this way, but they are very, very preliminary (see below).
In my view there is a lack of convenient possibilities in R to document
empirical data by variable labels, value labels, etc. I would prefer to
have these possibilities in the "standard" configuration.
So I sketched a concept, but in my view it would only be useful, if there
was some acceptance by the core developers of R.

The concept would be to define a class. For now I call it "source.data".
To design it more flexible than the Hmisc class "labelled" I would define a
related option "source.data.attributes" with default c('value.labels',
'variable.name', 'label')). This option contains all attributes that should
persist in subsetting/indexing.

I made only some very, very preliminary tests with these functions, mainly
because I am not happy with defining a new class. Instead I would prefer,
if this functionality could be integrated in the Hmisc class "labelled",
since this is in my view the best known starting point for data
documentation in R.

I would be happy, if there were some discussion about the wishes/needs of
other Rusers concerning data documentation.

Greetings,

Heinz


### intention and concept
#   There should be a convenient possibility to keep source data numerical
#   coded and at the same time have labelled categories.
#   Such labelled categorical numerical data should be easily converted
#   to factors.
#   Indexing/subsetting should preserve the concerned attributes of this data.

### description of (intended!!!) functionality
#   - a class source.data is defined. It is intended only for atomic objects.
#   - option source.data.attributes defines which attributes will be copied
#     in indexing/subsetting objects of class source.data
#   - option source.data.is.ordered sets defining factors as ordered, when
#     built from objects of class source.data by the function factsd
#   - function 'value.labels<-' assigns an attribute value.labels and sets
#     class source.data
#   - function value.labels reads the attribute value.labels
#   - the indexing method '[.source.data' defines indexing for source.data
#   - the print method print.source.data ignores source.data.attributes in
#     printing
#   - the as.data.frame method as.data.frame.source.data enables inclusion
#     of objects of class source.data in data.frames
#   - function factsd should in general behave as function factor but should
#     in case of an object of class source.data by default use the
value.labels
#     as levels and the names(value.labels) as the labels of the new built
#     factor.
#     If the parameter ordered is NULL it should create ordered factors
#     according to the option source.data.is.ordered.

### set option for source.data.attributes
options(source.data.attributes=c('value.labels', 'variable.name', 'label'))
### set option for converting source.data class in ordered factors
options(source.data.is.ordered=TRUE)

### function to assign value.labels
'value.labels<-' <- function (x, value)
  ## adapted from Hmisc function label 30.6.2006
{
  if(!is.atomic(x)) stop('value.labels<- is applicabel to atomic objects
only')
  structure(x, value.labels = value, class = c("source.data",
               attr(x, "class")[attr(x, "class") != "source.data"]))
}

### function to read value.labels
value.labels <- function (x) { attr(x, 'value.labels') }

### definition of indexing method for class=source.data
##  source.data.attributes shall be conserved
"[.source.data" <- function(x, ...)
{
  atr <- attributes(x)
  atr.names <- names(atr)
  sda <- options()$'source.data.attributes'
  sda.match <- match(atr.names, sda)
  sda.match <- sda.match[!is.na(sda.match)]
  x <- NextMethod("[")
  ## assign source.data.attributes to result
  if(length(sda.match))
    for (i in sda.match) attr(x, sda[i]) <- atr[[sda[i]]]
  ## assign class source.data to result
  class(x) <- c('source.data', attr(x, "class")[attr(x, "class")
                                                != "source.data"])       
  x
}

### print method for source.data
'print.source.data' <- function (x, ...) 
{
  ## adapted from Hmisc print.labelled 31.5.2006
  x.orig <- x
  ## look if there are source.data.attributes
  sda <- options()$'source.data.attributes'
  sda.match <- match(names(attributes(x)), sda)
  sda.match <- sda.match[!is.na(sda.match)]
  ## delete source.data.attributes for printing
  if(length(sda.match))
    for (i in sda.match) attr(x, sda[i]) <- NULL
  ## delete class source.data for printing
  class(x) <- if (length(class(x)) == 1 && class(x) == "source.data") 
    NULL
  else class(x)[class(x) != "source.data"]
  NextMethod("print")
  invisible(x.orig)
}

### Define function as.data.frame.source.data (copy from as.data.frame.vector)
#   many as.data.frame methods are identical to this
##  different functions as.data.frame are besides others:
#   as.data.frame.list, as.data.frame.default, as.data.frame.data.frame,
#   as.data.frame.character, as.data.frame.AsIs, as.data.frame.array,

as.data.frame.source.data <- 
  function (x, row.names = NULL, optional = FALSE)
  ## copy from as.data.frame.vector 1.6.2006
{
  nrows <- length(x)
  nm <- paste(deparse(substitute(x), width.cutoff = 500), collapse = " ")
  if (is.null(row.names)) {
    if (nrows == 0) 
      row.names <- character(0)
    else if (length(row.names <- names(x)) == nrows &&
!any(duplicated(row.names))) {
    }
    else if (optional) 
      row.names <- character(nrows)
    else row.names <- as.character(1:nrows)
  }
  names(x) <- NULL
  value <- list(x)
  if (!optional) 
    names(value) <- nm
  attr(value, "row.names") <- row.names
  class(value) <- "data.frame"
  value
}

### function to create factor from source.data class applying variable.labels
#   and copying all source.data.attributes
#   remark: factor(factsd(x)) drops unused factor levels and source.data class
#           factsd(x)[, drop=TRUE] drops unused factor levels but keeps
#           source.data class and attributes

factsd <- function(x = character(),
                   levels = sort(unique.default(x), na.last = TRUE),
                   labels = levels, exclude = NA, ordered = NULL)
{
  ## check if is of class source.data
  if ('source.data' %in% class(x))
    {
      if(is.null(ordered)) ordered <- options()$source.data.is.ordered
      fx <- factor(x = x, levels = value.labels(x),
                   labels = names(value.labels(x)),
                   exclude = exclude,
                   ordered = ordered)
      ## copy source.data.attributes
      atr <- attributes(x)
      atr.names <- names(atr)
      sda <- options()$'source.data.attributes'
      sda.match <- match(atr.names, sda)
      sda.match <- sda.match[!is.na(sda.match)]
      ## assign source.data.attributes to result
      if(length(sda.match))
        for (i in sda.match) attr(fx, sda[i]) <- atr[[sda[i]]]
      ## add class source.data to result
      class(fx) <- c('source.data', attr(fx, 'class'))       
    }
  else {
    if(is.null(ordered)) ordered <- is.ordered(x)
    fx <- factor(x = x, levels = levels, labels = labels,
                 exclude = exclude, ordered = ordered)
  }
  fx
}


From julie.black at imperial.ac.uk  Thu Jul 13 11:38:52 2006
From: julie.black at imperial.ac.uk (jb1103)
Date: Thu, 13 Jul 2006 02:38:52 -0700 (PDT)
Subject: [R] SAR with count data
Message-ID: <5304944.post@talk.nabble.com>


I would like to undertake a Spatial autoregression with count data, how can I
specify a negative binomial distribution? I am using the lagsarlm function
in the spdep package. 

Thanks

Julie
-- 
View this message in context: http://www.nabble.com/SAR-with-count-data-tf1936174.html#a5304944
Sent from the R help forum at Nabble.com.


From oarabile at stams.strath.ac.uk  Thu Jul 13 12:32:38 2006
From: oarabile at stams.strath.ac.uk (Oarabile Molaodi)
Date: Thu, 13 Jul 2006 11:32:38 +0100
Subject: [R] package GLLMGibbs
Message-ID: <44B62146.8050004@stams.strath.ac.uk>

Tried to look for package GLLMGibbs-(package for fitting mixed models by 
Gibbs samppling contributed by Myles' and Clayton) in CRAN but not found 
it, Does anybody know if  it  is still available?

thanks
Oarabile


From bibiko at eva.mpg.de  Thu Jul 13 12:47:09 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Thu, 13 Jul 2006 12:47:09 +0200
Subject: [R] VERY TINY question: missing function to clear the console?
Message-ID: <C7F643AD-0201-40BE-BBB5-F13ADCBF61E2@eva.mpg.de>


Hi,

for presentation purposes I would like to clear to whole console  
window (like in a UNIX terminal: 'clear').

Is there such a function?

If not, I could image that is not too hard to write such a function.

Cheers,

Hans


From pacocuacco at libero.it  Thu Jul 13 13:02:20 2006
From: pacocuacco at libero.it (pacocuacco)
Date: Thu, 13 Jul 2006 13:02:20 +0200
Subject: [R] R --gui=GNOME problem
Message-ID: <J2C9BW$FFE220ECF39B9605844EED10FB20B0E2@libero.it>

I've installed GNU R statistical comupting language and environment 2.2.1-2 by Synaptic (on Ubunti Linux 6.06 Dapper Drake). I've installed also r-gnome to have a GUI. I can't start my GUI. Why R --gui=GNOME don't start? It cannot find rgnome. R --gui=tk command instead is good. What's the best GUI for R?
How can I start my GUI?

Thanks
Paco


From bibiko at eva.mpg.de  Thu Jul 13 13:13:08 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Thu, 13 Jul 2006 13:13:08 +0200
Subject: [R] VERY TINY question: missing function to clear the console?
Message-ID: <8A760A55-1B68-4550-9656-627E8579C008@eva.mpg.de>

Sorry, I forgot to mention that I'm using R 2.3.1 on Mac OSX 10.4.7  
via R.app GUI.

On 13 Jul 2006, at 12:53, Joris De Wolf wrote:


> ctrl+l
>
>

Thanks, but unfortunately this doesn't work on my Mac.

Hans


>
> Hi,
>
> for presentation purposes I would like to clear to whole console
> window (like in a UNIX terminal: 'clear').
>
> Is there such a function?
>
> If not, I could image that is not too hard to write such a function.
>
> Cheers,
>
> Hans
>


From jtk at cmp.uea.ac.uk  Thu Jul 13 13:15:55 2006
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Thu, 13 Jul 2006 12:15:55 +0100
Subject: [R] VERY TINY question: missing function to clear the console?
In-Reply-To: <C7F643AD-0201-40BE-BBB5-F13ADCBF61E2@eva.mpg.de>
References: <C7F643AD-0201-40BE-BBB5-F13ADCBF61E2@eva.mpg.de>
Message-ID: <20060713111555.GA22695@jtkpc.cmp.uea.ac.uk>

On Thu, Jul 13, 2006 at 12:47:09PM +0200, Hans-Joerg Bibiko wrote:
> 
> Hi,
> 
> for presentation purposes I would like to clear to whole console  
> window (like in a UNIX terminal: 'clear').
> 
> Is there such a function?
> 
> If not, I could image that is not too hard to write such a function.

At the risk of this being a stupid answer: An easy way would be

    system("clear");

This should work under unixes that provide the "clear" command, don't know
about windows.

If there is no "clear screen" function in R (I haven't seriously looked
for one), I think one of the reasons for that might be that writing that
in a multi-platform portable way is actually a somewhat more laborious
thing to do than it seems.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |             email: jtk at cmp.uea.ac.uk                               |
 |             WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*


From ripley at stats.ox.ac.uk  Thu Jul 13 13:22:30 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 13 Jul 2006 12:22:30 +0100 (BST)
Subject: [R] package GLLMGibbs
In-Reply-To: <44B62146.8050004@stams.strath.ac.uk>
References: <44B62146.8050004@stams.strath.ac.uk>
Message-ID: <Pine.LNX.4.64.0607131219280.22308@gannet.stats.ox.ac.uk>

On Thu, 13 Jul 2006, Oarabile Molaodi wrote:

> Tried to look for package GLLMGibbs-(package for fitting mixed models by 
> Gibbs samppling contributed by Myles' and Clayton) in CRAN but not found 
> it, Does anybody know if  it  is still available?

I presume you mean GLMMGibbs ....

It is where it has always been, on the Devel area on CRAN, e.g.

http://cran.r-project.org/src/contrib/Devel/

(That version needs some changes to work under current versions of R.)

A compiled Windows binary is also available, and will be found from the 
RGui menus.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From j.logsdon at quantex-research.com  Thu Jul 13 13:23:23 2006
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Thu, 13 Jul 2006 12:23:23 +0100 (GMT)
Subject: [R] Extracting Phi from gls/lme
Message-ID: <Pine.LNX.4.10.10607131220250.12144-100000@mercury.quantex>

I am trying to extract into a scalar the value of Phi from the printed
output of gls or lme using corAR1 correlation.  ie I want the estimate of
the autocorrelation.  I can't see how to do this and haven't seen it
anywhere in str(model.lme).

I can get all the other information - fixed and random effects etc.

Is there an obvious way so that I can save the brick wall some damage?

TIA

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com


From plummer at iarc.fr  Thu Jul 13 13:32:27 2006
From: plummer at iarc.fr (Martyn Plummer)
Date: Thu, 13 Jul 2006 13:32:27 +0200
Subject: [R] package GLLMGibbs
In-Reply-To: <44B62146.8050004@stams.strath.ac.uk>
References: <44B62146.8050004@stams.strath.ac.uk>
Message-ID: <1152790347.2375.17.camel@seurat.iarc.fr>

On Thu, 2006-07-13 at 11:32 +0100, Oarabile Molaodi wrote:
> Tried to look for package GLLMGibbs-(package for fitting mixed models by 
> Gibbs samppling contributed by Myles' and Clayton) in CRAN but not found 
> it, Does anybody know if  it  is still available?
> 
> thanks
> Oarabile

You will find it in the "Devel" directory. See the section "Related
Directories" at the bottom of the contributed packages page on CRAN.

Martyn

-----------------------------------------------------------------------
This message and its attachments are strictly confidential. ...{{dropped}}


From sundar.dorai-raj at pdf.com  Thu Jul 13 13:43:56 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 13 Jul 2006 04:43:56 -0700
Subject: [R] RODBC, missing values, and Excel
In-Reply-To: <971536df0607122004x4827e0f6pf4eb962cec10b5cd@mail.gmail.com>
References: <44B577A1.9080904@pdf.com>	<971536df0607121759x56dc7c14k38f14e29696fe994@mail.gmail.com>	<44B5A747.60306@pdf.com>	<971536df0607121924m495802devbed554f664d270f6@mail.gmail.com>
	<971536df0607122004x4827e0f6pf4eb962cec10b5cd@mail.gmail.com>
Message-ID: <44B631FC.1020507@pdf.com>

Hi, Gabor,

Thanks for the code. When I tried this I get an error when trying to use 
a relative path name:

read.excel <- function(file, sheet, ...) {
   require(rcom)
   require(gdata)
   oxl <- comCreateObject('Excel.Application')
   comSetProperty(oxl, "Visible", TRUE)  # this line optional
   owb <- comGetProperty(oxl, "Workbooks")
   ob <- comInvoke(owb, "Open", file)
   osheets <- comGetProperty(ob, "Worksheets")
   n <- comGetProperty(osheets, "Count")
   ithSheetName <- function(i)
     comGetProperty(comGetProperty(osheets, "Item", i), "Name")
   sheetNames <- sapply(1:n, ithSheetName)
   comInvoke(oxl, "Quit")
   read.xls(file, match(sheet, sheetNames), ...)
}

 > read.excel("tmp.xls", "Sheet2", na.strings = "na")
Error in 1:n : NA/NaN argument
 > read.excel("D:/Users/sundard/frm/config/R/tmp.xls",
+            "Sheet2", na.strings = "na")
       x
1 0.11
2 0.11
3   NA
4   NA
5   NA
6   NA
7 0.11

Any reason I need an absolute path?

Thanks again,

--sundar

Gabor Grothendieck wrote:
> In thinking about this some more I have a better idea.  Use rcom (or
> RDCOMClient)
> to get a list of the sheet names and then use that to determine which sheet you
> need.  Then use read.xls to get it like this assuming that the Excel
> file and path are C:\test.xls and that one of the sheets in that spreadsheet
> is xyz.  In my version the na.strings had a space at the end so you may
> need to change the na.strings= setting:
> 
> library(rcom)
> xls <- "C:\\test.xls"
> oxl <- comCreateObject('Excel.Application')
> comSetProperty(oxl, "Visible", TRUE)  # this line optional
> owb <- comGetProperty(oxl, "Workbooks")
> ob <- comInvoke(owb, "Open", xls)
> osheets <- comGetProperty(ob, "Worksheets")
> n <- comGetProperty(osheets, "Count")
> ithSheetName <- function(i)
> 	comGetProperty(comGetProperty(osheets, "Item", i), "Name")
> sheetNames <- sapply(1:n, ithSheetName)
> comInvoke(oxl, "Quit")
> 
> library(gdata)
> read.xls(xls, match("xyz", sheetNames), na.strings = "na ")
> 
> 
> On 7/12/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> 
>>Would it be good enough to just read all the sheets in?
>>
>>The perl program can do that and although the read.xls R function does not
>>interface to that aspect of its functionality its not that difficult to access
>>it yourself.  Assume your excel file is in \test.xls .  Just
>>switch to that folder.  paste together a command to run the perl
>>program, run it, get a list of the file names it produced and read them in:
>>
>>library(gdata)
>>setwd("/")
>>cmd <- paste("perl", system.file("perl/xls2csv.pl", package = "gdata"), "test")
>>system(cmd)
>>ff <- list.files(patt = "test_Sheet.*.csv")
>>sapply(ff, read.csv, na.strings = "na ", simplify = FALSE)
>>
>>
>>On 7/12/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
>>
>>>Hi, Gabor,
>>>
>>>Thanks for the reply. Perhaps Prof. Ripley will enlighten us as he is
>>>the RODBC maintainer.
>>>
>>>Unfortunately, gdata::read.xls will not work for me (at least I don't
>>>think it will) because I need to refer to each worksheet by name and not
>>>by number. For example, I need extract data from "Sheet1" and not simply
>>>the first sheet.
>>>
>>>Thanks,
>>>
>>>--sundar
>>>
>>>Gabor Grothendieck wrote:
>>>
>>>>I also got a strange result too (I renamed it sdr.read.xls
>>>>to distinguish it from read.xls in gdata and noticed that a
>>>>space got into my na's somehow so I used "na " for my
>>>>na.strings:
>>>>
>>>>
>>>>
>>>>>sdr.read.xls("/test.xls", "Sheet2", na.strings = "na ")
>>>>
>>>>     x
>>>>1 <NA>
>>>>2 <NA>
>>>>3   na
>>>>4   na
>>>>5   na
>>>>6   na
>>>>7 <NA>
>>>>
>>>>I had more success using read.xls in the gdata package.
>>>>Note that we need to install perl first if not already present:
>>>>
>>>>
>>>>
>>>>>library(gdata)  # for read.xls
>>>>>read.xls("/test.xls", 2, na.strings = "na ")
>>>>
>>>>     x
>>>>1 0.11
>>>>2 0.11
>>>>3   NA
>>>>4   NA
>>>>5   NA
>>>>6   NA
>>>>7 0.11
>>>>
>>>>
>>>>
>>>>>R.version.string # XP
>>>>
>>>>[1] "Version 2.3.1 Patched (2006-06-04 r38279)"
>>>>
>>>>
>>>>>packageDescription("gdata")$Version
>>>>
>>>>[1] "2.1.2"
>>>>
>>>>
>>>>>packageDescription("RODBC")$Version
>>>>
>>>>[1] "1.1-7"
>>>>
>>>>
>>>>On 7/12/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
>>>>
>>>>
>>>>>Hi, all,
>>>>>
>>>>>I'm trying to use RODBC to read data from Excel. However, I'm having
>>>>>trouble converting missing values to NA and rather perplexed by the
>>>>>output. Below illustrates my problem:
>>>>>
>>>>>## DATA - copy to Excel and save as "tmp.xls"
>>>>>## tmp.xls!Sheet1
>>>>>x
>>>>>0.11
>>>>>0.11
>>>>>na
>>>>>na
>>>>>na
>>>>>0.11
>>>>>
>>>>>## tmp.xls!Sheet2
>>>>>x
>>>>>0.11
>>>>>0.11
>>>>>na
>>>>>na
>>>>>na
>>>>>na
>>>>>0.11
>>>>>
>>>>>## R Code
>>>>>read.xls <- function(file, sheet = "Sheet1", ...) {
>>>>> require(RODBC)
>>>>> channel <- odbcConnectExcel(file)
>>>>> sheet <- sprintf("select * from `%s$`", sheet)
>>>>> x <- sqlQuery(channel, sheet, ...)
>>>>> odbcClose(channel)
>>>>> x
>>>>>}
>>>>>
>>>>>read.xls("./tmp.xls", "Sheet1", na.strings = "na")
>>>>>## works as expected
>>>>>#     x
>>>>>#1 0.11
>>>>>#2 0.11
>>>>>#3   NA
>>>>>#4   NA
>>>>>#5   NA
>>>>>#6 0.11
>>>>>
>>>>>read.xls("./tmp.xls", "Sheet2", na.strings = "na")
>>>>>## Huh? What happened?
>>>>>#   x
>>>>>#1 NA
>>>>>#2 NA
>>>>>#3 NA
>>>>>#4 NA
>>>>>#5 NA
>>>>>#6 NA
>>>>>#7 NA
>>>>>
>>>>>
>>>>>>sessionInfo()
>>>>>
>>>>>Version 2.3.1 (2006-06-01)
>>>>>i386-pc-mingw32
>>>>>
>>>>>attached base packages:
>>>>>[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
>>>>>[7] "base"
>>>>>
>>>>>other attached packages:
>>>>> RODBC
>>>>>"1.1-7"
>>>>>
>>>>>______________________________________________
>>>>>R-help at stat.math.ethz.ch mailing list
>>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>>>
>>>>
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ggrothendieck at gmail.com  Thu Jul 13 13:50:47 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 13 Jul 2006 07:50:47 -0400
Subject: [R] RODBC, missing values, and Excel
In-Reply-To: <44B631FC.1020507@pdf.com>
References: <44B577A1.9080904@pdf.com>
	<971536df0607121759x56dc7c14k38f14e29696fe994@mail.gmail.com>
	<44B5A747.60306@pdf.com>
	<971536df0607121924m495802devbed554f664d270f6@mail.gmail.com>
	<971536df0607122004x4827e0f6pf4eb962cec10b5cd@mail.gmail.com>
	<44B631FC.1020507@pdf.com>
Message-ID: <971536df0607130450o3e1c05dw6814c98228b41dcc@mail.gmail.com>

Perhaps the Excel API requires an absolute path name.
Try:

chartr("/", "\\", file.path(getwd(), "myfile.xls"))

where we make sure its using backslashes in case that's also
required.

On 7/13/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> Hi, Gabor,
>
> Thanks for the code. When I tried this I get an error when trying to use
> a relative path name:
>
> read.excel <- function(file, sheet, ...) {
>   require(rcom)
>   require(gdata)
>   oxl <- comCreateObject('Excel.Application')
>   comSetProperty(oxl, "Visible", TRUE)  # this line optional
>   owb <- comGetProperty(oxl, "Workbooks")
>   ob <- comInvoke(owb, "Open", file)
>   osheets <- comGetProperty(ob, "Worksheets")
>   n <- comGetProperty(osheets, "Count")
>   ithSheetName <- function(i)
>     comGetProperty(comGetProperty(osheets, "Item", i), "Name")
>   sheetNames <- sapply(1:n, ithSheetName)
>   comInvoke(oxl, "Quit")
>   read.xls(file, match(sheet, sheetNames), ...)
> }
>
>  > read.excel("tmp.xls", "Sheet2", na.strings = "na")
> Error in 1:n : NA/NaN argument
>  > read.excel("D:/Users/sundard/frm/config/R/tmp.xls",
> +            "Sheet2", na.strings = "na")
>       x
> 1 0.11
> 2 0.11
> 3   NA
> 4   NA
> 5   NA
> 6   NA
> 7 0.11
>
> Any reason I need an absolute path?
>
> Thanks again,
>
> --sundar
>
> Gabor Grothendieck wrote:
> > In thinking about this some more I have a better idea.  Use rcom (or
> > RDCOMClient)
> > to get a list of the sheet names and then use that to determine which sheet you
> > need.  Then use read.xls to get it like this assuming that the Excel
> > file and path are C:\test.xls and that one of the sheets in that spreadsheet
> > is xyz.  In my version the na.strings had a space at the end so you may
> > need to change the na.strings= setting:
> >
> > library(rcom)
> > xls <- "C:\\test.xls"
> > oxl <- comCreateObject('Excel.Application')
> > comSetProperty(oxl, "Visible", TRUE)  # this line optional
> > owb <- comGetProperty(oxl, "Workbooks")
> > ob <- comInvoke(owb, "Open", xls)
> > osheets <- comGetProperty(ob, "Worksheets")
> > n <- comGetProperty(osheets, "Count")
> > ithSheetName <- function(i)
> >       comGetProperty(comGetProperty(osheets, "Item", i), "Name")
> > sheetNames <- sapply(1:n, ithSheetName)
> > comInvoke(oxl, "Quit")
> >
> > library(gdata)
> > read.xls(xls, match("xyz", sheetNames), na.strings = "na ")
> >
> >
> > On 7/12/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >
> >>Would it be good enough to just read all the sheets in?
> >>
> >>The perl program can do that and although the read.xls R function does not
> >>interface to that aspect of its functionality its not that difficult to access
> >>it yourself.  Assume your excel file is in \test.xls .  Just
> >>switch to that folder.  paste together a command to run the perl
> >>program, run it, get a list of the file names it produced and read them in:
> >>
> >>library(gdata)
> >>setwd("/")
> >>cmd <- paste("perl", system.file("perl/xls2csv.pl", package = "gdata"), "test")
> >>system(cmd)
> >>ff <- list.files(patt = "test_Sheet.*.csv")
> >>sapply(ff, read.csv, na.strings = "na ", simplify = FALSE)
> >>
> >>
> >>On 7/12/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> >>
> >>>Hi, Gabor,
> >>>
> >>>Thanks for the reply. Perhaps Prof. Ripley will enlighten us as he is
> >>>the RODBC maintainer.
> >>>
> >>>Unfortunately, gdata::read.xls will not work for me (at least I don't
> >>>think it will) because I need to refer to each worksheet by name and not
> >>>by number. For example, I need extract data from "Sheet1" and not simply
> >>>the first sheet.
> >>>
> >>>Thanks,
> >>>
> >>>--sundar
> >>>
> >>>Gabor Grothendieck wrote:
> >>>
> >>>>I also got a strange result too (I renamed it sdr.read.xls
> >>>>to distinguish it from read.xls in gdata and noticed that a
> >>>>space got into my na's somehow so I used "na " for my
> >>>>na.strings:
> >>>>
> >>>>
> >>>>
> >>>>>sdr.read.xls("/test.xls", "Sheet2", na.strings = "na ")
> >>>>
> >>>>     x
> >>>>1 <NA>
> >>>>2 <NA>
> >>>>3   na
> >>>>4   na
> >>>>5   na
> >>>>6   na
> >>>>7 <NA>
> >>>>
> >>>>I had more success using read.xls in the gdata package.
> >>>>Note that we need to install perl first if not already present:
> >>>>
> >>>>
> >>>>
> >>>>>library(gdata)  # for read.xls
> >>>>>read.xls("/test.xls", 2, na.strings = "na ")
> >>>>
> >>>>     x
> >>>>1 0.11
> >>>>2 0.11
> >>>>3   NA
> >>>>4   NA
> >>>>5   NA
> >>>>6   NA
> >>>>7 0.11
> >>>>
> >>>>
> >>>>
> >>>>>R.version.string # XP
> >>>>
> >>>>[1] "Version 2.3.1 Patched (2006-06-04 r38279)"
> >>>>
> >>>>
> >>>>>packageDescription("gdata")$Version
> >>>>
> >>>>[1] "2.1.2"
> >>>>
> >>>>
> >>>>>packageDescription("RODBC")$Version
> >>>>
> >>>>[1] "1.1-7"
> >>>>
> >>>>
> >>>>On 7/12/06, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
> >>>>
> >>>>
> >>>>>Hi, all,
> >>>>>
> >>>>>I'm trying to use RODBC to read data from Excel. However, I'm having
> >>>>>trouble converting missing values to NA and rather perplexed by the
> >>>>>output. Below illustrates my problem:
> >>>>>
> >>>>>## DATA - copy to Excel and save as "tmp.xls"
> >>>>>## tmp.xls!Sheet1
> >>>>>x
> >>>>>0.11
> >>>>>0.11
> >>>>>na
> >>>>>na
> >>>>>na
> >>>>>0.11
> >>>>>
> >>>>>## tmp.xls!Sheet2
> >>>>>x
> >>>>>0.11
> >>>>>0.11
> >>>>>na
> >>>>>na
> >>>>>na
> >>>>>na
> >>>>>0.11
> >>>>>
> >>>>>## R Code
> >>>>>read.xls <- function(file, sheet = "Sheet1", ...) {
> >>>>> require(RODBC)
> >>>>> channel <- odbcConnectExcel(file)
> >>>>> sheet <- sprintf("select * from `%s$`", sheet)
> >>>>> x <- sqlQuery(channel, sheet, ...)
> >>>>> odbcClose(channel)
> >>>>> x
> >>>>>}
> >>>>>
> >>>>>read.xls("./tmp.xls", "Sheet1", na.strings = "na")
> >>>>>## works as expected
> >>>>>#     x
> >>>>>#1 0.11
> >>>>>#2 0.11
> >>>>>#3   NA
> >>>>>#4   NA
> >>>>>#5   NA
> >>>>>#6 0.11
> >>>>>
> >>>>>read.xls("./tmp.xls", "Sheet2", na.strings = "na")
> >>>>>## Huh? What happened?
> >>>>>#   x
> >>>>>#1 NA
> >>>>>#2 NA
> >>>>>#3 NA
> >>>>>#4 NA
> >>>>>#5 NA
> >>>>>#6 NA
> >>>>>#7 NA
> >>>>>
> >>>>>
> >>>>>>sessionInfo()
> >>>>>
> >>>>>Version 2.3.1 (2006-06-01)
> >>>>>i386-pc-mingw32
> >>>>>
> >>>>>attached base packages:
> >>>>>[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> >>>>>[7] "base"
> >>>>>
> >>>>>other attached packages:
> >>>>> RODBC
> >>>>>"1.1-7"
> >>>>>
> >>>>>______________________________________________
> >>>>>R-help at stat.math.ethz.ch mailing list
> >>>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>>>>
> >>>>
> >>>>
> >>>>______________________________________________
> >>>>R-help at stat.math.ethz.ch mailing list
> >>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ottorini at nancy.inra.fr  Thu Jul 13 13:51:48 2006
From: ottorini at nancy.inra.fr (Jean-Marc Ottorini)
Date: Thu, 13 Jul 2006 13:51:48 +0200
Subject: [R] Access to conditioning values in "xyplot"
Message-ID: <03ed4edff96c1355ccd3d2239aa374ba@nancy.inra.fr>


Dear R-help subscribers,

    Many thanks for all the answers I received either by mail or through 
the list, and that were most helpful.

  For the sake of the list records, I wanted to post the solution, I 
eventually could obtain, to the problem I  have submitted.

  I am summarizing  the question here. It was how to use the panel 
function "panel.curve" when the expression used in this function to to 
add a fitted line to the points in each panel depends not only on x, 
but also on the value of the conditioning variable for the considered 
panel.

This solution is based on hints given to me by Deepayan Sarkar, it 
relies on the use of "packet.number" and "panel.number":

xyplot(n ~ cg | di, data = myData,
        scale = list(y = "free", x = "free"),
        groups = bloc,
        as.table = T,
        xlab = "Cg",
        ylab = "N / ha",
        panel = function(x, y, subscripts, groups, packet.number = di, 
panel.number, ...) {
          panel.grid(h = -1, v = -1, col = "grey", lwd = 1, lty = 2)
          panel.curve(expr = 
f.fit(unique(rev(packet.number))[panel.number], a, b, a1, b1, x),
                      n = 50, curve.type = "l", col = "lightblue", ...)
          panel.superpose(x, y, pch = c(1, 2), col = 
c("deeppink","blue"),
                          panel.groups = "panel.xyplot", subscripts, 
groups)
        },

        key = list( space = "top", transparent = TRUE, columns = 2,
          points = list( pch = c(1, 2), col = c("deeppink", "blue") ),
          text = list( c("bloc 3", "bloc 4"))),
        )

Some comments:

- the arguments  "packet.number = di, panel.number" must appear on the 
panel function prototype
- a, b, a1, b1 are parameters set to a certain value before calling 
"xyplot"
- "unique" is needed to reduce to 1 the multiple instances of the value 
of the conditioning variable di for a given panel (one for each point)
- I was surprised by the need to use "rev" in order for the interval of 
the values to draw the curve  f.fit to correspond to the right panel.

Any further comment would be welcome

Best regards

Jean-Marc
  ----
Jean-Marc Ottorini               LERFoB, UMR INRA-ENGREF 1092
  email  ottorini at nancy.inra.fr          INRA - Centre de Nancy
  voice  +33-0383-394046                    F54280 - Champenoux
  fax    +33-0383-394034                                 France


From p.dalgaard at biostat.ku.dk  Thu Jul 13 13:59:14 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Jul 2006 13:59:14 +0200
Subject: [R] Extracting Phi from gls/lme
In-Reply-To: <Pine.LNX.4.10.10607131220250.12144-100000@mercury.quantex>
References: <Pine.LNX.4.10.10607131220250.12144-100000@mercury.quantex>
Message-ID: <x28xmx90ml.fsf@turmalin.kubism.ku.dk>

John Logsdon <j.logsdon at quantex-research.com> writes:

> I am trying to extract into a scalar the value of Phi from the printed
> output of gls or lme using corAR1 correlation.  ie I want the estimate of
> the autocorrelation.  I can't see how to do this and haven't seen it
> anywhere in str(model.lme).
> 
> I can get all the other information - fixed and random effects etc.
> 
> Is there an obvious way so that I can save the brick wall some damage?

Just be soft in the head...

Seriously, I think the recipe is to drill down into model.lme until
you find the corAR1 class object,  like this, I think.

x <- model.lme$modelStruct$corStruct
coef(x,unconstrained=FALSE). 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From j.logsdon at quantex-research.com  Thu Jul 13 14:29:47 2006
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Thu, 13 Jul 2006 13:29:47 +0100 (GMT)
Subject: [R] Extracting Phi from gls/lme
In-Reply-To: <x28xmx90ml.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.10.10607131315270.12144-100000@mercury.quantex>

Peter

This looks very promising:

x<-mod.lme$model$Struct$corStruct
Correlation structure of class corAR1 representing
       Phi
-0.1996813

which is the value I want.  

Yippee (save the bricks)

But:

coef(x,unconstrained=FALSE)
[1] -0.4048011

and any attempt to coerce x into a scalar always returns -0.404...

This is not an obvious transformation of the -0.1996813 I think.

Looking at str(x) returns the first line:

Classes 'corAR1', 'corStruct' atomic [1:1] -0.405

Not a -0.199 ... in sight in the attributes various.

How does summary.lme/gls do it?

Best wishes

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com


On 13 Jul 2006, Peter Dalgaard wrote:

> John Logsdon <j.logsdon at quantex-research.com> writes:
> 
> > I am trying to extract into a scalar the value of Phi from the printed
> > output of gls or lme using corAR1 correlation.  ie I want the estimate of
> > the autocorrelation.  I can't see how to do this and haven't seen it
> > anywhere in str(model.lme).
> > 
> > I can get all the other information - fixed and random effects etc.
> > 
> > Is there an obvious way so that I can save the brick wall some damage?
> 
> Just be soft in the head...
> 
> Seriously, I think the recipe is to drill down into model.lme until
> you find the corAR1 class object,  like this, I think.
> 
> x <- model.lme$modelStruct$corStruct
> coef(x,unconstrained=FALSE). 
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>


From roderick.castillo at metanomics.de  Thu Jul 13 15:21:24 2006
From: roderick.castillo at metanomics.de (roderick.castillo at metanomics.de)
Date: Thu, 13 Jul 2006 15:21:24 +0200
Subject: [R] Problem installing ROracle
Message-ID: <OFC4760ECF.4BDB713D-ONC12571AA.004417F5-C12571AA.004950FC@basf-c-s.be>

Hello
I have tried to install ROracle (V. 0.5-7 and 0.5-5) the usual way
1 - set/export variables of Oracle environment (ORACLE_HOME,
LD_LIBRARY_PATH, PATH)
2 - R CMD INSTALL ROracle_0.5-7.tar.gz (R Version 2.3.1, patched per 11.
July, and other versions)

but I get following error:

checking for gcc... gcc
checking for C compiler default output... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
configure: creating ./config.status
config.status: creating src/Makevars
sed: file ./confstat6HLjQx/subs-1.sed line 43: Unterminated `s' command
config.status: creating src/Makefile
sed: file ./confstat6HLjQx/subs-1.sed line 43: Unterminated `s' command
** libs
** arch -
make: *** Keine Targets..  Schluss.
ERROR: compilation failed for package 'ROracle'
** Removing '/localhome/tuner/R-patched/library/ROracle'

>From German:
      "Keine Targets"= no targets
      "Schluss"= finished

It seems to be some trivial sed error, but I can't figure out which file to
edit in order
to correct it. This is using Linux RHAS 2.1 with gcc V. 2.96 and V. 3.4.4,
and
sed (GNU) Version 3.02

Thanks a lot

Rick


From alessandro at idsia.ch  Thu Jul 13 14:44:49 2006
From: alessandro at idsia.ch (Alessandro Antonucci)
Date: Thu, 13 Jul 2006 14:44:49 +0200
Subject: [R] Problems plotting a function defined as a product
Message-ID: <20060713124449.GA7935@idsia.ch>

In order to define a function f as:

> f <- function(x) (x+1)*(x+2)

I want to use the notation:

> v = c(1,2)
> g <- function(x) prod((v+x))

That apparently works and, for instance,
the loop:

>for (i in 1:100) {  print(f(i)-g(i)) }

Produces a sequence of zeros.

Nevertheless, if I try to plot
the function g by:

>t = seq(0,100,1)
>plot(t,g(t),type="l")

I obtain the following errors/warning:

> Error in xy.coords(x, y, xlabel, ylabel, log) : 
>        'x' and 'y' lengths differ
>In addition: Warning message:
>longer object length
>        is not a multiple of shorter object length in: v + x 
>Execution halted

Any idea about that?

Kind regards,
Alessandro


-- 
============================================================
Alessandro Antonucci
Dalle Molle Institute for Artificial Intelligence (IDSIA)
at Idsia			e-mail: alessandro at idsia.ch
Galleria 2			web:   idsia.ch/~alessandro
Via Cantonale			mobile:   +39 339-567-23-28
CH-6928				tel:       +41 58-666-66-69
Manno - Lugano			fax:       +41 58-666-66-61
Switzerland			skype: alessandro.antonucci


From K.Birycki at DomBank.pl  Thu Jul 13 14:40:43 2006
From: K.Birycki at DomBank.pl (Birycki Konrad DomBank Warszawa)
Date: Thu, 13 Jul 2006 14:40:43 +0200
Subject: [R] step method in glm()
Message-ID: <13419A75C284364EAF87B80D7CA52C07AA8F18@ex000a01.gbg.pl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060713/2a7f09d6/attachment.pl 

From jacques.veslot at good.ibl.fr  Thu Jul 13 14:49:11 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Thu, 13 Jul 2006 14:49:11 +0200
Subject: [R] Problems plotting a function defined as a product
In-Reply-To: <20060713124449.GA7935@idsia.ch>
References: <20060713124449.GA7935@idsia.ch>
Message-ID: <44B64147.7090500@good.ibl.fr>

plot(t, sapply(t,g))
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Alessandro Antonucci a ?crit :
> In order to define a function f as:
> 
> 
>>f <- function(x) (x+1)*(x+2)
> 
> 
> I want to use the notation:
> 
> 
>>v = c(1,2)
>>g <- function(x) prod((v+x))
> 
> 
> That apparently works and, for instance,
> the loop:
> 
> 
>>for (i in 1:100) {  print(f(i)-g(i)) }
> 
> 
> Produces a sequence of zeros.
> 
> Nevertheless, if I try to plot
> the function g by:
> 
> 
>>t = seq(0,100,1)
>>plot(t,g(t),type="l")
> 
> 
> I obtain the following errors/warning:
> 
> 
>>Error in xy.coords(x, y, xlabel, ylabel, log) : 
>>       'x' and 'y' lengths differ
>>In addition: Warning message:
>>longer object length
>>       is not a multiple of shorter object length in: v + x 
>>Execution halted
> 
> 
> Any idea about that?
> 
> Kind regards,
> Alessandro
> 
>


From f.harrell at vanderbilt.edu  Thu Jul 13 15:11:52 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 13 Jul 2006 08:11:52 -0500
Subject: [R] Keep value lables with data frame manipulation
In-Reply-To: <3.0.6.32.20060713095939.00ace1f8@pop.gmx.net>
References: <2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>	<2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>
	<3.0.6.32.20060713095939.00ace1f8@pop.gmx.net>
Message-ID: <44B64698.3010202@vanderbilt.edu>

Heinz Tuechler wrote:
> At 13:14 12.07.2006 -0500, Marc Schwartz (via MN) wrote:
>> On Wed, 2006-07-12 at 17:41 +0100, Jol, Arne wrote:
>>> Dear R,
>>>
>>> I import data from spss into a R data.frame. On this rawdata I do some
>>> data processing (selection of observations, normalization, recoding of
>>> variables etc..). The result is stored in a new data.frame, however, in
>>> this new data.frame the value labels are lost.
>>>
>>> Example of what I do in code:
>>>
>>> # read raw data from spss
>>> rawdata <- read.spss("./data/T50937.SAV",
>>> 	use.value.labels=FALSE,to.data.frame=TRUE)
>>>
>>> # select the observations that we need
>>> diarydata <- rawdata[rawdata$D22==2 | rawdata$D22==3 | rawdata$D22==17 |
>>> rawdata$D22==18 | rawdata$D22==20 | rawdata$D22==22 |
>>>  			rawdata$D22==24 | rawdata$D22==33,]
>>>
>>> The result is that rawdata$D22 has value labels and that diarydata$D22
>>> is numeric without value labels.
>>>
>>> Question: How can I prevent this from happening?
>>>
>>> Thanks in advance!
>>> Groeten,
>>> Arne
>> Two things:
>>
>> 1. With respect to your subsetting, your lengthy code can be replaced
>> with the following:
>>
>>  diarydata <- subset(rawdata, D22 %in% c(2, 3, 17, 18, 20, 22, 24, 33))
>>
>> See ?subset and ?"%in%" for more information.
>>
>>
>> 2. With respect to keeping the label related attributes, the
>> 'value.labels' attribute and the 'variable.labels' attribute will not by
>> default survive the use of "[".data.frame in R (see ?Extract
>> and ?"[.data.frame").
>>
>> On the other hand, based upon my review of ?read.spss, the SPSS value
>> labels should be converted to the factor levels of the respective
>> columns when 'use.value.labels = TRUE' and these would survive a
>> subsetting.
>>
>> If you want to consider a solution to the attribute subsetting issue,
>> you might want to review the following post by Gabor Grothendieck in
>> May, which provides a possible solution:
>>
>>  https://stat.ethz.ch/pipermail/r-help/2006-May/106308.html
>>
>> and this post by me, for an explanation of what is happening in Gabor's
>> solution:
>>
>>  https://stat.ethz.ch/pipermail/r-help/2006-May/106351.html
>>
>> HTH,
>>
>> Marc Schwartz
>>
> Hello Mark and Arne,
> 
> I worked on the suggestions of Gabor and Mark and programmed some functions
> in this way, but they are very, very preliminary (see below).
> In my view there is a lack of convenient possibilities in R to document
> empirical data by variable labels, value labels, etc. I would prefer to
> have these possibilities in the "standard" configuration.
> So I sketched a concept, but in my view it would only be useful, if there
> was some acceptance by the core developers of R.
> 
> The concept would be to define a class. For now I call it "source.data".
> To design it more flexible than the Hmisc class "labelled" I would define a
> related option "source.data.attributes" with default c('value.labels',
> 'variable.name', 'label')). This option contains all attributes that should
> persist in subsetting/indexing.
> 
> I made only some very, very preliminary tests with these functions, mainly
> because I am not happy with defining a new class. Instead I would prefer,
> if this functionality could be integrated in the Hmisc class "labelled",
> since this is in my view the best known starting point for data
> documentation in R.
> 
> I would be happy, if there were some discussion about the wishes/needs of
> other Rusers concerning data documentation.
> 
> Greetings,
> 
> Heinz

I feel that separating variable labels and value labels and just using 
factors for value labels works fine, and I would urge you not to create 
a new system that will not benefit from the many Hmisc functions that 
use variable labels and units.  [.data.frame in Hmisc keeps all attributes.

Frank

> 
> 
> ### intention and concept
> #   There should be a convenient possibility to keep source data numerical
> #   coded and at the same time have labelled categories.
> #   Such labelled categorical numerical data should be easily converted
> #   to factors.
> #   Indexing/subsetting should preserve the concerned attributes of this data.
> 
> ### description of (intended!!!) functionality
> #   - a class source.data is defined. It is intended only for atomic objects.
> #   - option source.data.attributes defines which attributes will be copied
> #     in indexing/subsetting objects of class source.data
> #   - option source.data.is.ordered sets defining factors as ordered, when
> #     built from objects of class source.data by the function factsd
> #   - function 'value.labels<-' assigns an attribute value.labels and sets
> #     class source.data
> #   - function value.labels reads the attribute value.labels
> #   - the indexing method '[.source.data' defines indexing for source.data
> #   - the print method print.source.data ignores source.data.attributes in
> #     printing
> #   - the as.data.frame method as.data.frame.source.data enables inclusion
> #     of objects of class source.data in data.frames
> #   - function factsd should in general behave as function factor but should
> #     in case of an object of class source.data by default use the
> value.labels
> #     as levels and the names(value.labels) as the labels of the new built
> #     factor.
> #     If the parameter ordered is NULL it should create ordered factors
> #     according to the option source.data.is.ordered.
> 
> ### set option for source.data.attributes
> options(source.data.attributes=c('value.labels', 'variable.name', 'label'))
> ### set option for converting source.data class in ordered factors
> options(source.data.is.ordered=TRUE)
> 
> ### function to assign value.labels
> 'value.labels<-' <- function (x, value)
>   ## adapted from Hmisc function label 30.6.2006
> {
>   if(!is.atomic(x)) stop('value.labels<- is applicabel to atomic objects
> only')
>   structure(x, value.labels = value, class = c("source.data",
>                attr(x, "class")[attr(x, "class") != "source.data"]))
> }
> 
> ### function to read value.labels
> value.labels <- function (x) { attr(x, 'value.labels') }
> 
> ### definition of indexing method for class=source.data
> ##  source.data.attributes shall be conserved
> "[.source.data" <- function(x, ...)
> {
>   atr <- attributes(x)
>   atr.names <- names(atr)
>   sda <- options()$'source.data.attributes'
>   sda.match <- match(atr.names, sda)
>   sda.match <- sda.match[!is.na(sda.match)]
>   x <- NextMethod("[")
>   ## assign source.data.attributes to result
>   if(length(sda.match))
>     for (i in sda.match) attr(x, sda[i]) <- atr[[sda[i]]]
>   ## assign class source.data to result
>   class(x) <- c('source.data', attr(x, "class")[attr(x, "class")
>                                                 != "source.data"])       
>   x
> }
> 
> ### print method for source.data
> 'print.source.data' <- function (x, ...) 
> {
>   ## adapted from Hmisc print.labelled 31.5.2006
>   x.orig <- x
>   ## look if there are source.data.attributes
>   sda <- options()$'source.data.attributes'
>   sda.match <- match(names(attributes(x)), sda)
>   sda.match <- sda.match[!is.na(sda.match)]
>   ## delete source.data.attributes for printing
>   if(length(sda.match))
>     for (i in sda.match) attr(x, sda[i]) <- NULL
>   ## delete class source.data for printing
>   class(x) <- if (length(class(x)) == 1 && class(x) == "source.data") 
>     NULL
>   else class(x)[class(x) != "source.data"]
>   NextMethod("print")
>   invisible(x.orig)
> }
> 
> ### Define function as.data.frame.source.data (copy from as.data.frame.vector)
> #   many as.data.frame methods are identical to this
> ##  different functions as.data.frame are besides others:
> #   as.data.frame.list, as.data.frame.default, as.data.frame.data.frame,
> #   as.data.frame.character, as.data.frame.AsIs, as.data.frame.array,
> 
> as.data.frame.source.data <- 
>   function (x, row.names = NULL, optional = FALSE)
>   ## copy from as.data.frame.vector 1.6.2006
> {
>   nrows <- length(x)
>   nm <- paste(deparse(substitute(x), width.cutoff = 500), collapse = " ")
>   if (is.null(row.names)) {
>     if (nrows == 0) 
>       row.names <- character(0)
>     else if (length(row.names <- names(x)) == nrows &&
> !any(duplicated(row.names))) {
>     }
>     else if (optional) 
>       row.names <- character(nrows)
>     else row.names <- as.character(1:nrows)
>   }
>   names(x) <- NULL
>   value <- list(x)
>   if (!optional) 
>     names(value) <- nm
>   attr(value, "row.names") <- row.names
>   class(value) <- "data.frame"
>   value
> }
> 
> ### function to create factor from source.data class applying variable.labels
> #   and copying all source.data.attributes
> #   remark: factor(factsd(x)) drops unused factor levels and source.data class
> #           factsd(x)[, drop=TRUE] drops unused factor levels but keeps
> #           source.data class and attributes
> 
> factsd <- function(x = character(),
>                    levels = sort(unique.default(x), na.last = TRUE),
>                    labels = levels, exclude = NA, ordered = NULL)
> {
>   ## check if is of class source.data
>   if ('source.data' %in% class(x))
>     {
>       if(is.null(ordered)) ordered <- options()$source.data.is.ordered
>       fx <- factor(x = x, levels = value.labels(x),
>                    labels = names(value.labels(x)),
>                    exclude = exclude,
>                    ordered = ordered)
>       ## copy source.data.attributes
>       atr <- attributes(x)
>       atr.names <- names(atr)
>       sda <- options()$'source.data.attributes'
>       sda.match <- match(atr.names, sda)
>       sda.match <- sda.match[!is.na(sda.match)]
>       ## assign source.data.attributes to result
>       if(length(sda.match))
>         for (i in sda.match) attr(fx, sda[i]) <- atr[[sda[i]]]
>       ## add class source.data to result
>       class(fx) <- c('source.data', attr(fx, 'class'))       
>     }
>   else {
>     if(is.null(ordered)) ordered <- is.ordered(x)
>     fx <- factor(x = x, levels = levels, labels = labels,
>                  exclude = exclude, ordered = ordered)
>   }
>   fx
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From ripley at stats.ox.ac.uk  Thu Jul 13 15:04:15 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 13 Jul 2006 14:04:15 +0100 (BST)
Subject: [R] step method in glm()
In-Reply-To: <13419A75C284364EAF87B80D7CA52C07AA8F18@ex000a01.gbg.pl>
References: <13419A75C284364EAF87B80D7CA52C07AA8F18@ex000a01.gbg.pl>
Message-ID: <Pine.LNX.4.64.0607131401280.9335@gannet.stats.ox.ac.uk>

As the posting guide requested of you:

0) Do not send HTML mail.

1) Tell us your version of R.  Some problems in this area have been 
corrected recently: if it is not at least 2.3.1, update as asked.

2) Give a reproducible example.

On Thu, 13 Jul 2006, Birycki Konrad DomBank Warszawa wrote:

> Hello,
>  
> I estimaded two logit models via glm(). A null model (called glm00) and
> "full" model with all accessible covariates and interactions between them
> (glm1).
>  
> Then I tried to get even better model by step procedure. I tried the
> following code:
>  
> > step(glm00, scope=formula(glm1), method="both")   
>  
> and another one:
>  
> > step(glm00, scope=formula(glm1), method="forward")
>  
> In both cases step procedure terminated after several steps with a warning:
>  
> > Error in factor.scope(ffac, list(add = fadd, drop = fdrop)) : 
>             > upper scope does not include model
>  
> What does the word "model" refer to? To glm00 (the null model)?  To my
> knowledge and understanding, the initial null model should have been
> included in every step of the search. 
>  
> Am I wrong? 
>  
> Thanks for all comments :-)
>  
> Konrad Birycki
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From dnlsutcliffe at yahoo.co.uk  Thu Jul 13 15:08:33 2006
From: dnlsutcliffe at yahoo.co.uk (Daniel sutcliffe)
Date: Thu, 13 Jul 2006 14:08:33 +0100 (BST)
Subject: [R]  ts and stl functions - still a problem
In-Reply-To: <2DC3A0F67A1E894BB03AEF915D9BC3F17CAC18@srmessagerie.chm.com>
Message-ID: <20060713130834.64028.qmail@web26908.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060713/4ba32e06/attachment.pl 

From sachinj.2006 at yahoo.com  Thu Jul 13 15:08:58 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Thu, 13 Jul 2006 06:08:58 -0700 (PDT)
Subject: [R] AICc vs AIC for model selection
Message-ID: <20060713130858.25902.qmail@web37613.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060713/73bd2595/attachment.pl 

From g.comte at alliance-ir.net  Thu Jul 13 15:26:29 2006
From: g.comte at alliance-ir.net (COMTE Guillaume)
Date: Thu, 13 Jul 2006 15:26:29 +0200
Subject: [R] colors on graph
Message-ID: <15C100200A5F4E45AF8CFB45A926EF3415E8FA@allexch01.alliance.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060713/b7d79f4d/attachment.pl 

From p.dalgaard at biostat.ku.dk  Thu Jul 13 15:24:11 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Jul 2006 15:24:11 +0200
Subject: [R] Extracting Phi from gls/lme
In-Reply-To: <Pine.LNX.4.10.10607131315270.12144-100000@mercury.quantex>
References: <Pine.LNX.4.10.10607131315270.12144-100000@mercury.quantex>
Message-ID: <x2y7uxeiys.fsf@turmalin.kubism.ku.dk>

John Logsdon <j.logsdon at quantex-research.com> writes:

> Peter
> 
> This looks very promising:
> 
> x<-mod.lme$model$Struct$corStruct
> Correlation structure of class corAR1 representing
>        Phi
> -0.1996813
> 
> which is the value I want.  
> 
> Yippee (save the bricks)
> 
> But:
> 
> coef(x,unconstrained=FALSE)
> [1] -0.4048011
> 
> and any attempt to coerce x into a scalar always returns -0.404...

Works for me. Is there a rogue coef() around?? Or did you misspell
"unconstrained" and not tell us?

> coef(x,uncostrained=FALSE)
[1] 0.1171201
> coef(x,unconstrained=FALSE)
       Phi
0.05849318
 

 
> This is not an obvious transformation of the -0.1996813 I think.

Obvious is in the eyes of the beholder, but:

> aux <- exp(-0.4048011)
> (aux - 1) / (aux + 1)
[1] -0.1996813

> Looking at str(x) returns the first line:
> 
> Classes 'corAR1', 'corStruct' atomic [1:1] -0.405
> 
> Not a -0.199 ... in sight in the attributes various.
> 
> How does summary.lme/gls do it?

I don't think they do anything, but their print methods call
print.corStruct which calls coef.corAR1. And

> getAnywhere(coef.corAR1)
A single object matching coef.corAR1 was found
It was found in the following places
  registered S3 method for coef from namespace nlme
  namespace:nlme
with value

function (object, unconstrained = TRUE, ...)
{
    if (unconstrained) {
        if (attr(object, "fixed")) {
            return(numeric(0))
        }
        else {
            return(as.vector(object))
        }
    }
    aux <- exp(as.vector(object))
    aux <- c((aux - 1)/(aux + 1))
    names(aux) <- "Phi"
    aux
}
<environment: namespace:nlme>
> aux <- exp(-0.4048011


 
> Best wishes
> 
> John
> 
> John Logsdon                               "Try to make things as simple
> Quantex Research Ltd, Manchester UK         as possible but not simpler"
> j.logsdon at quantex-research.com              a.einstein at relativity.org
> +44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com
> 
> 
> On 13 Jul 2006, Peter Dalgaard wrote:
> 
> > John Logsdon <j.logsdon at quantex-research.com> writes:
> > 
> > > I am trying to extract into a scalar the value of Phi from the printed
> > > output of gls or lme using corAR1 correlation.  ie I want the estimate of
> > > the autocorrelation.  I can't see how to do this and haven't seen it
> > > anywhere in str(model.lme).
> > > 
> > > I can get all the other information - fixed and random effects etc.
> > > 
> > > Is there an obvious way so that I can save the brick wall some damage?
> > 
> > Just be soft in the head...
> > 
> > Seriously, I think the recipe is to drill down into model.lme until
> > you find the corAR1 class object,  like this, I think.
> > 
> > x <- model.lme$modelStruct$corStruct
> > coef(x,unconstrained=FALSE). 
> > 
> > -- 
> >    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
> >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> >  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
> > 
> 
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From jacques.veslot at good.ibl.fr  Thu Jul 13 15:31:34 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Thu, 13 Jul 2006 15:31:34 +0200
Subject: [R] colors on graph
In-Reply-To: <15C100200A5F4E45AF8CFB45A926EF3415E8FA@allexch01.alliance.com>
References: <15C100200A5F4E45AF8CFB45A926EF3415E8FA@allexch01.alliance.com>
Message-ID: <44B64B36.90001@good.ibl.fr>

?image
Cf. See Also
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


COMTE Guillaume a ?crit :
> Hy all,
> 
>  
> 
> I need to draw something in 2 dimension that has 3 dimension, the choice
> has been made to use colors to display the third dimension into the
> graph.
> 
>  
> 
> Has someone done something like that, i can't figure out how to
> parametize the colors.
> 
>  
> 
> Thks for all ideas, 
> 
>  
> 
> COMTE Guillaume
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From fomchenko at iet.ru  Thu Jul 13 15:46:44 2006
From: fomchenko at iet.ru (Denis Fomchenko)
Date: Thu, 13 Jul 2006 17:46:44 +0400
Subject: [R] sem question
Message-ID: <003401c6a682$c7619be0$0f22a8c0@intiet.ru>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060713/b027745d/attachment.pl 

From tuechler at gmx.at  Thu Jul 13 15:48:55 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Thu, 13 Jul 2006 14:48:55 +0100
Subject: [R] Keep value lables with data frame manipulation
In-Reply-To: <44B64698.3010202@vanderbilt.edu>
References: <3.0.6.32.20060713095939.00ace1f8@pop.gmx.net>
	<2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>
	<2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>
	<3.0.6.32.20060713095939.00ace1f8@pop.gmx.net>
Message-ID: <3.0.6.32.20060713144855.00a5e700@pop.gmx.net>

At 08:11 13.07.2006 -0500, Frank E Harrell Jr wrote:
>Heinz Tuechler wrote:
>> At 13:14 12.07.2006 -0500, Marc Schwartz (via MN) wrote:
>>> On Wed, 2006-07-12 at 17:41 +0100, Jol, Arne wrote:
>>>> Dear R,
>>>>
>>>> I import data from spss into a R data.frame. On this rawdata I do some
>>>> data processing (selection of observations, normalization, recoding of
>>>> variables etc..). The result is stored in a new data.frame, however, in
>>>> this new data.frame the value labels are lost.
>>>>
>>>> Example of what I do in code:
>>>>
>>>> # read raw data from spss
>>>> rawdata <- read.spss("./data/T50937.SAV",
>>>> 	use.value.labels=FALSE,to.data.frame=TRUE)
>>>>
>>>> # select the observations that we need
>>>> diarydata <- rawdata[rawdata$D22==2 | rawdata$D22==3 | rawdata$D22==17 |
>>>> rawdata$D22==18 | rawdata$D22==20 | rawdata$D22==22 |
>>>>  			rawdata$D22==24 | rawdata$D22==33,]
>>>>
>>>> The result is that rawdata$D22 has value labels and that diarydata$D22
>>>> is numeric without value labels.
>>>>
>>>> Question: How can I prevent this from happening?
>>>>
>>>> Thanks in advance!
>>>> Groeten,
>>>> Arne
>>> Two things:
>>>
>>> 1. With respect to your subsetting, your lengthy code can be replaced
>>> with the following:
>>>
>>>  diarydata <- subset(rawdata, D22 %in% c(2, 3, 17, 18, 20, 22, 24, 33))
>>>
>>> See ?subset and ?"%in%" for more information.
>>>
>>>
>>> 2. With respect to keeping the label related attributes, the
>>> 'value.labels' attribute and the 'variable.labels' attribute will not by
>>> default survive the use of "[".data.frame in R (see ?Extract
>>> and ?"[.data.frame").
>>>
>>> On the other hand, based upon my review of ?read.spss, the SPSS value
>>> labels should be converted to the factor levels of the respective
>>> columns when 'use.value.labels = TRUE' and these would survive a
>>> subsetting.
>>>
>>> If you want to consider a solution to the attribute subsetting issue,
>>> you might want to review the following post by Gabor Grothendieck in
>>> May, which provides a possible solution:
>>>
>>>  https://stat.ethz.ch/pipermail/r-help/2006-May/106308.html
>>>
>>> and this post by me, for an explanation of what is happening in Gabor's
>>> solution:
>>>
>>>  https://stat.ethz.ch/pipermail/r-help/2006-May/106351.html
>>>
>>> HTH,
>>>
>>> Marc Schwartz
>>>
>> Hello Mark and Arne,
>> 
>> I worked on the suggestions of Gabor and Mark and programmed some functions
>> in this way, but they are very, very preliminary (see below).
>> In my view there is a lack of convenient possibilities in R to document
>> empirical data by variable labels, value labels, etc. I would prefer to
>> have these possibilities in the "standard" configuration.
>> So I sketched a concept, but in my view it would only be useful, if there
>> was some acceptance by the core developers of R.
>> 
>> The concept would be to define a class. For now I call it "source.data".
>> To design it more flexible than the Hmisc class "labelled" I would define a
>> related option "source.data.attributes" with default c('value.labels',
>> 'variable.name', 'label')). This option contains all attributes that should
>> persist in subsetting/indexing.
>> 
>> I made only some very, very preliminary tests with these functions, mainly
>> because I am not happy with defining a new class. Instead I would prefer,
>> if this functionality could be integrated in the Hmisc class "labelled",
>> since this is in my view the best known starting point for data
>> documentation in R.
>> 
>> I would be happy, if there were some discussion about the wishes/needs of
>> other Rusers concerning data documentation.
>> 
>> Greetings,
>> 
>> Heinz
>
>I feel that separating variable labels and value labels and just using 
>factors for value labels works fine, and I would urge you not to create 
>a new system that will not benefit from the many Hmisc functions that 
>use variable labels and units.  [.data.frame in Hmisc keeps all attributes.
>
>Frank
>

Frank,

of course I aggree with you about the importance of Hmisc and as I said, I
do not want to define a new class, but in my view factors are no good
substitute for value labels.
As the language definition (version 2.3.1 (2006-06-05) Draft, page 7) says:
"Factors are currently implemented using an integer array to specify the
actual levels and a second array of names that are mapped to the integers.
Rather unfortunately users often make use of the implementation in order to
make some calculations easier." 
So, in my view, the levels represent the "values" of the factor.
This has inconveniencies if you want to use value labels in different
languages. Further I do not see a simple method to label numerical
variables. I often encounter discrete, but still metric data, as e.g. risk
scores. Usually it would be nice to use them in their original coding,
which may include zero or decimal places and to label them at the same time.
Personally at the moment I try to solve this problem by following a
suggestion of Martin, Dimitis and others to use names instead. I doubt,
however, that this is a good solution, but at least it makes it possible to
have the source data numerically coded and in this sense "language free"
(see first attempts of functions below).

Heinz


### These are very preliminary and untested versions.
### They are inteded only to demonstrate the concept, but not for productive
### work.

### function "value.names<-" - version 0.3.0 - 11.7.2006
### function to assign names of elements according to their value
##
##  value.names<-
##  - arguments:
##    - action 
##      - set:           alle eventuell vorhandenen names l?schen, valuenames
##                       setzen
##      - add.overwrite: leere und nicht leere names durch neue ersetzen
##      - add:           nur leere names durch neue ersetzen
##    - tolerance:       ordnet names den values innerhalb der Toleranz zu.
##                       Liegt ein Wert innerhalb des Toleranzbereiches
##                       mehrerer names, dann wird geringste Toleranz gew?hlt.
##    - round:           rounds values in value before matching
##                       This may lead to collapsing of different names in
##                       value to one name (and one value)
##    - col.str:         string used when collapsing several names
##    - others:          name for values not named by other names
##    - value:
##      
##
##  function description:
##  - x must be atomic, preferably numeric or character
##  - if tolerance is given, it must not be NA. tolerance < 0 is ignored
##  - to ensure consistency, value is processed by value.names()
##  - new.names are built by matching with/without tolerance
##  - new.names are assigned to names depending on argument action
##  - if argument others is given, others-name is assigned to all valid values
##    without name
##

"value.names<-" <- function(x, action='set', tolerance=NULL, round=NULL,
                            col.str=' ', others=NULL, value)
{
  ## checking parameters
  if(!is.atomic(x)) stop('x must be an atomic object')
  if(!is.null(tolerance) &&
     is.na(tolerance)) stop('if given, tolerance must not be NA')
  ## to ensure consistency, process value by value.names
  value <- value.names(value, round=round, col.str=col.str)
  ## delete values with NA-name from value
  value <- value[!is.na(names(value))]
  old.names <- names(x) # store original names
  ## -- building names
  ##    - matching with/without tolerance
  if(!is.null(tolerance) && tolerance > 0 && is.numeric(x))
    ##      - matching with tolerance
    { dif <- abs(outer(x, value, '-'))
      dif[dif>tolerance] <- NA
      within.tolerance <- apply(dif, 1, function(x) sum(!is.na(x)))
      old.option.warn <- options('warn')[[1]]
      options(warn=-1)
      min.dif <- apply(dif, 1, function(x) which(x==min(x, na.rm=TRUE))[1])
      options(warn=old.option.warn)
      new.names <- names(value)[min.dif] }
  else
    ##      - matching without tolerance, i.e. exact matching
    new.names <- names(value)[match( x, value)]
  ##      - matching names for NA-values
  if(length(names(value[is.na(value)]))==1)
    new.names[is.na(x)] <- names(value[is.na(value)])
  ## assign names depending on action
  if (action=='set') new.names <- new.names
  if (action=='add.overwrite') new.names[is.na(new.names)] <-
    old.names[is.na(new.names)]
  if (action=='add') new.names[!is.na(old.names)] <-
    old.names[!is.na(old.names)]
  ## assigning others-name to all valid values without name
  if (!is.null(others)) new.names[!is.na(x) & is.na(new.names)] <-
    as.character(others)
  names(x) <- new.names
  return(x)
}


### function value.names - version 0.3.0 - 11.7.2006
### function to return names of elements according to their value
##
##  - arguments:
##    - x         source vector with names for (some) elements
##                x must be atomic ().
##                If x is a factor, value will be a factor. Consequently
##                names are only seen, if unclass() or print.default is used.
##    - col.str:         string used when collapsing several names
##                       default: "/"
##    - round:           rounds values in x
##                       This may lead to collapsing of different names for
##                       one value of x to one name (and one value)
##
##  - value:
##  - vector of the same class as x with sorted unique values and their names,
##    NULL, if x is NULL
##    - NA-values in x appear at the end
##    - if there is a 1:1 realtion between values and names in x, value
##      contains all unique combinations of value and name.
##    - if identical values in x have different (non NA), names these names
##      get collapsed to one new name, seperated by the string col.str
##      This applies also to NA-values in x with different names.
##    - NA-names get suppressed, if non-NA-names for the same x-value exist.
##    - Differen values in x with identical names remain seperated.
##    - values in x without name appear in value with name NA

value.names <- function(x, col.str=' ', round=NULL) {
  ## checking parameters
  if(!is.atomic(x)) stop('x must be an atomic object')
## -- define function for pasting unique non empty names
  pasteunique <- function(names.i, col.str)
    { names.i <- sort(unique(names.i))
      names.i <- names.i[!names.i=='' & !is.na(names.i)] # exclude ''
      if (length(names.i))
        names.i <- paste(names.i, sep='', collapse=col.str)
      else names.i <- NA
      invisible(names.i)
    }
  ## branching: if x is.null or has no names
  if (is.null(x)) {
    return(NULL) }
  else {
    x <- sort(x, na.last=TRUE) # sort x
    if (!is.null(round)) x <- round(x, round)
    ## vector of unique values
    values <- unique(x, na.last = TRUE)
    ## names per value
    nam <- NA
    for (i in seq(along=values)) {
      names.i <- names(x)[x==values[i]]
      if (!is.null(names.i)) nam[i] <- pasteunique(names.i, col.str)
      else nam[i] <- NA
    }
    ## names for NA
    if (is.na(values[length(values)]))
      { names.i <- names(x)[is.na(x)]
        nam[length(values)] <- pasteunique(names.i, col.str)
      }
    names(values) <- nam
    return(values)
  }
}


### function factvn - version 0.3.0 - 11.7.2006
### function to build a factor from vector with named elements
##
##  function description:
##  - if fromvaluesnames is not given factvn calls factor
##  - if fromvaluesnames is in c('values', 'names') a factor based on
##    names(x) is constructed
##
##  - arguments:
##    - x         source vector with names for (some) elements
##                x must be numeric or character.
##    - fromvaluesnames:
##      - fromvaluenames='values': levels are ordered according to the values
##        of x
##      - fromvaluenames='names': levels are ordered according to the names
##        of x
##    - ordered:
##      - fromvaluesnames is not given: ordered=is.ordered(x)
##      - fromvaluesnames='values': ordered=TRUE
##      - fromvaluesnames='names': ordered=FALSE
##
##  - value:
##  - if fromvaluesnames is not given see factor
##  - if fromvaluesnames is in c('values', 'names') a factor based on
##    names(x) is constructed. All x-values without names are NA.
##    The (final) levels of value are the unique(names(x)).

factvn <- function (x = character(), levels = sort(unique.default(x),
                    na.last = TRUE), labels = levels, exclude = NA,
                    ordered = is.ordered(x), fromvaluesnames=NULL)
{
  ## set ordered depending on fromvaluesnames
  if (!missing(fromvaluesnames))
    if (missing(ordered)) {
      if (fromvaluesnames=='values') ord <- TRUE
      if (fromvaluesnames=='names') ord <- FALSE
    } else ord <- ordered
  if (!missing(fromvaluesnames)) {
    if (fromvaluesnames=='values')
      fx <- factor(names(x), levels=unique(names(value.names(x))),
                   exclude=exclude, ordered=ord)
    if (fromvaluesnames=='names')
      fx <- factor(names(x), levels=sort(unique(names(value.names(x)))),
                   exclude=exclude, ordered=ord)
  } else  fx <- factor(x, levels, labels, ordered)
  return(fx)
}


>> 
...snip...

>-- 
>Frank E Harrell Jr   Professor and Chair           School of Medicine
>                      Department of Biostatistics   Vanderbilt University
>


From bibiko at eva.mpg.de  Thu Jul 13 15:54:43 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Thu, 13 Jul 2006 15:54:43 +0200
Subject: [R] VERY TINY question: missing function to clear the console?
In-Reply-To: <20060713111555.GA22695@jtkpc.cmp.uea.ac.uk>
References: <C7F643AD-0201-40BE-BBB5-F13ADCBF61E2@eva.mpg.de>
	<20060713111555.GA22695@jtkpc.cmp.uea.ac.uk>
Message-ID: <37249BC2-3501-4F26-A581-FE59265B5946@eva.mpg.de>

Hi,

many thanks for the hints!

>>
>> Hi,
>>
>> for presentation purposes I would like to clear to whole console
>> window (like in a UNIX terminal: 'clear').
>>
>> Is there such a function?
>>
>> If not, I could image that is not too hard to write such a function.
>
> At the risk of this being a stupid answer: An easy way would be
>
>     system("clear");
>
> This should work under unixes that provide the "clear" command,  
> don't know
> about windows.
>

OK. That works fine on a Mac running R in X11. But I'm using the nice  
R Mac GUI.


> If there is no "clear screen" function in R (I haven't seriously  
> looked
> for one), I think one of the reasons for that might be that writing  
> that
> in a multi-platform portable way is actually a somewhat more laborious
> thing to do than it seems.

My question is very specific to R Mac GUI, so I will post my tiny  
question to that developer mailing list.

All the best,

Hans


From e.pavlidis at lancaster.ac.uk  Thu Jul 13 16:02:29 2006
From: e.pavlidis at lancaster.ac.uk (Pavlidis, Efthymios)
Date: Thu, 13 Jul 2006 15:02:29 +0100
Subject: [R] Question for LM intercept
Message-ID: <E36B692FEB773C4BB3EC8B1828516B87149D7387@exchange-be2.lancs.ac.uk>

Hello,

I am having the following silly problem with lm. 

Let X be a dataframe with  X[,1] the dependent variable and X[,-1] the independent variables. I want to run the following
but without including an intercept.

for(i in 1:100 ){
    lm( X[,100-i] )  # this works fine but it returns an intercept
}

Can anyone help me? Thank you in advance!


Regards,
Themis


From j.logsdon at quantex-research.com  Thu Jul 13 16:03:23 2006
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Thu, 13 Jul 2006 15:03:23 +0100 (GMT)
Subject: [R] Extracting Phi from gls/lme
In-Reply-To: <x2y7uxeiys.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.10.10607131445470.12144-100000@mercury.quantex>

Peter

Yes spelling would help!  I transcribed this from one screen to another
and still the penny didn't drop.  And the transformation works too.

getAnywhere() is yet another useful function that I hadn't heard about...

Many thanks for saving my house.

Best wishes

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com


On 13 Jul 2006, Peter Dalgaard wrote:

> John Logsdon <j.logsdon at quantex-research.com> writes:
> 
> > Peter
> > 
> > This looks very promising:
> > 
> > x<-mod.lme$model$Struct$corStruct
> > Correlation structure of class corAR1 representing
> >        Phi
> > -0.1996813
> > 
> > which is the value I want.  
> > 
> > Yippee (save the bricks)
> > 
> > But:
> > 
> > coef(x,unconstrained=FALSE)
> > [1] -0.4048011
> > 
> > and any attempt to coerce x into a scalar always returns -0.404...
> 
> Works for me. Is there a rogue coef() around?? Or did you misspell
> "unconstrained" and not tell us?
> 
> > coef(x,uncostrained=FALSE)
> [1] 0.1171201
> > coef(x,unconstrained=FALSE)
>        Phi
> 0.05849318
>  
> 
>  
> > This is not an obvious transformation of the -0.1996813 I think.
> 
> Obvious is in the eyes of the beholder, but:
> 
> > aux <- exp(-0.4048011)
> > (aux - 1) / (aux + 1)
> [1] -0.1996813
> 
> > Looking at str(x) returns the first line:
> > 
> > Classes 'corAR1', 'corStruct' atomic [1:1] -0.405
> > 
> > Not a -0.199 ... in sight in the attributes various.
> > 
> > How does summary.lme/gls do it?
> 
> I don't think they do anything, but their print methods call
> print.corStruct which calls coef.corAR1. And
> 
> > getAnywhere(coef.corAR1)
> A single object matching coef.corAR1 was found
> It was found in the following places
>   registered S3 method for coef from namespace nlme
>   namespace:nlme
> with value
> 
> function (object, unconstrained = TRUE, ...)
> {
>     if (unconstrained) {
>         if (attr(object, "fixed")) {
>             return(numeric(0))
>         }
>         else {
>             return(as.vector(object))
>         }
>     }
>     aux <- exp(as.vector(object))
>     aux <- c((aux - 1)/(aux + 1))
>     names(aux) <- "Phi"
>     aux
> }
> <environment: namespace:nlme>
> > aux <- exp(-0.4048011
> 
> 
>  
> > Best wishes
> > 
> > John
> > 
> > John Logsdon                               "Try to make things as simple
> > Quantex Research Ltd, Manchester UK         as possible but not simpler"
> > j.logsdon at quantex-research.com              a.einstein at relativity.org
> > +44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com
> > 
> > 
> > On 13 Jul 2006, Peter Dalgaard wrote:
> > 
> > > John Logsdon <j.logsdon at quantex-research.com> writes:
> > > 
> > > > I am trying to extract into a scalar the value of Phi from the printed
> > > > output of gls or lme using corAR1 correlation.  ie I want the estimate of
> > > > the autocorrelation.  I can't see how to do this and haven't seen it
> > > > anywhere in str(model.lme).
> > > > 
> > > > I can get all the other information - fixed and random effects etc.
> > > > 
> > > > Is there an obvious way so that I can save the brick wall some damage?
> > > 
> > > Just be soft in the head...
> > > 
> > > Seriously, I think the recipe is to drill down into model.lme until
> > > you find the corAR1 class object,  like this, I think.
> > > 
> > > x <- model.lme$modelStruct$corStruct
> > > coef(x,unconstrained=FALSE). 
> > > 
> > > -- 
> > >    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
> > >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> > >  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> > > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
> > > 
> > 
> > 
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>


From nvj at fys.ku.dk  Thu Jul 13 16:17:13 2006
From: nvj at fys.ku.dk (Niels Vestergaard Jensen)
Date: Thu, 13 Jul 2006 16:17:13 +0200 (CEST)
Subject: [R] colors on graph
In-Reply-To: <44B64B36.90001@good.ibl.fr>
References: <15C100200A5F4E45AF8CFB45A926EF3415E8FA@allexch01.alliance.com>
	<44B64B36.90001@good.ibl.fr>
Message-ID: <Pine.LNX.4.64.0607131612530.30330@scharff.fys.ku.dk>

What's not mentioned in ?image "See Also" is levelplot and friends from 
library(lattice) . I'm using it myself at the moment instead of image for 
some reason I forgot. Worth a try.

best

 	Niels


On Thu, 13 Jul 2006, Jacques VESLOT wrote:

> ?image
> Cf. See Also
> -------------------------------------------------------------------
> Jacques VESLOT
>
> CNRS UMR 8090
> I.B.L (2?me ?tage)
> 1 rue du Professeur Calmette
> B.P. 245
> 59019 Lille Cedex
>
> Tel : 33 (0)3.20.87.10.44
> Fax : 33 (0)3.20.87.10.31
>
> http://www-good.ibl.fr
> -------------------------------------------------------------------
>
>
> COMTE Guillaume a ?crit :
>> Hy all,
>>
>>
>>
>> I need to draw something in 2 dimension that has 3 dimension, the choice
>> has been made to use colors to display the third dimension into the
>> graph.
>>
>>
>>
>> Has someone done something like that, i can't figure out how to
>> parametize the colors.
>>
>>
>>
>> Thks for all ideas,
>>
>>
>>
>> COMTE Guillaume
>>
>>
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

From goedman at mac.com  Thu Jul 13 16:19:24 2006
From: goedman at mac.com (Rob J Goedman)
Date: Thu, 13 Jul 2006 07:19:24 -0700
Subject: [R] Matrix binary for Mac OS X
In-Reply-To: <878be2c60607130147x629b163dt42ed22a65311b662@mail.gmail.com>
References: <878be2c60607130147x629b163dt42ed22a65311b662@mail.gmail.com>
Message-ID: <4D0EB66E-AB44-4633-BA81-4C6A7F6179B8@mac.com>

Just checked the list from Bristol. Matrix 0.995-11 is on there.

Regards,
Rob

On Jul 13, 2006, at 1:47 AM, A.R. Criswell wrote:

> Hello all,
>
> I can't seem to find package "Matrix" for Mac OS X. The R package
> installer, when pointed to Bristol (UK) does not have Matrix amongst
> its list.
>
> Thank you
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html


From Mike.Prager at noaa.gov  Thu Jul 13 16:21:00 2006
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Thu, 13 Jul 2006 10:21:00 -0400
Subject: [R] colors on graph
In-Reply-To: <15C100200A5F4E45AF8CFB45A926EF3415E8FA@allexch01.alliance.com>
References: <15C100200A5F4E45AF8CFB45A926EF3415E8FA@allexch01.alliance.com>
Message-ID: <44B656CC.1070100@noaa.gov>

Is filled.contour() what you are looking for?

MHP


on 7/13/2006 9:26 AM COMTE Guillaume said the following:
> Hy all,
>
> I need to draw something in 2 dimension that has 3 dimension, the choice
> has been made to use colors to display the third dimension into the
> graph.
>
> Has someone done something like that, i can't figure out how to
> parametize the colors.
>
> Thks for all ideas, 
> COMTE Guillaume 

-- 
Michael Prager, Ph.D.
Southeast Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
** Opinions expressed are personal, not official.  No
** official endorsement of any product is made or implied.


From mariayzw at hotmail.com  Thu Jul 13 16:25:59 2006
From: mariayzw at hotmail.com (Maria Wang)
Date: Thu, 13 Jul 2006 14:25:59 +0000
Subject: [R] how to make a contour plot using data in long format
Message-ID: <BAY104-F32C1C6824F8B25463494BCD6E0@phx.gbl>

Hi List,

There is a dataset with Latitude Longitude information and corresponding 
value shaped like this:
Lati                     Longi                  Value
18.0001             -159.6667             123
18.0023             -159.6665             124
19.1001             -158.2988             125
...                      ...                        ...

I want to use this data to make a filled contour map imposed on an USA map. 
I notice filled.contour command can be used to make contour plots, but it 
requires the data in a matrix. Would someone please tell me if 
filled.contour is the right thing to look at, if so, how to reshape the data 
into a matrix since my Latitude and Longitude are not evenly distributed?

Thanks,
Maria


From epistat at gmail.com  Thu Jul 13 16:31:29 2006
From: epistat at gmail.com (zhijie zhang)
Date: Thu, 13 Jul 2006 22:31:29 +0800
Subject: [R] set the bahavior that R deal with missing values?
Message-ID: <2fc17e30607130731g43d2d077m201daae6d9d13c9e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060713/3f1a4684/attachment.pl 

From epistat at gmail.com  Thu Jul 13 16:33:10 2006
From: epistat at gmail.com (zhijie zhang)
Date: Thu, 13 Jul 2006 22:33:10 +0800
Subject: [R] set the bahavior that R deal with missing values?
Message-ID: <2fc17e30607130733j2003eeadv2ec74eaff0d272a7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060713/91885e4c/attachment.pl 

From fernandomayer at gmail.com  Thu Jul 13 16:44:37 2006
From: fernandomayer at gmail.com (Fernando Mayer)
Date: Thu, 13 Jul 2006 11:44:37 -0300
Subject: [R] R --gui=GNOME problem
Message-ID: <44B65C55.4070307@gmail.com>

I had the same problem, and I fixed following Dirk Eddelbuettel's 
suggestion on R-sig-Debian some time ago: install also the package 
gnomeGUI to have it working.

HTH,
---
Fernando Mayer
Grupo de Estudos Pesqueiros - GEP
Centro de Ci?ncias Tecnol?gicas, da Terra e do Mar - CTTMar
Universidade do Vale do Itaja? - UNIVALI
Itaja? - SC - Brasil

pacocuacco escreveu:
> I've installed GNU R statistical comupting language and environment 2.2.1-2 by Synaptic (on Ubunti Linux 6.06 Dapper Drake). I've installed also r-gnome to have a GUI. I can't start my GUI. Why R --gui=GNOME don't start? It cannot find rgnome. R --gui=tk command instead is good. What's the best GUI for R?
> How can I start my GUI?
>
> Thanks
> Paco
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>


From bonfigli at inmi.it  Thu Jul 13 16:59:15 2006
From: bonfigli at inmi.it (Bonfigli Sandro)
Date: Thu, 13 Jul 2006 16:59:15 +0200
Subject: [R] Inspecting a dataframe inside a dataframe
Message-ID: <WorldClient-F200607131659.AA59150067@inmi.it>

I have a rather peculiar dataframe and I have problems at inspecting it
(I'm working with R 2.3.0 on Win XP).

Let say that example is a dataframe with two variables: "first" and "second";
now while "first" is a quite common numeric variable, second is a
variable which contains a dataframe (this of course means that each element
of "second" IS a dataframe). All the dataframes contained in "second" have
the same structure, this means that they have the same number of variables
with the same names and formats.
The problem for me is that some of the dataframes in "second" are zero-row
and this fact blocks a procedure that I apply on "example".

I'd like to replace the zero-row dataframes in "second" with some other 
thing (perhaps a 0 or a NULL, I still have to determine what would be fine
for my subsequent procedure).

I didn't manage to find a logical condition to identify the rows in wich 
"second" contains zero-row dataframes. In fact suppose that the third row
of example contains a 'guilty dataframe' in "the second" variable; then if
I do
> class(example$second[3])
list
> class(example[2,3])
list
> class(example[2,3][[1]])
data.frame
> class(example[[2,3]])
data.frame
> nrow(example[[2,3]])
0

this means that if I create
> example2 <- example[nrow(example$second)==0]
I don't have the desired value because in this way I pose the logical
condition on example$second[i] {for i 1:nrow(example)} and I understood 
that example$second[i] is a list wich contains a dataframe.
I tried to remove the list in various ways without success; for example
I explored the function unlist but 
> class(unlist(example$second[3]))
"numeric"
and this means that 
> nrow(unlist(example$second[3]))
doesn't return 0

I'd be grateful for any suggestion

TIA

  Sandro Bonfigli


From researchjj at gmail.com  Thu Jul 13 17:25:11 2006
From: researchjj at gmail.com (j.joshua thomas)
Date: Thu, 13 Jul 2006 08:25:11 -0700
Subject: [R] Clique- Method-Package
Message-ID: <b4485c4c0607130825y52441608x77cfe28e80f6b092@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060713/e5379e46/attachment.pl 

From krcabrer at une.net.co  Thu Jul 13 17:25:12 2006
From: krcabrer at une.net.co (Kenneth Cabrera)
Date: Thu, 13 Jul 2006 10:25:12 -0500
Subject: [R] Problem installing local additional packages of Rcmdr (WinXP)
Message-ID: <op.tcmvsab03mu6w9@davinci.epm.net.co>

Hi R users:

I download the .zip files and also PACKAGES and PACKAGES.gz files
in a local directory, but when I call

library(Rcmdr)

It shows me a tcl/tk windows telling me to install the additional
packages, the it takes me to another window where I must specify
the path where the .zip files are located.
I do it, but I obtain the following message:

Loading required package: tcltk
Loading Tcl/Tk interface ... done
Loading required package: car
Erro en gzfile(file, "r") : no fue posible abrir la conexi?n

Versi?n del Rcmdr 1.1-7


And the Rcmdr frame opens but without installing the additional
packages. I am sure I download the required packages and the
PACKAGES and PACKAGES.gz files.

What am I doing wrong?


If I make the same procedure with the internet conexion it
works fine, but I need to install all the packages in computers
without internet conexion.

Thank you for your help.

Kenneth

$platform
[1] "i386-pc-mingw32"
$arch
[1] "i386"
$os
[1] "mingw32"
$system
[1] "i386, mingw32"
$status
[1] ""
$major
[1] "2"
$minor
[1] "3.1"
$year
[1] "2006"
$month
[1] "06"
$day
[1] "01"
$`svn rev`
[1] "38247"
$language
[1] "R"
$version.string
[1] "Version 2.3.1 (2006-06-01)"


--


From sfalcon at fhcrc.org  Thu Jul 13 17:27:11 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 13 Jul 2006 08:27:11 -0700
Subject: [R] Rgraphviz: Setting the edge width
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC03878366@DJFPOST01.djf.agrsci.dk>
	(=?iso-8859-1?Q?S=F8ren_H=F8jsgaard's?= message of "Fri, 7 Jul 2006
	01:42:35 +0200")
References: <C83C5E3DEEE97E498B74729A33F6EAEC03878366@DJFPOST01.djf.agrsci.dk>
Message-ID: <m2zmfdcypc.fsf@ziti.local>

S?ren H?jsgaard <Soren.Hojsgaard at agrsci.dk> writes:

> I create an undirected graph with Rgraphviz (see code below). I would
> like to make the edges thicker. Can anyone help on this??  

Questions on Rgraphviz are best directed to the bioconductor mail
list.  I think you already discovered that, but for the archives...

As to your question, adjusting edge display options such as width and
style is not supported in the current release, but is supported in the
current development version of Rgraphviz.

+ seth


From researchjj at gmail.com  Thu Jul 13 17:33:25 2006
From: researchjj at gmail.com (j.joshua thomas)
Date: Thu, 13 Jul 2006 08:33:25 -0700
Subject: [R] Clique-Method-Package-Help
Message-ID: <b4485c4c0607130833j69352e61qbcb87a012409d44e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060713/898eacf8/attachment.pl 

From falimadhi at iq.harvard.edu  Thu Jul 13 17:38:24 2006
From: falimadhi at iq.harvard.edu (Ferdinand Alimadhi)
Date: Thu, 13 Jul 2006 11:38:24 -0400
Subject: [R] set the bahavior that R deal with missing values?
In-Reply-To: <2fc17e30607130731g43d2d077m201daae6d9d13c9e@mail.gmail.com>
References: <2fc17e30607130731g43d2d077m201daae6d9d13c9e@mail.gmail.com>
Message-ID: <44B668F0.2020704@iq.harvard.edu>

You can deal with the missing values before performing any regression model.
if you dataset is :

 > D
  X1 X2 X3
1 NA  2  4
2  1 NA NA
3  2  2  6
4 NA 34 NA
5  3 NA  7

then the following will replace all NA with the mean value

 > f<-function(col){
+ col[is.na(col)]<-mean(col,na.rm=TRUE)
+ col}
 > sapply(D,f)
     X1       X2       X3
[1,]  2  2.00000 4.000000
[2,]  1 12.66667 5.666667
[3,]  2  2.00000 6.000000
[4,]  2 34.00000 5.666667
[5,]  3 12.66667 7.000000


Hope this is what you want.

zhijie zhang wrote:

>Dear Rusers,
>The default behavior in R when performing a regression model with missing
>values is to exclude any case that contains a
>missing value? How could i set the bahavior that R deal with missing values?
>e.g.:
>exclude cases listwise
>exclude cases pairwise
>replace with mean
>
>Thanks very much!
>
>  
>


-- 
Ferdinand Alimadhi
Programmer / Analyst
Harvard University
The Institute for Quantitative Social Science
(617) 496-0187
falimadhi at iq.harvard.edu
www.iq.harvard.edu


From maud.pousset at noos.fr  Thu Jul 13 17:47:21 2006
From: maud.pousset at noos.fr (Pousset)
Date: Thu, 13 Jul 2006 17:47:21 +0200
Subject: [R] TR: Latent Class Analysis
Message-ID: <20060713154619.6A2385A0D0@zeus.cochin.inserm.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060713/265f309b/attachment.pl 

From spencer.graves at pdf.com  Thu Jul 13 17:46:39 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 13 Jul 2006 08:46:39 -0700
Subject: [R] which data structure for a set of time series ?
In-Reply-To: <a0b6f2b80607061042x54e00653o21b7190557af3eb4@mail.gmail.com>
References: <a0b6f2b80607061042x54e00653o21b7190557af3eb4@mail.gmail.com>
Message-ID: <44B66ADF.10102@pdf.com>

	  A data.frame is perfect for storing both qualitative and quantitative 
variables together when they all have more or less the same length and 
where the observations are connected by something like time.  Without 
substantive common connection, people usually put variables in different 
data.frames.

	  Have you reviewed the information on data.frames in "An Introduction 
to R", available via 'help.start()'?  Also regarding coding time, have 
you reviewed the "zoo" vignette?  (See, e.g., 
"http://finzi.psych.upenn.edu/R/Rhelp02a/archive/67006.html".)

	  Hope this helps.
	  Spencer Graves

Roolio wrote:
> Hello,
> I'm a R newcomer and I'm wondering the kind of data structure that would
> best fit to my problem:
> my data are equities (stocks) : so I have a time serie (say 1 year of weekly
> data), and a bunch of qualitative + quantitative variables :
> the sector of the stock (biotech/finance...), the geographical region, the
> name, ISIN code, P/E ratios, whatever...
> 
> The data.frame is perfect for the qualitative variables : a row is a stock,
> a column is a variable .
> But where can I store the time serie? If I put each weekly data in a column
> of the data frame
> , I'm not sure that it's the best structure when I will want to test
> some models on these
> (regressions/arma/whatever).
> I would prefer to have a new data frame, where each column is a stock, each
> row a date, to do the regressions (across time more easily)
> 
> So I would end up with 2 data frames, one of each transposed... No good,
> probably.
> 
> I thought of a set of 2 objects : 1 data frame (for the qualitative
> variables), and one list of time series, in the same order, with the risk of
> 
> forgetting to subset one objet and not the other, etc...
> 
> Do you happen to have the same kind of problem?
> Is there something equivalent in R as the matlab 'structures' ? (kinda R
> data frame, where you can store multi-dimensional objects in a column)
> 
> Many thanks in advance.
> 
> Regards
> 
> Roolio
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From spencer.graves at pdf.com  Thu Jul 13 17:56:57 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 13 Jul 2006 08:56:57 -0700
Subject: [R] Access values in kpssstat-class
In-Reply-To: <20060706194253.73277.qmail@web37608.mail.mud.yahoo.com>
References: <20060706194253.73277.qmail@web37608.mail.mud.yahoo.com>
Message-ID: <44B66D49.5010407@pdf.com>

	  Have you tried 'str(test)'?  This tells me it is "Formal class 
'kpssstat' [package "uroot"] with 5 slots".  I can access the slot 
'ltrunc' and the component 'rank' of the 'lmkpss' slot as follows:

 > test at ltrunc
[1] 7
 > test at lmkpss$rank
[1] 2

	  Hope this helps.
	  Spencer Graves
p.s.  I'm not familiar with the 'KPSS.test'.  I used 
RSiteSearch("KPSS.test", "functions") to find it was in package 'uroot'.

Sachin J wrote:
> Hi,
>    
>   How can I access the Values stored in kpssstat-class given by KPSS.test function and store it in a variable. 
>    
>   For example:
>    
>   >x <- rnorm(1000)
>   >test  <- KPSS.test(ts(x))
>   >test
>     ---- ----
>   KPSS test
>   ---- ----
>     Null hypotheses: Level stationarity and stationarity around a linear trend.
>   Alternative hypothesis: Unit root.
> ----
>   Statistic for the null hypothesis of 
>    level stationarity: 0.138 
>       Critical values:
>     0.10  0.05 0.025  0.01
>  0.347 0.463 0.574 0.739
> ----
>   Statistic for the null hypothesis of 
>    trend stationarity: 0.038 
>       Critical values:
>     0.10  0.05 0.025  0.01
>  0.119 0.146 0.176 0.216
> ----
>   Lag truncation parameter: 7 
> 
>   then store the test stat values in some variable say - result
>    
>   Thanx in advance.
>    
>    
> 
>  		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From f.harrell at vanderbilt.edu  Thu Jul 13 18:02:01 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 13 Jul 2006 11:02:01 -0500
Subject: [R] Keep value lables with data frame manipulation
In-Reply-To: <3.0.6.32.20060713144855.00a5e700@pop.gmx.net>
References: <3.0.6.32.20060713095939.00ace1f8@pop.gmx.net>
	<2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>
	<2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>
	<3.0.6.32.20060713095939.00ace1f8@pop.gmx.net>
	<3.0.6.32.20060713144855.00a5e700@pop.gmx.net>
Message-ID: <44B66E79.1010106@vanderbilt.edu>

Heinz Tuechler wrote:
> At 08:11 13.07.2006 -0500, Frank E Harrell Jr wrote:
>> Heinz Tuechler wrote:
>>> At 13:14 12.07.2006 -0500, Marc Schwartz (via MN) wrote:
>>>> On Wed, 2006-07-12 at 17:41 +0100, Jol, Arne wrote:
>>>>> Dear R,
>>>>>
>>>>> I import data from spss into a R data.frame. On this rawdata I do some
>>>>> data processing (selection of observations, normalization, recoding of
>>>>> variables etc..). The result is stored in a new data.frame, however, in
>>>>> this new data.frame the value labels are lost.
>>>>>
>>>>> Example of what I do in code:
>>>>>
>>>>> # read raw data from spss
>>>>> rawdata <- read.spss("./data/T50937.SAV",
>>>>> 	use.value.labels=FALSE,to.data.frame=TRUE)
>>>>>
>>>>> # select the observations that we need
>>>>> diarydata <- rawdata[rawdata$D22==2 | rawdata$D22==3 | rawdata$D22==17 |
>>>>> rawdata$D22==18 | rawdata$D22==20 | rawdata$D22==22 |
>>>>>  			rawdata$D22==24 | rawdata$D22==33,]
>>>>>
>>>>> The result is that rawdata$D22 has value labels and that diarydata$D22
>>>>> is numeric without value labels.
>>>>>
>>>>> Question: How can I prevent this from happening?
>>>>>
>>>>> Thanks in advance!
>>>>> Groeten,
>>>>> Arne
>>>> Two things:
>>>>
>>>> 1. With respect to your subsetting, your lengthy code can be replaced
>>>> with the following:
>>>>
>>>>  diarydata <- subset(rawdata, D22 %in% c(2, 3, 17, 18, 20, 22, 24, 33))
>>>>
>>>> See ?subset and ?"%in%" for more information.
>>>>
>>>>
>>>> 2. With respect to keeping the label related attributes, the
>>>> 'value.labels' attribute and the 'variable.labels' attribute will not by
>>>> default survive the use of "[".data.frame in R (see ?Extract
>>>> and ?"[.data.frame").
>>>>
>>>> On the other hand, based upon my review of ?read.spss, the SPSS value
>>>> labels should be converted to the factor levels of the respective
>>>> columns when 'use.value.labels = TRUE' and these would survive a
>>>> subsetting.
>>>>
>>>> If you want to consider a solution to the attribute subsetting issue,
>>>> you might want to review the following post by Gabor Grothendieck in
>>>> May, which provides a possible solution:
>>>>
>>>>  https://stat.ethz.ch/pipermail/r-help/2006-May/106308.html
>>>>
>>>> and this post by me, for an explanation of what is happening in Gabor's
>>>> solution:
>>>>
>>>>  https://stat.ethz.ch/pipermail/r-help/2006-May/106351.html
>>>>
>>>> HTH,
>>>>
>>>> Marc Schwartz
>>>>
>>> Hello Mark and Arne,
>>>
>>> I worked on the suggestions of Gabor and Mark and programmed some functions
>>> in this way, but they are very, very preliminary (see below).
>>> In my view there is a lack of convenient possibilities in R to document
>>> empirical data by variable labels, value labels, etc. I would prefer to
>>> have these possibilities in the "standard" configuration.
>>> So I sketched a concept, but in my view it would only be useful, if there
>>> was some acceptance by the core developers of R.
>>>
>>> The concept would be to define a class. For now I call it "source.data".
>>> To design it more flexible than the Hmisc class "labelled" I would define a
>>> related option "source.data.attributes" with default c('value.labels',
>>> 'variable.name', 'label')). This option contains all attributes that should
>>> persist in subsetting/indexing.
>>>
>>> I made only some very, very preliminary tests with these functions, mainly
>>> because I am not happy with defining a new class. Instead I would prefer,
>>> if this functionality could be integrated in the Hmisc class "labelled",
>>> since this is in my view the best known starting point for data
>>> documentation in R.
>>>
>>> I would be happy, if there were some discussion about the wishes/needs of
>>> other Rusers concerning data documentation.
>>>
>>> Greetings,
>>>
>>> Heinz
>> I feel that separating variable labels and value labels and just using 
>> factors for value labels works fine, and I would urge you not to create 
>> a new system that will not benefit from the many Hmisc functions that 
>> use variable labels and units.  [.data.frame in Hmisc keeps all attributes.
>>
>> Frank
>>
> 
> Frank,
> 
> of course I aggree with you about the importance of Hmisc and as I said, I
> do not want to define a new class, but in my view factors are no good
> substitute for value labels.
> As the language definition (version 2.3.1 (2006-06-05) Draft, page 7) says:
> "Factors are currently implemented using an integer array to specify the
> actual levels and a second array of names that are mapped to the integers.
> Rather unfortunately users often make use of the implementation in order to
> make some calculations easier." 
> So, in my view, the levels represent the "values" of the factor.
> This has inconveniencies if you want to use value labels in different
> languages. Further I do not see a simple method to label numerical
> variables. I often encounter discrete, but still metric data, as e.g. risk
> scores. Usually it would be nice to use them in their original coding,
> which may include zero or decimal places and to label them at the same time.
> Personally at the moment I try to solve this problem by following a
> suggestion of Martin, Dimitis and others to use names instead. I doubt,
> however, that this is a good solution, but at least it makes it possible to
> have the source data numerically coded and in this sense "language free"
> (see first attempts of functions below).
> 
> Heinz
> 
Those are excellent points Heinz.  I addressed that problem partially in 
sas.get - see the sascodes attribute.

Frank

> 
> ### These are very preliminary and untested versions.
> ### They are inteded only to demonstrate the concept, but not for productive
> ### work.
> 
> ### function "value.names<-" - version 0.3.0 - 11.7.2006
> ### function to assign names of elements according to their value
> ##
> ##  value.names<-
> ##  - arguments:
> ##    - action 
> ##      - set:           alle eventuell vorhandenen names l?schen, valuenames
> ##                       setzen
> ##      - add.overwrite: leere und nicht leere names durch neue ersetzen
> ##      - add:           nur leere names durch neue ersetzen
> ##    - tolerance:       ordnet names den values innerhalb der Toleranz zu.
> ##                       Liegt ein Wert innerhalb des Toleranzbereiches
> ##                       mehrerer names, dann wird geringste Toleranz gew?hlt.
> ##    - round:           rounds values in value before matching
> ##                       This may lead to collapsing of different names in
> ##                       value to one name (and one value)
> ##    - col.str:         string used when collapsing several names
> ##    - others:          name for values not named by other names
> ##    - value:
> ##      
> ##
> ##  function description:
> ##  - x must be atomic, preferably numeric or character
> ##  - if tolerance is given, it must not be NA. tolerance < 0 is ignored
> ##  - to ensure consistency, value is processed by value.names()
> ##  - new.names are built by matching with/without tolerance
> ##  - new.names are assigned to names depending on argument action
> ##  - if argument others is given, others-name is assigned to all valid values
> ##    without name
> ##
> 
> "value.names<-" <- function(x, action='set', tolerance=NULL, round=NULL,
>                             col.str=' ', others=NULL, value)
> {
>   ## checking parameters
>   if(!is.atomic(x)) stop('x must be an atomic object')
>   if(!is.null(tolerance) &&
>      is.na(tolerance)) stop('if given, tolerance must not be NA')
>   ## to ensure consistency, process value by value.names
>   value <- value.names(value, round=round, col.str=col.str)
>   ## delete values with NA-name from value
>   value <- value[!is.na(names(value))]
>   old.names <- names(x) # store original names
>   ## -- building names
>   ##    - matching with/without tolerance
>   if(!is.null(tolerance) && tolerance > 0 && is.numeric(x))
>     ##      - matching with tolerance
>     { dif <- abs(outer(x, value, '-'))
>       dif[dif>tolerance] <- NA
>       within.tolerance <- apply(dif, 1, function(x) sum(!is.na(x)))
>       old.option.warn <- options('warn')[[1]]
>       options(warn=-1)
>       min.dif <- apply(dif, 1, function(x) which(x==min(x, na.rm=TRUE))[1])
>       options(warn=old.option.warn)
>       new.names <- names(value)[min.dif] }
>   else
>     ##      - matching without tolerance, i.e. exact matching
>     new.names <- names(value)[match( x, value)]
>   ##      - matching names for NA-values
>   if(length(names(value[is.na(value)]))==1)
>     new.names[is.na(x)] <- names(value[is.na(value)])
>   ## assign names depending on action
>   if (action=='set') new.names <- new.names
>   if (action=='add.overwrite') new.names[is.na(new.names)] <-
>     old.names[is.na(new.names)]
>   if (action=='add') new.names[!is.na(old.names)] <-
>     old.names[!is.na(old.names)]
>   ## assigning others-name to all valid values without name
>   if (!is.null(others)) new.names[!is.na(x) & is.na(new.names)] <-
>     as.character(others)
>   names(x) <- new.names
>   return(x)
> }
> 
> 
> ### function value.names - version 0.3.0 - 11.7.2006
> ### function to return names of elements according to their value
> ##
> ##  - arguments:
> ##    - x         source vector with names for (some) elements
> ##                x must be atomic ().
> ##                If x is a factor, value will be a factor. Consequently
> ##                names are only seen, if unclass() or print.default is used.
> ##    - col.str:         string used when collapsing several names
> ##                       default: "/"
> ##    - round:           rounds values in x
> ##                       This may lead to collapsing of different names for
> ##                       one value of x to one name (and one value)
> ##
> ##  - value:
> ##  - vector of the same class as x with sorted unique values and their names,
> ##    NULL, if x is NULL
> ##    - NA-values in x appear at the end
> ##    - if there is a 1:1 realtion between values and names in x, value
> ##      contains all unique combinations of value and name.
> ##    - if identical values in x have different (non NA), names these names
> ##      get collapsed to one new name, seperated by the string col.str
> ##      This applies also to NA-values in x with different names.
> ##    - NA-names get suppressed, if non-NA-names for the same x-value exist.
> ##    - Differen values in x with identical names remain seperated.
> ##    - values in x without name appear in value with name NA
> 
> value.names <- function(x, col.str=' ', round=NULL) {
>   ## checking parameters
>   if(!is.atomic(x)) stop('x must be an atomic object')
> ## -- define function for pasting unique non empty names
>   pasteunique <- function(names.i, col.str)
>     { names.i <- sort(unique(names.i))
>       names.i <- names.i[!names.i=='' & !is.na(names.i)] # exclude ''
>       if (length(names.i))
>         names.i <- paste(names.i, sep='', collapse=col.str)
>       else names.i <- NA
>       invisible(names.i)
>     }
>   ## branching: if x is.null or has no names
>   if (is.null(x)) {
>     return(NULL) }
>   else {
>     x <- sort(x, na.last=TRUE) # sort x
>     if (!is.null(round)) x <- round(x, round)
>     ## vector of unique values
>     values <- unique(x, na.last = TRUE)
>     ## names per value
>     nam <- NA
>     for (i in seq(along=values)) {
>       names.i <- names(x)[x==values[i]]
>       if (!is.null(names.i)) nam[i] <- pasteunique(names.i, col.str)
>       else nam[i] <- NA
>     }
>     ## names for NA
>     if (is.na(values[length(values)]))
>       { names.i <- names(x)[is.na(x)]
>         nam[length(values)] <- pasteunique(names.i, col.str)
>       }
>     names(values) <- nam
>     return(values)
>   }
> }
> 
> 
> ### function factvn - version 0.3.0 - 11.7.2006
> ### function to build a factor from vector with named elements
> ##
> ##  function description:
> ##  - if fromvaluesnames is not given factvn calls factor
> ##  - if fromvaluesnames is in c('values', 'names') a factor based on
> ##    names(x) is constructed
> ##
> ##  - arguments:
> ##    - x         source vector with names for (some) elements
> ##                x must be numeric or character.
> ##    - fromvaluesnames:
> ##      - fromvaluenames='values': levels are ordered according to the values
> ##        of x
> ##      - fromvaluenames='names': levels are ordered according to the names
> ##        of x
> ##    - ordered:
> ##      - fromvaluesnames is not given: ordered=is.ordered(x)
> ##      - fromvaluesnames='values': ordered=TRUE
> ##      - fromvaluesnames='names': ordered=FALSE
> ##
> ##  - value:
> ##  - if fromvaluesnames is not given see factor
> ##  - if fromvaluesnames is in c('values', 'names') a factor based on
> ##    names(x) is constructed. All x-values without names are NA.
> ##    The (final) levels of value are the unique(names(x)).
> 
> factvn <- function (x = character(), levels = sort(unique.default(x),
>                     na.last = TRUE), labels = levels, exclude = NA,
>                     ordered = is.ordered(x), fromvaluesnames=NULL)
> {
>   ## set ordered depending on fromvaluesnames
>   if (!missing(fromvaluesnames))
>     if (missing(ordered)) {
>       if (fromvaluesnames=='values') ord <- TRUE
>       if (fromvaluesnames=='names') ord <- FALSE
>     } else ord <- ordered
>   if (!missing(fromvaluesnames)) {
>     if (fromvaluesnames=='values')
>       fx <- factor(names(x), levels=unique(names(value.names(x))),
>                    exclude=exclude, ordered=ord)
>     if (fromvaluesnames=='names')
>       fx <- factor(names(x), levels=sort(unique(names(value.names(x)))),
>                    exclude=exclude, ordered=ord)
>   } else  fx <- factor(x, levels, labels, ordered)
>   return(fx)
> }
> 
> 
> ...snip...
> 
>


From deepayan.sarkar at gmail.com  Thu Jul 13 18:14:01 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 13 Jul 2006 11:14:01 -0500
Subject: [R] Access to conditioning values in "xyplot"
In-Reply-To: <03ed4edff96c1355ccd3d2239aa374ba@nancy.inra.fr>
References: <03ed4edff96c1355ccd3d2239aa374ba@nancy.inra.fr>
Message-ID: <eb555e660607130914h46917c2ckd3c50e10c7043019@mail.gmail.com>

On 7/13/06, Jean-Marc Ottorini <ottorini at nancy.inra.fr> wrote:
>
> Dear R-help subscribers,
>
>     Many thanks for all the answers I received either by mail or through
> the list, and that were most helpful.
>
>   For the sake of the list records, I wanted to post the solution, I
> eventually could obtain, to the problem I  have submitted.
>
>   I am summarizing  the question here. It was how to use the panel
> function "panel.curve" when the expression used in this function to to
> add a fitted line to the points in each panel depends not only on x,
> but also on the value of the conditioning variable for the considered
> panel.
>
> This solution is based on hints given to me by Deepayan Sarkar, it
> relies on the use of "packet.number" and "panel.number":
>
> xyplot(n ~ cg | di, data = myData,
>         scale = list(y = "free", x = "free"),
>         groups = bloc,
>         as.table = T,
>         xlab = "Cg",
>         ylab = "N / ha",
>         panel = function(x, y, subscripts, groups, packet.number = di,
> panel.number, ...) {
>           panel.grid(h = -1, v = -1, col = "grey", lwd = 1, lty = 2)
>           panel.curve(expr =
> f.fit(unique(rev(packet.number))[panel.number], a, b, a1, b1, x),
>                       n = 50, curve.type = "l", col = "lightblue", ...)
>           panel.superpose(x, y, pch = c(1, 2), col =
> c("deeppink","blue"),
>                           panel.groups = "panel.xyplot", subscripts,
> groups)
>         },
>
>         key = list( space = "top", transparent = TRUE, columns = 2,
>           points = list( pch = c(1, 2), col = c("deeppink", "blue") ),
>           text = list( c("bloc 3", "bloc 4"))),
>         )
>
> Some comments:
>
> - the arguments  "packet.number = di, panel.number" must appear on the
> panel function prototype
> - a, b, a1, b1 are parameters set to a certain value before calling
> "xyplot"
> - "unique" is needed to reduce to 1 the multiple instances of the value
> of the conditioning variable di for a given panel (one for each point)
> - I was surprised by the need to use "rev" in order for the interval of
> the values to draw the curve  f.fit to correspond to the right panel.
>
> Any further comment would be welcome

It's difficult to do so without a reproducible example.

Deepayan


From gavin.simpson at ucl.ac.uk  Thu Jul 13 18:24:44 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 13 Jul 2006 17:24:44 +0100
Subject: [R] Question for LM intercept
In-Reply-To: <E36B692FEB773C4BB3EC8B1828516B87149D7387@exchange-be2.lancs.ac.uk>
References: <E36B692FEB773C4BB3EC8B1828516B87149D7387@exchange-be2.lancs.ac.uk>
Message-ID: <1152807884.23680.46.camel@gsimpson.geog.ucl.ac.uk>

On Thu, 2006-07-13 at 15:02 +0100, Pavlidis, Efthymios wrote:
> Hello,
> 
> I am having the following silly problem with lm. 
> 
> Let X be a dataframe with  X[,1] the dependent variable and X[,-1] the independent variables. I want to run the following
> but without including an intercept.
> 
> for(i in 1:100 ){
>     lm( X[,100-i] )  # this works fine but it returns an intercept
> }
> 
> Can anyone help me? Thank you in advance!

Are you really sure that works? Looks like the last loop will be
indexing column 0 and this does fail for me but for a different reason
first:

dat <- data.frame(matrix(rnorm(10000), ncol = 100))
for(i in 1:100)
    lm(dat[, 101-i])

Gives this error:
Error in terms.default(formula, data = data) :
        no terms component

I'm not exactly sure what you want to do, but if you mean you want to
fit a model to your y (X[,1]) using a single column at a time from the
rest of X, then does this do what you want?

mods <- vector("list", length = ncol(dat))
for(i in seq(along = mods))
    mods[[i]] <- lm(X1 ~ . -1, data = dat[, c(1, i)])

The -1 in the formular removes the intercept. To get the coefficients:

sapply(mods, coef)

Even then though, the first iteration of the loop is fitting a model of
X1 ~ X1 and gets a coefficient of 1:

> mods[[1]]

Call:
lm(formula = X1 ~ . - 1, data = dat[, c(1, i)])

Coefficients:
X1.1
   1

If you meant to only regress X1 on the remaining 99 variables then you
need something like this:

dat <- data.frame(matrix(rnorm(10000), ncol = 100))
## list to hold the results
mods <- vector("list", length = ncol(dat) - 1)
## indexer for the list
ind <- c(1, seq(2, ncol(dat)-1))
for(i in seq(2, ncol(dat)))  {
    j <- ind[i-1]
    mods[[j]] <- lm(X1 ~ . -1, data = dat[, c(1, i)])
}
## return the coefficients
sapply(mods, coef)

Of course it would be easier if you subset X first in that case

dat <- data.frame(matrix(rnorm(10000), ncol = 100))
X1 <- dat[, 1] # dependent variables
dat <- dat[, -1] # predictors
## list to hold the results
mods <- vector("list", length = ncol(dat))
for(i in seq(along = mods))
    mods[[i]] <- lm(X1 ~ . -1, data = dat[, i, drop = FALSE])
## return the coefficients
sapply(mods, coef)

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
 London, UK. WC1E 6BT.         [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From rmh at temple.edu  Thu Jul 13 18:36:37 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu, 13 Jul 2006 12:36:37 -0400 (EDT)
Subject: [R] Keep value lables with data frame manipulation
Message-ID: <20060713123637.BEC60457@po-d.temple.edu>

> Further I do not see a simple method to label numerical
> variables. I often encounter discrete, but still metric data, as e.g. risk
> scores. Usually it would be nice to use them in their original coding,
> which may include zero or decimal places and to label them at the same time.

## For this specific case, I use a "position" attribute.


tmp <- data.frame(y=rnorm(30), x=factor(rep(c(0,1,2,4,8), 6)))
attr(tmp$x, "position") <- as.numeric(as.character(tmp$x))

tmp
as.numeric(tmp$x)
attr(tmp$x, "position")

bwplot(y ~ x, data=tmp)

panel.bwplot.position <- function(x, y, ..., x.at) {
         for (x.i in x.at) {
           y.i <- y[x.i==x]
           panel.bwplot(rep(x.i, length(y.i)), y.i, ...)
         }
       }

bwplot.position <- function(formula, data, ..., x.at) {
  if (missing(x.at)) {
    x.name <- dimnames(attr(terms(formula),"factors"))[[2]]
    x.at <- attr(data[[x.name]], "position")
  }
  bwplot(formula, data, ...,
         x.at=x.at,
         panel=panel.bwplot.position,
         scales=list(x=list(at=x.at, limits=x.at+c(-1,1))))
}

bwplot.position(y ~ x, data=tmp)


## The above is a simplified version of
##     panel.bwplot.intermediate.hh
## in the online files for
##                 Statistical Analysis and Data Display
##                 Richard M. Heiberger and Burt Holland
##    http://springeronline.com/0-387-40270-5
## 
## An example of a boxplot with both placement and color of the boxes
## under user control is in
## 
## http://astro.ocis.temple.edu/~rmh/HH/bwplot-color.pdf


From francis.debrabandere at cropdesign.com  Thu Jul 13 18:58:09 2006
From: francis.debrabandere at cropdesign.com (Francis De Brabandere)
Date: Thu, 13 Jul 2006 18:58:09 +0200
Subject: [R] R-2.3.1 --enable-R-shlib
Message-ID: <44B67BA1.2060902@cropdesign.com>

Hi,

I've upgraded our server to the latest R version, everything went ok
until I wanted to install RServe for our java clients.

RServe needs the shared library which can be compiled using configure
--enable-R-shlib. All went ok but after running make I got this error:

gcc -shared -L/usr/local/lib64  -o libR.so Rembedded.o CConverters.o
CommandLineArgs.o Rdynload.o Renviron.o RNG.o apply.o arithmetic.o
apse.o array.o attrib.o base.o bind.o builtin.o character.o coerce.o
colors.o complex.o connections.o context.o cov.o cum.o dcf.o datetime.o
debug.o deparse.o deriv.o dotcode.o dounzip.o dstruct.o duplicate.o
engine.o envir.o errors.o eval.o format.o fourier.o gevents.o gram.o
gram-ex.o graphics.o identical.o internet.o iosupport.o lapack.o list.o
localecharset.o logic.o main.o mapply.o match.o memory.o model.o names.o
objects.o optim.o optimize.o options.o par.o paste.o pcre.o platform.o
plot.o plot3d.o plotmath.o print.o printarray.o printvector.o
printutils.o qsort.o random.o regex.o registration.o relop.o rlocale.o
saveload.o scan.o seq.o serialize.o size.o sort.o source.o split.o
sprintf.o startup.o subassign.o subscript.o subset.o summary.o
sysutils.o unique.o util.o version.o vfonts.o xxxpr.o   `ls ../appl/*.o
../nmath/*.o ../unix/*.o  2>/dev/null`  -lg2c -lm -lgcc_s 
../extra/zlib/libz.a ../extra/bzip2/libbz2.a ../extra/pcre/libpcre.a 
-lreadline  -ldl -lm
/usr/lib64/gcc-lib/x86_64-suse-linux/3.3.4/../../../../x86_64-suse-linux/bin/ld:
CConverters.o: relocation R_X86_64_32S against `R_FunTab' can not be
used when making a shared object; recompile with -fPIC
CConverters.o: could not read symbols: Bad value
collect2: ld returned 1 exit status

we tried setting the -fPIC using the export CFLAGS=-fPIC but that didn't
help (more output but same bug later on)

Thanks in advance,

Francis


here is the full log + more info:

uname -a
Linux stat 2.6.8-24-smp #1 SMP Wed Oct 6 09:16:23 UTC 2004 x86_64 x86_64
x86_64 GNU/Linux

.......(last part of configure)....

R is now configured for x86_64-unknown-linux-gnu

  Source directory:          .
  Installation directory:    /usr/local

  C compiler:                gcc  -g -O2 -std=gnu99
  Fortran 77 compiler:       g77  -g -O2

  C++ compiler:              g++  -g -O2
  Fortran 90/95 compiler:    g77 -g -O2

  Interfaces supported:      X11
  External libraries:        readline
  Additional capabilities:   iconv, MBCS, NLS
  Options enabled:           shared library, R profiling

  Recommended packages:      yes

configure: WARNING: you cannot build PDF versions of all the help pages
configure: WARNING: I could not determine a browser
configure: WARNING: I could not determine a PDF viewer

[1806][root at stat:/opt/R]$ make
make[1]: Entering directory `/opt/R-2.3.1/m4'
make[1]: Nothing to be done for `R'.
make[1]: Leaving directory `/opt/R-2.3.1/m4'
make[1]: Entering directory `/opt/R-2.3.1/tools'
make[1]: Nothing to be done for `R'.
make[1]: Leaving directory `/opt/R-2.3.1/tools'
make[1]: Entering directory `/opt/R-2.3.1/doc'
make[2]: Entering directory `/opt/R-2.3.1/doc/html'
make[3]: Entering directory `/opt/R-2.3.1/doc/html/search'
make[3]: Leaving directory `/opt/R-2.3.1/doc/html/search'
make[2]: Leaving directory `/opt/R-2.3.1/doc/html'
make[2]: Entering directory `/opt/R-2.3.1/doc/manual'
make[2]: Nothing to be done for `R'.
make[2]: Leaving directory `/opt/R-2.3.1/doc/manual'
make[1]: Leaving directory `/opt/R-2.3.1/doc'
make[1]: Entering directory `/opt/R-2.3.1/etc'
make[1]: Leaving directory `/opt/R-2.3.1/etc'
make[1]: Entering directory `/opt/R-2.3.1/share'
make[1]: Leaving directory `/opt/R-2.3.1/share'
make[1]: Entering directory `/opt/R-2.3.1/src'
make[2]: Entering directory `/opt/R-2.3.1/src/scripts'
creating src/scripts/R.fe
make[3]: Entering directory `/opt/R-2.3.1/src/scripts'
make[3]: Leaving directory `/opt/R-2.3.1/src/scripts'
make[2]: Leaving directory `/opt/R-2.3.1/src/scripts'
make[2]: Entering directory `/opt/R-2.3.1/src/include'
config.status: creating src/include/config.h
config.status: src/include/config.h is unchanged
Rmath.h is unchanged
make[3]: Entering directory `/opt/R-2.3.1/src/include/R_ext'
make[3]: Nothing to be done for `R'.
make[3]: Leaving directory `/opt/R-2.3.1/src/include/R_ext'
make[2]: Leaving directory `/opt/R-2.3.1/src/include'
make[2]: Entering directory `/opt/R-2.3.1/src/extra'
make[3]: Entering directory `/opt/R-2.3.1/src/extra/bzip2'
make[4]: Entering directory `/opt/R-2.3.1/src/extra/bzip2'
make[4]: Leaving directory `/opt/R-2.3.1/src/extra/bzip2'
make[4]: Entering directory `/opt/R-2.3.1/src/extra/bzip2'
make[4]: `libbz2.a' is up to date.
make[4]: Leaving directory `/opt/R-2.3.1/src/extra/bzip2'
make[3]: Leaving directory `/opt/R-2.3.1/src/extra/bzip2'
make[3]: Entering directory `/opt/R-2.3.1/src/extra/pcre'
make[4]: Entering directory `/opt/R-2.3.1/src/extra/pcre'
make[4]: Leaving directory `/opt/R-2.3.1/src/extra/pcre'
make[4]: Entering directory `/opt/R-2.3.1/src/extra/pcre'
make[4]: `libpcre.a' is up to date.
make[4]: Leaving directory `/opt/R-2.3.1/src/extra/pcre'
make[3]: Leaving directory `/opt/R-2.3.1/src/extra/pcre'
make[3]: Entering directory `/opt/R-2.3.1/src/extra/zlib'
make[4]: Entering directory `/opt/R-2.3.1/src/extra/zlib'
make[4]: Leaving directory `/opt/R-2.3.1/src/extra/zlib'
make[4]: Entering directory `/opt/R-2.3.1/src/extra/zlib'
make[4]: `libz.a' is up to date.
make[4]: Leaving directory `/opt/R-2.3.1/src/extra/zlib'
make[3]: Leaving directory `/opt/R-2.3.1/src/extra/zlib'
make[2]: Leaving directory `/opt/R-2.3.1/src/extra'
make[2]: Entering directory `/opt/R-2.3.1/src/appl'
make[3]: Entering directory `/opt/R-2.3.1/src/appl'
make[3]: Leaving directory `/opt/R-2.3.1/src/appl'
make[3]: Entering directory `/opt/R-2.3.1/src/appl'
make[3]: `stamp-lo' is up to date.
make[3]: Leaving directory `/opt/R-2.3.1/src/appl'
make[2]: Leaving directory `/opt/R-2.3.1/src/appl'
make[2]: Entering directory `/opt/R-2.3.1/src/nmath'
make[3]: Entering directory `/opt/R-2.3.1/src/nmath'
make[3]: Leaving directory `/opt/R-2.3.1/src/nmath'
make[3]: Entering directory `/opt/R-2.3.1/src/nmath'
make[3]: `stamp-lo' is up to date.
make[3]: Leaving directory `/opt/R-2.3.1/src/nmath'
make[2]: Leaving directory `/opt/R-2.3.1/src/nmath'
make[2]: Entering directory `/opt/R-2.3.1/src/unix'
make[3]: Entering directory `/opt/R-2.3.1/src/unix'
make[3]: Leaving directory `/opt/R-2.3.1/src/unix'
make[3]: Entering directory `/opt/R-2.3.1/src/unix'
make[3]: `stamp-lo' is up to date.
make[3]: Leaving directory `/opt/R-2.3.1/src/unix'
make[2]: Leaving directory `/opt/R-2.3.1/src/unix'
make[2]: Entering directory `/opt/R-2.3.1/src/main'
make[3]: Entering directory `/opt/R-2.3.1/src/main'
make[3]: Leaving directory `/opt/R-2.3.1/src/main'
make[3]: Entering directory `/opt/R-2.3.1/src/main'
gcc -shared -L/usr/local/lib64  -o libR.so Rembedded.o CConverters.o
CommandLineArgs.o Rdynload.o Renviron.o RNG.o apply.o arithmetic.o
apse.o array.o attrib.o base.o bind.o builtin.o character.o coerce.o
colors.o complex.o connections.o context.o cov.o cum.o dcf.o datetime.o
debug.o deparse.o deriv.o dotcode.o dounzip.o dstruct.o duplicate.o
engine.o envir.o errors.o eval.o format.o fourier.o gevents.o gram.o
gram-ex.o graphics.o identical.o internet.o iosupport.o lapack.o list.o
localecharset.o logic.o main.o mapply.o match.o memory.o model.o names.o
objects.o optim.o optimize.o options.o par.o paste.o pcre.o platform.o
plot.o plot3d.o plotmath.o print.o printarray.o printvector.o
printutils.o qsort.o random.o regex.o registration.o relop.o rlocale.o
saveload.o scan.o seq.o serialize.o size.o sort.o source.o split.o
sprintf.o startup.o subassign.o subscript.o subset.o summary.o
sysutils.o unique.o util.o version.o vfonts.o xxxpr.o   `ls ../appl/*.o
../nmath/*.o ../unix/*.o  2>/dev/null`  -lg2c -lm -lgcc_s 
../extra/zlib/libz.a ../extra/bzip2/libbz2.a ../extra/pcre/libpcre.a 
-lreadline  -ldl -lm
/usr/lib64/gcc-lib/x86_64-suse-linux/3.3.4/../../../../x86_64-suse-linux/bin/ld:
CConverters.o: relocation R_X86_64_32S against `R_FunTab' can not be
used when making a shared object; recompile with -fPIC
CConverters.o: could not read symbols: Bad value
collect2: ld returned 1 exit status
make[3]: *** [libR.so] Error 1
make[3]: Leaving directory `/opt/R-2.3.1/src/main'
make[2]: *** [R] Error 2
make[2]: Leaving directory `/opt/R-2.3.1/src/main'
make[1]: *** [R] Error 1
make[1]: Leaving directory `/opt/R-2.3.1/src'
make: *** [R] Error 1








confidentiality notice:
The information contained in this e-mail is confidential and...{{dropped}}


From spencer.graves at pdf.com  Thu Jul 13 19:24:40 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 13 Jul 2006 10:24:40 -0700
Subject: [R] convert ms() to optim()
In-Reply-To: <20060707131817.83116.qmail@web31206.mail.mud.yahoo.com>
References: <20060707131817.83116.qmail@web31206.mail.mud.yahoo.com>
Message-ID: <44B681D8.7030205@pdf.com>

	  Have you worked through the examples in the help pages for 'optim' 
and 'nlminb'?  Also, 'nls', 'nlm', and 'mle{stats4}' provide related 
capabilities.

	  If you'd like further help from this groups, please submit another 
post.  To increase your chances of a quick, informative response, I 
suggest you include a simple, self-contained example of something you 
tried that didn't produce what you expected, as suggested in the posting 
guide! "www.R-project.org/posting-guide.html".

	  Best Wishes,
	  Spencer Graves	  	

Xiaodong Jin wrote:
>   How to convert the following ms() in Splus to Optim in R? The "Calc" function is also attached.
>    
>   ms(~ Calc(a.init, B, v, off, d, P.a, lambda.a, P.y, lambda.y,
>   10^(-8), FALSE, 20, TRUE)$Bic,
>   start = list(lambda.a = 0.5, lambda.y = 240),
>   control = list(maxiter = 10, tol = 0.1))
>    
>   Calc <- function(A.INIT., X., V., OFF., D.,
>   P1., LAMBDA1., P2., LAMBDA2.,
>   TOL., MONITOR., MAX.ITER., TRACE.){
>   lambda1 <- abs(LAMBDA1.)
>   lambda2 <- abs(LAMBDA2.)
>   P <- lambda1 * P1. + lambda2 * P2.
>   a <- Estimate(A.INIT., X., V., OFF., D., P,
>   TOL., MONITOR., MAX.ITER.)
>   Ita <- OFF. + X. %*% a
>   Mu <- c(exp(Ita))
>   Wt <- Mu * V.
>   Bt.W.B <- t(X.) %*% (Wt * X.)
>   BtWBplusP <- Bt.W.B + P
>   Rhs <- Bt.W.B %*% a + t(X.) %*% (V. * (D. - Mu))
>   a <- solve(BtWBplusP, Rhs)
>   Tr <- sum(diag(solve(BtWBplusP, Bt.W.B)))
>   y.init <- D.
>   y.init[D.==0] <- 10^(-4)
>   Dev <- 2*sum( V. * D.*log(y.init/Mu) )
>   Bic <- Dev + log(sum(V.)) * Tr
>   Hazard <- Ita - OFF.
>   if (TRACE. == TRUE) cat(lambda1, lambda2, Bic, "\n")
>   return(a, Hazard, Tr, Dev, Bic, BtWBplusP)
>    
>   Thanks, 
>   Shelton
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From gangchen at mail.nih.gov  Thu Jul 13 20:32:39 2006
From: gangchen at mail.nih.gov (Gang Chen)
Date: Thu, 13 Jul 2006 14:32:39 -0400
Subject: [R]  Invoke operating system command
Message-ID: <9BCF36DF-01B9-48C0-945C-69E803D99C74@mail.nih.gov>

Hi all,

How can I invoke an operating system command in R? I mean something  
like exclamation mark (!) inside Matlab.

Thanks,
Gang


From cpowell at broncus.com  Thu Jul 13 20:38:45 2006
From: cpowell at broncus.com (Corey Powell)
Date: Thu, 13 Jul 2006 11:38:45 -0700
Subject: [R] References verifying accuracy of R for basic statistical
	calculations and tests
Message-ID: <23F2120BE0FF5B47BC998610E35B5C25B0B2E1@broncus2.broncus.int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060713/5d963f6a/attachment.pl 

From rolf at erdos.math.unb.ca  Thu Jul 13 20:38:43 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Thu, 13 Jul 2006 15:38:43 -0300 (ADT)
Subject: [R] Invoke operating system command
Message-ID: <200607131838.k6DIchuL004661@erdos.math.unb.ca>


?system


From ligges at statistik.uni-dortmund.de  Thu Jul 13 20:43:11 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 13 Jul 2006 20:43:11 +0200
Subject: [R] Invoke operating system command
In-Reply-To: <9BCF36DF-01B9-48C0-945C-69E803D99C74@mail.nih.gov>
References: <9BCF36DF-01B9-48C0-945C-69E803D99C74@mail.nih.gov>
Message-ID: <44B6943F.402@statistik.uni-dortmund.de>

Gang Chen wrote:
> Hi all,
> 
> How can I invoke an operating system command in R? I mean something  
> like exclamation mark (!) inside Matlab.


See ?system (and additionally ?shell if you are on Windows).

Uwe Ligges

> Thanks,
> Gang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ligges at statistik.uni-dortmund.de  Thu Jul 13 20:47:20 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 13 Jul 2006 20:47:20 +0200
Subject: [R] Problem installing local additional packages of Rcmdr
	(WinXP)
In-Reply-To: <op.tcmvsab03mu6w9@davinci.epm.net.co>
References: <op.tcmvsab03mu6w9@davinci.epm.net.co>
Message-ID: <44B69538.1060503@statistik.uni-dortmund.de>

Kenneth Cabrera wrote:
> Hi R users:
> 
> I download the .zip files and also PACKAGES and PACKAGES.gz files
> in a local directory, but when I call
> 
> library(Rcmdr)
> 
> It shows me a tcl/tk windows telling me to install the additional
> packages, the it takes me to another window where I must specify
> the path where the .zip files are located.
> I do it, but I obtain the following message:
> 
> Loading required package: tcltk
> Loading Tcl/Tk interface ... done
> Loading required package: car
> Erro en gzfile(file, "r") : no fue posible abrir la conexi?n
> 
> Versi?n del Rcmdr 1.1-7
> 
> 
> And the Rcmdr frame opens but without installing the additional
> packages. I am sure I download the required packages and the
> PACKAGES and PACKAGES.gz files.
> 
> What am I doing wrong?


You do not need PACKAGES and PACKAGES.gz, but you should install all 
required packages (incuding package "car" - "Rcmdr" depends on it) 
correctly. Looks like your "car" installation is broken. Simply 
reinstall it.
This can be done using the Windows GUI or with install.packages(). How 
to install a local zip file is explained in ?install.packages.
Be sure you have downloaded a binary version of the package for Windows 
for R-2.3.x.

Uwe Ligges



> 
> If I make the same procedure with the internet conexion it
> works fine, but I need to install all the packages in computers
> without internet conexion.
> 
> Thank you for your help.
> 
> Kenneth
> 
> $platform
> [1] "i386-pc-mingw32"
> $arch
> [1] "i386"
> $os
> [1] "mingw32"
> $system
> [1] "i386, mingw32"
> $status
> [1] ""
> $major
> [1] "2"
> $minor
> [1] "3.1"
> $year
> [1] "2006"
> $month
> [1] "06"
> $day
> [1] "01"
> $`svn rev`
> [1] "38247"
> $language
> [1] "R"
> $version.string
> [1] "Version 2.3.1 (2006-06-01)"
> 
> 
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From clint at ecy.wa.gov  Thu Jul 13 20:45:56 2006
From: clint at ecy.wa.gov (Clint Bowman)
Date: Thu, 13 Jul 2006 11:45:56 -0700 (PDT)
Subject: [R] Invoke operating system command
In-Reply-To: <9BCF36DF-01B9-48C0-945C-69E803D99C74@mail.nih.gov>
References: <9BCF36DF-01B9-48C0-945C-69E803D99C74@mail.nih.gov>
Message-ID: <Pine.LNX.4.62.0607131145140.17472@aeolus.ecy.wa.gov>

?system

Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Dispersion Modeler		INTERNET:	clint at math.utah.edu
Air Quality Program		VOICE:		(360) 407-6815
Department of Ecology		FAX:		(360) 407-7534

	USPS:  		PO Box 47600, Olympia, WA 98504-7600
	Parcels:	300 Desmond Drive, Lacey, WA 98503-1274

On Thu, 13 Jul 2006, Gang Chen wrote:

> Hi all,
>
> How can I invoke an operating system command in R? I mean something
> like exclamation mark (!) inside Matlab.
>
> Thanks,
> Gang
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From clint at ecy.wa.gov  Thu Jul 13 20:47:39 2006
From: clint at ecy.wa.gov (Clint Bowman)
Date: Thu, 13 Jul 2006 11:47:39 -0700 (PDT)
Subject: [R] References verifying accuracy of R for basic statistical
 calculations and tests
In-Reply-To: <23F2120BE0FF5B47BC998610E35B5C25B0B2E1@broncus2.broncus.int>
References: <23F2120BE0FF5B47BC998610E35B5C25B0B2E1@broncus2.broncus.int>
Message-ID: <Pine.LNX.4.62.0607131146380.17472@aeolus.ecy.wa.gov>

Actually, you may not want R to agree so precisely with some of the
packages since some well-known packages have some not-so-well-known
inaccuracies.

Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Dispersion Modeler		INTERNET:	clint at math.utah.edu
Air Quality Program		VOICE:		(360) 407-6815
Department of Ecology		FAX:		(360) 407-7534

	USPS:  		PO Box 47600, Olympia, WA 98504-7600
	Parcels:	300 Desmond Drive, Lacey, WA 98503-1274

On Thu, 13 Jul 2006, Corey Powell wrote:

> Do you know of any references that verify the accuracy of R for basic statistical calculations and tests.  The results of these studies should indicate that R results are the same as the results of other statistical packages to a certain number of decimal places on some benchmark calculations.
>
> Thanks,
>
> Corey Powell
> Clinical Data Analyst
> Broncus Technologies
> cpowell at broncus.com
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ricardosilva at serasa.com.br  Thu Jul 13 20:51:49 2006
From: ricardosilva at serasa.com.br (ricardosilva at serasa.com.br)
Date: Thu, 13 Jul 2006 15:51:49 -0300
Subject: [R] MLE and QR classes
Message-ID: <OF39C0C230.808EC0BF-ON032571AA.00674426-032571AA.00679F6C@serasa.com.br>

Hi,

I load my data set and separate it as folowing:

presu <- read.table("C:/_Ricardo/Paty/qtdata_f.txt", header=TRUE, sep="\t",
na.strings="NA", dec=".", strip.white=TRUE)
dep<-presu[,3];
exo<-presu[,4:92];

Now, I want to use it using the wls and quantreg packages. How I change the
data classes for mle and rq objects?

Thanks a lot,
________________________________________
Ricardo Gon?alves Silva, M. Sc.
Apoio aos Processos de Modelagem Matem?tica
Econometria & Inadimpl?ncia
Serasa S.A.
(11) - 6847-8889
ricardosilva at serasa.com.br

**********************************************************************************

As informa??es contidas nesta mensagem e no(s) arquivo(s) anexo(s) s?o
endere?adas exclusivamente ?(s) pessoa(s) e/ou institui??o(?es) acima
indicada(s), podendo conter dados confidenciais, os quais n?o podem, sob
qualquer forma ou pretexto, ser utilizados, divulgados, alterados,
impressos ou copiados, total ou parcialmente, por pessoas n?o autorizadas.
Caso n?o seja o destinat?rio, favor providenciar sua exclus?o e notificar o
remetente imediatamente.  O uso impr?prio ser? tratado conforme as normas
da empresa e da legisla??o em vigor.
Esta mensagem expressa o posicionamento pessoal do subscritor e n?o reflete
necessariamente a opini?o da Serasa.


From listar at sun.soci.niu.edu  Thu Jul 13 15:29:32 2006
From: listar at sun.soci.niu.edu (NIU Sociology Listserver)
Date: Thu, 13 Jul 2006 15:29:32 %z (CDT)
Subject: [R] List 'sssitalk' closed to public posts
Message-ID: <listar-07132006152932.12169.1@sun>

Sorry, but your posting is being returned to you.

The list is temporarily restricted.

If your post is substantive, dealing with SSSI issues, it will be
forwarded to the list. If it is intended as private email, it
will be forwarded accordingly.

If you wish to subscribe to SSSITALK, send a message that says:
    subscribe sssitalk   firstname  lastname

(firstname/lastname are YOUR first and last name)

If you have questions about SSSI, see:
   http://www.soci.niu.edu/~sssi

If you have questions, contact:
  
        Jim Thomas at:   jthomas at sun.soci.niu.edu


---
Listar v1.0.0 - job execution complete.


From torsten.mathies at matec-gmbh.com  Thu Jul 13 22:39:37 2006
From: torsten.mathies at matec-gmbh.com (Torsten Mathies)
Date: Thu, 13 Jul 2006 22:39:37 +0200
Subject: [R] Write a summary or a longer text to a graphical device
Message-ID: <000501c6a6bc$75a7c6f0$5a03a8c0@msc.de>

How can I write a text, such as a result of a function or an explanation
to a graphic device?

When I try plot, I'm unable to reduce the axes.

Greetings

torsten


From gerifalte28 at hotmail.com  Thu Jul 13 22:57:12 2006
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 13 Jul 2006 20:57:12 +0000
Subject: [R] Write a summary or a longer text to a graphical device
In-Reply-To: <000501c6a6bc$75a7c6f0$5a03a8c0@msc.de>
Message-ID: <BAY103-F55C55ACD316CCE2C92ADCA66E0@phx.gbl>

?text
?plotmath

Cheers

Francisco

Dr. Francisco J. Zagmutt
College of Veterinary Medicine and Biomedical Sciences
Colorado State University




>From: "Torsten Mathies" <torsten.mathies at matec-gmbh.com>
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] Write a summary or a longer text to a graphical device
>Date: Thu, 13 Jul 2006 22:39:37 +0200
>
>How can I write a text, such as a result of a function or an explanation
>to a graphic device?
>
>When I try plot, I'm unable to reduce the axes.
>
>Greetings
>
>torsten
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


From liuwensui at gmail.com  Thu Jul 13 23:22:01 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Thu, 13 Jul 2006 17:22:01 -0400
Subject: [R] comparison between parametric and semiparametric models
Message-ID: <1115a2b00607131422q37d67499q6099fad6f86f4bfd@mail.gmail.com>

Dear Listers,

I have a question somehow off-topic but think I can find a good answer here.

Using the same dataset, I've built a logistic regression and
generalized additive model. Might I use likelihood ratio test to see
if there is significant improvement in GAM model? Something like:
(deviance of GLM - deviance of GAM) with degree of free = additional
number of DF in GAM using chisquare test.

Thank you so much!


From afshart at exchange.sba.miami.edu  Thu Jul 13 23:42:20 2006
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Thu, 13 Jul 2006 17:42:20 -0400
Subject: [R] collapsing plot lines
Message-ID: <6BCB4D493A447546A8126F24332056E803C932D9@school1.business.edu>

 
All,

When plotting a group data object, is there a way to have the plots
of all the groups collapsed onto one plot, i.e., instead of 
separate panels as would be produced from example below?

grouped.data.example <- groupedData(Y ~ Time | Patient_no, 
	data = example.frm)
plot(grouped.data.example)

cheers,
dave
ps - I'm on windows


From falimadhi at iq.harvard.edu  Fri Jul 14 00:10:48 2006
From: falimadhi at iq.harvard.edu (Ferdinand Alimadhi)
Date: Thu, 13 Jul 2006 18:10:48 -0400
Subject: [R] set the bahavior that R deal with missing values?
In-Reply-To: <44B668F0.2020704@iq.harvard.edu>
References: <2fc17e30607130731g43d2d077m201daae6d9d13c9e@mail.gmail.com>
	<44B668F0.2020704@iq.harvard.edu>
Message-ID: <44B6C4E8.1060303@iq.harvard.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060713/8213bf13/attachment.pl 

From jesse.canchola.b at bayer.com  Fri Jul 14 00:40:46 2006
From: jesse.canchola.b at bayer.com (Jesse Albert Canchola)
Date: Thu, 13 Jul 2006 15:40:46 -0700
Subject: [R] looping using combinatorics
Message-ID: <OFAC2F0299.BA936621-ON882571AA.00790AFA-882571AA.007C94EA@bayer.com>

I have a problem where I need to loop over the total combinations of 
vectors (combined once chosen via combinatorics).  Here is a 
simplification of the problem:

STEP 1:  Define three vectors a, b, c.
STEP 2:  Combine all possible pairwise vectors (i.e., 3 choose 2 = 3 
possible pairs of vectors: ab,ac, bc)
NOTE:  the actual problem has 8 choose 4, 8 choose 5 and 8 choose 6 
combinations.
STEP 3:  Do the same math on each pairwise combination and spit out 
answers each time

####### BEGIN CODE #######
#STEP 1
a1 <- c(1,2,3,4,5,6,7,8,9,10,11,12)
a <- matrix(a1,2,3,byrow=T)
a

b1 <- c(13,14,15,16,17,18,19,20,21,22,23,24)
b <- matrix(b1,2,3,byrow=T)
b

c1 <- c(25,26,27,28,29,30,31,32,33,34,35,36)
c <- matrix(b1,2,3,byrow=T)
c

# example:  combine the first two vectors "a" and "b"
combab <- rbind(a,b)
 
# the a,b combined data from the algorithm later below should look like
# something like the following:
combab 

# use the combinatorics "combn" function found in the "combinat" package 
on CRAN
m <- combn(3,2) # three choose two combinations
m

# the first assignment below should be numeric and then subsequent 
# assignments as character since the first time you assign a number to 
# a character in a matrix the rest of the numbers in the matrix are 
coerced to character
m[m==1]='a'; m[m=='2']='b'; m[m=='3']='c'
m

#STEP 2: combine pairwise vectors into a matrix or frame
for (i in dim(m)[1])
    for (j in dim(m)[2])
        {
            combined <- 
rbind(cat(format(m[i]),"\n"),cat(format(m[j]),"\n")) #cat/format removes 
the quotes
            combined
        }
traceback() 


#STEP 3: {not there yet}
################# END CODE ################

The problem is that in STEP 2 (not complete), the results in the rbind are 
not recognized as the objects they represent (i.e., the "a" without quotes 
is not recognized as the data object we defined in STEP 1.  Perhaps this 
is a parsing problem.  Perhaps there is an alterative way to do this.  I 
looked pretty long and hard in the CRAN libraries but alas, I am stuck. 
BTW, I picked up R about a month ago (I used primarily SAS, Stata and 
SPSS).

Regards and TIA,
Jesse






Jesse A. Canchola
Biostatistician III
Bayer Healthcare
725 Potter St.
Berkeley, CA 94710
P: 510.705.5855
F: 510.705.5718
E: Jesse.Canchola.b at Bayer.Com




_______________________________________________________________________________________________

The information contained in this e-mail is for the exclusive use of the intended recipient(s) and may be confidential, proprietary, and/or legally privileged.  Inadvertent disclosure of this message does not constitute a waiver of any privilege.  If you receive this message in error, please do not directly or indirectly use, print, copy, forward, or disclose any part of this message.  Please also delete this e-mail and all copies and notify the sender.  Thank you.

For alternate languages please go to http://bayerdisclaimer.bayerweb.com


From tuechler at gmx.at  Fri Jul 14 01:22:27 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Fri, 14 Jul 2006 00:22:27 +0100
Subject: [R] Keep value lables with data frame manipulation
In-Reply-To: <20060713123637.BEC60457@po-d.temple.edu>
Message-ID: <3.0.6.32.20060714002227.00ada2e0@pop.gmx.net>

At 12:36 13.07.2006 -0400, Richard M. Heiberger wrote:
>> Further I do not see a simple method to label numerical
>> variables. I often encounter discrete, but still metric data, as e.g. risk
>> scores. Usually it would be nice to use them in their original coding,
>> which may include zero or decimal places and to label them at the same
time.
>
>## For this specific case, I use a "position" attribute.
>
>
>tmp <- data.frame(y=rnorm(30), x=factor(rep(c(0,1,2,4,8), 6)))
>attr(tmp$x, "position") <- as.numeric(as.character(tmp$x))
>
>tmp
>as.numeric(tmp$x)
>attr(tmp$x, "position")
>
>bwplot(y ~ x, data=tmp)
>
>panel.bwplot.position <- function(x, y, ..., x.at) {
>         for (x.i in x.at) {
>           y.i <- y[x.i==x]
>           panel.bwplot(rep(x.i, length(y.i)), y.i, ...)
>         }
>       }
>
>bwplot.position <- function(formula, data, ..., x.at) {
>  if (missing(x.at)) {
>    x.name <- dimnames(attr(terms(formula),"factors"))[[2]]
>    x.at <- attr(data[[x.name]], "position")
>  }
>  bwplot(formula, data, ...,
>         x.at=x.at,
>         panel=panel.bwplot.position,
>         scales=list(x=list(at=x.at, limits=x.at+c(-1,1))))
>}
>
>bwplot.position(y ~ x, data=tmp)
>
>
>## The above is a simplified version of
>##     panel.bwplot.intermediate.hh
>## in the online files for
>##                 Statistical Analysis and Data Display
>##                 Richard M. Heiberger and Burt Holland
>##    http://springeronline.com/0-387-40270-5
>## 
>## An example of a boxplot with both placement and color of the boxes
>## under user control is in
>## 
>## http://astro.ocis.temple.edu/~rmh/HH/bwplot-color.pdf
>
Richard, 

I recognized your solution already last time you mentioned it and I am
thinking about a similar one, (ab)using the names attribute.
In principle it seems easy to solve this kind of problems with additional
attributes, but without defining a new class and corresponding methods
additional attributes get easily lost when indexing/subsetting.
The names attribute seems to be rather "resistent". As far as I see, it
survives indexing/subsetting and even sorting and this seems to be true
also for factors.

Greetings,

Heinz


From mihainica at yahoo.com  Fri Jul 14 01:27:16 2006
From: mihainica at yahoo.com (Mihai Nica)
Date: Thu, 13 Jul 2006 16:27:16 -0700 (PDT)
Subject: [R] ols/gls or systemfit (OLS, WLS, SUR) give identical results
Message-ID: <20060713232716.38834.qmail@web52507.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060713/22446f91/attachment.pl 

From tplate at acm.org  Fri Jul 14 02:27:31 2006
From: tplate at acm.org (Tony Plate)
Date: Thu, 13 Jul 2006 18:27:31 -0600
Subject: [R] References verifying accuracy of R for basic statistical
 calculations and tests
In-Reply-To: <23F2120BE0FF5B47BC998610E35B5C25B0B2E1@broncus2.broncus.int>
References: <23F2120BE0FF5B47BC998610E35B5C25B0B2E1@broncus2.broncus.int>
Message-ID: <44B6E4F3.7050209@acm.org>

This might be a place to start:

http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html

Among the references listed there are:

Assessing the Reliability of Statistical Software: Part I by B. D. 
McCullough (1998)
http://www.amstat.org/publications/tas/mccull-1.pdf

Assessing the Reliability of Statistical Software: Part II by B. D. 
McCullough (1999)
http://www.amstat.org/publications/tas/mccull.pdf

Those might have some relevance

Then, doing within an R session:

 > RSiteSearch("Assessing Reliability Statistical Software")

turns up 14 hits, many of them looking relevant

[leaving "the" and "of" in the query results in the search engine timing 
out - odd?]

-- Tony Plate


Corey Powell wrote:
> Do you know of any references that verify the accuracy of R for basic statistical calculations and tests.  The results of these studies should indicate that R results are the same as the results of other statistical packages to a certain number of decimal places on some benchmark calculations.
> 
> Thanks,
> 
> Corey Powell
> Clinical Data Analyst
> Broncus Technologies
> cpowell at broncus.com
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Fri Jul 14 04:39:08 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 13 Jul 2006 22:39:08 -0400
Subject: [R] looping using combinatorics
In-Reply-To: <OFAC2F0299.BA936621-ON882571AA.00790AFA-882571AA.007C94EA@bayer.com>
References: <OFAC2F0299.BA936621-ON882571AA.00790AFA-882571AA.007C94EA@bayer.com>
Message-ID: <971536df0607131939w7fe8fabcy7a255f981735fa31@mail.gmail.com>

I assume your question is given 3 vectors of the same length: a, b and c
how do we loop over pairs of them.  In the following each iteration displays
one pair:

   library(combinat)
   DF <- data.frame(a = 1:4, b = 5:8, c = 9:12)
   for(idx in as.data.frame(combn(3,2))) print(DF[,idx])

On 7/13/06, Jesse Albert Canchola <jesse.canchola.b at bayer.com> wrote:
> I have a problem where I need to loop over the total combinations of
> vectors (combined once chosen via combinatorics).  Here is a
> simplification of the problem:
>
> STEP 1:  Define three vectors a, b, c.
> STEP 2:  Combine all possible pairwise vectors (i.e., 3 choose 2 = 3
> possible pairs of vectors: ab,ac, bc)
> NOTE:  the actual problem has 8 choose 4, 8 choose 5 and 8 choose 6
> combinations.
> STEP 3:  Do the same math on each pairwise combination and spit out
> answers each time
>
> ####### BEGIN CODE #######
> #STEP 1
> a1 <- c(1,2,3,4,5,6,7,8,9,10,11,12)
> a <- matrix(a1,2,3,byrow=T)
> a
>
> b1 <- c(13,14,15,16,17,18,19,20,21,22,23,24)
> b <- matrix(b1,2,3,byrow=T)
> b
>
> c1 <- c(25,26,27,28,29,30,31,32,33,34,35,36)
> c <- matrix(b1,2,3,byrow=T)
> c
>
> # example:  combine the first two vectors "a" and "b"
> combab <- rbind(a,b)
>
> # the a,b combined data from the algorithm later below should look like
> # something like the following:
> combab
>
> # use the combinatorics "combn" function found in the "combinat" package
> on CRAN
> m <- combn(3,2) # three choose two combinations
> m
>
> # the first assignment below should be numeric and then subsequent
> # assignments as character since the first time you assign a number to
> # a character in a matrix the rest of the numbers in the matrix are
> coerced to character
> m[m==1]='a'; m[m=='2']='b'; m[m=='3']='c'
> m
>
> #STEP 2: combine pairwise vectors into a matrix or frame
> for (i in dim(m)[1])
>    for (j in dim(m)[2])
>        {
>            combined <-
> rbind(cat(format(m[i]),"\n"),cat(format(m[j]),"\n")) #cat/format removes
> the quotes
>            combined
>        }
> traceback()
>
>
> #STEP 3: {not there yet}
> ################# END CODE ################
>
> The problem is that in STEP 2 (not complete), the results in the rbind are
> not recognized as the objects they represent (i.e., the "a" without quotes
> is not recognized as the data object we defined in STEP 1.  Perhaps this
> is a parsing problem.  Perhaps there is an alterative way to do this.  I
> looked pretty long and hard in the CRAN libraries but alas, I am stuck.
> BTW, I picked up R about a month ago (I used primarily SAS, Stata and
> SPSS).
>
> Regards and TIA,
> Jesse
>
>
>
>
>
>
> Jesse A. Canchola
> Biostatistician III
> Bayer Healthcare
> 725 Potter St.
> Berkeley, CA 94710
> P: 510.705.5855
> F: 510.705.5718
> E: Jesse.Canchola.b at Bayer.Com
>
>
>
>
> _______________________________________________________________________________________________
>
> The information contained in this e-mail is for the exclusive use of the intended recipient(s) and may be confidential, proprietary, and/or legally privileged.  Inadvertent disclosure of this message does not constitute a waiver of any privilege.  If you receive this message in error, please do not directly or indirectly use, print, copy, forward, or disclose any part of this message.  Please also delete this e-mail and all copies and notify the sender.  Thank you.
>
> For alternate languages please go to http://bayerdisclaimer.bayerweb.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From spencer.graves at pdf.com  Fri Jul 14 05:18:12 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 13 Jul 2006 20:18:12 -0700
Subject: [R] weights in glmrob
In-Reply-To: <f6b7dfdc0607110239m5d2ab64fx3ef2871bc4d2c028@mail.gmail.com>
References: <f6b7dfdc0607110239m5d2ab64fx3ef2871bc4d2c028@mail.gmail.com>
Message-ID: <44B70CF4.40204@pdf.com>

	  Have you worked through the several examples in the 'glmrob' help 
page?  Those include at least one case that produces sensible standard 
errors and at least one that return NAs for standard errors.

	  If this does not answer your question, I have two further suggestions:

	  1.  Make a local copy of the 'glmrob' function.  Then skim the 
documentation for 'debug', try 'debug(glmrob)' and issue your problem 
call to 'glmrob(...)'.  This will allow you to walk through the 
algorithm line by line.  At each step, you can look at the values of any 
of the variables created, change them if you like, etc.  With luck and 
perserverance, this may produce enlightenment.

	  2.  If you would still like help from this listserve, please submit 
another post.  Before you do, however, PLEASE do read the posting guide! 
"www.R-project.org/posting-guide.html".  In particular, if your example 
had been completely self contained, I might have been able to say more. 
  As it is, I can't think of any other way I can help you.

	  Hope this helps.
	  Spencer Graves

Celso Barros wrote:
> Dear List members,
> 
> 
>          MY APOLOGIES IF YOU ALREADY RECEIVED THIS. A COLLEAGUE TOLD ME HE
> COULD NOT OPEN MY PREVIOUS MESSAGE.
> 
> 
>         I am runnning a logistic regression that includes sample weights
> (expressed by a variable called WEIGHT) using glmob (from robustbase). My
> model looks like this:
> 
>        B12<-glmrob(SERE~COMP+COM,family=binomial,weights=WEIGHT)
> 
>        When I include the weights, my standard error columns, as well as the
> p-values, show only NAs. If I exclude weights=WEIGHTS, everything works. I
> would like to use the weights. Does anyone know what could be the problem?
> 
>        Thanks in advance,
> 
>                                            Celso
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From gyadav at ccilindia.co.in  Fri Jul 14 06:04:42 2006
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Fri, 14 Jul 2006 09:34:42 +0530
Subject: [R] regarding filtering the data
Message-ID: <OFD2949B89.D5F9A3B1-ON652571AB.0015BA14-652571A4.00168CFF@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060714/fdfa4889/attachment.pl 

From ggrothendieck at gmail.com  Fri Jul 14 06:35:55 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 14 Jul 2006 00:35:55 -0400
Subject: [R] regarding filtering the data
In-Reply-To: <OFD2949B89.D5F9A3B1-ON652571AB.0015BA14-652571A4.00168CFF@ccilindia.co.in>
References: <OFD2949B89.D5F9A3B1-ON652571AB.0015BA14-652571A4.00168CFF@ccilindia.co.in>
Message-ID: <971536df0607132135p685cb055jc8a22c7b1ad9d20f@mail.gmail.com>

1. read.csv just calls read.table.  Enter this into R to see its source code:

   read.csv

2.  See ?subset . For example using the builtin swiss data frame:

   subset(swiss, Fertility < Agriculture)

or

   swiss[swiss$Fertility < swiss$Agriculture, ]


On 7/14/06, gyadav at ccilindia.co.in <gyadav at ccilindia.co.in> wrote:
>
> Hi all
>
> a very happy greeting to you all.
> i have just joined the list, i am newbie in R and this is my first message
> ( before this i was a member of Linux kernel mailing list) and hence i
> woulk like to add that i know how to behave in the mailing lists.
>
> I have a data something like this which i am reading from a csv file using
> read.table (Is there any difference between read.csv and read.table as i
> can give sep="," in both of them).
>
> x=[1,2,3,4]
>
> and permutations are
> 1,2,3,4
> 2,1,3,4
> 3,1,2,4
> 4,1,2,3
> etc.
>
> actually these are the permutations where n = 4 and k = 1 to 4 using the
> combs function. Well I want to filter only the data in which the first
> value is smaller that second, second value is smaller than
> third...extending the logic to all the k ( whatever is its value at that
> time). please help me in this regards


From robert-mcfadden at o2.pl  Fri Jul 14 09:13:09 2006
From: robert-mcfadden at o2.pl (Robert Mcfadden)
Date: Fri, 14 Jul 2006 09:13:09 +0200
Subject: [R] TR: Latent Class Analysis
In-Reply-To: <20060713154619.6A2385A0D0@zeus.cochin.inserm.fr>
Message-ID: <000f01c6a714$f5e7c6f0$1191680a@robert>



> De : Pousset [mailto:maud.pousset w noos.fr]
> Envoyi : mardi 4 juillet 2006 18:38
> @ : 'r-help w stat.math.ethz.ch'
> Objet : Latent Class Analysis
> 
> 
> 
> Hello everybody,
> 
> 
> 
> I am working on latent class analysis and have already used the R
> function
> + lca ; (in the e1071 package). I ve got interesting results but I cant
> simply find out the methodology used by this routine :
> 
> 1) What kind of model is behind the routine (mixture model? If so, can you
> choose among different kind of distributions such as normal, Poisson,
> binomial)
> 
> 2) What kind of algorithm is used (hierarchical methods? Relocation
> methods?)
> 
> 3) Which criterion allows determining the best model?
> 
> In addition, I wonder if it is possible, with R software, to determine the
> best number of class or do one have to fix it a priori.
> 
> If one can help, thanks a lot,
> 
> 
> 
> Maud
> 
> INSERM U669, Cochin Hospital, Paris
> 
> 
> 


You can use package (available at R page) poLCA that has documentation
describing what you want. Look also at http://dlinzer.bol.ucla.edu/poLCA/
As far as you know BIC, AIC (based on chi-sqr and G^2 statistics) and
Cressie-Read allow you to choose the most appropriate model.
Best,
Robert


From dieter.menne at menne-biomed.de  Fri Jul 14 09:50:10 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 14 Jul 2006 07:50:10 +0000 (UTC)
Subject: [R] set the bahavior that R deal with missing values?
References: <2fc17e30607130733j2003eeadv2ec74eaff0d272a7@mail.gmail.com>
Message-ID: <loom.20060714T094523-625@post.gmane.org>

zhijie zhang <epistat <at> gmail.com> writes:

>  The default behavior in R when performing a regression model with missing
> values is to exclude any case that contains a
> missing value? How could i set the bahavior that R deal with missing values?

Difficult to say, since there is no such thing as a regression model in R, but
certainly more than a few dozens in several packages. Best show a self-contained
runnably sample where you would prefer some na.action (see, for example,
parameter na.action in lm).

Dieter


From dieter.menne at menne-biomed.de  Fri Jul 14 09:59:28 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 14 Jul 2006 07:59:28 +0000 (UTC)
Subject: [R] how to make a contour plot using data in long format
References: <BAY104-F32C1C6824F8B25463494BCD6E0@phx.gbl>
Message-ID: <loom.20060714T095800-585@post.gmane.org>

Maria Wang <mariayzw <at> hotmail.com> writes:

> 
> There is a dataset with Latitude Longitude information and corresponding 
> value shaped like this:
> Lati                     Longi                  Value
> 18.0001             -159.6667             123
> 18.0023             -159.6665             124
> I want to use this data to make a filled contour map imposed on an USA map. 
....

Best use package geoR. There are also some examples in MASS (chapter 14).

Dieter


From ferri.leberl at gmx.at  Fri Jul 14 10:23:13 2006
From: ferri.leberl at gmx.at (Mag. Ferri Leberl)
Date: Fri, 14 Jul 2006 10:23:13 +0200
Subject: [R] Export to LaTeX
Message-ID: <200607141023.14053.ferri.leberl@gmx.at>

Dear Everybody!
I want to export data to LaTeX. As I want to employ the data as freely as 
possible I want to avoid the xtable-command and instead generate some List 
like

\MyOwnPrettyCommand{Adam}{Auer}{17}
\MyOwnPrettyCommand{Bertram}{Bauer}{14}
\MyOwnPrettyCommand{Christoph}{Huber}{75}
\MyOwnPrettyCommand{Damian}{Dorfer}{69}
\MyOwnPrettyCommand{Emanuel}{Eder}{43}

with \MyOwnPrittyCommand defined elsewhere.
As a pitty, if I make up about such a table in r, lets call it "A", and use 
the commands

sink("tabelle.tex")
A
sink("anderedatei")

tabelle.tex will look like this:

     [,1]                  [,2]        [,3] [,4]     [,5] [,6] [,7]
[1,] "MyOwnPrettyCommand{" "Adam"      "}{" "Auer"   "}{" "17" "}" 
[2,] "MyOwnPrettyCommand{" "Bertram"   "}{" "Bauer"  "}{" "14" "}" 
[3,] "MyOwnPrettyCommand{" "Christoph" "}{" "Huber"  "}{" "75" "}" 
[4,] "MyOwnPrettyCommand{" "Damian"    "}{" "Dorfer" "}{" "69" "}" 
[5,] "MyOwnPrettyCommand{" "Emanuel"   "}{" "Eder"   "}{" "43" "}" 

So my question is how to export the data properly, without line-indices, 
without quotes but WITH backslashes.
Thank you in advance.
Yours,
Mag. Ferri Leberl


From scarrizo at it.usyd.edu.au  Fri Jul 14 10:23:44 2006
From: scarrizo at it.usyd.edu.au (Savrina Carrizo)
Date: Fri, 14 Jul 2006 18:23:44 +1000
Subject: [R] mgcv::gam error message
Message-ID: <001901c6a71e$d20f3fb0$b0734e81@laptopscarrizo>

Hi 

Could anyone please tell me what to do to resolve this error message?

I tried to run a gam with the mgcv package and got the following error:
"Error in qr.qty(qrc, sm$S[[1]]): NA/NaN/Inf in foreign function call (arg
5)"

(I have 116 covariates, I'm using the "cr" basis to speed things up, the
binomial family and, where necessary, have set the required k to lower than
the number of distinct values for a given covarate when less than the
default)

Thank you,
Savrina

scarrizo at it.usyd.edu.au
School of Information Technologies
Univeristy of Sydney


From tobias.verbeke at gmail.com  Fri Jul 14 10:44:06 2006
From: tobias.verbeke at gmail.com (Tobias Verbeke)
Date: Fri, 14 Jul 2006 10:44:06 +0200
Subject: [R] Export to LaTeX
In-Reply-To: <200607141023.14053.ferri.leberl@gmx.at>
References: <200607141023.14053.ferri.leberl@gmx.at>
Message-ID: <44B75956.7070702@telenet.be>

Mag. Ferri Leberl wrote:
> Dear Everybody!
> I want to export data to LaTeX. As I want to employ the data as freely as 
> possible I want to avoid the xtable-command and instead generate some List 
> like
>
> \MyOwnPrettyCommand{Adam}{Auer}{17}
> \MyOwnPrettyCommand{Bertram}{Bauer}{14}
> \MyOwnPrettyCommand{Christoph}{Huber}{75}
> \MyOwnPrettyCommand{Damian}{Dorfer}{69}
> \MyOwnPrettyCommand{Emanuel}{Eder}{43}
>
> with \MyOwnPrittyCommand defined elsewhere.
> As a pitty, if I make up about such a table in r, lets call it "A", and use 
> the commands
>
> sink("tabelle.tex")
> A
> sink("anderedatei")
>
> tabelle.tex will look like this:
>
>      [,1]                  [,2]        [,3] [,4]     [,5] [,6] [,7]
> [1,] "MyOwnPrettyCommand{" "Adam"      "}{" "Auer"   "}{" "17" "}" 
> [2,] "MyOwnPrettyCommand{" "Bertram"   "}{" "Bauer"  "}{" "14" "}" 
> [3,] "MyOwnPrettyCommand{" "Christoph" "}{" "Huber"  "}{" "75" "}" 
> [4,] "MyOwnPrettyCommand{" "Damian"    "}{" "Dorfer" "}{" "69" "}" 
> [5,] "MyOwnPrettyCommand{" "Emanuel"   "}{" "Eder"   "}{" "43" "}" 
>
> So my question is how to export the data properly, without line-indices, 
> without quotes but WITH backslashes.
>   
mydf <- as.data.frame(
      rbind(c("Adam",      "Auer",   17),
      c("Bertram",   "Bauer",  14),
      c("Christoph", "Huber",  75),
      c("Damian",    "Dorfer", 69),
      c("Emanuel",   "Eder",   43))
      )
cat(
    paste("\\MyCommand{", mydf[,"V1"], "}{", mydf[,"V2"], "}{", 
mydf[,"V3"], "}",
          sep ="", collapse = "\n")
    , file = "tabelle.tex")

HTH,
Tobias
> Thank you in advance.
> Yours,
> Mag. Ferri Leberl
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>


From Rau at demogr.mpg.de  Fri Jul 14 10:42:25 2006
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Fri, 14 Jul 2006 10:42:25 +0200
Subject: [R] References verifying accuracy of R for basic
 statisticalcalculations and tests
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E6013DB51B@HERMES.demogr.mpg.de>

Hi,
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Corey Powell
> 
> Do you know of any references that verify the accuracy of R 
> for basic statistical calculations and tests.  The results of 
> these studies should indicate that R results are the same as 
> the results of other statistical packages to a certain number 
> of decimal places on some benchmark calculations.

I don't know of any references, but maybe you can somehow "verify the
accuracy of R" by running some analysis with the "NIST Statistical
Reference Datasets"; the URL is http://www.itl.nist.gov/div898/strd/ 
So maybe you can run the analyses mentioned there and say that R
(hopefully) returned the correct results.

Hope this helps,
Roland

----------
This mail has been sent through the MPI for Demographic Rese...{{dropped}}


From jim at bitwrit.com.au  Sat Jul 15 01:11:27 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Fri, 14 Jul 2006 19:11:27 -0400
Subject: [R] colors on graph
In-Reply-To: <15C100200A5F4E45AF8CFB45A926EF3415E8FA@allexch01.alliance.com>
References: <15C100200A5F4E45AF8CFB45A926EF3415E8FA@allexch01.alliance.com>
Message-ID: <44B8249F.7090603@bitwrit.com.au>

COMTE Guillaume wrote:
> Hy all,
> 
>  
> 
> I need to draw something in 2 dimension that has 3 dimension, the choice
> has been made to use colors to display the third dimension into the
> graph.
> 
>  
> 
> Has someone done something like that, i can't figure out how to
> parametize the colors.
> 
Hi Guilllaume,

Have a look at color.scale in the plotrix package.

Jim


From dieter.menne at menne-biomed.de  Fri Jul 14 11:06:19 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 14 Jul 2006 09:06:19 +0000 (UTC)
Subject: [R] Export to LaTeX
References: <200607141023.14053.ferri.leberl@gmx.at>
Message-ID: <loom.20060714T110040-103@post.gmane.org>

Mag. Ferri Leberl <ferri.leberl <at> gmx.at> writes:

> I want to export data to LaTeX. As I want to employ the data as freely as 
> possible I want to avoid the xtable-command and instead generate some List 
> like
> 
> \MyOwnPrettyCommand{Adam}{Auer}{17}
> \MyOwnPrettyCommand{Bertram}{Bauer}{14}
> \MyOwnPrettyCommand{Christoph}{Huber}{75}
> 
> with \MyOwnPrittyCommand defined elsewhere.


Method 1: Use latex in package Hmisc. It is VERY flexible, and works for me in
most cases.

Hint 1: "\" rarely works in R as in C, it must be escaped to "\\"

Hint 2: To increase you changes of getting a reply, always provide a full
example generating the sample data. For example, in most cases "Adam" will be
stored as a factor in the dataframe, which make things a bit more tricky. I used
I() to avoid this in the example below

d = data.frame(name=I(c("Auer","Caesar")),
               vname=I(c("Dieter","Karl")),age=c(10,30))
for (i in 1:nrow(d)) {
  d0 = d[i,]
  cat("\\MyNiceCommand{",d0$name,"}{",d0$vname,"}{",d0$age,"}\n",sep="")
}

You may nest this with sink, or better use the file argument in cat, don't
forget append.
Or directly use Rweave, which should not be too difficult for a LaTeXer, and
does not require the sink stuff.

Dieter


From j.van_den_hoff at fz-rossendorf.de  Fri Jul 14 11:46:22 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Fri, 14 Jul 2006 11:46:22 +0200
Subject: [R] colors on graph
In-Reply-To: <44B8249F.7090603@bitwrit.com.au>
References: <15C100200A5F4E45AF8CFB45A926EF3415E8FA@allexch01.alliance.com>
	<44B8249F.7090603@bitwrit.com.au>
Message-ID: <44B767EE.1020907@fz-rossendorf.de>

Jim Lemon wrote:
> COMTE Guillaume wrote:
>> Hy all,
>>
>>  
>>
>> I need to draw something in 2 dimension that has 3 dimension, the choice
>> has been made to use colors to display the third dimension into the
>> graph.
>>
>>  
>>
>> Has someone done something like that, i can't figure out how to
>> parametize the colors.
>>
> Hi Guilllaume,
> 
> Have a look at color.scale in the plotrix package.
> 
> Jim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


?image
?filled.contour

and the "See Also"s therein


From xux3 at nhlbi.nih.gov  Wed Jul 12 00:12:32 2006
From: xux3 at nhlbi.nih.gov (Xu, Xiuli (NIH/NHLBI) [E])
Date: Tue, 11 Jul 2006 18:12:32 -0400
Subject: [R] least square fit with non-negativity constraints for absorption
	spectra fitting
Message-ID: <B0F504209244B14EA9A4C1DFB599B9225AA8D6@NIHCESMLBX6.nih.gov>

I would really appreciate it if someone can give suggestions on how to
do spectra fitting in R using ordinary least square fitting and
non-negativity constraints. The lm() function works well for ordinary
least square fitting, but how to specify non-negativity constraints? It
wouldn't make sense if the fitting coefficients coming out as negative
in absorption spectra deconvolution.

Thanks. 

Xiuli


From seanpor at acm.org  Fri Jul 14 11:57:43 2006
From: seanpor at acm.org (Sean O'Riordain)
Date: Fri, 14 Jul 2006 10:57:43 +0100
Subject: [R] References verifying accuracy of R for basic
	statisticalcalculations and tests
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E6013DB51B@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DB51B@HERMES.demogr.mpg.de>
Message-ID: <8ed68eed0607140257u15a64ee4odb132bda892e3f60@mail.gmail.com>

Please don't shoot!

q: would it be a good idea to use these datasets as a basis for some
regression tests?

Sean



On 14/07/06, Rau, Roland <Rau at demogr.mpg.de> wrote:
> Hi,
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Corey Powell
> >
> > Do you know of any references that verify the accuracy of R
> > for basic statistical calculations and tests.  The results of
> > these studies should indicate that R results are the same as
> > the results of other statistical packages to a certain number
> > of decimal places on some benchmark calculations.
>
> I don't know of any references, but maybe you can somehow "verify the
> accuracy of R" by running some analysis with the "NIST Statistical
> Reference Datasets"; the URL is http://www.itl.nist.gov/div898/strd/
> So maybe you can run the analyses mentioned there and say that R
> (hopefully) returned the correct results.
>
> Hope this helps,
> Roland
>
> ----------
> This mail has been sent through the MPI for Demographic Rese...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From Jesus.Frias at dit.ie  Fri Jul 14 11:56:12 2006
From: Jesus.Frias at dit.ie (Jesus Frias)
Date: Fri, 14 Jul 2006 10:56:12 +0100 (IST)
Subject: [R] References verifying accuracy of R for basic
 statisticalcalculations and tests
In-Reply-To: <8ed68eed0607140257u15a64ee4odb132bda892e3f60@mail.gmail.com>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DB51B@HERMES.demogr.mpg.de>
	<8ed68eed0607140257u15a64ee4odb132bda892e3f60@mail.gmail.com>
Message-ID: <27852.194.125.105.28.1152870972.squirrel@squirrelmail.dit.ie>

Hi all,

There is a NISTnls package with the nonlinear regression examples in CRAN.
This work might have been done already.

regards,

Jesus


On Fri, July 14, 2006 10:57 am, Sean O'Riordain said:
> Please don't shoot!
>
> q: would it be a good idea to use these datasets as a basis for some
> regression tests?
>
> Sean
>
>
>
> On 14/07/06, Rau, Roland <Rau at demogr.mpg.de> wrote:
>> Hi,
>> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Corey Powell
>> >
>> > Do you know of any references that verify the accuracy of R
>> > for basic statistical calculations and tests.  The results of
>> > these studies should indicate that R results are the same as
>> > the results of other statistical packages to a certain number
>> > of decimal places on some benchmark calculations.
>>
>> I don't know of any references, but maybe you can somehow "verify the
>> accuracy of R" by running some analysis with the "NIST Statistical
>> Reference Datasets"; the URL is http://www.itl.nist.gov/div898/strd/
>> So maybe you can run the analyses mentioned there and say that R
>> (hopefully) returned the correct results.
>>
>> Hope this helps,
>> Roland
>>
>> ----------
>> This mail has been sent through the MPI for Demographic
>> Rese...{{dropped}}
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>
> This message has been scanned for content and viruses by the
> DIT Information Services MailScanner Service
> and is believed to be clean.
> http://www.dit.ie
>
>
>


School of Food Science and Environmental Health
Dublin Institute of Technology
t + 1 402 4459
f + 1 402 4495
w www.dit.ie/DIT/tourismfood/science/



This message has been scanned for content and viruses by the
DIT Information Services MailScanner Service
and is believed to be clean.
http://www.dit.ie


From patrick at pdrechsler.de  Fri Jul 14 12:11:56 2006
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Fri, 14 Jul 2006 11:11:56 +0100
Subject: [R] test regression against given slope for reduced major axis
	regression (RMA)
References: <87lkr1osjm.fsf@pdrechsler.de>
Message-ID: <87odvs7axf.fsf@pdrechsler.de>


Patrick Drechsler wrote on 11 Jul 2006 02:10:21 MET:

[...]
> I am now confronted with the problem that I have data which
> requires a modelII regression (also called reduced major axes
> regression (RMA) or geometric mean regression). For this I use
> the function "modelII" (see below).
>
> What would be a good way of adapting
> "test_regression_against_slope" for use with RMA regression?
>
> The question I am trying to answer is: "Does the slope acquired
> from experimental data differ significantly from theoretical
> predictions?"

JFTR: David Warton's "smatr" package solves the problem.
-- 
"You know the world is going crazy when the best rapper is a white
guy, the best golfer is a black guy, the Swiss hold the America's Cup,
France is accusing the US of arrogance, and Germany doesn't want to go
to war."          -- Charles Barkley


From ripley at stats.ox.ac.uk  Fri Jul 14 13:36:24 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 14 Jul 2006 12:36:24 +0100 (BST)
Subject: [R] References verifying accuracy of R for basic
 statisticalcalculations and tests
In-Reply-To: <8ed68eed0607140257u15a64ee4odb132bda892e3f60@mail.gmail.com>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DB51B@HERMES.demogr.mpg.de>
	<8ed68eed0607140257u15a64ee4odb132bda892e3f60@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0607141230270.1273@gannet.stats.ox.ac.uk>

On Fri, 14 Jul 2006, Sean O'Riordain wrote:

> Please don't shoot!
> 
> q: would it be a good idea to use these datasets as a basis for some
> regression tests?

See package NISTnls

> On 14/07/06, Rau, Roland <Rau at demogr.mpg.de> wrote:
> > Hi,
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Corey Powell
> > >
> > > Do you know of any references that verify the accuracy of R
> > > for basic statistical calculations and tests.  The results of
> > > these studies should indicate that R results are the same as
> > > the results of other statistical packages to a certain number
> > > of decimal places on some benchmark calculations.
> >
> > I don't know of any references, but maybe you can somehow "verify the
> > accuracy of R" by running some analysis with the "NIST Statistical
> > Reference Datasets"; the URL is http://www.itl.nist.gov/div898/strd/
> > So maybe you can run the analyses mentioned there and say that R
> > (hopefully) returned the correct results.

`Correct' as in `as obtained by NIST'?  It is a considerable assumption 
that the reference results are 'correct' or 'accurate'.

I learnt from my work with analytical chemists that the outlying result 
could be the only reasonably accurate one: all the other analysts had made 
the same error.

> > Hope this helps,
> > Roland

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From neuro3000 at hotmail.com  Fri Jul 14 14:14:58 2006
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Fri, 14 Jul 2006 08:14:58 -0400
Subject: [R] Generating random normal distribution with mean 0 and standard
	deviation 1
Message-ID: <BAY112-F31EC3AFC69B71C4E1F51F2AF6F0@phx.gbl>

Hello,

This must be really simple, but I can't find it on R Site search.  I need to 
generate a random normally distributed series with mean 0 and sd 1. In 
Matlab, this code is randn(n).

The closest I found is runif(20,-1,1) but this forces a maximum and a 
minimum, and there's no way to specify a standard deviation of 1.

>sd(runif(20,-1,1))
[1] 0.578164


From seanpor at acm.org  Fri Jul 14 14:19:32 2006
From: seanpor at acm.org (Sean O'Riordain)
Date: Fri, 14 Jul 2006 13:19:32 +0100
Subject: [R] Generating random normal distribution with mean 0 and
	standard deviation 1
In-Reply-To: <BAY112-F31EC3AFC69B71C4E1F51F2AF6F0@phx.gbl>
References: <BAY112-F31EC3AFC69B71C4E1F51F2AF6F0@phx.gbl>
Message-ID: <8ed68eed0607140519r2ded8561m8173862b7f8147a9@mail.gmail.com>

?rnorm

On 14/07/06, Neuro LeSuperH?ros <neuro3000 at hotmail.com> wrote:
> Hello,
>
> This must be really simple, but I can't find it on R Site search.  I need to
> generate a random normally distributed series with mean 0 and sd 1. In
> Matlab, this code is randn(n).
>
> The closest I found is runif(20,-1,1) but this forces a maximum and a
> minimum, and there's no way to specify a standard deviation of 1.
>
> >sd(runif(20,-1,1))
> [1] 0.578164
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From michael.watson at bbsrc.ac.uk  Fri Jul 14 14:19:50 2006
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 14 Jul 2006 13:19:50 +0100
Subject: [R] Generating random normal distribution with mean 0 and
	standarddeviation 1
In-Reply-To: <BAY112-F31EC3AFC69B71C4E1F51F2AF6F0@phx.gbl>
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950374D014@iahce2ksrv1.iah.bbsrc.ac.uk>

See rnorm() 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Neuro LeSuperH?ros
Sent: 14 July 2006 13:15
To: r-help at stat.math.ethz.ch
Subject: [R] Generating random normal distribution with mean 0 and standarddeviation 1

Hello,

This must be really simple, but I can't find it on R Site search.  I need to 
generate a random normally distributed series with mean 0 and sd 1. In 
Matlab, this code is randn(n).

The closest I found is runif(20,-1,1) but this forces a maximum and a 
minimum, and there's no way to specify a standard deviation of 1.

>sd(runif(20,-1,1))
[1] 0.578164

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From MSchwartz at mn.rr.com  Fri Jul 14 14:26:42 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 14 Jul 2006 07:26:42 -0500
Subject: [R] Generating random normal distribution with mean 0
	and	standard deviation 1
In-Reply-To: <BAY112-F31EC3AFC69B71C4E1F51F2AF6F0@phx.gbl>
References: <BAY112-F31EC3AFC69B71C4E1F51F2AF6F0@phx.gbl>
Message-ID: <1152880002.18787.3.camel@localhost.localdomain>

On Fri, 2006-07-14 at 08:14 -0400, Neuro LeSuperH?ros wrote:
> Hello,
> 
> This must be really simple, but I can't find it on R Site search.  I need to 
> generate a random normally distributed series with mean 0 and sd 1. In 
> Matlab, this code is randn(n).
> 
> The closest I found is runif(20,-1,1) but this forces a maximum and a 
> minimum, and there's no way to specify a standard deviation of 1.
> 
> >sd(runif(20,-1,1))
> [1] 0.578164

help.search("normal") leads you to "Normal(stats)".

?Normal leads you to rnorm().

More generally, help.search("distributions") will give you an overview
of the various distributions available in your installation.

HTH,

Marc Schwartz


From dieter.menne at menne-biomed.de  Fri Jul 14 14:43:19 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 14 Jul 2006 12:43:19 +0000 (UTC)
Subject: [R]
	=?utf-8?q?least_square_fit_with_non-negativity_constraints_fo?=
	=?utf-8?q?r_absorption=09spectra_fitting?=
References: <B0F504209244B14EA9A4C1DFB599B9225AA8D6@NIHCESMLBX6.nih.gov>
Message-ID: <loom.20060714T143834-651@post.gmane.org>

Xu, Xiuli (NIH/NHLBI) [E] <xux3 <at> nhlbi.nih.gov> writes:

> 
> I would really appreciate it if someone can give suggestions on how to
> do spectra fitting in R using ordinary least square fitting and
> non-negativity constraints. The lm() function works well for ordinary
> least square fitting, but how to specify non-negativity constraints? It
> wouldn't make sense if the fitting coefficients coming out as negative
> in absorption spectra deconvolution.

I don't see much reasons to use lm() to fit a spectrum, but if you want to do
that,  negative coefficients simply mean a shift by pi, sothe sign can be
discarded for most practical purposed. 

But better look at one of the specialized packages like spectrino.

Dieter


From j.van_den_hoff at fz-rossendorf.de  Fri Jul 14 14:46:02 2006
From: j.van_den_hoff at fz-rossendorf.de (Joerg van den Hoff)
Date: Fri, 14 Jul 2006 14:46:02 +0200
Subject: [R] least square fit with non-negativity constraints for
 absorption spectra fitting
In-Reply-To: <B0F504209244B14EA9A4C1DFB599B9225AA8D6@NIHCESMLBX6.nih.gov>
References: <B0F504209244B14EA9A4C1DFB599B9225AA8D6@NIHCESMLBX6.nih.gov>
Message-ID: <44B7920A.3050404@fz-rossendorf.de>

Xu, Xiuli (NIH/NHLBI) [E] wrote:
> I would really appreciate it if someone can give suggestions on how to
> do spectra fitting in R using ordinary least square fitting and
> non-negativity constraints. The lm() function works well for ordinary
> least square fitting, but how to specify non-negativity constraints? It
> wouldn't make sense if the fitting coefficients coming out as negative
> in absorption spectra deconvolution.
> 
> Thanks. 
> 
> Xiuli
> 

I'm not sure, but would presume that constraints could not be imposed on 
a linear least squares fit. maybe someone can correct me.

if you move to `nls', i.e. non-linear least squares fitting, you should 
be able to transform your model function. say, you want some parameter 
`a' to stay positive. then you could e.g. substitute

`a = exp(b)' in the model function and fit `b' without constraints in 
the "new" model and calculate `a' afterwards (which obviously is 
guaranteed now to be positive). note that error estimates would than 
have to be computed by gaussian error propagation from `b' to `a'.


joerg


From dassybr at gmail.com  Fri Jul 14 15:12:52 2006
From: dassybr at gmail.com (Hadassa Brunschwig)
Date: Fri, 14 Jul 2006 15:12:52 +0200
Subject: [R] Negative Binomial: Simulation
Message-ID: <83cfc0bd0607140612uaf02a98rae2ae6e69dc28eaa@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060714/34981572/attachment.pl 

From dassybr at gmail.com  Fri Jul 14 15:15:40 2006
From: dassybr at gmail.com (Hadassa Brunschwig)
Date: Fri, 14 Jul 2006 15:15:40 +0200
Subject: [R] Negative Binomial: Simulation
Message-ID: <83cfc0bd0607140615p1b68b071g243ad6cf407d6609@mail.gmail.com>

Hi R-Users!

(sorry about the last email)
I fitted a negative binomial distribution to my count data using the
function glm.nb() and obtained the calculated parameters
theta (dispersion) and mu.

I would like to simulate values from this negative binomial distribution.
Looking at the function rnbinom() I was looking at the relationship
between the two possible parametrizations of the negative binomial and found
that for this fuction I must use:

prob = theta/(theta+mu)     and       size = theta

Theta, however, is not an integer. So how can size (which is the number of
successes) equal theta?

I know there is a function rnegbin which does what I want. I would still
like to raise the above question (probably a more statistical one
than R).

Thanks a lot for any comments.

Hadassa


From kate at few.vu.nl  Fri Jul 14 15:16:24 2006
From: kate at few.vu.nl (Katharine Mullen)
Date: Fri, 14 Jul 2006 15:16:24 +0200 (CEST)
Subject: [R] Write a summary or a longer text to a graphical device
Message-ID: <Pine.GSO.4.56.0607141455530.14274@laurel.few.vu.nl>

one option is to display your information in a tcltk window - load tcltk
and say demo(tkfaq) for an example of loading a txt file into a window
with a scrollbar (to apply this example directly you would first write
what you wanted to file)

>From: "Torsten Mathies" <torsten.mathies at matec-gmbh.com>
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] Write a summary or a longer text to a graphical device
>Date: Thu, 13 Jul 2006 22:39:37 +0200
>
>How can I write a text, such as a result of a function or an explanation
>to a graphic device?
>
>When I try plot, I'm unable to reduce the axes.
>
>Greetings
>
>torsten
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html

----
Katharine Mullen
Department of Physics and Astronomy
Faculty of Sciences
Vrije Universiteit Amsterdam
de Boelelaan 1081
1081 HV Amsterdam
The Netherlands
room: T.1.06
tel: +31 205987870
fax: +31 205987992
e-mail: kate at nat.vu.nl
http://www.nat.vu.nl/~kate/


From ricardosilva at serasa.com.br  Fri Jul 14 15:17:19 2006
From: ricardosilva at serasa.com.br (ricardosilva at serasa.com.br)
Date: Fri, 14 Jul 2006 10:17:19 -0300
Subject: [R]  MLE and QR classes
Message-ID: <OFFD8E7B7D.92ED7520-ON032571AB.0048F32F-032571AB.0048FEDD@serasa.com.br>

Hi,

I load my data set and separate it as folowing:

presu <- read.table("C:/_Ricardo/Paty/qtdata_f.txt", header=TRUE, sep="\t",
na.strings="NA", dec=".", strip.white=TRUE)
dep<-presu[,3];
exo<-presu[,4:92];

Now, I want to use it using the wls and quantreg packages. How I change the
data classes for mle and rq objects?

Thanks a lot,
________________________________________
Ricardo Gon?alves Silva


From dassybr at gmail.com  Fri Jul 14 15:18:34 2006
From: dassybr at gmail.com (Hadassa Brunschwig)
Date: Fri, 14 Jul 2006 15:18:34 +0200
Subject: [R] Negative Binomial: Simulation
In-Reply-To: <83cfc0bd0607140615p1b68b071g243ad6cf407d6609@mail.gmail.com>
References: <83cfc0bd0607140615p1b68b071g243ad6cf407d6609@mail.gmail.com>
Message-ID: <83cfc0bd0607140618u53a21e59pa84a52f3c36ccb08@mail.gmail.com>

Addition to last email:

The problem basically arises also when I want to calculate (plot) the
cumulative distribution of the negative binomial with the calculated
parameters mu and theta!

On 7/14/06, Hadassa Brunschwig <dassybr at gmail.com> wrote:
> Hi R-Users!
>
> (sorry about the last email)
> I fitted a negative binomial distribution to my count data using the
> function glm.nb() and obtained the calculated parameters
> theta (dispersion) and mu.
>
> I would like to simulate values from this negative binomial distribution.
> Looking at the function rnbinom() I was looking at the relationship
> between the two possible parametrizations of the negative binomial and found
> that for this fuction I must use:
>
> prob = theta/(theta+mu)     and       size = theta
>
> Theta, however, is not an integer. So how can size (which is the number of
> successes) equal theta?
>
> I know there is a function rnegbin which does what I want. I would still
> like to raise the above question (probably a more statistical one
> than R).
>
> Thanks a lot for any comments.
>
> Hadassa
>


From Rau at demogr.mpg.de  Fri Jul 14 16:03:17 2006
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Fri, 14 Jul 2006 16:03:17 +0200
Subject: [R] References verifying accuracy of R for basic
 statisticalcalculations and tests
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E6013DB521@HERMES.demogr.mpg.de>


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
> Brian Ripley
> 
> `Correct' as in `as obtained by NIST'?  It is a considerable 
> assumption 
> that the reference results are 'correct' or 'accurate'.
> 
> I learnt from my work with analytical chemists that the 
> outlying result 
> could be the only reasonably accurate one: all the other 
> analysts had made 
> the same error.
> 

Sorry for being a bit off-topic, but what would you recommend then to
convince people that the results of an analysis are correct?
Simulating data with the characteristics you want to "capture" in the
data, analyze those artifical data first and if everything goes well
proceed with your "real" data?

Thanks,
Roland

----------
This mail has been sent through the MPI for Demographic Rese...{{dropped}}


From wade.wall at gmail.com  Fri Jul 14 16:10:11 2006
From: wade.wall at gmail.com (Wade Wall)
Date: Fri, 14 Jul 2006 10:10:11 -0400
Subject: [R] Cluster Analysis with flexible beta linkage method
Message-ID: <e23082be0607140710o626da2c8p9e53af3cca26c53e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060714/668f5a32/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Jul 14 16:18:10 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 14 Jul 2006 15:18:10 +0100 (BST)
Subject: [R] least square fit with non-negativity constraints for
 absorption spectra fitting
In-Reply-To: <44B7920A.3050404@fz-rossendorf.de>
References: <B0F504209244B14EA9A4C1DFB599B9225AA8D6@NIHCESMLBX6.nih.gov>
	<44B7920A.3050404@fz-rossendorf.de>
Message-ID: <Pine.LNX.4.64.0607141508100.16204@gannet.stats.ox.ac.uk>

On Fri, 14 Jul 2006, Joerg van den Hoff wrote:

> Xu, Xiuli (NIH/NHLBI) [E] wrote:
> > I would really appreciate it if someone can give suggestions on how to
> > do spectra fitting in R using ordinary least square fitting and
> > non-negativity constraints. The lm() function works well for ordinary
> > least square fitting, but how to specify non-negativity constraints? It
> > wouldn't make sense if the fitting coefficients coming out as negative
> > in absorption spectra deconvolution.
> > 
> > Thanks. 
> > 
> > Xiuli
> > 
> 
> I'm not sure, but would presume that constraints could not be imposed on 
> a linear least squares fit. maybe someone can correct me.

They can, and you get a simple quadratic programming problem.  So quadprog 
could be used to solve this one, but optim(methods="L-BFGS-B") may be as 
easy (and is pretty efficient on this class of QP problems).

S-PLUS has a function nnls.fit() for 'non-negative least squares'.

> if you move to `nls', i.e. non-linear least squares fitting, you should 
> be able to transform your model function. say, you want some parameter 
> `a' to stay positive. then you could e.g. substitute
> 
> `a = exp(b)' in the model function and fit `b' without constraints in 
> the "new" model and calculate `a' afterwards (which obviously is 
> guaranteed now to be positive). note that error estimates would than 
> have to be computed by gaussian error propagation from `b' to `a'.

The problem here is that a = 0 is a possible (and indeed plausible) value.

See MASS4 p.227 for nnls.fit and alternatives for use in R.  The MASS3 
ch08 script had an example of a regression with non-negative slope:

data(whiteside)
attach(whiteside)
Gas <- Gas[Insul=="Before"]
Temp <- -Temp[Insul=="Before"]
#nnls.fit(cbind(1, -1, Temp), Gas)
# can use box-constrained optimizer
fn <- function(par) sum((Gas - par[1] - par[2]*Temp)^2)
optim(rep(0,2), fn, lower=c(-Inf,0), method="L-BFGS-B")$par
rm(Gas, Temp)
detach()


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Jul 14 16:21:19 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 14 Jul 2006 15:21:19 +0100 (BST)
Subject: [R] Negative Binomial: Simulation
In-Reply-To: <83cfc0bd0607140618u53a21e59pa84a52f3c36ccb08@mail.gmail.com>
References: <83cfc0bd0607140615p1b68b071g243ad6cf407d6609@mail.gmail.com>
	<83cfc0bd0607140618u53a21e59pa84a52f3c36ccb08@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0607141519020.16204@gannet.stats.ox.ac.uk>

On Fri, 14 Jul 2006, Hadassa Brunschwig wrote:

> Addition to last email:
> 
> The problem basically arises also when I want to calculate (plot) the
> cumulative distribution of the negative binomial with the calculated
> parameters mu and theta!
> 
> On 7/14/06, Hadassa Brunschwig <dassybr at gmail.com> wrote:
> > Hi R-Users!
> >
> > (sorry about the last email)
> > I fitted a negative binomial distribution to my count data using the
> > function glm.nb() and obtained the calculated parameters
> > theta (dispersion) and mu.
> >
> > I would like to simulate values from this negative binomial distribution.
> > Looking at the function rnbinom() I was looking at the relationship
> > between the two possible parametrizations of the negative binomial and found
> > that for this fuction I must use:
> >
> > prob = theta/(theta+mu)     and       size = theta
> >
> > Theta, however, is not an integer. So how can size (which is the number of
> > successes) equal theta?

>From ?rnegbin

    size: target for number of successful trials, or dispersion
          parameter (the shape parameter of the gamma mixing
          distribution). Must be strictly positive.

Notice the 'or', and please see also the Details.

> > I know there is a function rnegbin which does what I want. I would still
> > like to raise the above question (probably a more statistical one
> > than R).
> >
> > Thanks a lot for any comments.
> >
> > Hadassa

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

It does seem to be a question of following the posting guide.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From lu.yuefeng at gmail.com  Fri Jul 14 16:34:07 2006
From: lu.yuefeng at gmail.com (Lu Yuefeng)
Date: Fri, 14 Jul 2006 10:34:07 -0400
Subject: [R] least square fit with non-negativity constraints for
	absorption spectra fitting
In-Reply-To: <B0F504209244B14EA9A4C1DFB599B9225AA8D6@NIHCESMLBX6.nih.gov>
References: <B0F504209244B14EA9A4C1DFB599B9225AA8D6@NIHCESMLBX6.nih.gov>
Message-ID: <bc808e8d0607140734t4bb1e2dfx1c6a9194adc508db@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060714/e4cb34a3/attachment.pl 

From spencer.graves at pdf.com  Fri Jul 14 16:43:53 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 14 Jul 2006 07:43:53 -0700
Subject: [R] non positive-definite G matrix in mixed models: bootstrap?
In-Reply-To: <000301c6a4ee$53cf63f0$6400a8c0@brungio>
References: <000301c6a4ee$53cf63f0$6400a8c0@brungio>
Message-ID: <44B7ADA9.9020700@pdf.com>

	  Have you considered 'simulate.lme'?  I believe that this is what 
Bates included in the 'nlme' package for obtaining confidence intervals, 
joint confidence regions, etc., when there were questions about the 
results for whatever reason.  I have not tried it with a singular model, 
but I believe Bates has.  In particular, have you reviewed ch. 2 in 
Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus (Springer)?

	  I am not a fan of bootstrapping.  In mixed-effects applications, 
bootsrapping needs to incorporate the constraints imposed by the 
mixed-effects model.  For example, if you have several samples per batch 
and several batches per lot, you need to bootstrap lots as well as 
batches within lot and samples within batch.

	  The advantage of bootstrapping is that if the normality assumptions 
behind the mixed model do not hold, the bootstrap results will still 
have some validity.  However, the range of extrapolability of bootstrap 
results is limited to other situations whose distribution is plausibly 
like the particular sample you bootstrapped, and I wouldn't know how to 
evaluate that.  By contrast, I know how to extrapolate simulation 
results.  To decide whether such results apply, I make normal 
probability plots of the data, random effects, and residuals.  If they 
all seem normal, I feel it is reasonable to use the simulation results.

	  Others may offer a different perspective (or a correction, as the 
case may be).  However, you asked for comments about bootstrapping mixed 
models.  At least you've got one.

	  Hope this helps.
	  Spencer Graves

Bruno L. Giordano wrote:
> Dear list,
> In a mixed model I selected I find a non positive definite random effects 
> variance-covariance matrix G, where some parameters are estimated close to 
> zero, and related confidence intervals are incredibly large.
> 
> Since simplification of the random portion is not an option, for both 
> interest in the parameters and significant increase in the model fit, I 
> would like to collect "unbiased" random effects estimates.
> 
> I used bootstrap to this purpose, creating a linear model for each cluster 
> and bootstraping the variance of the coefficients. Is this procedure 
> reasonable? Would it be reasonable in this case to keep the marginal portion 
> of the mixed model?
> Note that in presence of positive-definite G matrix this bootstrap approach 
> and the mixed effect model give highly similar estimates and that in the non 
> positive-definite model the bootstrap and mixed model marginal-model 
> estimates are highly similar as well.
> 
> Thank you
>     Bruno
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From daniele at medri.org  Fri Jul 14 16:53:54 2006
From: daniele at medri.org (Daniele Medri)
Date: Fri, 14 Jul 2006 16:53:54 +0200
Subject: [R] party - ctree() - terminal nodes reference for every obs
Message-ID: <1152888834.5518.6.camel@localhost.localdomain>

Dear R.Users,

using ctree() (from "party" library) on a data.frame, I want to append a
column with the references for the groups/segments detected. While these
nodes are easy readable in output, I need a vector for my obs.

Hints?


Cheers
-- 
Daniele Medri


From georg.otto at tuebingen.mpg.de  Fri Jul 14 16:57:05 2006
From: georg.otto at tuebingen.mpg.de (Georg Otto)
Date: Fri, 14 Jul 2006 16:57:05 +0200
Subject: [R] Generate object names from variables
Message-ID: <m18xmwfd4u.fsf@tuebingen.mpg.de>

Hi,

I want to generate object names out of variables in a sort of variable
substitution.

first i generate some vectors and an empty list:

> vector.a<-c("a","b")
> vector.b<-c("c","d")
> vector.c<-c("e","f")
> vectors<-c("vector.a", "vector.b", "vector.c")
> vectors
[1] "vector.a" "vector.b" "vector.c"

> vectorlist<-list()

What I would then like to do is to generate elements of the list by
using variables, somehow like this (does not work):

>for (i in vectors) {
+ list$i<-i
+ }

To end up with a list like this:

> list
$vector.a
 [1] "a" "b"
$vector.b
 [1] "c" "d"
$vector.c
 [1] "e" "f"


Any hint will be appreciated.

Cheers,

Georg


From ggrothendieck at gmail.com  Fri Jul 14 17:06:22 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 14 Jul 2006 11:06:22 -0400
Subject: [R] Generate object names from variables
In-Reply-To: <m18xmwfd4u.fsf@tuebingen.mpg.de>
References: <m18xmwfd4u.fsf@tuebingen.mpg.de>
Message-ID: <971536df0607140806rb9dbfc3nb8c0425787c361eb@mail.gmail.com>

Try this:

sapply(vectors, get, simplify = FALSE)

or

lapply(vectors, get)

although the last one does not give you the names.

On 7/14/06, Georg Otto <georg.otto at tuebingen.mpg.de> wrote:
> Hi,
>
> I want to generate object names out of variables in a sort of variable
> substitution.
>
> first i generate some vectors and an empty list:
>
> > vector.a<-c("a","b")
> > vector.b<-c("c","d")
> > vector.c<-c("e","f")
> > vectors<-c("vector.a", "vector.b", "vector.c")
> > vectors
> [1] "vector.a" "vector.b" "vector.c"
>
> > vectorlist<-list()
>
> What I would then like to do is to generate elements of the list by
> using variables, somehow like this (does not work):
>
> >for (i in vectors) {
> + list$i<-i
> + }
>
> To end up with a list like this:
>
> > list
> $vector.a
>  [1] "a" "b"
> $vector.b
>  [1] "c" "d"
> $vector.c
>  [1] "e" "f"
>
>
> Any hint will be appreciated.
>
> Cheers,
>
> Georg
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From mschwartz at mn.rr.com  Fri Jul 14 17:16:22 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 14 Jul 2006 10:16:22 -0500
Subject: [R] Generate object names from variables
In-Reply-To: <m18xmwfd4u.fsf@tuebingen.mpg.de>
References: <m18xmwfd4u.fsf@tuebingen.mpg.de>
Message-ID: <1152890182.4245.8.camel@localhost.localdomain>

On Fri, 2006-07-14 at 16:57 +0200, Georg Otto wrote:
> Hi,
> 
> I want to generate object names out of variables in a sort of variable
> substitution.
> 
> first i generate some vectors and an empty list:
> 
> > vector.a<-c("a","b")
> > vector.b<-c("c","d")
> > vector.c<-c("e","f")
> > vectors<-c("vector.a", "vector.b", "vector.c")
> > vectors
> [1] "vector.a" "vector.b" "vector.c"
> 
> > vectorlist<-list()
> 
> What I would then like to do is to generate elements of the list by
> using variables, somehow like this (does not work):
> 
> >for (i in vectors) {
> + list$i<-i
> + }
> 
> To end up with a list like this:
> 
> > list
> $vector.a
>  [1] "a" "b"
> $vector.b
>  [1] "c" "d"
> $vector.c
>  [1] "e" "f"
> 
> 
> Any hint will be appreciated.
> 
> Cheers,
> 
> Georg

Presuming that your vectors fit a naming pattern of "vector.x":

# Use grep() to get the vector names from ls()
> vectors <- grep("vector[\.]", ls(), value = TRUE)

> vectors
[1] "vector.a" "vector.b" "vector.c"

# Use sapply to create the list
> sapply(vectors, get, simplify = FALSE)
$vector.a
[1] "a" "b"

$vector.b
[1] "c" "d"

$vector.c
[1] "e" "f"


HTH,

Marc Schwartz


From ivan.kalafatic at gmail.com  Fri Jul 14 17:26:23 2006
From: ivan.kalafatic at gmail.com (Ivan Kalafatic)
Date: Fri, 14 Jul 2006 16:26:23 +0100
Subject: [R] Help for updating package
Message-ID: <6dbf89a50607140826g4afe4996lf5e5e500af7412ec@mail.gmail.com>

I have a problem with garchFit fuction in fSeries package. I found the
following reply on one of the R list:
"GARCH-Modelling is not easy, and indeed for your dataset the default
"Sequential Quadratic Programming" solver doesn't converge. I observed
this also for some other time series. There is already an updated
version on the server,
https://svn.r-project.org/Rmetrics/trunk/fSeries/
which uses improved control parameter settings as default values. With
this version there exist no convergence problems"

How to update my version of fSeries with the provided link?

Thank you.


From pauljohn32 at gmail.com  Fri Jul 14 17:49:40 2006
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Fri, 14 Jul 2006 10:49:40 -0500
Subject: [R] Generate object names from variables
In-Reply-To: <1152890182.4245.8.camel@localhost.localdomain>
References: <m18xmwfd4u.fsf@tuebingen.mpg.de>
	<1152890182.4245.8.camel@localhost.localdomain>
Message-ID: <13e802630607140849x176df0d1m2da8f1dea83947e0@mail.gmail.com>

I collected some advice about this question a couple of  years ago.
This might help.

http://pj.freefaculty.org/R/Rtips.html#2.1 "Add variables to a data
frame (or list)"
and the next one after that.


On 7/14/06, Marc Schwartz (via MN) <mschwartz at mn.rr.com> wrote:
> On Fri, 2006-07-14 at 16:57 +0200, Georg Otto wrote:
> > Hi,
> >
> > I want to generate object names out of variables in a sort of variable
> > substitution.
> >
> > first i generate some vectors and an empty list:
> >
> > > vector.a<-c("a","b")
> > > vector.b<-c("c","d")
> > > vector.c<-c("e","f")
> > > vectors<-c("vector.a", "vector.b", "vector.c")
> > > vectors
> > [1] "vector.a" "vector.b" "vector.c"
> >
> > > vectorlist<-list()
> >
> > What I would then like to do is to generate elements of the list by
> > using variables, somehow like this (does not work):
> >
> > >for (i in vectors) {
> > + list$i<-i
> > + }
> >
> > To end up with a list like this:
> >
> > > list
> > $vector.a
> >  [1] "a" "b"
> > $vector.b
> >  [1] "c" "d"
> > $vector.c
> >  [1] "e" "f"
> >
> >
> > Any hint will be appreciated.
> >
> > Cheers,
> >
> > Georg
>

-- 
Paul E. Johnson
Professor, Political Science
1541 Lilac Lane, Room 504
University of Kansas


From HUL at stowers-institute.org  Fri Jul 14 18:18:33 2006
From: HUL at stowers-institute.org (Li, Hua)
Date: Fri, 14 Jul 2006 11:18:33 -0500
Subject: [R] Questions about extract-lme.cov
Message-ID: <CED81D34E37D5043A1211565277A51E505435C96@exchkc02.stowers-institute.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060714/3e35f033/attachment.pl 

From ricardosilva at serasa.com.br  Fri Jul 14 18:26:39 2006
From: ricardosilva at serasa.com.br (ricardosilva at serasa.com.br)
Date: Fri, 14 Jul 2006 13:26:39 -0300
Subject: [R] Quantile Regression Object
Message-ID: <OFA5DA2C5A.70272885-ON032571AB.005A4622-032571AB.005A54A1@serasa.com.br>

Hi,

I load my data set and separate it as folowing:

presu <- read.table("C:/_Ricardo/Paty/qtdata_f.txt", header=TRUE, sep="\t",
na.strings="NA", dec=".", strip.white=TRUE)
dep<-presu[,3];
exo<-presu[,4:92];

Now, I want to use it using the wls and quantreg packages. How I change the
data classes for mle and rq objects?

Thanks a lot,
Ricardo


From somel at eva.mpg.de  Fri Jul 14 18:35:25 2006
From: somel at eva.mpg.de (Mehmet Somel)
Date: Fri, 14 Jul 2006 18:35:25 +0200
Subject: [R] SAS to R translator for particular procedures
Message-ID: <44B7C7CD.5010406@eva.mpg.de>

Dear /Bill Paterson,

while trying to find a way to convert SAS code into R, I came across 
your one time e-mail message 
(http://www.ens.gu.edu.au/robertk/R/help/99b/0908.html). I'd appreciate 
to learn if anything came out of this, or any suggestions.

Thank you in advance,

Mehmet Somel
/

-- 
Mehmet Somel
Max Planck Institute for Evolutionary Anthropology
Department of Evolutionary Genetics
Deutscher Platz 6, D-04103 Leipzig

Tel: +49-(0)341-3550-530
Fax: +49-(0)341-3550-555
E-mail: somel at eva.mpg.de


From helenzhang234 at hotmail.com  Fri Jul 14 15:35:25 2006
From: helenzhang234 at hotmail.com (miao)
Date: Fri, 14 Jul 2006 14:35:25 +0100
Subject: [R] Listing all of combinations
Message-ID: <BAY110-DAV25B048ACDE89CDD38623BEA6F0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060714/a5460d70/attachment.pl 

From ricardosilva at serasa.com.br  Fri Jul 14 18:43:32 2006
From: ricardosilva at serasa.com.br (ricardosilva at serasa.com.br)
Date: Fri, 14 Jul 2006 13:43:32 -0300
Subject: [R] Error in Quantile Regression - Clear Message
Message-ID: <OF7891BF03.85B3C118-ON032571AB.005B45AF-032571AB.005BE038@serasa.com.br>

Dear Users,
I loaded my dataset as following:

presu <- read.table("C:/_Ricardo/Paty/qtdata_f.txt", header=TRUE, sep="\t",
na.strings="NA", dec=".", strip.white=TRUE)
dep<-presu[,3];
exo<-presu[,4:92];

When I try:

rq(dep ~ exo, ...) or mle.stepwise(dep ~ exo, ...)
I got the same error:
> rq(dep ~ exo)
Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  :
        invalid variable type for 'exo'

Any hint in how to fix it? I think this is due my data format.

Thanks,

________________________________________
Ricardo Gon?alves Silva, M. Sc.
Apoio aos Processos de Modelagem Matem?tica
Econometria & Inadimpl?ncia
Serasa S.A.
(11) - 6847-8889
ricardosilva at serasa.com.br

__________________


Hi,

I load my data set and separate it as folowing:

presu <- read.table("C:/_Ricardo/Paty/qtdata_f.txt", header=TRUE, sep="\t",
na.strings="NA", dec=".", strip.white=TRUE)
dep<-presu[,3];
exo<-presu[,4:92];

Now, I want to use it using the wls and quantreg packages. How I change the
data classes for mle and rq objects?

Thanks a lot,
Ricardo

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
 PLEASE do read the posting guide!
 http://www.R-project.org/posting-guide.html


From klaus.langohr at upc.edu  Fri Jul 14 16:44:44 2006
From: klaus.langohr at upc.edu (klaus.langohr at upc.edu)
Date: Fri, 14 Jul 2006 16:44:44 +0200
Subject: [R] Competings risks
Message-ID: <1152888284.44b7addc3e26e@nobel.upc.es>

Hello!
Is there any possibility in R to carry out competing risks analysis with left-
truncated data (late entries)?

Thanks a lot for any help!!

Klaus.


From CarnellR at BATTELLE.ORG  Wed Jul 12 17:05:20 2006
From: CarnellR at BATTELLE.ORG (Carnell, Rob C)
Date: Wed, 12 Jul 2006 11:05:20 -0400
Subject: [R] [R-pkgs] New R-Packages:  Triangle and LHS
Message-ID: <0F923E9EBBF1554ABA641ACA04BEBEC004ED4F22@WS-BCO-MSE7.milky-way.battelle.org>

Two new packages have been added to CRAN.  

The first is a simple package which provides the standard functions for
a triangle distribution (rtriangle, dtriangle, ptriangle, qtriangle).  

The second package generates and augments Latin Hypercube Samples.
Functions for generating random and optimized designs are provided.

I look forward hearing your feedback and improving the documentation!
 

Rob Carnell

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From goran.brostrom at gmail.com  Wed Jul 12 18:59:08 2006
From: goran.brostrom at gmail.com (=?UTF-8?Q?G=C3=B6ran_Brostr=C3=B6m?=)
Date: Wed, 12 Jul 2006 18:59:08 +0200
Subject: [R] [R-pkgs] glmmML updated
Message-ID: <148ed8180607120959j361f7893h3aeab3551fae490@mail.gmail.com>

I have uploaded a new version (0.30-2) of glmmML to CRAN today.

This is a rather extensive upgrade, mostly internal. Adaptive
Gauss-Hermite quadrature (GHQ) is now used for the evaluation of the
integrals in the log likelihood function. The user can choose the number
of points (default is 16), I _think_ that choosing 1 point will result
in a Laplace approximation. The integrals in the score and hessian
are evaluated by the QUADPACK function 'Rdqagi' which is the C code
behind the R function 'integrate'. This specific combination of the two
methods seems to work best. (I often get _exactly_ (up to seven digits)
the same value with the two methods, but in some extreme cases one may
fail and not the other.)

New components in the output from 'glmmML' are 'posterior.means' and
posterior.modes'. The modes are found by using 'vmmin' (behind R's
'optim') on the integrands in the GHQ, the means by numerical
integration. Usually, they do not differ much.

A special problem is situations where the random effects variance is
very small or zero. I may happen that glmmML is unable to get the
likelihood value above the value given by 'glm' on the corresponding
model with no clustering. In such a case zero variance is reported, with
a standard error that is NA. A warning is also given. If a test of the
hypothesis that sigma = 0 is on the wish list, a p-value can be
estimated by bootstrapping, see the input parameter 'boot'. The only
option now is a parametric bootstrap; I have removed the 'conditional'
approach.

As usual, comments, and error and bug reports are welcome.

G?ran

-- 
G?ran Brostr?m

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From mschwartz at mn.rr.com  Fri Jul 14 18:55:58 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 14 Jul 2006 11:55:58 -0500
Subject: [R] SAS to R translator for particular procedures
In-Reply-To: <44B7C7CD.5010406@eva.mpg.de>
References: <44B7C7CD.5010406@eva.mpg.de>
Message-ID: <1152896158.4245.31.camel@localhost.localdomain>

On Fri, 2006-07-14 at 18:35 +0200, Mehmet Somel wrote:
> Dear /Bill Paterson,
> 
> while trying to find a way to convert SAS code into R, I came across 
> your one time e-mail message 
> (http://www.ens.gu.edu.au/robertk/R/help/99b/0908.html). I'd appreciate 
> to learn if anything came out of this, or any suggestions.
> 
> Thank you in advance,
> 
> Mehmet Somel
> /

There was this post back in 2004:

https://stat.ethz.ch/pipermail/r-help/2004-April/048750.html

HTH,

Marc Schwartz

N.B. Pay attention to the date...  :-)


From John.Kerpel at infores.com  Fri Jul 14 19:07:59 2006
From: John.Kerpel at infores.com (Kerpel, John)
Date: Fri, 14 Jul 2006 12:07:59 -0500
Subject: [R] Help for updating package
Message-ID: <44A8B25381923D4F93B74B2676A50F6D02E0055C@MAIL1.infores.com>

Ivan:

I'm guessing you've got the latest version of fSeries if you downloaded
it recently from CRAN.

I've noticed the convergence problems too.  I changed to
algorithm="lbfgsb" and haven't had a problem since.  (I'm running
Windows XP and R 2.3.1)

John

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ivan Kalafatic
Sent: Friday, July 14, 2006 10:26 AM
To: r-help at stat.math.ethz.ch; r-sig-finance at stat.math.ethz.ch
Subject: [R] Help for updating package

I have a problem with garchFit fuction in fSeries package. I found the
following reply on one of the R list:
"GARCH-Modelling is not easy, and indeed for your dataset the default
"Sequential Quadratic Programming" solver doesn't converge. I observed
this also for some other time series. There is already an updated
version on the server,
https://svn.r-project.org/Rmetrics/trunk/fSeries/
which uses improved control parameter settings as default values. With
this version there exist no convergence problems"

How to update my version of fSeries with the provided link?

Thank you.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From msubianto at gmail.com  Fri Jul 14 19:09:25 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Fri, 14 Jul 2006 19:09:25 +0200
Subject: [R] Listing all of combinations
Message-ID: <c7c17cef0607141009n7ca88ae0o27be1b05ad1f91e5@mail.gmail.com>

On this day 14/07/2006 15:35, miao wrote:
> Dear All:
>
>     I want to list all of combination among (a,b,c,d,e,f,g,h). I try to use lp<-ist(A=c(a,b,c,d,e,f,g,h),B=(a,b,c,d,e,f,g,h)....etc.).and then use expand.grid(p). this does not work for the same vector A,B...
>
>     Anyone had this experince?
>
>
>    Thanks!
>
>    Xin
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Try this below:

A <- letters[1:8]
B <- letters[1:8]
lp<-list(A,B)
expand.grid(lp)

Best, Muhammad Subianto


From rhelp.zhao at gmail.com  Fri Jul 14 19:09:41 2006
From: rhelp.zhao at gmail.com (Iris Zhao)
Date: Fri, 14 Jul 2006 13:09:41 -0400
Subject: [R] Optim()
Message-ID: <d95bc7680607141009r34735112sb3b2d9db5e3efdec@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060714/31756ceb/attachment.pl 

From pinard at iro.umontreal.ca  Fri Jul 14 19:17:08 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Fri, 14 Jul 2006 13:17:08 -0400
Subject: [R] [Rd] R as shell script
In-Reply-To: <b96098070607140715hc7ca785xa5ca190348f24aa3@mail.gmail.com>
References: <b96098070607140715hc7ca785xa5ca190348f24aa3@mail.gmail.com>
Message-ID: <20060714171708.GA7949@phenix.progiciels-bpi.ca>

[Juha Vierinen]
>Hi,

Hello, Juha.  Your request, quoted below, is likely more appropriate for
R help than for R devel, so I'm redirecting this reply there.

>I am considering if I should invest in learning R. Based on the
>language definition and introductory documents, it seems nice. But now
>I am faced with a problem: I want to be able to run R programs easily
>from the unix shell, and write scripts that can automatically select R
>as the interpreter:

>#!/usr/bin/R
>cat("Hello world.\n")

>This of course doesn't work, because /usr/bin/R is a shell script.

>I have been able to create a binary wrapper that calls R with the
>correct arguments, which is documented here:

>http://kavaro.fi/mediawiki/index.php/Using_R_from_the_shell

>This still lacks eg. standard input (but I have no idea how I can
>implement it in R) and full command line argument passing (can be
>done), but am I on the right track, or is there already something that
>does what I need?

I'm often using something like:

   #!/bin/sh
   R --slave --vanilla <<EOF

   # Your R source code goes here!

   EOF

Within your script, shell substitution for $1, etc., will occur.  So 
with a bit of imagination, you can do about anything :-).  Simple 
enough!  Make sure you `cat' or `print' explicitly whatever has to be 
written on standard output: for one, I usually prefer full control in 
scripts over automatic printing of given expressions.

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From jesse.canchola.b at bayer.com  Fri Jul 14 19:49:25 2006
From: jesse.canchola.b at bayer.com (Jesse Albert Canchola)
Date: Fri, 14 Jul 2006 10:49:25 -0700
Subject: [R] looping using combinatorics
In-Reply-To: <971536df0607131939w7fe8fabcy7a255f981735fa31@mail.gmail.com>
Message-ID: <OF9477140C.69759691-ON882571AB.005B65DF-882571AB.0061E824@bayer.com>

Many thanks, Gabor.  This is very close to what would be ideal.  You gave 
me an idea as follows:

Rather than combine pairs of data vectors/frames AFTER the "combn" 
function, combine all data before (though I belive this would be less 
efficient) and add an index then use that index to choose your pairs (or 
whatever combinatorics you are using; e.g., 8 choose 4  so all 
combinations of 4 out of 8 for a total of 70 combinations.)

Example data frames with variable names:

Data frame "a" where I add an "index":
id measure index
1  1.1  1
2  1.2  1
3  1.3  1

Data frame "b" where I add an "index":
id measure index
4  2.1  2
5  2.2  2
6  2.3  2

Data frame "c" where I add an index:
id measure index
7  3.1  3
8  3.2  3
9  3.3  3

If we combine all these data at once using rbind, we get:
id measure index
1  1.1  1
2  1.2  1
3  1.3  1
4  2.1  2
5  2.2  2
6  2.3  2
7  3.1  3
8  3.2  3
9  3.3  3

We can then use something similar to your code and the index to choose the 
required pairs as derived from the "combn" function. 
For example, the "combn" function will choose the data pairs
(1,2)
(1,3)
(2,3)

where, for example, the pairs (1,2) will have data from frames "a" and 
"b":
1  1.1  1
2  1.2  1
3  1.3  1
4  2.1  2
5  2.2  2
6  2.3  2

so that we can go down the list subsetting what we need and doing 
operations on each combined pair as we go. 

Is there an easy way in R to do this operation?

For the above, an attempt might be:

########## STAB CODE #########
DF <- rbind(a,b,c)
DF
for(index in as.data.frame(combn(3,2))) print(DF[,index])
######## END STAB CODE ######

but this is choosing 3 choose 2 COLUMNS within the combined file rather 
than 3 choose 2 ROWS.


Best regards and TIAA,
Jesse 





"Gabor Grothendieck" <ggrothendieck at gmail.com> 
07/13/2006 07:39 PM

To
"Jesse Albert Canchola" <jesse.canchola.b at bayer.com>
cc
r-help at stat.math.ethz.ch
Subject
Re: [R] looping using combinatorics






I assume your question is given 3 vectors of the same length: a, b and c
how do we loop over pairs of them.  In the following each iteration 
displays
one pair:

   library(combinat)
   DF <- data.frame(a = 1:4, b = 5:8, c = 9:12)
   for(idx in as.data.frame(combn(3,2))) print(DF[,idx])

On 7/13/06, Jesse Albert Canchola <jesse.canchola.b at bayer.com> wrote:
> I have a problem where I need to loop over the total combinations of
> vectors (combined once chosen via combinatorics).  Here is a
> simplification of the problem:
>
> STEP 1:  Define three vectors a, b, c.
> STEP 2:  Combine all possible pairwise vectors (i.e., 3 choose 2 = 3
> possible pairs of vectors: ab,ac, bc)
> NOTE:  the actual problem has 8 choose 4, 8 choose 5 and 8 choose 6
> combinations.
> STEP 3:  Do the same math on each pairwise combination and spit out
> answers each time
>
> ####### BEGIN CODE #######
> #STEP 1
> a1 <- c(1,2,3,4,5,6,7,8,9,10,11,12)
> a <- matrix(a1,2,3,byrow=T)
> a
>
> b1 <- c(13,14,15,16,17,18,19,20,21,22,23,24)
> b <- matrix(b1,2,3,byrow=T)
> b
>
> c1 <- c(25,26,27,28,29,30,31,32,33,34,35,36)
> c <- matrix(b1,2,3,byrow=T)
> c
>
> # example:  combine the first two vectors "a" and "b"
> combab <- rbind(a,b)
>
> # the a,b combined data from the algorithm later below should look like
> # something like the following:
> combab
>
> # use the combinatorics "combn" function found in the "combinat" package
> on CRAN
> m <- combn(3,2) # three choose two combinations
> m
>
> # the first assignment below should be numeric and then subsequent
> # assignments as character since the first time you assign a number to
> # a character in a matrix the rest of the numbers in the matrix are
> coerced to character
> m[m==1]='a'; m[m=='2']='b'; m[m=='3']='c'
> m
>
> #STEP 2: combine pairwise vectors into a matrix or frame
> for (i in dim(m)[1])
>    for (j in dim(m)[2])
>        {
>            combined <-
> rbind(cat(format(m[i]),"\n"),cat(format(m[j]),"\n")) #cat/format removes
> the quotes
>            combined
>        }
> traceback()
>
>
> #STEP 3: {not there yet}
> ################# END CODE ################
>
> The problem is that in STEP 2 (not complete), the results in the rbind 
are
> not recognized as the objects they represent (i.e., the "a" without 
quotes
> is not recognized as the data object we defined in STEP 1.  Perhaps this
> is a parsing problem.  Perhaps there is an alterative way to do this.  I
> looked pretty long and hard in the CRAN libraries but alas, I am stuck.
> BTW, I picked up R about a month ago (I used primarily SAS, Stata and
> SPSS).
>
> Regards and TIA,
> Jesse
>
>
>
>
>
>
> Jesse A. Canchola
> Biostatistician III
> Bayer Healthcare
> 725 Potter St.
> Berkeley, CA 94710
> P: 510.705.5855
> F: 510.705.5718
> E: Jesse.Canchola.b at Bayer.Com
>
>
>
>
> 
_______________________________________________________________________________________________
>
> The information contained in this e-mail is for the exclusive use of the 
intended recipient(s) and may be confidential, proprietary, and/or legally 
privileged.  Inadvertent disclosure of this message does not constitute a 
waiver of any privilege.  If you receive this message in error, please do 
not directly or indirectly use, print, copy, forward, or disclose any part 
of this message.  Please also delete this e-mail and all copies and notify 
the sender.  Thank you.
>
> For alternate languages please go to http://bayerdisclaimer.bayerweb.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html
>


From sachinj.2006 at yahoo.com  Fri Jul 14 19:56:49 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Fri, 14 Jul 2006 10:56:49 -0700 (PDT)
Subject: [R] Recreate new dataframe based on condition
Message-ID: <20060714175649.95318.qmail@web37601.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060714/0e17f4d2/attachment.pl 

From ggrothendieck at gmail.com  Fri Jul 14 20:01:28 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 14 Jul 2006 14:01:28 -0400
Subject: [R] looping using combinatorics
In-Reply-To: <OF9477140C.69759691-ON882571AB.005B65DF-882571AB.0061E824@bayer.com>
References: <971536df0607131939w7fe8fabcy7a255f981735fa31@mail.gmail.com>
	<OF9477140C.69759691-ON882571AB.005B65DF-882571AB.0061E824@bayer.com>
Message-ID: <971536df0607141101tf3aed41qf436e85189775ea1@mail.gmail.com>

Use data.frame, not rbind, e.g. DF <- data.frame(a, b, c)

On 7/14/06, Jesse Albert Canchola <jesse.canchola.b at bayer.com> wrote:
> Many thanks, Gabor.  This is very close to what would be ideal.  You gave
> me an idea as follows:
>
> Rather than combine pairs of data vectors/frames AFTER the "combn"
> function, combine all data before (though I belive this would be less
> efficient) and add an index then use that index to choose your pairs (or
> whatever combinatorics you are using; e.g., 8 choose 4  so all
> combinations of 4 out of 8 for a total of 70 combinations.)
>
> Example data frames with variable names:
>
> Data frame "a" where I add an "index":
> id measure index
> 1  1.1  1
> 2  1.2  1
> 3  1.3  1
>
> Data frame "b" where I add an "index":
> id measure index
> 4  2.1  2
> 5  2.2  2
> 6  2.3  2
>
> Data frame "c" where I add an index:
> id measure index
> 7  3.1  3
> 8  3.2  3
> 9  3.3  3
>
> If we combine all these data at once using rbind, we get:
> id measure index
> 1  1.1  1
> 2  1.2  1
> 3  1.3  1
> 4  2.1  2
> 5  2.2  2
> 6  2.3  2
> 7  3.1  3
> 8  3.2  3
> 9  3.3  3
>
> We can then use something similar to your code and the index to choose the
> required pairs as derived from the "combn" function.
> For example, the "combn" function will choose the data pairs
> (1,2)
> (1,3)
> (2,3)
>
> where, for example, the pairs (1,2) will have data from frames "a" and
> "b":
> 1  1.1  1
> 2  1.2  1
> 3  1.3  1
> 4  2.1  2
> 5  2.2  2
> 6  2.3  2
>
> so that we can go down the list subsetting what we need and doing
> operations on each combined pair as we go.
>
> Is there an easy way in R to do this operation?
>
> For the above, an attempt might be:
>
> ########## STAB CODE #########
> DF <- rbind(a,b,c)
> DF
> for(index in as.data.frame(combn(3,2))) print(DF[,index])
> ######## END STAB CODE ######
>
> but this is choosing 3 choose 2 COLUMNS within the combined file rather
> than 3 choose 2 ROWS.
>
>
> Best regards and TIAA,
> Jesse
>
>
>
>
>
> "Gabor Grothendieck" <ggrothendieck at gmail.com>
> 07/13/2006 07:39 PM
>
> To
> "Jesse Albert Canchola" <jesse.canchola.b at bayer.com>
> cc
> r-help at stat.math.ethz.ch
> Subject
> Re: [R] looping using combinatorics
>
>
>
>
>
>
> I assume your question is given 3 vectors of the same length: a, b and c
> how do we loop over pairs of them.  In the following each iteration
> displays
> one pair:
>
>   library(combinat)
>   DF <- data.frame(a = 1:4, b = 5:8, c = 9:12)
>   for(idx in as.data.frame(combn(3,2))) print(DF[,idx])
>
> On 7/13/06, Jesse Albert Canchola <jesse.canchola.b at bayer.com> wrote:
> > I have a problem where I need to loop over the total combinations of
> > vectors (combined once chosen via combinatorics).  Here is a
> > simplification of the problem:
> >
> > STEP 1:  Define three vectors a, b, c.
> > STEP 2:  Combine all possible pairwise vectors (i.e., 3 choose 2 = 3
> > possible pairs of vectors: ab,ac, bc)
> > NOTE:  the actual problem has 8 choose 4, 8 choose 5 and 8 choose 6
> > combinations.
> > STEP 3:  Do the same math on each pairwise combination and spit out
> > answers each time
> >
> > ####### BEGIN CODE #######
> > #STEP 1
> > a1 <- c(1,2,3,4,5,6,7,8,9,10,11,12)
> > a <- matrix(a1,2,3,byrow=T)
> > a
> >
> > b1 <- c(13,14,15,16,17,18,19,20,21,22,23,24)
> > b <- matrix(b1,2,3,byrow=T)
> > b
> >
> > c1 <- c(25,26,27,28,29,30,31,32,33,34,35,36)
> > c <- matrix(b1,2,3,byrow=T)
> > c
> >
> > # example:  combine the first two vectors "a" and "b"
> > combab <- rbind(a,b)
> >
> > # the a,b combined data from the algorithm later below should look like
> > # something like the following:
> > combab
> >
> > # use the combinatorics "combn" function found in the "combinat" package
> > on CRAN
> > m <- combn(3,2) # three choose two combinations
> > m
> >
> > # the first assignment below should be numeric and then subsequent
> > # assignments as character since the first time you assign a number to
> > # a character in a matrix the rest of the numbers in the matrix are
> > coerced to character
> > m[m==1]='a'; m[m=='2']='b'; m[m=='3']='c'
> > m
> >
> > #STEP 2: combine pairwise vectors into a matrix or frame
> > for (i in dim(m)[1])
> >    for (j in dim(m)[2])
> >        {
> >            combined <-
> > rbind(cat(format(m[i]),"\n"),cat(format(m[j]),"\n")) #cat/format removes
> > the quotes
> >            combined
> >        }
> > traceback()
> >
> >
> > #STEP 3: {not there yet}
> > ################# END CODE ################
> >
> > The problem is that in STEP 2 (not complete), the results in the rbind
> are
> > not recognized as the objects they represent (i.e., the "a" without
> quotes
> > is not recognized as the data object we defined in STEP 1.  Perhaps this
> > is a parsing problem.  Perhaps there is an alterative way to do this.  I
> > looked pretty long and hard in the CRAN libraries but alas, I am stuck.
> > BTW, I picked up R about a month ago (I used primarily SAS, Stata and
> > SPSS).
> >
> > Regards and TIA,
> > Jesse
> >
> >
> >
> >
> >
> >
> > Jesse A. Canchola
> > Biostatistician III
> > Bayer Healthcare
> > 725 Potter St.
> > Berkeley, CA 94710
> > P: 510.705.5855
> > F: 510.705.5718
> > E: Jesse.Canchola.b at Bayer.Com
> >
> >
> >
> >
> >
> _______________________________________________________________________________________________
> >
> > The information contained in this e-mail is for the exclusive use of the
> intended recipient(s) and may be confidential, proprietary, and/or legally
> privileged.  Inadvertent disclosure of this message does not constitute a
> waiver of any privilege.  If you receive this message in error, please do
> not directly or indirectly use, print, copy, forward, or disclose any part
> of this message.  Please also delete this e-mail and all copies and notify
> the sender.  Thank you.
> >
> > For alternate languages please go to http://bayerdisclaimer.bayerweb.com
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>
>
>


From ggrothendieck at gmail.com  Fri Jul 14 20:08:07 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 14 Jul 2006 14:08:07 -0400
Subject: [R] Recreate new dataframe based on condition
In-Reply-To: <20060714175649.95318.qmail@web37601.mail.mud.yahoo.com>
References: <20060714175649.95318.qmail@web37601.mail.mud.yahoo.com>
Message-ID: <971536df0607141108r76e0ddeck75142cc1e96c2c19@mail.gmail.com>

Try this using DF since df is the name of an R function:

# test data from post
DF <-  structure(list(x = c(2, 4, 1, 3, 3, 2)), .Names = "x", row.names = c("1",
 "2", "3", "4", "5", "6"), class = "data.frame")


rowsum(DF, gl(nrow(DF),2,nrow(DF)))


On 7/14/06, Sachin J <sachinj.2006 at yahoo.com> wrote:
> Hi,
>
>  How can I achieve this in R. Dataset is as follows:
>
>  >df
>    x
> 1 2
> 2 4
> 3 1
> 4 3
> 5 3
> 6 2
>
>  structure(list(x = c(2, 4, 1, 3, 3, 2)), .Names = "x", row.names = c("1",
> "2", "3", "4", "5", "6"), class = "data.frame")
>
>  I want to recreate a new data frame whose rows are sum of (1&2, 3&4, 5&6) of original df. For example
>
>  >newdf
>     x
>  1 6
>  2 4
>  3 5
>
>  Thanx in advance for the help.
>
>  Sachin
>
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jholtman at gmail.com  Fri Jul 14 20:11:54 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 14 Jul 2006 14:11:54 -0400
Subject: [R] Recreate new dataframe based on condition
In-Reply-To: <20060714175649.95318.qmail@web37601.mail.mud.yahoo.com>
References: <20060714175649.95318.qmail@web37601.mail.mud.yahoo.com>
Message-ID: <644e1f320607141111k41b6ec0fm7f0390396de40bf0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060714/f74dcc11/attachment.pl 

From aaron at atmos.colostate.edu  Fri Jul 14 06:18:12 2006
From: aaron at atmos.colostate.edu (Jih-Wang (Aaron) Wang)
Date: Fri, 14 Jul 2006 12:18:12 +0800
Subject: [R] EOF: object needs to be "field"?
Message-ID: <000601c6a6fc$995287d0$47335281@pielke.atmos.colostate.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060714/2530bcc3/attachment.pl 

From dgerlanc at gmail.com  Fri Jul 14 20:23:30 2006
From: dgerlanc at gmail.com (Daniel Gerlanc)
Date: Fri, 14 Jul 2006 14:23:30 -0400
Subject: [R] Splitting the left and right hand terms of a formula
Message-ID: <84c9e3cb0607141123t3316e55cwbfd6461359d0d4b8@mail.gmail.com>

Let's say I have the following formula:

a.formula <- x ~ y + z

I want to extract the left and right-hand sides of the function so
that I have two character vectors like the ones you would create using
the following assignments:

left.hand.side <- "x"
right.hand.side <- c("y", "z")

One way to do this follows:

left.hand.side <- unlist(dimnames(attr(terms(a.formula), "factors"))[1])
right.hand.side <- unlist(dimnames(attr(terms(a.formula), "factors"))[-1])

Is there a better or cleaner way to do this?

Thanks!

Daniel Gerlanc
Williams College '07


From ggrothendieck at gmail.com  Fri Jul 14 20:27:57 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 14 Jul 2006 14:27:57 -0400
Subject: [R] Splitting the left and right hand terms of a formula
In-Reply-To: <84c9e3cb0607141123t3316e55cwbfd6461359d0d4b8@mail.gmail.com>
References: <84c9e3cb0607141123t3316e55cwbfd6461359d0d4b8@mail.gmail.com>
Message-ID: <971536df0607141127x424b9d5csd4bfe92c8944cc88@mail.gmail.com>

Try this:

> all.vars(update(a.formula, .~0))
[1] "x"

> all.vars(update(a.formula, 0~.))
[1] "y" "z"

On 7/14/06, Daniel Gerlanc <dgerlanc at gmail.com> wrote:
> Let's say I have the following formula:
>
> a.formula <- x ~ y + z
>
> I want to extract the left and right-hand sides of the function so
> that I have two character vectors like the ones you would create using
> the following assignments:
>
> left.hand.side <- "x"
> right.hand.side <- c("y", "z")
>
> One way to do this follows:
>
> left.hand.side <- unlist(dimnames(attr(terms(a.formula), "factors"))[1])
> right.hand.side <- unlist(dimnames(attr(terms(a.formula), "factors"))[-1])
>
> Is there a better or cleaner way to do this?
>
> Thanks!
>
> Daniel Gerlanc
> Williams College '07
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From jesse.canchola.b at bayer.com  Fri Jul 14 20:52:20 2006
From: jesse.canchola.b at bayer.com (Jesse Albert Canchola)
Date: Fri, 14 Jul 2006 11:52:20 -0700
Subject: [R] looping using combinatorics
In-Reply-To: <971536df0607141101tf3aed41qf436e85189775ea1@mail.gmail.com>
Message-ID: <OF9762A957.F01F1C5D-ON882571AB.00661EF0-882571AB.0067AAED@bayer.com>

Thanks.  It is actually the rows I want to choose from, not the columns 
(the columns will remain the same with the same names). A slighly 
abbreviated and modifed example:

data frame "a" has
ID  meas index
1   1.1  1
2   2.1  1

data frame "b" has
ID meas index
3  1.2  2
4  2.2  2

data frame "c" has
ID meas index
5 1.3  3
6 1.4  3

rbind the three frames "a", "b", and "c" into "d":
ID  meas  index
1   1.1  1
2   2.1  1
3   1.2   2
4   2.2   2
5  1.3   3 
6  1.4   3

The three (3 choose 2) pairs we want will be as follows.
Using "combn" from the "combinat" package on CRAN, we get the pairs (1,2), 
(1,3), (2,3) which can be used as the index in the "for" loop (as you have 
used below):
In this case, the pairs (1,2) refer to the actual subset of the data frame 
"d", above, where the actual variable named index=1 or index=2 (and so on 
for the other pairs). 

So the firstly chosen pair would be (1,2) and the resulting subset of the 
data frame "d" looks like this:
ID  meas  index
1   1.1  1
2   2.1  1
3   1.2   2
4   2.2   2

and so on for the other pairs.  So the "rbind" is correct it is the "for" 
loop that needs to be modified to grab the subsets from the DF frame below 
(i.e., the (1,2) pair selected by the combn function will select data from 
DF where the actual variable index=1 or index=2 ; using the example 
above).

#### BEGIN CODE ####
DF <- rbind(a,b,c)
DF
for(index in as.data.frame(combn(3,2))) print(DF[,index])
#### END CODE ####



Regards,
Jesse








"Gabor Grothendieck" <ggrothendieck at gmail.com> 
07/14/2006 11:01 AM

To
"Jesse Albert Canchola" <jesse.canchola.b at bayer.com>
cc
r-help at stat.math.ethz.ch
Subject
Re: [R] looping using combinatorics






Use data.frame, not rbind, e.g. DF <- data.frame(a, b, c)

On 7/14/06, Jesse Albert Canchola <jesse.canchola.b at bayer.com> wrote:
> Many thanks, Gabor.  This is very close to what would be ideal.  You 
gave
> me an idea as follows:
>
> Rather than combine pairs of data vectors/frames AFTER the "combn"
> function, combine all data before (though I belive this would be less
> efficient) and add an index then use that index to choose your pairs (or
> whatever combinatorics you are using; e.g., 8 choose 4  so all
> combinations of 4 out of 8 for a total of 70 combinations.)
>
> Example data frames with variable names:
>
> Data frame "a" where I add an "index":
> id measure index
> 1  1.1  1
> 2  1.2  1
> 3  1.3  1
>
> Data frame "b" where I add an "index":
> id measure index
> 4  2.1  2
> 5  2.2  2
> 6  2.3  2
>
> Data frame "c" where I add an index:
> id measure index
> 7  3.1  3
> 8  3.2  3
> 9  3.3  3
>
> If we combine all these data at once using rbind, we get:
> id measure index
> 1  1.1  1
> 2  1.2  1
> 3  1.3  1
> 4  2.1  2
> 5  2.2  2
> 6  2.3  2
> 7  3.1  3
> 8  3.2  3
> 9  3.3  3
>
> We can then use something similar to your code and the index to choose 
the
> required pairs as derived from the "combn" function.
> For example, the "combn" function will choose the data pairs
> (1,2)
> (1,3)
> (2,3)
>
> where, for example, the pairs (1,2) will have data from frames "a" and
> "b":
> 1  1.1  1
> 2  1.2  1
> 3  1.3  1
> 4  2.1  2
> 5  2.2  2
> 6  2.3  2
>
> so that we can go down the list subsetting what we need and doing
> operations on each combined pair as we go.
>
> Is there an easy way in R to do this operation?
>
> For the above, an attempt might be:
>
> ########## STAB CODE #########
> DF <- rbind(a,b,c)
> DF
> for(index in as.data.frame(combn(3,2))) print(DF[,index])
> ######## END STAB CODE ######
>
> but this is choosing 3 choose 2 COLUMNS within the combined file rather
> than 3 choose 2 ROWS.
>
>
> Best regards and TIAA,
> Jesse
>
>
>
>
>
> "Gabor Grothendieck" <ggrothendieck at gmail.com>
> 07/13/2006 07:39 PM
>
> To
> "Jesse Albert Canchola" <jesse.canchola.b at bayer.com>
> cc
> r-help at stat.math.ethz.ch
> Subject
> Re: [R] looping using combinatorics
>
>
>
>
>
>
> I assume your question is given 3 vectors of the same length: a, b and c
> how do we loop over pairs of them.  In the following each iteration
> displays
> one pair:
>
>   library(combinat)
>   DF <- data.frame(a = 1:4, b = 5:8, c = 9:12)
>   for(idx in as.data.frame(combn(3,2))) print(DF[,idx])
>
> On 7/13/06, Jesse Albert Canchola <jesse.canchola.b at bayer.com> wrote:
> > I have a problem where I need to loop over the total combinations of
> > vectors (combined once chosen via combinatorics).  Here is a
> > simplification of the problem:
> >
> > STEP 1:  Define three vectors a, b, c.
> > STEP 2:  Combine all possible pairwise vectors (i.e., 3 choose 2 = 3
> > possible pairs of vectors: ab,ac, bc)
> > NOTE:  the actual problem has 8 choose 4, 8 choose 5 and 8 choose 6
> > combinations.
> > STEP 3:  Do the same math on each pairwise combination and spit out
> > answers each time
> >
> > ####### BEGIN CODE #######
> > #STEP 1
> > a1 <- c(1,2,3,4,5,6,7,8,9,10,11,12)
> > a <- matrix(a1,2,3,byrow=T)
> > a
> >
> > b1 <- c(13,14,15,16,17,18,19,20,21,22,23,24)
> > b <- matrix(b1,2,3,byrow=T)
> > b
> >
> > c1 <- c(25,26,27,28,29,30,31,32,33,34,35,36)
> > c <- matrix(b1,2,3,byrow=T)
> > c
> >
> > # example:  combine the first two vectors "a" and "b"
> > combab <- rbind(a,b)
> >
> > # the a,b combined data from the algorithm later below should look 
like
> > # something like the following:
> > combab
> >
> > # use the combinatorics "combn" function found in the "combinat" 
package
> > on CRAN
> > m <- combn(3,2) # three choose two combinations
> > m
> >
> > # the first assignment below should be numeric and then subsequent
> > # assignments as character since the first time you assign a number to
> > # a character in a matrix the rest of the numbers in the matrix are
> > coerced to character
> > m[m==1]='a'; m[m=='2']='b'; m[m=='3']='c'
> > m
> >
> > #STEP 2: combine pairwise vectors into a matrix or frame
> > for (i in dim(m)[1])
> >    for (j in dim(m)[2])
> >        {
> >            combined <-
> > rbind(cat(format(m[i]),"\n"),cat(format(m[j]),"\n")) #cat/format 
removes
> > the quotes
> >            combined
> >        }
> > traceback()
> >
> >
> > #STEP 3: {not there yet}
> > ################# END CODE ################
> >
> > The problem is that in STEP 2 (not complete), the results in the rbind
> are
> > not recognized as the objects they represent (i.e., the "a" without
> quotes
> > is not recognized as the data object we defined in STEP 1.  Perhaps 
this
> > is a parsing problem.  Perhaps there is an alterative way to do this. 
I
> > looked pretty long and hard in the CRAN libraries but alas, I am 
stuck.
> > BTW, I picked up R about a month ago (I used primarily SAS, Stata and
> > SPSS).
> >
> > Regards and TIA,
> > Jesse
> >
> >
> >
> >
> >
> >
> > Jesse A. Canchola
> > Biostatistician III
> > Bayer Healthcare
> > 725 Potter St.
> > Berkeley, CA 94710
> > P: 510.705.5855
> > F: 510.705.5718
> > E: Jesse.Canchola.b at Bayer.Com
> >
> >
> >
> >
> >
> 
_______________________________________________________________________________________________
> >
> > The information contained in this e-mail is for the exclusive use of 
the
> intended recipient(s) and may be confidential, proprietary, and/or 
legally
> privileged.  Inadvertent disclosure of this message does not constitute 
a
> waiver of any privilege.  If you receive this message in error, please 
do
> not directly or indirectly use, print, copy, forward, or disclose any 
part
> of this message.  Please also delete this e-mail and all copies and 
notify
> the sender.  Thank you.
> >
> > For alternate languages please go to 
http://bayerdisclaimer.bayerweb.com
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>
>
>


From rhelp.zhao at gmail.com  Fri Jul 14 20:58:07 2006
From: rhelp.zhao at gmail.com (Iris Zhao)
Date: Fri, 14 Jul 2006 14:58:07 -0400
Subject: [R] optim()
Message-ID: <d95bc7680607141158s33b307f9j7899f318c41566b2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060714/af06ccbb/attachment.pl 

From ggrothendieck at gmail.com  Fri Jul 14 20:59:09 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 14 Jul 2006 14:59:09 -0400
Subject: [R] looping using combinatorics
In-Reply-To: <OF9762A957.F01F1C5D-ON882571AB.00661EF0-882571AB.0067AAED@bayer.com>
References: <971536df0607141101tf3aed41qf436e85189775ea1@mail.gmail.com>
	<OF9762A957.F01F1C5D-ON882571AB.00661EF0-882571AB.0067AAED@bayer.com>
Message-ID: <971536df0607141159r359939ccq4d8e25c508ac446d@mail.gmail.com>

If a, b and c are numeric vectors then rbind(a,b,c) and cbind(a,b,c) produce
matrices, not data frames and iterating over a matrix in a for loop iterates
over the elements of the matrix whereas iterating over a data frame in
a for loop iterates over the columns.  You can use as.data.frame(my.matrix)
to convert a matrix to a data frame.

Compare:

> for(i in matrix(1:4,2)) print(i)
[1] 1
[1] 2
[1] 3
[1] 4
> for(i in as.data.frame(matrix(1:4,2))) print(i)
[1] 1 2
[1] 3 4



On 7/14/06, Jesse Albert Canchola <jesse.canchola.b at bayer.com> wrote:
> Thanks.  It is actually the rows I want to choose from, not the columns
> (the columns will remain the same with the same names). A slighly
> abbreviated and modifed example:
>
> data frame "a" has
> ID  meas index
> 1   1.1  1
> 2   2.1  1
>
> data frame "b" has
> ID meas index
> 3  1.2  2
> 4  2.2  2
>
> data frame "c" has
> ID meas index
> 5 1.3  3
> 6 1.4  3
>
> rbind the three frames "a", "b", and "c" into "d":
> ID  meas  index
> 1   1.1  1
> 2   2.1  1
> 3   1.2   2
> 4   2.2   2
> 5  1.3   3
> 6  1.4   3
>
> The three (3 choose 2) pairs we want will be as follows.
> Using "combn" from the "combinat" package on CRAN, we get the pairs (1,2),
> (1,3), (2,3) which can be used as the index in the "for" loop (as you have
> used below):
> In this case, the pairs (1,2) refer to the actual subset of the data frame
> "d", above, where the actual variable named index=1 or index=2 (and so on
> for the other pairs).
>
> So the firstly chosen pair would be (1,2) and the resulting subset of the
> data frame "d" looks like this:
> ID  meas  index
> 1   1.1  1
> 2   2.1  1
> 3   1.2   2
> 4   2.2   2
>
> and so on for the other pairs.  So the "rbind" is correct it is the "for"
> loop that needs to be modified to grab the subsets from the DF frame below
> (i.e., the (1,2) pair selected by the combn function will select data from
> DF where the actual variable index=1 or index=2 ; using the example
> above).
>
> #### BEGIN CODE ####
> DF <- rbind(a,b,c)
> DF
> for(index in as.data.frame(combn(3,2))) print(DF[,index])
> #### END CODE ####
>
>
>
> Regards,
> Jesse
>
>
>
>
>
>
>
>
> "Gabor Grothendieck" <ggrothendieck at gmail.com>
> 07/14/2006 11:01 AM
>
> To
> "Jesse Albert Canchola" <jesse.canchola.b at bayer.com>
> cc
> r-help at stat.math.ethz.ch
> Subject
> Re: [R] looping using combinatorics
>
>
>
>
>
>
> Use data.frame, not rbind, e.g. DF <- data.frame(a, b, c)
>
> On 7/14/06, Jesse Albert Canchola <jesse.canchola.b at bayer.com> wrote:
> > Many thanks, Gabor.  This is very close to what would be ideal.  You
> gave
> > me an idea as follows:
> >
> > Rather than combine pairs of data vectors/frames AFTER the "combn"
> > function, combine all data before (though I belive this would be less
> > efficient) and add an index then use that index to choose your pairs (or
> > whatever combinatorics you are using; e.g., 8 choose 4  so all
> > combinations of 4 out of 8 for a total of 70 combinations.)
> >
> > Example data frames with variable names:
> >
> > Data frame "a" where I add an "index":
> > id measure index
> > 1  1.1  1
> > 2  1.2  1
> > 3  1.3  1
> >
> > Data frame "b" where I add an "index":
> > id measure index
> > 4  2.1  2
> > 5  2.2  2
> > 6  2.3  2
> >
> > Data frame "c" where I add an index:
> > id measure index
> > 7  3.1  3
> > 8  3.2  3
> > 9  3.3  3
> >
> > If we combine all these data at once using rbind, we get:
> > id measure index
> > 1  1.1  1
> > 2  1.2  1
> > 3  1.3  1
> > 4  2.1  2
> > 5  2.2  2
> > 6  2.3  2
> > 7  3.1  3
> > 8  3.2  3
> > 9  3.3  3
> >
> > We can then use something similar to your code and the index to choose
> the
> > required pairs as derived from the "combn" function.
> > For example, the "combn" function will choose the data pairs
> > (1,2)
> > (1,3)
> > (2,3)
> >
> > where, for example, the pairs (1,2) will have data from frames "a" and
> > "b":
> > 1  1.1  1
> > 2  1.2  1
> > 3  1.3  1
> > 4  2.1  2
> > 5  2.2  2
> > 6  2.3  2
> >
> > so that we can go down the list subsetting what we need and doing
> > operations on each combined pair as we go.
> >
> > Is there an easy way in R to do this operation?
> >
> > For the above, an attempt might be:
> >
> > ########## STAB CODE #########
> > DF <- rbind(a,b,c)
> > DF
> > for(index in as.data.frame(combn(3,2))) print(DF[,index])
> > ######## END STAB CODE ######
> >
> > but this is choosing 3 choose 2 COLUMNS within the combined file rather
> > than 3 choose 2 ROWS.
> >
> >
> > Best regards and TIAA,
> > Jesse
> >
> >
> >
> >
> >
> > "Gabor Grothendieck" <ggrothendieck at gmail.com>
> > 07/13/2006 07:39 PM
> >
> > To
> > "Jesse Albert Canchola" <jesse.canchola.b at bayer.com>
> > cc
> > r-help at stat.math.ethz.ch
> > Subject
> > Re: [R] looping using combinatorics
> >
> >
> >
> >
> >
> >
> > I assume your question is given 3 vectors of the same length: a, b and c
> > how do we loop over pairs of them.  In the following each iteration
> > displays
> > one pair:
> >
> >   library(combinat)
> >   DF <- data.frame(a = 1:4, b = 5:8, c = 9:12)
> >   for(idx in as.data.frame(combn(3,2))) print(DF[,idx])
> >
> > On 7/13/06, Jesse Albert Canchola <jesse.canchola.b at bayer.com> wrote:
> > > I have a problem where I need to loop over the total combinations of
> > > vectors (combined once chosen via combinatorics).  Here is a
> > > simplification of the problem:
> > >
> > > STEP 1:  Define three vectors a, b, c.
> > > STEP 2:  Combine all possible pairwise vectors (i.e., 3 choose 2 = 3
> > > possible pairs of vectors: ab,ac, bc)
> > > NOTE:  the actual problem has 8 choose 4, 8 choose 5 and 8 choose 6
> > > combinations.
> > > STEP 3:  Do the same math on each pairwise combination and spit out
> > > answers each time
> > >
> > > ####### BEGIN CODE #######
> > > #STEP 1
> > > a1 <- c(1,2,3,4,5,6,7,8,9,10,11,12)
> > > a <- matrix(a1,2,3,byrow=T)
> > > a
> > >
> > > b1 <- c(13,14,15,16,17,18,19,20,21,22,23,24)
> > > b <- matrix(b1,2,3,byrow=T)
> > > b
> > >
> > > c1 <- c(25,26,27,28,29,30,31,32,33,34,35,36)
> > > c <- matrix(b1,2,3,byrow=T)
> > > c
> > >
> > > # example:  combine the first two vectors "a" and "b"
> > > combab <- rbind(a,b)
> > >
> > > # the a,b combined data from the algorithm later below should look
> like
> > > # something like the following:
> > > combab
> > >
> > > # use the combinatorics "combn" function found in the "combinat"
> package
> > > on CRAN
> > > m <- combn(3,2) # three choose two combinations
> > > m
> > >
> > > # the first assignment below should be numeric and then subsequent
> > > # assignments as character since the first time you assign a number to
> > > # a character in a matrix the rest of the numbers in the matrix are
> > > coerced to character
> > > m[m==1]='a'; m[m=='2']='b'; m[m=='3']='c'
> > > m
> > >
> > > #STEP 2: combine pairwise vectors into a matrix or frame
> > > for (i in dim(m)[1])
> > >    for (j in dim(m)[2])
> > >        {
> > >            combined <-
> > > rbind(cat(format(m[i]),"\n"),cat(format(m[j]),"\n")) #cat/format
> removes
> > > the quotes
> > >            combined
> > >        }
> > > traceback()
> > >
> > >
> > > #STEP 3: {not there yet}
> > > ################# END CODE ################
> > >
> > > The problem is that in STEP 2 (not complete), the results in the rbind
> > are
> > > not recognized as the objects they represent (i.e., the "a" without
> > quotes
> > > is not recognized as the data object we defined in STEP 1.  Perhaps
> this
> > > is a parsing problem.  Perhaps there is an alterative way to do this.
> I
> > > looked pretty long and hard in the CRAN libraries but alas, I am
> stuck.
> > > BTW, I picked up R about a month ago (I used primarily SAS, Stata and
> > > SPSS).
> > >
> > > Regards and TIA,
> > > Jesse
> > >
> > >
> > >
> > >
> > >
> > >
> > > Jesse A. Canchola
> > > Biostatistician III
> > > Bayer Healthcare
> > > 725 Potter St.
> > > Berkeley, CA 94710
> > > P: 510.705.5855
> > > F: 510.705.5718
> > > E: Jesse.Canchola.b at Bayer.Com
> > >
> > >
> > >
> > >
> > >
> >
> _______________________________________________________________________________________________
> > >
> > > The information contained in this e-mail is for the exclusive use of
> the
> > intended recipient(s) and may be confidential, proprietary, and/or
> legally
> > privileged.  Inadvertent disclosure of this message does not constitute
> a
> > waiver of any privilege.  If you receive this message in error, please
> do
> > not directly or indirectly use, print, copy, forward, or disclose any
> part
> > of this message.  Please also delete this e-mail and all copies and
> notify
> > the sender.  Thank you.
> > >
> > > For alternate languages please go to
> http://bayerdisclaimer.bayerweb.com
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > >
> >
> >
> >
>
>
>


From tuechler at gmx.at  Fri Jul 14 21:20:55 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Fri, 14 Jul 2006 20:20:55 +0100
Subject: [R] Keep value lables with data frame manipulation
In-Reply-To: <44B66E79.1010106@vanderbilt.edu>
References: <3.0.6.32.20060713144855.00a5e700@pop.gmx.net>
	<3.0.6.32.20060713095939.00ace1f8@pop.gmx.net>
	<2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>
	<2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>
	<3.0.6.32.20060713095939.00ace1f8@pop.gmx.net>
	<3.0.6.32.20060713144855.00a5e700@pop.gmx.net>
Message-ID: <3.0.6.32.20060714202055.00aceea8@pop.gmx.net>

At 11:02 13.07.2006 -0500, Frank E Harrell Jr wrote:
>Heinz Tuechler wrote:
>> At 08:11 13.07.2006 -0500, Frank E Harrell Jr wrote:
>>> Heinz Tuechler wrote:
>>>> At 13:14 12.07.2006 -0500, Marc Schwartz (via MN) wrote:
>>>>> On Wed, 2006-07-12 at 17:41 +0100, Jol, Arne wrote:
>>>>>> Dear R,
>>>>>>
>>>>>> I import data from spss into a R data.frame. On this rawdata I do some
>>>>>> data processing (selection of observations, normalization, recoding of
>>>>>> variables etc..). The result is stored in a new data.frame, however, in
>>>>>> this new data.frame the value labels are lost.
>>>>>>
>>>>>> Example of what I do in code:
>>>>>>
>>>>>> # read raw data from spss
>>>>>> rawdata <- read.spss("./data/T50937.SAV",
>>>>>> 	use.value.labels=FALSE,to.data.frame=TRUE)
>>>>>>
>>>>>> # select the observations that we need
>>>>>> diarydata <- rawdata[rawdata$D22==2 | rawdata$D22==3 |
rawdata$D22==17 |
>>>>>> rawdata$D22==18 | rawdata$D22==20 | rawdata$D22==22 |
>>>>>>  			rawdata$D22==24 | rawdata$D22==33,]
>>>>>>
>>>>>> The result is that rawdata$D22 has value labels and that diarydata$D22
>>>>>> is numeric without value labels.
>>>>>>
>>>>>> Question: How can I prevent this from happening?
>>>>>>
>>>>>> Thanks in advance!
>>>>>> Groeten,
>>>>>> Arne
>>>>> Two things:
>>>>>
>>>>> 1. With respect to your subsetting, your lengthy code can be replaced
>>>>> with the following:
>>>>>
>>>>>  diarydata <- subset(rawdata, D22 %in% c(2, 3, 17, 18, 20, 22, 24, 33))
>>>>>
>>>>> See ?subset and ?"%in%" for more information.
>>>>>
>>>>>
>>>>> 2. With respect to keeping the label related attributes, the
>>>>> 'value.labels' attribute and the 'variable.labels' attribute will not by
>>>>> default survive the use of "[".data.frame in R (see ?Extract
>>>>> and ?"[.data.frame").
>>>>>
>>>>> On the other hand, based upon my review of ?read.spss, the SPSS value
>>>>> labels should be converted to the factor levels of the respective
>>>>> columns when 'use.value.labels = TRUE' and these would survive a
>>>>> subsetting.
>>>>>
>>>>> If you want to consider a solution to the attribute subsetting issue,
>>>>> you might want to review the following post by Gabor Grothendieck in
>>>>> May, which provides a possible solution:
>>>>>
>>>>>  https://stat.ethz.ch/pipermail/r-help/2006-May/106308.html
>>>>>
>>>>> and this post by me, for an explanation of what is happening in Gabor's
>>>>> solution:
>>>>>
>>>>>  https://stat.ethz.ch/pipermail/r-help/2006-May/106351.html
>>>>>
>>>>> HTH,
>>>>>
>>>>> Marc Schwartz
>>>>>
>>>> Hello Mark and Arne,
>>>>
>>>> I worked on the suggestions of Gabor and Mark and programmed some
functions
>>>> in this way, but they are very, very preliminary (see below).
>>>> In my view there is a lack of convenient possibilities in R to document
>>>> empirical data by variable labels, value labels, etc. I would prefer to
>>>> have these possibilities in the "standard" configuration.
>>>> So I sketched a concept, but in my view it would only be useful, if there
>>>> was some acceptance by the core developers of R.
>>>>
>>>> The concept would be to define a class. For now I call it "source.data".
>>>> To design it more flexible than the Hmisc class "labelled" I would
define a
>>>> related option "source.data.attributes" with default c('value.labels',
>>>> 'variable.name', 'label')). This option contains all attributes that
should
>>>> persist in subsetting/indexing.
>>>>
>>>> I made only some very, very preliminary tests with these functions,
mainly
>>>> because I am not happy with defining a new class. Instead I would prefer,
>>>> if this functionality could be integrated in the Hmisc class "labelled",
>>>> since this is in my view the best known starting point for data
>>>> documentation in R.
>>>>
>>>> I would be happy, if there were some discussion about the wishes/needs of
>>>> other Rusers concerning data documentation.
>>>>
>>>> Greetings,
>>>>
>>>> Heinz
>>> I feel that separating variable labels and value labels and just using 
>>> factors for value labels works fine, and I would urge you not to create 
>>> a new system that will not benefit from the many Hmisc functions that 
>>> use variable labels and units.  [.data.frame in Hmisc keeps all
attributes.
>>>
>>> Frank
>>>
>> 
>> Frank,
>> 
>> of course I aggree with you about the importance of Hmisc and as I said, I
>> do not want to define a new class, but in my view factors are no good
>> substitute for value labels.
>> As the language definition (version 2.3.1 (2006-06-05) Draft, page 7) says:
>> "Factors are currently implemented using an integer array to specify the
>> actual levels and a second array of names that are mapped to the integers.
>> Rather unfortunately users often make use of the implementation in order to
>> make some calculations easier." 
>> So, in my view, the levels represent the "values" of the factor.
>> This has inconveniencies if you want to use value labels in different
>> languages. Further I do not see a simple method to label numerical
>> variables. I often encounter discrete, but still metric data, as e.g. risk
>> scores. Usually it would be nice to use them in their original coding,
>> which may include zero or decimal places and to label them at the same
time.
>> Personally at the moment I try to solve this problem by following a
>> suggestion of Martin, Dimitis and others to use names instead. I doubt,
>> however, that this is a good solution, but at least it makes it possible to
>> have the source data numerically coded and in this sense "language free"
>> (see first attempts of functions below).
>> 
>> Heinz
>> 
>Those are excellent points Heinz.  I addressed that problem partially in 
>sas.get - see the sascodes attribute.
>
>Frank
>

Frank, I looked at your function sas.get. You solved the problem with a lot
of effort. Don't you think that it would be easier to create just one new
class, say "documented", which offers the possibility to represent the
original data as it is and to add all the useful descriptions like variable
labels, value labels, units, special missing values, and may be others.
If I remember correctly SAS, SPSS and BMDP offer these possibilities since
many years, and in my view for good reason. I am thinking about this
questions since I started using R about two years ago and I wonder, why
there seems to be so little interest in these questions.
In my work good documentation of the _unchanged_ data is very important,
also because it eases checking the data for errors.

Heinz


>> ...snip...


From jesse.canchola.b at bayer.com  Fri Jul 14 22:39:39 2006
From: jesse.canchola.b at bayer.com (Jesse Albert Canchola)
Date: Fri, 14 Jul 2006 13:39:39 -0700
Subject: [R] looping using combinatorics
In-Reply-To: <971536df0607141159r359939ccq4d8e25c508ac446d@mail.gmail.com>
Message-ID: <OF95EF68E9.EFD30BC1-ON882571AB.007150F5-882571AB.00717E44@bayer.com>

Great.  I will work on the problem with those definitions in mind. Thanks 
for your help, Gabor.  I'll post the final solution when it is ready.

Best,
Jesse




"Gabor Grothendieck" <ggrothendieck at gmail.com> 
07/14/2006 11:59 AM

To
"Jesse Albert Canchola" <jesse.canchola.b at bayer.com>
cc
r-help at stat.math.ethz.ch
Subject
Re: [R] looping using combinatorics






If a, b and c are numeric vectors then rbind(a,b,c) and cbind(a,b,c) 
produce
matrices, not data frames and iterating over a matrix in a for loop 
iterates
over the elements of the matrix whereas iterating over a data frame in
a for loop iterates over the columns.  You can use 
as.data.frame(my.matrix)
to convert a matrix to a data frame.

Compare:

> for(i in matrix(1:4,2)) print(i)
[1] 1
[1] 2
[1] 3
[1] 4
> for(i in as.data.frame(matrix(1:4,2))) print(i)
[1] 1 2
[1] 3 4



On 7/14/06, Jesse Albert Canchola <jesse.canchola.b at bayer.com> wrote:
> Thanks.  It is actually the rows I want to choose from, not the columns
> (the columns will remain the same with the same names). A slighly
> abbreviated and modifed example:
>
> data frame "a" has
> ID  meas index
> 1   1.1  1
> 2   2.1  1
>
> data frame "b" has
> ID meas index
> 3  1.2  2
> 4  2.2  2
>
> data frame "c" has
> ID meas index
> 5 1.3  3
> 6 1.4  3
>
> rbind the three frames "a", "b", and "c" into "d":
> ID  meas  index
> 1   1.1  1
> 2   2.1  1
> 3   1.2   2
> 4   2.2   2
> 5  1.3   3
> 6  1.4   3
>
> The three (3 choose 2) pairs we want will be as follows.
> Using "combn" from the "combinat" package on CRAN, we get the pairs 
(1,2),
> (1,3), (2,3) which can be used as the index in the "for" loop (as you 
have
> used below):
> In this case, the pairs (1,2) refer to the actual subset of the data 
frame
> "d", above, where the actual variable named index=1 or index=2 (and so 
on
> for the other pairs).
>
> So the firstly chosen pair would be (1,2) and the resulting subset of 
the
> data frame "d" looks like this:
> ID  meas  index
> 1   1.1  1
> 2   2.1  1
> 3   1.2   2
> 4   2.2   2
>
> and so on for the other pairs.  So the "rbind" is correct it is the 
"for"
> loop that needs to be modified to grab the subsets from the DF frame 
below
> (i.e., the (1,2) pair selected by the combn function will select data 
from
> DF where the actual variable index=1 or index=2 ; using the example
> above).
>
> #### BEGIN CODE ####
> DF <- rbind(a,b,c)
> DF
> for(index in as.data.frame(combn(3,2))) print(DF[,index])
> #### END CODE ####
>
>
>
> Regards,
> Jesse
>
>
>
>
>
>
>
>
> "Gabor Grothendieck" <ggrothendieck at gmail.com>
> 07/14/2006 11:01 AM
>
> To
> "Jesse Albert Canchola" <jesse.canchola.b at bayer.com>
> cc
> r-help at stat.math.ethz.ch
> Subject
> Re: [R] looping using combinatorics
>
>
>
>
>
>
> Use data.frame, not rbind, e.g. DF <- data.frame(a, b, c)
>
> On 7/14/06, Jesse Albert Canchola <jesse.canchola.b at bayer.com> wrote:
> > Many thanks, Gabor.  This is very close to what would be ideal.  You
> gave
> > me an idea as follows:
> >
> > Rather than combine pairs of data vectors/frames AFTER the "combn"
> > function, combine all data before (though I belive this would be less
> > efficient) and add an index then use that index to choose your pairs 
(or
> > whatever combinatorics you are using; e.g., 8 choose 4  so all
> > combinations of 4 out of 8 for a total of 70 combinations.)
> >
> > Example data frames with variable names:
> >
> > Data frame "a" where I add an "index":
> > id measure index
> > 1  1.1  1
> > 2  1.2  1
> > 3  1.3  1
> >
> > Data frame "b" where I add an "index":
> > id measure index
> > 4  2.1  2
> > 5  2.2  2
> > 6  2.3  2
> >
> > Data frame "c" where I add an index:
> > id measure index
> > 7  3.1  3
> > 8  3.2  3
> > 9  3.3  3
> >
> > If we combine all these data at once using rbind, we get:
> > id measure index
> > 1  1.1  1
> > 2  1.2  1
> > 3  1.3  1
> > 4  2.1  2
> > 5  2.2  2
> > 6  2.3  2
> > 7  3.1  3
> > 8  3.2  3
> > 9  3.3  3
> >
> > We can then use something similar to your code and the index to choose
> the
> > required pairs as derived from the "combn" function.
> > For example, the "combn" function will choose the data pairs
> > (1,2)
> > (1,3)
> > (2,3)
> >
> > where, for example, the pairs (1,2) will have data from frames "a" and
> > "b":
> > 1  1.1  1
> > 2  1.2  1
> > 3  1.3  1
> > 4  2.1  2
> > 5  2.2  2
> > 6  2.3  2
> >
> > so that we can go down the list subsetting what we need and doing
> > operations on each combined pair as we go.
> >
> > Is there an easy way in R to do this operation?
> >
> > For the above, an attempt might be:
> >
> > ########## STAB CODE #########
> > DF <- rbind(a,b,c)
> > DF
> > for(index in as.data.frame(combn(3,2))) print(DF[,index])
> > ######## END STAB CODE ######
> >
> > but this is choosing 3 choose 2 COLUMNS within the combined file 
rather
> > than 3 choose 2 ROWS.
> >
> >
> > Best regards and TIAA,
> > Jesse
> >
> >
> >
> >
> >
> > "Gabor Grothendieck" <ggrothendieck at gmail.com>
> > 07/13/2006 07:39 PM
> >
> > To
> > "Jesse Albert Canchola" <jesse.canchola.b at bayer.com>
> > cc
> > r-help at stat.math.ethz.ch
> > Subject
> > Re: [R] looping using combinatorics
> >
> >
> >
> >
> >
> >
> > I assume your question is given 3 vectors of the same length: a, b and 
c
> > how do we loop over pairs of them.  In the following each iteration
> > displays
> > one pair:
> >
> >   library(combinat)
> >   DF <- data.frame(a = 1:4, b = 5:8, c = 9:12)
> >   for(idx in as.data.frame(combn(3,2))) print(DF[,idx])
> >
> > On 7/13/06, Jesse Albert Canchola <jesse.canchola.b at bayer.com> wrote:
> > > I have a problem where I need to loop over the total combinations of
> > > vectors (combined once chosen via combinatorics).  Here is a
> > > simplification of the problem:
> > >
> > > STEP 1:  Define three vectors a, b, c.
> > > STEP 2:  Combine all possible pairwise vectors (i.e., 3 choose 2 = 3
> > > possible pairs of vectors: ab,ac, bc)
> > > NOTE:  the actual problem has 8 choose 4, 8 choose 5 and 8 choose 6
> > > combinations.
> > > STEP 3:  Do the same math on each pairwise combination and spit out
> > > answers each time
> > >
> > > ####### BEGIN CODE #######
> > > #STEP 1
> > > a1 <- c(1,2,3,4,5,6,7,8,9,10,11,12)
> > > a <- matrix(a1,2,3,byrow=T)
> > > a
> > >
> > > b1 <- c(13,14,15,16,17,18,19,20,21,22,23,24)
> > > b <- matrix(b1,2,3,byrow=T)
> > > b
> > >
> > > c1 <- c(25,26,27,28,29,30,31,32,33,34,35,36)
> > > c <- matrix(b1,2,3,byrow=T)
> > > c
> > >
> > > # example:  combine the first two vectors "a" and "b"
> > > combab <- rbind(a,b)
> > >
> > > # the a,b combined data from the algorithm later below should look
> like
> > > # something like the following:
> > > combab
> > >
> > > # use the combinatorics "combn" function found in the "combinat"
> package
> > > on CRAN
> > > m <- combn(3,2) # three choose two combinations
> > > m
> > >
> > > # the first assignment below should be numeric and then subsequent
> > > # assignments as character since the first time you assign a number 
to
> > > # a character in a matrix the rest of the numbers in the matrix are
> > > coerced to character
> > > m[m==1]='a'; m[m=='2']='b'; m[m=='3']='c'
> > > m
> > >
> > > #STEP 2: combine pairwise vectors into a matrix or frame
> > > for (i in dim(m)[1])
> > >    for (j in dim(m)[2])
> > >        {
> > >            combined <-
> > > rbind(cat(format(m[i]),"\n"),cat(format(m[j]),"\n")) #cat/format
> removes
> > > the quotes
> > >            combined
> > >        }
> > > traceback()
> > >
> > >
> > > #STEP 3: {not there yet}
> > > ################# END CODE ################
> > >
> > > The problem is that in STEP 2 (not complete), the results in the 
rbind
> > are
> > > not recognized as the objects they represent (i.e., the "a" without
> > quotes
> > > is not recognized as the data object we defined in STEP 1.  Perhaps
> this
> > > is a parsing problem.  Perhaps there is an alterative way to do 
this.
> I
> > > looked pretty long and hard in the CRAN libraries but alas, I am
> stuck.
> > > BTW, I picked up R about a month ago (I used primarily SAS, Stata 
and
> > > SPSS).
> > >
> > > Regards and TIA,
> > > Jesse
> > >
> > >
> > >
> > >
> > >
> > >
> > > Jesse A. Canchola
> > > Biostatistician III
> > > Bayer Healthcare
> > > 725 Potter St.
> > > Berkeley, CA 94710
> > > P: 510.705.5855
> > > F: 510.705.5718
> > > E: Jesse.Canchola.b at Bayer.Com
> > >
> > >
> > >
> > >
> > >
> >
> 
_______________________________________________________________________________________________
> > >
> > > The information contained in this e-mail is for the exclusive use of
> the
> > intended recipient(s) and may be confidential, proprietary, and/or
> legally
> > privileged.  Inadvertent disclosure of this message does not 
constitute
> a
> > waiver of any privilege.  If you receive this message in error, please
> do
> > not directly or indirectly use, print, copy, forward, or disclose any
> part
> > of this message.  Please also delete this e-mail and all copies and
> notify
> > the sender.  Thank you.
> > >
> > > For alternate languages please go to
> http://bayerdisclaimer.bayerweb.com
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > >
> >
> >
> >
>
>
>


From jtokle at math.washington.edu  Fri Jul 14 23:13:46 2006
From: jtokle at math.washington.edu (Joshua Tokle)
Date: Fri, 14 Jul 2006 14:13:46 -0700 (PDT)
Subject: [R] R newbie: logical subsets
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB4B77D5@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBB4B77D5@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <Pine.LNX.4.64.0607141410140.10466@zeno1.math.washington.edu>

This is exactly what I needed -- thanks for your help Greg and Gabor.

I'm looking forward to replacing a dozen stored procedures, temp tables, 
and database calls with a one page R script.

Josh

On Wed, 12 Jul 2006, Greg Snow wrote:

> Gabor, your solution does not take into account the groups.  How about
> something like:
>
> iris2 <- iris
> iris2$m <- ave(iris2$Sepal.Length, iris2$Species)
> iris2$s <- ave(iris2$Sepal.Length, iris2$Species, FUN=sd)
>
> iris2 <- transform(iris2, z= (Sepal.Length-m)/s)
>
> iris2.2 <- subset(iris2, abs(z) < 2)
>
> aggregate(iris2.2, list(iris2.2$Species), FUN=mean)
>
>
>
> -- 
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at intermountainmail.org
> (801) 408-8111
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor
> Grothendieck
> Sent: Tuesday, July 11, 2006 1:06 PM
> To: Joshua Tokle
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] R newbie: logical subsets
>
> Try this, using the built in anscombe data set:
>
> anscombe[!rowSums(abs(scale(anscombe)) > 2),]
>
>
>
> On 7/11/06, Joshua Tokle <jtokle at math.washington.edu> wrote:
>> Hello!  I'm a newcomer to R hoping to replace some convoluted database
>
>> code with an R script.  Unfortunately, I haven't been able to figure
>> out how to implement the following logic.
>>
>> Essentially, we have a database of transactions that are coded with a
>> geographic locale and a type.  These are being loaded into a
>> data.frame with named variables city, type, and price.  E.g.,
>> trans$city and all that.
>>
>> We want to calculate mean prices by city and type, AFTER excluding
>> outliers.  That is, we want to calculate the mean price in 3 steps:
>>
>> 1. calculate a mean and standard deviation by city and type over all
>> transactions 2. create a subset of the original data frame, excluding
>> transactions that differ from the relevant mean by more than 2
>> standard deviations 3. calculate a final mean by city and type based
>> on this subset.
>>
>> I'm stuck on step 2.  I would like to do something like the following:
>>
>> fs <- list(factor(trans$city), factor(trans$type)) means <-
>> tapply(trans$price, fs, mean) stdevs <- tapply(trans$price, fs, sd)
>>
>> filter <- abs(trans$price - means[trans$city, trans$type]) <
>>             2*stdevs[trans$city, trans$type]
>>
>> sub <- subset(trans, filter)
>>
>> The above code doesn't work.  What's the correct way to do this?
>>
>> Thanks,
>> Josh
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>


From jennytimp at hotmail.com  Sat Jul 15 03:02:03 2006
From: jennytimp at hotmail.com (jenny tan)
Date: Sat, 15 Jul 2006 09:02:03 +0800
Subject: [R] lm: Displaying 95% confidence and prediction intervals on
	scatterplots
Message-ID: <BAY118-F49F13DE25B24B64F0AD65B96C0@phx.gbl>

Hi,

May I know  how does one superimpose the 95% confidence and prediction 
intervals on the linear regression line of a scatterplot?

Thanks in advance!

jenny


My data:

CEU8	EA
0.033	0
0.014	0
0.019	0
0.023	0
0.033	0
0.033	0
0.033	0
0.037	0
0.033	0
0.014	0
0.019	0
0.023	0
0.014	0
0.019	0
0.023	0
0.014	0
0.019	0
0.023	0
0.017	0
0.018	0
0.022	0
0.014	0
0.019	0
0.023	0
0.114	0.004
0.114	0.004
0.042	0.004
0.012	0.007
0.13	0.008
0.13	0.008
0.122	0.008
0.122	0.008
0.122	0.008
0.122	0.008
0.112	0.008
0.112	0.008
0.105	0.009
0.111	0.009
0.232	0.009
0.242	0.009
0.005	0.014
0.006	0.014
0.006	0.014
0.007	0.014
0.047	0.016
0.05	0.017
0.05	0.017
0.04	0.018
0.04	0.018
0.04	0.018
0.04	0.018
0.034	0.018
0.04	0.018
0.01	0.018
0.02	0.021
0.022	0.021
0.021	0.021
0.032	0.022
0.011	0.022
0.011	0.022
0	0.022
0	0.022
0.011	0.022
0.011	0.022
0.011	0.022
0.011	0.022
0.033	0.023
0.032	0.023
0.035	0.023
0.033	0.023
0.049	0.026
0.047	0.026
0.179	0.026
0.038	0.026
0.012	0.028
0.012	0.028
0.013	0.034
0.013	0.034
0.033	0.036
0.057	0.041
0.06	0.048
0.06	0.048
0.023	0.05
0.031	0.05
0.031	0.05
0.033	0.05
0.034	0.05
0.418	0.052
0.094	0.059
0.106	0.059
0.1	0.059
0.012	0.059
0.019	0.059
0.018	0.059
0.017	0.059
0.025	0.059
0.024	0.059
0.02	0.059
0.027	0.059
0.022	0.059
0.02	0.059
0.029	0.059
0.024	0.059
0.06	0.062
0.043	0.062
0.047	0.062
0.037	0.062
0.036	0.062
0.06	0.062
0.174	0.066
0.04	0.067
0.052	0.067
0.048	0.067
0.055	0.067
0.027	0.07
0.011	0.07
0.011	0.07
0.014	0.07
0.037	0.071
0.034	0.071
0.054	0.071
0.052	0.071
0.05	0.071
0.048	0.071
0.057	0.071
0.055	0.071
0.151	0.076
0.373	0.082
0.408	0.082
0.408	0.082
0.418	0.082
0.057	0.086
0.057	0.086
0.042	0.086
0.046	0.086
0.04	0.086
0.04	0.086
0.09	0.09
0.112	0.091
0.131	0.091
0.124	0.091
0.095	0.091
0.09	0.091
0.588	0.094
0.026	0.1
0.038	0.1
0.038	0.1
0.042	0.1
0.191	0.107
0.191	0.107
0.191	0.107
0.191	0.107
0.193	0.107
0.191	0.107
0.045	0.107
0.446	0.116
0.042	0.12
0.017	0.12
0.019	0.12
0.015	0.12
0.016	0.12
0.157	0.125
0.082	0.128
0.103	0.128
0.114	0.128
0.114	0.128
0.143	0.129
0.166	0.129
0.157	0.129
0.143	0.129
0.166	0.129
0.157	0.129
0.143	0.129
0.166	0.129
0.157	0.129
0.143	0.129
0.166	0.129
0.157	0.129
0.149	0.129
0.175	0.129
0.166	0.129
0.143	0.129
0.166	0.129
0.157	0.129
0.064	0.129
0.067	0.13
0.064	0.13
0.05	0.13
0.081	0.138
0.081	0.138
0.269	0.145
0.194	0.152
0.02	0.158
0.135	0.159
0.074	0.163
0.074	0.163
0.074	0.163
0.073	0.163
0.104	0.167
0.104	0.167
0.104	0.167
0.201	0.175
0.187	0.175
0.084	0.175
0.084	0.175
0.088	0.175
0.086	0.175
0.08	0.175
0.08	0.175
0.083	0.175
0.081	0.175
0.08	0.175
0.08	0.175
0.083	0.175
0.081	0.175
0.072	0.175
0.073	0.175
0.073	0.175
0.072	0.175
0.187	0.176
0.234	0.176
0.216	0.176
0.446	0.176
0.412	0.176
0.412	0.176
0.41	0.176
0.11	0.179
0.109	0.179
0.117	0.179
0.132	0.185
0.135	0.185
0.135	0.188
0.132	0.188
0.14	0.188
0.136	0.188
0.252	0.189
0.256	0.192
0.147	0.2
0.165	0.2
0.111	0.2
0.096	0.2
0.111	0.2
0.197	0.2
0.187	0.2
0.197	0.2
0.143	0.2
0.142	0.2
0.151	0.2
0.401	0.2
0.435	0.2
0.435	0.2
0.447	0.2
0.083	0.205
0.136	0.206
0.246	0.214
0.246	0.214
0.246	0.214
0.246	0.214
0.266	0.214
0.246	0.214
0.282	0.219
0.289	0.219
0.244	0.219
0.234	0.219
0.266	0.219
0.274	0.219
0.228	0.219
0.217	0.219
0.086	0.219
0.091	0.219
0.091	0.219
0.081	0.219
0.177	0.222
0.422	0.222
0.303	0.231
0.325	0.236
0.283	0.236
0.153	0.236
0.745	0.239
0.745	0.239
0.745	0.239
0.745	0.239
0.765	0.239
0.745	0.239
0.175	0.25
0.181	0.25
0.093	0.25
0.325	0.257
0.344	0.263
0.346	0.263
0.363	0.263
0.322	0.266
0.325	0.266
0.28	0.266
0.283	0.266
0.097	0.266
0.101	0.266
0.101	0.266
0.091	0.266
0.558	0.273
0.328	0.273
0.299	0.273
0.451	0.273
0.31	0.273
0.277	0.273
0.212	0.286
0.169	0.286
0.193	0.286
0.152	0.286
0.209	0.286
0.241	0.286
0.258	0.286
0.601	0.288
0.334	0.289
0.322	0.289
0.325	0.289
0.17	0.289
0.344	0.3
0.356	0.3
0.346	0.3
0.359	0.3
0.363	0.3
0.376	0.3
0.214	0.304
0.223	0.304
0.063	0.327
0.063	0.327
0.064	0.327
0.064	0.327
0.424	0.347
0.602	0.347
0.296	0.357
0.424	0.375
0.44	0.375
0.46	0.398
0.46	0.398
0.269	0.429
0.284	0.429
0.269	0.429
0.284	0.429
0.269	0.429
0.284	0.429
0.269	0.429
0.274	0.429
0.269	0.429
0.284	0.429
0.309	0.429
0.284	0.429
0.562	0.449
0.607	0.449
0.607	0.449
0.609	0.449
0.637	0.476
0.574	0.5
0.595	0.5
0.583	0.576
0.674	0.576
0.653	0.576
0.324	0.591
0.324	0.591
0.312	0.591
0.416	0.618
0.416	0.641
0.402	0.641
0.803	0.657
0.802	0.657
0.803	0.657
0.803	0.657
0.81	0.675
0.804	0.675
0.856	0.675
0.79	0.71
0.79	0.71
0.79	0.71
0.79	0.71
0.785	0.71
0.79	0.71
0.681	0.8
0.703	0.8
0.868	0.8
0.862	0.8
0.687	0.8
0.685	0.8
0.686	0.8
0.687	0.8
0.901	0.821
0.96	0.833
0.924	0.865
0.966	0.865
0.966	0.865
1	0.865
1	1
1	1
1	1
1	1
1	1
0.849	1
0.849	1
0.849	1
1	1
1	1
1	1
1	1
1	1
1	1
1	1
1	1
1	1
1	1
1	1
1	1
1	1
1	1
0.769	1
0.746	1
1	1
1	1
1	1
1	1
1	1
1	1
1	1
1	1
0.93	1
0.926	1
0.936	1
0.933	1
1	1
1	1
0.946	1
0.944	1
0.721	1
1	1
0.965	1
0.965	1
1	1
1	1
1	1
0.963	1
0.963	1
0.96	1
1	1
1	1
1	1
1	1
1	1
1	1
1	1
1	1
1	1
1	1

_________________________________________________________________
Get MSN Hotmail alerts on your mobile.


From f.harrell at vanderbilt.edu  Sat Jul 15 03:39:42 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 14 Jul 2006 20:39:42 -0500
Subject: [R] Keep value lables with data frame manipulation
In-Reply-To: <3.0.6.32.20060714202055.00aceea8@pop.gmx.net>
References: <3.0.6.32.20060713144855.00a5e700@pop.gmx.net>
	<3.0.6.32.20060713095939.00ace1f8@pop.gmx.net>
	<2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>
	<2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>
	<3.0.6.32.20060713095939.00ace1f8@pop.gmx.net>
	<3.0.6.32.20060713144855.00a5e700@pop.gmx.net>
	<3.0.6.32.20060714202055.00aceea8@pop.gmx.net>
Message-ID: <44B8475E.2070403@vanderbilt.edu>

Heinz Tuechler wrote:
> At 11:02 13.07.2006 -0500, Frank E Harrell Jr wrote:
>> Heinz Tuechler wrote:
>>> At 08:11 13.07.2006 -0500, Frank E Harrell Jr wrote:
>>>> Heinz Tuechler wrote:
>>>>> At 13:14 12.07.2006 -0500, Marc Schwartz (via MN) wrote:
>>>>>> On Wed, 2006-07-12 at 17:41 +0100, Jol, Arne wrote:
>>>>>>> Dear R,
>>>>>>>
>>>>>>> I import data from spss into a R data.frame. On this rawdata I do some
>>>>>>> data processing (selection of observations, normalization, recoding of
>>>>>>> variables etc..). The result is stored in a new data.frame, however, in
>>>>>>> this new data.frame the value labels are lost.
>>>>>>>
>>>>>>> Example of what I do in code:
>>>>>>>
>>>>>>> # read raw data from spss
>>>>>>> rawdata <- read.spss("./data/T50937.SAV",
>>>>>>> 	use.value.labels=FALSE,to.data.frame=TRUE)
>>>>>>>
>>>>>>> # select the observations that we need
>>>>>>> diarydata <- rawdata[rawdata$D22==2 | rawdata$D22==3 |
> rawdata$D22==17 |
>>>>>>> rawdata$D22==18 | rawdata$D22==20 | rawdata$D22==22 |
>>>>>>>  			rawdata$D22==24 | rawdata$D22==33,]
>>>>>>>
>>>>>>> The result is that rawdata$D22 has value labels and that diarydata$D22
>>>>>>> is numeric without value labels.
>>>>>>>
>>>>>>> Question: How can I prevent this from happening?
>>>>>>>
>>>>>>> Thanks in advance!
>>>>>>> Groeten,
>>>>>>> Arne
>>>>>> Two things:
>>>>>>
>>>>>> 1. With respect to your subsetting, your lengthy code can be replaced
>>>>>> with the following:
>>>>>>
>>>>>>  diarydata <- subset(rawdata, D22 %in% c(2, 3, 17, 18, 20, 22, 24, 33))
>>>>>>
>>>>>> See ?subset and ?"%in%" for more information.
>>>>>>
>>>>>>
>>>>>> 2. With respect to keeping the label related attributes, the
>>>>>> 'value.labels' attribute and the 'variable.labels' attribute will not by
>>>>>> default survive the use of "[".data.frame in R (see ?Extract
>>>>>> and ?"[.data.frame").
>>>>>>
>>>>>> On the other hand, based upon my review of ?read.spss, the SPSS value
>>>>>> labels should be converted to the factor levels of the respective
>>>>>> columns when 'use.value.labels = TRUE' and these would survive a
>>>>>> subsetting.
>>>>>>
>>>>>> If you want to consider a solution to the attribute subsetting issue,
>>>>>> you might want to review the following post by Gabor Grothendieck in
>>>>>> May, which provides a possible solution:
>>>>>>
>>>>>>  https://stat.ethz.ch/pipermail/r-help/2006-May/106308.html
>>>>>>
>>>>>> and this post by me, for an explanation of what is happening in Gabor's
>>>>>> solution:
>>>>>>
>>>>>>  https://stat.ethz.ch/pipermail/r-help/2006-May/106351.html
>>>>>>
>>>>>> HTH,
>>>>>>
>>>>>> Marc Schwartz
>>>>>>
>>>>> Hello Mark and Arne,
>>>>>
>>>>> I worked on the suggestions of Gabor and Mark and programmed some
> functions
>>>>> in this way, but they are very, very preliminary (see below).
>>>>> In my view there is a lack of convenient possibilities in R to document
>>>>> empirical data by variable labels, value labels, etc. I would prefer to
>>>>> have these possibilities in the "standard" configuration.
>>>>> So I sketched a concept, but in my view it would only be useful, if there
>>>>> was some acceptance by the core developers of R.
>>>>>
>>>>> The concept would be to define a class. For now I call it "source.data".
>>>>> To design it more flexible than the Hmisc class "labelled" I would
> define a
>>>>> related option "source.data.attributes" with default c('value.labels',
>>>>> 'variable.name', 'label')). This option contains all attributes that
> should
>>>>> persist in subsetting/indexing.
>>>>>
>>>>> I made only some very, very preliminary tests with these functions,
> mainly
>>>>> because I am not happy with defining a new class. Instead I would prefer,
>>>>> if this functionality could be integrated in the Hmisc class "labelled",
>>>>> since this is in my view the best known starting point for data
>>>>> documentation in R.
>>>>>
>>>>> I would be happy, if there were some discussion about the wishes/needs of
>>>>> other Rusers concerning data documentation.
>>>>>
>>>>> Greetings,
>>>>>
>>>>> Heinz
>>>> I feel that separating variable labels and value labels and just using 
>>>> factors for value labels works fine, and I would urge you not to create 
>>>> a new system that will not benefit from the many Hmisc functions that 
>>>> use variable labels and units.  [.data.frame in Hmisc keeps all
> attributes.
>>>> Frank
>>>>
>>> Frank,
>>>
>>> of course I aggree with you about the importance of Hmisc and as I said, I
>>> do not want to define a new class, but in my view factors are no good
>>> substitute for value labels.
>>> As the language definition (version 2.3.1 (2006-06-05) Draft, page 7) says:
>>> "Factors are currently implemented using an integer array to specify the
>>> actual levels and a second array of names that are mapped to the integers.
>>> Rather unfortunately users often make use of the implementation in order to
>>> make some calculations easier." 
>>> So, in my view, the levels represent the "values" of the factor.
>>> This has inconveniencies if you want to use value labels in different
>>> languages. Further I do not see a simple method to label numerical
>>> variables. I often encounter discrete, but still metric data, as e.g. risk
>>> scores. Usually it would be nice to use them in their original coding,
>>> which may include zero or decimal places and to label them at the same
> time.
>>> Personally at the moment I try to solve this problem by following a
>>> suggestion of Martin, Dimitis and others to use names instead. I doubt,
>>> however, that this is a good solution, but at least it makes it possible to
>>> have the source data numerically coded and in this sense "language free"
>>> (see first attempts of functions below).
>>>
>>> Heinz
>>>
>> Those are excellent points Heinz.  I addressed that problem partially in 
>> sas.get - see the sascodes attribute.
>>
>> Frank
>>
> 
> Frank, I looked at your function sas.get. You solved the problem with a lot
> of effort. Don't you think that it would be easier to create just one new
> class, say "documented", which offers the possibility to represent the
> original data as it is and to add all the useful descriptions like variable
> labels, value labels, units, special missing values, and may be others.
> If I remember correctly SAS, SPSS and BMDP offer these possibilities since
> many years, and in my view for good reason. I am thinking about this
> questions since I started using R about two years ago and I wonder, why
> there seems to be so little interest in these questions.
> In my work good documentation of the _unchanged_ data is very important,
> also because it eases checking the data for errors.
> 
> Heinz
> 
> 
>>> ...snip...
> 
> 
> 

Heinz - the code is quite small and simple, not much effort.  And 
variable labels need to be attributes to individual variables, otherwise 
   plotting, latex, and other functions can't get access to them (e.g., 
in Hmisc xYplot(y ~ x) labels for x and y, and units of measurement, get 
plotted on axes.  I've been having all the SAS, SPSS, and BMDP 
capabilities you've mentioned in R/S-Plus (plus units attributes not 
available in those) for years.

What would make all this even easier is for R to be told a list of 
attribute names that would always carry with subsetting, so that 
specially subsetting methods such as [.labeled would not be necessary.

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From ng296 at cam.ac.uk  Sat Jul 15 03:41:11 2006
From: ng296 at cam.ac.uk (N. Goodacre)
Date: 15 Jul 2006 02:41:11 +0100
Subject: [R] R installation - WINDOWS - problem
Message-ID: <Prayer.1.0.17.0607150241110.19998@hermes-1.csi.cam.ac.uk>

Dear R mailing group,

 I am having extreme - and mysterious - trouble installing R. I tried 
simply dl ing R-2.3.1.tar.gz, and unzipping it, but after doing so, nowhere 
could I find a file called R-2.3.1.exe. I did however find an 
R-2.3.1.exe.manifest.

 I therefore tried the command line installation command - given in 
R-admin.pdf. This command is: tar zxvf R-2.3.1.tgz

This is strange because the file I dl'ed was a .tar.gz. I tried this line 
as well as: tar zxvf R-2.3.1.tar.gz. The command was not recognized.

 Can anyone help me with this trange problem?

 Sincerely,

Norman Goodacre


From edd at debian.org  Sat Jul 15 04:00:36 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 14 Jul 2006 21:00:36 -0500
Subject: [R] R installation - WINDOWS - problem
In-Reply-To: <Prayer.1.0.17.0607150241110.19998@hermes-1.csi.cam.ac.uk>
References: <Prayer.1.0.17.0607150241110.19998@hermes-1.csi.cam.ac.uk>
Message-ID: <17592.19524.743344.597034@basebud.nulle.part>


On 15 July 2006 at 02:41, N. Goodacre wrote:
| Dear R mailing group,
| 
|  I am having extreme - and mysterious - trouble installing R. I tried 
| simply dl ing R-2.3.1.tar.gz, and unzipping it, but after doing so, nowhere 
| could I find a file called R-2.3.1.exe. I did however find an 
| R-2.3.1.exe.manifest.
| 
|  I therefore tried the command line installation command - given in 
| R-admin.pdf. This command is: tar zxvf R-2.3.1.tgz
| 
| This is strange because the file I dl'ed was a .tar.gz. I tried this line 
| as well as: tar zxvf R-2.3.1.tar.gz. The command was not recognized.
| 
|  Can anyone help me with this trange problem?

You are download the source, you want the binary. Start here (assumming that
you're actually physically/network-close to cam.ac.uk:

	http://cran.uk.r-project.org/bin/windows/base

aka

	http://www.stats.bris.ac.uk/R/bin/windows/base/

and pick the file R-2.3.1-win32.exe.

Hope this helps, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From ghos0033 at umn.edu  Sat Jul 15 04:24:44 2006
From: ghos0033 at umn.edu (Debarchana Ghosh)
Date: Fri, 14 Jul 2006 22:24:44 -0400
Subject: [R] Ordered Logistic Regression in survey command
Message-ID: <44B851EC.9070205@umn.edu>

Hi,

How can I do ordered logistic regression in svyglm?

Thanks,
D.

-- 
Debarchana Ghosh
Research Assistant
Department of Geography
University of Minnesota
PH: 8143607580
email to: ghos0033 at umn.edu
www.tc.umn.edu/~ghos0033


From dieter.menne at menne-biomed.de  Sat Jul 15 10:40:50 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sat, 15 Jul 2006 08:40:50 +0000 (UTC)
Subject: [R]
	=?utf-8?q?lm=3A_Displaying_95=25_confidence_and_prediction_in?=
	=?utf-8?q?tervals_on=09scatterplots?=
References: <BAY118-F49F13DE25B24B64F0AD65B96C0@phx.gbl>
Message-ID: <loom.20060715T104003-780@post.gmane.org>

jenny tan <jennytimp <at> hotmail.com> writes:

> May I know  how does one superimpose the 95% confidence and prediction 
> intervals on the linear regression line of a scatterplot?

Check

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/8380.html

Dieter


From dieter.menne at menne-biomed.de  Sat Jul 15 10:50:26 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sat, 15 Jul 2006 08:50:26 +0000 (UTC)
Subject: [R] Questions about extract-lme.cov
References: <CED81D34E37D5043A1211565277A51E505435C96@exchkc02.stowers-institute.org>
Message-ID: <loom.20060715T104818-562@post.gmane.org>

Li, Hua <HUL <at> stowers-institute.org> writes:

> I am using extract.lme.cov to extract the covariance matrix of lme. But
> the results are not expected. 
> 
>  b <- lme(travel~1,Rail,~1|Rail)
> 
> The default correlation for lme is no correlation within groups.
> 
> >extract.lme.cov(b,Rail)
> 
> The part of covariance matrix looks like:
> 
>          1        2        3        4        5        6
> 1 631.4778 615.3111 615.3111   0.0000   0.0000   0.0000
> 2 615.3111 631.4778 615.3111   0.0000   0.0000   0.0000
> 3 615.3111 615.3111 631.4778   0.0000   0.0000   0.0000
> 4   0.0000   0.0000   0.0000 631.4778 615.3111 615.3111
> 5   0.0000   0.0000   0.0000 615.3111 631.4778 615.3111
> 6   0.0000   0.0000   0.0000 615.3111 615.3111 631.4778
> 
> Where does the covariance come from? Maybe I'm missing sth here?

Maybe 

VarCorr(b) 

gives all you need, because

extract.lme.cov2(b,Rail,start.level=2)

or even 
extract.lme.cov(b,Rail,start.level=2)

are a bit redundant.

Dieter


From h.wickham at gmail.com  Sat Jul 15 11:03:18 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Sat, 15 Jul 2006 10:03:18 +0100
Subject: [R] lm: Displaying 95% confidence and prediction intervals on
	scatterplots
In-Reply-To: <BAY118-F49F13DE25B24B64F0AD65B96C0@phx.gbl>
References: <BAY118-F49F13DE25B24B64F0AD65B96C0@phx.gbl>
Message-ID: <f8e6ff050607150203k798f69c4uf8cd5bda0c8ff6ce@mail.gmail.com>

> May I know  how does one superimpose the 95% confidence and prediction
> intervals on the linear regression line of a scatterplot?

You could use ggplot:

install.packages("ggplot")
library(ggplot)
qplot(wt, mpg, data=mtcars, type=c("point","smooth"), method=lm)

(which gives the 95% confidence interval)

Hadley


From phdhwang at gmail.com  Sat Jul 15 13:51:45 2006
From: phdhwang at gmail.com (Kum-Hoe Hwang)
Date: Sat, 15 Jul 2006 20:51:45 +0900
Subject: [R] How to Interpret Results of Regression in R
Message-ID: <b040cbb00607150451r3fbbb136gfaa1b0c4637b2f3a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060715/de513ec6/attachment.pl 

From ulriks at ruc.dk  Sat Jul 15 13:55:26 2006
From: ulriks at ruc.dk (Ulrik Stervbo)
Date: Sat, 15 Jul 2006 13:55:26 +0200
Subject: [R] Find peaks in histograms / Analysis of cumulative frequency
Message-ID: <3483f8d50607150455o637381d7s88d113ac5b7a7a5e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060715/43ff3675/attachment.pl 

From AndiBuck at gmx.de  Sat Jul 15 15:10:14 2006
From: AndiBuck at gmx.de (Andreas Beyerlein)
Date: Sat, 15 Jul 2006 15:10:14 +0200
Subject: [R] termplot and ylim
Message-ID: <20060715131014.232320@gmx.net>

Hi together,

I always get an error message with using ylim in termplot(), like this:

> x<-(1:10)
> y<-(10:1)
> l<-lm(y~x)
> termplot(l,ylim=c(1,2))

Is this a bug, or is there another possibility to do that? Especially, I would like to use term.plot() for gamlss objects.

Thanks for your help!
Andreas















-- 


Echte DSL-Flatrate dauerhaft f?r 0,- Euro*!


From ligges at statistik.uni-dortmund.de  Sat Jul 15 16:02:44 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 15 Jul 2006 16:02:44 +0200
Subject: [R] Error in Quantile Regression - Clear Message
In-Reply-To: <OF7891BF03.85B3C118-ON032571AB.005B45AF-032571AB.005BE038@serasa.com.br>
References: <OF7891BF03.85B3C118-ON032571AB.005B45AF-032571AB.005BE038@serasa.com.br>
Message-ID: <44B8F584.2020401@statistik.uni-dortmund.de>

ricardosilva at serasa.com.br wrote:
> Dear Users,
> I loaded my dataset as following:
> 
> presu <- read.table("C:/_Ricardo/Paty/qtdata_f.txt", header=TRUE, sep="\t",
> na.strings="NA", dec=".", strip.white=TRUE)
> dep<-presu[,3];
> exo<-presu[,4:92];
> 
> When I try:
> 
> rq(dep ~ exo, ...) or mle.stepwise(dep ~ exo, ...)
> I got the same error:
>> rq(dep ~ exo)
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  :
>         invalid variable type for 'exo'
> 
> Any hint in how to fix it? I think this is due my data format.


Maybe, but you will have to tell us the class() and mode() of "exo", 
some output of the str() function would be fine.

Uwe Ligges


> Thanks,
> 
> ________________________________________
> Ricardo Gon?alves Silva, M. Sc.
> Apoio aos Processos de Modelagem Matem?tica
> Econometria & Inadimpl?ncia
> Serasa S.A.
> (11) - 6847-8889
> ricardosilva at serasa.com.br
> 
> __________________
> 
> 
> Hi,
> 
> I load my data set and separate it as folowing:
> 
> presu <- read.table("C:/_Ricardo/Paty/qtdata_f.txt", header=TRUE, sep="\t",
> na.strings="NA", dec=".", strip.white=TRUE)
> dep<-presu[,3];
> exo<-presu[,4:92];
> 
> Now, I want to use it using the wls and quantreg packages. How I change the
> data classes for mle and rq objects?
> 
> Thanks a lot,
> Ricardo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
>  PLEASE do read the posting guide!
>  http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ggrothendieck at gmail.com  Sat Jul 15 16:30:28 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 15 Jul 2006 10:30:28 -0400
Subject: [R] termplot and ylim
In-Reply-To: <20060715131014.232320@gmx.net>
References: <20060715131014.232320@gmx.net>
Message-ID: <971536df0607150730k60b593efnfd0886df2a2e7143@mail.gmail.com>

It looks like a bug or at least an omission.

Try the following.  It creates a new termplot function which
in turn defines a plot function which looks for a ylims= arg
and, if present, replaces the ylim= arg with ylims= and
then calls the real plot from the graphics package with the
new set of arguments.

It then defines a new proto object, i.e. an environment,
whose parent is the environment within the new termplot
we are setting up.  It places a copy of termplot from the
stats package in that proto object naming the copy f.
This has the side effect of resetting f''s parent to
the proto object so that when f calls plot it finds
the plot we just defined instead of the usual plot.

Note that one uses ylims= instead of ylim in the call.

termplot <- function(...) {
	plot <- function(...) { # replace ylim= with ylims=
		args <- list(...)
		if ("ylims" %in% names(args)) {
			args$ylim <- args$ylims
			args$ylims <- NULL
		}
		do.call(graphics::plot, args)
	}
	proto(f = stats::termplot)[["f"]](...)
}

# test
library(proto)
L <- lm(y ~ x, data.frame(x = 1:10, y = 10:1))
termplot(L, ylims = 1:2)



On 7/15/06, Andreas Beyerlein <AndiBuck at gmx.de> wrote:
> Hi together,
>
> I always get an error message with using ylim in termplot(), like this:
>
> > x<-(1:10)
> > y<-(10:1)
> > l<-lm(y~x)
> > termplot(l,ylim=c(1,2))
>
> Is this a bug, or is there another possibility to do that? Especially, I would like to use term.plot() for gamlss objects.
>
> Thanks for your help!
> Andreas
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> --
>
>
> Echte DSL-Flatrate dauerhaft f?r 0,- Euro*!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From tura at centroin.com.br  Sat Jul 15 16:50:14 2006
From: tura at centroin.com.br (Bernardo Rangel tura)
Date: Sat, 15 Jul 2006 11:50:14 -0300
Subject: [R] Excel to R
In-Reply-To: <44B37251.2090606@good.ibl.fr>
References: <BAY118-F7C2901D545BC8FD874403B9680@phx.gbl>
	<44B37251.2090606@good.ibl.fr>
Message-ID: <7.0.0.16.2.20060715113721.0526b5a0@centroin.com.br>


>Hi peolple!


I have a many excel tables with mode than 100 variables. And I want 
use R to analize that.

But I have a problem, a group of this variables (more than 50) in any 
table is a factor and other part is a number.

Tha factors variables have tha values enconde this form (1=Yes,2=No and 9 = NA)

Well I use this scripts to import the database

require(RODBC)
channel <- odbcConnectExcel("f:/teste.xls")
data <- sqlFetch(channel, "Sheet1")
  summary(data)
        qw              ee
  Min.   :1.000   Min.   :1.000
  1st Qu.:1.000   1st Qu.:1.500
  Median :1.000   Median :2.000
  Mean   :1.333   Mean   :2.429
  3rd Qu.:1.750   3rd Qu.:3.500
  Max.   :2.000   Max.   :4.000
  NA's   :1.000


But qw is a factor (and is colnum type isvtext)

Is possible modify my script for this utcome

 > summary(data)
     qw          ee
  1   :4   Min.   :1.000
  2   :2   1st Qu.:1.500
  NA's:1   Median :2.000
           Mean   :2.429
           3rd Qu.:3.500
           Max.   :4.000


Thanks in advance

Bernardo Rangel Tura, MD, MSc
National Institute of Cardiology Laranjeiras
Rio de Janeiro Brazil


From rstatistics at gmail.com  Sat Jul 15 17:22:50 2006
From: rstatistics at gmail.com (A.R. Criswell)
Date: Sat, 15 Jul 2006 22:22:50 +0700
Subject: [R] names() function and lmer()
Message-ID: <878be2c60607150822r5914a231h41d537f2f5b446b4@mail.gmail.com>

Hello All,

I would like to retrieve some of the results from the lmer(...)
function in library lme4. If I run a model, say

fm.1 <- lmer(y ~ 1 + (1 | x), data = dog)

and try names(fm.1), I get NULL. Is there anyway to retrieve the information?

Thanks


From spencer.graves at pdf.com  Sat Jul 15 17:32:42 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 15 Jul 2006 08:32:42 -0700
Subject: [R] Multiple tests on 2 way-ANOVA
In-Reply-To: <63E04C5ADEDACB4989972239CDABF05782BED1@chlsne01.nestle.com>
References: <63E04C5ADEDACB4989972239CDABF05782BED1@chlsne01.nestle.com>
Message-ID: <44B90A9A.3040702@pdf.com>

<comments in line>

Grathwohl, Dominik, LAUSANNE, NRC-BAS wrote:
> Dear r-helpers,
> 
> I have a question about multiple testing.
> Here an example that puzzles me:
> All matrixes and contrast vectors are presented in treatment contrasts.
> 
> 1. example:
> library(multcomp)
> n<-60; sigma<-20
> # n = sample size per group
> # sigma standard deviation of the residuals
> 
> cov1 <- matrix(c(3/4,-1/2,-1/2,-1/2,1,0,-1/2,0,1), nrow = 3, ncol=3, byrow=TRUE, 
> 	dimnames = list(c("A", "B", "C"), c("C.1", "C.2", "C.3")))
> # cov1 = variance covariance matrix of the beta coefficients of a 
> # 2x2 factorial design (see Piantadosi 2005, p. 509)
> 
> cm1 <- matrix(c(0, 1, 0, 0, 0, 1), nrow = 2, ncol=3, byrow=TRUE, 
> 	dimnames = list(c("A", "B"), c("C.1", "C.2", "C.3")))
> # cm1 = contrast matrix for main effects
> 
> v1 <- csimint(estpar=c(100, 6, 5), df=4*n-3, covm=cov1*sigma^2/n, cmatrix=cm1, conf.level=0.95)
> summary(v1)
> 
> The adjusted p-values are almost the Bonferroni p-values.
> If I understood right: You need not to adjust for multiple testing 
> on main effects in a 2x2 factorial design 
> assuming the absence of interaction. 

SG:  Where did you get this idea?  A p value of 0.05 says that if the 
null hypothesis of no effect is true, a result at least as extreme as 
that observed work actually occur with probability 0.05.  Thus, with 2 
independent tests, the probability of getting a result at least that 
extreme in one or both of the tests is 1-(1-0.05)^2 = 0.0975, which is 
almost 2*0.05.  Thus, if I were to consider only main effects in a 2x2 
factorial design, this is what I would get from Bonferroni.

> I do not think that there is a bug, 
> I want to understand, why multcomp does adjust for multiple tests 
> having all information about the design of the trial (variance covariance matrix)?
> Or do I have to introduce somehow more information?
> 
> 2. example:
> And I have second question: How do I proper correct for multiple testing 
> if I want to estimate in the presence of interaction the two average main effects.
> Can some one point me to some literature where I can learn these things?
> Here the example, 2x2 factorial with interaction, estimation of average main effects:
> 
> cov2 <- matrix(
> c(1,-1,-1, 1,
>  -1, 2, 1,-2,
>  -1, 1, 2,-2,
>   1,-2,-2, 4)
> , nrow=4, ncol=4, byrow=TRUE)
> cm2 <- matrix(c(0, 1, 0, 1/2, 0, 0, 1, 1/2), nrow = 2, ncol=4, byrow=TRUE, 
> 	dimnames = list(c("A", "B"), c("C.1", "C.2", "C.3", "C.4")))
> v2 <- csimint(estpar=c(100, 6, 5, 2), df=4*n-4, covm=cov2*sigma^2/n, cmatrix=cm2, conf.level=0.95)
> summary(v2)

SG:  The Bonferroni p-value is the observed times the number of rows in 
the contrast matrix.  The number of columns is irrelevant to Bonferroni.

SG:  I'm not sure, but I believe that the the adjusted p value would 
likely be close to (if not exactly) the rank of the contrast matrix; 
given the rank, it is (I think) independent of the number of rows and 
columns.

SG:  These two assertions are consistent with the following example, 
where I increase the number of dimensions by a factor of 4 without 
changing the rank.  The Bonferroni p value increased by a factor of 4 
while the adjusted p value did not change, as predicted.

cm2.4 <- rbind(cm2, cm2, cm2, cm2)
v2.4 <- csimint(estpar=c(100, 6, 5, 2), df=4*n-4,
               covm=cov2*sigma^2/n,
               cmatrix=cm2.4, conf.level=0.95)
summary(v2.4)
> 
> I do not believe that this is the most efficient way for doing this, 
> since I made already bad experience with the first example.

SG:  I hope this reply converts the "bad experience" to "good".  As for 
efficiency, you did very well by including such simple but elegant 
examples.  Your post might have more efficiently elicited more and more 
elegant responses sooner with a more carefully chosen Subject, perhaps 
like "Multiple Comparisons Questions".  However, the selection of a 
possible better subject might rely on information you didn't have.

Hope this helps.
Spencer Graves
> 
> My R.version:
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    2.1            
> year     2005           
> month    12             
> day      20             
> svn rev  36812          
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From bates at stat.wisc.edu  Sat Jul 15 18:29:23 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 15 Jul 2006 11:29:23 -0500
Subject: [R] names() function and lmer()
In-Reply-To: <878be2c60607150822r5914a231h41d537f2f5b446b4@mail.gmail.com>
References: <878be2c60607150822r5914a231h41d537f2f5b446b4@mail.gmail.com>
Message-ID: <40e66e0b0607150929v26ade2eo21702cab7f3fde4b@mail.gmail.com>

On 7/15/06, A.R. Criswell <rstatistics at gmail.com> wrote:
> Hello All,
>
> I would like to retrieve some of the results from the lmer(...)
> function in library lme4. If I run a model, say
>
> fm.1 <- lmer(y ~ 1 + (1 | x), data = dog)
>
> and try names(fm.1), I get NULL. Is there anyway to retrieve the information?

Yes.

The recommended way of retrieving information form a fitted model
object like an lmer object is with extractor functions.  See

?lmer-class

for a list of such methods.  That help page also documents the slots
in the object.  The lmer class is an S4 class and uses typed slots
instead of  named components.

The str function displays the structure of pretty well any type of R
object, including S3 classed objects or S4 classed objects.  That is
my favorite way of checking the structure of an object.

Please remember that it is risky to count on being able to reach in to
an object and pull out slots or components and operate on them.  The
names and contents of slots are not guaranteed to stay constant.  The
lme4 and Matrix packages have been under development for a long time
and should continue to be regarded as under development.  When we
change the internal representation we do change the extractor
functions accordingly.  It is a bug if an internal change causes an
extractor function in the package to fail to return correct results.
It is not a bug if an internal change causes your code that assumes a
particular, obsolete representation to break.


From ronggui.huang at gmail.com  Sat Jul 15 18:31:39 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Sun, 16 Jul 2006 00:31:39 +0800
Subject: [R] names() function and lmer()
In-Reply-To: <878be2c60607150822r5914a231h41d537f2f5b446b4@mail.gmail.com>
References: <878be2c60607150822r5914a231h41d537f2f5b446b4@mail.gmail.com>
Message-ID: <38b9f0350607150931m3a777b69ud01dde33b88d0bd@mail.gmail.com>

lme4 is S4 package.So you should use slotNames to see what slots an
object has,and use @ instead of $ to extract the slot you want.
> library(lme4)
Loading required package: Matrix
Loading required package: lattice
Loading required package: lattice
> example(lmer)
> slotNames(fm2)
 [1] "assign"   "frame"    "terms"    "flist"    "Zt"       "X"
 [7] "y"        "wts"      "wrkres"   "method"   "useScale" "family"
[13] "call"     "cnames"   "nc"       "Gp"       "XtX"      "ZtZ"
[19] "ZtX"      "Zty"      "Xty"      "Omega"    "L"        "RZX"
[25] "RXX"      "rZy"      "rXy"      "devComp"  "deviance" "fixef"
[31] "ranef"    "RZXinv"   "bVar"     "gradComp" "status"
> fm2 at ranef
 [1]   1.5127787 -40.3739988 -39.1811143  24.5188772  22.9144206   9.2219771
 [7]  17.1561300  -7.4517287   0.5786303  34.7680264 -25.7543295 -13.8649853
[13]   4.9159565  20.9290870   3.2586571 -26.4758005   0.9056393  12.4217767
[19]   9.3234692  -8.5991469  -5.3877753  -4.9686374  -3.1939301  -0.3084938
[25]  -0.2872083   1.1159883 -10.9059430   8.6275943   1.2806874   6.7563873
[31]  -3.0751268   3.5122002   0.8730485   4.9837782  -1.0052909   1.2583989

2006/7/15, A.R. Criswell <rstatistics at gmail.com>:
> Hello All,
>
> I would like to retrieve some of the results from the lmer(...)
> function in library lme4. If I run a model, say
>
> fm.1 <- lmer(y ~ 1 + (1 | x), data = dog)
>
> and try names(fm.1), I get NULL. Is there anyway to retrieve the information?
>
> Thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


-- 
??????
Department of Sociology
Fudan University


From spencer.graves at pdf.com  Sat Jul 15 19:05:03 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 15 Jul 2006 10:05:03 -0700
Subject: [R] storing the estimates from lmer
In-Reply-To: <bb47cf700607110818m782a98c5le08396d2fca203a@mail.gmail.com>
References: <bb47cf700607110818m782a98c5le08396d2fca203a@mail.gmail.com>
Message-ID: <44B9203F.6090006@pdf.com>

	  The structure of 'lmer' objects and helper functions is outlined in 
the 'lmer' and 'lmer-class' help pages.  The latter mentions "'vcov 
'signature(object = "mer")': Calculate variance-covariance matrix of the 
_fixed_ effect terms, see also 'vcov'."  Thus, 
sqrt(diag(vcov(lmer.object))) should give you standard errors of the 
fixed effects.

	  The parameter estimates can be obtained using 'VarCorr'.  However, 
characterizing their random variability is harder, because their 
distribution is not as simply summarized.  The 'lmer-class' help page 
says that an 'lmer' object includes a slot, "'Omega': A list of 
positive-definite matrices stored as '"dpoMatrix"' objects that are the 
relative precision matrices of the random effects associated with each 
of the grouping factors."  However, I don't know how to use this.  For 
problems like this, the 'lme4' and 'Matrix' packages include a function 
'simulate', which is what I might use, at least until I figured out how 
to get essentially the same answer from the Omega slot.

	  Hope this helps,
	  Spencer Graves

prabhu bhaga wrote:
> Dear all,
> 
> I'm trying to store/extract the mean& standard error of the fixed effects
> parameter and the variance of the random effects parameter from "lmer"
> procedure from mlmre4 package developed by bates n pinheiro. while storing
> fixed effects parameter is straight forward, the same is not true for
> storing the variance parameter of the random effects. kindly help me
> 
> ~prabhu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From spencer.graves at pdf.com  Sat Jul 15 19:11:25 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 15 Jul 2006 10:11:25 -0700
Subject: [R] storing the estimates from lmer
In-Reply-To: <bb47cf700607110818m782a98c5le08396d2fca203a@mail.gmail.com>
References: <bb47cf700607110818m782a98c5le08396d2fca203a@mail.gmail.com>
Message-ID: <44B921BD.8090401@pdf.com>

p.s.  I intended to include the following extension to an example from 
the 'lmer' help page:

fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
(fm1.fix <- fixef(fm1))
(fm1.fix.se <- sqrt(diag(vcov(fm1))))
fm1.fix/fm1.fix.se

fm1.ran <- VarCorr(fm1)
diag(fm1.ran$Subject)

########################
	  The structure of 'lmer' objects and helper functions is outlined in
the 'lmer' and 'lmer-class' help pages.  The latter mentions "'vcov
'signature(object = "mer")': Calculate variance-covariance matrix of the
_fixed_ effect terms, see also 'vcov'."  Thus,
sqrt(diag(vcov(lmer.object))) should give you standard errors of the
fixed effects.

	  The parameter estimates can be obtained using 'VarCorr'.  However,
characterizing their random variability is harder, because their
distribution is not as simply summarized.  The 'lmer-class' help page
says that an 'lmer' object includes a slot, "'Omega': A list of
positive-definite matrices stored as '"dpoMatrix"' objects that are the
relative precision matrices of the random effects associated with each
of the grouping factors."  However, I don't know how to use this.  For
problems like this, the 'lme4' and 'Matrix' packages include a function
'simulate', which is what I might use, at least until I figured out how
to get essentially the same answer from the Omega slot.

	  Hope this helps,
	  Spencer Graves

prabhu bhaga wrote:
> Dear all,
> 
> I'm trying to store/extract the mean& standard error of the fixed effects
> parameter and the variance of the random effects parameter from "lmer"
> procedure from mlmre4 package developed by bates n pinheiro. while storing
> fixed effects parameter is straight forward, the same is not true for
> storing the variance parameter of the random effects. kindly help me
> 
> ~prabhu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From bates at stat.wisc.edu  Sat Jul 15 20:39:43 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 15 Jul 2006 13:39:43 -0500
Subject: [R] storing the estimates from lmer
In-Reply-To: <44B921BD.8090401@pdf.com>
References: <bb47cf700607110818m782a98c5le08396d2fca203a@mail.gmail.com>
	<44B921BD.8090401@pdf.com>
Message-ID: <40e66e0b0607151139i7350d22bteacfaa61aa94a1f8@mail.gmail.com>

Hi Spencer,

On 7/15/06, Spencer Graves <spencer.graves at pdf.com> wrote:
> p.s.  I intended to include the following extension to an example from
> the 'lmer' help page:
>
> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> (fm1.fix <- fixef(fm1))
> (fm1.fix.se <- sqrt(diag(vcov(fm1))))
> fm1.fix/fm1.fix.se
>
> fm1.ran <- VarCorr(fm1)
> diag(fm1.ran$Subject)

I'm confident that you are aware that sqrt(diag(vcov(fm1))) and
sqrt(diag(fm1.ran$Subject)) refer to different parameters the model.
However, some readers of your reply may not see the distinction.
Allow me to try to clarify.

The summary for this fitted model is

lmer> (fm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy))
Linear mixed-effects model fit by REML
Formula: Reaction ~ Days + (Days | Subject)
	  Data: sleepstudy
      AIC      BIC    logLik MLdeviance REMLdeviance
 1753.628 1769.593 -871.8141   1751.986     1743.628
Random effects:
 Groups   Name        Variance Std.Dev. Corr
 Subject  (Intercept) 612.090  24.7405
          Days         35.072   5.9221  0.066
 Residual             654.941  25.5918
number of obs: 180, groups: Subject, 18

Fixed effects:
            Estimate Std. Error t value
(Intercept) 251.4051     6.8246  36.838
Days         10.4673     1.5458   6.771

Correlation of Fixed Effects:
     (Intr)
Days -0.138

The fixed effects described in the lower part of this summary are a
familiar type of parameter in a statistical model.  They are
coefficients in a linear predictor.  We produce estimates of these
parameters and also provide a measure of the precision of these
estimates - their standard errors.  The vcov generic function returns
an estimate of the precision of the estimated parameters (typically
parameters that are coefficients in a linear predictor).  Thus

> sqrt(diag(vcov(fm1)))
[1] 6.824558 1.545789

provides the standard errors of the fixed effects estimates.

The random effects, although they also appear in the linear predictor,
are not formally parameters in the model.  They are unobserved random
variables whose variance covariance matrix has a known form but
unknown value.  It is the values that determine the
variance-covariance matrix that are parameters in the model.  These
parameters are returned by VarCorr

> VarCorr(fm1)
$Subject
2 x 2 Matrix of class "dpoMatrix"
            (Intercept)     Days
(Intercept)   612.09032  9.60428
Days            9.60428 35.07165

attr(,"sc")
[1] 25.59182

In other words, the 2 x 2 matrix shown above is the estimate of the
variance-covariance matrix of the random effects associated with the
grouping factor "Subject".

Thus

> sqrt(diag(VarCorr(fm1)$Subject))
[1] 24.740459  5.922132

gives the estimated standard deviations of the random effects.  These
are estimates of parameters in the model.  They are *not* standard
errors of parameter estimates.  The lmer function and related software
does not return standard errors of the estimates of variance
components.  This is intentional.  Below I give my familiar rant on
why I think returning such standard errors is a bad practice.  I
encourage users of lmer who wish to determine the precision of the
estimates of the variance components to create a Markov chain Monte
Carlo sample of the parameters and evaluate the HPDintervals.

> sm1 <- mcmcsamp(fm1, 50000)
> library(coda)
Warning message:
use of NULL environment is deprecated
> HPDinterval(sm1)
                          lower        upper
(Intercept)         236.6518363  266.5465536
Days                  7.0136243   13.8947993
log(sigma^2)          6.2550082    6.7295329
log(Sbjc.(In))        5.4928205    7.5751372
log(Sbjc.Days)        2.8197523    4.6337518
atanh(Sbj.(I).Dys)   -0.6988632    0.9836688
deviance           1752.5158501 1766.6461469
attr(,"Probability")
[1] 0.95

<rant>
Some software, notably SAS PROC MIXED, does produce standard errors
for the estimates of variances and covariances of random effects.  In
my opinion this is more harmful than helpful.  The only use I can
imagine for such standard errors is to form confidence intervals or to
evaluate a z-statistic or something like that to be used in a
hypothesis test.  However, those uses require that the distribution of
the parameter estimate be symmetric, or at least approximately
symmetric, and we know that the distribution of the estimate of a
variance component is more like a scaled chi-squared distribution
which is anything but symmetric.  It is misleading to attempt to
summarize our information about a variance component by giving only
the estimate and a standard error of the estimate.
</rant>
>
> ########################
>           The structure of 'lmer' objects and helper functions is outlined in
> the 'lmer' and 'lmer-class' help pages.  The latter mentions "'vcov
> 'signature(object = "mer")': Calculate variance-covariance matrix of the
> _fixed_ effect terms, see also 'vcov'."  Thus,
> sqrt(diag(vcov(lmer.object))) should give you standard errors of the
> fixed effects.
>
>           The parameter estimates can be obtained using 'VarCorr'.  However,
> characterizing their random variability is harder, because their
> distribution is not as simply summarized.  The 'lmer-class' help page
> says that an 'lmer' object includes a slot, "'Omega': A list of
> positive-definite matrices stored as '"dpoMatrix"' objects that are the
> relative precision matrices of the random effects associated with each
> of the grouping factors."  However, I don't know how to use this.  For
> problems like this, the 'lme4' and 'Matrix' packages include a function
> 'simulate', which is what I might use, at least until I figured out how
> to get essentially the same answer from the Omega slot.
>
>           Hope this helps,
>           Spencer Graves
>
> prabhu bhaga wrote:
> > Dear all,
> >
> > I'm trying to store/extract the mean& standard error of the fixed effects
> > parameter and the variance of the random effects parameter from "lmer"
> > procedure from mlmre4 package developed by bates n pinheiro. while storing
> > fixed effects parameter is straight forward, the same is not true for
> > storing the variance parameter of the random effects. kindly help me
> >
> > ~prabhu
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From bi-info at home.nl  Sat Jul 15 20:48:50 2006
From: bi-info at home.nl (Bi-Info (http://members.home.nl/bi-info))
Date: Sat, 15 Jul 2006 20:48:50 +0200
Subject: [R] Some problems with latex(ftable)
In-Reply-To: <44B37251.2090606@good.ibl.fr>
References: <BAY118-F7C2901D545BC8FD874403B9680@phx.gbl>	<44B37251.2090606@good.ibl.fr>
Message-ID: <44B93892.8080409@home.nl>

Dear Users,

I've got some problems with LaTeX from the Hmisc package.

I write a table to LaTeX from ftable. The table is correctly converted 
but the headers vanish. What should I do to copy the headers of the 
table into LaTeX?

Thanks,

Wilfred


From andrej.kastrin at siol.net  Sat Jul 15 21:03:40 2006
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Sat, 15 Jul 2006 21:03:40 +0200
Subject: [R] intersect() question
Message-ID: <44B93C0C.3090202@siol.net>

Hi,

I have a matrix containing ID numbers in each column. I would like to 
program function which calculate common number of ID numbers between 
each pair of columns.

Suppose:

5 6 7
1 5 3
6 7 2

Then the result should be:

0 2 0
2 0 1
0 1 0

The main problem is how to implement intersect() function to walk 
through each pair of columns and write result to result matrix.

Thanks in advance for any suggestion, Andrej


From ggrothendieck at gmail.com  Sat Jul 15 21:35:33 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 15 Jul 2006 15:35:33 -0400
Subject: [R] intersect() question
In-Reply-To: <44B93C0C.3090202@siol.net>
References: <44B93C0C.3090202@siol.net>
Message-ID: <971536df0607151235t10d4ba48uc5f6609b05b7450e@mail.gmail.com>

Define a generalized crossproduct and then apply it with
the indicated function.  Multiply the diagonal elements by
zero as the sample output seems to be forcing them that way.

mm <- matrix(c(5, 1, 6, 6, 5, 7, 7, 3, 2), 3) # test matrix

# generalized crossproduct
inner <- function(a,b=a,f=crossprod)
	apply(b,2,function(x)apply(a,2,function(y)f(x,y)))

inner(mm, f = function(x,y) length(intersect(x,y))) * !diag(ncol(mm))



On 7/15/06, Andrej Kastrin <andrej.kastrin at siol.net> wrote:
> Hi,
>
> I have a matrix containing ID numbers in each column. I would like to
> program function which calculate common number of ID numbers between
> each pair of columns.
>
> Suppose:
>
> 5 6 7
> 1 5 3
> 6 7 2
>
> Then the result should be:
>
> 0 2 0
> 2 0 1
> 0 1 0
>
> The main problem is how to implement intersect() function to walk
> through each pair of columns and write result to result matrix.
>
> Thanks in advance for any suggestion, Andrej
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From andrej.kastrin at siol.net  Sat Jul 15 22:11:07 2006
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Sat, 15 Jul 2006 22:11:07 +0200
Subject: [R] intersect() question
In-Reply-To: <971536df0607151235t10d4ba48uc5f6609b05b7450e@mail.gmail.com>
References: <44B93C0C.3090202@siol.net>
	<971536df0607151235t10d4ba48uc5f6609b05b7450e@mail.gmail.com>
Message-ID: <44B94BDB.4000402@siol.net>

Gabor Grothendieck wrote:
> Define a generalized crossproduct and then apply it with
> the indicated function.  Multiply the diagonal elements by
> zero as the sample output seems to be forcing them that way.
>
> mm <- matrix(c(5, 1, 6, 6, 5, 7, 7, 3, 2), 3) # test matrix
>
> # generalized crossproduct
> inner <- function(a,b=a,f=crossprod)
>     apply(b,2,function(x)apply(a,2,function(y)f(x,y)))
>
> inner(mm, f = function(x,y) length(intersect(x,y))) * !diag(ncol(mm))
>
>
>
> On 7/15/06, Andrej Kastrin <andrej.kastrin at siol.net> wrote:
>> Hi,
>>
>> I have a matrix containing ID numbers in each column. I would like to
>> program function which calculate common number of ID numbers between
>> each pair of columns.
>>
>> Suppose:
>>
>> 5 6 7
>> 1 5 3
>> 6 7 2
>>
>> Then the result should be:
>>
>> 0 2 0
>> 2 0 1
>> 0 1 0
>>
>> The main problem is how to implement intersect() function to walk
>> through each pair of columns and write result to result matrix.
>>
>> Thanks in advance for any suggestion, Andrej
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
Thanks for fine solution.


From f.harrell at vanderbilt.edu  Sat Jul 15 22:24:39 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 15 Jul 2006 15:24:39 -0500
Subject: [R] Some problems with latex(ftable)
In-Reply-To: <44B93892.8080409@home.nl>
References: <BAY118-F7C2901D545BC8FD874403B9680@phx.gbl>	<44B37251.2090606@good.ibl.fr>
	<44B93892.8080409@home.nl>
Message-ID: <44B94F07.8030100@vanderbilt.edu>

Bi-Info (http://members.home.nl/bi-info) wrote:
> Dear Users,
> 
> I've got some problems with LaTeX from the Hmisc package.
> 
> I write a table to LaTeX from ftable. The table is correctly converted 
> but the headers vanish. What should I do to copy the headers of the 
> table into LaTeX?
> 
> Thanks,
> 
> Wilfred

I'm not sure what you mean by ftable, and you did not include the code 
you are trying to run.  I suspect you are trying to latex( ) something 
other than a list, matrix, or data frame and that you will have to use 
arguments to latex.default to get the desired result.  You may have to 
write your own latex method.  As an example print 
latex.summary.formula.cross.
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From spencer.graves at pdf.com  Sat Jul 15 20:30:50 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 15 Jul 2006 11:30:50 -0700
Subject: [R] AICc vs AIC for model selection
In-Reply-To: <20060712135834.75532.qmail@web37611.mail.mud.yahoo.com>
References: <20060712135834.75532.qmail@web37611.mail.mud.yahoo.com>
Message-ID: <44B9345A.8030900@pdf.com>

	   Regarding AIC.c, have you tried RSiteSearch("AICc") and 
RSiteSearch("AIC.c")?  This produced several comments that looked to me 
like they might help answer your question.   Beyond that, I've never 
heard of the "forecast" package, and I got zero hits for 
RSiteSearch("best.arima"), so I can't comment directly on your question.

	  Do you have only one series or multiple?  If you have only one, I 
think it would be hard to justify more than a simple AR(1) model. 
Almost anything else would likely be overfitting.

	  If you have multiple series, have you considered using 'lme' in the 
'nlme' package?  Are you familiar with Pinheiro and Bates (2000) 
Mixed-Effects Models in S and S-Plus (Springer)?  If not, I encourage 
you to spend some quality time with this book.  My study of it has been 
amply rewarded, and I believe yours will likely also.

	  Best Wishes,
	  Spencer Graves

Sachin J wrote:
> Hi,
>    
>   I am using 'best.arima' function from forecast package 
to obtain point forecast for a time series data set. The
documentation says it utilizes AIC value to select best ARIMA
model. But in my case the sample size very small - 26
observations (demand data). Is it the right to use AIC value for
model selection in this case. Should I use AICc instead of AIC.
If so how can I modify best.arima function to change the selection
creteria? Any pointers would be of great help.
>    
>   Thanx in advance.
>    
>   Sachin
>    
>    
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From spencer.graves at pdf.com  Sat Jul 15 20:44:33 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 15 Jul 2006 11:44:33 -0700
Subject: [R] DTW - dynamic time warping - and time series in R
In-Reply-To: <Pine.SGI.4.60.0607121628360.19268155@aisa.fi.muni.cz>
References: <Pine.SGI.4.60.0607121628360.19268155@aisa.fi.muni.cz>
Message-ID: <44B93791.2060105@pdf.com>

	  I found references on Google but not RSiteSearch for dynamic time 
warping.

	  What do you want to do?  Unless you've received a conflicting reply 
that I haven't seen, it looks like you would have to code it yourself. 
If you can find something written in another language, you could either 
translate that into R or like from R to it.

	  Why do you want dynamic time warping?  If you want to do research in 
it, then I suspect you would be best creating your own code.  If you 
want to try it as a solution to some other problem, I suggest you 
reconsider the other problem:  What are your real objectives?

	  If you'd like further help from this listserve, please submit another 
post.  First, however, I suggest you read the posting guide! 
"www.R-project.org/posting-guide.html".  There is substantial logic and 
anecdotal evidence to suggest that posts more consistent with that guide 
are more likely to get more useful replies quicker.

	  Hope this helps.
	  Spencer Graves	

Ondrej Vyborny wrote:
> Hello,
> 
> can anybody tell me if there exists functions for DTW in R? I didn't find 
> anything at CRAN's search page... Also any information about packages 
> for time series preprocessing (for pattern matching) would be useful...
> 
> Thanks a lot,
> 
> ondra
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From rmh at temple.edu  Sat Jul 15 23:03:57 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Sat, 15 Jul 2006 17:03:57 -0400 (EDT)
Subject: [R] Some problems with latex(ftable)
Message-ID: <20060715170357.BEF63228@po-d.temple.edu>

The ftable structure is not an ordinary matrix.  Instead, it has the
body of the table with several cbind- and rbind-ed rows and columns of
label information.  The example in ?ftable has two row factors and two
column factors.

Continuing with the example in ?ftable, enter


tmp <- ftable(mtcars$cyl, mtcars$vs, mtcars$am, mtcars$gear, row.vars = c(2, 4), 
              dnn = c("Cylinders", "V/S", "Transmission", "Gears"))

print.default(tmp)

To get what you are looking for, you will need to intercept write.ftable
with, for example,

trace(write.ftable, exit=recover)

then do 

3
tmp.latex <- latex(t(x))
print.default(tmp.latex)

Now open up t.latex and prepend
\documentstyle{article}
\begin{document}

and append
\end{document}

then latex it.

This gets you close to what you want and you can work with the generated
t.tex file to get the rest of the detail.  Or you can work with the
numerous arguments we built into latex (see ?latex) to get some of them
automatically generated.

tmp2.latex <- latex(t(x), col.just=rep(c("l","r"), c(3,6)),
                    n.rgroup=c(3,6), file="t2.tex")


Now open up t2.latex and pre- and append the latex controls to it.


This works well for one or two examples.  To do many, then you will
need to follow Frank's suggestion and build all of this into a method.
Once the method works well, send it to Frank and he will consider
including it in the next release (Frank, I hope that's a fair offer I
make for you to do).


This raises a question from me to the R developers.  I would have
written write.ftable to return t(x), not ox.  print is constrained to
return its argument.  I thought write had more freedom.  Then I would
respecify

print.ftable <- function (x, digits = getOption("digits"), ...) {
     write.ftable(x, quote = FALSE, digits = digits)
     invisible(x)
}

Had this been done then the current task would simplify to

latex(write(tmp))

Te question is: why was write.ftable designed to follow the
print.ftable constraint on returned value.

ps. I just designed the method.  Take write.ftable, drop the last line
and give it a new name.  then you can latex the output of that new
function.

Rich


From rcoppock at cox.net  Sat Jul 15 23:24:31 2006
From: rcoppock at cox.net (Roger Coppock)
Date: Sat, 15 Jul 2006 14:24:31 -0700
Subject: [R] last OSX 3.9 release
Message-ID: <e9035e71a359ec9c8b2132f8d9d23b35@cox.net>

I am looking for ready to install binaries of the last,
and I assume most stable, version of "R" for OSX 3.9.
I believe this is 2.2.1.  I have downloaded the sources,
but to not have to time or resources to compile them.

-.-. --.- Roger Coppock <rcoppock at cox.net>


From kjetilbrinchmannhalvorsen at gmail.com  Sun Jul 16 00:05:44 2006
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Sat, 15 Jul 2006 18:05:44 -0400
Subject: [R] AICc vs AIC for model selection
In-Reply-To: <44B9345A.8030900@pdf.com>
References: <20060712135834.75532.qmail@web37611.mail.mud.yahoo.com>
	<44B9345A.8030900@pdf.com>
Message-ID: <44B966B8.7050606@gmail.com>

Spencer Graves wrote:
> 	   Regarding AIC.c, have you tried RSiteSearch("AICc") and 
> RSiteSearch("AIC.c")?  This produced several comments that looked to me 
> like they might help answer your question.   Beyond that, I've never 
> heard of the "forecast" package, and I got zero hits for 
> RSiteSearch("best.arima"), so I can't comment directly on your question.

The forecast package is at
http://www-personal.buseco.monash.edu.au/~hyndman/Rlibrary/forecast/

Kjetil

> 
> 	  Do you have only one series or multiple?  If you have only one, I 
> think it would be hard to justify more than a simple AR(1) model. 
> Almost anything else would likely be overfitting.
> 
> 	  If you have multiple series, have you considered using 'lme' in the 
> 'nlme' package?  Are you familiar with Pinheiro and Bates (2000) 
> Mixed-Effects Models in S and S-Plus (Springer)?  If not, I encourage 
> you to spend some quality time with this book.  My study of it has been 
> amply rewarded, and I believe yours will likely also.
> 
> 	  Best Wishes,
> 	  Spencer Graves
> 
> Sachin J wrote:
>> Hi,
>>    
>>   I am using 'best.arima' function from forecast package 
> to obtain point forecast for a time series data set. The
> documentation says it utilizes AIC value to select best ARIMA
> model. But in my case the sample size very small - 26
> observations (demand data). Is it the right to use AIC value for
> model selection in this case. Should I use AICc instead of AIC.
> If so how can I modify best.arima function to change the selection
> creteria? Any pointers would be of great help.
>>    
>>   Thanx in advance.
>>    
>>   Sachin
>>    
>>    
>>
>>  		
>> ---------------------------------
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From spencer.graves at pdf.com  Sun Jul 16 01:07:34 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 15 Jul 2006 16:07:34 -0700
Subject: [R] AICc vs AIC for model selection
In-Reply-To: <44B966B8.7050606@gmail.com>
References: <20060712135834.75532.qmail@web37611.mail.mud.yahoo.com>
	<44B9345A.8030900@pdf.com> <44B966B8.7050606@gmail.com>
Message-ID: <44B97536.6000807@pdf.com>

Hi, Kjetil:  Thanks.  Spencer Graves

Kjetil Brinchmann Halvorsen wrote:
> Spencer Graves wrote:
>>        Regarding AIC.c, have you tried RSiteSearch("AICc") and 
>> RSiteSearch("AIC.c")?  This produced several comments that looked to 
>> me like they might help answer your question.   Beyond that, I've 
>> never heard of the "forecast" package, and I got zero hits for 
>> RSiteSearch("best.arima"), so I can't comment directly on your question.
> 
> The forecast package is at
> http://www-personal.buseco.monash.edu.au/~hyndman/Rlibrary/forecast/
> 
> Kjetil
> 
>>
>>       Do you have only one series or multiple?  If you have only one, 
>> I think it would be hard to justify more than a simple AR(1) model. 
>> Almost anything else would likely be overfitting.
>>
>>       If you have multiple series, have you considered using 'lme' in 
>> the 'nlme' package?  Are you familiar with Pinheiro and Bates (2000) 
>> Mixed-Effects Models in S and S-Plus (Springer)?  If not, I encourage 
>> you to spend some quality time with this book.  My study of it has 
>> been amply rewarded, and I believe yours will likely also.
>>
>>       Best Wishes,
>>       Spencer Graves
>>
>> Sachin J wrote:
>>> Hi,
>>>      I am using 'best.arima' function from forecast package 
>> to obtain point forecast for a time series data set. The
>> documentation says it utilizes AIC value to select best ARIMA
>> model. But in my case the sample size very small - 26
>> observations (demand data). Is it the right to use AIC value for
>> model selection in this case. Should I use AICc instead of AIC.
>> If so how can I modify best.arima function to change the selection
>> creteria? Any pointers would be of great help.
>>>      Thanx in advance.
>>>      Sachin
>>>      
>>>         
>>> ---------------------------------
>>>
>>>     [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>


From hodgess at gator.dt.uh.edu  Sun Jul 16 01:44:00 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Sat, 15 Jul 2006 18:44:00 -0500
Subject: [R]  put R on a web server
Message-ID: <200607152344.k6FNi0Om030065@gator.dt.uh.edu>

Dear R People:

Has anyone put R on a web server any time, recently, please?
(Red Hat Linux)


The University of Montana put a version up in 2003, but
I was wondering if anyone had done so, please?

Also, where would I find information on such an installation, please?

thanks,
Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu


From maj at waikato.ac.nz  Sun Jul 16 06:12:07 2006
From: maj at waikato.ac.nz (Murray Jorgensen)
Date: Sun, 16 Jul 2006 16:12:07 +1200
Subject: [R] princomp and eigen
Message-ID: <44B9BC97.6080207@waikato.ac.nz>

Consider the following output [R2.2.0; Windows XP]

 > set.seed(160706)
 > X <- matrix(rnorm(40),nrow=10,ncol=4)
 > Xpc <- princomp(X,cor=FALSE)
 > summary(Xpc,loadings=TRUE, cutoff=0)
Importance of components:
                           Comp.1    Comp.2    Comp.3     Comp.4
Standard deviation     1.2268300 0.9690865 0.7918504 0.55295970
Proportion of Variance 0.4456907 0.2780929 0.1856740 0.09054235
Cumulative Proportion  0.4456907 0.7237836 0.9094576 1.00000000

Loadings:
      Comp.1 Comp.2 Comp.3 Comp.4
[1,] -0.405 -0.624  0.466  0.479
[2,] -0.199 -0.636 -0.346 -0.660
[3,]  0.884 -0.443  0.023  0.148
[4,]  0.122  0.099  0.814 -0.559
 > eigen(var(X))
$values
[1] 1.6723465 1.0434763 0.6966967 0.3397382

$vectors
            [,1]       [,2]        [,3]       [,4]
[1,] -0.4048158  0.6240510  0.46563382  0.4794473
[2,] -0.1994853  0.6361009 -0.34634256 -0.6600213
[3,]  0.8839775  0.4429553  0.02261302  0.1478618
[4,]  0.1221215 -0.0986234  0.81407655 -0.5591414


I would have expected the princomp component standard deviations to be 
the square roots of the eigen() $values and they clearly are not.

Murray Jorgensen

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862


From spencer.graves at pdf.com  Sun Jul 16 06:23:27 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 15 Jul 2006 21:23:27 -0700
Subject: [R] Prediction interval of Y using BMA
In-Reply-To: <859726A76C47574081B445F2D0BD25FF28C4E4@PEGASUS.hus.chru-strasbourg.fr>
References: <859726A76C47574081B445F2D0BD25FF28C4E4@PEGASUS.hus.chru-strasbourg.fr>
Message-ID: <44B9BF3F.40104@pdf.com>

	  From what I've heard, the primary reason for using Bayesian Model 
Averaging (BMA) is to get better predictions, both better point 
estimates and better estimates of the uncertainties.  I could not find a 
function to compute that.

	  However, if you've got point estimates of predictions, I can give you 
a formula for the variance of the prediction error:

	  var(Y|x) = var(E(Y|x,i)over i)+E(var(Y|x,i)over i),

where Y is the response variable, and 'i' indicates which model.  This 
is a companion to the formula for the predictions:

	  E(Y|x) = E(E(Y|x,i)over i).

	  If you've got a function to compute E(Y|x,i) and compute their 
expectation over i, it should not be too difficult to modify that 
function to also compute var(Y|x,i) and then to compute both 
E(var(Y|x,i)over i) and var(E(Y|x,i)over i).  Then if distributions are 
roughly normal, you've got what you need.

	  Does this make sense?
	  Hope this helps.
	  Spencer Graves
p.s.  I've copied the maintainer for the BMA package on this email.  I 
hope he will correct any misstatement in these comments and update us on 
any relevant capabilities we may have missed.
	
Nicolas.Meyer at chru-strasbourg.fr wrote:
> Hello everybody, 
> 
> In order to predict income for different time points, I fitted a linear
> model with polynomial effects using BMA (bicreg(...)). It works fine, the
> results are consistent with what we are looking for. 
> Now, we would like to predict income for a future time point t_next and of
> course draw the prediction interval around the estimated value for this
> point t_next. I've found the formulae for the ponctual estimation of t_next
> but I cannot succeed in finding the formula to compute the prediction
> interval based on BMA sets of models, which requires the adequate variance.
> The paper of Hoeting et al. on BMA gives the posterior variance of a
> parameter but what I want is the posterior variance of a predicted Y and not
> the posterior variance of a beta, since the goal here is not to build an
> explanatory model but an accurately predictive model. 
> Is there a way to get around it with the function of the BMA package or does
> anybody has indication on where to find the formulae ? I searched the web
> for hours but nothing like this is to be found. 
> Any help will be appreciated. 
> Thank's in advance,  
> 
>  
>  
>  
> Nicolas Meyer
> D?partement de Sant? Publique
> Unit? de Biostatistique et M?thodologie
> 03 88 11 63 58
> nicolas.meyer at chru-strasbourg.fr <mailto:nicolas.meyer at chru-strasbourg.fr> 
>  
> La conjecture de Feynman : 
> "To report a significant result and reject the null in favor of an
> alternative hypothesis is meaningless unless the alternative hypothesis has
> been stated before the data was obtained"
> "The meaning of it all", 1998. 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From bhs2 at mevik.net  Sun Jul 16 06:29:12 2006
From: bhs2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Sun, 16 Jul 2006 06:29:12 +0200
Subject: [R] princomp and eigen
In-Reply-To: <44B9BC97.6080207@waikato.ac.nz> (Murray Jorgensen's message of
	"Sun, 16 Jul 2006 16:12:07 +1200")
References: <44B9BC97.6080207@waikato.ac.nz>
Message-ID: <m0vepykw9z.fsf@bar.nemo-project.org>

Murray Jorgensen wrote:

>  > set.seed(160706)
>  > X <- matrix(rnorm(40),nrow=10,ncol=4)
>  > Xpc <- princomp(X,cor=FALSE)
>  > summary(Xpc,loadings=TRUE, cutoff=0)
> Importance of components:
>                            Comp.1    Comp.2    Comp.3     Comp.4
> Standard deviation     1.2268300 0.9690865 0.7918504 0.55295970
[...]
>
> I would have expected the princomp component standard deviations to be 
> the square roots of the eigen() $values and they clearly are not.

It's an 1/n vs. 1/(n-1) thing:

> eX <- eigen(var(X))
> sqrt(eX$values)
[1] 1.2931924 1.0215069 0.8346836 0.5828707
> sqrt(9/10 * eX$values)
[1] 1.2268300 0.9690865 0.7918504 0.5529597

-- 
Bj?rn-Helge Mevik


From f.harrell at vanderbilt.edu  Sun Jul 16 07:41:21 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 16 Jul 2006 00:41:21 -0500
Subject: [R] Some problems with latex(ftable)
In-Reply-To: <20060715170357.BEF63228@po-d.temple.edu>
References: <20060715170357.BEF63228@po-d.temple.edu>
Message-ID: <44B9D181.8010003@vanderbilt.edu>

Richard M. Heiberger wrote:
> The ftable structure is not an ordinary matrix.  Instead, it has the
> body of the table with several cbind- and rbind-ed rows and columns of
> label information.  The example in ?ftable has two row factors and two
> column factors.
> 
> Continuing with the example in ?ftable, enter
> 
> 
> tmp <- ftable(mtcars$cyl, mtcars$vs, mtcars$am, mtcars$gear, row.vars = c(2, 4), 
>               dnn = c("Cylinders", "V/S", "Transmission", "Gears"))
> 
> print.default(tmp)
> 
> To get what you are looking for, you will need to intercept write.ftable
> with, for example,
> 
> trace(write.ftable, exit=recover)
> 
> then do 
> 
> 3
> tmp.latex <- latex(t(x))
> print.default(tmp.latex)
> 
> Now open up t.latex and prepend
> \documentstyle{article}
> \begin{document}
> 
> and append
> \end{document}
> 
> then latex it.
> 
> This gets you close to what you want and you can work with the generated
> t.tex file to get the rest of the detail.  Or you can work with the
> numerous arguments we built into latex (see ?latex) to get some of them
> automatically generated.
> 
> tmp2.latex <- latex(t(x), col.just=rep(c("l","r"), c(3,6)),
>                     n.rgroup=c(3,6), file="t2.tex")
> 
> 
> Now open up t2.latex and pre- and append the latex controls to it.
> 
> 
> This works well for one or two examples.  To do many, then you will
> need to follow Frank's suggestion and build all of this into a method.
> Once the method works well, send it to Frank and he will consider
> including it in the next release (Frank, I hope that's a fair offer I
> make for you to do).

Yes, working with Charles Thomas Dupont we'll be glad to add such a 
contribution to latex.  Thanks for your excellent ideas above Rich.

-Frank

> 
> 
> This raises a question from me to the R developers.  I would have
> written write.ftable to return t(x), not ox.  print is constrained to
> return its argument.  I thought write had more freedom.  Then I would
> respecify
> 
> print.ftable <- function (x, digits = getOption("digits"), ...) {
>      write.ftable(x, quote = FALSE, digits = digits)
>      invisible(x)
> }
> 
> Had this been done then the current task would simplify to
> 
> latex(write(tmp))
> 
> Te question is: why was write.ftable designed to follow the
> print.ftable constraint on returned value.
> 
> ps. I just designed the method.  Take write.ftable, drop the last line
> and give it a new name.  then you can latex the output of that new
> function.
> 
> Rich
>


From hb at stat.berkeley.edu  Sun Jul 16 11:11:02 2006
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Sun, 16 Jul 2006 11:11:02 +0200
Subject: [R] Generating valid R code using R
Message-ID: <59d7961d0607160211v5941f05dwd8bc7de1ee5489ab@mail.gmail.com>

Hi,

I'm trying to generate valid R code using R. Parts of the task is to
read a sequence of characters from file and escape them such that they
can be put in quotation marks to form a valid R code string.  Example:

Let the input file be (four rows containing ASCII 0-255 characters):
abcdef<tab>ghijk\nlmno
second row\t\a\\

fourth and so on...
<EOF>

Now, find escapeString() such that the following piece of code
generates a second file called 'file2.txt' which is identical to
'file1.txt':

inStr <- readChar("file1.txt", nchars=999)
esStr <- escapeString(inStr)
rCode <- sprintf('cat(file="file2.txt", "%s")', esStr)
cat(file="foo.R", rCode)
source("foo.R")

For instance, quotation marks has to be escaped in order for 'rCode'
to be valid, same with newlines etc.  What's the best way to do this?
Currently I use an ad hoc sequence of gsub() substitutions to do this,
but is there a better way to create the 'rCode' string?

Thanks

Henrik


From lorenzo.isella at gmail.com  Sun Jul 16 14:09:17 2006
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Sun, 16 Jul 2006 14:09:17 +0200
Subject: [R] CFD Plots in R and Other Things
Message-ID: <a2b3004b0607160509x5c15ff71g83acd36473924463@mail.gmail.com>

Dear All,

I am getting some data from fluid dynamics simulations (air mixing in
a pipe, 2D axial symmetry, geometry described by a radial coordinate r
and an axial coordinate z) which I'd like to plot and analyze with R.
Think about slicing the cylinder along its axial direction to get a
set of cross sections which are orthogonal to the z axis.
For each section, I have a set of velocity readings, i.e. the data I
would like to plot are in the form:

           v_11 v_12  v_13 ..... v_1n
           v_21 v_22  v_23 ..... v_2n
           ......................................
           v_m1 v_m2  v_m3 ..... v_mn

where v_ij is the velocity reading on the i-th position along z and
the j-th position along r.
Tipically, these sets of data are plotted in 2D with r and z as axis
and the velocity  field represented by using colours explained by a
legenda.
Can R do anything like this?
Second (and much simpler question): I have a list of data which I can
plot easily, but I would like to have the time during the day to label
the x axis (starting from 19:00 for forty hours).
How can I do that?
Many thanks

Lorenzo


From mihai.p.nica at gmail.com  Sun Jul 16 15:01:12 2006
From: mihai.p.nica at gmail.com (Mihai Nica)
Date: Sun, 16 Jul 2006 08:01:12 -0500
Subject: [R] dataframe computation behaviour
Message-ID: <6b30be820607160601p6f1eb6e2o78f4f213afcac196@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060716/a9bf0af0/attachment.pl 

From henrik.parn at bio.ntnu.no  Sun Jul 16 15:35:03 2006
From: henrik.parn at bio.ntnu.no (Henrik Parn)
Date: Sun, 16 Jul 2006 15:35:03 +0200
Subject: [R] break axis using plotrix
Message-ID: <44BA4087.4080509@bio.ntnu.no>

Dear all,

I am trying to plot some data with differing range in y-values with 
type="b", adding error bars and break the y-axis into two parts, one 
lower part from 12 to 20, and one upper part from 34 to 40.

I have tried to follow the basic ideas from the script provided here by 
Jim Lemon:
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/56487.html


My attempt looks like this:

###########################
# x-values
x <- 1:4

# small y-values with corresponding standard errors
meansarr <- c(14.9, 18.2, 14.5, 18.3)
searr <- c(0.47, 1.27, 1.22, 0.49)

# large values
meanslay <- c(36.4, 39.0, 35.3, 38.6)
selay <- c(0.51, 0.34, 0.57, 0.40)

library(plotrix)

# plot small values
plot(x, meansarr, ylim=c(12, 30), axes=F, type="b", xlab="", ylab="Day")
arrows(x, meansarr-searr, x, meansarr+searr, code = 3, angle = 90, 
length = 0.03)
box()

# x-axis
axis(1, tck=0.01, las=1, at=1:4,
    labels=c("1998", "1999", "2002", "2003"), mgp=c(3, 0.5, 0))

# y-axis
axis(2,at=c(12, 14, 16, 18, 20, 24, 26, 28, 
30),labels=c("12","14","16","18","20", "34","36","38","40"))


# break axis
axis.break(2, 22, style="zigzag")

# add large values to same plot
par(new=TRUE)
plot(x, meanslay, ylim=c(30, 40), type="b", xlab="", ylab="Day", axes=F)
arrows(x, meanslay-selay, x, meanslay+selay, code = 3, angle = 90, 
length = 0.03)


################################


As you can see, I have problems adding the larger y-values - they end up 
in the wrong place in the graph. I suppose Jim's warning 'just be 
careful that the ylim= and labels= arguments match up' is relevant here, 
but I don't manage to fix it...


Can anyone help me to plot the large y-values on the right 
placeaccording to the y-axis labeling?
I am using R 2.3.1 and WinXp.

Thanks a lot in advance!

Henrik
////

-- 
************************
Henrik P?rn
Department of Biology
NTNU
7491 Trondheim
Norway

+47 735 96282 (office)
+47 909 89 255 (mobile)
+47 735 96100 (fax)


From ggrothendieck at gmail.com  Sun Jul 16 15:48:02 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 16 Jul 2006 09:48:02 -0400
Subject: [R] CFD Plots in R and Other Things
In-Reply-To: <a2b3004b0607160509x5c15ff71g83acd36473924463@mail.gmail.com>
References: <a2b3004b0607160509x5c15ff71g83acd36473924463@mail.gmail.com>
Message-ID: <971536df0607160648v34c27194ja593a23dc8ca1c7b@mail.gmail.com>

Look at ?image, ?contour and in the lattice package ?contourplot,
?levelplot, ?wireframe, ?cloud and in scatterplot3d package
scatterplot3d.  Try example() on each one, e.g. example(image),
to get an idea if its what you want. Also google for the R Graph Gallery
and look through the charts there.

For the second question you can customize the x axis labels
by not drawing them in plot and drawing them yourself using axis:

tt <- 19 + 0:40
plot(tt, tt, xaxt = "n")
axis(1, tt, paste(tt, "00", sep = ":"), cex.axis = 0.5)
abline(v = tt, col = "grey")  # optional



On 7/16/06, Lorenzo Isella <lorenzo.isella at gmail.com> wrote:
> Dear All,
>
> I am getting some data from fluid dynamics simulations (air mixing in
> a pipe, 2D axial symmetry, geometry described by a radial coordinate r
> and an axial coordinate z) which I'd like to plot and analyze with R.
> Think about slicing the cylinder along its axial direction to get a
> set of cross sections which are orthogonal to the z axis.
> For each section, I have a set of velocity readings, i.e. the data I
> would like to plot are in the form:
>
>           v_11 v_12  v_13 ..... v_1n
>           v_21 v_22  v_23 ..... v_2n
>           ......................................
>           v_m1 v_m2  v_m3 ..... v_mn
>
> where v_ij is the velocity reading on the i-th position along z and
> the j-th position along r.
> Tipically, these sets of data are plotted in 2D with r and z as axis
> and the velocity  field represented by using colours explained by a
> legenda.
> Can R do anything like this?
> Second (and much simpler question): I have a list of data which I can
> plot easily, but I would like to have the time during the day to label
> the x axis (starting from 19:00 for forty hours).
> How can I do that?
> Many thanks
>
> Lorenzo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From lobry at biomserv.univ-lyon1.fr  Sun Jul 16 16:12:33 2006
From: lobry at biomserv.univ-lyon1.fr (Jean lobry)
Date: Sun, 16 Jul 2006 16:12:33 +0200
Subject: [R] put R on a web server
In-Reply-To: <mailman.9.1153044003.7937.r-help@stat.math.ethz.ch>
References: <mailman.9.1153044003.7937.r-help@stat.math.ethz.ch>
Message-ID: <p06002005c0dff4fdcab2@[134.214.34.142]>

>Has anyone put R on a web server any time, recently, please?

It depends on what you mean by "recently".

R 2.3.1 and 1054 packages at http://kryton.cc.unt.edu/cgi-bin/R/Rprog
R 2.3.1 and 1046 packages at http://www.unt.edu/rss/Rinterface.htm
R 2.3.1 and 499  packages at http://pbil.univ-lyon1.fr/Rweb/
R 2.3.0 and 85   packages at http://rweb.stat.umn.edu/Rweb/
R 2.2.1 and 359  packages at http://actin.ucd.ie/Rweb/
R 2.2.0 and 25   packages at http://www.digitalhermit.com/math/Rweb.html
R 2.2.0 and 25   packages at http://r.nakama.ne.jp/Rweb-jp/
R 2.1.0 and 25   packages at http://dssm.unipa.it/R-php/
R 2.1.0 and 25   packages at 
http://homeworld.rutgers.edu/Rweb/Rweb.general.html
R 2.0.1 and 526  packages at http://www.ms.uky.edu/~statweb/
R 1.9.1 and 35   packages at http://origin.scic.ulst.ac.uk/Rweb/
R 1.9.0 and 106  packages at http://hermes.sdu.dk/cgi-bin/go/
R 1.9.0 and 48   packages at http://claree.univ-lille1.fr/Rweb/
R 1.8.1 and 30   packages at http://www.er.uqam.ca/nobel/r17165/Rweb/Rweb.html
R 1.8.1 and 30   packages at http://bayes.math.montana.edu/Rweb/
R 1.7.0 and 49   packages at http://bic.uams.edu/Rweb/
R 1.6.1 and 28   packages at 
http://user.cs.tu-berlin.de/~ulfi/cgi-bin/r-online/r-online.cgi
R 1.5.0 and 32   packages at http://www.nku.edu/~longa/Rweb/
R 1.3.0 and 11   packages at http://genome1.beatson.gla.ac.uk/Rweb/

-- 
Jean R. Lobry            (lobry at biomserv.univ-lyon1.fr)
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
allo  : +33 472 43 12 87     fax    : +33 472 43 13 88
http://pbil.univ-lyon1.fr/members/lobry/


From christoph.clases at fhnw.ch  Sun Jul 16 16:11:25 2006
From: christoph.clases at fhnw.ch (Christoph Clases)
Date: Sun, 16 Jul 2006 16:11:25 +0200
Subject: [R] problem with installation of older R-version
Message-ID: <001c01c6a8e1$b9ad05f0$2301a8c0@staff.so.fhnw.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060716/9fb5554c/attachment.pl 

From spencer.graves at pdf.com  Sun Jul 16 18:35:27 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 16 Jul 2006 09:35:27 -0700
Subject: [R] ts and stl functions - still a problem
In-Reply-To: <20060713130834.64028.qmail@web26908.mail.ukl.yahoo.com>
References: <20060713130834.64028.qmail@web26908.mail.ukl.yahoo.com>
Message-ID: <44BA6ACF.60306@pdf.com>

	  The 'ts' function retains the 'dim' attribute of 'tkr'.  When 'stl' 
finds this 'dim' attribute, it thinks 'tstkr' is a multivariate time 
series.  This causes it to stop with the error, "only univariate series 
allowed".

	  Consider the following modification of an example from the 'stl' help 
file:

 > not.1 <- stl(nottem, "per")
 > Nottem <- ts(array(nottem, dim=c(240, 1)))
 > Not.1 <- stl(Nottem)
Error in stl(Nottem) : only univariate series are allowed

	  Solution:

tstkr <- ts(as.numeric(tkr), deltat=1/12)

	  After converting tkr and tstkr from a matrix to a vector like this, 
please try 'stl(tstkr)'.  If it doesn't work, please submit another post.

	  Hope this helps.
	  Spencer Graves
p.s.  Your example was not quite self-contained, because I didn't know 
for sure the format, class, and attributes of your 'tkr' object.  The 
absence of these details made it harder for me (and, I believe, anyone 
else) to reply.  You might get better replies quicker with greater 
attention to such details.
	
Daniel sutcliffe wrote:
> Hi
>    
>   I am still having problems with using the stl 
function, when I read the csv file into R into a
file called tkr and use dim(tkr) the result is 132 x 1
which is fine.
>    
>   When coerce it into a trime series using ts either:
>    
>    
>   tstkr <- ts(t(tkr), deltat=1/12) or
>    
>   tstkr <- ts(c(tkr), deltat=1/12) 
>    
>   and use the stl function I get the following error:
>    
>   Error in stl(tstkr) : only univariate series are allowed
>    
>   id just use the tkr file I get the same error..does anyibe have an idea what to do next, here is my data...it's not sensitive so if anyone wants to try then you are very welcome!
>               Rate    184.0222    180.517    222.5792    173.5066    192.7852    198.0429    182.2696    189.28    178.7644    206.8059    236.6    155.9807    231.7314    249.2868    222.9537    198.3761    201.8872    208.9094    242.2646    221.1982    228.2203    245.7757    244.0202    194.865    239.3664    234.0862    251.6867    235.8463    197.1253    237.6063    267.5271    228.8061    241.1264    249.9267    256.9669    188.325    258.8788    239.5069    214.8518    234.2237    211.3296    234.2237    237.7458    156.7361    239.5069    225.4183    257.1177    170.8248    230.1611    265.3001    296.9253    193.265    233.675    240.7028    249.4876    205.5637    237.1889    237.1889    289.8975    245.9736    283.1755    316.3875    372.3234    234.2316    263.9476    314.6395    286.6715    272.6875    323.3795    295.4115    300.6555    174.7997    227.184    277.4767    317.364    234.121    286.1478    279.2109    280.9452    235.8552    319.0982   
>  296.5532    303.4901    173.4229    302.6072    312.8651    389.7991    223.9635    254.7371    329.9615    288.93    317.994    312.8651    381.2509    333.3808    194.8996    285.5743    304.0526    335.9698    307.4123    346.0489    288.934    335.9698    243.5781    384.6854    359.4876    357.8078    221.74    336.3712    344.6562    389.3952    286.6611    338.0282    407.6222    356.2552    241.9221    347.9702    400.9942    381.1102    304.8882    383.9077    418.5089    476.1774    331.1822    378.9647    375.6694    339.4206    364.1357    420.1565    446.5193    410.2705    296.5811
>    
>   Cheers and thanks to everyone who offered suggestions before.
>    
>   Daniel
>    
>    
>    
>   
> 
> SAULEAU Erik-Andr? <SAULEAUEA at ch-mulhouse.fr> wrote:
>   Perhaps ts(t(tkr))?
> 
>> -----Message d'origine-----
>> De : Daniel sutcliffe [mailto:dnlsutcliffe at yahoo.co.uk] 
>> Envoy? : mercredi 12 juillet 2006 15:53
>> ? : r-help at stat.math.ethz.ch
>> Objet : [R] ts and stl functions
>>
>>
>> Hi, 
>>
>> I have imported a csv file into R which contains one column 
>> (the rate er 100,000 population of a disease, by month over 
>> 11 years) I coerced into a time series using the following function, 
>>
>> tstkr<-ts(tkr,deltat=1/12) 
>>
>> This seems to work fine, and when I check for the class of 
>> the object using class(tstkr) I get "ts" as the response. 
>>
>> When I try to use the stl function in stats I get the error message: 
>>
>> Error in stl(tstkr)only univariate series are allowed 
>>
>> I then tried this: 
>>
>> tstkr <- ts(c(tkr), deltat=1/12) 
>>
>> however this made no difference...I still get an error - does 
>> anybody know what is wrong? 
>>
>> Regards, 
>>
>> Daniel 
>>
>>
>> ---------------------------------
>>
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read 
>> the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>> **************************************************************
>> ********************
>> Afin d'eviter toute propagation de virus informatique, et en 
>> complement 
>> des dispositifs en place, ce message (et ses pieces jointes 
>> s'il y en a) 
>> a ete automatiquement analyse par un antivirus de messagerie. 
>> **************************************************************
>> ********************
>>
> 
> 
> **********************************************************************************
> Afin d'eviter toute propagation de virus informatique, et en complement 
> des dispositifs en place, ce message (et ses pieces jointes s'il y en a) 
> a ete automatiquement analyse par un antivirus de messagerie. 
> **********************************************************************************
> 
> 
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ggrothendieck at gmail.com  Sun Jul 16 18:46:43 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 16 Jul 2006 12:46:43 -0400
Subject: [R] Trailing on r-help messages
Message-ID: <971536df0607160946v7e0a0c1al9e3956885bab73e8@mail.gmail.com>

I would like to propose that we change the trailer on r-help
messages which is currently:

   R-help at stat.math.ethz.ch mailing list
   https://stat.ethz.ch/mailman/listinfo/r-help
   PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

to add the following 4th line:

   and provide minimal, self-contained, reproducible code.

The posting guide is so long that I suspect few people really read
it so at least this way the most important part of the message about
posting would be readily visible without further user action.

Of course minimal refers to cutting the code down to remove
anything not related to the question at hand while self-contained
and reproducible refer to being able to copy the code from
the post and paste it into an R session to reproduce the
problem.


From spencer.graves at pdf.com  Sun Jul 16 19:37:45 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 16 Jul 2006 10:37:45 -0700
Subject: [R] Trailing on r-help messages
In-Reply-To: <971536df0607160946v7e0a0c1al9e3956885bab73e8@mail.gmail.com>
References: <971536df0607160946v7e0a0c1al9e3956885bab73e8@mail.gmail.com>
Message-ID: <44BA7969.3070409@pdf.com>

Hi, Gabor:

	  Sounds great.  May I suggest a minor modification something like the 
following;

 >    R-help at stat.math.ethz.ch mailing list
 >    https://stat.ethz.ch/mailman/listinfo/r-help
 >    PLEASE include in your post minimal, self-contained, reproducible 
code as suggested in posting guide, 
"www.R-project.org/posting-guide.html".  Posts more consistent with this 
standard tend to get quicker, more useful replies.

	  I'd also suggest adding something like this to the Special Interest 
Group posts as well.  I also reply to questions on R-sig-finance, and I 
find myself going to R-help to copy the "PLEASE do read the posting 
guide" comment for r-sig-finance.

	  Thanks for suggesting this.
	  Spencer Graves

Gabor Grothendieck wrote:
> I would like to propose that we change the trailer on r-help
> messages which is currently:
> 
>    R-help at stat.math.ethz.ch mailing list
>    https://stat.ethz.ch/mailman/listinfo/r-help
>    PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> to add the following 4th line:
> 
>    and provide minimal, self-contained, reproducible code.
> 
> The posting guide is so long that I suspect few people really read
> it so at least this way the most important part of the message about
> posting would be readily visible without further user action.
> 
> Of course minimal refers to cutting the code down to remove
> anything not related to the question at hand while self-contained
> and reproducible refer to being able to copy the code from
> the post and paste it into an R session to reproduce the
> problem.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From h.wickham at gmail.com  Sun Jul 16 20:16:12 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 16 Jul 2006 19:16:12 +0100
Subject: [R] Trailing on r-help messages
In-Reply-To: <971536df0607160946v7e0a0c1al9e3956885bab73e8@mail.gmail.com>
References: <971536df0607160946v7e0a0c1al9e3956885bab73e8@mail.gmail.com>
Message-ID: <f8e6ff050607161116m76f9ff32j1c1ad66c5810db52@mail.gmail.com>

Personally, I doubt anyone actually reads the bottom of the emails,
and generally it is too late anyway, as they have already sent the
message.  Much like the opening text in R, details in footer tend to
trigger legalese neurons and are largely ignored.

I think it would be more useful to radically redesign the signup page
so that pertinent information is highlighted and the page made easily
scannable.  As it is, the eye tends to skip to the action part and
fill out name and email address without reading the rest of the page.

Hadley

On 7/16/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> I would like to propose that we change the trailer on r-help
> messages which is currently:
>
>    R-help at stat.math.ethz.ch mailing list
>    https://stat.ethz.ch/mailman/listinfo/r-help
>    PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> to add the following 4th line:
>
>    and provide minimal, self-contained, reproducible code.
>
> The posting guide is so long that I suspect few people really read
> it so at least this way the most important part of the message about
> posting would be readily visible without further user action.
>
> Of course minimal refers to cutting the code down to remove
> anything not related to the question at hand while self-contained
> and reproducible refer to being able to copy the code from
> the post and paste it into an R session to reproduce the
> problem.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From mihainica at yahoo.com  Sun Jul 16 20:39:23 2006
From: mihainica at yahoo.com (Mihai Nica)
Date: Sun, 16 Jul 2006 11:39:23 -0700 (PDT)
Subject: [R] Trailing on r-help messages
In-Reply-To: <f8e6ff050607161116m76f9ff32j1c1ad66c5810db52@mail.gmail.com>
Message-ID: <20060716183923.24576.qmail@web52515.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060716/83e858ea/attachment.pl 

From ggrothendieck at gmail.com  Sun Jul 16 20:49:19 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 16 Jul 2006 14:49:19 -0400
Subject: [R] Trailing on r-help messages
In-Reply-To: <44BA7969.3070409@pdf.com>
References: <971536df0607160946v7e0a0c1al9e3956885bab73e8@mail.gmail.com>
	<44BA7969.3070409@pdf.com>
Message-ID: <971536df0607161149i755ef39vc89b3517919494c8@mail.gmail.com>

I was trying to keep it to 4 lines making it more likely read
than a longer description.

On 7/16/06, Spencer Graves <spencer.graves at pdf.com> wrote:
> Hi, Gabor:
>
>          Sounds great.  May I suggest a minor modification something like the
> following;
>
>  >    R-help at stat.math.ethz.ch mailing list
>  >    https://stat.ethz.ch/mailman/listinfo/r-help
>  >    PLEASE include in your post minimal, self-contained, reproducible
> code as suggested in posting guide,
> "www.R-project.org/posting-guide.html".  Posts more consistent with this
> standard tend to get quicker, more useful replies.
>
>          I'd also suggest adding something like this to the Special Interest
> Group posts as well.  I also reply to questions on R-sig-finance, and I
> find myself going to R-help to copy the "PLEASE do read the posting
> guide" comment for r-sig-finance.
>
>          Thanks for suggesting this.
>          Spencer Graves
>
> Gabor Grothendieck wrote:
> > I would like to propose that we change the trailer on r-help
> > messages which is currently:
> >
> >    R-help at stat.math.ethz.ch mailing list
> >    https://stat.ethz.ch/mailman/listinfo/r-help
> >    PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> > to add the following 4th line:
> >
> >    and provide minimal, self-contained, reproducible code.
> >
> > The posting guide is so long that I suspect few people really read
> > it so at least this way the most important part of the message about
> > posting would be readily visible without further user action.
> >
> > Of course minimal refers to cutting the code down to remove
> > anything not related to the question at hand while self-contained
> > and reproducible refer to being able to copy the code from
> > the post and paste it into an R session to reproduce the
> > problem.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Sun Jul 16 20:51:17 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 16 Jul 2006 14:51:17 -0400
Subject: [R] Trailing on r-help messages
In-Reply-To: <f8e6ff050607161116m76f9ff32j1c1ad66c5810db52@mail.gmail.com>
References: <971536df0607160946v7e0a0c1al9e3956885bab73e8@mail.gmail.com>
	<f8e6ff050607161116m76f9ff32j1c1ad66c5810db52@mail.gmail.com>
Message-ID: <971536df0607161151y54a8baceu9bb9edaf5a628f94@mail.gmail.com>

On 7/16/06, hadley wickham <h.wickham at gmail.com> wrote:
> Personally, I doubt anyone actually reads the bottom of the emails,
> and generally it is too late anyway, as they have already sent the
> message.  Much like the opening text in R, details in footer tend to
> trigger legalese neurons and are largely ignored.

They might read it when reading other people's messages since
it would be at the bottom of every single one.

> I think it would be more useful to radically redesign the signup page
> so that pertinent information is highlighted and the page made easily
> scannable.  As it is, the eye tends to skip to the action part and
> fill out name and email address without reading the rest of the page.

Good idea.   Of course these are not mutually exclusive.

>
> Hadley
>
> On 7/16/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > I would like to propose that we change the trailer on r-help
> > messages which is currently:
> >
> >    R-help at stat.math.ethz.ch mailing list
> >    https://stat.ethz.ch/mailman/listinfo/r-help
> >    PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> > to add the following 4th line:
> >
> >    and provide minimal, self-contained, reproducible code.
> >
> > The posting guide is so long that I suspect few people really read
> > it so at least this way the most important part of the message about
> > posting would be readily visible without further user action.
> >
> > Of course minimal refers to cutting the code down to remove
> > anything not related to the question at hand while self-contained
> > and reproducible refer to being able to copy the code from
> > the post and paste it into an R session to reproduce the
> > problem.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>


From robert-mcfadden at o2.pl  Sun Jul 16 21:10:38 2006
From: robert-mcfadden at o2.pl (Robert Mcfadden)
Date: Sun, 16 Jul 2006 21:10:38 +0200
Subject: [R] rbind, array
Message-ID: <000001c6a90b$85ef6170$1191680a@robert>

B??dnie zakodowany tekst zosta? usuni?ty...
Plik: nie znany
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060716/a00859ed/attachment.pl 

From rmh at temple.edu  Sun Jul 16 21:19:36 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Sun, 16 Jul 2006 15:19:36 -0400 (EDT)
Subject: [R] rbind, array
Message-ID: <20060716151936.BEG60531@po-d.temple.edu>

download library(abind).


From ggrothendieck at gmail.com  Sun Jul 16 21:25:08 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 16 Jul 2006 15:25:08 -0400
Subject: [R] rbind, array
In-Reply-To: <000001c6a90b$85ef6170$1191680a@robert>
References: <000001c6a90b$85ef6170$1191680a@robert>
Message-ID: <971536df0607161225i3fd64352ia9f1e265d674364b@mail.gmail.com>

Try this:

y <- array(1:27, c(3, 3, 3))   # test array
rbind(y[,,1], y[,,2], y[,,3])
apply(y, 2, I)

# verify that last two lines give same result
identical(apply(y, 2, I), rbind(y[,,1], y[,,2], y[,,3]))  # TRUE

On 7/16/06, Robert Mcfadden <robert-mcfadden at o2.pl> wrote:
> Hello,
>
> I'm looping something like rbind for array. More precisely: if I define
>
> y<-array(c(1:27),dim=c(3,3,3)) and I take 3 matrix: y[,,1], y[,,2], y[,,3]
> and write rbind(y[,,1], y[,,2], y[,,3]) I get what I want. But what if I
> have hundreds such matrix. How to do it.
>
> Robert
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From spencer.graves at pdf.com  Sun Jul 16 21:53:29 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 16 Jul 2006 12:53:29 -0700
Subject: [R] Trailing on r-help messages
In-Reply-To: <971536df0607161149i755ef39vc89b3517919494c8@mail.gmail.com>
References: <971536df0607160946v7e0a0c1al9e3956885bab73e8@mail.gmail.com>	<44BA7969.3070409@pdf.com>
	<971536df0607161149i755ef39vc89b3517919494c8@mail.gmail.com>
Message-ID: <44BA9939.9070805@pdf.com>

Hi, Gabor:  Yes.  Saying more often communicates less.  Spencer Graves

Gabor Grothendieck wrote:
> I was trying to keep it to 4 lines making it more likely read
> than a longer description.
> 
> On 7/16/06, Spencer Graves <spencer.graves at pdf.com> wrote:
>> Hi, Gabor:
>>
>>          Sounds great.  May I suggest a minor modification something like the
>> following;
>>
>>  >    R-help at stat.math.ethz.ch mailing list
>>  >    https://stat.ethz.ch/mailman/listinfo/r-help
>>  >    PLEASE include in your post minimal, self-contained, reproducible
>> code as suggested in posting guide,
>> "www.R-project.org/posting-guide.html".  Posts more consistent with this
>> standard tend to get quicker, more useful replies.
>>
>>          I'd also suggest adding something like this to the Special Interest
>> Group posts as well.  I also reply to questions on R-sig-finance, and I
>> find myself going to R-help to copy the "PLEASE do read the posting
>> guide" comment for r-sig-finance.
>>
>>          Thanks for suggesting this.
>>          Spencer Graves
>>
>> Gabor Grothendieck wrote:
>>> I would like to propose that we change the trailer on r-help
>>> messages which is currently:
>>>
>>>    R-help at stat.math.ethz.ch mailing list
>>>    https://stat.ethz.ch/mailman/listinfo/r-help
>>>    PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>> to add the following 4th line:
>>>
>>>    and provide minimal, self-contained, reproducible code.
>>>
>>> The posting guide is so long that I suspect few people really read
>>> it so at least this way the most important part of the message about
>>> posting would be readily visible without further user action.
>>>
>>> Of course minimal refers to cutting the code down to remove
>>> anything not related to the question at hand while self-contained
>>> and reproducible refer to being able to copy the code from
>>> the post and paste it into an R session to reproduce the
>>> problem.
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From h.wickham at gmail.com  Sun Jul 16 22:24:44 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 16 Jul 2006 21:24:44 +0100
Subject: [R] Trailing on r-help messages
In-Reply-To: <971536df0607161151y54a8baceu9bb9edaf5a628f94@mail.gmail.com>
References: <971536df0607160946v7e0a0c1al9e3956885bab73e8@mail.gmail.com>
	<f8e6ff050607161116m76f9ff32j1c1ad66c5810db52@mail.gmail.com>
	<971536df0607161151y54a8baceu9bb9edaf5a628f94@mail.gmail.com>
Message-ID: <f8e6ff050607161324l7e85d3b3t7fd89046c22d36f4@mail.gmail.com>

> They might read it when reading other people's messages since
> it would be at the bottom of every single one.

Perhaps, it might also trigger "same"-blindness: seeing the same thing
again and again makes it less and less likely to really percieve it.

Hadley


From ggrothendieck at gmail.com  Sun Jul 16 22:31:13 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 16 Jul 2006 16:31:13 -0400
Subject: [R] Trailing on r-help messages
In-Reply-To: <f8e6ff050607161324l7e85d3b3t7fd89046c22d36f4@mail.gmail.com>
References: <971536df0607160946v7e0a0c1al9e3956885bab73e8@mail.gmail.com>
	<f8e6ff050607161116m76f9ff32j1c1ad66c5810db52@mail.gmail.com>
	<971536df0607161151y54a8baceu9bb9edaf5a628f94@mail.gmail.com>
	<f8e6ff050607161324l7e85d3b3t7fd89046c22d36f4@mail.gmail.com>
Message-ID: <971536df0607161331h546bc59fkd911ca87b1fd6a36@mail.gmail.com>

On 7/16/06, hadley wickham <h.wickham at gmail.com> wrote:
> > They might read it when reading other people's messages since
> > it would be at the bottom of every single one.
>
> Perhaps, it might also trigger "same"-blindness: seeing the same thing
> again and again makes it less and less likely to really percieve it.
>
> Hadley
>

I think that is likely true but it would at least mean that they had
seen it repeatedly and there would really be no excuse for not following it
(unlike the current situation where one needs to take action to follow
the posting guide link and then read a lengthy page).


From dmgatti at mindspring.com  Sun Jul 16 23:08:27 2006
From: dmgatti at mindspring.com (Daniel Gatti)
Date: Sun, 16 Jul 2006 17:08:27 -0400
Subject: [R] install.packages for local zip files
Message-ID: <44BAAACB.80901@mindspring.com>

O/S: Linux
R version : 2.2.1

The R server doesn't have http internet access.  And the sys admins will 
not install the R libraries that I requested.  So I have downloaded the 
packages that I want to intall and have moved them into my home 
directory on the server.  These are a series of *.tar.gz files.  I want 
to install the R libraries in my home directory, but I can't get it to 
work.  According to the install.packages documentation :

install.packages(pkgs, lib, repos = getOption("repos"),  contriburl = 
contrib.url(repos, type), method, available = NULL, destdir = NULL, 
installWithVers = FALSE, dependencies = FALSE, type = getOption("pkgType"))

repos: character vector, the base URL(s) of the repositories to use,
       i.e., .... Can be 'NULL' to install from local '.tar.gz' files.

contriburl: URL(s) of the contrib section of the repositories. ......
       Can be 'NULL' to install from local '.tar.gz' files.


 pkgs: character vector of the short names of packages/bundles whose
          current versions should be downloaded from the repositories.
          If 'repos = NULL', a character vector of file paths of
          '.tar.gz' files.  These can be source archives or binary
          package/bundle archive files (as created by 'R CMD build
          --binary'). ......

 lib: character vector giving the library directories where to install 
the packages.  Recycled as needed.

So I have issued a command like this:

 > install.packages(pkgs="~/Rdownloads/hgug4112a_1.12.0.tar.gz", lib = 
"~/Rlib", repos=NULL, contriburl=NULL)

Warning in download.packages(pkgs, destdir = tmpd, available = 
available,  :
         no package '~/Rdownloads/hgug4112a_1.12.0.tar.gz' at the 
repositories

As far as I can tell, I've given it the full path to the zip file, the 
directory in which to install the library and I've set the repository 
path to 'NULL' to indicate that I'm installing from an already 
downloaded zip file.  But I'm missing something.  Any ideas?

Thanks,
Dan


From rvaradhan at jhmi.edu  Mon Jul 17 00:11:34 2006
From: rvaradhan at jhmi.edu (RAVI VARADHAN)
Date: Sun, 16 Jul 2006 18:11:34 -0400
Subject: [R] Manipulation involving arrays
Message-ID: <f4a0f6a67532.44ba8156@johnshopkins.edu>

Hi,

I have the following piece of code that is part of a larger function.  This piece is the most time consuming part of the function, and I would like to make this a bit more efficient.  Could anyone suggest a way to do this faster?  

In particular, I would like to replace the nested "for" loop with a faster construct.  I tried things like "kronecker" and "outer" combined with apply, but couldn't get it to work.


Here is a sample code:

 ##########################
 n <- 120
 sigerr <- 5
 covmat <- diag(c(8,6,3.5))
 mu <- c(105,12,10)
 mcsamp <- 10000
 
 Tbar <- array(0, dim=c(3,3,n))
 
 # theta is a mcsamp x 3 matrix
 theta <- mvrnorm(mcsamp, mu = mu, Sigma = covmat)
 
 wt <- matrix(runif(n*mcsamp),n,mcsamp) 
 wti <- apply(wt,1,sum)
 
 tarray <- array(apply(theta,1,function(x)outer(x,x)),dim=c(3,3,mcsamp))
 
 for (i in 1:n) {
 for (k in 1:mcsamp) {
 Tbar[,,i] <- Tbar[,,i] + wt[i,k] * tarray[,,k]
 } 
 Tbar[,,i] <- Tbar[,,i] / wti[i]
 }

###############################################

Thanks very much,
Ravi.


From berr3415 at uidaho.edu  Mon Jul 17 01:30:51 2006
From: berr3415 at uidaho.edu (Erin Berryman)
Date: Sun, 16 Jul 2006 16:30:51 -0700
Subject: [R] Hmisc xYplot
Message-ID: <63b87b8e36d65a2185fae544688619a8@uidaho.edu>

Dear R community,

I am having trouble with a particular plot that I am trying to produce 
using Hmisc's xYplot function. I've been using primarily lattice and 
Hmisc packages for my plotting needs for the past few years, with great 
success.
However, what I want to do now with xYplot is plot more than one data 
trend in the same panel, much as I would use xyplot from package 
lattice in conjunction with superpose, with type="b". My problem is I 
can't get the error bars to plot when I use xYplot this way. I've 
attached my data set for reference.

Here are my inputs:

 >p1sum<-read.csv(file="p1sum.csv", header=T)
 >library(Hmisc)
 >xYplot(Cbind(DRP, SE) + Cbind(Fe, FeSE) ~ Day | Group + Port,  p1sum, 
type='b')

The plotted result is two lines per panel, one labeled "Cbind(DRP, SE)" 
and the other labeled "Cbind(Fe, FeSE). However, the error bars are not 
plotted at all (I want the error bars to be DRP +/- SE, and Fe +/- 
FeSE). Any advice on this is greatly appreciated.

Thank you,

Erin


-------------- next part --------------


Erin Berryman
***PLEASE- NOTE NEW EMAIL ADDRESS***berr3415 at uidaho.edu

From ggrothendieck at gmail.com  Mon Jul 17 01:19:00 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 16 Jul 2006 19:19:00 -0400
Subject: [R] Manipulation involving arrays
In-Reply-To: <f4a0f6a67532.44ba8156@johnshopkins.edu>
References: <f4a0f6a67532.44ba8156@johnshopkins.edu>
Message-ID: <971536df0607161619j9e1ee32s3a03c8c2df21963c@mail.gmail.com>

The double loop is the same as:

 Tbar[] <- matrix(tarray, 9) %*% t(wt) / rep(wti, each = 9)


On 7/16/06, RAVI VARADHAN <rvaradhan at jhmi.edu> wrote:
> Hi,
>
> I have the following piece of code that is part of a larger function.  This piece is the most time consuming part of the function, and I would like to make this a bit more efficient.  Could anyone suggest a way to do this faster?
>
> In particular, I would like to replace the nested "for" loop with a faster construct.  I tried things like "kronecker" and "outer" combined with apply, but couldn't get it to work.
>
>
> Here is a sample code:
>
>  ##########################
>  n <- 120
>  sigerr <- 5
>  covmat <- diag(c(8,6,3.5))
>  mu <- c(105,12,10)
>  mcsamp <- 10000
>
>  Tbar <- array(0, dim=c(3,3,n))
>
>  # theta is a mcsamp x 3 matrix
>  theta <- mvrnorm(mcsamp, mu = mu, Sigma = covmat)
>
>  wt <- matrix(runif(n*mcsamp),n,mcsamp)
>  wti <- apply(wt,1,sum)
>
>  tarray <- array(apply(theta,1,function(x)outer(x,x)),dim=c(3,3,mcsamp))
>
>  for (i in 1:n) {
>  for (k in 1:mcsamp) {
>  Tbar[,,i] <- Tbar[,,i] + wt[i,k] * tarray[,,k]
>  }
>  Tbar[,,i] <- Tbar[,,i] / wti[i]
>  }
>


From spencer.graves at pdf.com  Mon Jul 17 02:28:33 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 16 Jul 2006 17:28:33 -0700
Subject: [R] sem question
In-Reply-To: <003401c6a682$c7619be0$0f22a8c0@intiet.ru>
References: <003401c6a682$c7619be0$0f22a8c0@intiet.ru>
Message-ID: <44BAD9B1.90706@pdf.com>



MODEL UNDERIDENTIFIED?

	  I've looked at 'sem' for many years but never found that application 
that seemed to me to require that machinery.  However, I know that it's 
very easy to get models that are "underidentified."  One of the simplest 
cases is the classical "errors in x regression" problem:

Observe:
	  X = xi + e.x, e.x~N(0, s2.x)
	  Y = eta + e.y, e.y~N(0, s2.y)
Model:
	  eta = a+b*xi

	  If I'm not mistaken, I believe that it is theoretically impossible to 
estimate a, b, s2.x, and s2.y without additional information, like for 
example the ratio between s2.x and s2.y.


LAGS IN BOTH TIME AND SPACE?

	  I've copied John Fox, the 'sem' package author and maintainer, on 
this reply.  He might educate us both on how to include lags in both 
time and space into an 'sem' model.

	  Failing that, are you familiar with Pinheiro and Bates (2000) 
Mixed-Effects Models in S and S-Plus (Springer).  This book and the 
companion 'nlme' packages include facilities for linear and nonlinear 
models in both space and time.  The follow-on 'lme4' package and 
accompanying 'lmer' function will also handle non-normal response 
distributions.  I'm a firm believer in trying the simple things first, 
and I think the mixed-effects models are simpler than 'sem', though 
Prof. Fox may wish to disabuse me of my ignorance on that point.


MORE HELP?

	  If you would like more from this listserve than just this, please 
submit another post.  When you do, however, please include a simple, 
self contained example to illustrate briefly what you want, what you 
tried, and the deficiencies with what you tried, as suggested in the 
posting guide! "www.R-project.org/posting-guide.html".

	  Hope this helps.
	  Spencer Graves 	  	

Denis Fomchenko wrote:
> Dear all,
> 
> I am trying to estimate simultaneous equation model concerning growth in russian regions.
> I run the analysis by means of FIML in R sem package.
> I am not familiar with SEM yet, but I've just got several suitable estimated specifications.
> Nevertheless, sometimes R gives the following warning message:
> 
> Warning message:
> Negative parameter variances.
> Model is probably underidentified.
>  in: sem.default(ram = ram, S = S, N = N, param.names = pars, var.names = vars,  
> 
> I check for rank condition - all three equations in the system are turned out to be exact...
> 
> Does anybody know what it means? and how to handle with that problem?
> 
> P.S.
> Do you know any examples of models estimated in SEM by means of FIML, incorporating spatial lag on endogenous variable?
> 
> Thanks, in advance
> 
> Denis Fomchenko
> research fellow
> Department for Economic Development Problems
> Institute for the Economy in Transition
> 5, Gazetny lane, Moscow 125993, Russia
> e-mail: fomchenko at iet.ru
> http://www.iet.ru
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From spencer.graves at pdf.com  Mon Jul 17 02:59:26 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 16 Jul 2006 17:59:26 -0700
Subject: [R] MLE and QR classes
In-Reply-To: <OF39C0C230.808EC0BF-ON032571AA.00674426-032571AA.00679F6C@serasa.com.br>
References: <OF39C0C230.808EC0BF-ON032571AA.00674426-032571AA.00679F6C@serasa.com.br>
Message-ID: <44BAE0EE.10609@pdf.com>

	  I don't understand your question.  First, I'm not familiar with the 
'wls' package;  I found no such package by that name via 
"www.r-project.org" -> CRAN -> (select a local mirror) -> Packages.

	  The 'qr' function in the 'quandreg' package looks straightforward to 
me.  Have you worked through the examples in the 'qr' help page?  The 
look to me like they follow the standard syntax of 'lm'.  If you don't 
understand the 'lm' syntax, I encourage you to spend some quality time 
with appropriate sections of Venables and Ripley (2003) Modern Applied 
Statistics with S, 4th ed. (Springer).

	  If you'd like more help from this listserve, please submit another 
question.  However, please include a simple, self-contained example of 
something you tried to help illustrate your question (as suggested in 
the posting guide! "www.R-project.org/posting-guide.html").

	  Hope this helps.
	  Spencer Graves

ricardosilva at serasa.com.br wrote:
> Hi,
> 
> I load my data set and separate it as folowing:
> 
> presu <- read.table("C:/_Ricardo/Paty/qtdata_f.txt", header=TRUE, sep="\t",
> na.strings="NA", dec=".", strip.white=TRUE)
> dep<-presu[,3];
> exo<-presu[,4:92];
> 
> Now, I want to use it using the wls and quantreg packages. How I change the
> data classes for mle and rq objects?
> 
> Thanks a lot,
> ________________________________________
> Ricardo Gon?alves Silva, M. Sc.
> Apoio aos Processos de Modelagem Matem?tica
> Econometria & Inadimpl?ncia
> Serasa S.A.
> (11) - 6847-8889
> ricardosilva at serasa.com.br
> 
> **********************************************************************************
> 
> As informa??es contidas nesta mensagem e no(s) arquivo(s) anexo(s) s?o
> endere?adas exclusivamente ?(s) pessoa(s) e/ou institui??o(?es) acima
> indicada(s), podendo conter dados confidenciais, os quais n?o podem, sob
> qualquer forma ou pretexto, ser utilizados, divulgados, alterados,
> impressos ou copiados, total ou parcialmente, por pessoas n?o autorizadas.
> Caso n?o seja o destinat?rio, favor providenciar sua exclus?o e notificar o
> remetente imediatamente.  O uso impr?prio ser? tratado conforme as normas
> da empresa e da legisla??o em vigor.
> Esta mensagem expressa o posicionamento pessoal do subscritor e n?o reflete
> necessariamente a opini?o da Serasa.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From jfox at mcmaster.ca  Mon Jul 17 04:01:07 2006
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 16 Jul 2006 22:01:07 -0400
Subject: [R] sem question
In-Reply-To: <44BAD9B1.90706@pdf.com>
Message-ID: <20060717020108.KZOX24981.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Spencer and Denis,

I've been traveling for a while and away from r-help, so I didn't see
Denis's question until now.

I'm not familiar with applications of SEMs that have lags in time and space.
As to the identification status of Denis's model, it's hard to know about
that in the abstract. What's the model? 

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Spencer Graves [mailto:spencer.graves at pdf.com] 
> Sent: Sunday, July 16, 2006 7:29 PM
> To: Denis Fomchenko
> Cc: r-help at stat.math.ethz.ch; John Fox
> Subject: Re: [R] sem question
> 
> 
> 
> MODEL UNDERIDENTIFIED?
> 
> 	  I've looked at 'sem' for many years but never found 
> that application that seemed to me to require that machinery. 
>  However, I know that it's very easy to get models that are 
> "underidentified."  One of the simplest cases is the 
> classical "errors in x regression" problem:
> 
> Observe:
> 	  X = xi + e.x, e.x~N(0, s2.x)
> 	  Y = eta + e.y, e.y~N(0, s2.y)
> Model:
> 	  eta = a+b*xi
> 
> 	  If I'm not mistaken, I believe that it is 
> theoretically impossible to estimate a, b, s2.x, and s2.y 
> without additional information, like for example the ratio 
> between s2.x and s2.y.
> 
> 
> LAGS IN BOTH TIME AND SPACE?
> 
> 	  I've copied John Fox, the 'sem' package author and 
> maintainer, on this reply.  He might educate us both on how 
> to include lags in both time and space into an 'sem' model.
> 
> 	  Failing that, are you familiar with Pinheiro and 
> Bates (2000) Mixed-Effects Models in S and S-Plus (Springer). 
>  This book and the companion 'nlme' packages include 
> facilities for linear and nonlinear models in both space and 
> time.  The follow-on 'lme4' package and accompanying 'lmer' 
> function will also handle non-normal response distributions.  
> I'm a firm believer in trying the simple things first, and I 
> think the mixed-effects models are simpler than 'sem', though 
> Prof. Fox may wish to disabuse me of my ignorance on that point.
> 
> 
> MORE HELP?
> 
> 	  If you would like more from this listserve than just 
> this, please submit another post.  When you do, however, 
> please include a simple, self contained example to illustrate 
> briefly what you want, what you tried, and the deficiencies 
> with what you tried, as suggested in the posting guide! 
> "www.R-project.org/posting-guide.html".
> 
> 	  Hope this helps.
> 	  Spencer Graves 	  	
> 
> Denis Fomchenko wrote:
> > Dear all,
> > 
> > I am trying to estimate simultaneous equation model 
> concerning growth in russian regions.
> > I run the analysis by means of FIML in R sem package.
> > I am not familiar with SEM yet, but I've just got several 
> suitable estimated specifications.
> > Nevertheless, sometimes R gives the following warning message:
> > 
> > Warning message:
> > Negative parameter variances.
> > Model is probably underidentified.
> >  in: sem.default(ram = ram, S = S, N = N, param.names = pars, 
> > var.names = vars,
> > 
> > I check for rank condition - all three equations in the 
> system are turned out to be exact...
> > 
> > Does anybody know what it means? and how to handle with 
> that problem?
> > 
> > P.S.
> > Do you know any examples of models estimated in SEM by 
> means of FIML, incorporating spatial lag on endogenous variable?
> > 
> > Thanks, in advance
> > 
> > Denis Fomchenko
> > research fellow
> > Department for Economic Development Problems Institute for 
> the Economy 
> > in Transition 5, Gazetny lane, Moscow 125993, Russia
> > e-mail: fomchenko at iet.ru
> > http://www.iet.ru
> >  
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


From xmeng at capitalbio.com  Mon Jul 17 04:56:24 2006
From: xmeng at capitalbio.com (XinMeng)
Date: Mon, 17 Jul 2006 10:56:24 +0800
Subject: [R] a question about data combination
Message-ID: <353104984.31462@capitalbio.com>

Hello sir: 

Here's the data: a b c d 1 2 3 4 

I wanna know how to get: 
ab ac ad bc bd cd 
12 13 14 23 24 34 

Thanks a lot! My best  
 




------------------------------
*******************************************
Xin Meng 
Capitalbio Corporation
National Engineering Research Center 
for Beijing Biochip Technology 
BioPharma-informatics & Software Dept. 
Research Engineer
Tel: +86-10-80715888/80726868-6438
Fax: +86-10-80726790
Email??xmeng at capitalbio.com 
Address:18 Life Science Parkway, 
Changping District, Beijing 102206, China


From kemerson at uoregon.edu  Mon Jul 17 04:18:10 2006
From: kemerson at uoregon.edu (Kevin J Emerson)
Date: Sun, 16 Jul 2006 19:18:10 -0700
Subject: [R] Getting rid of for loops
Message-ID: <BBEAIDIAANOEADEFKHPEAEFJCBAA.kemerson@uoregon.edu>

Hello R-users!

I have a style question.  I know that for loops are somewhat frowned upon in
R, and I was trying to figure out a nice way to do something without using
loops, but figured that i could get it done quickly using them.  I am now
looking to see what kind of tricks I can use to make this code a bit more
aesthetically appealing to other R users (and learn something about R along
the way...).

Here's the problem.  I have a data.frame with 4 columns of dependent
variables and then ~35 columns of predictor variables (factors) [for those
interested, it is a qtl problem, where the predictors are genotypes at DNA
markers and the dependent variable is a biological trait].  I want to go
through all pairwise combinations of predictor variables and perform an
anova with two predictors and their interaction on a given dependent
variable.  I then want to store the p.value of the interaction term, along
with the predictor variable information.  So I want to end up with a
dataframe at the end with the two variable names and the interaction p value
in each row, for all pairwise combinations of predictors.  I used the
following code:

# qtl is the original data.frame, and my dependent var in this case is
# qtl$CPP.

marker1 <- NULL
marker2 <- NULL
p.interaction <- NULL
for ( i in 5:40) {   # cols 5 - 41 are the predictor factors
	for (j in (i+1):41) {
		marker1 <- rbind(marker1,names(qtl)[i])
		marker2 <- rbind(marker2,names(qtl)[j])
		tmp2 <- summary(aov(tmp$CPP ~ tmp[,i] * tmp[,j]))[[1]]
		p.interaction <- rbind(p.interaction, tmp2$"Pr(>F)"[3])
	}
}

I have two questions:
(1) is there a nicer way to do this without having to invoke for loops?
(2) my other dependent variables are categorical in nature.  I need
basically the same information - I am looking for information regarding the
interaction of predictors on a categorical variable.  Any ideas on what
tests to use? (I am new to analysis of all-categorical data).

Thanks in advance!
Kevin

--------------------------------------
--------------------------------------
Kevin Emerson
Center for Ecology and Evolutionary Biology
1210 University of Oregon
Eugene, OR 97403
USA
kemerson at uoregon.edu


From spencer.graves at pdf.com  Mon Jul 17 04:33:06 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 16 Jul 2006 19:33:06 -0700
Subject: [R] ols/gls or systemfit (OLS, WLS, SUR) give identical results
In-Reply-To: <20060713232716.38834.qmail@web52507.mail.yahoo.com>
References: <20060713232716.38834.qmail@web52507.mail.yahoo.com>
Message-ID: <44BAF6E2.30407@pdf.com>

	  I can't say without more information.  If the system were saturated 
(i.e., has as many equations as unknowns), you should get the same 
answer from all the different methods.  However, I just tried a 
saturated model in 'systemfit', with the following results:

 > DF2 <- data.frame(y=1:2, x=3:4)
 > lm(y~x, DF2)
Call:
lm(formula = y ~ x, data = DF2)

Coefficients:
(Intercept)            x
          -2            1
 > library(systemfit)
 > systemfit("OLS", list(eqn=y~x), data=DF2)
Error in solve.default(sigma, tol = solvetol) :
	system is computationally singular: reciprocal condition number = 0
 >
	  If you'd like more help from this listserve, please supply a simple, 
self-contained example to illustrate your question (as suggested in the 
posting guide! "www.R-project.org/posting-guide.html").

	  Hope this helps.
	  Spencer Graves

Mihai Nica wrote:
>    I might be sorry for asking this question :-)
> 
>     I have two equations and I tried to estimate 
them individually with "lm" and "gls", and then in a
system (using systemfit)  with "OLS", "WLS" and "SUR".
Quite surprisingly (for myself at least) the results
are identical to the last digit.
> 
>     Could someone (please!) give a hint as to what 
am I doing wrong?
> 
> Thanks,
> 
> mihai
>  		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From jdrapp at gmail.com  Mon Jul 17 05:15:14 2006
From: jdrapp at gmail.com (justin rapp)
Date: Sun, 16 Jul 2006 23:15:14 -0400
Subject: [R] Correlation Mapping
Message-ID: <af81db5a0607162015s33576ce6nd33dc801ceb135fc@mail.gmail.com>

On the cover of Zivot and Wang's Modeling Financial Time Series with S
Plus, there is a correlation plot that seems to indicate the strength
of correlation with color-coded squares, so that more highly
correlated stocks appear darker red.  If anybody out there is familiar
with the book or understands what I am talking about, I am curious as
to whether or not there is a similar function in R and how I can call
it up.
Thank you.

jdr


From darrenleeweber at gmail.com  Mon Jul 17 06:23:17 2006
From: darrenleeweber at gmail.com (Darren Weber)
Date: Sun, 16 Jul 2006 21:23:17 -0700
Subject: [R] planned comparisons for ANOVA
Message-ID: <d2095b8c0607162123s464336c7j38a8d25911203473@mail.gmail.com>

Hi,

we need some help to define planned comparisons.  I've based my
understanding of the problem on reading Tabachnick and Fidell (2006),
ie:

http://www.ablongman.com/catalog/academic/product/0,1144,0205459382,00.html

I don't understand how to specify planned comparisons in R.  I've not
found explanations for this in MASS or elsewhere.  There is only
discussion of the contrast option to ANOVA in general terms, there are
no examples for the analysis of planned comparisons.

I have an ANOVA design, described as a factorial design, with both
between-subjects and within-subjects factors.  There are 2 subject
groups.  Although there are matched individuals across groups (matched
for extraneous demographic variables), we consider them a
between-subject factor with 2 categorical levels (controls, patients).
 The dependent variable is a multivariate recording from 124
electrodes on the scalp, to measure electric potential from the scalp
surface.  These recordings are summarised into regional activity for
the left and right hemisphere.   So hemisphere is a within-subjects
factor that has 2 levels (left and right).  The last factor is an
experimental manipulation, a visual task contains three types of
events.  This is a  within-subjects factor with three levels (S1, S2,
S3).

Our planned comparisons are:

1. test the group mean difference for S1 vs S2 (in the absence of S3)
2. test the group mean difference for S2 vs S3 (in the absence of S1)

This is the current form of the ANOVA specification for R:

aov( Y ~ (Task*Hemisphere*Group) +
                Error( Subject/(Task*Hemisphere) )

How can we add planned comparisons to this specification?  Can we add
just one planned comparison matrix, with rows for 1 & 2 above, or do
we need to run the model twice, once for each planned comparison?
Alternatively, is there a function  to compute the planned comparisons
after running the full ANOVA model?

Thanks, Darren

PS, I was trained well on using SPSS, but I am trying to make a switch
to R.  You help with this would be really appreciated.  We need to get
our results revised and published soon.


From jaydeep_chovatia at persistent.co.in  Mon Jul 17 06:52:05 2006
From: jaydeep_chovatia at persistent.co.in (jaydeep chovatia)
Date: Mon, 17 Jul 2006 10:22:05 +0530
Subject: [R] Handshake exception in Rserve
Message-ID: <200607170452.AMI89989@persistent.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060717/ef7e0ef8/attachment.pl 

From blomsp at ozemail.com.au  Mon Jul 17 07:01:01 2006
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Mon, 17 Jul 2006 15:01:01 +1000
Subject: [R] planned comparisons for ANOVA
In-Reply-To: <d2095b8c0607162123s464336c7j38a8d25911203473@mail.gmail.com>
References: <d2095b8c0607162123s464336c7j38a8d25911203473@mail.gmail.com>
Message-ID: <44BB198D.7060707@ozemail.com.au>

Darren Weber wrote:

[snip]
> Our planned comparisons are:
>
> 1. test the group mean difference for S1 vs S2 (in the absence of S3)
> 2. test the group mean difference for S2 vs S3 (in the absence of S1)
>
> This is the current form of the ANOVA specification for R:
>
> aov( Y ~ (Task*Hemisphere*Group) +
>                 Error( Subject/(Task*Hemisphere) )
>   
> How can we add planned comparisons to this specification?  Can we add
> just one planned comparison matrix, with rows for 1 & 2 above, or do
> we need to run the model twice, once for each planned comparison?
>   
There are a couple of ways to do this in R. Perhaps the easiest is to 
use make.contrasts in the gmodels package.

cmat <- rbind("S1 v S2" = c(1, -1, 0),
                      "S2 v S3" = c(0, 1, -1))
library(gmodels)
fit <- aov( Y ~ Task*Hemisphere*Group + Error( 
Subject/(Task*Hemisphere), contrasts=list("Task"=make.contrasts(cmat )))
summary(fit)

> Alternatively, is there a function  to compute the planned comparisons
> after running the full ANOVA model?
>   
see ?fit.contrast in gmodels  for this.

HTH,

Simon.
> Thanks, Darren
>
> PS, I was trained well on using SPSS, but I am trying to make a switch
> to R.  You help with this would be really appreciated.  We need to get
> our results revised and published soon.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>   


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C


From ggrothendieck at gmail.com  Mon Jul 17 07:52:35 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 17 Jul 2006 01:52:35 -0400
Subject: [R] Correlation Mapping
In-Reply-To: <af81db5a0607162015s33576ce6nd33dc801ceb135fc@mail.gmail.com>
References: <af81db5a0607162015s33576ce6nd33dc801ceb135fc@mail.gmail.com>
Message-ID: <971536df0607162252k5224ccb2x91698b1380761481@mail.gmail.com>

I have not seen that book cover but I assume the
question is how to plot the cells of a correlation
matrix in different colors.  Try heatmap or the gplot
package function heatmap.2 .  For example, we create a
correlation matrix, K, from the first 4 columns of
the iris data set and create a heatmap using the
bluered color scheme:

# heatmap.2
library(gplots)
K <- cor(iris[,1:4])
heatmap.2(K, col = bluered(16), cexRow = .7, cexCol = .7, symm = TRUE,
	dend = "row", trace = "none", main = "Iris Data")

balloonplot, also in the gplots package, and image in graphics (i.e. core R)
might be other functions to look at.

On 7/16/06, justin rapp <jdrapp at gmail.com> wrote:
> On the cover of Zivot and Wang's Modeling Financial Time Series with S
> Plus, there is a correlation plot that seems to indicate the strength
> of correlation with color-coded squares, so that more highly
> correlated stocks appear darker red.  If anybody out there is familiar
> with the book or understands what I am talking about, I am curious as
> to whether or not there is a similar function in R and how I can call
> it up.
> Thank you.
>
> jdr
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From vincent at 7d4.com  Sun Jul 16 15:33:40 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Sun, 16 Jul 2006 15:33:40 +0200
Subject: [R] CFD Plots in R and Other Things
In-Reply-To: <a2b3004b0607160509x5c15ff71g83acd36473924463@mail.gmail.com>
References: <a2b3004b0607160509x5c15ff71g83acd36473924463@mail.gmail.com>
Message-ID: <44BA4034.501@7d4.com>

Lorenzo Isella a ?crit :

> Tipically, these sets of data are plotted in 2D with r and z as axis
> and the velocity  field represented by using colours explained by a
> legenda.
> Can R do anything like this?

?image

hih


From petr.pikal at precheza.cz  Mon Jul 17 09:04:59 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 17 Jul 2006 09:04:59 +0200
Subject: [R] Excel to R
In-Reply-To: <7.0.0.16.2.20060715113721.0526b5a0@centroin.com.br>
References: <44B37251.2090606@good.ibl.fr>
Message-ID: <44BB52BB.17403.31C71A@localhost>

Hi

based on your output qw is not a factor. What does say

str(data)

If your file is coded 1,2,9 it was imported as numeric and changing 9 
to NA inside R does not change its nature to factor. You has to 
explicitly convert qw to factor e.g.

data$qw<as.factor(data$qw)

HTH
Petr


On 15 Jul 2006 at 11:50, Bernardo Rangel tura wrote:

Date sent:      	Sat, 15 Jul 2006 11:50:14 -0300
To:             	R-help at stat.math.ethz.ch
From:           	Bernardo Rangel tura <tura at centroin.com.br>
Subject:        	[R] Excel to R

> 
> >Hi peolple!
> 
> 
> I have a many excel tables with mode than 100 variables. And I want
> use R to analize that.
> 
> But I have a problem, a group of this variables (more than 50) in any
> table is a factor and other part is a number.
> 
> Tha factors variables have tha values enconde this form (1=Yes,2=No
> and 9 = NA)
> 
> Well I use this scripts to import the database
> 
> require(RODBC)
> channel <- odbcConnectExcel("f:/teste.xls")
> data <- sqlFetch(channel, "Sheet1")
>   summary(data)
>         qw              ee
>   Min.   :1.000   Min.   :1.000
>   1st Qu.:1.000   1st Qu.:1.500
>   Median :1.000   Median :2.000
>   Mean   :1.333   Mean   :2.429
>   3rd Qu.:1.750   3rd Qu.:3.500
>   Max.   :2.000   Max.   :4.000
>   NA's   :1.000
> 
> 
> But qw is a factor (and is colnum type isvtext)
> 
> Is possible modify my script for this utcome
> 
>  > summary(data)
>      qw          ee
>   1   :4   Min.   :1.000
>   2   :2   1st Qu.:1.500
>   NA's:1   Median :2.000
>            Mean   :2.429
>            3rd Qu.:3.500
>            Max.   :4.000
> 
> 
> Thanks in advance
> 
> Bernardo Rangel Tura, MD, MSc
> National Institute of Cardiology Laranjeiras
> Rio de Janeiro Brazil
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From vincent at 7d4.com  Mon Jul 17 09:17:20 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Mon, 17 Jul 2006 09:17:20 +0200
Subject: [R] Correlation Mapping
In-Reply-To: <af81db5a0607162015s33576ce6nd33dc801ceb135fc@mail.gmail.com>
References: <af81db5a0607162015s33576ce6nd33dc801ceb135fc@mail.gmail.com>
Message-ID: <44BB3980.2000300@7d4.com>

justin rapp a ?crit :

> On the cover of Zivot and Wang's Modeling Financial Time Series with S
> Plus, there is a correlation plot that seems to indicate the strength
> of correlation with color-coded squares, so that more highly
> correlated stocks appear darker red.  If anybody out there is familiar
> with the book or understands what I am talking about, I am curious as
> to whether or not there is a similar function in R and how I can call
> it up.
> Thank you.
> jdr

Have a look at : http://7d4.com/r/


From petr.pikal at precheza.cz  Mon Jul 17 09:20:14 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 17 Jul 2006 09:20:14 +0200
Subject: [R] Find peaks in histograms / Analysis of cumulative frequency
In-Reply-To: <3483f8d50607150455o637381d7s88d113ac5b7a7a5e@mail.gmail.com>
Message-ID: <44BB564E.27793.3FBD42@localhost>

Hi

There are some mail archives about peaks
eg.
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/33097.html 

HTH
Petr



On 15 Jul 2006 at 13:55, Ulrik Stervbo wrote:

Date sent:      	Sat, 15 Jul 2006 13:55:26 +0200
From:           	"Ulrik Stervbo" <ulriks at ruc.dk>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Find peaks in histograms / Analysis of cumulative frequency

> Hello all,
> 
> I have some histograms of amount of DNA in some cells (DU145 cells
> overexpressing Bax and Bcl-xL for those who wish to know). The
> histograms show not only two peaks as expected, but three, indicating
> that some cells have more than normal amounts of DNA.
> 
> I am interested in knowing how much of the cell populations are in
> each peak as well as between.
> 
> I am not really sure how to go about it; I have been considering
> fitting a gaussian distribution to each peak and integrate the part
> between the peaks as described by Watson et al (1987 Cytometry 8:1-8).
> A more straight forward and more visual approach appears to be
> plotting the cumulative frequencies. In either case, I should like to
> find the peaks in the histogram automatically, as well as getting
> proper information about the peaks.
> 
> How would I go about finding peaks using R? Also I have really not
> been able to figure out how to fit a distribution.
> 
> Is there a way to analyse the cumulative frequencies? the knots()
> function appears to return far too many knots.
> 
> I am relatively new to R, but do have good programming experience,
> though I am mostly biologist.
> 
> Thank you in advance for any inputs.
> 
> PS. An example of the histogram can be found
> here<http://photos1.blogger.com/blogger/7029/2724/1600/DU145-Bax3-Bcl-
> xL.png>
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From Johan.VanKerckhoven at econ.kuleuven.be  Mon Jul 17 09:21:41 2006
From: Johan.VanKerckhoven at econ.kuleuven.be (Van Kerckhoven, Johan)
Date: Mon, 17 Jul 2006 09:21:41 +0200
Subject: [R] Questions concerning function 'svm' in e1071 package
	(solved)
In-Reply-To: <mailman.5.1151920802.10328.r-help@stat.math.ethz.ch>
Message-ID: <F9E424F22569F3469181E1BA1EE24CED876218@ECONSRVEX5.econ.kuleuven.ac.be>

Greetings everyone,

The problem has been solved. A faulty evaluation of the decision
Function was the culprit.

Signed,

Johan Van Kerckhoven

> Greetings everyone,
>
> I have the following problem (illustrating R-code at bottom of mail):
> Given a training sample with binary outcomes (-1/+1), I train a linear
> Support Vector Machine to separate them. Afterwards, I compute the
> weight vector w in the usual way, and obtain the fitted values as
> w'x + b > 0  ==>  yfitted = 1, otherwise -1.
>
> However, upon verifying with the 'predict' method, the outcomes do not
> match up as they should. I've already tried to find information
> concerning this issue on the R-help board, but to no avail. Can any of
> you point me in the right direction?
>
> Signed,
>
> Johan Van Kerckhoven
> ORSTAT and University Center of Statistics
> Katholieke Universiteit Leuven

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From dieter.menne at menne-biomed.de  Mon Jul 17 09:41:39 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 17 Jul 2006 07:41:39 +0000 (UTC)
Subject: [R] put R on a web server
References: <200607152344.k6FNi0Om030065@gator.dt.uh.edu>
Message-ID: <loom.20060717T093452-633@post.gmane.org>

Erin Hodgess <hodgess <at> gator.dt.uh.edu> writes:

> Has anyone put R on a web server any time, recently, please?
> (Red Hat Linux)
> 
> The University of Montana put a version up in 2003, but
> I was wondering if anyone had done so, please?
> 
> Also, where would I find information on such an installation, please?

Mmh, putting R on a web server is not much different from putting it on an other
linux installation. The question is  probably more how to get R to work. RPad is
a  package that works, but as far as I remember the focus is on flexibility for
Intranets, with high risks when used in the wild. If you want to run some
specialized tasks only with fixed code, you could try our phpSerialize, which
can run in the wild if your php code is ok, but it is by design not end-user
flexible.

Dieter


From h.wickham at gmail.com  Mon Jul 17 10:39:36 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 17 Jul 2006 09:39:36 +0100
Subject: [R] Trailing on r-help messages
In-Reply-To: <971536df0607161331h546bc59fkd911ca87b1fd6a36@mail.gmail.com>
References: <971536df0607160946v7e0a0c1al9e3956885bab73e8@mail.gmail.com>
	<f8e6ff050607161116m76f9ff32j1c1ad66c5810db52@mail.gmail.com>
	<971536df0607161151y54a8baceu9bb9edaf5a628f94@mail.gmail.com>
	<f8e6ff050607161324l7e85d3b3t7fd89046c22d36f4@mail.gmail.com>
	<971536df0607161331h546bc59fkd911ca87b1fd6a36@mail.gmail.com>
Message-ID: <f8e6ff050607170139q1e0c6565s3dc6c19954af74c0@mail.gmail.com>

> I think that is likely true but it would at least mean that they had
> seen it repeatedly and there would really be no excuse for not following it
> (unlike the current situation where one needs to take action to follow
> the posting guide link and then read a lengthy page).

Logically, that makes sense.  However, try asking a group of R users
how to cite R (or how to find out how to cite R).

I think it's a good idea to make the default list signature as good as
possible, I just don't think it will have that much effect.  Supplying
a small reproducible example when reporting an error is "common-sense"
(how else will the developers/others be able to replicate it?) and yet
it is very rare (not just in R, but in any project).

Hadley


From mineoeli at unipa.it  Mon Jul 17 11:25:47 2006
From: mineoeli at unipa.it (Elio Mineo)
Date: Mon, 17 Jul 2006 11:25:47 +0200
Subject: [R] put R on a web server
In-Reply-To: <loom.20060717T093452-633@post.gmane.org>
References: <200607152344.k6FNi0Om030065@gator.dt.uh.edu>
	<loom.20060717T093452-633@post.gmane.org>
Message-ID: <44BB579B.50602@unipa.it>

Maybe R-php?
See the URL: http://dssm.unipa.it/R-php/

Dieter Menne wrote:

>Erin Hodgess <hodgess <at> gator.dt.uh.edu> writes:
>
>  
>
>>Has anyone put R on a web server any time, recently, please?
>>(Red Hat Linux)
>>
>>The University of Montana put a version up in 2003, but
>>I was wondering if anyone had done so, please?
>>
>>Also, where would I find information on such an installation, please?
>>    
>>
>
>Mmh, putting R on a web server is not much different from putting it on an other
>linux installation. The question is  probably more how to get R to work. RPad is
>a  package that works, but as far as I remember the focus is on flexibility for
>Intranets, with high risks when used in the wild. If you want to run some
>specialized tasks only with fixed code, you could try our phpSerialize, which
>can run in the wild if your php code is ok, but it is by design not end-user
>flexible.
>
>Dieter
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>

-- 
----------------------------------------------------------------------------------
Prof. Angelo M. Mineo
Dipartimento di Scienze Statistiche e Matematiche "S. Vianelli"
Universit? degli Studi di Palermo
Viale delle Scienze
90128 Palermo
url: http://dssm.unipa.it/elio


From msubianto at gmail.com  Mon Jul 17 12:53:22 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Mon, 17 Jul 2006 12:53:22 +0200
Subject: [R] install.packages for local zip files
In-Reply-To: <44BAAACB.80901@mindspring.com>
References: <44BAAACB.80901@mindspring.com>
Message-ID: <c7c17cef0607170353i3eb9dcrc851dfacd71c0d84@mail.gmail.com>

On 7/16/06, Daniel Gatti <dmgatti at mindspring.com> wrote:
> O/S: Linux
> R version : 2.2.1
>
> The R server doesn't have http internet access.  And the sys admins will
> not install the R libraries that I requested.  So I have downloaded the
> packages that I want to intall and have moved them into my home
> directory on the server.  These are a series of *.tar.gz files.  I want
> to install the R libraries in my home directory, but I can't get it to
> work.  According to the install.packages documentation :
>
> install.packages(pkgs, lib, repos = getOption("repos"),  contriburl =
> contrib.url(repos, type), method, available = NULL, destdir = NULL,
> installWithVers = FALSE, dependencies = FALSE, type = getOption("pkgType"))
>
> repos: character vector, the base URL(s) of the repositories to use,
>        i.e., .... Can be 'NULL' to install from local '.tar.gz' files.
>
> contriburl: URL(s) of the contrib section of the repositories. ......
>        Can be 'NULL' to install from local '.tar.gz' files.
>
>
>  pkgs: character vector of the short names of packages/bundles whose
>           current versions should be downloaded from the repositories.
>           If 'repos = NULL', a character vector of file paths of
>           '.tar.gz' files.  These can be source archives or binary
>           package/bundle archive files (as created by 'R CMD build
>           --binary'). ......
>
>  lib: character vector giving the library directories where to install
> the packages.  Recycled as needed.
>
> So I have issued a command like this:
>
>  > install.packages(pkgs="~/Rdownloads/hgug4112a_1.12.0.tar.gz", lib =
> "~/Rlib", repos=NULL, contriburl=NULL)
>
> Warning in download.packages(pkgs, destdir = tmpd, available =
> available,  :
>          no package '~/Rdownloads/hgug4112a_1.12.0.tar.gz' at the
> repositories
>
> As far as I can tell, I've given it the full path to the zip file, the
> directory in which to install the library and I've set the repository
> path to 'NULL' to indicate that I'm installing from an already
> downloaded zip file.  But I'm missing something.  Any ideas?
>
> Thanks,
> Dan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



Put your packages, for example, in directory "/dir/of/pkgs"
library(tools)
write_PACKAGES("/dir/of/pkgs")

and the package will be installed to "/dir/of/R/libs"

install.packages("NameOfPkgs",
                 lib="/dir/of/R/libs",
                 repos=NULL,
                 dependencies=TRUE,
                 contriburl="file:////dir/of/pkgs")

Best, Muhammad Subianto


From hflick at gmail.com  Mon Jul 17 13:05:37 2006
From: hflick at gmail.com (Holger Flick)
Date: Mon, 17 Jul 2006 13:05:37 +0200
Subject: [R] information about a function
Message-ID: <7c7d8ad40607170405m32b36d32s25af9c363439eff9@mail.gmail.com>

Hi people,

I am new in this list and could not find a FAQ for it in particular,
furthermore I could not find my question answered in the official R
FAQ or docs.

I have simply something like this:

> f<-approxfun(data[,1],data[,2])

and f is:

> f
function (v)
.C("R_approx", as.double(x), as.double(y), as.integer(n), xout = as.double(v),
    as.integer(length(v)), as.integer(method), as.double(yleft),
    as.double(yright), as.double(f), NAOK = TRUE, PACKAGE = "base")$xout
<environment: 02106C24>

I also used "locPoly".

Both yield either a function or a data frame of values.

Is there a way to get a mathematical representation of the function,
e.g. a polynomial of any order? Something like
 ax^3+bx^2+cx^1+d or similar if it is of degree 3?

Basically, I want to create functions of measured values.

Thanks in advance for any hints.

--
Holger



-- 
Holger


From petr.pikal at precheza.cz  Mon Jul 17 13:51:43 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 17 Jul 2006 13:51:43 +0200
Subject: [R] information about a function
In-Reply-To: <7c7d8ad40607170405m32b36d32s25af9c363439eff9@mail.gmail.com>
Message-ID: <44BB95EF.21560.1385314@localhost>

Hi

seems to me like you are looking for lm or nls.

If not you shall be more specific.

HTH
Petr


On 17 Jul 2006 at 13:05, Holger Flick wrote:

Date sent:      	Mon, 17 Jul 2006 13:05:37 +0200
From:           	"Holger Flick" <hflick at gmail.com>
To:             	R-help at stat.math.ethz.ch
Subject:        	[R] information about a function

> Hi people,
> 
> I am new in this list and could not find a FAQ for it in particular,
> furthermore I could not find my question answered in the official R
> FAQ or docs.
> 
> I have simply something like this:
> 
> > f<-approxfun(data[,1],data[,2])
> 
> and f is:
> 
> > f
> function (v)
> .C("R_approx", as.double(x), as.double(y), as.integer(n), xout =
> as.double(v),
>     as.integer(length(v)), as.integer(method), as.double(yleft),
>     as.double(yright), as.double(f), NAOK = TRUE, PACKAGE =
>     "base")$xout
> <environment: 02106C24>
> 
> I also used "locPoly".
> 
> Both yield either a function or a data frame of values.
> 
> Is there a way to get a mathematical representation of the function,
> e.g. a polynomial of any order? Something like
>  ax^3+bx^2+cx^1+d or similar if it is of degree 3?
> 
> Basically, I want to create functions of measured values.
> 
> Thanks in advance for any hints.
> 
> --
> Holger
> 
> 
> 
> -- 
> Holger
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From ripley at stats.ox.ac.uk  Mon Jul 17 13:53:15 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 17 Jul 2006 12:53:15 +0100 (BST)
Subject: [R] install.packages from local .tar.gz (was 'for local zip')
 files
In-Reply-To: <c7c17cef0607170353i3eb9dcrc851dfacd71c0d84@mail.gmail.com>
References: <44BAAACB.80901@mindspring.com>
	<c7c17cef0607170353i3eb9dcrc851dfacd71c0d84@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0607171245001.20661@gannet.stats.ox.ac.uk>

On Mon, 17 Jul 2006, Muhammad Subianto wrote:

> On 7/16/06, Daniel Gatti <dmgatti at mindspring.com> wrote:
> > O/S: Linux
> > R version : 2.2.1
                ^^^^^
That appears to be the problem.  As the posting guide asked you to 
(***before posting***), please update.

And I presume we really are talking about .tar.gz files here, not 'zip 
files' (used on Windows, which did work in 2.2.1).

> install.packages("~/R/packages/contrib/ash_1.0-9.tar.gz", repos=NULL)

works perfectly in current R, but not in the obselete 2.2.1.

> > The R server doesn't have http internet access.  And the sys admins will
> > not install the R libraries that I requested.  So I have downloaded the
> > packages that I want to intall and have moved them into my home
> > directory on the server.  These are a series of *.tar.gz files.  I want
> > to install the R libraries in my home directory, but I can't get it to
> > work.  According to the install.packages documentation :
> >
> > install.packages(pkgs, lib, repos = getOption("repos"),  contriburl =
> > contrib.url(repos, type), method, available = NULL, destdir = NULL,
> > installWithVers = FALSE, dependencies = FALSE, type = getOption("pkgType"))
> >
> > repos: character vector, the base URL(s) of the repositories to use,
> >        i.e., .... Can be 'NULL' to install from local '.tar.gz' files.
> >
> > contriburl: URL(s) of the contrib section of the repositories. ......
> >        Can be 'NULL' to install from local '.tar.gz' files.
> >
> >
> >  pkgs: character vector of the short names of packages/bundles whose
> >           current versions should be downloaded from the repositories.
> >           If 'repos = NULL', a character vector of file paths of
> >           '.tar.gz' files.  These can be source archives or binary
> >           package/bundle archive files (as created by 'R CMD build
> >           --binary'). ......
> >
> >  lib: character vector giving the library directories where to install
> > the packages.  Recycled as needed.
> >
> > So I have issued a command like this:
> >
> >  > install.packages(pkgs="~/Rdownloads/hgug4112a_1.12.0.tar.gz", lib =
> > "~/Rlib", repos=NULL, contriburl=NULL)
> >
> > Warning in download.packages(pkgs, destdir = tmpd, available =
> > available,  :
> >          no package '~/Rdownloads/hgug4112a_1.12.0.tar.gz' at the
> > repositories
> >
> > As far as I can tell, I've given it the full path to the zip file, the
> > directory in which to install the library and I've set the repository
> > path to 'NULL' to indicate that I'm installing from an already
> > downloaded zip file.  But I'm missing something.  Any ideas?
> >
> > Thanks,
> > Dan
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> 
> 
> Put your packages, for example, in directory "/dir/of/pkgs"
> library(tools)
> write_PACKAGES("/dir/of/pkgs")
> 
> and the package will be installed to "/dir/of/R/libs"
> 
> install.packages("NameOfPkgs",
>                  lib="/dir/of/R/libs",
>                  repos=NULL,
>                  dependencies=TRUE,
>                  contriburl="file:////dir/of/pkgs")
> 
> Best, Muhammad Subianto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Mon Jul 17 13:55:42 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 17 Jul 2006 12:55:42 +0100 (BST)
Subject: [R] information about a function
In-Reply-To: <7c7d8ad40607170405m32b36d32s25af9c363439eff9@mail.gmail.com>
References: <7c7d8ad40607170405m32b36d32s25af9c363439eff9@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0607171253490.20661@gannet.stats.ox.ac.uk>

On Mon, 17 Jul 2006, Holger Flick wrote:

> Hi people,
> 
> I am new in this list and could not find a FAQ for it in particular,
> furthermore I could not find my question answered in the official R
> FAQ or docs.
> 
> I have simply something like this:
> 
> > f<-approxfun(data[,1],data[,2])
> 
> and f is:
> 
> > f
> function (v)
> .C("R_approx", as.double(x), as.double(y), as.integer(n), xout = as.double(v),
>     as.integer(length(v)), as.integer(method), as.double(yleft),
>     as.double(yright), as.double(f), NAOK = TRUE, PACKAGE = "base")$xout
> <environment: 02106C24>
> 
> I also used "locPoly".
> 
> Both yield either a function or a data frame of values.
> 
> Is there a way to get a mathematical representation of the function,
> e.g. a polynomial of any order? Something like
>  ax^3+bx^2+cx^1+d or similar if it is of degree 3?

No, because they are not polynomials or any other simple-to-describe 
function.

> Basically, I want to create functions of measured values.

Perhaps you should use tools that fit the sort of functions you are 
interested in:  see e.g. lm() and poly().

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rolf at erdos.math.unb.ca  Mon Jul 17 13:59:26 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Mon, 17 Jul 2006 08:59:26 -0300 (ADT)
Subject: [R] information about a function
Message-ID: <200607171159.k6HBxQF4020327@erdos.math.unb.ca>

It's not clear to me what you want;  the on-line help states that
approxfun() does linear or constant interpolation (the default is
linear).  It is trivial to write down a ``mathematical
representation'' of the resulting function --- but why would you want
to?  The function f() you created does the calculations for you; all
you would be doing is reinventing the wheel.

			cheers,

				Rolf Turner
				rolf at math.unb.ca


From sachinj.2006 at yahoo.com  Mon Jul 17 14:12:47 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Mon, 17 Jul 2006 05:12:47 -0700 (PDT)
Subject: [R] AICc vs AIC for model selection
In-Reply-To: <44B9345A.8030900@pdf.com>
Message-ID: <20060717121247.89448.qmail@web37605.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060717/558fba43/attachment.pl 

From jim at bitwrit.com.au  Tue Jul 18 04:49:29 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Mon, 17 Jul 2006 22:49:29 -0400
Subject: [R] break axis using plotrix
In-Reply-To: <44BA4087.4080509@bio.ntnu.no>
References: <44BA4087.4080509@bio.ntnu.no>
Message-ID: <44BC4C39.6080306@bitwrit.com.au>

Henrik Parn wrote:
> ...
> As you can see, I have problems adding the larger y-values - they end up 
> in the wrong place in the graph. I suppose Jim's warning 'just be 
> careful that the ylim= and labels= arguments match up' is relevant here, 
> but I don't manage to fix it...
> 
> 
> Can anyone help me to plot the large y-values on the right 
> placeaccording to the y-axis labeling?
> I am using R 2.3.1 and WinXp.
> 
Hi Henrik,

Try this instead.

# plot small values
plot(x, meansarr, ylim=c(12, 30), axes=F, type="b", xlab="", ylab="Day")
arrows(x, meansarr-searr, x, meansarr+searr, code = 3,
  angle = 90, length = 0.03)
box()
# x-axis
axis(1, tck=0.01, las=1, at=1:4,
     labels=c("1998", "1999", "2002", "2003"), mgp=c(3, 0.5, 0))
# y-axis
axis(2,at=c(12, 14, 16, 18, 20, 24, 26, 28,
30),labels=c("12","14","16","18","20", "34","36","38","40"))
# break axis
axis.break(2, 22, style="zigzag")
# now just add the points by subtracting the "gap" you have
# used in the axis labels like gap.plot does
points(x, meanslay, type="b")
# and add the error bars using the same trick
arrows(x, meanslay-10-selay, x, meanslay-10+selay, code = 3,
  angle = 90, length=0.03)

This seems to do the job on my Linux box. If you find that there is
still a problem with Windows, email me again and I'll try it at work.

Jim


From matthew_wiener at merck.com  Mon Jul 17 14:46:27 2006
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Mon, 17 Jul 2006 08:46:27 -0400
Subject: [R] Handshake exception in Rserve  [Broadcast]
Message-ID: <4E9A692D8755DF478B56A2892388EE1FDB3944@usctmx1118.merck.com>

We had something similar, and as far as I recall the problem turned out to
be that we had not set the config file to allow remote connections.

This would probably be better put on the Rosuda mailing list:
http://mailman.rz.uni-augsburg.de/mailman/listinfo/stats-rosuda-devel

Hope this helps,

Matt Wiener 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of jaydeep chovatia
Sent: Monday, July 17, 2006 12:52 AM
To: r-help at stat.math.ethz.ch
Cc: washu_feclient at persistent.co.in
Subject: [R] Handshake exception in Rserve [Broadcast]

Hello,

 

We are facing some problem while calling a Rserve API from our Java client.
The details are as below:

 

1) We have installed Rserve on linux machine "Nagarajan"(Port: 6311) and
related modules.

 

2) We have started the Rserve in daemon mode perfectly by executing the
comman " R CMD Rserve" on the linux machine.

 

But when the Java client executes the statement: Rconnection c = new
Rconnection ("Nagrajan", 6311), we are getting an exception: "handshake
exception:  expected  32 bytes received -1 byte".

 

It would be really great if you could help us out to resolve the issue or
provide a probable root cause of the problem.

 

Thank you,

Jaydeep

 


DISCLAIMER\ ==========\ This e-mail may contain privileged a...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From jim at bitwrit.com.au  Tue Jul 18 04:54:31 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Mon, 17 Jul 2006 22:54:31 -0400
Subject: [R] Correlation Mapping
Message-ID: <44BC4D67.2070305@bitwrit.com.au>

justin rapp wrote:

 > On the cover of Zivot and Wang's Modeling Financial Time Series with S
 > Plus, there is a correlation plot that seems to indicate the strength
 > of correlation with color-coded squares, so that more highly
 > correlated stocks appear darker red.  If anybody out there is familiar
 > with the book or understands what I am talking about, I am curious as
 > to whether or not there is a similar function in R and how I can call
 > it up.

Hi justin,

I think that color2D.matplot in the plotrix package will do exactly what 
you want.

Jim


From f.harrell at vanderbilt.edu  Mon Jul 17 15:27:27 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 17 Jul 2006 08:27:27 -0500
Subject: [R] Hmisc xYplot
In-Reply-To: <63b87b8e36d65a2185fae544688619a8@uidaho.edu>
References: <63b87b8e36d65a2185fae544688619a8@uidaho.edu>
Message-ID: <44BB903F.3010709@vanderbilt.edu>

Erin Berryman wrote:
> Dear R community,
> 
> I am having trouble with a particular plot that I am trying to produce 
> using Hmisc's xYplot function. I've been using primarily lattice and 
> Hmisc packages for my plotting needs for the past few years, with great 
> success.
> However, what I want to do now with xYplot is plot more than one data 
> trend in the same panel, much as I would use xyplot from package lattice 
> in conjunction with superpose, with type="b". My problem is I can't get 
> the error bars to plot when I use xYplot this way. I've attached my data 
> set for reference.
> 
> Here are my inputs:
> 
>  >p1sum<-read.csv(file="p1sum.csv", header=T)
>  >library(Hmisc)
>  >xYplot(Cbind(DRP, SE) + Cbind(Fe, FeSE) ~ Day | Group + Port,  p1sum, 
> type='b')
> 
> The plotted result is two lines per panel, one labeled "Cbind(DRP, SE)" 
> and the other labeled "Cbind(Fe, FeSE). However, the error bars are not 
> plotted at all (I want the error bars to be DRP +/- SE, and Fe +/- 
> FeSE). Any advice on this is greatly appreciated.
> 
> Thank you,
> 
> Erin

xYplot does not accept multiple Cbind terms.  You'll have to string out 
your data "tall and thin" to construct a single Cbind(Y, SE) and use 
superposition with groups=.

Sometimes confidence intervals are better than using +-SE, which also 
has the advantage of allowing the intervals to be asymmetric.

Frank
> 
> 
> 
> 
> Erin Berryman
> ***PLEASE-- NOTE NEW EMAIL ADDRESS***berr3415 at uidaho.edu
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From Torsten.Hothorn at rzmail.uni-erlangen.de  Mon Jul 17 15:39:01 2006
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Mon, 17 Jul 2006 15:39:01 +0200 (CEST)
Subject: [R] party - ctree() - terminal nodes reference for every obs
In-Reply-To: <1152888834.5518.6.camel@localhost.localdomain>
References: <1152888834.5518.6.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.64.0607171537190.26998@imbe153.imbe.med.uni-erlangen.de>


On Fri, 14 Jul 2006, Daniele Medri wrote:

> Dear R.Users,
>
> using ctree() (from "party" library) on a data.frame, I want to append a
> column with the references for the groups/segments detected. While these
> nodes are easy readable in output, I need a vector for my obs.
>

Daniele,

do you mean a factor coding the terminal node numbers for each 
observation? This is the `where' slot in objects returned from `ctree', 
for example :

>  airq <- subset(airquality, !is.na(Ozone))
>          airct <- ctree(Ozone ~ ., data = airq,
+                         controls = ctree_control(maxsurrogate = 3))
> factor(airct at where)
   [1] 5 5 5 5 5 5 5 5 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 3 5 6 9 9 6 5 5 5 5 
5 8 9
  [38] 6 8 9 8 8 8 8 5 6 6 3 6 8 8 9 3 8 8 6 9 8 8 8 6 3 6 6 8 8 8 8 9 8 9 
6 6 5
  [75] 3 5 6 6 5 5 6 3 8 9 8 8 8 8 8 8 8 8 9 6 6 5 5 6 5 3 5 5 3 5 5 5 6 5 
5 6 5
[112] 5 3 5 5 5
Levels: 3 5 6 8 9

Hope that helps,

Torsten


> Hints?
>
>
> Cheers
> -- 
> Daniele Medri
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>


From mmiller3 at iupui.edu  Mon Jul 17 15:51:49 2006
From: mmiller3 at iupui.edu (Michael A. Miller)
Date: Mon, 17 Jul 2006 09:51:49 -0400
Subject: [R] Excel to R
In-Reply-To: <7.0.0.16.2.20060715113721.0526b5a0@centroin.com.br> (Bernardo
	Rangel's message of "Sat, 15 Jul 2006 11:50:14 -0300")
References: <7.0.0.16.2.20060715113721.0526b5a0@centroin.com.br>
Message-ID: <87u05gxrt6.fsf@lumen.indyrad.iupui.edu>

>>>>> "Bernardo" == Bernardo Rangel tura <tura at centroin.com.br> writes:

    > Well I use this scripts to import the database

    > require(RODBC)
    > channel <- odbcConnectExcel("f:/teste.xls")
    > data <- sqlFetch(channel, "Sheet1")

Just convert qw to a factor:

require(RODBC)
channel <- odbcConnectExcel("f:/teste.xls")
data <- sqlFetch(channel, "Sheet1")
data$qw <- factor(data$qw)

    > Tha factors variables have tha values enconde this form
    > (1=Yes,2=No and 9 = NA 

If you want to use those levels, you could use

data$qw <- factor(data$qw, levels=c('Yes', 'No', 'NA'))

Mike


From Saghir.Bashir at UCB-Group.com  Mon Jul 17 16:07:11 2006
From: Saghir.Bashir at UCB-Group.com (Bashir Saghir (Aztek Global))
Date: Mon, 17 Jul 2006 16:07:11 +0200
Subject: [R] String manipulation and formatting
Message-ID: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A7D@ntbraexc104.dir.ucb-group.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060717/d19c3b8e/attachment.pl 

From dnlsutcliffe at yahoo.co.uk  Mon Jul 17 16:05:47 2006
From: dnlsutcliffe at yahoo.co.uk (Daniel sutcliffe)
Date: Mon, 17 Jul 2006 15:05:47 +0100 (BST)
Subject: [R] ts and stl functions - still a problem
In-Reply-To: <44BA6ACF.60306@pdf.com>
Message-ID: <20060717140547.59468.qmail@web26902.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060717/0cb99d9f/attachment.pl 

From christos at nuverabio.com  Mon Jul 17 16:21:09 2006
From: christos at nuverabio.com (Christos Hatzis)
Date: Mon, 17 Jul 2006 10:21:09 -0400
Subject: [R] String manipulation and formatting
In-Reply-To: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A7D@ntbraexc104.dir.ucb-group.com>
Message-ID: <001a01c6a9ac$40be7310$0e010a0a@headquarters.silicoinsights>

See ?formatC

You might need to write a simple wrapper function to implement the interface
that you want. 

-Christos 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bashir Saghir (Aztek
Global)
Sent: Monday, July 17, 2006 10:07 AM
To: 'r-help at R-project.org'
Subject: [R] String manipulation and formatting

I'm trying to write a simple function that does the following:

  [command] xify(5.2)
  [output] XXX.XX

  [command] xify(3)
  [output] XXX

Any simple solutions (without using python/perl/unix script/...)?

Thanks,
Saghir


---------------------------------------------------------
Legal Notice: This electronic mail and its attachments are i...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From maechler at stat.math.ethz.ch  Mon Jul 17 16:23:06 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 17 Jul 2006 16:23:06 +0200
Subject: [R] Cluster Analysis with flexible beta linkage method
In-Reply-To: <e23082be0607140710o626da2c8p9e53af3cca26c53e@mail.gmail.com>
References: <e23082be0607140710o626da2c8p9e53af3cca26c53e@mail.gmail.com>
Message-ID: <17595.40266.494267.185551@stat.math.ethz.ch>

>>>>> "Wade" == Wade Wall <wade.wall at gmail.com>
>>>>>     on Fri, 14 Jul 2006 10:10:11 -0400 writes:

    Wade> I am trying to run a cluster analysis using Sorenson
    Wade> (Bray-Curtis) distance measure with flexible beta
    Wade> linkage method.  However, I can't seem to find
    Wade> flexible beta in any of the functions/packages I have
    Wade> looked at.

Maybe you explain what the above are, rather than us having to
look up the information ?

    Wade> Any help would be appreciated.

    Wade> [[alternative HTML version deleted]]
	   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	   would not appear here,  had you read and followed
	   the posting guide :
    
    Wade> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From ricardosilva at serasa.com.br  Mon Jul 17 16:27:33 2006
From: ricardosilva at serasa.com.br (ricardosilva at serasa.com.br)
Date: Mon, 17 Jul 2006 11:27:33 -0300
Subject: [R] Quantreg error
Message-ID: <OFFE191828.7D9F8D12-ON032571AE.004F47E4-032571AE.004F6D79@serasa.com.br>

Dear User,
I got the following error running a regression quantile:

> rq1<-rq(dep ~ ., model=TRUE, data=exo, tau=0.5 );
> summary(rq1)
Erro em rq.fit.fnb(x, y, tau = tau + h) :
        Error info =  75 in stepy: singular design

Any hint about the problem?


Thanks a lot,
________________________________________
Ricardo Gon?alves Silva, M. Sc.
Apoio aos Processos de Modelagem Matem?tica
Econometria & Inadimpl?ncia
Serasa S.A.
(11) - 6847-8889
ricardosilva at serasa.com.br

**********************************************************************************

As informa??es contidas nesta mensagem e no(s) arquivo(s) anexo(s) s?o
endere?adas exclusivamente ?(s) pessoa(s) e/ou institui??o(?es) acima
indicada(s), podendo conter dados confidenciais, os quais n?o podem, sob
qualquer forma ou pretexto, ser utilizados, divulgados, alterados,
impressos ou copiados, total ou parcialmente, por pessoas n?o autorizadas.
Caso n?o seja o destinat?rio, favor providenciar sua exclus?o e notificar o
remetente imediatamente.  O uso impr?prio ser? tratado conforme as normas
da empresa e da legisla??o em vigor.
Esta mensagem expressa o posicionamento pessoal do subscritor e n?o reflete
necessariamente a opini?o da Serasa.


From dieter.menne at menne-biomed.de  Mon Jul 17 16:26:35 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 17 Jul 2006 14:26:35 +0000 (UTC)
Subject: [R] storing the estimates from lmer
References: <bb47cf700607110818m782a98c5le08396d2fca203a@mail.gmail.com>
	<44B921BD.8090401@pdf.com>
	<40e66e0b0607151139i7350d22bteacfaa61aa94a1f8@mail.gmail.com>
Message-ID: <loom.20060717T162026-642@post.gmane.org>

Douglas Bates <bates <at> stat.wisc.edu> writes:
.....

> I encourage users of lmer who wish to determine the precision of the
> estimates of the variance components to create a Markov chain Monte
> Carlo sample of the parameters and evaluate the HPDintervals.
> 
> > sm1 <- mcmcsamp(fm1, 50000)
> > library(coda)
> Warning message:
> use of NULL environment is deprecated
> > HPDinterval(sm1)
>                           lower        upper
> (Intercept)         236.6518363  266.5465536
> Days                  7.0136243   13.8947993
> log(sigma^2)          6.2550082    6.7295329
> log(Sbjc.(In))        5.4928205    7.5751372
> log(Sbjc.Days)        2.8197523    4.6337518
> atanh(Sbj.(I).Dys)   -0.6988632    0.9836688
> deviance           1752.5158501 1766.6461469
> attr(,"Probability")
> [1] 0.95
> 


And DB wrote in 
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/76742.html

"Evaluating entire terms is more difficult but you can always calculate the F
ratio and put a lower bound on the denominator degrees of freedom."

As a mcmc-challenged subject, it would be nice if someone could provide an
example for this; or how to get CI estimates for an arbitrary contrast with
mcmcsamp.

Dieter


From Saghir.Bashir at UCB-Group.com  Mon Jul 17 16:38:57 2006
From: Saghir.Bashir at UCB-Group.com (Bashir Saghir (Aztek Global))
Date: Mon, 17 Jul 2006 16:38:57 +0200
Subject: [R] String manipulation and formatting
Message-ID: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A7E@ntbraexc104.dir.ucb-group.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060717/305049a0/attachment.pl 

From christos at nuverabio.com  Mon Jul 17 16:39:06 2006
From: christos at nuverabio.com (Christos Hatzis)
Date: Mon, 17 Jul 2006 10:39:06 -0400
Subject: [R] String manipulation and formatting
In-Reply-To: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A7E@ntbraexc104.dir.ucb-group.com>
Message-ID: <001e01c6a9ae$c2521240$0e010a0a@headquarters.silicoinsights>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060717/9e81e916/attachment.pl 

From maechler at stat.math.ethz.ch  Mon Jul 17 16:41:24 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 17 Jul 2006 16:41:24 +0200
Subject: [R] Trailing on r-help messages
In-Reply-To: <f8e6ff050607170139q1e0c6565s3dc6c19954af74c0@mail.gmail.com>
References: <971536df0607160946v7e0a0c1al9e3956885bab73e8@mail.gmail.com>
	<f8e6ff050607161116m76f9ff32j1c1ad66c5810db52@mail.gmail.com>
	<971536df0607161151y54a8baceu9bb9edaf5a628f94@mail.gmail.com>
	<f8e6ff050607161324l7e85d3b3t7fd89046c22d36f4@mail.gmail.com>
	<971536df0607161331h546bc59fkd911ca87b1fd6a36@mail.gmail.com>
	<f8e6ff050607170139q1e0c6565s3dc6c19954af74c0@mail.gmail.com>
Message-ID: <17595.41364.405007.246323@stat.math.ethz.ch>

Thanks to Gabor, Spencer, and Hadley,
for the constructive propositions.

>>>>> "hadley" == hadley wickham <h.wickham at gmail.com>
>>>>>     on Mon, 17 Jul 2006 09:39:36 +0100 writes:

    >> I think that is likely true but it would at least mean that they had
    >> seen it repeatedly and there would really be no excuse for not following it
    >> (unlike the current situation where one needs to take action to follow
    >> the posting guide link and then read a lengthy page).

    hadley> Logically, that makes sense.  However, try asking a group of R users
    hadley> how to cite R (or how to find out how to cite R).

    hadley> I think it's a good idea to make the default list
    hadley> signature as good as possible, I just don't think it
    hadley> will have that much effect.  Supplying a small
    hadley> reproducible example when reporting an error is
    hadley> "common-sense" (how else will the developers/others
    hadley> be able to replicate it?) and yet it is very rare
    hadley> (not just in R, but in any project).

I can well agree to slightly extend the R-help message footer,
e.g., to 4 instead of 3 lines.
Maybe we can move discussion of the final wording "off list".
I also agree that the effect from that change will probably not
be very big.

To other points made:

- Most R-SIG-* lists have their own maintainers (i.e. different
  from me).  It's really the maintainers decision (and "job") to
  change the default footer.
  I cannot change the default footer for all newly generated
  mailing lists here, because some are non-R related.

- The "inscription" page is dynamically generated by mailman, i.e.,
  typically python scripts.
  If any of you are interested in patching Mailman's sources in
  a reasonable way (i.e. easily reproducible for the next
  version of mailman), I'd consider a change there; otherwise not.

Martin Maechler, ETH Zurich
(Maintainer of R-help and a few other R-* mailing lists)


From rmh at temple.edu  Mon Jul 17 16:42:05 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Mon, 17 Jul 2006 10:42:05 -0400 (EDT)
Subject: [R] String manipulation and formatting
Message-ID: <20060717104205.BEH49787@po-d.temple.edu>

xify <- function(number) {
  fn <- format(number)
  fn3 <- unlist(strsplit(fn, "\\."))
  if (length(fn3) == 1) paste(rep("X", as.numeric(fn3)), collapse="")
  else
  paste(paste(rep("X", as.numeric(fn3)[1]-as.numeric(fn3)[2]), collapse=""),
        paste(rep("X", as.numeric(fn3)[2]), collapse=""),
        sep=".")
}


From Saghir.Bashir at UCB-Group.com  Mon Jul 17 16:51:16 2006
From: Saghir.Bashir at UCB-Group.com (Bashir Saghir (Aztek Global))
Date: Mon, 17 Jul 2006 16:51:16 +0200
Subject: [R] String manipulation and formatting
Message-ID: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A7F@ntbraexc104.dir.ucb-group.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060717/97ca1d34/attachment.pl 

From Saghir.Bashir at UCB-Group.com  Mon Jul 17 16:53:32 2006
From: Saghir.Bashir at UCB-Group.com (Bashir Saghir (Aztek Global))
Date: Mon, 17 Jul 2006 16:53:32 +0200
Subject: [R] String manipulation and formatting
Message-ID: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A80@ntbraexc104.dir.ucb-group.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060717/cd6d1f88/attachment.pl 

From rkoenker at uiuc.edu  Mon Jul 17 16:44:56 2006
From: rkoenker at uiuc.edu (roger koenker)
Date: Mon, 17 Jul 2006 09:44:56 -0500
Subject: [R] Quantreg error
In-Reply-To: <OFFE191828.7D9F8D12-ON032571AE.004F47E4-032571AE.004F6D79@serasa.com.br>
References: <OFFE191828.7D9F8D12-ON032571AE.004F47E4-032571AE.004F6D79@serasa.com.br>
Message-ID: <B51F866F-8714-4945-8A04-6A448875C6B5@uiuc.edu>

As I have already told you once, and as the posting guide suggests,

"If the question relates to a contributed package , e.g., one  
downloaded from CRAN, try contacting the package maintainer first.  
You can also use find("functionname") and packageDescription 
("packagename") to find this information. Only send such questions to  
R-help or R-devel if you get no reply or need further assistance.  
This applies to both requests for help and to bug reports."

the error message seems quite clear:  it means that the model that  
you have specified
implicitly with the formula has a singular X matrix.  The quantile  
regression fitting
functions don't understand about singular designs;  some day they may  
but it isn't
a high priority for me.


url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Jul 17, 2006, at 9:27 AM, ricardosilva at serasa.com.br wrote:

> Dear User,
> I got the following error running a regression quantile:
>
>> rq1<-rq(dep ~ ., model=TRUE, data=exo, tau=0.5 );
>> summary(rq1)
> Erro em rq.fit.fnb(x, y, tau = tau + h) :
>         Error info =  75 in stepy: singular design
>
> Any hint about the problem?
>
>
> Thanks a lot,
> ________________________________________
> Ricardo Gon?alves Silva, M. Sc.
> Apoio aos Processos de Modelagem Matem?tica
> Econometria & Inadimpl?ncia
> Serasa S.A.
> (11) - 6847-8889
> ricardosilva at serasa.com.br
>
> ********************************************************************** 
> ************
>
> As informa??es contidas nesta mensagem e no(s) arquivo(s) anexo(s) s?o
> endere?adas exclusivamente ?(s) pessoa(s) e/ou institui??o(?es) acima
> indicada(s), podendo conter dados confidenciais, os quais n?o  
> podem, sob
> qualquer forma ou pretexto, ser utilizados, divulgados, alterados,
> impressos ou copiados, total ou parcialmente, por pessoas n?o  
> autorizadas.
> Caso n?o seja o destinat?rio, favor providenciar sua exclus?o e  
> notificar o
> remetente imediatamente.  O uso impr?prio ser? tratado conforme as  
> normas
> da empresa e da legisla??o em vigor.
> Esta mensagem expressa o posicionamento pessoal do subscritor e n?o  
> reflete
> necessariamente a opini?o da Serasa.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html


From ligges at statistik.uni-dortmund.de  Mon Jul 17 16:54:52 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 17 Jul 2006 16:54:52 +0200
Subject: [R] Quantreg error
In-Reply-To: <OFFE191828.7D9F8D12-ON032571AE.004F47E4-032571AE.004F6D79@serasa.com.br>
References: <OFFE191828.7D9F8D12-ON032571AE.004F47E4-032571AE.004F6D79@serasa.com.br>
Message-ID: <44BBA4BC.9090502@statistik.uni-dortmund.de>

ricardosilva at serasa.com.br wrote:
> Dear User,
> I got the following error running a regression quantile:
> 
>> rq1<-rq(dep ~ ., model=TRUE, data=exo, tau=0.5 );
>> summary(rq1)
> Erro em rq.fit.fnb(x, y, tau = tau + h) :
>         Error info =  75 in stepy: singular design
> 
> Any hint about the problem?


Well, some of the variables are probably linear dependent, or you do not 
have any degrees of freedom left...

Uwe Ligges

> 
> Thanks a lot,
> ________________________________________
> Ricardo Gon?alves Silva, M. Sc.
> Apoio aos Processos de Modelagem Matem?tica
> Econometria & Inadimpl?ncia
> Serasa S.A.
> (11) - 6847-8889
> ricardosilva at serasa.com.br
> 
> **********************************************************************************
> 
> As informa??es contidas nesta mensagem e no(s) arquivo(s) anexo(s) s?o
> endere?adas exclusivamente ?(s) pessoa(s) e/ou institui??o(?es) acima
> indicada(s), podendo conter dados confidenciais, os quais n?o podem, sob
> qualquer forma ou pretexto, ser utilizados, divulgados, alterados,
> impressos ou copiados, total ou parcialmente, por pessoas n?o autorizadas.
> Caso n?o seja o destinat?rio, favor providenciar sua exclus?o e notificar o
> remetente imediatamente.  O uso impr?prio ser? tratado conforme as normas
> da empresa e da legisla??o em vigor.
> Esta mensagem expressa o posicionamento pessoal do subscritor e n?o reflete
> necessariamente a opini?o da Serasa.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From mschwartz at mn.rr.com  Mon Jul 17 16:55:08 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 17 Jul 2006 09:55:08 -0500
Subject: [R] String manipulation and formatting
In-Reply-To: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A7D@ntbraexc104.dir.ucb-group.com>
References: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A7D@ntbraexc104.dir.ucb-group.com>
Message-ID: <1153148108.4897.22.camel@localhost.localdomain>

On Mon, 2006-07-17 at 16:07 +0200, Bashir Saghir (Aztek Global) wrote:
> I'm trying to write a simple function that does the following:
> 
>   [command] xify(5.2) 
>   [output] XXX.XX
> 
>   [command] xify(3)
>   [output] XXX
> 
> Any simple solutions (without using python/perl/unix script/...)?
> 
> Thanks,
> Saghir

Here are two variations:

xify <- function(x)
{
  exxes <- as.numeric(unlist(strsplit(as.character(x), "\\.")))
  ifelse(length(exxes) == 2,
         paste(paste(rep("X", exxes[1] - exxes[2]), collapse = ""), 
               paste(rep("X", exxes[2]), collapse = ""), 
               sep = "."),
         paste(rep("X", exxes[1]), collapse = ""))
}


xify <- function(x)
{
  exxes <- as.numeric(unlist(strsplit(as.character(x), "\\.")))
  tmp <- sapply(exxes, function(x) paste(rep("X", x), collapse = ""))
  ifelse(length(tmp) == 2, 
         paste(substr(tmp[1], 1, exxes[1] - exxes[2]), tmp[2], 
               sep = "."),
         tmp)
}


HTH,

Marc Schwartz


From ggrothendieck at gmail.com  Mon Jul 17 17:06:20 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 17 Jul 2006 11:06:20 -0400
Subject: [R] String manipulation and formatting
In-Reply-To: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A7D@ntbraexc104.dir.ucb-group.com>
References: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A7D@ntbraexc104.dir.ucb-group.com>
Message-ID: <971536df0607170806v5cd689cx717728e6c5945ce2@mail.gmail.com>

First we define repx(n) to produce n X's.  This
simple function is used subsequently.  f is a
function that takes 4 strings which represent the
entire string and the three backreferenced substrings
in the subsequent gsub pattern and does the paste.
x1 will represent the part before the dot and x3 after
the dot. x2 represents the dot plus x3 and is not used.
Finally we run gsubfn using the appropriate regex pattern.

library(gsubfn)
xify <- function(fmt) {
   repx <- function(n) paste(rep("X", n), collapse = "")
   f <- function(x, x1= "", x2 = "", x3 = "")
     if (x3 == "") repx(x1) else paste(repx(x1), ".", repx(x3), sep = "")
   as.vector(gsubfn("^([[:digit:]]+)(.([[:digit:]])|)", f, as.character(fmt)))
}

xify(5.2) # XXXXX.XX
xify(5)   # XXXXX


On 7/17/06, Bashir Saghir (Aztek Global) <Saghir.Bashir at ucb-group.com> wrote:
> I'm trying to write a simple function that does the following:
>
>  [command] xify(5.2)
>  [output] XXX.XX
>
>  [command] xify(3)
>  [output] XXX
>
> Any simple solutions (without using python/perl/unix script/...)?
>
> Thanks,
> Saghir
>
>
> ---------------------------------------------------------
> Legal Notice: This electronic mail and its attachments are i...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ggrothendieck at gmail.com  Mon Jul 17 17:22:06 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 17 Jul 2006 11:22:06 -0400
Subject: [R] String manipulation and formatting
In-Reply-To: <971536df0607170806v5cd689cx717728e6c5945ce2@mail.gmail.com>
References: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A7D@ntbraexc104.dir.ucb-group.com>
	<971536df0607170806v5cd689cx717728e6c5945ce2@mail.gmail.com>
Message-ID: <971536df0607170822k7a940e8doc9ca5f0c86daa2c6@mail.gmail.com>

In thinking about this some more it can be substantially simplified.
We don't need f at all -- we can just use repx and we don't need
any backreferences either since we can repeatedly apply the
numeric pattern.  This reduces it to a two line body:

library(gsubfn)
xify <- function(fmt) {
   repx <- function(n) paste(rep("X", n), collapse = "")
   as.vector(gsubfn("[[:digit:]]+", repx, as.character(fmt)))
}

xify(5.2) # XXXXX.XX
xify(5)   # XXXXX


On 7/17/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> First we define repx(n) to produce n X's.  This
> simple function is used subsequently.  f is a
> function that takes 4 strings which represent the
> entire string and the three backreferenced substrings
> in the subsequent gsub pattern and does the paste.
> x1 will represent the part before the dot and x3 after
> the dot. x2 represents the dot plus x3 and is not used.
> Finally we run gsubfn using the appropriate regex pattern.
>
> library(gsubfn)
> xify <- function(fmt) {
>   repx <- function(n) paste(rep("X", n), collapse = "")
>   f <- function(x, x1= "", x2 = "", x3 = "")
>     if (x3 == "") repx(x1) else paste(repx(x1), ".", repx(x3), sep = "")
>   as.vector(gsubfn("^([[:digit:]]+)(.([[:digit:]])|)", f, as.character(fmt)))
> }
>
> xify(5.2) # XXXXX.XX
> xify(5)   # XXXXX
>
>
> On 7/17/06, Bashir Saghir (Aztek Global) <Saghir.Bashir at ucb-group.com> wrote:
> > I'm trying to write a simple function that does the following:
> >
> >  [command] xify(5.2)
> >  [output] XXX.XX
> >
> >  [command] xify(3)
> >  [output] XXX
> >
> > Any simple solutions (without using python/perl/unix script/...)?
> >
> > Thanks,
> > Saghir
> >
> >
> > ---------------------------------------------------------
> > Legal Notice: This electronic mail and its attachments are i...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>


From adi at roda.ro  Mon Jul 17 17:40:30 2006
From: adi at roda.ro (Adrian DUSA)
Date: Mon, 17 Jul 2006 18:40:30 +0300
Subject: [R] Getting rid of for loops
In-Reply-To: <BBEAIDIAANOEADEFKHPEAEFJCBAA.kemerson@uoregon.edu>
References: <BBEAIDIAANOEADEFKHPEAEFJCBAA.kemerson@uoregon.edu>
Message-ID: <200607171840.30277.adi@roda.ro>

Hi Kevin,

Regarding your first question, try this:

library(combinat)
all.pairs <- combn2(5:40)
marker1 <- as.matrix(names(qtl)[all.pairs[, 1]])
marker1 <- as.matrix(names(qtl)[all.pairs[, 2]])
myfun <- function(idx) {
    summary(aov(qtl$CPP ~ qtl[,idx[1]] * qtl[,idx[2]]))[[1]]$"Pr(>F)"[3])
    }
p.interaction <- as.matrix(apply(all.pairs, 1, myfun)

HTH,
Adrian


On Monday 17 July 2006 05:18, Kevin J Emerson wrote:
> Hello R-users!
>
> I have a style question.  I know that for loops are somewhat frowned upon
> in R, and I was trying to figure out a nice way to do something without
> using loops, but figured that i could get it done quickly using them.  I am
> now looking to see what kind of tricks I can use to make this code a bit
> more aesthetically appealing to other R users (and learn something about R
> along the way...).
>
> Here's the problem.  I have a data.frame with 4 columns of dependent
> variables and then ~35 columns of predictor variables (factors) [for those
> interested, it is a qtl problem, where the predictors are genotypes at DNA
> markers and the dependent variable is a biological trait].  I want to go
> through all pairwise combinations of predictor variables and perform an
> anova with two predictors and their interaction on a given dependent
> variable.  I then want to store the p.value of the interaction term, along
> with the predictor variable information.  So I want to end up with a
> dataframe at the end with the two variable names and the interaction p
> value in each row, for all pairwise combinations of predictors.  I used the
> following code:
>
> # qtl is the original data.frame, and my dependent var in this case is
> # qtl$CPP.
>
> marker1 <- NULL
> marker2 <- NULL
> p.interaction <- NULL
> for ( i in 5:40) {   # cols 5 - 41 are the predictor factors
> 	for (j in (i+1):41) {
> 		marker1 <- rbind(marker1,names(qtl)[i])
> 		marker2 <- rbind(marker2,names(qtl)[j])
> 		tmp2 <- summary(aov(tmp$CPP ~ tmp[,i] * tmp[,j]))[[1]]
> 		p.interaction <- rbind(p.interaction, tmp2$"Pr(>F)"[3])
> 	}
> }
>
> I have two questions:
> (1) is there a nicer way to do this without having to invoke for loops?
> (2) my other dependent variables are categorical in nature.  I need
> basically the same information - I am looking for information regarding the
> interaction of predictors on a categorical variable.  Any ideas on what
> tests to use? (I am new to analysis of all-categorical data).
>
> Thanks in advance!
> Kevin
>
> --------------------------------------
> --------------------------------------
> Kevin Emerson
> Center for Ecology and Evolutionary Biology
> 1210 University of Oregon
> Eugene, OR 97403
> USA
> kemerson at uoregon.edu

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From spencer.graves at pdf.com  Mon Jul 17 17:37:32 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 17 Jul 2006 08:37:32 -0700
Subject: [R] AICc vs AIC for model selection
In-Reply-To: <20060717121247.89448.qmail@web37605.mail.mud.yahoo.com>
References: <20060717121247.89448.qmail@web37605.mail.mud.yahoo.com>
Message-ID: <44BBAEBC.6050404@pdf.com>

	  I understand that you have only 26 observations.  Model 
identification always requires more observations than estimating a model 
you already think you know.  If it were my problem, I think I'd first 
plot the data over time and make a normal probability plot of the data. 
  Then I'd fit the simplest, most parsimonious model I could think of 
that would include the trend and seasonal.  Then I'd examine the 
residuals and check the p values.  If everything looked sensible, 
wouldn't push it further.  If I had several such series, I'd study 
Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus 
(Springer) and use the 'nlme' package to do more.

	  Hope this helps.
	  Spencer Graves

Sachin J wrote:
> Hi Spencer,
>  
> I did go through the previous postings in the mailing list. But couldn't 
> find satisfactory answer to my question. I am dealing with univariate 
> time series. I suspect that my  data may contain some trend and seasonal 
> components. Hence, rather than just fitting just AR(1) model, I am 
> trying to find the right model which fits the data well and then use 
> that model to forecast. In order to achieve this I am using best.arima 
> model. If you have any other thoughts on this please let me know.
>  
> Thanx in advance for your help.
>  
> Regards
> Sachin
>  
> 
> */Spencer Graves <spencer.graves at pdf.com>/* wrote:
> 
>     Regarding AIC.c, have you tried RSiteSearch("AICc") and
>     RSiteSearch("AIC.c")? This produced several comments that looked to me
>     like they might help answer your question. Beyond that, I've never
>     heard of the "forecast" package, and I got zero hits for
>     RSiteSearch("best.arima"), so I can't comment directly on your question.
> 
>     Do you have only one series or multiple? If you have only one, I
>     think it would be hard to justify more than a simple AR(1) model.
>     Almost anything else would likely be overfitting.
> 
>     If you have multiple series, have you considered using 'lme' in the
>     'nlme' package? Are you familiar with Pinheiro and Bates (2000)
>     Mixed-Effects Models in S and S-Plus (Springer)? If not, I encourage
>     you to spend some quality time with this book. My study of it has been
>     amply rewarded, and I believe yours will likely also.
> 
>     Best Wishes,
>     Spencer Graves
> 
>     Sachin J wrote:
>      > Hi,
>      >
>      > I am using 'best.arima' function from forecast package
>     to obtain point forecast for a time series data set. The
>     documentation says it utilizes AIC value to select best ARIMA
>     model. But in my case the sample size very small - 26
>     observations (demand data). Is it the right to use AIC value for
>     model selection in this case. Should I use AICc instead of AIC.
>     If so how can I modify best.arima function to change the selection
>     creteria? Any pointers would be of great help.
>      >
>      > Thanx in advance.
>      >
>      > Sachin
>      >
>      >
>      >
>      >
>      > ---------------------------------
>      >
>      > [[alternative HTML version deleted]]
>      >
>      > ______________________________________________
>      > R-help at stat.math.ethz.ch mailing list
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>      > PLEASE do read the posting guide!
>     http://www.R-project.org/posting-guide.html
> 
> 
> ------------------------------------------------------------------------
> Do you Yahoo!?
> Next-gen email? Have it all with the all-new Yahoo! Mail Beta. 
> <http://us.rd.yahoo.com/evt=42241/*http://advision.webevents.yahoo.com/handraisers> 
>


From adi at roda.ro  Mon Jul 17 17:48:19 2006
From: adi at roda.ro (Adrian DUSA)
Date: Mon, 17 Jul 2006 18:48:19 +0300
Subject: [R] Getting rid of for loops
In-Reply-To: <200607171840.30277.adi@roda.ro>
References: <BBEAIDIAANOEADEFKHPEAEFJCBAA.kemerson@uoregon.edu>
	<200607171840.30277.adi@roda.ro>
Message-ID: <200607171848.19159.adi@roda.ro>

Hi again,

There is a slight error there, it should have been "marker2" at the fourth 
line:
all.pairs <- combn2(5:40)
marker1 <- names(qtl)[all.pairs[, 1]]
marker2 <- names(qtl)[all.pairs[, 2]]
myfun <- function(idx) {
? ? summary(aov(qtl$CPP ~ qtl[,idx[1]] * qtl[,idx[2]]))[[1]]$"Pr(>F)"[3])
? ? }
p.interaction <- apply(all.pairs, 1, myfun)

Actually, you don't need "as.matrix" there, just cbind all your vectors to 
obtain the final dataframe:

finally <- as.data.frame(cbind(marker1, marker2, p.interaction))

Adrian


On Monday 17 July 2006 18:40, Adrian DUSA wrote:
> Hi Kevin,
>
> Regarding your first question, try this:
>
> library(combinat)
> all.pairs <- combn2(5:40)
> marker1 <- as.matrix(names(qtl)[all.pairs[, 1]])
> marker1 <- as.matrix(names(qtl)[all.pairs[, 2]])
> myfun <- function(idx) {
>     summary(aov(qtl$CPP ~ qtl[,idx[1]] * qtl[,idx[2]]))[[1]]$"Pr(>F)"[3])
>     }
> p.interaction <- as.matrix(apply(all.pairs, 1, myfun)
>
> HTH,
> Adrian
>
> On Monday 17 July 2006 05:18, Kevin J Emerson wrote:
> > Hello R-users!
> >
> > I have a style question.  I know that for loops are somewhat frowned upon
> > in R, and I was trying to figure out a nice way to do something without
> > using loops, but figured that i could get it done quickly using them.  I
> > am now looking to see what kind of tricks I can use to make this code a
> > bit more aesthetically appealing to other R users (and learn something
> > about R along the way...).
> >
> > Here's the problem.  I have a data.frame with 4 columns of dependent
> > variables and then ~35 columns of predictor variables (factors) [for
> > those interested, it is a qtl problem, where the predictors are genotypes
> > at DNA markers and the dependent variable is a biological trait].  I want
> > to go through all pairwise combinations of predictor variables and
> > perform an anova with two predictors and their interaction on a given
> > dependent variable.  I then want to store the p.value of the interaction
> > term, along with the predictor variable information.  So I want to end up
> > with a dataframe at the end with the two variable names and the
> > interaction p value in each row, for all pairwise combinations of
> > predictors.  I used the following code:
> >
> > # qtl is the original data.frame, and my dependent var in this case is
> > # qtl$CPP.
> >
> > marker1 <- NULL
> > marker2 <- NULL
> > p.interaction <- NULL
> > for ( i in 5:40) {   # cols 5 - 41 are the predictor factors
> > 	for (j in (i+1):41) {
> > 		marker1 <- rbind(marker1,names(qtl)[i])
> > 		marker2 <- rbind(marker2,names(qtl)[j])
> > 		tmp2 <- summary(aov(tmp$CPP ~ tmp[,i] * tmp[,j]))[[1]]
> > 		p.interaction <- rbind(p.interaction, tmp2$"Pr(>F)"[3])
> > 	}
> > }
> >
> > I have two questions:
> > (1) is there a nicer way to do this without having to invoke for loops?
> > (2) my other dependent variables are categorical in nature.  I need
> > basically the same information - I am looking for information regarding
> > the interaction of predictors on a categorical variable.  Any ideas on
> > what tests to use? (I am new to analysis of all-categorical data).
> >
> > Thanks in advance!
> > Kevin
> >
> > --------------------------------------
> > --------------------------------------
> > Kevin Emerson
> > Center for Ecology and Evolutionary Biology
> > 1210 University of Oregon
> > Eugene, OR 97403
> > USA
> > kemerson at uoregon.edu

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From ggrothendieck at gmail.com  Mon Jul 17 17:51:27 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 17 Jul 2006 11:51:27 -0400
Subject: [R] Getting rid of for loops
In-Reply-To: <200607171848.19159.adi@roda.ro>
References: <BBEAIDIAANOEADEFKHPEAEFJCBAA.kemerson@uoregon.edu>
	<200607171840.30277.adi@roda.ro> <200607171848.19159.adi@roda.ro>
Message-ID: <971536df0607170851k61d27e06nbcb8d13fabc25335@mail.gmail.com>

On 7/17/06, Adrian DUSA <adi at roda.ro> wrote:
> Hi again,
>
> There is a slight error there, it should have been "marker2" at the fourth
> line:
> all.pairs <- combn2(5:40)
> marker1 <- names(qtl)[all.pairs[, 1]]
> marker2 <- names(qtl)[all.pairs[, 2]]
> myfun <- function(idx) {
>   summary(aov(qtl$CPP ~ qtl[,idx[1]] * qtl[,idx[2]]))[[1]]$"Pr(>F)"[3])
>   }
> p.interaction <- apply(all.pairs, 1, myfun)
>
> Actually, you don't need "as.matrix" there, just cbind all your vectors to
> obtain the final dataframe:
>
> finally <- as.data.frame(cbind(marker1, marker2, p.interaction))

or even simpler:

   finally <- data.frame(marker1, marker2, p.interaction)


From h.wickham at gmail.com  Mon Jul 17 17:59:15 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 17 Jul 2006 16:59:15 +0100
Subject: [R] Trailing on r-help messages
In-Reply-To: <17595.41364.405007.246323@stat.math.ethz.ch>
References: <971536df0607160946v7e0a0c1al9e3956885bab73e8@mail.gmail.com>
	<f8e6ff050607161116m76f9ff32j1c1ad66c5810db52@mail.gmail.com>
	<971536df0607161151y54a8baceu9bb9edaf5a628f94@mail.gmail.com>
	<f8e6ff050607161324l7e85d3b3t7fd89046c22d36f4@mail.gmail.com>
	<971536df0607161331h546bc59fkd911ca87b1fd6a36@mail.gmail.com>
	<f8e6ff050607170139q1e0c6565s3dc6c19954af74c0@mail.gmail.com>
	<17595.41364.405007.246323@stat.math.ethz.ch>
Message-ID: <f8e6ff050607170859w114ddc09lb26d228fd82e1ca6@mail.gmail.com>

> - The "inscription" page is dynamically generated by mailman, i.e.,
>   typically python scripts.
>   If any of you are interested in patching Mailman's sources in
>   a reasonable way (i.e. easily reproducible for the next
>   version of mailman), I'd consider a change there; otherwise not.

The other alternative is to simply create a static page that points to
the same places.  This is what I did for GGobi :
http://ggobi.org/support/.

Hadley


From vincent at 7d4.com  Mon Jul 17 18:20:03 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Mon, 17 Jul 2006 18:20:03 +0200
Subject: [R] R and DDE (Dynamic Data Exchange)
Message-ID: <44BBB8B3.8070505@7d4.com>

R and DDE (Dynamic Data Exchange)

Dear Rusers,
I run an application (not mine) which acts as a DDE server.
I would like to use R to get data from this application,
say once per minute, and do some processing on it.
I didn't find much info on the R DDE abilities, apart the tcltk2
package in which I will try to go deeper.
I would be very thankful for any info, pointer or advice about the
"good ways" to make R program get online data from a DDE server.
Thanks
Vincent


From spencer.graves at pdf.com  Mon Jul 17 18:24:11 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 17 Jul 2006 09:24:11 -0700
Subject: [R] Trailing on r-help messages
In-Reply-To: <f8e6ff050607170139q1e0c6565s3dc6c19954af74c0@mail.gmail.com>
References: <971536df0607160946v7e0a0c1al9e3956885bab73e8@mail.gmail.com>	<f8e6ff050607161116m76f9ff32j1c1ad66c5810db52@mail.gmail.com>	<971536df0607161151y54a8baceu9bb9edaf5a628f94@mail.gmail.com>	<f8e6ff050607161324l7e85d3b3t7fd89046c22d36f4@mail.gmail.com>	<971536df0607161331h546bc59fkd911ca87b1fd6a36@mail.gmail.com>
	<f8e6ff050607170139q1e0c6565s3dc6c19954af74c0@mail.gmail.com>
Message-ID: <44BBB9AB.8020604@pdf.com>

<see in line>

hadley wickham wrote:
>> I think that is likely true but it would at least mean that they had
>> seen it repeatedly and there would really be no excuse for not following it
>> (unlike the current situation where one needs to take action to follow
>> the posting guide link and then read a lengthy page).
> 
> Logically, that makes sense.  However, try asking a group of R users
> how to cite R (or how to find out how to cite R).
> 
> I think it's a good idea to make the default list signature as good as
> possible, I just don't think it will have that much effect.  Supplying
> a small reproducible example when reporting an error is "common-sense"
> (how else will the developers/others be able to replicate it?) and yet
> it is very rare (not just in R, but in any project).
> 
	  I believe the introduction of the posting guide actually increased 
the quality of the posts.  I didn't affect all posts nor even a 
majority, but it did impact enough that we are seeing some benefit, I 
think.  My only solid evidence of this are the infrequent but nonzero 
number of comments from people who acknowledging finding answers to 
their own problems by actually walking through the Posting Guide. 
Polya's "How To Solve It" has NOT been read by every mathematician or 
statistician, but many people have read it, and what they've learned 
from Polya comes out in contributions to this listserve.

	  Best Wishes,
	  Spencer Graves

> Hadley
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From zython at gmail.com  Mon Jul 17 18:24:44 2006
From: zython at gmail.com (Zython)
Date: Mon, 17 Jul 2006 09:24:44 -0700 (PDT)
Subject: [R] glmmPQL help
Message-ID: <5363876.post@talk.nabble.com>


I need to use the glmmPQL function for an assignment, but when I call for the
summary of the function, it gives the AIC a value of NA. How do I get R to
give me the AIC value?
-- 
View this message in context: http://www.nabble.com/glmmPQL-help-tf1955675.html#a5363876
Sent from the R help forum at Nabble.com.


From thilo at izkf.rwth-aachen.de  Mon Jul 17 18:56:50 2006
From: thilo at izkf.rwth-aachen.de (Thilo Kellermann)
Date: Mon, 17 Jul 2006 18:56:50 +0200
Subject: [R] Variance functions in package nlme
Message-ID: <200607171856.50534.thilo@izkf.rwth-aachen.de>

Dear R-help,

I am trying to set up linear mixed effects models in R using the (recommended) 
nlme package (R version 2.3.1 on a Linux platform). When trying to reproduce 
an example from Jose Pinheiro & Douglas Bates (2000, p 210) I get the 
following error message (code to produce message pasted as well):

library("nlme")
data("Orthodont")
vf1Ident <- varIdent( c(Female = 0.5), form = ~ 1 | Sex )
vf1Ident <- initialize(vf1Ident, Orthodont)
Error in getClass(Class) : c("\"varIdent\" is not a defined class", 
"\"varFunc\" is not a defined class")
In addition: Warning message:
the condition has length > 1 and only the first element will be used in: if 
(!is.na(match(Class, .BasicClasses))) return(newBasic(Class,  

Can anybody give me a hint, what's going wrong here?
Thanks a lot,

Thilo

Reference:
Jose C. Pinheiro & Douglas M. Bates (2000) Mixed-Effects Models in S and 
S-PLUS. Springer, New York.

-- 
Thilo Kellermann
Department of Psychiatry and Psychotherapy
RWTH Aachen University
Pauwelstr. 30
52074 Aachen
Tel.: +49 (0)241 / 8089977
Fax.: +49 (0)241 / 8082401
E-Mail: thilo at izkf.rwth-aachen.de


From ripley at stats.ox.ac.uk  Mon Jul 17 18:57:24 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 17 Jul 2006 17:57:24 +0100 (BST)
Subject: [R] glmmPQL help
In-Reply-To: <5363876.post@talk.nabble.com>
References: <5363876.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0607171754430.9101@gannet.stats.ox.ac.uk>

On Mon, 17 Jul 2006, Zython wrote:

> 
> I need to use the glmmPQL function for an assignment, but when I call for the
> summary of the function, it gives the AIC a value of NA. How do I get R to
> give me the AIC value?

You did!

Hint: have you read the reference of which this is part of the support 
software?  If so you will know that you need a maximized likelihood to 
evaluate AIC, and your fit is not an MLE.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mihainica at yahoo.com  Mon Jul 17 19:06:11 2006
From: mihainica at yahoo.com (Mihai Nica)
Date: Mon, 17 Jul 2006 10:06:11 -0700 (PDT)
Subject: [R] ols/gls or systemfit (OLS, WLS, SUR) give identical results
In-Reply-To: <44BAF6E2.30407@pdf.com>
Message-ID: <20060717170611.86971.qmail@web52501.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060717/f8a4385a/attachment.pl 

From joe-byers at utulsa.edu  Mon Jul 17 19:13:15 2006
From: joe-byers at utulsa.edu (Joe Byers)
Date: Mon, 17 Jul 2006 12:13:15 -0500
Subject: [R] Auto update package scripts
Message-ID: <44BBC52B.3070901@utulsa.edu>

I am trying to set up my home, office, and a server to auto update my R 
packages once a week.  I have a cron job on the server and a scheduled 
task on the other two pc's that are scripts.  The batch script is
    rem #!/bin/bash
    rem cron job for updating R weekly
    rem #options(echo = FALSE)
    set Rloc="\Program Files\R\bin\"
    set script=\Libraries\R\

%Rloc%R CMD BATCH %script%updatepackages.R

I also have a similar bash shell script for the server.

These scripts call the R script updatepackages.R which is
    options(echo = FALSE)
    update.packages(ask=FALSE,repos="http://cran.r-project.org")

My question is there a way to pipe the two R commands in the shell 
scripts so I do not have to have a seperate R script on my pc's or my 
server?

If anyone has a better idea, I would appreciate their comments and 
suggestions.

Thank you
Joe W. Byers


From NordlDJ at dshs.wa.gov  Mon Jul 17 19:24:36 2006
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS))
Date: Mon, 17 Jul 2006 10:24:36 -0700
Subject: [R] Variance functions in package nlme
Message-ID: <592E8923DB6EA348BE8E33FCAADEFFFC13EED929@dshs-exch2.dshs.wa.lcl>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Thilo Kellermann
> Sent: Monday, July 17, 2006 9:57 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Variance functions in package nlme
> 
> Dear R-help,
> 
> I am trying to set up linear mixed effects models in R using the
> (recommended)
> nlme package (R version 2.3.1 on a Linux platform). When trying to
> reproduce
> an example from Jose Pinheiro & Douglas Bates (2000, p 210) I get the
> following error message (code to produce message pasted as well):
> 
> library("nlme")
> data("Orthodont")
> vf1Ident <- varIdent( c(Female = 0.5), form = ~ 1 | Sex )
> vf1Ident <- initialize(vf1Ident, Orthodont)
> Error in getClass(Class) : c("\"varIdent\" is not a defined class",
> "\"varFunc\" is not a defined class")
> In addition: Warning message:
> the condition has length > 1 and only the first element will be used in:
> if
> (!is.na(match(Class, .BasicClasses))) return(newBasic(Class,
> 
> Can anybody give me a hint, what's going wrong here?
> Thanks a lot,
> 
> Thilo

I believe the function initialize() has been renamed Initialize() in R.

Hope this helps,

Dan

Daniel J. Nordlund
Research and Data Analysis
Washington State Department of Social and Health Services
Olympia, WA  98504-5204


From darrenleeweber at gmail.com  Mon Jul 17 19:46:00 2006
From: darrenleeweber at gmail.com (Darren Weber)
Date: Mon, 17 Jul 2006 10:46:00 -0700
Subject: [R] planned comparisons for ANOVA
In-Reply-To: <44BB198D.7060707@ozemail.com.au>
References: <d2095b8c0607162123s464336c7j38a8d25911203473@mail.gmail.com>
	<44BB198D.7060707@ozemail.com.au>
Message-ID: <d2095b8c0607171046g7f116301p2a697ead8389efcc@mail.gmail.com>

On 7/16/06, Simon Blomberg <blomsp at ozemail.com.au> wrote:
> Darren Weber wrote:
>
> [snip]
> > Our planned comparisons are:
> >
> > 1. test the group mean difference for S1 vs S2 (in the absence of S3)
> > 2. test the group mean difference for S2 vs S3 (in the absence of S1)
> >
> > This is the current form of the ANOVA specification for R:
> >
> > aov( Y ~ (Task*Hemisphere*Group) +
> >                 Error( Subject/(Task*Hemisphere) )
> >
> > How can we add planned comparisons to this specification?  Can we add
> > just one planned comparison matrix, with rows for 1 & 2 above, or do
> > we need to run the model twice, once for each planned comparison?
> >
> There are a couple of ways to do this in R. Perhaps the easiest is to
> use make.contrasts in the gmodels package.
>
> cmat <- rbind("S1 v S2" = c(1, -1, 0),
>                       "S2 v S3" = c(0, 1, -1))
> library(gmodels)
> fit <- aov( Y ~ Task*Hemisphere*Group + Error(
> Subject/(Task*Hemisphere), contrasts=list("Task"=make.contrasts(cmat )))
> summary(fit)
>
> > Alternatively, is there a function  to compute the planned comparisons
> > after running the full ANOVA model?
> >
> see ?fit.contrast in gmodels  for this.
>
> HTH,
>
> Simon.

Thanks, Simon.

In the summary(fit) display, it is difficult to identify the contrast
outputs.  There appears to be just one set of ANOVA results, eg:


Error: Subject
          Df  Sum Sq Mean Sq F value Pr(>F)
Group      1    7.04    7.04  0.1262 0.7265
Residuals 18 1004.00   55.78

Error: Subject:Cond
           Df Sum Sq Mean Sq F value    Pr(>F)
Cond        2 416.11  208.06 22.3016 5.001e-07 ***
Cond:Group  2  25.02   12.51  1.3409    0.2744
Residuals  36 335.85    9.33
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Error: Subject:Hemisphere
                 Df Sum Sq Mean Sq F value Pr(>F)
Hemisphere        1  3.666   3.666  1.6901 0.2100
Hemisphere:Group  1  1.804   1.804  0.8315 0.3739
Residuals        18 39.043   2.169

Error: Subject:Cond:Hemisphere
                      Df Sum Sq Mean Sq F value Pr(>F)
Cond:Hemisphere        2  6.328   3.164  2.0160 0.1480
Cond:Hemisphere:Group  2  0.786   0.393  0.2506 0.7797
Residuals             36 56.501   1.569


For some reason I expected to see two separate tests of the Task
"Cond"ition effect, one for the comparison of "S1 vs S2" and another
for the comparison of "S2 vs S3".  I seem to have missed something.

Also, I wanted to start with an easy example of using planned
comparisons with our experimental design.  In fact, our planned
comparisons involve the interaction of the group and task conditions.
How do we specify the planned comparisons for an interaction effect?
For example, we expect the task conditions to generate different brain
activity in controls and patients.  In our first planned comparison,
we actually expect "S2 > S1" for controls, but not for patients (they
have "S1 = S2").  What is the contrast matrix for this kind of
interaction effect and how do we specify it in the aov function?  Is
there some reading, with examples using R, on this topic?

Take care, Darren


From goran.brostrom at gmail.com  Mon Jul 17 19:59:31 2006
From: goran.brostrom at gmail.com (=?UTF-8?Q?G=C3=B6ran_Brostr=C3=B6m?=)
Date: Mon, 17 Jul 2006 19:59:31 +0200
Subject: [R] storing the estimates from lmer
In-Reply-To: <40e66e0b0607151139i7350d22bteacfaa61aa94a1f8@mail.gmail.com>
References: <bb47cf700607110818m782a98c5le08396d2fca203a@mail.gmail.com>
	<44B921BD.8090401@pdf.com>
	<40e66e0b0607151139i7350d22bteacfaa61aa94a1f8@mail.gmail.com>
Message-ID: <148ed8180607171059p59327568o9a5c9a2dfda0f813@mail.gmail.com>

On 7/15/06, Douglas Bates <bates at stat.wisc.edu> wrote:
> Hi Spencer,
[....]
> <rant>
> Some software, notably SAS PROC MIXED, does produce standard errors
> for the estimates of variances and covariances of random effects.  In
> my opinion this is more harmful than helpful.  The only use I can
> imagine for such standard errors is to form confidence intervals or to
> evaluate a z-statistic or something like that to be used in a
> hypothesis test.  However, those uses require that the distribution of
> the parameter estimate be symmetric, or at least approximately
> symmetric, and we know that the distribution of the estimate of a
> variance component is more like a scaled chi-squared distribution
> which is anything but symmetric.

You should add ..."when the true value of the variance is (close to)
zero", I guess. Or does not standard asymptotic ML theory apply to
these models? BTW, what is a
"scaled chi-squared distribution"?

G?ran


From leaflovesun at yahoo.ca  Mon Jul 17 20:18:08 2006
From: leaflovesun at yahoo.ca (Leaf Sun)
Date: Mon, 17 Jul 2006 12:18:08 -0600
Subject: [R] Weibull distribution
References: <1bff52c20606140033j3ac8e9fdxa85d4edded2ad94a@mail.gmail.com>
	<1bff52c20606140101r25188de1g6573c52f3cb5d152@mail.gmail.com>
	<38b9f0350606140223g507112dau3478586790daad3d@mail.gmail.com>
	<200606140735459773486@yahoo.ca>
	<449285BD.3020101@statistik.uni-dortmund.de>
	<200606161041072848729@yahoo.ca>
	<40e66e0b0606180515t759f1400od071251942d8dac6@mail.gmail.com>
	<200606191251227312041@yahoo.ca>
Message-ID: <200607171218066725256@yahoo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060717/344ea936/attachment.pl 

From rmh at temple.edu  Mon Jul 17 20:19:06 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Mon, 17 Jul 2006 14:19:06 -0400 (EDT)
Subject: [R] planned comparisons for ANOVA
Message-ID: <20060717141906.BEH85013@po-d.temple.edu>

It looks to me like you are asking for the split= argument to
the summary.aov method.  Look at ?summary.aov and the
"# Cochran and Cox (1957, p.164)" example.

I have more examples in my book
                Statistical Analysis and Data Display
                Richard M. Heiberger and Burt Holland
                http://springeronline.com/0-387-40270-5


From duncan at wald.ucdavis.edu  Mon Jul 17 20:35:46 2006
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Mon, 17 Jul 2006 11:35:46 -0700
Subject: [R] put R on a web server
In-Reply-To: <loom.20060717T093452-633@post.gmane.org>
References: <200607152344.k6FNi0Om030065@gator.dt.uh.edu>
	<loom.20060717T093452-633@post.gmane.org>
Message-ID: <44BBD882.8090306@wald.ucdavis.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Indeed, security is a real issue.

The CGIwithR package is another, relatively simple way
to use R scripts as a CGI tool.

 D

Dieter Menne wrote:
> Erin Hodgess <hodgess <at> gator.dt.uh.edu> writes:
> 
> 
>>Has anyone put R on a web server any time, recently, please?
>>(Red Hat Linux)
>>
>>The University of Montana put a version up in 2003, but
>>I was wondering if anyone had done so, please?
>>
>>Also, where would I find information on such an installation, please?
> 
> 
> Mmh, putting R on a web server is not much different from putting it on an other
> linux installation. The question is  probably more how to get R to work. RPad is
> a  package that works, but as far as I remember the focus is on flexibility for
> Intranets, with high risks when used in the wild. If you want to run some
> specialized tasks only with fixed code, you could try our phpSerialize, which
> can run in the wild if your php code is ok, but it is by design not end-user
> flexible.
> 
> Dieter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

- --
Duncan Temple Lang                    duncan at wald.ucdavis.edu
Department of Statistics              work:  (530) 752-4782
4210 Mathematical Sciences Building   fax:   (530) 752-7099
One Shields Ave.
University of California at Davis
Davis,
CA 95616,
USA
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.3 (Darwin)

iD8DBQFEu9iC9p/Jzwa2QP4RAtWYAJ47MIqgVddfNwoxiChs2lzyzc9oCQCfXMTH
jJJEkX4/Fsahvyb1MiPe2nw=
=27X/
-----END PGP SIGNATURE-----


From fomchenko at iet.ru  Mon Jul 17 20:39:37 2006
From: fomchenko at iet.ru (Denis Fomchenko)
Date: Mon, 17 Jul 2006 22:39:37 +0400
Subject: [R] sem: negative parameter variances
Message-ID: <005001c6a9d0$5b806270$0f22a8c0@intiet.ru>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060717/8d06600c/attachment.pl 

From basu.15 at osu.edu  Mon Jul 17 21:00:55 2006
From: basu.15 at osu.edu (Deepankar Basu)
Date: Mon, 17 Jul 2006 15:00:55 -0400
Subject: [R] Large datasets in R
Message-ID: <1153162855.15457.8.camel@localhost.localdomain>

Hi!

I am a student of economics and currently do most of my statistical work
using STATA. For various reasons (not least of which is an aversion for
proprietary software), I am thinking of shifting to R. At the current
juncture my concern is the following: would I be able to work on
relatively large data-sets using R? For instance, I am currently working
on a data-set which is about 350MB in size. Would be possible to work
data-sets of such sizes using R?

I have been trying to read up the posting on the R-archive on this
topic; but I could not really understand all the discussion, nor could I
reach the "end". So, I am not aware of the current state of consensus on
the issue. 

It would help a lot if some current user could throw some light on this
issue of large data-sets in R.

Thanks in advance.

Deepankar Basu


From ggrothendieck at gmail.com  Mon Jul 17 21:10:07 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 17 Jul 2006 15:10:07 -0400
Subject: [R] Large datasets in R
In-Reply-To: <1153162855.15457.8.camel@localhost.localdomain>
References: <1153162855.15457.8.camel@localhost.localdomain>
Message-ID: <971536df0607171210l7d4b9c11kcede01b640f2164a@mail.gmail.com>

You may or may not have problems.  R keeps its data in memory so
you will have to have sufficient memory to hold the data plus all
derived data and code.   Since R is free you can try it out.  If your
problems are
too large you can always get more memory or use S-Plus which can
handle larger datasets and the code is similar to R so you can largely
reuse your code.

On 7/17/06, Deepankar Basu <basu.15 at osu.edu> wrote:
> Hi!
>
> I am a student of economics and currently do most of my statistical work
> using STATA. For various reasons (not least of which is an aversion for
> proprietary software), I am thinking of shifting to R. At the current
> juncture my concern is the following: would I be able to work on
> relatively large data-sets using R? For instance, I am currently working
> on a data-set which is about 350MB in size. Would be possible to work
> data-sets of such sizes using R?
>
> I have been trying to read up the posting on the R-archive on this
> topic; but I could not really understand all the discussion, nor could I
> reach the "end". So, I am not aware of the current state of consensus on
> the issue.
>
> It would help a lot if some current user could throw some light on this
> issue of large data-sets in R.
>
> Thanks in advance.
>
> Deepankar Basu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From tlumley at u.washington.edu  Mon Jul 17 21:20:42 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 17 Jul 2006 12:20:42 -0700 (PDT)
Subject: [R] Large datasets in R
In-Reply-To: <1153162855.15457.8.camel@localhost.localdomain>
References: <1153162855.15457.8.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.64.0607171207000.9430@homer23.u.washington.edu>

On Mon, 17 Jul 2006, Deepankar Basu wrote:

> Hi!
>
> I am a student of economics and currently do most of my statistical work
> using STATA. For various reasons (not least of which is an aversion for
> proprietary software), I am thinking of shifting to R. At the current
> juncture my concern is the following: would I be able to work on
> relatively large data-sets using R? For instance, I am currently working
> on a data-set which is about 350MB in size. Would be possible to work
> data-sets of such sizes using R?


The answer depends on a lot of things, but most importantly
1) What you are going to do with the data
2) Whether you have a 32-bit or 64-bit version of R
3) How much memory your computer has.

In a 32-bit version of R (where R will not be allowed to address more than 
2-3Gb of memory) an object of size 350Mb is large enough to cause problems 
(see eg the R Installation and Adminstration Guide).

If your 350Mb data set has lots of variables and you only use a few at a 
time then you may not have any trouble even on a 32-bit system once you 
have read in the data.

If you have a 64-bit version of R and a few Gb of memory then there should 
be no real difficulty in working with that size of data set for most 
analyses.  You might come across some analyses (eg some cluster analysis 
functions) that use n^2 memory for n observations and so break down.


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From bates at stat.wisc.edu  Mon Jul 17 21:42:11 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 17 Jul 2006 14:42:11 -0500
Subject: [R] storing the estimates from lmer
In-Reply-To: <148ed8180607171059p59327568o9a5c9a2dfda0f813@mail.gmail.com>
References: <bb47cf700607110818m782a98c5le08396d2fca203a@mail.gmail.com>
	<44B921BD.8090401@pdf.com>
	<40e66e0b0607151139i7350d22bteacfaa61aa94a1f8@mail.gmail.com>
	<148ed8180607171059p59327568o9a5c9a2dfda0f813@mail.gmail.com>
Message-ID: <40e66e0b0607171242i73f7238rf2c2be99911878b4@mail.gmail.com>

On 7/17/06, G?ran Brostr?m <goran.brostrom at gmail.com> wrote:
> On 7/15/06, Douglas Bates <bates at stat.wisc.edu> wrote:
> [....]
> > <rant>
> > Some software, notably SAS PROC MIXED, does produce standard errors
> > for the estimates of variances and covariances of random effects.  In
> > my opinion this is more harmful than helpful.  The only use I can
> > imagine for such standard errors is to form confidence intervals or to
> > evaluate a z-statistic or something like that to be used in a
> > hypothesis test.  However, those uses require that the distribution of
> > the parameter estimate be symmetric, or at least approximately
> > symmetric, and we know that the distribution of the estimate of a
> > variance component is more like a scaled chi-squared distribution
> > which is anything but symmetric.
>
> You should add ..."when the true value of the variance is (close to)
> zero", I guess. Or does not standard asymptotic ML theory apply to
> these models? BTW, what is a
> "scaled chi-squared distribution"?

Consider a simple case of an iid sample from a normal distribution
with mean $\mu$ and variance $\sigma^2$.  In that case the sample
variance $s^2$ has a $\sigma^2\chi^2$ distribution with n-1 degrees of
freedom.  (Either that or I have been seriously misinforming my intro
statistics classes for several years now.)  That's all I meant by a
"scaled chi-squared distribution".

All I am claiming here is that estimates of other variance components
in more complicated models have a similar behavior, not exactly this
behavior.  The point is that they would not be expected to have nice,
symmetric distributions that can be characterized by the estimate and
a standard error of the estimate.  If you create a Markov chain Monte
Carlo sample from a fitted lmer object you generally find that the
logarithm of a variance component has a posterior distribution that is
close to symmetric.  Depending on how precisely the variance component
is estimated, the distribution of the variance component itself can be
far from symmetric.

If it still seems that I am stating things too loosely then perhaps we
could correspond off-list and I could try to explain more clearly what
I am claiming.


From jfox at mcmaster.ca  Mon Jul 17 21:59:27 2006
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 17 Jul 2006 15:59:27 -0400
Subject: [R] sem: negative parameter variances
In-Reply-To: <005001c6a9d0$5b806270$0f22a8c0@intiet.ru>
Message-ID: <20060717195928.QFQP1747.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Denis,

I'm not in a position to comment on the spatial and time-series aspects of
the data, though I'd be concerned about treating the observations as
independent. (I understand that you have some way of accounting for spatial
and temporal dependence.)

Except for the allowance that you've made for cross-equation correlated
errors, this model could be fit by equation-by-equation OLS regression, and
in its current form could be fit by equation-by-equation 2SLS. That FIML has
run into numerical problems is a signal, however, that the combination of
model and data are ill-conditioned in some way. 

The output is a bit hard to follow because, for some reason, the table of
parameter estimates, standard errors, etc., is ravelled in your email. Some
of the parameter estimates are apparently very small numbers (I'm not sure
why you're not seeing more significant digits in some values), and you might
be able to make the computations more stable by changing the units of some
of the variables. Eliminating the intercepts and working with the covariance
matrix might also make the problem better conditioned.

"NaN" stands for "not a number," and is produced here when the summary
method for sem objects tries to take the square-root of a negative variance;
consequently, the standard error, z-statistic, and p-value are all
undefined. 

BTW, I count 15, not 13, exogenous variables (including the intercept).

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Denis Fomchenko
> Sent: Monday, July 17, 2006 1:40 PM
> To: John Fox; 'Spencer Graves'
> Cc: r-help at stat.math.ethz.ch
> Subject: [R] sem: negative parameter variances
> 
> Dear Spencer and Prof. Fox,
> 
>  
> 
> Thank you for your replies. I'll very appreciate, if you have 
> any ideas concerning the problem described below.
> 
>  
> 
> First, I'd like to describe the model in brief. 
> 
> In general I consider a model with three equations.
> 
> First one is for annual GRP growth - in general it looks like:
> 
>  
> 
> 1) GRP growth per capita = G(investment, migration, initial 
> GRP per capita, spatial lag on GRP growth + some additional 
> explanatory variables to control for regional disparities),
> 
>  
> 
> where spatial lag on GRP growth  in year t is simply spatial 
> weights matrix W(n*n) (weights - are inverse square distances 
> between regional centers; with zeros on the main diagonal) 
> times GRP growth in year t, (n*1). I compute it manually for 
> every year. 
> 
>  
> 
> Two others are for migration (labor supply) and investment 
> (capital supply):
> 
>  
> 
> 2) Migration = M(factors, explaining migration)
> 
> 3) Investment = I(factors, explaining investment)
> 
>  
> 
> I consider GRP growth, migration and investment to be 
> endogenous and all others vars to be exogenous to the model. 
> The data are over 77 regions (n=77) and 8 years (t = 
> 1997..2004), so that I have total 616 observations.
> 
>  
> 
> Actually, the final goal of the study is to estimate a model 
> 1) simultaneously by FIML 2) with Mundlak (1978, 1981) 
> specification of panels to capture for fixed and between 
> effects 3) with spatial lag on GRP growth. As for spatial lag 
> - the model needs to be estimated by maximum likelihood (as 
> it is shown in Anselin 1988, for example)
> 
> The main problems with Mundlak are increasing number of 
> variables: as well as dimension of a model (additional 
> between equations). So to begin with, I try to estimate pool, 
> which is obviously easier, and proceed as follows:
> 
>  
> 
> Consider the following specification:
> 
>  
> 
> model.3eq.33 <- specify.model( )
> 
> ln.inv.p0.pc                              ->        
> ln.grp.phvi.pc   ,           beta13             ,           NA
> 
> ln.migr.new                              ->        
> ln.grp.phvi.pc   ,           beta12             ,           NA
> 
> ln.grp.corr.pc97                       ->        
> ln.grp.phvi.pc   ,           gamma11         ,           NA
> 
> ln.sh.ind.rawwide                     ->        
> ln.grp.phvi.pc   ,           gamma12         ,           NA
> 
> port                                         ->        
> ln.grp.phvi.pc   ,           gamma13         ,           NA
> 
> t12                                          ->        
> ln.grp.phvi.pc   ,           gamma14         ,           NA
> 
> wd.ln.grp.phvi.pc                     ->        
> ln.grp.phvi.pc   ,           rho                   ,           NA
> 
> (Intercept)                               ->        
> ln.grp.phvi.pc   ,           alpha1              ,           NA
> 
> ln.income.pc.fcb97                   ->        ln.migr.new    
>   ,           gamma21         ,           NA
> 
> ln.unempl.level97                     ->        ln.migr.new   
>    ,           gamma26         ,           NA
> 
> avertemp.jan.cs                        ->        ln.migr.new  
>     ,           gamma22         ,           NA
> 
> ln.pop.gr.89.26mbe                 ->        ln.migr.new      
> ,           gamma23         ,           NA
> 
> ln.pass.railway.percap              ->        ln.migr.new     
>  ,           gamma24         ,           NA
> 
> ln.city                                      ->        
> ln.migr.new      ,           gamma25         ,           NA
> 
>  (Intercept)                              ->        
> ln.migr.new      ,           alpha2              ,           NA
> 
> ln.grp.corr.pc97                       ->        ln.inv.p0.pc 
>      ,           gamma31         ,           NA
> 
> avertemp.jan.cs                        ->        ln.inv.p0.pc 
>      ,           gamma32         ,           NA
> 
> permafrost                               ->        
> ln.inv.p0.pc      ,           gamma33         ,           NA
> 
> ln.indoutput.fuel.p0.pc              ->        ln.inv.p0.pc   
>    ,           gamma34         ,           NA
> 
> ln.phone1995                           ->        ln.inv.p0.pc 
>      ,           gamma35         ,           NA
> 
> (Intercept)                               ->        
> ln.inv.p0.pc      ,           alpha3              ,           NA
> 
> ln.grp.phvi.pc                           <->      
> ln.grp.phvi.pc   ,           sigma11           ,           NA
> 
> ln.grp.phvi.pc                           <->      ln.migr.new 
>      ,           sigma12           ,           NA
> 
> ln.grp.phvi.pc                           <->      
> ln.inv.p0.pc      ,           sigma13           ,           NA
> 
> ln.migr.new                              <->      ln.migr.new 
>      ,           sigma22           ,           NA
> 
> ln.migr.new                              <->      
> ln.inv.p0.pc      ,           sigma23           ,           NA
> 
> ln.inv.p0.pc                              <->      
> ln.inv.p0.pc      ,           sigma33           ,           NA
> 
>  
> 
> So I consider a model with 3 equations, 3 endogenous and 13 
> exogenous variables and 6 double-arrow relations for error covariance.
> 
>  
> 
> Then, I use raw moments to capture (Intercept) in each equation:
> 
>  
> 
> raw.3eq.33 <- raw.moments(~ ln.grp.phvi.pc + ln.inv.p0.pc + 
> ln.migr.new + ln.grp.corr.pc97 + ln.sh.ind.rawwide + port + 
> t12 + wd.ln.grp.phvi.pc + ln.income.pc.fcb97 + 
> ln.unempl.level97 + avertemp.jan.cs + ln.pop.gr.89.26mbe + 
> ln.pass.railway.percap + ln.city + permafrost + 
> ln.indoutput.fuel.p0.pc + ln.phone1995, data=pool.spat)
> 
>  
> 
> Next, I estimate model by sem-function (here, I declare fixed 
> exogenous variables, including spatial lag 
> 'wd.ln.grp.phvi.pc' ... though I am not sure about that)
> 
>  
> 
> model.3eq.33.estim <- sem(model.3eq.33, raw.3eq.33, 616, 
> fixed.x=c('ln.grp.corr.pc97', 'ln.sh.ind.rawwide', 'port', 
> 't12', 'wd.ln.grp.phvi.pc', 'ln.income.pc.fcb97', 
> 'ln.unempl.level97', 'avertemp.jan.cs', 'ln.pop.gr.89.26mbe', 
> 'ln.pass.railway.percap', 'ln.city', 'permafrost', 
> 'ln.indoutput.fuel.p0.pc', 'ln.phone1995', '(Intercept)'), raw=TRUE)
> 
>  
> 
> Then, R returns:
> 
>  
> 
> Warning message:
> Negative parameter variances.
> Model is probably underidentified.
>  in: sem.default(ram = ram, S = S, N = N, param.names = pars, 
> var.names = vars,  :
> 
>  
> 
> Obtained summary:
> 
>  
> 
> summary(model.3eq.33.estim)
> 
>  
> 
> Model fit to raw moment matrix.
> 
> Model Chisquare =  398.81   Df =  24 Pr(>Chisq) = 0
> 
> Goodness-of-fit index =  0.944
> 
> Adjusted goodness-of-fit index =  0.60099
> 
> RMSEA index =  0.15923   90 % CI: (0.14569, 0.17316)
> 
> BIC =  175.29 
> 
> Normalized Residuals
> 
> Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
> 
> -0.9120  0.0000  0.0000  0.0268  0.0000  1.5600
> 
>  
> 
>       Parameter
>      Estimate
>      Std Error
>      z value
>      Pr(>|z|)
>       
>       
>       
>      
>       beta13
>      0.0151
>      0.0031
>      4.9157
>      0.0000
>      ln.grp.phvi.pc
>      <---
>      ln.inv.p0.pc
>      
>       beta12
>      0.9983
>      0.4831
>      2.0663
>      0.0388
>      ln.grp.phvi.pc
>      <---
>      ln.migr.new
>      
>       gamma11
>      -0.0268
>      0.0074
>      -3.6420
>      0.0003
>      ln.grp.phvi.pc
>      <---
>      ln.grp.corr.pc97
>      
>       gamma12
>      0.0120
>      0.0174
>      0.6867
>      0.4923
>      ln.grp.phvi.pc
>      <---
>      ln.sh.ind.rawwide
>      
>       gamma13
>      0.0116
>      0.0062
>      1.8803
>      0.0601
>      ln.grp.phvi.pc
>      <---
>      port
>      
>       gamma14
>      -0.0063
>      0.0058
>      -1.0996
>      0.2715
>      ln.grp.phvi.pc
>      <---
>      t12
>      
>       rho
>      0.7542
>      0.0479
>      15.7449
>      0.0000
>      ln.grp.phvi.pc
>      <---
>      wd.ln.grp.phvi.pc
>      
>       alpha1
>      0.1488
>      0.0667
>      2.2323
>      0.0256
>      ln.grp.phvi.pc
>      <---
>      (Intercept)
>      
>       gamma25
>      0.0027
>      0.0013
>      2.1277
>      0.0334
>      ln.migr.new
>      <---
>      ln.income.pc.fcb97
>      
>       gamma26
>      0.0038
>      0.0012
>      3.1503
>      0.0016
>      ln.migr.new
>      <---
>      ln.unempl.level97
>      
>       gamma27
>      0.0003
>      0.0000
>      9.7510
>      0.0000
>      ln.migr.new
>      <---
>      avertemp.jan.cs
>      
>       gamma28
>      -0.0028
>      0.0003
>      -9.6679
>      0.0000
>      ln.migr.new
>      <---
>      ln.pop.gr.89.26mbe
>      
>       gamma29
>      0.0020
>      0.0009
>      2.2828
>      0.0224
>      ln.migr.new
>      <---
>      ln.pass.railway.percap
>      
>       gamma210
>      0.0030
>      0.0004
>      8.2058
>      0.0000
>      ln.migr.new
>      <---
>      ln.city
>      
>       alpha2
>      -0.0244
>      0.0041
>      -5.9237
>      0.0000
>      ln.migr.new
>      <---
>      (Intercept)
>      
>       gamma31
>      0.7727
>      0.1070
>      7.2237
>      0.0000
>      ln.inv.p0.pc
>      <---
>      ln.grp.corr.pc97
>      
>       gamma37
>      0.0123
>      0.0063
>      1.9596
>      0.0500
>      ln.inv.p0.pc
>      <---
>      avertemp.jan.cs
>      
>       gamma311
>      0.1208
>      0.1118
>      1.0799
>      0.2802
>      ln.inv.p0.pc
>      <---
>      permafrost
>      
>       gamma312
>      0.1805
>      0.0246
>      7.3497
>      0.0000
>      ln.inv.p0.pc
>      <---
>      ln.indoutput.fuel.p0.pc
>      
>       gamma313
>      0.3888
>      0.1360
>      2.8591
>      0.0042
>      ln.inv.p0.pc
>      <---
>      ln.phone1995
>      
>       alpha3
>      -1.1867
>      0.8601
>      -1.3798
>      0.1677
>      ln.inv.p0.pc
>      <---
>      (Intercept)
>      
>       sigma11
>      0.0025
>      0.0001
>      17.3196
>      0.0000
>      ln.grp.phvi.pc
>      <-->
>      ln.grp.phvi.pc
>      
>       sigma12
>      0.0000
>      0.0000
>      -1.3206
>      0.1867
>      ln.migr.new
>      <-->
>      ln.grp.phvi.pc
>      
>       sigma13
>      0.0001
>      NaN
>      NaN
>      NaN
>      ln.inv.p0.pc
>      <-->
>      ln.grp.phvi.pc
>      
>       sigma22
>      0.0000
>      0.0000
>      17.5513
>      0.0000
>      ln.migr.new
>      <-->
>      ln.migr.new
>      
>       sigma23
>      -0.0001
>      0.0001
>      -0.5696
>      0.5690
>      ln.inv.p0.pc
>      <-->
>      ln.migr.new
>      
>       sigma33
>      0.5859
>      0.0334
>      17.5470
>      0.0000
>      ln.inv.p0.pc
>      <-->
>      ln.inv.p0.pc
>      
> 
>  
> 
> Iterations =  205 
> 
> Aliased parameters: sigma13 
> 
> Warning message:
> 
> NaN in: sqrt(diag(object$cov))
> 
>  
> 
> So R estimates the model with NaN for sigma13: I'm not sure 
> what it means... As you can see, maximum is occurred at a 
> negative value for sigma13. At the same time one can check 
> for rank condition - if I'm not mistaken, all three equations 
> are exact.
> 
>  
> 
> After that, I estimate another specification, for example, 
> including 'ln.postgrad.students.pc.be' (post-graduate 
> students) instead of  't12' (dummy for poor and depressed 
> regions), so that these two specifications differ only in one 
> exogenous variable. I proceed in the same manner as before 
> (including 'ln.postgrad.students.pc.be' instead of 't12' in 
> specifying the model, computing raw moments and declaring new 
> var to be fixed exogenous). So, I do not provide sem's code 
> for the sake of space saving. 
> 
> Now R estimates the system "correctly", without any warning messages: 
> 
>  
> 
> Model fit to raw moment matrix.
> 
> Model Chisquare =  405.73   Df =  24 Pr(>Chisq) = 0
> 
> Goodness-of-fit index =  0.94308
> 
> Adjusted goodness-of-fit index =  0.59441
> 
> RMSEA index =  0.16069   90 % CI: (0.14715, 0.17462)
> 
> BIC =  182.21 
> 
> Normalized Residuals
> 
>  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
> 
> -0.9210  0.0000  0.0000  0.0286  0.0000  1.6000
> 
>  
> 
>       Parameter
>      Estimate
>      Std Error
>      z value
>      Pr(>|z|)
>       
>       
>       
>      
>       beta13
>      0.0157
>      0.0036
>      4.3162
>      0.0000
>      ln.grp.phvi.pc
>      <---
>      ln.inv.p0.pc
>      
>       beta12
>      0.9143
>      0.5337
>      1.7132
>      0.0867
>      ln.grp.phvi.pc
>      <---
>      ln.migr.new
>      
>       gamma11
>      -0.0268
>      0.0068
>      -3.9642
>      0.0001
>      ln.grp.phvi.pc
>      <---
>      ln.grp.corr.pc97
>      
>       gamma12
>      0.0221
>      0.0171
>      1.2919
>      0.1964
>      ln.grp.phvi.pc
>      <---
>      ln.sh.ind.rawwide
>      
>       gamma13
>      0.0141
>      0.0069
>      2.0499
>      0.0404
>      ln.grp.phvi.pc
>      <---
>      port
>      
>       gamma14
>      0.0061
>      0.0032
>      1.8769
>      0.0605
>      ln.grp.phvi.pc
>      <---
>      ln.postgrad.students.pc.be
>      
>       rho
>      0.7483
>      0.0480
>      15.5823
>      0.0000
>      ln.grp.phvi.pc
>      <---
>      wd.ln.grp.phvi.pc
>      
>       alpha1
>      0.1433
>      0.0591
>      2.4256
>      0.0153
>      ln.grp.phvi.pc
>      <---
>      (Intercept)
>      
>       gamma25
>      0.0026
>      0.0014
>      1.8682
>      0.0617
>      ln.migr.new
>      <---
>      ln.income.pc.fcb97
>      
>       gamma26
>      0.0038
>      0.0013
>      2.8600
>      0.0042
>      ln.migr.new
>      <---
>      ln.unempl.level97
>      
>       gamma27
>      0.0003
>      0.0000
>      9.3817
>      0.0000
>      ln.migr.new
>      <---
>      avertemp.jan.cs
>      
>       gamma28
>      -0.0028
>      0.0003
>      -9.5709
>      0.0000
>      ln.migr.new
>      <---
>      ln.pop.gr.89.26mbe
>      
>       gamma29
>      0.0021
>      0.0010
>      2.1621
>      0.0306
>      ln.migr.new
>      <---
>      ln.pass.railway.percap
>      
>       gamma210
>      0.0030
>      0.0004
>      7.9666
>      0.0000
>      ln.migr.new
>      <---
>      ln.city
>      
>       alpha2
>      -0.0242
>      0.0042
>      -5.7363
>      0.0000
>      ln.migr.new
>      <---
>      (Intercept)
>      
>       gamma31
>      0.7733
>      0.1096
>      7.0574
>      0.0000
>      ln.inv.p0.pc
>      <---
>      ln.grp.corr.pc97
>      
>       gamma37
>      0.0123
>      0.0067
>      1.8285
>      0.0675
>      ln.inv.p0.pc
>      <---
>      avertemp.jan.cs
>      
>       gamma311
>      0.1209
>      0.1201
>      1.0065
>      0.3142
>      ln.inv.p0.pc
>      <---
>      permafrost
>      
>       gamma312
>      0.1806
>      0.0250
>      7.2323
>      0.0000
>      ln.inv.p0.pc
>      <---
>      ln.indoutput.fuel.p0.pc
>      
>       gamma313
>      0.3879
>      0.1379
>      2.8129
>      0.0049
>      ln.inv.p0.pc
>      <---
>      ln.phone1995
>      
>       alpha3
>      -1.1881
>      0.8754
>      -1.3572
>      0.1747
>      ln.inv.p0.pc
>      <---
>      (Intercept)
>      
>       sigma11
>      0.0025
>      0.0001
>      17.2039
>      0.0000
>      ln.grp.phvi.pc
>      <-->
>      ln.grp.phvi.pc
>      
>       sigma12
>      0.0000
>      0.0000
>      -1.1470
>      0.2514
>      ln.migr.new
>      <-->
>      ln.grp.phvi.pc
>      
>       sigma13
>      -0.0001
>      0.0010
>      -0.0820
>      0.9346
>      ln.inv.p0.pc
>      <-->
>      ln.grp.phvi.pc
>      
>       sigma22
>      0.0000
>      0.0000
>      17.5416
>      0.0000
>      ln.migr.new
>      <-->
>      ln.migr.new
>      
>       sigma23
>      -0.0001
>      0.0002
>      -0.4366
>      0.6624
>      ln.inv.p0.pc
>      <-->
>      ln.migr.new
>      
>       sigma33
>      0.5859
>      0.0334
>      17.5370
>      0.0000
>      ln.inv.p0.pc
>      <-->
>      ln.inv.p0.pc
>      
> 
> Iterations = 137
> 
>  
> 
> In fact, I've estimated about 30 specifications trying to 
> experiment with different additional explanatory variables in 
> the first equation, and about two thirds of 30 are turned out 
> to be estimated with that warning message (Negative parameter 
> variances with NaN for sigma's variances). I've not been 
> clarified yet with the final specification... though I am not 
> sure that's the case. Perhaps, I do something wrong 
> concerning specification of error covariance structure. 
> 
>  
> 
> Do you have any idea? I'll be pleased by any suggestion. 
> 
>  
> 
> Sorry to trouble you.
> 
> Thanks in advance,
> 
>  
> 
> Denis Fomchenko
> 
> Research fellow
> 
> Department for Economic Development Problems
> 
> Institute for the Economy in Transition
> 
> 5, Gazetny lane, Moscow 125993, Russia
> 
> e-mail: fomchenko at iet.ru
> 
> http://www.iet.ru
> 
>  
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


From wiedenhoeft at gmx.net  Mon Jul 17 22:34:34 2006
From: wiedenhoeft at gmx.net (John Wiedenhoeft)
Date: Mon, 17 Jul 2006 22:34:34 +0200
Subject: [R] Nested functions
Message-ID: <1153168475.5952.16.camel@localhost>

Hi there,

I'm having myself a hard time writing an algorithm for finding patterns
within a given melody. In a vector I'd like to find ALL sequences that
occur at least twice, without having to check all possible patterns via
pattern matching.

I finally found a solution in a style that I'm used from C, i.e. calling
one function from within another. GNU R doesn't seem to like that, it
complains about too deep nesting and infinite recursion (I can't see
that...). I've tried options(expressions=500000), but even then the
variable a doesn't make it over 2 (my vectors have about 100-300
elements).

I'm not a software engineer, so I guess that algo is rather badly
designed. I'd appreciate any help on how to make it suitable for R, or
about alternative approaches (I guess something like this must be used
in bioinformatics, but I didn't find it implemented in R. Any hints are
welcome ;-) )

Cheers and thanx in advance,
John



CODECODCODECODECODECODECODECODECODECODECODECODECODECODECODECODE

antiphonar <- function(v)
{
	a <- 1;
	b <- 2;
	n <- length(v);
	alessn(a, b, n, v, x);
}


alessn <- function(a, b, n, v, x)
{
	if(a<n)
	{vavb(a, b, n, v, x);}
	else{print("That's all, folks ;-)");}
}


vavb <- function(a, b, n, v, x)
{
	if(v[a]==v[b])
	{
		x <- 1;
		while( v[a+x] == v[b+x] && b+x<n)
			{x <- x+1;}
		m <- 0;
		for(k in 0:(x-1))
			{m <- v[a+k]*10^(x-1-k)+m;}
		p <- c(x, a, b, m);
		print(p);
		baba(a, b, n, v, x);
	}
	else baba(a, b, n, v, x);
}


baba <- function(a, b, n, v, x)
{
	b <- b+1;
	if(b<=n)
	{vavb(a, b, n, v, x);}
	else
	{
		a <- a+1;
		b <- a+1;
		alessn(a, b, n, v, x);
	}
}

ENDENDENDENDENDENDENDENDENDENDENDENDENDENDENDENDENDENDENDENDEND


From karim at mast.queensu.ca  Tue Jul 18 00:05:59 2006
From: karim at mast.queensu.ca (Karim Rahim)
Date: Mon, 17 Jul 2006 18:05:59 -0400
Subject: [R] dpss.taper for spectral estimation
Message-ID: <44BC09C7.6090306@mast.queensu.ca>

Hi Rouyer,

You can redefine dpss.taper as follows

dpss.taper.2 <- function (n, k, nw = 4, nmax = 2^(ceiling(log(n, 2))))
{
     if (n > nmax)
         stop("length of taper is greater than nmax")
     w <- nw/n
     if (w > 0.5)
         stop("half-bandwidth parameter (w) is greater than 1/2")
     if (k <= 0)
         stop("positive dpss order (k) required")
     v <- matrix(0, nrow = nmax, ncol = (k + 1))

     storage.mode(v) <- "double"
     out <- .Fortran("dpss", nmax = as.integer(nmax), kmax = as.integer(k),
         n = as.integer(n), w = as.double(w), v = v, sig = double(k +
             1), totit = integer(1), sines = double(n), vold = double(n),
         u = double(n), scr1 = double(n), ifault = integer(1),
         PACKAGE = "waveslim")
     return(list(  v=out$v[1:n, 1:k],
                   eigen=1+out$sig[1:k],
                   iter=out$totit,
                   n=n,
                   w=w,
                   ifault=out$ifault) );
}

or you can calculate the eigenvalues from the tapers as done in
the tridiagonal dpss calculation at

http://lib.stat.cmu.edu/sapaclisp/multitaper.lisp

Karim


From wiedenhoeft at gmx.net  Tue Jul 18 00:29:40 2006
From: wiedenhoeft at gmx.net (John Wiedenhoeft)
Date: Tue, 18 Jul 2006 00:29:40 +0200
Subject: [R] Nested functions
In-Reply-To: <644e1f320607171436l1e25c0deq7fb070c07ac1ad8f@mail.gmail.com>
References: <1153168475.5952.16.camel@localhost>
	<644e1f320607171436l1e25c0deq7fb070c07ac1ad8f@mail.gmail.com>
Message-ID: <1153175380.3693.19.camel@localhost>

Thanks for your response!

Am Montag, den 17.07.2006, 17:36 -0400 schrieb jim holtman:
> It would help if you could provide the calling script and the data
> that you are using.  

The code I sent is included in .Rprofile. One of my data vectors would
be v = c(1, 1, 1, 5, 6, 1, 1, 1, 1, 6, 1, 1, 6, 6, 6, 6, 2, 6, 6, 1, 5,
5, 2, 1, 1, 1, 5, 6, 1, 2, 1, 1, 6, 5, 1, 1, 6, 6, 6, 6, 1, 6, 1, 2, 6,
6, 1, 5, 1, 2, 1, 6, 1, 1, 5, 6, 6, 1, 1, 6, 5, 1, 6, 6, 1, 5, 4, 1, 6,
6, 4, 6, 5, 1, 5, 4, 6, 5, 1, 5, 6, 1, 1, 1, 5, 6, 1, 1, 6) for example.
After defining it, I call antiphonar(v). This returns 53 entries, but as
I said, this is quite too few, as the program breaks before a=3.

> It sounds like your algorithm is not correct because it is recursing
> so deep.  You have recursive calls, so you are not terminating your
> search correctly.

Wouldn't it terminate on some call of alessn for a>=n?

> Your functions are not returning values.  

Should they? They just perform tests on v. As I said, I'm not an expert
programmer. I think in C++ there was something like a void type for
functions, i.e. functions that only perform operations on variables
without having a value for themselves.

> Put some debug print statements to see what is happening in your code.
> debug(antiphonar)
> antiphonar(v)
debugging in: antiphonar(v)
debug: {
    a <- 1
    b <- 2
    n <- length(v)
    alessn(a, b, n, v, x)
}
Browse[1]> n
debug: a <- 1
Browse[1]> n
debug: b <- 2
Browse[1]> n
debug: n <- length(v)
Browse[1]> n
debug: alessn(a, b, n, v, x)
Browse[1]> n
Fehler: Auswertung zu tief verschachtelt: unendliche Rekursion /
options(expressions=)?

Not very helpful :-(

Cheers,
John


From tuechler at gmx.at  Tue Jul 18 00:41:07 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Mon, 17 Jul 2006 23:41:07 +0100
Subject: [R] Keep value lables with data frame manipulation
In-Reply-To: <44B8475E.2070403@vanderbilt.edu>
References: <3.0.6.32.20060714202055.00aceea8@pop.gmx.net>
	<3.0.6.32.20060713144855.00a5e700@pop.gmx.net>
	<3.0.6.32.20060713095939.00ace1f8@pop.gmx.net>
	<2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>
	<2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>
	<3.0.6.32.20060713095939.00ace1f8@pop.gmx.net>
	<3.0.6.32.20060713144855.00a5e700@pop.gmx.net>
	<3.0.6.32.20060714202055.00aceea8@pop.gmx.net>
Message-ID: <3.0.6.32.20060717234107.00acf830@pop.gmx.net>

At 20:39 14.07.2006 -0500, Frank E Harrell Jr wrote:
>Heinz Tuechler wrote:
>> At 11:02 13.07.2006 -0500, Frank E Harrell Jr wrote:
>>> Heinz Tuechler wrote:
>>>> At 08:11 13.07.2006 -0500, Frank E Harrell Jr wrote:
>>>>> Heinz Tuechler wrote:
>>>>>> At 13:14 12.07.2006 -0500, Marc Schwartz (via MN) wrote:
>>>>>>> On Wed, 2006-07-12 at 17:41 +0100, Jol, Arne wrote:
>>>>>>>> Dear R,
>>>>>>>>
>>>>>>>> I import data from spss into a R data.frame. On this rawdata I do
some
>>>>>>>> data processing (selection of observations, normalization,
recoding of
>>>>>>>> variables etc..). The result is stored in a new data.frame,
however, in
>>>>>>>> this new data.frame the value labels are lost.
>>>>>>>>
>>>>>>>> Example of what I do in code:
>>>>>>>>
>>>>>>>> # read raw data from spss
>>>>>>>> rawdata <- read.spss("./data/T50937.SAV",
>>>>>>>> 	use.value.labels=FALSE,to.data.frame=TRUE)
>>>>>>>>
>>>>>>>> # select the observations that we need
>>>>>>>> diarydata <- rawdata[rawdata$D22==2 | rawdata$D22==3 |
>> rawdata$D22==17 |
>>>>>>>> rawdata$D22==18 | rawdata$D22==20 | rawdata$D22==22 |
>>>>>>>>  			rawdata$D22==24 | rawdata$D22==33,]
>>>>>>>>
>>>>>>>> The result is that rawdata$D22 has value labels and that
diarydata$D22
>>>>>>>> is numeric without value labels.
>>>>>>>>
>>>>>>>> Question: How can I prevent this from happening?
>>>>>>>>
>>>>>>>> Thanks in advance!
>>>>>>>> Groeten,
>>>>>>>> Arne
>>>>>>> Two things:
>>>>>>>
>>>>>>> 1. With respect to your subsetting, your lengthy code can be replaced
>>>>>>> with the following:
>>>>>>>
>>>>>>>  diarydata <- subset(rawdata, D22 %in% c(2, 3, 17, 18, 20, 22, 24,
33))
>>>>>>>
>>>>>>> See ?subset and ?"%in%" for more information.
>>>>>>>
>>>>>>>
>>>>>>> 2. With respect to keeping the label related attributes, the
>>>>>>> 'value.labels' attribute and the 'variable.labels' attribute will
not by
>>>>>>> default survive the use of "[".data.frame in R (see ?Extract
>>>>>>> and ?"[.data.frame").
>>>>>>>
>>>>>>> On the other hand, based upon my review of ?read.spss, the SPSS value
>>>>>>> labels should be converted to the factor levels of the respective
>>>>>>> columns when 'use.value.labels = TRUE' and these would survive a
>>>>>>> subsetting.
>>>>>>>
>>>>>>> If you want to consider a solution to the attribute subsetting issue,
>>>>>>> you might want to review the following post by Gabor Grothendieck in
>>>>>>> May, which provides a possible solution:
>>>>>>>
>>>>>>>  https://stat.ethz.ch/pipermail/r-help/2006-May/106308.html
>>>>>>>
>>>>>>> and this post by me, for an explanation of what is happening in
Gabor's
>>>>>>> solution:
>>>>>>>
>>>>>>>  https://stat.ethz.ch/pipermail/r-help/2006-May/106351.html
>>>>>>>
>>>>>>> HTH,
>>>>>>>
>>>>>>> Marc Schwartz
>>>>>>>
>>>>>> Hello Mark and Arne,
>>>>>>
>>>>>> I worked on the suggestions of Gabor and Mark and programmed some
>> functions
>>>>>> in this way, but they are very, very preliminary (see below).
>>>>>> In my view there is a lack of convenient possibilities in R to document
>>>>>> empirical data by variable labels, value labels, etc. I would prefer to
>>>>>> have these possibilities in the "standard" configuration.
>>>>>> So I sketched a concept, but in my view it would only be useful, if
there
>>>>>> was some acceptance by the core developers of R.
>>>>>>
>>>>>> The concept would be to define a class. For now I call it
"source.data".
>>>>>> To design it more flexible than the Hmisc class "labelled" I would
>> define a
>>>>>> related option "source.data.attributes" with default c('value.labels',
>>>>>> 'variable.name', 'label')). This option contains all attributes that
>> should
>>>>>> persist in subsetting/indexing.
>>>>>>
>>>>>> I made only some very, very preliminary tests with these functions,
>> mainly
>>>>>> because I am not happy with defining a new class. Instead I would
prefer,
>>>>>> if this functionality could be integrated in the Hmisc class
"labelled",
>>>>>> since this is in my view the best known starting point for data
>>>>>> documentation in R.
>>>>>>
>>>>>> I would be happy, if there were some discussion about the
wishes/needs of
>>>>>> other Rusers concerning data documentation.
>>>>>>
>>>>>> Greetings,
>>>>>>
>>>>>> Heinz
>>>>> I feel that separating variable labels and value labels and just using 
>>>>> factors for value labels works fine, and I would urge you not to create 
>>>>> a new system that will not benefit from the many Hmisc functions that 
>>>>> use variable labels and units.  [.data.frame in Hmisc keeps all
>> attributes.
>>>>> Frank
>>>>>
>>>> Frank,
>>>>
>>>> of course I aggree with you about the importance of Hmisc and as I
said, I
>>>> do not want to define a new class, but in my view factors are no good
>>>> substitute for value labels.
>>>> As the language definition (version 2.3.1 (2006-06-05) Draft, page 7)
says:
>>>> "Factors are currently implemented using an integer array to specify the
>>>> actual levels and a second array of names that are mapped to the
integers.
>>>> Rather unfortunately users often make use of the implementation in
order to
>>>> make some calculations easier." 
>>>> So, in my view, the levels represent the "values" of the factor.
>>>> This has inconveniencies if you want to use value labels in different
>>>> languages. Further I do not see a simple method to label numerical
>>>> variables. I often encounter discrete, but still metric data, as e.g.
risk
>>>> scores. Usually it would be nice to use them in their original coding,
>>>> which may include zero or decimal places and to label them at the same
>> time.
>>>> Personally at the moment I try to solve this problem by following a
>>>> suggestion of Martin, Dimitis and others to use names instead. I doubt,
>>>> however, that this is a good solution, but at least it makes it
possible to
>>>> have the source data numerically coded and in this sense "language free"
>>>> (see first attempts of functions below).
>>>>
>>>> Heinz
>>>>
>>> Those are excellent points Heinz.  I addressed that problem partially in 
>>> sas.get - see the sascodes attribute.
>>>
>>> Frank
>>>
>> 
>> Frank, I looked at your function sas.get. You solved the problem with a lot
>> of effort. Don't you think that it would be easier to create just one new
>> class, say "documented", which offers the possibility to represent the
>> original data as it is and to add all the useful descriptions like variable
>> labels, value labels, units, special missing values, and may be others.
>> If I remember correctly SAS, SPSS and BMDP offer these possibilities since
>> many years, and in my view for good reason. I am thinking about this
>> questions since I started using R about two years ago and I wonder, why
>> there seems to be so little interest in these questions.
>> In my work good documentation of the _unchanged_ data is very important,
>> also because it eases checking the data for errors.
>> 
>> Heinz
>> 
>> 
>>>> ...snip...
>> 
>> 
>> 
>
>Heinz - the code is quite small and simple, not much effort.  And 
>variable labels need to be attributes to individual variables, otherwise 
>   plotting, latex, and other functions can't get access to them (e.g., 
>in Hmisc xYplot(y ~ x) labels for x and y, and units of measurement, get 
>plotted on axes.  I've been having all the SAS, SPSS, and BMDP 
>capabilities you've mentioned in R/S-Plus (plus units attributes not 
>available in those) for years.
>
>What would make all this even easier is for R to be told a list of 
>attribute names that would always carry with subsetting, so that 
>specially subsetting methods such as [.labeled would not be necessary.
>
>Frank
>
>-- 
>Frank E Harrell Jr   Professor and Chair           School of Medicine
>                      Department of Biostatistics   Vanderbilt University
>
>

Frank - maybe I did not understand you right, but it seems that you propose
exactly what I did initially. Yes, I aggree with you that it would ease the
situation, if there were a list of respected attributes. However, I suspect
that it could be a computational burden to copy these attributes in any
case. So I would suggest to define a class that typically would be assigned
to raw data and to define an option that sets all the attributes which
should be copied.
Would you think this issue could/should be discussed in r-devel?

Heinz


From darrenleeweber at gmail.com  Tue Jul 18 01:17:40 2006
From: darrenleeweber at gmail.com (Darren Weber)
Date: Mon, 17 Jul 2006 16:17:40 -0700
Subject: [R] planned comparisons for ANOVA
In-Reply-To: <20060717141906.BEH85013@po-d.temple.edu>
References: <20060717141906.BEH85013@po-d.temple.edu>
Message-ID: <d2095b8c0607171617w6dae1d2cwb5ff712e14f51e58@mail.gmail.com>

On 7/17/06, Richard M. Heiberger <rmh at temple.edu> wrote:
> It looks to me like you are asking for the split= argument to
> the summary.aov method.  Look at ?summary.aov and the
> "# Cochran and Cox (1957, p.164)" example.
>
> I have more examples in my book
>                 Statistical Analysis and Data Display
>                 Richard M. Heiberger and Burt Holland
>                 http://springeronline.com/0-387-40270-5
>

Thanks for the additional advice.  At first glance, your book looks
good.  However, I have only taken undergraduate studies in statistics
(mostly taught by psychologists, not mathematicians), so the material
is most likely beyond my comprehension.  As a new user of R, it is
difficult to understand the methods and utilities.  The help pages are
consistent with most gnu/linux tools, they provide the essential
information, without much elaboration.  Most of the text books for R
are similar.  I am more familiar with stats texts in psychology, where
greater emphasis is placed on explanation (eg, Tabachnick & Fidell).

Anyhow, I can see now there are at least three utilities for contrasts
- the contrast option to aov, fit.contrast, summary.aov, and
estimable.  I don't have any clear rationale for using one or the
other.  I guess what I need to do now is generate a small example
dataset for which the planned comparison F values are known, to
validate the output of one or another of these routines.

Thanks, Darren


From llei at bccrc.ca  Tue Jul 18 01:57:47 2006
From: llei at bccrc.ca (Linda Lei)
Date: Mon, 17 Jul 2006 16:57:47 -0700
Subject: [R] use "factor" for categorical covariate in Cox PH model
Message-ID: <90B06673D826C64E8ED8EEA6B6FDF8CAE72C28@crcmail1.BCCRC.CA>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060717/1218e3a7/attachment.pl 

From f.harrell at vanderbilt.edu  Tue Jul 18 04:47:09 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 17 Jul 2006 21:47:09 -0500
Subject: [R] Keep value lables with data frame manipulation
In-Reply-To: <3.0.6.32.20060717234107.00acf830@pop.gmx.net>
References: <3.0.6.32.20060714202055.00aceea8@pop.gmx.net>
	<3.0.6.32.20060713144855.00a5e700@pop.gmx.net>
	<3.0.6.32.20060713095939.00ace1f8@pop.gmx.net>
	<2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>
	<2D0E2123711DE7478FE1EB933CD3046E31B85F@BRBSEVS20001.s2.ms.unilever.com>
	<3.0.6.32.20060713095939.00ace1f8@pop.gmx.net>
	<3.0.6.32.20060713144855.00a5e700@pop.gmx.net>
	<3.0.6.32.20060714202055.00aceea8@pop.gmx.net>
	<3.0.6.32.20060717234107.00acf830@pop.gmx.net>
Message-ID: <44BC4BAD.7060304@vanderbilt.edu>

Heinz Tuechler wrote:
> At 20:39 14.07.2006 -0500, Frank E Harrell Jr wrote:
>> Heinz Tuechler wrote:
>>> At 11:02 13.07.2006 -0500, Frank E Harrell Jr wrote:
>>>> Heinz Tuechler wrote:
>>>>> At 08:11 13.07.2006 -0500, Frank E Harrell Jr wrote:
>>>>>> Heinz Tuechler wrote:
>>>>>>> At 13:14 12.07.2006 -0500, Marc Schwartz (via MN) wrote:
>>>>>>>> On Wed, 2006-07-12 at 17:41 +0100, Jol, Arne wrote:
>>>>>>>>> Dear R,
>>>>>>>>>
>>>>>>>>> I import data from spss into a R data.frame. On this rawdata I do
> some
>>>>>>>>> data processing (selection of observations, normalization,
> recoding of
>>>>>>>>> variables etc..). The result is stored in a new data.frame,
> however, in
>>>>>>>>> this new data.frame the value labels are lost.
>>>>>>>>>
>>>>>>>>> Example of what I do in code:
>>>>>>>>>
>>>>>>>>> # read raw data from spss
>>>>>>>>> rawdata <- read.spss("./data/T50937.SAV",
>>>>>>>>> 	use.value.labels=FALSE,to.data.frame=TRUE)
>>>>>>>>>
>>>>>>>>> # select the observations that we need
>>>>>>>>> diarydata <- rawdata[rawdata$D22==2 | rawdata$D22==3 |
>>> rawdata$D22==17 |
>>>>>>>>> rawdata$D22==18 | rawdata$D22==20 | rawdata$D22==22 |
>>>>>>>>>  			rawdata$D22==24 | rawdata$D22==33,]
>>>>>>>>>
>>>>>>>>> The result is that rawdata$D22 has value labels and that
> diarydata$D22
>>>>>>>>> is numeric without value labels.
>>>>>>>>>
>>>>>>>>> Question: How can I prevent this from happening?
>>>>>>>>>
>>>>>>>>> Thanks in advance!
>>>>>>>>> Groeten,
>>>>>>>>> Arne
>>>>>>>> Two things:
>>>>>>>>
>>>>>>>> 1. With respect to your subsetting, your lengthy code can be replaced
>>>>>>>> with the following:
>>>>>>>>
>>>>>>>>  diarydata <- subset(rawdata, D22 %in% c(2, 3, 17, 18, 20, 22, 24,
> 33))
>>>>>>>> See ?subset and ?"%in%" for more information.
>>>>>>>>
>>>>>>>>
>>>>>>>> 2. With respect to keeping the label related attributes, the
>>>>>>>> 'value.labels' attribute and the 'variable.labels' attribute will
> not by
>>>>>>>> default survive the use of "[".data.frame in R (see ?Extract
>>>>>>>> and ?"[.data.frame").
>>>>>>>>
>>>>>>>> On the other hand, based upon my review of ?read.spss, the SPSS value
>>>>>>>> labels should be converted to the factor levels of the respective
>>>>>>>> columns when 'use.value.labels = TRUE' and these would survive a
>>>>>>>> subsetting.
>>>>>>>>
>>>>>>>> If you want to consider a solution to the attribute subsetting issue,
>>>>>>>> you might want to review the following post by Gabor Grothendieck in
>>>>>>>> May, which provides a possible solution:
>>>>>>>>
>>>>>>>>  https://stat.ethz.ch/pipermail/r-help/2006-May/106308.html
>>>>>>>>
>>>>>>>> and this post by me, for an explanation of what is happening in
> Gabor's
>>>>>>>> solution:
>>>>>>>>
>>>>>>>>  https://stat.ethz.ch/pipermail/r-help/2006-May/106351.html
>>>>>>>>
>>>>>>>> HTH,
>>>>>>>>
>>>>>>>> Marc Schwartz
>>>>>>>>
>>>>>>> Hello Mark and Arne,
>>>>>>>
>>>>>>> I worked on the suggestions of Gabor and Mark and programmed some
>>> functions
>>>>>>> in this way, but they are very, very preliminary (see below).
>>>>>>> In my view there is a lack of convenient possibilities in R to document
>>>>>>> empirical data by variable labels, value labels, etc. I would prefer to
>>>>>>> have these possibilities in the "standard" configuration.
>>>>>>> So I sketched a concept, but in my view it would only be useful, if
> there
>>>>>>> was some acceptance by the core developers of R.
>>>>>>>
>>>>>>> The concept would be to define a class. For now I call it
> "source.data".
>>>>>>> To design it more flexible than the Hmisc class "labelled" I would
>>> define a
>>>>>>> related option "source.data.attributes" with default c('value.labels',
>>>>>>> 'variable.name', 'label')). This option contains all attributes that
>>> should
>>>>>>> persist in subsetting/indexing.
>>>>>>>
>>>>>>> I made only some very, very preliminary tests with these functions,
>>> mainly
>>>>>>> because I am not happy with defining a new class. Instead I would
> prefer,
>>>>>>> if this functionality could be integrated in the Hmisc class
> "labelled",
>>>>>>> since this is in my view the best known starting point for data
>>>>>>> documentation in R.
>>>>>>>
>>>>>>> I would be happy, if there were some discussion about the
> wishes/needs of
>>>>>>> other Rusers concerning data documentation.
>>>>>>>
>>>>>>> Greetings,
>>>>>>>
>>>>>>> Heinz
>>>>>> I feel that separating variable labels and value labels and just using 
>>>>>> factors for value labels works fine, and I would urge you not to create 
>>>>>> a new system that will not benefit from the many Hmisc functions that 
>>>>>> use variable labels and units.  [.data.frame in Hmisc keeps all
>>> attributes.
>>>>>> Frank
>>>>>>
>>>>> Frank,
>>>>>
>>>>> of course I aggree with you about the importance of Hmisc and as I
> said, I
>>>>> do not want to define a new class, but in my view factors are no good
>>>>> substitute for value labels.
>>>>> As the language definition (version 2.3.1 (2006-06-05) Draft, page 7)
> says:
>>>>> "Factors are currently implemented using an integer array to specify the
>>>>> actual levels and a second array of names that are mapped to the
> integers.
>>>>> Rather unfortunately users often make use of the implementation in
> order to
>>>>> make some calculations easier." 
>>>>> So, in my view, the levels represent the "values" of the factor.
>>>>> This has inconveniencies if you want to use value labels in different
>>>>> languages. Further I do not see a simple method to label numerical
>>>>> variables. I often encounter discrete, but still metric data, as e.g.
> risk
>>>>> scores. Usually it would be nice to use them in their original coding,
>>>>> which may include zero or decimal places and to label them at the same
>>> time.
>>>>> Personally at the moment I try to solve this problem by following a
>>>>> suggestion of Martin, Dimitis and others to use names instead. I doubt,
>>>>> however, that this is a good solution, but at least it makes it
> possible to
>>>>> have the source data numerically coded and in this sense "language free"
>>>>> (see first attempts of functions below).
>>>>>
>>>>> Heinz
>>>>>
>>>> Those are excellent points Heinz.  I addressed that problem partially in 
>>>> sas.get - see the sascodes attribute.
>>>>
>>>> Frank
>>>>
>>> Frank, I looked at your function sas.get. You solved the problem with a lot
>>> of effort. Don't you think that it would be easier to create just one new
>>> class, say "documented", which offers the possibility to represent the
>>> original data as it is and to add all the useful descriptions like variable
>>> labels, value labels, units, special missing values, and may be others.
>>> If I remember correctly SAS, SPSS and BMDP offer these possibilities since
>>> many years, and in my view for good reason. I am thinking about this
>>> questions since I started using R about two years ago and I wonder, why
>>> there seems to be so little interest in these questions.
>>> In my work good documentation of the _unchanged_ data is very important,
>>> also because it eases checking the data for errors.
>>>
>>> Heinz
>>>
>>>
>>>>> ...snip...
>>>
>>>
>> Heinz - the code is quite small and simple, not much effort.  And 
>> variable labels need to be attributes to individual variables, otherwise 
>>   plotting, latex, and other functions can't get access to them (e.g., 
>> in Hmisc xYplot(y ~ x) labels for x and y, and units of measurement, get 
>> plotted on axes.  I've been having all the SAS, SPSS, and BMDP 
>> capabilities you've mentioned in R/S-Plus (plus units attributes not 
>> available in those) for years.
>>
>> What would make all this even easier is for R to be told a list of 
>> attribute names that would always carry with subsetting, so that 
>> specially subsetting methods such as [.labeled would not be necessary.
>>
>> Frank
>>
>> -- 
>> Frank E Harrell Jr   Professor and Chair           School of Medicine
>>                      Department of Biostatistics   Vanderbilt University
>>
>>
> 
> Frank - maybe I did not understand you right, but it seems that you propose
> exactly what I did initially. Yes, I aggree with you that it would ease the
> situation, if there were a list of respected attributes. However, I suspect
> that it could be a computational burden to copy these attributes in any
> case. So I would suggest to define a class that typically would be assigned
> to raw data and to define an option that sets all the attributes which
> should be copied.
> Would you think this issue could/should be discussed in r-devel?

Yes r-devel would be the place.  In retrospect a single attribute such 
as varExtras would have been good - it could contain label, units, etc. 
  But my functions are too well established for me to change now.  I'd 
have to change too much code.

Frank

> 
> Heinz


From spencer.graves at pdf.com  Tue Jul 18 07:49:01 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 17 Jul 2006 22:49:01 -0700
Subject: [R] optim()
In-Reply-To: <d95bc7680607141158s33b307f9j7899f318c41566b2@mail.gmail.com>
References: <d95bc7680607141158s33b307f9j7899f318c41566b2@mail.gmail.com>
Message-ID: <44BC764D.8020208@pdf.com>

	  I had good luck translating constrained into unconstrained problems 
and then optimizing the unconstrained problem.  Have you tried something 
like the following:

Define:
	  z = c(z1, z2, z3), where p1=1/(1+exp(-z1), etc.  This translates the 
constraints on the p's to

	  G(z) = P*(f1(z)-r12*f2(z))^2-f1(z)

where f1(z) = f1(p1(z1), p2(z2), p3(z3), and similarly for f2(z), and 
where P = a penalty term,
and r12 = (1-c)*k1/(c*(1-k1).

	  Can f2(z) ever go outside (0, 1)?  If yes, I would modify G(z) by 
adding a term like (min(0, f2(z), 1-f2(z))^2)

	  If I haven't made a math error, your problem should translate into 
this form.  I first solve this problem for z with P small like 1.  Then 
after I've got a solution for that, I increase P to 2, then 10, then 
100, etc., until the penalty is so great that the desired equality has 
been effectively achieved.

	  With 'P' fixed, 'optim' should handle this kind of problem handily. 
To learn how, I suggest you work through the examples in the ?optim help 
page.  I'd ignore the gradient, at least initially.  A silly math error 
in computing the gradient can delay a solutions unnecessarily.  If you 
need to solve thousands of problems like this for  different values of 
k1 and 'c', I might later program the gradient.  However, I would not do 
that initially.

	  Also, if you are not already familiar with Venables and Ripley (2002) 
Modern Applied Statistics with S, 4th ed. (Springer -- or an earlier 
edition), I would encourage you to spend some quality time with this 
book.  It can help you with 'optim', with contour plots, etc.

	  Hope this helps,
	  Spencer Graves

Iris Zhao wrote:
> Dear all,
> 
> 
> 
> I am working on optimization problem and have some trouble running optim().
> I have two functions (f1, f2) and 4 unknown parameters (p1, p2, p3, p4).
> Both f1 and f2 are functions of p1, p2, and p3, denoted by f1(p1, p2, p3)
> and f2(p1,p2,p3) respectively.
> 
> 
> 
> The goal is to maximize f1(p1, p2, p3) subject to two constraints:
> 
> (1)  c = k1*p4/(k1*p4+(1-k1)*f1(p1,p2,p3)), where c and k1 are some known
> constants
> 
> (2)  p4 = f2(p1, p2, p3)
> 
> In addition, each parameter ranges from 0 to 1, and both f1 and f2 involve
> integrations.
> 
> 
> 
> I tried to use lagrange multipliers to eliminate two equality constraints
> and then use optim() to find the maximum value and optimal parameter
> estimates.
> 
> So I let fn be f1+lambda1*(c- k1*p4/(k1*p4+(1-k1)*f1(p1,p2,p3))) +
> lambda2(p4-f2(p1,p2,p3)). The error message I got was "Error in fn(par, ...)
> : recursive default argument reference."
> 
> 
> 
> I wonder whether current build-in functions in R can do this type of jobs.
> Any suggestion will be greatly appreciated.
> 
> 
> 
> Iris
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From dieter.menne at menne-biomed.de  Tue Jul 18 09:09:36 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 18 Jul 2006 07:09:36 +0000 (UTC)
Subject: [R] use "factor" for categorical covariate in Cox PH model
References: <90B06673D826C64E8ED8EEA6B6FDF8CAE72C28@crcmail1.BCCRC.CA>
Message-ID: <loom.20060718T090653-884@post.gmane.org>

Linda Lei <llei <at> bccrc.ca> writes:

> 
> When dealing with the categorical covariates (for example 3 groups), it
> will come out different results if we add the command "factor" in front
> of the categorical covariate or not 

The catch is here: the covariate is not a "categorial", but a number. In that
case a regression is computed, and the ONE ration printed out has the meaning of
a slope.

> if we don't add "factor", there is
> only one hazard ratio; if we add "factor", there are two hazard ratios.
> 
> So does the "factor" actually create some dummy variables for the
> calculation?

Factor make the covariate a categorial one, and two contrasts are printed out,
relative to the first level of the covariate.

Dieter


From sharonsnowdon at fastmail.fm  Mon Jul 17 22:35:38 2006
From: sharonsnowdon at fastmail.fm (sharon snowdon)
Date: Tue, 18 Jul 2006 06:35:38 +1000
Subject: [R] Output and Word
Message-ID: <000601c6a9e0$96822d80$0401010a@SHARONC>

Hi

I have just started to have a look at R. I have used most stats software
packages and can use perl, visual basic etc. I am interested in how well
it handles lots of output e.g. tables or charts. How would you get lots
of output most easily and quickly into a Word document?

Sharon Snowdon



------------------------------------------------------------------------
-
FIGHT BACK AGAINST SPAM!
Download Spam Inspector, the Award Winning Anti-Spam Filter
http://mail.giantcompany.com



-- 



7/14/2006


From Saghir.Bashir at UCB-Group.com  Tue Jul 18 10:15:02 2006
From: Saghir.Bashir at UCB-Group.com (Bashir Saghir (Aztek Global))
Date: Tue, 18 Jul 2006 10:15:02 +0200
Subject: [R] String manipulation and formatting
Message-ID: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A85@ntbraexc104.dir.ucb-group.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060718/4f3d22f7/attachment.pl 

From bibiko at eva.mpg.de  Tue Jul 18 10:18:17 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Tue, 18 Jul 2006 10:18:17 +0200
Subject: [R] String manipulation and formatting
In-Reply-To: <971536df0607170822k7a940e8doc9ca5f0c86daa2c6@mail.gmail.com>
References: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A7D@ntbraexc104.dir.ucb-group.com>
	<971536df0607170806v5cd689cx717728e6c5945ce2@mail.gmail.com>
	<971536df0607170822k7a940e8doc9ca5f0c86daa2c6@mail.gmail.com>
Message-ID: <3EBB8761-8BED-4E21-A04E-C8E1EF4878CE@eva.mpg.de>


Hi,

an other way without any libraries and written as a one-line-command  
would be something like this:


xify <- function(x)
{
	gsub("[0-9]","X",
	sprintf(
		paste(
			"%",
			ifelse(
				format(x,digit=1)==as.character(x),
				paste(as.character(x),".0",sep=""),
				as.character(x)
			),
			"f",sep=""
		)
		,10^(trunc(x)-10*x+10*trunc(x)-1)
	)
	)
}


The only disadvantage is that something like xify(4.8) produces [1]  
"X.XXXXXXXX".

Hans-Joerg


>> On 7/17/06, Bashir Saghir (Aztek Global) <Saghir.Bashir at ucb- 
>> group.com> wrote:
>>> I'm trying to write a simple function that does the following:
>>>
>>>  [command] xify(5.2)
>>>  [output] XXX.XX
>>>
>>>  [command] xify(3)
>>>  [output] XXX
>>>
>>> Any simple solutions (without using python/perl/unix script/...)?
>>>
>>> Thanks,
>>> Saghir
>>>
>>>
>>> ---------------------------------------------------------
>>> Legal Notice: This electronic mail and its attachments are i... 
>>> {{dropped}}
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! http://www.R-project.org/ 
>>> posting-guide.html
>>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>
>


From ripley at stats.ox.ac.uk  Tue Jul 18 10:24:12 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 18 Jul 2006 09:24:12 +0100 (BST)
Subject: [R] String manipulation and formatting
In-Reply-To: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A85@ntbraexc104.dir.ucb-group.com>
References: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A85@ntbraexc104.dir.ucb-group.com>
Message-ID: <Pine.LNX.4.64.0607180920500.13079@gannet.stats.ox.ac.uk>

On Tue, 18 Jul 2006, Bashir Saghir (Aztek Global) wrote:

> Thanks to Richard, Gabor and Marc for some nice solutions to my request.
> 
> I have a new problem:
> 
>   > xify(30.10)
>   [1] "XXXXXXXXXXXXXXXXXXXXXXXXXXXXX.X"
>   > xify(30.11)
>   [1] "XXXXXXXXXXXXXXXXXXX.XXXXXXXXXXX"
> 
> The problem originates from:
> 
>   > as.numeric(unlist(strsplit(as.character(15.10), "\\.")))
>   [1] 15  1
>   > as.numeric(unlist(strsplit(as.character(15.11), "\\.")))
>   [1] 15 11
> 
> It seems to boils down to:
> 
>   > as.character(15.10)
>   [1] "15.1"
> 
> A simple solution is:
> 
>   > xify("15.10")
>   [1] "XXXXX.XXXXXXXXXX"
> 
> I was wondering if there is a simple way for xify to see the zero of the 10
> without having to force the user to add quotes around the format?

No, as the parser has converted '15.10' to a numeric constant.

What is the problem with asking users to enter strings as strings?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From wiedenhoeft at gmx.net  Tue Jul 18 10:25:49 2006
From: wiedenhoeft at gmx.net (John Wiedenhoeft)
Date: Tue, 18 Jul 2006 10:25:49 +0200
Subject: [R] Nested functions
In-Reply-To: <644e1f320607171642x1195b8e7y169e9d739f9d05ee@mail.gmail.com>
References: <1153168475.5952.16.camel@localhost>
	<644e1f320607171436l1e25c0deq7fb070c07ac1ad8f@mail.gmail.com>
	<1153175380.3693.19.camel@localhost>
	<644e1f320607171642x1195b8e7y169e9d739f9d05ee@mail.gmail.com>
Message-ID: <1153211149.5846.33.camel@localhost>

Am Montag, den 17.07.2006, 19:42 -0400 schrieb jim holtman:
> You were down at least 5300 levels in subroutine calls.  Here is the
> first couple of lines from 'traceback()':
>  
> 5363: vavb(a, b, n, v, x)
...
> 5316: vavb(a, b, n, v, x) 


> The other thing is that there is no 'x' defined in the function;
>  
> antiphonar <- function(v)
> {
>        a <- 1;
>        b <- 2;
>        n <- length(v);
>        alessn(a, b, n, v, x);
> }
>  
> and this is the initial call to the rest of the functions.

Ooops... normally I don't program THAT bad ... ;-)

> What is the problem you are trying to solve? 

I'm analyzing a 16th century music manuscript. There is no rhythm
notated there, but I can to a certain degree restore it by finding
self-similarities in the melodic structure. The vector v is an ordered
list of intervals. I could find them by hand, but this takes time and
things are easily missed. Besides, for a 200 pages manuscript coding the
vectors takes long, but finding similarities by hand takes
endless... :-(

The problem remains. I'm not sure I did completely understand the way R
works. My plan was to jump out of one function to another. I think what
R does is assigning a function's return value to an internal variable
and tries to evaluate another function WITHIN a calling function, which
of course leads to deep nesting and a bunch of internal variables. Too
bad there is no "goto" statement in R... Isn't there a way to make R
"forget" that it just evaluated e.g. alessn the moment it calles vavb?
That algorithm itself isn't that hard. In the end it's just a decision
diagram of the form "if THIS, goto THIS knot, otherwise goto THAT knot".
Each knot has only one arrow pointing away from it, but sometimes more
than one pointing toward it. I could send the diagram as an attachement, if you like.

Cheers,
John

P.S.: Please keep messages on the list. Thank you!


From seanpor at acm.org  Tue Jul 18 10:50:09 2006
From: seanpor at acm.org (Sean O'Riordain)
Date: Tue, 18 Jul 2006 09:50:09 +0100
Subject: [R] String manipulation and formatting
In-Reply-To: <Pine.LNX.4.64.0607180920500.13079@gannet.stats.ox.ac.uk>
References: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A85@ntbraexc104.dir.ucb-group.com>
	<Pine.LNX.4.64.0607180920500.13079@gannet.stats.ox.ac.uk>
Message-ID: <8ed68eed0607180150p10ed8cf9r47b126fd7a07af74@mail.gmail.com>

Does it have to be a stop char ".", or could it be a separate
parameter, i.e. put in a comma, then the 10 becomes an integer...

s/


On 18/07/06, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Tue, 18 Jul 2006, Bashir Saghir (Aztek Global) wrote:
>
> > Thanks to Richard, Gabor and Marc for some nice solutions to my request.
> >
> > I have a new problem:
> >
> >   > xify(30.10)
> >   [1] "XXXXXXXXXXXXXXXXXXXXXXXXXXXXX.X"
> >   > xify(30.11)
> >   [1] "XXXXXXXXXXXXXXXXXXX.XXXXXXXXXXX"
> >
> > The problem originates from:
> >
> >   > as.numeric(unlist(strsplit(as.character(15.10), "\\.")))
> >   [1] 15  1
> >   > as.numeric(unlist(strsplit(as.character(15.11), "\\.")))
> >   [1] 15 11
> >
> > It seems to boils down to:
> >
> >   > as.character(15.10)
> >   [1] "15.1"
> >
> > A simple solution is:
> >
> >   > xify("15.10")
> >   [1] "XXXXX.XXXXXXXXXX"
> >
> > I was wondering if there is a simple way for xify to see the zero of the 10
> > without having to force the user to add quotes around the format?
>
> No, as the parser has converted '15.10' to a numeric constant.
>
> What is the problem with asking users to enter strings as strings?
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Saghir.Bashir at UCB-Group.com  Tue Jul 18 10:59:05 2006
From: Saghir.Bashir at UCB-Group.com (Bashir Saghir (Aztek Global))
Date: Tue, 18 Jul 2006 10:59:05 +0200
Subject: [R] String manipulation and formatting
Message-ID: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A87@ntbraexc104.dir.ucb-group.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060718/e709d947/attachment.pl 

From scruveil at genoscope.cns.fr  Tue Jul 18 10:50:03 2006
From: scruveil at genoscope.cns.fr (=?ISO-8859-1?Q?St=E9phane_Cruveiller?=)
Date: Tue, 18 Jul 2006 10:50:03 +0200
Subject: [R] Object name and Strings?
Message-ID: <44BCA0BB.6090009@genoscope.cns.fr>

Hi all,

Is there a simple way to convert an object name to a characters string?


St?phane.


From Saghir.Bashir at UCB-Group.com  Tue Jul 18 11:04:16 2006
From: Saghir.Bashir at UCB-Group.com (Bashir Saghir (Aztek Global))
Date: Tue, 18 Jul 2006 11:04:16 +0200
Subject: [R] String manipulation and formatting
Message-ID: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A88@ntbraexc104.dir.ucb-group.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060718/963f31f6/attachment.pl 

From jacques.veslot at good.ibl.fr  Tue Jul 18 10:56:20 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Tue, 18 Jul 2006 10:56:20 +0200
Subject: [R] Object name and Strings?
In-Reply-To: <44BCA0BB.6090009@genoscope.cns.fr>
References: <44BCA0BB.6090009@genoscope.cns.fr>
Message-ID: <44BCA234.90906@good.ibl.fr>

 > deparse(substitute(a))
[1] "a"

-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


St?phane Cruveiller a ?crit :
> Hi all,
> 
> Is there a simple way to convert an object name to a characters string?
> 
> 
> St?phane.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Tue Jul 18 11:00:59 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 18 Jul 2006 10:00:59 +0100 (BST)
Subject: [R] Object name and Strings?
In-Reply-To: <44BCA0BB.6090009@genoscope.cns.fr>
References: <44BCA0BB.6090009@genoscope.cns.fr>
Message-ID: <Pine.LNX.4.64.0607180957520.13881@gannet.stats.ox.ac.uk>

On Tue, 18 Jul 2006, St?phane Cruveiller wrote:

> Hi all,
> 
> Is there a simple way to convert an object name to a characters string?

Yes, as.character, as in

> x <- as.name("foo")
> x
foo
> as.character(x)
[1] "foo"

However, I suspect you are not using the words in their technical sense
(a name is a synonym for a symbol in R), so if this is not the answer, 
please give us an example of what you are trying to do (which might be 
deparse).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From dmedri at gmail.com  Tue Jul 18 11:12:51 2006
From: dmedri at gmail.com (Daniele Medri)
Date: Tue, 18 Jul 2006 11:12:51 +0200
Subject: [R] Large datasets in R
In-Reply-To: <1153162855.15457.8.camel@localhost.localdomain>
References: <1153162855.15457.8.camel@localhost.localdomain>
Message-ID: <1153213972.13455.2.camel@localhost.localdomain>

Il giorno lun, 17/07/2006 alle 15.00 -0400, Deepankar Basu ha scritto:
> I have been trying to read up the posting on the R-archive on this
> topic; but I could not really understand all the discussion, nor could I
> reach the "end". So, I am not aware of the current state of consensus on
> the issue. 

a general hint is to store this dataset in a database and manage your
data with RODBC or other db-related packages.

cheers
-- 
Daniele Medri


From scruveil at genoscope.cns.fr  Tue Jul 18 11:12:43 2006
From: scruveil at genoscope.cns.fr (=?ISO-8859-1?Q?St=E9phane_Cruveiller?=)
Date: Tue, 18 Jul 2006 11:12:43 +0200
Subject: [R] Object name and Strings?
In-Reply-To: <Pine.LNX.4.64.0607180957520.13881@gannet.stats.ox.ac.uk>
References: <44BCA0BB.6090009@genoscope.cns.fr>
	<Pine.LNX.4.64.0607180957520.13881@gannet.stats.ox.ac.uk>
Message-ID: <44BCA60B.7000107@genoscope.cns.fr>

Thanks for your answer.

Here is what I am trying to do.
I have a list which is called MyList. I would like to get only the "name"
of this object as a simple characters string. i.e. Is there a function 
in R which is able
to give:

 > name<-fun(Mylist)
 > name
"MyList"


thanks in advance,

St?phane.



Prof Brian Ripley a ?crit :
> On Tue, 18 Jul 2006, St?phane Cruveiller wrote:
>
>   
>> Hi all,
>>
>> Is there a simple way to convert an object name to a characters string?
>>     
>
> Yes, as.character, as in
>
>   
>> x <- as.name("foo")
>> x
>>     
> foo
>   
>> as.character(x)
>>     
> [1] "foo"
>
> However, I suspect you are not using the words in their technical sense
> (a name is a synonym for a symbol in R), so if this is not the answer, 
> please give us an example of what you are trying to do (which might be 
> deparse).
>
>


From J.Demmler at swansea.ac.uk  Tue Jul 18 11:14:58 2006
From: J.Demmler at swansea.ac.uk (Demmler J.)
Date: Tue, 18 Jul 2006 10:14:58 +0100
Subject: [R] package installation problems
Message-ID: <C58FC3574012FF48A6A6F52DFABC2B4D0276FC2E@lsntex3.clyne.swan.ac.uk>

Hello,

I just updated from R 2.1.1 to R 2.3.1 (I also updated from Fedora 3 to
Fedora 4 if that is of any importance). However, several packages (e.g.
fields etc.) refuse to be installed. I get the following error message:

[root at localhost R-files]# R CMD INSTALL fields_2.3.tar.gz
* Installing *source* package 'fields' ...
** libs
g77   -fpic  -O2 -g -pipe -m32 -march=i386 -mtune=pentium4 -c css.f -o css.o
f771: invalid option `tune=pentium4'
make: *** [css.o] Error 1
ERROR: compilation failed for package 'fields'
** Removing '/usr/lib/R/library/fields'

For some reason -mtune is set to Pentium 4, although I'm using an Athlon-XP.
I searched the files in the fields package for this line but couldn't find
it. Other packages, like f.i. maps didn't have any problems.

I hope anyone can help me, as I need those packages urgently for my PhD
project.

Joanne


From ripley at stats.ox.ac.uk  Tue Jul 18 11:26:22 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 18 Jul 2006 10:26:22 +0100 (BST)
Subject: [R] Object name and Strings?
In-Reply-To: <44BCA60B.7000107@genoscope.cns.fr>
References: <44BCA0BB.6090009@genoscope.cns.fr>
	<Pine.LNX.4.64.0607180957520.13881@gannet.stats.ox.ac.uk>
	<44BCA60B.7000107@genoscope.cns.fr>
Message-ID: <Pine.LNX.4.64.0607181024160.10951@gannet.stats.ox.ac.uk>

On Tue, 18 Jul 2006, St?phane Cruveiller wrote:

> Thanks for your answer.
> 
> Here is what I am trying to do.
> I have a list which is called MyList. I would like to get only the "name"
> of this object as a simple characters string. i.e. Is there a function in R
> which is able
> to give:
> 
> > name<-fun(Mylist)
> > name
> "MyList"

myfun <- function(x) deparse(substitute(x))

That's how to find the name (if it was a name) given as a function 
argument.

> 
> 
> thanks in advance,
> 
> St?phane.
> 
> 
> 
> Prof Brian Ripley a ?crit :
> > On Tue, 18 Jul 2006, St?phane Cruveiller wrote:
> >
> >   
> > > Hi all,
> > >
> > > Is there a simple way to convert an object name to a characters string?
> > >     
> >
> > Yes, as.character, as in
> >
> >   
> > > x <- as.name("foo")
> > > x
> > >     
> > foo
> >   
> > > as.character(x)
> > >     
> > [1] "foo"
> >
> > However, I suspect you are not using the words in their technical sense
> > (a name is a synonym for a symbol in R), so if this is not the answer,
> > please give us an example of what you are trying to do (which might be
> > deparse).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From scruveil at genoscope.cns.fr  Tue Jul 18 11:34:20 2006
From: scruveil at genoscope.cns.fr (=?ISO-8859-1?Q?St=E9phane_Cruveiller?=)
Date: Tue, 18 Jul 2006 11:34:20 +0200
Subject: [R] Object name and Strings?
In-Reply-To: <Pine.LNX.4.64.0607181024160.10951@gannet.stats.ox.ac.uk>
References: <44BCA0BB.6090009@genoscope.cns.fr>
	<Pine.LNX.4.64.0607180957520.13881@gannet.stats.ox.ac.uk>
	<44BCA60B.7000107@genoscope.cns.fr>
	<Pine.LNX.4.64.0607181024160.10951@gannet.stats.ox.ac.uk>
Message-ID: <44BCAB1C.2080201@genoscope.cns.fr>

That is exactly what I wanted to do...

Thanks for the hint...

St?phane.

Prof Brian Ripley a ?crit :
> On Tue, 18 Jul 2006, St?phane Cruveiller wrote:
>
>   
>> Thanks for your answer.
>>
>> Here is what I am trying to do.
>> I have a list which is called MyList. I would like to get only the "name"
>> of this object as a simple characters string. i.e. Is there a function in R
>> which is able
>> to give:
>>
>>     
>>> name<-fun(Mylist)
>>> name
>>>       
>> "MyList"
>>     
>
> myfun <- function(x) deparse(substitute(x))
>
> That's how to find the name (if it was a name) given as a function 
> argument.
>
>   
>> thanks in advance,
>>
>> St?phane.
>>
>>
>>
>> Prof Brian Ripley a ?crit :
>>     
>>> On Tue, 18 Jul 2006, St?phane Cruveiller wrote:
>>>
>>>   
>>>       
>>>> Hi all,
>>>>
>>>> Is there a simple way to convert an object name to a characters string?
>>>>     
>>>>         
>>> Yes, as.character, as in
>>>
>>>   
>>>       
>>>> x <- as.name("foo")
>>>> x
>>>>     
>>>>         
>>> foo
>>>   
>>>       
>>>> as.character(x)
>>>>     
>>>>         
>>> [1] "foo"
>>>
>>> However, I suspect you are not using the words in their technical sense
>>> (a name is a synonym for a symbol in R), so if this is not the answer,
>>> please give us an example of what you are trying to do (which might be
>>> deparse).
>>>       
>
>


From dirk.debecker at biw.kuleuven.be  Tue Jul 18 11:42:20 2006
From: dirk.debecker at biw.kuleuven.be (Dirk De Becker)
Date: Tue, 18 Jul 2006 11:42:20 +0200
Subject: [R] Spectral data analysis - parameter selection
Message-ID: <44BCACFC.6080906@biw.kuleuven.be>

Hello all,

I am doing some spectral data analysis using R, and I already got some 
help on how to do savitsky-golay preprocessing. Now I would like to 
perform parameter selection (i.e. I would like to assess which spectral 
lines are relevant to keep in the model, and which are not).
There exist algorithms like the jacknife algorithm, but I was wondering 
whether anyone has already implemented something like that, and if not, 
if anyone could give me a few hints on how to start implementing it myself.

Thanks in advance,

Dirk

-- 
Dirk De Becker
Work: Kasteelpark Arenberg 30
      3001 Heverlee
      phone: ++32(0)16/32.14.44
      fax: ++32(0)16/32.85.90
Home: Waversebaan 90
      3001 Heverlee
      phone: ++32(0)16/23.36.65
dirk.debecker at biw.kuleuven.be
mobile phone: ++32(0)498/51.19.86


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ripley at stats.ox.ac.uk  Tue Jul 18 11:43:21 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 18 Jul 2006 10:43:21 +0100 (BST)
Subject: [R] package installation problems
In-Reply-To: <C58FC3574012FF48A6A6F52DFABC2B4D0276FC2E@lsntex3.clyne.swan.ac.uk>
References: <C58FC3574012FF48A6A6F52DFABC2B4D0276FC2E@lsntex3.clyne.swan.ac.uk>
Message-ID: <Pine.LNX.4.64.0607181034280.10951@gannet.stats.ox.ac.uk>

On Tue, 18 Jul 2006, Demmler J. wrote:

> Hello,
> 
> I just updated from R 2.1.1 to R 2.3.1 (I also updated from Fedora 3 to

How?  Did you install an RPM?  Before or after OS update?
I checked the RPM for fc4 on CRAN which has

  C compiler:                gcc  -O2 -g -pipe -Wp,-D_FORTIFY_SOURCE=2 
-fexceptions -m32 -march=i386 -mtune=pentium4 -fasynchronous-unwind-tables
  Fortran 77 compiler:       gfortran  -O2 -g -pipe 
-Wp,-D_FORTIFY_SOURCE=2 -fexceptions -m32 -march=i386 -mtune=pentium4 
-fasynchronous-unwind-tables

  C++ compiler:              g++  -O2 -g -pipe -Wp,-D_FORTIFY_SOURCE=2 
-fexceptions -m32 -march=i386 -mtune=pentium4 -fasynchronous-unwind-tables
  Fortran 90/95 compiler:    gfortran -g -O2

not as you show.  However, read on.

> Fedora 4 if that is of any importance). However, several packages (e.g.
> fields etc.) refuse to be installed. I get the following error message:
> 
> [root at localhost R-files]# R CMD INSTALL fields_2.3.tar.gz
> * Installing *source* package 'fields' ...
> ** libs
> g77   -fpic  -O2 -g -pipe -m32 -march=i386 -mtune=pentium4 -c css.f -o css.o
> f771: invalid option `tune=pentium4'
> make: *** [css.o] Error 1
> ERROR: compilation failed for package 'fields'
> ** Removing '/usr/lib/R/library/fields'
> 
> For some reason -mtune is set to Pentium 4, although I'm using an Athlon-XP.

That may not actually be a problem: the problem is that your compiler is 
not recognizing -mtune.   (I seem to have run R RPMs for fc3 with 
-march=i386 -mtune=pentium4 on a dual Athlon MP.  'tune' should generate 
code that runs on other CPUs, just not optimally.)

> I searched the files in the fields package for this line but couldn't find
> it. Other packages, like f.i. maps didn't have any problems.
> 
> I hope anyone can help me, as I need those packages urgently for my PhD
> project.

The options are coming from the version of R you installed.  I would 
expect R compiled for FC4 to be using gfortran not g77 (as in the CRAN 
RPM).  The simplest way out here is compile R from the sources on your 
actual machine, but you could try replacing your RPM with the CRAN fc4 
one.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bibiko at eva.mpg.de  Tue Jul 18 11:47:28 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Tue, 18 Jul 2006 11:47:28 +0200
Subject: [R] String manipulation and formatting
In-Reply-To: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A88@ntbraexc104.dir.ucb-group.com>
References: <4BB0B5AF849F8B47BCDD4B6B1A94519F033E1A88@ntbraexc104.dir.ucb-group.com>
Message-ID: <DB57ACFF-944E-4973-BC04-ACE7F2478C91@eva.mpg.de>

Hi,

only if you allow to input x as a string then you can use, maybe,  
simply the following one-line-command:

xify <- function(x)
{
	gsub(
		"[0-9]",
		"X",
		sprintf(
			paste("%",ifelse(regexpr("\\.",x) > 0, x, x<-paste(x,". 
0",sep="")), "f",sep=""),
			10^(trunc(as.numeric(x))-as.integer(gsub("(.*?)\\.(.*?)","\\2",x))-1)
		)
	)
}

 > xify("30.10")
  [1] "XXXXXXXXXXXXXXXXXXXX.XXXXXXXXXX"

Hans-Joerg


>>> Thanks to Richard, Gabor and Marc for some nice solutions to my  
>>> request.
>>>
>>> I have a new problem:
>>>
>>>> xify(30.10)
>>>   [1] "XXXXXXXXXXXXXXXXXXXXXXXXXXXXX.X"
>>>> xify(30.11)
>>>   [1] "XXXXXXXXXXXXXXXXXXX.XXXXXXXXXXX"
>>>
>>> The problem originates from:
>>>
>>>> as.numeric(unlist(strsplit(as.character(15.10), "\\.")))
>>>   [1] 15  1
>>>> as.numeric(unlist(strsplit(as.character(15.11), "\\.")))
>>>   [1] 15 11
>>>
>>> It seems to boils down to:
>>>
>>>> as.character(15.10)
>>>   [1] "15.1"
>>>
>>> A simple solution is:
>>>
>>>> xify("15.10")
>>>   [1] "XXXXX.XXXXXXXXXX"
>>>
>>> I was wondering if there is a simple way for xify to see the zero  
>>> of the
> 10
>>> without having to force the user to add quotes around the format?
>>
>> No, as the parser has converted '15.10' to a numeric constant.
>>
>> What is the problem with asking users to enter strings as strings?
>>


From mail at friedrich-schuster.de  Tue Jul 18 11:49:53 2006
From: mail at friedrich-schuster.de (mail at friedrich-schuster.de)
Date: Tue, 18 Jul 2006 11:49:53 +0200
Subject: [R] Output and Word
Message-ID: <10233914.515771153216191718.JavaMail.servlet@kundenserver>

Hi, 

you might try to use the R2HTML package and then import the html files into word. please see. http://cran.r-project.org/doc/Rnews/Rnews_2003-3.pdf

Another way: use sweave for tex/latex output and transform it to rtf.

Friedrich Schuster

--- 
Your post was: 
sharon snowdon sharonsnowdon at fastmail.fm 
Hi

I have just started to have a look at R. I have used most stats software
packages and can use perl, visual basic etc. I am interested in how well
it handles lots of output e.g. tables or charts. How would you get lots
of output most easily and quickly into a Word document?

Sharon Snowdon


From wiedenhoeft at gmx.net  Tue Jul 18 11:56:15 2006
From: wiedenhoeft at gmx.net (John Wiedenhoeft)
Date: Tue, 18 Jul 2006 11:56:15 +0200
Subject: [R] Nested functions
In-Reply-To: <44BD7242.1050106@bitwrit.com.au>
References: <1153168475.5952.16.camel@localhost>
	<44BD7242.1050106@bitwrit.com.au>
Message-ID: <1153216575.5846.41.camel@localhost>

> I was checking out your function and thinking of trying to make it more 
> efficient when I noticed your comment above. 

:-D

> Yes, bioinformatics is full 
> of this stuff - have a look at Bioconductor.

I'm about it, but it'll take some time. It's a vast repository...

For the meantime, I've attached a graph for clarification ;-)
Dashed lines are followed if the test returns FALSE, solid if TRUE.

Cheers,
John
-------------- next part --------------
A non-text attachment was scrubbed...
Name: diagramm.ps
Type: application/postscript
Size: 13108 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060718/d619ac3d/attachment.ps 

From roderick.castillo at metanomics.de  Tue Jul 18 13:01:48 2006
From: roderick.castillo at metanomics.de (roderick.castillo at metanomics.de)
Date: Tue, 18 Jul 2006 13:01:48 +0200
Subject: [R] Running R as root
Message-ID: <OF301D160D.B5235C5D-ONC12571AF.003A75BE-C12571AF.003C9A16@basf-c-s.be>

Hello
Using R v. 2.3.1 which I installed recently, when I start it as root, there
is nothing
I can do. I get "Error: function xx not found" even when trying to leave R
using
q(). R works fine when started by an ordinary user. I rechecked the root
environment
several times and even have set it similar to that of an ordinary user
(values of
PATH etc.) but nothing helps. Using the command "env" did not reveal
anything
valuable. Going to root through the identity of an ordinary user and
retaining his
environment by doing "su user" instead of su - user" does not help either.

This does not seem related to the particular version of R.
Using: Linux RedHat AS 2.1

Any hint?

Bye

Rick


From p.dalgaard at biostat.ku.dk  Tue Jul 18 13:36:37 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Jul 2006 13:36:37 +0200
Subject: [R] Running R as root
In-Reply-To: <OF301D160D.B5235C5D-ONC12571AF.003A75BE-C12571AF.003C9A16@basf-c-s.be>
References: <OF301D160D.B5235C5D-ONC12571AF.003A75BE-C12571AF.003C9A16@basf-c-s.be>
Message-ID: <x24pxfb0vu.fsf@turmalin.kubism.ku.dk>

roderick.castillo at metanomics.de writes:

> Hello
> Using R v. 2.3.1 which I installed recently, when I start it as root, there
> is nothing
> I can do. I get "Error: function xx not found" even when trying to leave R
> using
> q(). R works fine when started by an ordinary user. I rechecked the root
> environment
> several times and even have set it similar to that of an ordinary user
> (values of
> PATH etc.) but nothing helps. Using the command "env" did not reveal
> anything
> valuable. Going to root through the identity of an ordinary user and
> retaining his
> environment by doing "su user" instead of su - user" does not help either.
> 
> This does not seem related to the particular version of R.
> Using: Linux RedHat AS 2.1
> 
> Any hint?

File permissions? (it is possible to set them so that files can be
read by anyone except the user...)

How exactly did you install R, and are there any other clues upon
startup? 

I get, BTW

> aa()
Error: could not find function "aa"

so I suspect you paraphrased the error message.

One other thing that can cause trouble is networked directories. Files
that are physically on another machine can sometimes not be accessed
by root on the local machine.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From irubio at miranda.ecologia.unam.mx  Tue Jul 18 14:10:29 2006
From: irubio at miranda.ecologia.unam.mx (Ivan Rubio Perez)
Date: Tue, 18 Jul 2006 07:10:29 -0500
Subject: [R] Package for autocorrelation analysis?
In-Reply-To: <x24pxfb0vu.fsf@turmalin.kubism.ku.dk>
References: <OF301D160D.B5235C5D-ONC12571AF.003A75BE-C12571AF.003C9A16@basf-c-s.be>
	<x24pxfb0vu.fsf@turmalin.kubism.ku.dk>
Message-ID: <20060718120901.M26528@www.ecologia.unam.mx>

Hi!
Dear All,

I want to implement an autocorrelation analysis and estimated the Moran's I in a set of
ecological traits. I seek for an R package that do this analysis, however, I couldn't
found none that implement it. May be I'm lost in the universe of the Contributed
Packages but I couldn't found information into the index of some packages that I suppose
do it. Would you give me some recommendations to do an autocorrelation analysis?

Thanks!

Ivan

Ivan V. Rubio P?rez
irubio at ecologia.unam.mx
Instituto de Ecolog?a, UNAM
www.ecologia.unam.mx
--
Open WebMail Project (http://openwebmail.org)


From roderick.castillo at metanomics.de  Tue Jul 18 14:27:02 2006
From: roderick.castillo at metanomics.de (roderick.castillo at metanomics.de)
Date: Tue, 18 Jul 2006 14:27:02 +0200
Subject: [R] Running R as root
Message-ID: <OFD2FE7376.F78D71B6-ONC12571AF.00430ABB-C12571AF.004467AB@basf-c-s.be>

Problem solved: I had a .Renviron file in root's Home directory pointing to
the
wrong R installation...

Thanks anyway
Rick


From ggrothendieck at gmail.com  Tue Jul 18 14:18:36 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 18 Jul 2006 08:18:36 -0400
Subject: [R] Output and Word
In-Reply-To: <000601c6a9e0$96822d80$0401010a@SHARONC>
References: <000601c6a9e0$96822d80$0401010a@SHARONC>
Message-ID: <971536df0607180518r67a24874m5cba337ee2ad0759@mail.gmail.com>

If its text that you want included in a larger Word document
you can create a text or HTML file from R and then when inserting it
into Word insert it as a link rather than copying it in.

output a file from R
In Word
   Insert | File
   browse to the file so its name appears in the File name box
   click on the down arrow to the right of the Insert button
     and choose Insert as Link

If you regenerate the file in the future your Word document gets
automatically uipdated.

Another possibility is either the rcom and RDCOMClient packages
which allow one to control Word from R by treating Word as a COM object.
This allows detailed control of Word but is more work.  e.g. Here
we create a Word document from scratch.  Note that you must
be running Windows and have Word on the machine where you do
this:

library(RDCOMClient)
ow <- COMCreate("Word.Application")
ow[["Visible"]] <- TRUE  # optional
od <- ow[["Documents"]]$Add()

od[["PageSetup"]][[LeftMargin"]] <- 20
od[["PageSetup"]][["TopMargin"]] <- 20

od[["Content"]][["Font"]][["Size"]] <- 11
od[["Content"]][["Text"]] <- "Hello World"

od$SaveAs("\\mydoc.doc")
ow$Quit()


On 7/17/06, sharon snowdon <sharonsnowdon at fastmail.fm> wrote:
> Hi
>
> I have just started to have a look at R. I have used most stats software
> packages and can use perl, visual basic etc. I am interested in how well
> it handles lots of output e.g. tables or charts. How would you get lots
> of output most easily and quickly into a Word document?
>
> Sharon Snowdon
>
>
>
> ------------------------------------------------------------------------
> -
> FIGHT BACK AGAINST SPAM!
> Download Spam Inspector, the Award Winning Anti-Spam Filter
> http://mail.giantcompany.com
>
>
>
> --
>
>
>
> 7/14/2006
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From z.dalton at lancaster.ac.uk  Tue Jul 18 14:37:06 2006
From: z.dalton at lancaster.ac.uk (z.dalton at lancaster.ac.uk)
Date: Tue, 18 Jul 2006 13:37:06 +0100 (BST)
Subject: [R] Surv analysis with multiple internal time-dep covariates
 measured over different time intervals
Message-ID: <E1G2op0-0002H7-00@wing1.lancs.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060718/1ee3b75e/attachment.pl 

From bibiko at eva.mpg.de  Tue Jul 18 14:39:35 2006
From: bibiko at eva.mpg.de (Hans-Joerg Bibiko)
Date: Tue, 18 Jul 2006 14:39:35 +0200
Subject: [R] engineering notation format
Message-ID: <8476F29A-6817-4634-8CFA-B35178CD29DD@eva.mpg.de>

Hi,

One week ago Sam Walker asked for a way to format a number to the  
engineering notation?

After my suggestion I received many mails how to use this function  
formatEng within plots dealing, e.g., with physics and SI prefixes.

So, I wrote a humble wiki article about that.

If you are interested have a look at

http://wiki.r-project.org/rwiki/doku.php?id=tips:data- 
strings:formatengineering

Comments are welcome!


Cheers,

Hans-Joerg


From neuro3000 at hotmail.com  Tue Jul 18 14:48:18 2006
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Tue, 18 Jul 2006 08:48:18 -0400
Subject: [R] Output and Word
In-Reply-To: <000601c6a9e0$96822d80$0401010a@SHARONC>
Message-ID: <BAY112-F2955A318B455B33AC78113AF630@phx.gbl>

R can handle as many graphs/table as you ask it to.

I don't know what you're trying to do, but I generate lots of graphs using 
loops and generating filenames on the spot.

See FAQ 7.34 to see how it's done.
http://cran.r-project.org/doc/FAQ/R-FAQ.html#How-can-I-save-the-result-of-each-iteration-in-a-loop-into-a-separate-file_003f

For graphs, save the graphs as something windows can recognize (jpeg, png, 
emf, wmf...) and in Word, you can easily import all the graphs in a 
directory by doing Insert/Picture/From File and shift-click all the graphs 
you want.  Word will import them all.

For tables, it really depends on what you want because usually there is 
formatting involved.  Personnaly, if I had to produce several tables to be 
imported in Word, I would use the R2HTML library in a loop (as mentionned 
above) and import the multiple html files using Insert/File (and 
shift-click) in Word.

Neuro

>From: "sharon snowdon" <sharonsnowdon at fastmail.fm>
>Reply-To: sharonsnowdon at fastmail.fm
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] Output and Word
>Date: Tue, 18 Jul 2006 06:35:38 +1000
>
>Hi
>
>I have just started to have a look at R. I have used most stats software
>packages and can use perl, visual basic etc. I am interested in how well
>it handles lots of output e.g. tables or charts. How would you get lots
>of output most easily and quickly into a Word document?
>
>Sharon Snowdon
>
>
>
>------------------------------------------------------------------------
>-
>FIGHT BACK AGAINST SPAM!
>Download Spam Inspector, the Award Winning Anti-Spam Filter
>http://mail.giantcompany.com
>
>
>
>--
>
>
>
>7/14/2006
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



>From: "sharon snowdon" <sharonsnowdon at fastmail.fm>
>Reply-To: sharonsnowdon at fastmail.fm
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] Output and Word
>Date: Tue, 18 Jul 2006 06:35:38 +1000
>
>Hi
>
>I have just started to have a look at R. I have used most stats software
>packages and can use perl, visual basic etc. I am interested in how well
>it handles lots of output e.g. tables or charts. How would you get lots
>of output most easily and quickly into a Word document?
>
>Sharon Snowdon
>
>
>
>------------------------------------------------------------------------
>-
>FIGHT BACK AGAINST SPAM!
>Download Spam Inspector, the Award Winning Anti-Spam Filter
>http://mail.giantcompany.com
>
>
>
>--
>
>
>
>7/14/2006
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


From rmh at temple.edu  Tue Jul 18 15:05:54 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Tue, 18 Jul 2006 09:05:54 -0400 (EDT)
Subject: [R] Nested functions
Message-ID: <20060718090554.BEJ01902@po-d.temple.edu>

Please look at

http://www.turbulence.org/Works/song/

This is a website by Martin Wattenberg that visually displays
the types of patterns you are looking for.  He gave a paper
at the Joint Statistics Meetings in Minneapolis 2005.


From gavin.simpson at ucl.ac.uk  Tue Jul 18 15:10:02 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 18 Jul 2006 14:10:02 +0100
Subject: [R] Package for autocorrelation analysis?
In-Reply-To: <20060718120901.M26528@www.ecologia.unam.mx>
References: <OF301D160D.B5235C5D-ONC12571AF.003A75BE-C12571AF.003C9A16@basf-c-s.be>
	<x24pxfb0vu.fsf@turmalin.kubism.ku.dk>
	<20060718120901.M26528@www.ecologia.unam.mx>
Message-ID: <1153228202.8072.11.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2006-07-18 at 07:10 -0500, Ivan Rubio Perez wrote:
> Hi!
> Dear All,
> 
> I want to implement an autocorrelation analysis and estimated the Moran's I in a set of
> ecological traits. I seek for an R package that do this analysis, however, I couldn't
> found none that implement it. May be I'm lost in the universe of the Contributed
> Packages but I couldn't found information into the index of some packages that I suppose
> do it. Would you give me some recommendations to do an autocorrelation analysis?
> 
> Thanks!
> 
> Ivan

Please, learn to help yourself and use the tools provided. Also, don't
post a new message to the list by replying to an existing thread it
really messes up the archives!

Try, from within R:

RSiteSearch("Moran")

and 

RSiteSearch("spatial autocorrelation", restrict = "functions")

Both seem to produce suitable results.

You should also check out the spatial Task View on CRAN to get an
overview of suitable packages out there that might do what you want.
Choose a mirror near to you from here
http://cran.r-project.org/mirrors.html and then select Task Views in
menu on the left. 

HTH

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
 London, UK. WC1E 6BT.         [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From marsh at uri.edu  Tue Jul 18 15:20:06 2006
From: marsh at uri.edu (Marshall Feldman)
Date: Tue, 18 Jul 2006 09:20:06 -0400
Subject: [R] FW:  Large datasets in R
Message-ID: <00db01c6aa6c$e765b4b0$52db8083@MFELDMAN>

Hi,

I have two further comments/questions about large datasets in R.
 
1. Does R's ability to handle large datasets depend on the operating
system's use of virtual memory? In theory, at least, VM should make the
difference between installed RAM and virtual memory on a hard drive
primarily a determinant of how fast R will calculate rather than whether or
not it can do the calculations. However, if R has some low-level routines
that have to be memory resident and use more memory as the amount of data
increases, this may not hold. Can someone shed light on this?

2. Is What 64-bit versions of R are available at present?

	Marsh Feldman
	The University of Rhode Island

-----Original Message-----
From: Thomas Lumley [mailto:tlumley at u.washington.edu] 
Sent: Monday, July 17, 2006 3:21 PM
To: Deepankar Basu
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Large datasets in R

On Mon, 17 Jul 2006, Deepankar Basu wrote:

> Hi!
>
> I am a student of economics and currently do most of my statistical work
> using STATA. For various reasons (not least of which is an aversion for
> proprietary software), I am thinking of shifting to R. At the current
> juncture my concern is the following: would I be able to work on
> relatively large data-sets using R? For instance, I am currently working
> on a data-set which is about 350MB in size. Would be possible to work
> data-sets of such sizes using R?


The answer depends on a lot of things, but most importantly
1) What you are going to do with the data
2) Whether you have a 32-bit or 64-bit version of R
3) How much memory your computer has.

In a 32-bit version of R (where R will not be allowed to address more than 
2-3Gb of memory) an object of size 350Mb is large enough to cause problems 
(see eg the R Installation and Adminstration Guide).

If your 350Mb data set has lots of variables and you only use a few at a 
time then you may not have any trouble even on a 32-bit system once you 
have read in the data.

If you have a 64-bit version of R and a few Gb of memory then there should 
be no real difficulty in working with that size of data set for most 
analyses.  You might come across some analyses (eg some cluster analysis 
functions) that use n^2 memory for n observations and so break down.


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From gavin.simpson at ucl.ac.uk  Tue Jul 18 15:26:08 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 18 Jul 2006 14:26:08 +0100
Subject: [R] RSiteSearch() not in posting guide
Message-ID: <1153229168.8072.26.camel@gsimpson.geog.ucl.ac.uk>

Hi,

In a recent reply I sent to the list, my initial response was to
suggest, politely that by reading the posting guide and learning to use
the supplied tools one can often help oneself. I then went on to say
that reading the posting guide would have led you to RSiteSearch("insert
query"). Luckily I went to the posting guide to check this was there
before posting and much to my surprise that page does not once mention
this excellent facility!

In the section "Do your homework before posting:", help.search()
and ?foo are recommended, but these only work for installed, and
possibly loaded, packages. Wouldn't it be better to suggest a user try
RSiteSearch("foo") as well? This way they are far more likely to find
something relevant if that something resides on CRAN or the mailing list
archives.

Can I suggest adding another bullet with the following text to the "Do
your homework before posting:" section:

Do RSiteSearch("keyword") with different keywords (type this at the R
prompt) to search R, contributed packages and R-Help postings. To
restrict the search to functions only, use RSiteSearch("keyword",
restrict = "functions"). See ?RSiteSearch.

or shorter:

Do RSiteSearch("keyword") with different keywords (type this at the R
prompt) to search R functions, contributed packages and R-Help postings.
See ?RSiteSearch for further options and to restrict searches.

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
 London, UK. WC1E 6BT.         [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From ggrothendieck at gmail.com  Tue Jul 18 15:29:07 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 18 Jul 2006 09:29:07 -0400
Subject: [R] RSiteSearch() not in posting guide
In-Reply-To: <1153229168.8072.26.camel@gsimpson.geog.ucl.ac.uk>
References: <1153229168.8072.26.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <971536df0607180629k486944eemd2b90fbead3a550c@mail.gmail.com>

I think the posting guide may have been written prior to the
existence of RSiteSearch as a builtin command.

On 7/18/06, Gavin Simpson <gavin.simpson at ucl.ac.uk> wrote:
> Hi,
>
> In a recent reply I sent to the list, my initial response was to
> suggest, politely that by reading the posting guide and learning to use
> the supplied tools one can often help oneself. I then went on to say
> that reading the posting guide would have led you to RSiteSearch("insert
> query"). Luckily I went to the posting guide to check this was there
> before posting and much to my surprise that page does not once mention
> this excellent facility!
>
> In the section "Do your homework before posting:", help.search()
> and ?foo are recommended, but these only work for installed, and
> possibly loaded, packages. Wouldn't it be better to suggest a user try
> RSiteSearch("foo") as well? This way they are far more likely to find
> something relevant if that something resides on CRAN or the mailing list
> archives.
>
> Can I suggest adding another bullet with the following text to the "Do
> your homework before posting:" section:
>
> Do RSiteSearch("keyword") with different keywords (type this at the R
> prompt) to search R, contributed packages and R-Help postings. To
> restrict the search to functions only, use RSiteSearch("keyword",
> restrict = "functions"). See ?RSiteSearch.
>
> or shorter:
>
> Do RSiteSearch("keyword") with different keywords (type this at the R
> prompt) to search R functions, contributed packages and R-Help postings.
> See ?RSiteSearch for further options and to restrict searches.
>
> G
> --
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>  Gavin Simpson                 [t] +44 (0)20 7679 0522
>  ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
>  Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
>  Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
>  London, UK. WC1E 6BT.         [w] http://www.ucl.ac.uk/~ucfagls/
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rdpeng at gmail.com  Tue Jul 18 15:40:27 2006
From: rdpeng at gmail.com (Roger D. Peng)
Date: Tue, 18 Jul 2006 09:40:27 -0400
Subject: [R] FW:  Large datasets in R
In-Reply-To: <00db01c6aa6c$e765b4b0$52db8083@MFELDMAN>
References: <00db01c6aa6c$e765b4b0$52db8083@MFELDMAN>
Message-ID: <44BCE4CB.9050807@gmail.com>

In my experience, the OS's use of virtual memory is only relevant in the rough 
sense that the OS can store *other* running applications in virtual memory so 
that R can use as much of the physical memory as possible.  Once R itself 
overflows into virtual memory it quickly becomes unusable.

I'm not sure I understand your second question.  As R is available in source 
code form, it can be compiled for many 64-bit operating systems.

-roger

Marshall Feldman wrote:
> Hi,
> 
> I have two further comments/questions about large datasets in R.
>  
> 1. Does R's ability to handle large datasets depend on the operating
> system's use of virtual memory? In theory, at least, VM should make the
> difference between installed RAM and virtual memory on a hard drive
> primarily a determinant of how fast R will calculate rather than whether or
> not it can do the calculations. However, if R has some low-level routines
> that have to be memory resident and use more memory as the amount of data
> increases, this may not hold. Can someone shed light on this?
> 
> 2. Is What 64-bit versions of R are available at present?
> 
> 	Marsh Feldman
> 	The University of Rhode Island
> 
> -----Original Message-----
> From: Thomas Lumley [mailto:tlumley at u.washington.edu] 
> Sent: Monday, July 17, 2006 3:21 PM
> To: Deepankar Basu
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Large datasets in R
> 
> On Mon, 17 Jul 2006, Deepankar Basu wrote:
> 
>> Hi!
>>
>> I am a student of economics and currently do most of my statistical work
>> using STATA. For various reasons (not least of which is an aversion for
>> proprietary software), I am thinking of shifting to R. At the current
>> juncture my concern is the following: would I be able to work on
>> relatively large data-sets using R? For instance, I am currently working
>> on a data-set which is about 350MB in size. Would be possible to work
>> data-sets of such sizes using R?
> 
> 
> The answer depends on a lot of things, but most importantly
> 1) What you are going to do with the data
> 2) Whether you have a 32-bit or 64-bit version of R
> 3) How much memory your computer has.
> 
> In a 32-bit version of R (where R will not be allowed to address more than 
> 2-3Gb of memory) an object of size 350Mb is large enough to cause problems 
> (see eg the R Installation and Adminstration Guide).
> 
> If your 350Mb data set has lots of variables and you only use a few at a 
> time then you may not have any trouble even on a 32-bit system once you 
> have read in the data.
> 
> If you have a 64-bit version of R and a few Gb of memory then there should 
> be no real difficulty in working with that size of data set for most 
> analyses.  You might come across some analyses (eg some cluster analysis 
> functions) that use n^2 memory for n observations and so break down.
> 
> 
>  	-thomas
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From phgrosjean at sciviews.org  Tue Jul 18 15:56:39 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Tue, 18 Jul 2006 15:56:39 +0200
Subject: [R] R and DDE (Dynamic Data Exchange)
In-Reply-To: <44BBB8B3.8070505@7d4.com>
References: <44BBB8B3.8070505@7d4.com>
Message-ID: <44BCE897.2010900@sciviews.org>

Hello Vincent,

I think there is not much else than tcltk2. It is a complete 
implementation of DDE (client/server), so it should fit your needs. 
There are lots of examples on the man page. Please, if you think it 
could be helpful for other users, do submit examples. I think real-time 
acquisition of data through DDE under Windows would interest a couple of 
users (including myself).
Best,

Philippe Grosjean

..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Belgium
( ( ( ( (
..............................................................

vincent at 7d4.com wrote:
> R and DDE (Dynamic Data Exchange)
> 
> Dear Rusers,
> I run an application (not mine) which acts as a DDE server.
> I would like to use R to get data from this application,
> say once per minute, and do some processing on it.
> I didn't find much info on the R DDE abilities, apart the tcltk2
> package in which I will try to go deeper.
> I would be very thankful for any info, pointer or advice about the
> "good ways" to make R program get online data from a DDE server.
> Thanks
> Vincent
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>


From ripley at stats.ox.ac.uk  Tue Jul 18 15:58:34 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 18 Jul 2006 14:58:34 +0100 (BST)
Subject: [R] FW:  Large datasets in R
In-Reply-To: <00db01c6aa6c$e765b4b0$52db8083@MFELDMAN>
References: <00db01c6aa6c$e765b4b0$52db8083@MFELDMAN>
Message-ID: <Pine.LNX.4.64.0607181443050.20229@gannet.stats.ox.ac.uk>

On Tue, 18 Jul 2006, Marshall Feldman wrote:

> Hi,
> 
> I have two further comments/questions about large datasets in R.
>  
> 1. Does R's ability to handle large datasets depend on the operating
> system's use of virtual memory? In theory, at least, VM should make the
> difference between installed RAM and virtual memory on a hard drive
> primarily a determinant of how fast R will calculate rather than whether or
> not it can do the calculations. However, if R has some low-level routines
> that have to be memory resident and use more memory as the amount of data
> increases, this may not hold. Can someone shed light on this?

The issue is address space, not RAM.  The limits Thomas mentions are on 
VM, not RAM, and it is common to have at least as much RAM installed as 
the VM address space for a user process.

There is no low-level code in R that has any idea if it is 
memory-resident, nor AFAIK is there any portable way to do so in a user 
process in a modern OS.  (R is as far as possible written to C99 and POSIX 
standards.)

> 2. Is What 64-bit versions of R are available at present?

Any OS with a 64-bit CPU that you can find a viable 64-bit compiler suite 
for.  We've had 64-bit versions of R since the last millenium on Solaris, 
IRIX, HP-UX, OSF/1 and more recently on AIX, FreeBSD, Linux, MacOS X (on 
so-called G5) and probably others.

The exception is probably Windows, for which there is no known free 
`viable 64-bit compiler suite', but it is likely that there are commercial 
ones.
 

> 
> 	Marsh Feldman
> 	The University of Rhode Island
> 
> -----Original Message-----
> From: Thomas Lumley [mailto:tlumley at u.washington.edu] 
> Sent: Monday, July 17, 2006 3:21 PM
> To: Deepankar Basu
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Large datasets in R
> 
> On Mon, 17 Jul 2006, Deepankar Basu wrote:
> 
> > Hi!
> >
> > I am a student of economics and currently do most of my statistical work
> > using STATA. For various reasons (not least of which is an aversion for
> > proprietary software), I am thinking of shifting to R. At the current
> > juncture my concern is the following: would I be able to work on
> > relatively large data-sets using R? For instance, I am currently working
> > on a data-set which is about 350MB in size. Would be possible to work
> > data-sets of such sizes using R?
> 
> 
> The answer depends on a lot of things, but most importantly
> 1) What you are going to do with the data
> 2) Whether you have a 32-bit or 64-bit version of R
> 3) How much memory your computer has.
> 
> In a 32-bit version of R (where R will not be allowed to address more than 
> 2-3Gb of memory) an object of size 350Mb is large enough to cause problems 
> (see eg the R Installation and Adminstration Guide).
> 
> If your 350Mb data set has lots of variables and you only use a few at a 
> time then you may not have any trouble even on a 32-bit system once you 
> have read in the data.
> 
> If you have a 64-bit version of R and a few Gb of memory then there should 
> be no real difficulty in working with that size of data set for most 
> analyses.  You might come across some analyses (eg some cluster analysis 
> functions) that use n^2 memory for n observations and so break down.
> 
> 
>  	-thomas
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bill.shipley at usherbrooke.ca  Tue Jul 18 16:01:47 2006
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Tue, 18 Jul 2006 10:01:47 -0400
Subject: [R] using split.screen?
Message-ID: <001101c6aa72$b5eb05b0$bb1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060718/c06c281e/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Jul 18 15:58:34 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 18 Jul 2006 14:58:34 +0100 (BST)
Subject: [R] FW:  Large datasets in R
In-Reply-To: <00db01c6aa6c$e765b4b0$52db8083@MFELDMAN>
References: <00db01c6aa6c$e765b4b0$52db8083@MFELDMAN>
Message-ID: <Pine.LNX.4.64.0607181443050.20229@gannet.stats.ox.ac.uk>

On Tue, 18 Jul 2006, Marshall Feldman wrote:

> Hi,
> 
> I have two further comments/questions about large datasets in R.
>  
> 1. Does R's ability to handle large datasets depend on the operating
> system's use of virtual memory? In theory, at least, VM should make the
> difference between installed RAM and virtual memory on a hard drive
> primarily a determinant of how fast R will calculate rather than whether or
> not it can do the calculations. However, if R has some low-level routines
> that have to be memory resident and use more memory as the amount of data
> increases, this may not hold. Can someone shed light on this?

The issue is address space, not RAM.  The limits Thomas mentions are on 
VM, not RAM, and it is common to have at least as much RAM installed as 
the VM address space for a user process.

There is no low-level code in R that has any idea if it is 
memory-resident, nor AFAIK is there any portable way to do so in a user 
process in a modern OS.  (R is as far as possible written to C99 and POSIX 
standards.)

> 2. Is What 64-bit versions of R are available at present?

Any OS with a 64-bit CPU that you can find a viable 64-bit compiler suite 
for.  We've had 64-bit versions of R since the last millenium on Solaris, 
IRIX, HP-UX, OSF/1 and more recently on AIX, FreeBSD, Linux, MacOS X (on 
so-called G5) and probably others.

The exception is probably Windows, for which there is no known free 
`viable 64-bit compiler suite', but it is likely that there are commercial 
ones.
 

> 
> 	Marsh Feldman
> 	The University of Rhode Island
> 
> -----Original Message-----
> From: Thomas Lumley [mailto:tlumley at u.washington.edu] 
> Sent: Monday, July 17, 2006 3:21 PM
> To: Deepankar Basu
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Large datasets in R
> 
> On Mon, 17 Jul 2006, Deepankar Basu wrote:
> 
> > Hi!
> >
> > I am a student of economics and currently do most of my statistical work
> > using STATA. For various reasons (not least of which is an aversion for
> > proprietary software), I am thinking of shifting to R. At the current
> > juncture my concern is the following: would I be able to work on
> > relatively large data-sets using R? For instance, I am currently working
> > on a data-set which is about 350MB in size. Would be possible to work
> > data-sets of such sizes using R?
> 
> 
> The answer depends on a lot of things, but most importantly
> 1) What you are going to do with the data
> 2) Whether you have a 32-bit or 64-bit version of R
> 3) How much memory your computer has.
> 
> In a 32-bit version of R (where R will not be allowed to address more than 
> 2-3Gb of memory) an object of size 350Mb is large enough to cause problems 
> (see eg the R Installation and Adminstration Guide).
> 
> If your 350Mb data set has lots of variables and you only use a few at a 
> time then you may not have any trouble even on a 32-bit system once you 
> have read in the data.
> 
> If you have a 64-bit version of R and a few Gb of memory then there should 
> be no real difficulty in working with that size of data set for most 
> analyses.  You might come across some analyses (eg some cluster analysis 
> functions) that use n^2 memory for n observations and so break down.
> 
> 
>  	-thomas
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From dassybr at gmail.com  Tue Jul 18 16:06:49 2006
From: dassybr at gmail.com (Hadassa Brunschwig)
Date: Tue, 18 Jul 2006 16:06:49 +0200
Subject: [R] Inflated Array
Message-ID: <83cfc0bd0607180706u3731963ei358ccbc000cf8f1@mail.gmail.com>

Hi R-users!

I am trying to create a what I call inflated array (maybe there is
already some other name for that). It is an array that changes
dinamically its dimensions, e.g. the higher the number of third
dimensions, the more rows in the array. So for example the array could
look like the following:

, ,1

1 2 3 4

, , 2

5   6   7  8
9 10 11 12

, , 3

13 14 15 16
17 18 19 20
21 22 23 24

Has anybody a comment on how to build this in the most efficient
manner (I know i could do looping)?

Thanks!

Hadassa


From ggrothendieck at gmail.com  Tue Jul 18 16:27:49 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 18 Jul 2006 10:27:49 -0400
Subject: [R] R and DDE (Dynamic Data Exchange)
In-Reply-To: <44BBB8B3.8070505@7d4.com>
References: <44BBB8B3.8070505@7d4.com>
Message-ID: <971536df0607180727l6dc978d3g1ada0ce27c92cad9@mail.gmail.com>

You can access DDE via COM as in this example
which uses DDE to open an Excel file.  Note that
Excel also supports COM directly and normally
one would use COM with Excel, not DDE, so you
might check if your application also supports COM.

# opens an excel spreadsheet c:\test.xls using dde
library(RDCOMClient)
sh <- COMCreate("Shell.Application")
sh$Namespace("C:\\")$ParseName("test.xls")$InvokeVerb("&Open")

Also if you are going to access DDE via COM or just COM also
check out the rcom package which is similar to RDCOMClient.

On 7/17/06, vincent at 7d4.com <vincent at 7d4.com> wrote:
> R and DDE (Dynamic Data Exchange)
>
> Dear Rusers,
> I run an application (not mine) which acts as a DDE server.
> I would like to use R to get data from this application,
> say once per minute, and do some processing on it.
> I didn't find much info on the R DDE abilities, apart the tcltk2
> package in which I will try to go deeper.
> I would be very thankful for any info, pointer or advice about the
> "good ways" to make R program get online data from a DDE server.
> Thanks
> Vincent
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From ripley at stats.ox.ac.uk  Tue Jul 18 16:29:40 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 18 Jul 2006 15:29:40 +0100 (BST)
Subject: [R] using split.screen?
In-Reply-To: <001101c6aa72$b5eb05b0$bb1ad284@BIO041>
References: <001101c6aa72$b5eb05b0$bb1ad284@BIO041>
Message-ID: <Pine.LNX.4.64.0607181517410.21324@gannet.stats.ox.ac.uk>

On Tue, 18 Jul 2006, Bill Shipley wrote:

> Hello.  I am having trouble understanding the use of split.screen.  I want
> to divide the device surface first into 4 equal screens:
> split.screen(figs=c(2,2))
>  
> This works.
>  
> I next want to subdivide each of these 4 screens into 10 subscreens.  I do,
> for the first of these 4 screens:
>  
> screen(1,new=T)
> and then: split.screen(figs=c(10,2))
>  
> My understanding is that this should split screen 1 (i.e. top right) 

That is screen 2: 1 is top left.

> into 10
> screens within its area.  However, this does not work and I get the
> following error message: Error in plot.new() : figure margins too large

well, only when you try to plot on one of those screens.

> Does anyone know what I am doing wrong? 

I suspect not using a big enough device region to allow such small plot 
regions.  You are asking for 20 rows in your device region.

Oh, and sending HTML mail.

> Bill Shipley
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From matthew_wiener at merck.com  Tue Jul 18 16:45:10 2006
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Tue, 18 Jul 2006 10:45:10 -0400
Subject: [R] using split.screen?  [Broadcast]
Message-ID: <4E9A692D8755DF478B56A2892388EE1FDB3A24@usctmx1118.merck.com>

This means that the margins for 10 screens would take up more room than you
have - essentially the plot area is being squeezed to nothing.  You can try
reducing your margins using par.

Also, it looks like you're trying to split into 20 screens there.

Hope this helps,

Matt 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bill Shipley
Sent: Tuesday, July 18, 2006 10:02 AM
To: R help list
Subject: [R] using split.screen? [Broadcast]

Hello.  I am having trouble understanding the use of split.screen.  I want
to divide the device surface first into 4 equal screens:
split.screen(figs=c(2,2))
 
This works.
 
I next want to subdivide each of these 4 screens into 10 subscreens.  I do,
for the first of these 4 screens:
 
screen(1,new=T)
and then: split.screen(figs=c(10,2))
 
My understanding is that this should split screen 1 (i.e. top right) into 10
screens within its area.  However, this does not work and I get the
following error message: Error in plot.new() : figure margins too large

Does anyone know what I am doing wrong? 
 

Bill Shipley

 

 

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ritwik.sinha at gmail.com  Tue Jul 18 16:53:40 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Tue, 18 Jul 2006 10:53:40 -0400
Subject: [R] FW: Large datasets in R
In-Reply-To: <Pine.LNX.4.64.0607181443050.20229@gannet.stats.ox.ac.uk>
References: <00db01c6aa6c$e765b4b0$52db8083@MFELDMAN>
	<Pine.LNX.4.64.0607181443050.20229@gannet.stats.ox.ac.uk>
Message-ID: <42bc98300607180753y56dfd453r4bb4a7f1d4037bd3@mail.gmail.com>

Hi,

I have a related question. How differently do other statistical
softwares handle large data?

The original post claims that 350 MB is fine on Stata. Some one
suggested S-Plus. I have heard people say that SAS can handle large
data sets. Why can others do it and R seem to have a problem? Don't
these softwares load the data onto RAM.

-- 
Ritwik Sinha
Graduate Student
Epidemiology and Biostatistics
Case Western Reserve University

http://darwin.cwru.edu/~rsinha


From tlumley at u.washington.edu  Tue Jul 18 16:55:31 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 18 Jul 2006 07:55:31 -0700 (PDT)
Subject: [R] Nested functions
In-Reply-To: <1153168475.5952.16.camel@localhost>
References: <1153168475.5952.16.camel@localhost>
Message-ID: <Pine.LNX.4.64.0607171705590.9430@homer23.u.washington.edu>

On Mon, 17 Jul 2006, John Wiedenhoeft wrote:

> Hi there,
>
> I'm having myself a hard time writing an algorithm for finding patterns
> within a given melody. In a vector I'd like to find ALL sequences that
> occur at least twice, without having to check all possible patterns via
> pattern matching.
>

Another approach, which works for not-too-long vectors like you have is:

   n <- length(v)
   matches <- outer(v, v, "==") & outer(1:n,1:n,">=")

Now matches has TRUE where v[i]==v[j]. For a longer match you would also 
need v[i+1]==v[j+1] and so on, making a diagonal line through the matrix. 
Diagonal lines are hard, so let's turn them into horizontal lines

   matches <- matrix(cbind(matches, FALSE), ncol=n)

now row i+1 column j of matches is TRUE for a single entry match starting 
at position j at a separation of i.  If there is a match of length 2, then 
column j+1 will also be TRUE, and so on.

Now rle() applied to a row will return the lengths of consecutive 
sequences of TRUE and FALSE. The lengths of consecutive sequences of TRUE 
are the lengths of the matches. To get rid of trivial matches of length 
less than 2 do
   match2 <-  t(apply(matches,1,function(row){
                      r<-rle(row)
                      r$values[r$lengths<2]<-FALSE
 	             inverse.rle(r)
                   }))



And finally, to extract the matches
   results <- apply(match2, 1, function(row){
                            r<-rle(row)
                            n<-length(r$lengths)
                            ends<-cumsum(r$lengths)
                            starts<-cumsum(c(1,r$lengths))[1:n]
                            list(starts[r$values],ends[r$values])
                     })
for starts and ends of matches or
   results <- apply(match2, 1, function(row){
                          r<-rle(row)
                          n<-length(r$lengths)
                          ends<-cumsum(r$lengths)[r$values]
                          starts<-cumsum(c(1,r$lengths))[1:n][r$values]
                          mapply(function(stt,end) v[stt:end],starts,ends,
                                  SIMPLIFY=FALSE)
                     })
to get a list of the actual matching sequences.


 	-thomas


From ggrothendieck at gmail.com  Tue Jul 18 17:00:23 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 18 Jul 2006 11:00:23 -0400
Subject: [R] FW: Large datasets in R
In-Reply-To: <42bc98300607180753y56dfd453r4bb4a7f1d4037bd3@mail.gmail.com>
References: <00db01c6aa6c$e765b4b0$52db8083@MFELDMAN>
	<Pine.LNX.4.64.0607181443050.20229@gannet.stats.ox.ac.uk>
	<42bc98300607180753y56dfd453r4bb4a7f1d4037bd3@mail.gmail.com>
Message-ID: <971536df0607180800xd19cccbh77bfa67dd34f6cf5@mail.gmail.com>

S-Plus stores objects as files whereas R stores them in memory.
SAS was developed many years ago when optimizing computer
resources was more important than it is now.

On 7/18/06, Ritwik Sinha <ritwik.sinha at gmail.com> wrote:
> Hi,
>
> I have a related question. How differently do other statistical
> softwares handle large data?
>
> The original post claims that 350 MB is fine on Stata. Some one
> suggested S-Plus. I have heard people say that SAS can handle large
> data sets. Why can others do it and R seem to have a problem? Don't
> these softwares load the data onto RAM.
>
> --
> Ritwik Sinha
> Graduate Student
> Epidemiology and Biostatistics
> Case Western Reserve University
>
> http://darwin.cwru.edu/~rsinha
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From maechler at stat.math.ethz.ch  Tue Jul 18 17:12:42 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 18 Jul 2006 17:12:42 +0200
Subject: [R] RSiteSearch() not in posting guide
In-Reply-To: <971536df0607180629k486944eemd2b90fbead3a550c@mail.gmail.com>
References: <1153229168.8072.26.camel@gsimpson.geog.ucl.ac.uk>
	<971536df0607180629k486944eemd2b90fbead3a550c@mail.gmail.com>
Message-ID: <17596.64106.353688.455724@stat.math.ethz.ch>

>>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at gmail.com>
>>>>>     on Tue, 18 Jul 2006 09:29:07 -0400 writes:

    Gabor> I think the posting guide may have been written prior
    Gabor> to the existence of RSiteSearch as a builtin command.

yes, that's correct.

Thank you, Gavin, for the suggestion.
I have added such a "bullet".  Visible now at svn.r-project.org,
and from tomorrow at the URL at the tail of every R-help
message.

Martin

    Gabor> On 7/18/06, Gavin Simpson <gavin.simpson at ucl.ac.uk>
    Gabor> wrote:
    >> Hi,
    >> 
    >> In a recent reply I sent to the list, my initial response
    >> was to suggest, politely that by reading the posting
    >> guide and learning to use the supplied tools one can
    >> often help oneself. I then went on to say that reading
    >> the posting guide would have led you to
    >> RSiteSearch("insert query"). Luckily I went to the
    >> posting guide to check this was there before posting and
    >> much to my surprise that page does not once mention this
    >> excellent facility!
    >> 
    >> In the section "Do your homework before posting:",
    >> help.search() and ?foo are recommended, but these only
    >> work for installed, and possibly loaded,
    >> packages. Wouldn't it be better to suggest a user try
    >> RSiteSearch("foo") as well? This way they are far more
    >> likely to find something relevant if that something
    >> resides on CRAN or the mailing list archives.
    >> 
    >> Can I suggest adding another bullet with the following
    >> text to the "Do your homework before posting:" section:
    >> 
    >> Do RSiteSearch("keyword") with different keywords (type
    >> this at the R prompt) to search R, contributed packages
    >> and R-Help postings. To restrict the search to functions
    >> only, use RSiteSearch("keyword", restrict =
    >> "functions"). See ?RSiteSearch.
    >> 
    >> or shorter:
    >> 
    >> Do RSiteSearch("keyword") with different keywords (type
    >> this at the R prompt) to search R functions, contributed
    >> packages and R-Help postings.  See ?RSiteSearch for
    >> further options and to restrict searches.
    >> 
    >> G
    >> --
    >> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
    >> Gavin Simpson [t] +44 (0)20 7679 0522 ECRC & ENSIS, UCL
    >> Geography, [f] +44 (0)20 7679 0565 Pearson Building, [e]
    >> gavin.simpsonATNOSPAMucl.ac.uk Gower Street, London [w]
    >> http://www.ucl.ac.uk/~ucfagls/cv/ London, UK. WC1E 6BT.
    >> [w] http://www.ucl.ac.uk/~ucfagls/
    >> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
    >>


From kaniovsk at wifo.ac.at  Tue Jul 18 17:18:27 2006
From: kaniovsk at wifo.ac.at (Serguei Kaniovski)
Date: Tue, 18 Jul 2006 17:18:27 +0200
Subject: [R] A contingency table of counts by case
Message-ID: <44BCFBC3.8060503@wifo.ac.at>

Here is an example of the data.frame that I have,

df<-data.frame("case"=rep(1:5,each=9),"id"=rep(1:9,times=5),"x"=round(runif(length(rep(1:5,each=9)))))

"case" represents the cases,
"id" the persons, and
"x" is the binary state.

I would like to know in how many cases any two persons

a. both have "1",
b. the first has "0" - the second has "1",
c. the first has "0" - the second has "0",
d. both have "0".

There will be choose(9,2) sums, denoted s_ij for 1<=i<j<=9,
where i and j are running indices for an "id"-pair.

Please help, this is way beyond my knowledge of R!

Thank you,
Serguei Kaniovski
-- 
___________________________________________________________________

Austrian Institute of Economic Research (WIFO)

Name: Serguei Kaniovski			P.O.Box 91
Tel.: +43-1-7982601-231			Arsenal Objekt 20
Fax:  +43-1-7989386			1103 Vienna, Austria
Mail: Serguei.Kaniovski at wifo.ac.at

http://www.wifo.ac.at/Serguei.Kaniovski


From tlumley at u.washington.edu  Tue Jul 18 17:34:02 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 18 Jul 2006 08:34:02 -0700 (PDT)
Subject: [R] FW: Large datasets in R
In-Reply-To: <42bc98300607180753y56dfd453r4bb4a7f1d4037bd3@mail.gmail.com>
References: <00db01c6aa6c$e765b4b0$52db8083@MFELDMAN>
	<Pine.LNX.4.64.0607181443050.20229@gannet.stats.ox.ac.uk>
	<42bc98300607180753y56dfd453r4bb4a7f1d4037bd3@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0607180802140.839@homer23.u.washington.edu>

On Tue, 18 Jul 2006, Ritwik Sinha wrote:

> Hi,
>
> I have a related question. How differently do other statistical
> softwares handle large data?
>
> The original post claims that 350 MB is fine on Stata. Some one
> suggested S-Plus. I have heard people say that SAS can handle large
> data sets. Why can others do it and R seem to have a problem? Don't
> these softwares load the data onto RAM.
>

Stata does load the data into RAM and does have limits for the same reason 
that R does. However, Stata has a less flexible representation of its data 
(basically one rectangular dataset) and so it can handle somewhat larger 
data sets for any given memory size. For example, even with 512Gb of 
memory a 350Mb data set might be usable in Stata and with 1Gb it would 
certainly be. Stata is also faster for a given memory load, apparently 
because of its simpler language design [some evidence for this is that the 
recent language additions to support flexible graphics run rather more 
slowly than eg lattice in R].

The other approach is to write the estimation routines so that only part 
of the data need be in memory at a given time.  *Some* procedures in SAS 
and SPSS work this way, and this is the idea of the S-PLUS 7.0 system for 
handling large data sets.   This approach requires the programmer to 
handle the reading of sections of code from memory, something that can 
only be automated to a limited extent.

People have used R in this way, storing data in a database and reading it 
as required. There are also some efforts to provide facilities to support 
this sort of programming (such as the current project funded by Google 
Summer of Code:  http://tolstoy.newcastle.edu.au/R/devel/06/05/5525.html). 
One reason there isn't more of this is that relying on Moore's Law has 
worked very well over the years.


          -thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From wiedenhoeft at gmx.net  Tue Jul 18 17:35:16 2006
From: wiedenhoeft at gmx.net (John Wiedenhoeft)
Date: Tue, 18 Jul 2006 17:35:16 +0200
Subject: [R] Nested functions
In-Reply-To: <44BD9467.1090507@bitwrit.com.au>
References: <1153168475.5952.16.camel@localhost>
	<44BD9467.1090507@bitwrit.com.au>
Message-ID: <1153236916.5846.59.camel@localhost>

Am Dienstag, den 18.07.2006, 22:09 -0400 schrieb Jim Lemon:
> Hi John,
> 
> Minor bug - I zeroed the hit counter in the wrong place.
> 
> find.replay<-function(tunestring,maxlen) {
....
>   return(matchlist)
> }

Dear Jim,

many, many thanks for your effords :-D!!! Your program is great and very
elegant (guess I was thinking to mathematical and iterative...). I have
made some minor adjustments to it. Most important, I have changed
starttest to startpat+1, since patterns can overlap of course (1111
consists of 2 times 111). Hope this doesn't cause any inconsistency.

Here is what I'm using now:


CODECODECODECODECODECODECODECODECODECODECODECODECODECODECODE

find.replay<-function(tunestring,filename,maxlen) 
{
tunelen<-length(tunestring)
if(missing(maxlen)) maxlen<-floor(tunelen/2)
if(missing(filename)) filename<-"output"
matchlist <- list()
startpat<-1
endpat<-2
finishpos<-tunelen-maxlen
pattern<-tunestring[startpat:endpat]
patlen<-length(pattern)
while(patlen <= maxlen) 
	{
	while(endpat < tunelen-patlen) 
		{
		starttest<-startpat+1	#changed from endpat+1 to detect overlapping
patterns
		endtest<-starttest+(endpat-startpat)
		# step through the rest of tunestring with this pattern
		while(endtest <= tunelen) 
			{
			testpat<-tunestring[starttest:endtest]
			if(identical(pattern,testpat))
				{
				m <- 0;
				w <- 0;
				for(k in 1:patlen)
					{m <- pattern[k]*10^(patlen-k)+m;}
				for(l in 1:patlen)
					{w <- testpat[l]*10^(patlen-l)+w;}
				#just in case... ;-)
				if (m!=w)
					{
					warn <- paste("Unmatching patterns", m, "and", w, "detected
errorneously!")
					print(warn)
					write.table(warn, file=filename, append=TRUE, sep="\t", eol="\n",
row.names=FALSE, col.names=FALSE);
					}
				p <- c(patlen, m, startpat, starttest);
				write.table(t(p), file=filename, append=TRUE, sep="\t", eol="\n",
row.names=FALSE, col.names=FALSE);
#				print(p);
				}
			# step to the next substring
			starttest<-starttest+1
			endtest<-endtest+1
			}
		# now move pattern one step along the string
		startpat<-startpat+1
		endpat<-endpat+1
		pattern<-tunestring[startpat:endpat]
		}
	# now increase the length of the pattern
	startpat<- 1
	endpat<-startpat+patlen
	pattern<-tunestring[startpat:endpat]
	patlen<-length(pattern)
	}
print(paste("Output written to file '", filename, "'. Have fun!",
sep=""))
}

ENDENDENDENDENDENDENDENDENDENDENDENDENDENDENDENDENDENDENDEND


Again, many, many thanks. You brightened up my day!

Cheers,
John


From jacques.veslot at good.ibl.fr  Tue Jul 18 17:35:32 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Tue, 18 Jul 2006 17:35:32 +0200
Subject: [R] A contingency table of counts by case
In-Reply-To: <44BCFBC3.8060503@wifo.ac.at>
References: <44BCFBC3.8060503@wifo.ac.at>
Message-ID: <44BCFFC4.3040100@good.ibl.fr>

library(gtools)
apply(combinations(9,2), 1, function(x) with(df[df$id %in% x, ], table(x, id)))
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Serguei Kaniovski a ?crit :
> Here is an example of the data.frame that I have,
> 
> df<-data.frame("case"=rep(1:5,each=9),"id"=rep(1:9,times=5),"x"=round(runif(length(rep(1:5,each=9)))))
> 
> "case" represents the cases,
> "id" the persons, and
> "x" is the binary state.
> 
> I would like to know in how many cases any two persons
> 
> a. both have "1",
> b. the first has "0" - the second has "1",
> c. the first has "0" - the second has "0",
> d. both have "0".
> 
> There will be choose(9,2) sums, denoted s_ij for 1<=i<j<=9,
> where i and j are running indices for an "id"-pair.
> 
> Please help, this is way beyond my knowledge of R!
> 
> Thank you,
> Serguei Kaniovski


From ghos0033 at UMN.EDU  Tue Jul 18 17:42:37 2006
From: ghos0033 at UMN.EDU (Debarchana Ghosh)
Date: Tue, 18 Jul 2006 10:42:37 -0500
Subject: [R] Survey-weighted ordered logistic regression
Message-ID: <d1b8ff630607180842s659aeeaavc3ce51674f6d4fd0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060718/ad720565/attachment.pl 

From xbarron at yahoo.com  Tue Jul 18 17:43:42 2006
From: xbarron at yahoo.com (Xavier Barron)
Date: Tue, 18 Jul 2006 17:43:42 +0200 (CEST)
Subject: [R] I think this is a bug
Message-ID: <20060718154343.19726.qmail@web34114.mail.mud.yahoo.com>

Hello!

I work with:

R : Copyright 2006, The R Foundation for
Statistical Computing
Version 2.3.1 (2006-06-01)

On Windows XP Professional (Version 2002) SP2

I think there is a bug in the conditional
execution if (expr1) {expr2} else {expr3}

If I try: 

"if (expr1) expr2 else expr3" 

it works well but when I put the expression expr2
and expr3 between {} I receive an error message
like this one:

"Erreur dans parse(file, n = -1, NULL, "?") :
erreur de syntaxe ? la ligne
4:      }
5:      else"

...which translated in english gives:

"Error in parse(file, n = -1, NULL, "?") : syntax
error at the line
4:      }
5:      else"

Maybe, there is something I don't understand. I
should be very grateful if you would help me to
solve this issue.

Best regards,

Xavier

___________________________
Xavier Barron 
20, rue de la Pierre Lev?e
75011 Paris 
0143381141 / 0675062109


From xbarron at yahoo.com  Tue Jul 18 18:06:00 2006
From: xbarron at yahoo.com (Xavier Barron)
Date: Tue, 18 Jul 2006 18:06:00 +0200 (CEST)
Subject: [R] How can I extract information from list which class is nls
Message-ID: <20060718160600.9918.qmail@web34106.mail.mud.yahoo.com>

Hello!

I work with :
R : Copyright 2006, The R Foundation for
Statistical Computing
Version 2.3.1 (2006-06-01)
On Windows XP Professional (Version 2002) SP2.

At this moment I use the function "nls" combined
with a selfStar model (SSmicmen, related to
Michaelis-Menten equation, and provided by the
"stats" package). 

When I realise the following operation (cf. p 59
of the "An Introduction to R" manual,
http://www.r-project.org/, for more details):

> fit<-nls(y~SSmicmen(x, Vm, K), df)
> summary(fit)

I obtain the values of Vm and K. The object "fit"
is a list of the class "nls". However I cannot
identify the objects which are inside of "fit"
and  which contain the values of Vm and K. 

Actually, I would like to extract these values to
introduce them into new objects but I don't know
how?!

Maybe, somebody could help me to solve this
problem. It would be very helpful for me.

Best regards,

Xavier


From bolker at ufl.edu  Tue Jul 18 18:07:39 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 18 Jul 2006 16:07:39 +0000 (UTC)
Subject: [R] Inflated Array
References: <83cfc0bd0607180706u3731963ei358ccbc000cf8f1@mail.gmail.com>
Message-ID: <loom.20060718T180034-507@post.gmane.org>

Hadassa Brunschwig <dassybr <at> gmail.com> writes:

> 
> Hi R-users!
> 
> I am trying to create a what I call inflated array (maybe there is
> already some other name for that). It is an array that changes
> dinamically its dimensions, e.g. the higher the number of third
> dimensions, the more rows in the array. 

  base R doesn't have a data structure for this: arrays in R
must not be "ragged" (i.e., every sub-array must have the
same dimensions).

  So you would need to use a list of matrices in your
example.  I don't quite know the logic that you're using
to decide what goes in each sub-table.

The basic example you give could be built 
by hand as follows:

list(1:4,matrix(5:12,byrow=TRUE,ncol=4),
         matrix(13:24,byrow=TRUE,ncol=4))

  if you had used scan() or something else to
get a long, flat vector (x) and also had
a vector (v) that indicated the number of
rows in each sub-table:

x <- 1:24
nrow <- c(1,2,3)
ncol <- 4

ind <- rep(1:length(nrow),ncol*nrow)
lapply(split(x,ind),
    matrix,ncol=ncol,byrow=TRUE)

  seems to work.

  cheers
   Ben Bolker


From vincent at 7d4.com  Tue Jul 18 18:15:06 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Tue, 18 Jul 2006 18:15:06 +0200
Subject: [R] R and DDE (Dynamic Data Exchange)
In-Reply-To: <44BBB8B3.8070505@7d4.com>
References: <44BBB8B3.8070505@7d4.com>
Message-ID: <44BD090A.7070900@7d4.com>


Thanks Gabor and Philippe.
Special thanks to Philippe for his tcltk2 nice job.
I'm testing differents approaches for my problem.
I'll return info if I use DDE + tcltk2.
Vincent


From Leandro_Magnusson at brown.edu  Tue Jul 18 18:21:52 2006
From: Leandro_Magnusson at brown.edu (Leandro Magnusson)
Date: Tue, 18 Jul 2006 13:21:52 -0300
Subject: [R] Endogenous Tobit/Probit model
Message-ID: <44BD0AA0.2040005@brown.edu>

Hi

I would like to know if there is a package that allows me to implement 
endogenous Tobit/Probit  model.
Example:

res1 <- rnorm(N);
res2 <- res1*0.5 + rnorm(N)
x      <-  z[,1]*2 + res1;
ys     <- x*b + res2;
d      <- (ys>0); #dummy variable
 y      <- d*ys;

y is censored  and x is correlated with res1. z is my instrument. 
(continuously observed) .
Therefore

survreg(Surv(y, y > 0, type ='left')~ 0 + x , dist = 'gaussian')

will give biased estimates.

I also need to know the variance-covariance matrix for the parameters of 
the model

Thanks for any information

Leandro


From tlumley at u.washington.edu  Tue Jul 18 18:28:02 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 18 Jul 2006 09:28:02 -0700 (PDT)
Subject: [R] I think this is a bug
In-Reply-To: <20060718154343.19726.qmail@web34114.mail.mud.yahoo.com>
References: <20060718154343.19726.qmail@web34114.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0607180926460.24347@homer21.u.washington.edu>

On Tue, 18 Jul 2006, Xavier Barron wrote:

> Hello!
>
> I work with:
>
> R : Copyright 2006, The R Foundation for
> Statistical Computing
> Version 2.3.1 (2006-06-01)
>
> On Windows XP Professional (Version 2002) SP2
>
> I think there is a bug in the conditional
> execution if (expr1) {expr2} else {expr3}
>
> If I try:
>
> "if (expr1) expr2 else expr3"
>
> it works well but when I put the expression expr2
> and expr3 between {} I receive an error message
> like this one:


It's not a bug. You need the } on the same line as the else

if (expr1){
   expr2
} else {
   expr3
}

as otherwise R doesn't know there is going to be an 'else'.

 	-thomas


> "Erreur dans parse(file, n = -1, NULL, "?") :
> erreur de syntaxe ? la ligne
> 4:      }
> 5:      else"
>
> ...which translated in english gives:
>
> "Error in parse(file, n = -1, NULL, "?") : syntax
> error at the line
> 4:      }
> 5:      else"
>
> Maybe, there is something I don't understand. I
> should be very grateful if you would help me to
> solve this issue.
>
> Best regards,
>
> Xavier
>
> ___________________________
> Xavier Barron
> 20, rue de la Pierre Lev?e
> 75011 Paris
> 0143381141 / 0675062109
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

From petr.pikal at precheza.cz  Tue Jul 18 18:31:34 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 18 Jul 2006 18:31:34 +0200
Subject: [R] I think this is a bug
In-Reply-To: <20060718154343.19726.qmail@web34114.mail.mud.yahoo.com>
Message-ID: <44BD2906.85.11F821C@localhost>

Hi
not at all

works for me

> a<-1
> b<-2
>  if (a>b) {print("Hallo")} else {print("OK")}
[1] "OK"
>  if (a<b) {print("Hallo")} else {print("OK")}
[1] "Hallo"
>

you probably started your else on new line e.g.

>  if (a<b) {print("Hallo")} {
Error: syntax error in " if (a<b) {print("Hallo")} {"


>  if (a<b)
+ {print("Hallo")} else
+ {print("OK")}
[1] "Hallo"
>

HTH
Petr


On 18 Jul 2006 at 17:43, Xavier Barron wrote:

Date sent:      	Tue, 18 Jul 2006 17:43:42 +0200 (CEST)
From:           	Xavier Barron <xbarron at yahoo.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] I think this is a bug

> Hello!
> 
> I work with:
> 
> R : Copyright 2006, The R Foundation for
> Statistical Computing
> Version 2.3.1 (2006-06-01)
> 
> On Windows XP Professional (Version 2002) SP2
> 
> I think there is a bug in the conditional
> execution if (expr1) {expr2} else {expr3}
> 
> If I try: 
> 
> "if (expr1) expr2 else expr3" 
> 
> it works well but when I put the expression expr2
> and expr3 between {} I receive an error message
> like this one:
> 
> "Erreur dans parse(file, n = -1, NULL, "?") :
> erreur de syntaxe ? la ligne
> 4:      }
> 5:      else"
> 
> ...which translated in english gives:
> 
> "Error in parse(file, n = -1, NULL, "?") : syntax
> error at the line
> 4:      }
> 5:      else"
> 
> Maybe, there is something I don't understand. I
> should be very grateful if you would help me to
> solve this issue.
> 
> Best regards,
> 
> Xavier
> 
> ___________________________
> Xavier Barron 
> 20, rue de la Pierre Lev?e
> 75011 Paris 
> 0143381141 / 0675062109
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Tue Jul 18 18:35:39 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 18 Jul 2006 18:35:39 +0200
Subject: [R] How can I extract information from list which class is nls
In-Reply-To: <20060718160600.9918.qmail@web34106.mail.mud.yahoo.com>
Message-ID: <44BD29FB.510.1233DAD@localhost>

Hi

your fit is an object (list) and you could use some functions like 
summary or coef to extract usefull information from it or you can 
call its components on your own.


> DNase1 <- subset(DNase, Run == 1)
> 
> ## using a selfStart model
> fm1DNase1 <- nls(density ~ SSlogis(log(conc), Asym, xmid, scal), 
DNase1)
> summary(fm1DNase1)

Formula: density ~ SSlogis(log(conc), Asym, xmid, scal)

Parameters:
     Estimate Std. Error t value Pr(>|t|)    
Asym  2.34518    0.07815   30.01 2.17e-13 ***
xmid  1.48309    0.08135   18.23 1.22e-10 ***
scal  1.04146    0.03227   32.27 8.51e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.01919 on 13 degrees of freedom

> coef(fm1DNase1)
    Asym     xmid     scal 
2.345180 1.483090 1.041455 
> coef(fm1DNase1)[1]
   Asym 
2.34518 
>

HTH
Petr



On 18 Jul 2006 at 18:06, Xavier Barron wrote:

Date sent:      	Tue, 18 Jul 2006 18:06:00 +0200 (CEST)
From:           	Xavier Barron <xbarron at yahoo.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] How can I extract information from list which class is nls

> Hello!
> 
> I work with :
> R : Copyright 2006, The R Foundation for
> Statistical Computing
> Version 2.3.1 (2006-06-01)
> On Windows XP Professional (Version 2002) SP2.
> 
> At this moment I use the function "nls" combined
> with a selfStar model (SSmicmen, related to
> Michaelis-Menten equation, and provided by the
> "stats" package). 
> 
> When I realise the following operation (cf. p 59
> of the "An Introduction to R" manual,
> http://www.r-project.org/, for more details):
> 
> > fit<-nls(y~SSmicmen(x, Vm, K), df)
> > summary(fit)
> 
> I obtain the values of Vm and K. The object "fit"
> is a list of the class "nls". However I cannot
> identify the objects which are inside of "fit"
> and  which contain the values of Vm and K. 
> 
> Actually, I would like to extract these values to
> introduce them into new objects but I don't know
> how?!
> 
> Maybe, somebody could help me to solve this
> problem. It would be very helpful for me.
> 
> Best regards,
> 
> Xavier
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From dieter.menne at menne-biomed.de  Tue Jul 18 18:36:35 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 18 Jul 2006 18:36:35 +0200
Subject: [R] Sweave and multipage lattice
Message-ID: <LPEJLJACLINDNMBMFAFIGEHOCDAA.dieter.menne@menne-biomed.de>

Dear R-Listeners,

as the Sweave faq says:

http://www.ci.tuwien.ac.at/~leisch/Sweave/FAQ.html

creating several figures from one figure chunk does not work, and for
standard graphics, a workaround is given. Now I have a multipage trellis
plot with an a-priori unknown number of pages, and I don't see an elegant
way of dividing it up into multiple pdf-files.

I noted there is a "page" event handler in the ... parameters, which would
provide a handle to open/close the file.

Any good idea would be appreciated.

Dieter


From Greg.Snow at intermountainmail.org  Tue Jul 18 18:39:11 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 18 Jul 2006 10:39:11 -0600
Subject: [R] Output and Word
Message-ID: <07E228A5BE53C24CAD490193A7381BBB4B7BFF@LP-EXCHVS07.CO.IHC.COM>

Others have suggested using R2HTML (which is a good option).

Another option is to use Sweave and specifically the new odfWeave
package for R.  This works on OpenOffice files rather than word files
(but OpenOffice  http://www.openoffice.org/ can inport and export word
documents).

The basic idea is to write your report in OpenOffice (or LaTeX or HTML),
but anywhere that you want statisticial output (graphs, tables) you
include instead the R code to produce the table, graph, or whatever.
Run this file through R (using Sweave or odfWeave) and the resulting
file has replaced all the code segments with their output.

The documentation with odfWeave has examples.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of sharon snowdon
Sent: Monday, July 17, 2006 2:36 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Output and Word

Hi

I have just started to have a look at R. I have used most stats software
packages and can use perl, visual basic etc. I am interested in how well
it handles lots of output e.g. tables or charts. How would you get lots
of output most easily and quickly into a Word document?

Sharon Snowdon



------------------------------------------------------------------------
-






-- 





______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


From tlumley at u.washington.edu  Tue Jul 18 18:41:47 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 18 Jul 2006 09:41:47 -0700 (PDT)
Subject: [R] Survey-weighted ordered logistic regression
In-Reply-To: <d1b8ff630607180842s659aeeaavc3ce51674f6d4fd0@mail.gmail.com>
References: <d1b8ff630607180842s659aeeaavc3ce51674f6d4fd0@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0607180935000.24347@homer21.u.washington.edu>

On Tue, 18 Jul 2006, Debarchana Ghosh wrote:

> Hi,
>
> I am trying to fit a model with an ordered response variable (3 levels) and
> 13 predictor variables. The sample has complex survey design and I've used
> 'svydesign' command from the survey package to specify the sampling design.
> After reading the manual of 'svyglm' command, I've found that you can fit a
> logistic regression (binary response variable) by specifying the
> family=binomial in svyglm function. However I'm unable to fit an ordered
> logistic model in 'svyglm function'.
>

You can do this most easily using withReplicates().  If you have a survey 
object created with svydesign, use as.svrepdesign() to turn it into a 
replicate-weights object and then do eg

library(MASS) 
ologit <- withReplicates( repwtdesign,
                 quote(coef(polr(y~x1+x2+x3+x4, weights=.weights))))

You will get a lot of (harmless) warnings about "non-integer #successes in 
a binomial glm!".  This will not give you standard errors for the 
intercept terms - if you want them you can do

ologit.int <- withReplicates( repwtdesign,
                 quote(polr(y~x1+x2+x3+x4, weights=.weights)$zeta))


You could also use svymle() and get Taylor linearisation standard errors, 
but that would require writing out the loglikelihood and its gradient, 
which is tedious (although you could borrow most of it from MASS::polr)


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From jesse.canchola.b at bayer.com  Tue Jul 18 18:42:29 2006
From: jesse.canchola.b at bayer.com (Jesse Albert Canchola)
Date: Tue, 18 Jul 2006 09:42:29 -0700
Subject: [R] Reconfiguring wide frame to long frame
Message-ID: <OF366D3C4A.69FD15ED-ON882571AF.005A8170-882571AF.005BC465@bayer.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060718/cf84516b/attachment.pl 

From basu.15 at osu.edu  Tue Jul 18 19:08:10 2006
From: basu.15 at osu.edu (DEEPANKAR BASU)
Date: Tue, 18 Jul 2006 13:08:10 -0400
Subject: [R] Large datasets in R
Message-ID: <51dcd5a51d93fc.51d93fc51dcd5a@osu.edu>

Thanks a lot for all the responses. The general drift of all the messages was the suggestion to use some database management package that has a nice interface with R; and most of the suggestions pointed in the direction of SQL. I will look into the SQL package and start learning to use it along with R. 

Thanks once again for all your suggestions.

Deepankar


From ggrothendieck at gmail.com  Tue Jul 18 19:12:55 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 18 Jul 2006 13:12:55 -0400
Subject: [R] Reconfiguring wide frame to long frame
In-Reply-To: <OF366D3C4A.69FD15ED-ON882571AF.005A8170-882571AF.005BC465@bayer.com>
References: <OF366D3C4A.69FD15ED-ON882571AF.005A8170-882571AF.005BC465@bayer.com>
Message-ID: <971536df0607181012v75dbb83ai807fce07b5dfa375@mail.gmail.com>

Try this:

# set up test data
Lines <- "ID  meas  ID.1   meas.1
1   1.1        3      1.2
2   2.1        4      2.2
"
DF <- read.table(textConnection(Lines), header = TRUE)

# reshape
matrix(t(DF), nc = 2, byrow = TRUE, dimnames = list(NULL, colnames(DF)[1:2]))


On 7/18/06, Jesse Albert Canchola <jesse.canchola.b at bayer.com> wrote:
> Greetings, fellow R'ers.
>
> How can I get this frame in R:
>
> ID  meas  ID.1   meas.1
> 1   1.1        3      1.2
> 2   2.1        4      2.2
>
> to look like this (stacking):
>
> ID meas
> 1  1.1
> 2  2.1
> 3  1.2
> 4  2.2
>
> It's not really the reshape function (or is it?) because we can consider
> the additional columns, viz., ID.1 and meas.1, as independent of ID and
> meas so it is basically a stacking problem (no longitudinal component).  I
> can't seem to find a good example to do this in the docs.  Thanks for your
> help.
>
> Regards,
> Jesse
>
>
>
>
>
>
>
>
> Jesse A. Canchola
> Biostatistician III
> Bayer Healthcare
> 725 Potter St.
> Berkeley, CA 94710
> P: 510.705.5855
> F: 510.705.5718
> E: Jesse.Canchola.b at Bayer.Com
>
>
>
>
>
> _______________________________________________________________________________________________
>
> The information contained in this e-mail is for the exclusive use of the intended recipient(s) and may be confidential, proprietary, and/or legally privileged.  Inadvertent disclosure of this message does not constitute a waiver of any privilege.  If you receive this message in error, please do not directly or indirectly use, print, copy, forward, or disclose any part of this message.  Please also delete this e-mail and all copies and notify the sender.  Thank you.
>
> For alternate languages please go to http://bayerdisclaimer.bayerweb.com
> _______________________________________________________________________________________________
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From quin.wills at googlemail.com  Tue Jul 18 19:31:40 2006
From: quin.wills at googlemail.com (Quin Wills)
Date: Tue, 18 Jul 2006 18:31:40 +0100
Subject: [R] How best to deal with returned errors?
Message-ID: <44bd1b12.4832fe2a.1433.75ca@mx.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060718/38e10400/attachment.pl 

From ggrothendieck at gmail.com  Tue Jul 18 19:36:22 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 18 Jul 2006 13:36:22 -0400
Subject: [R] Reconfiguring wide frame to long frame
In-Reply-To: <971536df0607181012v75dbb83ai807fce07b5dfa375@mail.gmail.com>
References: <OF366D3C4A.69FD15ED-ON882571AF.005A8170-882571AF.005BC465@bayer.com>
	<971536df0607181012v75dbb83ai807fce07b5dfa375@mail.gmail.com>
Message-ID: <971536df0607181036m6f38ef86u64a83f7a5d211462@mail.gmail.com>

Sorry, in looking at this again my previous code did not give the
same ordering you indicated.  Instead using the same DF try
this:

rbind(as.matrix(DF[,1:2]), as.matrix(DF[,3:4]))

Both this and the last piece of code produce matrices so
use as.data.frame if you want a data frame.

On 7/18/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try this:
>
> # set up test data
> Lines <- "ID  meas  ID.1   meas.1
> 1   1.1        3      1.2
> 2   2.1        4      2.2
> "
> DF <- read.table(textConnection(Lines), header = TRUE)
>
> # reshape
> matrix(t(DF), nc = 2, byrow = TRUE, dimnames = list(NULL, colnames(DF)[1:2]))
>
>
> On 7/18/06, Jesse Albert Canchola <jesse.canchola.b at bayer.com> wrote:
> > Greetings, fellow R'ers.
> >
> > How can I get this frame in R:
> >
> > ID  meas  ID.1   meas.1
> > 1   1.1        3      1.2
> > 2   2.1        4      2.2
> >
> > to look like this (stacking):
> >
> > ID meas
> > 1  1.1
> > 2  2.1
> > 3  1.2
> > 4  2.2
> >
> > It's not really the reshape function (or is it?) because we can consider
> > the additional columns, viz., ID.1 and meas.1, as independent of ID and
> > meas so it is basically a stacking problem (no longitudinal component).  I
> > can't seem to find a good example to do this in the docs.  Thanks for your
> > help.
> >
> > Regards,
> > Jesse
> >
> >
> >
> >
> >
> >
> >
> >
> > Jesse A. Canchola
> > Biostatistician III
> > Bayer Healthcare
> > 725 Potter St.
> > Berkeley, CA 94710
> > P: 510.705.5855
> > F: 510.705.5718
> > E: Jesse.Canchola.b at Bayer.Com
> >
> >
> >
> >
> >
> > _______________________________________________________________________________________________
> >
> > The information contained in this e-mail is for the exclusive use of the intended recipient(s) and may be confidential, proprietary, and/or legally privileged.  Inadvertent disclosure of this message does not constitute a waiver of any privilege.  If you receive this message in error, please do not directly or indirectly use, print, copy, forward, or disclose any part of this message.  Please also delete this e-mail and all copies and notify the sender.  Thank you.
> >
> > For alternate languages please go to http://bayerdisclaimer.bayerweb.com
> > _______________________________________________________________________________________________
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From David.Crabb.1 at city.ac.uk  Tue Jul 18 19:36:17 2006
From: David.Crabb.1 at city.ac.uk (Crabb, David)
Date: Tue, 18 Jul 2006 18:36:17 +0100
Subject: [R] bilinear regression
Message-ID: <B275C8556CC10248B260951E0DB0719B01C90D3C@nsq038ex.enterprise.internal.city.ac.uk>

I think this is an easy question, but I would be grateful for any advice
on how to implement this in R.

I simply have a response variable (y) that I am trying to predict with
one explanatory variable (x) but the shape of the scatter plot is
distinctly bilinear. It would be best described by two straight lines.
Is there a way of fitting a linear model to give me a bilinear fit and
(more importantly) automatically determine the 'cut off' point? I would
also want some statistic to convince myself that the bilinear fit is
better.

Many thanks for your help.

David


Dr. David Crabb
Department of Optometry and Visual Science,
City University, 
Northampton Square, London EC1V OHB
Tel: 44 207 040 0191   d.crabb at city.ac.uk
http://www.city.ac.uk/optometry/html/david_crabb.html


From rmh at temple.edu  Tue Jul 18 19:40:31 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Tue, 18 Jul 2006 13:40:31 -0400 (EDT)
Subject: [R] R and DDE (Dynamic Data Exchange)
Message-ID: <20060718134031.BEJ44728@po-d.temple.edu>

I am thrilled to learn tcltk2 has DDE capability.
It is the piece I have been needing to make ESS work directly
with the RGUI on Windows.  GNU emacs on Windows has a ddeclient,
but no access to COM.  So if R, or tcltk2 talking in both directions to R,
has a ddeserver, all should be possible.  I will be reading the documentation
closely in a few weeks to tie it together and then intend to make it happen.

Do you, or any other list member, have a sense of the size, complexity, ease,
magnitude of the task I just defined?  Any advice as I get started on it?

Rich


From jesse.canchola.b at bayer.com  Tue Jul 18 20:01:08 2006
From: jesse.canchola.b at bayer.com (Jesse Albert Canchola)
Date: Tue, 18 Jul 2006 11:01:08 -0700
Subject: [R] Reconfiguring wide frame to long frame
In-Reply-To: <971536df0607181012v75dbb83ai807fce07b5dfa375@mail.gmail.com>
Message-ID: <OF2A5098A5.8E081E31-ON882571AF.0062DC81-882571AF.0062F7AD@bayer.com>

Many thanks, Gabor.  That worked great!  I'm ecstatic. 

Best regards,
Jesse




"Gabor Grothendieck" <ggrothendieck at gmail.com> 
07/18/2006 10:12 AM

To
"Jesse Albert Canchola" <jesse.canchola.b at bayer.com>
cc
r-help at stat.math.ethz.ch
Subject
Re: [R] Reconfiguring wide frame to long frame






Try this:

# set up test data
Lines <- "ID  meas  ID.1   meas.1
1   1.1        3      1.2
2   2.1        4      2.2
"
DF <- read.table(textConnection(Lines), header = TRUE)

# reshape
matrix(t(DF), nc = 2, byrow = TRUE, dimnames = list(NULL, 
colnames(DF)[1:2]))


On 7/18/06, Jesse Albert Canchola <jesse.canchola.b at bayer.com> wrote:
> Greetings, fellow R'ers.
>
> How can I get this frame in R:
>
> ID  meas  ID.1   meas.1
> 1   1.1        3      1.2
> 2   2.1        4      2.2
>
> to look like this (stacking):
>
> ID meas
> 1  1.1
> 2  2.1
> 3  1.2
> 4  2.2
>
> It's not really the reshape function (or is it?) because we can consider
> the additional columns, viz., ID.1 and meas.1, as independent of ID and
> meas so it is basically a stacking problem (no longitudinal component). 
I
> can't seem to find a good example to do this in the docs.  Thanks for 
your
> help.
>
> Regards,
> Jesse
>
>
>
>
>
>
>
>
> Jesse A. Canchola
> Biostatistician III
> Bayer Healthcare
> 725 Potter St.
> Berkeley, CA 94710
> P: 510.705.5855
> F: 510.705.5718
> E: Jesse.Canchola.b at Bayer.Com
>
>
>
>
>
> 
_______________________________________________________________________________________________
>
> The information contained in this e-mail is for the exclusive use of the 
intended recipient(s) and may be confidential, proprietary, and/or legally 
privileged.  Inadvertent disclosure of this message does not constitute a 
waiver of any privilege.  If you receive this message in error, please do 
not directly or indirectly use, print, copy, forward, or disclose any part 
of this message.  Please also delete this e-mail and all copies and notify 
the sender.  Thank you.
>
> For alternate languages please go to http://bayerdisclaimer.bayerweb.com
> 
_______________________________________________________________________________________________
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From christos at nuverabio.com  Tue Jul 18 20:07:40 2006
From: christos at nuverabio.com (Christos Hatzis)
Date: Tue, 18 Jul 2006 14:07:40 -0400
Subject: [R] bilinear regression
In-Reply-To: <B275C8556CC10248B260951E0DB0719B01C90D3C@nsq038ex.enterprise.internal.city.ac.uk>
Message-ID: <004101c6aa95$0fa68c60$0e010a0a@headquarters.silicoinsights>

It appears that you might have a latent (hidden) explanatory variable that
causes the two-population appearance.  If you have some ideas on what that
other factor might be, you could try two separate linear regressions for
each value of the latent factor and compare the slopes and intercepts.  You
can then do some formal tests on the slopes and intercepts to see if you can
further simplify the model.  Depending on what you find, you can formulate a
linear regression model that incorporates such dependence on the slopes or
intercepts to fit the "bilinear" trend.

You might find helpful the discussion and example in Ch.10 of Venables &
Ripley, 4th ed, that introduces the concepts behind random and mixed effects
models.

-Christos

Christos Hatzis, Ph.D.
Vice President, Technology
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 5350
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com
 


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Crabb, David
Sent: Tuesday, July 18, 2006 1:36 PM
To: r-help at stat.math.ethz.ch
Subject: [R] bilinear regression

I think this is an easy question, but I would be grateful for any advice on
how to implement this in R.

I simply have a response variable (y) that I am trying to predict with one
explanatory variable (x) but the shape of the scatter plot is distinctly
bilinear. It would be best described by two straight lines.
Is there a way of fitting a linear model to give me a bilinear fit and (more
importantly) automatically determine the 'cut off' point? I would also want
some statistic to convince myself that the bilinear fit is better.

Many thanks for your help.

David


Dr. David Crabb
Department of Optometry and Visual Science, City University, Northampton
Square, London EC1V OHB
Tel: 44 207 040 0191   d.crabb at city.ac.uk
http://www.city.ac.uk/optometry/html/david_crabb.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jesse.canchola.b at bayer.com  Tue Jul 18 20:08:32 2006
From: jesse.canchola.b at bayer.com (Jesse Albert Canchola)
Date: Tue, 18 Jul 2006 11:08:32 -0700
Subject: [R] Reconfiguring wide frame to long frame
In-Reply-To: <971536df0607181036m6f38ef86u64a83f7a5d211462@mail.gmail.com>
Message-ID: <OFB72142CB.A8AEE052-ON882571AF.006367B5-882571AF.0063A55B@bayer.com>

Thanks, Gabor.  Since the data stacking components are independent, that 
didn't matter much but I am grateful for your follow-up code to match the 
desired output specifically. 

Regards,
Jesse





"Gabor Grothendieck" <ggrothendieck at gmail.com> 
07/18/2006 10:36 AM

To
"Jesse Albert Canchola" <jesse.canchola.b at bayer.com>
cc
r-help at stat.math.ethz.ch
Subject
Re: [R] Reconfiguring wide frame to long frame






Sorry, in looking at this again my previous code did not give the
same ordering you indicated.  Instead using the same DF try
this:

rbind(as.matrix(DF[,1:2]), as.matrix(DF[,3:4]))

Both this and the last piece of code produce matrices so
use as.data.frame if you want a data frame.

On 7/18/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try this:
>
> # set up test data
> Lines <- "ID  meas  ID.1   meas.1
> 1   1.1        3      1.2
> 2   2.1        4      2.2
> "
> DF <- read.table(textConnection(Lines), header = TRUE)
>
> # reshape
> matrix(t(DF), nc = 2, byrow = TRUE, dimnames = list(NULL, 
colnames(DF)[1:2]))
>
>
> On 7/18/06, Jesse Albert Canchola <jesse.canchola.b at bayer.com> wrote:
> > Greetings, fellow R'ers.
> >
> > How can I get this frame in R:
> >
> > ID  meas  ID.1   meas.1
> > 1   1.1        3      1.2
> > 2   2.1        4      2.2
> >
> > to look like this (stacking):
> >
> > ID meas
> > 1  1.1
> > 2  2.1
> > 3  1.2
> > 4  2.2
> >
> > It's not really the reshape function (or is it?) because we can 
consider
> > the additional columns, viz., ID.1 and meas.1, as independent of ID 
and
> > meas so it is basically a stacking problem (no longitudinal 
component).  I
> > can't seem to find a good example to do this in the docs.  Thanks for 
your
> > help.
> >
> > Regards,
> > Jesse
> >
> >
> >
> >
> >
> >
> >
> >
> > Jesse A. Canchola
> > Biostatistician III
> > Bayer Healthcare
> > 725 Potter St.
> > Berkeley, CA 94710
> > P: 510.705.5855
> > F: 510.705.5718
> > E: Jesse.Canchola.b at Bayer.Com
> >
> >
> >
> >
> >
> > 
_______________________________________________________________________________________________
> >
> > The information contained in this e-mail is for the exclusive use of 
the intended recipient(s) and may be confidential, proprietary, and/or 
legally privileged.  Inadvertent disclosure of this message does not 
constitute a waiver of any privilege.  If you receive this message in 
error, please do not directly or indirectly use, print, copy, forward, or 
disclose any part of this message.  Please also delete this e-mail and all 
copies and notify the sender.  Thank you.
> >
> > For alternate languages please go to 
http://bayerdisclaimer.bayerweb.com
> > 
_______________________________________________________________________________________________
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From uhkeller at web.de  Tue Jul 18 20:15:12 2006
From: uhkeller at web.de (Ulrich Keller)
Date: Tue, 18 Jul 2006 20:15:12 +0200
Subject: [R] Test for equality of coefficients in multivariate multiple
	regression
Message-ID: <44BD2530.2020502@web.de>

Hello,

suppose I have a multivariate multiple regression model such as the 
following:

 > DF<-data.frame(x1=rep(c(0,1),each=50),x2=rep(c(0,1),50))
 > tmp<-rnorm(100)
 > DF$y1<-tmp+DF$x1*.5+DF$x2*.3+rnorm(100,0,.5)
 > DF$y2<-tmp+DF$x1*.5+DF$x2*.7+rnorm(100,0,.5)
 > x.mlm<-lm(cbind(y1,y2)~x1+x2,data=DF)
 > coef(x.mlm)
                    y1        y2
(Intercept) 0.07800993 0.2303557
x1          0.52936947 0.3728513
x2          0.13853332 0.4604842

How can I test whether x1 and x2 respectively have the same effect on y1 
and y2? In other words, how can I test if coef(x.mlm)[2,1] is 
statistically equal to coef(x.mlm)[2,2] and coef(x.mlm)[3,1] to 
coef(x.mlm)[3,2]? I looked at linear.hypothesis {car} and glh.test 
{gmodels}, but these do not seem the apply to multivariate models.
Thank you in advance,

Uli Keller


From kubovy at virginia.edu  Tue Jul 18 20:18:16 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Tue, 18 Jul 2006 14:18:16 -0400
Subject: [R] JGR & help()
Message-ID: <8F93231C-79FC-41A0-8290-3167498F0E4D@virginia.edu>

Dear R-helpers,

In JGR, how to I get the help() to update when I install a new package?

 > sessionInfo()
Version 2.3.1 (2006-06-01)
powerpc-apple-darwin8.6.0

attached base packages:
[1] "datasets"  "methods"   "stats"     "graphics"  "grDevices"  
"utils"     "base"

other attached packages:
    lattice       MASS        JGR     JavaGD      rJava
   "0.13-9" "7.2-27.1"    "1.4-2"    "0.3-3"    "0.4-3"

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From grieve at u.washington.edu  Tue Jul 18 20:28:41 2006
From: grieve at u.washington.edu (grieve at u.washington.edu)
Date: Tue, 18 Jul 2006 11:28:41 -0700 (PDT)
Subject: [R] Using corStruct in nlme
Message-ID: <Pine.LNX.4.43.0607181128410.9698@hymn03.u.washington.edu>

I am having trouble fitting correlation structures within nlme. I would like to 
fit corCAR1, corGaus and corExp correlation structures to my data.  I either 
get the error "step halving reduced below minimum in pnls step" or 
alternatively R crashes.

My dataset is similar to the CO2 example in the nlme package. The one major 
difference is that in my case the 'conc' steps are not the same for each 'Plant'. I have replicated the problem using the CO2 data in nlme (based off of the Ch08.R script).

This works (when 'conc' is the same for each 'Plant':

(fm1CO2.lis <- nlsList(SSasympOff, CO2))
(fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2)))
(fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1))
CO2.nlme.var <- update(fm2CO2.nlme,
 fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
 start = c(32.412, 0, 0, 0, -4.5603, 49.344), 
 weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)

CO2.nlme.CAR<-update(CO2.nlme.var, corr=corCAR1())

CO2.nlme.gauss<-update(CO2.nlme.var, 
correlation=corGaus(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)

CO2.nlme.exp<-update(CO2.nlme.var, 
correlation=corExp(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)  

But, if i change each of the 'conc' numbers slightly so that they are no longer identical between subjects i can only get the corCAR1 correlation to work while R crashes for both corExp and corGaus:

for(i in 1:length(CO2$conc)){
    CO2$conc[i]<-(CO2$conc[i]+rnorm(1))
}

(fm1CO2.lis <- nlsList(SSasympOff, CO2))
(fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2)))
(fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1))
CO2.nlme.var <- update(fm2CO2.nlme,
 fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
 start = c(32.412, 0, 0, 0, -4.5603, 49.344), 
 weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)

CO2.nlme.CAR<-update(CO2.nlme.var, corr=corCAR1())

CO2.nlme.gauss<-update(CO2.nlme.var, 
correlation=corGaus(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)

CO2.nlme.exp<-update(CO2.nlme.var, 
correlation=corExp(form=~as.numeric(conc)|Plant,nugget=F), data=CO2) 

I have read Pinheiro & Bates (2000) and i think that it should be possible to fit these correlation structures to my data, but maybe i am mistaken.

I am running R 2.3.1 and have recently updated all packages.

Thanks,
Katie Grieve

Quantitative Ecology & Resource Management
University of Washington


From darrenleeweber at gmail.com  Tue Jul 18 20:30:53 2006
From: darrenleeweber at gmail.com (Darren Weber)
Date: Tue, 18 Jul 2006 11:30:53 -0700
Subject: [R] R-help in a newsgroup
Message-ID: <d2095b8c0607181130n7e2b0762t9e7674430f5f706e@mail.gmail.com>

Hi,

I find a lot of the R-help email traffic overloads my inbox.  My IT
managers are not really happy for me to be subscribed to several
high-traffic email lists.  I don't want to lose my contact with the
R-help emails, so I'm having to consider various ways of handling the
traffic.  Anyhow, I'm wondering how many people on the R-help email
list would prefer that most of the traffic were in a newsgroup?  In
case your interested in that option, there is a group available at:

The-R-Project-for-Statistical-Computing at googlegroups.com

I think the subscription is through normal news group channels.  The
google search services on this group are nice too.  This group is not
divided into useful categories, like help, admin, develop etc., but
it's not too difficult to create new groups for that.

Best, Darren


From pheilman at tucson.ars.ag.gov  Tue Jul 18 20:48:25 2006
From: pheilman at tucson.ars.ag.gov (Phil Heilman)
Date: Tue, 18 Jul 2006 11:48:25 -0700
Subject: [R] Reproducible Research - Examples
Message-ID: <200607181155562.SM01092@PW42SWRCHOTH>

All,

Recently I ran across a URL documenting published research using R: 

http://www.cgd.ucar.edu/ccr/ammann/millennium/CODES_MBH.html

A note on the site indicates that the code is being revised. The code and
data are provided, so that one could reproduce the results without having to
buy a proprietary software program. In poking around the R website it is
clear that a lot of thought has gone into documenting reproducible research,
notably by Harrell, Gentleman, and the Sweave effort, among others.

http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/StatReport
http://cran.ssds.ucdavis.edu/doc/contrib/Harrell-statcomp-notes.pdf
http://www.bioconductor.org/docs/papers/2003/Compendium

Question: Reproducible research is clearly desirable and feasible. Could
anyone provide examples (stand-alone URLs or supplementary material in
journals) that you would recommend as a models for reproducible research in
R?

Thanks,

Phil

Philip Heilman
USDA-ARS Southwest Watershed Research Center
2000 E. Allen Rd.
Tucson, AZ 85704
(520) 670-6380 x148
pheilman at tucson.ars.ag.gov
http://www.tucson.ars.ag.gov/


From mschwartz at mn.rr.com  Tue Jul 18 20:45:36 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 18 Jul 2006 13:45:36 -0500
Subject: [R] R-help in a newsgroup
In-Reply-To: <d2095b8c0607181130n7e2b0762t9e7674430f5f706e@mail.gmail.com>
References: <d2095b8c0607181130n7e2b0762t9e7674430f5f706e@mail.gmail.com>
Message-ID: <1153248336.3796.18.camel@localhost.localdomain>

On Tue, 2006-07-18 at 11:30 -0700, Darren Weber wrote:
> Hi,
> 
> I find a lot of the R-help email traffic overloads my inbox.  My IT
> managers are not really happy for me to be subscribed to several
> high-traffic email lists.  I don't want to lose my contact with the
> R-help emails, so I'm having to consider various ways of handling the
> traffic.  Anyhow, I'm wondering how many people on the R-help email
> list would prefer that most of the traffic were in a newsgroup?  In
> case your interested in that option, there is a group available at:
> 
> The-R-Project-for-Statistical-Computing at googlegroups.com
> 
> I think the subscription is through normal news group channels.  The
> google search services on this group are nice too.  This group is not
> divided into useful categories, like help, admin, develop etc., but
> it's not too difficult to create new groups for that.
> 
> Best, Darren

r-help is already available with an NNTP interface at gmane.org:

  http://dir.gmane.org/gmane.comp.lang.r.general

There is also a web based interface, where you can see that your post is
already available:

  http://news.gmane.org/gmane.comp.lang.r.general

Similarly, r-devel is also present:

  http://dir.gmane.org/gmane.comp.lang.r.devel


The Google group you reference is completely independent of the R e-mail
lists, whereas the gmane interface is synchronized with the R e-mail
lists.

HTH,

Marc Schwartz


From murdoch at stats.uwo.ca  Tue Jul 18 20:57:05 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 18 Jul 2006 14:57:05 -0400
Subject: [R] R-help in a newsgroup
In-Reply-To: <d2095b8c0607181130n7e2b0762t9e7674430f5f706e@mail.gmail.com>
References: <d2095b8c0607181130n7e2b0762t9e7674430f5f706e@mail.gmail.com>
Message-ID: <44BD2F01.6010805@stats.uwo.ca>

On 7/18/2006 2:30 PM, Darren Weber wrote:
> Hi,
> 
> I find a lot of the R-help email traffic overloads my inbox.  My IT
> managers are not really happy for me to be subscribed to several
> high-traffic email lists.  I don't want to lose my contact with the
> R-help emails, so I'm having to consider various ways of handling the
> traffic.  Anyhow, I'm wondering how many people on the R-help email
> list would prefer that most of the traffic were in a newsgroup?  In
> case your interested in that option, there is a group available at:
> 
> The-R-Project-for-Statistical-Computing at googlegroups.com
> 
> I think the subscription is through normal news group channels.  The
> google search services on this group are nice too.  This group is not
> divided into useful categories, like help, admin, develop etc., but
> it's not too difficult to create new groups for that.

If you prefer the newsgroup interface, you should also look at gmane. 
Gabor G posted a list of the newsgroups here:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/75239.html

recently.

Duncan Murdoch
> 
> Best, Darren
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From James.Frederick at uncp.edu  Tue Jul 18 21:27:39 2006
From: James.Frederick at uncp.edu (Jim Frederick)
Date: Tue, 18 Jul 2006 15:27:39 -0400
Subject: [R] Question about summing to zero
Message-ID: <44BD362B.6050408@uncp.edu>

Hi,

When I used
x <- c(10,10,10,10,5,5,5,5)
sum(x)
x <- x - .Last.value/8
the result was zero, as I expected.
However, when I used
x <- rnorm(101,0,10)
sum(x)
x <- x - .Last.value/101
sum(x)
I did not get zero, but  -2.664535e-15.  OK, that's fairly close to 
zero, but I tried to improve the figure.  Each time I repeated the last 
two commands, the sum got closer to zero.  The series seemed to converge 
on -1.30e-15, so changed the 101 to 50, then to 20, and then to 10.  I 
had sum(x) down to -5.551115e-17 before I quit.

If R can store values of x which sum to -5.551115e-17, then why can't I 
get that value on the first try?  Is this a bug or just rounding error?

Jim Frederick


From jfox at mcmaster.ca  Tue Jul 18 21:33:22 2006
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 18 Jul 2006 15:33:22 -0400
Subject: [R] Test for equality of coefficients in multivariate
 multiple	regression
In-Reply-To: <44BD2530.2020502@web.de>
Message-ID: <web-132629439@cgpsrv2.cis.mcmaster.ca>

Dear Ulrich,

I'll look into generalizing linear.hypothesis() so that it handles
multivariate linear models.

Meanwhile, vcov(x.mlm) will give you the covariance matrix of the
coefficients, so you could construct your own test by ravelling
coef(x.mlm) into a vector.

I hope that this helps,
 John

On Tue, 18 Jul 2006 20:15:12 +0200
 Ulrich Keller <uhkeller at web.de> wrote:
> Hello,
> 
> suppose I have a multivariate multiple regression model such as the 
> following:
> 
>  > DF<-data.frame(x1=rep(c(0,1),each=50),x2=rep(c(0,1),50))
>  > tmp<-rnorm(100)
>  > DF$y1<-tmp+DF$x1*.5+DF$x2*.3+rnorm(100,0,.5)
>  > DF$y2<-tmp+DF$x1*.5+DF$x2*.7+rnorm(100,0,.5)
>  > x.mlm<-lm(cbind(y1,y2)~x1+x2,data=DF)
>  > coef(x.mlm)
>                     y1        y2
> (Intercept) 0.07800993 0.2303557
> x1          0.52936947 0.3728513
> x2          0.13853332 0.4604842
> 
> How can I test whether x1 and x2 respectively have the same effect on
> y1 
> and y2? In other words, how can I test if coef(x.mlm)[2,1] is 
> statistically equal to coef(x.mlm)[2,2] and coef(x.mlm)[3,1] to 
> coef(x.mlm)[3,2]? I looked at linear.hypothesis {car} and glh.test 
> {gmodels}, but these do not seem the apply to multivariate models.
> Thank you in advance,
> 
> Uli Keller
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/


From tlumley at u.washington.edu  Tue Jul 18 22:12:43 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 18 Jul 2006 13:12:43 -0700 (PDT)
Subject: [R] Question about summing to zero
In-Reply-To: <44BD362B.6050408@uncp.edu>
References: <44BD362B.6050408@uncp.edu>
Message-ID: <Pine.LNX.4.64.0607181245110.24347@homer21.u.washington.edu>

On Tue, 18 Jul 2006, Jim Frederick wrote:

> Hi,
>
> When I used
> x <- c(10,10,10,10,5,5,5,5)
> sum(x)
> x <- x - .Last.value/8
> the result was zero, as I expected.
> However, when I used
> x <- rnorm(101,0,10)
> sum(x)
> x <- x - .Last.value/101
> sum(x)
> I did not get zero, but  -2.664535e-15.  OK, that's fairly close to
> zero, but I tried to improve the figure.  Each time I repeated the last
> two commands, the sum got closer to zero.  The series seemed to converge
> on -1.30e-15, so changed the 101 to 50, then to 20, and then to 10.  I
> had sum(x) down to -5.551115e-17 before I quit.
>
> If R can store values of x which sum to -5.551115e-17, then why can't I
> get that value on the first try?  Is this a bug or just rounding error?
>

It's just rounding error.  The *relative* error is always a multiple of 
2^-52, or about 2e-16 but as sum(x) gets smaller a given relative error is 
a smaller absolute error.

Comparing values from doing this once or twice is not very reliable. To 
see how much variation there is from sample to sample, do something like

a<-replicate(10000, {x<-rnorm(101,0,10); y<-sum(x); sum(x-y/101)/.Machine$double.eps})

A histogram or qqplot shows an approximately Normal distribution, with 
mean close to zero and standard deviation about 70.  Zero error does 
happen: I got 20/10000 exactly zero.

There aren't any simple answers to exactly how big the error is for 
particular computations.  The exact answers are complicated and the simple 
answers are approximate. For example, in your case a reasonable upper 
bound for the error might be 102 computations multiplied by 10 as a 
typical value of the numbers, multiplied by .Machine$double.eps. This is 
much larger than the typical error, of course.  A crude estimate of the 
typical error might be sqrt(102)*10*.Machine$double.eps, treating the 
errors as a random walk, and this is not far off (though perhaps only 
coincidentally so).


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From kubovy at virginia.edu  Tue Jul 18 22:40:52 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Tue, 18 Jul 2006 16:40:52 -0400
Subject: [R] Extended example of R2HTML?
Message-ID: <0940CFC4-C3EE-425B-9E4A-D7B87E91E68C@virginia.edu>

Dear R-helpers,

Can someone point me to an extended example of use of R2HTML?
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From marsh at uri.edu  Tue Jul 18 23:11:32 2006
From: marsh at uri.edu (Marshall Feldman)
Date: Tue, 18 Jul 2006 17:11:32 -0400
Subject: [R] FW: Large datasets in R
In-Reply-To: <42bc98300607180753y56dfd453r4bb4a7f1d4037bd3@mail.gmail.com>
Message-ID: <004d01c6aaae$c6f78720$52db8083@MFELDMAN>

Well, SPSS used to claim that all its algorithms dealt with only one case at
a time and therefore that it could handle very large files. I suppose a
large correlation matrix could cause it problems.

	Marsh Feldmman

-----Original Message-----
From: Ritwik Sinha [mailto:ritwik.sinha at gmail.com] 
Sent: Tuesday, July 18, 2006 10:54 AM
To: Prof Brian Ripley
Cc: Marshall Feldman; r-help at stat.math.ethz.ch
Subject: Re: [R] FW: Large datasets in R

Hi,

I have a related question. How differently do other statistical
softwares handle large data?

The original post claims that 350 MB is fine on Stata. Some one
suggested S-Plus. I have heard people say that SAS can handle large
data sets. Why can others do it and R seem to have a problem? Don't
these softwares load the data onto RAM.

-- 
Ritwik Sinha
Graduate Student
Epidemiology and Biostatistics
Case Western Reserve University

http://darwin.cwru.edu/~rsinha


From water21water at yahoo.com  Tue Jul 18 23:23:23 2006
From: water21water at yahoo.com (junguo liu)
Date: Tue, 18 Jul 2006 14:23:23 -0700 (PDT)
Subject: [R] How to write a function in a graph
Message-ID: <20060718212323.73754.qmail@web31804.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060718/c1ded695/attachment.pl 

From tlumley at u.washington.edu  Tue Jul 18 23:35:13 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 18 Jul 2006 14:35:13 -0700 (PDT)
Subject: [R] How to write a function in a graph
In-Reply-To: <20060718212323.73754.qmail@web31804.mail.mud.yahoo.com>
References: <20060718212323.73754.qmail@web31804.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0607181435000.24347@homer21.u.washington.edu>

On Tue, 18 Jul 2006, junguo liu wrote:

> Dear R-ers,
>
>  I conducted a regression analysis, and then intended to add the 
> regression function (y=4.33+1.07x) in a graph. But the following code 
> can only give me a text like y=a+bx. Who can help me out? Thank you very 
> much in advance.

This is FAQ 7.13.

 	-thomas

>
>
>  CODE
>
>  # Read data
>  x <- c(1, 2, 3, 4, 5, 6, 7, 8, 9)
>  y <- c(6, 5, 8, 9, 11, 10, 11, 12, 15)
>  data01 <- data.frame(x, y)
>
>  # Regression analysis
>  res.lm.y <- nls(y~a+b*x, start=list(a=1, b=2),data=data01)
>
>  # Obtain parameters
>  a<- coef(res.lm.y)["a"]
>  b<- coef(res.lm.y)["b"]
>  a
>  b
>  #a=4.33
>  #b=1.07
>
>  # Plot the results
>  def.par <- par()
>  par(mfrow=c(1,1),xaxs="i",yaxs="i")
>  plot(data01$x,data01$y,main="Fit",xlab="x",ylab="y")
>  lines(data01$x,predict(res.lm.y))
>
>  #=======
>  text (6, 13, expression(y==a+b*x))
>  #=======
>
>  ## I intended to add text like y=4.33+1.07x
>  ## but the above code added y=a+bx
>
>
>
> Swiss Federal Institute for Environmental Science and Technology (EAWAG)
> Ueberlandstrasse 133
> P.O.Box 611
> CH-8600 Duebendorf
> Switzerland
> Phone: 0041-18235012
> Fax: 0041-18235375
>
> ---------------------------------
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From pinard at iro.umontreal.ca  Wed Jul 19 00:56:26 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Tue, 18 Jul 2006 18:56:26 -0400
Subject: [R] FW: Large datasets in R
In-Reply-To: <Pine.LNX.4.64.0607180802140.839@homer23.u.washington.edu>
References: <00db01c6aa6c$e765b4b0$52db8083@MFELDMAN>
	<Pine.LNX.4.64.0607181443050.20229@gannet.stats.ox.ac.uk>
	<42bc98300607180753y56dfd453r4bb4a7f1d4037bd3@mail.gmail.com>
	<Pine.LNX.4.64.0607180802140.839@homer23.u.washington.edu>
Message-ID: <20060718225626.GB5151@alcyon.progiciels-bpi.ca>

[Thomas Lumley]

>People have used R in this way, storing data in a database and reading it 
>as required. There are also some efforts to provide facilities to support 
>this sort of programming (such as the current project funded by Google 
>Summer of Code:  http://tolstoy.newcastle.edu.au/R/devel/06/05/5525.html). 

Interesting project indeed!  However, if R requires uses more swapping 
because arrays do not all fit in physical memory, crudely replacing 
swapping with database accesses is not necessarily going to buy
a drastic speed improvement: the paging gets done in user space instead 
of being done in the kernel.

Long ago, while working on CDC mainframes, astonishing at the time but 
tiny by nowadays standards, there was a program able to invert or do 
simplexes on very big matrices.  I do not remember the name of the 
program, and never studied it but superficially (I was in computer 
support for researchers, but not a researcher myself).  The program was 
documented as being extremely careful at organising accesses to rows and 
columns (or parts thereof) in such a way that real memory was best used.
In other words, at the core of this program was a paging system very 
specialised and cooperative with the problems meant to be solved.

However, the source of this program was just plain huge (let's say from 
memory, about three or four times the size of the optimizing FORTRAN 
compiler, which I already knew better as an impressive algorithmic 
undertaking).  So, good or wrong, the prejudice stuck solidly in me at 
the time, if nothing else, that handling big arrays the right way, 
speed-wise, ought to be very difficult.

>One reason there isn't more of this is that relying on Moore's Law has 
>worked very well over the years.

On the other hand, the computational needs for scientific problems grow 
fairly quickly to the size of our ability to solve them.  Let me take
weather forecasting for example.  3-D geographical grids are never fine 
enough for the resolution meteorologists would like to get, and the time 
required for each prediction step grows very rapidly, to increase 
precision by not so much.  By merely tuning a few parameters, these 
people may easily pump nearly all the available cycles out the 
supercomputers given to them, and they do so without hesitation.  
Moore's Law will never succeed at calming their starving hunger! :-).

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From gregor.gorjanc at gmail.com  Wed Jul 19 01:16:17 2006
From: gregor.gorjanc at gmail.com (Gregor Gorjanc)
Date: Wed, 19 Jul 2006 01:16:17 +0200
Subject: [R] Plot fit of a "generic" function
Message-ID: <44BD6BC1.50902@bfro.uni-lj.si>

Hello!

Say I have a function, which creates a design matrix i.e.

myFunc <- function(x)
{
  ret <- cbind(x, x*x, x*x*x)
  colnames(ret) <- 1:ncol(ret)
  return(ret)
}

n <- 200
x <- runif(n=n, min=0, max=100)
y <- myFunc(x) %*% c(1, 0.2, -0.0002) +  rnorm(n=n, sd=100)

then I can use this in formulae as here

(fit <- lm(y ~ myFunc(x)))

Now I would like to plot data and fitted function on the plot, but I do
not want to access each parameter estimate from object "fit" i.e. I
would like to use something similar to abline for linear regression but
in a generic way. Is there anything similar to my case?

plot(y=y, x=x)

???plotMyFunc???

Thanks!

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From gunter.berton at gene.com  Wed Jul 19 01:19:43 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 18 Jul 2006 16:19:43 -0700
Subject: [R] FW: Large datasets in R
In-Reply-To: <20060718225626.GB5151@alcyon.progiciels-bpi.ca>
Message-ID: <006901c6aac0$a7a4dc80$725afea9@gne.windows.gene.com>

Or, more succinctly, "Pinard's Law":

The demands of ever more data always exceed the capabilities of ever better
hardware.

;-D

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
  

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Fran?ois Pinard
> Sent: Tuesday, July 18, 2006 3:56 PM
> To: Thomas Lumley
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] FW: Large datasets in R
> 
> [Thomas Lumley]
> 
> >People have used R in this way, storing data in a database 
> and reading it 
> >as required. There are also some efforts to provide 
> facilities to support 
> >this sort of programming (such as the current project funded 
> by Google 
> >Summer of Code:  
> http://tolstoy.newcastle.edu.au/R/devel/06/05/5525.html). 
> 
> Interesting project indeed!  However, if R requires uses more 
> swapping 
> because arrays do not all fit in physical memory, crudely replacing 
> swapping with database accesses is not necessarily going to buy
> a drastic speed improvement: the paging gets done in user 
> space instead 
> of being done in the kernel.
> 
> Long ago, while working on CDC mainframes, astonishing at the 
> time but 
> tiny by nowadays standards, there was a program able to invert or do 
> simplexes on very big matrices.  I do not remember the name of the 
> program, and never studied it but superficially (I was in computer 
> support for researchers, but not a researcher myself).  The 
> program was 
> documented as being extremely careful at organising accesses 
> to rows and 
> columns (or parts thereof) in such a way that real memory was 
> best used.
> In other words, at the core of this program was a paging system very 
> specialised and cooperative with the problems meant to be solved.
> 
> However, the source of this program was just plain huge 
> (let's say from 
> memory, about three or four times the size of the optimizing FORTRAN 
> compiler, which I already knew better as an impressive algorithmic 
> undertaking).  So, good or wrong, the prejudice stuck solidly 
> in me at 
> the time, if nothing else, that handling big arrays the right way, 
> speed-wise, ought to be very difficult.
> 
> >One reason there isn't more of this is that relying on 
> Moore's Law has 
> >worked very well over the years.
> 
> On the other hand, the computational needs for scientific 
> problems grow 
> fairly quickly to the size of our ability to solve them.  Let me take
> weather forecasting for example.  3-D geographical grids are 
> never fine 
> enough for the resolution meteorologists would like to get, 
> and the time 
> required for each prediction step grows very rapidly, to increase 
> precision by not so much.  By merely tuning a few parameters, these 
> people may easily pump nearly all the available cycles out the 
> supercomputers given to them, and they do so without hesitation.  
> Moore's Law will never succeed at calming their starving hunger! :-).
> 
> -- 
> Fran?ois Pinard   http://pinard.progiciels-bpi.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Wed Jul 19 02:19:18 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 18 Jul 2006 20:19:18 -0400
Subject: [R] Plot fit of a "generic" function
In-Reply-To: <44BD6BC1.50902@bfro.uni-lj.si>
References: <44BD6BC1.50902@bfro.uni-lj.si>
Message-ID: <971536df0607181719s7bfd3b4aqe983b50f7983f9d0@mail.gmail.com>

You could plot y vs. fitted(y.lm) where y.lm is the output of lm or
plot both y and fitted(y.lm) against x on the same chart.


On 7/18/06, Gregor Gorjanc <gregor.gorjanc at gmail.com> wrote:
> Hello!
>
> Say I have a function, which creates a design matrix i.e.
>
> myFunc <- function(x)
> {
>  ret <- cbind(x, x*x, x*x*x)
>  colnames(ret) <- 1:ncol(ret)
>  return(ret)
> }
>
> n <- 200
> x <- runif(n=n, min=0, max=100)
> y <- myFunc(x) %*% c(1, 0.2, -0.0002) +  rnorm(n=n, sd=100)
>
> then I can use this in formulae as here
>
> (fit <- lm(y ~ myFunc(x)))
>
> Now I would like to plot data and fitted function on the plot, but I do
> not want to access each parameter estimate from object "fit" i.e. I
> would like to use something similar to abline for linear regression but
> in a generic way. Is there anything similar to my case?
>
> plot(y=y, x=x)
>
> ???plotMyFunc???
>
> Thanks!
>
> --
> Lep pozdrav / With regards,
>    Gregor Gorjanc
>
> ----------------------------------------------------------------------
> University of Ljubljana     PhD student
> Biotechnical Faculty
> Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si
>
> SI-1230 Domzale             tel: +386 (0)1 72 17 861
> Slovenia, Europe            fax: +386 (0)1 72 17 888
>
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From MCastalanelli at agric.wa.gov.au  Wed Jul 19 03:08:23 2006
From: MCastalanelli at agric.wa.gov.au (Castalanelli, Mark)
Date: Wed, 19 Jul 2006 09:08:23 +0800
Subject: [R] JavaGD
Message-ID: <5E1F237DF7ECD7118ACC000629390EFB05814F6A@agspsrv08.agric.wa.gov.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060719/cde52110/attachment.pl 

From A.Robinson at ms.unimelb.edu.au  Wed Jul 19 03:51:44 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 19 Jul 2006 11:51:44 +1000
Subject: [R] Test for equality of coefficients in multivariate multiple
	regression
In-Reply-To: <44BD2530.2020502@web.de>
References: <44BD2530.2020502@web.de>
Message-ID: <20060719015144.GA40766@ms.unimelb.edu.au>

Hi Uli,

I suggest that you try to rewrite the model system into a single
mixed-effects model, which would allow direct parameterization of the
tests that you're interested in.  A useful article, which may be
overkill for your needs, is:

Hall, D.B. and Clutter, M. (2004). Multivariate multilevel nonlinear
mixed effects models for timber yield predictions,  Biometrics, 60:
16-24.

See the publications link on Daniel Hall's website:

http://www.stat.uga.edu/~dhall/

Cheers

Andrew

On Tue, Jul 18, 2006 at 08:15:12PM +0200, Ulrich Keller wrote:
> Hello,
> 
> suppose I have a multivariate multiple regression model such as the 
> following:
> 
>  > DF<-data.frame(x1=rep(c(0,1),each=50),x2=rep(c(0,1),50))
>  > tmp<-rnorm(100)
>  > DF$y1<-tmp+DF$x1*.5+DF$x2*.3+rnorm(100,0,.5)
>  > DF$y2<-tmp+DF$x1*.5+DF$x2*.7+rnorm(100,0,.5)
>  > x.mlm<-lm(cbind(y1,y2)~x1+x2,data=DF)
>  > coef(x.mlm)
>                     y1        y2
> (Intercept) 0.07800993 0.2303557
> x1          0.52936947 0.3728513
> x2          0.13853332 0.4604842
> 
> How can I test whether x1 and x2 respectively have the same effect on y1 
> and y2? In other words, how can I test if coef(x.mlm)[2,1] is 
> statistically equal to coef(x.mlm)[2,2] and coef(x.mlm)[3,1] to 
> coef(x.mlm)[3,2]? I looked at linear.hypothesis {car} and glh.test 
> {gmodels}, but these do not seem the apply to multivariate models.
> Thank you in advance,
> 
> Uli Keller
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From Max.Kuhn at pfizer.com  Wed Jul 19 05:09:03 2006
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Tue, 18 Jul 2006 23:09:03 -0400
Subject: [R] [R-pkgs] odfWeave Package
Message-ID: <71257D09F114DA4A8E134DEAC70F25D305A521F5@groamrexm03.amer.pfizer.com>

I've been meaning send an announcement for this package, but Greg Snow
beat me to the punch today.

Max

<snip>

The odfWeave package is now available on CRAN at

http://lib.stat.cmu.edu/R/CRAN/src/contrib/Descriptions/odfWeave.html

and your local mirror.

The package extends Sweave to Open Document Format (ODF) text document 
files. Latex-style code chunks and in-line Sexpr commands can be used 
to embed R output into ODF file, which can then be exported to doc, 
rtf, html, pdf and other formats.

Other functions in the package facilitate ODF formatted tables, text 
and lists.

The current limitations of the package are:

 O tangling is not yet implement (but will be)

 O testing has been done on text documents generated by OpenOffice. 
   Other formats/editors may work, but are not (yet) supported.

Please send comments, suggestions and/or contributions.

Max Kuhn
Research Statistics
Pfizer Global R&D
Max.Kuhn at pfizer.com
----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From spencer.graves at pdf.com  Wed Jul 19 05:38:17 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 18 Jul 2006 20:38:17 -0700
Subject: [R] How to find S4 generics? (was: inames() function and lmer())
In-Reply-To: <878be2c60607150822r5914a231h41d537f2f5b446b4@mail.gmail.com>
References: <878be2c60607150822r5914a231h41d537f2f5b446b4@mail.gmail.com>
Message-ID: <44BDA929.8000407@pdf.com>

*****
***** "methods" *****
*****
	  You have asked an excellent question.  I can provide a partial answer 
below.  First, however, I wish to pose a question of my own, which could 
help answer your question:

	  How can one obtain a simple list of the available generics for a 
class?  For an S3 class, the 'methods' functions provide that.  What 
about an S4 class?  That's entirely opaque to me, if I somehow can't 
find the relevant information in other ways.  For example, ?lmer-class 
lists many but not all of the methods available for objects of class 
'lmer'.  I think I once found a way to get that, but I'm not able to 
find documentation on it now.

*****
***** Retrieving information from an 'lmer' object *****
*****
	  There are many ways to retrieve information from an object.  For 
'lmer', I think the best source of information is probably the help 
pages for 'lmer' and 'lmer-class'.  The latter tells you that there are 
"Methods" anova, coef, coerce, deviance, logLik, update, simulate, 
solve, terms, vcov, ranef and VarCorr for extracting information or 
modifying an object of class 'lmer'.  There are also other methods not 
listed on that page like 'fixef'.  I don't know how to find those easily.

	  There is also the excellent vignette("MlmSoftRev") in the 'mlmRev' 
package.  (For help with it, see, e.g., 
"http://finzi.psych.upenn.edu/R/Rhelp02a/archive/76134.html".)

	  Finally, the function 'str' will in most cases expose the structure 
of an object, thereby making it relatively easy to figure out how to get 
what is needed for many applications.  That does not always work, 
however, because 'str' is a generic function, not a primitive.  'str' 
seems to expose everything one might possibly want to know about an 
'lmer' object (apart from the theory behind it).  This is not the case 
for an object of class 'logLik', because a function 'str.logLik' 
provides a succinct summary of a 'logLik' object in a non-standard 
format.  The following will defeat 'str.logLik':

 > fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
 > lglk1 <- logLik(fm1)
 > str(lglk1)
Class 'logLik' : -871.8 (df=5) # I don't understand this.
 > str(unclass(lglk1)) # This is in the standard 'str' format:
  atomic [1:1] -872
  - attr(*, "nobs")= int 180
  - attr(*, "nall")= int 180
  - attr(*, "df")= num 5
  - attr(*, "REML")= logi TRUE

	  Best Wishes,
	  Spencer Graves

A.R. Criswell wrote:
> Hello All,
> 
> I would like to retrieve some of the results from the lmer(...)
> function in library lme4. If I run a model, say
> 
> fm.1 <- lmer(y ~ 1 + (1 | x), data = dog)
> 
> and try names(fm.1), I get NULL. Is there anyway to retrieve the information?
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From binabina at bellsouth.net  Wed Jul 19 05:46:03 2006
From: binabina at bellsouth.net (zubin)
Date: Tue, 18 Jul 2006 23:46:03 -0400
Subject: [R] voronoi tessellations
In-Reply-To: <44A70DB2.9080106@bellsouth.net>
References: <446D1C5F.9060002@bellsouth.net> <4498072C.5010600@bellsouth.net>
	<449DA9F1.90700@bellsouth.net> <44A70DB2.9080106@bellsouth.net>
Message-ID: <44BDAAFB.4050306@bellsouth.net>

Hello, looking to draw a voronoi tessellations in R - can anyone 
recommend a package that has tackled this?

some background:

i have a economic data set and created a sammons projection, like to now 
overlay a voronoi tessellation over the sammons 2-D solution for a slick 
visual, and potentially color each tessellation element based on a metric.

home.u <- unique(home1)
home.dist <- dist(home.u)
home.sam <- sammon(home.dist,k=2)
plot(home.sam$points)


From ghos0033 at umn.edu  Wed Jul 19 06:17:50 2006
From: ghos0033 at umn.edu (Debarchana Ghosh)
Date: Wed, 19 Jul 2006 00:17:50 -0400
Subject: [R] Problem with ordered logistic regression using polr function.
Message-ID: <44BDB26E.1050106@umn.edu>

Hi,
I'm trying to fit a ordered logistic regression. The response variable 
(y) has three levels (0,1,2).
The command I've used is:

/ordlog<-polr(y~x1+x2+x3+x4, data=finalbase, subset=heard, weight=wt, 
na.action=na.omit)
/
(There are no NA's in y but there are NA's in X's)

The error I'm getting is:
/Warning messages:
1: non-integer #successes in a binomial glm! in: eval(expr, envir, enclos)
2: design appears to be rank-deficient, so dropping some coefs in: 
polr(right.ans ~ S107 + children + work + rel + media + V013 +
/
Also, if I write
/summary(ordlog)/

I'm getting the following error:

/Re-fitting to get Hessian

Error in if (all(pr > 0)) -sum(wt * log(pr)) else Inf :
        missing value where TRUE/FALSE needed

/Could anybody point out the problem?

Thanks,
Debarchana.

-- 
Debarchana Ghosh
Research Assistant
Department of Geography
University of Minnesota
PH: 8143607580
email to: ghos0033 at umn.edu
www.tc.umn.edu/~ghos0033


From ghos0033 at umn.edu  Wed Jul 19 06:20:13 2006
From: ghos0033 at umn.edu (Debarchana Ghosh)
Date: Wed, 19 Jul 2006 00:20:13 -0400
Subject: [R] Problem with ordered logistic regression using polr function
Message-ID: <44BDB2FD.6070105@umn.edu>

Hi,
I'm trying to fit a ordered logistic regression. The response variable 
(y) has three levels (0,1,2).
The command I've used is:

ordlog<-polr(y~x1+x2+x3+x4, data=finalbase, subset=heard, weight=wt, 
na.action=na.omit)

(There are no NA's in y but there are NA's in X's)

The error I'm getting is:
Warning messages:
1: non-integer #successes in a binomial glm! in: eval(expr, envir, enclos)
2: design appears to be rank-deficient, so dropping some coefs in: 
polr(right.ans ~ S107 + children + work + rel + media + V013 +

Also, if I write
summary(ordlog)

I'm getting the following error:

Re-fitting to get Hessian

Error in if (all(pr > 0)) -sum(wt * log(pr)) else Inf :
       missing value where TRUE/FALSE needed

Could anybody point out the problem?

Thanks,
Debarchana.

-- 
Debarchana Ghosh
Research Assistant
Department of Geography
University of Minnesota
PH: 8143607580
email to: ghos0033 at umn.edu
www.tc.umn.edu/~ghos0033


From macq at llnl.gov  Wed Jul 19 06:52:09 2006
From: macq at llnl.gov (Don MacQueen)
Date: Tue, 18 Jul 2006 21:52:09 -0700
Subject: [R] voronoi tessellations
In-Reply-To: <44BDAAFB.4050306@bellsouth.net>
References: <446D1C5F.9060002@bellsouth.net>
	<4498072C.5010600@bellsouth.net>	<449DA9F1.90700@bellsouth.net>
	<44A70DB2.9080106@bellsouth.net> <44BDAAFB.4050306@bellsouth.net>
Message-ID: <p06230902c0e369bc9225@[192.168.2.2]>

I'll suggest going to the CRAN packages page and doing a search for "voronoi".
Also, search for 'triangulation', since that is one of the uses of them.

-Don

At 11:46 PM -0400 7/18/06, zubin wrote:
>Hello, looking to draw a voronoi tessellations in R - can anyone
>recommend a package that has tackled this?
>
>some background:
>
>i have a economic data set and created a sammons projection, like to now
>overlay a voronoi tessellation over the sammons 2-D solution for a slick
>visual, and potentially color each tessellation element based on a metric.
>
>home.u <- unique(home1)
>home.dist <- dist(home.u)
>home.sam <- sammon(home.dist,k=2)
>plot(home.sam$points)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


-- 
---------------------------------
Don MacQueen
Lawrence Livermore National Laboratory
Livermore, CA, USA


From manojsw at gmail.com  Wed Jul 19 07:01:04 2006
From: manojsw at gmail.com (Manoj)
Date: Wed, 19 Jul 2006 14:01:04 +0900
Subject: [R] conditional plot
Message-ID: <829e6c8a0607182201l1ceb3ac0s2a5315b90658ab14@mail.gmail.com>

Hi,
     Can anyone pls help me in plotting the following data?

     The data-set contains company name, specification-1, specification-2.

     The graph would basically plot company name with specification-1
on x-axis, & specification-2 on y-axis.

     Simply put - company name should get classified into one of the
four quardrants created by specification 1 & specification2.

Thanks.

Manoj


From spencer.graves at pdf.com  Wed Jul 19 07:04:41 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 18 Jul 2006 22:04:41 -0700
Subject: [R] ts and stl functions - still a problem
In-Reply-To: <20060717140547.59468.qmail@web26902.mail.ukl.yahoo.com>
References: <20060717140547.59468.qmail@web26902.mail.ukl.yahoo.com>
Message-ID: <44BDBD69.3010203@pdf.com>

	  Have you tried the following:

tstkr <- ts(tkr[, 1], deltat=1/12)

	  I had previously thought that 'tkr' was a numeric matrix;  if that 
had been the case, as.numeric(tkr) would have converted it to a numeric 
vector.  You've now said that class(tkr) = 'data.frame' of dim = c(131, 
1).  If this is the case, then tkr[, 1] should be a (numeric) vector.

	  This should solve your problem.  However, I encourage you to spend 
more time studying the similarities and differences between vectors, 
matrices and data.frames, especially the examples in the ?matrix and 
?data.frame help pages, plus the corresponding sections of "An 
Introduction to R", the first of the "Manuals" available via 
'help.start()'.

	  Hope this helps.
	  Spencer Graves
	
Daniel sutcliffe wrote:
> I am still having a problem with my coercing my data frame (tkr) into a time series using ts  to use the stl function.
>    
>   When I read the data into R and do dim tkr the result is 132 1, when I type class I get (data.frame).
>    
>   I have tried several methods so far to to coerce it into a a time series and use the stl function,( see thread) .
>    
>   A suggestion was to  use the following:
>    
>   tstkr <- ts(as.numeric(tkr), deltat=1/12)
>    
>   before using the stl function, however I now get this error message:
>    
>   Error in as.double.default(tkr) : (list) object cannot be coerced to 'double'
>    
>   I don't know what this means....does anybody have any other ideas...? Hope I have put enough information on here for people to be able to help..if not let me know, also if anyone wants to send me to email them my data then I am happy to do so..
>    
>   Thanks to everyone who has tried to help so far much appreciated...
> 
> Spencer Graves <spencer.graves at pdf.com> wrote:
>   The 'ts' function retains the 'dim' attribute of 'tkr'. When 'stl' 
> finds this 'dim' attribute, it thinks 'tstkr' is a multivariate time 
> series. This causes it to stop with the error, "only univariate series 
> allowed".
> 
> Consider the following modification of an example from the 'stl' help 
> file:
> 
>> not.1 <- stl(nottem, "per")
>> Nottem <- ts(array(nottem, dim=c(240, 1)))
>> Not.1 <- stl(Nottem)
> Error in stl(Nottem) : only univariate series are allowed
> 
> Solution:
> 
> tstkr <- ts(as.numeric(tkr), deltat=1/12)
> 
> After converting tkr and tstkr from a matrix to a vector like this, 
> please try 'stl(tstkr)'. If it doesn't work, please submit another post.
> 
> Hope this helps.
> Spencer Graves
> p.s. Your example was not quite self-contained, because I didn't know 
> for sure the format, class, and attributes of your 'tkr' object. The 
> absence of these details made it harder for me (and, I believe, anyone 
> else) to reply. You might get better replies quicker with greater 
> attention to such details.
> 
> Daniel sutcliffe wrote:
>> Hi
>>
>> I am still having problems with using the stl 
> function, when I read the csv file into R into a
> file called tkr and use dim(tkr) the result is 132 x 1
> which is fine.
>> When coerce it into a trime series using ts either:
>>
>>
>> tstkr <- ts(t(tkr), deltat=1/12) or
>>
>> tstkr <- ts(c(tkr), deltat=1/12) 
>>
>> and use the stl function I get the following error:
>>
>> Error in stl(tstkr) : only univariate series are allowed
>>
>> id just use the tkr file I get the same error..does anyibe have an idea what to do next, here is my data...it's not sensitive so if anyone wants to try then you are very welcome!
>> Rate 184.0222 180.517 222.5792 173.5066 192.7852 198.0429 182.2696 189.28 178.7644 206.8059 236.6 155.9807 231.7314 249.2868 222.9537 198.3761 201.8872 208.9094 242.2646 221.1982 228.2203 245.7757 244.0202 194.865 239.3664 234.0862 251.6867 235.8463 197.1253 237.6063 267.5271 228.8061 241.1264 249.9267 256.9669 188.325 258.8788 239.5069 214.8518 234.2237 211.3296 234.2237 237.7458 156.7361 239.5069 225.4183 257.1177 170.8248 230.1611 265.3001 296.9253 193.265 233.675 240.7028 249.4876 205.5637 237.1889 237.1889 289.8975 245.9736 283.1755 316.3875 372.3234 234.2316 263.9476 314.6395 286.6715 272.6875 323.3795 295.4115 300.6555 174.7997 227.184 277.4767 317.364 234.121 286.1478 279.2109 280.9452 235.8552 319.0982 
>> 296.5532 303.4901 173.4229 302.6072 312.8651 389.7991 223.9635 254.7371 329.9615 288.93 317.994 312.8651 381.2509 333.3808 194.8996 285.5743 304.0526 335.9698 307.4123 346.0489 288.934 335.9698 243.5781 384.6854 359.4876 357.8078 221.74 336.3712 344.6562 389.3952 286.6611 338.0282 407.6222 356.2552 241.9221 347.9702 400.9942 381.1102 304.8882 383.9077 418.5089 476.1774 331.1822 378.9647 375.6694 339.4206 364.1357 420.1565 446.5193 410.2705 296.5811
>>
>> Cheers and thanks to everyone who offered suggestions before.
>>
>> Daniel
>>
>>
>>
>>
>>
>> SAULEAU Erik-Andr? wrote:
>> Perhaps ts(t(tkr))?
>>
>>> -----Message d'origine-----
>>> De : Daniel sutcliffe [mailto:dnlsutcliffe at yahoo.co.uk] 
>>> Envoy? : mercredi 12 juillet 2006 15:53
>>> ? : r-help at stat.math.ethz.ch
>>> Objet : [R] ts and stl functions
>>>
>>>
>>> Hi, 
>>>
>>> I have imported a csv file into R which contains one column 
>>> (the rate er 100,000 population of a disease, by month over 
>>> 11 years) I coerced into a time series using the following function, 
>>>
>>> tstkr<-ts(tkr,deltat=1/12) 
>>>
>>> This seems to work fine, and when I check for the class of 
>>> the object using class(tstkr) I get "ts" as the response. 
>>>
>>> When I try to use the stl function in stats I get the error message: 
>>>
>>> Error in stl(tstkr)only univariate series are allowed 
>>>
>>> I then tried this: 
>>>
>>> tstkr <- ts(c(tkr), deltat=1/12) 
>>>
>>> however this made no difference...I still get an error - does 
>>> anybody know what is wrong? 
>>>
>>> Regards, 
>>>
>>> Daniel 
>>>
>>>
>>> ---------------------------------
>>>
>>> [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list 
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read 
>>> the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>>
>>> **************************************************************
>>> ********************
>>> Afin d'eviter toute propagation de virus informatique, et en 
>>> complement 
>>> des dispositifs en place, ce message (et ses pieces jointes 
>>> s'il y en a) 
>>> a ete automatiquement analyse par un antivirus de messagerie. 
>>> **************************************************************
>>> ********************
>>>
>>
>> **********************************************************************************
>> Afin d'eviter toute propagation de virus informatique, et en complement 
>> des dispositifs en place, ce message (et ses pieces jointes s'il y en a) 
>> a ete automatiquement analyse par un antivirus de messagerie. 
>> **********************************************************************************
>>
>>
>>
>>
>> ---------------------------------
>>
>> [[alternative HTML version deleted]]
>>
>>
>>
>> ------------------------------------------------------------------------
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


From gregor.gorjanc at bfro.uni-lj.si  Wed Jul 19 08:27:07 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Wed, 19 Jul 2006 08:27:07 +0200
Subject: [R] Plot fit of a "generic" function
In-Reply-To: <971536df0607181719s7bfd3b4aqe983b50f7983f9d0@mail.gmail.com>
References: <44BD6BC1.50902@bfro.uni-lj.si>
	<971536df0607181719s7bfd3b4aqe983b50f7983f9d0@mail.gmail.com>
Message-ID: <44BDD0BB.8090508@bfro.uni-lj.si>

Yes, that will do.

Thank you Gabor!

Gabor Grothendieck wrote:
> You could plot y vs. fitted(y.lm) where y.lm is the output of lm or
> plot both y and fitted(y.lm) against x on the same chart.
> 
> 
> On 7/18/06, Gregor Gorjanc <gregor.gorjanc at gmail.com> wrote:
>> Hello!
>>
>> Say I have a function, which creates a design matrix i.e.
>>
>> myFunc <- function(x)
>> {
>>  ret <- cbind(x, x*x, x*x*x)
>>  colnames(ret) <- 1:ncol(ret)
>>  return(ret)
>> }
>>
>> n <- 200
>> x <- runif(n=n, min=0, max=100)
>> y <- myFunc(x) %*% c(1, 0.2, -0.0002) +  rnorm(n=n, sd=100)
>>
>> then I can use this in formulae as here
>>
>> (fit <- lm(y ~ myFunc(x)))
>>
>> Now I would like to plot data and fitted function on the plot, but I do
>> not want to access each parameter estimate from object "fit" i.e. I
>> would like to use something similar to abline for linear regression but
>> in a generic way. Is there anything similar to my case?
>>
>> plot(y=y, x=x)
>>
>> ???plotMyFunc???
>>
>> Thanks!
>>
>> -- 
>> Lep pozdrav / With regards,
>>    Gregor Gorjanc
>>
>> ----------------------------------------------------------------------
>> University of Ljubljana     PhD student
>> Biotechnical Faculty
>> Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
>> Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si
>>
>> SI-1230 Domzale             tel: +386 (0)1 72 17 861
>> Slovenia, Europe            fax: +386 (0)1 72 17 888
>>
>> ----------------------------------------------------------------------
>> "One must learn by doing the thing; for though you think you know it,
>>  you have no certainty until you try." Sophocles ~ 450 B.C.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From vito_ricci at yahoo.com  Wed Jul 19 08:37:19 2006
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 19 Jul 2006 08:37:19 +0200 (CEST)
Subject: [R] R & Visual Basic
Message-ID: <20060719063719.85732.qmail@web36115.mail.mud.yahoo.com>

Der R-UserRs,

I need a little help, I wish to know if exists a way
to use R in Visual Basic environment, in creation of
VB applications, embedding R in VB. Is it possible?
Can anyone help me? Is there something such as books,
articles, other available on the web?
Thanks in advance.
Regards.
Vito

Se non ora, quando?
Se non qui, dove?
Se non tu, chi?

Personal Web Space: http://vr71.spaces.msn.com/

Chiacchiera con i tuoi amici in tempo reale! 
 http://it.yahoo.com/mail_it/foot/*http://it.messenger.yahoo.com


From ripley at stats.ox.ac.uk  Wed Jul 19 08:59:31 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 19 Jul 2006 07:59:31 +0100 (BST)
Subject: [R] How to find S4 generics? (was: inames() function and lmer())
In-Reply-To: <44BDA929.8000407@pdf.com>
References: <878be2c60607150822r5914a231h41d537f2f5b446b4@mail.gmail.com>
	<44BDA929.8000407@pdf.com>
Message-ID: <Pine.LNX.4.64.0607190739540.32733@gannet.stats.ox.ac.uk>

On Tue, 18 Jul 2006, Spencer Graves wrote:

> *****
> ***** "methods" *****
> *****
> 	  You have asked an excellent question.  I can provide a partial answer 
> below.  First, however, I wish to pose a question of my own, which could 
> help answer your question:
> 
> 	  How can one obtain a simple list of the available generics for a 
> class?  For an S3 class, the 'methods' functions provide that.  What 
> about an S4 class?  That's entirely opaque to me, if I somehow can't 
> find the relevant information in other ways.  For example, ?lmer-class 
> lists many but not all of the methods available for objects of class 
> 'lmer'.  I think I once found a way to get that, but I'm not able to 
> find documentation on it now.

It doesn't work the same way.  S3 generics are defined on a single 
argument and hence have methods for a class, and so it is relevant to ask 
what generics there are which have methods for a given class - but even 
then there can be other generics and other methods which dispatch on 
object from that class by inheritance (e.g. on "lm" for "glm" objects).

S4 generics dispatch on a signature which can involve two or more classes, 
and I guess the simplest interpretation of your question is

`what S4 generics are there which have methods with signatures mentioning 
this class'.

Given the decentralized way such information is stored, I think the only 
way to do that is to find all the generics currently available (via 
getGenerics or its synonym allGenerics) and then call showMethods on each 
generic.  In particular, methods are stored in the S4 generic and not in 
the package defining the method.

However, I suspect inheritance is much more important here, and there is 
no way to know if methods for class "ANY" actually work for a specific S4 
class.

[...]


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From r_o_sanchez2003 at yahoo.com.ar  Tue Jul 18 17:25:47 2006
From: r_o_sanchez2003 at yahoo.com.ar (raul sanchez)
Date: Tue, 18 Jul 2006 12:25:47 -0300 (ART)
Subject: [R] how can I delete rows?
Message-ID: <20060718152547.71330.qmail@web30615.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060718/13c3a167/attachment.pl 

From anthony.staines at gmail.com  Wed Jul 19 00:22:01 2006
From: anthony.staines at gmail.com (Anthony Staines)
Date: Tue, 18 Jul 2006 23:22:01 +0100
Subject: [R] Classification error rate increased by bagging - any ideas?
Message-ID: <6d3975af0607181522h615ec128ue2fe22d5fbd55ab9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060718/6ef390db/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Jul 19 09:15:51 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 19 Jul 2006 08:15:51 +0100 (BST)
Subject: [R] voronoi tessellations
In-Reply-To: <p06230902c0e369bc9225@[192.168.2.2]>
References: <446D1C5F.9060002@bellsouth.net> <4498072C.5010600@bellsouth.net>
	<449DA9F1.90700@bellsouth.net> <44A70DB2.9080106@bellsouth.net>
	<44BDAAFB.4050306@bellsouth.net> <p06230902c0e369bc9225@[192.168.2.2]>
Message-ID: <Pine.LNX.4.64.0607190800470.32733@gannet.stats.ox.ac.uk>

On Tue, 18 Jul 2006, Don MacQueen wrote:

> I'll suggest going to the CRAN packages page and doing a search for "voronoi".

The problem here is that `Voronoi tessellation' is a secondary name.  The 
concept has many names, including Dirichlet tessellation and Thiessen 
polygons, and Dirichlet has priority over Voronoi.

> Also, search for 'triangulation', since that is one of the uses of them.

I know of packages deldir, tripack and perhaps geometry.

> -Don
> 
> At 11:46 PM -0400 7/18/06, zubin wrote:
> >Hello, looking to draw a voronoi tessellations in R - can anyone
> >recommend a package that has tackled this?
> >
> >some background:
> >
> >i have a economic data set and created a sammons projection, like to now
> >overlay a voronoi tessellation over the sammons 2-D solution for a slick
> >visual, and potentially color each tessellation element based on a metric.
> >
> >home.u <- unique(home1)
> >home.dist <- dist(home.u)
> >home.sam <- sammon(home.dist,k=2)
> >plot(home.sam$points)

Wait a minute.  If this is sammon() from MASS (uncredited), it is not a 
projection, and there is no relevant concept of distance between points in 
the mapped space apart from between the supplied points.

I suggest Zubin reads carefully the reference whose support software he 
appears to be using.  (It would also have answered his question.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Jul 19 09:19:03 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 19 Jul 2006 08:19:03 +0100 (BST)
Subject: [R] R & Visual Basic
In-Reply-To: <20060719063719.85732.qmail@web36115.mail.mud.yahoo.com>
References: <20060719063719.85732.qmail@web36115.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0607190816400.32733@gannet.stats.ox.ac.uk>

On Wed, 19 Jul 2006, Vito Ricci wrote:

> Der R-UserRs,
> 
> I need a little help, I wish to know if exists a way
> to use R in Visual Basic environment, in creation of
> VB applications, embedding R in VB. Is it possible?
> Can anyone help me? Is there something such as books,
> articles, other available on the web?

See the rw-FAQ Q2.18 and the projects it points you to.

> Thanks in advance.
> Regards.
> Vito


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Max.Kuhn at pfizer.com  Wed Jul 19 04:58:21 2006
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Tue, 18 Jul 2006 22:58:21 -0400
Subject: [R] [R-pkgs] odfWeave Package
Message-ID: <71257D09F114DA4A8E134DEAC70F25D305A521F2@groamrexm03.amer.pfizer.com>

The odfWeave package is now available on CRAN at

http://lib.stat.cmu.edu/R/CRAN/src/contrib/Descriptions/odfWeave.html

and your local mirror.

The package extends Sweave to Open Document Format (ODF) text document 
files. Latex-style code chunks and in-line Sexpr commands can be used 
to embed R output into ODF file, which can then be exported to doc, 
rtf, html, pdf and other formats.

Other functions in the package facilitate ODF formatted tables, text 
and lists.

The current limitations of the package are:

 O tangling is not yet implement (but will be)

 O testing has been done on text documents generated by OpenOffice. 
   Other formats/editors may work, but are not (yet) supported.

Please send comments, suggestions and/or contributions.

Max Kuhn
Research Statistics
Pfizer Global R&D
Max.Kuhn at pfizer.com


----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From philipp.pagel.lists at t-online.de  Wed Jul 19 10:01:16 2006
From: philipp.pagel.lists at t-online.de (Philipp Pagel)
Date: Wed, 19 Jul 2006 10:01:16 +0200
Subject: [R] how can I delete rows?
In-Reply-To: <20060718152547.71330.qmail@web30615.mail.mud.yahoo.com>
References: <20060718152547.71330.qmail@web30615.mail.mud.yahoo.com>
Message-ID: <20060719080116.GB4691@gsf.de>

On Tue, Jul 18, 2006 at 12:25:47PM -0300, raul sanchez wrote:
>   I have the Barro-Lee data set which contains 98 countries and I want
>   to run the regressions only for the Latin America countries, so what
>   do you recomend? How can I delete all the other countries or how can
>   I select the countries of Lat. Am. thank you

>   this is the list of countries
>   SHCODE     COUNTRY  NAME           WBCTRY       (1)       (2)
> _________________________________________________________________
> 
>   1        Algeria                  DZA          +         +
>   2        Angola                   AGO          -         -
>   3        Benin                    BEN          -         +
>   4        Botswana                 BWA          +         +
>   5        Burkina Faso             HVO          -         -
>   6        Burundi                  BDI          +         -
>   7        Cameroon                 CMR          +         +
[...]


Assuming the data is stored in a data frame called bl you could do
something like this:

First you generate a list of all countries you are interested in - e.g.

> set = c('Benin', 'Congo', 'Mali')


And then you subset the data frame with it

> bl[bl$COUNTRY %in% set, ]
   SHCODE COUNTRY NAME WBCTRY X.1. X.2.
3       3   Benin  BEN      -    +   NA
12     12   Congo  COG      -    +   NA
26     26    Mali  MLI      -    +   NA


cu
	Philipp

-- 
Dr. Philipp Pagel                            Tel.  +49-8161-71 2131
Dept. of Genome Oriented Bioinformatics      Fax.  +49-8161-71 2186
Technical University of Munich
Science Center Weihenstephan
85350 Freising, Germany

 and

Institute for Bioinformatics / MIPS          Tel.  +49-89-3187 3675
GSF - National Research Center               Fax.  +49-89-3187 3585
      for Environment and Health
Ingolst?dter Landstrasse 1
85764 Neuherberg, Germany
http://mips.gsf.de/staff/pagel


From phgrosjean at sciviews.org  Wed Jul 19 10:02:26 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 19 Jul 2006 10:02:26 +0200
Subject: [R] R and DDE (Dynamic Data Exchange)
In-Reply-To: <20060718134031.BEJ44728@po-d.temple.edu>
References: <20060718134031.BEJ44728@po-d.temple.edu>
Message-ID: <44BDE712.3010904@sciviews.org>

Richard M. Heiberger wrote:
> I am thrilled to learn tcltk2 has DDE capability.
> It is the piece I have been needing to make ESS work directly
> with the RGUI on Windows.  GNU emacs on Windows has a ddeclient,
> but no access to COM.  So if R, or tcltk2 talking in both directions to R,
> has a ddeserver, all should be possible.  I will be reading the documentation
> closely in a few weeks to tie it together and then intend to make it happen.
> 
> Do you, or any other list member, have a sense of the size, complexity, ease,
> magnitude of the task I just defined?  Any advice as I get started on it?
> 
> Rich

Well, to be honest, DDE is an old exchange protocol (the first one 
proposed by M$ in Windows version 1 or 2). It is not that reliable. In 
practice, when the communication is working fine, you have no problems 
with it. But if something fails in either the server or the client, you 
got a very bad behaviour sometimes.

I think there is some interest to have DDE available for R (WinEdt uses 
DDE, I think... Uwe???), together with (D)COM, and socket server. 
Currently, I am improving the socket server build in svSocket (SciViews 
bundle) because it is the communication protocol we decided to push 
forward in Tinn-R, but there are other implementations out there. I 
think that using a socket server is more reliable and it is also a 
cross-platform solution. So, I would personnally prefer that solution.

Best,

Philippe Grosjean


From phgrosjean at sciviews.org  Wed Jul 19 10:06:35 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 19 Jul 2006 10:06:35 +0200
Subject: [R] R and DDE (Dynamic Data Exchange)
In-Reply-To: <971536df0607180727l6dc978d3g1ada0ce27c92cad9@mail.gmail.com>
References: <44BBB8B3.8070505@7d4.com>
	<971536df0607180727l6dc978d3g1ada0ce27c92cad9@mail.gmail.com>
Message-ID: <44BDE80B.30307@sciviews.org>

Gabor Grothendieck wrote:
> You can access DDE via COM as in this example
> which uses DDE to open an Excel file.  Note that
> Excel also supports COM directly and normally
> one would use COM with Excel, not DDE, so you
> might check if your application also supports COM.
> 
> # opens an excel spreadsheet c:\test.xls using dde
> library(RDCOMClient)
> sh <- COMCreate("Shell.Application")
> sh$Namespace("C:\\")$ParseName("test.xls")$InvokeVerb("&Open")

Well, I think you are really using COM here, not DDE. M$ implemented the 
same DDE commands in Excel and Word in COM to ease upgrading from DDE to 
COM... but the internal is completelly different!
Best,

Philippe Grosjean

> Also if you are going to access DDE via COM or just COM also
> check out the rcom package which is similar to RDCOMClient.
> 
> On 7/17/06, vincent at 7d4.com <vincent at 7d4.com> wrote:
> 
>>R and DDE (Dynamic Data Exchange)
>>
>>Dear Rusers,
>>I run an application (not mine) which acts as a DDE server.
>>I would like to use R to get data from this application,
>>say once per minute, and do some processing on it.
>>I didn't find much info on the R DDE abilities, apart the tcltk2
>>package in which I will try to go deeper.
>>I would be very thankful for any info, pointer or advice about the
>>"good ways" to make R program get online data from a DDE server.
>>Thanks
>>Vincent
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From jacques.veslot at good.ibl.fr  Wed Jul 19 10:09:39 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Wed, 19 Jul 2006 10:09:39 +0200
Subject: [R] A contingency table of counts by case
In-Reply-To: <44BCFBC3.8060503@wifo.ac.at>
References: <44BCFBC3.8060503@wifo.ac.at>
Message-ID: <44BDE8C3.7050706@good.ibl.fr>

sorry, answered to quickly...
actually it's easier using paste():


df <- df[order(df$case),]
apply(combinations(9,2), 1, function(y) table(factor(do.call(paste, c(with(df[df$id %in% y, ], 
split(x, id)), sep="")), levels=c("00","01","10","11"))))

-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Serguei Kaniovski a ?crit :
> Here is an example of the data.frame that I have,
> 
> df<-data.frame("case"=rep(1:5,each=9),"id"=rep(1:9,times=5),"x"=round(runif(length(rep(1:5,each=9)))))
> 
> "case" represents the cases,
> "id" the persons, and
> "x" is the binary state.
> 
> I would like to know in how many cases any two persons
> 
> a. both have "1",
> b. the first has "0" - the second has "1",
> c. the first has "0" - the second has "0",
> d. both have "0".
> 
> There will be choose(9,2) sums, denoted s_ij for 1<=i<j<=9,
> where i and j are running indices for an "id"-pair.
> 
> Please help, this is way beyond my knowledge of R!
> 
> Thank you,
> Serguei Kaniovski


From robert-mcfadden at o2.pl  Wed Jul 19 10:27:07 2006
From: robert-mcfadden at o2.pl (Robert Mcfadden)
Date: Wed, 19 Jul 2006 10:27:07 +0200
Subject: [R] get rid of error in Factor Analysis
Message-ID: <000601c6ab0d$1fb5a3c0$1191680a@robert>

B??dnie zakodowany tekst zosta? usuni?ty...
Plik: nie znany
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060719/3a883e4f/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Jul 19 10:51:26 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 19 Jul 2006 09:51:26 +0100 (BST)
Subject: [R] get rid of error in Factor Analysis
In-Reply-To: <000601c6ab0d$1fb5a3c0$1191680a@robert>
References: <000601c6ab0d$1fb5a3c0$1191680a@robert>
Message-ID: <Pine.LNX.4.64.0607190951090.8592@gannet.stats.ox.ac.uk>

?try

On Wed, 19 Jul 2006, Robert Mcfadden wrote:

> Dear All,
> 
> I wrote a program and there is a loop. At each iteration I use maximum
> likelihood factor analysis (?factanal). Output of factor analysis I use
> later (in this loop). Unfortunately from time to time I get an error message
> (I paste it below) and everything is stopped. Is it possible to write a
> condition that if error appears in factor analysis (FA), change input data
> for FA and do it again? Example
> 
>  
> 
> for (i in 1:1000){
> 
> FA<-factanal(data,factor=3)
> 
> If error appears change data to data2 and do factor analysis again
> 
> #rest of the program  
> 
> ............
> 
> ............
> 
> }
> 
>  
> 
> Best,
> 
> Robert 
> 
>     
> 
> Error in optim(start, FAfn, FAgr, method = "L-BFGS-B", lower = lower,  : 
> 
>         L-BFGS-B needs finite values of 'fn'
> 
> In addition: Warning message:
> 
> NaNs produced in: log(x)
> 
>  
> 
>  
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From dhajage at gmail.com  Wed Jul 19 10:58:59 2006
From: dhajage at gmail.com (David Hajage)
Date: Wed, 19 Jul 2006 10:58:59 +0200
Subject: [R] Output and Word
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB4B7BFF@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBB4B7BFF@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <a725cda30607190158s17c7c1dbydd1e4aca64cbd03@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060719/77ff81ff/attachment.pl 

From h.wickham at gmail.com  Wed Jul 19 11:03:15 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 19 Jul 2006 10:03:15 +0100
Subject: [R] Aligning ragged text columns
Message-ID: <f8e6ff050607190203k332f1ebatd828e80d5cfdfacb@mail.gmail.com>

Can anyone please suggest how I can print:

a <- matrix(c(
	"Heading 1",  "This is some info\nabout heading 1",
	"Heading 2",  "This is some info\nabout heading 2",
), byrow=T, nrow=2)

to look like:

Heading 1  This is some info
           about heading 1
Heading 2  This is some info
           about heading 2

(if you're not using a fixed width font, I want the text in the second
column to line up)

I've looked at encodeString and format, but neither seems to quite be
the right tool

Thanks!

Hadley


From ccleland at optonline.net  Wed Jul 19 11:13:53 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 19 Jul 2006 05:13:53 -0400
Subject: [R] conditional plot
In-Reply-To: <829e6c8a0607182201l1ceb3ac0s2a5315b90658ab14@mail.gmail.com>
References: <829e6c8a0607182201l1ceb3ac0s2a5315b90658ab14@mail.gmail.com>
Message-ID: <44BDF7D1.8070800@optonline.net>

Manoj wrote:
> Hi,
>      Can anyone pls help me in plotting the following data?
> 
>      The data-set contains company name, specification-1, specification-2.
> 
>      The graph would basically plot company name with specification-1
> on x-axis, & specification-2 on y-axis.
> 
>      Simply put - company name should get classified into one of the
> four quardrants created by specification 1 & specification2.

You could do something along these lines, replacing LETTERS with your 
company names:

df <- data.frame(SPEC1 = runif(26), SPEC2 = runif(26), COMPANY = LETTERS)

par(lab=c(10,10,7), las=1)
plot(df$SPEC1, df$SPEC2, type="n", xlab="Specification 1", 
ylab="Specification 2", ylim=c(0,1), xlim=c(0,1))
text(df$SPEC1, df$SPEC2, df$COMPANY)
axis(side=1, at = .5, tck=1, col="grey", lty=2)
axis(side=2, at = .5, tck=1, col="grey", lty=2)

> Thanks.
> 
> Manoj
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From wiedenhoeft at gmx.net  Wed Jul 19 11:17:14 2006
From: wiedenhoeft at gmx.net (John Wiedenhoeft)
Date: Wed, 19 Jul 2006 11:17:14 +0200
Subject: [R] Aligning ragged text columns
In-Reply-To: <f8e6ff050607190203k332f1ebatd828e80d5cfdfacb@mail.gmail.com>
References: <f8e6ff050607190203k332f1ebatd828e80d5cfdfacb@mail.gmail.com>
Message-ID: <1153300634.5913.7.camel@localhost>

Am Mittwoch, den 19.07.2006, 10:03 +0100 schrieb hadley wickham:
> Can anyone please suggest how I can print:
> 
> a <- matrix(c(
> 	"Heading 1",  "This is some info\nabout heading 1",
> 	"Heading 2",  "This is some info\nabout heading 2",
> ), byrow=T, nrow=2)
> 
> to look like:
> 
> Heading 1  This is some info
>            about heading 1
> Heading 2  This is some info
>            about heading 2


Guess its

heading1 <- "Heading1"
heading2 <- "Heading2"

a <- matrix(c(
	"Heading 1",  paste("This is some info\nabout", heading1, sep=""),
	"Heading 2",  paste("This is some info\nabout", heading2, sep=""),
), byrow=T, nrow=2)


Cheers,
John


From h.wickham at gmail.com  Wed Jul 19 11:29:03 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 19 Jul 2006 10:29:03 +0100
Subject: [R] Aligning ragged text columns
In-Reply-To: <1153300634.5913.7.camel@localhost>
References: <f8e6ff050607190203k332f1ebatd828e80d5cfdfacb@mail.gmail.com>
	<1153300634.5913.7.camel@localhost>
Message-ID: <f8e6ff050607190229i7869cf8di9c7741cc9f75edf7@mail.gmail.com>

> heading1 <- "Heading1"
> heading2 <- "Heading2"
>
> a <- matrix(c(
>         "Heading 1",  paste("This is some info\nabout", heading1, sep=""),
>         "Heading 2",  paste("This is some info\nabout", heading2, sep=""),
> ), byrow=T, nrow=2)

I wasn't so concerned about the redundancy in my example, but how it looks - eg.

> somefunction(h)
Heading 1  This is some info
          about heading 1
Heading 2  This is some info
          about heading 2


Hadley


From knoblauch at lyon.inserm.fr  Wed Jul 19 11:40:02 2006
From: knoblauch at lyon.inserm.fr (ken knoblauch)
Date: Wed, 19 Jul 2006 11:40:02 +0200
Subject: [R]  Aligning ragged text columns
Message-ID: <c56e60254746fb156a70d2be734de864@lyon.inserm.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060719/4630850d/attachment.pl 

From knoblauch at lyon.inserm.fr  Wed Jul 19 11:44:41 2006
From: knoblauch at lyon.inserm.fr (ken knoblauch)
Date: Wed, 19 Jul 2006 11:44:41 +0200
Subject: [R]  Aligning ragged text columns
In-Reply-To: <c56e60254746fb156a70d2be734de864@lyon.inserm.fr>
References: <c56e60254746fb156a70d2be734de864@lyon.inserm.fr>
Message-ID: <de493a9521178651659993533290b9c8@lyon.inserm.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060719/dc4fdb80/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Jul 19 11:47:11 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 19 Jul 2006 10:47:11 +0100 (BST)
Subject: [R] Output and Word
In-Reply-To: <a725cda30607190158s17c7c1dbydd1e4aca64cbd03@mail.gmail.com>
References: <07E228A5BE53C24CAD490193A7381BBB4B7BFF@LP-EXCHVS07.CO.IHC.COM>
	<a725cda30607190158s17c7c1dbydd1e4aca64cbd03@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0607191042340.9979@gannet.stats.ox.ac.uk>

On Wed, 19 Jul 2006, David Hajage wrote:

> thank you Greg Snow for this information !
> 
> But I have this message :
> 
> > odfWeave("c:/simple.odt", "c:/essai.odt")
>   Setting wd
>   Copying  c:/simple.odt
>   Decompressing ODF file using unzip -o
> "C:\DOCUME~1\Maud\LOCALS~1\Temp\RtmpF0hdqb/simple.odt"
> Erreur dans odfWeave("c:/simple.odt", "c:/essai.odt") :
>         Error unzipping file
> De plus : Warning message:
> unzip introuvable
> 
> R says that it doesn't find "unzip"... What is this program ?

It is in the Rtools.zip bundle used to build source packages for R under 
Windows (your unstated OS, it seems).

However, you should take this up with the package maintainer, as R has the 
capability to unzip files builtin (utils::zip.unpack) on Windows.

> 
> 2006/7/18, Greg Snow <Greg.Snow at intermountainmail.org>:
> >
> > Others have suggested using R2HTML (which is a good option).
> >
> > Another option is to use Sweave and specifically the new odfWeave
> > package for R.  This works on OpenOffice files rather than word files
> > (but OpenOffice  http://www.openoffice.org/ can inport and export word
> > documents).
> >
> > The basic idea is to write your report in OpenOffice (or LaTeX or HTML),
> > but anywhere that you want statisticial output (graphs, tables) you
> > include instead the R code to produce the table, graph, or whatever.
> > Run this file through R (using Sweave or odfWeave) and the resulting
> > file has replaced all the code segments with their output.
> >
> > The documentation with odfWeave has examples.
> >
> > Hope this helps,
> >
> > --
> > Gregory (Greg) L. Snow Ph.D.
> > Statistical Data Center
> > Intermountain Healthcare
> > greg.snow at intermountainmail.org
> > (801) 408-8111
> >
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of sharon snowdon
> > Sent: Monday, July 17, 2006 2:36 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Output and Word
> >
> > Hi
> >
> > I have just started to have a look at R. I have used most stats software
> > packages and can use perl, visual basic etc. I am interested in how well
> > it handles lots of output e.g. tables or charts. How would you get lots
> > of output most easily and quickly into a Word document?
> >
> > Sharon Snowdon
> >
> >
> >
> > ------------------------------------------------------------------------
> > -
> >
> >
> >
> >
> >
> >
> > --
> >
> >
> >
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ulriks at ruc.dk  Wed Jul 19 11:58:38 2006
From: ulriks at ruc.dk (Ulrik Stervbo)
Date: Wed, 19 Jul 2006 11:58:38 +0200
Subject: [R] Fitting a distribution to peaks in histogram
Message-ID: <3483f8d50607190258n1f711443p869f79850e81fa79@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060719/73ee2342/attachment.pl 

From uhkeller at web.de  Wed Jul 19 12:09:36 2006
From: uhkeller at web.de (Ulrich Keller)
Date: Wed, 19 Jul 2006 12:09:36 +0200
Subject: [R] Test for equality of coefficients in multivariate multiple
 regression
In-Reply-To: <20060719025837.GD40766@ms.unimelb.edu.au>
References: <44BD2530.2020502@web.de>
	<20060719015144.GA40766@ms.unimelb.edu.au>
	<17597.39064.544971.397470@bossiaea.maths.uwa.edu.au>
	<20060719025837.GD40766@ms.unimelb.edu.au>
Message-ID: <44BE04E0.6000306@web.de>

Hello and thank you for your answers, Andrew and Berwin. If I'm not 
mistaken, the mixed-model version of Berwin's approach would be:

#My stuff:
DF<-data.frame(x1=rep(c(0,1),each=50),x2=rep(c(0,1),50))
tmp<-rnorm(100)
DF$y1<-tmp+DF$x1*.5+DF$x2*.3+rnorm(100,0,.5)
DF$y2<-tmp+DF$x1*.5+DF$x2*.7+rnorm(100,0,.5)
x.mlm<-lm(cbind(y1,y2)~x1+x2,data=DF)

#1st part of Andrew's suggestion:
DF2 <- with(DF, data.frame(y=c(y1,y2)))
DF2$x11 <- with(DF, c(x1, rep(0,100)))
DF2$x21 <- with(DF, c(x2, rep(0,100)))
DF2$x12 <- with(DF, c(rep(0,100), x1))
DF2$x22 <- with(DF, c(rep(0,100), x2))
DF2$x1 <- with(DF, c(x1, x1))
DF2$wh <- rep(c(0,1), each=100)

#Mixed version of models:
 > DF2$unit <- rep(c(1:100), 2)
 > library(nlme)
 > mm1 <- lme(y~wh + x11 + x21 + x12 + x22, random= ~1 | unit, DF2, 
method="ML")
 > fixef(mm1)
(Intercept)          wh         x11         x21         x12         x22
 0.07800993  0.15234579  0.52936947  0.13853332  0.37285132  0.46048418
 > coef(x.mlm)
                    y1        y2
(Intercept) 0.07800993 0.2303557
x1          0.52936947 0.3728513
x2          0.13853332 0.4604842
 > mm2 <- update(mm1, y~wh + x1 + x12 + x22)
 > anova(mm1, mm2)
    Model df      AIC      BIC    logLik   Test  L.Ratio p-value
mm1     1  8 523.6284 550.0149 -253.8142                       
mm2     2  7 522.0173 545.1055 -254.0086 1 vs 2 0.388908  0.5329

This seems to be correct. What anova() tells me is that the effect of x1 
is the same for y1 and y2. What I don't understand then is why the 
coefficients for x12 and x22 differ so much between mm1 and mm2:

 > fixef(mm2)
(Intercept)          wh          x1         x12         x22
  0.1472766   0.1384474   0.5293695  -0.1565182   0.3497476
 > fixef(mm1)
(Intercept)          wh         x11         x21         x12         x22
 0.07800993  0.15234579  0.52936947  0.13853332  0.37285132  0.46048418

Sorry for being a bit slow here, I'm (obviously) not a statistician.
Thanks again,

Uli

Andrew Robinson wrote:
> G'day Berwin,
>
> my major reason for preferring the mixed-effect approach is that, as
> you can see below, the residual df for your models are 195 and 194,
> respectively.  The 100 units are each contributing two degrees of
> freedom to your inference, and presumably to your estimate of the
> variance.  I feel a little sqeueamish about that, because it's not
> clear to me that they can be assumed to be independent.  I worry about
> the effects on the size of the test.  With a mixed-effects model each
> unit could be a cluster of two observations, and I would guess the
> size would be closer to nominal, if not nominal.
>
> Cheers
>
> Andrew


From jholtman at gmail.com  Wed Jul 19 12:41:15 2006
From: jholtman at gmail.com (jim holtman)
Date: Wed, 19 Jul 2006 06:41:15 -0400
Subject: [R] conditional plot
In-Reply-To: <829e6c8a0607182201l1ceb3ac0s2a5315b90658ab14@mail.gmail.com>
References: <829e6c8a0607182201l1ceb3ac0s2a5315b90658ab14@mail.gmail.com>
Message-ID: <644e1f320607190341k3c749178gd0e6599fd07d3b1b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060719/5d78b2a2/attachment.pl 

From h.wickham at gmail.com  Wed Jul 19 12:53:59 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 19 Jul 2006 11:53:59 +0100
Subject: [R] Aligning ragged text columns
In-Reply-To: <c56e60254746fb156a70d2be734de864@lyon.inserm.fr>
References: <c56e60254746fb156a70d2be734de864@lyon.inserm.fr>
Message-ID: <f8e6ff050607190353o456cfd69i94c2985859e2c237@mail.gmail.com>

> I find that things line up better in data.frames

That's a good idea, although I was hoping there would be something in
R to do it for me.

I have ended up with:

	fwidth <- max(nchar(x[,1]))
	descs <- strwrap(x[,2], width=width - fwidth - 5, simplify=FALSE)
	
output <- do.call(rbind, mapply(function(name, desc) {
	cbind(c(name, rep("", length=length(desc)-1)), desc)
}, x[,1], descs))
	
	cat(paste(format(output[,1]), output[,2], collapse="\n"), "\n")

Thanks,

Hadley


From michael.watson at bbsrc.ac.uk  Wed Jul 19 12:58:08 2006
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 19 Jul 2006 11:58:08 +0100
Subject: [R] Plotting lines and points on the second plot when using
	gap.plot in plotrix
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950374D12C@iahce2ksrv1.iah.bbsrc.ac.uk>

Hi

My question is simple - the gap.plot function in the plotrix package
allows users to draw graphs that have a broken axis.  However, I want to
then add a line to the "second" plot, but can't.

Eg:

twogrp<-c(rnorm(10)+4,rnorm(10)+20)
gap.plot(twogrp,rnorm(20),gap.bounds=c(8,16),gap.axis="x",xlab="X
values",
       xtics=c(4,7,17,20),ylab="Y values",main="Plot gap on X axis") 

# this doesn't work
points(17,0,col="red")

# this does work
points(4,0,col="green")

I somehow need to set the focus to the second plot so that I can draw
lines and points on it.

Any help?

Mick


From michael.watson at bbsrc.ac.uk  Wed Jul 19 13:11:04 2006
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 19 Jul 2006 12:11:04 +0100
Subject: [R] Plotting lines and points on the second plot when
	usinggap.plot in plotrix
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950374D12C@iahce2ksrv1.iah.bbsrc.ac.uk>
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950374D12D@iahce2ksrv1.iah.bbsrc.ac.uk>

I've answered my own question, the x-values of the line/points I want to
plot must be adjusted by the gap size

Thanks
Mick 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of michael watson
(IAH-C)
Sent: 19 July 2006 11:58
To: r-help at stat.math.ethz.ch
Subject: [R] Plotting lines and points on the second plot when
usinggap.plot in plotrix

Hi

My question is simple - the gap.plot function in the plotrix package
allows users to draw graphs that have a broken axis.  However, I want to
then add a line to the "second" plot, but can't.

Eg:

twogrp<-c(rnorm(10)+4,rnorm(10)+20)
gap.plot(twogrp,rnorm(20),gap.bounds=c(8,16),gap.axis="x",xlab="X
values",
       xtics=c(4,7,17,20),ylab="Y values",main="Plot gap on X axis") 

# this doesn't work
points(17,0,col="red")

# this does work
points(4,0,col="green")

I somehow need to set the focus to the second plot so that I can draw
lines and points on it.

Any help?

Mick

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From rene.eschen at unifr.ch  Wed Jul 19 13:36:45 2006
From: rene.eschen at unifr.ch (ESCHEN Rene)
Date: Wed, 19 Jul 2006 13:36:45 +0200
Subject: [R] Random structure of nested design in lme
Message-ID: <E632249B3E11B14AAABA75FDB5F32B24132A0A@EXCHANGE4.unifr.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060719/d31525bc/attachment.pl 

From HDoran at air.org  Wed Jul 19 13:53:42 2006
From: HDoran at air.org (Doran, Harold)
Date: Wed, 19 Jul 2006 07:53:42 -0400
Subject: [R] Random structure of nested design in lme
Message-ID: <2323A6D37908A847A7C32F1E3662C80E132CEE@dc1ex01.air.org>

Can you provide an example of what you have done with lme so we might be able to evaluate the issue? 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ESCHEN Rene
> Sent: Wednesday, July 19, 2006 7:37 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Random structure of nested design in lme
> 
> All,
> 
> I'm trying to analyze the results of a reciprocal transplant 
> experiment using lme(). While I get the error-term right in 
> aov(), in lme() it appears impossible to get as expected. I 
> would be greatful for any help.
> 
> My experiment aimed to identify whether two fixed factors 
> (habitat type and soil type) affect the development of 
> plants. I took soil from six random sites each of two types 
> (arable and grassland) and transplanted them back into the 
> sites of origin in such way that in each of the sites there 
> were six pots containing arable soil and six pots of 
> grassland soil, each containing a seedling.
> 
> With aov(), I got the analysis as I expected, with habitat 
> type tested against destination site, and soil type tested 
> against origin site:
> 
> summary(aov(response~soiltype*habitat+Error(destination+origin)))
> #
> #Error: destination
> #          Df  Sum Sq Mean Sq F value Pr(>F)
> #habitat    1  1.0000  1.0000   0.699 0.4226
> #Residuals 10 14.3056  1.4306               
> #
> #Error: origin
> #          Df  Sum Sq Mean Sq F value   Pr(>F)   
> #soiltype   1 1.77778 1.77778  11.636 0.006645 **
> #Residuals 10 1.52778 0.15278                    
> #---
> #Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #
> #Error: Within
> #                  Df  Sum Sq Mean Sq F value Pr(>F)
> #soiltype:habitat   1  0.2500  0.2500  2.1774 0.1427
> #Residuals        120 13.7778  0.1148     
> 
> However, when I try to replicate this analysis in lme, I am 
> unable to get the structure of the random factors (origin and 
> destination) correct. Does anyone have a suggestion how to 
> resolve this problem?
> 
> Thanks in advance.
> 
> Ren? Eschen
> 
> CABI Bioscience Centre Switzerland
> Rue des Grillons 1
> 2800 Del?mont
> Switzerland
> 
> 	[[alternative HTML version deleted]]
> 
>


From jfox at mcmaster.ca  Wed Jul 19 13:56:13 2006
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 19 Jul 2006 07:56:13 -0400
Subject: [R] Test for equality of coefficients in multivariate
	multipleregression
In-Reply-To: <17597.39064.544971.397470@bossiaea.maths.uwa.edu.au>
Message-ID: <20060719115614.XEAJ13653.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Berwin,

Simply stacking the problems and treating the resulting observations as
independent will give you the correct coefficients, but incorrect
coefficient variances and artificially zero covariances.

The approach that I suggested originally -- testing a linear hypothesis
using the coefficient estimates and covariances from the multivariate linear
model -- seems simple enough. For example, to test that all three
coefficients are the same across the two equations,

 b <- as.vector(coef(x.mlm))
 
 V <- vcov(x.mlm)
 
 L <- c(1, 0, 0,-1, 0, 0,
        0, 1, 0, 0,-1, 0,
        0, 0, 1, 0, 0,-1)
 L <- matrix(L, nrow=3, byrow=TRUE)
 
 t(L %*% b)  %*% (L %*% V %*% t(L)) %*% (L %*% b)

The test statistic is chi-square with 3 df under the null hypothesis. (Note:
not checked carefully.)

(BTW, it's a bit unclear to me how much of this exchange was on r-help, but
I'm copying to r-help since at least one of Ulrich's messages referring to
alternative approaches appeared there. I hope that's OK.)

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Berwin A Turlach 
> [mailto:berwin at bossiaea.maths.uwa.edu.au] On Behalf Of Berwin 
> A Turlach
> Sent: Tuesday, July 18, 2006 9:28 PM
> To: Andrew Robinson
> Cc: Ulrich Keller; John Fox
> Subject: Re: [R] Test for equality of coefficients in 
> multivariate multipleregression
> 
> G'day all,
> 
> >>>>> "AR" == Andrew Robinson <A.Robinson at ms.unimelb.edu.au> writes:
> 
>     AR> I suggest that you try to rewrite the model system into a
>     AR> single mixed-effects model, [...] Why a mixed-effect 
> model, wouldn't a fixed effect be o.k. too?
> 
> Something like:
> 
> > DF<-data.frame(x1=rep(c(0,1),each=50),x2=rep(c(0,1),50))
> > tmp<-rnorm(100)
> > DF$y1<-tmp+DF$x1*.5+DF$x2*.3+rnorm(100,0,.5)
> > DF$y2<-tmp+DF$x1*.5+DF$x2*.7+rnorm(100,0,.5)
> > x.mlm<-lm(cbind(y1,y2)~x1+x2,data=DF)
> > coef(x.mlm)
>                      y1          y2
> (Intercept) -0.08885266 -0.05749196
> x1           0.33749086  0.60395258
> x2           0.72017894  1.11932077
> 
> 
> > DF2 <- with(DF, data.frame(y=c(y1,y2)))
> > DF2$x11 <- with(DF, c(x1, rep(0,100)))
> > DF2$x21 <- with(DF, c(x2, rep(0,100)))
> > DF2$x12 <- with(DF, c(rep(0,100), x1))
> > DF2$x22 <- with(DF, c(rep(0,100), x2))
> > DF2$x1 <- with(DF, c(x1, x1))
> > DF2$wh <- rep(c(0,1), each=100)
> > fm1 <- lm(y~wh + x11 + x21 + x12 + x22, DF2)
> > fm1
> 
> Call:
> lm(formula = y ~ wh + x11 + x21 + x12 + x22, data = DF2)
> 
> Coefficients:
> (Intercept)           wh          x11          x21          
> x12          x22  
>    -0.08885      0.03136      0.33749      0.72018      
> 0.60395      1.11932  
> 
> > fm2 <- lm(y~wh + x1 + x21 + x22, DF2)
> > anova(fm2,fm1)
> Analysis of Variance Table
> 
> Model 1: y ~ wh + x1 + x21 + x22
> Model 2: y ~ wh + x11 + x21 + x12 + x22
>   Res.Df     RSS  Df Sum of Sq      F Pr(>F)
> 1    195 246.919                            
> 2    194 246.031   1     0.888 0.6998 0.4039
> 
> 
> Cheers,
> 
>         Berwin
> 
> ========================== Full address ============================
> Berwin A Turlach                      Tel.: +61 (8) 6488 3338 
> (secr)   
> School of Mathematics and Statistics        +61 (8) 6488 3383 
> (self)      
> The University of Western Australia   FAX : +61 (8) 6488 1028
> 35 Stirling Highway                   
> Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
> Australia                        http://www.maths.uwa.edu.au/~berwin
>


From berwin at maths.uwa.edu.au  Wed Jul 19 14:43:41 2006
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Wed, 19 Jul 2006 20:43:41 +0800
Subject: [R] Test for equality of coefficients in multivariate
	multipleregression
In-Reply-To: <20060719115614.XEAJ13653.tomts36-srv.bellnexxia.net@JohnDesktop8300>
References: <17597.39064.544971.397470@bossiaea.maths.uwa.edu.au>
	<20060719115614.XEAJ13653.tomts36-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <17598.10493.114412.205581@bossiaea.maths.uwa.edu.au>

G'day John,

>>>>> "JF" == John Fox <jfox at mcmaster.ca> writes:

    JF> Simply stacking the problems and treating the resulting
    JF> observations as independent will give you the correct
    JF> coefficients, but incorrect coefficient variances
Yes, after Andrew's (off-list) answer I realised this too.  If I am
not mistaken, all variances/covariances should be off by a factor of
1/2 or something like that.

    JF> and artificially zero covariances.
Well, I must admit that I misread Ulrich's code for most of the day.
I hadn't realised that the variable `tmp' introduces a correlation
between `y1' and `y2' in his code:

> DF<-data.frame(x1=rep(c(0,1),each=50),x2=rep(c(0,1),50))
> tmp<-rnorm(100)
> DF$y1<-tmp+DF$x1*.5+DF$x2*.3+rnorm(100,0,.5)
> DF$y2<-tmp+DF$x1*.5+DF$x2*.7+rnorm(100,0,.5)

for some reason, my brain kept parsing this as generate *one* random
intercept for y1 and *one* random intercept for y2, not that each
individual observation has a random intercept.  Under the model that
my brain kept parsing, one would have zero covariances. :)

Now I understand why Andrew suggested the use of mixed models and
would go down that way too.  But I believe your approach is valid too.

    JF> (BTW, it's a bit unclear to me how much of this exchange was
    JF> on r-help,
Easy, all those that have r-help either in the TO: or CC: field.
Those were Ulrich's original message and the answer by you and Andrew,
I kept all my mails so far off-list.

    JF> but I'm copying to r-help since at least one of Ulrich's
    JF> messages referring to alternative approaches appeared there.
Yes, I noticed that and answered off-list.  In that message, if I read
it correctly, he had confused Andrew and me.

    JF> I hope that's OK.)
Sure, why not? :)

Cheers,

        Berwin


From Pierre.Lapointe at nbf.ca  Wed Jul 19 14:45:42 2006
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Wed, 19 Jul 2006 08:45:42 -0400
Subject: [R] How would you export a 3-dimensional array to an SQL database?
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0DF36063@lbmsg002.fbn-nbf.local>

Hello,

How would you export a 3-dimensional array to an SQL database?

a<-array(1:24, 2:4)

Is there an open source DB that would be more adequate for this type of
operation?

Is there a way to reshape/flatten a 3-dimensional array?

Regards,

Pierre Lapointe

**************************************************
AVIS DE NON-RESPONSABILITE: Ce document transmis par courrie...{{dropped}}


From petr.pikal at precheza.cz  Wed Jul 19 14:54:51 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 19 Jul 2006 14:54:51 +0200
Subject: [R] Fitting a distribution to peaks in histogram
In-Reply-To: <3483f8d50607190258n1f711443p869f79850e81fa79@mail.gmail.com>
Message-ID: <44BE47BB.19703.1340D34@localhost>

Hi

There are some packages for mass spectra processing (spectrino, 
caMassClass). I did not use them so I do not know how they suit your 
needs.

However you can compute area (integrate) by these functions

# uses information interactively from plot(x,y)
# first it replots data between corners *replot(x,y)*
# then it computes sum between x axis and y values - osum -
# and between "baseline" and y values - cista - based
# on locator positions

integ<-function (x,y)
{
replot(x,y)
meze<-locator(2)
dm<-meze$x[1]
hm<-meze$x[2]
abline(v=c(dm,hm),col=2)
vyber<-x<=hm&x>=dm
f3 <- splinefun(x, y)
osum<-integrate(f3, dm, hm)$value
o1<-(y[x==min(x[vyber])]+y[x==max(x[vyber])])*(max(x[vyber])-
min(x[vyber]))/2
cista<-osum-o1
return(c(osum,cista))
}

# similar as integ but you has to supply upper and lower limits (dm, 
# hm) manually if you do not want to perform "integration" of whole # 
area under the curve.


integ1<-function (x,y,dm=-Inf,hm=+Inf)
{
ifelse(dm==-Inf, dm<-min(x), dm<-dm)
ifelse(hm==+Inf, hm<-max(x), hm<-hm)
vyber<-x<=hm&x>=dm
f3 <- splinefun(x, y)
osum<-integrate(f3, dm, hm)$value
o1<-(y[x==min(x[vyber])]+y[x==max(x[vyber])])*(max(x[vyber])-
min(x[vyber]))/2
cista<-osum-o1
return(c(osum,cista))
}



On 19 Jul 2006 at 11:58, Ulrik Stervbo wrote:

Date sent:      	Wed, 19 Jul 2006 11:58:38 +0200
From:           	"Ulrik Stervbo" <ulriks at ruc.dk>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Fitting a distribution to peaks in histogram

> Hello list!
> 
> I would like to fit a distribution to each of the peaks in a
> histogram, such as this:
> http://photos1.blogger.com/blogger/7029/2724/1600/DU145-Bax3-Bcl-xL.pn
> g .
> 
> The peaks are identified using Petr Pikal peaks function (
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/33097.html), but after
> that I am quite stuck.
> 
> Any idea as to how I can:
> Fit a distribution to each peak
> Integrate the area between each two peaks, using the means and widths
> of the distributions fitted to the two peaks. I will be using the
> integrate function
> 
> The histogram is based on approximately 15000 events, which makes
> Mclust and pam (which both delivers the information I need) less
> useful.
> 
> The whole point of this exercise is to find the percentage of cells in
> peak 1, 2, 3, and so on, and between peak 1-2, peak 2-3, peak 3-4 and
> so on. Having more that 6 peaks does not appears likely.
> 
> I am quite new to R and apologise if the solution is fairly basic.
> 
> Thank you in advance for any help and suggestions
> 
> Sincerely,
> Ulrik
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From r.hankin at noc.soton.ac.uk  Wed Jul 19 15:25:21 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 19 Jul 2006 14:25:21 +0100
Subject: [R] Stirling numbers
Message-ID: <871C3471-E69F-45C2-B63D-CA06C51387FD@soc.soton.ac.uk>

Hi

anyone coded up Stirling numbers in R?

[I need unsigned Stirling numbers of the first kind]


cheers

Robin




--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From Max.Kuhn at pfizer.com  Wed Jul 19 15:31:15 2006
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Wed, 19 Jul 2006 09:31:15 -0400
Subject: [R]  Output and Word
Message-ID: <71257D09F114DA4A8E134DEAC70F25D305A5234E@groamrexm03.amer.pfizer.com>

On page 2 of the manual, Section 2 ("Requirements") has

  "The package also requires a utility to zip and unzip compressed
files, 
  such as unzip, Winzip or jar."

With a footnote that unzip can be found at

  http://www.info-zip.org/

for free. You can use any utility that can zip/unzip pkzip formatted
files.
If you want to use something else, you can. See ?odfWeaveControl.

I didn't use utils::zip.unpack since there did not appear to be an
analogous
utility for re-zipping the file. Is this incorrect?

I will put an automatic check on package startup for the above utilities
and print a warning if nothing is found.

Max

> 0n Wed, 19 Jul 2006, David Hajage wrote:
> 
> > thank you Greg Snow for this information !
> > 
> > But I have this message :
> > 
> > > odfWeave("c:/simple.odt", "c:/essai.odt")
> >   Setting wd
> >   Copying  c:/simple.odt
> >   Decompressing ODF file using unzip -o
> > "C:\DOCUME~1\Maud\LOCALS~1\Temp\RtmpF0hdqb/simple.odt"
> > Erreur dans odfWeave("c:/simple.odt", "c:/essai.odt") :
> >         Error unzipping file
> > De plus : Warning message:
> > unzip introuvable
> > 
> > R says that it doesn't find "unzip"... What is this program ?
> 
> It is in the Rtools.zip bundle used to build source packages for R
under 
> Windows (your unstated OS, it seems).
> 
> However, you should take this up with the package maintainer, as R has
the 
> capability to unzip files builtin (utils::zip.unpack) on Windows.
> 
> > 
> > 2006/7/18, Greg Snow <Greg.Snow at intermountainmail.org>:
> > >
----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From ggrothendieck at gmail.com  Wed Jul 19 16:07:08 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 19 Jul 2006 10:07:08 -0400
Subject: [R] R and DDE (Dynamic Data Exchange)
In-Reply-To: <44BDE712.3010904@sciviews.org>
References: <20060718134031.BEJ44728@po-d.temple.edu>
	<44BDE712.3010904@sciviews.org>
Message-ID: <971536df0607190707h7e84ba9cwcbe8498178d3b77c@mail.gmail.com>

On 7/19/06, Philippe Grosjean <phgrosjean at sciviews.org> wrote:
> Richard M. Heiberger wrote:
> > I am thrilled to learn tcltk2 has DDE capability.
> > It is the piece I have been needing to make ESS work directly
> > with the RGUI on Windows.  GNU emacs on Windows has a ddeclient,
> > but no access to COM.  So if R, or tcltk2 talking in both directions to R,
> > has a ddeserver, all should be possible.  I will be reading the documentation
> > closely in a few weeks to tie it together and then intend to make it happen.
> >
> > Do you, or any other list member, have a sense of the size, complexity, ease,
> > magnitude of the task I just defined?  Any advice as I get started on it?
> >
> > Rich
>
> Well, to be honest, DDE is an old exchange protocol (the first one
> proposed by M$ in Windows version 1 or 2). It is not that reliable. In
> practice, when the communication is working fine, you have no problems
> with it. But if something fails in either the server or the client, you
> got a very bad behaviour sometimes.

There is a good discussion in this DDE FAQ of when DDE would
be preferred and when COM would be:

http://www.angelfire.com/biz/rhaminisys/ddeinfo.html#DDEpreferred


From f_bresson at yahoo.fr  Wed Jul 19 16:12:38 2006
From: f_bresson at yahoo.fr (Florent Bresson)
Date: Wed, 19 Jul 2006 16:12:38 +0200 (CEST)
Subject: [R] Progress in a loop
Message-ID: <20060719141238.61289.qmail@web26810.mail.ukl.yahoo.com>

Hi, I have to use a loop to perform a quite computer
intensive estimation and I would like to know the
progress of the loop during the process. I tried to
include something like

  print(paste(k,date(),sep=" : "))

where k is the number of the iteration, but the result
appears only at the end of the loop. Can someone help
me please ?


	

	
		
___________________________________________________________________________ 
D?couvrez un nouveau moyen de poser toutes vos questions quelque soit le sujet !


From maechler at stat.math.ethz.ch  Wed Jul 19 16:19:15 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 19 Jul 2006 16:19:15 +0200
Subject: [R] Stirling numbers
In-Reply-To: <871C3471-E69F-45C2-B63D-CA06C51387FD@soc.soton.ac.uk>
References: <871C3471-E69F-45C2-B63D-CA06C51387FD@soc.soton.ac.uk>
Message-ID: <17598.16227.87508.715297@stat.math.ethz.ch>

>>>>> "Robin" == Robin Hankin <r.hankin at noc.soton.ac.uk>
>>>>>     on Wed, 19 Jul 2006 14:25:21 +0100 writes:

    Robin> Hi anyone coded up Stirling numbers in R?

"Sure" ;-)

    Robin> [I need unsigned Stirling numbers of the first kind]

but with my quick search, I can only see those for which I had
"2nd kind" :

-o<--o<-------------------------------------------------------------------

##-- Stirling numbers of the 2nd kind
##-- (Abramowitz/Stegun: 24,1,4 (p. 824-5 ; Table 24.4, p.835)

##> S^{(m)}_n = number of ways of partitioning a set of $n$ elements into $m$
##>	non-empty subsets

Stirling2 <- function(n,m)
{
    ## Purpose:  Stirling Numbers of the 2-nd kind
    ## 		S^{(m)}_n = number of ways of partitioning a set of
    ##                      $n$ elements into $m$ non-empty subsets
    ## Author: Martin Maechler, Date:  May 28 1992, 23:42
    ## ----------------------------------------------------------------
    ## Abramowitz/Stegun: 24,1,4 (p. 824-5 ; Table 24.4, p.835)
    ## Closed Form : p.824 "C."
    ## ----------------------------------------------------------------

    if (0 > m || m > n) stop("'m' must be in 0..n !")
    k <- 0:m
    sig <- rep(c(1,-1)*(-1)^m, length= m+1)# 1 for m=0; -1 1 (m=1)
    ## The following gives rounding errors for (25,5) :
    ## r <- sum( sig * k^n /(gamma(k+1)*gamma(m+1-k)) )
    ga <- gamma(k+1)
    round(sum( sig * k^n /(ga * rev(ga))))
}

options(digits=15)
for (n in 11:31) cat("n =", n," S_n(5) =", Stirling2(n,5), "\n")
n <- 25
for(k in c(3,5,10))
    cat(" S_",n,"^(",formatC(k,wid=2),") = ", Stirling2(n,k),"\n",sep = "")

for (n in 1:15)
    cat(formatC(n,w=2),":", sapply(1:n, Stirling2, n = n),"\n")
##  1 : 1
##  2 : 1 1
##  3 : 1 3 1
##  4 : 1 7 6 1
##  5 : 1 15 25 10 1
##  6 : 1 31 90 65 15 1
##  7 : 1 63 301 350 140 21 1
##  8 : 1 127 966 1701 1050 266 28 1
##  9 : 1 255 3025 7770 6951 2646 462 36 1
## 10 : 1 511 9330 34105 42525 22827 5880 750 45 1
## 11 : 1 1023 28501 145750 246730 179487 63987 11880 1155 55 1
## 12 : 1 2047 86526 611501 1379400 1323652 627396 159027 22275 1705 66 1
## 13 : 1 4095 261625 2532530 7508501 9321312 5715424 1899612 359502 39325 2431 78 1
## 14 : 1 8191 788970 10391745 40075035 63436373 49329280 20912320 5135130 752752 66066 3367 91 1
## 15 : 1 16383 2375101 42355950 210766920 420693273 408741333 216627840 67128490 12662650 1479478 106470 4550 105 1


plot(6:30, sapply(6:30, Stirling2, m=5), log = "y", type = "l")
## Abramowitz says something like  S2(n,m) ~ m^n / m!
##                                 ------------------
nn <- 6:30; sapply(nn, Stirling2, m=5)  / (5^nn / gamma(5+1))
## 0.114 0.21 ...... 0.99033 0.992266 0.993812

-o<--o<-------------------------------------------------------------------


From ligges at statistik.uni-dortmund.de  Wed Jul 19 16:22:07 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 19 Jul 2006 16:22:07 +0200
Subject: [R] Progress in a loop
In-Reply-To: <20060719141238.61289.qmail@web26810.mail.ukl.yahoo.com>
References: <20060719141238.61289.qmail@web26810.mail.ukl.yahoo.com>
Message-ID: <44BE400F.9020306@statistik.uni-dortmund.de>

Florent Bresson wrote:
> Hi, I have to use a loop to perform a quite computer
> intensive estimation and I would like to know the
> progress of the loop during the process. I tried to
> include something like
> 
>   print(paste(k,date(),sep=" : "))
> 
> where k is the number of the iteration, but the result
> appears only at the end of the loop. Can someone help
> me please ?


Please read the "R for Windows FAQ":
When using Rgui the output to the console seems to be delayed.

Uwe Ligges



> 
> 	
> 
> 	
> 		
> ___________________________________________________________________________ 
> D?couvrez un nouveau moyen de poser toutes vos questions quelque soit le sujet !
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bennfine at yahoo.com  Wed Jul 19 16:22:13 2006
From: bennfine at yahoo.com (Benn Fine)
Date: Wed, 19 Jul 2006 07:22:13 -0700 (PDT)
Subject: [R] WLS ins systemfit question
Message-ID: <20060719142213.85902.qmail@web61324.mail.yahoo.com>

How does one specify the weights for WLS in the
systemfit command ?

That is, there is a weight option in lm(), but there
doesn't seem to be weight option for systemfit("WLS")

Thanks!


From HDoran at air.org  Wed Jul 19 16:21:21 2006
From: HDoran at air.org (Doran, Harold)
Date: Wed, 19 Jul 2006 10:21:21 -0400
Subject: [R] Progress in a loop
Message-ID: <2323A6D37908A847A7C32F1E3662C80E132D10@dc1ex01.air.org>

Look for progress() in the svMisc package

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Florent Bresson
> Sent: Wednesday, July 19, 2006 10:13 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Progress in a loop
> 
> Hi, I have to use a loop to perform a quite computer 
> intensive estimation and I would like to know the progress of 
> the loop during the process. I tried to include something like
> 
>   print(paste(k,date(),sep=" : "))
> 
> where k is the number of the iteration, but the result 
> appears only at the end of the loop. Can someone help me please ?
> 
> 
> 	
> 
> 	
> 		
> ______________________________________________________________
> _____________
> D?couvrez un nouveau moyen de poser toutes vos questions 
> quelque soit le sujet !
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gavin.simpson at ucl.ac.uk  Wed Jul 19 16:31:35 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 19 Jul 2006 15:31:35 +0100
Subject: [R] Progress in a loop
In-Reply-To: <20060719141238.61289.qmail@web26810.mail.ukl.yahoo.com>
References: <20060719141238.61289.qmail@web26810.mail.ukl.yahoo.com>
Message-ID: <1153319495.18110.55.camel@gsimpson.geog.ucl.ac.uk>

On Wed, 2006-07-19 at 16:12 +0200, Florent Bresson wrote:
> Hi, I have to use a loop to perform a quite computer
> intensive estimation and I would like to know the
> progress of the loop during the process. I tried to
> include something like
> 
>   print(paste(k,date(),sep=" : "))
> 
> where k is the number of the iteration, but the result
> appears only at the end of the loop. Can someone help
> me please ?

Windows I assume - you are asked to provide some rudimentary information
about your system when asking for help as the posting guide states.

You need to use flush.console() on Windows, as in:

dat <- matrix(runif(20), ncol = 2)
res <- numeric(length = 10)
for(i in 1:10) {
   res[i] <- sum(dat[i, ])
   cat(paste("Iteration #:", i, "-", date(), "\n"))
   flush.console()
}

Depending on how you want the printed statements to appear on the
screen, you'd be better off with cat not print, e.g.: 

cat(paste(k,date(),sep=" : "))

If this isn't the solution, post a reply containing information about
your R version, OS, and a reproducible example (simple code & data as I
did) to show us the problem.

HTH

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/cv/
 UK. WC1E 6BT.                 [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From kie at ucla.edu  Wed Jul 19 16:47:32 2006
From: kie at ucla.edu (Kie Zuraw)
Date: Wed, 19 Jul 2006 07:47:32 -0700
Subject: [R] plain shading (not residuals) in mosaic plot
Message-ID: <20060719074732.syj6nbjo08g0w8cc@mail.ucla.edu>

Hello. I've been using R for a couple of months and enjoying it a lot. 
This is my first post to R-help.

I'm using the vcd package to make mosaic plots with labels on the tiles 
indicating the number of items in each cell.

For example, I've made this plot:


> allmorph<-structure(c(10, 26, 17, 100, 70, 97, 253, 430, 185, 177, 
> 25, 1), .Dim = as.integer(c(6, 2)), .Dimnames = 
> structure(list(Stem.initial.obstruent = c("p", "t,s", 
> "k","b","d","g"),Subst.behavior=c("unsubstituted","substituted")), 
> .Names = c("Stem-initial obstruent","Behavior according to 
> dictionary")), class = "table")
> mosaic(allmorph,direction="v",pop=FALSE)
> labeling_cells(text=allmorphs,margin=0)(allmorph)


So far so good. What I can't figure out how to do--after searching 
through the vcd documentation 
(http://cran.r-project.org/doc/packages/vcd.pdf), Googling, and 
checking the r-help archive--is how to shade the tiles according to 
their values for the variables rather than to reflect residuals. That 
is, I want all the tiles at the bottom, whose value for the x-axis 
variable is "substituted", to be dark grey, and those at the top, in 
the "unsubstituted" category, to be light grey.

I know how to do it with mosaicplot():

> mosaicplot(morphs3,color=c(grey(0.8),grey(0.4)))

...but this doesn't work with mosaic(): the command 
"mosaic(morphs3,color=c(grey(0.8),grey(0.4)))" yields a plot with all 
tiles the same color. And conversely, I can't find a way to use 
mosaicplot() and add numeric labels to the tiles--without much hope of 
success, I tried combining mosaicplot() with labeling_cells(), but, 
unsurprisingly, it didn't work:

> mosaicplot(morphs3,color=c(grey(0.8),grey(0.4)),pop=FALSE)
Warning message:
extra argument(s) 'pop' will be disregarded in: 
mosaicplot.default(morphs3, color = c(grey(0.8), grey(0.4)),
> labeling_cells(text=morphs3,margin=0)(morphs3)
Error in downViewport.vpPath(vpPathDirect(name), strict, recording = 
recording) :        Viewport 'cell:Stem-initial obstruent=p,Behavior 
according to dictionary=unsubstituted' was not found


Does anyone know how to get both the shading I want and the labels I 
want, whether with mosaic(), with mosaicplot(), or in some other way?

Thanks for your attention.

-Kie Zuraw


From rmh at temple.edu  Wed Jul 19 17:09:09 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 19 Jul 2006 11:09:09 -0400 (EDT)
Subject: [R] R and DDE (Dynamic Data Exchange)
Message-ID: <20060719110909.BEK78924@po-d.temple.edu>

Gabor,

Thank you for that information.

The reason for choosing DDE is that GNU emacs doesn't speak COM.
Every year or so I ask on the emacs-for-windows list, and every year
I get the answer no.

DDE is the technology that I have been using in ESS for sending
information to the S-Plus Commands window in the S-Plus GUI on
Windows.  I want to send information to the Rgui window on
Windows in the same way.

The Rterm window runs on Windows emacs in an *R* buffer exactly
the way R runs on Unix.  But it doesn't interact smoothly with
other Windows programs.

The specific difficulty that I want to solve now is the
interaction with RExcel.  RExcel and RGui work together smoothly.
RExcel and Rterm in an emacs *R* buffer do not work
smoothly (emacs buffers freeze requiring ^G to get control back).


Rich


From tlumley at u.washington.edu  Wed Jul 19 17:11:33 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 19 Jul 2006 08:11:33 -0700 (PDT)
Subject: [R] Problem with ordered logistic regression using polr
	function.
In-Reply-To: <44BDB26E.1050106@umn.edu>
References: <44BDB26E.1050106@umn.edu>
Message-ID: <Pine.LNX.4.64.0607190803520.6767@homer23.u.washington.edu>

On Wed, 19 Jul 2006, Debarchana Ghosh wrote:

> Hi,
> I'm trying to fit a ordered logistic regression. The response variable
> (y) has three levels (0,1,2).
> The command I've used is:
>
> /ordlog<-polr(y~x1+x2+x3+x4, data=finalbase, subset=heard, weight=wt,
> na.action=na.omit)
> /

> (There are no NA's in y but there are NA's in X's)
>
> The error I'm getting is:
> /Warning messages:
> 1: non-integer #successes in a binomial glm! in: eval(expr, envir, enclos)
> 2: design appears to be rank-deficient, so dropping some coefs in:
> polr(right.ans ~ S107 + children + work + rel + media + V013 +
> /

Those are not errors. They are warnings (and I told you yesterday that the 
first one would happen and would be harmless with probability weights). 
The second warning may indicate a real problem or not: you can tell by 
looking at which coefficients have been dropped.

Also, if that is the warning message then the polr() call you used is not 
the one you said you were using.

> Also, if I write
> /summary(ordlog)/
>
> I'm getting the following error:
>
> /Re-fitting to get Hessian
>
> Error in if (all(pr > 0)) -sum(wt * log(pr)) else Inf :
>        missing value where TRUE/FALSE needed
>
> /Could anybody point out the problem?

You can specify Hess=TRUE in the original polr() call so that refitting 
would not be necessary.  This is sensible anyway if you know you are going 
to be computing standard errors.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From f_bresson at yahoo.fr  Wed Jul 19 17:16:47 2006
From: f_bresson at yahoo.fr (Florent Bresson)
Date: Wed, 19 Jul 2006 17:16:47 +0200 (CEST)
Subject: [R] RE : Re:  Progress in a loop
In-Reply-To: <1153319495.18110.55.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <20060719151647.68870.qmail@web26808.mail.ukl.yahoo.com>

Thanks, it is exactly what I was looking for.

Florent Bresson

--- Gavin Simpson <gavin.simpson at ucl.ac.uk> a ?crit?:

> On Wed, 2006-07-19 at 16:12 +0200, Florent Bresson
> wrote:
> > Hi, I have to use a loop to perform a quite
> computer
> > intensive estimation and I would like to know the
> > progress of the loop during the process. I tried
> to
> > include something like
> > 
> >   print(paste(k,date(),sep=" : "))
> > 
> > where k is the number of the iteration, but the
> result
> > appears only at the end of the loop. Can someone
> help
> > me please ?
> 
> Windows I assume - you are asked to provide some
> rudimentary information
> about your system when asking for help as the
> posting guide states.
> 
> You need to use flush.console() on Windows, as in:
> 
> dat <- matrix(runif(20), ncol = 2)
> res <- numeric(length = 10)
> for(i in 1:10) {
>    res[i] <- sum(dat[i, ])
>    cat(paste("Iteration #:", i, "-", date(), "\n"))
>    flush.console()
> }
> 
> Depending on how you want the printed statements to
> appear on the
> screen, you'd be better off with cat not print,
> e.g.: 
> 
> cat(paste(k,date(),sep=" : "))
> 
> If this isn't the solution, post a reply containing
> information about
> your R version, OS, and a reproducible example
> (simple code & data as I
> did) to show us the problem.
> 
> HTH
> 
> G
> -- 
>
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>  Gavin Simpson                 [t] +44 (0)20 7679
> 0522
>  ECRC & ENSIS, UCL Geography,  [f] +44 (0)20 7679
> 0565
>  Pearson Building,             [e]
> gavin.simpsonATNOSPAMucl.ac.uk
>  Gower Street, London          [w]
> http://www.ucl.ac.uk/~ucfagls/cv/
>  UK. WC1E 6BT.                 [w]
> http://www.ucl.ac.uk/~ucfagls/
>
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> 
> 



	

	
		
___________________________________________________________________________ 
D?couvrez un nouveau moyen de poser toutes vos questions quelque soit le sujet !


From neuro3000 at hotmail.com  Wed Jul 19 17:23:17 2006
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Wed, 19 Jul 2006 11:23:17 -0400
Subject: [R] How would you export a 3-dimensional array to an SQL
	database?
In-Reply-To: <834204C0D7C6D611A3BB000255FC6E9D0DF36063@lbmsg002.fbn-nbf.local>
Message-ID: <BAY112-F13CD6352187EE29DAC465DAF600@phx.gbl>

Here's a way to flatten and unflatten your array.

a <-array(1:24, 2:4)
d <-dim(a)

flattened <-cbind(expand.grid(1:d[1],1:d[2],1:d[3]),as.vector(a))

unflattened <-array(flattened[,4],d)

identical(a,unflattened)
a==unflattened

NeuroRox


>From: "Lapointe, Pierre" <Pierre.Lapointe at nbf.ca>
>To: "'r-help at stat.math.ethz.ch'" <r-help at stat.math.ethz.ch>
>Subject: [R] How would you export a 3-dimensional array to an SQL database?
>Date: Wed, 19 Jul 2006 08:45:42 -0400
>
>Hello,
>
>How would you export a 3-dimensional array to an SQL database?
>
>a<-array(1:24, 2:4)
>
>Is there an open source DB that would be more adequate for this type of
>operation?
>
>Is there a way to reshape/flatten a 3-dimensional array?
>
>Regards,
>
>Pierre Lapointe
>
>**************************************************
>AVIS DE NON-RESPONSABILITE: Ce document transmis par courrie...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From guillaume.blanchet.1 at umontreal.ca  Wed Jul 19 17:29:11 2006
From: guillaume.blanchet.1 at umontreal.ca (Guillaume Blanchet)
Date: Wed, 19 Jul 2006 11:29:11 -0400
Subject: [R] connection network - influence of site
Message-ID: <a06230900c0e3f00c48b2@[192.168.0.12]>

Dear R users,

I'm trying to construct a distance matrix based on a nb object 
created by spdep where sites must have a larger influence in one 
direction then in the other.

Here is an example to better illustrate what I need:

Let say I have the following Gabriel connection network


library(spdep)
library(ade4)
data(orbatid)

nbgab<-graph2nb(gabrielneigh(as.matrix(oribatid$xy)))
plot(nbgab,oribatid$xy)

text(oribatid$xy,as.character(1:70),pos=1) # site number
arrows(-0.3,0,-0.3,10) # gradient direction


In that example, site 1 shouldn't influence site 2 (so it should have 
a distance if 0 in the distance matrix). However site 1 should 
influence site 7 (1 in the distance matrix), it should also influence 
site 12 and 10 (2 in the distance matrix), and site 13,14,17 and 20 
(3 in the distance matrix) and so on for every site. Also, the 
smallest number of "jumps" from one site to the other has to be 
considered (e.g. 1 to 20 can be connected through 3 links (1 -> 7 -> 
10 -> 20) or through 4 links (1 -> 7 ->  12 -> 13 -> 20)).

Considering the gradient, it has to be noted that the distance matrix 
will be asymmetric (e.g. site 7 doesn't influence site 1).

This needs to be done for various connection network, going from 
regular grid (rook connections) to networks such as the one presented 
above.

Can it be done ? If it can, I would be very happy for some one to 
give me a hand. And if it has already been done I would be glad to be 
pointed in the right direction.


Thanks in advance !!

Guillaume Blanchet


From ggrothendieck at gmail.com  Wed Jul 19 17:36:52 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 19 Jul 2006 11:36:52 -0400
Subject: [R] plain shading (not residuals) in mosaic plot
In-Reply-To: <20060719074732.syj6nbjo08g0w8cc@mail.ucla.edu>
References: <20060719074732.syj6nbjo08g0w8cc@mail.ucla.edu>
Message-ID: <971536df0607190836i7b7934b2sb73c304b929fcccf@mail.gmail.com>

If you look at ?mosaic the ... argument says it gets passed to strucplot and
looking at ?strucplot we see it accepts a gp= arg so try this (same
as your plus gp= arg):

cols <- c(grey(0.8),grey(0.4))
mosaic(allmorph, direction = "v", pop = FALSE, gp = list(col = cols))

On 7/19/06, Kie Zuraw <kie at ucla.edu> wrote:
> Hello. I've been using R for a couple of months and enjoying it a lot.
> This is my first post to R-help.
>
> I'm using the vcd package to make mosaic plots with labels on the tiles
> indicating the number of items in each cell.
>
> For example, I've made this plot:
>
>
> > allmorph<-structure(c(10, 26, 17, 100, 70, 97, 253, 430, 185, 177,
> > 25, 1), .Dim = as.integer(c(6, 2)), .Dimnames =
> > structure(list(Stem.initial.obstruent = c("p", "t,s",
> > "k","b","d","g"),Subst.behavior=c("unsubstituted","substituted")),
> > .Names = c("Stem-initial obstruent","Behavior according to
> > dictionary")), class = "table")
> > mosaic(allmorph,direction="v",pop=FALSE)
> > labeling_cells(text=allmorphs,margin=0)(allmorph)
>
>
> So far so good. What I can't figure out how to do--after searching
> through the vcd documentation
> (http://cran.r-project.org/doc/packages/vcd.pdf), Googling, and
> checking the r-help archive--is how to shade the tiles according to
> their values for the variables rather than to reflect residuals. That
> is, I want all the tiles at the bottom, whose value for the x-axis
> variable is "substituted", to be dark grey, and those at the top, in
> the "unsubstituted" category, to be light grey.
>
> I know how to do it with mosaicplot():
>
> > mosaicplot(morphs3,color=c(grey(0.8),grey(0.4)))
>
> ...but this doesn't work with mosaic(): the command
> "mosaic(morphs3,color=c(grey(0.8),grey(0.4)))" yields a plot with all
> tiles the same color. And conversely, I can't find a way to use
> mosaicplot() and add numeric labels to the tiles--without much hope of
> success, I tried combining mosaicplot() with labeling_cells(), but,
> unsurprisingly, it didn't work:
>
> > mosaicplot(morphs3,color=c(grey(0.8),grey(0.4)),pop=FALSE)
> Warning message:
> extra argument(s) 'pop' will be disregarded in:
> mosaicplot.default(morphs3, color = c(grey(0.8), grey(0.4)),
> > labeling_cells(text=morphs3,margin=0)(morphs3)
> Error in downViewport.vpPath(vpPathDirect(name), strict, recording =
> recording) :        Viewport 'cell:Stem-initial obstruent=p,Behavior
> according to dictionary=unsubstituted' was not found
>
>
> Does anyone know how to get both the shading I want and the labels I
> want, whether with mosaic(), with mosaicplot(), or in some other way?
>
> Thanks for your attention.
>
> -Kie Zuraw
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From anthony.staines at gmail.com  Wed Jul 19 17:40:28 2006
From: anthony.staines at gmail.com (Anthony Staines)
Date: Wed, 19 Jul 2006 16:40:28 +0100
Subject: [R] Fwd: Classification error rate increased by bagging - any ideas?
In-Reply-To: <6d3975af0607181522h615ec128ue2fe22d5fbd55ab9@mail.gmail.com>
References: <6d3975af0607181522h615ec128ue2fe22d5fbd55ab9@mail.gmail.com>
Message-ID: <6d3975af0607190840t1a838f9dv6ca82512c522afa2@mail.gmail.com>

Hi,

I'm analysing some anthropometric data on fifty odd skull bases. We know the
gender of each skull,  and we are trying to develop a predictor to identify the
sex of unknown skulls.

Rpart with cross-validation produces two models - one of which predicts gender
for Males well, and Females poorly, and the other does the opposite (Females
well, and Males poorly). In both cases the error rate for the worse predicted
gender is close to 50%, and for the better predicted gender about 15%.

Bagging tree models produces a model which classifies both males and
females equally well (or equally poorly), but has an overall error rate
(just over 30%) higher than either of the rpart models (about 25%).

My instinct is to go for the bagging results, as they seem more reasonable,
but my colleagues really like the lower overall error rate. Any thoughts?

Ta,
Anthony Staines--
Dr. Anthony Staines, Senior Lecturer in Epidemiology.
School  of Public Health and Population Sciences, UCD, Earlsfort
Terrace, Dublin 2, Ireland.
Tel:- +353 1 716 7345. Fax:- +353 1 716 7407 Mobile:- +353 86 606 9713
Web:- http://phm.ucd.ie


From YCH at softcomputing.com  Wed Jul 19 17:41:38 2006
From: YCH at softcomputing.com (Yohan CHOUKROUN)
Date: Wed, 19 Jul 2006 17:41:38 +0200
Subject: [R]  how to use large data set ?
Message-ID: <C8F48FD780E12D4DB197507B897D00DD072F59C8@ntexch.softcomputing.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060719/a0844373/attachment.pl 

From Mat.Soukup at fda.hhs.gov  Wed Jul 19 17:41:16 2006
From: Mat.Soukup at fda.hhs.gov (Soukup, Mat)
Date: Wed, 19 Jul 2006 11:41:16 -0400
Subject: [R] trellis.focus with postscript device
Message-ID: <27CA3827C6B33E40874682C469E774DD02AC3107@FMD3CT001.fda.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060719/8c0ebca6/attachment.pl 

From deepayan.sarkar at gmail.com  Wed Jul 19 17:58:06 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 19 Jul 2006 10:58:06 -0500
Subject: [R] trellis.focus with postscript device
In-Reply-To: <27CA3827C6B33E40874682C469E774DD02AC3107@FMD3CT001.fda.gov>
References: <27CA3827C6B33E40874682C469E774DD02AC3107@FMD3CT001.fda.gov>
Message-ID: <eb555e660607190858rbec81f0r7c4c3da1acf55cba@mail.gmail.com>

On 7/19/06, Soukup, Mat <Mat.Soukup at fda.hhs.gov> wrote:
> Hello.
>
> First: R 2.3.1 on Windows XP.
>
> I am trying to add information (sample size) to the Trellis strips which
> I am successful using the trellis.focus function with the default
> Windows device. However, I typically use the postscript device as I use
> LaTeX and \includegraphic for incorporating graphs into stat reviews.
>
> Here's some example code (apologies for the lack of creativity and
> resemblance to a real example)
>
> yy <- c(rnorm(20,2),rnorm(35,3), rnorm(30,2),rnorm(20,3),rnorm(4,2),
> rnorm(10,3))
> xx <- c(1:20,1:35,1:30,1:20,1:4,1:10)
> gg <- rep(c('A','B','A','B','A','B'), c(20,35,30,20,4,10))
> pp <- rep(c('Cond 1','Cond 2','Cond 3'), c(55, 50, 14))
>
> xyplot(yy ~ xx | pp, groups=gg)
> trellis.focus('strip', 1, 1)
> ltext(0,.5,'20',col='red', pos=4)
> ltext(1,.5,'35',col='black', pos=2)
> trellis.unfocus()
> trellis.focus('strip', 2, 1)
> ltext(0,.5,'30',col='red', pos=4)
> ltext(1,.5,'20',col='black', pos=2)
> trellis.unfocus()
> trellis.focus('strip', 1, 2)
> ltext(0,.5,'4',col='red', pos=4)
> ltext(1,.5,'10',col='black', pos=2)
> trellis.unfocus()
>
> This works. But if I do,
>
> postscript('C:/TEMP/example.eps')
> # All code as above
> dev.off()
>
> I notice a problem with the graphic. When looking at the EPS figure, the
> only strip with added data is the first one (bottom left) with the strip
> still highlighted in red (i.e. it doesn't appear that trellis.unfocus()
> was executed).

Actually, you have produced a multiple-page postscript file, with what
you really want in the last page. If you highlight the strips when
calling trelis.focus, they have to be un-highlighted by
trellis.unfocus. In theory, this is just a removal of a rectangle
object. In practice, grid achieves this by drawing a new page. You
need to avoid this.

Your options are:

(1) add 'highlight = FALSE' to all trellis.focus() calls
(2) run the script in batch mode, where the default highlight =
interactive() is FALSE

I'll think about adding an option to control the default.

Deepayan


From rene.eschen at unifr.ch  Wed Jul 19 18:00:38 2006
From: rene.eschen at unifr.ch (ESCHEN Rene)
Date: Wed, 19 Jul 2006 18:00:38 +0200
Subject: [R] Random structure of nested design in lme
References: <2323A6D37908A847A7C32F1E3662C80E132CEE@dc1ex01.air.org>
Message-ID: <E632249B3E11B14AAABA75FDB5F32B24132A0B@EXCHANGE4.unifr.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060719/5975b243/attachment.pl 

From sundar.dorai-raj at pdf.com  Wed Jul 19 18:01:02 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 19 Jul 2006 11:01:02 -0500
Subject: [R] Sweave and multipage lattice
In-Reply-To: <LPEJLJACLINDNMBMFAFIGEHOCDAA.dieter.menne@menne-biomed.de>
References: <LPEJLJACLINDNMBMFAFIGEHOCDAA.dieter.menne@menne-biomed.de>
Message-ID: <44BE573E.7010901@pdf.com>



Dieter Menne wrote:
> Dear R-Listeners,
> 
> as the Sweave faq says:
> 
> http://www.ci.tuwien.ac.at/~leisch/Sweave/FAQ.html
> 
> creating several figures from one figure chunk does not work, and for
> standard graphics, a workaround is given. Now I have a multipage trellis
> plot with an a-priori unknown number of pages, and I don't see an elegant
> way of dividing it up into multiple pdf-files.
> 
> I noted there is a "page" event handler in the ... parameters, which would
> provide a handle to open/close the file.
> 
> Any good idea would be appreciated.
> 
> Dieter
> 

Hi, Dieter,

I haven't seen a reply to this and I don't know Sweave. However, will 
the following example work? It does require you know the layout for one 
page.

--sundar

library(lattice)

trellis.device(postscript, file = "barley%02d.eps",
                width = 5, height = 3, onefile = FALSE,
                paper = "special")
## from ?xyplot
dotplot(variety ~ yield | site, data = barley, groups = year,
         key = simpleKey(levels(barley$year), space = "right"),
         xlab = "Barley Yield (bushels/acre) ",
         aspect=0.5, layout = c(1, 1), ylab=NULL)
dev.off()

files <- list.files(pattern = glob2rx("barley*.eps"))
for(file in files)
   cat("\\includegraphics{", file, "}\n\n", sep="")


From rmh at temple.edu  Wed Jul 19 18:10:31 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 19 Jul 2006 12:10:31 -0400 (EDT)
Subject: [R] trellis.focus with postscript device
Message-ID: <20060719121031.BEK87364@po-d.temple.edu>

It works fine as long as you give it the layout.  I used

xyplot(yy ~ xx | pp, groups=gg, layout=c(2,2))

This is needed to make sure that the rows and columns of the trellis
that you reference are consistent with the layout that xyplot has chosen.

Another way to handle this is to change the factor labels

pp <- factor(pp)
levels(pp) <- c("20  Cond 1  35", "30  Cond 2  20", "4  Cond 3  10")
xyplot(yy ~ xx | pp, groups=gg)


Rich


From h.wickham at gmail.com  Wed Jul 19 18:20:32 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 19 Jul 2006 17:20:32 +0100
Subject: [R] Fitting a distribution to peaks in histogram
In-Reply-To: <3483f8d50607190258n1f711443p869f79850e81fa79@mail.gmail.com>
References: <3483f8d50607190258n1f711443p869f79850e81fa79@mail.gmail.com>
Message-ID: <f8e6ff050607190920o366c8a6dvcecc24fbed7eb7f@mail.gmail.com>

> I would like to fit a distribution to each of the peaks in a histogram, such
> as this: http://photos1.blogger.com/blogger/7029/2724/1600/DU145-Bax3-Bcl-xL.png

As a first shot, I'd try fitting a mixture of gamma distributions (say
3), plus a constant term for the highest bin.  You could do this using
ML.  If the number of peaks is truly unknown, this will be a little
trickier but still possible and you could use the LRT to chose between
them.

> Integrate the area between each two peaks, using the means and widths of the
> distributions fitted to the two peaks. I will be using the integrate
> function

Why do you want to do this?

>
> The histogram is based on approximately 15000 events, which makes Mclust and
> pam (which both delivers the information I need) less useful.

If you have unbinned data, it would be better (more precise/powerful)
to use that.

Regards,

Hadley


From ahenningsen at agric-econ.uni-kiel.de  Wed Jul 19 18:25:39 2006
From: ahenningsen at agric-econ.uni-kiel.de (Arne Henningsen)
Date: Wed, 19 Jul 2006 18:25:39 +0200
Subject: [R] WLS ins systemfit question
In-Reply-To: <20060719142213.85902.qmail@web61324.mail.yahoo.com>
References: <20060719142213.85902.qmail@web61324.mail.yahoo.com>
Message-ID: <200607191825.39511.ahenningsen@agric-econ.uni-kiel.de>

On Wednesday 19 July 2006 16:22, Benn Fine wrote:
> How does one specify the weights for WLS in the
> systemfit command ?
>
> That is, there is a weight option in lm(), but there
> doesn't seem to be weight option for systemfit("WLS")

WLS in systemfit means that the different _equations_ (not the observations) 
are weighted differently. The weights of the equations are taken from a 
first-step OLS regression by taking the (inverse of the) residual variances 
of each equation. Of course, this can be iterated. The WLS estimation is 
similar to SUR but in WLS all off-diagonal elements of the residual 
covariance matrix are constrained to be zero.

Best regards,
Arne


>
> Thanks!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/


From Mat.Soukup at fda.hhs.gov  Wed Jul 19 18:35:48 2006
From: Mat.Soukup at fda.hhs.gov (Soukup, Mat)
Date: Wed, 19 Jul 2006 12:35:48 -0400
Subject: [R] trellis.focus with postscript device
In-Reply-To: <eb555e660607190858rbec81f0r7c4c3da1acf55cba@mail.gmail.com>
Message-ID: <27CA3827C6B33E40874682C469E774DD02AC3108@FMD3CT001.fda.gov>

Thanks Deepayan.

Adding the argument highlight=FALSE to each trellis.focus() call worked
marvelously.

Cheers,

Mat 

-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com] 
Sent: Wednesday, July 19, 2006 11:58 AM
To: Soukup, Mat
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] trellis.focus with postscript device

On 7/19/06, Soukup, Mat <Mat.Soukup at fda.hhs.gov> wrote:
> Hello.
>
> First: R 2.3.1 on Windows XP.
>
> I am trying to add information (sample size) to the Trellis strips
which
> I am successful using the trellis.focus function with the default
> Windows device. However, I typically use the postscript device as I
use
> LaTeX and \includegraphic for incorporating graphs into stat reviews.
>
> Here's some example code (apologies for the lack of creativity and
> resemblance to a real example)
>
> yy <- c(rnorm(20,2),rnorm(35,3), rnorm(30,2),rnorm(20,3),rnorm(4,2),
> rnorm(10,3))
> xx <- c(1:20,1:35,1:30,1:20,1:4,1:10)
> gg <- rep(c('A','B','A','B','A','B'), c(20,35,30,20,4,10))
> pp <- rep(c('Cond 1','Cond 2','Cond 3'), c(55, 50, 14))
>
> xyplot(yy ~ xx | pp, groups=gg)
> trellis.focus('strip', 1, 1)
> ltext(0,.5,'20',col='red', pos=4)
> ltext(1,.5,'35',col='black', pos=2)
> trellis.unfocus()
> trellis.focus('strip', 2, 1)
> ltext(0,.5,'30',col='red', pos=4)
> ltext(1,.5,'20',col='black', pos=2)
> trellis.unfocus()
> trellis.focus('strip', 1, 2)
> ltext(0,.5,'4',col='red', pos=4)
> ltext(1,.5,'10',col='black', pos=2)
> trellis.unfocus()
>
> This works. But if I do,
>
> postscript('C:/TEMP/example.eps')
> # All code as above
> dev.off()
>
> I notice a problem with the graphic. When looking at the EPS figure,
the
> only strip with added data is the first one (bottom left) with the
strip
> still highlighted in red (i.e. it doesn't appear that
trellis.unfocus()
> was executed).

Actually, you have produced a multiple-page postscript file, with what
you really want in the last page. If you highlight the strips when
calling trelis.focus, they have to be un-highlighted by
trellis.unfocus. In theory, this is just a removal of a rectangle
object. In practice, grid achieves this by drawing a new page. You
need to avoid this.

Your options are:

(1) add 'highlight = FALSE' to all trellis.focus() calls
(2) run the script in batch mode, where the default highlight =
interactive() is FALSE

I'll think about adding an option to control the default.

Deepayan


From ulriks at ruc.dk  Wed Jul 19 18:56:32 2006
From: ulriks at ruc.dk (Ulrik Stervbo)
Date: Wed, 19 Jul 2006 18:56:32 +0200
Subject: [R] Fitting a distribution to peaks in histogram
In-Reply-To: <f8e6ff050607190920o366c8a6dvcecc24fbed7eb7f@mail.gmail.com>
References: <3483f8d50607190258n1f711443p869f79850e81fa79@mail.gmail.com>
	<f8e6ff050607190920o366c8a6dvcecc24fbed7eb7f@mail.gmail.com>
Message-ID: <3483f8d50607190956h45119bf5h282550e58939f7b5@mail.gmail.com>

On 7/19/06, hadley wickham <h.wickham at gmail.com> wrote:
> > I would like to fit a distribution to each of the peaks in a histogram, such
> > as this: http://photos1.blogger.com/blogger/7029/2724/1600/DU145-Bax3-Bcl-xL.png
>
> As a first shot, I'd try fitting a mixture of gamma distributions (say
> 3), plus a constant term for the highest bin.  You could do this using
> ML.  If the number of peaks is truly unknown, this will be a little
> trickier but still possible and you could use the LRT to chose between
> them.

Can you be a bit more excact? I a biologist and relatively new to R

>
> > Integrate the area between each two peaks, using the means and widths of the
> > distributions fitted to the two peaks. I will be using the integrate
> > function
>
> Why do you want to do this?

I am measureing the amount of DNA in cells, and I need to know the
percentage of cells in a part of the cell cycle; that the percentage
of cells in the first peak, in the second peak and so on. I want to
integrate the area between to two cells, because that apparently is
how its none (as far as I can tell from the literature)

>
> >
> > The histogram is based on approximately 15000 events, which makes Mclust and
> > pam (which both delivers the information I need) less useful.
>
> If you have unbinned data, it would be better (more precise/powerful)
> to use that.

It very probably is better, but mclust had no result after running for
at least 2 hours (I terminated the function after two hours), and as I
generate 50-100 datasets, such as the one used for the histogram, as
week, I would like to find a faster solution
>
> Regards,
>
> Hadley
>

Thanks
Ulrik
-- 
Blog: http://ulrikstervbo.blogspot.com


From davidr at rhotrading.com  Wed Jul 19 19:00:33 2006
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Wed, 19 Jul 2006 12:00:33 -0500
Subject: [R] Stirling numbers
Message-ID: <F9F2A641C593D7408925574C05A7BE77075E94@rhopost.rhotrading.com>

Well, given Martin's 2nd kind function and the fact that the 1st and 2nd

kind matrices are inverses, you can get at least some of what you want
by:

... # fill a matrix S2 with second kind numbers and zeros
> S2
     [,1] [,2] [,3] [,4] [,5] [,6] [,7]
[1,]    1    0    0    0    0    0    0
[2,]    1    1    0    0    0    0    0
[3,]    1    3    1    0    0    0    0
[4,]    1    7    6    1    0    0    0
[5,]    1   15   25   10    1    0    0
[6,]    1   31   90   65   15    1    0
[7,]    1   63  301  350  140   21    1
> abs(round(solve(S2)))
     [,1] [,2] [,3] [,4] [,5] [,6] [,7]
[1,]    1    0    0    0    0    0    0
[2,]    1    1    0    0    0    0    0
[3,]    2    3    1    0    0    0    0
[4,]    6   11    6    1    0    0    0
[5,]   24   50   35   10    1    0    0
[6,]  120  274  225   85   15    1    0
[7,]  720 1764 1624  735  175   21    1 

Not sure how big arguments you need.

HTH,
David L. Reiner
Rho Trading Securities, LLC
Chicago  IL  60605
312-362-4963

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin Maechler
Sent: Wednesday, July 19, 2006 9:19 AM
To: Robin Hankin
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Stirling numbers

>>>>> "Robin" == Robin Hankin <r.hankin at noc.soton.ac.uk>
>>>>>     on Wed, 19 Jul 2006 14:25:21 +0100 writes:

    Robin> Hi anyone coded up Stirling numbers in R?

"Sure" ;-)

    Robin> [I need unsigned Stirling numbers of the first kind]

but with my quick search, I can only see those for which I had
"2nd kind" :

-o<--o<-----------------------------------------------------------------
--

##-- Stirling numbers of the 2nd kind
##-- (Abramowitz/Stegun: 24,1,4 (p. 824-5 ; Table 24.4, p.835)

##> S^{(m)}_n = number of ways of partitioning a set of $n$ elements
into $m$
##>	non-empty subsets

Stirling2 <- function(n,m)
{
    ## Purpose:  Stirling Numbers of the 2-nd kind
    ## 		S^{(m)}_n = number of ways of partitioning a set of
    ##                      $n$ elements into $m$ non-empty subsets
    ## Author: Martin Maechler, Date:  May 28 1992, 23:42
    ## ----------------------------------------------------------------
    ## Abramowitz/Stegun: 24,1,4 (p. 824-5 ; Table 24.4, p.835)
    ## Closed Form : p.824 "C."
    ## ----------------------------------------------------------------

    if (0 > m || m > n) stop("'m' must be in 0..n !")
    k <- 0:m
    sig <- rep(c(1,-1)*(-1)^m, length= m+1)# 1 for m=0; -1 1 (m=1)
    ## The following gives rounding errors for (25,5) :
    ## r <- sum( sig * k^n /(gamma(k+1)*gamma(m+1-k)) )
    ga <- gamma(k+1)
    round(sum( sig * k^n /(ga * rev(ga))))
}

options(digits=15)
for (n in 11:31) cat("n =", n," S_n(5) =", Stirling2(n,5), "\n")
n <- 25
for(k in c(3,5,10))
    cat(" S_",n,"^(",formatC(k,wid=2),") = ", Stirling2(n,k),"\n",sep =
"")

for (n in 1:15)
    cat(formatC(n,w=2),":", sapply(1:n, Stirling2, n = n),"\n")
##  1 : 1
##  2 : 1 1
##  3 : 1 3 1
##  4 : 1 7 6 1
##  5 : 1 15 25 10 1
##  6 : 1 31 90 65 15 1
##  7 : 1 63 301 350 140 21 1
##  8 : 1 127 966 1701 1050 266 28 1
##  9 : 1 255 3025 7770 6951 2646 462 36 1
## 10 : 1 511 9330 34105 42525 22827 5880 750 45 1
## 11 : 1 1023 28501 145750 246730 179487 63987 11880 1155 55 1
## 12 : 1 2047 86526 611501 1379400 1323652 627396 159027 22275 1705 66
1
## 13 : 1 4095 261625 2532530 7508501 9321312 5715424 1899612 359502
39325 2431 78 1
## 14 : 1 8191 788970 10391745 40075035 63436373 49329280 20912320
5135130 752752 66066 3367 91 1
## 15 : 1 16383 2375101 42355950 210766920 420693273 408741333 216627840
67128490 12662650 1479478 106470 4550 105 1


plot(6:30, sapply(6:30, Stirling2, m=5), log = "y", type = "l")
## Abramowitz says something like  S2(n,m) ~ m^n / m!
##                                 ------------------
nn <- 6:30; sapply(nn, Stirling2, m=5)  / (5^nn / gamma(5+1))
## 0.114 0.21 ...... 0.99033 0.992266 0.993812

-o<--o<-----------------------------------------------------------------
--

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From msubianto at gmail.com  Wed Jul 19 19:04:03 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Wed, 19 Jul 2006 19:04:03 +0200
Subject: [R] plain shading (not residuals) in mosaic plot
In-Reply-To: <971536df0607190836i7b7934b2sb73c304b929fcccf@mail.gmail.com>
References: <20060719074732.syj6nbjo08g0w8cc@mail.ucla.edu>
	<971536df0607190836i7b7934b2sb73c304b929fcccf@mail.gmail.com>
Message-ID: <c7c17cef0607191004v5531c7e4of9952ead741a6d5@mail.gmail.com>

Maybe like this:
mosaic(allmorph, direction = "v", pop = FALSE,
gp=gpar(fill=c(grey(0.8),grey(0.4))))

Best, Muhammad Subianto

On 7/19/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> If you look at ?mosaic the ... argument says it gets passed to strucplot and
> looking at ?strucplot we see it accepts a gp= arg so try this (same
> as your plus gp= arg):
>
> cols <- c(grey(0.8),grey(0.4))
> mosaic(allmorph, direction = "v", pop = FALSE, gp = list(col = cols))
>
> On 7/19/06, Kie Zuraw <kie at ucla.edu> wrote:
> > Hello. I've been using R for a couple of months and enjoying it a lot.
> > This is my first post to R-help.
> >
> > I'm using the vcd package to make mosaic plots with labels on the tiles
> > indicating the number of items in each cell.
> >
> > For example, I've made this plot:
> >
> >
> > > allmorph<-structure(c(10, 26, 17, 100, 70, 97, 253, 430, 185, 177,
> > > 25, 1), .Dim = as.integer(c(6, 2)), .Dimnames =
> > > structure(list(Stem.initial.obstruent = c("p", "t,s",
> > > "k","b","d","g"),Subst.behavior=c("unsubstituted","substituted")),
> > > .Names = c("Stem-initial obstruent","Behavior according to
> > > dictionary")), class = "table")
> > > mosaic(allmorph,direction="v",pop=FALSE)
> > > labeling_cells(text=allmorphs,margin=0)(allmorph)
> >
> >
> > So far so good. What I can't figure out how to do--after searching
> > through the vcd documentation
> > (http://cran.r-project.org/doc/packages/vcd.pdf), Googling, and
> > checking the r-help archive--is how to shade the tiles according to
> > their values for the variables rather than to reflect residuals. That
> > is, I want all the tiles at the bottom, whose value for the x-axis
> > variable is "substituted", to be dark grey, and those at the top, in
> > the "unsubstituted" category, to be light grey.
> >
> > I know how to do it with mosaicplot():
> >
> > > mosaicplot(morphs3,color=c(grey(0.8),grey(0.4)))
> >
> > ...but this doesn't work with mosaic(): the command
> > "mosaic(morphs3,color=c(grey(0.8),grey(0.4)))" yields a plot with all
> > tiles the same color. And conversely, I can't find a way to use
> > mosaicplot() and add numeric labels to the tiles--without much hope of
> > success, I tried combining mosaicplot() with labeling_cells(), but,
> > unsurprisingly, it didn't work:
> >
> > > mosaicplot(morphs3,color=c(grey(0.8),grey(0.4)),pop=FALSE)
> > Warning message:
> > extra argument(s) 'pop' will be disregarded in:
> > mosaicplot.default(morphs3, color = c(grey(0.8), grey(0.4)),
> > > labeling_cells(text=morphs3,margin=0)(morphs3)
> > Error in downViewport.vpPath(vpPathDirect(name), strict, recording =
> > recording) :        Viewport 'cell:Stem-initial obstruent=p,Behavior
> > according to dictionary=unsubstituted' was not found
> >
> >
> > Does anyone know how to get both the shading I want and the labels I
> > want, whether with mosaic(), with mosaicplot(), or in some other way?
> >
> > Thanks for your attention.
> >
> > -Kie Zuraw
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From water21water at yahoo.com  Wed Jul 19 19:05:56 2006
From: water21water at yahoo.com (junguo liu)
Date: Wed, 19 Jul 2006 10:05:56 -0700 (PDT)
Subject: [R] Answer found: Re:  How to write a function in a graph
In-Reply-To: <Pine.LNX.4.64.0607181435000.24347@homer21.u.washington.edu>
Message-ID: <20060719170556.62997.qmail@web31802.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060719/62c8a742/attachment.pl 

From msubianto at gmail.com  Wed Jul 19 19:06:04 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Wed, 19 Jul 2006 19:06:04 +0200
Subject: [R] plain shading (not residuals) in mosaic plot
In-Reply-To: <971536df0607190836i7b7934b2sb73c304b929fcccf@mail.gmail.com>
References: <20060719074732.syj6nbjo08g0w8cc@mail.ucla.edu>
	<971536df0607190836i7b7934b2sb73c304b929fcccf@mail.gmail.com>
Message-ID: <c7c17cef0607191006v178908cha8646cc5abe794a4@mail.gmail.com>

Maybe like this:
mosaic(allmorph, direction = "v", pop = FALSE,
gp=gpar(fill=c(grey(0.8),grey(0.4))))

Best, Muhammad Subianto


On 7/19/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> If you look at ?mosaic the ... argument says it gets passed to strucplot and
> looking at ?strucplot we see it accepts a gp= arg so try this (same
> as your plus gp= arg):
>
> cols <- c(grey(0.8),grey(0.4))
> mosaic(allmorph, direction = "v", pop = FALSE, gp = list(col = cols))
>
> On 7/19/06, Kie Zuraw <kie at ucla.edu> wrote:
> > Hello. I've been using R for a couple of months and enjoying it a lot.
> > This is my first post to R-help.
> >
> > I'm using the vcd package to make mosaic plots with labels on the tiles
> > indicating the number of items in each cell.
> >
> > For example, I've made this plot:
> >
> >
> > > allmorph<-structure(c(10, 26, 17, 100, 70, 97, 253, 430, 185, 177,
> > > 25, 1), .Dim = as.integer(c(6, 2)), .Dimnames =
> > > structure(list(Stem.initial.obstruent = c("p", "t,s",
> > > "k","b","d","g"),Subst.behavior=c("unsubstituted","substituted")),
> > > .Names = c("Stem-initial obstruent","Behavior according to
> > > dictionary")), class = "table")
> > > mosaic(allmorph,direction="v",pop=FALSE)
> > > labeling_cells(text=allmorphs,margin=0)(allmorph)
> >
> >
> > So far so good. What I can't figure out how to do--after searching
> > through the vcd documentation
> > (http://cran.r-project.org/doc/packages/vcd.pdf), Googling, and
> > checking the r-help archive--is how to shade the tiles according to
> > their values for the variables rather than to reflect residuals. That
> > is, I want all the tiles at the bottom, whose value for the x-axis
> > variable is "substituted", to be dark grey, and those at the top, in
> > the "unsubstituted" category, to be light grey.
> >
> > I know how to do it with mosaicplot():
> >
> > > mosaicplot(morphs3,color=c(grey(0.8),grey(0.4)))
> >
> > ...but this doesn't work with mosaic(): the command
> > "mosaic(morphs3,color=c(grey(0.8),grey(0.4)))" yields a plot with all
> > tiles the same color. And conversely, I can't find a way to use
> > mosaicplot() and add numeric labels to the tiles--without much hope of
> > success, I tried combining mosaicplot() with labeling_cells(), but,
> > unsurprisingly, it didn't work:
> >
> > > mosaicplot(morphs3,color=c(grey(0.8),grey(0.4)),pop=FALSE)
> > Warning message:
> > extra argument(s) 'pop' will be disregarded in:
> > mosaicplot.default(morphs3, color = c(grey(0.8), grey(0.4)),
> > > labeling_cells(text=morphs3,margin=0)(morphs3)
> > Error in downViewport.vpPath(vpPathDirect(name), strict, recording =
> > recording) :        Viewport 'cell:Stem-initial obstruent=p,Behavior
> > according to dictionary=unsubstituted' was not found
> >
> >
> > Does anyone know how to get both the shading I want and the labels I
> > want, whether with mosaic(), with mosaicplot(), or in some other way?
> >
> > Thanks for your attention.
> >
> > -Kie Zuraw
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gunter.berton at gene.com  Wed Jul 19 19:10:02 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 19 Jul 2006 10:10:02 -0700
Subject: [R] Fitting a distribution to peaks in histogram
In-Reply-To: <f8e6ff050607190920o366c8a6dvcecc24fbed7eb7f@mail.gmail.com>
Message-ID: <001401c6ab56$2c668050$725afea9@gne.windows.gene.com>

With this much data, I think it makes more sense to fit a nonparametric
density estimate. ?density does this via a kernel density procedure, but
RSiteSearch('nonparametric density') will find many alternatives. The ash
and mclust packages are two that come to mind, but there are certainly
others.

Of course, if you must have a parametric fit, then you'll have to fit a
mixture of some sort.  But when both the number of components and individual
distributions are to be estimated, this is a nontrivial problem, as one runs
into identifiability issues and corresponding convergence problems. V&R's
discussion of density estimation in MASS has some useful things to say about
these issues, and Ripley's book, "PATTERN RECOGNITION AND NEURAL NETWORKS"
has even more. As both sources indicate, there's a large literature on this
issue and much software.

Cheers,
Bert Gunter
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of hadley wickham
> Sent: Wednesday, July 19, 2006 9:21 AM
> To: Ulrik Stervbo
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Fitting a distribution to peaks in histogram
> 
> > I would like to fit a distribution to each of the peaks in 
> a histogram, such
> > as this: 
> http://photos1.blogger.com/blogger/7029/2724/1600/DU145-Bax3-B
> cl-xL.png
> 
> As a first shot, I'd try fitting a mixture of gamma distributions (say
> 3), plus a constant term for the highest bin.  You could do this using
> ML.  If the number of peaks is truly unknown, this will be a little
> trickier but still possible and you could use the LRT to chose between
> them.
> 
> > Integrate the area between each two peaks, using the means 
> and widths of the
> > distributions fitted to the two peaks. I will be using the integrate
> > function
> 
> Why do you want to do this?
> 
> >
> > The histogram is based on approximately 15000 events, which 
> makes Mclust and
> > pam (which both delivers the information I need) less useful.
> 
> If you have unbinned data, it would be better (more precise/powerful)
> to use that.
> 
> Regards,
> 
> Hadley
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From lobry at biomserv.univ-lyon1.fr  Wed Jul 19 19:49:24 2006
From: lobry at biomserv.univ-lyon1.fr (Jean lobry)
Date: Wed, 19 Jul 2006 19:49:24 +0200
Subject: [R] Reproducible Research - Examples
In-Reply-To: <mailman.11.1153303205.32321.r-help@stat.math.ethz.ch>
References: <mailman.11.1153303205.32321.r-help@stat.math.ethz.ch>
Message-ID: <p0600200ec0e41aa092a0@[134.214.34.142]>

>  Question: Reproducible research is clearly desirable and feasible. Could
>  anyone provide examples (stand-alone URLs or supplementary material in
>  journals) that you would recommend as a models for reproducible research in
>  R?

Here are some attempts based on RWeb, contributed by Jeff Banfield, to allow
for the reproduction of research papers:

http://biomserv.univ-lyon1.fr/~necsulea/repro/
http://pbil.univ-lyon1.fr/members/lobry/repro/gene06/
http://biomserv.univ-lyon1.fr/~palmeira/repro/
http://pbil.univ-lyon1.fr/members/lobry/repro/toxo/
http://pbil.univ-lyon1.fr/datasets/SemonLobryDuret2005/
http://pbil.univ-lyon1.fr/members/lobry/repro/gene05/
http://pbil.univ-lyon1.fr/members/lobry/repro/bioinfo04/
http://pbil.univ-lyon1.fr/members/lobry/repro/lncs04/
http://pbil.univ-lyon1.fr/members/lobry/repro/jag03/
http://pbil.univ-lyon1.fr/members/lobry/repro/jme97/
http://pbil.univ-lyon1.fr/members/lobry/repro/jme95/
http://pbil.univ-lyon1.fr/members/lobry/repro/cabios95/
http://pbil.univ-lyon1.fr/members/lobry/repro/nar94/

I would not recommend these as a "models", but I would like to point out that:

1) If you are already using Sweave (many thanks to Friedrich Leisch) there
    is virtually no extra-cost for the authors of the paper to write the
    corresponding web page.

2) There is no way to be more cristal clear with respect to your reviewers
    during the submission process: data are available online, methods are
    available online, the parameters of the methods are available and
    editable online.

This is good science, but, problem: what would be the equivalent
of JSTOR for this? How people can, say 100 years in the future,
reproduce your results?

Best,

Jean Lobry


P.S. If you want to play with the data used by Student (aka William Sealy
Gosset) for the experimental validation of the famous t-test
(The probable error of a mean. Biometrika, 6:1-25,1908) they are at
your R prompt:
crim<-read.table(file="http://pbil.univ-lyon1.fr/R/donnees/criminals1902.txt")
Can you reproduce Student's results nowadays?

-- 
Jean R. Lobry            (lobry at biomserv.univ-lyon1.fr)
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
allo  : +33 472 43 12 87     fax    : +33 472 43 13 88
http://pbil.univ-lyon1.fr/members/lobry/


From spencer.graves at pdf.com  Wed Jul 19 20:34:59 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 19 Jul 2006 11:34:59 -0700
Subject: [R] How to find S4 generics?
In-Reply-To: <Pine.LNX.4.64.0607190739540.32733@gannet.stats.ox.ac.uk>
References: <878be2c60607150822r5914a231h41d537f2f5b446b4@mail.gmail.com>
	<44BDA929.8000407@pdf.com>
	<Pine.LNX.4.64.0607190739540.32733@gannet.stats.ox.ac.uk>
Message-ID: <44BE7B53.4070104@pdf.com>

Dear Prof. Ripley:

Thanks very much.

	  Am I correct then that the 'methods' function could, at least 
theoretically, be revised so methods(class=...) could identify both S3 
and S4 methods (ignoring inheritance, as it does now, I believe)?

	  I ask, because it's not always obvious (at least to me) what helper 
functions are available.  Some but not all are mentioned in the 
documentation, and even if they are, they may not be featured 
prominently.  For the S3 standard, methods(class=...) reports what's 
available, and it's faster and more reliable than scanning the help 
files.  After I find something with methods(class=...), then I have 
better ideas for what to look for in the documentation.

	  Best Wishes,
	  Spencer Graves

Prof Brian Ripley wrote:
> On Tue, 18 Jul 2006, Spencer Graves wrote:
> 
>> *****
>> ***** "methods" *****
>> *****
>> 	  You have asked an excellent question.  I can provide a partial answer 
>> below.  First, however, I wish to pose a question of my own, which could 
>> help answer your question:
>>
>> 	  How can one obtain a simple list of the available generics for a 
>> class?  For an S3 class, the 'methods' functions provide that.  What 
>> about an S4 class?  That's entirely opaque to me, if I somehow can't 
>> find the relevant information in other ways.  For example, ?lmer-class 
>> lists many but not all of the methods available for objects of class 
>> 'lmer'.  I think I once found a way to get that, but I'm not able to 
>> find documentation on it now.
> 
> It doesn't work the same way.  S3 generics are defined on a single 
> argument and hence have methods for a class, and so it is relevant to ask 
> what generics there are which have methods for a given class - but even 
> then there can be other generics and other methods which dispatch on 
> object from that class by inheritance (e.g. on "lm" for "glm" objects).
> 
> S4 generics dispatch on a signature which can involve two or more classes, 
> and I guess the simplest interpretation of your question is
> 
> `what S4 generics are there which have methods with signatures mentioning 
> this class'.
> 
> Given the decentralized way such information is stored, I think the only 
> way to do that is to find all the generics currently available (via 
> getGenerics or its synonym allGenerics) and then call showMethods on each 
> generic.  In particular, methods are stored in the S4 generic and not in 
> the package defining the method.
> 
> However, I suspect inheritance is much more important here, and there is 
> no way to know if methods for class "ANY" actually work for a specific S4 
> class.
> 
> [...]
>


From ripley at stats.ox.ac.uk  Wed Jul 19 20:41:24 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 19 Jul 2006 19:41:24 +0100 (BST)
Subject: [R] How to find S4 generics?
In-Reply-To: <44BE7B53.4070104@pdf.com>
References: <878be2c60607150822r5914a231h41d537f2f5b446b4@mail.gmail.com>
	<44BDA929.8000407@pdf.com>
	<Pine.LNX.4.64.0607190739540.32733@gannet.stats.ox.ac.uk>
	<44BE7B53.4070104@pdf.com>
Message-ID: <Pine.LNX.4.64.0607191938240.9320@gannet.stats.ox.ac.uk>

On Wed, 19 Jul 2006, Spencer Graves wrote:

> Dear Prof. Ripley:
> 
> Thanks very much.
> 
> 	  Am I correct then that the 'methods' function could, at least
> theoretically, be revised so methods(class=...) could identify both S3 and S4
> methods (ignoring inheritance, as it does now, I believe)?

It is correct that in principle there could be such a function in the 
methods package, but not that methods() in utils could be revised to do 
so.  (For efficiency reasons on the stats4 package in the base 
distribution may depend on methods)

> 	  I ask, because it's not always obvious (at least to me) what helper
> functions are available.  Some but not all are mentioned in the documentation,
> and even if they are, they may not be featured prominently.  For the S3
> standard, methods(class=...) reports what's available, and it's faster and
> more reliable than scanning the help files.  After I find something with
> methods(class=...), then I have better ideas for what to look for in the
> documentation.
> 
> 	  Best Wishes,
> 	  Spencer Graves
> 
> Prof Brian Ripley wrote:
> > On Tue, 18 Jul 2006, Spencer Graves wrote:
> > 
> > > *****
> > > ***** "methods" *****
> > > *****
> > > 	  You have asked an excellent question.  I can provide a partial
> > > answer below.  First, however, I wish to pose a question of my own, which
> > > could help answer your question:
> > >
> > > 	  How can one obtain a simple list of the available generics for a
> > > class?  For an S3 class, the 'methods' functions provide that.  What about
> > > an S4 class?  That's entirely opaque to me, if I somehow can't find the
> > > relevant information in other ways.  For example, ?lmer-class lists many
> > > but not all of the methods available for objects of class 'lmer'.  I think
> > > I once found a way to get that, but I'm not able to find documentation on
> > > it now.
> > 
> > It doesn't work the same way.  S3 generics are defined on a single argument
> > and hence have methods for a class, and so it is relevant to ask what
> > generics there are which have methods for a given class - but even then
> > there can be other generics and other methods which dispatch on object from
> > that class by inheritance (e.g. on "lm" for "glm" objects).
> > 
> > S4 generics dispatch on a signature which can involve two or more classes,
> > and I guess the simplest interpretation of your question is
> > 
> > `what S4 generics are there which have methods with signatures mentioning
> > this class'.
> > 
> > Given the decentralized way such information is stored, I think the only way
> > to do that is to find all the generics currently available (via getGenerics
> > or its synonym allGenerics) and then call showMethods on each generic.  In
> > particular, methods are stored in the S4 generic and not in the package
> > defining the method.
> > 
> > However, I suspect inheritance is much more important here, and there is no
> > way to know if methods for class "ANY" actually work for a specific S4
> > class.
> > 
> > [...]
> > 
> 
> 
> 	
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From sw283 at maths.bath.ac.uk  Wed Jul 19 23:13:20 2006
From: sw283 at maths.bath.ac.uk (Simon Wood)
Date: Wed, 19 Jul 2006 22:13:20 +0100 (BST)
Subject: [R] gamm
In-Reply-To: <20060703182603.87603.qmail@web27613.mail.ukl.yahoo.com>
References: <20060703182603.87603.qmail@web27613.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.64.0607192147400.4652@thoth.maths.bath.ac.uk>

Sorry for the delay replying: I was on holiday, but have foolishly come 
back.

>  I am a bit confused about gamm in mgcv. Consulting Wood (2006) or 
> Ruppert et al. (2003) hasn't taken away my confusion.
>
>  In this code from the gamm help file:
>
>     b2<-gamm(y~s(x0)+s(x1)+s(x2)+s(x3),family=poisson,random=list(fac=~1))
>
>  Am I correct in assuming that we have a random intercept here....but 
> that the amount of smoothing is also changing per level of the factor?? 
> Or is it only the intercept that is changing?
>
- the degree of smoothing is the same for each level of `fac' but there is 
a different intercept for each level of `fac': these intercepts are 
assumed Normally distributed. i.e. all `s()' terms apply to all data, 
they are not nested within groups.

>  And where can I find some explanation on the magic output below?

mgcv::gamm is basically a wrapper that turns a GAMM into the sort of model 
that nlme::lme or MASS::glmmPQL expects to see.... `gamm' returns an 
object with two parts - the `lme' part is the object that was returned by 
glmmPQL or lme; the `gam' part is an object with most of the attributes of 
a `gam' object (all those that can be reconstructed from an lme/glmmPQL 
fitted model object).

So the `lme' object contains a bunch of opaque stuff that results from 
turning the original GAMM into something that lme/glmmPQL can work with... 
further detail below.

>
>  summary(b2$lme)
>  Random effects:

- The output starting here relates to the random components of the 4 
smooths in the model. Each smooth starts with 10 degrees of freedom, but 
one of these is lost to the GAM centering constraint that ensures additive 
identifiability, and one is treated as a fixed effect (see Wood, 2006, for 
details), so each smooth have 8 random coefficients. Each of these 
coefficients is treated as having the same variance (after some 
preparatory reparameterization), but this variance (which plays the role 
of a smoothing parameter) is unknown and is estimated as part of model 
estimation.

[Note that g.1 to g.4 below are dummy grouping factors, each having 
only one level - they force the smooths to apply to all the data, rather 
than being nested within, e.g. the levels of `fac'].

> Formula: ~Xr.1 - 1 | g.1
> Structure: pdIdnot
>           Xr.11    Xr.12    Xr.13    Xr.14    Xr.15    Xr.16    Xr.17    Xr.18
> StdDev: 1.680679 1.680679 1.680679 1.680679 1.680679 1.680679 1.680679 1.680679
>   Formula: ~Xr.2 - 1 | g.2 %in% g.1
> Structure: pdIdnot
>          Xr.21   Xr.22   Xr.23   Xr.24   Xr.25   Xr.26   Xr.27   Xr.28
> StdDev: 1.57598 1.57598 1.57598 1.57598 1.57598 1.57598 1.57598 1.57598
>   Formula: ~Xr.3 - 1 | g.3 %in% g.2 %in% g.1
> Structure: pdIdnot
>           Xr.31    Xr.32    Xr.33    Xr.34    Xr.35    Xr.36    Xr.37    Xr.38
> StdDev: 20.06377 20.06377 20.06377 20.06377 20.06377 20.06377 20.06377 20.06377
>   Formula: ~Xr.4 - 1 | g.4 %in% g.3 %in% g.2 %in% g.1
> Structure: pdIdnot
>               Xr.41        Xr.42        Xr.43        Xr.44        Xr.45
> StdDev: 0.0001063304 0.0001063304 0.0001063304 0.0001063304 0.0001063304
>               Xr.46        Xr.47        Xr.48
> StdDev: 0.0001063304 0.0001063304 0.0001063304

- that's the random component information relating to the smooths done 
with. What follows is the information realting to the random intercept. 
Note that the rather complicated Formula is really equivalent to
  ~1|fac
since the g.j are degenerate.

>   Formula: ~1 | fac %in% g.4 %in% g.3 %in% g.2 %in% g.1
>        (Intercept) Residual
> StdDev:   0.6621173 1.007227
>  Variance function:
> Structure: fixed weights
> Formula: ~invwt

- What follows is information about the fixed effects. In this case there 
is one fixed effect for each smooth, and an overall intercept: 
`X(Intercept)'.

> Fixed effects: y.0 ~ X - 1
>                  Value Std.Error  DF   t-value p-value
> X(Intercept)  2.0870364 0.3337787 392  6.252755  0.0000
> Xs(x0)Fx1    -0.0000325 0.1028794 392 -0.000316  0.9997
> Xs(x1)Fx1     0.3831694 0.0957323 392  4.002509  0.0001
> Xs(x2)Fx1     1.4584330 0.3909237 392  3.730736  0.0002
> Xs(x3)Fx1    -0.0123951 0.0143162 392 -0.865809  0.3871
> Correlation:
>

Hope that's some use.

Simon

>- Simon Wood, Mathematical Sciences, University of Bath, Bath BA2 7AY 
>-             +44 (0)1225 386603         www.maths.bath.ac.uk/~sw283/


From sw283 at maths.bath.ac.uk  Wed Jul 19 23:17:38 2006
From: sw283 at maths.bath.ac.uk (Simon Wood)
Date: Wed, 19 Jul 2006 22:17:38 +0100 (BST)
Subject: [R] mgcv::gam error message
In-Reply-To: <001901c6a71e$d20f3fb0$b0734e81@laptopscarrizo>
References: <001901c6a71e$d20f3fb0$b0734e81@laptopscarrizo>
Message-ID: <Pine.LNX.4.64.0607192215060.4652@thoth.maths.bath.ac.uk>

> Could anyone please tell me what to do to resolve this error message?

- Any chance you could send me some code (and associated data) that I can 
cut and paste in order to chase this up? Whatever's wrong here, `gam' 
should probably pick it up before things get this far, but I can't tell 
what's happening without a reproducible example.

Thanks,
Simon

>
> I tried to run a gam with the mgcv package and got the following error:
> "Error in qr.qty(qrc, sm$S[[1]]): NA/NaN/Inf in foreign function call (arg
> 5)"
>
> (I have 116 covariates, I'm using the "cr" basis to speed things up, the
> binomial family and, where necessary, have set the required k to lower than
> the number of distinct values for a given covarate when less than the
> default)
>
> Thank you,
> Savrina
>
> scarrizo at it.usyd.edu.au
> School of Information Technologies
> Univeristy of Sydney
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


From h.wickham at gmail.com  Wed Jul 19 21:27:53 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 19 Jul 2006 20:27:53 +0100
Subject: [R] Fitting a distribution to peaks in histogram
In-Reply-To: <3483f8d50607190956h45119bf5h282550e58939f7b5@mail.gmail.com>
References: <3483f8d50607190258n1f711443p869f79850e81fa79@mail.gmail.com>
	<f8e6ff050607190920o366c8a6dvcecc24fbed7eb7f@mail.gmail.com>
	<3483f8d50607190956h45119bf5h282550e58939f7b5@mail.gmail.com>
Message-ID: <f8e6ff050607191227y6b598284p7aae305f0aee6aa5@mail.gmail.com>

> Can you be a bit more excact? I a biologist and relatively new to R

In that case, I would _strongly_ advise that you get advice from a
local statistician.

> I am measureing the amount of DNA in cells, and I need to know the
> percentage of cells in a part of the cell cycle; that the percentage
> of cells in the first peak, in the second peak and so on. I want to
> integrate the area between to two cells, because that apparently is
> how its none (as far as I can tell from the literature)

That doesn't sound quite right to me, because you also need to take
into account the fact that some cells between peak 1 and 2 belong to
peak 1, and some to peak 2.  This is something that will come out
immediately from a mixture based approach. If you know that peaks
correspond to certain parts of the cell cycle, then this is important
information that should be included in the analysis.

> It very probably is better, but mclust had no result after running for
> at least 2 hours (I terminated the function after two hours), and as I
> generate 50-100 datasets, such as the one used for the histogram, as
> week, I would like to find a faster solution

I doubt that mclust is appropriate for this task, so letting it run
longer wouldn't help anyway.

Again I would suggest that you seek local statistical help.

Hadley


From mhaltuch at u.washington.edu  Wed Jul 19 21:42:01 2006
From: mhaltuch at u.washington.edu (Melissa Ann Haltuch)
Date: Wed, 19 Jul 2006 12:42:01 -0700 (PDT)
Subject: [R] fracdiff
Message-ID: <Pine.LNX.4.43.0607191242010.11072@hymn04.u.washington.edu>

Hi, I'm using the function fracdiff and can not figure out how to get the estimated values for sigma2 or confidence intervals for the parameter estimates. Does anyone know how to obtain these values?
Thanks,
Melissa


From Greg.Snow at intermountainmail.org  Wed Jul 19 21:57:41 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 19 Jul 2006 13:57:41 -0600
Subject: [R] how to use large data set ?
Message-ID: <07E228A5BE53C24CAD490193A7381BBB4B7DE3@LP-EXCHVS07.CO.IHC.COM>

You did not say what analysis you want to do, but many common analyses
can be done as special cases of regression models and you can use the
biglm package to do regression models.

Here is an example that worked for me to get the mean and standard
deviation by day from an oracle database with over 23 million rows (I
had previously set up 'edw' as an odbc connection to the database under
widows, any of the database connections packages should work for you
though):

library(RODBC)
library(biglm)

con <- odbcConnect('edw',uid='glsnow',pwd=pass)

odbcQuery(con, "select ADMSN_WEEKDAY_CD, LOS_DYS from CM.CASEMIX_SMRY")

t1 <- Sys.time()

tmp <- sqlGetResults(con, max=100000)

names(tmp) <- c("Day","LoS")
tmp$Day <- factor(tmp$Day, levels=as.character(1:7))
tmp <- na.omit(tmp)
tmp <- subset(tmp, LoS > 0)

ff <- log(LoS) ~ Day

fit <- biglm(ff, tmp)

i <- nrow(tmp)
while( !is.null(nrow( tmp <- sqlGetResults(con, max=100000) ) ) ){
	names(tmp) <- c("Day","LoS")
	tmp$Day <- factor(tmp$Day, levels=as.character(1:7))
	tmp <- na.omit(tmp)
	tmp <- subset(tmp, LoS > 0)

	fit <- update(fit,tmp)
	
	i <- i + nrow(tmp)
	cat(format(i,big.mark=',')," rows processed\n")
}

summary(fit)

t2 <- Sys.time()

t2-t1
 
Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Yohan CHOUKROUN
Sent: Wednesday, July 19, 2006 9:42 AM
To: 'r-help at stat.math.ethz.ch'
Subject: [R] how to use large data set ?

Hello R users,

 

Sorry for my English, i'm French.

 

I want to use a large dataset (3 millions of rows and 70 var) but I
don't know how to do because my computer crash quickly (P4 2.8Ghz, 1Go
).

I have also a bi Xeon with 2Go so I want to do computation on this
computer and show the results on mine. Both of them are on Windows XP...

 

To do shortly I have: 

 

1 server with a MySQL database

1computer

and I want to use them with a large dataset. 

 

I'm trying to use RDCOM to connect the database and installing (but it's
hard for me..) Rpad.

 

Is there another solutions ?

 

Thanks in advance

 

 

Yohan C.



----------------------------------------------------------------------
Ce message est confidentiel. Son contenu ne represente en aucun cas un
engagement de la part du Groupe Soft Computing sous reserve de tout
accord conclu par ecrit entre vous et le Groupe Soft Computing. Toute
publication, utilisation ou diffusion, meme partielle, doit etre
autorisee prealablement.
Si vous n'etes pas destinataire de ce message, merci d'en avertir
immediatement l'expediteur. 
This message is confidential. Its content does not constitute a
commitment by Soft Computing Group except where provided for in a
written agreement between you and Soft Computing Group. Any unauthorised
disclosure, use or dissemination, either whole or partial, is
prohibited. If you are not the intended recipient of this message,
please notify the sender immediately. 
---------------------------------------------------------------------- 



	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From HDoran at air.org  Wed Jul 19 22:09:04 2006
From: HDoran at air.org (Doran, Harold)
Date: Wed, 19 Jul 2006 16:09:04 -0400
Subject: [R] Wrap a loop inside a function
Message-ID: <2323A6D37908A847A7C32F1E3662C80E132D7A@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060719/95ce9c67/attachment.pl 

From sundar.dorai-raj at pdf.com  Wed Jul 19 22:17:09 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 19 Jul 2006 15:17:09 -0500
Subject: [R] Wrap a loop inside a function
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E132D7A@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E132D7A@dc1ex01.air.org>
Message-ID: <44BE9345.9090700@pdf.com>



Doran, Harold wrote:
> I need to wrap a loop inside a function and am having a small bit of
> difficulty getting the results I need. Below is a replicable example.
> 
> 
> # define functions
> pcm <- function(theta,d,score){
>      exp(rowSums(outer(theta,d[1:score],'-')))/
>      apply(exp(apply(outer(theta,d, '-'), 1, cumsum)), 2, sum)
>    }
> 
> foo <- function(theta,items, score){ 
>    like.mat <- matrix(numeric(length(items) * length(theta)), ncol =
> length(theta))   
>    for(i in 1:length(items)) like.mat[i, ] <- pcm(theta, items[[i]],
> score[[i]]) 
>    }
> 
> # begin example
> theta <- c(-1,-.5,0,.5,1)
> items <- list(item1 = c(0,1,2), item2 = c(0,1), item3 = c(0,1,2,3,4),
> item4 = c(0,1))
> score <- c(2,1,3,1) 
> (foo(theta, items, score))
> 
> # R output from function foo
> [1] 0.8807971 0.8175745 0.7310586 0.6224593 0.5000000
> 
> 
> However, what I am expecting from the function foo is
> 
> 
>>like.mat
> 
>             [,1]       [,2]       [,3]       [,4]      [,5]
> [1,] 0.118499655 0.17973411 0.25949646 0.34820743 0.4223188
> [2,] 0.880797078 0.81757448 0.73105858 0.62245933 0.5000000
> [3,] 0.005899109 0.01474683 0.03505661 0.07718843 0.1520072
> [4,] 0.880797078 0.81757448 0.73105858 0.62245933 0.5000000
> 
> 
> I cannot seem to track down why the function foo is only keeping the
> last row of the full matrix I need. Can anyone see where my error is?
> 
> Thanks,
> Harold
> 
> platform       i386-pc-mingw32           
> arch           i386                      
> os             mingw32                   
> system         i386, mingw32             
> status                                   
> major          2                         
> minor          3.0                       
> year           2006                      
> month          04                        
> day            24                        
> svn rev        37909                     
> language       R                         
> version.string Version 2.3.0 (2006-04-24)
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


Hi, Harold,

Your function "foo" is only returning the last call from "for". Try:

foo <- function(theta,items, score){
   like.mat <- matrix(numeric(length(items) * length(theta)),
                      ncol = length(theta))
   for(i in 1:length(items))
     like.mat[i, ] <- pcm(theta, items[[i]], score[[i]])
   ## return like.mat, not just the last line from "for"
   like.mat
}


HTH,

--sundar


From bates at stat.wisc.edu  Wed Jul 19 22:20:21 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 19 Jul 2006 15:20:21 -0500
Subject: [R] Wrap a loop inside a function
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E132D7A@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E132D7A@dc1ex01.air.org>
Message-ID: <40e66e0b0607191320t7e40d50dw412c7fa04e4a511a@mail.gmail.com>

I'm sure when you read this there will be the sound of a palm slapping
a forehead.  :-)

The value of a function is the value of the last expression evaluated
in the function.  If you rewrite foo as

foo <- function(theta,items, score){
  like.mat <- matrix(numeric(length(items) * length(theta)),
                     ncol = length(theta))
  for(i in 1:length(items))
      like.mat[i, ] <- pcm(theta, items[[i]], score[[i]])
  like.mat
  }

you will get the result that you want.

On 7/19/06, Doran, Harold <HDoran at air.org> wrote:
> I need to wrap a loop inside a function and am having a small bit of
> difficulty getting the results I need. Below is a replicable example.
>
>
> # define functions
> pcm <- function(theta,d,score){
>      exp(rowSums(outer(theta,d[1:score],'-')))/
>      apply(exp(apply(outer(theta,d, '-'), 1, cumsum)), 2, sum)
>    }
>
> foo <- function(theta,items, score){
>    like.mat <- matrix(numeric(length(items) * length(theta)), ncol =
> length(theta))
>    for(i in 1:length(items)) like.mat[i, ] <- pcm(theta, items[[i]],
> score[[i]])
>    }
>
> # begin example
> theta <- c(-1,-.5,0,.5,1)
> items <- list(item1 = c(0,1,2), item2 = c(0,1), item3 = c(0,1,2,3,4),
> item4 = c(0,1))
> score <- c(2,1,3,1)
> (foo(theta, items, score))
>
> # R output from function foo
> [1] 0.8807971 0.8175745 0.7310586 0.6224593 0.5000000
>
>
> However, what I am expecting from the function foo is
>
> > like.mat
>             [,1]       [,2]       [,3]       [,4]      [,5]
> [1,] 0.118499655 0.17973411 0.25949646 0.34820743 0.4223188
> [2,] 0.880797078 0.81757448 0.73105858 0.62245933 0.5000000
> [3,] 0.005899109 0.01474683 0.03505661 0.07718843 0.1520072
> [4,] 0.880797078 0.81757448 0.73105858 0.62245933 0.5000000
>
>
> I cannot seem to track down why the function foo is only keeping the
> last row of the full matrix I need. Can anyone see where my error is?
>
> Thanks,
> Harold
>
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          3.0
> year           2006
> month          04
> day            24
> svn rev        37909
> language       R
> version.string Version 2.3.0 (2006-04-24)
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From binabina at bellsouth.net  Wed Jul 19 22:21:04 2006
From: binabina at bellsouth.net (binabina at bellsouth.net)
Date: Wed, 19 Jul 2006 16:21:04 -0400
Subject: [R] voronoi tessellations
Message-ID: <20060719202105.MIIP17714.ibm68aec.bellsouth.net@mail.bellsouth.net>

Okay, been working with tripack, seems the most mature package for this.  Got it to work well with their test data set - data(tritest).  When i tried random numbers to explore further, i am getting some results that don't reconcile.  

example run this:

library(tripack)
y <- runif(100)
x <- runif(100)
vm <- voronoi.mosaic(x,y)
plot(vm)
par(new=T)
plot(x,y,col='blue')

when you look at the plot of the mosaic overlayed with the raw data, the mosaic should have each  data point in 1 cell, however that is not the case - any help would be appreciated.  

However if you run

data(tritest)
x <- tritest$x
y <- tritest$y
vm <- voronoi.mosaic(x,y)
plot(vm)
par(new=T)
plot(x,y,col='blue')

it looks at it should.

-zubin


> 
> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> Date: 2006/07/19 Wed AM 03:15:51 EDT
> To: Don MacQueen <macq at llnl.gov>
> CC: zubin <binabina at bellsouth.net>,  r-help at stat.math.ethz.ch
> Subject: Re: [R] voronoi tessellations
> 
> On Tue, 18 Jul 2006, Don MacQueen wrote:
> 
> > I'll suggest going to the CRAN packages page and doing a search for "voronoi".
> 
> The problem here is that `Voronoi tessellation' is a secondary name.  The 
> concept has many names, including Dirichlet tessellation and Thiessen 
> polygons, and Dirichlet has priority over Voronoi.
> 
> > Also, search for 'triangulation', since that is one of the uses of them.
> 
> I know of packages deldir, tripack and perhaps geometry.
> 
> > -Don
> > 
> > At 11:46 PM -0400 7/18/06, zubin wrote:
> > >Hello, looking to draw a voronoi tessellations in R - can anyone
> > >recommend a package that has tackled this?
> > >
> > >some background:
> > >
> > >i have a economic data set and created a sammons projection, like to now
> > >overlay a voronoi tessellation over the sammons 2-D solution for a slick
> > >visual, and potentially color each tessellation element based on a metric.
> > >
> > >home.u <- unique(home1)
> > >home.dist <- dist(home.u)
> > >home.sam <- sammon(home.dist,k=2)
> > >plot(home.sam$points)
> 
> Wait a minute.  If this is sammon() from MASS (uncredited), it is not a 
> projection, and there is no relevant concept of distance between points in 
> the mapped space apart from between the supplied points.
> 
> I suggest Zubin reads carefully the reference whose support software he 
> appears to be using.  (It would also have answered his question.)
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From jdrapp at gmail.com  Wed Jul 19 22:22:33 2006
From: jdrapp at gmail.com (justin rapp)
Date: Wed, 19 Jul 2006 16:22:33 -0400
Subject: [R] Correlation Mapping
In-Reply-To: <44BC4D67.2070305@bitwrit.com.au>
References: <44BC4D67.2070305@bitwrit.com.au>
Message-ID: <af81db5a0607191322j4a5755dem19ebc75aef19c490@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060719/c599c170/attachment.pl 

From rumahesh45 at gmail.com  Wed Jul 19 22:22:44 2006
From: rumahesh45 at gmail.com (mahesh r)
Date: Wed, 19 Jul 2006 15:22:44 -0500
Subject: [R] how to use large data set ?
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB4B7DE3@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBB4B7DE3@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <f6bf155b0607191322r457b4a6bu8eb98145f9648646@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060719/eb366bae/attachment.pl 

From HDoran at air.org  Wed Jul 19 22:25:29 2006
From: HDoran at air.org (Doran, Harold)
Date: Wed, 19 Jul 2006 16:25:29 -0400
Subject: [R] Wrap a loop inside a function
Message-ID: <2323A6D37908A847A7C32F1E3662C80E132D84@dc1ex01.air.org>

Very true, the resounding echo was large.

Thanks, Doug. 

> -----Original Message-----
> From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf 
> Of Douglas Bates
> Sent: Wednesday, July 19, 2006 4:20 PM
> To: Doran, Harold
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Wrap a loop inside a function
> 
> I'm sure when you read this there will be the sound of a palm 
> slapping a forehead.  :-)
> 
> The value of a function is the value of the last expression 
> evaluated in the function.  If you rewrite foo as
> 
> foo <- function(theta,items, score){
>   like.mat <- matrix(numeric(length(items) * length(theta)),
>                      ncol = length(theta))
>   for(i in 1:length(items))
>       like.mat[i, ] <- pcm(theta, items[[i]], score[[i]])
>   like.mat
>   }
> 
> you will get the result that you want.
> 
> On 7/19/06, Doran, Harold <HDoran at air.org> wrote:
> > I need to wrap a loop inside a function and am having a 
> small bit of 
> > difficulty getting the results I need. Below is a 
> replicable example.
> >
> >
> > # define functions
> > pcm <- function(theta,d,score){
> >      exp(rowSums(outer(theta,d[1:score],'-')))/
> >      apply(exp(apply(outer(theta,d, '-'), 1, cumsum)), 2, sum)
> >    }
> >
> > foo <- function(theta,items, score){
> >    like.mat <- matrix(numeric(length(items) * length(theta)), ncol =
> > length(theta))
> >    for(i in 1:length(items)) like.mat[i, ] <- pcm(theta, items[[i]],
> > score[[i]])
> >    }
> >
> > # begin example
> > theta <- c(-1,-.5,0,.5,1)
> > items <- list(item1 = c(0,1,2), item2 = c(0,1), item3 = 
> c(0,1,2,3,4),
> > item4 = c(0,1))
> > score <- c(2,1,3,1)
> > (foo(theta, items, score))
> >
> > # R output from function foo
> > [1] 0.8807971 0.8175745 0.7310586 0.6224593 0.5000000
> >
> >
> > However, what I am expecting from the function foo is
> >
> > > like.mat
> >             [,1]       [,2]       [,3]       [,4]      [,5]
> > [1,] 0.118499655 0.17973411 0.25949646 0.34820743 0.4223188 [2,] 
> > 0.880797078 0.81757448 0.73105858 0.62245933 0.5000000 [3,] 
> > 0.005899109 0.01474683 0.03505661 0.07718843 0.1520072 [4,] 
> > 0.880797078 0.81757448 0.73105858 0.62245933 0.5000000
> >
> >
> > I cannot seem to track down why the function foo is only 
> keeping the 
> > last row of the full matrix I need. Can anyone see where my 
> error is?
> >
> > Thanks,
> > Harold
> >
> > platform       i386-pc-mingw32
> > arch           i386
> > os             mingw32
> > system         i386, mingw32
> > status
> > major          2
> > minor          3.0
> > year           2006
> > month          04
> > day            24
> > svn rev        37909
> > language       R
> > version.string Version 2.3.0 (2006-04-24)
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From murdoch at stats.uwo.ca  Wed Jul 19 22:32:43 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 19 Jul 2006 16:32:43 -0400
Subject: [R] voronoi tessellations
In-Reply-To: <20060719202105.MIIP17714.ibm68aec.bellsouth.net@mail.bellsouth.net>
References: <20060719202105.MIIP17714.ibm68aec.bellsouth.net@mail.bellsouth.net>
Message-ID: <44BE96EB.60904@stats.uwo.ca>

On 7/19/2006 4:21 PM, binabina at bellsouth.net wrote:
> Okay, been working with tripack, seems the most mature package for this.  Got it to work well with their test data set - data(tritest).  When i tried random numbers to explore further, i am getting some results that don't reconcile.  
> 
> example run this:
> 
> library(tripack)
> y <- runif(100)
> x <- runif(100)
> vm <- voronoi.mosaic(x,y)
> plot(vm)
> par(new=T)

Don't do that!  That throws away the coordinate system that has already 
been established.  Do it this way:

library(tripack)
y <- runif(100)
x <- runif(100)
vm <- voronoi.mosaic(x,y)
plot(vm)
points(x,y,col='blue')

and if you want the axes,

axis(1)
axis(2)
box()

Duncan Murdoch

> plot(x,y,col='blue')
> 
> when you look at the plot of the mosaic overlayed with the raw data, the mosaic should have each  data point in 1 cell, however that is not the case - any help would be appreciated.  
> 
> However if you run
> 
> data(tritest)
> x <- tritest$x
> y <- tritest$y
> vm <- voronoi.mosaic(x,y)
> plot(vm)
> par(new=T)
> plot(x,y,col='blue')
> 
> it looks at it should.
> 
> -zubin
> 
> 
>> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>> Date: 2006/07/19 Wed AM 03:15:51 EDT
>> To: Don MacQueen <macq at llnl.gov>
>> CC: zubin <binabina at bellsouth.net>,  r-help at stat.math.ethz.ch
>> Subject: Re: [R] voronoi tessellations
>>
>> On Tue, 18 Jul 2006, Don MacQueen wrote:
>>
>>> I'll suggest going to the CRAN packages page and doing a search for "voronoi".
>> The problem here is that `Voronoi tessellation' is a secondary name.  The 
>> concept has many names, including Dirichlet tessellation and Thiessen 
>> polygons, and Dirichlet has priority over Voronoi.
>>
>>> Also, search for 'triangulation', since that is one of the uses of them.
>> I know of packages deldir, tripack and perhaps geometry.
>>
>>> -Don
>>>
>>> At 11:46 PM -0400 7/18/06, zubin wrote:
>>>> Hello, looking to draw a voronoi tessellations in R - can anyone
>>>> recommend a package that has tackled this?
>>>>
>>>> some background:
>>>>
>>>> i have a economic data set and created a sammons projection, like to now
>>>> overlay a voronoi tessellation over the sammons 2-D solution for a slick
>>>> visual, and potentially color each tessellation element based on a metric.
>>>>
>>>> home.u <- unique(home1)
>>>> home.dist <- dist(home.u)
>>>> home.sam <- sammon(home.dist,k=2)
>>>> plot(home.sam$points)
>> Wait a minute.  If this is sammon() from MASS (uncredited), it is not a 
>> projection, and there is no relevant concept of distance between points in 
>> the mapped space apart from between the supplied points.
>>
>> I suggest Zubin reads carefully the reference whose support software he 
>> appears to be using.  (It would also have answered his question.)
>>
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tlumley at u.washington.edu  Wed Jul 19 22:34:05 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 19 Jul 2006 13:34:05 -0700 (PDT)
Subject: [R] voronoi tessellations
In-Reply-To: <20060719202105.MIIP17714.ibm68aec.bellsouth.net@mail.bellsouth.net>
References: <20060719202105.MIIP17714.ibm68aec.bellsouth.net@mail.bellsouth.net>
Message-ID: <Pine.LNX.4.64.0607191333320.18735@homer21.u.washington.edu>

On Wed, 19 Jul 2006, binabina at bellsouth.net wrote:

> Okay, been working with tripack, seems the most mature package for this. 
> Got it to work well with their test data set - data(tritest).  When i 
> tried random numbers to explore further, i am getting some results that 
> don't reconcile.
>
> example run this:
>
> library(tripack)
> y <- runif(100)
> x <- runif(100)
> vm <- voronoi.mosaic(x,y)
> plot(vm)
> par(new=T)
> plot(x,y,col='blue')

plot() changes the axis scales. Use points(x,y)

 	-thomas

> when you look at the plot of the mosaic overlayed with the raw data, the 
> mosaic should have each data point in 1 cell, however that is not the 
> case - any help would be appreciated.
>
> However if you run
>
> data(tritest)
> x <- tritest$x
> y <- tritest$y
> vm <- voronoi.mosaic(x,y)
> plot(vm)
> par(new=T)
> plot(x,y,col='blue')
>
> it looks at it should.
>
> -zubin
>
>
>>
>> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>> Date: 2006/07/19 Wed AM 03:15:51 EDT
>> To: Don MacQueen <macq at llnl.gov>
>> CC: zubin <binabina at bellsouth.net>,  r-help at stat.math.ethz.ch
>> Subject: Re: [R] voronoi tessellations
>>
>> On Tue, 18 Jul 2006, Don MacQueen wrote:
>>
>>> I'll suggest going to the CRAN packages page and doing a search for "voronoi".
>>
>> The problem here is that `Voronoi tessellation' is a secondary name.  The
>> concept has many names, including Dirichlet tessellation and Thiessen
>> polygons, and Dirichlet has priority over Voronoi.
>>
>>> Also, search for 'triangulation', since that is one of the uses of them.
>>
>> I know of packages deldir, tripack and perhaps geometry.
>>
>>> -Don
>>>
>>> At 11:46 PM -0400 7/18/06, zubin wrote:
>>>> Hello, looking to draw a voronoi tessellations in R - can anyone
>>>> recommend a package that has tackled this?
>>>>
>>>> some background:
>>>>
>>>> i have a economic data set and created a sammons projection, like to now
>>>> overlay a voronoi tessellation over the sammons 2-D solution for a slick
>>>> visual, and potentially color each tessellation element based on a metric.
>>>>
>>>> home.u <- unique(home1)
>>>> home.dist <- dist(home.u)
>>>> home.sam <- sammon(home.dist,k=2)
>>>> plot(home.sam$points)
>>
>> Wait a minute.  If this is sammon() from MASS (uncredited), it is not a
>> projection, and there is no relevant concept of distance between points in
>> the mapped space apart from between the supplied points.
>>
>> I suggest Zubin reads carefully the reference whose support software he
>> appears to be using.  (It would also have answered his question.)
>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From dylan.beaudette at gmail.com  Wed Jul 19 23:11:31 2006
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Wed, 19 Jul 2006 14:11:31 -0700
Subject: [R] how to use large data set ?
In-Reply-To: <f6bf155b0607191322r457b4a6bu8eb98145f9648646@mail.gmail.com>
References: <07E228A5BE53C24CAD490193A7381BBB4B7DE3@LP-EXCHVS07.CO.IHC.COM>
	<f6bf155b0607191322r457b4a6bu8eb98145f9648646@mail.gmail.com>
Message-ID: <200607191411.31915.dylan.beaudette@gmail.com>

Hi,

While R is generally flexible enough for just about anything you can throw at 
it, detailed analysis of imagery might be better accomplished in a 
specialized piece of software. One option might be GRASS, which would allow 
you to do further processing on a subset of the original data in R.

Cheers,

Dylan

On Wednesday 19 July 2006 13:22, mahesh r wrote:
> Hi,
> I would like to extend to the query posted earlier on using large data
> bases. I am trying to use Rgdal to mine within the remote sensing
> imageries. I dont have problems bring the images within the R environment.
> But when I try to convert the images to a data.frame I receive an warning
> message from R saying "1: Reached total allocation of 510Mb: see
> help(memory.size)" and the process terminates. Due to project constarints I
> am given a very old 2.4Ghz computer with only 512 MB RAM. I think what R is
> currently doing is
> trying to store the results in the RAM and since the image size is very big
> (some 9 million pixels), I think it gets out of memory.
>
> My question is
> 1. Is there any possibility to dump the temporary variables in a temp
> folder within the hard disk (as many softwares do) instead of leting R
> store them in RAM
> 2. Could this be possible without creating a connection to a any back hand
> database like Oracle.
>
> Thanks,
>
> Mahesh
>
> On 7/19/06, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
> > You did not say what analysis you want to do, but many common analyses
> > can be done as special cases of regression models and you can use the
> > biglm package to do regression models.
> >
> > Here is an example that worked for me to get the mean and standard
> > deviation by day from an oracle database with over 23 million rows (I
> > had previously set up 'edw' as an odbc connection to the database under
> > widows, any of the database connections packages should work for you
> > though):
> >
> > library(RODBC)
> > library(biglm)
> >
> > con <- odbcConnect('edw',uid='glsnow',pwd=pass)
> >
> > odbcQuery(con, "select ADMSN_WEEKDAY_CD, LOS_DYS from CM.CASEMIX_SMRY")
> >
> > t1 <- Sys.time()
> >
> > tmp <- sqlGetResults(con, max=100000)
> >
> > names(tmp) <- c("Day","LoS")
> > tmp$Day <- factor(tmp$Day, levels=as.character(1:7))
> > tmp <- na.omit(tmp)
> > tmp <- subset(tmp, LoS > 0)
> >
> > ff <- log(LoS) ~ Day
> >
> > fit <- biglm(ff, tmp)
> >
> > i <- nrow(tmp)
> > while( !is.null(nrow( tmp <- sqlGetResults(con, max=100000) ) ) ){
> >         names(tmp) <- c("Day","LoS")
> >         tmp$Day <- factor(tmp$Day, levels=as.character(1:7))
> >         tmp <- na.omit(tmp)
> >         tmp <- subset(tmp, LoS > 0)
> >
> >         fit <- update(fit,tmp)
> >
> >         i <- i + nrow(tmp)
> >         cat(format(i,big.mark=',')," rows processed\n")
> > }
> >
> > summary(fit)
> >
> > t2 <- Sys.time()
> >
> > t2-t1
> >
> > Hope this helps,
> >
> > --
> > Gregory (Greg) L. Snow Ph.D.
> > Statistical Data Center
> > Intermountain Healthcare
> > greg.snow at intermountainmail.org
> > (801) 408-8111
> >
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Yohan CHOUKROUN
> > Sent: Wednesday, July 19, 2006 9:42 AM
> > To: 'r-help at stat.math.ethz.ch'
> > Subject: [R] how to use large data set ?
> >
> > Hello R users,
> >
> >
> >
> > Sorry for my English, i'm French.
> >
> >
> >
> > I want to use a large dataset (3 millions of rows and 70 var) but I
> > don't know how to do because my computer crash quickly (P4 2.8Ghz, 1Go
> > ).
> >
> > I have also a bi Xeon with 2Go so I want to do computation on this
> > computer and show the results on mine. Both of them are on Windows XP...
> >
> >
> >
> > To do shortly I have:
> >
> >
> >
> > 1 server with a MySQL database
> >
> > 1computer
> >
> > and I want to use them with a large dataset.
> >
> >
> >
> > I'm trying to use RDCOM to connect the database and installing (but it's
> > hard for me..) Rpad.
> >
> >
> >
> > Is there another solutions ?
> >
> >
> >
> > Thanks in advance
> >
> >
> >
> >
> >
> > Yohan C.
> >
> >
> >
> > ----------------------------------------------------------------------
> > Ce message est confidentiel. Son contenu ne represente en aucun cas un
> > engagement de la part du Groupe Soft Computing sous reserve de tout
> > accord conclu par ecrit entre vous et le Groupe Soft Computing. Toute
> > publication, utilisation ou diffusion, meme partielle, doit etre
> > autorisee prealablement.
> > Si vous n'etes pas destinataire de ce message, merci d'en avertir
> > immediatement l'expediteur.
> > This message is confidential. Its content does not constitute a
> > commitment by Soft Computing Group except where provided for in a
> > written agreement between you and Soft Computing Group. Any unauthorised
> > disclosure, use or dissemination, either whole or partial, is
> > prohibited. If you are not the intended recipient of this message,
> > please notify the sender immediately.
> > ----------------------------------------------------------------------
> >
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
Dylan Beaudette
Soils and Biogeochemistry Graduate Group
University of California at Davis
530.754.7341


From choid at ohsu.edu  Wed Jul 19 22:53:40 2006
From: choid at ohsu.edu (Dongseok Choi)
Date: Wed, 19 Jul 2006 13:53:40 -0700
Subject: [R] error when compiling "stats" library in R-2.3.1 on Solaris x86
Message-ID: <s4be3970.098@ohsu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060719/253b4e8e/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Jul 19 23:39:09 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 19 Jul 2006 22:39:09 +0100 (BST)
Subject: [R] error when compiling "stats" library in R-2.3.1 on Solaris
 x86
In-Reply-To: <s4be3970.098@ohsu.edu>
References: <s4be3970.098@ohsu.edu>
Message-ID: <Pine.LNX.4.64.0607192226340.17461@gannet.stats.ox.ac.uk>

How did 'cc -xtarget=generic64' get there?  AFAIK R does not know about 
it, so presumably you specified it for CC.  You need to do the same thing 
for *all* the compilers, that is CC, CXX, F77 and FC.

The INSTALL file asked you to read the R-admin manual: there you will find 
a very similar example for 64-bit Sparc Solaris.  That uses -xarch, which 
seems preferred to -xtarget (or at least to generic targets).

On Wed, 19 Jul 2006, Dongseok Choi wrote:

> Hello,
>  
>   I tried to compile v2.3.1 on Solaris x86 with SUN Pro compilers.
>   I had an error while stats libarary was being compiled and I notice that  "-xtarget=generic64" was not passed to f95 while cc used it.
>   Could you tell me how to fix this problem?
>  
> f95   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c sgram.f -o sgram.o
> f95   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c sinerp.f -o sinerp.o
> f95   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c sslvrg.f -o sslvrg.o
> f95   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c stxwx.f -o stxwx.o
> f95   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c hclust.f -o hclust.o
> f95   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c kmns.f -o kmns.o
> f95   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c eureka.f -o eureka.o
> f95   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c stl.f -o stl.o
> f95   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c portsrc.f -o portsrc.o
> cc -xtarget=generic64 -G -L/mounts/devel/SUNWspro/lib/amd64 -L/usr/sfw/lib/amd64 -L/mounts/devel/GNU/repoz/readline43/lib/amd64 -o stats.so init.o kmeans.o  ansari.o bandwidths.o chisqsim.o d2x2xk.o fexact.o kendall.o ks.o  line.o smooth.o  prho.o swilk.o  ksmooth.o loessc.o isoreg.o Srunmed.o Trunmed.o  dblcen.o distance.o hclust-utils.o  nls.o  HoltWinters.o PPsum.o arima.o burg.o filter.o  mAR.o pacf.o starma.o port.o family.o bsplvd.o bvalue.o bvalus.o loessf.o ppr.o qsbart.o sbart.o  sgram.o sinerp.o sslvrg.o stxwx.o  hclust.o kmns.o  eureka.o stl.o portsrc.o -xlic_lib=sunperf -lsunmath -Rreg -R/mounts/devel/SUNWspro/lib/amd64:/opt/SUNWspro/lib/amd64 -L/mounts/devel/SUNWspro/prod/lib/amd64 -L/lib/amd64 -L/usr/lib/amd64 -lfui -lfai -lfsu -lsunmath -lmtsk -lm
> ld: fatal: file bsplvd.o: wrong ELF class: ELFCLASS32
> ld: fatal: File processing errors. No output written to stats.so
> *** Error code 1
> make: Fatal error: Command failed for target `stats.so'
> 
>   I did not have any problem when I compiled v2.2.1 as below:
>  
> cc -xtarget=generic64 -I../../../../include  -I/mounts/devel/SUNWspro/prod/include -I/mounts/devel/GNU/repoz/readline43/include -D__NO_MATH_INLINES  -KPIC  -O -I/mounts/devel/SUNWspro/prod/include -c family.c -o family.o
> f95 -xtarget=generic64   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c bsplvd.f -o bsplvd.o
> f95 -xtarget=generic64   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c bvalue.f -o bvalue.o
> f95 -xtarget=generic64   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c bvalus.f -o bvalus.o
> f95 -xtarget=generic64   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c loessf.f -o loessf.o
> f95 -xtarget=generic64   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c ppr.f -o ppr.o
> f95 -xtarget=generic64   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c qsbart.f -o qsbart.o
> cc -xtarget=generic64 -I../../../../include  -I/mounts/devel/SUNWspro/prod/include -I/mounts/devel/GNU/repoz/readline43/include -D__NO_MATH_INLINES  -KPIC  -O -I/mounts/devel/SUNWspro/prod/include -c sbart.c -o sbart.o
> f95 -xtarget=generic64   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c sgram.f -o sgram.o
> f95 -xtarget=generic64   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c sinerp.f -o sinerp.o
> f95 -xtarget=generic64   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c sslvrg.f -o sslvrg.o
> f95 -xtarget=generic64   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c stxwx.f -o stxwx.o
> f95 -xtarget=generic64   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c hclust.f -o hclust.o
> f95 -xtarget=generic64   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c kmns.f -o kmns.o
> f95 -xtarget=generic64   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c eureka.f -o eureka.o
> f95 -xtarget=generic64   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c stl.f -o stl.o
> f95 -xtarget=generic64   -PIC  -O -I/mounts/devel/SUNWspro/prod/include -c portsrc.f -o portsrc.o
> cc -xtarget=generic64 -G -L/mounts/devel/SUNWspro/lib/amd64 -L/usr/sfw/lib/amd64 -L/mounts/devel/GNU/repoz/readline43/lib/amd64 -o stats.so init.o kmeans.o  ansari.o bandwidths.o chisqsim.o d2x2xk.o fexact.o kendall.o ks.o  line.o smooth.o  prho.o swilk.o  ksmooth.o loessc.o isoreg.o Srunmed.o Trunmed.o  dblcen.o distance.o hclust-utils.o  nls.o  HoltWinters.o PPsum.o arima.o burg.o carray.o filter.o  mburg.o myw.o pacf.o qr.o starma.o port.o family.o bsplvd.o bvalue.o bvalus.o loessf.o ppr.o qsbart.o sbart.o  sgram.o sinerp.o sslvrg.o stxwx.o  hclust.o kmns.o  eureka.o stl.o portsrc.o -xlic_lib=sunperf -lsunmath  -Rreg -R/mounts/devel/SUNWspro/lib/amd64:/opt/SUNWspro/lib/amd64 -L/mounts/devel/SUNWspro/prod/lib/amd64 -L/lib/amd64 -L/usr/lib/amd64 -lfui -lfai -lfsu -lsunmath -lmtsk -lm
> mkdir ../../../../library/stats/libs
> 
>  
> Thank you very much!!
>  
>  
>  
>  
> Dongseok Choi, Ph.D.
> Assistant Professor
> Division of Biostatistics
> Department of Public Health & Preventive Medicine
> Oregon Health & Science University
> 3181 SW Sam Jackson Park Road, CB-669
> Portland, OR 97239-3098
> TEL) 503-494-5336
> FAX) 503-494-4981
> choid at ohsu.edu
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From choid at ohsu.edu  Thu Jul 20 00:02:39 2006
From: choid at ohsu.edu (Dongseok Choi)
Date: Wed, 19 Jul 2006 15:02:39 -0700
Subject: [R] error when compiling "stats" library in R-2.3.1 on Solaris
 x86
Message-ID: <s4be49a2.034@OHSU.EDU>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060719/7be4f9be/attachment.pl 

From tlumley at u.washington.edu  Thu Jul 20 00:05:16 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 19 Jul 2006 15:05:16 -0700 (PDT)
Subject: [R] How to find S4 generics?
In-Reply-To: <44BE7B53.4070104@pdf.com>
References: <878be2c60607150822r5914a231h41d537f2f5b446b4@mail.gmail.com>
	<44BDA929.8000407@pdf.com>
	<Pine.LNX.4.64.0607190739540.32733@gannet.stats.ox.ac.uk>
	<44BE7B53.4070104@pdf.com>
Message-ID: <Pine.LNX.4.64.0607191449030.18735@homer21.u.washington.edu>

On Wed, 19 Jul 2006, Spencer Graves wrote:
> 	  Am I correct then that the 'methods' function could, at least
> theoretically, be revised so methods(class=...) could identify both S3
> and S4 methods (ignoring inheritance, as it does now, I believe)?
>

Here is a function to find methods for a formal class. It returns a list 
with elements corresponding to a generic, and each element is a list of 
strings showing all the signatures that contain any of the specified 
classes.

If super=TRUE it looks at all superclasses, if ANY=TRUE it also returns 
methods for ANY class.

If you have lme4 loaded, try
   methods4("lmer"
   methods4("ddiMatrix")
   methods4("ddiMatrix",super=TRUE)

 	-thomas

methods4<-function(classes, super=FALSE, ANY=FALSE){
   if (super) classes<-unlist(sapply(classes, function(cl) getAllSuperClasses(getClass(cl))))
   if (ANY) classes<-c(classes,"ANY")
   gens<-allGenerics()@.Data
   sigs<-lapply(gens, function(g) linearizeMlist(getMethods(g))@classes)
   names(sigs)<-gens at .Data
   sigs<-lapply(sigs, function(gen){ gen[unlist(sapply(gen, function(sig) any(sig %in% classes)))]})
   sigs[sapply(sigs,length)>0]
}


Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From mai at ms.uky.edu  Thu Jul 20 00:23:05 2006
From: mai at ms.uky.edu (Mai Zhou)
Date: Wed, 19 Jul 2006 18:23:05 -0400 (EDT)
Subject: [R] Data from Ying, Jung and Wei (1995)
Message-ID: <200607192223.k6JMN5TJ002346@t2.ms.uky.edu>

Dear all, 

I am looking for the Small Cell Lung Cancer
data from the paper

Ying, Z., Jung, S. H., and Wei, L. J. (1995), 
Survival Analysis with Median Regression Models,
Journal of the American Statistical Association} {\bf 90}, 178--184.

Do any one know if it exist in an R readable format somewhere?

Thanks!

Mai Zhou


From spencer.graves at pdf.com  Thu Jul 20 00:23:58 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 19 Jul 2006 15:23:58 -0700
Subject: [R] How to find S4 generics?
In-Reply-To: <Pine.LNX.4.64.0607191449030.18735@homer21.u.washington.edu>
References: <878be2c60607150822r5914a231h41d537f2f5b446b4@mail.gmail.com>	<44BDA929.8000407@pdf.com>	<Pine.LNX.4.64.0607190739540.32733@gannet.stats.ox.ac.uk>	<44BE7B53.4070104@pdf.com>
	<Pine.LNX.4.64.0607191449030.18735@homer21.u.washington.edu>
Message-ID: <44BEB0FE.90707@pdf.com>

Hi, Thomas:  Thanks very much.  I haven't tried it yet, but it looks 
very useful.  Best Wishes, Spencer Graves

Thomas Lumley wrote:
> On Wed, 19 Jul 2006, Spencer Graves wrote:
>> 	  Am I correct then that the 'methods' function could, at least
>> theoretically, be revised so methods(class=...) could identify both S3
>> and S4 methods (ignoring inheritance, as it does now, I believe)?
>>
> 
> Here is a function to find methods for a formal class. It returns a list 
> with elements corresponding to a generic, and each element is a list of 
> strings showing all the signatures that contain any of the specified 
> classes.
> 
> If super=TRUE it looks at all superclasses, if ANY=TRUE it also returns 
> methods for ANY class.
> 
> If you have lme4 loaded, try
>    methods4("lmer"
>    methods4("ddiMatrix")
>    methods4("ddiMatrix",super=TRUE)
> 
>  	-thomas
> 
> methods4<-function(classes, super=FALSE, ANY=FALSE){
>    if (super) classes<-unlist(sapply(classes, function(cl) getAllSuperClasses(getClass(cl))))
>    if (ANY) classes<-c(classes,"ANY")
>    gens<-allGenerics()@.Data
>    sigs<-lapply(gens, function(g) linearizeMlist(getMethods(g))@classes)
>    names(sigs)<-gens at .Data
>    sigs<-lapply(sigs, function(gen){ gen[unlist(sapply(gen, function(sig) any(sig %in% classes)))]})
>    sigs[sapply(sigs,length)>0]
> }
> 
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Thu Jul 20 00:33:45 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 19 Jul 2006 18:33:45 -0400
Subject: [R] Wrap a loop inside a function
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E132D7A@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E132D7A@dc1ex01.air.org>
Message-ID: <971536df0607191533t79e1d50cm142d2f58cc46dba@mail.gmail.com>

foo can be written as a mapply:

t(mapply(pcm, items, score, MoreArgs = list(theta = theta)))


On 7/19/06, Doran, Harold <HDoran at air.org> wrote:
> I need to wrap a loop inside a function and am having a small bit of
> difficulty getting the results I need. Below is a replicable example.
>
>
> # define functions
> pcm <- function(theta,d,score){
>     exp(rowSums(outer(theta,d[1:score],'-')))/
>     apply(exp(apply(outer(theta,d, '-'), 1, cumsum)), 2, sum)
>   }
>
> foo <- function(theta,items, score){
>   like.mat <- matrix(numeric(length(items) * length(theta)), ncol =
> length(theta))
>   for(i in 1:length(items)) like.mat[i, ] <- pcm(theta, items[[i]],
> score[[i]])
>   }
>
> # begin example
> theta <- c(-1,-.5,0,.5,1)
> items <- list(item1 = c(0,1,2), item2 = c(0,1), item3 = c(0,1,2,3,4),
> item4 = c(0,1))
> score <- c(2,1,3,1)
> (foo(theta, items, score))
>
> # R output from function foo
> [1] 0.8807971 0.8175745 0.7310586 0.6224593 0.5000000
>
>
> However, what I am expecting from the function foo is
>
> > like.mat
>            [,1]       [,2]       [,3]       [,4]      [,5]
> [1,] 0.118499655 0.17973411 0.25949646 0.34820743 0.4223188
> [2,] 0.880797078 0.81757448 0.73105858 0.62245933 0.5000000
> [3,] 0.005899109 0.01474683 0.03505661 0.07718843 0.1520072
> [4,] 0.880797078 0.81757448 0.73105858 0.62245933 0.5000000
>
>
> I cannot seem to track down why the function foo is only keeping the
> last row of the full matrix I need. Can anyone see where my error is?
>
> Thanks,
> Harold
>
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          3.0
> year           2006
> month          04
> day            24
> svn rev        37909
> language       R
> version.string Version 2.3.0 (2006-04-24)
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From choid at ohsu.edu  Thu Jul 20 00:38:10 2006
From: choid at ohsu.edu (Dongseok Choi)
Date: Wed, 19 Jul 2006 15:38:10 -0700
Subject: [R] error when compiling "stats" library in R-2.3.1 on Solaris
 x86
Message-ID: <s4be51ed.095@OHSU.EDU>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060719/ada1ffde/attachment.pl 

From cgb at datanalytics.com  Thu Jul 20 01:24:02 2006
From: cgb at datanalytics.com (Carlos J. Gil Bellosta)
Date: Thu, 20 Jul 2006 01:24:02 +0200
Subject: [R] Automating package building packages and repository uploading
Message-ID: <1153351442.9477.20.camel@lenin>

Dear Rusers,

I have developed two packages for a client of mine. After new features
are added or bugs corrected, I upload them to my own web repository. I
create both source and binary versions.

In fact, I made an script that checks, builds, and uploads them via ftp.
However, I am facing two nuisances that do make it difficult to
automate:

1) Even if I build the binary version with the command

R CMD build --use-zip --binary $package

within my script, the output package still gets tarballed and gzipped
instead than simply zipped. I come around this automatically extracting
and compressing back the files but, am I missing something some other
option that would make all this simpler?

2) I expect my packages to be named something like
mypackage_1.3.12.tar.gz or mypackage_1.3.12.zip. However, "sometimes"
--I haven't looked at the code that decides the name to give to the
packages, so it looks quite "random" to me-- they get renamed into
something like mypackage_1.3.12_R_i486-pc-linux-gnu.tar.gz or
mypackage_1.3.12_R_i486-pc-linux-gnu.zip. The problem is that, then, the
update.packages() function cannot find them. Is there a way to prevent
this trailing string from appearing in the file name? Or else, is there
a way to have update.packages() find the package regardless of it?

I am running

platform       i486-pc-linux-gnu
arch           i486
os             linux-gnu
system         i486, linux-gnu
status
major          2
minor          3.1
year           2006
month          06
day            01
svn rev        38247
language       R
version.string Version 2.3.1 (2006-06-01)

on Debian Etch with kernel 2.6.15-1-k7.

Thank you very much.

Carlos J. Gil Bellosta
http://www.datanalytics.com
http://www.data-mining-blog.com


From A.Robinson at ms.unimelb.edu.au  Thu Jul 20 03:07:28 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Thu, 20 Jul 2006 11:07:28 +1000
Subject: [R] get rid of error in Factor Analysis
In-Reply-To: <000601c6ab0d$1fb5a3c0$1191680a@robert>
References: <000601c6ab0d$1fb5a3c0$1191680a@robert>
Message-ID: <20060720010728.GQ40766@ms.unimelb.edu.au>

Robert,

try try().

Andrew.

On Wed, Jul 19, 2006 at 10:27:07AM +0200, Robert Mcfadden wrote:
> Dear All,
> 
> I wrote a program and there is a loop. At each iteration I use maximum
> likelihood factor analysis (?factanal). Output of factor analysis I use
> later (in this loop). Unfortunately from time to time I get an error message
> (I paste it below) and everything is stopped. Is it possible to write a
> condition that if error appears in factor analysis (FA), change input data
> for FA and do it again? Example
> 
>  
> 
> for (i in 1:1000){
> 
> FA<-factanal(data,factor=3)
> 
> If error appears change data to data2 and do factor analysis again
> 
> #rest of the program  
> 
> ............
> 
> ............
> 
> }
> 
>  
> 
> Best,
> 
> Robert 
> 
>     
> 
> Error in optim(start, FAfn, FAgr, method = "L-BFGS-B", lower = lower,  : 
> 
>         L-BFGS-B needs finite values of 'fn'
> 
> In addition: Warning message:
> 
> NaNs produced in: log(x)
> 
>  
> 
>  
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From jfkincaid at gmail.com  Thu Jul 20 03:34:04 2006
From: jfkincaid at gmail.com (Joel kincaid)
Date: Wed, 19 Jul 2006 21:34:04 -0400
Subject: [R] Sweave and multipage lattice
In-Reply-To: <LPEJLJACLINDNMBMFAFIGEHOCDAA.dieter.menne@menne-biomed.de>
References: <LPEJLJACLINDNMBMFAFIGEHOCDAA.dieter.menne@menne-biomed.de>
Message-ID: <159bc9be0607191834g1da5dbd6s4f67398422e54456@mail.gmail.com>

On 7/18/06, Dieter Menne <dieter.menne at menne-biomed.de> wrote:
> Dear R-Listeners,
>
> as the Sweave faq says:
>
> http://www.ci.tuwien.ac.at/~leisch/Sweave/FAQ.html
>
> creating several figures from one figure chunk does not work, and for
> standard graphics, a workaround is given. Now I have a multipage trellis
> plot with an a-priori unknown number of pages, and I don't see an elegant
> way of dividing it up into multiple pdf-files.

try searching the listserv for the following message title (no quotes):
"Sweave and Printing Lattice Figures From Loop"
In that message I report code and give a snw file that you can try
out. Its very long and not well written, but it got the job done for
me....good luck


-- 
Joel F. Kincaid
Associate Professor of Economics
School of Business and Economics
Winston Salem State University
Winston-Salem, NC 27110
Telephone: (336) 750-2348
Fax: (336) 750-2335


From ritwik.sinha at gmail.com  Thu Jul 20 04:56:32 2006
From: ritwik.sinha at gmail.com (Ritwik Sinha)
Date: Wed, 19 Jul 2006 22:56:32 -0400
Subject: [R] Rearrange data.
Message-ID: <42bc98300607191956o687a31bfnde964fc833359d36@mail.gmail.com>

Hi,

I am trying to rearrange the following data

d.f <- data.frame(x=c(1,1,2,2), y=c(1,2,1,2), vals=c("a11", "a12",
"a21", "a22"))

to look like a table with x as the rows and y as the columns, something like

    y  1             2
x
1    a11         a12
2    a21         a22

I tried doing this

funny <- function(x,y){d.f[d.f$x==x & d.f$y==y,3]}

outer(1:2,1:2, FUN="funny")
But get the error

Error in outer(1:2, 1:2, FUN = "funny") : dim<- : dims [product 4] do
not match the length of object [2]

What am I doing wrong? I am sure there are a hundred different ways of
doing this.

version

platform i486-pc-linux-gnu
arch     i486
os       linux-gnu
system   i486, linux-gnu
status
major    2
minor    2.1
year     2005
month    12
day      20
svn rev  36812
language R

-- 
Ritwik Sinha
Graduate Student
Epidemiology and Biostatistics
Case Western Reserve University

http://darwin.cwru.edu/~rsinha


From ggrothendieck at gmail.com  Thu Jul 20 05:04:38 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 19 Jul 2006 23:04:38 -0400
Subject: [R] Rearrange data.
In-Reply-To: <42bc98300607191956o687a31bfnde964fc833359d36@mail.gmail.com>
References: <42bc98300607191956o687a31bfnde964fc833359d36@mail.gmail.com>
Message-ID: <971536df0607192004r8eaeaf0t2a174b9d25629ca6@mail.gmail.com>

This is FAQ 7.17:

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-does-outer_0028_0029-behave-strangely-with-my-function_003f

On 7/19/06, Ritwik Sinha <ritwik.sinha at gmail.com> wrote:
> Hi,
>
> I am trying to rearrange the following data
>
> d.f <- data.frame(x=c(1,1,2,2), y=c(1,2,1,2), vals=c("a11", "a12",
> "a21", "a22"))
>
> to look like a table with x as the rows and y as the columns, something like
>
>    y  1             2
> x
> 1    a11         a12
> 2    a21         a22
>
> I tried doing this
>
> funny <- function(x,y){d.f[d.f$x==x & d.f$y==y,3]}
>
> outer(1:2,1:2, FUN="funny")
> But get the error
>
> Error in outer(1:2, 1:2, FUN = "funny") : dim<- : dims [product 4] do
> not match the length of object [2]
>
> What am I doing wrong? I am sure there are a hundred different ways of
> doing this.
>
> version
>
> platform i486-pc-linux-gnu
> arch     i486
> os       linux-gnu
> system   i486, linux-gnu
> status
> major    2
> minor    2.1
> year     2005
> month    12
> day      20
> svn rev  36812
> language R
>
> --
> Ritwik Sinha
> Graduate Student
> Epidemiology and Biostatistics
> Case Western Reserve University
>
> http://darwin.cwru.edu/~rsinha
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From savio.outros at oi.com.br  Thu Jul 20 05:26:13 2006
From: savio.outros at oi.com.br (Savio Ramos)
Date: Thu, 20 Jul 2006 00:26:13 -0300
Subject: [R]
 =?utf-8?q?Confirma=C3=A7=C3=A3o_de_pedido_para_entrar_no_grup?=
 =?utf-8?q?o_python-brasil?=
In-Reply-To: <1153361132.37.1127.wi23@yahoogrupos.com.br>
References: <1153361132.37.1127.wi23@yahoogrupos.com.br>
Message-ID: <20060720002613.043437bd@tijuca>

On 20 Jul 2006 02:05:32 -0000
Yahoo! Grupos <confirm-s2-pywddWlXNxf=67g6yNq6a5yrLpQ-savio.outros=oi.com.br em yahoogrupos.com.br> wrote:

> 
> Ol? savio.outros em oi.com.br,
> 
> Recebemos sua solicita??o para entrar no grupo python-brasil 
> do Yahoo! Grupos, um servi?o de comunidades online gratuito e 
> super f?cil de usar.
> 
> Este pedido expirar? em 7 dias.
> 
> PARA ENTRAR NESTE GRUPO: 
> 
> 1) V? para o site do Yahoo! Grupos clicando neste link:
> 
>    http://br.groups.yahoo.com/i?i=pywddWlXNxf-67g6yNq6a5yrLpQ&e=savio%2Eoutros%40oi%2Ecom%2Ebr 
> 
>   (Se n?o funcionar, use os comandos para cortar e colar o link acima na
>    barra de endere?o do seu navegador.)
> 
> -OU-
> 
> 2) RESPONDA a este e-mail clicando em "Responder" e depois em "Enviar",
>    no seu programa de e-mail.
> 
> Se voc? n?o fez esta solicita??o ou se n?o tem interesse em entrar no grupo
> python-brasil, por favor, ignore esta mensagem.
> 
> Sauda??es,
> 
> Atendimento ao usu?rio do Yahoo! Grupos 
> 
> 
> O uso que voc? faz do Yahoo! Grupos est? sujeito aos
> http://br.yahoo.com/info/utos.html 
> 
> 
> 
> 
> 
> 
> 
> 


-- 
S?vio Martins Ramos -  Arquiteto
Rio de Janeiro  ICQ 174972645
Pirataria n?o! Seja livre: Linux
http://www.debian.org


From ripley at stats.ox.ac.uk  Thu Jul 20 08:12:42 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 20 Jul 2006 07:12:42 +0100 (BST)
Subject: [R] Automating package building packages and repository
	uploading
In-Reply-To: <1153351442.9477.20.camel@lenin>
References: <1153351442.9477.20.camel@lenin>
Message-ID: <Pine.LNX.4.64.0607200707450.10763@gannet.stats.ox.ac.uk>

On Thu, 20 Jul 2006, Carlos J. Gil Bellosta wrote:

> Dear Rusers,
> 
> I have developed two packages for a client of mine. After new features
> are added or bugs corrected, I upload them to my own web repository. I
> create both source and binary versions.

binary Linux packages, it seems.  The latter are .tar.gz with the arch as 
part of the name.

.zip is used for Windows packages only.

update.packages for Linux is designed to look for source packages only: 
see the 'type' argument.  You can use the distro's packaging facilities 
for binary packages, and Dirk does for the Debian R distribution.

I think those misconceptions explain your confusion.


> In fact, I made an script that checks, builds, and uploads them via ftp.
> However, I am facing two nuisances that do make it difficult to
> automate:
> 
> 1) Even if I build the binary version with the command
> 
> R CMD build --use-zip --binary $package
> 
> within my script, the output package still gets tarballed and gzipped
> instead than simply zipped. I come around this automatically extracting
> and compressing back the files but, am I missing something some other
> option that would make all this simpler?
> 
> 2) I expect my packages to be named something like
> mypackage_1.3.12.tar.gz or mypackage_1.3.12.zip. However, "sometimes"
> --I haven't looked at the code that decides the name to give to the
> packages, so it looks quite "random" to me-- they get renamed into
> something like mypackage_1.3.12_R_i486-pc-linux-gnu.tar.gz or
> mypackage_1.3.12_R_i486-pc-linux-gnu.zip. The problem is that, then, the
> update.packages() function cannot find them. Is there a way to prevent
> this trailing string from appearing in the file name? Or else, is there
> a way to have update.packages() find the package regardless of it?
> 
> I am running
> 
> platform       i486-pc-linux-gnu
> arch           i486
> os             linux-gnu
> system         i486, linux-gnu
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
> 
> on Debian Etch with kernel 2.6.15-1-k7.
> 
> Thank you very much.
> 
> Carlos J. Gil Bellosta
> http://www.datanalytics.com
> http://www.data-mining-blog.com


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From friedrich.leisch at stat.uni-muenchen.de  Thu Jul 20 08:25:01 2006
From: friedrich.leisch at stat.uni-muenchen.de (Friedrich Leisch)
Date: Thu, 20 Jul 2006 08:25:01 +0200
Subject: [R] Sweave and multipage lattice
In-Reply-To: <44BE573E.7010901@pdf.com>
References: <LPEJLJACLINDNMBMFAFIGEHOCDAA.dieter.menne@menne-biomed.de>
	<44BE573E.7010901@pdf.com>
Message-ID: <17599.8637.528321.138312@lxh5.stat.uni-muenchen.de>

>>>>> On Wed, 19 Jul 2006 11:01:02 -0500,
>>>>> Sundar Dorai-Raj (SD) wrote:

  > Dieter Menne wrote:
  >> Dear R-Listeners,
  >> 
  >> as the Sweave faq says:
  >> 
  >> http://www.ci.tuwien.ac.at/~leisch/Sweave/FAQ.html
  >> 
  >> creating several figures from one figure chunk does not work, and for
  >> standard graphics, a workaround is given. Now I have a multipage trellis
  >> plot with an a-priori unknown number of pages, and I don't see an elegant
  >> way of dividing it up into multiple pdf-files.
  >> 
  >> I noted there is a "page" event handler in the ... parameters, which would
  >> provide a handle to open/close the file.
  >> 
  >> Any good idea would be appreciated.
  >> 
  >> Dieter
  >> 

  > Hi, Dieter,

  > I haven't seen a reply to this and I don't know Sweave. However, will 
  > the following example work? It does require you know the layout for one 
  > page.

  > --sundar

  > library(lattice)

  > trellis.device(postscript, file = "barley%02d.eps",
  >                 width = 5, height = 3, onefile = FALSE,
  >                 paper = "special")
  > ## from ?xyplot
  > dotplot(variety ~ yield | site, data = barley, groups = year,
  >          key = simpleKey(levels(barley$year), space = "right"),
  >          xlab = "Barley Yield (bushels/acre) ",
  >          aspect=0.5, layout = c(1, 1), ylab=NULL)
  > dev.off()

  > files <- list.files(pattern = glob2rx("barley*.eps"))
  > for(file in files)
  >    cat("\\includegraphics{", file, "}\n\n", sep="")

Yes, doing the above embedded in a chunk with results=tex should do
the trick. You may need to enclose the call to dotplot() in print()
though.

Best,
Fritz

-- 
-----------------------------------------------------------------------
Prof. Dr. Friedrich Leisch 

Institut f?r Statistik                          Tel: (+49 89) 2180 3165
Ludwig-Maximilians-Universit?t                  Fax: (+49 89) 2180 5308
Ludwigstra?e 33
D-80539 M?nchen                 http://www.stat.uni-muenchen.de/~leisch


From jlvw at na.rau.ac.za  Thu Jul 20 09:08:43 2006
From: jlvw at na.rau.ac.za (Jacob van Wyk)
Date: Thu, 20 Jul 2006 09:08:43 +0200
Subject: [R] Permutation Distribution
Message-ID: <44BF481B020000DE00008B93@rauzen.rau.ac.za>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060720/2e6db287/attachment.pl 

From jacques.veslot at good.ibl.fr  Thu Jul 20 09:43:53 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Thu, 20 Jul 2006 09:43:53 +0200
Subject: [R] Permutation Distribution
In-Reply-To: <44BF481B020000DE00008B93@rauzen.rau.ac.za>
References: <44BF481B020000DE00008B93@rauzen.rau.ac.za>
Message-ID: <44BF3439.2000602@good.ibl.fr>

 > data1 <- expand.grid(var1=1:15, var2=1:2)
 > test <- replicate(1000, with(data.frame(var1=data1$var1, var2=sample(data1$var2)), 
diff(tapply(var1, var2, mean))))
 > hist(test)
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------

Jacob van Wyk a ?crit :
> Hallo
>  
> Is there an elegant way to do the following:
>  
> Dataset consists of 2 variables: var1: some measurements, and var2: a grouping variable with two values, 1 and 2.
> There are (say) 10 measurements from group 1 and 15 measurements from group 2.
> The idea is to study the permutation distribution of mean(group 1) * mean(group2).
> One way would be to permute 1s and 2s and select the corresponding measurements; calculate the difference in means.
> Redo this 1000 times, say. Etc.
>  
> Any help is much appreciated.
> Thanks
> Jacob
>  
>  
> Jacob L van Wyk
> Department of Statistics
> University of Johannesburg, APK
> P O Box 524
> Auckland Park 2006
> South Africa
> Tel: +27 11 489 3080
> Fax: +27 11 489 2832
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From g.comte at alliance-ir.net  Thu Jul 20 09:55:42 2006
From: g.comte at alliance-ir.net (COMTE Guillaume)
Date: Thu, 20 Jul 2006 09:55:42 +0200
Subject: [R] hist or barplot
Message-ID: <15C100200A5F4E45AF8CFB45A926EF3415E9D6@allexch01.alliance.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060720/5eb20be1/attachment.pl 

From maechler at stat.math.ethz.ch  Thu Jul 20 09:55:22 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 20 Jul 2006 09:55:22 +0200
Subject: [R] showMethods()! Re:  How to find S4 generics?
In-Reply-To: <44BEB0FE.90707@pdf.com>
References: <878be2c60607150822r5914a231h41d537f2f5b446b4@mail.gmail.com>
	<44BDA929.8000407@pdf.com>
	<Pine.LNX.4.64.0607190739540.32733@gannet.stats.ox.ac.uk>
	<44BE7B53.4070104@pdf.com>
	<Pine.LNX.4.64.0607191449030.18735@homer21.u.washington.edu>
	<44BEB0FE.90707@pdf.com>
Message-ID: <17599.14058.449041.415808@stat.math.ethz.ch>

>>>>> "SpG" == Spencer Graves <spencer.graves at pdf.com>
>>>>>     on Wed, 19 Jul 2006 15:23:58 -0700 writes:

    SpG> Hi, Thomas: Thanks very much.  I haven't tried it yet,
    SpG> but it looks very useful.  Best Wishes, Spencer Graves

Hmm,  ?methods  has been containing for a while

methods> Note:
methods> 
methods>    This scheme is called _S3_ (S version 3).  For new projects, it is
methods>    recommended to use the more flexible and robust _S4_ scheme
methods>    provided in the 'methods' package.  Functions can have both S3 and
methods>    S4 methods, and function 'showMethods' will list the S4 methods
methods>    (possibly none).

So I wonder why nowbody mentioned the official

  showMethods()

  (or are you all not reading the help pages  ;-\) )

It does  not always return / print what I want exactly, and in
the past, at one point I put some effort to make its output more
customizable.  
I didn't put that effort to the end (*)
but for the present case

    showMethods(class = "ddiMatrix", where = "package:Matrix")

at least is pretty useful.

(*)  I think I had added the  'showEmpty = TRUE'  argument
     where the current documentation says

 showM>   showEmpty: logical indicating if methods with empty
 showM>              method lists should be shown at all.  
 showM>     Note that 'FALSE' is _not yet implemented_.

and the non-implementation had a reason: implementation was
definitely much less simple than I had hoped ...

Martin


    SpG> Thomas Lumley wrote:
    >> On Wed, 19 Jul 2006, Spencer Graves wrote:
    >>> Am I correct then that the 'methods' function could, at
    >>> least theoretically, be revised so methods(class=...)
    >>> could identify both S3 and S4 methods (ignoring
    >>> inheritance, as it does now, I believe)?
    >>> 
    >>  Here is a function to find methods for a formal
    >> class. It returns a list with elements corresponding to a
    >> generic, and each element is a list of strings showing
    >> all the signatures that contain any of the specified
    >> classes.
    >> 
    >> If super=TRUE it looks at all superclasses, if ANY=TRUE
    >> it also returns methods for ANY class.
    >> 
    >> If you have lme4 loaded, try methods4("lmer"
    >> methods4("ddiMatrix") methods4("ddiMatrix",super=TRUE)
    >> 
    >> -thomas
    >> 
    >> methods4<-function(classes, super=FALSE, ANY=FALSE){ if
    >> (super) classes<-unlist(sapply(classes, function(cl)
    >> getAllSuperClasses(getClass(cl)))) if (ANY)
    >> classes<-c(classes,"ANY") gens<-allGenerics()@.Data
    >> sigs<-lapply(gens, function(g)
    >> linearizeMlist(getMethods(g))@classes)
    >> names(sigs)<-gens at .Data sigs<-lapply(sigs, function(gen){
    >> gen[unlist(sapply(gen, function(sig) any(sig %in%
    >> classes)))]}) sigs[sapply(sigs,length)>0] }
    >> 
    >> 
    >> Thomas Lumley Assoc. Professor, Biostatistics
    >> tlumley at u.washington.edu University of Washington,
    >> Seattle
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
    >> read the posting guide
    >> http://www.R-project.org/posting-guide.html and provide
    >> commented, minimal, self-contained, reproducible code.

    SpG> ______________________________________________
    SpG> R-help at stat.math.ethz.ch mailing list
    SpG> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
    SpG> read the posting guide
    SpG> http://www.R-project.org/posting-guide.html and provide
    SpG> commented, minimal, self-contained, reproducible code.


From jacques.veslot at good.ibl.fr  Thu Jul 20 10:03:39 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Thu, 20 Jul 2006 10:03:39 +0200
Subject: [R] hist or barplot
In-Reply-To: <15C100200A5F4E45AF8CFB45A926EF3415E9D6@allexch01.alliance.com>
References: <15C100200A5F4E45AF8CFB45A926EF3415E9D6@allexch01.alliance.com>
Message-ID: <44BF38DB.3060409@good.ibl.fr>

 > s1 <- runif(10,0,10)
 > s1
  [1] 8.328396 2.840870 7.401377 9.998165 5.045539 9.568728 5.372493 5.232439
  [9] 5.774790 4.224103
 > s2 <- runif(10,0,10)
 > s2
  [1] 1.230750 3.855060 8.652698 7.846725 9.100171 7.309179 9.235562 1.581741
  [9] 6.979521 1.918997
 > ss <- cbind(s1,s2)
 > smin <- apply(ss,1,min)
 > smax <- apply(ss,1,max)
 > barplot(smax)
 > barplot(smin, add=T, col="white")

-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


COMTE Guillaume a ?crit :
>  
> 
> Hi all,
> 
>  
> 
> I wish to draw  2 hist (or barplot) on the same graph.
> 
>  
> 
> I can do it simply by using par(new=TRUE) , but it overlap with the
> first drawn, how to tell R to put in front of the graph the min value of
> the two graph in order for it to be seen and don't hide each other.
> 
>  
> 
> I've been looking at help for barplot or hist but didn't find
> anything... (or my english is too poor to understand)
> 
>  
> 
> thks
> 
> COMTE Guillaume
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From YCH at softcomputing.com  Thu Jul 20 10:10:09 2006
From: YCH at softcomputing.com (Yohan CHOUKROUN)
Date: Thu, 20 Jul 2006 10:10:09 +0200
Subject: [R] RE :   how to use large data set ?
Message-ID: <C8F48FD780E12D4DB197507B897D00DD072F59CC@ntexch.softcomputing.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060720/99a0e85f/attachment.pl 

From ulriks at ruc.dk  Thu Jul 20 11:08:27 2006
From: ulriks at ruc.dk (Ulrik Stervbo)
Date: Thu, 20 Jul 2006 11:08:27 +0200
Subject: [R] Fitting a distribution to peaks in histogram
In-Reply-To: <f8e6ff050607191227y6b598284p7aae305f0aee6aa5@mail.gmail.com>
References: <3483f8d50607190258n1f711443p869f79850e81fa79@mail.gmail.com>
	<f8e6ff050607190920o366c8a6dvcecc24fbed7eb7f@mail.gmail.com>
	<3483f8d50607190956h45119bf5h282550e58939f7b5@mail.gmail.com>
	<f8e6ff050607191227y6b598284p7aae305f0aee6aa5@mail.gmail.com>
Message-ID: <3483f8d50607200208x29a745b1j2d5c772a3abf41ac@mail.gmail.com>

On 7/19/06, hadley wickham <h.wickham at gmail.com> wrote:
> > Can you be a bit more excact? I a biologist and relatively new to R
>
> In that case, I would _strongly_ advise that you get advice from a
> local statistician.

I am afraid that, by comparison, I am the local statistican. I am also
the local R-guru, and neither is saying much  - so please bear with
me.

Do you know of some functions (built in hopefully) that I can try?

I did try the density estimate from the Mclust package, but got an out
of memory error. I did look at the Ash package, but I am afraid I
failed to see how I can use it.

At the moment, I am estimating the density, using the stats density(),
identify the peaks in the density estimate by Petr's function, and can
thus extract a very good suggestion for a mean and intensity for each
peak - surely that must be useful for something? Based on the
literature I also have a very good suggestion for at upper and lower
width of the distribution.

>
> > I am measureing the amount of DNA in cells, and I need to know the
> > percentage of cells in a part of the cell cycle; that the percentage
> > of cells in the first peak, in the second peak and so on. I want to
> > integrate the area between to two cells, because that apparently is
> > how its none (as far as I can tell from the literature)
>
> That doesn't sound quite right to me, because you also need to take
> into account the fact that some cells between peak 1 and 2 belong to
> peak 1, and some to peak 2.  This is something that will come out
> immediately from a mixture based approach. If you know that peaks
> correspond to certain parts of the cell cycle, then this is important
> information that should be included in the analysis.

I realise that some cells between to peaks belong to the peaks, but
thought that this was a general problem, usually sacrificed for speed.
One of the most widely used programs for analysing cell cycle use a
variant of my strategy as far as I can tell; fitting Gaussian
distributions to the two peaks and integrate the part between. The
reason why I am not using this program is that I cannot afford it, and
it does a very poor job when analysing cells with abnormal amounts of
DNA.

Ulrik

-- 
Blog: http://ulrikstervbo.blogspot.com


From johannes at huesing.name  Thu Jul 20 11:32:51 2006
From: johannes at huesing.name (Johannes =?iso-8859-1?Q?H=FCsing?=)
Date: Thu, 20 Jul 2006 11:32:51 +0200 (CEST)
Subject: [R] dotchart with log scale?
Message-ID: <17559.129.206.90.2.1153387971.squirrel@mail.panix.com>

Dear all,
I would like to draw a dot chart on a log scale.
What is the syntax for this? A barchart may use
log="x", but trying this with dotchart() leads
to an error message.

Greetings


Johannes


From kie at ucla.edu  Thu Jul 20 11:45:25 2006
From: kie at ucla.edu (Kie Zuraw)
Date: Thu, 20 Jul 2006 02:45:25 -0700
Subject: [R] plain shading (not residuals) in mosaic plot
In-Reply-To: <c7c17cef0607191006v178908cha8646cc5abe794a4@mail.gmail.com>
References: <20060719074732.syj6nbjo08g0w8cc@mail.ucla.edu>
	<971536df0607190836i7b7934b2sb73c304b929fcccf@mail.gmail.com>
	<c7c17cef0607191006v178908cha8646cc5abe794a4@mail.gmail.com>
Message-ID: <20060720024525.26lzuuxu3kgw8088@mail.ucla.edu>

Thank you very much, Gabor Grothendieck and Muhammad Subianto. Both of 
these work perfectly. I think I was misunderstanding gp and gpar() 
before. Again, thank you both.

-KZ

Quoting Muhammad Subianto <msubianto at gmail.com>:

> Maybe like this:
> mosaic(allmorph, direction = "v", pop = FALSE,
> gp=gpar(fill=c(grey(0.8),grey(0.4))))
>
> Best, Muhammad Subianto
>
>
> On 7/19/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>> If you look at ?mosaic the ... argument says it gets passed to strucplot and
>> looking at ?strucplot we see it accepts a gp= arg so try this (same
>> as your plus gp= arg):
>>
>> cols <- c(grey(0.8),grey(0.4))
>> mosaic(allmorph, direction = "v", pop = FALSE, gp = list(col = cols))
>>
>> On 7/19/06, Kie Zuraw <kie at ucla.edu> wrote:
>> > Hello. I've been using R for a couple of months and enjoying it a lot.
>> > This is my first post to R-help.
>> >
>> > I'm using the vcd package to make mosaic plots with labels on the tiles
>> > indicating the number of items in each cell.
>> >
>> > For example, I've made this plot:
>> >
>> >
>> > > allmorph<-structure(c(10, 26, 17, 100, 70, 97, 253, 430, 185, 177,
>> > > 25, 1), .Dim = as.integer(c(6, 2)), .Dimnames =
>> > > structure(list(Stem.initial.obstruent = c("p", "t,s",
>> > > "k","b","d","g"),Subst.behavior=c("unsubstituted","substituted")),
>> > > .Names = c("Stem-initial obstruent","Behavior according to
>> > > dictionary")), class = "table")
>> > > mosaic(allmorph,direction="v",pop=FALSE)
>> > > labeling_cells(text=allmorphs,margin=0)(allmorph)
>> >
>> >
>> > So far so good. What I can't figure out how to do--after searching
>> > through the vcd documentation
>> > (http://cran.r-project.org/doc/packages/vcd.pdf), Googling, and
>> > checking the r-help archive--is how to shade the tiles according to
>> > their values for the variables rather than to reflect residuals. That
>> > is, I want all the tiles at the bottom, whose value for the x-axis
>> > variable is "substituted", to be dark grey, and those at the top, in
>> > the "unsubstituted" category, to be light grey.
>> >
>> > I know how to do it with mosaicplot():
>> >
>> > > mosaicplot(morphs3,color=c(grey(0.8),grey(0.4)))
>> >
>> > ...but this doesn't work with mosaic(): the command
>> > "mosaic(morphs3,color=c(grey(0.8),grey(0.4)))" yields a plot with all
>> > tiles the same color. And conversely, I can't find a way to use
>> > mosaicplot() and add numeric labels to the tiles--without much hope of
>> > success, I tried combining mosaicplot() with labeling_cells(), but,
>> > unsurprisingly, it didn't work:
>> >
>> > > mosaicplot(morphs3,color=c(grey(0.8),grey(0.4)),pop=FALSE)
>> > Warning message:
>> > extra argument(s) 'pop' will be disregarded in:
>> > mosaicplot.default(morphs3, color = c(grey(0.8), grey(0.4)),
>> > > labeling_cells(text=morphs3,margin=0)(morphs3)
>> > Error in downViewport.vpPath(vpPathDirect(name), strict, recording =
>> > recording) :        Viewport 'cell:Stem-initial obstruent=p,Behavior
>> > according to dictionary=unsubstituted' was not found
>> >
>> >
>> > Does anyone know how to get both the shading I want and the labels I
>> > want, whether with mosaic(), with mosaicplot(), or in some other way?
>> >
>> > Thanks for your attention.
>> >
>> > -Kie Zuraw
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From alessandro at idsia.ch  Thu Jul 20 12:12:31 2006
From: alessandro at idsia.ch (Alessandro Antonucci)
Date: Thu, 20 Jul 2006 12:12:31 +0200
Subject: [R] Function with an array (extracted from a matrix) as argument
Message-ID: <20060720101231.GA8126@idsia.ch>

I wrote a function f whose argument is an array

If I set

x <- c(5,1,2)

and I run f(x) everything works fine.

On the other hand, If I extract the same array
from a matrix by:

x <- my.matrix[1,1:3]

such that the first three elements of the
first row of my.matrix are exactly 5  1  2,
something seems to work differently.

Any idea about that?

Thanks in advance,
Alessandro

-- 
============================================================
Alessandro Antonucci
Dalle Molle Institute for Artificial Intelligence (IDSIA)
at Idsia			e-mail: alessandro at idsia.ch
Galleria 2			web:   idsia.ch/~alessandro
Via Cantonale			mobile:   +39 339-567-23-28
CH-6928				tel:       +41 58-666-66-69
Manno - Lugano			fax:       +41 58-666-66-61
Switzerland			skype: alessandro.antonucci


From ivan.kalafatic at gmail.com  Thu Jul 20 12:34:48 2006
From: ivan.kalafatic at gmail.com (Ivan Kalafatic)
Date: Thu, 20 Jul 2006 11:34:48 +0100
Subject: [R] Question about functions in R
Message-ID: <6dbf89a50607200334m48efb371p21981765cade1a94@mail.gmail.com>

I tried to make the following function:

function(x, y){
   dates<-intersect(x[,1],y[,1])
   m<-matrix(NA,length(dates),3)
   m[,1]<-dates
   j<-1
   k<-1
   for(i in fdax[,1]){
      if(is.element(i,dates)){
         m[j,3]<-as.numeric(fdax[k,2])
         j<-j+1
      }
      k<-k+1
   }
   return(m)
}

When I try to import it into R with edit( file="name.txt",
editor="someeditor") I get the response that there is an error on the
line where last bracket is.
When I execute this code manualy in R command by command everything works ok.
This happened to me before with other functions. Sometimes I can
import them and they work perfectly but sometimes I get the error on
the last line of code which is alvays the final bracket.

Anyone has any idea why this is happening?


From cgb at datanalytics.com  Thu Jul 20 12:24:58 2006
From: cgb at datanalytics.com (Carlos J. Gil Bellosta)
Date: Thu, 20 Jul 2006 12:24:58 +0200
Subject: [R] Automating package building packages and
	repository	uploading
In-Reply-To: <Pine.LNX.4.64.0607200707450.10763@gannet.stats.ox.ac.uk>
References: <1153351442.9477.20.camel@lenin>
	<Pine.LNX.4.64.0607200707450.10763@gannet.stats.ox.ac.uk>
Message-ID: <20060720122458.jxfdqk6c6bk08s0w@webmail.datanalytics.com>

Dear Rusers,

Well, then it seems that the problem is that I am building "linux binary
packages". Since I do not have any compiled code within --just R code--, their
contents should --and, in fact, are-- directly installable on Windows 
platforms
(which is what I intend to do).

If I understand things right, I could just rebundle the packages using zip
instead of tar | gzip and getting rid of the "arch" string in the file name.
They they would work on Windows. And they actually do when I do this by hand.

But, is there a less involved way to generate these binary Windows 
packages with
proper file names and compression method directly from my linux box?

Thank you very much,

Carlos J. Gil Bellosta
http://www.datanalytics.com
http://www.data-mining-blog.com

Quoting Prof Brian Ripley <ripley at stats.ox.ac.uk>:

> On Thu, 20 Jul 2006, Carlos J. Gil Bellosta wrote:
>
>> Dear Rusers,
>>
>> I have developed two packages for a client of mine. After new features
>> are added or bugs corrected, I upload them to my own web repository. I
>> create both source and binary versions.
>
> binary Linux packages, it seems.  The latter are .tar.gz with the arch as
> part of the name.
>
> .zip is used for Windows packages only.
>
> update.packages for Linux is designed to look for source packages only:
> see the 'type' argument.  You can use the distro's packaging facilities
> for binary packages, and Dirk does for the Debian R distribution.
>
> I think those misconceptions explain your confusion.
>
>
>> In fact, I made an script that checks, builds, and uploads them via ftp.
>> However, I am facing two nuisances that do make it difficult to
>> automate:
>>
>> 1) Even if I build the binary version with the command
>>
>> R CMD build --use-zip --binary $package
>>
>> within my script, the output package still gets tarballed and gzipped
>> instead than simply zipped. I come around this automatically extracting
>> and compressing back the files but, am I missing something some other
>> option that would make all this simpler?
>>
>> 2) I expect my packages to be named something like
>> mypackage_1.3.12.tar.gz or mypackage_1.3.12.zip. However, "sometimes"
>> --I haven't looked at the code that decides the name to give to the
>> packages, so it looks quite "random" to me-- they get renamed into
>> something like mypackage_1.3.12_R_i486-pc-linux-gnu.tar.gz or
>> mypackage_1.3.12_R_i486-pc-linux-gnu.zip. The problem is that, then, the
>> update.packages() function cannot find them. Is there a way to prevent
>> this trailing string from appearing in the file name? Or else, is there
>> a way to have update.packages() find the package regardless of it?
>>
>> I am running
>>
>> platform       i486-pc-linux-gnu
>> arch           i486
>> os             linux-gnu
>> system         i486, linux-gnu
>> status
>> major          2
>> minor          3.1
>> year           2006
>> month          06
>> day            01
>> svn rev        38247
>> language       R
>> version.string Version 2.3.1 (2006-06-01)
>>
>> on Debian Etch with kernel 2.6.15-1-k7.
>>
>> Thank you very much.
>>
>> Carlos J. Gil Bellosta
>> http://www.datanalytics.com
>> http://www.data-mining-blog.com
>
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From alessandro at idsia.ch  Thu Jul 20 12:44:54 2006
From: alessandro at idsia.ch (Alessandro Antonucci)
Date: Thu, 20 Jul 2006 12:44:54 +0200
Subject: [R] Function with an array (extracted from a matrix) as argument
In-Reply-To: <20060720101231.GA8126@idsia.ch>
References: <20060720101231.GA8126@idsia.ch>
Message-ID: <20060720104454.GA9260@idsia.ch>

Sorry to disturb again the list subscribers,
but I fixed my problem, by simply using the c
operator:

x <- c(my.matrix[1,1:3])

Alessandro


* Alessandro Antonucci <alessandro at idsia.ch> [200706, 12:12]:
> I wrote a function f whose argument is an array
> 
> If I set
> 
> x <- c(5,1,2)
> 
> and I run f(x) everything works fine.
> 
> On the other hand, If I extract the same array
> from a matrix by:
> 
> x <- my.matrix[1,1:3]
> 
> such that the first three elements of the
> first row of my.matrix are exactly 5  1  2,
> something seems to work differently.
> 
> Any idea about that?
> 
> Thanks in advance,
> Alessandro
> 
> -- 
> ============================================================
> Alessandro Antonucci
> Dalle Molle Institute for Artificial Intelligence (IDSIA)
> at Idsia			e-mail: alessandro at idsia.ch
> Galleria 2			web:   idsia.ch/~alessandro
> Via Cantonale			mobile:   +39 339-567-23-28
> CH-6928				tel:       +41 58-666-66-69
> Manno - Lugano			fax:       +41 58-666-66-61
> Switzerland			skype: alessandro.antonucci
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
============================================================
Alessandro Antonucci
Dalle Molle Institute for Artificial Intelligence (IDSIA)
at Idsia			e-mail: alessandro at idsia.ch
Galleria 2			web:   idsia.ch/~alessandro
Via Cantonale			mobile:   +39 339-567-23-28
CH-6928				tel:       +41 58-666-66-69
Manno - Lugano			fax:       +41 58-666-66-61
Switzerland			skype: alessandro.antonucci


From p.dalgaard at biostat.ku.dk  Thu Jul 20 13:00:11 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Jul 2006 13:00:11 +0200
Subject: [R] Question about functions in R
In-Reply-To: <6dbf89a50607200334m48efb371p21981765cade1a94@mail.gmail.com>
References: <6dbf89a50607200334m48efb371p21981765cade1a94@mail.gmail.com>
Message-ID: <x2mzb4sfr8.fsf@turmalin.kubism.ku.dk>

"Ivan Kalafatic" <ivan.kalafatic at gmail.com> writes:

> I tried to make the following function:
> 
> function(x, y){
>    dates<-intersect(x[,1],y[,1])
>    m<-matrix(NA,length(dates),3)
>    m[,1]<-dates
>    j<-1
>    k<-1
>    for(i in fdax[,1]){
>       if(is.element(i,dates)){
>          m[j,3]<-as.numeric(fdax[k,2])
>          j<-j+1
>       }
>       k<-k+1
>    }
>    return(m)
> }
> 
> When I try to import it into R with edit( file="name.txt",
> editor="someeditor") I get the response that there is an error on the
> line where last bracket is.
> When I execute this code manualy in R command by command everything works ok.
> This happened to me before with other functions. Sometimes I can
> import them and they work perfectly but sometimes I get the error on
> the last line of code which is alvays the final bracket.
> 
> Anyone has any idea why this is happening?

It's not happening for me with

> edit( file="tmp/xxx.R", editor="emacs")
function(x, y){
   dates<-intersect(x[,1],y[,1])
   m<-matrix(NA,length(dates),3)
   m[,1]<-dates
   j<-1
   k<-1
   for(i in fdax[,1]){
      if(is.element(i,dates)){
         m[j,3]<-as.numeric(fdax[k,2])
         j<-j+1
      }
      k<-k+1
   }
   return(m)
}


(after copying your function into tmp/xxx.R of course).

So there is something you're not telling us... (could there be an
incomplete last line, by any chance?)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From h.wickham at gmail.com  Thu Jul 20 13:23:13 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 20 Jul 2006 12:23:13 +0100
Subject: [R] Fitting a distribution to peaks in histogram
In-Reply-To: <3483f8d50607200208x29a745b1j2d5c772a3abf41ac@mail.gmail.com>
References: <3483f8d50607190258n1f711443p869f79850e81fa79@mail.gmail.com>
	<f8e6ff050607190920o366c8a6dvcecc24fbed7eb7f@mail.gmail.com>
	<3483f8d50607190956h45119bf5h282550e58939f7b5@mail.gmail.com>
	<f8e6ff050607191227y6b598284p7aae305f0aee6aa5@mail.gmail.com>
	<3483f8d50607200208x29a745b1j2d5c772a3abf41ac@mail.gmail.com>
Message-ID: <f8e6ff050607200423u5b5021d1r77752422a7017cf2@mail.gmail.com>

> I am afraid that, by comparison, I am the local statistican. I am also
> the local R-guru, and neither is saying much  - so please bear with
> me.
>
> Do you know of some functions (built in hopefully) that I can try?

I'm a bit leery of offering advice without really sitting down and
discussing your problem, but I've expanded my initial thoughts into
hopefully something you can follow.

You mentioned that the peaks in the density correspond to different
stages of the cell cycle.  For this reason, I am going to assume that
there are known number of peaks.  If the number of peaks isn't known
then this becomes more complicated.  However, it sounds like even in
that case there is a fairly low upper limit, so I think this approach
should be ok. (And a model selection process based on AIC/BIC, if used
with care, should be ok)

I think your problem boils down to fitting your data to a mixture of
distributions.  A good place to start would be a literature search to
see what others have tried in the past, especially what distributions
they used.  I suspect you have already tried this with little luck.
Because the patterns in the histogram you showed looked rather skewed,
I would start with a mixture of gammas (this being a fairly flexible
distribution esp wrt skewness).

To do this, the first step is to write out the likelihood equation for
a mixture of gammas (you will need to do most of this yourself), which
will be of the form:

f(x) = a_1 g(alpha_1, beta_1)(x) + a_2 g(alpha_2, beta_2)(x) + ... +
(1- sum(a_i) g(alpha_n, beta_n)(x)

(this is for fixed n).  This gives you (n-1) + 2*n parameters to
estimate, which should be fine given the large amount of data you
have.  I would then use mle or optim to fit to estimate the parameters
from the data.  (You will need starting values for the algorthim,
which you should be able to generate from the output you already
have).  This should be pretty fast if you vectorise appropriately.

(you will also need something to account for the final value which
looked to have a high occurence in your histogram (perhaps from
truncation?))

Once you have these estimates you should be able to get all the other
data from them.  You can also get standard errors for your estimates
using standard ml theory (which should be reasonable given the large
number of events)

Now for the caveats: I am only a PhD student, so there may be good
reasons not to take this approach.  If someone else suggested this
approach to me, I would criticise the arbitrary choice of
distribution, and would need reassurance there wouldn't be aliasing
problems (eg. where very different combinations of the parameters
given precisely the same results, will be somewhat alleviated with
good defaults).  There may be other problems that I am not aware of,
and as I don't really know that much about what you are trying to do,
there may be radically simpler or more accurate methods.

Regards,

Hadley


From ripley at stats.ox.ac.uk  Thu Jul 20 13:30:13 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 20 Jul 2006 12:30:13 +0100 (BST)
Subject: [R] Automating package building packages and repository
	uploading
In-Reply-To: <20060720122458.jxfdqk6c6bk08s0w@webmail.datanalytics.com>
References: <1153351442.9477.20.camel@lenin>
	<Pine.LNX.4.64.0607200707450.10763@gannet.stats.ox.ac.uk>
	<20060720122458.jxfdqk6c6bk08s0w@webmail.datanalytics.com>
Message-ID: <Pine.LNX.4.64.0607201219590.15030@gannet.stats.ox.ac.uk>

On Thu, 20 Jul 2006, Carlos J. Gil Bellosta wrote:

> Dear Rusers,
> 
> Well, then it seems that the problem is that I am building "linux binary
> packages". Since I do not have any compiled code within --just R code--, their
> contents should --and, in fact, are-- directly installable on Windows
> platforms
> (which is what I intend to do).
> 
> If I understand things right, I could just rebundle the packages using zip
> instead of tar | gzip and getting rid of the "arch" string in the file name.
> They they would work on Windows. And they actually do when I do this by hand.
> 
> But, is there a less involved way to generate these binary Windows 
> packages with proper file names and compression method directly from my 
> linux box?

`involved'?  Not anything like as involved as working out what it is you 
are trying to do from your description which made no mention anywhere of 
Windows!

You can cross-compile the packages on your Linux box if you prefer, which 
will give you zips with the right file names.

Either way, you will be missing Compiled HTML help, which is likely to be 
the default form on Windows help in the next release of R.

> 
> Thank you very much,
> 
> Carlos J. Gil Bellosta
> http://www.datanalytics.com
> http://www.data-mining-blog.com
> 
> Quoting Prof Brian Ripley <ripley at stats.ox.ac.uk>:
> 
> > On Thu, 20 Jul 2006, Carlos J. Gil Bellosta wrote:
> >
> > > Dear Rusers,
> > >
> > > I have developed two packages for a client of mine. After new features
> > > are added or bugs corrected, I upload them to my own web repository. I
> > > create both source and binary versions.
> >
> > binary Linux packages, it seems.  The latter are .tar.gz with the arch as
> > part of the name.
> >
> > .zip is used for Windows packages only.
> >
> > update.packages for Linux is designed to look for source packages only:
> > see the 'type' argument.  You can use the distro's packaging facilities
> > for binary packages, and Dirk does for the Debian R distribution.
> >
> > I think those misconceptions explain your confusion.
> >
> >
> > > In fact, I made an script that checks, builds, and uploads them via ftp.
> > > However, I am facing two nuisances that do make it difficult to
> > > automate:
> > >
> > > 1) Even if I build the binary version with the command
> > >
> > > R CMD build --use-zip --binary $package
> > >
> > > within my script, the output package still gets tarballed and gzipped
> > > instead than simply zipped. I come around this automatically extracting
> > > and compressing back the files but, am I missing something some other
> > > option that would make all this simpler?
> > >
> > > 2) I expect my packages to be named something like
> > > mypackage_1.3.12.tar.gz or mypackage_1.3.12.zip. However, "sometimes"
> > > --I haven't looked at the code that decides the name to give to the
> > > packages, so it looks quite "random" to me-- they get renamed into
> > > something like mypackage_1.3.12_R_i486-pc-linux-gnu.tar.gz or
> > > mypackage_1.3.12_R_i486-pc-linux-gnu.zip. The problem is that, then, the
> > > update.packages() function cannot find them. Is there a way to prevent
> > > this trailing string from appearing in the file name? Or else, is there
> > > a way to have update.packages() find the package regardless of it?
> > >
> > > I am running
> > >
> > > platform       i486-pc-linux-gnu
> > > arch           i486
> > > os             linux-gnu
> > > system         i486, linux-gnu
> > > status
> > > major          2
> > > minor          3.1
> > > year           2006
> > > month          06
> > > day            01
> > > svn rev        38247
> > > language       R
> > > version.string Version 2.3.1 (2006-06-01)
> > >
> > > on Debian Etch with kernel 2.6.15-1-k7.
> > >
> > > Thank you very much.
> > >
> > > Carlos J. Gil Bellosta
> > > http://www.datanalytics.com
> > > http://www.data-mining-blog.com

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jim at bitwrit.com.au  Fri Jul 21 04:13:24 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Thu, 20 Jul 2006 22:13:24 -0400
Subject: [R] Question about functions in R
In-Reply-To: <6dbf89a50607200334m48efb371p21981765cade1a94@mail.gmail.com>
References: <6dbf89a50607200334m48efb371p21981765cade1a94@mail.gmail.com>
Message-ID: <44C03844.2000106@bitwrit.com.au>

Ivan Kalafatic wrote:
> I tried to make the following function:
> 
> function(x, y){
>    dates<-intersect(x[,1],y[,1])
>    m<-matrix(NA,length(dates),3)
>    m[,1]<-dates
>    j<-1
>    k<-1
>    for(i in fdax[,1]){
>       if(is.element(i,dates)){
>          m[j,3]<-as.numeric(fdax[k,2])
>          j<-j+1
>       }
>       k<-k+1
>    }
>    return(m)
> }
> 
> When I try to import it into R with edit( file="name.txt",
> editor="someeditor") I get the response that there is an error on the
> line where last bracket is.
> When I execute this code manualy in R command by command everything works ok.
> This happened to me before with other functions. Sometimes I can
> import them and they work perfectly but sometimes I get the error on
> the last line of code which is alvays the final bracket.
> 
> Anyone has any idea why this is happening?

Hi Ivan,

The only thing I can see is that you haven't assigned the function to a 
name. That is, I would usually start with:

my.date.function<-function(...

I don't know where this might cause a problem, but see if it makes a 
difference.

Jim


From HDoran at air.org  Thu Jul 20 14:14:37 2006
From: HDoran at air.org (Doran, Harold)
Date: Thu, 20 Jul 2006 08:14:37 -0400
Subject: [R] Timing benefits of mapply() vs. for loop was: Wrap a loop
	inside a function
Message-ID: <2323A6D37908A847A7C32F1E3662C80E132D9F@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060720/2d520491/attachment.pl 

From h.wickham at gmail.com  Thu Jul 20 14:32:03 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 20 Jul 2006 13:32:03 +0100
Subject: [R] dotchart with log scale?
In-Reply-To: <17559.129.206.90.2.1153387971.squirrel@mail.panix.com>
References: <17559.129.206.90.2.1153387971.squirrel@mail.panix.com>
Message-ID: <f8e6ff050607200532i68c40331t3635d80d4aa50df3@mail.gmail.com>

> I would like to draw a dot chart on a log scale.
> What is the syntax for this? A barchart may use
> log="x", but trying this with dotchart() leads
> to an error message.

You can do this easily with ggplot:

install.packages("ggplot")
library(ggplot)
qplot(mpg, factor(cyl), data=mtcars, log="x")

Regards,

Hadley


From dmedri at gmail.com  Thu Jul 20 14:38:02 2006
From: dmedri at gmail.com (Daniele Medri)
Date: Thu, 20 Jul 2006 14:38:02 +0200
Subject: [R] how can I delete rows?
In-Reply-To: <20060718152547.71330.qmail@web30615.mail.mud.yahoo.com>
References: <20060718152547.71330.qmail@web30615.mail.mud.yahoo.com>
Message-ID: <1153399083.16214.3.camel@localhost.localdomain>

Il giorno mar, 18/07/2006 alle 12.25 -0300, raul sanchez ha scritto:
> how can I select the countries of Lat. Am. thank you

hint: add to the data.frame a column with the continental ID so this
could be usefull for other needs.

hint: ?subset



--
Daniele Medri


From ggrothendieck at gmail.com  Thu Jul 20 14:56:20 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 20 Jul 2006 08:56:20 -0400
Subject: [R] Timing benefits of mapply() vs. for loop was: Wrap a loop
	inside a function
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E132D9F@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E132D9F@dc1ex01.air.org>
Message-ID: <971536df0607200556y2ef617bev6de10fa6457b3514@mail.gmail.com>

Note that if you use mapply in the way I suggested, which is not
the same as in your post, then its just as fast.  (Also the version
of mapply in your post gives different numerical results than
the for loop whereas mine gives the same.)   like.mat is the for
loop version, like.mat2 is your mapply version and like.mat3
is my mapply version.

> like.mat <- function(score, items, theta){
+   like.mat <- matrix(numeric(length(items) * length(theta)), ncol =
+     length(theta))
+   for(i in 1:length(items)) like.mat[i, ] <- pcm(theta, items[[i]],
score[[i]])
+   like.mat
+ }
> system.time(for(i in 1:100) like.mat(score,items,theta))
[1] 1.30 0.00 1.34   NA   NA
>
> like.mat2 <- function(score, items, theta)
+   matrix(mapply(pcm, rep(theta,length(items)), items, score),
+     ncol = length(theta), byrow = TRUE)
> system.time(for(i in 1:100) like.mat2(score,items,theta))
[1] 5.70 0.00 5.91   NA   NA
> all.equal(like.mat(score, items, theta), like.mat2(score, items, theta))
[1] "Mean relative  difference: 1.268095"
>
> like.mat3 <- function(score, items, theta)
+ t(mapply(pcm, items, score, MoreArgs = list(theta = theta)))
> system.time(for(i in 1:100) like.mat3(score,items,theta))
[1] 1.32 0.01 1.39   NA   NA
> m3 <- like.mat3(score, items, theta)
> dimnames(m3) <- NULL
> all.equal(like.mat(score, items, theta), m3)
[1] TRUE

On 7/20/06, Doran, Harold <HDoran at air.org> wrote:
>
>
>
> List:
>
> Thank you for the replies to my post yesterday. Gabor and Phil also gave
> useful replies on how to improve the function by relying on mapply rather
> than the explicit for loop. In general, I try and use the family of apply
> functions rather than the looping constructs such as for, while etc as a
> matter of practice.
>
> However, it seems the mapply function in this case is slower (in terms of
> CPU speed) than the for loop. Here is an example.
>
> # data needed for example
>
> items <- list(item1 = c(0,1,2), item2 = c(0,1), item3 = c(0,1,2,3,4), item4
> = c(0,1), item5=c(0,1,2,3,4),
> item6=c(0,1,2,3))
> score <- c(2,1,3,1,3,2)
> theta <- c(-1,-.5,0,.5,1)
>
> # My old function using the for loop
>
> like.mat <- function(score, items, theta){
>    like.mat <- matrix(numeric(length(items) * length(theta)), ncol =
> length(theta))
>    for(i in 1:length(items)) like.mat[i, ] <- pcm(theta, items[[i]],
> score[[i]])
>    like.mat
>    }
>
> system.time(like.mat(score,items,theta))
> [1]  0  0  0 NA NA
>
> # Revised using mapply
>
> like.mat <- function(score, items, theta){
> matrix(mapply(pcm,rep(theta,length(items)),items,score),ncol=length(theta),byrow=TRUE)
> }
>
> > system.time(like.mat(score,items,theta))
> [1] 0.03 0.00 0.03   NA   NA
>
>
> It is obviously slower to use mapply, but nominaly. So, let's actually look
> at this within the context of the full program I am working on. For context,
> I am evaluating an integral using Gaussian quadrature. This is a
> psychometric problem where the function 'pcm" is Master's partial credit
> model and 'score' is the student's score on test item i. When an item has
> two categories (0,1), pcm reduces to the Rasch model for dichotomous data.
> The dnorm is set at N(0,1) by default, but the parameters of the population
> distribution are estimated from a separate procedure and are normally input
> into the function, but this default works for the example.
>
> Here is the full program.
>
> library(statmod)
>
> # Master's partial credit model
> pcm <- function(theta,d,score){
>      exp(rowSums(outer(theta,d[1:score],'-')))/
>      apply(exp(apply(outer(theta,d, '-'), 1, cumsum)), 2, sum)
>    }
>
> like.mat <- function(score, items, theta){
>    like.mat <- matrix(numeric(length(items) * length(theta)), ncol =
> length(theta))
>    for(i in 1:length(items)) like.mat[i, ] <- pcm(theta, items[[i]],
> score[[i]])
>    like.mat
>    }
>
> # turn this off for now
> #like.mat <- function(score, items, theta){
> #matrix(mapply(pcm,rep(theta,length(items)),items,score),ncol=length(theta),byrow=TRUE)
> #}
>
> class.numer <- function(score,items, prof_cut, mu=0, sigma=1, aboveQ){
>    gauss_numer <- gauss.quad(49,kind="laguerre")
>    if(aboveQ==FALSE){
>       mat <- rbind(like.mat(score,items, (prof_cut-gauss_numer$nodes)),
> dnorm(prof_cut-gauss_numer$nodes, mean=mu, sd=sigma))
>
>       } else { mat <- rbind(like.mat(score,items,
> (gauss_numer$nodes+prof_cut)),
> dnorm(gauss_numer$nodes+prof_cut, mean=mu,
> sd=sigma))
>
>    }
>    f_y <- rbind(apply(mat, 2, prod), exp(gauss_numer$nodes),
> gauss_numer$weights)
>    sum(apply(f_y,2,prod))
>    }
>
> class.denom <- function(score,items, mu=0, sigma=1){
>    gauss_denom <- gauss.quad.prob(49, dist='normal', mu=mu, sigma=sigma)
>    mat <-
> rbind(like.mat(score,items,gauss_denom$nodes),gauss_denom$weights)
>    sum(apply(mat, 2, prod))
>    }
>
> class.acc <-function(score,items,prof_cut, mu=0, sigma=1,
> aboveQ=TRUE){
>    result <- class.numer(score,items,prof_cut, mu,sigma,
> aboveQ)/class.denom(score,items, mu, sigma)
>    return(result)
>    }
>
> # Test the function "class.acc"
> items <- list(item1 = c(0,1,2), item2 = c(0,1), item3 = c(0,1,2,3,4), item4
> = c(0,1), item5=c(0,1,2,3,4),
> item6=c(0,1,2,3))
> score <- c(2,1,3,1,3,2)
>
> # This is the system time when using the for loop for the like.mat function
> system.time(class.acc(score,items,1,aboveQ=T))
> [1] 0.04 0.00 0.04   NA   NA
>
> # This is the system time using the mapply for the like.mat function
> system.time(class.acc(score,items,1,aboveQ=T))
> [1] 0.70 0.00 0.73   NA   NA
>
>
> There is a substantial improvement in CPU seconds when the for loop is
> applied rather than using the mapply function. I experimented with adding
> more items and varying the quadrature points and it always turned out the
> for loop was faster.
>
> Given this result, I wonder what advice might be offered. Is there an
> inherent reason one might opt for the mapply function (such as reliability,
> etc) even when it compromises computational speed? Or, should the issue of
> computational speed be considered ahead of common advice to rely on the
> family of apply functions instead of the explicit loops.
>
> Thanks for your consideration of my question,
> Harold
>
> orm       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          3.0
> year           2006
> month          04
> day            24
> svn rev        37909
> language       R
> version.string Version 2.3.0 (2006-04-24)
>
>
>


From br44114 at gmail.com  Thu Jul 20 14:57:23 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Thu, 20 Jul 2006 08:57:23 -0400
Subject: [R] how to use large data set ?
Message-ID: <8d5a36350607200557v58593bd6i27f364c576b91a47@mail.gmail.com>

By far, the cheapest and easiest solution (and the very first to try)
is to add more memory. The cost depends on what kind you need, but
here's for example 2 GB you can buy for only $150:
http://www.newegg.com/Product/Product.asp?Item=N82E16820144157

Project constraints?! If they don't want to spend a couple hundred USD
for memory, you're working on the wrong project (and/or for the wrong
organization). Buying more memory (say up to a few GB) is orders of
magnitude cheaper than the licenses for some proprietary software that
can get around memory constraints, and probably (much) cheaper than
the loss of productivity caused by the extra training and setup time
needed to try to implement an alternative solution (such as a
connection to a DBMS). And even if the extra memory needed for R were
as expensive as the license for a proprietary software, which choice
would be more reasonable?


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of mahesh r
> Sent: Wednesday, July 19, 2006 4:23 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] how to use large data set ?
>
> Hi,
> I would like to extend to the query posted earlier on using large data
> bases. I am trying to use Rgdal to mine within the remote
> sensing imageries.
> I dont have problems bring the images within the R
> environment. But when I
> try to convert the images to a data.frame I receive an
> warning message from
> R saying "1: Reached total allocation of 510Mb: see
> help(memory.size)" and
> the process terminates. Due to project constarints I am given a very
> old 2.4Ghz computer with only 512 MB RAM. I think what R is currently
> doing is
> trying to store the results in the RAM and since the image
> size is very big
> (some 9 million pixels), I think it gets out of memory.
>
> My question is
> 1. Is there any possibility to dump the temporary variables
> in a temp folder
> within the hard disk (as many softwares do) instead of leting
> R store them
> in RAM
> 2. Could this be possible without creating a connection to a
> any back hand
> database like Oracle.
>
> Thanks,
>
> Mahesh
>
>
> On 7/19/06, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
> >
> > You did not say what analysis you want to do, but many
> common analyses
> > can be done as special cases of regression models and you
> can use the
> > biglm package to do regression models.
> >
> > Here is an example that worked for me to get the mean and standard
> > deviation by day from an oracle database with over 23
> million rows (I
> > had previously set up 'edw' as an odbc connection to the
> database under
> > widows, any of the database connections packages should work for you
> > though):
> >
> > library(RODBC)
> > library(biglm)
> >
> > con <- odbcConnect('edw',uid='glsnow',pwd=pass)
> >
> > odbcQuery(con, "select ADMSN_WEEKDAY_CD, LOS_DYS from
> CM.CASEMIX_SMRY")
> >
> > t1 <- Sys.time()
> >
> > tmp <- sqlGetResults(con, max=100000)
> >
> > names(tmp) <- c("Day","LoS")
> > tmp$Day <- factor(tmp$Day, levels=as.character(1:7))
> > tmp <- na.omit(tmp)
> > tmp <- subset(tmp, LoS > 0)
> >
> > ff <- log(LoS) ~ Day
> >
> > fit <- biglm(ff, tmp)
> >
> > i <- nrow(tmp)
> > while( !is.null(nrow( tmp <- sqlGetResults(con, max=100000) ) ) ){
> >         names(tmp) <- c("Day","LoS")
> >         tmp$Day <- factor(tmp$Day, levels=as.character(1:7))
> >         tmp <- na.omit(tmp)
> >         tmp <- subset(tmp, LoS > 0)
> >
> >         fit <- update(fit,tmp)
> >
> >         i <- i + nrow(tmp)
> >         cat(format(i,big.mark=',')," rows processed\n")
> > }
> >
> > summary(fit)
> >
> > t2 <- Sys.time()
> >
> > t2-t1
> >
> > Hope this helps,
> >
> > --
> > Gregory (Greg) L. Snow Ph.D.
> > Statistical Data Center
> > Intermountain Healthcare
> > greg.snow at intermountainmail.org
> > (801) 408-8111
> >
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> Yohan CHOUKROUN
> > Sent: Wednesday, July 19, 2006 9:42 AM
> > To: 'r-help at stat.math.ethz.ch'
> > Subject: [R] how to use large data set ?
> >
> > Hello R users,
> >
> >
> >
> > Sorry for my English, i'm French.
> >
> >
> >
> > I want to use a large dataset (3 millions of rows and 70 var) but I
> > don't know how to do because my computer crash quickly (P4
> 2.8Ghz, 1Go
> > ).
> >
> > I have also a bi Xeon with 2Go so I want to do computation on this
> > computer and show the results on mine. Both of them are on
> Windows XP...
> >
> >
> >
> > To do shortly I have:
> >
> >
> >
> > 1 server with a MySQL database
> >
> > 1computer
> >
> > and I want to use them with a large dataset.
> >
> >
> >
> > I'm trying to use RDCOM to connect the database and
> installing (but it's
> > hard for me..) Rpad.
> >
> >
> >
> > Is there another solutions ?
> >
> >
> >
> > Thanks in advance
> >
> >
> >
> >
> >
> > Yohan C.
> >
> >
> >
> >
> ----------------------------------------------------------------------
> > Ce message est confidentiel. Son contenu ne represente en
> aucun cas un
> > engagement de la part du Groupe Soft Computing sous reserve de tout
> > accord conclu par ecrit entre vous et le Groupe Soft
> Computing. Toute
> > publication, utilisation ou diffusion, meme partielle, doit etre
> > autorisee prealablement.
> > Si vous n'etes pas destinataire de ce message, merci d'en avertir
> > immediatement l'expediteur.
> > This message is confidential. Its content does not constitute a
> > commitment by Soft Computing Group except where provided for in a
> > written agreement between you and Soft Computing Group. Any
> unauthorised
> > disclosure, use or dissemination, either whole or partial, is
> > prohibited. If you are not the intended recipient of this message,
> > please notify the sender immediately.
> >
> ----------------------------------------------------------------------
> >
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Torsten.Hothorn at rzmail.uni-erlangen.de  Thu Jul 20 15:03:41 2006
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Thu, 20 Jul 2006 15:03:41 +0200 (CEST)
Subject: [R] Permutation Distribution
In-Reply-To: <44BF481B020000DE00008B93@rauzen.rau.ac.za>
References: <44BF481B020000DE00008B93@rauzen.rau.ac.za>
Message-ID: <Pine.LNX.4.64.0607201500380.21736@imbe153.imbe.med.uni-erlangen.de>


On Thu, 20 Jul 2006, Jacob van Wyk wrote:

> Hallo
>
> Is there an elegant way to do the following:
>
> Dataset consists of 2 variables: var1: some measurements, and var2: a grouping variable with two values, 1 and 2.
> There are (say) 10 measurements from group 1 and 15 measurements from group 2.
> The idea is to study the permutation distribution of
> mean(group 1) * mean(group2).
   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^

> One way would be to permute 1s and 2s and select the corresponding
> measurements; calculate the difference in means.
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

what test statistic do you want to use? Product or difference of means? 
The latter one is easy:

library("coin")
independence_test(var1 ~ var2, data = <your data>, ...)

Torsten


> Redo this 1000 times, say. Etc.
>
> Any help is much appreciated.
> Thanks
> Jacob
>
>
> Jacob L van Wyk
> Department of Statistics
> University of Johannesburg, APK
> P O Box 524
> Auckland Park 2006
> South Africa
> Tel: +27 11 489 3080
> Fax: +27 11 489 2832
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From f.harrell at vanderbilt.edu  Thu Jul 20 15:13:02 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 20 Jul 2006 08:13:02 -0500
Subject: [R] Timing benefits of mapply() vs. for loop was: Wrap a loop
 inside a function
In-Reply-To: <971536df0607200556y2ef617bev6de10fa6457b3514@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E132D9F@dc1ex01.air.org>
	<971536df0607200556y2ef617bev6de10fa6457b3514@mail.gmail.com>
Message-ID: <44BF815E.8050606@vanderbilt.edu>

Gabor Grothendieck wrote:
> Note that if you use mapply in the way I suggested, which is not
> the same as in your post, then its just as fast.  (Also the version
> of mapply in your post gives different numerical results than
> the for loop whereas mine gives the same.)   like.mat is the for
> loop version, like.mat2 is your mapply version and like.mat3
> is my mapply version.

You might also test mApply in Hmisc.

Cheers
Frank

> 
>> like.mat <- function(score, items, theta){
> +   like.mat <- matrix(numeric(length(items) * length(theta)), ncol =
> +     length(theta))
> +   for(i in 1:length(items)) like.mat[i, ] <- pcm(theta, items[[i]],
> score[[i]])
> +   like.mat
> + }
>> system.time(for(i in 1:100) like.mat(score,items,theta))
> [1] 1.30 0.00 1.34   NA   NA
>> like.mat2 <- function(score, items, theta)
> +   matrix(mapply(pcm, rep(theta,length(items)), items, score),
> +     ncol = length(theta), byrow = TRUE)
>> system.time(for(i in 1:100) like.mat2(score,items,theta))
> [1] 5.70 0.00 5.91   NA   NA
>> all.equal(like.mat(score, items, theta), like.mat2(score, items, theta))
> [1] "Mean relative  difference: 1.268095"
>> like.mat3 <- function(score, items, theta)
> + t(mapply(pcm, items, score, MoreArgs = list(theta = theta)))
>> system.time(for(i in 1:100) like.mat3(score,items,theta))
> [1] 1.32 0.01 1.39   NA   NA
>> m3 <- like.mat3(score, items, theta)
>> dimnames(m3) <- NULL
>> all.equal(like.mat(score, items, theta), m3)
> [1] TRUE
> 
> On 7/20/06, Doran, Harold <HDoran at air.org> wrote:
>>
>>
>> List:
>>
>> Thank you for the replies to my post yesterday. Gabor and Phil also gave
>> useful replies on how to improve the function by relying on mapply rather
>> than the explicit for loop. In general, I try and use the family of apply
>> functions rather than the looping constructs such as for, while etc as a
>> matter of practice.
>>
>> However, it seems the mapply function in this case is slower (in terms of
>> CPU speed) than the for loop. Here is an example.
>>
>> # data needed for example
>>
>> items <- list(item1 = c(0,1,2), item2 = c(0,1), item3 = c(0,1,2,3,4), item4
>> = c(0,1), item5=c(0,1,2,3,4),
>> item6=c(0,1,2,3))
>> score <- c(2,1,3,1,3,2)
>> theta <- c(-1,-.5,0,.5,1)
>>
>> # My old function using the for loop
>>
>> like.mat <- function(score, items, theta){
>>    like.mat <- matrix(numeric(length(items) * length(theta)), ncol =
>> length(theta))
>>    for(i in 1:length(items)) like.mat[i, ] <- pcm(theta, items[[i]],
>> score[[i]])
>>    like.mat
>>    }
>>
>> system.time(like.mat(score,items,theta))
>> [1]  0  0  0 NA NA
>>
>> # Revised using mapply
>>
>> like.mat <- function(score, items, theta){
>> matrix(mapply(pcm,rep(theta,length(items)),items,score),ncol=length(theta),byrow=TRUE)
>> }
>>
>>> system.time(like.mat(score,items,theta))
>> [1] 0.03 0.00 0.03   NA   NA
>>
>>
>> It is obviously slower to use mapply, but nominaly. So, let's actually look
>> at this within the context of the full program I am working on. For context,
>> I am evaluating an integral using Gaussian quadrature. This is a
>> psychometric problem where the function 'pcm" is Master's partial credit
>> model and 'score' is the student's score on test item i. When an item has
>> two categories (0,1), pcm reduces to the Rasch model for dichotomous data.
>> The dnorm is set at N(0,1) by default, but the parameters of the population
>> distribution are estimated from a separate procedure and are normally input
>> into the function, but this default works for the example.
>>
>> Here is the full program.
>>
>> library(statmod)
>>
>> # Master's partial credit model
>> pcm <- function(theta,d,score){
>>      exp(rowSums(outer(theta,d[1:score],'-')))/
>>      apply(exp(apply(outer(theta,d, '-'), 1, cumsum)), 2, sum)
>>    }
>>
>> like.mat <- function(score, items, theta){
>>    like.mat <- matrix(numeric(length(items) * length(theta)), ncol =
>> length(theta))
>>    for(i in 1:length(items)) like.mat[i, ] <- pcm(theta, items[[i]],
>> score[[i]])
>>    like.mat
>>    }
>>
>> # turn this off for now
>> #like.mat <- function(score, items, theta){
>> #matrix(mapply(pcm,rep(theta,length(items)),items,score),ncol=length(theta),byrow=TRUE)
>> #}
>>
>> class.numer <- function(score,items, prof_cut, mu=0, sigma=1, aboveQ){
>>    gauss_numer <- gauss.quad(49,kind="laguerre")
>>    if(aboveQ==FALSE){
>>       mat <- rbind(like.mat(score,items, (prof_cut-gauss_numer$nodes)),
>> dnorm(prof_cut-gauss_numer$nodes, mean=mu, sd=sigma))
>>
>>       } else { mat <- rbind(like.mat(score,items,
>> (gauss_numer$nodes+prof_cut)),
>> dnorm(gauss_numer$nodes+prof_cut, mean=mu,
>> sd=sigma))
>>
>>    }
>>    f_y <- rbind(apply(mat, 2, prod), exp(gauss_numer$nodes),
>> gauss_numer$weights)
>>    sum(apply(f_y,2,prod))
>>    }
>>
>> class.denom <- function(score,items, mu=0, sigma=1){
>>    gauss_denom <- gauss.quad.prob(49, dist='normal', mu=mu, sigma=sigma)
>>    mat <-
>> rbind(like.mat(score,items,gauss_denom$nodes),gauss_denom$weights)
>>    sum(apply(mat, 2, prod))
>>    }
>>
>> class.acc <-function(score,items,prof_cut, mu=0, sigma=1,
>> aboveQ=TRUE){
>>    result <- class.numer(score,items,prof_cut, mu,sigma,
>> aboveQ)/class.denom(score,items, mu, sigma)
>>    return(result)
>>    }
>>
>> # Test the function "class.acc"
>> items <- list(item1 = c(0,1,2), item2 = c(0,1), item3 = c(0,1,2,3,4), item4
>> = c(0,1), item5=c(0,1,2,3,4),
>> item6=c(0,1,2,3))
>> score <- c(2,1,3,1,3,2)
>>
>> # This is the system time when using the for loop for the like.mat function
>> system.time(class.acc(score,items,1,aboveQ=T))
>> [1] 0.04 0.00 0.04   NA   NA
>>
>> # This is the system time using the mapply for the like.mat function
>> system.time(class.acc(score,items,1,aboveQ=T))
>> [1] 0.70 0.00 0.73   NA   NA
>>
>>
>> There is a substantial improvement in CPU seconds when the for loop is
>> applied rather than using the mapply function. I experimented with adding
>> more items and varying the quadrature points and it always turned out the
>> for loop was faster.
>>
>> Given this result, I wonder what advice might be offered. Is there an
>> inherent reason one might opt for the mapply function (such as reliability,
>> etc) even when it compromises computational speed? Or, should the issue of
>> computational speed be considered ahead of common advice to rely on the
>> family of apply functions instead of the explicit loops.
>>
>> Thanks for your consideration of my question,
>> Harold


From bernat at creaf.uab.es  Thu Jul 20 15:15:12 2006
From: bernat at creaf.uab.es (Bernat Claramunt i Lopez)
Date: Thu, 20 Jul 2006 15:15:12 +0200
Subject: [R] Can make no plots !!!
Message-ID: <200607201515.12277.bernat@creaf.uab.es>

Dear all
I have Kubuntu linux and have updated to the latest version (6.06 dapper). I 
do not know why but now I can not make no plots. For instance, when I type

>hist(...)
 
this is the message I get:

>can't find X11 font
> Error in X11 (display, width, heigth......)
> unable to start devide X11
> In addition: Warning messages:
> 1:locale not supported by Xlib: some X ops will operate in C locale
>2: X cannot set locale modifiers

Any suggestion ? Maybe it is because kubuntu now works with Xorg instead of 
X11 ? I really ignore how to solve it and I need to make plots !!!

Thanks in advance	

-- 
Bernat Claramunt i Lopez
CREAF (Centre de Recerca Ecologica i Aplicacions Forestals ) i Departament de 
Biologia Vegetal, Biologia Animal i Ecologia
Universitat Autonoma de Barcelona
08193 Bellaterra, Catalunya

Telf: +34935811920
FAX:  +34935814151


From HDoran at air.org  Thu Jul 20 15:16:58 2006
From: HDoran at air.org (Doran, Harold)
Date: Thu, 20 Jul 2006 09:16:58 -0400
Subject: [R] Timing benefits of mapply() vs. for loop was: Wrap a loop
	inside a function
Message-ID: <2323A6D37908A847A7C32F1E3662C80E132DAE@dc1ex01.air.org>

Yes, that significantly improves the speed so that the for loop and the
mapply function both return the same CPU time.

Also, thank you for your diagnosis of the likelihood matrix.

Harold
 

> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
> Sent: Thursday, July 20, 2006 8:56 AM
> To: Doran, Harold
> Cc: r-help at stat.math.ethz.ch; Phil Spector
> Subject: Re: Timing benefits of mapply() vs. for loop was: 
> [R] Wrap a loop inside a function
> 
> Note that if you use mapply in the way I suggested, which is 
> not the same as in your post, then its just as fast.  (Also 
> the version of mapply in your post gives different numerical 
> results than
> the for loop whereas mine gives the same.)   like.mat is the for
> loop version, like.mat2 is your mapply version and like.mat3 
> is my mapply version.
> 
> > like.mat <- function(score, items, theta){
> +   like.mat <- matrix(numeric(length(items) * length(theta)), ncol =
> +     length(theta))
> +   for(i in 1:length(items)) like.mat[i, ] <- pcm(theta, items[[i]],
> score[[i]])
> +   like.mat
> + }
> > system.time(for(i in 1:100) like.mat(score,items,theta))
> [1] 1.30 0.00 1.34   NA   NA
> >
> > like.mat2 <- function(score, items, theta)
> +   matrix(mapply(pcm, rep(theta,length(items)), items, score),
> +     ncol = length(theta), byrow = TRUE)
> > system.time(for(i in 1:100) like.mat2(score,items,theta))
> [1] 5.70 0.00 5.91   NA   NA
> > all.equal(like.mat(score, items, theta), like.mat2(score, items, 
> > theta))
> [1] "Mean relative  difference: 1.268095"
> >
> > like.mat3 <- function(score, items, theta)
> + t(mapply(pcm, items, score, MoreArgs = list(theta = theta)))
> > system.time(for(i in 1:100) like.mat3(score,items,theta))
> [1] 1.32 0.01 1.39   NA   NA
> > m3 <- like.mat3(score, items, theta)
> > dimnames(m3) <- NULL
> > all.equal(like.mat(score, items, theta), m3)
> [1] TRUE
> 
> On 7/20/06, Doran, Harold <HDoran at air.org> wrote:
> >
> >
> >
> > List:
> >
> > Thank you for the replies to my post yesterday. Gabor and Phil also 
> > gave useful replies on how to improve the function by relying on 
> > mapply rather than the explicit for loop. In general, I try and use 
> > the family of apply functions rather than the looping 
> constructs such 
> > as for, while etc as a matter of practice.
> >
> > However, it seems the mapply function in this case is 
> slower (in terms 
> > of CPU speed) than the for loop. Here is an example.
> >
> > # data needed for example
> >
> > items <- list(item1 = c(0,1,2), item2 = c(0,1), item3 = 
> c(0,1,2,3,4), 
> > item4 = c(0,1), item5=c(0,1,2,3,4),
> > item6=c(0,1,2,3))
> > score <- c(2,1,3,1,3,2)
> > theta <- c(-1,-.5,0,.5,1)
> >
> > # My old function using the for loop
> >
> > like.mat <- function(score, items, theta){
> >    like.mat <- matrix(numeric(length(items) * length(theta)), ncol =
> > length(theta))
> >    for(i in 1:length(items)) like.mat[i, ] <- pcm(theta, items[[i]],
> > score[[i]])
> >    like.mat
> >    }
> >
> > system.time(like.mat(score,items,theta))
> > [1]  0  0  0 NA NA
> >
> > # Revised using mapply
> >
> > like.mat <- function(score, items, theta){
> > 
> matrix(mapply(pcm,rep(theta,length(items)),items,score),ncol=length(th
> > eta),byrow=TRUE)
> > }
> >
> > > system.time(like.mat(score,items,theta))
> > [1] 0.03 0.00 0.03   NA   NA
> >
> >
> > It is obviously slower to use mapply, but nominaly. So, 
> let's actually 
> > look at this within the context of the full program I am 
> working on. 
> > For context, I am evaluating an integral using Gaussian quadrature. 
> > This is a psychometric problem where the function 'pcm" is Master's 
> > partial credit model and 'score' is the student's score on 
> test item 
> > i. When an item has two categories (0,1), pcm reduces to 
> the Rasch model for dichotomous data.
> > The dnorm is set at N(0,1) by default, but the parameters of the 
> > population distribution are estimated from a separate procedure and 
> > are normally input into the function, but this default 
> works for the example.
> >
> > Here is the full program.
> >
> > library(statmod)
> >
> > # Master's partial credit model
> > pcm <- function(theta,d,score){
> >      exp(rowSums(outer(theta,d[1:score],'-')))/
> >      apply(exp(apply(outer(theta,d, '-'), 1, cumsum)), 2, sum)
> >    }
> >
> > like.mat <- function(score, items, theta){
> >    like.mat <- matrix(numeric(length(items) * length(theta)), ncol =
> > length(theta))
> >    for(i in 1:length(items)) like.mat[i, ] <- pcm(theta, items[[i]],
> > score[[i]])
> >    like.mat
> >    }
> >
> > # turn this off for now
> > #like.mat <- function(score, items, theta){
> > 
> #matrix(mapply(pcm,rep(theta,length(items)),items,score),ncol=length(t
> > heta),byrow=TRUE)
> > #}
> >
> > class.numer <- function(score,items, prof_cut, mu=0, 
> sigma=1, aboveQ){
> >    gauss_numer <- gauss.quad(49,kind="laguerre")
> >    if(aboveQ==FALSE){
> >       mat <- rbind(like.mat(score,items, 
> > (prof_cut-gauss_numer$nodes)), dnorm(prof_cut-gauss_numer$nodes, 
> > mean=mu, sd=sigma))
> >
> >       } else { mat <- rbind(like.mat(score,items, 
> > (gauss_numer$nodes+prof_cut)), dnorm(gauss_numer$nodes+prof_cut, 
> > mean=mu,
> > sd=sigma))
> >
> >    }
> >    f_y <- rbind(apply(mat, 2, prod), exp(gauss_numer$nodes),
> > gauss_numer$weights)
> >    sum(apply(f_y,2,prod))
> >    }
> >
> > class.denom <- function(score,items, mu=0, sigma=1){
> >    gauss_denom <- gauss.quad.prob(49, dist='normal', mu=mu, 
> sigma=sigma)
> >    mat <-
> > rbind(like.mat(score,items,gauss_denom$nodes),gauss_denom$weights)
> >    sum(apply(mat, 2, prod))
> >    }
> >
> > class.acc <-function(score,items,prof_cut, mu=0, sigma=1, 
> > aboveQ=TRUE){
> >    result <- class.numer(score,items,prof_cut, mu,sigma, 
> > aboveQ)/class.denom(score,items, mu, sigma)
> >    return(result)
> >    }
> >
> > # Test the function "class.acc"
> > items <- list(item1 = c(0,1,2), item2 = c(0,1), item3 = 
> c(0,1,2,3,4), 
> > item4 = c(0,1), item5=c(0,1,2,3,4),
> > item6=c(0,1,2,3))
> > score <- c(2,1,3,1,3,2)
> >
> > # This is the system time when using the for loop for the like.mat 
> > function
> > system.time(class.acc(score,items,1,aboveQ=T))
> > [1] 0.04 0.00 0.04   NA   NA
> >
> > # This is the system time using the mapply for the like.mat function
> > system.time(class.acc(score,items,1,aboveQ=T))
> > [1] 0.70 0.00 0.73   NA   NA
> >
> >
> > There is a substantial improvement in CPU seconds when the 
> for loop is 
> > applied rather than using the mapply function. I experimented with 
> > adding more items and varying the quadrature points and it always 
> > turned out the for loop was faster.
> >
> > Given this result, I wonder what advice might be offered. 
> Is there an 
> > inherent reason one might opt for the mapply function (such as 
> > reliability,
> > etc) even when it compromises computational speed? Or, should the 
> > issue of computational speed be considered ahead of common 
> advice to 
> > rely on the family of apply functions instead of the explicit loops.
> >
> > Thanks for your consideration of my question, Harold
> >
> > orm       i386-pc-mingw32
> > arch           i386
> > os             mingw32
> > system         i386, mingw32
> > status
> > major          2
> > minor          3.0
> > year           2006
> > month          04
> > day            24
> > svn rev        37909
> > language       R
> > version.string Version 2.3.0 (2006-04-24)
> >
> >
> >
>


From Achim.Zeileis at wu-wien.ac.at  Thu Jul 20 15:41:12 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 20 Jul 2006 15:41:12 +0200 (CEST)
Subject: [R] plain shading (not residuals) in mosaic plot
In-Reply-To: <20060720024525.26lzuuxu3kgw8088@mail.ucla.edu>
References: <20060719074732.syj6nbjo08g0w8cc@mail.ucla.edu>
	<971536df0607190836i7b7934b2sb73c304b929fcccf@mail.gmail.com>
	<c7c17cef0607191006v178908cha8646cc5abe794a4@mail.gmail.com>
	<20060720024525.26lzuuxu3kgw8088@mail.ucla.edu>
Message-ID: <Pine.LNX.4.58.0607201534560.6097@thorin.ci.tuwien.ac.at>

On Thu, 20 Jul 2006, Kie Zuraw wrote:

> Thank you very much, Gabor Grothendieck and Muhammad Subianto. Both of
> these work perfectly. I think I was misunderstanding gp and gpar()
> before. Again, thank you both.

Two further additions to this:

> > Maybe like this:
> > mosaic(allmorph, direction = "v", pop = FALSE,
> > gp=gpar(fill=c(grey(0.8),grey(0.4))))

This works because the fill pattern is expanded along the "dependent"
variable (last splitting variable) in the plot. In general you can supply
arbitrary patterns by supplying a fill pattern of the same dimension as
the original table, e.g.,

  mycol <- allmorph
  mycol[,1] <- grey.colors(2)[2]
  mycol[,2] <- grey.colors(2)[1]
  mycol[1,2] <- grey(0)
  mosaic(allmorph, split_vertical = TRUE, gp = gpar(fill = mycol))

Furthermore, several high-level functions producing similar pictures are
available, in particular

  doubledecker(allmorph)
  doubledecker(allmorph, col = grey.colors(2)[2:1])

or

  spine(allmorph)

Note that the latter is not based on strucplot() and hence can not be
labeled with the labeling functions for mosaic().

Best,
Z


From gpagnon at emory.edu  Thu Jul 20 16:02:10 2006
From: gpagnon at emory.edu (Giuseppe Pagnoni)
Date: Thu, 20 Jul 2006 10:02:10 -0400
Subject: [R] (robust) mixed-effects model with covariate
Message-ID: <60DC693F-2010-42A0-94B2-22AA729C8FD9@emory.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060720/c1b3867f/attachment.pl 

From mathematician4 at hotmail.com  Thu Jul 20 16:38:13 2006
From: mathematician4 at hotmail.com (Emanuele Mazzola)
Date: Thu, 20 Jul 2006 14:38:13 +0000
Subject: [R] Correspondence analysis with R
Message-ID: <BAY107-F8CD5BE4688EFCE7F8213C9A610@phx.gbl>

Hello everybody,

i'm having some trouble performing correspondence analysis with R for Mac OS 
X. Does anyone know about some useful package?
And also, if i had found coordinates of points representing row and column 
profiles, how do i put them in a single figure with labels identifying each 
one of them?
This thing is getting me almost crazy...

Thank you in advance for answering,
bye
Emanuele


From spencer.graves at pdf.com  Thu Jul 20 16:43:14 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 20 Jul 2006 07:43:14 -0700
Subject: [R] showMethods()! Re:  How to find S4 generics?
In-Reply-To: <17599.14058.449041.415808@stat.math.ethz.ch>
References: <878be2c60607150822r5914a231h41d537f2f5b446b4@mail.gmail.com>	<44BDA929.8000407@pdf.com>	<Pine.LNX.4.64.0607190739540.32733@gannet.stats.ox.ac.uk>	<44BE7B53.4070104@pdf.com>	<Pine.LNX.4.64.0607191449030.18735@homer21.u.washington.edu>	<44BEB0FE.90707@pdf.com>
	<17599.14058.449041.415808@stat.math.ethz.ch>
Message-ID: <44BF9682.6060503@pdf.com>

Hi, Martin:

	  I looked for 'showMethods' before I posted this question.  I see two 
possible reasons others might not have mentioned it previously -- and 
why my post didn't mention it:

HARD TO FIND

	  Now that you have mentioned it, I recall having stumbled across it 
and used it once before.  It would help mightily if other relevant help 
pages included 'showMethods' under 'See Also', including 'methods'. 
Looking just now, I found it there under 'GenericFunctions'.  However, 
before I raised this question to the list, I spent maybe 20-30 minutes 
looking for it in different help pages, using RSiteSearch, etc., without 
success.

NOT AS USEFUL AS THE NAME IMPLIES

	  Just now I tried '> showMethods(classes="lm")' and (with 'lme4' in 
the path' showMethods(classes="lmer").  With the results below.  I 
didn't find 'coef' or 'predict' or ... .

	  Thanks for the suggestion.
	  Spencer Graves
###################
 > sessionInfo()
Version 2.3.1 (2006-06-01)
i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
       lme4     Matrix    lattice  robustLog
  "0.995-2" "0.995-11"   "0.13-9"    "0.0-2"
 > showMethods(classes="lm")

Function "addNextMethod":
<Empty Methods List>

Function "Arith":
<Empty Methods List>

Function "body<-":
<Empty Methods List>

Function "cbind2":
<Empty Methods List>

Function "coerce":
<Empty Methods List>

Function "Compare":
<Empty Methods List>

Function "initialize":
<Empty Methods List>

Function "loadMethod":
<Empty Methods List>

Function "Math":
<Empty Methods List>

Function "Math2":
<Empty Methods List>

Function "rbind2":
<Empty Methods List>

Function "show":
<Empty Methods List>

Function "Summary":
<Empty Methods List>
NULL
 > showMethods(classes="lmer")

Function "addNextMethod":
<Empty Methods List>

Function "Arith":
<Empty Methods List>

Function "body<-":
<Empty Methods List>

Function "cbind2":
<Empty Methods List>

Function "coerce":
<Empty Methods List>

Function "Compare":
<Empty Methods List>

Function "initialize":
<Empty Methods List>

Function "loadMethod":
<Empty Methods List>

Function "Math":
<Empty Methods List>

Function "Math2":
<Empty Methods List>

Function "rbind2":
<Empty Methods List>

Function "show":
<Empty Methods List>

Function "Summary":
<Empty Methods List>
NULL

Martin Maechler wrote:
>>>>>> "SpG" == Spencer Graves <spencer.graves at pdf.com>
>>>>>>     on Wed, 19 Jul 2006 15:23:58 -0700 writes:
> 
>     SpG> Hi, Thomas: Thanks very much.  I haven't tried it yet,
>     SpG> but it looks very useful.  Best Wishes, Spencer Graves
> 
> Hmm,  ?methods  has been containing for a while
> 
> methods> Note:
> methods> 
> methods>    This scheme is called _S3_ (S version 3).  For new projects, it is
> methods>    recommended to use the more flexible and robust _S4_ scheme
> methods>    provided in the 'methods' package.  Functions can have both S3 and
> methods>    S4 methods, and function 'showMethods' will list the S4 methods
> methods>    (possibly none).
> 
> So I wonder why nowbody mentioned the official
> 
>   showMethods()
> 
>   (or are you all not reading the help pages  ;-\) )
> 
> It does  not always return / print what I want exactly, and in
> the past, at one point I put some effort to make its output more
> customizable.  
> I didn't put that effort to the end (*)
> but for the present case
> 
>     showMethods(class = "ddiMatrix", where = "package:Matrix")
> 
> at least is pretty useful.
> 
> (*)  I think I had added the  'showEmpty = TRUE'  argument
>      where the current documentation says
> 
>  showM>   showEmpty: logical indicating if methods with empty
>  showM>              method lists should be shown at all.  
>  showM>     Note that 'FALSE' is _not yet implemented_.
> 
> and the non-implementation had a reason: implementation was
> definitely much less simple than I had hoped ...
> 
> Martin
> 
> 
>     SpG> Thomas Lumley wrote:
>     >> On Wed, 19 Jul 2006, Spencer Graves wrote:
>     >>> Am I correct then that the 'methods' function could, at
>     >>> least theoretically, be revised so methods(class=...)
>     >>> could identify both S3 and S4 methods (ignoring
>     >>> inheritance, as it does now, I believe)?
>     >>> 
>     >>  Here is a function to find methods for a formal
>     >> class. It returns a list with elements corresponding to a
>     >> generic, and each element is a list of strings showing
>     >> all the signatures that contain any of the specified
>     >> classes.
>     >> 
>     >> If super=TRUE it looks at all superclasses, if ANY=TRUE
>     >> it also returns methods for ANY class.
>     >> 
>     >> If you have lme4 loaded, try methods4("lmer"
>     >> methods4("ddiMatrix") methods4("ddiMatrix",super=TRUE)
>     >> 
>     >> -thomas
>     >> 
>     >> methods4<-function(classes, super=FALSE, ANY=FALSE){ if
>     >> (super) classes<-unlist(sapply(classes, function(cl)
>     >> getAllSuperClasses(getClass(cl)))) if (ANY)
>     >> classes<-c(classes,"ANY") gens<-allGenerics()@.Data
>     >> sigs<-lapply(gens, function(g)
>     >> linearizeMlist(getMethods(g))@classes)
>     >> names(sigs)<-gens at .Data sigs<-lapply(sigs, function(gen){
>     >> gen[unlist(sapply(gen, function(sig) any(sig %in%
>     >> classes)))]}) sigs[sapply(sigs,length)>0] }
>     >> 
>     >> 
>     >> Thomas Lumley Assoc. Professor, Biostatistics
>     >> tlumley at u.washington.edu University of Washington,
>     >> Seattle
>     >> 
>     >> ______________________________________________
>     >> R-help at stat.math.ethz.ch mailing list
>     >> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
>     >> read the posting guide
>     >> http://www.R-project.org/posting-guide.html and provide
>     >> commented, minimal, self-contained, reproducible code.
> 
>     SpG> ______________________________________________
>     SpG> R-help at stat.math.ethz.ch mailing list
>     SpG> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
>     SpG> read the posting guide
>     SpG> http://www.R-project.org/posting-guide.html and provide
>     SpG> commented, minimal, self-contained, reproducible code.


From bennfine at yahoo.com  Thu Jul 20 16:47:40 2006
From: bennfine at yahoo.com (Benn Fine)
Date: Thu, 20 Jul 2006 07:47:40 -0700 (PDT)
Subject: [R] Help on making code faster-would C help ?
Message-ID: <20060720144740.55009.qmail@web61312.mail.yahoo.com>

Using R on windows....

I have the following code 

for(i in 1:10000) {

draw some random weights

perfom a weighted least squares regression

some simple addition and multiplication
}

The code works fine but is slow. 

I have mingw installed and can dyn.load, although I am
more used to doing this on Unix than Windows.

Would it make sense to re-write the whole thing in
C ? I am vectorizing the random draws-the speed
culprit is looping and the matrix manipulations.

My guess is to use the matrix routines from gls-does
this sound feasible ?

Thanks!


From ripley at stats.ox.ac.uk  Thu Jul 20 17:05:43 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 20 Jul 2006 16:05:43 +0100 (BST)
Subject: [R] Help on making code faster-would C help ?
In-Reply-To: <20060720144740.55009.qmail@web61312.mail.yahoo.com>
References: <20060720144740.55009.qmail@web61312.mail.yahoo.com>
Message-ID: <Pine.LNX.4.64.0607201602510.31494@gannet.stats.ox.ac.uk>

Without details it is hard to say, but if you do the WLS via lm.wfit, 
probably not.  If you are using lm() and taking the overhead 10000 times, 
that is the first thing to avoid.

As an example of this sort of thing, look at the C code for lqs{MASS} 
which does thousands of regressions quite quickly.

On Thu, 20 Jul 2006, Benn Fine wrote:

> Using R on windows....
> 
> I have the following code 
> 
> for(i in 1:10000) {
> 
> draw some random weights
> 
> perfom a weighted least squares regression
> 
> some simple addition and multiplication
> }
> 
> The code works fine but is slow. 
> 
> I have mingw installed and can dyn.load, although I am
> more used to doing this on Unix than Windows.
> 
> Would it make sense to re-write the whole thing in
> C ? I am vectorizing the random draws-the speed
> culprit is looping and the matrix manipulations.
> 
> My guess is to use the matrix routines from gls-does
> this sound feasible ?
> 
> Thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bates at stat.wisc.edu  Thu Jul 20 17:21:58 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 20 Jul 2006 10:21:58 -0500
Subject: [R] showMethods()! Re: How to find S4 generics?
In-Reply-To: <44BF9682.6060503@pdf.com>
References: <878be2c60607150822r5914a231h41d537f2f5b446b4@mail.gmail.com>
	<44BDA929.8000407@pdf.com>
	<Pine.LNX.4.64.0607190739540.32733@gannet.stats.ox.ac.uk>
	<44BE7B53.4070104@pdf.com>
	<Pine.LNX.4.64.0607191449030.18735@homer21.u.washington.edu>
	<44BEB0FE.90707@pdf.com> <17599.14058.449041.415808@stat.math.ethz.ch>
	<44BF9682.6060503@pdf.com>
Message-ID: <40e66e0b0607200821l214e2d37kc74c628a2b17307e@mail.gmail.com>

Notice that in Martin's example he used

showMethods(class = "lmer", where = "package:Matrix")

The result is still not optimal but it is slightly more useful than
trying the default location.   The approach that Thomas contributed
earlier in this discussion is probably the better route.

On 7/20/06, Spencer Graves <spencer.graves at pdf.com> wrote:
> Hi, Martin:
>
>           I looked for 'showMethods' before I posted this question.  I see two
> possible reasons others might not have mentioned it previously -- and
> why my post didn't mention it:
>
> HARD TO FIND
>
>           Now that you have mentioned it, I recall having stumbled across it
> and used it once before.  It would help mightily if other relevant help
> pages included 'showMethods' under 'See Also', including 'methods'.
> Looking just now, I found it there under 'GenericFunctions'.  However,
> before I raised this question to the list, I spent maybe 20-30 minutes
> looking for it in different help pages, using RSiteSearch, etc., without
> success.
>
> NOT AS USEFUL AS THE NAME IMPLIES
>
>           Just now I tried '> showMethods(classes="lm")' and (with 'lme4' in
> the path' showMethods(classes="lmer").  With the results below.  I
> didn't find 'coef' or 'predict' or ... .
>
>           Thanks for the suggestion.
>           Spencer Graves
> ###################
>  > sessionInfo()
> Version 2.3.1 (2006-06-01)
> i386-pc-mingw32
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
>
> other attached packages:
>        lme4     Matrix    lattice  robustLog
>   "0.995-2" "0.995-11"   "0.13-9"    "0.0-2"
>  > showMethods(classes="lm")
>
> Function "addNextMethod":
> <Empty Methods List>
>
> Function "Arith":
> <Empty Methods List>
>
> Function "body<-":
> <Empty Methods List>
>
> Function "cbind2":
> <Empty Methods List>
>
> Function "coerce":
> <Empty Methods List>
>
> Function "Compare":
> <Empty Methods List>
>
> Function "initialize":
> <Empty Methods List>
>
> Function "loadMethod":
> <Empty Methods List>
>
> Function "Math":
> <Empty Methods List>
>
> Function "Math2":
> <Empty Methods List>
>
> Function "rbind2":
> <Empty Methods List>
>
> Function "show":
> <Empty Methods List>
>
> Function "Summary":
> <Empty Methods List>
> NULL
>  > showMethods(classes="lmer")
>
> Function "addNextMethod":
> <Empty Methods List>
>
> Function "Arith":
> <Empty Methods List>
>
> Function "body<-":
> <Empty Methods List>
>
> Function "cbind2":
> <Empty Methods List>
>
> Function "coerce":
> <Empty Methods List>
>
> Function "Compare":
> <Empty Methods List>
>
> Function "initialize":
> <Empty Methods List>
>
> Function "loadMethod":
> <Empty Methods List>
>
> Function "Math":
> <Empty Methods List>
>
> Function "Math2":
> <Empty Methods List>
>
> Function "rbind2":
> <Empty Methods List>
>
> Function "show":
> <Empty Methods List>
>
> Function "Summary":
> <Empty Methods List>
> NULL
>
> Martin Maechler wrote:
> >>>>>> "SpG" == Spencer Graves <spencer.graves at pdf.com>
> >>>>>>     on Wed, 19 Jul 2006 15:23:58 -0700 writes:
> >
> >     SpG> Hi, Thomas: Thanks very much.  I haven't tried it yet,
> >     SpG> but it looks very useful.  Best Wishes, Spencer Graves
> >
> > Hmm,  ?methods  has been containing for a while
> >
> > methods> Note:
> > methods>
> > methods>    This scheme is called _S3_ (S version 3).  For new projects, it is
> > methods>    recommended to use the more flexible and robust _S4_ scheme
> > methods>    provided in the 'methods' package.  Functions can have both S3 and
> > methods>    S4 methods, and function 'showMethods' will list the S4 methods
> > methods>    (possibly none).
> >
> > So I wonder why nowbody mentioned the official
> >
> >   showMethods()
> >
> >   (or are you all not reading the help pages  ;-\) )
> >
> > It does  not always return / print what I want exactly, and in
> > the past, at one point I put some effort to make its output more
> > customizable.
> > I didn't put that effort to the end (*)
> > but for the present case
> >
> >     showMethods(class = "ddiMatrix", where = "package:Matrix")
> >
> > at least is pretty useful.
> >
> > (*)  I think I had added the  'showEmpty = TRUE'  argument
> >      where the current documentation says
> >
> >  showM>   showEmpty: logical indicating if methods with empty
> >  showM>              method lists should be shown at all.
> >  showM>     Note that 'FALSE' is _not yet implemented_.
> >
> > and the non-implementation had a reason: implementation was
> > definitely much less simple than I had hoped ...
> >
> > Martin
> >
> >
> >     SpG> Thomas Lumley wrote:
> >     >> On Wed, 19 Jul 2006, Spencer Graves wrote:
> >     >>> Am I correct then that the 'methods' function could, at
> >     >>> least theoretically, be revised so methods(class=...)
> >     >>> could identify both S3 and S4 methods (ignoring
> >     >>> inheritance, as it does now, I believe)?
> >     >>>
> >     >>  Here is a function to find methods for a formal
> >     >> class. It returns a list with elements corresponding to a
> >     >> generic, and each element is a list of strings showing
> >     >> all the signatures that contain any of the specified
> >     >> classes.
> >     >>
> >     >> If super=TRUE it looks at all superclasses, if ANY=TRUE
> >     >> it also returns methods for ANY class.
> >     >>
> >     >> If you have lme4 loaded, try methods4("lmer"
> >     >> methods4("ddiMatrix") methods4("ddiMatrix",super=TRUE)
> >     >>
> >     >> -thomas
> >     >>
> >     >> methods4<-function(classes, super=FALSE, ANY=FALSE){ if
> >     >> (super) classes<-unlist(sapply(classes, function(cl)
> >     >> getAllSuperClasses(getClass(cl)))) if (ANY)
> >     >> classes<-c(classes,"ANY") gens<-allGenerics()@.Data
> >     >> sigs<-lapply(gens, function(g)
> >     >> linearizeMlist(getMethods(g))@classes)
> >     >> names(sigs)<-gens at .Data sigs<-lapply(sigs, function(gen){
> >     >> gen[unlist(sapply(gen, function(sig) any(sig %in%
> >     >> classes)))]}) sigs[sapply(sigs,length)>0] }
> >     >>
> >     >>
> >     >> Thomas Lumley Assoc. Professor, Biostatistics
> >     >> tlumley at u.washington.edu University of Washington,
> >     >> Seattle
> >     >>
> >     >> ______________________________________________
> >     >> R-help at stat.math.ethz.ch mailing list
> >     >> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
> >     >> read the posting guide
> >     >> http://www.R-project.org/posting-guide.html and provide
> >     >> commented, minimal, self-contained, reproducible code.
> >
> >     SpG> ______________________________________________
> >     SpG> R-help at stat.math.ethz.ch mailing list
> >     SpG> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
> >     SpG> read the posting guide
> >     SpG> http://www.R-project.org/posting-guide.html and provide
> >     SpG> commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch at stats.uwo.ca  Thu Jul 20 17:24:16 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 20 Jul 2006 11:24:16 -0400
Subject: [R] Correspondence analysis with R
In-Reply-To: <BAY107-F8CD5BE4688EFCE7F8213C9A610@phx.gbl>
References: <BAY107-F8CD5BE4688EFCE7F8213C9A610@phx.gbl>
Message-ID: <44BFA020.8040401@stats.uwo.ca>

On 7/20/2006 10:38 AM, Emanuele Mazzola wrote:
> Hello everybody,
> 
> i'm having some trouble performing correspondence analysis with R for Mac OS 
> X. Does anyone know about some useful package?
> And also, if i had found coordinates of points representing row and column 
> profiles, how do i put them in a single figure with labels identifying each 
> one of them?
> This thing is getting me almost crazy...

See the corresp() function in MASS and the cca() function in vegan. 
Both of these can produce biplots.

Duncan Murdoch


From bady at univ-lyon1.fr  Thu Jul 20 17:29:37 2006
From: bady at univ-lyon1.fr (bady at univ-lyon1.fr)
Date: Thu, 20 Jul 2006 17:29:37 +0200
Subject: [R] Correspondence analysis with R
In-Reply-To: <BAY107-F8CD5BE4688EFCE7F8213C9A610@phx.gbl>
References: <BAY107-F8CD5BE4688EFCE7F8213C9A610@phx.gbl>
Message-ID: <1153409377.44bfa161d9def@webmail.univ-lyon1.fr>

Hi, Hi all,

> i'm having some trouble performing correspondence analysis with R for Mac OS
> X. Does anyone know about some useful package?
> And also, if i had found coordinates of points representing row and column
> profiles, how do i put them in a single figure with labels identifying each
> one of them?
> This thing is getting me almost crazy.

you can see the package vegan and/or ade4:

library(vegan)
?cca
?plot.cca # argument: type

library(ade4)
?dudi.coa
?scatter.coa
?s.label # arguments: add.plot and label

hope this help :)

Pierre


From tlumley at u.washington.edu  Thu Jul 20 17:31:22 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 20 Jul 2006 08:31:22 -0700 (PDT)
Subject: [R] showMethods()! Re: How to find S4 generics?
In-Reply-To: <40e66e0b0607200821l214e2d37kc74c628a2b17307e@mail.gmail.com>
References: <878be2c60607150822r5914a231h41d537f2f5b446b4@mail.gmail.com> 
	<44BDA929.8000407@pdf.com>
	<Pine.LNX.4.64.0607190739540.32733@gannet.stats.ox.ac.uk>
	<44BE7B53.4070104@pdf.com>
	<Pine.LNX.4.64.0607191449030.18735@homer21.u.washington.edu>
	<44BEB0FE.90707@pdf.com> <17599.14058.449041.415808@stat.math.ethz.ch> 
	<44BF9682.6060503@pdf.com>
	<40e66e0b0607200821l214e2d37kc74c628a2b17307e@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0607200828590.5650@homer24.u.washington.edu>

On Thu, 20 Jul 2006, Douglas Bates wrote:

> Notice that in Martin's example he used
>
> showMethods(class = "lmer", where = "package:Matrix")
>
> The result is still not optimal but it is slightly more useful than
> trying the default location.   The approach that Thomas contributed
> earlier in this discussion is probably the better route.
>

The main remaining problem in my code is that with super=TRUE it will 
report all inherited methods, even where more specific methods override 
them.  That should be fixable.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From ryszard.czerminski at novartis.com  Thu Jul 20 17:29:58 2006
From: ryszard.czerminski at novartis.com (ryszard.czerminski at novartis.com)
Date: Thu, 20 Jul 2006 11:29:58 -0400
Subject: [R] how to print table with more columns per row?
Message-ID: <OF581DF905.9F719FAD-ON852571B1.00538044-852571B1.00552460@EU.novartis.net>

When printing a table it is broken at some point (depending how long are 
the associated names)
>>> see example below.

Is there a way to control number of columns being printed for a given 
chunk of the table?

Best regards,
Ryszard

> z5
        AAAAAAA BBBBBBB CCCCCCC DDDDDDD EEEEEEE FFFFFFF GGGGGGG HHHHHHH 
IIIIIII
AAAAAAA    1.00   -0.69   -0.54   -0.88      NA      NA      NA      NA 
-0.88
BBBBBBB   -0.69    1.00    0.65    0.82      NA      NA      NA       1 
0.83
CCCCCCC   -0.54    0.65    1.00    0.49      NA      NA      NA      NA 
0.94
DDDDDDD   -0.88    0.82    0.49    1.00      NA      NA      NA       1 
0.90
EEEEEEE      NA      NA      NA      NA      NA      NA      NA      NA  
NA
FFFFFFF      NA      NA      NA      NA      NA      NA      NA      NA  
NA
GGGGGGG      NA      NA      NA      NA      NA      NA      NA      NA  
NA
HHHHHHH      NA    1.00      NA    1.00      NA      NA      NA       1  
NA
IIIIIII   -0.88    0.83    0.94    0.90      NA      NA      NA      NA 
1.00
JJJJJJJ      NA      NA      NA      NA      NA      NA      NA      NA  
NA
KKKKKKK    0.05    0.21    0.11   -0.11      NA      NA      NA       1  
NA
LLLLLLL    0.73   -0.68   -0.16   -0.91      NA      NA      NA      -1 
-0.35
        JJJJJJJ KKKKKKK LLLLLLL
AAAAAAA      NA    0.05    0.73
BBBBBBB      NA    0.21   -0.68
CCCCCCC      NA    0.11   -0.16
DDDDDDD      NA   -0.11   -0.91
EEEEEEE      NA      NA      NA
FFFFFFF      NA      NA      NA
GGGGGGG      NA      NA      NA
HHHHHHH      NA    1.00   -1.00
IIIIIII      NA      NA   -0.35
JJJJJJJ      NA      NA      NA
KKKKKKK      NA    1.00    0.24
LLLLLLL      NA    0.24    1.00
>

CONFIDENTIALITY NOTICE\ \ The information contained in this ...{{dropped}}


From ripley at stats.ox.ac.uk  Thu Jul 20 17:51:56 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 20 Jul 2006 16:51:56 +0100 (BST)
Subject: [R] how to print table with more columns per row?
In-Reply-To: <OF581DF905.9F719FAD-ON852571B1.00538044-852571B1.00552460@EU.novartis.net>
References: <OF581DF905.9F719FAD-ON852571B1.00538044-852571B1.00552460@EU.novartis.net>
Message-ID: <Pine.LNX.4.64.0607201651150.6037@gannet.stats.ox.ac.uk>

This is controlled by options(width): characters not columns.

On Thu, 20 Jul 2006, ryszard.czerminski at novartis.com wrote:

> When printing a table it is broken at some point (depending how long are 
> the associated names)
> >>> see example below.
> 
> Is there a way to control number of columns being printed for a given 
> chunk of the table?
> 
> Best regards,
> Ryszard
> 
> > z5
>         AAAAAAA BBBBBBB CCCCCCC DDDDDDD EEEEEEE FFFFFFF GGGGGGG HHHHHHH 
> IIIIIII
> AAAAAAA    1.00   -0.69   -0.54   -0.88      NA      NA      NA      NA 
> -0.88
> BBBBBBB   -0.69    1.00    0.65    0.82      NA      NA      NA       1 
> 0.83
> CCCCCCC   -0.54    0.65    1.00    0.49      NA      NA      NA      NA 
> 0.94
> DDDDDDD   -0.88    0.82    0.49    1.00      NA      NA      NA       1 
> 0.90
> EEEEEEE      NA      NA      NA      NA      NA      NA      NA      NA  
> NA
> FFFFFFF      NA      NA      NA      NA      NA      NA      NA      NA  
> NA
> GGGGGGG      NA      NA      NA      NA      NA      NA      NA      NA  
> NA
> HHHHHHH      NA    1.00      NA    1.00      NA      NA      NA       1  
> NA
> IIIIIII   -0.88    0.83    0.94    0.90      NA      NA      NA      NA 
> 1.00
> JJJJJJJ      NA      NA      NA      NA      NA      NA      NA      NA  
> NA
> KKKKKKK    0.05    0.21    0.11   -0.11      NA      NA      NA       1  
> NA
> LLLLLLL    0.73   -0.68   -0.16   -0.91      NA      NA      NA      -1 
> -0.35
>         JJJJJJJ KKKKKKK LLLLLLL
> AAAAAAA      NA    0.05    0.73
> BBBBBBB      NA    0.21   -0.68
> CCCCCCC      NA    0.11   -0.16
> DDDDDDD      NA   -0.11   -0.91
> EEEEEEE      NA      NA      NA
> FFFFFFF      NA      NA      NA
> GGGGGGG      NA      NA      NA
> HHHHHHH      NA    1.00   -1.00
> IIIIIII      NA      NA   -0.35
> JJJJJJJ      NA      NA      NA
> KKKKKKK      NA    1.00    0.24
> LLLLLLL      NA    0.24    1.00
> >
> 
> CONFIDENTIALITY NOTICE\ \ The information contained in this ...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From leaflovesun at yahoo.ca  Thu Jul 20 17:57:08 2006
From: leaflovesun at yahoo.ca (Leaf Sun)
Date: Thu, 20 Jul 2006 09:57:08 -0600
Subject: [R] Weibull distribution
References: <1bff52c20606140033j3ac8e9fdxa85d4edded2ad94a@mail.gmail.com>
	<1bff52c20606140101r25188de1g6573c52f3cb5d152@mail.gmail.com>
	<38b9f0350606140223g507112dau3478586790daad3d@mail.gmail.com>
	<200606140735459773486@yahoo.ca>
	<449285BD.3020101@statistik.uni-dortmund.de>
	<200606161041072848729@yahoo.ca>
	<40e66e0b0606180515t759f1400od071251942d8dac6@mail.gmail.com>
	<200606191251227312041@yahoo.ca> <200607171218066725256@yahoo.ca>
	<878440A9-8768-4874-B46A-60B6F1D75F38@austin.rr.com>
Message-ID: <200607200957069985478@yahoo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060720/d94e817b/attachment.pl 

From wiedenhoeft at gmx.net  Thu Jul 20 18:15:01 2006
From: wiedenhoeft at gmx.net (John Wiedenhoeft)
Date: Thu, 20 Jul 2006 18:15:01 +0200
Subject: [R] throwaway() function
Message-ID: <1153412101.5989.11.camel@localhost>

Dear all,

I apologize if this is a FAQ (seems a bit like one, but I didn't find
anything).

I'm looking for an easy way to cut one value out of a vector and shorten
the vector accordingly. Something like:

x <- c(1, 1, 0, 6, 2)
throwaway(x[3])

which will return x = 1 1 6 2, with length(x) = 4. I know one could do
this by hand, but then one would have to create a second vector y in a
loop which has to be 1 shorter then x and then assign x <- y, as there
is no "anti-c()" function which shortens a vector (at least to my
knowledge).

Is there some kind of a throwaway() function in R?

Best regards,
John


From ggrothendieck at gmail.com  Thu Jul 20 18:20:10 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 20 Jul 2006 12:20:10 -0400
Subject: [R] how to print table with more columns per row?
In-Reply-To: <OF581DF905.9F719FAD-ON852571B1.00538044-852571B1.00552460@EU.novartis.net>
References: <OF581DF905.9F719FAD-ON852571B1.00538044-852571B1.00552460@EU.novartis.net>
Message-ID: <971536df0607200920i1e3c55fara132acd840accb2d@mail.gmail.com>

In some cases it may be sufficient to abbreviate the colnames:

> library(MASS); data(survey)
> head(survey)
     Sex Wr.Hnd NW.Hnd W.Hnd    Fold Pulse    Clap Exer Smoke Height      M.I
1 Female   18.5   18.0 Right  R on L    92    Left Some Never 173.00   Metric
2   Male   19.5   20.5  Left  R on L   104    Left None Regul 177.80 Imperial
3   Male   18.0   13.3 Right  L on R    87 Neither None Occas     NA     <NA>
4   Male   18.8   18.9 Right  R on L    NA Neither None Never 160.00   Metric
5   Male   20.0   20.0 Right Neither    35   Right Some Never 165.00   Metric
6 Female   18.0   17.7 Right  L on R    64   Right Some Never 172.72 Imperial
     Age
1 18.250
2 17.583
3 16.917
4 20.333
5 23.667
6 21.000
> local({ colnames(survey) <- abbreviate(colnames(survey), 3); head(survey)})
     Sex Wr.H  NW.  W.Hn     Fld Pls     Clp  Exr   Smk    Hgh      M.I    Age
1 Female 18.5 18.0 Right  R on L  92    Left Some Never 173.00   Metric 18.250
2   Male 19.5 20.5  Left  R on L 104    Left None Regul 177.80 Imperial 17.583
3   Male 18.0 13.3 Right  L on R  87 Neither None Occas     NA     <NA> 16.917
4   Male 18.8 18.9 Right  R on L  NA Neither None Never 160.00   Metric 20.333
5   Male 20.0 20.0 Right Neither  35   Right Some Never 165.00   Metric 23.667
6 Female 18.0 17.7 Right  L on R  64   Right Some Never 172.72 Imperial 21.000

On 7/20/06, ryszard.czerminski at novartis.com
<ryszard.czerminski at novartis.com> wrote:
> When printing a table it is broken at some point (depending how long are
> the associated names)
> >>> see example below.
>
> Is there a way to control number of columns being printed for a given
> chunk of the table?
>
> Best regards,
> Ryszard
>
> > z5
>        AAAAAAA BBBBBBB CCCCCCC DDDDDDD EEEEEEE FFFFFFF GGGGGGG HHHHHHH
> IIIIIII
> AAAAAAA    1.00   -0.69   -0.54   -0.88      NA      NA      NA      NA
> -0.88
> BBBBBBB   -0.69    1.00    0.65    0.82      NA      NA      NA       1
> 0.83
> CCCCCCC   -0.54    0.65    1.00    0.49      NA      NA      NA      NA
> 0.94
> DDDDDDD   -0.88    0.82    0.49    1.00      NA      NA      NA       1
> 0.90
> EEEEEEE      NA      NA      NA      NA      NA      NA      NA      NA
> NA
> FFFFFFF      NA      NA      NA      NA      NA      NA      NA      NA
> NA
> GGGGGGG      NA      NA      NA      NA      NA      NA      NA      NA
> NA
> HHHHHHH      NA    1.00      NA    1.00      NA      NA      NA       1
> NA
> IIIIIII   -0.88    0.83    0.94    0.90      NA      NA      NA      NA
> 1.00
> JJJJJJJ      NA      NA      NA      NA      NA      NA      NA      NA
> NA
> KKKKKKK    0.05    0.21    0.11   -0.11      NA      NA      NA       1
> NA
> LLLLLLL    0.73   -0.68   -0.16   -0.91      NA      NA      NA      -1
> -0.35
>        JJJJJJJ KKKKKKK LLLLLLL
> AAAAAAA      NA    0.05    0.73
> BBBBBBB      NA    0.21   -0.68
> CCCCCCC      NA    0.11   -0.16
> DDDDDDD      NA   -0.11   -0.91
> EEEEEEE      NA      NA      NA
> FFFFFFF      NA      NA      NA
> GGGGGGG      NA      NA      NA
> HHHHHHH      NA    1.00   -1.00
> IIIIIII      NA      NA   -0.35
> JJJJJJJ      NA      NA      NA
> KKKKKKK      NA    1.00    0.24
> LLLLLLL      NA    0.24    1.00
> >
>
> CONFIDENTIALITY NOTICE\ \ The information contained in this ...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From kevin.thorpe at utoronto.ca  Thu Jul 20 18:25:21 2006
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Thu, 20 Jul 2006 12:25:21 -0400
Subject: [R] throwaway() function
In-Reply-To: <1153412101.5989.11.camel@localhost>
References: <1153412101.5989.11.camel@localhost>
Message-ID: <44BFAE71.4060401@utoronto.ca>

John Wiedenhoeft wrote:
> Dear all,
> 
> I apologize if this is a FAQ (seems a bit like one, but I didn't find
> anything).
> 
> I'm looking for an easy way to cut one value out of a vector and shorten
> the vector accordingly. Something like:
> 
> x <- c(1, 1, 0, 6, 2)
> throwaway(x[3])
> 
> which will return x = 1 1 6 2, with length(x) = 4. I know one could do
> this by hand, but then one would have to create a second vector y in a
> loop which has to be 1 shorter then x and then assign x <- y, as there
> is no "anti-c()" function which shortens a vector (at least to my
> knowledge).
> 
> Is there some kind of a throwaway() function in R?
> 
> Best regards,
> John

x[-3] works.


-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.946.3297


From sundar.dorai-raj at pdf.com  Thu Jul 20 18:26:21 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 20 Jul 2006 11:26:21 -0500
Subject: [R] throwaway() function
In-Reply-To: <1153412101.5989.11.camel@localhost>
References: <1153412101.5989.11.camel@localhost>
Message-ID: <44BFAEAD.8050309@pdf.com>

John Wiedenhoeft wrote:
> Dear all,
> 
> I apologize if this is a FAQ (seems a bit like one, but I didn't find
> anything).
> 
> I'm looking for an easy way to cut one value out of a vector and shorten
> the vector accordingly. Something like:
> 
> x <- c(1, 1, 0, 6, 2)
> throwaway(x[3])
> 
> which will return x = 1 1 6 2, with length(x) = 4. I know one could do
> this by hand, but then one would have to create a second vector y in a
> loop which has to be 1 shorter then x and then assign x <- y, as there
> is no "anti-c()" function which shortens a vector (at least to my
> knowledge).
> 
> Is there some kind of a throwaway() function in R?
> 
> Best regards,
> John
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


Use indexing (explained in R-intro)

x <- x[-3] ## drops the third element
x <- x[-c(1, 3)] ## drops the first and third element

HTH,

--sundar


From mathematician4 at hotmail.com  Thu Jul 20 18:40:41 2006
From: mathematician4 at hotmail.com (Emanuele Mazzola)
Date: Thu, 20 Jul 2006 16:40:41 +0000
Subject: [R] Correspondence analysis with R -follow up
Message-ID: <BAY107-F13E57825D196D824A9F3829A610@phx.gbl>

Hi all,

thank you for your answers; i've tried both cca from vegan library, and 
dudi.coa from ade4 library; one last question: my deal is mainly with 
contingency tables, like the one i'm posting here

acciaieria<-c(.41,.02,.44,.04,.09)
laminatoio<-c(.34,.28,.26,.01,.11)
fonderia<-c(.48,.05,.34,.08,.05)
leghe<-c(.45,.19,.25,.03,.08)
pct<-cbind(acciaieria,laminatoio,fonderia,leghe)
pct<-data.frame(pct,row.names=c("normale","preparaz.","manutenz.","installaz.","trasferim."))

BUT...cca and dudi.coa seem to return quite different graphical results; 
where am i doing wrong?
Do they do the same to you with the table above?

Thank you very much again!
Bye
Emanuele


From cberry at tajo.ucsd.edu  Thu Jul 20 19:03:27 2006
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Thu, 20 Jul 2006 10:03:27 -0700
Subject: [R] Fitting a distribution to peaks in histogram
Message-ID: <Pine.LNX.4.64.0607200920320.29696@tajo.ucsd.edu>

On Wed, 19 Jul 2006, Ulrik Stervbo wrote:

[much deleted]

>
> I am measureing the amount of DNA in cells, and I need to know the
> percentage of cells in a part of the cell cycle; that the percentage
> of cells in the first peak, in the second peak and so on. I want to
> integrate the area between to two cells, because that apparently is
> how its none (as far as I can tell from the literature)
>

I do not doubt that that is how some do it, but a better approach is takes 
account of errors in the values used to form the histogram.

This is reviewed in

T. Lynn Eudey
Statistical Considerations in DNA Flow Cytometry
Statistical Science 1996, Vol. 11, No. 4, 320{334

A classic approach for this purpose is

John Mendelsohn; John Rice
Deconvolution of Microfluorometric Histograms with B Splines
Journal of the American Statistical Association. Vol. 77, No. 380, 1982.

A FORTRAN program is mentioned there and the authors say that they will 
provide listings on request (but that WAS a long time ago).

I don't know if this was ever implemented in S/R, but an email to 
Professor John Rice (UC Berkeley Dept of Stats) might help.

HTH,

Chuck

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0717


From vsdimitrov at yahoo.com  Thu Jul 20 19:13:23 2006
From: vsdimitrov at yahoo.com (Valentin Dimitrov)
Date: Thu, 20 Jul 2006 10:13:23 -0700 (PDT)
Subject: [R] Question about functions in R
In-Reply-To: <6dbf89a50607200334m48efb371p21981765cade1a94@mail.gmail.com>
Message-ID: <20060720171323.66059.qmail@web30809.mail.mud.yahoo.com>

Try to insert a space before the last bracket! R
sometimes does not accept a curly bracket as a first
character on a line of a script. 

Regards,
Valentin

--- Ivan Kalafatic <ivan.kalafatic at gmail.com> wrote:

> I tried to make the following function:
> 
> function(x, y){
>    dates<-intersect(x[,1],y[,1])
>    m<-matrix(NA,length(dates),3)
>    m[,1]<-dates
>    j<-1
>    k<-1
>    for(i in fdax[,1]){
>       if(is.element(i,dates)){
>          m[j,3]<-as.numeric(fdax[k,2])
>          j<-j+1
>       }
>       k<-k+1
>    }
>    return(m)
> }
> 
> When I try to import it into R with edit(
> file="name.txt",
> editor="someeditor") I get the response that there
> is an error on the
> line where last bracket is.
> When I execute this code manualy in R command by
> command everything works ok.
> This happened to me before with other functions.
> Sometimes I can
> import them and they work perfectly but sometimes I
> get the error on
> the last line of code which is alvays the final
> bracket.
> 
> Anyone has any idea why this is happening?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From spencer.graves at pdf.com  Thu Jul 20 19:32:08 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 20 Jul 2006 10:32:08 -0700
Subject: [R] Package for autocorrelation analysis?
In-Reply-To: <20060718120901.M26528@www.ecologia.unam.mx>
References: <OF301D160D.B5235C5D-ONC12571AF.003A75BE-C12571AF.003C9A16@basf-c-s.be>	<x24pxfb0vu.fsf@turmalin.kubism.ku.dk>
	<20060718120901.M26528@www.ecologia.unam.mx>
Message-ID: <44BFBE18.2050400@pdf.com>

I just got 145 hits for > RSiteSearch("ecology", "functions") and 28 for 
RSiteSearch("Moran's I", "functions").  Might any of these help you?

	  Spencer Graves

Ivan Rubio Perez wrote:
> Hi!
> Dear All,
> 
> I want to implement an autocorrelation analysis and estimated the Moran's I in a set of
> ecological traits. I seek for an R package that do this analysis, however, I couldn't
> found none that implement it. May be I'm lost in the universe of the Contributed
> Packages but I couldn't found information into the index of some packages that I suppose
> do it. Would you give me some recommendations to do an autocorrelation analysis?
> 
> Thanks!
> 
> Ivan
> 
> Ivan V. Rubio P?rez
> irubio at ecologia.unam.mx
> Instituto de Ecolog?a, UNAM
> www.ecologia.unam.mx
> --
> Open WebMail Project (http://openwebmail.org)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From spencer.graves at pdf.com  Thu Jul 20 19:43:27 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 20 Jul 2006 10:43:27 -0700
Subject: [R] 'nlme' crashes R (was:  Using corStruct in nlme)
In-Reply-To: <Pine.LNX.4.43.0607181128410.9698@hymn03.u.washington.edu>
References: <Pine.LNX.4.43.0607181128410.9698@hymn03.u.washington.edu>
Message-ID: <44BFC0BF.7070606@pdf.com>

	  Thanks for providing such a self-contained example by which 'nlme' 
crashes R.  Could you please also give us 'sessionInfo()'?  I don't have 
time to test it myself now, but perhaps if you identify your platform, 
you might interest someone else in checking it.

	  I'm sorry I couldn't be more helpful.
	  Spencer Graves

grieve at u.washington.edu wrote:
> I am having trouble fitting correlation structures within nlme. I would like to 
> fit corCAR1, corGaus and corExp correlation structures to my data.  I either 
> get the error "step halving reduced below minimum in pnls step" or 
> alternatively R crashes.
> 
> My dataset is similar to the CO2 example in the nlme package. The one major 
> difference is that in my case the 'conc' steps are not the same for each 'Plant'. 
I have replicated the problem using the CO2 data in nlme (based off of 
the Ch08.R
script).
> 
> This works (when 'conc' is the same for each 'Plant':
> 
> (fm1CO2.lis <- nlsList(SSasympOff, CO2))
> (fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2)))
> (fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1))
> CO2.nlme.var <- update(fm2CO2.nlme,
>  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
>  start = c(32.412, 0, 0, 0, -4.5603, 49.344), 
>  weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
> 
> CO2.nlme.CAR<-update(CO2.nlme.var, corr=corCAR1())
> 
> CO2.nlme.gauss<-update(CO2.nlme.var, 
> correlation=corGaus(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)
> 
> CO2.nlme.exp<-update(CO2.nlme.var, 
> correlation=corExp(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)  
> 
> But, if i change each of the 'conc' numbers slightly so that they are no longer
identical between subjects i can only get the corCAR1 correlation to 
work while R
crashes for both corExp and corGaus:
> 
> for(i in 1:length(CO2$conc)){
>     CO2$conc[i]<-(CO2$conc[i]+rnorm(1))
> }
> 
> (fm1CO2.lis <- nlsList(SSasympOff, CO2))
> (fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2)))
> (fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1))
> CO2.nlme.var <- update(fm2CO2.nlme,
>  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
>  start = c(32.412, 0, 0, 0, -4.5603, 49.344), 
>  weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
> 
> CO2.nlme.CAR<-update(CO2.nlme.var, corr=corCAR1())
> 
> CO2.nlme.gauss<-update(CO2.nlme.var, 
> correlation=corGaus(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)
> 
> CO2.nlme.exp<-update(CO2.nlme.var, 
> correlation=corExp(form=~as.numeric(conc)|Plant,nugget=F), data=CO2) 
> 
> I have read Pinheiro & Bates (2000) and i think that it should be possible to fit these correlation structures to my data, but maybe i am mistaken.
> 
> I am running R 2.3.1 and have recently updated all packages.
> 
> Thanks,
> Katie Grieve
> 
> Quantitative Ecology & Resource Management
> University of Washington
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From cberry at tajo.ucsd.edu  Thu Jul 20 19:45:38 2006
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Thu, 20 Jul 2006 10:45:38 -0700
Subject: [R] Permutation Distribution
Message-ID: <Pine.LNX.4.64.0607201032210.29696@tajo.ucsd.edu>

On Thu, 20 Jul 2006, Jacob van Wyk wrote:

> Hallo
>
> Is there an elegant way to do the following:
>
> Dataset consists of 2 variables: var1: some measurements, and var2: a grouping variable with two values, 1 and 2.
> There are (say) 10 measurements from group 1 and 15 measurements from group 2.
> The idea is to study the permutation distribution of mean(group 1) * mean(group2).
> One way would be to permute 1s and 2s and select the corresponding measurements; calculate the difference in means.
> Redo this 1000 times, say. Etc.



There are only choose(25,10) == 3268760 values.

You can enumerate all the sums of the values in one group following:

 	http://finzi.psych.upenn.edu/R/Rhelp02a/archive/76450.html

from which the products of the means are easily obtained.

Takes about 5 seconds on my 2GHz AMD running Windows XP.


HTH,

Chuck

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0717


From wiedenhoeft at gmx.net  Thu Jul 20 19:50:12 2006
From: wiedenhoeft at gmx.net (John Wiedenhoeft)
Date: Thu, 20 Jul 2006 19:50:12 +0200
Subject: [R] throwaway() function
In-Reply-To: <52A8091888A23F47A013223014B6E9FE074B27D5@03-CSEXCH.uopnet.plymouth.ac.uk>
References: <52A8091888A23F47A013223014B6E9FE074B27D5@03-CSEXCH.uopnet.plymouth.ac.uk>
Message-ID: <1153417813.5989.16.camel@localhost>

Easier than I thought. Thanks a lot!

Am Donnerstag, den 20.07.2006, 17:25 +0100 schrieben zig Leute ;-):
> x[-3]


From grieve at u.washington.edu  Thu Jul 20 20:07:42 2006
From: grieve at u.washington.edu (grieve at u.washington.edu)
Date: Thu, 20 Jul 2006 11:07:42 -0700 (PDT)
Subject: [R] 'nlme' crashes R (was:  Using corStruct in nlme)
In-Reply-To: <44BFC0BF.7070606@pdf.com>
Message-ID: <Pine.LNX.4.43.0607201107420.10140@hymn05.u.washington.edu>

Thanks Spencer. Here is my sessionInfo():

Version 2.3.1 (2006-06-01) 
i386-pc-mingw32 

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[7] "base"     

other attached packages:
    nlme 
"3.1-75" 

On Thu, 20 Jul 2006, Spencer Graves wrote:

> 	  Thanks for providing such a self-contained example by which 'nlme' 
> crashes R.  Could you please also give us 'sessionInfo()'?  I don't have 
> time to test it myself now, but perhaps if you identify your platform, you 
> might interest someone else in checking it.
> 
> 	  I'm sorry I couldn't be more helpful.
> 	  Spencer Graves
> 
> grieve at u.washington.edu wrote:
>> I am having trouble fitting correlation structures within nlme. I would 
>> like to fit corCAR1, corGaus and corExp correlation structures to my data. 
>> I either get the error "step halving reduced below minimum in pnls step" 
>> or alternatively R crashes.
>> 
>> My dataset is similar to the CO2 example in the nlme package. The one 
>> major difference is that in my case the 'conc' steps are not the same for 
>> each 'Plant'. 
> I have replicated the problem using the CO2 data in nlme (based off of the 
> Ch08.R
> script).
>> 
>> This works (when 'conc' is the same for each 'Plant':
>> 
>> (fm1CO2.lis <- nlsList(SSasympOff, CO2))
>> (fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2)))
>> (fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1))
>> CO2.nlme.var <- update(fm2CO2.nlme,
>>  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
>>  start = c(32.412, 0, 0, 0, -4.5603, 49.344), 
>> weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
>> 
>> CO2.nlme.CAR<-update(CO2.nlme.var, corr=corCAR1())
>> 
>> CO2.nlme.gauss<-update(CO2.nlme.var, 
>> correlation=corGaus(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)
>> 
>> CO2.nlme.exp<-update(CO2.nlme.var, 
>> correlation=corExp(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)  But, 
>> if i change each of the 'conc' numbers slightly so that they are no longer
> identical between subjects i can only get the corCAR1 correlation to work 
> while R
> crashes for both corExp and corGaus:
>> 
>> for(i in 1:length(CO2$conc)){
>>     CO2$conc[i]<-(CO2$conc[i]+rnorm(1))
>> }
>> 
>> (fm1CO2.lis <- nlsList(SSasympOff, CO2))
>> (fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2)))
>> (fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1))
>> CO2.nlme.var <- update(fm2CO2.nlme,
>>  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
>>  start = c(32.412, 0, 0, 0, -4.5603, 49.344), 
>> weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
>> 
>> CO2.nlme.CAR<-update(CO2.nlme.var, corr=corCAR1())
>> 
>> CO2.nlme.gauss<-update(CO2.nlme.var, 
>> correlation=corGaus(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)
>> 
>> CO2.nlme.exp<-update(CO2.nlme.var, 
>> correlation=corExp(form=~as.numeric(conc)|Plant,nugget=F), data=CO2) I 
>> have read Pinheiro & Bates (2000) and i think that it should be possible 
>> to fit these correlation structures to my data, but maybe i am mistaken.
>> 
>> I am running R 2.3.1 and have recently updated all packages.
>> 
>> Thanks,
>> Katie Grieve
>> 
>> Quantitative Ecology & Resource Management
>> University of Washington
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From deb37 at columbia.edu  Thu Jul 20 20:15:25 2006
From: deb37 at columbia.edu (Daniel E. Bunker)
Date: Thu, 20 Jul 2006 14:15:25 -0400
Subject: [R] Convergence warnings from zeroinfl (package pscl)
Message-ID: <44BFC83D.50109@columbia.edu>

Dear R-Helpers,

Can anyone please help me to interpret warning messages from zeroinfl 
(package pscl) while fitting a zero inflated negative binomial model?

The console reports convergence and the parameters seam reasonable, but 
these

<<Warning messages:
1: algorithm did not converge in: glm.fit(X, Y, family = poisson())
2: fitted rates numerically 0 occurred in: glm.fit(X, Y, family = 
poisson()) >>

suggest that it did not converge.  (See full output below.)

Could some possibly help me to interpret these results?

Thanks for your time,

Dan


 > zip3=zeroinfl(count=round(modAbun*1000) ~ .,
+ x=~mod*intact,
+ z=~1,
+ dist="negbin",
+ trace=TRUE,
+ data=beetles)
Zero-Inflated Count Model
Using logit to model zero vs non-zero
Using Negative Binomial for counts
dependent variable y:
Y
      0     10     11     14     17     19     21     23     25     28 
    31     33     34     37     40     42     46
   1056      3      9      1      4      1      1      8      2      2 
     5      7      3      5      2     22      1
<<snip>>
385000 407000 416000 470250 519000 537750 560000 754680
      1      1      1      1      1      1      1      1
generating start values...done
MLE begins...
initial  value 10878.408362
iter   2 value 10600.864576
<<snip>>
iter  52 value 9531.456792
final  value 9531.456792
converged
done
Warning messages:
1: algorithm did not converge in: glm.fit(X, Y, family = poisson())
2: fitted rates numerically 0 occurred in: glm.fit(X, Y, family = 
poisson())
 > summary(zip3)
Zero-Inflated Count Model Summary
---------------------------

Call:
zeroinfl(count = round(modAbun * 1000) ~ ., x = ~mod * intact,
     z = ~1, data = beetles, dist = "negbin", trace = TRUE)

Total Log-likelihood: -9531.45679243448

  Zero-Inflated Model was fit with a logit link
Coefficients:
   Estimate Std. Error    z value   Pr(>|z|)
   -0.01853    0.05685   -0.32598    0.74444
---------------------------------------------------------------------------

Count Model (Negative Binomial)
Coefficients:
                  Estimate Std. Error  z value   Pr(>|z|)
(Intercept)      7.316434   0.217397  33.6547 2.659e-248
modes           -0.329699   0.363943  -0.9059  3.650e-01
modls            0.451599   0.267710   1.6869  9.162e-02
modmag           1.713005   0.296333   5.7807  7.440e-09
modpag           1.654261   0.266674   6.2033  5.529e-10
modsl            1.946231   0.308218   6.3145  2.711e-10
modacpas         2.379625   0.275172   8.6478  5.252e-18
intact          -0.002080   0.001198  -1.7368  8.241e-02
modes:intact     0.075579   0.020012   3.7768  1.589e-04
modls:intact     0.036582   0.006215   5.8862  3.952e-09
modmag:intact    0.005915   0.002770   2.1351  3.276e-02
modpag:intact    0.021262   0.004977   4.2721  1.937e-05
modsl:intact     0.008784   0.002573   3.4136  6.411e-04
modacpas:intact -0.009994   0.002735  -3.6546  2.576e-04
log(theta)      -1.405175   0.062412 -22.5145 2.993e-112

Theta = 0.2453



-- 
Daniel E. Bunker
BioMERGE Associate Director
Post-Doctoral Research Scientist
Columbia University
Department of Ecology, Evolution and Environmental Biology
1200 Amsterdam Avenue
New York, NY 10027-5557

deb37ATcolumbiaDOTedu
212-851-1888 phone
212-854-8188 fax


From spector at stat.Berkeley.EDU  Thu Jul 20 20:22:44 2006
From: spector at stat.Berkeley.EDU (Phil Spector)
Date: Thu, 20 Jul 2006 11:22:44 -0700 (PDT)
Subject: [R] Can make no plots !!!
In-Reply-To: <200607201515.12277.bernat@creaf.uab.es>
References: <200607201515.12277.bernat@creaf.uab.es>
Message-ID: <Pine.LNX.4.64.0607201121480.31944@springer.berkeley.edu>

Bernat -
    Just a guess, but maybe you need to install the packages

       xfonts-100dpi 
and
       xfonts-75dpi

                                                         - Phil


On Thu, 20 Jul 2006, Bernat Claramunt i Lopez wrote:

> Dear all
> I have Kubuntu linux and have updated to the latest version (6.06 dapper). I
> do not know why but now I can not make no plots. For instance, when I type
>
>> hist(...)
>
> this is the message I get:
>
>> can't find X11 font
>> Error in X11 (display, width, heigth......)
>> unable to start devide X11
>> In addition: Warning messages:
>> 1:locale not supported by Xlib: some X ops will operate in C locale
>> 2: X cannot set locale modifiers
>
> Any suggestion ? Maybe it is because kubuntu now works with Xorg instead of
> X11 ? I really ignore how to solve it and I need to make plots !!!
>
> Thanks in advance
>
> -- 
> Bernat Claramunt i Lopez
> CREAF (Centre de Recerca Ecologica i Aplicacions Forestals ) i Departament de
> Biologia Vegetal, Biologia Animal i Ecologia
> Universitat Autonoma de Barcelona
> 08193 Bellaterra, Catalunya
>
> Telf: +34935811920
> FAX:  +34935814151
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bunny at lautloscrew.com  Thu Jul 20 21:42:38 2006
From: bunny at lautloscrew.com (bunny , lautloscrew.com)
Date: Thu, 20 Jul 2006 21:42:38 +0200
Subject: [R] function names in a vector used by for (){} character problem ?
Message-ID: <38BDA3B3-69AD-4657-AA45-57A10A14BED0@lautloscrew.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060720/6414f66a/attachment.pl 

From smckinney at bccrc.ca  Thu Jul 20 21:49:14 2006
From: smckinney at bccrc.ca (Steven McKinney)
Date: Thu, 20 Jul 2006 12:49:14 -0700
Subject: [R] How do I modify an exported function in a locked environment?
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB01D554A2@crcmail1.BCCRC.CA>



Running R.app on Mac OS X 10.4

> version
               _                         
platform       powerpc-apple-darwin8.6.0 
arch           powerpc                   
os             darwin8.6.0               
system         powerpc, darwin8.6.0      
status                                   
major          2                         
minor          3.1                       
year           2006                      
month          06                        
day            01                        
svn rev        38247                     
language       R                         
version.string Version 2.3.1 (2006-06-01)
> 


I am trying to learn how to modify functions
in a locked environment.

For an example, suppose I'm using the package "zoo".

zoo contains function "rollmean.default"

> rollmean.default
function (x, k, na.pad = FALSE, align = c("center", "left", "right"), 
    ...) 
{
    x <- unclass(x)
    n <- length(x)
    y <- x[k:n] - x[c(1, 1:(n - k))]
    y[1] <- sum(x[1:k])
    rval <- cumsum(y)/k
    if (na.pad) {
        rval <- switch(match.arg(align), left = {
            c(rval, rep(NA, k - 1))
        }, center = {
            c(rep(NA, floor((k - 1)/2)), rval, rep(NA, ceiling((k - 
                1)/2)))
        }, right = {
            c(rep(NA, k - 1), rval)
        })
    }
    return(rval)
}
<environment: namespace:zoo>

Suppose for whatever reason I want output to be
in percent, so I'd like to modify the result to be
rval <- 100 * cumsum(y)/k

I cannot just copy the function and change it, as the namespace
mechanism ensures the rollmean.default in 'zoo' continues to be used.

If I use
fixInNamespace("rollmean.default", ns = "zoo")
I can edit the rval <- cumsum(y)/k line to read
rval <- 100 * cumsum(y)/k
save the file and exit the R.app GUI editor.

But this does not update the exported copy of the
function (the documentation for fixInNamespace says
this is the case) - how do I accomplish this last step?

If I list the function after editing, I see the original
copy.  But if I reinvoke the editor via fixInNamespace(),
I do see my modification.
Where is my copy residing?  How do I push it out
to replace the exported copy?

Is this the proper way to modify a package function?
Are there other ways?  I've searched webpages, R news,
help files and have been unable to find out how to
get this process fully completed.

Any guidance appreciated.


Steven McKinney

Statistician
Molecular Oncology and Breast Cancer Program
British Columbia Cancer Research Centre

email: smckinney at bccrc.ca

tel: 604-675-8000 x7561

BCCRC
Molecular Oncology
675 West 10th Ave, Floor 4
Vancouver B.C. 
V5Z 1L3
Canada


From murdoch at stats.uwo.ca  Thu Jul 20 21:51:00 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 20 Jul 2006 15:51:00 -0400
Subject: [R] Using corStruct in nlme
In-Reply-To: <Pine.LNX.4.43.0607181128410.9698@hymn03.u.washington.edu>
References: <Pine.LNX.4.43.0607181128410.9698@hymn03.u.washington.edu>
Message-ID: <44BFDEA4.5000206@stats.uwo.ca>

On 7/18/2006 2:28 PM, grieve at u.washington.edu wrote:
> I am having trouble fitting correlation structures within nlme. I would like to 
> fit corCAR1, corGaus and corExp correlation structures to my data.  I either 
> get the error "step halving reduced below minimum in pnls step" or 
> alternatively R crashes.
> 
> My dataset is similar to the CO2 example in the nlme package. The one major 
> difference is that in my case the 'conc' steps are not the same for each 'Plant'. I have replicated the problem using the CO2 data in nlme (based off of the Ch08.R script).
> 
> This works (when 'conc' is the same for each 'Plant':
> 
> (fm1CO2.lis <- nlsList(SSasympOff, CO2))
> (fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2)))
> (fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1))
> CO2.nlme.var <- update(fm2CO2.nlme,
>  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
>  start = c(32.412, 0, 0, 0, -4.5603, 49.344), 
>  weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
> 
> CO2.nlme.CAR<-update(CO2.nlme.var, corr=corCAR1())
> 
> CO2.nlme.gauss<-update(CO2.nlme.var, 
> correlation=corGaus(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)
> 
> CO2.nlme.exp<-update(CO2.nlme.var, 
> correlation=corExp(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)  
> 
> But, if i change each of the 'conc' numbers slightly so that they are no longer identical between subjects i can only get the corCAR1 correlation to work while R crashes for both corExp and corGaus:
> 
> for(i in 1:length(CO2$conc)){
>     CO2$conc[i]<-(CO2$conc[i]+rnorm(1))
> }
> 
> (fm1CO2.lis <- nlsList(SSasympOff, CO2))
> (fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2)))
> (fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1))
> CO2.nlme.var <- update(fm2CO2.nlme,
>  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
>  start = c(32.412, 0, 0, 0, -4.5603, 49.344), 
>  weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
> 
> CO2.nlme.CAR<-update(CO2.nlme.var, corr=corCAR1())
> 
> CO2.nlme.gauss<-update(CO2.nlme.var, 
> correlation=corGaus(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)
> 
> CO2.nlme.exp<-update(CO2.nlme.var, 
> correlation=corExp(form=~as.numeric(conc)|Plant,nugget=F), data=CO2) 
> 
> I have read Pinheiro & Bates (2000) and i think that it should be possible to fit these correlation structures to my data, but maybe i am mistaken.
> 
> I am running R 2.3.1 and have recently updated all packages.

I reproduced this once in R-patched, but since then have been unable to 
do so.  I can reproduce it reliably with "set.seed(1)" at the start in R 
2.3.1.  So it looks to me as though we've probably done something to 
make the error less likely, but not completely fixed it.

If you can find a script (including set.seed() to some value) that 
reliably causes a crash in R-patched, could you let me know?

You can get R-patched from CRAN in the bin/windows/base directory.

Duncan Murdoch


From murdoch at stats.uwo.ca  Thu Jul 20 21:56:54 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 20 Jul 2006 15:56:54 -0400
Subject: [R] function names in a vector used by for (){} character
 problem ?
In-Reply-To: <38BDA3B3-69AD-4657-AA45-57A10A14BED0@lautloscrew.com>
References: <38BDA3B3-69AD-4657-AA45-57A10A14BED0@lautloscrew.com>
Message-ID: <44BFE006.8020604@stats.uwo.ca>

On 7/20/2006 3:42 PM, bunny , lautloscrew.com wrote:
> Hi there,
> 
> i?m have vector of kernels. just like:
> kernels = c('gauss','epan','rectangular')
> 
> i know there are density.default$kernels, but thats not my question  
> here. my own kernel functions are running and working.
> my problem is the following is not working:
> 
> 
> dev.off()
> par(mfrow=c(3,3))
> 
> 
> for(i in 1:length(bw))
> {
>          for(j in 1:length(kernels))
>          {
>          par(mfg = c(i, j))	       	
> 		nawaline2(eruptions,waiting,kernels[j],bw[i],1000)
> 		# FYI: the following worked but ofcourse all rows were the same
> 		# nawaline2(eruptions,waiting,gauss,bw[i],1000)
> 
>          }
> }
> 
> # here are the standard arguments for nawaline, kern exspects a  
> function like gaussian, epan or rectangular.
> nawaline2(xi,yi,kern,h,N=1000)
> 
> there?s bw vector also that works well. the problem seems  that my  
> kernels vector ist a vector of characters.
> I want to loop it using for ;  but every time it runs through, there 
> ?s an error, that kern can?t be found.
> i think it?s only beacause  i don?t know how to get gauss instead of  
> "gauss" and so on...

You could set up kernels as

kernels <- list(gauss, epan, rectangular)

and then pass kernels[[j]] instead of kernels[j].

Or your nawaline2 function could have something like

if (is.character(kern)) kern <- get(kern, envir=parent.frame())

to go looking for the object with the name stored in kern.

Duncan Murdoch

> 
> has anybody an idea ? thanks so much in advance !!
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bates at stat.wisc.edu  Thu Jul 20 21:59:17 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 20 Jul 2006 14:59:17 -0500
Subject: [R] function names in a vector used by for (){} character
	problem ?
In-Reply-To: <38BDA3B3-69AD-4657-AA45-57A10A14BED0@lautloscrew.com>
References: <38BDA3B3-69AD-4657-AA45-57A10A14BED0@lautloscrew.com>
Message-ID: <40e66e0b0607201259k153dbb28wa12ec5424038b574@mail.gmail.com>

On 7/20/06, bunny , lautloscrew.com <bunny at lautloscrew.com> wrote:
> Hi there,
>
> i?m have vector of kernels. just like:
> kernels = c('gauss','epan','rectangular')
>
> i know there are density.default$kernels, but thats not my question
> here. my own kernel functions are running and working.
> my problem is the following is not working:
>
>
> dev.off()
> par(mfrow=c(3,3))
>
>
> for(i in 1:length(bw))
> {
>          for(j in 1:length(kernels))
>          {
>          par(mfg = c(i, j))
>                 nawaline2(eruptions,waiting,kernels[j],bw[i],1000)
>                 # FYI: the following worked but ofcourse all rows were the same
>                 # nawaline2(eruptions,waiting,gauss,bw[i],1000)
>
>          }
> }

> # here are the standard arguments for nawaline, kern exspects a
> function like gaussian, epan or rectangular.
> nawaline2(xi,yi,kern,h,N=1000)

Use get(kernels[i]), not kernels[i].  This is, I believe, a FAQ.

> there?s bw vector also that works well. the problem seems  that my
> kernels vector ist a vector of characters.
> I want to loop it using for ;  but every time it runs through, there
> ?s an error, that kern can?t be found.
> i think it?s only beacause  i don?t know how to get gauss instead of
> "gauss" and so on...
>
> has anybody an idea ? thanks so much in advance !!
>
>
>
>
>         [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From murdoch at stats.uwo.ca  Thu Jul 20 22:02:12 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 20 Jul 2006 16:02:12 -0400
Subject: [R] How do I modify an exported function in a locked
	environment?
In-Reply-To: <0BE438149FF2254DB4199E2682C8DFEB01D554A2@crcmail1.BCCRC.CA>
References: <0BE438149FF2254DB4199E2682C8DFEB01D554A2@crcmail1.BCCRC.CA>
Message-ID: <44BFE144.100@stats.uwo.ca>

On 7/20/2006 3:49 PM, Steven McKinney wrote:
> 
> Running R.app on Mac OS X 10.4
> 
>> version
>                _                         
> platform       powerpc-apple-darwin8.6.0 
> arch           powerpc                   
> os             darwin8.6.0               
> system         powerpc, darwin8.6.0      
> status                                   
> major          2                         
> minor          3.1                       
> year           2006                      
> month          06                        
> day            01                        
> svn rev        38247                     
> language       R                         
> version.string Version 2.3.1 (2006-06-01)
>> 
> 
> 
> I am trying to learn how to modify functions
> in a locked environment.
> 
> For an example, suppose I'm using the package "zoo".
> 
> zoo contains function "rollmean.default"
> 
>> rollmean.default
> function (x, k, na.pad = FALSE, align = c("center", "left", "right"), 
>     ...) 
> {
>     x <- unclass(x)
>     n <- length(x)
>     y <- x[k:n] - x[c(1, 1:(n - k))]
>     y[1] <- sum(x[1:k])
>     rval <- cumsum(y)/k
>     if (na.pad) {
>         rval <- switch(match.arg(align), left = {
>             c(rval, rep(NA, k - 1))
>         }, center = {
>             c(rep(NA, floor((k - 1)/2)), rval, rep(NA, ceiling((k - 
>                 1)/2)))
>         }, right = {
>             c(rep(NA, k - 1), rval)
>         })
>     }
>     return(rval)
> }
> <environment: namespace:zoo>
> 
> Suppose for whatever reason I want output to be
> in percent, so I'd like to modify the result to be
> rval <- 100 * cumsum(y)/k
> 
> I cannot just copy the function and change it, as the namespace
> mechanism ensures the rollmean.default in 'zoo' continues to be used.
> 
> If I use
> fixInNamespace("rollmean.default", ns = "zoo")
> I can edit the rval <- cumsum(y)/k line to read
> rval <- 100 * cumsum(y)/k
> save the file and exit the R.app GUI editor.
> 
> But this does not update the exported copy of the
> function (the documentation for fixInNamespace says
> this is the case) - how do I accomplish this last step?
> 
> If I list the function after editing, I see the original
> copy.  But if I reinvoke the editor via fixInNamespace(),
> I do see my modification.
> Where is my copy residing?  How do I push it out
> to replace the exported copy?
> 
> Is this the proper way to modify a package function?
> Are there other ways?  I've searched webpages, R news,
> help files and have been unable to find out how to
> get this process fully completed.

You could modify the zoo source and recompile it, or you could write 
your own function that calls the one in zoo and modifies the result.

I don't think you should modify zoo without really careful thought:  you 
should assume that the package has been tested the way it was written, 
and may give incorrect results if you go in and change one function 
without considering everything else in the package.

So I'd recommend writing a wrapper, e.g.

myrollmean <- function(x, k, na.pad = FALSE, align = c("center", "left", 
"right"), ...) {
   result <- rollmean(x, k, na.pad, align, ...)
   return(result*100)
}

 >     ...
Duncan Murdoch
> 
> Any guidance appreciated.
> 
> 
> Steven McKinney
> 
> Statistician
> Molecular Oncology and Breast Cancer Program
> British Columbia Cancer Research Centre
> 
> email: smckinney at bccrc.ca
> 
> tel: 604-675-8000 x7561
> 
> BCCRC
> Molecular Oncology
> 675 West 10th Ave, Floor 4
> Vancouver B.C. 
> V5Z 1L3
> Canada
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From leaflovesun at yahoo.ca  Thu Jul 20 22:25:05 2006
From: leaflovesun at yahoo.ca (Leaf Sun)
Date: Thu, 20 Jul 2006 14:25:05 -0600
Subject: [R] (no subject)
References: <1bff52c20606140033j3ac8e9fdxa85d4edded2ad94a@mail.gmail.com>
	<1bff52c20606140101r25188de1g6573c52f3cb5d152@mail.gmail.com>
	<38b9f0350606140223g507112dau3478586790daad3d@mail.gmail.com>
	<200606140735459773486@yahoo.ca>
	<449285BD.3020101@statistik.uni-dortmund.de>
	<200606161041072848729@yahoo.ca>
	<40e66e0b0606180515t759f1400od071251942d8dac6@mail.gmail.com>
	<200606191251227312041@yahoo.ca> <200607171218066725256@yahoo.ca>
	<878440A9-8768-4874-B46A-60B6F1D75F38@austin.rr.com>
	<200607200957069985478
Message-ID: <200607202025.k6KKP6c3029939@hypatia.math.ethz.ch>

@yahoo.ca>
Subject:  Re: [R] Weibull distribution
Message-ID: <200607201425040019536 at yahoo.ca>
X-mailer: Foxmail 6, 3, 103, 21 [cn]
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="=====003_Dragon527446281311_====="


This is a multi-part message in MIME format.

--=====003_Dragon527446281311_=====
Content-Type: text/plain;
	charset="gb2312"
Content-Transfer-Encoding: 7bit

Hi William,

Thanks a lot for your response. I checked the package and found that what I want to solve was the opposite, that is, from mean and sd to parameters shape and scale. Could anyone give some hints please? Any suggestion would be appreciated!

Leaf



----- Original Message -----

From: William Asquith,  wasquith at austin.rr.com
Sent: 2006-07-17,  16:18:31
To: Leaf Sun, leaflovesun at yahoo.ca
Subject:  Re: [R] Weibull distribution
  
Do  not  have  answer  per  se,  but  if  you  are  seeking  some  comparisons--  
try  three  parameter  Weibull  as  implemented  by  the  lmomco  package.

William
On  Jul  17,  2006,  at  1:18  PM,  Leaf  Sun  wrote:

>  Hi  all,
>
>  By  its  definition,  the  mean  and  variance  of  two-par.  Weibull    
>  distribution  are:
>
>
>
>
>
>    (www.wikipedia.org)
>
>
>  I  was  wondering,  if  given  mean  and  sd.  could  we  parameterize  the    
>  distribution?  I  tried  this  in  R.
>
>  gamma.fun   <-  function(mu,sd,start=100)
>  {
>  f.fn   <-  function(alpha)  sd^2-mu^2/(gamma(1+1/alpha))^2*(gamma(1+2/  
>  alpha)-(gamma(1+1/alpha))^2)
>  alpha   <-  optim(start,  f.fn,method='BFGS')
>  beta   <-  mu/gamma(1+1/alpha$par)
>  return(list=c(a=alpha$par,b=beta));
>  }
>
>
>  But  the  problems  come  up  here:
>
>  1)    the  return  values  of  a  and  b  are  only  related  to  the  input    
>  mean,  and  nothing  to  do  with  the  sd.  For  instance,  when  I  apply  a    
>  mean  mu  =  3  whatever  I  use  sd=2,  sd=4,  the  function  returned  the    
>  same  scale  and  shape  values.
>
> >  gamma.fun(3,4,10);
>                a                b
>  5.112554  3.263178
>
> >  gamma.fun(3,2,10);
>                a                b
>  5.112554  3.263178
>
>  2)  the  start  value  determines  the  results:  if  I  apply  mean  =  3,  and    
>  sd=2,  with  a  start  of  10,  it  would  return  alpha  close  to  10,  if  I    
>  use  a  start  =  100,  it  would  return  alpha  close  to  100.
>
> >  gamma.fun(3,2,10);
>                a                b
>  5.112554  3.263178
>
> >  gamma.fun(3,2,100);
>                  a                  b
>  99.999971    3.017120
>
>  Since  I  am  not  a  statistician,  I  guess  there  must  be  some    
>  theoretical  reasons  wrong  with  this  question.  So  I  am  looking    
>  forward  to  some  correction  and  advice  to  solve  these.  Thanks  a  lot    
>  in  advance!
>
>  Leaf
>
>   [[alternative  HTML  version  deleted]]
>
>  ______________________________________________
>  R-help at stat.math.ethz.ch  mailing  list
>  https://stat.ethz.ch/mailman/listinfo/r-help
>  PLEASE  do  read  the  posting  guide!  http://www.R-project.org/posting-  
>  guide.html

--=====003_Dragon527446281311_=====
Content-Type: text/html;
	charset="gb2312"
Content-Transfer-Encoding: 7bit

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML><HEAD>
<META http-equiv=Content-Type content="text/html; charset=gb2312">
<META content="MSHTML 6.00.2900.2912" name=GENERATOR></HEAD>
<BODY>
<DIV>Hi William,</DIV>
<DIV>&nbsp;</DIV>
<DIV>Thanks a lot for your response. I checked the package and found that what I 
want to solve was the opposite, that is, from mean and sd to parameters shape 
and scale. Could anyone give some hints please? Any suggestion would be 
appreciated!</DIV>
<DIV><BR>Leaf</DIV>
<DIV>&nbsp;</DIV>
<DIV>&nbsp;</DIV>
<DIV>&nbsp;</DIV>
<DIV>----- Original Message -----</DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT size=2><FONT face=Tahoma><STRONG>From:</STRONG> William 
Asquith,&nbsp;&nbsp;<A 
href="mailto:wasquith at austin.rr.com">wasquith at austin.rr.com</A><BR><B>Sent:</B> 
2006-07-17,&nbsp; 16:18:31<BR><B>To:</B> Leaf Sun, <A 
href="mailto:leaflovesun at yahoo.ca">leaflovesun at yahoo.ca</A><BR><B>Subject:</B>&nbsp; 
Re: [R] Weibull distribution</FONT></FONT></DIV>
<DIV>&nbsp;&nbsp;</DIV>
<DIV>
<TABLE width="100%">
  <TBODY>
  <TR>
    <TD width="100%">
      <BLOCKQUOTE 
      style="PADDING-RIGHT: 0px; PADDING-LEFT: 5px; MARGIN-LEFT: 5px; BORDER-LEFT: #000000 2px solid; MARGIN-RIGHT: 0px">
        <DIV>Do &nbsp;not &nbsp;have &nbsp;answer &nbsp;per &nbsp;se, &nbsp;but 
        &nbsp;if &nbsp;you &nbsp;are &nbsp;seeking &nbsp;some 
        &nbsp;comparisons-- &nbsp;</DIV>
        <DIV>try &nbsp;three &nbsp;parameter &nbsp;Weibull &nbsp;as 
        &nbsp;implemented &nbsp;by &nbsp;the &nbsp;lmomco &nbsp;package.</DIV>
        <DIV>&nbsp;</DIV>
        <DIV>William</DIV>
        <DIV>On &nbsp;Jul &nbsp;17, &nbsp;2006, &nbsp;at &nbsp;1:18 &nbsp;PM, 
        &nbsp;Leaf &nbsp;Sun &nbsp;wrote:</DIV>
        <DIV>&nbsp;</DIV>
        <DIV>&gt; &nbsp;Hi &nbsp;all,</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt; &nbsp;By &nbsp;its &nbsp;definition, &nbsp;the &nbsp;mean 
        &nbsp;and &nbsp;variance &nbsp;of &nbsp;two-par. &nbsp;Weibull &nbsp; 
        &nbsp;</DIV>
        <DIV>&gt; &nbsp;distribution &nbsp;are:</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt; &nbsp; &nbsp;(www.wikipedia.org)</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt; &nbsp;I &nbsp;was &nbsp;wondering, &nbsp;if &nbsp;given 
        &nbsp;mean &nbsp;and &nbsp;sd. &nbsp;could &nbsp;we &nbsp;parameterize 
        &nbsp;the &nbsp; &nbsp;</DIV>
        <DIV>&gt; &nbsp;distribution? &nbsp;I &nbsp;tried &nbsp;this &nbsp;in 
        &nbsp;R.</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt; &nbsp;gamma.fun &nbsp; &lt;- 
        &nbsp;function(mu,sd,start=100)</DIV>
        <DIV>&gt; &nbsp;{</DIV>
        <DIV>&gt; &nbsp;f.fn &nbsp; &lt;- &nbsp;function(alpha) 
        &nbsp;sd^2-mu^2/(gamma(1+1/alpha))^2*(gamma(1+2/ &nbsp;</DIV>
        <DIV>&gt; &nbsp;alpha)-(gamma(1+1/alpha))^2)</DIV>
        <DIV>&gt; &nbsp;alpha &nbsp; &lt;- &nbsp;optim(start, 
        &nbsp;f.fn,method='BFGS')</DIV>
        <DIV>&gt; &nbsp;beta &nbsp; &lt;- &nbsp;mu/gamma(1+1/alpha$par)</DIV>
        <DIV>&gt; &nbsp;return(list=c(a=alpha$par,b=beta));</DIV>
        <DIV>&gt; &nbsp;}</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt; &nbsp;But &nbsp;the &nbsp;problems &nbsp;come &nbsp;up 
        &nbsp;here:</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt; &nbsp;1) &nbsp; &nbsp;the &nbsp;return &nbsp;values &nbsp;of 
        &nbsp;a &nbsp;and &nbsp;b &nbsp;are &nbsp;only &nbsp;related &nbsp;to 
        &nbsp;the &nbsp;input &nbsp; &nbsp;</DIV>
        <DIV>&gt; &nbsp;mean, &nbsp;and &nbsp;nothing &nbsp;to &nbsp;do 
        &nbsp;with &nbsp;the &nbsp;sd. &nbsp;For &nbsp;instance, &nbsp;when 
        &nbsp;I &nbsp;apply &nbsp;a &nbsp; &nbsp;</DIV>
        <DIV>&gt; &nbsp;mean &nbsp;mu &nbsp;= &nbsp;3 &nbsp;whatever &nbsp;I 
        &nbsp;use &nbsp;sd=2, &nbsp;sd=4, &nbsp;the &nbsp;function 
        &nbsp;returned &nbsp;the &nbsp; &nbsp;</DIV>
        <DIV>&gt; &nbsp;same &nbsp;scale &nbsp;and &nbsp;shape 
        &nbsp;values.</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt; &gt; &nbsp;gamma.fun(3,4,10);</DIV>
        <DIV>&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;a 
        &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;b</DIV>
        <DIV>&gt; &nbsp;5.112554 &nbsp;3.263178</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt; &gt; &nbsp;gamma.fun(3,2,10);</DIV>
        <DIV>&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;a 
        &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;b</DIV>
        <DIV>&gt; &nbsp;5.112554 &nbsp;3.263178</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt; &nbsp;2) &nbsp;the &nbsp;start &nbsp;value &nbsp;determines 
        &nbsp;the &nbsp;results: &nbsp;if &nbsp;I &nbsp;apply &nbsp;mean &nbsp;= 
        &nbsp;3, &nbsp;and &nbsp; &nbsp;</DIV>
        <DIV>&gt; &nbsp;sd=2, &nbsp;with &nbsp;a &nbsp;start &nbsp;of &nbsp;10, 
        &nbsp;it &nbsp;would &nbsp;return &nbsp;alpha &nbsp;close &nbsp;to 
        &nbsp;10, &nbsp;if &nbsp;I &nbsp; &nbsp;</DIV>
        <DIV>&gt; &nbsp;use &nbsp;a &nbsp;start &nbsp;= &nbsp;100, &nbsp;it 
        &nbsp;would &nbsp;return &nbsp;alpha &nbsp;close &nbsp;to 
        &nbsp;100.</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt; &gt; &nbsp;gamma.fun(3,2,10);</DIV>
        <DIV>&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;a 
        &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;b</DIV>
        <DIV>&gt; &nbsp;5.112554 &nbsp;3.263178</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt; &gt; &nbsp;gamma.fun(3,2,100);</DIV>
        <DIV>&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
        &nbsp;a &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
        &nbsp;b</DIV>
        <DIV>&gt; &nbsp;99.999971 &nbsp; &nbsp;3.017120</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt; &nbsp;Since &nbsp;I &nbsp;am &nbsp;not &nbsp;a 
        &nbsp;statistician, &nbsp;I &nbsp;guess &nbsp;there &nbsp;must &nbsp;be 
        &nbsp;some &nbsp; &nbsp;</DIV>
        <DIV>&gt; &nbsp;theoretical &nbsp;reasons &nbsp;wrong &nbsp;with 
        &nbsp;this &nbsp;question. &nbsp;So &nbsp;I &nbsp;am &nbsp;looking 
        &nbsp; &nbsp;</DIV>
        <DIV>&gt; &nbsp;forward &nbsp;to &nbsp;some &nbsp;correction &nbsp;and 
        &nbsp;advice &nbsp;to &nbsp;solve &nbsp;these. &nbsp;Thanks &nbsp;a 
        &nbsp;lot &nbsp; &nbsp;</DIV>
        <DIV>&gt; &nbsp;in &nbsp;advance!</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt; &nbsp;Leaf</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt; &nbsp; [[alternative &nbsp;HTML &nbsp;version 
        &nbsp;deleted]]</DIV>
        <DIV>&gt;</DIV>
        <DIV>&gt; &nbsp;______________________________________________</DIV>
        <DIV>&gt; &nbsp;R-help at stat.math.ethz.ch &nbsp;mailing &nbsp;list</DIV>
        <DIV>&gt; &nbsp;https://stat.ethz.ch/mailman/listinfo/r-help</DIV>
        <DIV>&gt; &nbsp;PLEASE &nbsp;do &nbsp;read &nbsp;the &nbsp;posting 
        &nbsp;guide! &nbsp;<A 
        href="http://www.R-project.org/posting-">http://www.R-project.org/posting-</A> 
        &nbsp;</DIV>
        <DIV>&gt; &nbsp;guide.html</DIV>
        <DIV>&nbsp;</DIV></BLOCKQUOTE></TD></TR></TBODY></TABLE></DIV>
<DIV>&nbsp;</DIV>
<DIV>&nbsp;</DIV></BODY></HTML>

--=====003_Dragon527446281311_=====--


From gunter.berton at gene.com  Thu Jul 20 23:05:06 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 20 Jul 2006 14:05:06 -0700
Subject: [R] Job Openings: Nonclinical statistics Positions at Genentech,
	South San Francisco, CA
Message-ID: <004001c6ac40$2df54150$3d7f10ac@gne.windows.gene.com>


To all:

Genentech has immediate openings for two MS/PhD statisticians in its
preclinical/nonclinical statistics group in South San Francisco, CA. As the
name indicates, this group provides statistical services to all of
Genentech's R&D, manufacturing, and marketing activities **except** clinical
trials. In order not to abuse the good offices of this newslist, if you are
interested in more details about these positions, please send inquiries
directly to me. Please do **not** cc this list. 

Genentech is an equal opportunity employer.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box


From f.abian at gmx.net  Thu Jul 20 23:09:19 2006
From: f.abian at gmx.net (Fabian Scheipl)
Date: Thu, 20 Jul 2006 23:09:19 +0200
Subject: [R] Loss of numerical precision from conversion to list ?
Message-ID: <20060720210919.254440@gmx.net>


I?m working on an R-implementation of the simulation-based finite-sample null-distribution of (R)LR-Test in Mixed Models (i.e. testing for Var(RandomEffect)=0) derived by C. M. Crainiceanu and D. Ruppert.

I'm in the beginning stages of this project and while comparing quick and dirty grid-search-methods and more exact optim()/optimize()-based methods to find the maximum of a part of the RLR-Test-Statistic i stumbled upon the following problem:

It seems to me that R produces different results depending on whether originally identical numbers involved in the exact same computations are read from a matrix or a list.
(I need both in order to do quick vectorized computation for the grid-search with matrices and "list-based" computation so that i can put the function to be maximized in something like mapply(...,optim(foo),...)- I can elaborate if desired)

However, the problem goes away once a number involved in the computation is set from almost zero (e-15) to 4.
I'm completely mystified by this; especially since this number that I change is NOT one of the numbers that are switched from matrix to list.

Here's the code:

library(nlme)
data(Orthodont)    #108 dental measurements on 27 subjects
# m1<-lme(distance~age,random=~1|Subject,data=Orthodont)
# summary(m1)
# ...
# Random effects:
# Formula: ~1 | Subject
#          (Intercept) Residual
# StdDev:    2.114724 1.431592  -> lambda.REML=2.114^2/1.431^2 = 2.182382

#DesignMatrix for fixed Effects
X<-cbind(rep(1,108),Orthodont$age) 
#DesignMatrix of RandomEffects
Z<-matrix(data=c(rep(1,4),rep(0,108)),nrow=108,ncol=27) 

#Corr(RanEf)^0.5 = 27 x 27 Identity, since RandomIntercepts are independent
sqrt.Sigma<-diag(27) 

K<-27 #number of subjects/ random intercepts
n<-nrow(X)
p<-ncol(X)
lambda0 <- 2.182382 #actually not a sensible choice as Null-Hypothesis, but that doesn't pertain to the problem

#Projection-Matrix for Fixed-Effects-Model: Y -> errors
P0=diag(n)-X%*%solve((t(X)%*%X))%*%t(X) 

mu<-eigen(sqrt.Sigma%*%t(Z)%*%P0%*%Z%*%sqrt.Sigma)$values
# mu
# [1] 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00
#[11] 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00
#[21] 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 5.77316e-15
# ! Notice the last (27th) value very close to 0

nsim<-10
set.seed(10)
#nsim x K array of ChiSq(1)-variates
w.k.sq.mat<-matrix(rchisq(nsim*K,1),nrow=nsim) 
#nsim x 1 array of ChiSq(n-p-K)-variates
w.sum2<-rchisq(nsim,n-p-K)                     

### vectorized computation of nsim=10 realizations
### of a part of the RLR-statistic under the Null:
w.k.sq<- cbind(w.k.sq.mat,w.sum2)       #nsim x (K+1)
#vector-based results:
num.v<-  rowSums(((lambda-lambda0)*mu*w.k.sq[,-(K+1)])/(1+lambda*mu))
den.v<-  rowSums(((1+lambda0*mu)*w.k.sq[,-(K+1)]) / (1+lambda*mu)) + w.k.sq[,K+1]

### list-based computation of nsim=10 realizations
### of a part of the RLR-statistic under the Null:
w.k.sq<-list()
length(w.k.sq)<-nsim
#put the nsim rows into list-slots:
for(i in 1:nsim) w.k.sq[[i]]<-c(w.k.sq.mat[i,],w.sum2[i]) 
num.l<-numeric(0)
den.l<-numeric(0)
for(i in 1:nsim)
{
num.l[i]<-sum(((lambda-lambda0)*mu*w.k.sq[[i]][-(K+1)])/(1+lambda*mu))
#exactly analogous to num.v & den.v, except list-elements instead of vector
den.l[i]<-sum(((1+lambda0*mu)*w.k.sq[[i]][-(K+1)]) / (1+lambda*mu)) + w.k.sq[[i]][K+1]
}

#  Now the actual problem:
#  notice the discrepancies between the results from vectorized computation
#  and the results from list-based computation
#  Since discrepancies disappear if mu[27] is changed 
#  from 5.77316e-15 to 4, i'm guessing somewhere in the conversion to
#  "list" there must be a loss of precision or is there an entirely 
#  different problem?


num.l
# [1] -25.93322 -17.65486 -18.80239 -19.49974 ....
num.v
# [1] -23.84733 -17.62233 -27.22975 -19.50294 ....

den.l
# [1] 117.30246  92.59041  92.91491 112.90113 ...
den.v
# [1] 115.21657  92.55789 101.34228 112.90433 ...

#now i set
mu[27]<-4
#and reran the computation of num.l /.v and den.l /.v from above:

num.l
# [1] -26.25565 -17.67423 -27.47259 -20.97961 ...
num.v
# [1] -26.25565 -17.67423 -27.47259 -20.97961 ...
den.l
# [1] 117.62489  92.60979 101.58511 114.38100 ...
den.v
# [1] 117.62489  92.60979 101.58511 114.38100 ...

what i would like to know now is:

1) which of the two calculations yields a more precise result?
or rather:
2) how can i avoid these discrepancies in the future since i need to be able to compare these two methods? 
and, most importantly,
3) what in R.A.Fisher's name is happening here?

version information:

Version 2.3.1 (2006-06-01) 
i386-pc-mingw32 
.Machine$double.eps is 2.220446e-16 (does it matter?)

thanks for your time,



-- 
Fabian Scheipl

f.abian at gmx.net

"Feel free" ? 10 GB Mailbox, 100 FreeSMS/Monat ...


From ggrothendieck at gmail.com  Thu Jul 20 23:21:03 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 20 Jul 2006 17:21:03 -0400
Subject: [R] [Rd] How do I modify an exported function in a locked
	environment?
In-Reply-To: <0BE438149FF2254DB4199E2682C8DFEB01D554A2@crcmail1.BCCRC.CA>
References: <0BE438149FF2254DB4199E2682C8DFEB01D554A2@crcmail1.BCCRC.CA>
Message-ID: <971536df0607201421x5f510c9bn167a2842be1ef019@mail.gmail.com>

As others have mentioned its not really a good idea
to modify the namespace of a package and writing
a wrapper as Duncan suggested is much preferable.

An intermediate approach that
is not as good as the wrapper but better than modifying
the namespace is to copy the objects of interest
to your workspace, change their environments
appropriately and then modify their functionality:

library(zoo)

# copy objects of interest and set their environment

rollmean <- zoo:::rollmean
environment(rollmean) <- .GlobalEnv

rollmean.zoo <- zoo:::rollmean.zoo
environment(rollmean.zoo) <- .GlobalEnv

rollmean.default <- zoo:::rollmean.default
environment(rollmean.default) <- .GlobalEnv

# modify functionality

rollmean.default0 <- rollmean.default
rollmean.default <- function(x, ...) 100 * rollmean.default0(x, ...)

# test

rollmean(1:5, 3) # 100* used
rollmean(zoo(1:5), 3)  # 100* used


On 7/20/06, Steven McKinney <smckinney at bccrc.ca> wrote:
>
>
> Running R.app on Mac OS X 10.4
>
> > version
>               _
> platform       powerpc-apple-darwin8.6.0
> arch           powerpc
> os             darwin8.6.0
> system         powerpc, darwin8.6.0
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
> >
>
>
> I am trying to learn how to modify functions
> in a locked environment.
>
> For an example, suppose I'm using the package "zoo".
>
> zoo contains function "rollmean.default"
>
> > rollmean.default
> function (x, k, na.pad = FALSE, align = c("center", "left", "right"),
>    ...)
> {
>    x <- unclass(x)
>    n <- length(x)
>    y <- x[k:n] - x[c(1, 1:(n - k))]
>    y[1] <- sum(x[1:k])
>    rval <- cumsum(y)/k
>    if (na.pad) {
>        rval <- switch(match.arg(align), left = {
>            c(rval, rep(NA, k - 1))
>        }, center = {
>            c(rep(NA, floor((k - 1)/2)), rval, rep(NA, ceiling((k -
>                1)/2)))
>        }, right = {
>            c(rep(NA, k - 1), rval)
>        })
>    }
>    return(rval)
> }
> <environment: namespace:zoo>
>
> Suppose for whatever reason I want output to be
> in percent, so I'd like to modify the result to be
> rval <- 100 * cumsum(y)/k
>
> I cannot just copy the function and change it, as the namespace
> mechanism ensures the rollmean.default in 'zoo' continues to be used.
>
> If I use
> fixInNamespace("rollmean.default", ns = "zoo")
> I can edit the rval <- cumsum(y)/k line to read
> rval <- 100 * cumsum(y)/k
> save the file and exit the R.app GUI editor.
>
> But this does not update the exported copy of the
> function (the documentation for fixInNamespace says
> this is the case) - how do I accomplish this last step?
>
> If I list the function after editing, I see the original
> copy.  But if I reinvoke the editor via fixInNamespace(),
> I do see my modification.
> Where is my copy residing?  How do I push it out
> to replace the exported copy?
>
> Is this the proper way to modify a package function?
> Are there other ways?  I've searched webpages, R news,
> help files and have been unable to find out how to
> get this process fully completed.
>
> Any guidance appreciated.
>
>
> Steven McKinney
>
> Statistician
> Molecular Oncology and Breast Cancer Program
> British Columbia Cancer Research Centre
>
> email: smckinney at bccrc.ca
>
> tel: 604-675-8000 x7561
>
> BCCRC
> Molecular Oncology
> 675 West 10th Ave, Floor 4
> Vancouver B.C.
> V5Z 1L3
> Canada
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From grieve at u.washington.edu  Thu Jul 20 23:23:49 2006
From: grieve at u.washington.edu (grieve at u.washington.edu)
Date: Thu, 20 Jul 2006 14:23:49 -0700 (PDT)
Subject: [R] Using corStruct in nlme
In-Reply-To: <44BFDEA4.5000206@stats.uwo.ca>
Message-ID: <Pine.LNX.4.43.0607201423490.10140@hymn05.u.washington.edu>

Duncan, i could not find one seed which caused both corGaus and corExp to crash in R-patched. but, i found a seed that caused each to fail individually.
Thanks for your help.

# For corExp:

set.seed(26)
for(i in 1:length(CO2$conc)){
	CO2$conc[i]<-(CO2$conc[i]+rnorm(1))
}

fm1CO2.lis <- nlsList(SSasympOff, CO2)
fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2))
fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1)
CO2.nlme.var <- update(fm2CO2.nlme,
  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
  start = c(32.412, 0, 0, 0, -4.5603, 49.344), weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
  
CO2.nlme.exp<-update(CO2.nlme.var, correlation=corExp(form=~as.numeric(conc)|Plant,nugget=F), data=CO2) 

# For corGaus:

set.seed(4)
for(i in 1:length(CO2$conc)){
	CO2$conc[i]<-(CO2$conc[i]+rnorm(1))
}

fm1CO2.lis <- nlsList(SSasympOff, CO2)
fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2))
fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1)
CO2.nlme.var <- update(fm2CO2.nlme,
  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
  start = c(32.412, 0, 0, 0, -4.5603, 49.344), weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
  
CO2.nlme.gauss<-update(CO2.nlme.var, correlation=corGaus(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)


On Thu, 20 Jul 2006, Duncan Murdoch wrote:

> On 7/18/2006 2:28 PM, grieve at u.washington.edu wrote:
>> I am having trouble fitting correlation structures within nlme. I would 
>> like to fit corCAR1, corGaus and corExp correlation structures to my data. 
>> I either get the error "step halving reduced below minimum in pnls step" 
>> or alternatively R crashes.
>> 
>> My dataset is similar to the CO2 example in the nlme package. The one 
>> major difference is that in my case the 'conc' steps are not the same for 
>> each 'Plant'. I have replicated the problem using the CO2 data in nlme 
>> (based off of the Ch08.R script).
>> 
>> This works (when 'conc' is the same for each 'Plant':
>> 
>> (fm1CO2.lis <- nlsList(SSasympOff, CO2))
>> (fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2)))
>> (fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1))
>> CO2.nlme.var <- update(fm2CO2.nlme,
>>  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
>>  start = c(32.412, 0, 0, 0, -4.5603, 49.344), 
>> weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
>> 
>> CO2.nlme.CAR<-update(CO2.nlme.var, corr=corCAR1())
>> 
>> CO2.nlme.gauss<-update(CO2.nlme.var, 
>> correlation=corGaus(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)
>> 
>> CO2.nlme.exp<-update(CO2.nlme.var, 
>> correlation=corExp(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)  But, 
>> if i change each of the 'conc' numbers slightly so that they are no longer 
>> identical between subjects i can only get the corCAR1 correlation to work 
>> while R crashes for both corExp and corGaus:
>> 
>> for(i in 1:length(CO2$conc)){
>>     CO2$conc[i]<-(CO2$conc[i]+rnorm(1))
>> }
>> 
>> (fm1CO2.lis <- nlsList(SSasympOff, CO2))
>> (fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2)))
>> (fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1))
>> CO2.nlme.var <- update(fm2CO2.nlme,
>>  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
>>  start = c(32.412, 0, 0, 0, -4.5603, 49.344), 
>> weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
>> 
>> CO2.nlme.CAR<-update(CO2.nlme.var, corr=corCAR1())
>> 
>> CO2.nlme.gauss<-update(CO2.nlme.var, 
>> correlation=corGaus(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)
>> 
>> CO2.nlme.exp<-update(CO2.nlme.var, 
>> correlation=corExp(form=~as.numeric(conc)|Plant,nugget=F), data=CO2) I 
>> have read Pinheiro & Bates (2000) and i think that it should be possible 
>> to fit these correlation structures to my data, but maybe i am mistaken.
>> 
>> I am running R 2.3.1 and have recently updated all packages.
> 
> I reproduced this once in R-patched, but since then have been unable to do 
> so. I can reproduce it reliably with "set.seed(1)" at the start in R 2.3.1. 
> So it looks to me as though we've probably done something to make the error 
> less likely, but not completely fixed it.
> 
> If you can find a script (including set.seed() to some value) that reliably 
> causes a crash in R-patched, could you let me know?
> 
> You can get R-patched from CRAN in the bin/windows/base directory.
> 
> Duncan Murdoch
>


From murdoch at stats.uwo.ca  Fri Jul 21 01:07:05 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 20 Jul 2006 19:07:05 -0400
Subject: [R] Loss of numerical precision from conversion to list ?
In-Reply-To: <20060720210919.254440@gmx.net>
References: <20060720210919.254440@gmx.net>
Message-ID: <44C00C99.9080408@stats.uwo.ca>

R tries to use the maximum precision (64 bit mantissa) in the floating 
point registers when it can.  When it stores results to memory, they are 
stored in double precision (53 bit mantissa).  There's unlikely to be 
anything specific about conversion to a list that lost the precision, 
but I'm guessing one version of your code stored things to memory, 
whereas the other kept intermediate results in registers.

Using the maximum precision is a somewhat controversial choice:  there's 
an argument that it's best to get consistent results, even if they're 
wrong.  R has chosen to try to do the best it can, even if it means 
sometimes it is inconsistent.

Another argument is that algorithms that depend strongly on values 
beyond the first 53 bits are very unstable, and should be replaced by 
more stable ones that don't inflate small errors.

Or perhaps your problem has nothing to do with this; I didn't really 
look at it in detail.

Duncan Murdoch



On 7/20/2006 5:09 PM, Fabian Scheipl wrote:
> I?m working on an R-implementation of the simulation-based finite-sample null-distribution of (R)LR-Test in Mixed Models (i.e. testing for Var(RandomEffect)=0) derived by C. M. Crainiceanu and D. Ruppert.
> 
> I'm in the beginning stages of this project and while comparing quick and dirty grid-search-methods and more exact optim()/optimize()-based methods to find the maximum of a part of the RLR-Test-Statistic i stumbled upon the following problem:
> 
> It seems to me that R produces different results depending on whether originally identical numbers involved in the exact same computations are read from a matrix or a list.
> (I need both in order to do quick vectorized computation for the grid-search with matrices and "list-based" computation so that i can put the function to be maximized in something like mapply(...,optim(foo),...)- I can elaborate if desired)
> 
> However, the problem goes away once a number involved in the computation is set from almost zero (e-15) to 4.
> I'm completely mystified by this; especially since this number that I change is NOT one of the numbers that are switched from matrix to list.
> 
> Here's the code:
> 
> library(nlme)
> data(Orthodont)    #108 dental measurements on 27 subjects
> # m1<-lme(distance~age,random=~1|Subject,data=Orthodont)
> # summary(m1)
> # ...
> # Random effects:
> # Formula: ~1 | Subject
> #          (Intercept) Residual
> # StdDev:    2.114724 1.431592  -> lambda.REML=2.114^2/1.431^2 = 2.182382
> 
> #DesignMatrix for fixed Effects
> X<-cbind(rep(1,108),Orthodont$age) 
> #DesignMatrix of RandomEffects
> Z<-matrix(data=c(rep(1,4),rep(0,108)),nrow=108,ncol=27) 
> 
> #Corr(RanEf)^0.5 = 27 x 27 Identity, since RandomIntercepts are independent
> sqrt.Sigma<-diag(27) 
> 
> K<-27 #number of subjects/ random intercepts
> n<-nrow(X)
> p<-ncol(X)
> lambda0 <- 2.182382 #actually not a sensible choice as Null-Hypothesis, but that doesn't pertain to the problem
> 
> #Projection-Matrix for Fixed-Effects-Model: Y -> errors
> P0=diag(n)-X%*%solve((t(X)%*%X))%*%t(X) 
> 
> mu<-eigen(sqrt.Sigma%*%t(Z)%*%P0%*%Z%*%sqrt.Sigma)$values
> # mu
> # [1] 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00
> #[11] 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00
> #[21] 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 4.00000e+00 5.77316e-15
> # ! Notice the last (27th) value very close to 0
> 
> nsim<-10
> set.seed(10)
> #nsim x K array of ChiSq(1)-variates
> w.k.sq.mat<-matrix(rchisq(nsim*K,1),nrow=nsim) 
> #nsim x 1 array of ChiSq(n-p-K)-variates
> w.sum2<-rchisq(nsim,n-p-K)                     
> 
> ### vectorized computation of nsim=10 realizations
> ### of a part of the RLR-statistic under the Null:
> w.k.sq<- cbind(w.k.sq.mat,w.sum2)       #nsim x (K+1)
> #vector-based results:
> num.v<-  rowSums(((lambda-lambda0)*mu*w.k.sq[,-(K+1)])/(1+lambda*mu))
> den.v<-  rowSums(((1+lambda0*mu)*w.k.sq[,-(K+1)]) / (1+lambda*mu)) + w.k.sq[,K+1]
> 
> ### list-based computation of nsim=10 realizations
> ### of a part of the RLR-statistic under the Null:
> w.k.sq<-list()
> length(w.k.sq)<-nsim
> #put the nsim rows into list-slots:
> for(i in 1:nsim) w.k.sq[[i]]<-c(w.k.sq.mat[i,],w.sum2[i]) 
> num.l<-numeric(0)
> den.l<-numeric(0)
> for(i in 1:nsim)
> {
> num.l[i]<-sum(((lambda-lambda0)*mu*w.k.sq[[i]][-(K+1)])/(1+lambda*mu))
> #exactly analogous to num.v & den.v, except list-elements instead of vector
> den.l[i]<-sum(((1+lambda0*mu)*w.k.sq[[i]][-(K+1)]) / (1+lambda*mu)) + w.k.sq[[i]][K+1]
> }
> 
> #  Now the actual problem:
> #  notice the discrepancies between the results from vectorized computation
> #  and the results from list-based computation
> #  Since discrepancies disappear if mu[27] is changed 
> #  from 5.77316e-15 to 4, i'm guessing somewhere in the conversion to
> #  "list" there must be a loss of precision or is there an entirely 
> #  different problem?
> 
> 
> num.l
> # [1] -25.93322 -17.65486 -18.80239 -19.49974 ....
> num.v
> # [1] -23.84733 -17.62233 -27.22975 -19.50294 ....
> 
> den.l
> # [1] 117.30246  92.59041  92.91491 112.90113 ...
> den.v
> # [1] 115.21657  92.55789 101.34228 112.90433 ...
> 
> #now i set
> mu[27]<-4
> #and reran the computation of num.l /.v and den.l /.v from above:
> 
> num.l
> # [1] -26.25565 -17.67423 -27.47259 -20.97961 ...
> num.v
> # [1] -26.25565 -17.67423 -27.47259 -20.97961 ...
> den.l
> # [1] 117.62489  92.60979 101.58511 114.38100 ...
> den.v
> # [1] 117.62489  92.60979 101.58511 114.38100 ...
> 
> what i would like to know now is:
> 
> 1) which of the two calculations yields a more precise result?
> or rather:
> 2) how can i avoid these discrepancies in the future since i need to be able to compare these two methods? 
> and, most importantly,
> 3) what in R.A.Fisher's name is happening here?
> 
> version information:
> 
> Version 2.3.1 (2006-06-01) 
> i386-pc-mingw32 
> .Machine$double.eps is 2.220446e-16 (does it matter?)
> 
> thanks for your time,
> 
> 
>


From tlumley at u.washington.edu  Fri Jul 21 01:16:15 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 20 Jul 2006 16:16:15 -0700 (PDT)
Subject: [R] Using corStruct in nlme
In-Reply-To: <Pine.LNX.4.43.0607201423490.10140@hymn05.u.washington.edu>
References: <Pine.LNX.4.43.0607201423490.10140@hymn05.u.washington.edu>
Message-ID: <Pine.LNX.4.64.0607201612420.5650@homer24.u.washington.edu>

On Thu, 20 Jul 2006, grieve at u.washington.edu wrote:

> Duncan, i could not find one seed which caused both corGaus and corExp 
> to crash in R-patched. but, i found a seed that caused each to fail 
> individually. Thanks for your help.


Running these under Valgrind they both show the same problem

==10878== Invalid read of size 4
==10878==    at 0x1CCDB9EB: mult_mat (matrix.c:84)
==10878==    by 0x1CCD81B9: corStruct_recalc (corStruct.c:72)
==10878==    by 0x1CCDC06E: nlme_wtCorrAdj (nlme.c:152)
==10878==    by 0x1CCDC4F7: fit_nlme (nlme.c:347)
==10878==    by 0x80A518E: do_dotCode (dotcode.c:1777)
==10878==    by 0x80C45A9: Rf_eval (eval.c:444)
==10878==    by 0x80C600A: do_set (eval.c:1340)
==10878==    by 0x80C44A2: Rf_eval (eval.c:427)
==10878==    by 0x80C6097: do_begin (eval.c:1104)
==10878==    by 0x80C44A2: Rf_eval (eval.c:427)
==10878==    by 0x80C61AD: do_repeat (eval.c:1066)
==10878==    by 0x80C44A2: Rf_eval (eval.c:427)
==10878==  Address 0x1D4A60DC is 4 bytes after a block of size 6048 alloc'd
==10878==    at 0x1B909B71: calloc (vg_replace_malloc.c:175)
==10878==    by 0x80E09D5: R_chk_calloc (memory.c:2315)
==10878==    by 0x1CCDC415: fit_nlme (nlme.c:113)
==10878==    by 0x80A518E: do_dotCode (dotcode.c:1777)
==10878==    by 0x80C45A9: Rf_eval (eval.c:444)
==10878==    by 0x80C600A: do_set (eval.c:1340)
==10878==    by 0x80C44A2: Rf_eval (eval.c:427)
==10878==    by 0x80C6097: do_begin (eval.c:1104)
==10878==    by 0x80C44A2: Rf_eval (eval.c:427)
==10878==    by 0x80C61AD: do_repeat (eval.c:1066)
==10878==    by 0x80C44A2: Rf_eval (eval.c:427)
==10878==    by 0x80C6097: do_begin (eval.c:1104)


They also both go on to crash so hard that Valgrind crashes and says

--9895-- INTERNAL ERROR: Valgrind received a signal 11 (SIGSEGV) - exiting
--9895-- si_code=1 Fault EIP: 0xB00313AD; Faulting address: 0x3C439F95
--9895--   esp=0xB0653E48
valgrind: the `impossible' happened:
    Killed by fatal signal


Ouch.

 	-thomas



> # For corExp:
>
> set.seed(26)
> for(i in 1:length(CO2$conc)){
> 	CO2$conc[i]<-(CO2$conc[i]+rnorm(1))
> }
>
> fm1CO2.lis <- nlsList(SSasympOff, CO2)
> fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2))
> fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1)
> CO2.nlme.var <- update(fm2CO2.nlme,
>  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
>  start = c(32.412, 0, 0, 0, -4.5603, 49.344), weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
>
> CO2.nlme.exp<-update(CO2.nlme.var, correlation=corExp(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)
>
> # For corGaus:
>
> set.seed(4)
> for(i in 1:length(CO2$conc)){
> 	CO2$conc[i]<-(CO2$conc[i]+rnorm(1))
> }
>
> fm1CO2.lis <- nlsList(SSasympOff, CO2)
> fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2))
> fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1)
> CO2.nlme.var <- update(fm2CO2.nlme,
>  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
>  start = c(32.412, 0, 0, 0, -4.5603, 49.344), weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
>
> CO2.nlme.gauss<-update(CO2.nlme.var, correlation=corGaus(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)
>
>
> On Thu, 20 Jul 2006, Duncan Murdoch wrote:
>
>> On 7/18/2006 2:28 PM, grieve at u.washington.edu wrote:
>>> I am having trouble fitting correlation structures within nlme. I would
>>> like to fit corCAR1, corGaus and corExp correlation structures to my data.
>>> I either get the error "step halving reduced below minimum in pnls step"
>>> or alternatively R crashes.
>>>
>>> My dataset is similar to the CO2 example in the nlme package. The one
>>> major difference is that in my case the 'conc' steps are not the same for
>>> each 'Plant'. I have replicated the problem using the CO2 data in nlme
>>> (based off of the Ch08.R script).
>>>
>>> This works (when 'conc' is the same for each 'Plant':
>>>
>>> (fm1CO2.lis <- nlsList(SSasympOff, CO2))
>>> (fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2)))
>>> (fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1))
>>> CO2.nlme.var <- update(fm2CO2.nlme,
>>>  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
>>>  start = c(32.412, 0, 0, 0, -4.5603, 49.344),
>>> weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
>>>
>>> CO2.nlme.CAR<-update(CO2.nlme.var, corr=corCAR1())
>>>
>>> CO2.nlme.gauss<-update(CO2.nlme.var,
>>> correlation=corGaus(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)
>>>
>>> CO2.nlme.exp<-update(CO2.nlme.var,
>>> correlation=corExp(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)  But,
>>> if i change each of the 'conc' numbers slightly so that they are no longer
>>> identical between subjects i can only get the corCAR1 correlation to work
>>> while R crashes for both corExp and corGaus:
>>>
>>> for(i in 1:length(CO2$conc)){
>>>     CO2$conc[i]<-(CO2$conc[i]+rnorm(1))
>>> }
>>>
>>> (fm1CO2.lis <- nlsList(SSasympOff, CO2))
>>> (fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2)))
>>> (fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1))
>>> CO2.nlme.var <- update(fm2CO2.nlme,
>>>  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
>>>  start = c(32.412, 0, 0, 0, -4.5603, 49.344),
>>> weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
>>>
>>> CO2.nlme.CAR<-update(CO2.nlme.var, corr=corCAR1())
>>>
>>> CO2.nlme.gauss<-update(CO2.nlme.var,
>>> correlation=corGaus(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)
>>>
>>> CO2.nlme.exp<-update(CO2.nlme.var,
>>> correlation=corExp(form=~as.numeric(conc)|Plant,nugget=F), data=CO2) I
>>> have read Pinheiro & Bates (2000) and i think that it should be possible
>>> to fit these correlation structures to my data, but maybe i am mistaken.
>>>
>>> I am running R 2.3.1 and have recently updated all packages.
>>
>> I reproduced this once in R-patched, but since then have been unable to do
>> so. I can reproduce it reliably with "set.seed(1)" at the start in R 2.3.1.
>> So it looks to me as though we've probably done something to make the error
>> less likely, but not completely fixed it.
>>
>> If you can find a script (including set.seed() to some value) that reliably
>> causes a crash in R-patched, could you let me know?
>>
>> You can get R-patched from CRAN in the bin/windows/base directory.
>>
>> Duncan Murdoch
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From murdoch at stats.uwo.ca  Fri Jul 21 02:19:53 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 20 Jul 2006 20:19:53 -0400
Subject: [R] Using corStruct in nlme
In-Reply-To: <Pine.LNX.4.64.0607201612420.5650@homer24.u.washington.edu>
References: <Pine.LNX.4.43.0607201423490.10140@hymn05.u.washington.edu>
	<Pine.LNX.4.64.0607201612420.5650@homer24.u.washington.edu>
Message-ID: <44C01DA9.9060907@stats.uwo.ca>

On 7/20/2006 7:16 PM, Thomas Lumley wrote:
> On Thu, 20 Jul 2006, grieve at u.washington.edu wrote:
> 
>> Duncan, i could not find one seed which caused both corGaus and corExp 
>> to crash in R-patched. but, i found a seed that caused each to fail 
>> individually. Thanks for your help.
> 
> 
> Running these under Valgrind they both show the same problem
> 
> ==10878== Invalid read of size 4
> ==10878==    at 0x1CCDB9EB: mult_mat (matrix.c:84)
> ==10878==    by 0x1CCD81B9: corStruct_recalc (corStruct.c:72)
> ==10878==    by 0x1CCDC06E: nlme_wtCorrAdj (nlme.c:152)
> ==10878==    by 0x1CCDC4F7: fit_nlme (nlme.c:347)
> ==10878==    by 0x80A518E: do_dotCode (dotcode.c:1777)
> ==10878==    by 0x80C45A9: Rf_eval (eval.c:444)
> ==10878==    by 0x80C600A: do_set (eval.c:1340)
> ==10878==    by 0x80C44A2: Rf_eval (eval.c:427)
> ==10878==    by 0x80C6097: do_begin (eval.c:1104)
> ==10878==    by 0x80C44A2: Rf_eval (eval.c:427)
> ==10878==    by 0x80C61AD: do_repeat (eval.c:1066)
> ==10878==    by 0x80C44A2: Rf_eval (eval.c:427)
> ==10878==  Address 0x1D4A60DC is 4 bytes after a block of size 6048 alloc'd
> ==10878==    at 0x1B909B71: calloc (vg_replace_malloc.c:175)
> ==10878==    by 0x80E09D5: R_chk_calloc (memory.c:2315)
> ==10878==    by 0x1CCDC415: fit_nlme (nlme.c:113)
> ==10878==    by 0x80A518E: do_dotCode (dotcode.c:1777)
> ==10878==    by 0x80C45A9: Rf_eval (eval.c:444)
> ==10878==    by 0x80C600A: do_set (eval.c:1340)
> ==10878==    by 0x80C44A2: Rf_eval (eval.c:427)
> ==10878==    by 0x80C6097: do_begin (eval.c:1104)
> ==10878==    by 0x80C44A2: Rf_eval (eval.c:427)
> ==10878==    by 0x80C61AD: do_repeat (eval.c:1066)
> ==10878==    by 0x80C44A2: Rf_eval (eval.c:427)
> ==10878==    by 0x80C6097: do_begin (eval.c:1104)
> 
> 
> They also both go on to crash so hard that Valgrind crashes and says
> 
> --9895-- INTERNAL ERROR: Valgrind received a signal 11 (SIGSEGV) - exiting
> --9895-- si_code=1 Fault EIP: 0xB00313AD; Faulting address: 0x3C439F95
> --9895--   esp=0xB0653E48
> valgrind: the `impossible' happened:
>     Killed by fatal signal
> 
> 
> Ouch.

So that looks like an nlme problem, but there's nothing obviously wrong 
in the code.  Doug, can you spot what the problem might be?

Duncan Murdoch
> 
>  	-thomas
> 
> 
> 
>> # For corExp:
>>
>> set.seed(26)
>> for(i in 1:length(CO2$conc)){
>> 	CO2$conc[i]<-(CO2$conc[i]+rnorm(1))
>> }
>>
>> fm1CO2.lis <- nlsList(SSasympOff, CO2)
>> fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2))
>> fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1)
>> CO2.nlme.var <- update(fm2CO2.nlme,
>>  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
>>  start = c(32.412, 0, 0, 0, -4.5603, 49.344), weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
>>
>> CO2.nlme.exp<-update(CO2.nlme.var, correlation=corExp(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)
>>
>> # For corGaus:
>>
>> set.seed(4)
>> for(i in 1:length(CO2$conc)){
>> 	CO2$conc[i]<-(CO2$conc[i]+rnorm(1))
>> }
>>
>> fm1CO2.lis <- nlsList(SSasympOff, CO2)
>> fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2))
>> fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1)
>> CO2.nlme.var <- update(fm2CO2.nlme,
>>  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
>>  start = c(32.412, 0, 0, 0, -4.5603, 49.344), weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
>>
>> CO2.nlme.gauss<-update(CO2.nlme.var, correlation=corGaus(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)
>>
>>
>> On Thu, 20 Jul 2006, Duncan Murdoch wrote:
>>
>>> On 7/18/2006 2:28 PM, grieve at u.washington.edu wrote:
>>>> I am having trouble fitting correlation structures within nlme. I would
>>>> like to fit corCAR1, corGaus and corExp correlation structures to my data.
>>>> I either get the error "step halving reduced below minimum in pnls step"
>>>> or alternatively R crashes.
>>>>
>>>> My dataset is similar to the CO2 example in the nlme package. The one
>>>> major difference is that in my case the 'conc' steps are not the same for
>>>> each 'Plant'. I have replicated the problem using the CO2 data in nlme
>>>> (based off of the Ch08.R script).
>>>>
>>>> This works (when 'conc' is the same for each 'Plant':
>>>>
>>>> (fm1CO2.lis <- nlsList(SSasympOff, CO2))
>>>> (fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2)))
>>>> (fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1))
>>>> CO2.nlme.var <- update(fm2CO2.nlme,
>>>>  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
>>>>  start = c(32.412, 0, 0, 0, -4.5603, 49.344),
>>>> weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
>>>>
>>>> CO2.nlme.CAR<-update(CO2.nlme.var, corr=corCAR1())
>>>>
>>>> CO2.nlme.gauss<-update(CO2.nlme.var,
>>>> correlation=corGaus(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)
>>>>
>>>> CO2.nlme.exp<-update(CO2.nlme.var,
>>>> correlation=corExp(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)  But,
>>>> if i change each of the 'conc' numbers slightly so that they are no longer
>>>> identical between subjects i can only get the corCAR1 correlation to work
>>>> while R crashes for both corExp and corGaus:
>>>>
>>>> for(i in 1:length(CO2$conc)){
>>>>     CO2$conc[i]<-(CO2$conc[i]+rnorm(1))
>>>> }
>>>>
>>>> (fm1CO2.lis <- nlsList(SSasympOff, CO2))
>>>> (fm1CO2.nlme <- nlme(fm1CO2.lis, control = list(tolerance = 1e-2)))
>>>> (fm2CO2.nlme <- update(fm1CO2.nlme, random = Asym + lrc ~ 1))
>>>> CO2.nlme.var <- update(fm2CO2.nlme,
>>>>  fixed = list(Asym ~ Type * Treatment, lrc + c0 ~ 1),
>>>>  start = c(32.412, 0, 0, 0, -4.5603, 49.344),
>>>> weights=varConstPower(fixed=list(const=0.1, power=1)), verbose=T)
>>>>
>>>> CO2.nlme.CAR<-update(CO2.nlme.var, corr=corCAR1())
>>>>
>>>> CO2.nlme.gauss<-update(CO2.nlme.var,
>>>> correlation=corGaus(form=~as.numeric(conc)|Plant,nugget=F), data=CO2)
>>>>
>>>> CO2.nlme.exp<-update(CO2.nlme.var,
>>>> correlation=corExp(form=~as.numeric(conc)|Plant,nugget=F), data=CO2) I
>>>> have read Pinheiro & Bates (2000) and i think that it should be possible
>>>> to fit these correlation structures to my data, but maybe i am mistaken.
>>>>
>>>> I am running R 2.3.1 and have recently updated all packages.
>>> I reproduced this once in R-patched, but since then have been unable to do
>>> so. I can reproduce it reliably with "set.seed(1)" at the start in R 2.3.1.
>>> So it looks to me as though we've probably done something to make the error
>>> less likely, but not completely fixed it.
>>>
>>> If you can find a script (including set.seed() to some value) that reliably
>>> causes a crash in R-patched, could you let me know?
>>>
>>> You can get R-patched from CRAN in the bin/windows/base directory.
>>>
>>> Duncan Murdoch
>>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle


From bruno.giordano at music.mcgill.ca  Fri Jul 21 02:30:34 2006
From: bruno.giordano at music.mcgill.ca (Bruno L. Giordano)
Date: Thu, 20 Jul 2006 20:30:34 -0400
Subject: [R]  Order-restricted inference
Message-ID: <012701c6ac5c$e5007920$6400a8c0@brungio>

Hello,
I looked for R packages which focused on order-restricted statistical 
inference, but I could find only the isoreg() function.

I would need to test whether the means in my (repeated measures) data follow 
a given order, e.g. A<B=C<D.

I took a look at the monograph by Barlow et al. (1972) on this topic and 
found that for my case the null hypothesis is always A=B=C=D. This might be 
a naive question, but wouldn't it be more appropriate to use "at least one 
pair of means does not follow the alternative hypothesis order", or to use 
this latter hypothesis as the alternative and A<B=C<D as the null?

If R packages are not found for my purpose, can somebody please point me to 
some recent monographs on order-restricted inference?

Thank you,
    Bruno


From ng296 at cam.ac.uk  Fri Jul 21 04:22:59 2006
From: ng296 at cam.ac.uk (N. Goodacre)
Date: 21 Jul 2006 03:22:59 +0100
Subject: [R] Q. regarding optim()
Message-ID: <Prayer.1.0.17.0607210322590.9902@hermes-2.csi.cam.ac.uk>

Dear R mailing group,

  The second parameter for the function optim()is a function whose 
parameters are to be optimized. The description of this function given in 
the help file is the following:

      fn: A function to be minimized (or maximized), with first
          argument the vector of parameters over which minimization is
          to take place.  It should return a scalar result.

Let's say the second argument is x, a vector of x values

I would have thought that fn should return a vector full of y values for 
the x values entered as the second argument. If the function just takes one 
value at a time and outputs a scalar, how can I specify for fn which x 
value, of the vector x, to take?

 Sincerely,

Norman Goodacre


From maj at waikato.ac.nz  Fri Jul 21 07:19:33 2006
From: maj at waikato.ac.nz (Murray Jorgensen)
Date: Fri, 21 Jul 2006 17:19:33 +1200
Subject: [R] Parameterization puzzle
Message-ID: <44C063E5.3020703@waikato.ac.nz>

Consider the following example (based on an example in Pat Altham's GLM 
notes)

pyears <- scan()
18793 52407 10673 43248 5710 28612 2585 12663 1462 5317

deaths <- scan()
2 32 12 104 28 206 28 186 31 102

Smoke <- gl(2,1,10,labels=c("No","Yes"))
Age <- gl(5,2,10,labels=c("35-44","45-54","55-64","65-74","75-84"),
            ordered=TRUE)
mod1.glm <- glm(deaths ~ Age * Smoke + offset(l),family=poisson)
summary(mod1.glm)
age <- as.numeric(Age)
mod2.glm <- aso1.glm <- glm(deaths ~ poly(age,2) + Smoke +
                       poly(age,1):Smoke + offset(l),family=poisson)
summary(mod2.glm)



The business part of the summary for the first model

                Estimate Std. Error z value Pr(>|z|)
(Intercept)    -5.92706    0.16577 -35.754  < 2e-16 ***
Age.L           4.06490    0.47414   8.573  < 2e-16 ***
Age.Q          -1.08293    0.41326  -2.620 0.008781 **
Age.C           0.24158    0.31756   0.761 0.446816
Age^4           0.04244    0.23061   0.184 0.853986
SmokeYes        0.61916    0.17296   3.580 0.000344 ***
Age.L:SmokeYes -1.31234    0.49267  -2.664 0.007729 **
Age.Q:SmokeYes  0.39043    0.43008   0.908 0.363976
Age.C:SmokeYes -0.29593    0.33309  -0.888 0.374298
Age^4:SmokeYes -0.03682    0.24432  -0.151 0.880218

inspires me to fit the second model that omits the nonsignificant terms, 
however this produces the summary

                       Estimate Std. Error z value Pr(>|z|)
(Intercept)            -5.8368     0.1213 -48.103  < 2e-16 ***
poly(age, 2)1           3.9483     0.1755  22.497  < 2e-16 ***
poly(age, 2)2          -1.0460     0.1448  -7.223 5.08e-13 ***
SmokeYes                0.5183     0.1262   4.106 4.02e-05 ***
SmokeNo:poly(age, 1)    1.3755     0.4340   3.169  0.00153 **
SmokeYes:poly(age, 1)       NA         NA      NA       NA

Why do we get a SmokeNo:poly(age, 1) term? Can I re-express mod2.glm so 
that this term does not appear?

Cheers,  Murray Jorgensen

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862


From ripley at stats.ox.ac.uk  Fri Jul 21 08:12:59 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 21 Jul 2006 07:12:59 +0100 (BST)
Subject: [R] Q. regarding optim()
In-Reply-To: <Prayer.1.0.17.0607210322590.9902@hermes-2.csi.cam.ac.uk>
References: <Prayer.1.0.17.0607210322590.9902@hermes-2.csi.cam.ac.uk>
Message-ID: <Pine.LNX.4.64.0607210709300.12611@gannet.stats.ox.ac.uk>

On Fri, 21 Jul 2006, N. Goodacre wrote:

Dear R mailing group,
> 
>   The second parameter for the function optim()is a function whose 
> parameters are to be optimized. The description of this function given in 
> the help file is the following:
> 
>       fn: A function to be minimized (or maximized), with first
>           argument the vector of parameters over which minimization is
>           to take place.  It should return a scalar result.
> 
> Let's say the second argument is x, a vector of x values

But it says `parameters'.  Please look at the examples on that help page.  
optim is concerned with optimizing functions of more than one parameter. 
In a statistical setting 'x' may be the data, but e.g. for fitting a gamma 
distribution c(scale, shape) are the parameters.

> I would have thought that fn should return a vector full of y values for 
> the x values entered as the second argument. If the function just takes one 
> value at a time and outputs a scalar, how can I specify for fn which x 
> value, of the vector x, to take?
> 
>  Sincerely,
> 
> Norman Goodacre
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Jul 21 08:20:00 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 21 Jul 2006 07:20:00 +0100 (BST)
Subject: [R] Parameterization puzzle
In-Reply-To: <44C063E5.3020703@waikato.ac.nz>
References: <44C063E5.3020703@waikato.ac.nz>
Message-ID: <Pine.LNX.4.64.0607210716270.12611@gannet.stats.ox.ac.uk>

R does not know that poly(age,2) and poly(age,1) are linearly dependent.
(And indeed they only are for some functions 'poly'.)

I cannot reproduce your example ('l' is missing), but perhaps

glm(deaths ~ poly(age,2) + poly(age,1)*Smoke + offset(l), poisson)

was your intention?

On Fri, 21 Jul 2006, Murray Jorgensen wrote:

> Consider the following example (based on an example in Pat Altham's GLM 
> notes)
> 
> pyears <- scan()
> 18793 52407 10673 43248 5710 28612 2585 12663 1462 5317
> 
> deaths <- scan()
> 2 32 12 104 28 206 28 186 31 102
> 
> Smoke <- gl(2,1,10,labels=c("No","Yes"))
> Age <- gl(5,2,10,labels=c("35-44","45-54","55-64","65-74","75-84"),
>             ordered=TRUE)
> mod1.glm <- glm(deaths ~ Age * Smoke + offset(l),family=poisson)
> summary(mod1.glm)
> age <- as.numeric(Age)
> mod2.glm <- aso1.glm <- glm(deaths ~ poly(age,2) + Smoke +
>                        poly(age,1):Smoke + offset(l),family=poisson)
> summary(mod2.glm)
> 
> 
> 
> The business part of the summary for the first model
> 
>                 Estimate Std. Error z value Pr(>|z|)
> (Intercept)    -5.92706    0.16577 -35.754  < 2e-16 ***
> Age.L           4.06490    0.47414   8.573  < 2e-16 ***
> Age.Q          -1.08293    0.41326  -2.620 0.008781 **
> Age.C           0.24158    0.31756   0.761 0.446816
> Age^4           0.04244    0.23061   0.184 0.853986
> SmokeYes        0.61916    0.17296   3.580 0.000344 ***
> Age.L:SmokeYes -1.31234    0.49267  -2.664 0.007729 **
> Age.Q:SmokeYes  0.39043    0.43008   0.908 0.363976
> Age.C:SmokeYes -0.29593    0.33309  -0.888 0.374298
> Age^4:SmokeYes -0.03682    0.24432  -0.151 0.880218
> 
> inspires me to fit the second model that omits the nonsignificant terms, 
> however this produces the summary
> 
>                        Estimate Std. Error z value Pr(>|z|)
> (Intercept)            -5.8368     0.1213 -48.103  < 2e-16 ***
> poly(age, 2)1           3.9483     0.1755  22.497  < 2e-16 ***
> poly(age, 2)2          -1.0460     0.1448  -7.223 5.08e-13 ***
> SmokeYes                0.5183     0.1262   4.106 4.02e-05 ***
> SmokeNo:poly(age, 1)    1.3755     0.4340   3.169  0.00153 **
> SmokeYes:poly(age, 1)       NA         NA      NA       NA
> 
> Why do we get a SmokeNo:poly(age, 1) term? Can I re-express mod2.glm so 
> that this term does not appear?
> 
> Cheers,  Murray Jorgensen
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From gunther.hoening at ukmainz.de  Fri Jul 21 09:33:56 2006
From: gunther.hoening at ukmainz.de (=?iso-8859-1?Q?Gunther_H=F6ning?=)
Date: Fri, 21 Jul 2006 09:33:56 +0200
Subject: [R] Merge two dataframes of different column length and row length
	by two columns at a time
Message-ID: <001701c6ac98$0616c5c0$0f1e0b0a@3med.klinik.unimainz.de>


Hello,

I have two dataframes of the following structures:

str(a)
`data.frame':   1354896 obs. of  14 variables:
 $ V1 : int  0 1 2 3 4 5 6 7 8 9 ...
 $ V2 : int  0 0 0 0 0 0 0 0 0 0 ...
 $ V3 : int  74 12305 103 12337 46 57 12446 90 12097 79 ...
 $ V4 : num    11.8 1529.2   17.8 1579.4    6.7 ...
 $ V5 : int  88 11040 104 11557 56 58 11040 74 10991 81 ...
 $ V6 : num    15.5 1921.3   20.3 1888.2   12.6 ...
 $ V7 : int  87 8793 90 10142 67 64 9264 73 8589 71 ...
 $ V8 : num    16.0 1750.6   15.2 1783.7   11.0 ...
 $ V9 : int  68 11279 93 11831 43 61 11234 82 10919 76 ...
 $ V10: num    11.5 1999.5   39.0 1842.2    5.0 ...
 $ V11: int  110 12456 92 12063 60 59 12393 82 11831 77 ...
 $ V12: num    21.4 1974.7   33.9 1689.9   10.6 ...
 $ V13: int  81 10887 101 10874 62 74 11115 79 10789 93 ...
 $ V14: num    19.5 1812.3   31.7 1524.1   11.9 ...
> str(b)
`data.frame':   1213901 obs. of  4 variables:
 $ V1: int  0 1 2 3 5 6 7 8 9 10 ...
 $ V2: int  0 0 0 0 0 0 0 0 0 0 ...
 $ V3: Factor w/ 54676 levels "str23","str53",..: 54676 54676 54676 54676
54676 54676 54676 54676 54676 54676 ...
 $ V4: Factor w/ 3 levels "Match","NoMatch",..: 2 2 2 2 2 2 2 2 2 2 ...
> 

I want to merge these dataframes by V1 and V2 of a and b. The combination of
V1, V2 is a unique key.
Note that b is smaller than a.

Any suggestions to solve this problem ?





Gunther H?ning

Diplom Physiker
Bioinformatiker

Langenbeckstra?e1
55131 Mainz

gunther.hoening at ukmainz.de


From adsl665400 at tiscali.nl  Fri Jul 21 09:38:57 2006
From: adsl665400 at tiscali.nl (Marjo en Edwin)
Date: Fri, 21 Jul 2006 09:38:57 +0200
Subject: [R] insert insertRow?
Message-ID: <44C08491.9090604@tiscali.nl>

Dear all,

In the search for a command to insert a row between other rows in a data 
frame I found that there seems to be no such command in the base R 
package. There is however a very simple function insertRow in the 
micEcon package, that makes use of rbind. I wondered if it would not be 
possible to include the following micEcon functions in the base package:

insertRow
insertCol

Since the functions are already there, I would gather this is not a very 
big effort. It would save people that just want to insert rows and 
columns easily to install another two packages (since micEcon needs 
systemfit) or defining the functions themselves.

I hope my suggestion will be taken into consideration. Whether it will 
be adopted or not, I still think R is a fantastic statistical package 
that I love to use.

Greetings,
Edwin Commandeur


From gavin.simpson at ucl.ac.uk  Fri Jul 21 09:57:32 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 21 Jul 2006 08:57:32 +0100
Subject: [R] Correspondence analysis with R
In-Reply-To: <BAY107-F8CD5BE4688EFCE7F8213C9A610@phx.gbl>
References: <BAY107-F8CD5BE4688EFCE7F8213C9A610@phx.gbl>
Message-ID: <1153468652.2848.4.camel@dhcppc2.my.nat.localnet>

On Thu, 2006-07-20 at 14:38 +0000, Emanuele Mazzola wrote:
> Hello everybody,
> 
> i'm having some trouble performing correspondence analysis with R for Mac OS 
> X. Does anyone know about some useful package?
> And also, if i had found coordinates of points representing row and column 
> profiles, how do i put them in a single figure with labels identifying each 
> one of them?
> This thing is getting me almost crazy...
> 
> Thank you in advance for answering,
> bye
> Emanuele

A summary of packages/functions providing ordination methods including
CA is in the Environmetrics Task View on CRAN, e.g.:

http://www.stats.bris.ac.uk/R/src/contrib/Views/Environmetrics.html

HTH

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 *Note new Address and Fax and Telephone numbers from 10th April 2006*
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/cv/
WC1E 6BT                          [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From p.dalgaard at biostat.ku.dk  Fri Jul 21 10:07:31 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Jul 2006 10:07:31 +0200
Subject: [R] Loss of numerical precision from conversion to list ?
In-Reply-To: <44C00C99.9080408@stats.uwo.ca>
References: <20060720210919.254440@gmx.net> <44C00C99.9080408@stats.uwo.ca>
Message-ID: <x2vepr754s.fsf@turmalin.kubism.ku.dk>

Duncan Murdoch <murdoch at stats.uwo.ca> writes:

> R tries to use the maximum precision (64 bit mantissa) in the floating 
...
> Or perhaps your problem has nothing to do with this; I didn't really 
> look at it in detail.

It hasn't. I was off speculating about sum vs rowSums too, but:

> > num.v<-  rowSums(((lambda-lambda0)*mu*w.k.sq[,-(K+1)])/(1+lambda*mu))

Inside this, we have mu*w.k.sq[,-(K+1)] . mu is a vector of length 27,
and w.k.sq has 10 rows and 28 *columns*. Column-major storage and
vector recycling kicks in... If mu has identical elements (never mind
the magnitude), of course, the recycling doesn't matter.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From gavin.simpson at ucl.ac.uk  Fri Jul 21 10:10:56 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 21 Jul 2006 09:10:56 +0100
Subject: [R] Correspondence analysis with R -follow up
In-Reply-To: <BAY107-F13E57825D196D824A9F3829A610@phx.gbl>
References: <BAY107-F13E57825D196D824A9F3829A610@phx.gbl>
Message-ID: <1153469456.2848.16.camel@dhcppc2.my.nat.localnet>

On Thu, 2006-07-20 at 16:40 +0000, Emanuele Mazzola wrote:
> Hi all,
> 
> thank you for your answers; i've tried both cca from vegan library, and 
> dudi.coa from ade4 library; one last question: my deal is mainly with 
> contingency tables, like the one i'm posting here
> 
> acciaieria<-c(.41,.02,.44,.04,.09)
> laminatoio<-c(.34,.28,.26,.01,.11)
> fonderia<-c(.48,.05,.34,.08,.05)
> leghe<-c(.45,.19,.25,.03,.08)
> pct<-cbind(acciaieria,laminatoio,fonderia,leghe)
> pct<-data.frame(pct,row.names=c("normale","preparaz.","manutenz.","installaz.","trasferim."))
> 
> BUT...cca and dudi.coa seem to return quite different graphical results; 
> where am i doing wrong?
> Do they do the same to you with the table above?
> 
> Thank you very much again!
> Bye
> Emanuele

Hi, I haven't used ade4 at all, but as long as you did CA correctly
using ade4 functions, I doubt the plotted results differ really in all
but cosmetic ways or perhaps in terms of the scalings used.

You are plotting two bits of information in the biplot and you can only
represent one of those optimally, or you could compromise and plot with
symmetric scaling. Or you could plot them in a multitude of ways - there
are whole books on biplots!

Take a look at argument scaling in ?plot.cca (for vegan). Try plotting
your CA with different scalings and see if that better matches the CA
from ade4, eg:

library(vegan)
mod <- cca(pct)
plot(mod) # default is scaling = 2
plot(mod, scaling = 1)
plot(mod, scaling = 3)

If using vegan, you might want to look at Jari Oksanens Vegan Tutorial
for more info on using his functions:

http://cc.oulu.fi/~jarioksa/opetus/metodi/vegantutor.pdf

If this doesn't help your understanding or problem, post back with the
ade4 and vegan code you are using and I'll have a look.

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 *Note new Address and Fax and Telephone numbers from 10th April 2006*
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/cv/
WC1E 6BT                          [w] http://www.ucl.ac.uk/~ucfagls/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From jf117 at york.ac.uk  Thu Jul 20 22:46:35 2006
From: jf117 at york.ac.uk (James Foadi)
Date: Thu, 20 Jul 2006 21:46:35 +0100
Subject: [R] failed installing rgl
Message-ID: <200607202146.35670.jf117@york.ac.uk>

Dear all,
I have tried installing "rgl" with the usual command:

R CMD INSTALL rgl_0.67-2.tar.gz

Differently from what happened last time I have succesfully installed this 
package, this time there was a failure:

...
...g++ -I/usr/lib/R/include -I/usr/lib/R/include -I -DHAVE_PNG_H 
-I/usr/include/libpng12 -I/usr/local/include   -fpic  -O2 -g -pipe -Wall 
-Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector 
--param=ssp-buffer-size=4 -m32 -march=i386 -mtune=generic 
-fasynchronous-unwind-tables -c api.cpp -o api.o
In file included from glgui.hpp:9,
                 from gui.hpp:11,
                 from rglview.h:10,
                 from Device.hpp:11,
                 from DeviceManager.hpp:9,
                 from api.cpp:14:
opengl.hpp:23:20: error: GL/glu.h: No such file or directory
Disposable.hpp:13: warning: ???struct IDisposeListener??? has virtual functions 
but non-virtual destructor
types.h:77: warning: ???class DestroyHandler??? has virtual functions but 
non-virtual destructor
gui.hpp:56: warning: ???class gui::WindowImpl??? has virtual functions but 
non-virtual destructor
gui.hpp:90: warning: ???class gui::GUIFactory??? has virtual functions but 
non-virtual destructor
pixmap.h:39: warning: ???class PixmapFormat??? has virtual functions but 
non-virtual destructor
api.cpp: In function ???void rgl_user2window(int*, int*, double*, double*, 
double*, double*, int*)???:
api.cpp:707: error: ???gluProject??? was not declared in this scope
api.cpp: In function ???void rgl_window2user(int*, int*, double*, double*, 
double*, double*, int*)???:
api.cpp:735: error: ???gluUnProject??? was not declared in this scope
make: *** [api.o] Error 1
chmod: cannot access `/usr/lib/R/library/rgl/libs/*': No such file or 
directory
ERROR: compilation failed for package 'rgl'
** Removing '/usr/lib/R/library/rgl'
...
...

It is clear that glu.h cannot be found during installation and, indeed, I have 
checked and there is no glu.h under /usr/include/GL.

Any suggestion on how I could proceed?
I am using FEDORA CORE 5.
By typing glxinfo it seems that glu 1.3 is installed. But where's glu.h, then?

Many thanks

J

-- 
Dr James Foadi
Department of Physics
University of York
York YO10 5DD

email: jf117 at york.ac.uk
Tel: 0044 1904 434622
Mobile: 0044 7740 678548

		
___________________________________________________________ 
All New Yahoo! Mail ? Tired of Vi at gr@! come-ons? Let our SpamGuard protect you. http://uk.docs.yahoo.com/nowyoucan.html


From berwin at maths.uwa.edu.au  Fri Jul 21 10:16:32 2006
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Fri, 21 Jul 2006 16:16:32 +0800
Subject: [R] Parameterization puzzle
In-Reply-To: <Pine.LNX.4.64.0607210716270.12611@gannet.stats.ox.ac.uk>
References: <44C063E5.3020703@waikato.ac.nz>
	<Pine.LNX.4.64.0607210716270.12611@gannet.stats.ox.ac.uk>
Message-ID: <17600.36192.983823.42458@bossiaea.maths.uwa.edu.au>

G'day all,

>>>>> "BDR" == Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

    BDR> R does not know that poly(age,2) and poly(age,1) are linearly
    BDR> dependent.
Indeed, I also thought that this is the reason of the problem.

    BDR> (And indeed they only are for some functions 'poly'.)
I am surprised about this.  Should probably read the help page of
'poly' once more and more carefully.

    BDR> I cannot reproduce your example ('l' is missing), [...]
My guess is that 'l' is 'pyears'.  At least, I worked under that
assumption.  

Interestingly, on my machine (using R 2.3.1, 2.2.1 and 2.1.1) I cannot
fit any of the Poisson GLM that Murray tried.  I always get the error
message:

Error: no valid set of coefficients has been found: please supply starting values

But I have to investigate this further.  I can fit binomial models
that give me similar answers.

    BDR> [...] but perhaps
    BDR> glm(deaths ~ poly(age,2) + poly(age,1)*Smoke + offset(l),
    BDR> poisson)
    BDR> was your intention?
In this parameterisation a 'poly(age,1)' term will appear among the
coefficients with an estimated value of NA since it is aliased with
'poly(age, 2)1'.  So I don't believe that this was Murray's intention.

The only suggestion I can come up with is:

> summary(glm(cbind(deaths, l-deaths) ~ age*Smoke+I(age^2), family=binomial))

[...]

Coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -10.79895    0.45149 -23.918  < 2e-16 ***
age            2.37892    0.20877  11.395  < 2e-16 ***
SmokeYes       1.44573    0.37347   3.871 0.000108 ***
I(age^2)      -0.19706    0.02749  -7.168  7.6e-13 ***
age:SmokeYes  -0.30850    0.09756  -3.162 0.001566 ** 

[...]

Which doesn't use orthogonal polynomials anymore.  But I don't see how
you can fit the model that Murray want to fit using orthogonal
polynomials given the way R's model language operates.

So I guess the Poisson GLM that Murray wants to fit is:

        glm(deaths~ age*Smoke+I(age^2)+offset(l), family=poisson)

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin


From philipp.pagel.lists at t-online.de  Fri Jul 21 10:24:28 2006
From: philipp.pagel.lists at t-online.de (Philipp Pagel)
Date: Fri, 21 Jul 2006 10:24:28 +0200
Subject: [R] Merge two dataframes of different column length and row
	length by two columns at a time
In-Reply-To: <001701c6ac98$0616c5c0$0f1e0b0a@3med.klinik.unimainz.de>
References: <001701c6ac98$0616c5c0$0f1e0b0a@3med.klinik.unimainz.de>
Message-ID: <20060721082428.GB4262@gsf.de>

On Fri, Jul 21, 2006 at 09:33:56AM +0200, Gunther H?ning wrote:
> I have two dataframes of the following structures:
[...]

> I want to merge these dataframes by V1 and V2 of a and b. The combination of
> V1, V2 is a unique key.
> Note that b is smaller than a.
> 
> Any suggestions to solve this problem ?

Use merge()

cu
	Philipp

-- 
Dr. Philipp Pagel                            Tel.  +49-8161-71 2131
Dept. of Genome Oriented Bioinformatics      Fax.  +49-8161-71 2186
Technical University of Munich
Science Center Weihenstephan
85350 Freising, Germany

 and

Institute for Bioinformatics / MIPS          Tel.  +49-89-3187 3675
GSF - National Research Center               Fax.  +49-89-3187 3585
      for Environment and Health
Ingolst?dter Landstrasse 1
85764 Neuherberg, Germany
http://mips.gsf.de/staff/pagel


From ripley at stats.ox.ac.uk  Fri Jul 21 10:40:07 2006
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Fri, 21 Jul 2006 09:40:07 +0100 (GMT Daylight Time)
Subject: [R] Parameterization puzzle
In-Reply-To: <17600.36192.983823.42458@bossiaea.maths.uwa.edu.au>
References: <44C063E5.3020703@waikato.ac.nz>
	<Pine.LNX.4.64.0607210716270.12611@gannet.stats.ox.ac.uk>
	<17600.36192.983823.42458@bossiaea.maths.uwa.edu.au>
Message-ID: <Pine.WNT.4.64.0607210931001.1248@Petrel>

On Fri, 21 Jul 2006, Berwin A Turlach wrote:

> G'day all,
>
>>>>>> "BDR" == Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>
>    BDR> R does not know that poly(age,2) and poly(age,1) are linearly
>    BDR> dependent.
> Indeed, I also thought that this is the reason of the problem.
>
>    BDR> (And indeed they only are for some functions 'poly'.)
> I am surprised about this.  Should probably read the help page of
> 'poly' once more and more carefully.

My point was perhaps simpler: if you or I or Murray had a function poly() 
in our workspace, that one would be found, I think.  (I have not checked 
the ramifications of namespaces here, but I believe that would be the 
intention, that formulae are evaluated in their defining environment.)  So 
omly when the model matrix is set up could the linear dependence be known 
(and there is nothing in the system poly() to tell model.matrix).

What is sometimes called extrinsic aliasing is left to the fitting 
function, which seems to be to do a sensible job provided the main effect 
is in the model.  Indeed, including interactions without main effects (as 
Murray did) often makes the results hard to interpret.

>    BDR> I cannot reproduce your example ('l' is missing), [...]
> My guess is that 'l' is 'pyears'.  At least, I worked under that
> assumption.

Apparently l = log(pyears), which was my uncertain guess.

> Interestingly, on my machine (using R 2.3.1, 2.2.1 and 2.1.1) I cannot
> fit any of the Poisson GLM that Murray tried.  I always get the error
> message:
>
> Error: no valid set of coefficients has been found: please supply starting values

Related to the offset, I believe.

>
> But I have to investigate this further.  I can fit binomial models
> that give me similar answers.
>
>    BDR> [...] but perhaps
>    BDR> glm(deaths ~ poly(age,2) + poly(age,1)*Smoke + offset(l),
>    BDR> poisson)
>    BDR> was your intention?
> In this parameterisation a 'poly(age,1)' term will appear among the
> coefficients with an estimated value of NA since it is aliased with
> 'poly(age, 2)1'.  So I don't believe that this was Murray's intention.
>
> The only suggestion I can come up with is:
>
>> summary(glm(cbind(deaths, l-deaths) ~ age*Smoke+I(age^2), family=binomial))
>
> [...]
>
> Coefficients:
>              Estimate Std. Error z value Pr(>|z|)
> (Intercept)  -10.79895    0.45149 -23.918  < 2e-16 ***
> age            2.37892    0.20877  11.395  < 2e-16 ***
> SmokeYes       1.44573    0.37347   3.871 0.000108 ***
> I(age^2)      -0.19706    0.02749  -7.168  7.6e-13 ***
> age:SmokeYes  -0.30850    0.09756  -3.162 0.001566 **
>
> [...]
>
> Which doesn't use orthogonal polynomials anymore.  But I don't see how
> you can fit the model that Murray want to fit using orthogonal
> polynomials given the way R's model language operates.
>
> So I guess the Poisson GLM that Murray wants to fit is:
>
>        glm(deaths~ age*Smoke+I(age^2)+offset(l), family=poisson)
>
> Cheers,
>
>        Berwin
>
> ========================== Full address ============================
> Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)
> School of Mathematics and Statistics        +61 (8) 6488 3383 (self)
> The University of Western Australia   FAX : +61 (8) 6488 1028
> 35 Stirling Highway
> Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
> Australia                        http://www.maths.uwa.edu.au/~berwin
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bady at univ-lyon1.fr  Fri Jul 21 10:41:38 2006
From: bady at univ-lyon1.fr (bady at univ-lyon1.fr)
Date: Fri, 21 Jul 2006 10:41:38 +0200
Subject: [R] Correspondence analysis with R -follow up
In-Reply-To: <BAY107-F13E57825D196D824A9F3829A610@phx.gbl>
References: <BAY107-F13E57825D196D824A9F3829A610@phx.gbl>
Message-ID: <1153471298.44c093427726d@webmail.univ-lyon1.fr>

Hi, Hi all,

> thank you for your answers; i've tried both cca from vegan library, and
> dudi.coa from ade4 library; one last question: my deal is mainly with
> contingency tables, like the one i'm posting here
>
> acciaieria<-c(.41,.02,.44,.04,.09)
> laminatoio<-c(.34,.28,.26,.01,.11)
> fonderia<-c(.48,.05,.34,.08,.05)
> leghe<-c(.45,.19,.25,.03,.08)
> pct<-cbind(acciaieria,laminatoio,fonderia,leghe)
>
pct<-data.frame(pct,row.names=c("normale","preparaz.","manutenz.","installaz.","trasferim."))
>

the data.frame "pct" is not a contingency table.

> BUT...cca and dudi.coa seem to return quite different graphical results;
> where am i doing wrong?
> Do they do the same to you with the table above?

graphicals reults are similar.
see the argument "method" in the function scatter.coa (library ade4).
"...
  method: an integer between 1 and 3
           1 Rows and columns with the coordinates of lambda variance
           2 Rows variance 1 and columns by averaging
           3 Columns variance 1 and rows by averaging
..."

#example :
require(ade4)
data(rpjdl)
coa1 <- dudi.coa(rpjdl$fau, scannf = FALSE, nf = 4)
require(vegan)
coa2 <- cca(rpjdl$fau)

# biplot representations
par(mfrow=c(2,2))
plot(coa2,type="text")
? scatter.coa
scatter(coa1,method=1)
scatter(coa1,method=2)
scatter(coa1,method=3)


hope this help :)


Pierre


From ripley at stats.ox.ac.uk  Fri Jul 21 11:08:47 2006
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Fri, 21 Jul 2006 10:08:47 +0100 (GMT Daylight Time)
Subject: [R] failed installing rgl
In-Reply-To: <200607202146.35670.jf117@york.ac.uk>
References: <200607202146.35670.jf117@york.ac.uk>
Message-ID: <Pine.WNT.4.64.0607211005340.1248@Petrel>

On FC5:

roc% rpm -q --file /usr/include/GL/glu.h
mesa-libGLU-devel-6.4.2-6.FC5.3

Do check what the R-admin manual says about -devel RPMs.


On Thu, 20 Jul 2006, James Foadi wrote:

> Dear all,
> I have tried installing "rgl" with the usual command:
>
> R CMD INSTALL rgl_0.67-2.tar.gz
>
> Differently from what happened last time I have succesfully installed this
> package, this time there was a failure:
>
> ...
> ...g++ -I/usr/lib/R/include -I/usr/lib/R/include -I -DHAVE_PNG_H
> -I/usr/include/libpng12 -I/usr/local/include   -fpic  -O2 -g -pipe -Wall
> -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
> --param=ssp-buffer-size=4 -m32 -march=i386 -mtune=generic
> -fasynchronous-unwind-tables -c api.cpp -o api.o
> In file included from glgui.hpp:9,
>                 from gui.hpp:11,
>                 from rglview.h:10,
>                 from Device.hpp:11,
>                 from DeviceManager.hpp:9,
>                 from api.cpp:14:
> opengl.hpp:23:20: error: GL/glu.h: No such file or directory
> Disposable.hpp:13: warning: ??struct IDisposeListener?? has virtual functions
> but non-virtual destructor
> types.h:77: warning: ??class DestroyHandler?? has virtual functions but
> non-virtual destructor
> gui.hpp:56: warning: ??class gui::WindowImpl?? has virtual functions but
> non-virtual destructor
> gui.hpp:90: warning: ??class gui::GUIFactory?? has virtual functions but
> non-virtual destructor
> pixmap.h:39: warning: ??class PixmapFormat?? has virtual functions but
> non-virtual destructor
> api.cpp: In function ??void rgl_user2window(int*, int*, double*, double*,
> double*, double*, int*)??:
> api.cpp:707: error: ??gluProject?? was not declared in this scope
> api.cpp: In function ??void rgl_window2user(int*, int*, double*, double*,
> double*, double*, int*)??:
> api.cpp:735: error: ??gluUnProject?? was not declared in this scope
> make: *** [api.o] Error 1
> chmod: cannot access `/usr/lib/R/library/rgl/libs/*': No such file or
> directory
> ERROR: compilation failed for package 'rgl'
> ** Removing '/usr/lib/R/library/rgl'
> ...
> ...
>
> It is clear that glu.h cannot be found during installation and, indeed, I have
> checked and there is no glu.h under /usr/include/GL.
>
> Any suggestion on how I could proceed?
> I am using FEDORA CORE 5.
> By typing glxinfo it seems that glu 1.3 is installed. But where's glu.h, then?
>
> Many thanks
>
> J
>
> -- 
> Dr James Foadi
> Department of Physics
> University of York
> York YO10 5DD
>
> email: jf117 at york.ac.uk
> Tel: 0044 1904 434622
> Mobile: 0044 7740 678548
>
>
> ___________________________________________________________
> All New Yahoo! Mail ? Tired of Vi at gr@! come-ons? Let our SpamGuard protect you. http://uk.docs.yahoo.com/nowyoucan.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From f.abian at gmx.net  Fri Jul 21 11:51:07 2006
From: f.abian at gmx.net (Fabian Scheipl)
Date: Fri, 21 Jul 2006 11:51:07 +0200
Subject: [R] Loss of numerical precision from conversion to list ?
In-Reply-To: <x2vepr754s.fsf@turmalin.kubism.ku.dk>
References: <20060720210919.254440@gmx.net> <44C00C99.9080408@stats.uwo.ca>
	<x2vepr754s.fsf@turmalin.kubism.ku.dk>
Message-ID: <20060721095107.12610@gmx.net>

Thank you both very much for your help.

Peter Dalgaard is right- i  didn't consider the fact that elementwise multiplication is column-wise rather than row-wise.
Sorry for taking up time&space with such a trivial mistake.



-------- Original-Nachricht --------
Datum: 21 Jul 2006 10:07:31 +0200
Von: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
An: Duncan Murdoch <murdoch at stats.uwo.ca>
Betreff: Re: [R] Loss of numerical precision from conversion to list ?

> Duncan Murdoch <murdoch at stats.uwo.ca> writes:
> 
> > R tries to use the maximum precision (64 bit mantissa) in the floating 
> ...
> > Or perhaps your problem has nothing to do with this; I didn't really 
> > look at it in detail.
> 
> It hasn't. I was off speculating about sum vs rowSums too, but:
> 
> > > num.v<-  rowSums(((lambda-lambda0)*mu*w.k.sq[,-(K+1)])/(1+lambda*mu))
> 
> Inside this, we have mu*w.k.sq[,-(K+1)] . mu is a vector of length 27,
> and w.k.sq has 10 rows and 28 *columns*. Column-major storage and
> vector recycling kicks in... If mu has identical elements (never mind
> the magnitude), of course, the recycling doesn't matter.
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
> 35327907

-- 


Echte DSL-Flatrate dauerhaft f?r 0,- Euro*!


From jf117 at york.ac.uk  Fri Jul 21 12:00:20 2006
From: jf117 at york.ac.uk (James Foadi)
Date: Fri, 21 Jul 2006 11:00:20 +0100
Subject: [R] failed installing rgl
Message-ID: <200607211100.20675.jf117@york.ac.uk>


>Subject: Re: [R] failed installing rgl
>Date: Friday 21 July 2006 10:08
>From: Prof Brian D Ripley <ripley at stats.ox.ac.uk>
>To: James Foadi <jf117 at york.ac.uk>
>Cc: r-help at stat.math.ethz.ch

>On FC5:

>roc% rpm -q --file /usr/include/GL/glu.h
>mesa-libGLU-devel-6.4.2-6.FC5.3

>Do check what the R-admin manual says about -devel RPMs.


I have followed Brian Ripley advice to read the R-admin manual. In Appendix A 
it is reported:

"Remember that some package management systems (such as RPM and deb) make a 
distinction between the user version of a package and the development 
version. The latter usually has the same name but with the extension `-devel' 
or `-dev': you need both versions installed."

In fact file "glu.h" was not installed because only the user version of GLU 
was installed on my system. Thus, I have downloaded the development version 
of GLU (mesa-libGLU-devel-6.4.2-6.FC5.3.i386.rpm) and installed it.  After 
that the package rgl has installed fine.

Thank you, Brian.

J

-------------------------------------------------------

-- 
Dr James Foadi
Department of Physics
University of York
York YO10 5DD

email: jf117 at york.ac.uk
Tel: 0044 1904 434622
Mobile: 0044 7740 678548


From maj at waikato.ac.nz  Fri Jul 21 11:58:20 2006
From: maj at waikato.ac.nz (Murray Jorgensen)
Date: Fri, 21 Jul 2006 21:58:20 +1200
Subject: [R] [Fwd: Re:  Parameterization puzzle]
Message-ID: <44C0A53C.5070908@waikato.ac.nz>

Bother! This cold has made me accident-prone. I meant to hit Reply-all.
Clarification below.

-------- Original Message --------
Subject: Re: [R] Parameterization puzzle
Date: Fri, 21 Jul 2006 19:10:03 +1200
From: Murray Jorgensen <maj at waikato.ac.nz>
To: Prof Brian Ripley <ripley at stats.ox.ac.uk>
References: <44C063E5.3020703 at waikato.ac.nz> 
<Pine.LNX.4.64.0607210716270.12611 at gannet.stats.ox.ac.uk>

Apologies for a non-selfcontained example. Here is what I should have sent:

pyears <- scan()
18793 52407 10673 43248 5710 28612 2585 12663 1462 5317

deaths <- scan()
2 32 12 104 28 206 28 186 31 102

l <- log(pyears)
Smoke <- gl(2,1,10,labels=c("No","Yes"))
Age <- gl(5,2,10,labels=c("35-44","45-54","55-64","65-74","75-84"),
            ordered=TRUE)
mod1.glm <- glm(deaths ~ Age * Smoke + offset(l),family=poisson)
summary(mod1.glm)
age <- as.numeric(Age)
mod2.glm <- aso1.glm <- glm(deaths ~ poly(age,2) + Smoke +
                       poly(age,1):Smoke + offset(l),family=poisson)
summary(mod2.glm)


Cheers, Murray Jorgensen

Prof Brian Ripley wrote:
> R does not know that poly(age,2) and poly(age,1) are linearly dependent.
> (And indeed they only are for some functions 'poly'.)
> 
> I cannot reproduce your example ('l' is missing), but perhaps
> 
> glm(deaths ~ poly(age,2) + poly(age,1)*Smoke + offset(l), poisson)
> 
> was your intention?
> 
> On Fri, 21 Jul 2006, Murray Jorgensen wrote:
> 
>> Consider the following example (based on an example in Pat Altham's GLM 
>> notes)
>>
>> pyears <- scan()
>> 18793 52407 10673 43248 5710 28612 2585 12663 1462 5317
>>
>> deaths <- scan()
>> 2 32 12 104 28 206 28 186 31 102
>>
>> Smoke <- gl(2,1,10,labels=c("No","Yes"))
>> Age <- gl(5,2,10,labels=c("35-44","45-54","55-64","65-74","75-84"),
>>             ordered=TRUE)
>> mod1.glm <- glm(deaths ~ Age * Smoke + offset(l),family=poisson)
>> summary(mod1.glm)
>> age <- as.numeric(Age)
>> mod2.glm <- aso1.glm <- glm(deaths ~ poly(age,2) + Smoke +
>>                        poly(age,1):Smoke + offset(l),family=poisson)
>> summary(mod2.glm)
>>
>>
>>
>> The business part of the summary for the first model
>>
>>                 Estimate Std. Error z value Pr(>|z|)
>> (Intercept)    -5.92706    0.16577 -35.754  < 2e-16 ***
>> Age.L           4.06490    0.47414   8.573  < 2e-16 ***
>> Age.Q          -1.08293    0.41326  -2.620 0.008781 **
>> Age.C           0.24158    0.31756   0.761 0.446816
>> Age^4           0.04244    0.23061   0.184 0.853986
>> SmokeYes        0.61916    0.17296   3.580 0.000344 ***
>> Age.L:SmokeYes -1.31234    0.49267  -2.664 0.007729 **
>> Age.Q:SmokeYes  0.39043    0.43008   0.908 0.363976
>> Age.C:SmokeYes -0.29593    0.33309  -0.888 0.374298
>> Age^4:SmokeYes -0.03682    0.24432  -0.151 0.880218
>>
>> inspires me to fit the second model that omits the nonsignificant terms, 
>> however this produces the summary
>>
>>                        Estimate Std. Error z value Pr(>|z|)
>> (Intercept)            -5.8368     0.1213 -48.103  < 2e-16 ***
>> poly(age, 2)1           3.9483     0.1755  22.497  < 2e-16 ***
>> poly(age, 2)2          -1.0460     0.1448  -7.223 5.08e-13 ***
>> SmokeYes                0.5183     0.1262   4.106 4.02e-05 ***
>> SmokeNo:poly(age, 1)    1.3755     0.4340   3.169  0.00153 **
>> SmokeYes:poly(age, 1)       NA         NA      NA       NA
>>
>> Why do we get a SmokeNo:poly(age, 1) term? Can I re-express mod2.glm so 
>> that this term does not appear?
>>
>> Cheers,  Murray Jorgensen
>>
>>
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862


From f.calboli at imperial.ac.uk  Fri Jul 21 11:55:16 2006
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Fri, 21 Jul 2006 10:55:16 +0100
Subject: [R] from character to numeric over multiple columns
Message-ID: <44C0A484.1030906@imperial.ac.uk>

Hi All,

I have a data frame of three columns, all of which names. The names in the three 
  cols overlap up to a point, so I might have *Harry* in all three columns, 
*Tom* in cols 1 and 3 and *Bob* in col 3 only.

harry	paul	bob
anita	harry	tom
frank	jack	harry
tom	pete	ben
....	

I want to turn the names into numbers, BUT I want the numeric code for, say, 
*Harry*, to be the same on all columns.

Doing

cbind(as.numeric(col1), as.numeric(col2), as.numeric(col3))

does not work because the factors are different in each column, hence they get a 
different number even though the name is the same.

Ideas?

Cheers

Federico

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From antonio.fabio at gmail.com  Fri Jul 21 10:19:50 2006
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Fri, 21 Jul 2006 10:19:50 +0200
Subject: [R] [R-pkgs] tsDyn and RTisean packages on CRAN
Message-ID: <b0808fdc0607210119q34a4933re2f10d45309d7478@mail.gmail.com>

Dear R users,
I've just uploaded  2 packages on CRAN, RTisean and tsDyn, both for time
series analysis (joint research projects with members of the Statistics
Department, University of Udine). Brief descriptions follow.

RTisean is an R interface to TISEAN executables
(http://www.mpipks-dresden.mpg.de/~tisean/). TISEAN is a suite of C
and Fortran routines for nonlinear time series analysis, coded and
documented by Rainer Hegger, Holger Kantz and Thomas Schreiber. In
RTisean, almost all TISEAN routines are wrapped in a conventional R
function (which silently calls TISEAN executables), with online help
and examples (thanks to Gianluca Gazzola).

tsDyn is a package for nonlinear time series modelling. At this point
the package focuses on Nonlinear Autoregressive Models, often indicated as
NLAR (as a major reference, see Tong (1990)). Currently available are
threshold (SETAR and LSTAR), neural networks (NNET), and additive
autoregressive (AAR) models. An experimental cross-platform tcltk GUI is
included for model selection. Explorative and diagnostic tools are
also available. A vignette is included for a clearer presentation of the
package contents.

Comments and suggestions appreciated

Antonio, Fabio Di Narzo
Dipartimento di Statistica
Universit? degli studi di Bologna.

_______________________________________________
R-packages mailing list
R-packages a stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From ripley at stats.ox.ac.uk  Fri Jul 21 12:51:15 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 21 Jul 2006 11:51:15 +0100 (BST)
Subject: [R] from character to numeric over multiple columns
In-Reply-To: <44C0A484.1030906@imperial.ac.uk>
References: <44C0A484.1030906@imperial.ac.uk>
Message-ID: <Pine.LNX.4.64.0607211146450.21804@gannet.stats.ox.ac.uk>

Are the columns factors or character?  I'll try to write code that copes 
with both:

nm <- unique(c(as.character(col1), as.character(col2), as.character(col3)))

DF[] <- lapply(DF, function(x) match(x, nm))


On Fri, 21 Jul 2006, Federico Calboli wrote:

> Hi All,
> 
> I have a data frame of three columns, all of which names. The names in the three 
>   cols overlap up to a point, so I might have *Harry* in all three columns, 
> *Tom* in cols 1 and 3 and *Bob* in col 3 only.
> 
> harry	paul	bob
> anita	harry	tom
> frank	jack	harry
> tom	pete	ben
> ....	
> 
> I want to turn the names into numbers, BUT I want the numeric code for, say, 
> *Harry*, to be the same on all columns.
> 
> Doing
> 
> cbind(as.numeric(col1), as.numeric(col2), as.numeric(col3))
> 
> does not work because the factors are different in each column, hence they get a 
> different number even though the name is the same.
> 
> Ideas?
> 
> Cheers
> 
> Federico
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From f.calboli at imperial.ac.uk  Fri Jul 21 13:02:02 2006
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Fri, 21 Jul 2006 12:02:02 +0100
Subject: [R] from character to numeric over multiple columns
In-Reply-To: <Pine.LNX.4.64.0607211146450.21804@gannet.stats.ox.ac.uk>
References: <44C0A484.1030906@imperial.ac.uk>
	<Pine.LNX.4.64.0607211146450.21804@gannet.stats.ox.ac.uk>
Message-ID: <44C0B42A.4000005@imperial.ac.uk>

Prof Brian Ripley wrote:
> Are the columns factors or character?  I'll try to write code that copes 
> with both:
> 
> nm <- unique(c(as.character(col1), as.character(col2), as.character(col3)))
> 
> DF[] <- lapply(DF, function(x) match(x, nm))

Cheers,

it worked.

Federico

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From danbebber at forestecology.co.uk  Fri Jul 21 13:35:58 2006
From: danbebber at forestecology.co.uk (Dan Bebber)
Date: Fri, 21 Jul 2006 12:35:58 +0100
Subject: [R] glm cannot find valid starting values
Message-ID: <00ea01c6acb9$d5d59770$d22401a3@plants.ox.ac.uk>

glm(S ~ -1 + Mdif, family=quasipoisson(link=identity), start=strt, sdat)
gives error:

> Error in glm.fit(x = X, y = Y, weights = weights, start = start, etastart 
> =
> etastart,  :
>        cannot find valid starting values: please specify some

strt is set to be the coefficient for a similar fit
glm(S ~ -1 + I(Mdif + 1),...
i.e. (Mdif + 1) is a vector similar to Mdif.
The error appears to occur when some values of Mdif are negative,
though I have not had this problem with simulated datasets.

Any solutions greatly appreciated (full details and data below).

Dan Bebber
Department of Plant Sciences
University of Oxford

OS: WinXP Pro SP2 and Win ME (tried both)
Processor: Dual Intel Xeon and Pentium 4 (tried both)
R version: 2.3.0 and 2.3.1 (tried both)

#Full details (can be pasted directly into R):
#Data:
sdat <- data.frame(
S = c(0, 0, 0, 0, 28, 0, 1, 7, 0, 0, 39, 2, 0, 0, 40, 0, 0, 0, 0,
0, 0, 15, 0, 0, 3, 0, 0, 0, 2, 0, 3, 0, 30, 0, 20, 0, 1, 0, 0,
1, 21, 0, 0, 4, 14, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 3, 0,
2, 5, 0, 0, 0, 0, 0, 0, 0, 25, 0, 5, 0, 0, 0, 1, 0, 1, 0, 0,
0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
0, 0, 0, 0, 0, 1, 10, 0, 0, 0, 0, 0, 9, 1, 1, 1, 1, 0, 0, 3,
0, 27, 7, 0, 0, 0, 0, 0, 1, 1, 4, 2, 1, 2, 4, 1, 4, 6, 12, 4,
6, 3, 4, 0, 4, 0, 6, 1, 3, 1, 4, 4, 1, 1, 2, 1, 6, 1, 0, 0, 1,
0, 1, 0, 0, 6, 0, 0, 0, 0, 2, 2, 3, 1, 6, 2, 2, 1, 1, 4, 4, 3,
3, 7, 1, 3, 5, 6, 4, 0, 1, 4, 2, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
1, 0, 4, 0, 0, 1, 0, 2, 0, 0, 1, 2, 0, 4, 0, 2, 0, 3, 0, 2, 2,
0, 4, 0, 2, 0, 1, 2, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 3, 2, 1, 2,
1, 1, 2, 0, 0, 3, 3, 1, 0, 1, 1, 2, 0, 1, 0, 1, 0, 1, 3, 2, 0,
0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 4, 2, 4, 0, 2, 0, 0, 0, 0, 0,
0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 3, 2, 0, 0, 0, 0, 0,
0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 1, 0, 0, 1),
M = 620+c(0,cumsum(sdat$S[-328])))
#S is the (unknown) number of N individuals that irreversibly change state 
in a time
#interval t. The data provided are a subset of the full data set.
#M is the cumulative sum of individuals that have changed state up to t-1.
#Assume that the rate of state change is constant (S ~ kM), but the
#distribution of S is clustered.
#The goal is to estimate N.
#N can be estimated by fitting:
qpglm <- glm(S ~ M, family = quasipoisson(link = identity), sdat)
summary(qpglm)
N.est <- -coef(qpglm)[1]/coef(qpglm)[2]
N.est
#i.e. x-intercept is minus intercept/slope
#To estimate confidence limits on N.est, fit models without intercept to
#N.est - M + x, where x is an integer. The model will have the lowest 
deviance
#when x = 0.
x <- 0
Mdif <- N.est - M + x
qpglm2 <- glm(S ~ -1 + Mdif, family = quasipoisson(link = identity), sdat)
summary(qpglm2)
#Use analysis of deviance to estimate confidence limits on N.est:
disp <- summary(qpglm)$dispersion
dfres <- df.residual(qpglm)
dev.res <- deviance(qpglm)
#From MASS4, p. 210, assume that changes in deviance scaled by
#dispersion as |x| increases have an F distribution
dev.crit <- dev.res+qf(0.95,1,dfres)*disp
dev.crit
#values of x for which the deviance = dev.crit give approximate 95% 
confidence limits
#on N.est.
#The error occurs when x <= -91.7:
x <- -91.7
sdat$Mdif <- N.est - sdat$M + x
strt <- coef(glm(S ~ -1 + I(Mdif+1), family = quasipoisson(link = identity), 
sdat))
qpglm2 <- glm(S ~ -1 + Mdif, family = quasipoisson(link = identity), 
start=strt, sdat)
#The problem is that this interferes with optimization to find values of x 
for which
#deviance = dev.crit


From sirrahn at hotmail.com  Fri Jul 21 13:36:53 2006
From: sirrahn at hotmail.com (nathan)
Date: Fri, 21 Jul 2006 04:36:53 -0700 (PDT)
Subject: [R] unexpected results
Message-ID: <5432210.post@talk.nabble.com>


Hi, 

I'm just learning R and am encountering some unexpected results following a
guide on the internet. If anyone can help that would be great - I think it
is something about the way the data has been read in!

I've read a coma delimited text data file that was saved from excel:

> jacs.data <- read.table("/Users/natha/Desktop/JACSdata2.txt", header=TRUE,
> sep=",")

This seemed to work fine, but then I start to get unusual results that don't
seem right:

The guide says that names(file.data) will give output like "ID" "JURIS"
"RESCODE" , but I get ID.JURIS.RESCODE.

The guide says that file.data[5,1] will give me the data for the 5th subject
but i get: 
[1] 7\tACT\t\t\tSUMMONS\tACTCC321.001\tA.C.T. - MINOR THEFT (REPLACEMENT
VALUE $2000 OR LESS)\ etc - which seems scrambled

The guide says that file.data[var5>0] will give me the data for all subject
who meet that condition (ie greater than 0 on var5), but I get:

Error in "[.data.frame"(jacs.data, offend > 0) : 
	object "offend" not found

can anyone help? Thanks nathan
-- 
View this message in context: http://www.nabble.com/unexpected-results-tf1979786.html#a5432210
Sent from the R help forum at Nabble.com.


From helen.mills at yale.edu  Fri Jul 21 14:06:17 2006
From: helen.mills at yale.edu (helen.mills at yale.edu)
Date: Fri, 21 Jul 2006 08:06:17 -0400
Subject: [R] rpart unbalanced data
Message-ID: <20060721080617.7d8o5jpu88sgg00g@www.mail.yale.edu>

Hello all,
I am currently working with rpart to classify vegetation types by spectral
characteristics, and am comming up with poor classifications based on the fact
that I have some vegetation types that have only 15 observations, while others
have over 100. I have attempted to supply prior weights to the dataset, though
this does not improve the classification greatly. Could anyone supply some
hints about how to improve a classification for a badly unbalanced datase?

Thank you,
Helen Mills Poulos


From ripley at stats.ox.ac.uk  Fri Jul 21 14:10:14 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 21 Jul 2006 13:10:14 +0100 (BST)
Subject: [R] glm cannot find valid starting values
In-Reply-To: <00ea01c6acb9$d5d59770$d22401a3@plants.ox.ac.uk>
References: <00ea01c6acb9$d5d59770$d22401a3@plants.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0607211301340.23904@gannet.stats.ox.ac.uk>

On Fri, 21 Jul 2006, Dan Bebber wrote:

> glm(S ~ -1 + Mdif, family=quasipoisson(link=identity), start=strt, sdat)
> gives error:
> 
> > Error in glm.fit(x = X, y = Y, weights = weights, start = start, etastart 
> > =
> > etastart,  :
> >        cannot find valid starting values: please specify some
> 
> strt is set to be the coefficient for a similar fit
> glm(S ~ -1 + I(Mdif + 1),...
> i.e. (Mdif + 1) is a vector similar to Mdif.
> The error appears to occur when some values of Mdif are negative,
> though I have not had this problem with simulated datasets.

Right: if Mdif contains both positive and negative values there are no
coefficients that are valid for that model (you are bound to predict 
negative means).

You often do better to take etastart from another fit than start, but that 
will not help here, I believe.

BTW, your example cannot be pasted in as 'sdat' self-references.  It could 
be fixed, but I gave up at that point.


> 
> Any solutions greatly appreciated (full details and data below).
> 
> Dan Bebber
> Department of Plant Sciences
> University of Oxford
> 
> OS: WinXP Pro SP2 and Win ME (tried both)
> Processor: Dual Intel Xeon and Pentium 4 (tried both)
> R version: 2.3.0 and 2.3.1 (tried both)
> 
> #Full details (can be pasted directly into R):
> #Data:
> sdat <- data.frame(
> S = c(0, 0, 0, 0, 28, 0, 1, 7, 0, 0, 39, 2, 0, 0, 40, 0, 0, 0, 0,
> 0, 0, 15, 0, 0, 3, 0, 0, 0, 2, 0, 3, 0, 30, 0, 20, 0, 1, 0, 0,
> 1, 21, 0, 0, 4, 14, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 3, 0,
> 2, 5, 0, 0, 0, 0, 0, 0, 0, 25, 0, 5, 0, 0, 0, 1, 0, 1, 0, 0,
> 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
> 0, 0, 0, 0, 0, 1, 10, 0, 0, 0, 0, 0, 9, 1, 1, 1, 1, 0, 0, 3,
> 0, 27, 7, 0, 0, 0, 0, 0, 1, 1, 4, 2, 1, 2, 4, 1, 4, 6, 12, 4,
> 6, 3, 4, 0, 4, 0, 6, 1, 3, 1, 4, 4, 1, 1, 2, 1, 6, 1, 0, 0, 1,
> 0, 1, 0, 0, 6, 0, 0, 0, 0, 2, 2, 3, 1, 6, 2, 2, 1, 1, 4, 4, 3,
> 3, 7, 1, 3, 5, 6, 4, 0, 1, 4, 2, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
> 1, 0, 4, 0, 0, 1, 0, 2, 0, 0, 1, 2, 0, 4, 0, 2, 0, 3, 0, 2, 2,
> 0, 4, 0, 2, 0, 1, 2, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 3, 2, 1, 2,
> 1, 1, 2, 0, 0, 3, 3, 1, 0, 1, 1, 2, 0, 1, 0, 1, 0, 1, 3, 2, 0,
> 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 4, 2, 4, 0, 2, 0, 0, 0, 0, 0,
> 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 3, 2, 0, 0, 0, 0, 0,
> 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 1, 0, 0, 1),
> M = 620+c(0,cumsum(sdat$S[-328])))
> #S is the (unknown) number of N individuals that irreversibly change state 
> in a time
> #interval t. The data provided are a subset of the full data set.
> #M is the cumulative sum of individuals that have changed state up to t-1.
> #Assume that the rate of state change is constant (S ~ kM), but the
> #distribution of S is clustered.
> #The goal is to estimate N.
> #N can be estimated by fitting:
> qpglm <- glm(S ~ M, family = quasipoisson(link = identity), sdat)
> summary(qpglm)
> N.est <- -coef(qpglm)[1]/coef(qpglm)[2]
> N.est
> #i.e. x-intercept is minus intercept/slope
> #To estimate confidence limits on N.est, fit models without intercept to
> #N.est - M + x, where x is an integer. The model will have the lowest 
> deviance
> #when x = 0.
> x <- 0
> Mdif <- N.est - M + x
> qpglm2 <- glm(S ~ -1 + Mdif, family = quasipoisson(link = identity), sdat)
> summary(qpglm2)
> #Use analysis of deviance to estimate confidence limits on N.est:
> disp <- summary(qpglm)$dispersion
> dfres <- df.residual(qpglm)
> dev.res <- deviance(qpglm)
> #From MASS4, p. 210, assume that changes in deviance scaled by
> #dispersion as |x| increases have an F distribution
> dev.crit <- dev.res+qf(0.95,1,dfres)*disp
> dev.crit
> #values of x for which the deviance = dev.crit give approximate 95% 
> confidence limits
> #on N.est.
> #The error occurs when x <= -91.7:
> x <- -91.7
> sdat$Mdif <- N.est - sdat$M + x
> strt <- coef(glm(S ~ -1 + I(Mdif+1), family = quasipoisson(link = identity), 
> sdat))
> qpglm2 <- glm(S ~ -1 + Mdif, family = quasipoisson(link = identity), 
> start=strt, sdat)
> #The problem is that this interferes with optimization to find values of x 
> for which
> #deviance = dev.crit
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ggrothendieck at gmail.com  Fri Jul 21 14:16:52 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 21 Jul 2006 08:16:52 -0400
Subject: [R] rpart unbalanced data
In-Reply-To: <20060721080617.7d8o5jpu88sgg00g@www.mail.yale.edu>
References: <20060721080617.7d8o5jpu88sgg00g@www.mail.yale.edu>
Message-ID: <971536df0607210516w5c0a7522r2017d6102dae7890@mail.gmail.com>

Check this thread:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/40898.html

On 7/21/06, helen.mills at yale.edu <helen.mills at yale.edu> wrote:
> Hello all,
> I am currently working with rpart to classify vegetation types by spectral
> characteristics, and am comming up with poor classifications based on the fact
> that I have some vegetation types that have only 15 observations, while others
> have over 100. I have attempted to supply prior weights to the dataset, though
> this does not improve the classification greatly. Could anyone supply some
> hints about how to improve a classification for a badly unbalanced datase?
>
> Thank you,
> Helen Mills Poulos
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From kuonen at statoo.com  Fri Jul 21 14:21:44 2006
From: kuonen at statoo.com (Dr. Diego Kuonen)
Date: Fri, 21 Jul 2006 14:21:44 +0200
Subject: [R] rpart unbalanced data
In-Reply-To: <20060721080617.7d8o5jpu88sgg00g@www.mail.yale.edu>
References: <20060721080617.7d8o5jpu88sgg00g@www.mail.yale.edu>
Message-ID: <44C0C6D8.4060406@statoo.com>

Dear Helen,

You may want to have a look at

  http://www.togaware.com/datamining/survivor/Predicting_Fraud.html

Greets,

  Diego Kuonen


helen.mills at yale.edu wrote:
> Hello all,
> I am currently working with rpart to classify vegetation types by spectral
> characteristics, and am comming up with poor classifications based on the fact
> that I have some vegetation types that have only 15 observations, while others
> have over 100. I have attempted to supply prior weights to the dataset, though
> this does not improve the classification greatly. Could anyone supply some
> hints about how to improve a classification for a badly unbalanced datase?
> 
> Thank you,
> Helen Mills Poulos

-- 
Dr. ?s sc. Diego Kuonen, CEO            phone  +41 (0)21 693 5508
Statoo Consulting                       fax    +41 (0)21 693 8765
PO Box 107                              mobile +41 (0)78 709 5384
CH-1015 Lausanne 15                     email   kuonen at statoo.com
web   http://www.statoo.info       skype Kuonen.Statoo.Consulting
-----------------------------------------------------------------
| Statistical Consulting + Data Analysis + Data Mining Services |
-----------------------------------------------------------------
+  Are you drowning in information and starving for knowledge?  +
+  Have you ever been Statooed?          http://www.statoo.biz  +


From murdoch at stats.uwo.ca  Fri Jul 21 14:23:00 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 21 Jul 2006 08:23:00 -0400
Subject: [R] unexpected results
In-Reply-To: <5432210.post@talk.nabble.com>
References: <5432210.post@talk.nabble.com>
Message-ID: <44C0C724.7050002@stats.uwo.ca>

On 7/21/2006 7:36 AM, nathan wrote:
> Hi, 
> 
> I'm just learning R and am encountering some unexpected results following a
> guide on the internet. If anyone can help that would be great - I think it
> is something about the way the data has been read in!
> 
> I've read a coma delimited text data file that was saved from excel:
> 
>> jacs.data <- read.table("/Users/natha/Desktop/JACSdata2.txt", header=TRUE,
>> sep=",")
> 
> This seemed to work fine, but then I start to get unusual results that don't
> seem right:
> 
> The guide says that names(file.data) will give output like "ID" "JURIS"
> "RESCODE" , but I get ID.JURIS.RESCODE.
> 
> The guide says that file.data[5,1] will give me the data for the 5th subject
> but i get: 
> [1] 7\tACT\t\t\tSUMMONS\tACTCC321.001\tA.C.T. - MINOR THEFT (REPLACEMENT
> VALUE $2000 OR LESS)\ etc - which seems scrambled

The "\t" values are tabs.  I think your file was tab delimited, not 
comma delimited.  R thinks it has only one column, because it didn't 
fine any commas.
> 
> The guide says that file.data[var5>0] will give me the data for all subject
> who meet that condition (ie greater than 0 on var5), but I get:
> 
> Error in "[.data.frame"(jacs.data, offend > 0) : 
> 	object "offend" not found

It looks like you typed jacs.data[offend > 0].  There are two problems:

1.  You want to select rows matching the condition, so you need another 
comma, i.e.

jacs.data[offend > 0, ]

(the empty entry after the comma means "all columns").

2.  You need to have a variable named offend outside the dataframe.  The 
error message makes it look as though you don't.

If offend is a column in the dataframe, then you would use

jacs.data[jacs.data$offend > 0, ]

or

subset(jacs.data, offend > 0)

Duncan Murdoch
> can anyone help? Thanks nathan


From MSchwartz at mn.rr.com  Fri Jul 21 14:26:35 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 21 Jul 2006 07:26:35 -0500
Subject: [R] unexpected results
In-Reply-To: <5432210.post@talk.nabble.com>
References: <5432210.post@talk.nabble.com>
Message-ID: <1153484795.4316.12.camel@localhost.localdomain>

On Fri, 2006-07-21 at 04:36 -0700, nathan wrote:
> Hi, 
> 
> I'm just learning R and am encountering some unexpected results following a
> guide on the internet. If anyone can help that would be great - I think it
> is something about the way the data has been read in!
> 
> I've read a coma delimited text data file that was saved from excel:
> 
> > jacs.data <- read.table("/Users/natha/Desktop/JACSdata2.txt", header=TRUE,
> > sep=",")
> 
> This seemed to work fine, but then I start to get unusual results that don't
> seem right:
> 
> The guide says that names(file.data) will give output like "ID" "JURIS"
> "RESCODE" , but I get ID.JURIS.RESCODE.
> 
> The guide says that file.data[5,1] will give me the data for the 5th subject
> but i get: 
> [1] 7\tACT\t\t\tSUMMONS\tACTCC321.001\tA.C.T. - MINOR THEFT (REPLACEMENT
> VALUE $2000 OR LESS)\ etc - which seems scrambled
> 
> The guide says that file.data[var5>0] will give me the data for all subject
> who meet that condition (ie greater than 0 on var5), but I get:
> 
> Error in "[.data.frame"(jacs.data, offend > 0) : 
> 	object "offend" not found
> 
> can anyone help? Thanks nathan

It would appear that your data file is NOT comma delimited, but TAB
delimited.

The "\t" characters in the output above support this, since you asked
for just the first column for the 5th subject and it appears that you
got the entire row.

Re-run the import using:

jacs.data <- read.table("/Users/natha/Desktop/JACSdata2.txt",
                        header=TRUE, sep = "\t")

so that you are indicating that the delimiter is a TAB character, not a
comma.

HTH,

Marc Schwartz


From petr.pikal at precheza.cz  Fri Jul 21 14:28:19 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 21 Jul 2006 14:28:19 +0200
Subject: [R] unexpected results
In-Reply-To: <5432210.post@talk.nabble.com>
Message-ID: <44C0E483.17282.130E6A0@localhost>

Hi

On 21 Jul 2006 at 4:36, nathan wrote:

Date sent:      	Fri, 21 Jul 2006 04:36:53 -0700 (PDT)
From:           	nathan <sirrahn at hotmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] unexpected results

> 
> Hi, 
> 
> I'm just learning R and am encountering some unexpected results
> following a guide on the internet. If anyone can help that would be
> great - I think it is something about the way the data has been read
> in!
> 
> I've read a coma delimited text data file that was saved from excel:
> 
> > jacs.data <- read.table("/Users/natha/Desktop/JACSdata2.txt",
> > header=TRUE, sep=",")
> 
> This seemed to work fine, but then I start to get unusual results that
I do not think so. Probably separation character of your file is not 
"," as you set in sep=",".

> don't seem right:
> 
> The guide says that names(file.data) will give output like "ID"
> "JURIS" "RESCODE" , but I get ID.JURIS.RESCODE.

e.g. ID.JURIS.RESCODE was read into one column.

Best way how to copy data from Excel (if you have Excel) 

Select your data in Excel including first row with headers
Ctrl-C
Go to R
Write mydata <- read.delim("clipboard")


or look at JACSdata2.txt what is the separator between fields and set 
it in your read.table command accordingly. From later I suppose your 
txt file is tab delimited so
read.delim(....)
shall capture it.

HTH
Petr

> 
> The guide says that file.data[5,1] will give me the data for the 5th
> subject but i get: [1] 7\tACT\t\t\tSUMMONS\tACTCC321.001\tA.C.T. -
> MINOR THEFT (REPLACEMENT VALUE $2000 OR LESS)\ etc - which seems
> scrambled
> The guide says that file.data[var5>0] will give me the data for all
> subject who meet that condition (ie greater than 0 on var5), but I
> get:
> 
> Error in "[.data.frame"(jacs.data, offend > 0) : 
>  object "offend" not found
> 
> can anyone help? Thanks nathan
> -- 
> View this message in context:
> http://www.nabble.com/unexpected-results-tf1979786.html#a5432210 Sent
> from the R help forum at Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From ashgene at yahoo.co.in  Fri Jul 21 14:43:03 2006
From: ashgene at yahoo.co.in (Ahamarshan jn)
Date: Fri, 21 Jul 2006 05:43:03 -0700 (PDT)
Subject: [R] Reading data with blank elements
Message-ID: <20060721124304.20976.qmail@web7601.mail.in.yahoo.com>

Hi,
 I have a dataset saved in *.csv format, that contains
13 columns (the first column being the title name and
the rest experiments) and about 2500 rows.
Not all columns in the row have data in it
i.e for eg
					
BS00,-0.084,0.0136,-0.1569,-0.6484,1.103,1.7859,0.40287,0.5368,0.08461,-0.1935,-0.147974,0.30685

BS01,0.491270283,0.875826172,,,,,,,,,,

BS02,0.090794476,0.225858954,,,0.32643,0.34317,0.133145295,,,0.115832599,0.47636458,

BS03,0.019828221,-0.095735935,-0.122767219,-0.0676,0.002533,-0.1510361,0.736247,2.053192,-0.423658,0.4591219,1.1245015,

BS04,-0.435189342,-0.041595955,-0.781281128,-1.923036,-3.230167102,,,,0.152322609,-1.495513519,,
				

I am using R to perform a correlation, but I am
getting an error while trying to read the data as


">
person.data<-read.table("datafile.csv",header=TRUE,sep=',',row.names=1)

Error in scan (file = file, what = what, sep = sep,
quote = quote, dec = dec,  : 
        line 1919 did not have 13 elements
Execution halted "
 
The error looks as though there is a problem with the
last element being not read when it is blank. I could
introduce terms like "na" to the blank elements but I
donot want to do that because this will hinder my
future analysis. 

Can some one suggest me a solution to overcome this
problem while reading the data? , or is there
something that I have missed to make the data
readable. 

Thank you in advance, 

PS: The data was imported from a experiment and saved
in excel sheet as a *.csv and then used.


From jholtman at gmail.com  Fri Jul 21 14:48:47 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 21 Jul 2006 08:48:47 -0400
Subject: [R] Reading data with blank elements
In-Reply-To: <20060721124304.20976.qmail@web7601.mail.in.yahoo.com>
References: <20060721124304.20976.qmail@web7601.mail.in.yahoo.com>
Message-ID: <644e1f320607210548r3872c08fpfbd61a7ecaa595f2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060721/b2bf905b/attachment.pl 

From ggrothendieck at gmail.com  Fri Jul 21 14:56:07 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 21 Jul 2006 08:56:07 -0400
Subject: [R] Reading data with blank elements
In-Reply-To: <20060721124304.20976.qmail@web7601.mail.in.yahoo.com>
References: <20060721124304.20976.qmail@web7601.mail.in.yahoo.com>
Message-ID: <971536df0607210556v52fa65f0x19180749ca69766c@mail.gmail.com>

See if this works:

read.csv("datafile.csv", row.names = 1, fill = TRUE)


On 7/21/06, Ahamarshan jn <ashgene at yahoo.co.in> wrote:
> Hi,
>  I have a dataset saved in *.csv format, that contains
> 13 columns (the first column being the title name and
> the rest experiments) and about 2500 rows.
> Not all columns in the row have data in it
> i.e for eg
>
> BS00,-0.084,0.0136,-0.1569,-0.6484,1.103,1.7859,0.40287,0.5368,0.08461,-0.1935,-0.147974,0.30685
>
> BS01,0.491270283,0.875826172,,,,,,,,,,
>
> BS02,0.090794476,0.225858954,,,0.32643,0.34317,0.133145295,,,0.115832599,0.47636458,
>
> BS03,0.019828221,-0.095735935,-0.122767219,-0.0676,0.002533,-0.1510361,0.736247,2.053192,-0.423658,0.4591219,1.1245015,
>
> BS04,-0.435189342,-0.041595955,-0.781281128,-1.923036,-3.230167102,,,,0.152322609,-1.495513519,,
>
>
> I am using R to perform a correlation, but I am
> getting an error while trying to read the data as
>
>
> ">
> person.data<-read.table("datafile.csv",header=TRUE,sep=',',row.names=1)
>
> Error in scan (file = file, what = what, sep = sep,
> quote = quote, dec = dec,  :
>        line 1919 did not have 13 elements
> Execution halted "
>
> The error looks as though there is a problem with the
> last element being not read when it is blank. I could
> introduce terms like "na" to the blank elements but I
> donot want to do that because this will hinder my
> future analysis.
>
> Can some one suggest me a solution to overcome this
> problem while reading the data? , or is there
> something that I have missed to make the data
> readable.
>
> Thank you in advance,
>
> PS: The data was imported from a experiment and saved
> in excel sheet as a *.csv and then used.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From roger.bos at gmail.com  Fri Jul 21 14:59:44 2006
From: roger.bos at gmail.com (roger bos)
Date: Fri, 21 Jul 2006 08:59:44 -0400
Subject: [R] positive semi-definite matrix
Message-ID: <1db726800607210559x1293bf5cs3cbd66e128bb80e8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060721/784ab91d/attachment.pl 

From danbebber at forestecology.co.uk  Fri Jul 21 15:22:36 2006
From: danbebber at forestecology.co.uk (Dan Bebber)
Date: Fri, 21 Jul 2006 14:22:36 +0100
Subject: [R] glm cannot find valid starting values
References: <00ea01c6acb9$d5d59770$d22401a3@plants.ox.ac.uk>
	<Pine.LNX.4.64.0607211301340.23904@gannet.stats.ox.ac.uk>
Message-ID: <00fe01c6acc8$bba77b70$d22401a3@plants.ox.ac.uk>

Brian Ripley wrote:
>
> BTW, your example cannot be pasted in as 'sdat' self-references.  It could
> be fixed, but I gave up at that point.
>
Oh dear, I'm very sorry. I forgot to run rm(list=ls(all=TRUE)) before 
testing.
The corrected code is:

#Data:
S <- c(0, 0, 0, 0, 28, 0, 1, 7, 0, 0, 39, 2, 0, 0, 40, 0, 0, 0, 0,
0, 0, 15, 0, 0, 3, 0, 0, 0, 2, 0, 3, 0, 30, 0, 20, 0, 1, 0, 0,
1, 21, 0, 0, 4, 14, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 3, 0,
2, 5, 0, 0, 0, 0, 0, 0, 0, 25, 0, 5, 0, 0, 0, 1, 0, 1, 0, 0,
0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
0, 0, 0, 0, 0, 1, 10, 0, 0, 0, 0, 0, 9, 1, 1, 1, 1, 0, 0, 3,
0, 27, 7, 0, 0, 0, 0, 0, 1, 1, 4, 2, 1, 2, 4, 1, 4, 6, 12, 4,
6, 3, 4, 0, 4, 0, 6, 1, 3, 1, 4, 4, 1, 1, 2, 1, 6, 1, 0, 0, 1,
0, 1, 0, 0, 6, 0, 0, 0, 0, 2, 2, 3, 1, 6, 2, 2, 1, 1, 4, 4, 3,
3, 7, 1, 3, 5, 6, 4, 0, 1, 4, 2, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
1, 0, 4, 0, 0, 1, 0, 2, 0, 0, 1, 2, 0, 4, 0, 2, 0, 3, 0, 2, 2,
0, 4, 0, 2, 0, 1, 2, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 3, 2, 1, 2,
1, 1, 2, 0, 0, 3, 3, 1, 0, 1, 1, 2, 0, 1, 0, 1, 0, 1, 3, 2, 0,
0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 4, 2, 4, 0, 2, 0, 0, 0, 0, 0,
0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 3, 2, 0, 0, 0, 0, 0,
0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 1, 0, 0, 1)
M <- 620+c(0,cumsum(S[-328]))
sdat <- data.frame(S,M)
#S is the number of N individuals that irreversibly change state in a time
#interval t. The data provided are a subset of the full data set.
#M is the cumulative sum of individuals that have changed state up to t-1.
#Assume that the rate of state change is constant (S ~ kM), but the
#distribution of S is clustered.
#N can be estimated by fitting:
qpglm <- glm(S ~ M, family = quasipoisson(link = identity), sdat)
summary(qpglm)
N.est <- -coef(qpglm)[1]/coef(qpglm)[2]
N.est
#i.e. x-intercept is minus intercept/slope
#To estimate confidence limits on N.est, fit models without intercept to
#N.est - M + x, where x is an integer. The model will have the lowest 
deviance
#when x = 0.
x <- 0
Mdif <- N.est - M + x
qpglm2 <- glm(S ~ -1 + Mdif, family = quasipoisson(link = identity), sdat)
summary(qpglm2)
#Use analysis of deviance to estimate confidence limits on N.est:
disp <- summary(qpglm)$dispersion
dfres <- df.residual(qpglm)
dev.res <- deviance(qpglm)
#From MASS4, p. 210, assume that changes in deviance scaled by
#dispersion as |x| increases have an F distribution
dev.crit <- dev.res+qf(0.95,1,dfres)*disp
dev.crit
#values of x for which the deviance = dev.crit give approximate 95% 
confidence limits
#on N.est.
#The error occurs when x <= -91.7:
x <- -91.7
sdat$Mdif <- N.est - sdat$M + x
strt <- coef(glm(S ~ -1 + I(Mdif+1), family = quasipoisson(link = identity), 
sdat))
qpglm2 <- glm(S ~ -1 + Mdif, family = quasipoisson(link = identity), 
start=strt, sdat)


From philipp.pagel.lists at t-online.de  Fri Jul 21 15:34:36 2006
From: philipp.pagel.lists at t-online.de (Philipp Pagel)
Date: Fri, 21 Jul 2006 15:34:36 +0200
Subject: [R] Reading data with blank elements
In-Reply-To: <20060721124304.20976.qmail@web7601.mail.in.yahoo.com>
References: <20060721124304.20976.qmail@web7601.mail.in.yahoo.com>
Message-ID: <20060721133436.GA27927@gsf.de>


	Hi!

On Fri, Jul 21, 2006 at 05:43:03AM -0700, Ahamarshan jn wrote:
>  I have a dataset saved in *.csv format, that contains
[...]

> BS00,-0.084,0.0136,-0.1569,-0.6484,1.103,1.7859,0.40287,0.5368,0.08461,-0.1935,-0.147974,0.30685
> BS01,0.491270283,0.875826172,,,,,,,,,,
> BS02,0.090794476,0.225858954,,,0.32643,0.34317,0.133145295,,,0.115832599,0.47636458,
> BS03,0.019828221,-0.095735935,-0.122767219,-0.0676,0.002533,-0.1510361,0.736247,2.053192,-0.423658,0.4591219,1.1245015,
> BS04,-0.435189342,-0.041595955,-0.781281128,-1.923036,-3.230167102,,,,0.152322609,-1.495513519,,


> person.data<-read.table("datafile.csv",header=TRUE,sep=',',row.names=1)
> 
> Error in scan (file = file, what = what, sep = sep,
> quote = quote, dec = dec,  : 
>         line 1919 did not have 13 elements
> Execution halted "

R does handle empty elements fine. The error message you quote occurs if
a row does not contain the expected number of elements (empty or not.)

Did you have a look at row 1919? Does it really contain the same number
of separators (commas) as the other ones?

Some programs handle empty elements at the end of a row in a 'lazy' way
and simply ommit them. If this is the case you can use the option
'fill=TRUE' to tell read.table that you want it to silently pad short
rows with empty elements.

Another 'popular' reason for funny errors with read.table is the
unexpected occurence of quotation or comment characters in the data...

cu
	Philipp

-- 
Dr. Philipp Pagel                            Tel.  +49-8161-71 2131
Dept. of Genome Oriented Bioinformatics      Fax.  +49-8161-71 2186
Technical University of Munich
Science Center Weihenstephan
85350 Freising, Germany

 and

Institute for Bioinformatics / MIPS          Tel.  +49-89-3187 3675
GSF - National Research Center               Fax.  +49-89-3187 3585
      for Environment and Health
Ingolst?dter Landstrasse 1
85764 Neuherberg, Germany
http://mips.gsf.de/staff/pagel


From murdoch at stats.uwo.ca  Fri Jul 21 15:44:42 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 21 Jul 2006 09:44:42 -0400
Subject: [R] positive semi-definite matrix
In-Reply-To: <1db726800607210559x1293bf5cs3cbd66e128bb80e8@mail.gmail.com>
References: <1db726800607210559x1293bf5cs3cbd66e128bb80e8@mail.gmail.com>
Message-ID: <44C0DA4A.4030506@stats.uwo.ca>

On 7/21/2006 8:59 AM, roger bos wrote:
> I have a covariance matrix that is not positive semi-definite matrix and I
> need it to be via some sort of adjustment.  Is there any R routine or
> package to help me do this?

I think you need to be more specific about what have and what you want, 
but if the matrix is symmetric and nearly positive semi-definite (but 
not exactly because of rounding error), you might try something like

fixit <- function(A) {
   eig <- eigen(A, symmetric = TRUE)
   eig$values <- pmax(0, eig$values)
   return(eig$vectors %*% diag(eig$values) %*% t(eig$vectors))
}

Rounding error means this is not guaranteed to be positive 
semi-definite, but it will be very close.

Duncan Murdoch


From g.munoz at imperial.ac.uk  Fri Jul 21 16:32:00 2006
From: g.munoz at imperial.ac.uk (Munoz-Melendez, Gabriela)
Date: Fri, 21 Jul 2006 15:32:00 +0100
Subject: [R] (no subject)
Message-ID: <76ECA97A30510E498EB5A97AB4A6AE210119E1F8@icex1.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060721/d6200c18/attachment.pl 

From maechler at stat.math.ethz.ch  Fri Jul 21 17:00:41 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 21 Jul 2006 17:00:41 +0200
Subject: [R] positive semi-definite matrix
In-Reply-To: <44C0DA4A.4030506@stats.uwo.ca>
References: <1db726800607210559x1293bf5cs3cbd66e128bb80e8@mail.gmail.com>
	<44C0DA4A.4030506@stats.uwo.ca>
Message-ID: <17600.60441.937333.542946@stat.math.ethz.ch>


>>>>> "Duncan" == Duncan Murdoch <murdoch at stats.uwo.ca>
>>>>>     on Fri, 21 Jul 2006 09:44:42 -0400 writes:

    Duncan> On 7/21/2006 8:59 AM, roger bos wrote:
    >> I have a covariance matrix that is not positive semi-definite matrix and I
    >> need it to be via some sort of adjustment.  Is there any R routine or
    >> package to help me do this?

    Duncan> I think you need to be more specific about what have and what you want, 
    Duncan> but if the matrix is symmetric and nearly positive semi-definite (but 
    Duncan> not exactly because of rounding error), you might try something like

    Duncan> fixit <- function(A) {
    Duncan> eig <- eigen(A, symmetric = TRUE)
    Duncan> eig$values <- pmax(0, eig$values)
    Duncan> return(eig$vectors %*% diag(eig$values) %*% t(eig$vectors))
    Duncan> }

    Duncan> Rounding error means this is not guaranteed to be positive 
    Duncan> semi-definite, but it will be very close.

A slightly more general and stable version of the above
is available via  sfsmisc::posdefify(.) :

install.packages("sfsmisc")
?posdefify


Martin Maechler, ETH Zurich


From vsdimitrov at yahoo.com  Fri Jul 21 17:03:07 2006
From: vsdimitrov at yahoo.com (Valentin Dimitrov)
Date: Fri, 21 Jul 2006 08:03:07 -0700 (PDT)
Subject: [R] Weibull distribution
In-Reply-To: <200607171218066725256@yahoo.ca>
Message-ID: <20060721150307.87753.qmail@web30806.mail.mud.yahoo.com>

Dear Leaf,

I modified your code as follows:

gamma.fun <- function(mu,sd,start=100)    
 {
f.fn <- function(alpha) 
{abs(sd^2-mu^2/(gamma(1+1/alpha))^2*(gamma(1+2/alpha)-(gamma(1+1/alpha))^2))}
alpha <- optim(start, f.fn)
beta <- mu/gamma(1+1/alpha$par)
return(list=c(a=alpha$par,b=beta));
 }

Now it works properly.

First, I added an abs(). You tried to solve an
equation by means of the R-function optim(), which
finds a minimum. That's why you can find the solution
of f(x)=a through minimization of abs(f(x)-a).
Second, I deleted the optim-method BFGS from the
optim() function, because it is not appropriate in
this case. BFGS seeks to make the gradient (or here
the first derivative) zero and in your case f(x)
converges to a constant for big x, which means f'(x)
is approximately 0 for big x, which is why BFGS stops
almost immediately after the start value. The default
method of optim() ( Nelder and Mead ) is more
appropriate, since it does not need the first
derivative and works only with function values.

Best regards,
Valentin


--- Leaf Sun <leaflovesun at yahoo.ca> wrote:

> Hi all,
> 
> By its definition, the mean and variance of two-par.
> Weibull distribution are:
> 
>  
> 
>  
> 
>  (www.wikipedia.org)
> 
> 
> I was wondering, if given mean and sd. could we
> parameterize the distribution? I tried this in R.
> 
> gamma.fun <- function(mu,sd,start=100)    
> {
> f.fn <- function(alpha)
>
sd^2-mu^2/(gamma(1+1/alpha))^2*(gamma(1+2/alpha)-(gamma(1+1/alpha))^2)
> alpha <- optim(start, f.fn,method='BFGS')
> beta <- mu/gamma(1+1/alpha$par)
> return(list=c(a=alpha$par,b=beta));
> }
> 
> 
> But the problems come up here:
> 
> 1)  the return values of a and b are only related to
> the input mean, and nothing to do with the sd. For
> instance, when I apply a mean mu = 3 whatever I use
> sd=2, sd=4, the function returned the same scale and
> shape values.
> 
> > gamma.fun(3,4,10);
>        a        b 
> 5.112554 3.263178 
> 
> > gamma.fun(3,2,10);
>        a        b 
> 5.112554 3.263178 
> 
> 2) the start value determines the results: if I
> apply mean = 3, and sd=2, with a start of 10, it
> would return alpha close to 10, if I use a start =
> 100, it would return alpha close to 100.
> 
> > gamma.fun(3,2,10);
>        a        b 
> 5.112554 3.263178 
> 
> > gamma.fun(3,2,100);
>         a         b 
> 99.999971  3.017120 
> 
> Since I am not a statistician, I guess there must be
> some theoretical reasons wrong with this question.
> So I am looking forward to some correction and
> advice to solve these. Thanks a lot in advance!
> 
> Leaf
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


From peter at estg.ipvc.pt  Fri Jul 21 17:04:06 2006
From: peter at estg.ipvc.pt (Peter Ho)
Date: Fri, 21 Jul 2006 16:04:06 +0100
Subject: [R] : Arial font for text in lattice plots under Linux
Message-ID: <44C0ECE6.3050902@estg.ipvc.pt>

Hi,

I have been asked by a publisher to change the font style of a lattice 
plot in my manuscript. I have consulted documentation on trellis 
graphics and the excellent book "R graphics", but am still not sure how 
I could create plots with Arial as the font style for text in the plot.  
I am running R (Version 2.3.1 (2006-06-01)) under debian Linux. I have 
msttcorefonts installed.

Will it be possible to do this with jpeg() as a device? Or with 
postscript()?




Peter


From tlumley at u.washington.edu  Fri Jul 21 17:22:31 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 21 Jul 2006 08:22:31 -0700 (PDT)
Subject: [R] Order-restricted inference
In-Reply-To: <012701c6ac5c$e5007920$6400a8c0@brungio>
References: <012701c6ac5c$e5007920$6400a8c0@brungio>
Message-ID: <Pine.LNX.4.64.0607210821200.17202@homer21.u.washington.edu>

On Thu, 20 Jul 2006, Bruno L. Giordano wrote:
>
> If R packages are not found for my purpose, can somebody please point me to
> some recent monographs on order-restricted inference?
>

One more recent book is:
   Robertson, T., Wright, F., & Dykstra, R. (1988). Order restricted
   statistical inference. New York: John Wiley and Sons.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From gerifalte28 at hotmail.com  Fri Jul 21 17:27:17 2006
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 21 Jul 2006 15:27:17 +0000
Subject: [R] positive semi-definite matrix
In-Reply-To: <44C0DA4A.4030506@stats.uwo.ca>
Message-ID: <BAY103-F408D66A6D1DAEBA1489AE9A6660@phx.gbl>

Take a look at make.positive.definite in the corpcor package.  The 
implementation is very similar to what Duncan suggested.

Regards,

Francisco

Dr. Francisco J. Zagmutt
College of Veterinary Medicine and Biomedical Sciences
Colorado State University




>From: Duncan Murdoch <murdoch at stats.uwo.ca>
>To: roger bos <roger.bos at gmail.com>
>CC: RHelp <r-help at stat.math.ethz.ch>
>Subject: Re: [R] positive semi-definite matrix
>Date: Fri, 21 Jul 2006 09:44:42 -0400
>
>On 7/21/2006 8:59 AM, roger bos wrote:
> > I have a covariance matrix that is not positive semi-definite matrix and 
>I
> > need it to be via some sort of adjustment.  Is there any R routine or
> > package to help me do this?
>
>I think you need to be more specific about what have and what you want,
>but if the matrix is symmetric and nearly positive semi-definite (but
>not exactly because of rounding error), you might try something like
>
>fixit <- function(A) {
>    eig <- eigen(A, symmetric = TRUE)
>    eig$values <- pmax(0, eig$values)
>    return(eig$vectors %*% diag(eig$values) %*% t(eig$vectors))
>}
>
>Rounding error means this is not guaranteed to be positive
>semi-definite, but it will be very close.
>
>Duncan Murdoch
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From rvaradhan at jhmi.edu  Fri Jul 21 17:33:23 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Fri, 21 Jul 2006 11:33:23 -0400
Subject: [R] positive semi-definite matrix
In-Reply-To: <44C0DA4A.4030506@stats.uwo.ca>
Message-ID: <001001c6acdb$00cf78d0$7c94100a@win.ad.jhu.edu>

There is a paper by N.J. Higham (SIAM J Matrix Anal, 1998) on a modified
cholesky decomposition of symmetric and not necessarily positive definite
matrix (say, A), with an important goal of producing a "small-normed"
perturbation of A (say, delA), that makes (A + delA) positive definite.
http://epubs.siam.org/sam-bin/dbq/article/30289

There is also an algorithm in Gill, Murray and Wright's text - Practical
Optimization (section 4.4.2).

These may be relevant to your problem.  I am not sure if these algorithms
have been implemented in R, for example, in the "matrix" library. 

Ravi.


--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
Webpage: http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html 
--------------------------------------------------------------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Duncan Murdoch
> Sent: Friday, July 21, 2006 9:45 AM
> To: roger bos
> Cc: RHelp
> Subject: Re: [R] positive semi-definite matrix
> 
> On 7/21/2006 8:59 AM, roger bos wrote:
> > I have a covariance matrix that is not positive semi-definite matrix and
> I
> > need it to be via some sort of adjustment.  Is there any R routine or
> > package to help me do this?
> 
> I think you need to be more specific about what have and what you want,
> but if the matrix is symmetric and nearly positive semi-definite (but
> not exactly because of rounding error), you might try something like
> 
> fixit <- function(A) {
>    eig <- eigen(A, symmetric = TRUE)
>    eig$values <- pmax(0, eig$values)
>    return(eig$vectors %*% diag(eig$values) %*% t(eig$vectors))
> }
> 
> Rounding error means this is not guaranteed to be positive
> semi-definite, but it will be very close.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tlumley at u.washington.edu  Fri Jul 21 17:35:01 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 21 Jul 2006 08:35:01 -0700 (PDT)
Subject: [R] Weibull distribution
In-Reply-To: <20060721150307.87753.qmail@web30806.mail.mud.yahoo.com>
References: <20060721150307.87753.qmail@web30806.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0607210831420.17202@homer21.u.washington.edu>

On Fri, 21 Jul 2006, Valentin Dimitrov wrote:

> Dear Leaf,
>
> I modified your code as follows:
>
> gamma.fun <- function(mu,sd,start=100)
> {
> f.fn <- function(alpha)
> {abs(sd^2-mu^2/(gamma(1+1/alpha))^2*(gamma(1+2/alpha)-(gamma(1+1/alpha))^2))}
> alpha <- optim(start, f.fn)
> beta <- mu/gamma(1+1/alpha$par)
> return(list=c(a=alpha$par,b=beta));
> }
>
> Now it works properly.
>
> First, I added an abs(). You tried to solve an
> equation by means of the R-function optim(), which
> finds a minimum. That's why you can find the solution
> of f(x)=a through minimization of abs(f(x)-a).
> Second, I deleted the optim-method BFGS from the
> optim() function, because it is not appropriate in
> this case.

optim() is not appropriate at all in this case -- its help page says to 
use optimize() for one-dimensional problems.

In fact, in one dimension there isn't any need to resort to optimization 
when you really want root-finding, and uniroot() is more appropriate than 
optimize().


 	-thomas


From lindend1 at msu.edu  Fri Jul 21 17:43:46 2006
From: lindend1 at msu.edu (Dan Linden)
Date: Fri, 21 Jul 2006 08:43:46 -0700 (PDT)
Subject: [R] Problems with character spacing in windows metafiles...
Message-ID: <5435945.post@talk.nabble.com>


This problem was posted a couple years ago here:

http://tolstoy.newcastle.edu.au/R/help/04/01/0231.html

Using Windows XP with R 2.3.1, I am now experiencing the same problem again:
when a plot is saved and/or copied as a WMF, the labels do not have the
correct character spacing.  I am trying to insert the WMF into a MS Word
2003 document (my first mistake, I know), but even when the WMF is opened
with other graphics software, the problem remains.  Must be something with
the way the WMF is written.

The strange this is that I was able to do this on another computer with a
slilghtly older version of R (2.2.x?), and did not have the problem.  The
version of Microsoft Word was the same on both, and the code did not matter
(any basic plot with any font had its text labels squeezed).

I've done some searching and haven't come up with any good solutions.  For
Windows users, the WMF is the most convenient format for saving vector
graphics.  EPS is not an option for me.

Any help would be greatly appreciated.

Cheers,
Dan
-- 
View this message in context: http://www.nabble.com/Problems-with-character-spacing-in-windows-metafiles...-tf1980921.html#a5435945
Sent from the R help forum at Nabble.com.


From ripley at stats.ox.ac.uk  Fri Jul 21 18:02:42 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 21 Jul 2006 17:02:42 +0100 (BST)
Subject: [R] : Arial font for text in lattice plots under Linux
In-Reply-To: <44C0ECE6.3050902@estg.ipvc.pt>
References: <44C0ECE6.3050902@estg.ipvc.pt>
Message-ID: <Pine.LNX.4.64.0607211648070.7462@gannet.stats.ox.ac.uk>

This is a question about devices, not lattice per se, so applies to all 
forms of plotting.

For jpeg, you can specify the fonts via the 'fonts' argument: see ?X11.

For postscript, you can convert the fonts to Type 1 via e.g. ttf2pt1, or 
you can use ttf2afm to make .afm files and a postscript interpreter that 
can handle TrueType fonts.

On Fri, 21 Jul 2006, Peter Ho wrote:

> Hi,
> 
> I have been asked by a publisher to change the font style of a lattice 
> plot in my manuscript. I have consulted documentation on trellis 
> graphics and the excellent book "R graphics", but am still not sure how 
> I could create plots with Arial as the font style for text in the plot.  
> I am running R (Version 2.3.1 (2006-06-01)) under debian Linux. I have 
> msttcorefonts installed.
> 
> Will it be possible to do this with jpeg() as a device? Or with 
> postscript()?
> 
> 
> 
> 
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mschwartz at mn.rr.com  Fri Jul 21 18:08:51 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 21 Jul 2006 11:08:51 -0500
Subject: [R] Reading data with blank elements
In-Reply-To: <20060721124304.20976.qmail@web7601.mail.in.yahoo.com>
References: <20060721124304.20976.qmail@web7601.mail.in.yahoo.com>
Message-ID: <1153498131.3874.6.camel@localhost.localdomain>

On Fri, 2006-07-21 at 05:43 -0700, Ahamarshan jn wrote:
> Hi,
>  I have a dataset saved in *.csv format, that contains
> 13 columns (the first column being the title name and
> the rest experiments) and about 2500 rows.
> Not all columns in the row have data in it
> i.e for eg
> 					
> BS00,-0.084,0.0136,-0.1569,-0.6484,1.103,1.7859,0.40287,0.5368,0.08461,-0.1935,-0.147974,0.30685
> 
> BS01,0.491270283,0.875826172,,,,,,,,,,
> 
> BS02,0.090794476,0.225858954,,,0.32643,0.34317,0.133145295,,,0.115832599,0.47636458,
> 
> BS03,0.019828221,-0.095735935,-0.122767219,-0.0676,0.002533,-0.1510361,0.736247,2.053192,-0.423658,0.4591219,1.1245015,
> 
> BS04,-0.435189342,-0.041595955,-0.781281128,-1.923036,-3.230167102,,,,0.152322609,-1.495513519,,
> 				
> 
> I am using R to perform a correlation, but I am
> getting an error while trying to read the data as
> 
> 
> ">
> person.data<-read.table("datafile.csv",header=TRUE,sep=',',row.names=1)
> 
> Error in scan (file = file, what = what, sep = sep,
> quote = quote, dec = dec,  : 
>         line 1919 did not have 13 elements
> Execution halted "
>  
> The error looks as though there is a problem with the
> last element being not read when it is blank. I could
> introduce terms like "na" to the blank elements but I
> donot want to do that because this will hinder my
> future analysis. 
> 
> Can some one suggest me a solution to overcome this
> problem while reading the data? , or is there
> something that I have missed to make the data
> readable. 
> 
> Thank you in advance, 
> 
> PS: The data was imported from a experiment and saved
> in excel sheet as a *.csv and then used.

You have already had other replies, to which I would add, be sure to
read Chapter 8 in the R Import/Export Manual regarding importing Excel
files and other options besides exporting to a CSV file.

In addition, the issue of Excel generating CSV files with the last
column missing on some rows is a known issue and is reported in the MSKB
here:

http://support.microsoft.com/default.aspx?scid=kb;EN-US;q77295

Even though the latest version of Excel listed in the article as being
relevant is 97, I had this problem with 2000 and 2003 as well.

I would instead use OpenOffice.org's Calc to do the export when this was
required. Calc did not have this problem.

HTH,

Marc Schwartz


From mnair at iusb.edu  Fri Jul 21 18:21:57 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Fri, 21 Jul 2006 12:21:57 -0400
Subject: [R] Multcomp plotting
References: <5435945.post@talk.nabble.com>
Message-ID: <A32055BDEA88C34BB3DBBCD229380778050FA0@iu-mssg-mbx109.ads.iu.edu>

I am using the multcomp package for doing multiple comparisons. Since the data I am handling is huge the number of comparisons are also large. I am interested in:
1>  Breaking down my plots to get rid of the clutter that happens when plotting the entire data set. How do I pass only part of the data to the plot function ?
 
fungus.cirec<-simint(Fungus.yield~Habitat, data=fungus,conf.level=0.95,type =c("Tukey"))
plot(fungus.cirec)  #This plots the entire data. I want to plot part of the data only
 
2>I am also interested in getting rid of the field name associated with each categorical variable. 
Here is what the part of the data looks like
 
Habitat Fungus.yield
Birch 20.83829053
Birch 22.9718181
Birch 22.28216829
Birch 24.23136797
Birch 22.32147961
Birch 20.30783598
Oak 27.24047258
Oak 29.7730014
Oak 30.12608508
Oak 25.76088669
Oak 30.14750974
Hornbeam 17.05307949
Hornbeam 15.32805111
Hornbeam 18.26920177
Hornbeam 21.30987049
Hornbeam 21.7173223
When it plots labels it as HabitatBirch-HabitatOak for example.  How do I get rid of the field name Habitat in the plot?
 
3> How do I tell the method to mark the significant comparisons? (i.e those that do not intersect the zero line).
 
Thanks ../Murli


From alexisjdiamond at gmail.com  Fri Jul 21 18:55:30 2006
From: alexisjdiamond at gmail.com (Alexis Diamond)
Date: Fri, 21 Jul 2006 12:55:30 -0400
Subject: [R] seeking robust test for equality of variances w/ observation
	weights
Message-ID: <67be2ce30607210955t4430bbcbp56b81ad58c031203@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060721/dbe3db57/attachment.pl 

From mnair at iusb.edu  Fri Jul 21 19:04:03 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Fri, 21 Jul 2006 13:04:03 -0400
Subject: [R] multicomp plotting
In-Reply-To: <67be2ce30607210955t4430bbcbp56b81ad58c031203@mail.gmail.com>
Message-ID: <A32055BDEA88C34BB3DBBCD2293807784CB77A@iu-mssg-mbx109.ads.iu.edu>


I am using the multcomp package for doing multiple comparisons. Since
the data I am handling is huge the number of comparisons are also large.
I am interested in:
1>  Breaking down my plots to get rid of the clutter that happens when
plotting the entire data set. How do I pass only part of the data to the
plot function ?
 
fungus.cirec<-simint(Fungus.yield~Habitat,
data=fungus,conf.level=0.95,type =c("Tukey"))
plot(fungus.cirec)  #This plots the entire data. I want to plot part of
the data only
 
2>I am also interested in getting rid of the field name associated with
each categorical variable. 
Here is what the part of the data looks like
 
Habitat Fungus.yield
Birch 20.83829053
Birch 22.9718181
Birch 22.28216829
Birch 24.23136797
Birch 22.32147961
Birch 20.30783598
Oak 27.24047258
Oak 29.7730014
Oak 30.12608508
Oak 25.76088669
Oak 30.14750974
Hornbeam 17.05307949
Hornbeam 15.32805111
Hornbeam 18.26920177
Hornbeam 21.30987049
Hornbeam 21.7173223
It plots labels as HabitatBirch-HabitatOak for example.  How do I get
rid of the field name Habitat in the plot? 
 
3> How do I tell the method to mark the significant comparisons? (i.e
those that do not intersect the zero line).
 
Thanks ../Murli


From bi-info at home.nl  Fri Jul 21 19:12:16 2006
From: bi-info at home.nl (Bi-Info (http://members.home.nl/bi-info))
Date: Fri, 21 Jul 2006 19:12:16 +0200
Subject: [R]  A statistical question
Message-ID: <44C10AF0.1020303@home.nl>

Dear Users,

I have two particular problems that I need to solve.

I do an analysis of a survey about sexuality.

(Don't read any further if you don't like the subject.)

Problem (1)

In the survey there a two questions about monogamy.

(1) Are you monogamous? (At this moment.)
(2) Have you been in contact with men and / or women? (Past / Present)

For some other inferences I need to extract historical data out of these 
questions about monogamy, like past monogamy (which hasn't been asked). 
This should be possible. Is there a reasonable way out of here?

Problem (2)

I have to do some geographical testing. I have to check the geographical 
distribution of respondents and some other properties. Could you advise 
me what to read / use? An R package with help is sufficient (I hope).


Thanks,

Wilfred


From maechler at stat.math.ethz.ch  Fri Jul 21 19:13:39 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 21 Jul 2006 19:13:39 +0200
Subject: [R] positive semi-definite matrix
In-Reply-To: <001001c6acdb$00cf78d0$7c94100a@win.ad.jhu.edu>
References: <44C0DA4A.4030506@stats.uwo.ca>
	<001001c6acdb$00cf78d0$7c94100a@win.ad.jhu.edu>
Message-ID: <17601.2883.656841.280555@stat.math.ethz.ch>

>>>>> "Ravi" == Ravi Varadhan <rvaradhan at jhmi.edu>
>>>>>     on Fri, 21 Jul 2006 11:33:23 -0400 writes:

    Ravi> There is a paper by N.J. Higham (SIAM J Matrix Anal,
    Ravi> 1998) on a modified cholesky decomposition of
    Ravi> symmetric and not necessarily positive definite matrix
    Ravi> (say, A), with an important goal of producing a
    Ravi> "small-normed" perturbation of A (say, delA), that
    Ravi> makes (A + delA) positive definite.

    Ravi> http://epubs.siam.org/sam-bin/dbq/article/30289

    Ravi> There is also an algorithm in Gill, Murray and
    Ravi> Wright's text - Practical Optimization (section
    Ravi> 4.4.2).

Thanks a lot, Ravi,
for the interesting references, in the past I once had looked
for such things but did not find any --- most probably because I
used wrong keywords.

    Ravi> These may be relevant to your problem.  I am not sure
    Ravi> if these algorithms have been implemented in R, for
    Ravi> example, in the "matrix" library.

Ooooo... !  It's  "Matrix" and  `package', yes `package', yes `package' ..

but no, it hasn't been implemented there yet, AFAIK.
OTOH, it's not a bad idea to do there, since it's building on
the  LDL' cholesky factorization   which we are using
in "Matrix" in other places anyway.

Thanks again for your help!
Martin Maechler, ETH Zurich

    Ravi> --------------------------------------------------------------------------
    Ravi> Ravi Varadhan, Ph.D.
    Ravi> Assistant Professor,  The Center on Aging and Health
    Ravi> Division of Geriatric Medicine and Gerontology
    Ravi> Johns Hopkins University
    Ravi> Ph: (410) 502-2619
    Ravi> Fax: (410) 614-9625
    Ravi> Email:  rvaradhan at jhmi.edu
    Ravi> Webpage: http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html 
    Ravi> --------------------------------------------------------------------------

    >> -----Original Message-----
    >> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
    >> bounces at stat.math.ethz.ch] On Behalf Of Duncan Murdoch
    >> Sent: Friday, July 21, 2006 9:45 AM
    >> To: roger bos
    >> Cc: RHelp
    >> Subject: Re: [R] positive semi-definite matrix
    >> 
    >> On 7/21/2006 8:59 AM, roger bos wrote:
    >> > I have a covariance matrix that is not positive semi-definite matrix and
    >> I
    >> > need it to be via some sort of adjustment.  Is there any R routine or
    >> > package to help me do this?
    >> 
    >> I think you need to be more specific about what have and what you want,
    >> but if the matrix is symmetric and nearly positive semi-definite (but
    >> not exactly because of rounding error), you might try something like
    >> 
    >> fixit <- function(A) {
    >> eig <- eigen(A, symmetric = TRUE)
    >> eig$values <- pmax(0, eig$values)
    >> return(eig$vectors %*% diag(eig$values) %*% t(eig$vectors))
    >> }
    >> 
    >> Rounding error means this is not guaranteed to be positive
    >> semi-definite, but it will be very close.
    >> 
    >> Duncan Murdoch


From bruno.giordano at music.mcgill.ca  Fri Jul 21 19:25:25 2006
From: bruno.giordano at music.mcgill.ca (Bruno L. Giordano)
Date: Fri, 21 Jul 2006 13:25:25 -0400
Subject: [R] Order-restricted inference
References: <012701c6ac5c$e5007920$6400a8c0@brungio>
	<Pine.LNX.4.64.0607210821200.17202@homer21.u.washington.edu>
Message-ID: <00b101c6acea$a88e7170$6400a8c0@brungio>

I found this other more recent monograph:

Silvapulle and Sen (October 2004), Constrained Statistical Inference: Order,
Inequality, and Shape Constraints, Wiley-Interscience.

http://ca.wiley.com/WileyCDA/WileyTitle/productCd-0471208272,descCd-reviews.html

Thanks for the tip,
    Bruno


----- Original Message ----- 
From: "Thomas Lumley" <tlumley at u.washington.edu>
To: "Bruno L. Giordano" <bruno.giordano at music.mcgill.ca>
Cc: <r-help at stat.math.ethz.ch>
Sent: Friday, July 21, 2006 11:22 AM
Subject: Re: [R] Order-restricted inference


> On Thu, 20 Jul 2006, Bruno L. Giordano wrote:
>>
>> If R packages are not found for my purpose, can somebody please point me
>> to
>> some recent monographs on order-restricted inference?
>>
>
> One more recent book is:
>   Robertson, T., Wright, F., & Dykstra, R. (1988). Order restricted
>   statistical inference. New York: John Wiley and Sons.
>
>  -thomas
>
> Thomas Lumley Assoc. Professor, Biostatistics
> tlumley at u.washington.edu University of Washington, Seattle
>

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Bruno L. Giordano, Ph.D.
CIRMMT
Schulich School of Music, McGill University
555 Sherbrooke Street West
Montr?al, QC H3A 1E3
Canada
http://www.music.mcgill.ca/~bruno/


From gunter.berton at gene.com  Fri Jul 21 19:35:45 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 21 Jul 2006 10:35:45 -0700
Subject: [R] seeking robust test for equality of variances w/
	observationweights
In-Reply-To: <67be2ce30607210955t4430bbcbp56b81ad58c031203@mail.gmail.com>
Message-ID: <004301c6acec$195c3350$725afea9@gne.windows.gene.com>

You can always bootstrap any robust spread measure (e.g. mad or higher
efficiency versions from robustbase or other packages).

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Alexis Diamond
> Sent: Friday, July 21, 2006 9:56 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] seeking robust test for equality of variances w/ 
> observationweights
> 
> Hello R community,
> 
> I am looking for a robust test for equality of variances that can take
> observation weights.
> I realize I can do the F-test with weighted variances, but 
> I've read that
> this test is not very robust.
> 
> So I thought about maybe adding a "weights" argument to John 
> Fox's code for
> the Levene Test (in the "car" library, "levene.test"),
> substituting his "median" function for a " weighted.mean" and 
> also including
> the observation weights in his "lm" run--
> after all, Levene's original test used the mean, not the median.
> 
> I asked John about it and he doesn't know what the properties of this
> weighted Levene test would be.
> Does anyone have any thoughts or suggestions, or know of a 
> robust weighted
> hypothesis test for equality of variances?
> 
> Thank you in advance for any advice you can provide,
> 
> Alexis Diamond
> adiamond at fas.harvard.edu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From georg.otto at tuebingen.mpg.de  Fri Jul 21 19:39:09 2006
From: georg.otto at tuebingen.mpg.de (Georg Otto)
Date: Fri, 21 Jul 2006 19:39:09 +0200
Subject: [R] intersect of list elements
Message-ID: <m1ejweg8n6.fsf@tuebingen.mpg.de>


Hi,

i have a list of several vectors, for example:

> vectorlist
$vector.a.1
[1] "a" "b" "c"

$vector.a.2
[1] "a" "b" "d"

$vector.b.1
[1] "e" "f" "g"


I can use intersect to find elements that appear in $vector.a.1 and
$vector.a.2:

> intersect(vectorlist[[1]], vectorlist[[2]])
[1] "a" "b"


I would like to use grep to get the vectors by their names matching an
expression and to find the intersects between those vectors. For the
first step:

> vectorlist[grep ("vector.a", names(vectorlist))]
$vector.a.1
[1] "a" "b" "c"

$vector.a.2
[1] "a" "b" "d"


Unfortunately, I can not pass the two vectors as argument to intersect:

> intersect(vectorlist[grep ("vector.a", names(vectorlist))])
Error in unique(y[match(x, y, 0)]) : argument "y" is missing, with no default

I am running R Version 2.3.1 (2006-06-01) 


Could somone help me to solve this?

Cheers,

Georg


From gunter.berton at gene.com  Fri Jul 21 19:55:00 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 21 Jul 2006 10:55:00 -0700
Subject: [R] intersect of list elements
In-Reply-To: <m1ejweg8n6.fsf@tuebingen.mpg.de>
Message-ID: <006101c6acee$c9b4a190$725afea9@gne.windows.gene.com>

FAQ 7.21.

But there are perhaps slicker ways.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Georg Otto
> Sent: Friday, July 21, 2006 10:39 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] intersect of list elements
> 
> 
> Hi,
> 
> i have a list of several vectors, for example:
> 
> > vectorlist
> $vector.a.1
> [1] "a" "b" "c"
> 
> $vector.a.2
> [1] "a" "b" "d"
> 
> $vector.b.1
> [1] "e" "f" "g"
> 
> 
> I can use intersect to find elements that appear in $vector.a.1 and
> $vector.a.2:
> 
> > intersect(vectorlist[[1]], vectorlist[[2]])
> [1] "a" "b"
> 
> 
> I would like to use grep to get the vectors by their names matching an
> expression and to find the intersects between those vectors. For the
> first step:
> 
> > vectorlist[grep ("vector.a", names(vectorlist))]
> $vector.a.1
> [1] "a" "b" "c"
> 
> $vector.a.2
> [1] "a" "b" "d"
> 
> 
> Unfortunately, I can not pass the two vectors as argument to 
> intersect:
> 
> > intersect(vectorlist[grep ("vector.a", names(vectorlist))])
> Error in unique(y[match(x, y, 0)]) : argument "y" is missing, 
> with no default
> 
> I am running R Version 2.3.1 (2006-06-01) 
> 
> 
> Could somone help me to solve this?
> 
> Cheers,
> 
> Georg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sundar.dorai-raj at pdf.com  Fri Jul 21 20:02:11 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 21 Jul 2006 13:02:11 -0500
Subject: [R] intersect of list elements
In-Reply-To: <m1ejweg8n6.fsf@tuebingen.mpg.de>
References: <m1ejweg8n6.fsf@tuebingen.mpg.de>
Message-ID: <44C116A3.3060108@pdf.com>



Georg Otto wrote:
> Hi,
> 
> i have a list of several vectors, for example:
> 
> 
>>vectorlist
> 
> $vector.a.1
> [1] "a" "b" "c"
> 
> $vector.a.2
> [1] "a" "b" "d"
> 
> $vector.b.1
> [1] "e" "f" "g"
> 
> 
> I can use intersect to find elements that appear in $vector.a.1 and
> $vector.a.2:
> 
> 
>>intersect(vectorlist[[1]], vectorlist[[2]])
> 
> [1] "a" "b"
> 
> 
> I would like to use grep to get the vectors by their names matching an
> expression and to find the intersects between those vectors. For the
> first step:
> 
> 
>>vectorlist[grep ("vector.a", names(vectorlist))]
> 
> $vector.a.1
> [1] "a" "b" "c"
> 
> $vector.a.2
> [1] "a" "b" "d"
> 
> 
> Unfortunately, I can not pass the two vectors as argument to intersect:
> 
> 
>>intersect(vectorlist[grep ("vector.a", names(vectorlist))])
> 
> Error in unique(y[match(x, y, 0)]) : argument "y" is missing, with no default
> 
> I am running R Version 2.3.1 (2006-06-01) 
> 
> 
> Could somone help me to solve this?
> 
> Cheers,
> 
> Georg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


Will this work for you?

vectorlist <- list(vector.a.1 = c("a", "b", "c"),
                    vector.a.2 = c("a", "b", "d"),
                    vector.b.1 = c("e", "f", "g"))

intersect2 <- function(...) {
   args <- list(...)
   nargs <- length(args)
   if(nargs <= 1) {
     if(nargs == 1 && is.list(args[[1]])) {
       do.call("intersect2", args[[1]])
     } else {
       stop("cannot evaluate intersection fewer than 2 arguments")
     }
   } else if(nargs == 2) {
     intersect(args[[1]], args[[2]])
   } else {
     intersect(args[[1]], intersect2(args[-1]))
   }
}

vector.a <- vectorlist[grep ("vector.a", names(vectorlist))]
intersect2(vector.a)
intersect2(vectorlist)

HTH,

--sundar


From ggrothendieck at gmail.com  Fri Jul 21 20:18:03 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 21 Jul 2006 14:18:03 -0400
Subject: [R] intersect of list elements
In-Reply-To: <m1ejweg8n6.fsf@tuebingen.mpg.de>
References: <m1ejweg8n6.fsf@tuebingen.mpg.de>
Message-ID: <971536df0607211118s2a5bfb86q64a0f8b285896093@mail.gmail.com>

The following assumes that within each component of vectorlist
the vector elements are unique. In that case the first two lines
define vectorlist and perform the grep, as in your post.  Elements
of the intersection must occur n times where n is the number
of components of vectorlist that match the grep and those
elements are extracted in the last line.

# from your post
vectorlist <- list(vector.a.1 = c("a", "b", "c"), vector.a.2 = c("a",
   "b", "d"), vector.b.1. = c("e", "f", "g"))
idx <- grep("vector.a", names(vectorlist))

# get intersection
names(which(table(unlist(vectorlist[idx])) == length(idx)))

On 7/21/06, Georg Otto <georg.otto at tuebingen.mpg.de> wrote:
>
> Hi,
>
> i have a list of several vectors, for example:
>
> > vectorlist
> $vector.a.1
> [1] "a" "b" "c"
>
> $vector.a.2
> [1] "a" "b" "d"
>
> $vector.b.1
> [1] "e" "f" "g"
>
>
> I can use intersect to find elements that appear in $vector.a.1 and
> $vector.a.2:
>
> > intersect(vectorlist[[1]], vectorlist[[2]])
> [1] "a" "b"
>
>
> I would like to use grep to get the vectors by their names matching an
> expression and to find the intersects between those vectors. For the
> first step:
>
> > vectorlist[grep ("vector.a", names(vectorlist))]
> $vector.a.1
> [1] "a" "b" "c"
>
> $vector.a.2
> [1] "a" "b" "d"
>
>
> Unfortunately, I can not pass the two vectors as argument to intersect:
>
> > intersect(vectorlist[grep ("vector.a", names(vectorlist))])
> Error in unique(y[match(x, y, 0)]) : argument "y" is missing, with no default
>
> I am running R Version 2.3.1 (2006-06-01)
>
>
> Could somone help me to solve this?
>
> Cheers,
>
> Georg
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From apaquet at medsfgh.ucsf.edu  Fri Jul 21 20:47:53 2006
From: apaquet at medsfgh.ucsf.edu (Paquet, Agnes)
Date: Fri, 21 Jul 2006 11:47:53 -0700
Subject: [R] connection to X11 problem
Message-ID: <68925989EEAAE34D8104F8CF2BEE96A902FBB594@sfgh05.som.ucsf.edu>

Dear List,

I am a new Mac user and I am having problem generating png (or jpeg)
using the GUI version of R. I installed R-2.3.1.dmg (custom install with
everything selected) and X11User.pkg but I am still getting the
following X11 connection error when I try to generate a png (or a jpeg):

Error in X11(paste("png::", filename, sep = ""), width, height,
pointsize,  : 
    unable to start device PNG
In addition: Warning message:
unable to open connection to X11 display ''

I tried to set up the DISPLAY variable using the command:

Sys.putenv("DISPLAY"=":0")

but I am still running into the same problem. 

Is there anything else I need to do or install in order to use X11? I am
using a intel Core Duo processor and OSX 10.4.7.

Thank you for your help,

Agnes


From lindend1 at msu.edu  Fri Jul 21 20:48:54 2006
From: lindend1 at msu.edu (Dan Linden)
Date: Fri, 21 Jul 2006 11:48:54 -0700 (PDT)
Subject: [R] Problems with character spacing in windows metafiles...
In-Reply-To: <5435945.post@talk.nabble.com>
References: <5435945.post@talk.nabble.com>
Message-ID: <5438500.post@talk.nabble.com>


I finally determined the problem, but not the specific cause.

For some reason, the WMF is resized when saved or copied from R.  If you
compare the same plot pasted into PowerPoint as both a bitmap and a WMF, the
WMF has its height increased ~14% and its width decreased ~5% consistently. 
This is true whether you copy/paste or save the WMF.

At least, that is what occurs on my Windows XP machine with Office 2003 and
R 2.3.1 installed.
-- 
View this message in context: http://www.nabble.com/Problems-with-character-spacing-in-windows-metafiles...-tf1980921.html#a5438500
Sent from the R help forum at Nabble.com.


From mschwartz at mn.rr.com  Fri Jul 21 21:29:42 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 21 Jul 2006 14:29:42 -0500
Subject: [R] connection to X11 problem
In-Reply-To: <68925989EEAAE34D8104F8CF2BEE96A902FBB594@sfgh05.som.ucsf.edu>
References: <68925989EEAAE34D8104F8CF2BEE96A902FBB594@sfgh05.som.ucsf.edu>
Message-ID: <1153510182.3874.57.camel@localhost.localdomain>

On Fri, 2006-07-21 at 11:47 -0700, Paquet, Agnes wrote:
> Dear List,
> 
> I am a new Mac user and I am having problem generating png (or jpeg)
> using the GUI version of R. I installed R-2.3.1.dmg (custom install with
> everything selected) and X11User.pkg but I am still getting the
> following X11 connection error when I try to generate a png (or a jpeg):
> 
> Error in X11(paste("png::", filename, sep = ""), width, height,
> pointsize,  : 
>     unable to start device PNG
> In addition: Warning message:
> unable to open connection to X11 display ''
> 
> I tried to set up the DISPLAY variable using the command:
> 
> Sys.putenv("DISPLAY"=":0")
> 
> but I am still running into the same problem. 
> 
> Is there anything else I need to do or install in order to use X11? I am
> using a intel Core Duo processor and OSX 10.4.7.
> 
> Thank you for your help,
> 
> Agnes

I don't use a Mac, but the following might be helpful:

http://cran.r-project.org/bin/macosx/RMacOSX-FAQ.html#X11-window-server-_0028optional_0029

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/77424.html

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/46097.html


Also note that there is a R-sig-Mac e-mail list:

https://stat.ethz.ch/mailman/listinfo/r-sig-mac


HTH,

Marc Schwartz


From rvaradhan at jhmi.edu  Fri Jul 21 19:34:30 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Fri, 21 Jul 2006 13:34:30 -0400
Subject: [R] positive semi-definite matrix
In-Reply-To: <17601.2883.656841.280555@stat.math.ethz.ch>
Message-ID: <000601c6aceb$ebdc1940$7c94100a@win.ad.jhu.edu>

Martin,

You are most welcome.  I apologize for my faux pas.  I really did mean to
say "Matrix" package, but got sloppy!

There is also another (more recent) article by Higham:
http://www.maths.man.ac.uk/~nareports/narep369.pdf
 

Best,
Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
Webpage: http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html 
--------------------------------------------------------------------------
> -----Original Message-----
> From: Martin Maechler [mailto:maechler at stat.math.ethz.ch]
> Sent: Friday, July 21, 2006 1:14 PM
> To: Ravi Varadhan
> Cc: 'Duncan Murdoch'; 'roger bos'; 'RHelp'; bates at r-project.org
> Subject: Re: [R] positive semi-definite matrix
> 
> >>>>> "Ravi" == Ravi Varadhan <rvaradhan at jhmi.edu>
> >>>>>     on Fri, 21 Jul 2006 11:33:23 -0400 writes:
> 
>     Ravi> There is a paper by N.J. Higham (SIAM J Matrix Anal,
>     Ravi> 1998) on a modified cholesky decomposition of
>     Ravi> symmetric and not necessarily positive definite matrix
>     Ravi> (say, A), with an important goal of producing a
>     Ravi> "small-normed" perturbation of A (say, delA), that
>     Ravi> makes (A + delA) positive definite.
> 
>     Ravi> http://epubs.siam.org/sam-bin/dbq/article/30289
> 
>     Ravi> There is also an algorithm in Gill, Murray and
>     Ravi> Wright's text - Practical Optimization (section
>     Ravi> 4.4.2).
> 
> Thanks a lot, Ravi,
> for the interesting references, in the past I once had looked
> for such things but did not find any --- most probably because I
> used wrong keywords.
> 
>     Ravi> These may be relevant to your problem.  I am not sure
>     Ravi> if these algorithms have been implemented in R, for
>     Ravi> example, in the "matrix" library.
> 
> Ooooo... !  It's  "Matrix" and  `package', yes `package', yes `package' ..
> 
> but no, it hasn't been implemented there yet, AFAIK.
> OTOH, it's not a bad idea to do there, since it's building on
> the  LDL' cholesky factorization   which we are using
> in "Matrix" in other places anyway.
> 
> Thanks again for your help!
> Martin Maechler, ETH Zurich
> 
>     Ravi> ----------------------------------------------------------------
> ----------
>     Ravi> Ravi Varadhan, Ph.D.
>     Ravi> Assistant Professor,  The Center on Aging and Health
>     Ravi> Division of Geriatric Medicine and Gerontology
>     Ravi> Johns Hopkins University
>     Ravi> Ph: (410) 502-2619
>     Ravi> Fax: (410) 614-9625
>     Ravi> Email:  rvaradhan at jhmi.edu
>     Ravi> Webpage:
> http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
>     Ravi> ----------------------------------------------------------------
> ----------
> 
>     >> -----Original Message-----
>     >> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
>     >> bounces at stat.math.ethz.ch] On Behalf Of Duncan Murdoch
>     >> Sent: Friday, July 21, 2006 9:45 AM
>     >> To: roger bos
>     >> Cc: RHelp
>     >> Subject: Re: [R] positive semi-definite matrix
>     >>
>     >> On 7/21/2006 8:59 AM, roger bos wrote:
>     >> > I have a covariance matrix that is not positive semi-definite
> matrix and
>     >> I
>     >> > need it to be via some sort of adjustment.  Is there any R
> routine or
>     >> > package to help me do this?
>     >>
>     >> I think you need to be more specific about what have and what you
> want,
>     >> but if the matrix is symmetric and nearly positive semi-definite
> (but
>     >> not exactly because of rounding error), you might try something
> like
>     >>
>     >> fixit <- function(A) {
>     >> eig <- eigen(A, symmetric = TRUE)
>     >> eig$values <- pmax(0, eig$values)
>     >> return(eig$vectors %*% diag(eig$values) %*% t(eig$vectors))
>     >> }
>     >>
>     >> Rounding error means this is not guaranteed to be positive
>     >> semi-definite, but it will be very close.
>     >>
>     >> Duncan Murdoch
>


From mnair at iusb.edu  Fri Jul 21 21:51:46 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Fri, 21 Jul 2006 15:51:46 -0400
Subject: [R] multicomp plotting
Message-ID: <A32055BDEA88C34BB3DBBCD2293807784CB79C@iu-mssg-mbx109.ads.iu.edu>


I am using the multcomp package for doing multiple comparisons. Since
the data I am handling is huge the number of comparisons are also large.
I am interested in:
1>  Breaking down my plots to get rid of the clutter that happens when
plotting the entire data set. How do I pass only part of the data to the
plot function ?
 
fungus.cirec<-simint(Fungus.yield~Habitat,data=fungus,conf.level=0.95,ty
pe =c("Tukey"))
plot(fungus.cirec)  #This plots the entire data. I want to plot part of
the data only
 
2>I am also interested in getting rid of the field name associated with
each categorical variable. 
Here is what the part of the data looks like
 
Habitat Fungus.yield
Birch 20.83829053
Birch 22.9718181
Birch 22.28216829
Birch 24.23136797
Birch 22.32147961
Birch 20.30783598
Oak 27.24047258
Oak 29.7730014
Oak 30.12608508
Oak 25.76088669
Oak 30.14750974
Hornbeam 17.05307949
Hornbeam 15.32805111
Hornbeam 18.26920177
Hornbeam 21.30987049
Hornbeam 21.7173223


It plots labels as HabitatBirch-HabitatOak for example.  How do I get
rid of the field name Habitat in the plot? 
 
3> How do I tell the method to mark the significant comparisons? (i.e
those that do not intersect the zero line).
 
Thanks ../Murli


From ajkorman at mail.utexas.edu  Fri Jul 21 22:15:19 2006
From: ajkorman at mail.utexas.edu (ajkorman at mail.utexas.edu)
Date: Fri, 21 Jul 2006 15:15:19 -0500
Subject: [R] character to vector
Message-ID: <1153512919.44c135d73736c@webmailapp6.cc.utexas.edu>

I have an object of mode character that contains a long sequence of letters. 
How can I convert this object into a vector with each element of the vector
containing a single letter?  Essentially, I want to break the long string of
letters into many individual letters.

Thanks for the help.

Alex


From mathematician4 at hotmail.com  Fri Jul 21 22:23:20 2006
From: mathematician4 at hotmail.com (Emanuele Mazzola)
Date: Fri, 21 Jul 2006 20:23:20 +0000
Subject: [R] table elemets testing
Message-ID: <BAY107-F33CB688ABD3D01B2C158C99A660@phx.gbl>

Hi everybody,

i'm dealing with some percentage tables, of which i should test rowwise if 
the entries are sgnificantly equal or not. Namely, on row 1, test H0: 
element 1= element2, H0: element 1= element3...H0: element 2= element3...H0: 
element n-1= element n. The same on the other rows.

Anybody knows how this can be done in quick way? I don't have large 
matrices, but it seems quite boring...

Thank you very much in advance for your answering,
Emanuele


From mschwartz at mn.rr.com  Fri Jul 21 22:26:01 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 21 Jul 2006 15:26:01 -0500
Subject: [R] character to vector
In-Reply-To: <1153512919.44c135d73736c@webmailapp6.cc.utexas.edu>
References: <1153512919.44c135d73736c@webmailapp6.cc.utexas.edu>
Message-ID: <1153513561.3874.88.camel@localhost.localdomain>

On Fri, 2006-07-21 at 15:15 -0500, ajkorman at mail.utexas.edu wrote:
> I have an object of mode character that contains a long sequence of letters. 
> How can I convert this object into a vector with each element of the vector
> containing a single letter?  Essentially, I want to break the long string of
> letters into many individual letters.
> 
> Thanks for the help.
> 
> Alex

> letters
 [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q"
[18] "r" "s" "t" "u" "v" "w" "x" "y" "z"

> MyChar <- paste(letters, collapse = "")

> MyChar
[1] "abcdefghijklmnopqrstuvwxyz"

> MyVec <- unlist(strsplit(MyChar, ""))

> MyVec
 [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q"
[18] "r" "s" "t" "u" "v" "w" "x" "y" "z"

See ?strsplit and ?unlist and of course, ?paste for the reverse
operation as above.

HTH,

Marc Schwartz


From CodyH at BaylorHealth.edu  Fri Jul 21 22:33:54 2006
From: CodyH at BaylorHealth.edu (Hamilton, Cody)
Date: Fri, 21 Jul 2006 15:33:54 -0500
Subject: [R] One arm survival sample calculations
Message-ID: <E52E20F6B4A2F548B16BB259DD5168CF02FB6894@BHDAEXCH11.bhcs.pvt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060721/35ebcacb/attachment.pl 

From leaflovesun at yahoo.ca  Fri Jul 21 23:33:07 2006
From: leaflovesun at yahoo.ca (Leaf Sun)
Date: Fri, 21 Jul 2006 15:33:07 -0600
Subject: [R] Weibull distribution
References: <20060721150307.87753.qmail@web30806.mail.mud.yahoo.com>
	<Pine.LNX.4.64.0607210831420.17202@homer21.u.washington.edu>
Message-ID: <200607211533061537143@yahoo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060721/0bff35d9/attachment.pl 

From rhelp.zhao at gmail.com  Fri Jul 21 23:41:47 2006
From: rhelp.zhao at gmail.com (Iris Zhao)
Date: Fri, 21 Jul 2006 17:41:47 -0400
Subject: [R] optim()
In-Reply-To: <44BC764D.8020208@pdf.com>
References: <d95bc7680607141158s33b307f9j7899f318c41566b2@mail.gmail.com>
	<44BC764D.8020208@pdf.com>
Message-ID: <d95bc7680607211441s1ff8c329rc4d98237e1233434@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060721/6ab6d6db/attachment.pl 

From Ted.Harding at nessie.mcc.ac.uk  Sat Jul 22 00:34:09 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 21 Jul 2006 23:34:09 +0100 (BST)
Subject: [R] connection to X11 problem
In-Reply-To: <68925989EEAAE34D8104F8CF2BEE96A902FBB594@sfgh05.som.ucsf.edu>
Message-ID: <XFMail.060721233409.Ted.Harding@nessie.mcc.ac.uk>

On 21-Jul-06 Paquet, Agnes wrote:
> Dear List,
> 
> I am a new Mac user and I am having problem generating png (or jpeg)
> using the GUI version of R. I installed R-2.3.1.dmg (custom install
> with
> everything selected) and X11User.pkg but I am still getting the
> following X11 connection error when I try to generate a png (or a
> jpeg):
> 
> Error in X11(paste("png::", filename, sep = ""), width, height,
> pointsize,  : 
>     unable to start device PNG
> In addition: Warning message:
> unable to open connection to X11 display ''
> 
> I tried to set up the DISPLAY variable using the command:
> 
> Sys.putenv("DISPLAY"=":0")
> 
> but I am still running into the same problem. 

Like Marc, I don't use a Mac either. But the underlying BSD OS
is basically similar to Linux. On Linux, my primary X11 DISPLAY
envvar would be ":0.0", so (at a guess) I suggest you try

  Sys.putenv("DISPLAY"=":0.0")

Hoping this helps!
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 21-Jul-06                                       Time: 23:34:05
------------------------------ XFMail ------------------------------


From maj at waikato.ac.nz  Sat Jul 22 00:57:56 2006
From: maj at waikato.ac.nz (Murray Jorgensen)
Date: Sat, 22 Jul 2006 10:57:56 +1200
Subject: [R] Parameterization puzzle
In-Reply-To: <Pine.WNT.4.64.0607210931001.1248@Petrel>
References: <44C063E5.3020703@waikato.ac.nz>
	<Pine.LNX.4.64.0607210716270.12611@gannet.stats.ox.ac.uk>
	<17600.36192.983823.42458@bossiaea.maths.uwa.edu.au>
	<Pine.WNT.4.64.0607210931001.1248@Petrel>
Message-ID: <44C15BF4.2030709@waikato.ac.nz>

Thanks to Brian and Berwin with there help. I faced a double problem in 
that I not only wanted to fit the model but I also wanted to do so in 
such a way that it would seem natural for a classroom example.

The moral seems to be that I should do the orthogonal polynomial stuff 
outside the model formula. Here then is my solution:

pyears <- scan()
18793 52407 10673 43248 5710 28612 2585 12663 1462 5317

deaths <- scan()
2 32 12 104 28 206 28 186 31 102

l <- log(pyears)
Smoke <- gl(2,1,10,labels=c("No","Yes"))
Age <- gl(5,2,10,labels=c("35-44","45-54","55-64","65-74","75-84"),
            ordered=TRUE)
mod1.glm <- glm(deaths ~ Age * Smoke + offset(l),family=poisson)
summary(mod1.glm)
age <- as.numeric(Age)
age1 <- poly(age,2)[,1]
age2 <- poly(age,2)[,2]
mod2.glm <- aso1.glm <- glm(deaths ~ age1 + age2 + Smoke +
                       age1:Smoke + offset(l),family=poisson)
summary(mod2.glm)

The final summary then comes out looking like this:

               Estimate Std. Error z value Pr(>|z|)
(Intercept)    -5.8368     0.1213 -48.103  < 2e-16 ***
age1            5.3238     0.4129  12.893  < 2e-16 ***
age2           -1.0460     0.1448  -7.223 5.08e-13 ***
SmokeYes        0.5183     0.1262   4.106 4.02e-05 ***
age1:SmokeYes  -1.3755     0.4340  -3.169  0.00153 **


which is just what I wanted.

Cheers,  Murray Jorgensen

Prof Brian D Ripley wrote:
> On Fri, 21 Jul 2006, Berwin A Turlach wrote:
> 
>> G'day all,
>>
>>>>>>> "BDR" == Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>>
>>    BDR> R does not know that poly(age,2) and poly(age,1) are linearly
>>    BDR> dependent.
>> Indeed, I also thought that this is the reason of the problem.
>>
>>    BDR> (And indeed they only are for some functions 'poly'.)
>> I am surprised about this.  Should probably read the help page of
>> 'poly' once more and more carefully.
> 
> My point was perhaps simpler: if you or I or Murray had a function 
> poly() in our workspace, that one would be found, I think.  (I have not 
> checked the ramifications of namespaces here, but I believe that would 
> be the intention, that formulae are evaluated in their defining 
> environment.)  So omly when the model matrix is set up could the linear 
> dependence be known (and there is nothing in the system poly() to tell 
> model.matrix).
> 
> What is sometimes called extrinsic aliasing is left to the fitting 
> function, which seems to be to do a sensible job provided the main 
> effect is in the model.  Indeed, including interactions without main 
> effects (as Murray did) often makes the results hard to interpret.
> 
>>    BDR> I cannot reproduce your example ('l' is missing), [...]
>> My guess is that 'l' is 'pyears'.  At least, I worked under that
>> assumption.
> 
> Apparently l = log(pyears), which was my uncertain guess.
> 
>> Interestingly, on my machine (using R 2.3.1, 2.2.1 and 2.1.1) I cannot
>> fit any of the Poisson GLM that Murray tried.  I always get the error
>> message:
>>
>> Error: no valid set of coefficients has been found: please supply 
>> starting values
> 
> Related to the offset, I believe.
> 
>>
>> But I have to investigate this further.  I can fit binomial models
>> that give me similar answers.
>>
>>    BDR> [...] but perhaps
>>    BDR> glm(deaths ~ poly(age,2) + poly(age,1)*Smoke + offset(l),
>>    BDR> poisson)
>>    BDR> was your intention?
>> In this parameterisation a 'poly(age,1)' term will appear among the
>> coefficients with an estimated value of NA since it is aliased with
>> 'poly(age, 2)1'.  So I don't believe that this was Murray's intention.
>>
>> The only suggestion I can come up with is:
>>
>>> summary(glm(cbind(deaths, l-deaths) ~ age*Smoke+I(age^2), 
>>> family=binomial))
>>
>> [...]
>>
>> Coefficients:
>>              Estimate Std. Error z value Pr(>|z|)
>> (Intercept)  -10.79895    0.45149 -23.918  < 2e-16 ***
>> age            2.37892    0.20877  11.395  < 2e-16 ***
>> SmokeYes       1.44573    0.37347   3.871 0.000108 ***
>> I(age^2)      -0.19706    0.02749  -7.168  7.6e-13 ***
>> age:SmokeYes  -0.30850    0.09756  -3.162 0.001566 **
>>
>> [...]
>>
>> Which doesn't use orthogonal polynomials anymore.  But I don't see how
>> you can fit the model that Murray want to fit using orthogonal
>> polynomials given the way R's model language operates.
>>
>> So I guess the Poisson GLM that Murray wants to fit is:
>>
>>        glm(deaths~ age*Smoke+I(age^2)+offset(l), family=poisson)
>>
>> Cheers,
>>
>>        Berwin
>>
>> ========================== Full address ============================
>> Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)
>> School of Mathematics and Statistics        +61 (8) 6488 3383 (self)
>> The University of Western Australia   FAX : +61 (8) 6488 1028
>> 35 Stirling Highway
>> Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
>> Australia                        http://www.maths.uwa.edu.au/~berwin
>>
>>
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862


From apaquet at medsfgh.ucsf.edu  Sat Jul 22 01:03:55 2006
From: apaquet at medsfgh.ucsf.edu (Paquet, Agnes)
Date: Fri, 21 Jul 2006 16:03:55 -0700
Subject: [R] connection to X11 problem: problem fixed
Message-ID: <68925989EEAAE34D8104F8CF2BEE96A902FBB606@sfgh05.som.ucsf.edu>

Hi,

I finally managed to make it work (I just needed to have a X11 window
open). 

Thank you very much for your help.

Agnes

-----Original Message-----
From: Ted Harding [mailto:Ted.Harding at nessie.mcc.ac.uk] 
Sent: Friday, July 21, 2006 3:34 PM
To: Paquet, Agnes
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] connection to X11 problem

On 21-Jul-06 Paquet, Agnes wrote:
> Dear List,
> 
> I am a new Mac user and I am having problem generating png (or jpeg)
> using the GUI version of R. I installed R-2.3.1.dmg (custom install
> with
> everything selected) and X11User.pkg but I am still getting the
> following X11 connection error when I try to generate a png (or a
> jpeg):
> 
> Error in X11(paste("png::", filename, sep = ""), width, height,
> pointsize,  : 
>     unable to start device PNG
> In addition: Warning message:
> unable to open connection to X11 display ''
> 
> I tried to set up the DISPLAY variable using the command:
> 
> Sys.putenv("DISPLAY"=":0")
> 
> but I am still running into the same problem. 

Like Marc, I don't use a Mac either. But the underlying BSD OS
is basically similar to Linux. On Linux, my primary X11 DISPLAY
envvar would be ":0.0", so (at a guess) I suggest you try

  Sys.putenv("DISPLAY"=":0.0")

Hoping this helps!
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 21-Jul-06                                       Time: 23:34:05
------------------------------ XFMail ------------------------------


From sirrahn at hotmail.com  Sat Jul 22 01:27:52 2006
From: sirrahn at hotmail.com (nathan)
Date: Fri, 21 Jul 2006 16:27:52 -0700 (PDT)
Subject: [R] unexpected results
In-Reply-To: <5432210.post@talk.nabble.com>
References: <5432210.post@talk.nabble.com>
Message-ID: <5441821.post@talk.nabble.com>


Thanks,

You are right about it being a tab delimited file - I should have spotted
that.

I am now getting an error saying that line 4 did not have 27 elements but
will fiddle around and try to work it out - I'm guessing because I have some
empty feild its causing problems. 

Anyway thanks for the differnt bits of help
-- 
View this message in context: http://www.nabble.com/unexpected-results-tf1979786.html#a5441821
Sent from the R help forum at Nabble.com.


From wiedenhoeft at gmx.net  Sat Jul 22 02:18:41 2006
From: wiedenhoeft at gmx.net (John Wiedenhoeft)
Date: Sat, 22 Jul 2006 02:18:41 +0200
Subject: [R] Putting x into the right subset
Message-ID: <1153527522.20667.29.camel@localhost>

Dear all,

I'm sorry I have to bother you with this newbie stuff.

Within a loop I obtain pairs of values x <- c(a, b). An empty set M is
defined before the loop (as a list or whatever). Now I want to do the
following: if there is a vector y in M  that contains at least one of
the values of x, then x shall be concatenated to y. If y doesn't contain
any of the values in x, then x shall be put into M as a new vector. I
first imagined it to be trivial but I'm having a hard time with the
if-statement. I tried to define a function contains(value, vector),
which returns FALSE or TRUE, but R complains that I tried to execute a
"non-function"...

Could anybody help a despaired newbie, please!

Thanks a lot,
John


From vsdimitrov at yahoo.com  Sat Jul 22 02:39:14 2006
From: vsdimitrov at yahoo.com (Valentin Dimitrov)
Date: Fri, 21 Jul 2006 17:39:14 -0700 (PDT)
Subject: [R] Weibull distribution
In-Reply-To: <200607211533061537143@yahoo.ca>
Message-ID: <20060722003914.1728.qmail@web30808.mail.mud.yahoo.com>

It seems to me that not for all values of mu and sd
there is a Weibull distribution with mean=mu and
variance=sd^2.
the programm with optimize(f.fn) finds always a
solution, but this is not necessarily what we need,
because the minimum of (abs(f(x)) is not always 0.
Suppose f(x)=2+x^2, then optimize(x) finds x=0, but
x=0 is not a root of f(x)=0.
That's why I agree with Thomas Lumley, that uniroot()
could be more appropriate than optim and optimize.

Best regards,
Valentin

--- Leaf Sun <leaflovesun at yahoo.ca> wrote:

> Thanks for the suggestion! I switched to optimize(),
> al <- optimize(f.fn, lower = 0.1, upper
> =100,tol=0.001);
> the warnings were gone and it works stably. 
> But when I tried  al <- uniroot(f.fn, lower = 0.1,
> upper =100,tol=0.001);
> error occured: f() values at end points not of
> opposite sign. The error seems to me like there is
> no root found within the interval. I was not able to
> solve this problem.
> 
> Thanks!
> 
> Leaf
> 
> 
> 
> 
> 
> ----- Original Message -----
> 
> From: Thomas Lumley,  tlumley at u.washington.edu
> Sent: 2006-07-21,  09:35:11
> To: Valentin Dimitrov, vsdimitrov at yahoo.com
> Subject:  Re: [R] Weibull distribution
>   
> On  Fri,  21  Jul  2006,  Valentin  Dimitrov  wrote:
> 
> >  Dear  Leaf,
> >
> >  I  modified  your  code  as  follows:
> >
> >  gamma.fun   <-  function(mu,sd,start=100)
> >  {
> >  f.fn   <-  function(alpha)
> > 
>
{abs(sd^2-mu^2/(gamma(1+1/alpha))^2*(gamma(1+2/alpha)-(gamma(1+1/alpha))^2))}
> >  alpha   <-  optim(start,  f.fn)
> >  beta   <-  mu/gamma(1+1/alpha$par)
> >  return(list=c(a=alpha$par,b=beta));
> >  }
> >
> >  Now  it  works  properly.
> >
> >  First,  I  added  an  abs().  You  tried  to 
> solve  an
> >  equation  by  means  of  the  R-function 
> optim(),  which
> >  finds  a  minimum.  That's  why  you  can  find 
> the  solution
> >  of  f(x)=a  through  minimization  of 
> abs(f(x)-a).
> >  Second,  I  deleted  the  optim-method  BFGS 
> from  the
> >  optim()  function,  because  it  is  not 
> appropriate  in
> >  this  case.
> 
> optim()  is  not  appropriate  at  all  in  this 
> case  --  its  help  page  says  to  
> use  optimize()  for  one-dimensional  problems.
> 
> In  fact,  in  one  dimension  there  isn't  any 
> need  to  resort  to  optimization  
> when  you  really  want  root-finding,  and 
> uniroot()  is  more  appropriate  than  
> optimize().
> 
> 
>   -thomas
>


From mert0042 at umn.edu  Sat Jul 22 02:51:54 2006
From: mert0042 at umn.edu (Eric C Merten)
Date: Fri, 21 Jul 2006 19:51:54 CDT
Subject: [R] nested repeated measures in R
Message-ID: <200607220051.k6M0psvX023888@badlands.software.umn.edu>

R help,

How would I input data, verify assumptions, and run a nested repeated
measures ANOVA using R?  I have STATION nested in SITE nested in BLOCK with
measurements repeated for five YEARs.  All are random variables and it's only
slightly unbalanced.  I'm trying to characterize spatiotemporal variation in
stream habitat variables.  Thanks for your help!

Eric Merten


From spencer.graves at pdf.com  Sat Jul 22 07:54:56 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 21 Jul 2006 22:54:56 -0700
Subject: [R] optim()
In-Reply-To: <d95bc7680607211441s1ff8c329rc4d98237e1233434@mail.gmail.com>
References: <d95bc7680607141158s33b307f9j7899f318c41566b2@mail.gmail.com>	<44BC764D.8020208@pdf.com>
	<d95bc7680607211441s1ff8c329rc4d98237e1233434@mail.gmail.com>
Message-ID: <44C1BDB0.4070301@pdf.com>

	  1.  I'd be worried any time the answer depended very much on the 
starting values.  It suggests that the objective function is not well 
behaved, and I must be very careful to make sure I get an appropriate 
answer, and I'm not being misled by round-off, etc.

	  2.  I would NOT use a constant penalty;  I'd start with a small 
constant penalty, then increase it gradually until I had a solution that 
honestly satisfied the constraints.

	  3.  Alternatively, you could use Lagrange multipliers by redefining 
your objective function and including the Lagrange multiplier(s) as 
other paremeter(s) to be estimated.  That sounds like a sensible idea, 
but I have no experience trying that.  I would expect it might work fine 
provided the objective function with constraints were all differentiable 
and sufficiently smooth to admit only one optimum.

	  4.  However, I believe you will gain more insight into the problem by 
trying to reduce the number of unknowns rather than increase them.  For 
example, I noted in my earlier reply that your constraints are 
equivalent to f1 = r12*f2, with r12 = r12 = (1-c)*k1/(c*(1-k1) = a 
constant determined from known constants in your previous problem 
statement.  If it were my problem, I might focus on trying to use this 
constraint to determine one of (p1, p2, p3) as a function of the other 
two.  For example, for what combinations in the (p1, p2) space is there 
a single, unique solution, when is there 0 and when are there 2, 3, or 
more?

	  To accomplish that, I might use 'expand.grid' to generate many 
different combinations of (p1, p2) and then use 'optim' to minimize SSD 
= (f1-r12*f2)^2 over variations in p3.  (Of course, if it were easier to 
solve for p1 or p2 in terms of the other two, I might do that.)  For 
each combination of (p1, p2), I'd store the resulting values of SSD, p3, 
and f1.  Then if any of the SSD values were numerically greater than 0, 
I'd worry about those cases.  Otherwise, I'd look at contour and 
perspective plots of p3 and f1 vs. p1 and p2 to try to generate some 
insight into this problem -- and perhaps generate a simple way to solve 
for p3 and f1 from p1 and p2.

	  Make sense?
	  Hope this helps.
	  Spencer Graves

Iris Zhao wrote:
> Dear Spencer,
> 
> Thank you very much for your helpful reply. I was trying to reproduce a
> table in one paper. After I modified my code according to your suggestion, I
> was able to get the results that are very close to those in the paper. It
> seems the starting values of the parameters to be optimized are very
> crutial. So I will have different optimal values for different starting
> vectors. How can I be sure the min value returned by optim() is the true
> optimal value?
> 
> I am also curious why you choose the constant penalty to handle the
> constraint in the first place. Why not use lagrange multiplier method to
> eliminate the constraint?
> 
> Thanks again. I am grateful for your help.
> 
> Best regards,
> Iris
> 
> 
> On 7/18/06, Spencer Graves <spencer.graves at pdf.com> wrote:
>>          I had good luck translating constrained into unconstrained
>> problems
>> and then optimizing the unconstrained problem.  Have you tried something
>> like the following:
>>
>> Define:
>>          z = c(z1, z2, z3), where p1=1/(1+exp(-z1), etc.  This translates
>> the
>> constraints on the p's to
>>
>>          G(z) = P*(f1(z)-r12*f2(z))^2-f1(z)
>>
>> where f1(z) = f1(p1(z1), p2(z2), p3(z3), and similarly for f2(z), and
>> where P = a penalty term,
>> and r12 = (1-c)*k1/(c*(1-k1).
>>
>>          Can f2(z) ever go outside (0, 1)?  If yes, I would modify G(z) by
>> adding a term like (min(0, f2(z), 1-f2(z))^2)
>>
>>          If I haven't made a math error, your problem should translate
>> into
>> this form.  I first solve this problem for z with P small like 1.  Then
>> after I've got a solution for that, I increase P to 2, then 10, then
>> 100, etc., until the penalty is so great that the desired equality has
>> been effectively achieved.
>>
>>          With 'P' fixed, 'optim' should handle this kind of problem
>> handily.
>> To learn how, I suggest you work through the examples in the ?optim help
>> page.  I'd ignore the gradient, at least initially.  A silly math error
>> in computing the gradient can delay a solutions unnecessarily.  If you
>> need to solve thousands of problems like this for  different values of
>> k1 and 'c', I might later program the gradient.  However, I would not do
>> that initially.
>>
>>          Also, if you are not already familiar with Venables and Ripley
>> (2002)
>> Modern Applied Statistics with S, 4th ed. (Springer -- or an earlier
>> edition), I would encourage you to spend some quality time with this
>> book.  It can help you with 'optim', with contour plots, etc.
>>
>>          Hope this helps,
>>          Spencer Graves
>>
>> Iris Zhao wrote:
>>> Dear all,
>>>
>>>
>>>
>>> I am working on optimization problem and have some trouble running
>> optim().
>>> I have two functions (f1, f2) and 4 unknown parameters (p1, p2, p3, p4).
>>> Both f1 and f2 are functions of p1, p2, and p3, denoted by f1(p1, p2,
>> p3)
>>> and f2(p1,p2,p3) respectively.
>>>
>>>
>>>
>>> The goal is to maximize f1(p1, p2, p3) subject to two constraints:
>>>
>>> (1)  c = k1*p4/(k1*p4+(1-k1)*f1(p1,p2,p3)), where c and k1 are some
>> known
>>> constants
>>>
>>> (2)  p4 = f2(p1, p2, p3)
>>>
>>> In addition, each parameter ranges from 0 to 1, and both f1 and f2
>> involve
>>> integrations.
>>>
>>>
>>>
>>> I tried to use lagrange multipliers to eliminate two equality
>> constraints
>>> and then use optim() to find the maximum value and optimal parameter
>>> estimates.
>>>
>>> So I let fn be f1+lambda1*(c- k1*p4/(k1*p4+(1-k1)*f1(p1,p2,p3))) +
>>> lambda2(p4-f2(p1,p2,p3)). The error message I got was "Error in fn(par,
>> ...)
>>> : recursive default argument reference."
>>>
>>>
>>>
>>> I wonder whether current build-in functions in R can do this type of
>> jobs.
>>> Any suggestion will be greatly appreciated.
>>>
>>>
>>>
>>> Iris
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Sat Jul 22 11:24:15 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 22 Jul 2006 11:24:15 +0200
Subject: [R] table elemets testing
In-Reply-To: <BAY107-F33CB688ABD3D01B2C158C99A660@phx.gbl>
References: <BAY107-F33CB688ABD3D01B2C158C99A660@phx.gbl>
Message-ID: <44C1EEBF.1050308@statistik.uni-dortmund.de>

Emanuele Mazzola wrote:
> Hi everybody,
> 
> i'm dealing with some percentage tables, of which i should test rowwise if 
> the entries are sgnificantly equal or not. Namely, on row 1, test H0: 
> element 1= element2, H0: element 1= element3...H0: element 2= element3...H0: 
> element n-1= element n. The same on the other rows.

Do you want to know how to write the code or which method to use? We 
cannot tackle the first question unless we have answered the second, 
which is much more difficult.

Do you really want to a statistical test under H0: element 1 = element 
2? Then you have to make some assumptions re. the *exact* distributions 
of element 1 and element 2 (are you assuming iid?). And if you assume 
the exact distribtion, you do not need to test on equality any more, but 
you want to know if element x is from this distribution, and you can 
simply use the quantiles themselves.

Uwe Ligges




> Anybody knows how this can be done in quick way? I don't have large 
> matrices, but it seems quite boring...
> 
> Thank you very much in advance for your answering,
> Emanuele
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From anamaria at ufc.br  Fri Jul 21 23:30:46 2006
From: anamaria at ufc.br (anamaria at ufc.br)
Date: Fri, 21 Jul 2006 18:30:46 -0300 (BRT)
Subject: [R] equality constraint
Message-ID: <36515.200.245.102.143.1153517446.squirrel@200.245.102.143>

I would like to optimize a (log-)likelihood function subject to
linear equality constraints in parameters. My function has eight
parameters. The first one, third and fourth are greater than zero, the
second one must be less than zero. The problem is that the last four must
sum zero. Function constrOptim only fits inequality constraints.
Is there an alternative to lead with this problem?

Ana Maria


From elkenawy_ahmed at yahoo.co.uk  Sat Jul 22 08:07:17 2006
From: elkenawy_ahmed at yahoo.co.uk (ahmed el kenawy)
Date: Sat, 22 Jul 2006 07:07:17 +0100 (BST)
Subject: [R] (no subject)
Message-ID: <20060722060717.66411.qmail@web86808.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060722/f644e7b5/attachment.pl 

From bioconductor.cn at gmail.com  Sat Jul 22 11:34:34 2006
From: bioconductor.cn at gmail.com (Jiantao Shi)
Date: Sat, 22 Jul 2006 17:34:34 +0800
Subject: [R] How to add a line on the boxplot
Message-ID: <cedaa40b0607220234w4c9af6c8vb9ef41ecdb2cd332@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060722/ec7d0cde/attachment.pl 

From ligges at statistik.uni-dortmund.de  Sat Jul 22 11:45:36 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 22 Jul 2006 11:45:36 +0200
Subject: [R] Putting x into the right subset
In-Reply-To: <1153527522.20667.29.camel@localhost>
References: <1153527522.20667.29.camel@localhost>
Message-ID: <44C1F3C0.60002@statistik.uni-dortmund.de>

John Wiedenhoeft wrote:

> Dear all,
> 
> I'm sorry I have to bother you with this newbie stuff.
> 
> Within a loop I obtain pairs of values x <- c(a, b). An empty set M is
> defined before the loop (as a list or whatever). Now I want to do the
> following: if there is a vector y in M  that contains at least one of
> the values of x, then x shall be concatenated to y. If y doesn't contain
> any of the values in x, then x shall be put into M as a new vector. I
> first imagined it to be trivial but I'm having a hard time with the
> if-statement. I tried to define a function contains(value, vector),
> which returns FALSE or TRUE, but R complains that I tried to execute a
> "non-function"...
> 
> Could anybody help a despaired newbie, please!


Example (maybe not efficient, but should do the trick):


M <- list()
temp <- NA
for(i in 1:20){
     a <- sample(1:10, 1)
     b <- sample(11:20, 1)
     x <- c(a, b)
     if(i > 1)
         temp <- which(sapply(M, function(y) any(x %in% y)))[1]
     if(!is.na(temp))
         M[[temp]] <- c(M[[temp]], x)
     else
         M[[length(M) + 1]] <- x
}


> Thanks a lot,
> John
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Sat Jul 22 11:51:29 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 22 Jul 2006 11:51:29 +0200
Subject: [R] different csv files; was: (no subject)
In-Reply-To: <20060722060717.66411.qmail@web86808.mail.ukl.yahoo.com>
References: <20060722060717.66411.qmail@web86808.mail.ukl.yahoo.com>
Message-ID: <44C1F521.5080505@statistik.uni-dortmund.de>

ahmed el kenawy wrote:

> hi
>    
>   i created two files in excel with a dbf format in a similar way. the first is opened in R, however, when i try to open the second. i received the following message:
>    
>   sites.can <- read.csv("SITES.csv")
> Error in read.table(file = file, header = header, sep = sep, quote = quote,  : 
>         more columns than column names
> 
> The second file works with the same command
>   larynx.can <- read.csv("LARYNX.csv")
> 
> 
>   SO, WHAT IS THE SOLUTIO????????????

1. Please do not shout.
2. Please read the posting guide.
3. Please use a sensible subject.
4. Please read ?read.csv
5. You might want to check if the format of that csv files are really 
identical and  check if the column names are strange (including comments 
or separators). If it still does not work, send us the first few lines 
of that file. We cannot look at it otherwise!

Uwe Ligges





>   Regards
>   Ahmed
> 
>  		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sw283 at maths.bath.ac.uk  Sat Jul 22 11:53:22 2006
From: sw283 at maths.bath.ac.uk (Simon Wood)
Date: Sat, 22 Jul 2006 10:53:22 +0100 (BST)
Subject: [R] equality constraint
In-Reply-To: <36515.200.245.102.143.1153517446.squirrel@200.245.102.143>
References: <36515.200.245.102.143.1153517446.squirrel@200.245.102.143>
Message-ID: <Pine.LNX.4.64.0607221051050.749@archer.maths.bath.ac.uk>

> I would like to optimize a (log-)likelihood function subject to
> linear equality constraints in parameters. My function has eight
> parameters. The first one, third and fourth are greater than zero, the
> second one must be less than zero. The problem is that the last four must
> sum zero. Function constrOptim only fits inequality constraints.
> Is there an alternative to lead with this problem?

- Any reason not to just replace e.g. parameter 8 with minus the sum of 
parameters 5, 6, and 7?

best,
Simon

>- Simon Wood, Mathematical Sciences, University of Bath, Bath BA2 7AY 
>-             +44 (0)1225 386603         www.maths.bath.ac.uk/~sw283/


From ligges at statistik.uni-dortmund.de  Sat Jul 22 11:59:15 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 22 Jul 2006 11:59:15 +0200
Subject: [R] How to add a line on the boxplot
In-Reply-To: <cedaa40b0607220234w4c9af6c8vb9ef41ecdb2cd332@mail.gmail.com>
References: <cedaa40b0607220234w4c9af6c8vb9ef41ecdb2cd332@mail.gmail.com>
Message-ID: <44C1F6F3.7000306@statistik.uni-dortmund.de>

Jiantao Shi wrote:

> Hi,
> I have a data frame,
> 
> 
>>df=rnorm(1000)
>>dim(df)=c(100,10)
>>
> 
> And i can get the boxplot,
> 
> 
>>boxplot(data.frame(df))
> 
> 
> 
> So how can add a line (aline) on the existing boxplot,eg,
> 
> 
> Thanks in advance.

Example:


df <- rnorm(1000)
dim(df) <- c(100,10)
boxplot(data.frame(df))
aline <- apply(df, 2, max)

# , now you might want either
abline(h = aline, col = "green", lty="dotted")
# or
segments(seq(along = aline) - 0.2, aline,
          seq(along = aline) + 0.2, aline,
          lwd = 2, col = "red")



Uwe Ligges




> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bioconductor.cn at gmail.com  Sat Jul 22 13:50:19 2006
From: bioconductor.cn at gmail.com (Jiantao Shi)
Date: Sat, 22 Jul 2006 19:50:19 +0800
Subject: [R] How to add a line on the boxplot
In-Reply-To: <44C1F6F3.7000306@statistik.uni-dortmund.de>
References: <cedaa40b0607220234w4c9af6c8vb9ef41ecdb2cd332@mail.gmail.com>
	<44C1F6F3.7000306@statistik.uni-dortmund.de>
Message-ID: <cedaa40b0607220450i24b3d44rc76c5849c3303f7a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060722/477c7258/attachment.pl 

From ligges at statistik.uni-dortmund.de  Sat Jul 22 14:12:39 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 22 Jul 2006 14:12:39 +0200
Subject: [R] How to add a line on the boxplot
In-Reply-To: <cedaa40b0607220450i24b3d44rc76c5849c3303f7a@mail.gmail.com>
References: <cedaa40b0607220234w4c9af6c8vb9ef41ecdb2cd332@mail.gmail.com>	
	<44C1F6F3.7000306@statistik.uni-dortmund.de>
	<cedaa40b0607220450i24b3d44rc76c5849c3303f7a@mail.gmail.com>
Message-ID: <44C21637.8070903@statistik.uni-dortmund.de>

Jiantao Shi wrote:
> Thanks
> The first method,i get many lines,using the second,i get segments.I 
> actually
> need one line with y coordinate defined by aline,x along the boxplot.

Do you mean lines(aline)?
If not, please explain in another way how you want to arrange those 10 
values and make lines from it.

Uwe Ligges



> 
> On 7/22/06, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
>>
>> Jiantao Shi wrote:
>>
>> > Hi,
>> > I have a data frame,
>> >
>> >
>> >>df=rnorm(1000)
>> >>dim(df)=c(100,10)
>> >>
>> >
>> > And i can get the boxplot,
>> >
>> >
>> >>boxplot(data.frame(df))
>> >
>> >
>> >
>> > So how can add a line (aline) on the existing boxplot,eg,
>> >
>> >
>> > Thanks in advance.
>>
>> Example:
>>
>>
>> df <- rnorm(1000)
>> dim(df) <- c(100,10)
>> boxplot(data.frame(df))
>> aline <- apply(df, 2, max)
>>
>> # , now you might want either
>> abline(h = aline, col = "green", lty="dotted")
>> # or
>> segments(seq(along = aline) - 0.2, aline,
>>          seq(along = aline) + 0.2, aline,
>>          lwd = 2, col = "red")
>>
>>
>>
>> Uwe Ligges
>>
>>
>>
>>
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>>
>


From robert-mcfadden at o2.pl  Sat Jul 22 14:22:13 2006
From: robert-mcfadden at o2.pl (Robert Mcfadden)
Date: Sat, 22 Jul 2006 14:22:13 +0200
Subject: [R] R shutdown
Message-ID: <001301c6ad89$766a98a0$1191680a@robert>

Dear R Users,
I run simulation that takes very long time (R 2.2.1, Win XP pro., Rgui SDI
mode, editor Tinn-R). 
It's happened that R shuts down and Windows display the message: Rgui.exe
makes an error and the application will shut down. Unfortunately everything
I lost. Below I paste the message that is created when error appear. Maybe
You as an expert will figure out what is happening to me.
Best,
Robert McFadden       


<?xml version="1.0" encoding="UTF-16"?>
<DATABASE>
<EXE NAME="Rgui.exe" FILTER="GRABMI_FILTER_PRIVACY">
    <MATCHING_FILE NAME="md5check.exe" SIZE="15360" CHECKSUM="0x5685AC04"
BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
FILE_DESCRIPTION="R for Windows files checker" FILE_VERSION="2.2.1
(2005-12-20)" LEGAL_COPYRIGHT="R Development Core Team 1995-2005"
VERFILEDATEHI="0x0" VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x1"
MODULE_TYPE="WIN32" PE_CHECKSUM="0x3CD5" LINKER_VERSION="0x10000"
UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
LINK_DATE="12/20/2005 13:17:59" UPTO_LINK_DATE="12/20/2005 13:17:59"
VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
    <MATCHING_FILE NAME="R.dll" SIZE="2201600" CHECKSUM="0xFC7E40D1"
BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
FILE_DESCRIPTION="R for Windows DLL" FILE_VERSION="2.2.1    (2005-12-20)"
LEGAL_COPYRIGHT="R Development Core Team 1995-2005" VERFILEDATEHI="0x0"
VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x2" MODULE_TYPE="WIN32"
PE_CHECKSUM="0x21CDB3" LINKER_VERSION="0x10000"
UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
LINK_DATE="12/20/2005 13:16:00" UPTO_LINK_DATE="12/20/2005 13:16:00"
VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
    <MATCHING_FILE NAME="R.exe" SIZE="18432" CHECKSUM="0x4407B548"
BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
FILE_DESCRIPTION="R for Windows front-end" FILE_VERSION="2.2.1
(2005-12-20)" LEGAL_COPYRIGHT="R Development Core Team 1995-2005"
VERFILEDATEHI="0x0" VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x1"
MODULE_TYPE="WIN32" PE_CHECKSUM="0x60D3" LINKER_VERSION="0x10000"
UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
LINK_DATE="12/20/2005 13:17:58" UPTO_LINK_DATE="12/20/2005 13:17:58"
VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
    <MATCHING_FILE NAME="Rbitmap.dll" SIZE="238592" CHECKSUM="0x861028AC"
MODULE_TYPE="WIN32" PE_CHECKSUM="0x457B3" LINKER_VERSION="0x10000"
LINK_DATE="12/20/2005 13:25:12" UPTO_LINK_DATE="12/20/2005 13:25:12" />
    <MATCHING_FILE NAME="Rblas.dll" SIZE="106496" CHECKSUM="0x9A1FC9D3"
BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
FILE_DESCRIPTION="R for Windows DLL" FILE_VERSION="2.2.1    (2005-12-20)"
LEGAL_COPYRIGHT="R Development Core Team 1995-2005" VERFILEDATEHI="0x0"
VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x2" MODULE_TYPE="WIN32"
PE_CHECKSUM="0x28489" LINKER_VERSION="0x10000"
UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
LINK_DATE="12/20/2005 13:17:52" UPTO_LINK_DATE="12/20/2005 13:17:52"
VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
    <MATCHING_FILE NAME="Rchtml.dll" SIZE="6144" CHECKSUM="0x880344F"
MODULE_TYPE="WIN32" PE_CHECKSUM="0x4204" LINKER_VERSION="0x10000"
LINK_DATE="12/20/2005 13:17:52" UPTO_LINK_DATE="12/20/2005 13:17:52" />
    <MATCHING_FILE NAME="Rcmd.exe" SIZE="18432" CHECKSUM="0x19E72056"
BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
FILE_DESCRIPTION="R for Windows front-end" FILE_VERSION="2.2.1
(2005-12-20)" LEGAL_COPYRIGHT="R Development Core Team 1995-2005"
VERFILEDATEHI="0x0" VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x1"
MODULE_TYPE="WIN32" PE_CHECKSUM="0x4A89" LINKER_VERSION="0x10000"
UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
LINK_DATE="12/20/2005 13:17:54" UPTO_LINK_DATE="12/20/2005 13:17:54"
VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
    <MATCHING_FILE NAME="Rgui.exe" SIZE="10240" CHECKSUM="0x96A0D31D"
BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
FILE_DESCRIPTION="R for Windows GUI front-end" FILE_VERSION="2.2.1
(2005-12-20)" LEGAL_COPYRIGHT="R Development Core Team 1995-2005"
VERFILEDATEHI="0x0" VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x1"
MODULE_TYPE="WIN32" PE_CHECKSUM="0x6C46" LINKER_VERSION="0x10000"
UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
LINK_DATE="12/20/2005 13:17:53" UPTO_LINK_DATE="12/20/2005 13:17:53"
VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
    <MATCHING_FILE NAME="Rlapack.dll" SIZE="1184256" CHECKSUM="0x9FA3E37A"
BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
FILE_DESCRIPTION="R DLL for lapack" FILE_VERSION="2.2.1    (2005-12-20)"
LEGAL_COPYRIGHT="R Development Core Team 2002-2005" VERFILEDATEHI="0x0"
VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x2" MODULE_TYPE="WIN32"
PE_CHECKSUM="0x130713" LINKER_VERSION="0x10000"
UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
LINK_DATE="12/20/2005 13:18:52" UPTO_LINK_DATE="12/20/2005 13:18:52"
VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
    <MATCHING_FILE NAME="Rproxy.dll" SIZE="44544" CHECKSUM="0xDC26CFFF"
BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
FILE_DESCRIPTION="R for Windows terminal front-end" FILE_VERSION="2.2.1
(2005-12-20)" LEGAL_COPYRIGHT="R Development Core Team 1995-2005"
VERFILEDATEHI="0x0" VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x1"
MODULE_TYPE="WIN32" PE_CHECKSUM="0x13AFB" LINKER_VERSION="0x10000"
UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
LINK_DATE="12/20/2005 13:17:58" UPTO_LINK_DATE="12/20/2005 13:17:58"
VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
    <MATCHING_FILE NAME="RSetReg.exe" SIZE="11264" CHECKSUM="0x638EC344"
BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
FILE_DESCRIPTION="R for Windows front-end" FILE_VERSION="2.2.1
(2005-12-20)" LEGAL_COPYRIGHT="R Development Core Team 1995-2005"
VERFILEDATEHI="0x0" VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x1"
MODULE_TYPE="WIN32" PE_CHECKSUM="0xB021" LINKER_VERSION="0x10000"
UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
LINK_DATE="12/20/2005 13:17:55" UPTO_LINK_DATE="12/20/2005 13:17:55"
VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
    <MATCHING_FILE NAME="Rterm.exe" SIZE="11264" CHECKSUM="0xB3961B56"
BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
FILE_DESCRIPTION="R for Windows terminal front-end" FILE_VERSION="2.2.1
(2005-12-20)" LEGAL_COPYRIGHT="R Development Core Team 1995-2005"
VERFILEDATEHI="0x0" VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x1"
MODULE_TYPE="WIN32" PE_CHECKSUM="0x8BD1" LINKER_VERSION="0x10000"
UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
LINK_DATE="12/20/2005 13:17:54" UPTO_LINK_DATE="12/20/2005 13:17:54"
VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
</EXE>
<EXE NAME="R.dll" FILTER="GRABMI_FILTER_THISFILEONLY">
    <MATCHING_FILE NAME="R.dll" SIZE="2201600" CHECKSUM="0xFC7E40D1"
BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
FILE_DESCRIPTION="R for Windows DLL" FILE_VERSION="2.2.1    (2005-12-20)"
LEGAL_COPYRIGHT="R Development Core Team 1995-2005" VERFILEDATEHI="0x0"
VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x2" MODULE_TYPE="WIN32"
PE_CHECKSUM="0x21CDB3" LINKER_VERSION="0x10000"
UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
LINK_DATE="12/20/2005 13:16:00" UPTO_LINK_DATE="12/20/2005 13:16:00"
VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
</EXE>
<EXE NAME="kernel32.dll" FILTER="GRABMI_FILTER_THISFILEONLY">
    <MATCHING_FILE NAME="kernel32.dll" SIZE="1012224" CHECKSUM="0xABFFD815"
BIN_FILE_VERSION="5.1.2600.2180" BIN_PRODUCT_VERSION="5.1.2600.2180"
PRODUCT_VERSION="5.1.2600.2180" FILE_DESCRIPTION="Biblioteka DLL klienta
Windows NT BASE API" COMPANY_NAME="Microsoft Corporation"
PRODUCT_NAME="System operacyjny MicrosoftR WindowsR"
FILE_VERSION="5.1.2600.2180 (xpsp_sp2_rtm.040803-2158)"
ORIGINAL_FILENAME="kernel32" INTERNAL_NAME="kernel32" LEGAL_COPYRIGHT="C
Microsoft Corporation. Wszelkie prawa zastrze?one." VERFILEDATEHI="0x0"
VERFILEDATELO="0x0" VERFILEOS="0x40004" VERFILETYPE="0x2"
MODULE_TYPE="WIN32" PE_CHECKSUM="0x1048EA" LINKER_VERSION="0x50001"
UPTO_BIN_FILE_VERSION="5.1.2600.2180"
UPTO_BIN_PRODUCT_VERSION="5.1.2600.2180" LINK_DATE="08/04/2004 07:43:46"
UPTO_LINK_DATE="08/04/2004 07:43:46" VER_LANGUAGE="Polski [0x415]" />
</EXE>
</DATABASE>


From jasonshi510 at hotmail.com  Sat Jul 22 14:25:39 2006
From: jasonshi510 at hotmail.com (Xin)
Date: Sat, 22 Jul 2006 13:25:39 +0100
Subject: [R] ifelse command
Message-ID: <BAY117-DAV134943295600392ADBC2C1F0670@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060722/5c31df4b/attachment.pl 

From HDoran at air.org  Sat Jul 22 14:28:11 2006
From: HDoran at air.org (Doran, Harold)
Date: Sat, 22 Jul 2006 08:28:11 -0400
Subject: [R] nested repeated measures in R
References: <200607220051.k6M0psvX023888@badlands.software.umn.edu>
Message-ID: <2323A6D37908A847A7C32F1E3662C80E04E59F@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060722/0dbf908a/attachment.pl 

From ggrothendieck at gmail.com  Sat Jul 22 15:14:00 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 22 Jul 2006 09:14:00 -0400
Subject: [R] (no subject)
In-Reply-To: <20060722060717.66411.qmail@web86808.mail.ukl.yahoo.com>
References: <20060722060717.66411.qmail@web86808.mail.ukl.yahoo.com>
Message-ID: <971536df0607220614y6cb74adfmd5398bf14302d3cc@mail.gmail.com>

Check out this recent thread:
https://www.stat.math.ethz.ch/pipermail/r-help/2006-July/109731.html

On 7/22/06, ahmed el kenawy <elkenawy_ahmed at yahoo.co.uk> wrote:
> hi
>
>  i created two files in excel with a dbf format in a similar way. the first is opened in R, however, when i try to open the second. i received the following message:
>
>  sites.can <- read.csv("SITES.csv")
> Error in read.table(file = file, header = header, sep = sep, quote = quote,  :
>        more columns than column names
> >
> The second file works with the same command
>  larynx.can <- read.csv("LARYNX.csv")
> >
>
>  SO, WHAT IS THE SOLUTIO????????????
>
>  Regards
>  Ahmed
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wiedenhoeft at gmx.net  Sat Jul 22 15:21:31 2006
From: wiedenhoeft at gmx.net (John Wiedenhoeft)
Date: Sat, 22 Jul 2006 15:21:31 +0200
Subject: [R] Putting x into the right subset
In-Reply-To: <44C1F3C0.60002@statistik.uni-dortmund.de>
References: <1153527522.20667.29.camel@localhost>
	<44C1F3C0.60002@statistik.uni-dortmund.de>
Message-ID: <1153574492.6457.25.camel@localhost>

Dear Uwe,

Many thanks for your help! Unfortunately, your function doesn't do
exactly what I need (I slightly modified it, but I don't think that
caused the problem):

M <- list()
temp <- NA
for (i in 1:length(deppat))
   {
   a <- deppat[i]
   b <- deptest[i]
   x <- c(a, b)
   if (i > 1)
      temp <- which(sapply(M, function(y) any(x %in% y)))[1]
   if(!is.na(temp))
      M[[temp]] <- c(M[[temp]], x)
   else
      M[[length(M) + 1]] <- x
   }
   
print(M)
print(length(M))
for (k in 1:length(M))
   {
   M[[k]] <- sort(unique(M[[k]]))
   print(M[[k]])
   }

deppat and deptest are equal in length. Additionaly, they may contain
values more than once. At the end, M shall contain exactly
length(unique(c(deppat, deptest))) variables, but it doesn't. Also,
there are some matches that remained undetected: 

M[[1]] 1  5  6 11 22 24 28 29 31 35 49 57 58 62 65 75 80 81 85 86
M[[2]] 2  7 11 12 25 30 31 32 36 50 53 58 59 63 66 69 82 87

11, 31 and 58 occur in two subsets (there are more examples for larger
k).

I've tried to do it more graphic, but for some reason the same failure
occures:

M <- list()
for (j in 1:length(deppat))
   {
   for (listpos in 1:(length(M)+1))
      {
      if (listpos > length(M))
         {
         M[[listpos]] <- c(deppat[j], deptest[j])
         break
         }
      else
         {
         if (deppat[j] %in% M[[listpos]] || deptest[j] %in%
M[[listpos]])
            {
            M[[listpos]] <- c(M[[listpos]], deppat[j], deptest[j])
            break
            }
         }
      }
   }
print(M)
   
for (k in 1:length(M))
   {
   M[[k]] <- sort(unique(M[[k]]))
   print(M[[k]])
   }

There must be an error in reasoning, but I can't figure out where it
is...

Cheers,
John


From murdoch at stats.uwo.ca  Sat Jul 22 15:28:43 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 22 Jul 2006 09:28:43 -0400
Subject: [R] R shutdown
In-Reply-To: <001301c6ad89$766a98a0$1191680a@robert>
References: <001301c6ad89$766a98a0$1191680a@robert>
Message-ID: <44C2280B.40601@stats.uwo.ca>

On 7/22/2006 8:22 AM, Robert Mcfadden wrote:
> Dear R Users,
> I run simulation that takes very long time (R 2.2.1, Win XP pro., Rgui SDI
> mode, editor Tinn-R). 
> It's happened that R shuts down and Windows display the message: Rgui.exe
> makes an error and the application will shut down. Unfortunately everything
> I lost. Below I paste the message that is created when error appear. Maybe
> You as an expert will figure out what is happening to me.

I don't think there's any information below that is helpful to diagnose 
the reason for the crash.

What I'd recommend is that you write out partial results as your 
simulation progresses.  Then at least you'll have those partial results, 
but you may also have enough information to be able to be able to make 
the crash reproducible in a short script.

If you can do that, and you can make the crash reproducible in a current 
version of R (version 2.2.1 is obsolete; you should be using 2.3.1 at a 
minimum, perhaps 2.3.1 patched), then there's a good chance someone will 
be able to track down the bug and fix it.

Duncan Murdoch


> Best,
> Robert McFadden       
> 
> 
> <?xml version="1.0" encoding="UTF-16"?>
> <DATABASE>
> <EXE NAME="Rgui.exe" FILTER="GRABMI_FILTER_PRIVACY">
>     <MATCHING_FILE NAME="md5check.exe" SIZE="15360" CHECKSUM="0x5685AC04"
> BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
> FILE_DESCRIPTION="R for Windows files checker" FILE_VERSION="2.2.1
> (2005-12-20)" LEGAL_COPYRIGHT="R Development Core Team 1995-2005"
> VERFILEDATEHI="0x0" VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x1"
> MODULE_TYPE="WIN32" PE_CHECKSUM="0x3CD5" LINKER_VERSION="0x10000"
> UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
> LINK_DATE="12/20/2005 13:17:59" UPTO_LINK_DATE="12/20/2005 13:17:59"
> VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
>     <MATCHING_FILE NAME="R.dll" SIZE="2201600" CHECKSUM="0xFC7E40D1"
> BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
> FILE_DESCRIPTION="R for Windows DLL" FILE_VERSION="2.2.1    (2005-12-20)"
> LEGAL_COPYRIGHT="R Development Core Team 1995-2005" VERFILEDATEHI="0x0"
> VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x2" MODULE_TYPE="WIN32"
> PE_CHECKSUM="0x21CDB3" LINKER_VERSION="0x10000"
> UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
> LINK_DATE="12/20/2005 13:16:00" UPTO_LINK_DATE="12/20/2005 13:16:00"
> VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
>     <MATCHING_FILE NAME="R.exe" SIZE="18432" CHECKSUM="0x4407B548"
> BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
> FILE_DESCRIPTION="R for Windows front-end" FILE_VERSION="2.2.1
> (2005-12-20)" LEGAL_COPYRIGHT="R Development Core Team 1995-2005"
> VERFILEDATEHI="0x0" VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x1"
> MODULE_TYPE="WIN32" PE_CHECKSUM="0x60D3" LINKER_VERSION="0x10000"
> UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
> LINK_DATE="12/20/2005 13:17:58" UPTO_LINK_DATE="12/20/2005 13:17:58"
> VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
>     <MATCHING_FILE NAME="Rbitmap.dll" SIZE="238592" CHECKSUM="0x861028AC"
> MODULE_TYPE="WIN32" PE_CHECKSUM="0x457B3" LINKER_VERSION="0x10000"
> LINK_DATE="12/20/2005 13:25:12" UPTO_LINK_DATE="12/20/2005 13:25:12" />
>     <MATCHING_FILE NAME="Rblas.dll" SIZE="106496" CHECKSUM="0x9A1FC9D3"
> BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
> FILE_DESCRIPTION="R for Windows DLL" FILE_VERSION="2.2.1    (2005-12-20)"
> LEGAL_COPYRIGHT="R Development Core Team 1995-2005" VERFILEDATEHI="0x0"
> VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x2" MODULE_TYPE="WIN32"
> PE_CHECKSUM="0x28489" LINKER_VERSION="0x10000"
> UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
> LINK_DATE="12/20/2005 13:17:52" UPTO_LINK_DATE="12/20/2005 13:17:52"
> VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
>     <MATCHING_FILE NAME="Rchtml.dll" SIZE="6144" CHECKSUM="0x880344F"
> MODULE_TYPE="WIN32" PE_CHECKSUM="0x4204" LINKER_VERSION="0x10000"
> LINK_DATE="12/20/2005 13:17:52" UPTO_LINK_DATE="12/20/2005 13:17:52" />
>     <MATCHING_FILE NAME="Rcmd.exe" SIZE="18432" CHECKSUM="0x19E72056"
> BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
> FILE_DESCRIPTION="R for Windows front-end" FILE_VERSION="2.2.1
> (2005-12-20)" LEGAL_COPYRIGHT="R Development Core Team 1995-2005"
> VERFILEDATEHI="0x0" VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x1"
> MODULE_TYPE="WIN32" PE_CHECKSUM="0x4A89" LINKER_VERSION="0x10000"
> UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
> LINK_DATE="12/20/2005 13:17:54" UPTO_LINK_DATE="12/20/2005 13:17:54"
> VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
>     <MATCHING_FILE NAME="Rgui.exe" SIZE="10240" CHECKSUM="0x96A0D31D"
> BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
> FILE_DESCRIPTION="R for Windows GUI front-end" FILE_VERSION="2.2.1
> (2005-12-20)" LEGAL_COPYRIGHT="R Development Core Team 1995-2005"
> VERFILEDATEHI="0x0" VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x1"
> MODULE_TYPE="WIN32" PE_CHECKSUM="0x6C46" LINKER_VERSION="0x10000"
> UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
> LINK_DATE="12/20/2005 13:17:53" UPTO_LINK_DATE="12/20/2005 13:17:53"
> VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
>     <MATCHING_FILE NAME="Rlapack.dll" SIZE="1184256" CHECKSUM="0x9FA3E37A"
> BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
> FILE_DESCRIPTION="R DLL for lapack" FILE_VERSION="2.2.1    (2005-12-20)"
> LEGAL_COPYRIGHT="R Development Core Team 2002-2005" VERFILEDATEHI="0x0"
> VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x2" MODULE_TYPE="WIN32"
> PE_CHECKSUM="0x130713" LINKER_VERSION="0x10000"
> UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
> LINK_DATE="12/20/2005 13:18:52" UPTO_LINK_DATE="12/20/2005 13:18:52"
> VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
>     <MATCHING_FILE NAME="Rproxy.dll" SIZE="44544" CHECKSUM="0xDC26CFFF"
> BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
> FILE_DESCRIPTION="R for Windows terminal front-end" FILE_VERSION="2.2.1
> (2005-12-20)" LEGAL_COPYRIGHT="R Development Core Team 1995-2005"
> VERFILEDATEHI="0x0" VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x1"
> MODULE_TYPE="WIN32" PE_CHECKSUM="0x13AFB" LINKER_VERSION="0x10000"
> UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
> LINK_DATE="12/20/2005 13:17:58" UPTO_LINK_DATE="12/20/2005 13:17:58"
> VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
>     <MATCHING_FILE NAME="RSetReg.exe" SIZE="11264" CHECKSUM="0x638EC344"
> BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
> FILE_DESCRIPTION="R for Windows front-end" FILE_VERSION="2.2.1
> (2005-12-20)" LEGAL_COPYRIGHT="R Development Core Team 1995-2005"
> VERFILEDATEHI="0x0" VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x1"
> MODULE_TYPE="WIN32" PE_CHECKSUM="0xB021" LINKER_VERSION="0x10000"
> UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
> LINK_DATE="12/20/2005 13:17:55" UPTO_LINK_DATE="12/20/2005 13:17:55"
> VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
>     <MATCHING_FILE NAME="Rterm.exe" SIZE="11264" CHECKSUM="0xB3961B56"
> BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
> FILE_DESCRIPTION="R for Windows terminal front-end" FILE_VERSION="2.2.1
> (2005-12-20)" LEGAL_COPYRIGHT="R Development Core Team 1995-2005"
> VERFILEDATEHI="0x0" VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x1"
> MODULE_TYPE="WIN32" PE_CHECKSUM="0x8BD1" LINKER_VERSION="0x10000"
> UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
> LINK_DATE="12/20/2005 13:17:54" UPTO_LINK_DATE="12/20/2005 13:17:54"
> VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
> </EXE>
> <EXE NAME="R.dll" FILTER="GRABMI_FILTER_THISFILEONLY">
>     <MATCHING_FILE NAME="R.dll" SIZE="2201600" CHECKSUM="0xFC7E40D1"
> BIN_FILE_VERSION="2.21.51220.0" BIN_PRODUCT_VERSION="3.0.0.0"
> FILE_DESCRIPTION="R for Windows DLL" FILE_VERSION="2.2.1    (2005-12-20)"
> LEGAL_COPYRIGHT="R Development Core Team 1995-2005" VERFILEDATEHI="0x0"
> VERFILEDATELO="0x0" VERFILEOS="0x4" VERFILETYPE="0x2" MODULE_TYPE="WIN32"
> PE_CHECKSUM="0x21CDB3" LINKER_VERSION="0x10000"
> UPTO_BIN_FILE_VERSION="2.21.51220.0" UPTO_BIN_PRODUCT_VERSION="3.0.0.0"
> LINK_DATE="12/20/2005 13:16:00" UPTO_LINK_DATE="12/20/2005 13:16:00"
> VER_LANGUAGE="Angielski (Stany Zjednoczone) [0x409]" />
> </EXE>
> <EXE NAME="kernel32.dll" FILTER="GRABMI_FILTER_THISFILEONLY">
>     <MATCHING_FILE NAME="kernel32.dll" SIZE="1012224" CHECKSUM="0xABFFD815"
> BIN_FILE_VERSION="5.1.2600.2180" BIN_PRODUCT_VERSION="5.1.2600.2180"
> PRODUCT_VERSION="5.1.2600.2180" FILE_DESCRIPTION="Biblioteka DLL klienta
> Windows NT BASE API" COMPANY_NAME="Microsoft Corporation"
> PRODUCT_NAME="System operacyjny MicrosoftR WindowsR"
> FILE_VERSION="5.1.2600.2180 (xpsp_sp2_rtm.040803-2158)"
> ORIGINAL_FILENAME="kernel32" INTERNAL_NAME="kernel32" LEGAL_COPYRIGHT="C
> Microsoft Corporation. Wszelkie prawa zastrze?one." VERFILEDATEHI="0x0"
> VERFILEDATELO="0x0" VERFILEOS="0x40004" VERFILETYPE="0x2"
> MODULE_TYPE="WIN32" PE_CHECKSUM="0x1048EA" LINKER_VERSION="0x50001"
> UPTO_BIN_FILE_VERSION="5.1.2600.2180"
> UPTO_BIN_PRODUCT_VERSION="5.1.2600.2180" LINK_DATE="08/04/2004 07:43:46"
> UPTO_LINK_DATE="08/04/2004 07:43:46" VER_LANGUAGE="Polski [0x415]" />
> </EXE>
> </DATABASE>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From cooch17 at verizon.net  Sat Jul 22 15:37:23 2006
From: cooch17 at verizon.net (Evan Cooch)
Date: Sat, 22 Jul 2006 09:37:23 -0400
Subject: [R] compile R with ACML support | RHEL 4
Message-ID: <44C22A13.3080201@verizon.net>

Greetings -

I'm trying to compile R under GNU/Linux (RHEL 4) on a multi-Opteron box, 
with ACML support.

First, I downloaded and installed ACML 3.5 - GNU version, although I'm 
not entirely sure what the differences are - from the AMD website. The 
ACML libraries were installed to /opt/acml3.5.0/

Second, I ran ./configure --with-blas='-lacml'      

The configure went fine, except that at the end of the output, it 
reports that readline is the only external library configured.

External libraries: readline

OK, so next, I try

./configure --with-lapack

Same think - only readline is referenced.

So, clearly, I'm missing a particular step. I'm guessing that I need to 
change some environment variable (or two), or tweak something at some 
other stage, to get R to properly reference the ACML libraries. I'm 
puzzled why --with-blas='-acml' doesn't do the trick?

Suggestions? Pointers to the obvious mistake?

Thanks...


From helen.mills at yale.edu  Sat Jul 22 15:57:28 2006
From: helen.mills at yale.edu (helen.mills at yale.edu)
Date: Sat, 22 Jul 2006 09:57:28 -0400
Subject: [R] mvpart
Message-ID: <20060722095728.dsfuwju00gwwkc44@www.mail.yale.edu>

does anyone know of a tutorial that exists for the mvpart package other than the
package documentation and De'ath's original article in Ecology?

thanks,
Helen Mills Poulos
Yale School of Forestry


From jfox at mcmaster.ca  Sat Jul 22 16:02:35 2006
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 22 Jul 2006 10:02:35 -0400
Subject: [R] equality constraint
In-Reply-To: <36515.200.245.102.143.1153517446.squirrel@200.245.102.143>
Message-ID: <20060722140236.ZECN13241.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Ana Maria,

How about replacing the last of the four parameters with the negative of the
sum of the others?

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of anamaria at ufc.br
> Sent: Friday, July 21, 2006 4:31 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] equality constraint
> 
> I would like to optimize a (log-)likelihood function subject 
> to linear equality constraints in parameters. My function has 
> eight parameters. The first one, third and fourth are greater 
> than zero, the second one must be less than zero. The problem 
> is that the last four must sum zero. Function constrOptim 
> only fits inequality constraints.
> Is there an alternative to lead with this problem?
> 
> Ana Maria
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jasonshi510 at hotmail.com  Sat Jul 22 16:59:56 2006
From: jasonshi510 at hotmail.com (Xin)
Date: Sat, 22 Jul 2006 15:59:56 +0100
Subject: [R] Why the contrain does not work for selecting a particular range
	of data?
Message-ID: <BAY117-DAV146FC896CD8D5A50EDB9DF0670@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060722/4d14c82b/attachment.pl 

From mnair at iusb.edu  Sat Jul 22 18:23:49 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Sat, 22 Jul 2006 12:23:49 -0400
Subject: [R] multcomp plotting Please help :)
Message-ID: <A32055BDEA88C34BB3DBBCD229380778050FAB@iu-mssg-mbx109.ads.iu.edu>

I REALLY NEED HELP WITH THIS   PLEASE 
 
I am using the multcomp package for doing multiple comparisons (R 2.3.1 windows). Since the data I am handling is huge the number of comparisons are also large. I am interested in:

1>  Breaking down my plots to get rid of the clutter that happens when plotting the entire data set. How do I pass only part of the data to the plot function ?

fungus.cirec<-simint(Fungus.yield~Habitat, data=fungus,conf.level=0.95,type =c("Tukey"))
plot(fungus.cirec)  #This plots the entire data. I want to plot part of the data only

2>I am also interested in getting rid of the field name associated with each categorical variable.
Here is what the part of the data looks like

Habitat Fungus.yield
Birch 20.83829053
Birch 22.9718181
Birch 22.28216829
Birch 24.23136797
Birch 22.32147961
Birch 20.30783598
Oak 27.24047258
Oak 29.7730014
Oak 30.12608508
Oak 25.76088669
Oak 30.14750974
Hornbeam 17.05307949
Hornbeam 15.32805111
Hornbeam 18.26920177
Hornbeam 21.30987049
Hornbeam 21.7173223

When it plots labels it as HabitatBirch-HabitatOak for example.  How do I get rid of the field name Habitat in the plot?

3> How do I tell the method to mark the significant comparisons? (i.e those that do not intersect the zero line).

Thanks ../Murli


From ggrothendieck at gmail.com  Sat Jul 22 18:34:52 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 22 Jul 2006 12:34:52 -0400
Subject: [R] multcomp plotting Please help :)
In-Reply-To: <A32055BDEA88C34BB3DBBCD229380778050FAB@iu-mssg-mbx109.ads.iu.edu>
References: <A32055BDEA88C34BB3DBBCD229380778050FAB@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <971536df0607220934q770b678q2fc0697773bca2e7@mail.gmail.com>

On 7/22/06, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> I REALLY NEED HELP WITH THIS   PLEASE
>
[....]
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Suggest you follow the instructions to get better chance of a response.


From mnair at iusb.edu  Sat Jul 22 19:00:15 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Sat, 22 Jul 2006 13:00:15 -0400
Subject: [R] Multcomp
Message-ID: <A32055BDEA88C34BB3DBBCD229380778050FB0@iu-mssg-mbx109.ads.iu.edu>

Here it is again, hope this is more clear
 
I am using the following data (only a small subset is given):
 
Habitat Fungus.yield
Birch 20.83829053
Birch 22.9718181
Birch 22.28216829
Birch 24.23136797
Birch 22.32147961
Birch 20.30783598
Oak 27.24047258
Oak 29.7730014
Oak 30.12608508
Oak 25.76088669
Oak 30.14750974
Hornbeam 17.05307949
Hornbeam 15.32805111
Hornbeam 18.26920177
Hornbeam 21.30987049
Hornbeam 21.7173223

I am using the multcomp package to do multiple comparisons as follows 

library(multcomp) # loads the package

fungus<-read.table("fungi.txt", Header=T)    # Reads the data from file saved as fungi.txt


fungus.cirec<-simint(Fungus.yield~Habitat, data=fungus,conf.level=0.95,type =c("Tukey"))  # Computes cimultaneous intervals using Tukey's method


plot(fungus.cirec)   # plots the data

The plot function plots all the comparisons, I want to plot only part of the data since it clutters the graph. 

How do I plot only part of the data ?

How do I tell it to mark the significant comparisons?

How do I get rid of the field names in the plot? For eg. The plot labels are HabitatBirch-HabitatOak, I want it to be labeled as Birch-Oak.

 

Hope I have posted it according to the guidelines, let me know otherwise. 

Cheers .../Murli


From ggrothendieck at gmail.com  Sat Jul 22 19:37:01 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 22 Jul 2006 13:37:01 -0400
Subject: [R] Multcomp
In-Reply-To: <A32055BDEA88C34BB3DBBCD229380778050FB0@iu-mssg-mbx109.ads.iu.edu>
References: <A32055BDEA88C34BB3DBBCD229380778050FB0@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <971536df0607221037v70577f8dya900d10f8134b84f@mail.gmail.com>

On 7/22/06, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> Here it is again, hope this is more clear
>
> I am using the following data (only a small subset is given):
>
> Habitat Fungus.yield
> Birch 20.83829053
> Birch 22.9718181
> Birch 22.28216829
> Birch 24.23136797
> Birch 22.32147961
> Birch 20.30783598
> Oak 27.24047258
> Oak 29.7730014
> Oak 30.12608508
> Oak 25.76088669
> Oak 30.14750974
> Hornbeam 17.05307949
> Hornbeam 15.32805111
> Hornbeam 18.26920177
> Hornbeam 21.30987049
> Hornbeam 21.7173223
>
> I am using the multcomp package to do multiple comparisons as follows
>
> library(multcomp) # loads the package
>
> fungus<-read.table("fungi.txt", Header=T)    # Reads the data from file saved as fungi.txt
>
>
> fungus.cirec<-simint(Fungus.yield~Habitat, data=fungus,conf.level=0.95,type =c("Tukey"))  # Computes cimultaneous intervals using Tukey's method
>
>
> plot(fungus.cirec)   # plots the data
>
> The plot function plots all the comparisons, I want to plot only part of the data since it clutters the graph.
>
> How do I plot only part of the data ?

Don't understand what "part of the data" means.  Use data = fungus[1:10,]
in the simint call to just process the first 10 data rows.  To
eliminate a portion of
the plot note in ?plot.hmtest that there is a ... argument and its description
is that its passed to plot which in turn passes them to plot.default so you
could use ylim = 1:2, say, to show only part of the plot vertically.

>
> How do I tell it to mark the significant comparisons?

# after your plot statement:
pp <- locator()
# now click on a spot on the plot
# and then right click and choose stop
text(pp$x, pp$y, "some text")

>
> How do I get rid of the field names in the plot? For eg. The plot labels are HabitatBirch-HabitatOak, I want it to be labeled as Birch-Oak.

# change rownames of the estimates which has the effect
# of changing the y axis labels
rownames(fungus.cirec$estimate) <- LETTERS[1:3]
plot(fungus.cirec)


From spencer.graves at pdf.com  Sat Jul 22 20:03:46 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 23 Jul 2006 02:03:46 +0800
Subject: [R] Random structure of nested design in lme
In-Reply-To: <E632249B3E11B14AAABA75FDB5F32B24132A0B@EXCHANGE4.unifr.ch>
References: <2323A6D37908A847A7C32F1E3662C80E132CEE@dc1ex01.air.org>
	<E632249B3E11B14AAABA75FDB5F32B24132A0B@EXCHANGE4.unifr.ch>
Message-ID: <44C26882.2090209@pdf.com>

	  Have you considered the following:

	  anova(lme(NA.1~soiltype*habitat,random=~1|destination/origin))

	  This seems more closely to match the 'aov' command in your original 
post.  This model might be written in more detail as follows:

	  NA.1[s, h, i,j,k] = b0 + ST[s] + H[h] +
		ST.H[s[i],j[j] j] + d[i] + o[i,j] + e[i,j,k]

where 	  b0 = a constant to be estimated,

	  s = the soil type for that particular sample,

	  h = the habitat for that sample,

	  ST = soil type coefficients to be estimated subject to a constraint 
that they sum to 0,

	  H = habitat coefficients to be estimated subject to the constraint 
that they sum to 0,

	  ST.H = soil type by habitat interaction coefficients to be estimated 
subject to constraints that ST.H[s,.] sum to 0 and ST.H[., h] also sum 
to 0,

	  d[i] = a random deviation associated with each destination, assuming 
the d's are all normal, independent, with mean 0 and unknown but 
constant variance s2.d

	  o[i, j] = a random deviation associated with each destination / 
origin combination, assuming the o's are all normal, independent, with 
mean 0 and unknown variance s2.o,

and 	  e[i,j,j] = the standard unknown noise term, normal, independent 
with mean 0 and unknown variance s2.e.

	  The model you wrote includes nested noise terms for soil type and 
habitat as well.  These terms are not estimable, which makes the answers 
garbage, but the 'lme' function does not check for replicates and 
therefore sometimes gives garbage answers without warning.

	  To get more information from the fit, I suggest you first try 
'methods(class="lme")', and review help pages associated with what you 
see listed there.

	  Have you looked at Pinheiro and Bates (2000) Mixed-Effects Models in 
S and S-Plus (Springer)?  This is my all-time favorite reference on 
Bates has been one of the leading original contributors in variance 
components analysis and nonlinear estimation more generally for over 25 
years.  The 'nlme' package is the product of his work and the work of 
many of his graduate students prior to 2000.  The book, at least from my 
perspective, is very well written.  Moreover, the standard R 
distribution includes files named "ch01.R", "ch02.R", ..., "ch06.R", 
"ch08.R" with the R scripts accompanying each chapter in the book in 
"~\library\nlme\scripts" under the R installation directory on your hard 
drive, e.g. "D:\Program files\R\R-2.3.1\library\nlme\scripts", on my 
computer.  There are minor changes in the syntax in a few places between 
the book and the current R implementation that make it impossible to get 
some of the published answers.  Using these script files increases the 
likelihood that you will get essentially the book's answers and won't be 
defeated by subtle typographical errors or by the difference between x^2 
and I(x^2), for example.

	  If you would like further information from this listserver, please 
submit another post, preferably including a "commented, minimal, 
self-contained, reproducible code", as suggested in the posting guide 
"www.R-project.org/posting-guide.html".

	  Hope this helps.
	  Spencer Graves

ESCHEN Rene wrote:
> Although I know it's not correct, this is what I tried in lme:
> 
> anova(lme(NA.1~soiltype*habitat,random=~1|destination/habitat/origin/soiltype))
> 
> #                 numDF denDF   F-value p-value
> #(Intercept)          1   130 12.136195  0.0007
> #soiltype             1   130 15.099792  0.0002
> #habitat              1    10  0.699045  0.4226
> #soiltype:habitat     1   130  2.123408  0.1475
> 
> Ren?.
> 
> -----Original Message-----
> From: Doran, Harold [mailto:HDoran at air.org]
> Sent: Wed 2006-07-19 13:53
> To: ESCHEN Rene; r-help at stat.math.ethz.ch
> Subject: RE: [R] Random structure of nested design in lme
>  
> Can you provide an example of what you have done with lme so we might be able to evaluate the issue? 
> 
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ESCHEN Rene
>> Sent: Wednesday, July 19, 2006 7:37 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] Random structure of nested design in lme
>>
>> All,
>>
>> I'm trying to analyze the results of a reciprocal transplant 
>> experiment using lme(). While I get the error-term right in 
>> aov(), in lme() it appears impossible to get as expected. I 
>> would be greatful for any help.
>>
>> My experiment aimed to identify whether two fixed factors 
>> (habitat type and soil type) affect the development of 
>> plants. I took soil from six random sites each of two types 
>> (arable and grassland) and transplanted them back into the 
>> sites of origin in such way that in each of the sites there 
>> were six pots containing arable soil and six pots of 
>> grassland soil, each containing a seedling.
>>
>> With aov(), I got the analysis as I expected, with habitat 
>> type tested against destination site, and soil type tested 
>> against origin site:
>>
>> summary(aov(response~soiltype*habitat+Error(destination+origin)))
>> #
>> #Error: destination
>> #          Df  Sum Sq Mean Sq F value Pr(>F)
>> #habitat    1  1.0000  1.0000   0.699 0.4226
>> #Residuals 10 14.3056  1.4306               
>> #
>> #Error: origin
>> #          Df  Sum Sq Mean Sq F value   Pr(>F)   
>> #soiltype   1 1.77778 1.77778  11.636 0.006645 **
>> #Residuals 10 1.52778 0.15278                    
>> #---
>> #Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #
>> #Error: Within
>> #                  Df  Sum Sq Mean Sq F value Pr(>F)
>> #soiltype:habitat   1  0.2500  0.2500  2.1774 0.1427
>> #Residuals        120 13.7778  0.1148     
>>
>> However, when I try to replicate this analysis in lme, I am 
>> unable to get the structure of the random factors (origin and 
>> destination) correct. Does anyone have a suggestion how to 
>> resolve this problem?
>>
>> Thanks in advance.
>>
>> Ren? Eschen
>>
>> CABI Bioscience Centre Switzerland
>> Rue des Grillons 1
>> 2800 Del?mont
>> Switzerland
>>
>> 	[[alternative HTML version deleted]]
>>
>>
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From cgb at datanalytics.com  Sat Jul 22 21:10:20 2006
From: cgb at datanalytics.com (Carlos J. Gil Bellosta)
Date: Sat, 22 Jul 2006 21:10:20 +0200
Subject: [R] ifelse command
In-Reply-To: <BAY117-DAV134943295600392ADBC2C1F0670@phx.gbl>
References: <BAY117-DAV134943295600392ADBC2C1F0670@phx.gbl>
Message-ID: <1153595420.4339.21.camel@lenin>

Dear Xin,

Although I have no idea what your function does, I believe it would be
"formally" correct in the following way:

foo <- function (parameters,y,x1,x2)
{
	p <-parameters[1]
	alpha1<-parameters[2]
	beta1<-parameters[3]
	delta1<-parameters[4]
	alpha2<-parameters[5]

	mu<-alpha1*((x1)^beta1)*exp(-delta1*(x1^alpha2))

	if(y>0 & x1>0 & x2==1) return( lgamma(y+p)+p*(log(p)-log(mu
+p))+y*(log(mu)-log(mu+p))-lfactorial(y)-lgamma(p) )
	if(y>0 & x1>0 & x2==2) return( lgamma(y+p)+p*(log(p)-log(mu
+p))+y*(log(mu)-log(mu+p))-lfactorial(y)-lgamma(p) )
	if(y>0 & x1>0 & x2==3) return( lgamma(y+p)+p*(log(p)-log(mu
+p))+y*(log(mu)-log(mu+p))-lfactorial(y)-lgamma(p) )
	
	return( "what then?" )
}

Please, compare your code to mine. You will discover more than one error
in your code. See also where the if() parenthesis closes. I do not
believe your else's are necessary within a function. See also that the
code after your ifs is always the same.

Sincerely,

Carlos J. Gil Bellosta
http://www.datanalytics.com
http://www.data-mining-blog.com


El s?b, 22-07-2006 a las 13:25 +0100, Xin escribi?:
> Dear:
> 
>    I try to revise the maximum likelihood function below using something constrains. But it seems something wrong with it.  Becasue R would not allow me to edit the function like this. It is very appreciate if you can help.
>   
> function (parameters,y,x1,x2)
> 
> {
> 
> p<-parameters[1]
> 
> alpha1<-parameters[2]
> 
> beta1<-parameters[3)]
> 
> delta1<-parameters[4]
> 
> alpha2<-parameters[5]
> 
> mu<-alpha1*((x1)^beta1)*exp(-delta1*(x1^alpha2))
> 
> if(y>0 & x1>0 & x2==1,
> 
> L<-lgamma(y+p)+p*(log(p)-log(mu+p))+y*(log(mu)-log(mu+p))-lfactorial(y)-lgamma(p)
> 
> )
> 
> else
> 
> if(y>0 & x1>0 & x2==2,
> 
> L<-lgamma(y+p)+p*(log(p)-log(mu+p))+y*(log(mu)-log(mu+p))-lfactorial(y)-lgamma(p)
> 
> )
> 
> else
> 
> if(y>0 & x1>0 & x2==3,
> 
> L<-lgamma(y+p)+p*(log(p)-log(mu+p))+y*(log(mu)-log(mu+p))-lfactorial(y)-lgamma(p)
> 
> )
> 
> else
> 
> L
> 
> }
> 
> Thanks a lot!
> 
> Xin Shi
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wiedenhoeft at gmx.net  Sat Jul 22 21:37:21 2006
From: wiedenhoeft at gmx.net (John Wiedenhoeft)
Date: Sat, 22 Jul 2006 21:37:21 +0200
Subject: [R] Putting x into the right subset
In-Reply-To: <1153574492.6457.25.camel@localhost>
References: <1153527522.20667.29.camel@localhost>
	<44C1F3C0.60002@statistik.uni-dortmund.de>
	<1153574492.6457.25.camel@localhost>
Message-ID: <1153597041.6457.33.camel@localhost>

Ahh, got it:
> There must be an error in reasoning, but I can't figure out where it
> is...

Consider 3 pairs: (1, 2), (2, 3), (1, 3), first is put in the first
line, second in the second, and third in the first, resulting in the 3
being contained in both. Simply folding lines together solves it.


From spencer.graves at pdf.com  Sat Jul 22 21:40:08 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 23 Jul 2006 03:40:08 +0800
Subject: [R] fracdiff
In-Reply-To: <Pine.LNX.4.43.0607191242010.11072@hymn04.u.washington.edu>
References: <Pine.LNX.4.43.0607191242010.11072@hymn04.u.washington.edu>
Message-ID: <44C27F18.6060808@pdf.com>

	  Confidence intervals for the fracdiff parameter estimates shouldn't 
be too difficult:  the ?fracdiff help file says it returns a list with 
components including 'd', 'ar' 'ma', and 'stderror.dpq'.  From these one 
should easily get the coeffecients, standard errors and naive, Wald 
confidence intervals;  for your convenience, I've embedded these in the 
functions 'coef.fracdiff' and 'confint.fracdiff' given below.

	  By 'sigma2', I assume you mean the estimated variance of the whitened 
residuals.  That is not so obvious.  For that, I might try 
'arima(fitdiff(...))$sigma2', operating on a 'fracdiff' fit.

	  Hope this helps.
	  Spencer Graves
##### coef and confint functions for output of fracdiff:
coef.fracdiff <- function(object){
   unlist(object[c("d", "ar", "ma")])
}

confint.fracdiff <- function(object, parm, level=0.95, ...){
   b <- coef.fracdiff(object)
   se <- object$stderror.dpq
   names(se) <- names(b)
#
   if(missing(parm))
     parm <- 1:length(b)
   b. <- b[parm]
   se. <- se[parm]
#
   conf.c <- (1-level)/2
   conf2 <- c(conf.c, 1-conf.c)
   confNames <- paste(100*conf2, "%")
   k. <- length(b.)
   CI <- b+outer(se., qnorm(conf2))
   dimnames(CI)[[2]] <- confNames
   CI
}



Melissa Ann Haltuch wrote:
> Hi, I'm using the function fracdiff and can not 
figure out how to get the estimated values for
sigma2 or confidence intervals for the parameter
estimates. Does anyone know how to obtain these
values?
> Thanks,
> Melissa
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From p.dalgaard at biostat.ku.dk  Sat Jul 22 21:58:26 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Jul 2006 21:58:26 +0200
Subject: [R] connection to X11 problem
In-Reply-To: <XFMail.060721233409.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060721233409.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x21wsdh0nx.fsf@turmalin.kubism.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> On 21-Jul-06 Paquet, Agnes wrote:
> > Dear List,
> > 
> > I am a new Mac user and I am having problem generating png (or jpeg)
> > using the GUI version of R. I installed R-2.3.1.dmg (custom install
> > with
> > everything selected) and X11User.pkg but I am still getting the
> > following X11 connection error when I try to generate a png (or a
> > jpeg):
> > 
> > Error in X11(paste("png::", filename, sep = ""), width, height,
> > pointsize,  : 
> >     unable to start device PNG
> > In addition: Warning message:
> > unable to open connection to X11 display ''
> > 
> > I tried to set up the DISPLAY variable using the command:
> > 
> > Sys.putenv("DISPLAY"=":0")
> > 
> > but I am still running into the same problem. 
> 
> Like Marc, I don't use a Mac either. But the underlying BSD OS
> is basically similar to Linux. On Linux, my primary X11 DISPLAY
> envvar would be ":0.0", so (at a guess) I suggest you try
> 
>   Sys.putenv("DISPLAY"=":0.0")
> 
> Hoping this helps!
> Ted.

A silly question must be asked: One thing is *installing* the X11
package on a Mac, but you do need to be actually *running* an X
server. Are you?

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From mnair at iusb.edu  Sat Jul 22 23:28:02 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Sat, 22 Jul 2006 17:28:02 -0400
Subject: [R] Multcomp
In-Reply-To: <971536df0607221037v70577f8dya900d10f8134b84f@mail.gmail.com>
Message-ID: <A32055BDEA88C34BB3DBBCD2293807784CB7C4@iu-mssg-mbx109.ads.iu.edu>

-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Saturday, July 22, 2006 1:37 PM
To: Nair, Murlidharan T
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] Multcomp

On 7/22/06, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> Here it is again, hope this is more clear
>
> I am using the following data (only a small subset is given):
>
> Habitat Fungus.yield
> Birch 20.83829053
> Birch 22.9718181
> Birch 22.28216829
> Birch 24.23136797
> Birch 22.32147961
> Birch 20.30783598
> Oak 27.24047258
> Oak 29.7730014
> Oak 30.12608508
> Oak 25.76088669
> Oak 30.14750974
> Hornbeam 17.05307949
> Hornbeam 15.32805111
> Hornbeam 18.26920177
> Hornbeam 21.30987049
> Hornbeam 21.7173223
>
> I am using the multcomp package to do multiple comparisons as follows
>
> library(multcomp) # loads the package
>
> fungus<-read.table("fungi.txt", Header=T)    # Reads the data from
file saved as fungi.txt
>
>
> fungus.cirec<-simint(Fungus.yield~Habitat,
data=fungus,conf.level=0.95,type =c("Tukey"))  # Computes cimultaneous
intervals using Tukey's method
>
>
> plot(fungus.cirec)   # plots the data
>
> The plot function plots all the comparisons, I want to plot only part
of the data since it clutters the graph.
>
> How do I plot only part of the data ?

Don't understand what "part of the data" means.  Use data =
fungus[1:10,]
in the simint call to just process the first 10 data rows.  To
eliminate a portion of
the plot note in ?plot.hmtest that there is a ... argument and its
description
is that its passed to plot which in turn passes them to plot.default so
you
could use ylim = 1:2, say, to show only part of the plot vertically.


>>I have to use all the data for my computation. Since the number of
>>comparisons are many I want to plot it in different graphs so that the
>>graph does not look cluttered. So "part of the data" means a subset of
the >>comparisons in one graph and another subset in another and so
on....
>> Can you give me an example of the  "ylim" parameter. I tried
>>plot(fungus.cirec, ylim=1:2) and it was not happy with it. I am not
>>completely comfortable with the ... argument. I am trying to read up
on >>it. 


>
> How do I tell it to mark the significant comparisons?

# after your plot statement:
pp <- locator()
# now click on a spot on the plot
# and then right click and choose stop
text(pp$x, pp$y, "some text")

>> This is very useful

>
> How do I get rid of the field names in the plot? For eg. The plot
labels are HabitatBirch-HabitatOak, I want it to be labeled as
Birch-Oak.

# change rownames of the estimates which has the effect
# of changing the y axis labels
rownames(fungus.cirec$estimate) <- LETTERS[1:3]
plot(fungus.cirec)

>>This only labels it a A,B or C. What I wanted was to remove the field
name >>"Habitat" in this case and out put the label as Birch-Oak. 

Thanks ../Murli


From ggrothendieck at gmail.com  Sun Jul 23 01:01:39 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 22 Jul 2006 19:01:39 -0400
Subject: [R] Multcomp
In-Reply-To: <A32055BDEA88C34BB3DBBCD2293807784CB7C4@iu-mssg-mbx109.ads.iu.edu>
References: <971536df0607221037v70577f8dya900d10f8134b84f@mail.gmail.com>
	<A32055BDEA88C34BB3DBBCD2293807784CB7C4@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <971536df0607221601q2afa5545jf147d8fe2b049629@mail.gmail.com>

On 7/22/06, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> Sent: Saturday, July 22, 2006 1:37 PM
> To: Nair, Murlidharan T
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] Multcomp
>
> On 7/22/06, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> > Here it is again, hope this is more clear
> >
> > I am using the following data (only a small subset is given):
> >
> > Habitat Fungus.yield
> > Birch 20.83829053
> > Birch 22.9718181
> > Birch 22.28216829
> > Birch 24.23136797
> > Birch 22.32147961
> > Birch 20.30783598
> > Oak 27.24047258
> > Oak 29.7730014
> > Oak 30.12608508
> > Oak 25.76088669
> > Oak 30.14750974
> > Hornbeam 17.05307949
> > Hornbeam 15.32805111
> > Hornbeam 18.26920177
> > Hornbeam 21.30987049
> > Hornbeam 21.7173223
> >
> > I am using the multcomp package to do multiple comparisons as follows
> >
> > library(multcomp) # loads the package
> >
> > fungus<-read.table("fungi.txt", Header=T)    # Reads the data from
> file saved as fungi.txt
> >
> >
> > fungus.cirec<-simint(Fungus.yield~Habitat,
> data=fungus,conf.level=0.95,type =c("Tukey"))  # Computes cimultaneous
> intervals using Tukey's method
> >
> >
> > plot(fungus.cirec)   # plots the data
> >
> > The plot function plots all the comparisons, I want to plot only part
> of the data since it clutters the graph.
> >
> > How do I plot only part of the data ?
>
> Don't understand what "part of the data" means.  Use data =
> fungus[1:10,]
> in the simint call to just process the first 10 data rows.  To
> eliminate a portion of
> the plot note in ?plot.hmtest that there is a ... argument and its
> description
> is that its passed to plot which in turn passes them to plot.default so
> you
> could use ylim = 1:2, say, to show only part of the plot vertically.
>
>
> >>I have to use all the data for my computation. Since the number of
> >>comparisons are many I want to plot it in different graphs so that the
> >>graph does not look cluttered. So "part of the data" means a subset of
> the >>comparisons in one graph and another subset in another and so
> on....
> >> Can you give me an example of the  "ylim" parameter. I tried
> >>plot(fungus.cirec, ylim=1:2) and it was not happy with it. I am not
> >>completely comfortable with the ... argument. I am trying to read up
> on >>it.
>
>
> >
> > How do I tell it to mark the significant comparisons?
>
> # after your plot statement:
> pp <- locator()
> # now click on a spot on the plot
> # and then right click and choose stop
> text(pp$x, pp$y, "some text")
>
> >> This is very useful
>
> >
> > How do I get rid of the field names in the plot? For eg. The plot
> labels are HabitatBirch-HabitatOak, I want it to be labeled as
> Birch-Oak.
>
> # change rownames of the estimates which has the effect
> # of changing the y axis labels
> rownames(fungus.cirec$estimate) <- LETTERS[1:3]
> plot(fungus.cirec)
>
> >>This only labels it a A,B or C. What I wanted was to remove the field
> name >>"Habitat" in this case and out put the label as Birch-Oak.
>

See ?gsub


From stgries_lists at arcor.de  Sun Jul 23 03:48:47 2006
From: stgries_lists at arcor.de (Stefan Th. Gries)
Date: Sun, 23 Jul 2006 03:48:47 +0200 (CEST)
Subject: [R] RfW 2.3.1: regular expressions to detect pairs of identical
 word-final character sequences
Message-ID: <21027919.1153619327251.JavaMail.ngmail@webmail18>

Dear all

I use R for Windows 2.3.1 on a fully updated Windows XP Home SP2 machine and I have two related regular expression problems.

platform       i386-pc-mingw32           
arch           i386                      
os             mingw32                   
system         i386, mingw32             
status                                   
major          2                         
minor          3.1                       
year           2006                      
month          06                        
day            01                        
svn rev        38247                     
language       R                         
version.string Version 2.3.1 (2006-06-01)


I would like to find cases of words in elements of character vectors that end in the same character sequences; if I find such cases, I want to add <r> to both potentially rhyming sequences. An example:

INPUT:This is my dog.
DESIRED OUTPUT: This<r> is<r> my dog.

I found a solution for cases where the potentially rhyming words are adjacent:

text<-"This is my dog."
gsub("(\\w+?)(\\W\\w+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)

However, with another text vector, I came across two problems I cannot seem to solve and for which I would love to get some input.

(i) While I know what to do for non-adjacent words in general

gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", "This not is my dog", perl=TRUE) # I know this is not proper English ;-)

this runs into problems with overlapping matches:

text<-"And this is the second sentence"
gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)
[1] "And<r> this is the second<r> sentence"

It finds the "nd" match, but since the "is" match is within the two "nd"'s, it doesn't get it. Any ideas on how to get all pairwise matches?

(ii) How would one tell R to match only when there are 2+ characters matching? If the above expression is applied to another character string

text<-"this is an example sentence."
gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)

it also matches the "e"'s at the end of example and sentence. It's not possible to get rid of that by specifying a range such as {2,}

text<-"this is an example sentence."
gsub("(\\w{2,}?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)

because, as I understand it, this requires the 2+ cases of \\w to be identical characters:

text<-"doo yoo see mee?"
gsub("(\\w{2,}?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)

Again, any ideas?

I'd really appreciate any snippets of codes, pointers, etc.
Thanks so much,
STG
--
Stefan Th. Gries
-----------------------------------------------
University of California, Santa Barbara
http://www.linguistics.ucsb.edu/faculty/stgries


From kishor_1974 at hotmail.com  Sun Jul 23 04:07:37 2006
From: kishor_1974 at hotmail.com (Neil KM)
Date: Sat, 22 Jul 2006 22:07:37 -0400
Subject: [R] Warning Messages using rq -quantile regressions
Message-ID: <BAY123-F95B3BFB48C4470529589D9F640@phx.gbl>

I am a new to using quantile regressions in R. I have estimated a set of 
coefficients using the method="br" algorithm with the rq command at various 
quantiles along the entire distribution.

My data set contains approximately 2,500 observations and I have 7 predictor 
variables. I receive the following warning message:

Solution may be nonunique in: rq.fit.br(x, y, tau = tau, ...)

There are 13 warnings of this type after I run a single  model. My results 
are similiar to the results I received in other stat programs using quantile 
reg procedures. I am unclear what these warning messages imply and if there 
are problems with model fit/convergence that I may need to consider.
Any help would be appreciated. Thanks!


From ggrothendieck at gmail.com  Sun Jul 23 06:05:26 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 23 Jul 2006 00:05:26 -0400
Subject: [R] RfW 2.3.1: regular expressions to detect pairs of identical
	word-final character sequences
In-Reply-To: <21027919.1153619327251.JavaMail.ngmail@webmail18>
References: <21027919.1153619327251.JavaMail.ngmail@webmail18>
Message-ID: <971536df0607222105v742027c0p4e1a79d61f6194ca@mail.gmail.com>

The following requires more than just a single gsub but it does solve
the problem.  Modify to suit.

The first gsub places <...> around the first occurrence of any
duplicated suffixes.  We use the (?=...) zero width regexp
to circumvent the nesting problem.

Then we use strapply from the gsubfn package to extract
the suffixes so marked and paste them together to pass
to a second gsub which locates them in the original
string appending an <r> to each.   Uncomment the commented
pat if you only want to match 2+ character suffixes.

library(gsubfn)
# places <...> around first occurrences of repeated suffixes
text <- "And this is the second sentence"
pat <- "(\\w+)(?=\\b.+\\1\\b)"
# pat <- "(\\w\\w+)(?=\\b.+\\1\\b)"
out <- gsub(pat, "\\<\\1\\>", text, perl = TRUE)

suff <- strapply(out, "<([^>]+)>", function(x,y)y)[[1]]
gsub(paste("(", paste(suff, collapse = "|"), ")\\b", sep = ""), "\\1<r>", text)


On 7/22/06, Stefan Th. Gries <stgries_lists at arcor.de> wrote:
> Dear all
>
> I use R for Windows 2.3.1 on a fully updated Windows XP Home SP2 machine and I have two related regular expression problems.
>
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
>
>
> I would like to find cases of words in elements of character vectors that end in the same character sequences; if I find such cases, I want to add <r> to both potentially rhyming sequences. An example:
>
> INPUT:This is my dog.
> DESIRED OUTPUT: This<r> is<r> my dog.
>
> I found a solution for cases where the potentially rhyming words are adjacent:
>
> text<-"This is my dog."
> gsub("(\\w+?)(\\W\\w+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)
>
> However, with another text vector, I came across two problems I cannot seem to solve and for which I would love to get some input.
>
> (i) While I know what to do for non-adjacent words in general
>
> gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", "This not is my dog", perl=TRUE) # I know this is not proper English ;-)
>
> this runs into problems with overlapping matches:
>
> text<-"And this is the second sentence"
> gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)
> [1] "And<r> this is the second<r> sentence"
>
> It finds the "nd" match, but since the "is" match is within the two "nd"'s, it doesn't get it. Any ideas on how to get all pairwise matches?
>
> (ii) How would one tell R to match only when there are 2+ characters matching? If the above expression is applied to another character string
>
> text<-"this is an example sentence."
> gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)
>
> it also matches the "e"'s at the end of example and sentence. It's not possible to get rid of that by specifying a range such as {2,}
>
> text<-"this is an example sentence."
> gsub("(\\w{2,}?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)
>
> because, as I understand it, this requires the 2+ cases of \\w to be identical characters:
>
> text<-"doo yoo see mee?"
> gsub("(\\w{2,}?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)
>
> Again, any ideas?
>
> I'd really appreciate any snippets of codes, pointers, etc.
> Thanks so much,
> STG
> --
> Stefan Th. Gries
> -----------------------------------------------
> University of California, Santa Barbara
> http://www.linguistics.ucsb.edu/faculty/stgries
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bgreen at dyson.brisnet.org.au  Sun Jul 23 08:39:00 2006
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Sun, 23 Jul 2006 16:39:00 +1000
Subject: [R] constructing a dataframe from a database of newspaper articles
In-Reply-To: <mailman.11.1153303205.32321.r-help@stat.math.ethz.ch>
Message-ID: <5.1.0.14.0.20060723103511.00bf7d40@pop3.brisnet.org.au>



I am hoping for some assistance with formatting a large text file which 
consists of a series of individual records. Each record includes specific 
labels/field names (a sample of 1 record (one of the longest ones) is 
below  - at end of post. What I want to do is reformat the data, so that 
each individual record becomes a row (some cells will have a lot of text). 
For example, the column variables I want are (a) HD  in one column 
(b)    BY in one column (c) WC data in one column,  (d) PD data in one 
column, (e) SC data in one column (f) PG data in one column  & g) LP and TD 
text in one column  - this column can contain quite a lot of text, e.g 1900 
words. The other fields are unwanted

If there were 150 individual records, when formatted this would be a 7 
column by 150 row dataset.

I was advised to:

1. read in the file using readLines giving a character vector one element 
per input line.
2. convert that to lines of the form:
id op text
where each such line is a field and multiline fields have been collapsed 
into a single line of text. This step involves
detailed processing and you could do it in a loop or you could try a 
vectorized approach. A vectorized approach
will likely involve using
3. the lines created above could be converted to a data frame with three 
columns and
4. reshape used to create a "wide" data frame.
5. then write it out using write.csv.

I have got as far as being able to read the text into R  - I am unsure if 
the warning is a problem. I am however, not at all sure what I need to do next.

Any assistance is much appreciated,


Bob

(A) syntax

  mht <- scan(what="c:\\cm-mht1.txt").
readLines("c:\\cm-mht1.txt",n = -1)

[8376] "?? 2006 Dow Jones Reuters Business Interactive LLC (trading as 
Factiva). All "
[8377] "rights reserved. 
"
Warning message:
incomplete final line found by readLines on 'c:\cm-mht1.txt'

(B) sample data

       HD Was Charles Manson temporarily insane when he led a wild killing
       rampage in the US in 1969?
       BY By Deborah Cassrels.
       WC 1834 words
       PD 23 June 2001
       SN Courier Mail
       SC COUMAI
       PG 30
       LA English
       CY (c) 2001 Queensland Newspapers Pty Ltd

       LP Was Charles Manson temporarily insane when he led a wild killing
       rampage in the US in 1969? Clearly he was mad and bad. But would
       Queensland have placed him before its Mental Health Tribunal, found 
him of
       unsound mind at the time of his crimes, institutionalised him and
       "treated" his illness? WHY is Queensland the only jurisdiction in the
       Commonwealth with a Mental Health Tribunal which establishes if an 
accused
       is fit to face trial or of unsound mind at the time of an alleged 
offence?
       Why is mental incompetence not determined in an adversarial court by a
       jury? Under the Mental Health Act 1974, the tribunal, a statutory body
       operating since 1985, comprises three-yearly appointments of a Supreme
       Court judge and two assisting psychiatrists, whose advice does not 
have to
       be accepted. The judge alone constitutes the tribunal, an inquisitorial
       process conducted in the Supreme Court in Brisbane.

       TD Victims or family are not notified of hearings or allowed to submit
       victim impact statements. They are prohibited from talking to the media
       until 28 days after the decision. And when patients return to the
       community there is no requirement for neighbours or victims to be
       notified. Is this legislation enlightened or are we just suckers, 
falling
       for time and money-saving strategies? The tribunal has earned a 
reputation
       as progressive, humane and economical among some judges who have 
presided
       over it. The inaugural chair, former Supreme Court judge Angelo 
Vasta QC,
       thinks the tribunal system is "enlightened" and "it saves an enormous
       amount of expenditure". He points to the humane side of treating the 
ill
       in a secure hospital rather than punishing them for offences but is
       uncomfortable with borderline cases. "Whether people are mad or bad 
ought
       to be established by a very thorough investigation.
       The associated Patient Review Tribunals (of which there are five) 
consist
       of three to six members, including the chair who is a legal officer, a
       medical practitioner and a mental health professional. A 
psychiatrist is
       not required. The other three have no specific qualifications and can
       include former patients. The tribunals operate in closed hearings and
       patients of unsound mind or unfit for trial are reviewed every 12 
months.
       Leave is granted either by the Mental Health Tribunal or the Patient
       Review Tribunal, which determine when a restricted patient is 
discharged
       into the community. Says the Director of Mental Health, Dr Peggy Brown:
       "In the case of serious offences you can be assured the period of
       monitoring is quite lengthy." Under the Mental Health Act 2000 to be
       implemented late this year, the tribunal will be replaced by a Mental
       Health Court and the Patient Review Tribunal by the Mental Health 
Review
       Tribunal. Queensland Health Minister Wendy Edmond says the name change
       reflects transparency, with proceedings under oath and 
cross-examination
       of witnesses. The legislation represents "real change to the rights of
       victims of crime". But there is still an embargo on publishing 
decisions
       in the media.
       Dr Brown says when patients are granted leave, victims or families can
       apply to be notified but decisions will be made on individual cases. 
"The
       (new) tribunal has to establish that there are reasonable grounds 
for the
       notification order to be made ... and it's also an appealable 
decision,"
       returning to the Mental Health Court.
       Brown says there are efficiencies in the new legislation but "it's not
       about saving money". The main advantages were that victims could make
       submissions to both bodies. Concerns still might not be addressed but
       reasons were expected to be provided. The court's composition and sole
       power of the judge will be retained. Victims or relatives can be 
notified
       of hearings and decisions about the patient. If not, reasons must be
       provided. The Patient Review Tribunals will be replaced by one tribunal
       with hearings still closed. It will comprise up to five members 
including
       a president (a lawyer of at least seven years' standing), 
psychiatrist or
       medical practitioner and community members and it will be chaired by a
       legal officer. Leave will be approved by the corresponding previous
       bodies. Chief Justice Paul de Jersey who presided over the 1995 case of
       Ross Farrah, a paranoid schizophrenic, who after murdering his 
girlfriend,
       Christine Nash, was allowed out of the John Oxley Centre to play 
sport and
       see movies, says the proposed legislative changes to the Mental 
Health Act
       appear to be "refinements". Two weeks ago, Nash's teenage son Wade
       committed suicide after suffering years of torment following his 
mother's
       murder. In May 1996, a letter was sent to the tribunal by now former
       director of secure care services at John Oxley Dr Peter Fama. It said:
       "Should Ross be committed to the Tribunal for trial on a charge of
       manslaughter or murder, I have to report that he is now fit to be 
placed
       in corrective custody ... There is no clinical need for further 
detention
       of Ross in hospital." De Jersey has been involved in the process of
       amendments in the new Act and believes the "adjustments" are 
satisfactory:
       "It's probably a question of how they're implemented. I thought the
       changes were more concerned with image than effecting substantial 
change
       to the system, calling it a court rather than a tribunal. There is some
       attempt to enhance the openness of the procedures such as the advice 
given
       by the existing psychiatrists being revealed in open court to the judge
       but they're aspects of streamlining rather than substantive change." He
       says many people are irked by a perceived disproportion between the
       treatment of mentally ill offenders and their victims. "As a 
community we
       need much more positively to address the situation of victims." De 
Jersey
       points to the James Bulger murder in the UK eight years ago when two
       10-year-old boys abducted and battered James, two, to death. The 
killers
       are expected to be freed soon. Says de Jersey: "Whatever one thinks of
       future plans for the young offenders it is extraordinary, if reportedly
       correct, that so little help has been given to the bereft mother of the
       murdered toddler. "Similarly, here, it is generally indefensible where
       victims or the families of victims are not informed of details of the
       likely release of their offenders, and even before that where they 
are not
       given a proper explanation as to the process and counselling to help 
them
       comprehend that process and as well the consequences of the crime. 
We are
       as a community moving towards a greater focus on the position of 
victims
       but a lot more needs to be done. "The anguish of victims and the 
families
       of victims that insane offenders appear to escape punishment is
       understandable. The issue is whether the community is prepared to 
accept
       that insane offenders primarily need treatment." The Mental Health
       Tribunal worked on two assumptions, that offenders of unsound mind 
should,
       in the interests of the community, be treated rather than punished, and
       that a determination whether an offender was of unsound mind could
       responsibly be made by a Supreme Court judge with expert psychiatric
       assistance. "I have wondered whether with the ultimately serious crimes
       such as murder the community may not reasonably demand that in the
       interests of reassurance that the determination be made by a jury." He
       believes the community's longer term interests would best be served by
       medically treating insane offenders in a hospital rather than a prison,
       where if rehabilitated, they could contribute to the community. "I 
accept,
       however, that in many cases there will be serious residual concern, for
       example, can the offender be trusted, if left unsupervised, to 
continue to
       take the relevant medication?"
       De Jersey admits problems have arisen when offenders, granted leave,
       stopped taking medication but says if they can be relied upon to 
maintain
       stability through medication it would be inhumane to keep them 
locked up.
       Continued medical monitoring was necessary. If conditions were breached
       the person should be returned to restricted custody at the psychiatric
       hospital. While the most vulnerable in society deserve compassion it 
does
       not surprise there is public concern about lack of proper scrutiny, the
       capacity to re-offend and misuse of the legal process by using 
insanity as
       a defence. IN the general quest to improve treatment provisions for
       patients the 2000 Act says: "The new legislation provides for 
involuntary
       treatment in the community as an alternative to being an in-patient 
in a
       mental health service which reflects contemporary clinical practice and
       the principle of reform that involuntary treatment must be in the least
       restrictive form."
       Perhaps the overwhelming feeling is patients' rights have priority over
       victims' rights. Ted Flack, spokesman for the Queensland Homicide 
Victims
       Support Group says the new Act provides a better environment for 
victims'
       participation, but there are serious flaws. The rights of homicide 
victims
       were not guaranteed and this caused an inordinate amount of distress.
       "There's still considerable discretion in the hands of the Mental 
Health
       Court and the Mental Health Review Tribunal as to whether they would 
admit
       any evidence from the victims. The new Act is framed in such a way 
as to
       provide guaranteed rights to the person who's suffering from a mental
       illness and those rights come appropriately from the international
       conventions, but there are similar international conventions for 
victims
       and they are being completely ignored in the Act." Flack says the 
primary
       purpose of the Mental Health Tribunal is to save money and to safeguard
       the rights of the mentally disabled person. He believes the criminally
       insane can be catered for properly in jail. "The imprecise science of
       psychiatry is not an appropriate set of guidelines for the release into
       the community of dangerous killers," he says.

       NS
       GCAT : Political/General News | GCRIM : Crime/Courts | GHEA : Health |
       GHOME : Law Enforcement

       RE
       AUSNZ : Australia and New Zealand | AUSTR : Australia

       AN
       Document coumai0020010710dx6n005vl


From ripley at stats.ox.ac.uk  Sun Jul 23 09:12:28 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 23 Jul 2006 08:12:28 +0100 (BST)
Subject: [R] compile R with ACML support | RHEL 4
In-Reply-To: <44C22A13.3080201@verizon.net>
References: <44C22A13.3080201@verizon.net>
Message-ID: <Pine.LNX.4.64.0607230809260.31825@gannet.stats.ox.ac.uk>

I doubt if /opt/acml3.5.0/gnu/lib is in your library path (it might be in 
your ldcache paths).  So you need to set LD_LIBRARY_PATH or supply -L....

Look in config.log to find out what actually happened.

BTW: this was more of an R-devel question than R-help.

On Sat, 22 Jul 2006, Evan Cooch wrote:

> Greetings -
> 
> I'm trying to compile R under GNU/Linux (RHEL 4) on a multi-Opteron box, 
> with ACML support.
> 
> First, I downloaded and installed ACML 3.5 - GNU version, although I'm 
> not entirely sure what the differences are - from the AMD website. The 
> ACML libraries were installed to /opt/acml3.5.0/
> 
> Second, I ran ./configure --with-blas='-lacml'      
> 
> The configure went fine, except that at the end of the output, it 
> reports that readline is the only external library configured.
> 
> External libraries: readline
> 
> OK, so next, I try
> 
> ./configure --with-lapack
> 
> Same think - only readline is referenced.
> 
> So, clearly, I'm missing a particular step. I'm guessing that I need to 
> change some environment variable (or two), or tweak something at some 
> other stage, to get R to properly reference the ACML libraries. I'm 
> puzzled why --with-blas='-acml' doesn't do the trick?
> 
> Suggestions? Pointers to the obvious mistake?
> 
> Thanks...

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From patrick.giraudoux at univ-fcomte.fr  Sun Jul 23 09:17:10 2006
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sun, 23 Jul 2006 09:17:10 +0200
Subject: [R] diff, POSIXct, POSIXlt, POSIXt
Message-ID: <44C32276.30800@univ-fcomte.fr>

Dear Listers,

I have encountered a strange problem using diff() and POSIXt:

dts<-c("15/4/2003","15/7/2003","15/10/2003","15/04/2004","15/07/2004","15/10/2004","15/4/2005","15/07/2005","15/10/2005","15/4/2006")
dts <- strptime(dts, "%d/%m/%Y")
class(dts)

[1] "POSIXt"  "POSIXlt"

diff(dts)

Time differences of  7862400,  7948800, 15811200,  7862400,  7948800, 
15724800,  7862400,  7948800,        0 secs

In this case the result is not the one expected: expressed in seconds 
and not in days, and the difference between the two last dates is not 0.

Now, if one use a vector of 9 dates only (whatever the date removed), 
things come well:

diff(dts[-1])

Time differences of  92, 183,  91,  92, 182,  91,  92, 182 days

Also if one contrains dts to POSIXct

dts<-c("15/4/2003","15/7/2003","15/10/2003","15/04/2004","15/07/2004","15/10/2004","15/4/2005","15/07/2005","15/10/2005","15/4/2006")
dts <- as.POSIXct(strptime(dts, "%d/%m/%Y"))
diff(dts)

Time differences of  91,  92, 183,  91,  92, 182,  91,  92, 182 days

Any rational in that?

Patrick


From A.Robinson at ms.unimelb.edu.au  Sun Jul 23 09:27:00 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sun, 23 Jul 2006 17:27:00 +1000
Subject: [R] How to pass eval.max from lme() to nlminb?
Message-ID: <20060723072700.GE59927@ms.unimelb.edu.au>

Dear R community,

I'm fitting a complex mixed-effects model that requires numerous
iterations and function evaluations.  I note that nlminb accepts a
list of control parameters, including eval.max.  Is there a way to
change the default eval.max value for nlminb when it is being called
from lme?

Thanks for any thoughts,

Andrew
-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From jasonshi510 at hotmail.com  Sun Jul 23 10:07:17 2006
From: jasonshi510 at hotmail.com (Xin)
Date: Sun, 23 Jul 2006 09:07:17 +0100
Subject: [R] Why the contrain does not work for selecting a particular range
	of data?
Message-ID: <BAY117-DAV10F779D0FE0B2445925CEBF0640@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060723/462963bd/attachment.pl 

From berwin at maths.uwa.edu.au  Sun Jul 23 10:29:55 2006
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Sun, 23 Jul 2006 16:29:55 +0800
Subject: [R] How to pass eval.max from lme() to nlminb?
In-Reply-To: <20060723072700.GE59927@ms.unimelb.edu.au>
References: <20060723072700.GE59927@ms.unimelb.edu.au>
Message-ID: <17603.13187.978681.257019@bossiaea.maths.uwa.edu.au>

G'day Andrew,

>>>>> "AR" == Andrew Robinson <A.Robinson at ms.unimelb.edu.au> writes:

    AR> I'm fitting a complex mixed-effects model that requires
    AR> numerous iterations and function evaluations.  I note that
    AR> nlminb accepts a list of control parameters, including
    AR> eval.max.  Is there a way to change the default eval.max value
    AR> for nlminb when it is being called from lme?
Looking at the code of lme.formula, I can only find this snippet:

[...]
        optRes <- if (controlvals$opt == "nlminb") {
            nlminb(c(coef(lmeSt)), function(lmePars) -logLik(lmeSt, 
                lmePars), control = list(iter.max = controlvals$msMaxIter, 
                trace = controlvals$msVerbose))
        }
        else {
            optim(c(coef(lmeSt)), function(lmePars) -logLik(lmeSt, 
                lmePars), control = list(trace = controlvals$msVerbose, 
                maxit = controlvals$msMaxIter, reltol = if (numIter == 
                  0) controlvals$msTol else 100 * .Machine$double.eps), 
                method = controlvals$optimMethod)
        }
[...]

this seems to indicate that you can only change the values for
'iter.max' and 'trace' in the call to 'nlminb()' by setting values for
'msMaxIter' and 'msVerbose', using 'lmeControl', when calling 'lme()'.

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin


From jholtman at gmail.com  Sun Jul 23 12:41:40 2006
From: jholtman at gmail.com (jim holtman)
Date: Sun, 23 Jul 2006 06:41:40 -0400
Subject: [R] diff, POSIXct, POSIXlt, POSIXt
In-Reply-To: <44C32276.30800@univ-fcomte.fr>
References: <44C32276.30800@univ-fcomte.fr>
Message-ID: <644e1f320607230341v6d4cde82i1eff7881cc36a6ac@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060723/1e1d1338/attachment.pl 

From gregor.gorjanc at gmail.com  Sun Jul 23 13:05:02 2006
From: gregor.gorjanc at gmail.com (Gregor Gorjanc)
Date: Sun, 23 Jul 2006 13:05:02 +0200
Subject: [R] test
Message-ID: <44C357DE.7090404@bfro.uni-lj.si>

just a test
-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From roger at ysidro.econ.uiuc.edu  Sun Jul 23 13:25:41 2006
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Sun, 23 Jul 2006 06:25:41 -0500
Subject: [R] Warning Messages using rq -quantile regressions
In-Reply-To: <85384521-1C5B-40B9-AD78-55A2E6679A77@ysidro.econ.uiuc.edu>
References: <BAY123-F95B3BFB48C4470529589D9F640@phx.gbl>
	<85384521-1C5B-40B9-AD78-55A2E6679A77@ysidro.econ.uiuc.edu>
Message-ID: <D7522E57-748D-40EE-9DEA-D8176A613D7D@ysidro.econ.uiuc.edu>


On Jul 23, 2006, at 5:27 AM, roger koenker wrote:

> When computing the median from a sample with an even number of  
> distinct
> values there is inherently some ambiguity about its value:  any  
> value between
> the middle order statistics is "a" median.  Similarly, in  
> regression settings the
> optimization problem solved by the "br" version of the simplex  
> algorithm,
> modified to do general quantile regression identifies cases where  
> there may
> be non uniqueness of this type.  When there are "continuous"  
> covariates this
> is quite rare, when covariates are discrete then it is relatively  
> common, at
> least when tau is chosen from the rationals.  For univariate  
> quantiles R provides
> several methods of resolving this sort of ambiguity by  
> interpolation, "br" doesn't
> try to do this, instead returning the first vertex solution that it  
> comes to.  Should
> we worry about this?  My answer would be no.  Viewed from an  
> asymptotic
> perspective any choice of a unique value among the multiple  
> solutions is a
> 1/n perturbation  -- with 2500 observations this is unlikely to be  
> interesting.
> More to the point, inference about the coefficients of the model,  
> which provides
> O(1/sqrt(n)) intervals is perfectly capable of assessing the  
> meaningful uncertainty
> about these values.  Finally, if you would prefer an estimation  
> procedure that
> produced unique values more like the interpolation procedures in  
> the univariate
> setting, you could try the "fn" option for the algorithm.  Interior  
> point methods for
> solving linear programming problems have the "feature" that they  
> tend to converge
> to the centroid of solutions sets when such sets exist.  This  
> approach provides a
> means to assess the magnitude of the non-uniqueness in a particular  
> application.
>
> I hope that this helps,
>
> url:    www.econ.uiuc.edu/~roger                Roger Koenker
> email   rkoenker at uiuc.edu                       Department of  
> Economics
> vox:    217-333-4558                            University of Illinois
> fax:    217-244-6678                            Champaign, IL 61820
>
>
> On Jul 22, 2006, at 9:07 PM, Neil KM wrote:
>
>> I am a new to using quantile regressions in R. I have estimated a  
>> set of
>> coefficients using the method="br" algorithm with the rq command  
>> at various
>> quantiles along the entire distribution.
>>
>> My data set contains approximately 2,500 observations and I have 7  
>> predictor
>> variables. I receive the following warning message:
>>
>> Solution may be nonunique in: rq.fit.br(x, y, tau = tau, ...)
>>
>> There are 13 warnings of this type after I run a single  model. My  
>> results
>> are similiar to the results I received in other stat programs  
>> using quantile
>> reg procedures. I am unclear what these warning messages imply and  
>> if there
>> are problems with model fit/convergence that I may need to consider.
>> Any help would be appreciated. Thanks!
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch at stats.uwo.ca  Sun Jul 23 14:14:33 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 23 Jul 2006 08:14:33 -0400
Subject: [R] Why the contrain does not work for selecting a particular
 range	of data?
In-Reply-To: <BAY117-DAV10F779D0FE0B2445925CEBF0640@phx.gbl>
References: <BAY117-DAV10F779D0FE0B2445925CEBF0640@phx.gbl>
Message-ID: <44C36829.3050304@stats.uwo.ca>

On 7/23/2006 4:07 AM, Xin wrote:
> Dear:
> 
>     Continuing the issue of 'ifelse'! I selecting the data whose 'x2'=1 for maximizing likelihood. I used two way to do this but the results are different.

In the first case you used ifelse(), in the second you used if().  They 
behave differently:  ifelse() evaluates all tests in a vector, if() only 
evaluates one.  You probably want ifelse() in both cases.

Duncan Murdoch

>    
>     1.Way one I use the data for x2=1 and run the program. It works for me. Tthe program is described as below:
> function (parameters,y1,x11)
> {
> p<-parameters[1]
> alpha1<-parameters[2]
> beta1<-parameters[3]
> delta1<-parameters[4]
> lamda1<-parameters[5]
> 
> mu<-alpha1*((x11)^beta1)*exp(-delta1*(x11^lamda1))
> 
> ifelse(y1>0|x11>0,
> 
> L<-lgamma(y1+p)+p*(log(p)-log(mu+p))+y1*(log(mu)-log(mu+p))-lfactorial(y1)-lgamma(p)
> 
> ,Inf)
> 
> L
> 
> }
> 
>     This is working for me.
> 
>    2 Way two: I select the data whose x2=1 in the whole range of data. It works but it is not right comparing the value of MLE. the program is:
> function (parameters,y,x1,x2)
> 
> {
> 
> p<-parameters[1]
> 
> alpha1<-parameters[2]
> 
> beta1<-parameters[3]
> 
> delta1<-parameters[4]
> 
> alpha2<-parameters[5]
> 
> mu<-alpha1*((x1)^beta1)*exp(-delta1*(x1^alpha2))
> 
> if(x1>0 & x2==1)
> 
> {
> 
> L<-lgamma(y+p)+p*(log(p)-log(mu+p))+y*(log(mu)-log(mu+p))-lfactorial(y)-lgamma(p)
> 
> }
> 
> 
> 
> L
> 
> }
> 
>    The reason why I edit the program by the second way is I want to use one program for getting results of the different range of data.
> 
> Anyone can help? Please!
> 
> Thanks!
> 
> 
> 
> Xin Shi
> 
> 
> 
> My Estimation function for way two is :
> 
> function (parameters, y, x1,x2)
> 
> {
> 
> nx1 <- length(x1);
> 
> nx2 <- length(x2);
> 
> ny <- length(y);
> 
> x1 <- matrix(x1,nrow=nx1,ncol=1);
> 
> x2 <- matrix(x2,nrow=nx2,ncol=1);
> 
> y <- matrix(y,nrow=ny,ncol=1);
> 
> ##Likelihood
> 
> ##----------
> 
> Lvec <- matrix(0,nrow=nx1,ncol=1)
> 
> for (i in 1:ny)
> 
> {
> 
> Lvec[i] <- nb_L3(parameters, y[i],x1[i],x2[i])
> 
> LL <- -sum(Lvec)
> 
> }
> 
> LL
> 
> }
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From patrick.giraudoux at univ-fcomte.fr  Sun Jul 23 14:15:05 2006
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sun, 23 Jul 2006 14:15:05 +0200
Subject: [R] diff, POSIXct, POSIXlt, POSIXt
In-Reply-To: <644e1f320607230341v6d4cde82i1eff7881cc36a6ac@mail.gmail.com>
References: <44C32276.30800@univ-fcomte.fr>
	<644e1f320607230341v6d4cde82i1eff7881cc36a6ac@mail.gmail.com>
Message-ID: <44C36849.7070106@univ-fcomte.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060723/a358e416/attachment.pl 

From ggrothendieck at gmail.com  Sun Jul 23 15:02:35 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 23 Jul 2006 09:02:35 -0400
Subject: [R] diff, POSIXct, POSIXlt, POSIXt
In-Reply-To: <44C36849.7070106@univ-fcomte.fr>
References: <44C32276.30800@univ-fcomte.fr>
	<644e1f320607230341v6d4cde82i1eff7881cc36a6ac@mail.gmail.com>
	<44C36849.7070106@univ-fcomte.fr>
Message-ID: <971536df0607230602u33348346x8093c6b9a9984f97@mail.gmail.com>

Moving this to r-devel.

Looking at the diff.POSIXt code we see the problem is that it takes the
length of the input using length which is wrong since in the case
of POSIXlt the length is always 9 (or maybe length should be
defined differently for POSIXlt?).  Try this which gives the same
problem:

   dts[-1] - dts[-length(dts)]

We get a more sensible answer if length is calculated correctly:

  dts[-1] - dts[-length(dts[[1]])]


On 7/23/06, Patrick Giraudoux <patrick.giraudoux at univ-fcomte.fr> wrote:
> > Try converting to POSIXct:
> That's what I did finally (see the previous e-mail).
>
> dts<-c("15/4/2003","15/7/2003","15/10/2003","15/04/2004","15/07/2004","15/10/2004","15/4/2005","15/07/2005","15/10/2005","15/4/2006")
>
> dts <- as.POSIXct(strptime(dts, "%d/%m/%Y"))
> diff(dts)
>
> Time differences of  91,  92, 183,  91,  92, 182,  91,  92, 182 days
>
> > What is the problem you are trying to solve?
> Actually, I don't understand why using diff() and POSIXct provides the
> expected result and not using POSIXlt. Both POSIXct and POSIXlt are of
> class POSIXt. The doc of diff() stresses that <'diff' is a generic
> function with a default method and ones for classes '"ts"', '"POSIXt"'
> and '"Date"'>. It does not mention differences between POSIXct and POSIXlt.
>
> Moreover, using diff() with POSIXlt has provided (wrong) numbers... and
> not an error. This may be difficult to detect sometimes along programme
> lines. Must one keep in mind that diff() is reliably applicable only on
> POSIXct? In this case, should not it bve mentionned in the documentation?
>
> All the best,
>
> Patrick
>
>
>
>
>
>
>
> jim holtman a ?crit :
> > Try converting to POSIXct:
> >
> > > str(dts)
> > 'POSIXlt', format: chr [1:10] "2003-04-15" "2003-07-15" "2003-10-15"
> > "2004-04-15" "2004-07-15" "2004-10-15" "2005-04-15" ...
> > > dts
> >  [1] "2003-04-15" "2003-07-15" "2003-10-15" "2004-04-15" "2004-07-15"
> > "2004-10-15" "2005-04-15" "2005-07-15"
> >  [9] "2005-10-15" "2006-04-15"
> > > dts <- as.POSIXct(dts)
> > > dts
> >  [1] "2003-04-15 EDT" "2003-07-15 EDT" "2003-10-15 EDT" "2004-04-15
> > EDT" "2004-07-15 EDT" "2004-10-15 EDT"
> >  [7] "2005-04-15 EDT" "2005-07-15 EDT" "2005-10-15 EDT" "2006-04-15 EDT"
> > > diff(dts)
> > Time differences of  91,  92, 183,  91,  92, 182,  91,  92, 182 days
> > >
> >
> >
> >
> > On 7/23/06, *Patrick Giraudoux* <patrick.giraudoux at univ-fcomte.fr
> > <mailto:patrick.giraudoux at univ-fcomte.fr>> wrote:
> >
> >     Dear Listers,
> >
> >     I have encountered a strange problem using diff() and POSIXt:
> >
> >     dts<-c("15/4/2003","15/7/2003","15/10/2003","15/04/2004","15/07/2004","15/10/2004","15/4/2005","15/07/2005","15/10/2005","15/4/2006")
> >
> >     dts <- strptime(dts, "%d/%m/%Y")
> >     class(dts)
> >
> >     [1] "POSIXt"  "POSIXlt"
> >
> >     diff(dts)
> >
> >     Time differences of  7862400,  7948800, 15811200,  7862400,  7948800,
> >     15724800,  7862400,  7948800,        0 secs
> >
> >     In this case the result is not the one expected: expressed in seconds
> >     and not in days, and the difference between the two last dates is
> >     not 0.
> >
> >     Now, if one use a vector of 9 dates only (whatever the date removed),
> >     things come well:
> >
> >     diff(dts[-1])
> >
> >     Time differences of  92, 183,  91,  92, 182,  91,  92, 182 days
> >
> >     Also if one contrains dts to POSIXct
> >
> >     dts<-c("15/4/2003","15/7/2003","15/10/2003","15/04/2004","15/07/2004","15/10/2004","15/4/2005","15/07/2005","15/10/2005","15/4/2006")
> >
> >     dts <- as.POSIXct(strptime(dts, "%d/%m/%Y"))
> >     diff(dts)
> >
> >     Time differences of  91,  92, 183,  91,  92, 182,  91,  92, 182 days
> >
> >     Any rational in that?
> >
> >     Patrick
> >
> >     ______________________________________________
> >     R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch> mailing
> >     list
> >     https://stat.ethz.ch/mailman/listinfo/r-help
> >     PLEASE do read the posting guide
> >     http://www.R-project.org/posting-guide.html
> >     and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> >
> > --
> > Jim Holtman
> > Cincinnati, OH
> > +1 513 646 9390
> >
> > What is the problem you are trying to solve?
>
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From gregor.gorjanc at gmail.com  Sun Jul 23 17:14:25 2006
From: gregor.gorjanc at gmail.com (Gregor Gorjanc)
Date: Sun, 23 Jul 2006 17:14:25 +0200
Subject: [R] Is there anywhere recycle()?
Message-ID: <44C39251.4040105@bfro.uni-lj.si>

Hello!

I am writting a function, which should recycle one of its arguments if
length of the argument is approprate i.e. something like

foo <- function(x, a)
{
  n <- length(x)
  if(length(a) < n) { # recycle a
    oldA <- a
    a <- vector(length=n)
    a[1:n] <- oldA
  }
  ## ...
  return(a)
}

foo(c(1, 2), a=c(1, 2))
foo(c(1, 2), a=c(1))

I am now wondering if there is any general/generic functions for such task.

Thanks!

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From ggrothendieck at gmail.com  Sun Jul 23 17:23:01 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 23 Jul 2006 11:23:01 -0400
Subject: [R] Is there anywhere recycle()?
In-Reply-To: <44C39251.4040105@bfro.uni-lj.si>
References: <44C39251.4040105@bfro.uni-lj.si>
Message-ID: <971536df0607230823o1001e1deod5f90d61eedff8b0@mail.gmail.com>

Try:

foo2 <- function(x, a) cbind(x,a)[,2]


On 7/23/06, Gregor Gorjanc <gregor.gorjanc at gmail.com> wrote:
> Hello!
>
> I am writting a function, which should recycle one of its arguments if
> length of the argument is approprate i.e. something like
>
> foo <- function(x, a)
> {
>  n <- length(x)
>  if(length(a) < n) { # recycle a
>    oldA <- a
>    a <- vector(length=n)
>    a[1:n] <- oldA
>  }
>  ## ...
>  return(a)
> }
>
> foo(c(1, 2), a=c(1, 2))
> foo(c(1, 2), a=c(1))
>
> I am now wondering if there is any general/generic functions for such task.
>
> Thanks!
>
> --
> Lep pozdrav / With regards,
>    Gregor Gorjanc
>
> ----------------------------------------------------------------------
> University of Ljubljana     PhD student
> Biotechnical Faculty
> Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si
>
> SI-1230 Domzale             tel: +386 (0)1 72 17 861
> Slovenia, Europe            fax: +386 (0)1 72 17 888
>
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gregor.gorjanc at gmail.com  Sun Jul 23 18:17:01 2006
From: gregor.gorjanc at gmail.com (Gregor Gorjanc)
Date: Sun, 23 Jul 2006 18:17:01 +0200
Subject: [R] Is there anywhere recycle()?
In-Reply-To: <971536df0607230823o1001e1deod5f90d61eedff8b0@mail.gmail.com>
References: <44C39251.4040105@bfro.uni-lj.si>
	<971536df0607230823o1001e1deod5f90d61eedff8b0@mail.gmail.com>
Message-ID: <44C3A0FD.1040506@bfro.uni-lj.si>

Hi,

Gabor Grothendieck wrote:
> Try:
> 
> foo2 <- function(x, a) cbind(x,a)[,2]
> 

thank you for this. It does work to some extent, but not much better
than mine foo.

foo2(c(1, 2, 3), a=1)
[1] 1 1 1

18:14:08
R> foo2(c(1, 2, 3), a=c(1,2,3,4))
[1] 1 2 3 4
Warning message:
number of rows of result
        is not a multiple of vector length (arg 1) in: cbind(1, x, a)

18:14:13
R> foo2(c(1, 2, 3), a=c(1,2,3))
[1] 1 2 3

18:14:18
R> foo2(c(1, 2, 3), a=c(1,2))
[1] 1 2 1
Warning message:
number of rows of result
        is not a multiple of vector length (arg 2) in: cbind(1, x, a)



> On 7/23/06, Gregor Gorjanc <gregor.gorjanc at gmail.com> wrote:
>> Hello!
>>
>> I am writting a function, which should recycle one of its arguments if
>> length of the argument is approprate i.e. something like
>>
>> foo <- function(x, a)
>> {
>>  n <- length(x)
>>  if(length(a) < n) { # recycle a
>>    oldA <- a
>>    a <- vector(length=n)
>>    a[1:n] <- oldA
>>  }
>>  ## ...
>>  return(a)
>> }
>>
>> foo(c(1, 2), a=c(1, 2))
>> foo(c(1, 2), a=c(1))
>>
>> I am now wondering if there is any general/generic functions for such
>> task.
>>
>> Thanks!
>>
>> -- 
>> Lep pozdrav / With regards,
>>    Gregor Gorjanc
>>
>> ----------------------------------------------------------------------
>> University of Ljubljana     PhD student
>> Biotechnical Faculty
>> Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
>> Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si
>>
>> SI-1230 Domzale             tel: +386 (0)1 72 17 861
>> Slovenia, Europe            fax: +386 (0)1 72 17 888
>>
>> ----------------------------------------------------------------------
>> "One must learn by doing the thing; for though you think you know it,
>>  you have no certainty until you try." Sophocles ~ 450 B.C.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.


From ggrothendieck at gmail.com  Sun Jul 23 18:27:16 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 23 Jul 2006 12:27:16 -0400
Subject: [R] Is there anywhere recycle()?
In-Reply-To: <44C3A0FD.1040506@bfro.uni-lj.si>
References: <44C39251.4040105@bfro.uni-lj.si>
	<971536df0607230823o1001e1deod5f90d61eedff8b0@mail.gmail.com>
	<44C3A0FD.1040506@bfro.uni-lj.si>
Message-ID: <971536df0607230927h21d8abc1vc2a4904baaaa5e2b@mail.gmail.com>

Here is another possibility:

rep(a, length = length(x))

On 7/23/06, Gregor Gorjanc <gregor.gorjanc at gmail.com> wrote:
> Hi,
>
> Gabor Grothendieck wrote:
> > Try:
> >
> > foo2 <- function(x, a) cbind(x,a)[,2]
> >
>
> thank you for this. It does work to some extent, but not much better
> than mine foo.
>
> foo2(c(1, 2, 3), a=1)
> [1] 1 1 1
>
> 18:14:08
> R> foo2(c(1, 2, 3), a=c(1,2,3,4))
> [1] 1 2 3 4
> Warning message:
> number of rows of result
>        is not a multiple of vector length (arg 1) in: cbind(1, x, a)
>
> 18:14:13
> R> foo2(c(1, 2, 3), a=c(1,2,3))
> [1] 1 2 3
>
> 18:14:18
> R> foo2(c(1, 2, 3), a=c(1,2))
> [1] 1 2 1
> Warning message:
> number of rows of result
>        is not a multiple of vector length (arg 2) in: cbind(1, x, a)
>
>
>
> > On 7/23/06, Gregor Gorjanc <gregor.gorjanc at gmail.com> wrote:
> >> Hello!
> >>
> >> I am writting a function, which should recycle one of its arguments if
> >> length of the argument is approprate i.e. something like
> >>
> >> foo <- function(x, a)
> >> {
> >>  n <- length(x)
> >>  if(length(a) < n) { # recycle a
> >>    oldA <- a
> >>    a <- vector(length=n)
> >>    a[1:n] <- oldA
> >>  }
> >>  ## ...
> >>  return(a)
> >> }
> >>
> >> foo(c(1, 2), a=c(1, 2))
> >> foo(c(1, 2), a=c(1))
> >>
> >> I am now wondering if there is any general/generic functions for such
> >> task.
> >>
> >> Thanks!
> >>
> >> --
> >> Lep pozdrav / With regards,
> >>    Gregor Gorjanc
> >>
> >> ----------------------------------------------------------------------
> >> University of Ljubljana     PhD student
> >> Biotechnical Faculty
> >> Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
> >> Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si
> >>
> >> SI-1230 Domzale             tel: +386 (0)1 72 17 861
> >> Slovenia, Europe            fax: +386 (0)1 72 17 888
> >>
> >> ----------------------------------------------------------------------
> >> "One must learn by doing the thing; for though you think you know it,
> >>  you have no certainty until you try." Sophocles ~ 450 B.C.
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
>
>
> --
> Lep pozdrav / With regards,
>    Gregor Gorjanc
>
> ----------------------------------------------------------------------
> University of Ljubljana     PhD student
> Biotechnical Faculty
> Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si
>
> SI-1230 Domzale             tel: +386 (0)1 72 17 861
> Slovenia, Europe            fax: +386 (0)1 72 17 888
>
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
> ----------------------------------------------------------------------
>


From johnzhou at fas.harvard.edu  Sun Jul 23 18:52:56 2006
From: johnzhou at fas.harvard.edu (johnzhou at fas.harvard.edu)
Date: Sun, 23 Jul 2006 12:52:56 -0400
Subject: [R] Iterated Data Input/Output with Random Forests
Message-ID: <1153673576.44c3a968726a5@webmail.fas.harvard.edu>

Hi,

I am currently writing code to input a few thousand files, run them through the
Random Forests package, and then output corresponding results.

When I use the code below:

zz<-textConnection("ex.lm.out", "w")
sink(zz)
tempData<-read.delim(paste("allSnps",1,"Phenotype.phn",sep=""),header=TRUE,sep=",",quote="\"",dec=".")
tempData[[1]]<-factor(tempData[[1]])
tempData.rf<-randomForest(tempData[[1]]~.,data=tempData,importance=TRUE,proximity=TRUE,outscale=TRUE,replace=TRUE)
tempData.rf
zz<-file(paste("ex",1,".data",sep=""), "w")
cat(ex.lm.out, sep="\n", file=zz)
sink()
close(zz)

I am able to successfully input and output for one file. However, if I try to
use a for loop or a while statement e.g.

for(i in 1:2)
{
zz<-textConnection("ex.lm.out", "w")
sink(zz)
tempData<-read.delim(paste("allSnps",i,"Phenotype.phn",sep=""),header=TRUE,sep=",",quote="\"",dec=".")
tempData[[1]]<-factor(tempData[[1]])
tempData.rf<-randomForest(tempData[[1]]~.,data=tempData,importance=TRUE,proximity=TRUE,outscale=TRUE,replace=TRUE)
tempData.rf
zz<-file(paste("ex",i,".data",sep=""), "w")
cat(ex.lm.out, sep="\n", file=zz)
sink()
close(zz)
}

I get no error statements but the output is blank. Without the for statement,
setting i<-1 works fine.

One other related question is that right now I am trying to gett the loop to
work by using the paste() function with a variable (i). However, the paste
function returns a string.

If I wanted to make a loop of

tempData$pheno1
tempData$pheno2
tempData$pheno3
...

the paste() function will not work. Is there some other method to achieve the
desired effect? Thank you in advance! I have only been working with R for a few
days so please bear with my lack of knowledge!

John Zhou


From jholtman at gmail.com  Sun Jul 23 19:17:57 2006
From: jholtman at gmail.com (jim holtman)
Date: Sun, 23 Jul 2006 13:17:57 -0400
Subject: [R] constructing a dataframe from a database of newspaper
	articles
In-Reply-To: <5.1.0.14.0.20060723103511.00bf7d40@pop3.brisnet.org.au>
References: <mailman.11.1153303205.32321.r-help@stat.math.ethz.ch>
	<5.1.0.14.0.20060723103511.00bf7d40@pop3.brisnet.org.au>
Message-ID: <644e1f320607231017q79ce0bebm99ab2c449394236f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060723/482cd32a/attachment.pl 

From jholtman at gmail.com  Sun Jul 23 19:33:08 2006
From: jholtman at gmail.com (jim holtman)
Date: Sun, 23 Jul 2006 13:33:08 -0400
Subject: [R] Iterated Data Input/Output with Random Forests
In-Reply-To: <1153673576.44c3a968726a5@webmail.fas.harvard.edu>
References: <1153673576.44c3a968726a5@webmail.fas.harvard.edu>
Message-ID: <644e1f320607231033qee29390xd52b26ede2692a90@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060723/280d167a/attachment.pl 

From johnzhou at fas.harvard.edu  Sun Jul 23 19:52:19 2006
From: johnzhou at fas.harvard.edu (johnzhou at fas.harvard.edu)
Date: Sun, 23 Jul 2006 13:52:19 -0400
Subject: [R] Iterated Data Input/Output with Random Forests
In-Reply-To: <644e1f320607231033qee29390xd52b26ede2692a90@mail.gmail.com>
References: <1153673576.44c3a968726a5@webmail.fas.harvard.edu>
	<644e1f320607231033qee29390xd52b26ede2692a90@mail.gmail.com>
Message-ID: <1153677139.44c3b7531c717@webmail.fas.harvard.edu>

While tempData[paste('pheno',i,sep='')] does give the appropriate column, when I
try to use that expression in the factor function:

factor(tempData[paste('pheno',i,sep='')]) I get

Error in sort(unique.default(x), na.last=TRUE : 'x' must be atomic.


Quoting jim holtman <jholtman at gmail.com>:

> For your last question of the 'paste', try
>
> tempdata[paste('pheno', i, sep='')]
>
>
>
> On 7/23/06, johnzhou at fas.harvard.edu <johnzhou at fas.harvard.edu> wrote:
> >
> > Hi,
> >
> > I am currently writing code to input a few thousand files, run them
> > through the
> > Random Forests package, and then output corresponding results.
> >
> > When I use the code below:
> >
> > zz<-textConnection("ex.lm.out", "w")
> > sink(zz)
> > tempData<-read.delim(paste("allSnps",1,"Phenotype.phn
> > ",sep=""),header=TRUE,sep=",",quote="\"",dec=".")
> > tempData[[1]]<-factor(tempData[[1]])
> > tempData.rf
> >
>
<-randomForest(tempData[[1]]~.,data=tempData,importance=TRUE,proximity=TRUE,outscale=TRUE,replace=TRUE)
> > tempData.rf
> > zz<-file(paste("ex",1,".data",sep=""), "w")
> > cat(ex.lm.out, sep="\n", file=zz)
> > sink()
> > close(zz)
> >
> > I am able to successfully input and output for one file. However, if I try
> > to
> > use a for loop or a while statement e.g.
> >
> > for(i in 1:2)
> > {
> > zz<-textConnection("ex.lm.out", "w")
> > sink(zz)
> > tempData<-read.delim(paste("allSnps",i,"Phenotype.phn
> > ",sep=""),header=TRUE,sep=",",quote="\"",dec=".")
> > tempData[[1]]<-factor(tempData[[1]])
> > tempData.rf
> >
>
<-randomForest(tempData[[1]]~.,data=tempData,importance=TRUE,proximity=TRUE,outscale=TRUE,replace=TRUE)
> > tempData.rf
> > zz<-file(paste("ex",i,".data",sep=""), "w")
> > cat(ex.lm.out, sep="\n", file=zz)
> > sink()
> > close(zz)
> > }
> >
> > I get no error statements but the output is blank. Without the for
> > statement,
> > setting i<-1 works fine.
> >
> > One other related question is that right now I am trying to gett the loop
> > to
> > work by using the paste() function with a variable (i). However, the paste
> > function returns a string.
> >
> > If I wanted to make a loop of
> >
> > tempData$pheno1
> > tempData$pheno2
> > tempData$pheno3
> > ...
> >
> > the paste() function will not work. Is there some other method to achieve
> > the
> > desired effect? Thank you in advance! I have only been working with R for
> > a few
> > days so please bear with my lack of knowledge!
> >
> > John Zhou
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?
>


From M.Nash at iop.kcl.ac.uk  Sun Jul 23 20:50:28 2006
From: M.Nash at iop.kcl.ac.uk (M.Nash at iop.kcl.ac.uk)
Date: Sun, 23 Jul 2006 19:50:28 +0100
Subject: [R] Matthew Nash has left the SGDP
Message-ID: <OFF6D63E42.37538178-ON802571B4.00677F60-802571B4.00677F60@iop.kcl.ac.uk>





I will be out of the office starting Wed 02/01/2006 and will not return
until Sat 02/07/2060.

I have left the SGDP. I am contactable at matthewwilkesnash at hotmail.com.


From jholtman at gmail.com  Sun Jul 23 20:51:31 2006
From: jholtman at gmail.com (jim holtman)
Date: Sun, 23 Jul 2006 14:51:31 -0400
Subject: [R] constructing a dataframe from a database of newspaper
	articles
In-Reply-To: <5.1.0.14.0.20060723103511.00bf7d40@pop3.brisnet.org.au>
References: <mailman.11.1153303205.32321.r-help@stat.math.ethz.ch>
	<5.1.0.14.0.20060723103511.00bf7d40@pop3.brisnet.org.au>
Message-ID: <644e1f320607231151i6320b097ybc79e3efd8d4d9e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060723/9e60d667/attachment.pl 

From spencer.graves at pdf.com  Sun Jul 23 20:57:48 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 24 Jul 2006 02:57:48 +0800
Subject: [R] diff, POSIXct, POSIXlt, POSIXt
In-Reply-To: <971536df0607230602u33348346x8093c6b9a9984f97@mail.gmail.com>
References: <44C32276.30800@univ-fcomte.fr>	<644e1f320607230341v6d4cde82i1eff7881cc36a6ac@mail.gmail.com>	<44C36849.7070106@univ-fcomte.fr>
	<971536df0607230602u33348346x8093c6b9a9984f97@mail.gmail.com>
Message-ID: <44C3C6AC.6020905@pdf.com>

Hi, Gabor:

	  For my 0.02 euros, I vote to make length(POSIXlt) = length of the 
series, NOT the length of the list = 9 always.  I've stubbed my toe on 
that one many times.  I always fix it by converting first to POSIXct.

	  The key question is what would users naively expect to get from 
length(a_time_series)?  I think most people not familiar with the 
POSIXlt format would expect the number of observations.  After 
struggling for a while with code that did not perform as expected, I 
finally traced one such problem to the fact that length(a_time_series)= 
9 if class(a_time_series) = "POSIXlt", independent of the number of 
observations.

	  How much code would break if this was changed?  Each use of 
length(POSIXlt_object) would have to be replaced by something like 
length(as.list(POSIXlt_object)).  However, since length(POSIXlt_object) 
is always 9, I doubt if length(POSIXlt_object) occurs very often.

	  Currently to get the number of observations in a POSIXlt_object, you 
might find constructs like length(POSIXlt_object[[1]]).  Or you will 
find people converting the POSIXlt to POSIXct and then computing the 
length.  In either case, changing length(POSIXlt_object) to the number 
of observations would not break any of this code.

	  Thanks for raising this question.
	  Spencer Graves

Gabor Grothendieck wrote:
> Moving this to r-devel.
> 
> Looking at the diff.POSIXt code we see the problem is that it takes the
> length of the input using length which is wrong since in the case
> of POSIXlt the length is always 9 (or maybe length should be
> defined differently for POSIXlt?).  Try this which gives the same
> problem:
> 
>    dts[-1] - dts[-length(dts)]
> 
> We get a more sensible answer if length is calculated correctly:
> 
>   dts[-1] - dts[-length(dts[[1]])]
> 
> 
> On 7/23/06, Patrick Giraudoux <patrick.giraudoux at univ-fcomte.fr> wrote:
>>> Try converting to POSIXct:
>> That's what I did finally (see the previous e-mail).
>>
>> dts<-c("15/4/2003","15/7/2003","15/10/2003","15/04/2004","15/07/2004","15/10/2004","15/4/2005","15/07/2005","15/10/2005","15/4/2006")
>>
>> dts <- as.POSIXct(strptime(dts, "%d/%m/%Y"))
>> diff(dts)
>>
>> Time differences of  91,  92, 183,  91,  92, 182,  91,  92, 182 days
>>
>>> What is the problem you are trying to solve?
>> Actually, I don't understand why using diff() and POSIXct provides the
>> expected result and not using POSIXlt. Both POSIXct and POSIXlt are of
>> class POSIXt. The doc of diff() stresses that <'diff' is a generic
>> function with a default method and ones for classes '"ts"', '"POSIXt"'
>> and '"Date"'>. It does not mention differences between POSIXct and POSIXlt.
>>
>> Moreover, using diff() with POSIXlt has provided (wrong) numbers... and
>> not an error. This may be difficult to detect sometimes along programme
>> lines. Must one keep in mind that diff() is reliably applicable only on
>> POSIXct? In this case, should not it bve mentionned in the documentation?
>>
>> All the best,
>>
>> Patrick
>>
>>
>>
>>
>>
>>
>>
>> jim holtman a ?crit :
>>> Try converting to POSIXct:
>>>
>>>> str(dts)
>>> 'POSIXlt', format: chr [1:10] "2003-04-15" "2003-07-15" "2003-10-15"
>>> "2004-04-15" "2004-07-15" "2004-10-15" "2005-04-15" ...
>>>> dts
>>>  [1] "2003-04-15" "2003-07-15" "2003-10-15" "2004-04-15" "2004-07-15"
>>> "2004-10-15" "2005-04-15" "2005-07-15"
>>>  [9] "2005-10-15" "2006-04-15"
>>>> dts <- as.POSIXct(dts)
>>>> dts
>>>  [1] "2003-04-15 EDT" "2003-07-15 EDT" "2003-10-15 EDT" "2004-04-15
>>> EDT" "2004-07-15 EDT" "2004-10-15 EDT"
>>>  [7] "2005-04-15 EDT" "2005-07-15 EDT" "2005-10-15 EDT" "2006-04-15 EDT"
>>>> diff(dts)
>>> Time differences of  91,  92, 183,  91,  92, 182,  91,  92, 182 days
>>>
>>>
>>> On 7/23/06, *Patrick Giraudoux* <patrick.giraudoux at univ-fcomte.fr
>>> <mailto:patrick.giraudoux at univ-fcomte.fr>> wrote:
>>>
>>>     Dear Listers,
>>>
>>>     I have encountered a strange problem using diff() and POSIXt:
>>>
>>>     dts<-c("15/4/2003","15/7/2003","15/10/2003","15/04/2004","15/07/2004","15/10/2004","15/4/2005","15/07/2005","15/10/2005","15/4/2006")
>>>
>>>     dts <- strptime(dts, "%d/%m/%Y")
>>>     class(dts)
>>>
>>>     [1] "POSIXt"  "POSIXlt"
>>>
>>>     diff(dts)
>>>
>>>     Time differences of  7862400,  7948800, 15811200,  7862400,  7948800,
>>>     15724800,  7862400,  7948800,        0 secs
>>>
>>>     In this case the result is not the one expected: expressed in seconds
>>>     and not in days, and the difference between the two last dates is
>>>     not 0.
>>>
>>>     Now, if one use a vector of 9 dates only (whatever the date removed),
>>>     things come well:
>>>
>>>     diff(dts[-1])
>>>
>>>     Time differences of  92, 183,  91,  92, 182,  91,  92, 182 days
>>>
>>>     Also if one contrains dts to POSIXct
>>>
>>>     dts<-c("15/4/2003","15/7/2003","15/10/2003","15/04/2004","15/07/2004","15/10/2004","15/4/2005","15/07/2005","15/10/2005","15/4/2006")
>>>
>>>     dts <- as.POSIXct(strptime(dts, "%d/%m/%Y"))
>>>     diff(dts)
>>>
>>>     Time differences of  91,  92, 183,  91,  92, 182,  91,  92, 182 days
>>>
>>>     Any rational in that?
>>>
>>>     Patrick
>>>
>>>     ______________________________________________
>>>     R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch> mailing
>>>     list
>>>     https://stat.ethz.ch/mailman/listinfo/r-help
>>>     PLEASE do read the posting guide
>>>     http://www.R-project.org/posting-guide.html
>>>     and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>
>>>
>>> --
>>> Jim Holtman
>>> Cincinnati, OH
>>> +1 513 646 9390
>>>
>>> What is the problem you are trying to solve?
>>        [[alternative HTML version deleted]]
>>
>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From johannes at huesing.name  Sun Jul 23 22:35:50 2006
From: johannes at huesing.name (Johannes =?iso-8859-1?Q?H=FCsing?=)
Date: Sun, 23 Jul 2006 22:35:50 +0200 (CEST)
Subject: [R] dotchart with log scale?
In-Reply-To: <17559.129.206.90.2.1153387971.squirrel@mail.panix.com>
References: <17559.129.206.90.2.1153387971.squirrel@mail.panix.com>
Message-ID: <63333.85.216.81.28.1153686950.squirrel@mail.panix.com>

> Dear all,
> I would like to draw a dot chart on a log scale.
> What is the syntax for this? A barchart may use
> log="x", but trying this with dotchart() leads
> to an error message.

ok, I extended dotchart() with a log option.
The main reason I am sticking to dotchart() is
adjusting of axes with respect to long text
labels.

This is the definition I am using now:


mydotchart <-
  function (x, labels = NULL, groups = NULL, gdata = NULL, cex = par("cex"),
            pch = 21, gpch = 21, bg = par("bg"), color = par("fg"), gcolor
= par("fg"),
            lcolor = "gray", xlim = range(x[is.finite(x)]), main = NULL,
###                                   new option
            xlab = NULL, ylab = NULL, log=FALSE, ...) {
    opar <- par("mai", "cex", "yaxs")
    on.exit(par(opar))
    par(cex = cex, yaxs = "i")
    n <- length(x)
    if (is.matrix(x)) {
      if (is.null(labels))
        labels <- rownames(x)
      if (is.null(labels))
        labels <- as.character(1:nrow(x))
      labels <- rep(labels, length.out = n)
      if (is.null(groups))
        groups <- col(x, as.factor = TRUE)
      glabels <- levels(groups)
    }
    else {
      if (is.null(labels))
        labels <- names(x)
      glabels <- if (!is.null(groups))
        levels(groups)
    }
    plot.new()
    linch <- if (!is.null(labels))
      max(strwidth(labels, "inch"), na.rm = TRUE)
    else 0
    if (is.null(glabels)) {
      ginch <- 0
      goffset <- 0
    }
    else {
      ginch <- max(strwidth(glabels, "inch"), na.rm = TRUE)
      goffset <- 0.4
    }
    if (!(is.null(labels) && is.null(glabels))) {
      nmai <- par("mai")
      nmai[2] <- nmai[4] + max(linch + goffset, ginch) + 0.1
      par(mai = nmai)
    }
    if (is.null(groups)) {
      o <- 1:n
      y <- o
      ylim <- c(0, n + 1)
    }
    else {
      o <- sort.list(as.numeric(groups), decreasing = TRUE)
      x <- x[o]
      groups <- groups[o]
      color <- rep(color, length.out = length(groups))[o]
      lcolor <- rep(lcolor, length.out = length(groups))[o]
      offset <- cumsum(c(0, diff(as.numeric(groups)) != 0))
      y <- 1:n + 2 * offset
      ylim <- range(0, y + 2)
    }
###                                       instead of log=""
    plot.window(xlim = xlim, ylim = ylim, log = ifelse(log, "x", ""))
    lheight <- par("csi")
    if (!is.null(labels)) {
      linch <- max(strwidth(labels, "inch"), na.rm = TRUE)
      loffset <- (linch + 0.1)/lheight
      labs <- labels[o]
      mtext(labs, side = 2, line = loffset, at = y, adj = 0,
            col = color, las = 2, cex = cex, ...)
    }
    abline(h = y, lty = "dotted", col = lcolor)
    points(x, y, pch = pch, col = color, bg = bg)
    if (!is.null(groups)) {
      gpos <- rev(cumsum(rev(tapply(groups, groups, length)) +
                         2) - 1)
      ginch <- max(strwidth(glabels, "inch"), na.rm = TRUE)
      goffset <- (max(linch + 0.2, ginch, na.rm = TRUE) + 0.1)/lheight
      mtext(glabels, side = 2, line = goffset, at = gpos, adj = 0,
            col = gcolor, las = 2, cex = cex, ...)
      if (!is.null(gdata)) {
        abline(h = gpos, lty = "dotted")
        points(gdata, gpos, pch = gpch, col = gcolor, bg = bg,
               ...)
      }
    }
    axis(1)
    box()
    title(main = main, xlab = xlab, ylab = ylab, ...)
    invisible()
}

Many thanks for your attention.


From dataczarina at yahoo.com  Sun Jul 23 23:26:53 2006
From: dataczarina at yahoo.com (Data Meister)
Date: Sun, 23 Jul 2006 14:26:53 -0700 (PDT)
Subject: [R] Using DDF With Data Import into R
Message-ID: <20060723212653.18773.qmail@web35501.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060723/d3b59990/attachment.pl 

From David.Duffy at qimr.edu.au  Mon Jul 24 01:19:16 2006
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Mon, 24 Jul 2006 09:19:16 +1000 (EST)
Subject: [R] Re constructing a dataframe from a database of newspaper
	articles
In-Reply-To: <mailman.11.1153648803.2546.r-help@stat.math.ethz.ch>
References: <mailman.11.1153648803.2546.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0607240901120.2095@orpheus.qimr.edu.au>

> From: Bob Green <bgreen at dyson.brisnet.org.au>
>
> I am hoping for some assistance with formatting a large text file which
> consists of a series of individual records. Each record includes specific
> labels/field names (a sample of 1 record (one of the longest ones) is
> below  - at end of post. What I want to do is reformat the data, so that
> each individual record becomes a row (some cells will have a lot of text).
> For example, the column variables I want are (a) HD  in one column
> (b)    BY in one column (c) WC data in one column,  (d) PD data in one
> column, (e) SC data in one column (f) PG data in one column  & g) LP and TD
> text in one column  - this column can contain quite a lot of text, e.g 1900
> words. The other fields are unwanted
>
> If there were 150 individual records, when formatted this would be a 7
> column by 150 row dataset.

Most transparently,

txt <- readLines("c:\\cm-mht1.txt")
no_of_records <- length(grep("^HD",txt)
res <- matrix(nr=no_of_records, nc=8)
idx <- 0
for (i in 1:length(txt)) {
  if (regexpr("^HD", txt[i])!=-1) idx <- idx+1

  if (regexpr("^HD", txt[i])!=-1) res[idx, 1] <- txt[i]
  if (regexpr("^BY", txt[i])!=-1) res[idx, 2] <- txt[i]
  ...
  if (regexpr("^TD", txt[i])!=-1) res[idx, 8] <- txt[i]
}
res[,7] <- paste(res[,7], res[,8], sep="; ")
res <- res[,-8]


| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v


From David.Duffy at qimr.edu.au  Mon Jul 24 02:40:12 2006
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Mon, 24 Jul 2006 10:40:12 +1000 (EST)
Subject: [R] Re constructing a dataframe from a database of newspaper
 articles
In-Reply-To: <Pine.LNX.4.58.0607240901120.2095@orpheus.qimr.edu.au>
References: <mailman.11.1153648803.2546.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.58.0607240901120.2095@orpheus.qimr.edu.au>
Message-ID: <Pine.LNX.4.58.0607241025010.7265@orpheus.qimr.edu.au>

On Mon, 24 Jul 2006, David Duffy wrote:

> > From: Bob Green <bgreen at dyson.brisnet.org.au>
> >
> > I am hoping for some assistance with formatting a large text file which
> > consists of a series of individual records. Each record includes specific
> > labels/field names (a sample of 1 record (one of the longest ones) is
> > below  - at end of post. What I want to do is reformat the data, so that
> > each individual record becomes a row (some cells will have a lot of text).
> > For example, the column variables I want are (a) HD  in one column
> > (b)    BY in one column (c) WC data in one column,  (d) PD data in one
> > column, (e) SC data in one column (f) PG data in one column  & g) LP and TD
> > text in one column  - this column can contain quite a lot of text, e.g 1900
> > words. The other fields are unwanted
> >
> > If there were 150 individual records, when formatted this would be a 7
> > column by 150 row dataset.

Oops, I forgot to add the bit about multiple lines per field...

txt <- readLines("c:\\cm-mht1.txt")
txt <- gsub("[ ]+"," ",txt)
txt <- gsub("^[ ]+","",txt)
no_of_records <- length(grep("^HD",txt)
res <- matrix("", nr=no_of_records, nc=7)
idx <- 0
typ <- 0
for (i in 1:length(txt)) {
  if (regexpr("^HD", txt[i])!=-1) {
    idx <- idx+1
    typ <- 1
  }else if (regexpr("^BY", txt[i])!=-1) {
    typ <- 2
  }
  ...
  } else if (regexpr("(^LP)|(^TD)", txt[i])!=-1) {
    typ <- 7
  } else if (regexpr("^[A-Z][A-Z]", txt[i])!=-1) {
    typ <- 0
  }
  if (typ>0) {
    res[idx,typ] <- paste(res[idx,typ], txt[i], sep=" ")
  }
}


| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v


From mnair at iusb.edu  Mon Jul 24 03:36:08 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Sun, 23 Jul 2006 21:36:08 -0400
Subject: [R] Saving R objects
Message-ID: <A32055BDEA88C34BB3DBBCD229380778050FB4@iu-mssg-mbx109.ads.iu.edu>

I am trying to find the best way to save the follwoing object I am creating
 
library(multcomp)
data(recovery)
Dcirec<-simint(minutes~blanket, data=recovery, conf.level=0.9, alternative="less")
 
I am probably not doing it the most efficient way I think. 
Here is what I am doing 
 
a<-print(Dcirec)
write(a,file="mult_test.dat", append=T)
or
save(Dcirec, file="mult.out")
 
Which is the best way to save it, so that I can access its contents outside the R environment?


From ggrothendieck at gmail.com  Mon Jul 24 05:11:47 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 23 Jul 2006 23:11:47 -0400
Subject: [R] Saving R objects
In-Reply-To: <A32055BDEA88C34BB3DBBCD229380778050FB4@iu-mssg-mbx109.ads.iu.edu>
References: <A32055BDEA88C34BB3DBBCD229380778050FB4@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <971536df0607232011p28295a19kbc97ec9eee9d7e5@mail.gmail.com>

It depends on what information you want to save and how the
program on the other end needs it.

For the save version I would at least use ascii = TRUE to get it
in a more readable fashion.

Look at

file.show("mult_test.dat")
file.show("mult.out")  # but use ascii=TRUE on your save statement.

to see what you are getting.

Other possibilities are to use R2HTML or XML packages to output
to HTML or XML.   You might want to handle the various components
of Dcirec separately.  To see what's inside:

   unclass(Dcirec)
   str(Dcirec)
   dput(Dcirec)

and use cat statements to output the components in the format of
your choice possibly in conjunction with sprintf.


On 7/23/06, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> I am trying to find the best way to save the follwoing object I am creating
>
> library(multcomp)
> data(recovery)
> Dcirec<-simint(minutes~blanket, data=recovery, conf.level=0.9, alternative="less")
>
> I am probably not doing it the most efficient way I think.
> Here is what I am doing
>
> a<-print(Dcirec)
> write(a,file="mult_test.dat", append=T)
> or
> save(Dcirec, file="mult.out")
>
> Which is the best way to save it, so that I can access its contents outside the R environment?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jennytimp at hotmail.com  Mon Jul 24 05:21:14 2006
From: jennytimp at hotmail.com (jenny tan)
Date: Mon, 24 Jul 2006 11:21:14 +0800
Subject: [R] How to obtain 95th percentile of a normal distribution of a
	continuous variable
Message-ID: <BAY118-F6AC8CBEE86D0F1F11E0D2B9650@phx.gbl>

Hi,

How do I get R to output the 95% cutoff from a distribution of a continous 
variable?
summary() only displays a few statistics....

Thanks!


From liuwensui at gmail.com  Mon Jul 24 06:07:58 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 24 Jul 2006 00:07:58 -0400
Subject: [R] How to obtain 95th percentile of a normal distribution of a
	continuous variable
In-Reply-To: <BAY118-F6AC8CBEE86D0F1F11E0D2B9650@phx.gbl>
References: <BAY118-F6AC8CBEE86D0F1F11E0D2B9650@phx.gbl>
Message-ID: <1115a2b00607232107t492d507eq8d6eb06b49f7a27a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060724/ed821a6c/attachment.pl 

From blomsp at ozemail.com.au  Mon Jul 24 06:09:20 2006
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Mon, 24 Jul 2006 14:09:20 +1000
Subject: [R] How to obtain 95th percentile of a normal distribution of
 a	continuous variable
In-Reply-To: <BAY118-F6AC8CBEE86D0F1F11E0D2B9650@phx.gbl>
References: <BAY118-F6AC8CBEE86D0F1F11E0D2B9650@phx.gbl>
Message-ID: <44C447F0.9060601@ozemail.com.au>

?quantile

jenny tan wrote:
> Hi,
>
> How do I get R to output the 95% cutoff from a distribution of a continous 
> variable?
> summary() only displays a few statistics....
>
> Thanks!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>   


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C


From ggrothendieck at gmail.com  Mon Jul 24 06:27:41 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 24 Jul 2006 00:27:41 -0400
Subject: [R] diff, POSIXct, POSIXlt, POSIXt
In-Reply-To: <971536df0607230602u33348346x8093c6b9a9984f97@mail.gmail.com>
References: <44C32276.30800@univ-fcomte.fr>
	<644e1f320607230341v6d4cde82i1eff7881cc36a6ac@mail.gmail.com>
	<44C36849.7070106@univ-fcomte.fr>
	<971536df0607230602u33348346x8093c6b9a9984f97@mail.gmail.com>
Message-ID: <971536df0607232127i5f2bfdd8w51dd99282a415d83@mail.gmail.com>

Just one more comment. It is possible to define length.POSIXlt yourself
in which case diff works with POSIXlt objects.

> length.POSIXlt <- function(x) length(x[[1]])
> diff(dts)
Time differences of  91,  92, 183,  91,  92, 182,  91,  92, 182 days


On 7/23/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Moving this to r-devel.
>
> Looking at the diff.POSIXt code we see the problem is that it takes the
> length of the input using length which is wrong since in the case
> of POSIXlt the length is always 9 (or maybe length should be
> defined differently for POSIXlt?).  Try this which gives the same
> problem:
>
>   dts[-1] - dts[-length(dts)]
>
> We get a more sensible answer if length is calculated correctly:
>
>  dts[-1] - dts[-length(dts[[1]])]
>
>
> On 7/23/06, Patrick Giraudoux <patrick.giraudoux at univ-fcomte.fr> wrote:
> > > Try converting to POSIXct:
> > That's what I did finally (see the previous e-mail).
> >
> > dts<-c("15/4/2003","15/7/2003","15/10/2003","15/04/2004","15/07/2004","15/10/2004","15/4/2005","15/07/2005","15/10/2005","15/4/2006")
> >
> > dts <- as.POSIXct(strptime(dts, "%d/%m/%Y"))
> > diff(dts)
> >
> > Time differences of  91,  92, 183,  91,  92, 182,  91,  92, 182 days
> >
> > > What is the problem you are trying to solve?
> > Actually, I don't understand why using diff() and POSIXct provides the
> > expected result and not using POSIXlt. Both POSIXct and POSIXlt are of
> > class POSIXt. The doc of diff() stresses that <'diff' is a generic
> > function with a default method and ones for classes '"ts"', '"POSIXt"'
> > and '"Date"'>. It does not mention differences between POSIXct and POSIXlt.
> >
> > Moreover, using diff() with POSIXlt has provided (wrong) numbers... and
> > not an error. This may be difficult to detect sometimes along programme
> > lines. Must one keep in mind that diff() is reliably applicable only on
> > POSIXct? In this case, should not it bve mentionned in the documentation?
> >
> > All the best,
> >
> > Patrick
> >
> >
> >
> >
> >
> >
> >
> > jim holtman a ?crit :
> > > Try converting to POSIXct:
> > >
> > > > str(dts)
> > > 'POSIXlt', format: chr [1:10] "2003-04-15" "2003-07-15" "2003-10-15"
> > > "2004-04-15" "2004-07-15" "2004-10-15" "2005-04-15" ...
> > > > dts
> > >  [1] "2003-04-15" "2003-07-15" "2003-10-15" "2004-04-15" "2004-07-15"
> > > "2004-10-15" "2005-04-15" "2005-07-15"
> > >  [9] "2005-10-15" "2006-04-15"
> > > > dts <- as.POSIXct(dts)
> > > > dts
> > >  [1] "2003-04-15 EDT" "2003-07-15 EDT" "2003-10-15 EDT" "2004-04-15
> > > EDT" "2004-07-15 EDT" "2004-10-15 EDT"
> > >  [7] "2005-04-15 EDT" "2005-07-15 EDT" "2005-10-15 EDT" "2006-04-15 EDT"
> > > > diff(dts)
> > > Time differences of  91,  92, 183,  91,  92, 182,  91,  92, 182 days
> > > >
> > >
> > >
> > >
> > > On 7/23/06, *Patrick Giraudoux* <patrick.giraudoux at univ-fcomte.fr
> > > <mailto:patrick.giraudoux at univ-fcomte.fr>> wrote:
> > >
> > >     Dear Listers,
> > >
> > >     I have encountered a strange problem using diff() and POSIXt:
> > >
> > >     dts<-c("15/4/2003","15/7/2003","15/10/2003","15/04/2004","15/07/2004","15/10/2004","15/4/2005","15/07/2005","15/10/2005","15/4/2006")
> > >
> > >     dts <- strptime(dts, "%d/%m/%Y")
> > >     class(dts)
> > >
> > >     [1] "POSIXt"  "POSIXlt"
> > >
> > >     diff(dts)
> > >
> > >     Time differences of  7862400,  7948800, 15811200,  7862400,  7948800,
> > >     15724800,  7862400,  7948800,        0 secs
> > >
> > >     In this case the result is not the one expected: expressed in seconds
> > >     and not in days, and the difference between the two last dates is
> > >     not 0.
> > >
> > >     Now, if one use a vector of 9 dates only (whatever the date removed),
> > >     things come well:
> > >
> > >     diff(dts[-1])
> > >
> > >     Time differences of  92, 183,  91,  92, 182,  91,  92, 182 days
> > >
> > >     Also if one contrains dts to POSIXct
> > >
> > >     dts<-c("15/4/2003","15/7/2003","15/10/2003","15/04/2004","15/07/2004","15/10/2004","15/4/2005","15/07/2005","15/10/2005","15/4/2006")
> > >
> > >     dts <- as.POSIXct(strptime(dts, "%d/%m/%Y"))
> > >     diff(dts)
> > >
> > >     Time differences of  91,  92, 183,  91,  92, 182,  91,  92, 182 days
> > >
> > >     Any rational in that?
> > >
> > >     Patrick
> > >
> > >     ______________________________________________
> > >     R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch> mailing
> > >     list
> > >     https://stat.ethz.ch/mailman/listinfo/r-help
> > >     PLEASE do read the posting guide
> > >     http://www.R-project.org/posting-guide.html
> > >     and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> > >
> > >
> > > --
> > > Jim Holtman
> > > Cincinnati, OH
> > > +1 513 646 9390
> > >
> > > What is the problem you are trying to solve?
> >
> >        [[alternative HTML version deleted]]
> >
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
>


From patrick.giraudoux at univ-fcomte.fr  Mon Jul 24 07:54:35 2006
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Mon, 24 Jul 2006 07:54:35 +0200
Subject: [R] diff, POSIXct, POSIXlt, POSIXt
In-Reply-To: <971536df0607232127i5f2bfdd8w51dd99282a415d83@mail.gmail.com>
References: <44C32276.30800@univ-fcomte.fr>	
	<644e1f320607230341v6d4cde82i1eff7881cc36a6ac@mail.gmail.com>	
	<44C36849.7070106@univ-fcomte.fr>	
	<971536df0607230602u33348346x8093c6b9a9984f97@mail.gmail.com>
	<971536df0607232127i5f2bfdd8w51dd99282a415d83@mail.gmail.com>
Message-ID: <44C4609B.9060703@univ-fcomte.fr>

OK. Got it. Thanks a lot everybody.

I however feel that although the problem can be technically handled by 
any user aware of it, it should be fixed in R in a more general way, 
either by modifying the diff() code so that it really handles any kind 
of POSIXt (POSIXlt and POSIXct) with the same final result (as claimed 
in the documentation), or mentioning explicitely in de documentation 
that diff(), as it is written currently, can handle correctly only 
POSIXct (and not any POSIXt or POSIXlt).

There is a kind of danger of wrong output for users (even those reading 
the documentation) if things are left as they are, and I have detected 
this problem just by chance.

All the best,

Patrick

Gabor Grothendieck a ?crit :
> Just one more comment. It is possible to define length.POSIXlt yourself
> in which case diff works with POSIXlt objects.
>
>> length.POSIXlt <- function(x) length(x[[1]])
>> diff(dts)
> Time differences of  91,  92, 183,  91,  92, 182,  91,  92, 182 days
>
>
> On 7/23/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>> Moving this to r-devel.
>>
>> Looking at the diff.POSIXt code we see the problem is that it takes the
>> length of the input using length which is wrong since in the case
>> of POSIXlt the length is always 9 (or maybe length should be
>> defined differently for POSIXlt?).  Try this which gives the same
>> problem:
>>
>>   dts[-1] - dts[-length(dts)]
>>
>> We get a more sensible answer if length is calculated correctly:
>>
>>  dts[-1] - dts[-length(dts[[1]])]
>>
>>
>> On 7/23/06, Patrick Giraudoux <patrick.giraudoux at univ-fcomte.fr> wrote:
>> > > Try converting to POSIXct:
>> > That's what I did finally (see the previous e-mail).
>> >
>> > 
>> dts<-c("15/4/2003","15/7/2003","15/10/2003","15/04/2004","15/07/2004","15/10/2004","15/4/2005","15/07/2005","15/10/2005","15/4/2006") 
>>
>> >
>> > dts <- as.POSIXct(strptime(dts, "%d/%m/%Y"))
>> > diff(dts)
>> >
>> > Time differences of  91,  92, 183,  91,  92, 182,  91,  92, 182 days
>> >
>> > > What is the problem you are trying to solve?
>> > Actually, I don't understand why using diff() and POSIXct provides the
>> > expected result and not using POSIXlt. Both POSIXct and POSIXlt are of
>> > class POSIXt. The doc of diff() stresses that <'diff' is a generic
>> > function with a default method and ones for classes '"ts"', '"POSIXt"'
>> > and '"Date"'>. It does not mention differences between POSIXct and 
>> POSIXlt.
>> >
>> > Moreover, using diff() with POSIXlt has provided (wrong) numbers... 
>> and
>> > not an error. This may be difficult to detect sometimes along 
>> programme
>> > lines. Must one keep in mind that diff() is reliably applicable 
>> only on
>> > POSIXct? In this case, should not it bve mentionned in the 
>> documentation?
>> >
>> > All the best,
>> >
>> > Patrick
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> > jim holtman a ?crit :
>> > > Try converting to POSIXct:
>> > >
>> > > > str(dts)
>> > > 'POSIXlt', format: chr [1:10] "2003-04-15" "2003-07-15" "2003-10-15"
>> > > "2004-04-15" "2004-07-15" "2004-10-15" "2005-04-15" ...
>> > > > dts
>> > >  [1] "2003-04-15" "2003-07-15" "2003-10-15" "2004-04-15" 
>> "2004-07-15"
>> > > "2004-10-15" "2005-04-15" "2005-07-15"
>> > >  [9] "2005-10-15" "2006-04-15"
>> > > > dts <- as.POSIXct(dts)
>> > > > dts
>> > >  [1] "2003-04-15 EDT" "2003-07-15 EDT" "2003-10-15 EDT" "2004-04-15
>> > > EDT" "2004-07-15 EDT" "2004-10-15 EDT"
>> > >  [7] "2005-04-15 EDT" "2005-07-15 EDT" "2005-10-15 EDT" 
>> "2006-04-15 EDT"
>> > > > diff(dts)
>> > > Time differences of  91,  92, 183,  91,  92, 182,  91,  92, 182 days
>> > > >
>> > >
>> > >
>> > >
>> > > On 7/23/06, *Patrick Giraudoux* <patrick.giraudoux at univ-fcomte.fr
>> > > <mailto:patrick.giraudoux at univ-fcomte.fr>> wrote:
>> > >
>> > >     Dear Listers,
>> > >
>> > >     I have encountered a strange problem using diff() and POSIXt:
>> > >
>> > >     
>> dts<-c("15/4/2003","15/7/2003","15/10/2003","15/04/2004","15/07/2004","15/10/2004","15/4/2005","15/07/2005","15/10/2005","15/4/2006") 
>>
>> > >
>> > >     dts <- strptime(dts, "%d/%m/%Y")
>> > >     class(dts)
>> > >
>> > >     [1] "POSIXt"  "POSIXlt"
>> > >
>> > >     diff(dts)
>> > >
>> > >     Time differences of  7862400,  7948800, 15811200,  7862400,  
>> 7948800,
>> > >     15724800,  7862400,  7948800,        0 secs
>> > >
>> > >     In this case the result is not the one expected: expressed in 
>> seconds
>> > >     and not in days, and the difference between the two last 
>> dates is
>> > >     not 0.
>> > >
>> > >     Now, if one use a vector of 9 dates only (whatever the date 
>> removed),
>> > >     things come well:
>> > >
>> > >     diff(dts[-1])
>> > >
>> > >     Time differences of  92, 183,  91,  92, 182,  91,  92, 182 days
>> > >
>> > >     Also if one contrains dts to POSIXct
>> > >
>> > >     
>> dts<-c("15/4/2003","15/7/2003","15/10/2003","15/04/2004","15/07/2004","15/10/2004","15/4/2005","15/07/2005","15/10/2005","15/4/2006") 
>>
>> > >
>> > >     dts <- as.POSIXct(strptime(dts, "%d/%m/%Y"))
>> > >     diff(dts)
>> > >
>> > >     Time differences of  91,  92, 183,  91,  92, 182,  91,  92, 
>> 182 days
>> > >
>> > >     Any rational in that?
>> > >
>> > >     Patrick
>> > >
>> > >     ______________________________________________
>> > >     R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch> 
>> mailing
>> > >     list
>> > >     https://stat.ethz.ch/mailman/listinfo/r-help
>> > >     PLEASE do read the posting guide
>> > >     http://www.R-project.org/posting-guide.html
>> > >     and provide commented, minimal, self-contained, reproducible 
>> code.
>> > >
>> > >
>> > >
>> > >
>> > > --
>> > > Jim Holtman
>> > > Cincinnati, OH
>> > > +1 513 646 9390
>> > >
>> > > What is the problem you are trying to solve?
>> >
>> >        [[alternative HTML version deleted]]
>> >
>> >
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>> >
>> >
>>
>


From robert-mcfadden at o2.pl  Mon Jul 24 11:06:38 2006
From: robert-mcfadden at o2.pl (Robert Mcfadden)
Date: Mon, 24 Jul 2006 11:06:38 +0200
Subject: [R] change the name of file
Message-ID: <001701c6af00$786edcb0$1191680a@robert>


Dear R Users,
Is it possible to make file names dependent on a changing variable?
For instance. I generate random numbers in a loop and at each iteration I
want data to write to file (I do not want to write everything in one file
using 'append'):

for (i in 1:50){
x<-matrix(runif(100, min=0,max=1),nrow=5,ncol=20)
Write(t(x),file="Data_i.txt",ncolumns=5,sep="\t") 
}   

Of course file name Data_i.txt will be the same for changing i,
unfortunately. 

Any suggestion would be appreciate
Robert


From I.Visser at uva.nl  Mon Jul 24 11:30:14 2006
From: I.Visser at uva.nl (Ingmar Visser)
Date: Mon, 24 Jul 2006 11:30:14 +0200
Subject: [R] change the name of file
In-Reply-To: <001701c6af00$786edcb0$1191680a@robert>
Message-ID: <C0EA5FC6.7049%I.Visser@uva.nl>

Use paste("Data_",i, sep="")


> From: Robert Mcfadden <robert-mcfadden at o2.pl>
> Date: Mon, 24 Jul 2006 11:06:38 +0200
> To: <r-help at stat.math.ethz.ch>
> Subject: [R] change the name of file
> 
> 
> Dear R Users,
> Is it possible to make file names dependent on a changing variable?
> For instance. I generate random numbers in a loop and at each iteration I
> want data to write to file (I do not want to write everything in one file
> using 'append'):
> 
> for (i in 1:50){
> x<-matrix(runif(100, min=0,max=1),nrow=5,ncol=20)
> Write(t(x),file="Data_i.txt",ncolumns=5,sep="\t")
> }   
> 
> Of course file name Data_i.txt will be the same for changing i,
> unfortunately. 
> 
> Any suggestion would be appreciate
> Robert
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From kopp at soz.unibe.ch  Mon Jul 24 11:31:02 2006
From: kopp at soz.unibe.ch (Chris Kopp)
Date: Mon, 24 Jul 2006 11:31:02 +0200
Subject: [R] Referring on the value of a counter in a loop
Message-ID: <6C8C2D72-5035-4ECA-9D27-A23F83F815A0@soz.unibe.ch>

Dear all

The following is a very basic and beginner's question on loops.

Suppose you have data (say, 1000 cases) with two variables named  
"answer" (string) and "class" (numeric). The latter runs from 1 to 78  
and categorizes the data. I need to create a file "answer_1.txt" for  
the cases with class==1, and so on, up to "answer_78.txt"

I have been able to do this for one value of "class" in the following  
way

k_1<-answer[class==1]
write.table(k_1, "answer_1.txt")

Now, I would like to loop over "class", replacing the occurences of  
"1" in the code above successively with the integers from 1 to 78, so  
that I get my 78 files.
(equivalent to typing:

k_1<-answer[class==1]
write.table(k_1, "answer_1.txt")

k_2<-answer[class==2]
write.table(k_2, "answer_2.txt")

etc.)

I have tried

for (i in 1:78) k_i<-answer[class==(i)]

but this only generated the variable corresponding to the last value  
(78) of the counter i, and not the files in between.

Thanks a lot for any help or pointers in the right direction. I am an  
R beginner and I studied the help files (they got me up to here), but  
I could not solve this problem on my own.

Chris Kopp


From ramasamy at cancer.org.uk  Mon Jul 24 11:31:18 2006
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 24 Jul 2006 10:31:18 +0100
Subject: [R] change the name of file
In-Reply-To: <001701c6af00$786edcb0$1191680a@robert>
References: <001701c6af00$786edcb0$1191680a@robert>
Message-ID: <1153733478.10563.44.camel@dhcp-82.wolf.ox.ac.uk>

Do you mean write.table instead of Write() ? Try 

 fn <- paste("Data_", i, ".txt", sep="")
 write.table( t(x), file=fn, sep="\t" )

Regards, Adai


On Mon, 2006-07-24 at 11:06 +0200, Robert Mcfadden wrote:
> Dear R Users,
> Is it possible to make file names dependent on a changing variable?
> For instance. I generate random numbers in a loop and at each iteration I
> want data to write to file (I do not want to write everything in one file
> using 'append'):
> 
> for (i in 1:50){
> x<-matrix(runif(100, min=0,max=1),nrow=5,ncol=20)
> Write(t(x),file="Data_i.txt",ncolumns=5,sep="\t") 
> }   
> 
> Of course file name Data_i.txt will be the same for changing i,
> unfortunately. 
> 
> Any suggestion would be appreciate
> Robert
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From johannes at huesing.name  Mon Jul 24 11:33:02 2006
From: johannes at huesing.name (Johannes =?iso-8859-1?Q?H=FCsing?=)
Date: Mon, 24 Jul 2006 11:33:02 +0200 (CEST)
Subject: [R] change the name of file
In-Reply-To: <001701c6af00$786edcb0$1191680a@robert>
References: <001701c6af00$786edcb0$1191680a@robert>
Message-ID: <26304.129.206.90.2.1153733582.squirrel@mail.panix.com>

> Of course file name Data_i.txt will be the same for changing i,
> unfortunately.

?paste, e.g. paste("Data_", i, ".txt", sep="")


From robert-mcfadden at o2.pl  Mon Jul 24 11:34:10 2006
From: robert-mcfadden at o2.pl (Robert Mcfadden)
Date: Mon, 24 Jul 2006 11:34:10 +0200
Subject: [R] change the name of file
In-Reply-To: <2947853.1740911153732552679.JavaMail.root@vms071.mailsrvcs.net>
Message-ID: <001801c6af04$5125f590$1191680a@robert>

Great, little change and it works. Thank you.
Look
 

for (i in 1:50){
x<-matrix(runif(100, min=0,max=1),nrow=5,ncol=20)
write(t(x),file=paste("data_",i,".txt",sep=""),ncolumns=5,sep="\t") 
}



> -----Original Message-----
> From: markleeds w verizon.net [mailto:markleeds w verizon.net]
> Sent: Monday, July 24, 2006 11:16 AM
> To: Robert Mcfadden
> Subject: Re: [R] change the name of file
> 
> >From: Robert Mcfadden <robert-mcfadden w o2.pl>
> >Date: 2006/07/24 Mon AM 04:06:38 CDT
> >To: r-help w stat.math.ethz.ch
> >Subject: [R] change the name of file
> 
> i think file=paste("data_", i,"txt",sep="")
> should work but someone else will probably
> have a better solution. if you try my idesa,
> check it because i haven't tested it.
> 
> 
> 
> >
> >Dear R Users,
> >Is it possible to make file names dependent on a changing variable?
> >For instance. I generate random numbers in a loop and at each iteration I
> >want data to write to file (I do not want to write everything in one file
> >using 'append'):
> >
> >for (i in 1:50){
> >x<-matrix(runif(100, min=0,max=1),nrow=5,ncol=20)
> >Write(t(x),file="Data_i.txt",ncolumns=5,sep="\t")
> >}
> >
> >Of course file name Data_i.txt will be the same for changing i,
> >unfortunately.
> >
> >Any suggestion would be appreciate
> >Robert
> >
> >______________________________________________
> >R-help w stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>


From tkellermann at ukaachen.de  Mon Jul 24 11:34:28 2006
From: tkellermann at ukaachen.de (Thilo Kellermann)
Date: Mon, 24 Jul 2006 11:34:28 +0200
Subject: [R] change the name of file
In-Reply-To: <001701c6af00$786edcb0$1191680a@robert>
References: <001701c6af00$786edcb0$1191680a@robert>
Message-ID: <200607241134.28365.tkellermann@ukaachen.de>

Dear Robert,

maybe (!) the following might do the job:

for (i in 1:50){
x<-matrix(runif(100, min=0,max=1),nrow=5,ncol=20)
Write(t(x),file=paste("Data_" i ".txt", sep=""),ncolumns=5,sep="\t")
}

Hope it really does...
Thilo


On Monday 24 July 2006 11:06, Robert Mcfadden wrote:
> Dear R Users,
> Is it possible to make file names dependent on a changing variable?
> For instance. I generate random numbers in a loop and at each iteration I
> want data to write to file (I do not want to write everything in one file
> using 'append'):
>
> for (i in 1:50){
> x<-matrix(runif(100, min=0,max=1),nrow=5,ncol=20)
> Write(t(x),file="Data_i.txt",ncolumns=5,sep="\t")
> }
>
> Of course file name Data_i.txt will be the same for changing i,
> unfortunately.
>
> Any suggestion would be appreciate
> Robert
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
________________________
Thilo Kellermann
Department of Psychiatry und Psychotherapy
RWTH Aachen University
Pauwelstr. 30
52074 Aachen
Tel.: +49 (0)241 / 8089977
Fax.: +49 (0)241 / 8082401
E-Mail: tkellermann at ukaachen.de


From jholtman at gmail.com  Mon Jul 24 11:41:37 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 24 Jul 2006 05:41:37 -0400
Subject: [R] Referring on the value of a counter in a loop
In-Reply-To: <6C8C2D72-5035-4ECA-9D27-A23F83F815A0@soz.unibe.ch>
References: <6C8C2D72-5035-4ECA-9D27-A23F83F815A0@soz.unibe.ch>
Message-ID: <644e1f320607240241x1b367e4fk130c40181724e6b9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060724/cfb532bb/attachment.pl 

From maechler at stat.math.ethz.ch  Mon Jul 24 11:46:05 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 24 Jul 2006 11:46:05 +0200
Subject: [R] diff, POSIXct, POSIXlt, POSIXt
In-Reply-To: <971536df0607230602u33348346x8093c6b9a9984f97@mail.gmail.com>
References: <44C32276.30800@univ-fcomte.fr>
	<644e1f320607230341v6d4cde82i1eff7881cc36a6ac@mail.gmail.com>
	<44C36849.7070106@univ-fcomte.fr>
	<971536df0607230602u33348346x8093c6b9a9984f97@mail.gmail.com>
Message-ID: <17604.38621.799499.786355@stat.math.ethz.ch>

>>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at gmail.com>
>>>>>     on Sun, 23 Jul 2006 09:02:35 -0400 writes:

    Gabor> Moving this to r-devel.

[would have been a good idea ...  but you didn't;
 I think it's too late now; rather keep the msg thread together]

    Gabor> Looking at the diff.POSIXt
    Gabor> code we see the problem is that it takes the length
    Gabor> of the input using length which is wrong since in the
    Gabor> case of POSIXlt the length is always 9 (or maybe
    Gabor> length should be defined differently for POSIXlt?).

Though I agree with Spencer that a user may expect length() to behave
differently, but I don't think this would be a good idea.
Yes, length() is generic, but its help() emphasizes that for
lists, length() should be the number of list elements.
Of course anyone one *can* define  length() methods that behave
differently for his/her classes, but then one would also want to
make sure that e.g.  x[length(x)]  or  'x[length(x)] <- value'
works and -- in a case of simple S3 class built on a list, would
work differently than if x was a the simple list.

In my view, I would only consider redefing length() for "non-basic"
S4 classes, i.e. those with slots,  where no confusion is
possible, since these objects are definitely not simple vectors
nor lists (aka "generic" vectors).


    Gabor> Try this which gives the same problem:

    Gabor>    dts[-1] - dts[-length(dts)]

    Gabor> We get a more sensible answer if length is calculated
    Gabor> correctly:

    Gabor>   dts[-1] - dts[-length(dts[[1]])]

Yes, thanks Gabor, and thanks to Patrick who is right that this
is a bug and diff() should work for both kinds of POSIXt
objects. I'll fix this for both R-patched and R-devel
- but not via redefining  length(<POSIXlt>).

Martin Maechler, ETH Zurich



    Gabor> On 7/23/06, Patrick Giraudoux
    Gabor> <patrick.giraudoux at univ-fcomte.fr> wrote:
    >> > Try converting to POSIXct: That's what I did finally
    >> (see the previous e-mail).
    >> 
    >> dts<-c("15/4/2003","15/7/2003","15/10/2003","15/04/2004","15/07/2004","15/10/2004","15/4/2005","15/07/2005","15/10/2005","15/4/2006")
    >> 
    >> dts <- as.POSIXct(strptime(dts, "%d/%m/%Y")) diff(dts)
    >> 
    >> Time differences of 91, 92, 183, 91, 92, 182, 91, 92, 182
    >> days
    >> 
    >> > What is the problem you are trying to solve?  Actually,
    >> I don't understand why using diff() and POSIXct provides
    >> the expected result and not using POSIXlt. Both POSIXct
    >> and POSIXlt are of class POSIXt. The doc of diff()
    >> stresses that <'diff' is a generic function with a
    >> default method and ones for classes '"ts"', '"POSIXt"'
    >> and '"Date"'>. It does not mention differences between
    >> POSIXct and POSIXlt.
    >> 
    >> Moreover, using diff() with POSIXlt has provided (wrong)
    >> numbers... and not an error. This may be difficult to
    >> detect sometimes along programme lines. Must one keep in
    >> mind that diff() is reliably applicable only on POSIXct?
    >> In this case, should not it bve mentionned in the
    >> documentation?
    >> 
    >> All the best,
    >> 
    >> Patrick
    >> 
    >> 
    >> 
    >> 
    >> 
    >> 
    >> 
    >> jim holtman a ?crit : > Try converting to POSIXct:
    >> >
    >> > > str(dts) > 'POSIXlt', format: chr [1:10] "2003-04-15"
    >> "2003-07-15" "2003-10-15" > "2004-04-15" "2004-07-15"
    >> "2004-10-15" "2005-04-15" ...  > > dts > [1] "2003-04-15"
    >> "2003-07-15" "2003-10-15" "2004-04-15" "2004-07-15" >
    >> "2004-10-15" "2005-04-15" "2005-07-15" > [9] "2005-10-15"
    >> "2006-04-15" > > dts <- as.POSIXct(dts) > > dts > [1]
    >> "2003-04-15 EDT" "2003-07-15 EDT" "2003-10-15 EDT"
    >> "2004-04-15 > EDT" "2004-07-15 EDT" "2004-10-15 EDT" >
    >> [7] "2005-04-15 EDT" "2005-07-15 EDT" "2005-10-15 EDT"
    >> "2006-04-15 EDT" > > diff(dts) > Time differences of 91,
    >> 92, 183, 91, 92, 182, 91, 92, 182 days
    >> > >
    >> >
    >> >
    >> >
    >> > On 7/23/06, *Patrick Giraudoux*
    >> <patrick.giraudoux at univ-fcomte.fr >
    >> <mailto:patrick.giraudoux at univ-fcomte.fr>> wrote:
    >> >
    >> > Dear Listers,
    >> >
    >> > I have encountered a strange problem using diff() and
    >> POSIXt:
    >> >
    >> >
    >> dts<-c("15/4/2003","15/7/2003","15/10/2003","15/04/2004","15/07/2004","15/10/2004","15/4/2005","15/07/2005","15/10/2005","15/4/2006")
    >> >
    >> > dts <- strptime(dts, "%d/%m/%Y") > class(dts)
    >> >
    >> > [1] "POSIXt" "POSIXlt"
    >> >
    >> > diff(dts)
    >> >
    >> > Time differences of 7862400, 7948800, 15811200,
    >> 7862400, 7948800, > 15724800, 7862400, 7948800, 0 secs
    >> >
    >> > In this case the result is not the one expected:
    >> expressed in seconds > and not in days, and the
    >> difference between the two last dates is > not 0.
    >> >
    >> > Now, if one use a vector of 9 dates only (whatever the
    >> date removed), > things come well:
    >> >
    >> > diff(dts[-1])
    >> >
    >> > Time differences of 92, 183, 91, 92, 182, 91, 92, 182
    >> days
    >> >
    >> > Also if one contrains dts to POSIXct
    >> >
    >> >
    >> dts<-c("15/4/2003","15/7/2003","15/10/2003","15/04/2004","15/07/2004","15/10/2004","15/4/2005","15/07/2005","15/10/2005","15/4/2006")
    >> >
    >> > dts <- as.POSIXct(strptime(dts, "%d/%m/%Y")) >
    >> diff(dts)
    >> >
    >> > Time differences of 91, 92, 183, 91, 92, 182, 91, 92,
    >> 182 days
    >> >
    >> > Any rational in that?
    >> >
    >> > Patrick


From peterjl at bilkent.edu.tr  Mon Jul 24 12:34:11 2006
From: peterjl at bilkent.edu.tr (Peter J. Lee)
Date: Mon, 24 Jul 2006 13:34:11 +0300
Subject: [R] Correlations by group
Message-ID: <6.2.3.4.0.20060724130351.01bd8e78@pop3.bilkent.edu.tr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060724/e0c8ca72/attachment.pl 

From bernat at creaf.uab.es  Mon Jul 24 13:27:35 2006
From: bernat at creaf.uab.es (Bernat Claramunt i Lopez)
Date: Mon, 24 Jul 2006 13:27:35 +0200
Subject: [R] Can make no plots !!!
In-Reply-To: <Pine.LNX.4.64.0607201121480.31944@springer.berkeley.edu>
References: <200607201515.12277.bernat@creaf.uab.es>
	<Pine.LNX.4.64.0607201121480.31944@springer.berkeley.edu>
Message-ID: <200607241327.35469.bernat@creaf.uab.es>

Thank you for your suggestion Phil, but they are already installed... Any 
other idea, please ?
Thanks in advance
Bernat



A Dijous 20 Juliol 2006 20:22, Phil Spector va escriure:
> Bernat -
>     Just a guess, but maybe you need to install the packages
>
>        xfonts-100dpi
> and
>        xfonts-75dpi
>
>                                                          - Phil
>
> On Thu, 20 Jul 2006, Bernat Claramunt i Lopez wrote:
> > Dear all
> > I have Kubuntu linux and have updated to the latest version (6.06
> > dapper). I do not know why but now I can not make no plots. For instance,
> > when I type
> >
> >> hist(...)
> >
> > this is the message I get:
> >> can't find X11 font
> >> Error in X11 (display, width, heigth......)
> >> unable to start devide X11
> >> In addition: Warning messages:
> >> 1:locale not supported by Xlib: some X ops will operate in C locale
> >> 2: X cannot set locale modifiers
> >
> > Any suggestion ? Maybe it is because kubuntu now works with Xorg instead
> > of X11 ? I really ignore how to solve it and I need to make plots !!!
> >
> > Thanks in advance
> >
> > --
> > Bernat Claramunt i Lopez
> > CREAF (Centre de Recerca Ecologica i Aplicacions Forestals ) i
> > Departament de Biologia Vegetal, Biologia Animal i Ecologia
> > Universitat Autonoma de Barcelona
> > 08193 Bellaterra, Catalunya
> >
> > Telf: +34935811920
> > FAX:  +34935814151
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.

-- 
Bernat Claramunt i Lopez
CREAF (Centre de Recerca Ecologica i Aplicacions Forestals ) i Departament de 
Biologia Vegetal, Biologia Animal i Ecologia
Universitat Autonoma de Barcelona
08193 Bellaterra, Catalunya

Telf: +34935811920
FAX:  +34935814151


From dirk.enzmann at uni-hamburg.de  Mon Jul 24 13:55:44 2006
From: dirk.enzmann at uni-hamburg.de (Dirk Enzmann)
Date: Mon, 24 Jul 2006 13:55:44 +0200
Subject: [R] conflict of package "Matrix" and summary of lme object
Message-ID: <44C4B540.6040400@uni-hamburg.de>

After loading the package "Matrix" (version 0.995-12), using the summary 
function with an lme (package nlme version 3.1-75) object results in an 
error message saying

Fehler in dim(x) : kein Slot des Namens "Dim" f?r dieses Objekt der 
Klasse "correlation"

(translated: 'Error in dim(x) : no slot of the name "Dim" for this 
object of class "correlation")'.

Without loading "Matrix" this error message does not occur.

Any ideas?

------------------------------
R version: 2.3.1 Patched (2006-06-21 r38367)
Operating system: Windows XP (5.1 (Build 2600))
CPU: Pentium Model 2 Stepping 9

*************************************************
Dr. Dirk Enzmann
Institute of Criminal Sciences
Dept. of Criminology
Edmund-Siemers-Allee 1
D-20146 Hamburg
Germany

phone: +49-(0)40-42838.7498 (office)
        +49-(0)40-42838.4591 (Billon)
fax:   +49-(0)40-42838.2344
email: dirk.enzmann at uni-hamburg.de
www: 
http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html


From ggrothendieck at gmail.com  Mon Jul 24 14:32:04 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 24 Jul 2006 08:32:04 -0400
Subject: [R] Correlations by group
In-Reply-To: <6.2.3.4.0.20060724130351.01bd8e78@pop3.bilkent.edu.tr>
References: <6.2.3.4.0.20060724130351.01bd8e78@pop3.bilkent.edu.tr>
Message-ID: <971536df0607240532x560fdb74j55e78df6115477e8@mail.gmail.com>

On 7/24/06, Peter J. Lee <peterjl at bilkent.edu.tr> wrote:
> I'm aware that S N Krishna asked the same
> question. However, I have failed to implement the
> posted solution for running rank order
> correlations on multiple subsets of data using the by() function.
>
> Here is my problem:
>
> Take a set of data from two subjects, who
> provided numerical infant mortality (IM) estimates for five countries:
>
>         sub <- c(1, 1, 1, 1, 1, 2, 2, 2, 2, 2)
> #grouping variable = 5 rows x 2 subjects
>         est <- c(60, 20, 260, 160, 42, 2, 1, 3,
> 7, 12) #response variable = 5 estimates x 2 subjects
>         im <- c(4, 5, 7, 8, 10, 4, 5, 7, 8, 10) #actual IM values x 2 subjects
>         data <- cbind(sub, est, im)
>         data
>
> Using the by() function:
>
>         by(data, sub, function(x) cor(est, im, method = "spearman"))

The calculation in your function does not depend on x so its
giving a constant return value.  Try:

  by(data, sub, function(x) cor(x[,2], x[,3], method = "spearman"))

or

   tapply(1:length(sub), sub, function(i) cor(est[i], im[i], method =
"spearman"))

or either the following which returns correlation matrices instead of
the correlations:

   by(data[,2:3], sub, function(x) cor(x, method = "spearman"))
   by(data[,2:3], sub, cor, method = "spearman")

>
> does result in two correlation coefficients. But
> instead of by subject, the est x im correlation
> for the entire set is reported, and then assigned
> to both subjects. This can be checked using:
>
>         cor(est, im, method = "spearman")
>
> Nevertheless, the true coeff's and p-values should be:
>
>         sub[1] cor.coef = 0.1 p > .1
>         sub[2] cor.coef = 0.9 p < .05
>
> I find it peculiar that running a simple regression by groups does work:
>
>         by(data, sub, function(x) lm(est ~ im, data = x))
>
> indicating that perhaps I'm using the wrong
> grouping function for correlations. I'm using a
> fairly standard Pentium 4 running Windows XP.
>
> On occasion I am required to calculate up to a
> quarter of a million individual correlations, so
> any help would be very much appreciated.
>
> Best wishes,
>
> Peter James Lee
> _________________________
>
> Peter James Lee
> Assistant Professor
>
> Psikoloji B?l?m?
> Bilkent University
> Bilkent
> Ankara
> Turkey
> 06800
>
> e-mail: peterjl at bilkent.edu.tr
> office: (90) 312 290 1807
> home: (90) 312 290 3447
> website: http://www.bilkent.edu.tr/~peterjl/index.html
> _________________________
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From peterjl at bilkent.edu.tr  Mon Jul 24 14:48:16 2006
From: peterjl at bilkent.edu.tr (Peter J. Lee)
Date: Mon, 24 Jul 2006 15:48:16 +0300
Subject: [R] Correlations by group
Message-ID: <6.2.3.4.0.20060724154517.01bdd0c8@pop3.bilkent.edu.tr>

Many thanks,

your solutions solve a number of my problems (not least weening myself off SAS)

Best wishes,

Peter
_________________________

Peter James Lee
Assistant Professor

Psikoloji B?l?m?
Bilkent University
Bilkent
Ankara
Turkey
06800

e-mail: peterjl at bilkent.edu.tr
office: (90) 312 290 1807
home: (90) 312 290 3447
website: http://www.bilkent.edu.tr/~peterjl/index.html


From vettorazzi at econ.uni-hamburg.de  Mon Jul 24 14:04:05 2006
From: vettorazzi at econ.uni-hamburg.de (Eik Vettorazzi)
Date: Mon, 24 Jul 2006 14:04:05 +0200
Subject: [R]  Plotting league tables/ caterpillar plots
Message-ID: <op.tc6zs3uoj3tevv@econ.uni-hamburg.de>

Dear list,
I was wondering if there is a function to plot league tables, sometimes  
also known as "caterpillar plots"?
A league table is conceptually very similar to a box plot. One difference  
is that the inter-quartile ranges are not shown. If there isn't such a  
function a first attempt for a "selfmade" plot would be to tell boxplot  
not to plot boxes (sounds silly isn't it?). I've tried the option  
"boxwex=0" but the result is unsatisfactory.

An example for a league table can be found in Marshall, Spiegelhalter  
[1998], Reliability of league tables of in vitro fertilisation clinics,  
BMJ1998;316:1701-1705, you may find it at http://bmj.bmjjournals.com

Thanks in advance

Eik Vettorazzi


From et22 at leicester.ac.uk  Mon Jul 24 15:39:22 2006
From: et22 at leicester.ac.uk (Tauber, Dr E.)
Date: Mon, 24 Jul 2006 14:39:22 +0100
Subject: [R] (no subject)
Message-ID: <0F0BFB9DA110794CA64C94D36F6BF5A307556ADC@sumac.cfs.le.ac.uk>



 
Eran Tauber (PhD)
Lecturer in Molecular Evolution
Dept. of Genetics
University of Leicester
University Rd, Leicester LE1 7RH
England
____________________________________________________________
Phone: 44 (0)116 252-3455, 252-3421 (lab) Fax: 44 (0)116 252-3378
www.le.ac.uk/genetics/et22


From maechler at stat.math.ethz.ch  Mon Jul 24 15:41:52 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 24 Jul 2006 15:41:52 +0200
Subject: [R] conflict of package "Matrix" and summary of lme object
In-Reply-To: <44C4B540.6040400@uni-hamburg.de>
References: <44C4B540.6040400@uni-hamburg.de>
Message-ID: <17604.52768.920741.57715@stat.math.ethz.ch>

>>>>> "Dirk" == Dirk Enzmann <dirk.enzmann at uni-hamburg.de>
>>>>>     on Mon, 24 Jul 2006 13:55:44 +0200 writes:

    Dirk> After loading the package "Matrix" (version 0.995-12),
    Dirk> using the summary function with an lme (package nlme
    Dirk> version 3.1-75) object results in an error message
    Dirk> saying

    Dirk> Fehler in dim(x) : kein Slot des Namens "Dim" f?r
    Dirk> dieses Objekt der Klasse "correlation"

    Dirk> (translated: 'Error in dim(x) : no slot of the name
    Dirk> "Dim" for this object of class "correlation")'.

    Dirk> Without loading "Matrix" this error message does not
    Dirk> occur.

    Dirk> Any ideas?

Yes. Thank you for the report.
The error happens when PRINTing a summary.lme object, 
i.e.,
	ssl <- summary(<lme> )  ## works fine
	ssl                     ##-> gives the error

The short answer:  
   Don't use 'Matrix' together with 'nlme'  for the time being
   (but rather use 'lme4' and lmer() if possible)

A long answer:

The problem happens because
'Matrix' defines a (S4) class "correlation"  (which inherits from
"dpoMatrix").  Hence  dim( <correlation> ) is accessing the
'Dim' slot of the <correlation> object --- which obviously
must fail for that part of a  "summary.lme" object which is not
a proper S4 object at all.

Here is reproducible example of showing the problem without even
doing an nlme related thing: 

   > x <- round(cor(matrix(rnorm(100), 20,5)),2); class(x) <- "correlation" ; x
	 [,1]  [,2]  [,3]  [,4]  [,5]
   [1,]  1.00 -0.29 -0.07 -0.11  0.03
   [2,] -0.29  1.00  0.10 -0.02  0.20
   [3,] -0.07  0.10  1.00  0.05  0.13
   [4,] -0.11 -0.02  0.05  1.00 -0.21
   [5,]  0.03  0.20  0.13 -0.21  1.00
   attr(,"class")
   [1] "correlation"
   > dim(x)
   [1] 5 5
   > library(Matrix)
   Loading required package: lattice
   > dim(x)
   Error in dim(x) : no slot of name "Dim" for this object of class "correlation"
   > dim
   .Primitive("dim")
   > showMethods(dim)

   Function "dim":
    x = "ANY"
    x = "Matrix"
    x = "correlation"
       (inherited from x = "Matrix")
   > showClass("correlation")

   Slots:

   Name:         sd         x       Dim  Dimnames      uplo   factors
   Class:   numeric   numeric   integer      list character      list

   Extends: 
   Class "dpoMatrix", directly
   Class "dsyMatrix", by class "dpoMatrix"
   Class "ddenseMatrix", by class "dpoMatrix"
   Class "symmetricMatrix", by class "dpoMatrix"
   Class "dMatrix", by class "dpoMatrix"
   Class "denseMatrix", by class "dpoMatrix"
   Class "Matrix", by class "dpoMatrix"
   Class "Matrix", by class "dpoMatrix"
   Class "compMatrix", by class "dpoMatrix"
   Class "Matrix", by class "dpoMatrix"
   > 


A workaround -- not for you but the authors of 'Matrix' and/or 'nlme' --
is for one of the two packages to call the class differently.
At the moment, I tend to do the change in 'nlme', since there,
the "correlation" class has been almost entirely hidden from the user.

A workaround for you:  Redefine print.summary.lme, e.g., by
commenting the line 
    class(corr) <- "correlation"

Regards,
Martin


From wade.wall at gmail.com  Mon Jul 24 15:42:29 2006
From: wade.wall at gmail.com (Wade Wall)
Date: Mon, 24 Jul 2006 09:42:29 -0400
Subject: [R] trim function in R
Message-ID: <e23082be0607240642p3526c036rb738046e30c078db@mail.gmail.com>

Hi all,
I am looking for a function in R to trim the last two characters of an
8 character string in a vector.  For example, I have the codes
37-079-2, 370079-3,37-079-8 and want to trim them to 37-079 by
removing the last two characters.  Is sub the correct function to use,
and if so how can I specify trimming the last 2 characters?  I have
read the help file, but can't quite figure out how to do it.
Thanks,
Wade


From Markus.Gesmann at lloyds.com  Mon Jul 24 15:54:59 2006
From: Markus.Gesmann at lloyds.com (Gesmann, Markus)
Date: Mon, 24 Jul 2006 14:54:59 +0100
Subject: [R] trim function in R
Message-ID: <C3E3A3D81F4E0F438118DAA9722F12A9011963A9@LNVCNTEXCH01.corp.lloydsnet>

See ?substr and ?nchar
Try:
substr("Hello World", 0, nchar("Hello World")-2)

Regards

Markus Gesmann
FPMA
Lloyd's Market Analysis
Lloyd's * One Lime Street * London * EC3M 7HA
Telephone +44 (0)20 7327 6472
Facsimile +44 (0)20 7327 5718
http://www.lloyds.com


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wade Wall
Sent: 24 July 2006 14:42
To: r-help at stat.math.ethz.ch
Subject: [R] trim function in R


Hi all,
I am looking for a function in R to trim the last two characters of an
8 character string in a vector.  For example, I have the codes
37-079-2, 370079-3,37-079-8 and want to trim them to 37-079 by
removing the last two characters.  Is sub the correct function to use,
and if so how can I specify trimming the last 2 characters?  I have
read the help file, but can't quite figure out how to do it.
Thanks,
Wade

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
**********************************************************************
The information in this E-Mail and in any attachments is CON...{{dropped}}


From HDoran at air.org  Mon Jul 24 15:57:54 2006
From: HDoran at air.org (Doran, Harold)
Date: Mon, 24 Jul 2006 09:57:54 -0400
Subject: [R] trim function in R
Message-ID: <2323A6D37908A847A7C32F1E3662C80E132F32@dc1ex01.air.org>

One option is substr() 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wade Wall
> Sent: Monday, July 24, 2006 9:42 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] trim function in R
> 
> Hi all,
> I am looking for a function in R to trim the last two characters of an
> 8 character string in a vector.  For example, I have the 
> codes 37-079-2, 370079-3,37-079-8 and want to trim them to 
> 37-079 by removing the last two characters.  Is sub the 
> correct function to use, and if so how can I specify trimming 
> the last 2 characters?  I have read the help file, but can't 
> quite figure out how to do it.
> Thanks,
> Wade
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Mon Jul 24 16:13:20 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 24 Jul 2006 10:13:20 -0400
Subject: [R] trim function in R
In-Reply-To: <e23082be0607240642p3526c036rb738046e30c078db@mail.gmail.com>
References: <e23082be0607240642p3526c036rb738046e30c078db@mail.gmail.com>
Message-ID: <971536df0607240713k45239f2ds6eaa024ddceae83d@mail.gmail.com>

A . (dot) matches any character and $ matches the end of string so
this replaces the last two characters with the empty string:

   sub("..$", "", x)


On 7/24/06, Wade Wall <wade.wall at gmail.com> wrote:
> Hi all,
> I am looking for a function in R to trim the last two characters of an
> 8 character string in a vector.  For example, I have the codes
> 37-079-2, 370079-3,37-079-8 and want to trim them to 37-079 by
> removing the last two characters.  Is sub the correct function to use,
> and if so how can I specify trimming the last 2 characters?  I have
> read the help file, but can't quite figure out how to do it.
> Thanks,
> Wade
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From et22 at leicester.ac.uk  Mon Jul 24 17:16:07 2006
From: et22 at leicester.ac.uk (Tauber, Dr E.)
Date: Mon, 24 Jul 2006 16:16:07 +0100
Subject: [R] Identifying peaks (or offsets) in a time series
Message-ID: <0F0BFB9DA110794CA64C94D36F6BF5A307556ADD@sumac.cfs.le.ac.uk>

Dear R-users,

We are monitoring the activity of animals during a few days period. The
data from each animal (crossing of infra-red beam) are collected as a
time series (in 30 min bins). An example is attached below.

y <-
c(0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,3,28,27,46,76,77,60,19,35,55,59,48
,87,20,38,82,62,60,85,105,69,109,102,100,101,116,126,119,63,27,25,15,8,0
,0,3,0,0,3,0,0,5,3,0,0,6,1,29,73,56,56,57,92,34,51,30,76,30,38,47,87,22,
0,68,76,94,101,119,114,115,111,116,134,125,76,23,19,30,2,8,0,3,0,0,0,7,0
,0,0,0,4,0,7,0,21,4,49,51,56,43,55,55,34,48,16,0,61,22,94,63,102,47,100,
96,113,93,109,123,120,124,115,94,96,76,36,3,0,0,0,0,0,0,2,5,0,0,0,0,2,10
,33,34,15,0,47,22,20,33,52,4,41,45,0,21,18,38,32,21,78,82,72,102,103,118
,116,118,114,82,18,5,21,4,0,14,0,5,2,0,0,2,2,0,0,3,0,2,7,16,13,17,50,0,4
8,16,19,34,39,33,3,67,0,68,34,65,84,61,100,85,108,124,141,139,134,96,54,
91,54,12,0,0,0,0,0,0,0,0,0,0,0,4,11,0,19,27,15,12,20)
 
We would like to have an automatic way, using R, to identify the time
point of offset of each bout of activity (i.e. when activity goes down
to a minimum value, for a defined duration). In the example above the
offset times (the element number) should be approximately: 53, 99, 146,
191, 239, 283, 330 (the last bout of activity can be ignores). 
 
Any help or advice will be greatly appreciated, 

Many thanks, Eran

 
Eran Tauber (PhD)
Lecturer in Molecular Evolution
Dept. of Genetics
University of Leicester
University Rd, Leicester LE1 7RH
England
____________________________________________________________
Phone: 44 (0)116 252-3455, 252-3421 (lab) Fax: 44 (0)116 252-3378
www.le.ac.uk/genetics/et22


From wade.wall at gmail.com  Mon Jul 24 17:18:10 2006
From: wade.wall at gmail.com (Wade Wall)
Date: Mon, 24 Jul 2006 11:18:10 -0400
Subject: [R] random section of samples based on group membership
Message-ID: <e23082be0607240818i105ba357ya6abfb6b6d6e6102@mail.gmail.com>

Hi all,

I have a matrix of 474 rows (samples) with 565 columns (variables).
each of the 474 samples belong to one of 120 groups, with the
groupings as a column in the above matrix. For example, the group
column would be:

1
1
1
2
2
2
.
.
.
120
120

I  want to randomly select one from each group.  Not all the groups
have the same number of samples, some have 4, some 3 etc.  Is there a
function to do this, or would I need to write a looping statement to
look at each successive group?

I basically want to combine the randomly selected samples from the 120
groups into a new matrix in order to perform a cluster analysis.

Thanks,
Wade


From ggrothendieck at gmail.com  Mon Jul 24 17:34:20 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 24 Jul 2006 11:34:20 -0400
Subject: [R] Identifying peaks (or offsets) in a time series
In-Reply-To: <0F0BFB9DA110794CA64C94D36F6BF5A307556ADD@sumac.cfs.le.ac.uk>
References: <0F0BFB9DA110794CA64C94D36F6BF5A307556ADD@sumac.cfs.le.ac.uk>
Message-ID: <971536df0607240834y20722ae5y58142561648fc9ae@mail.gmail.com>

Try:

RSiteSearch("finding peaks")


On 7/24/06, Tauber, Dr E. <et22 at leicester.ac.uk> wrote:
> Dear R-users,
>
> We are monitoring the activity of animals during a few days period. The
> data from each animal (crossing of infra-red beam) are collected as a
> time series (in 30 min bins). An example is attached below.
>
> y <-
> c(0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,3,28,27,46,76,77,60,19,35,55,59,48
> ,87,20,38,82,62,60,85,105,69,109,102,100,101,116,126,119,63,27,25,15,8,0
> ,0,3,0,0,3,0,0,5,3,0,0,6,1,29,73,56,56,57,92,34,51,30,76,30,38,47,87,22,
> 0,68,76,94,101,119,114,115,111,116,134,125,76,23,19,30,2,8,0,3,0,0,0,7,0
> ,0,0,0,4,0,7,0,21,4,49,51,56,43,55,55,34,48,16,0,61,22,94,63,102,47,100,
> 96,113,93,109,123,120,124,115,94,96,76,36,3,0,0,0,0,0,0,2,5,0,0,0,0,2,10
> ,33,34,15,0,47,22,20,33,52,4,41,45,0,21,18,38,32,21,78,82,72,102,103,118
> ,116,118,114,82,18,5,21,4,0,14,0,5,2,0,0,2,2,0,0,3,0,2,7,16,13,17,50,0,4
> 8,16,19,34,39,33,3,67,0,68,34,65,84,61,100,85,108,124,141,139,134,96,54,
> 91,54,12,0,0,0,0,0,0,0,0,0,0,0,4,11,0,19,27,15,12,20)
>
> We would like to have an automatic way, using R, to identify the time
> point of offset of each bout of activity (i.e. when activity goes down
> to a minimum value, for a defined duration). In the example above the
> offset times (the element number) should be approximately: 53, 99, 146,
> 191, 239, 283, 330 (the last bout of activity can be ignores).
>
> Any help or advice will be greatly appreciated,
>
> Many thanks, Eran
>
>
> Eran Tauber (PhD)
> Lecturer in Molecular Evolution
> Dept. of Genetics
> University of Leicester
> University Rd, Leicester LE1 7RH
> England
> ____________________________________________________________
> Phone: 44 (0)116 252-3455, 252-3421 (lab) Fax: 44 (0)116 252-3378
> www.le.ac.uk/genetics/et22
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From kemerson at uoregon.edu  Mon Jul 24 17:45:11 2006
From: kemerson at uoregon.edu (Kevin J Emerson)
Date: Mon, 24 Jul 2006 08:45:11 -0700
Subject: [R] grouping by consecutive integers
Message-ID: <1153755911.10131.22.camel@kemerson-desktop>

Hello R-helpers!

I have a question concerning extracting sequence information from a
vector.  I have a vector (representing the bins of a time series where
the frequency of occurrences is greater than some threshold) where I
would like to extract the min, median and max of each group of
consecutive numbers.

For Example:

tmp <- c(24,25,29,35,36,37,38,39,40,41,42,43,44,45,46,47,68,69,70,71)

I would like to have the max,min,median of the following groups:

24,25
29
35,36,37,38,39,40,41,42,43,44,45,46,47,
68,69,70,71

I would like to be able to perform this for many time series so an
automated process would be nice.  I am hoping to use this as a peak
detection protocol.

Any advice would be greatly appreciated,
Kevin

-----
-----
Kevin J Emerson
Center for Ecology and Evolutionary Biology
1210 University of Oregon
Eugene, OR 97403
USA
kemerson at uoregon.edu


From cgb at datanalytics.com  Mon Jul 24 17:31:52 2006
From: cgb at datanalytics.com (Carlos J. Gil Bellosta)
Date: Mon, 24 Jul 2006 17:31:52 +0200
Subject: [R] random section of samples based on group membership
In-Reply-To: <e23082be0607240818i105ba357ya6abfb6b6d6e6102@mail.gmail.com>
References: <e23082be0607240818i105ba357ya6abfb6b6d6e6102@mail.gmail.com>
Message-ID: <20060724173152.u5ur19001knc48o8@webmail.datanalytics.com>

Dear Wade,

Say that your groups are

groups <- sort(sample(1:10, 100, replace = TRUE))

Create a dummy

rows <- 1:length(groups)

Then

tapply( rows, groups, function(x) sample(x, 1))

does the trick to select the row numbers you need for your sampling.

Sincerely,

Carlos J. Gil Bellosta
http://www.datanalytics.com
http://www.data-mining-blog.com


Quoting Wade Wall <wade.wall at gmail.com>:

> Hi all,
>
> I have a matrix of 474 rows (samples) with 565 columns (variables).
> each of the 474 samples belong to one of 120 groups, with the
> groupings as a column in the above matrix. For example, the group
> column would be:
>
> 1
> 1
> 1
> 2
> 2
> 2
> .
> .
> .
> 120
> 120
>
> I  want to randomly select one from each group.  Not all the groups
> have the same number of samples, some have 4, some 3 etc.  Is there a
> function to do this, or would I need to write a looping statement to
> look at each successive group?
>
> I basically want to combine the randomly selected samples from the 120
> groups into a new matrix in order to perform a cluster analysis.
>
> Thanks,
> Wade
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From nvj at fys.ku.dk  Mon Jul 24 17:50:44 2006
From: nvj at fys.ku.dk (Niels Vestergaard Jensen)
Date: Mon, 24 Jul 2006 17:50:44 +0200 (CEST)
Subject: [R] grouping by consecutive integers
In-Reply-To: <1153755911.10131.22.camel@kemerson-desktop>
References: <1153755911.10131.22.camel@kemerson-desktop>
Message-ID: <Pine.LNX.4.64.0607241749420.1460@scharff.fys.ku.dk>

Look at "index vectors" in the R intro.

Best

 	Niels

On Mon, 24 Jul 2006, Kevin J Emerson wrote:

> Hello R-helpers!
>
> I have a question concerning extracting sequence information from a
> vector.  I have a vector (representing the bins of a time series where
> the frequency of occurrences is greater than some threshold) where I
> would like to extract the min, median and max of each group of
> consecutive numbers.
>
> For Example:
>
> tmp <- c(24,25,29,35,36,37,38,39,40,41,42,43,44,45,46,47,68,69,70,71)
>
> I would like to have the max,min,median of the following groups:
>
> 24,25
> 29
> 35,36,37,38,39,40,41,42,43,44,45,46,47,
> 68,69,70,71
>
> I would like to be able to perform this for many time series so an
> automated process would be nice.  I am hoping to use this as a peak
> detection protocol.
>
> Any advice would be greatly appreciated,
> Kevin
>
> -----
> -----
> Kevin J Emerson
> Center for Ecology and Evolutionary Biology
> 1210 University of Oregon
> Eugene, OR 97403
> USA
> kemerson at uoregon.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mr.blacksheep at gmail.com  Mon Jul 24 17:52:04 2006
From: mr.blacksheep at gmail.com (Mike Nielsen)
Date: Mon, 24 Jul 2006 09:52:04 -0600
Subject: [R] random section of samples based on group membership
In-Reply-To: <e23082be0607240818i105ba357ya6abfb6b6d6e6102@mail.gmail.com>
References: <e23082be0607240818i105ba357ya6abfb6b6d6e6102@mail.gmail.com>
Message-ID: <46a360560607240852i4d6fb111s35b0cc5de953a79a@mail.gmail.com>

Well, how you do it might be a matter of taste with respect to how you
want the results.

You could try using "by" with "sample"

by(x,x[,3],function(y){y[sample(nrow(y),1),]})

This will return a list with one list element for each sample group.
You can the combine the list back into a matrix.

That's my naive solution; no doubt there will be half a dozen better
ways to go about it.

Also, some of the clustering functions I have seen will sample for you.


On 7/24/06, Wade Wall <wade.wall at gmail.com> wrote:
> Hi all,
>
> I have a matrix of 474 rows (samples) with 565 columns (variables).
> each of the 474 samples belong to one of 120 groups, with the
> groupings as a column in the above matrix. For example, the group
> column would be:
>
> 1
> 1
> 1
> 2
> 2
> 2
> .
> .
> .
> 120
> 120
>
> I  want to randomly select one from each group.  Not all the groups
> have the same number of samples, some have 4, some 3 etc.  Is there a
> function to do this, or would I need to write a looping statement to
> look at each successive group?
>
> I basically want to combine the randomly selected samples from the 120
> groups into a new matrix in order to perform a cluster analysis.
>
> Thanks,
> Wade
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Regards,

Mike Nielsen


From kemerson at uoregon.edu  Mon Jul 24 18:20:09 2006
From: kemerson at uoregon.edu (Kevin J Emerson)
Date: Mon, 24 Jul 2006 09:20:09 -0700
Subject: [R] grouping by consecutive integers
In-Reply-To: <Pine.LNX.4.64.0607241749420.1460@scharff.fys.ku.dk>
References: <1153755911.10131.22.camel@kemerson-desktop>
	<Pine.LNX.4.64.0607241749420.1460@scharff.fys.ku.dk>
Message-ID: <1153758009.10131.27.camel@kemerson-desktop>

Let me clarify one thing that I dont think I made clear in my posting.
I am looking for the max, min and median of the indicies, not of the
time series frequency counts.  I am looking to find the max, min, and
median time of peaks in a time series, so i am looking for the
information concerning that. 

so mostly my question is how to extract the information of max, min, and
median of sequential numbers in a vector.  I will reword my original
posting below.

> > Hello R-helpers!
> >
> > I have a question concerning extracting sequence information from a
> > vector.  I have a vector (representing the bins of a time series where
> > the frequency of occurrences is greater than some threshold) where I
> > would like to extract the min, median and max of each group of
> > consecutive numbers in the index vector..
> >
> > For Example:
> >
> > tmp <- c(24,25,29,35,36,37,38,39,40,41,42,43,44,45,46,47,68,69,70,71)
> >
> > I would like to have the max,min,median of the following groups:
> >
> > 24,25 - max = 25, min = 24 median = 24.5
> > 29 max=min=median = 29
> > 35,36,37,38,39,40,41,42,43,44,45,46,47, max = 45 min = 35 etc...
> > 68,69,70,71
> >
> > I would like to be able to perform this for many time series so an
> > automated process would be nice.  I am hoping to use this as a peak
> > detection protocol.
> >
> > Any advice would be greatly appreciated,
> > Kevin
> >
> > -----
> > -----
> > Kevin J Emerson
> > Center for Ecology and Evolutionary Biology
> > 1210 University of Oregon
> > Eugene, OR 97403
> > USA
> > kemerson at uoregon.edu
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >


From cgb at datanalytics.com  Mon Jul 24 18:10:02 2006
From: cgb at datanalytics.com (Carlos J. Gil Bellosta)
Date: Mon, 24 Jul 2006 18:10:02 +0200
Subject: [R] grouping by consecutive integers
In-Reply-To: <1153755911.10131.22.camel@kemerson-desktop>
References: <1153755911.10131.22.camel@kemerson-desktop>
Message-ID: <20060724181002.5oht4qnsq5lw4cww@webmail.datanalytics.com>

Dear Kevin,

Try something like

groups <- cut( tmp, c(-Inf, which(diff(tmp) > 1 ) + 0.5, Inf) )

Sincerely,

Carlos J. Gil Bellosta
http://www.datanalytics.com
http://www.data-mining-blog.com

Quoting Kevin J Emerson <kemerson at uoregon.edu>:

> Hello R-helpers!
>
> I have a question concerning extracting sequence information from a
> vector.  I have a vector (representing the bins of a time series where
> the frequency of occurrences is greater than some threshold) where I
> would like to extract the min, median and max of each group of
> consecutive numbers.
>
> For Example:
>
> tmp <- c(24,25,29,35,36,37,38,39,40,41,42,43,44,45,46,47,68,69,70,71)
>
> I would like to have the max,min,median of the following groups:
>
> 24,25
> 29
> 35,36,37,38,39,40,41,42,43,44,45,46,47,
> 68,69,70,71
>
> I would like to be able to perform this for many time series so an
> automated process would be nice.  I am hoping to use this as a peak
> detection protocol.
>
> Any advice would be greatly appreciated,
> Kevin
>
> -----
> -----
> Kevin J Emerson
> Center for Ecology and Evolutionary Biology
> 1210 University of Oregon
> Eugene, OR 97403
> USA
> kemerson at uoregon.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From davidr at rhotrading.com  Mon Jul 24 19:00:02 2006
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Mon, 24 Jul 2006 12:00:02 -0500
Subject: [R] unique, but keep LAST occurence
Message-ID: <F9F2A641C593D7408925574C05A7BE77075F59@rhopost.rhotrading.com>

?unique says

Value:

     An object of the same type of 'x'. but if an element is equal to
     one with a smaller index, it is removed.

However, I need to keep the one with the LARGEST index.
Can someone please show me the light? 
I thought about reversing the row order twice, but I couldn't get it to work right

(My data frame has 125000 rows and 7 columns, 
and I'm 'uniqueing' on column #1 (chron) only, although the class of the column may not matter.)

Say, e.g., 
> DF <- data.frame(t = c(1,2,3,1,4,5,1,2,3), x = c(0,1,2,3,4,5,6,7,8))

I would like the result to be (sorted as well)
 t x
 1 6
 2 7
 3 8
 4 4
 5 5

If I got the original rownames, that would be a bonus (for debugging.)

> R.version
               _                         
platform       i386-pc-mingw32           
arch           i386                      
os             mingw32                   
system         i386, mingw32             
status                                   
major          2                         
minor          3.1                       
year           2006                      
month          06                        
day            01                        
svn rev        38247                     
language       R                         
version.string Version 2.3.1 (2006-06-01)

Thanks for any hints!
David

David L. Reiner
Rho Trading Securities, LLC
Chicago? IL? 60605
312-362-4963


From chris at psyctc.org  Mon Jul 24 17:39:35 2006
From: chris at psyctc.org (Chris Evans)
Date: Mon, 24 Jul 2006 16:39:35 +0100
Subject: [R] Plotting league tables/ caterpillar plots
In-Reply-To: <op.tc6zs3uoj3tevv@econ.uni-hamburg.de>
References: <op.tc6zs3uoj3tevv@econ.uni-hamburg.de>
Message-ID: <44C4E9B7.5020004@psyctc.org>

Eik Vettorazzi sent the following  at 24/07/2006 13:04:
> Dear list,
> I was wondering if there is a function to plot league tables, sometimes  
> also known as "caterpillar plots"?
> A league table is conceptually very similar to a box plot. One difference  
> is that the inter-quartile ranges are not shown. If there isn't such a  
> function a first attempt for a "selfmade" plot would be to tell boxplot  
> not to plot boxes (sounds silly isn't it?). I've tried the option  
> "boxwex=0" but the result is unsatisfactory.
> 
> An example for a league table can be found in Marshall, Spiegelhalter  
> [1998], Reliability of league tables of in vitro fertilisation clinics,  
> BMJ1998;316:1701-1705, you may find it at http://bmj.bmjjournals.com


Interesting, never heard the term "caterpillar plot" but I like that.  I
also call related things "biplane plots": the body is the parameter and
the wings (not really biplanes at all are they) are the 95% CI for the
parameter.  However, these caterpillar plots are rather like forest
plots or "trees plots" (I argue that you can see both the wood and the
trees!)

I tend to plot them rotated by 90 degrees and without such good
labelling as in the Spiegelhalter paper, but this code may help you.
The "traffic lights" in this superimpose overall quartiles which is used
in some colleagues' way of looking at league tables.  I've used similar
plots for proportions, means, correlations and even alpha reliability
parameters, it's fairly easy to substitute different parameters and
their CIs.  I prefer to use a more exact estimator of the CI of the
median than the one that, if I remember rightly, is the default in the
boxplot which is, I think a much quicker but less robust estimator of
the CI for each sample.

I'd appreciate gentle constructive criticism of my coding, I know I'm a
much better psychotherapist than a programmer!

C

-- 
Chris Evans <chris at psyctc.org>
Professor of Psychotherapy, Nottingham University;
Consultant Psychiatrist in Psychotherapy, Rampton Hospital;
Research Programmes Director, Nottinghamshire NHS Trust;
Hon. SL Institute of Psychiatry, Hon. Con., Tavistock & Portman Trust
**If I am writing from one of those roles, it will be clear. Otherwise**

**my views are my own and not representative of those institutions    **
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Weimar-plot-median.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060724/932dfecc/attachment.pl 
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: conf.median.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060724/932dfecc/attachment-0001.pl 

From mschwartz at mn.rr.com  Mon Jul 24 19:14:23 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 24 Jul 2006 12:14:23 -0500
Subject: [R] unique, but keep LAST occurence
In-Reply-To: <F9F2A641C593D7408925574C05A7BE77075F59@rhopost.rhotrading.com>
References: <F9F2A641C593D7408925574C05A7BE77075F59@rhopost.rhotrading.com>
Message-ID: <1153761264.12958.1.camel@localhost.localdomain>

On Mon, 2006-07-24 at 12:00 -0500, davidr at rhotrading.com wrote:
> ?unique says
> 
> Value:
> 
>      An object of the same type of 'x'. but if an element is equal to
>      one with a smaller index, it is removed.
> 
> However, I need to keep the one with the LARGEST index.
> Can someone please show me the light? 
> I thought about reversing the row order twice, but I couldn't get it to work right
> 
> (My data frame has 125000 rows and 7 columns, 
> and I'm 'uniqueing' on column #1 (chron) only, although the class of the column may not matter.)
> 
> Say, e.g., 
> > DF <- data.frame(t = c(1,2,3,1,4,5,1,2,3), x = c(0,1,2,3,4,5,6,7,8))
> 
> I would like the result to be (sorted as well)
>  t x
>  1 6
>  2 7
>  3 8
>  4 4
>  5 5
> 
> If I got the original rownames, that would be a bonus (for debugging.)

Does this get it?

> DF[sapply(unique(DF$t), function(x) max(which(DF$t == x))), ]
  t x
7 1 6
8 2 7
9 3 8
5 4 4
6 5 5


HTH,

Marc Schwartz


From mnair at iusb.edu  Mon Jul 24 19:25:29 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Mon, 24 Jul 2006 13:25:29 -0400
Subject: [R] Saving R objects
References: <A32055BDEA88C34BB3DBBCD229380778050FB4@iu-mssg-mbx109.ads.iu.edu>
	<971536df0607232011p28295a19kbc97ec9eee9d7e5@mail.gmail.com>
Message-ID: <A32055BDEA88C34BB3DBBCD229380778050FB6@iu-mssg-mbx109.ads.iu.edu>

I explicitly did is this way 
 
isoforms<-as.vector(rownames(mult.comp$estimate))
estimate<-as.vector(mult.comp$estimate)
lower<-as.vector(mult.comp$conf.int[,1])
upper<-as.vector(mult.comp$conf.int[,2])
p.val.raw<-as.vector(mult.comp$p.value.raw)
p.val.bon<-as.vector(mult.comp$p.value.bon)
p.val.adj<-as.vector(mult.comp$p.value.adj) 
out.data.mat<-cbind(isoforms,estimate,lower,upper,p.val.raw,p.val.bon,p.val.adj)
write.table(out.data.mat, file=filename.csv, sep=",", qmethod="double", col.name=NA)

Thanks ../Murli

________________________________

From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
Sent: Sun 7/23/2006 10:11 PM
To: Nair, Murlidharan T
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Saving R objects



It depends on what information you want to save and how the
program on the other end needs it.

For the save version I would at least use ascii = TRUE to get it
in a more readable fashion.

Look at

file.show("mult_test.dat")
file.show("mult.out")  # but use ascii=TRUE on your save statement.

to see what you are getting.

Other possibilities are to use R2HTML or XML packages to output
to HTML or XML.   You might want to handle the various components
of Dcirec separately.  To see what's inside:

   unclass(Dcirec)
   str(Dcirec)
   dput(Dcirec)

and use cat statements to output the components in the format of
your choice possibly in conjunction with sprintf.


On 7/23/06, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> I am trying to find the best way to save the follwoing object I am creating
>
> library(multcomp)
> data(recovery)
> Dcirec<-simint(minutes~blanket, data=recovery, conf.level=0.9, alternative="less")
>
> I am probably not doing it the most efficient way I think.
> Here is what I am doing
>
> a<-print(Dcirec)
> write(a,file="mult_test.dat", append=T)
> or
> save(Dcirec, file="mult.out")
>
> Which is the best way to save it, so that I can access its contents outside the R environment?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gunter.berton at gene.com  Mon Jul 24 19:27:45 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 24 Jul 2006 10:27:45 -0700
Subject: [R] grouping by consecutive integers
In-Reply-To: <1153758009.10131.27.camel@kemerson-desktop>
Message-ID: <002301c6af46$7a5c7910$711f210a@gne.windows.gene.com>

As you do not seem to have received what you consider to be  satisfactory
reply, here is a function that I **think** does what you want:

sequences <- function(x,incr = 1)
{
	ix <- which(abs(diff(c(FALSE,diff(x) == 1))) ==incr)
	if(length(ix)%%2)c(ix,length(x))
	else ix
}

This function gives successive pairs of first and last values of sequences
of increasing values within x that differ by incr. You can then process
these pairs however you like either to summarize 
statistics on the indices and/or the values of the sequences.

Examples:
> sequences(c(1:5,50,3:7))
[1]  1  5  7 11
> sequences(c(10,1:5,50,3:7))
[1]  2  6  8 12
> sequences(c(1:5,50,3:7,10))
[1]  1  5  7 11
> sequences(c(10,1:5,50,3:7,10))
[1]  2  6  8 12

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kevin J Emerson
> Sent: Monday, July 24, 2006 9:20 AM
> To: Niels Vestergaard Jensen
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] grouping by consecutive integers
> 
> Let me clarify one thing that I dont think I made clear in my posting.
> I am looking for the max, min and median of the indicies, not of the
> time series frequency counts.  I am looking to find the max, min, and
> median time of peaks in a time series, so i am looking for the
> information concerning that. 
> 
> so mostly my question is how to extract the information of 
> max, min, and
> median of sequential numbers in a vector.  I will reword my original
> posting below.
> 
> > > Hello R-helpers!
> > >
> > > I have a question concerning extracting sequence 
> information from a
> > > vector.  I have a vector (representing the bins of a time 
> series where
> > > the frequency of occurrences is greater than some 
> threshold) where I
> > > would like to extract the min, median and max of each group of
> > > consecutive numbers in the index vector..
> > >
> > > For Example:
> > >
> > > tmp <- 
> c(24,25,29,35,36,37,38,39,40,41,42,43,44,45,46,47,68,69,70,71)
> > >
> > > I would like to have the max,min,median of the following groups:
> > >
> > > 24,25 - max = 25, min = 24 median = 24.5
> > > 29 max=min=median = 29
> > > 35,36,37,38,39,40,41,42,43,44,45,46,47, max = 45 min = 35 etc...
> > > 68,69,70,71
> > >
> > > I would like to be able to perform this for many time series so an
> > > automated process would be nice.  I am hoping to use this 
> as a peak
> > > detection protocol.
> > >
> > > Any advice would be greatly appreciated,
> > > Kevin
> > >
> > > -----
> > > -----
> > > Kevin J Emerson
> > > Center for Ecology and Evolutionary Biology
> > > 1210 University of Oregon
> > > Eugene, OR 97403
> > > USA
> > > kemerson at uoregon.edu
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From eduarmasrs at yahoo.com.br  Mon Jul 24 19:37:22 2006
From: eduarmasrs at yahoo.com.br (Eduardo Dutra de Armas)
Date: Mon, 24 Jul 2006 14:37:22 -0300
Subject: [R] Memory exceeding for split
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAlkN94tU7pE2294PPHpOIAsKAAAAQAAAAeGqXyZ84kEaaWpGCt3gkZgEAAAAA@yahoo.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060724/fecdedb4/attachment.pl 

From gunter.berton at gene.com  Mon Jul 24 19:51:18 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 24 Jul 2006 10:51:18 -0700
Subject: [R] unique, but keep LAST occurence
In-Reply-To: <F9F2A641C593D7408925574C05A7BE77075F59@rhopost.rhotrading.com>
Message-ID: <003301c6af49$c4a8fb80$711f210a@gne.windows.gene.com>

Try:

 largestDF <- DF[nrow(DF)- which(!duplicated(rev(DF$t)))+1,]

You can then sort this however you like in the usual way. Row names will be
preserved.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> davidr at rhotrading.com
> Sent: Monday, July 24, 2006 10:00 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] unique, but keep LAST occurence
> 
> ?unique says
> 
> Value:
> 
>      An object of the same type of 'x'. but if an element is equal to
>      one with a smaller index, it is removed.
> 
> However, I need to keep the one with the LARGEST index.
> Can someone please show me the light? 
> I thought about reversing the row order twice, but I couldn't 
> get it to work right
> 
> (My data frame has 125000 rows and 7 columns, 
> and I'm 'uniqueing' on column #1 (chron) only, although the 
> class of the column may not matter.)
> 
> Say, e.g., 
> > DF <- data.frame(t = c(1,2,3,1,4,5,1,2,3), x = c(0,1,2,3,4,5,6,7,8))
> 
> I would like the result to be (sorted as well)
>  t x
>  1 6
>  2 7
>  3 8
>  4 4
>  5 5
> 
> If I got the original rownames, that would be a bonus (for debugging.)
> 
> > R.version
>                _                         
> platform       i386-pc-mingw32           
> arch           i386                      
> os             mingw32                   
> system         i386, mingw32             
> status                                   
> major          2                         
> minor          3.1                       
> year           2006                      
> month          06                        
> day            01                        
> svn rev        38247                     
> language       R                         
> version.string Version 2.3.1 (2006-06-01)
> 
> Thanks for any hints!
> David
> 
> David L. Reiner
> Rho Trading Securities, LLC
> Chicago? IL? 60605
> 312-362-4963
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Mon Jul 24 19:55:01 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 24 Jul 2006 13:55:01 -0400
Subject: [R] grouping by consecutive integers
In-Reply-To: <1153755911.10131.22.camel@kemerson-desktop>
References: <1153755911.10131.22.camel@kemerson-desktop>
Message-ID: <644e1f320607241055s5b8512ffs11326d411cced831@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060724/c256ff06/attachment.pl 

From e.rapsomaniki at mail.cryst.bbk.ac.uk  Mon Jul 24 19:59:31 2006
From: e.rapsomaniki at mail.cryst.bbk.ac.uk (Eleni Rapsomaniki)
Date: Mon, 24 Jul 2006 18:59:31 +0100
Subject: [R] RandomForest vs. bayes & svm classification performance
Message-ID: <1153763971.44c50a838791d@webmail.cryst.bbk.ac.uk>

Hi

This is a question regarding classification performance using different methods.
So far I've tried NaiveBayes (klaR package), svm (e1071) package and
randomForest (randomForest). What has puzzled me is that randomForest seems to
perform far better (32% classification error) than svm and NaiveBayes, which
have similar classification errors (45%, 48% respectively). A similar
difference in performance is observed with different combinations of
parameters, priors and size of training data. 

Because I was expecting to see little difference in the perfomance of these
methods I am worried that I may have made a mistake in my randomForest call: 

my.rf=randomForest(x=train.df[,-response_index], y=train.df[,response_index],
xtest=test.df[,-response_index], ytest=test.df[,response_index],
importance=TRUE,proximity=FALSE, keep.forest=FALSE)

(where train.df and test.df are my train and test data.frames and response_index
is the column number specifiying the class)

My main question is: could there be a legitimate reason why random forest would
outperform the other two models (e.g. maybe one
method is more reliable with Gaussian data, handles categorical data
better etc)? Also, is there a way of evaluating the predictive ability of each
parameter in the bayesian model as it can be done for random Forests (through
the importance table)? 

I would appreciate any of your comments and suggestions on these.

Many thanks
Eleni Rapsomaniki


From davidr at rhotrading.com  Mon Jul 24 20:03:43 2006
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Mon, 24 Jul 2006 13:03:43 -0500
Subject: [R] unique, but keep LAST occurence
Message-ID: <F9F2A641C593D7408925574C05A7BE77075F63@rhopost.rhotrading.com>

Thank you, Bert and Mark.
I believe Mark's solution works, but it was taking a very long time.
Bert's is very fast.

My day is saved!

David L. Reiner
Rho Trading Securities, LLC
Chicago  IL  60605
312-362-4963

-----Original Message-----
From: Berton Gunter [mailto:gunter.berton at gene.com] 
Sent: Monday, July 24, 2006 12:51 PM
To: David Reiner <davidr at rhotrading.com>; r-help at stat.math.ethz.ch
Subject: RE: [R] unique, but keep LAST occurence

Try:

 largestDF <- DF[nrow(DF)- which(!duplicated(rev(DF$t)))+1,]

You can then sort this however you like in the usual way. Row names will be
preserved.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> davidr at rhotrading.com
> Sent: Monday, July 24, 2006 10:00 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] unique, but keep LAST occurence
> 
> ?unique says
> 
> Value:
> 
>      An object of the same type of 'x'. but if an element is equal to
>      one with a smaller index, it is removed.
> 
> However, I need to keep the one with the LARGEST index.
> Can someone please show me the light? 
> I thought about reversing the row order twice, but I couldn't 
> get it to work right
> 
> (My data frame has 125000 rows and 7 columns, 
> and I'm 'uniqueing' on column #1 (chron) only, although the 
> class of the column may not matter.)
> 
> Say, e.g., 
> > DF <- data.frame(t = c(1,2,3,1,4,5,1,2,3), x = c(0,1,2,3,4,5,6,7,8))
> 
> I would like the result to be (sorted as well)
>  t x
>  1 6
>  2 7
>  3 8
>  4 4
>  5 5
> 
> If I got the original rownames, that would be a bonus (for debugging.)
> 
> > R.version
>                _                         
> platform       i386-pc-mingw32           
> arch           i386                      
> os             mingw32                   
> system         i386, mingw32             
> status                                   
> major          2                         
> minor          3.1                       
> year           2006                      
> month          06                        
> day            01                        
> svn rev        38247                     
> language       R                         
> version.string Version 2.3.1 (2006-06-01)
> 
> Thanks for any hints!
> David
> 
> David L. Reiner
> Rho Trading Securities, LLC
> Chicago? IL? 60605
> 312-362-4963
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gpagnon at emory.edu  Mon Jul 24 20:16:58 2006
From: gpagnon at emory.edu (Giuseppe Pagnoni)
Date: Mon, 24 Jul 2006 14:16:58 -0400
Subject: [R] (robust) mixed-effects model with covariate
Message-ID: <44C50E9A.30905@emory.edu>

Dear all,

First of all I apologize if you received this twice: I was checking the 
archive and I noticed that the text was scrubbed from the message, 
probably due to some setting in my e-mail program.


I am unsure about how to specify a model in R and I thought of asking 
some advice to the list. I have two groups ("Group"= A, B) of subjects, 
with each subject undertaking a test before and after a certain 
treatment ("Time"= pre, post). Additionally, I want to enter
the age of the subject as a covariate (the performance on the test is 
affected by age), and I also want to allow different slopes for the 
effect of age in the two groups of subjects (age might affect the 
performance of the two groups differentially).

Is the right model to use something like the following?

aov (y ~ Group*Time + Group*Age + Error(Subj/Group), data=df1 )

(If I enter that command, within summary, I get the following:
Error() model is singular in: aov(y ~ Group * Time + Group * Age +
Error(Subj/Group), data = df1))


As a second question: I have an outlier in one of the two groups. The 
outlier is not due to a measurement error but simply to the performance 
of the subject (possibly related to his medical history, but I have no 
way to determine that with certainty). This subject is
signaled to be an outlier within its group: averaging the pre and post 
values for the performance of the subjects in his group, the Grubbs test 
yields a probability of 0.002 for the subject to be an outlier (the 
subject is marked as a significant outlier also if I
perform the test separately on the pre and the post data).

If I remove this subject from its group, I get significant effects of 
Group and Group X Age (not using the R formula above, but another stat 
software), but if I leave the subject in those effects disappear. Since 
I understand that removing outliers is always worrysome, I would like to 
know if it is possible in R to estimate a model similar to that outlined 
above but in a resistant/robust fashion, and what would be the actual 
syntax to do that. I will very much appreciate any help or suggestion 
about this.

thanks in advance and best regards

giuseppe

-- 
-----
Giuseppe Pagnoni
Psychiatry and Behavioral Sciences
Emory University School of Medicine
101 Woodruff Circle, Suite 4000
Atlanta, GA, 30322
tel: 404.712.8431
fax: 404.727.3233


From gunter.berton at gene.com  Mon Jul 24 20:23:24 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 24 Jul 2006 11:23:24 -0700
Subject: [R] grouping by consecutive integers: Correction
In-Reply-To: <1153758009.10131.27.camel@kemerson-desktop>
Message-ID: <003d01c6af4e$405bdd70$711f210a@gne.windows.gene.com>

Sorry, all. My previous post was mixed up. Here's the corrected version:

sequences <- function(x,incr = 1)
{
	ix <- which(abs(diff(c(FALSE,diff(x) == incr))) ==1)
	if(length(ix)%%2)c(ix,length(x))
	else ix
}



-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kevin J Emerson
> Sent: Monday, July 24, 2006 9:20 AM
> To: Niels Vestergaard Jensen
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] grouping by consecutive integers
> 
> Let me clarify one thing that I dont think I made clear in my posting.
> I am looking for the max, min and median of the indicies, not of the
> time series frequency counts.  I am looking to find the max, min, and
> median time of peaks in a time series, so i am looking for the
> information concerning that. 
> 
> so mostly my question is how to extract the information of 
> max, min, and
> median of sequential numbers in a vector.  I will reword my original
> posting below.
> 
> > > Hello R-helpers!
> > >
> > > I have a question concerning extracting sequence 
> information from a
> > > vector.  I have a vector (representing the bins of a time 
> series where
> > > the frequency of occurrences is greater than some 
> threshold) where I
> > > would like to extract the min, median and max of each group of
> > > consecutive numbers in the index vector..
> > >
> > > For Example:
> > >
> > > tmp <- 
> c(24,25,29,35,36,37,38,39,40,41,42,43,44,45,46,47,68,69,70,71)
> > >
> > > I would like to have the max,min,median of the following groups:
> > >
> > > 24,25 - max = 25, min = 24 median = 24.5
> > > 29 max=min=median = 29
> > > 35,36,37,38,39,40,41,42,43,44,45,46,47, max = 45 min = 35 etc...
> > > 68,69,70,71
> > >
> > > I would like to be able to perform this for many time series so an
> > > automated process would be nice.  I am hoping to use this 
> as a peak
> > > detection protocol.
> > >
> > > Any advice would be greatly appreciated,
> > > Kevin
> > >
> > > -----
> > > -----
> > > Kevin J Emerson
> > > Center for Ecology and Evolutionary Biology
> > > 1210 University of Oregon
> > > Eugene, OR 97403
> > > USA
> > > kemerson at uoregon.edu
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Mon Jul 24 20:30:30 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 24 Jul 2006 14:30:30 -0400
Subject: [R] Identifying peaks (or offsets) in a time series
In-Reply-To: <0F0BFB9DA110794CA64C94D36F6BF5A307556ADD@sumac.cfs.le.ac.uk>
References: <0F0BFB9DA110794CA64C94D36F6BF5A307556ADD@sumac.cfs.le.ac.uk>
Message-ID: <644e1f320607241130w382de0dfu6c7fc985c8af3962@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060724/a7f54e50/attachment.pl 

From ggrothendieck at gmail.com  Mon Jul 24 21:08:57 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 24 Jul 2006 15:08:57 -0400
Subject: [R] Saving R objects
In-Reply-To: <A32055BDEA88C34BB3DBBCD229380778050FB6@iu-mssg-mbx109.ads.iu.edu>
References: <A32055BDEA88C34BB3DBBCD229380778050FB4@iu-mssg-mbx109.ads.iu.edu>
	<971536df0607232011p28295a19kbc97ec9eee9d7e5@mail.gmail.com>
	<A32055BDEA88C34BB3DBBCD229380778050FB6@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <971536df0607241208o5f25e56dn4961469f6ae8396b@mail.gmail.com>

out.data.mat can be created compactly using with:

out.data.mat <- with(mult.comp,
   cbind(estimate, conf.int, p.value.raw = c(p.value.raw),
p.value.bon, p.value.adj)
)


On 7/24/06, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> I explicitly did is this way
>
> isoforms<-as.vector(rownames(mult.comp$estimate))
> estimate<-as.vector(mult.comp$estimate)
> lower<-as.vector(mult.comp$conf.int[,1])
> upper<-as.vector(mult.comp$conf.int[,2])
> p.val.raw<-as.vector(mult.comp$p.value.raw)
> p.val.bon<-as.vector(mult.comp$p.value.bon)
> p.val.adj<-as.vector(mult.comp$p.value.adj)
> out.data.mat<-cbind(isoforms,estimate,lower,upper,p.val.raw,p.val.bon,p.val.adj)
> write.table(out.data.mat, file=filename.csv, sep=",", qmethod="double", col.name=NA)
>
> Thanks ../Murli
>
> ________________________________
>
> From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> Sent: Sun 7/23/2006 10:11 PM
> To: Nair, Murlidharan T
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Saving R objects
>
>
>
> It depends on what information you want to save and how the
> program on the other end needs it.
>
> For the save version I would at least use ascii = TRUE to get it
> in a more readable fashion.
>
> Look at
>
> file.show("mult_test.dat")
> file.show("mult.out")  # but use ascii=TRUE on your save statement.
>
> to see what you are getting.
>
> Other possibilities are to use R2HTML or XML packages to output
> to HTML or XML.   You might want to handle the various components
> of Dcirec separately.  To see what's inside:
>
>   unclass(Dcirec)
>   str(Dcirec)
>   dput(Dcirec)
>
> and use cat statements to output the components in the format of
> your choice possibly in conjunction with sprintf.
>
>
> On 7/23/06, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> > I am trying to find the best way to save the follwoing object I am creating
> >
> > library(multcomp)
> > data(recovery)
> > Dcirec<-simint(minutes~blanket, data=recovery, conf.level=0.9, alternative="less")
> >
> > I am probably not doing it the most efficient way I think.
> > Here is what I am doing
> >
> > a<-print(Dcirec)
> > write(a,file="mult_test.dat", append=T)
> > or
> > save(Dcirec, file="mult.out")
> >
> > Which is the best way to save it, so that I can access its contents outside the R environment?
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>


From spluque at gmail.com  Mon Jul 24 21:25:54 2006
From: spluque at gmail.com (Sebastian Luque)
Date: Mon, 24 Jul 2006 14:25:54 -0500
Subject: [R] random section of samples based on group membership
References: <e23082be0607240818i105ba357ya6abfb6b6d6e6102@mail.gmail.com>
Message-ID: <87ac6yx0sd.fsf@arctocephalus.homelinux.org>

On Mon, 24 Jul 2006 11:18:10 -0400,
"Wade Wall" <wade.wall at gmail.com> wrote:

> Hi all, I have a matrix of 474 rows (samples) with 565 columns
> (variables).  each of the 474 samples belong to one of 120 groups, with
> the groupings as a column in the above matrix. For example, the group
> column would be:

> 1 1 1 2 2 2 .  .  .  120 120

> I want to randomly select one from each group.  Not all the groups have
> the same number of samples, some have 4, some 3 etc.  Is there a
> function to do this, or would I need to write a looping statement to
> look at each successive group?

I use the following for that (some of it hacked from help("sample")):

".resample" <- function(x, size, ...) {
    if(length(x) <= 1) {
        if(!missing(size) && size == 0) x[FALSE] else x
    } else sample(x, size, ...)
}


"randpick" <- function(x, by, size = 1, ...)
{
    nx <- seq(nrow(x))
    ind <- unlist(tapply(nx, by, .resample, size, ...))
    x[nx %in% ind, ]
}


So, for instance:

R> randpick(Indometh, Indometh$Subject, 3)
   Subject time conc
2        1 0.50 0.94
7        1 3.00 0.12
11       1 8.00 0.05
15       2 1.00 0.70
16       2 1.25 0.64
19       2 4.00 0.20
25       3 0.75 1.16
29       3 3.00 0.22
32       3 6.00 0.08
34       4 0.25 1.85
43       4 6.00 0.07
44       4 8.00 0.07
48       5 1.00 0.39
54       5 6.00 0.10
55       5 8.00 0.06
58       6 0.75 1.03
64       6 5.00 0.13
65       6 6.00 0.10
R> randpick(Indometh, Indometh$Subject, 2)
   Subject time conc
8        1 4.00 0.11
10       1 6.00 0.07
14       2 0.75 0.71
20       2 5.00 0.25
23       3 0.25 2.72
28       3 2.00 0.39
39       4 2.00 0.40
43       4 6.00 0.07
48       5 1.00 0.39
52       5 4.00 0.11
57       6 0.50 1.44
66       6 8.00 0.09


The 'by' argument allows to sample within any combination of factors
desired.


Cheers,

-- 
Seb


From mnair at iusb.edu  Mon Jul 24 21:30:12 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Mon, 24 Jul 2006 15:30:12 -0400
Subject: [R] running jobs in background
Message-ID: <A32055BDEA88C34BB3DBBCD2293807784CB894@iu-mssg-mbx109.ads.iu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060724/3e40cf09/attachment.pl 

From johanfaux at yahoo.com  Mon Jul 24 21:43:09 2006
From: johanfaux at yahoo.com (johan Faux)
Date: Mon, 24 Jul 2006 12:43:09 -0700 (PDT)
Subject: [R] unique, but keep LAST occurence
In-Reply-To: <F9F2A641C593D7408925574C05A7BE77075F63@rhopost.rhotrading.com>
Message-ID: <20060724194309.6368.qmail@web56202.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060724/4f0bff7e/attachment.pl 

From johanfaux at yahoo.com  Mon Jul 24 21:45:41 2006
From: johanfaux at yahoo.com (johan Faux)
Date: Mon, 24 Jul 2006 12:45:41 -0700 (PDT)
Subject: [R] deparse - width.cutoff
Message-ID: <20060724194541.48284.qmail@web56209.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060724/b0ea4c96/attachment.pl 

From dirk.enzmann at uni-hamburg.de  Mon Jul 24 22:04:57 2006
From: dirk.enzmann at uni-hamburg.de (Dirk Enzmann)
Date: Mon, 24 Jul 2006 22:04:57 +0200
Subject: [R] standardized random effects with ranef.lme()
Message-ID: <44C527E9.3080400@uni-hamburg.de>

Using ranef() (package nlme, version 3.1-75) with an 'lme' object I can 
obtain random effects for intercept and slope of a certain level (say: 
1) - this corresponds to (say level 1) "residuals" in MLWin. Maybe I'm 
mistaken here, but the results are identical.

However, if I try to get the standardized random effects adding the 
paramter "standard=T" to the specification of ranef(), the results 
differ considerably from the results of MLWin (although MLWin defines 
"standardized" in the same way as "divided by its estimated (diagnostic) 
standard error").

Why do the results differ although the estimates (random effects and 
thus their variances) are almost identical? I noticed that lme() does 
not compute the standard errors of the variances of the random effects - 
for several reasons, but if this is true, how does ranef() calculate the 
standardized random effects (the help says: '"standardized" (i.e. 
divided by the corresponding estimated standard error)').

Is there a way to obtain similar results as in MLWin (or: should I 
prefer the results of ranef() for certain reasons)?

Dirk

-----------------------------
R version: 2.3.1 Patched (2006-06-21 r38367)


*************************************************
Dr. Dirk Enzmann
Institute of Criminal Sciences
Dept. of Criminology
Edmund-Siemers-Allee 1
D-20146 Hamburg
Germany

phone: +49-(0)40-42838.7498 (office)
        +49-(0)40-42838.4591 (Billon)
fax:   +49-(0)40-42838.2344
email: dirk.enzmann at uni-hamburg.de
www: 
http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html


From roger.bos at gmail.com  Mon Jul 24 22:14:33 2006
From: roger.bos at gmail.com (roger bos)
Date: Mon, 24 Jul 2006 16:14:33 -0400
Subject: [R] RandomForest vs. bayes & svm classification performance
In-Reply-To: <1153763971.44c50a838791d@webmail.cryst.bbk.ac.uk>
References: <1153763971.44c50a838791d@webmail.cryst.bbk.ac.uk>
Message-ID: <1db726800607241314i23f7497dx84cda6328cd41db0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060724/8159e88b/attachment.pl 

From jholtman at gmail.com  Mon Jul 24 22:19:58 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 24 Jul 2006 16:19:58 -0400
Subject: [R] Memory exceeding for split
In-Reply-To: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAlkN94tU7pE2294PPHpOIAsKAAAAQAAAAeGqXyZ84kEaaWpGCt3gkZgEAAAAA@yahoo.com.br>
References: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAlkN94tU7pE2294PPHpOIAsKAAAAQAAAAeGqXyZ84kEaaWpGCt3gkZgEAAAAA@yahoo.com.br>
Message-ID: <644e1f320607241319q75bc36cch48e555ff9baf5664@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060724/4001952a/attachment.pl 

From mhaltuch at u.washington.edu  Mon Jul 24 23:31:34 2006
From: mhaltuch at u.washington.edu (Melissa Ann Haltuch)
Date: Mon, 24 Jul 2006 14:31:34 -0700 (PDT)
Subject: [R] fracdiff
In-Reply-To: <44C27F18.6060808@pdf.com>
Message-ID: <Pine.LNX.4.43.0607241431340.4287@hymn02.u.washington.edu>

Hi Spencer,
Thanks for the direction. Yes, I am after the sigma for the estimated variance of the whitened residuals. I'll give your code a try!
Cheers,
Melissa


On Sun, 23 Jul 2006, Spencer Graves wrote:

> 	  Confidence intervals for the fracdiff parameter estimates shouldn't be too 
> difficult:  the ?fracdiff help file says it returns a list with components 
> including 'd', 'ar' 'ma', and 'stderror.dpq'.  From these one should easily get 
> the coeffecients, standard errors and naive, Wald confidence intervals;  for 
> your convenience, I've embedded these in the functions 'coef.fracdiff' and 
> 'confint.fracdiff' given below.
>
> 	  By 'sigma2', I assume you mean the estimated variance of the whitened 
> residuals.  That is not so obvious.  For that, I might try 
> 'arima(fitdiff(...))$sigma2', operating on a 'fracdiff' fit.
>
> 	  Hope this helps.
> 	  Spencer Graves
> ##### coef and confint functions for output of fracdiff:
> coef.fracdiff <- function(object){
>  unlist(object[c("d", "ar", "ma")])
> }
>
> confint.fracdiff <- function(object, parm, level=0.95, ...){
>  b <- coef.fracdiff(object)
>  se <- object$stderror.dpq
>  names(se) <- names(b)
> #
>  if(missing(parm))
>    parm <- 1:length(b)
>  b. <- b[parm]
>  se. <- se[parm]
> #
>  conf.c <- (1-level)/2
>  conf2 <- c(conf.c, 1-conf.c)
>  confNames <- paste(100*conf2, "%")
>  k. <- length(b.)
>  CI <- b+outer(se., qnorm(conf2))
>  dimnames(CI)[[2]] <- confNames
>  CI
> }
>
>
>
> Melissa Ann Haltuch wrote:
>> Hi, I'm using the function fracdiff and can not 
> figure out how to get the estimated values for
> sigma2 or confidence intervals for the parameter
> estimates. Does anyone know how to obtain these
> values?
>> Thanks,
>> Melissa
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From john_d_mchenry at yahoo.com  Tue Jul 25 03:28:35 2006
From: john_d_mchenry at yahoo.com (John McHenry)
Date: Mon, 24 Jul 2006 18:28:35 -0700 (PDT)
Subject: [R] Overplotting: plot() invocation looks ugly ... suggestions?
Message-ID: <20060725012835.96890.qmail@web35405.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060724/e74981e9/attachment.pl 

From ggrothendieck at gmail.com  Tue Jul 25 03:44:37 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 24 Jul 2006 21:44:37 -0400
Subject: [R] Overplotting: plot() invocation looks ugly ... suggestions?
In-Reply-To: <20060725012835.96890.qmail@web35405.mail.mud.yahoo.com>
References: <20060725012835.96890.qmail@web35405.mail.mud.yahoo.com>
Message-ID: <971536df0607241844g2510ac51uc32233f4c855d8ff@mail.gmail.com>

Try:

matplot(levels(data$Quarter), matrix(data$Consumption, 4), type = "o")


On 7/24/06, John McHenry <john_d_mchenry at yahoo.com> wrote:
> Hi WizaRds,
>
> I'd like to overplot UK fuel consumption per quarter over the course of five years.
> Sounds simple enough?
>
> Unless I'm missing something, the following seems very involved for what I'm trying to do. Any suggestions on simplifications?
>
> The way I did it is awkward mainly because of the first call to plot ... but isn't this necessary, especially to set limits for the plot?
>
> The second call to plot(), in conjunction with by(), seems to be natural enough, and, IMHO, seems to be readable and succinct.
>
>    data<- read.table(textConnection("Year    Quarter        Consumption
>    1965    1        874
>    1965    2        679
>    1965    3        616
>    1965    4        816
>
>    1966    1        866
>    1966    2        700
>    1966    3        603
>    1966    4        814
>
>    1967    1        843
>    1967    2        719
>    1967    3        594
>    1967    4        819
>
>    1968    1        906
>    1968    2        703
>    1968    3        634
>    1968    4        844
>
>    1969    1        952
>    1969    2        745
>    1969    3        635
>    1969    4        871"), header=TRUE)
>    data$Quarter<- as.factor(data$Quarter)
>    #
>    # what follows is only marginally less involved than using a for loop
>    # (the culprit is, in part, the need to make the first, type="n", call to plot()):
>    windows(width=12,height=6)
>    with(data, plot(levels(Quarter), Consumption[Year==Year[1]], ylim=c(min(Consumption), max(Consumption)), type="n"))
>    with(data, by(Consumption, Year, function(x) lines(levels(Quarter), x, type="o")))
>
> Thanks,
>
> Jack.
>
>
>
> ---------------------------------
> Groups are talking. We?re listening. Check out the handy changes to Yahoo! Groups.
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Tue Jul 25 04:08:00 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 24 Jul 2006 22:08:00 -0400
Subject: [R] Overplotting: plot() invocation looks ugly ... suggestions?
In-Reply-To: <971536df0607241844g2510ac51uc32233f4c855d8ff@mail.gmail.com>
References: <20060725012835.96890.qmail@web35405.mail.mud.yahoo.com>
	<971536df0607241844g2510ac51uc32233f4c855d8ff@mail.gmail.com>
Message-ID: <971536df0607241908j12f4ce47yc390638e6e31a077@mail.gmail.com>

And if lattice is ok then try this:

library(lattice)
xyplot(Consumption ~ Quarter, group = Year, data, type = "o")

On 7/24/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try:
>
> matplot(levels(data$Quarter), matrix(data$Consumption, 4), type = "o")
>
>
> On 7/24/06, John McHenry <john_d_mchenry at yahoo.com> wrote:
> > Hi WizaRds,
> >
> > I'd like to overplot UK fuel consumption per quarter over the course of five years.
> > Sounds simple enough?
> >
> > Unless I'm missing something, the following seems very involved for what I'm trying to do. Any suggestions on simplifications?
> >
> > The way I did it is awkward mainly because of the first call to plot ... but isn't this necessary, especially to set limits for the plot?
> >
> > The second call to plot(), in conjunction with by(), seems to be natural enough, and, IMHO, seems to be readable and succinct.
> >
> >    data<- read.table(textConnection("Year    Quarter        Consumption
> >    1965    1        874
> >    1965    2        679
> >    1965    3        616
> >    1965    4        816
> >
> >    1966    1        866
> >    1966    2        700
> >    1966    3        603
> >    1966    4        814
> >
> >    1967    1        843
> >    1967    2        719
> >    1967    3        594
> >    1967    4        819
> >
> >    1968    1        906
> >    1968    2        703
> >    1968    3        634
> >    1968    4        844
> >
> >    1969    1        952
> >    1969    2        745
> >    1969    3        635
> >    1969    4        871"), header=TRUE)
> >    data$Quarter<- as.factor(data$Quarter)
> >    #
> >    # what follows is only marginally less involved than using a for loop
> >    # (the culprit is, in part, the need to make the first, type="n", call to plot()):
> >    windows(width=12,height=6)
> >    with(data, plot(levels(Quarter), Consumption[Year==Year[1]], ylim=c(min(Consumption), max(Consumption)), type="n"))
> >    with(data, by(Consumption, Year, function(x) lines(levels(Quarter), x, type="o")))
> >
> > Thanks,
> >
> > Jack.
> >
> >
> >
> > ---------------------------------
> > Groups are talking. We?re listening. Check out the handy changes to Yahoo! Groups.
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From quin.wills at googlemail.com  Tue Jul 25 04:34:51 2006
From: quin.wills at googlemail.com (Quin Wills)
Date: Tue, 25 Jul 2006 03:34:51 +0100
Subject: [R] PCA with not non-negative definite covariance
Message-ID: <44c58367.17d95b1c.084e.ffffbd20@mx.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060725/8405ca9f/attachment.pl 

From ulriks at ruc.dk  Tue Jul 25 08:33:31 2006
From: ulriks at ruc.dk (Ulrik Stervbo)
Date: Tue, 25 Jul 2006 08:33:31 +0200
Subject: [R] Identifying peaks (or offsets) in a time series
In-Reply-To: <0F0BFB9DA110794CA64C94D36F6BF5A307556ADD@sumac.cfs.le.ac.uk>
References: <0F0BFB9DA110794CA64C94D36F6BF5A307556ADD@sumac.cfs.le.ac.uk>
Message-ID: <3483f8d50607242333vc241997x4cffe08f1eea2fc5@mail.gmail.com>

Could Petr Pikal's peaks function
(http://finzi.psych.upenn.edu/R/Rhelp02a/archive/33097.html) be of any
use?

Ulrik

On 7/24/06, Tauber, Dr E. <et22 at leicester.ac.uk> wrote:
> Dear R-users,
>
> We are monitoring the activity of animals during a few days period. The
> data from each animal (crossing of infra-red beam) are collected as a
> time series (in 30 min bins). An example is attached below.
>
> y <-
> c(0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,3,28,27,46,76,77,60,19,35,55,59,48
> ,87,20,38,82,62,60,85,105,69,109,102,100,101,116,126,119,63,27,25,15,8,0
> ,0,3,0,0,3,0,0,5,3,0,0,6,1,29,73,56,56,57,92,34,51,30,76,30,38,47,87,22,
> 0,68,76,94,101,119,114,115,111,116,134,125,76,23,19,30,2,8,0,3,0,0,0,7,0
> ,0,0,0,4,0,7,0,21,4,49,51,56,43,55,55,34,48,16,0,61,22,94,63,102,47,100,
> 96,113,93,109,123,120,124,115,94,96,76,36,3,0,0,0,0,0,0,2,5,0,0,0,0,2,10
> ,33,34,15,0,47,22,20,33,52,4,41,45,0,21,18,38,32,21,78,82,72,102,103,118
> ,116,118,114,82,18,5,21,4,0,14,0,5,2,0,0,2,2,0,0,3,0,2,7,16,13,17,50,0,4
> 8,16,19,34,39,33,3,67,0,68,34,65,84,61,100,85,108,124,141,139,134,96,54,
> 91,54,12,0,0,0,0,0,0,0,0,0,0,0,4,11,0,19,27,15,12,20)
>
> We would like to have an automatic way, using R, to identify the time
> point of offset of each bout of activity (i.e. when activity goes down
> to a minimum value, for a defined duration). In the example above the
> offset times (the element number) should be approximately: 53, 99, 146,
> 191, 239, 283, 330 (the last bout of activity can be ignores).
>
> Any help or advice will be greatly appreciated,
>
> Many thanks, Eran
>
>
> Eran Tauber (PhD)
> Lecturer in Molecular Evolution
> Dept. of Genetics
> University of Leicester
> University Rd, Leicester LE1 7RH
> England
> ____________________________________________________________
> Phone: 44 (0)116 252-3455, 252-3421 (lab) Fax: 44 (0)116 252-3378
> www.le.ac.uk/genetics/et22
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Blog: http://ulrikstervbo.blogspot.com
Mailing-list: http://www.coollist.com/group.cgi?l=ulrik_i_berlin


From jswansmi at uoguelph.ca  Mon Jul 24 21:43:04 2006
From: jswansmi at uoguelph.ca (jswansmi at uoguelph.ca)
Date: Mon, 24 Jul 2006 15:43:04 -0400
Subject: [R] User defined covariate structure.
Message-ID: <1153770184.44c522c8eb19d@webmail.uoguelph.ca>

I am trying to use nlme but instead of using one of the ?identity? variance or
covariance matrixes such as compsymm or ar1.  Instead I want the covariance
matrix to be represented in the following manor.  Is it possible to define my
own covariance matrix?
I have search and found papers saying I can define my own covariance matrixes
and own correlation structures.  Said use corstruct but not sure how to
implement it.  Also found documentation to use re.structur.  If able to help me
out it be greatly appreciated as I am stuck.

|1	p1g	p2g	p3g	p4g	
|
|p1g	1	p1g	p2g	p3g	
|
|p2g 	p1g	1	p1g	p2g	
|
|p3g	p2g	p1g	1	p1g 	
|
|:	:	:	:	:	
|


From thilo at izkf.rwth-aachen.de  Tue Jul 25 09:37:03 2006
From: thilo at izkf.rwth-aachen.de (Thilo Kellermann)
Date: Tue, 25 Jul 2006 09:37:03 +0200
Subject: [R] (robust) mixed-effects model with covariate
In-Reply-To: <44C50E9A.30905@emory.edu>
References: <44C50E9A.30905@emory.edu>
Message-ID: <200607250937.03578.thilo@izkf.rwth-aachen.de>

On Monday 24 July 2006 20:16, Giuseppe Pagnoni wrote:
> Dear all,
>
> First of all I apologize if you received this twice: I was checking the
> archive and I noticed that the text was scrubbed from the message,
> probably due to some setting in my e-mail program.
>
>
> I am unsure about how to specify a model in R and I thought of asking
> some advice to the list. I have two groups ("Group"= A, B) of subjects,
> with each subject undertaking a test before and after a certain
> treatment ("Time"= pre, post). Additionally, I want to enter
> the age of the subject as a covariate (the performance on the test is
> affected by age), and I also want to allow different slopes for the
> effect of age in the two groups of subjects (age might affect the
> performance of the two groups differentially).
>
> Is the right model to use something like the following?
>
> aov (y ~ Group*Time + Group*Age + Error(Subj/Group), data=df1 )
>
> (If I enter that command, within summary, I get the following:
> Error() model is singular in: aov(y ~ Group * Time + Group * Age +
> Error(Subj/Group), data = df1))
>
try:
aov(y~Group*Time*Age + Error(Subj*Time*Age), data = df1)
which specifies an ANOVA (but not with mixed effects) with three main effects 
and all interaction terms plus an error term that is independent between 
groups (!) and relates to within subjects variability.

For a "real" mixed effects analysis you should use the (n)lme function from 
the nlme package and one possible model could look like this:

lme(y~Group*Time, random ~ age | Subj, data = df1)

but the exact specification depends on your assumptions, in which it is 
possible to specify two or three models and compare their fits with anova(). 
For more information on mixed effects you should consult:
Jose C. Pinheiro & Douglas M. Bates (2000) Mixed-Effects Models in S and 
S-PLUS. Springer, New York.

Good luck,
Thilo

>
> As a second question: I have an outlier in one of the two groups. The
> outlier is not due to a measurement error but simply to the performance
> of the subject (possibly related to his medical history, but I have no
> way to determine that with certainty). This subject is
> signaled to be an outlier within its group: averaging the pre and post
> values for the performance of the subjects in his group, the Grubbs test
> yields a probability of 0.002 for the subject to be an outlier (the
> subject is marked as a significant outlier also if I
> perform the test separately on the pre and the post data).
>
> If I remove this subject from its group, I get significant effects of
> Group and Group X Age (not using the R formula above, but another stat
> software), but if I leave the subject in those effects disappear. Since
> I understand that removing outliers is always worrysome, I would like to
> know if it is possible in R to estimate a model similar to that outlined
> above but in a resistant/robust fashion, and what would be the actual
> syntax to do that. I will very much appreciate any help or suggestion
> about this.
>
> thanks in advance and best regards
>
> giuseppe

-- 
Thilo Kellermann
Department of Psychiatry and Psychotherapy
RWTH Aachen University
Pauwelstr. 30
52074 Aachen
Tel.: +49 (0)241 / 8089977
Fax.: +49 (0)241 / 8082401
E-Mail: thilo.kellermann at rwth-aachen.de


From bady at univ-lyon1.fr  Tue Jul 25 10:24:27 2006
From: bady at univ-lyon1.fr (bady at univ-lyon1.fr)
Date: Tue, 25 Jul 2006 10:24:27 +0200
Subject: [R] PCA with not non-negative definite covariance
In-Reply-To: <44c58367.17d95b1c.084e.ffffbd20@mx.gmail.com>
References: <44c58367.17d95b1c.084e.ffffbd20@mx.gmail.com>
Message-ID: <1153815867.44c5d53b1a88b@webmail.univ-lyon1.fr>

Hi , hi all,

> Am I correct to understand from the previous discussions on this topic (a
> few years back) that if I have a matrix with missing values my PCA options
> seem dismal if:
> (1)     I don?t want to impute the missing values.
> (2)     I don?t want to completely remove cases with missing values.
> (3)     I do cov() with use=?pairwise.complete.obs?, as this produces
> negative eigenvalues (which it has in my case!).

(4) Maybe you can use the Non-linear Iterative Partial Least Squares (NIPALS)
algorithm (intensively used in chemometry). S. Dray proposes a version of this
procedure at http://pbil.univ-lyon1.fr/R/additifs.html.


Hope this help :)


Pierre


From ligges at statistik.uni-dortmund.de  Tue Jul 25 10:38:10 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 25 Jul 2006 10:38:10 +0200
Subject: [R] running jobs in background
In-Reply-To: <A32055BDEA88C34BB3DBBCD2293807784CB894@iu-mssg-mbx109.ads.iu.edu>
References: <A32055BDEA88C34BB3DBBCD2293807784CB894@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <44C5D872.7050105@statistik.uni-dortmund.de>

Nair, Murlidharan T wrote:
> Can I run jobs in the background and the check the status of it from
> time to time in Windows version of R?

Depends on your version of Windows. I am running the automatical 
building of R packages in the background on a Windows Server 2003. 
Unfortunately, if you are logged on, a windows appears when the job is 
running.
For other jobs such as simulations, I am always using a Linux machine.


Uwe Ligges


> 
>  
> 
> Thanks ../Murli
> 
>  
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rkrug at sun.ac.za  Tue Jul 25 11:46:23 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Tue, 25 Jul 2006 11:46:23 +0200
Subject: [R] some EPS rotated in journal preview
In-Reply-To: <43D131AA.3030702@biostatistic.de>
References: <43CE2EBC.1020303@biostatistic.de>	<Pine.LNX.4.61.0601181333060.17938@gannet.stats>
	<43D131AA.3030702@biostatistic.de>
Message-ID: <44C5E86F.8060403@sun.ac.za>

This is a very late rply to this topic, but I had a similar problem and
I got different results with using ps.options() then using
postscript(...) and the one with ps.options produced the results
expected. The problem were the size of the graph and the paper and the
rotation.

If you want to know details, I have to dig them out.

Rainer


Knut Krueger wrote:
> 
> Prof. Brian Ripley schrieb:
> CoverLetter.pdf
>> The problem is a well-known one in viewers looking at whole pages,
>> especially PS -> PDF converters.  R figures are particularly 
>> vulnerable as they have text running both horizontally and vertically 
>> (with normal axes).
>>
>> Please do follow exactly the advice on the postscript help page.
>>
>>      The postscript produced for a single R plot is EPS (_Encapsulated
>>      PostScript_) compatible, and can be included into other documents,
>>      e.g., into LaTeX, using '\includegraphics{<filename>}'.  For use
>>      in this way you will probably want to set 'horizontal = FALSE,
>>      onefile = FALSE, paper = "special"'.
>>
>> If you have done that, suggest to your publisher that they turn auto 
>> rotation off. 
> 
> 
> 
> There are the reproducible codes (from the help file). The first one 
> with the long text is rotated the second is not rotated.
> Seems that they rotate the page, if the text of the y-axes is longer 
> than the text of the x-axes.
> Are you able to see any other reason for the rotation.
> I will contact the journal again, if there is no other reason especially 
> of the R-code.
> 
> Regards
> Knut Krueger.
> 
> postscript("c:/r/test/regline2.eps",horizontal = FALSE,onefile=FALSE, 
> paper = "special" ,pointsize=20,
>         height=8,width=8,family = "Helvetica", font = "Helvetica")
>      data(Davis)
>      attach(Davis)
>      mod.M<-lm(repwt~weight, subset=sex=="M")
>      mod.F<-lm(repwt~weight, subset=sex=="F")
>      plot(weight, repwt, pch=c(1,2)[sex],ylab="aaaa bbbb cccc dddd eeee 
> ffff",xlab="aaaa bbbb")
> 
>      reg.line(mod.M)
>      reg.line(mod.F, lty=2)
> 
> dev.off()
> 
> postscript("c:/r/test/regline3.eps",horizontal = FALSE,onefile=FALSE, 
> paper = "special" ,pointsize=20,
>         height=8,width=8,family = "Helvetica", font = "Helvetica")
>      data(Davis)
>      attach(Davis)
>      mod.M<-lm(repwt~weight, subset=sex=="M")
>      mod.F<-lm(repwt~weight, subset=sex=="F")
>      plot(weight, repwt, pch=c(1,2)[sex])
> 
>      reg.line(mod.M)
>      reg.line(mod.F, lty=2)
> 
> dev.off()
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa

Tel:		+27 - (0)72 808 2975 (w)
Fax:		+27 - (0)21 808 3304
Cell:		+27 - (0)83 9479 042

email:	RKrug at sun.ac.za
      	Rainer at krugs.de


From dominik.grathwohl at rdls.nestle.com  Tue Jul 25 12:48:11 2006
From: dominik.grathwohl at rdls.nestle.com (Grathwohl, Dominik, LAUSANNE,
	NRC-BAS)
Date: Tue, 25 Jul 2006 12:48:11 +0200
Subject: [R] Multiple tests on repeated measurements
Message-ID: <63E04C5ADEDACB4989972239CDABF05782BEEE@chlsne01.nestle.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060725/ba39e501/attachment.pl 

From Ted.Harding at nessie.mcc.ac.uk  Tue Jul 25 13:21:29 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 25 Jul 2006 12:21:29 +0100 (BST)
Subject: [R] Identifying peaks (or offsets) in a time series
In-Reply-To: <3483f8d50607242333vc241997x4cffe08f1eea2fc5@mail.gmail.com>
Message-ID: <XFMail.060725122129.Ted.Harding@nessie.mcc.ac.uk>

On 25-Jul-06 Ulrik Stervbo wrote:
> Could Petr Pikal's peaks function
> (http://finzi.psych.upenn.edu/R/Rhelp02a/archive/33097.html)
> be of any use?
> 
> Ulrik
> 
> On 7/24/06, Tauber, Dr E. <et22 at leicester.ac.uk> wrote:
>> Dear R-users,
>>
>> We are monitoring the activity of animals during a few days
>> period. The data from each animal (crossing of infra-red beam)
>> are collected as a time series (in 30 min bins). An example is
>> attached below.
>>
>> y <-
>> c(0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,3,28,27,46,76,77,60,
>> 19,35,55,59,48,87,20,38,82,62,60,85,105,69,109,102,100,101,
>> 116,126,119,63,27,25,15,8,0,0,3,0,0,3,0,0,5,3,0,0,6,1,29,
>> 73,56,56,57,92,34,51,30,76,30,38,47,87,22,0,68,76,94,101,
>> 119,114,115,111,116,134,125,76,23,19,30,2,8,0,3,0,0,0,7,
>> 0,0,0,0,4,0,7,0,21,4,49,51,56,43,55,55,34,48,16,0,61,22,
>> 94,63,102,47,100,96,113,93,109,123,120,124,115,94,96,76,36,3,
>> 0,0,0,0,0,0,2,5,0,0,0,0,2,10,33,34,15,0,47,22,20,33,52,4,
>> 41,45,0,21,18,38,32,21,78,82,72,102,103,118,116,118,114,82,
>> 18,5,21,4,0,14,0,5,2,0,0,2,2,0,0,3,0,2,7,16,13,17,50,0,
>> 48,16,19,34,39,33,3,67,0,68,34,65,84,61,100,85,108,124,
>> 141,139,134,96,54,91,54,12,0,0,0,0,0,0,0,0,0,0,0,4,11,
>> 0,19,27,15,12,20)
>>
>> We would like to have an automatic way, using R, to identify
>> the time point of offset of each bout of activity (i.e. when
>> activity goes down to a minimum value, for a defined duration).
>> In the example above the offset times (the element number)
>> should be approximately: 53, 99, 146, 191, 239, 283, 330
>> (the last bout of activity can be ignores).
>>
>> Any help or advice will be greatly appreciated,
>>
>> Many thanks, Eran

First of all, I find only 255 values in your 'y' above, so I'm
not sure whether your "53, 99, 146, 191, 239, 283, 330" is
referring to the same series. So this may not be helpful in
interpreting what you mean by "goes down to a minimum value
for a defined duration".

To interpret that clearly, one needs a specified value for
"minimum value" and a specified value for "defined duration".

You can use R to see what happens in the series by methods
such as the following.

You can plot the series (with 24-hour indicators) with

  t0<-c(  0, 48, 96,144,192,240)
  y0<-c(  0,  0,  0,  0,  0,  0)
  y1<-c(140,140,140,140,140,140)
  plot(y)
  lines(y)
  for(i in (1:6)){lines(c(t0[i],t0[i]),c(y0[i],y1[i]),col="green")}

Adopting for illustration "minimum value" = 0,

  Z <- 1*(y==0) ## a series of 0 and 1, Z=1 whenever y=0

  Runs<-rle(Z)  ## "run length encoding" function -- see ?rle
  R.L<-Runs$lengths
  R.V<-Runs$values
  R.L[R.V==1]   ## gives the lengths of runs of "y=0"
## [1]  5 12  2  2  2  2  1  1  3  4
##[11]  1  1  1  6  4  1  1  1  1  2
##[21]  2  1  1  1 11  1

So you can see that available values for "defined duration"
in the series are:

  1 (13 times), 2 (6), 3 (1), 4 (2), 5 (1), 6 (1), 11 (1), 12 (1)

Next, there is clearly an overall 48-point periodicity (which
of course corresponds to a 48*30min = 24-hour periodicity), and
there are 5 complete 48-point segments in your series:

  1:48, 49:96, 97:144, 145:192, 192:240

with a trailing incomplete segment of 15 points. Overall (see
the plot) there are 6 stretches of "very low" activity every
24 hours, so from the above you could think you can obtain these
by choosing "defined duration" = 4 (any run of 0 of length at
least 4 is a "period of very low activity", since there are
2 (=4) + 1 (=5) + 1 (=6) + 1 (=11) + 1 (=12) = 6 of these. But,
as you can see from the plot, this will pick out wrong segments!

This is because the second period of "very low" activity is 0's
broken by relatively frequent positive values.

So you need to raise your threshold of "activity" a bit. Now
you're venturing into "groping" territory -- since the nice
clean "minimum value" = 0 doesn't work, you have to find one
which does.

If you go up to "minimum value = 5", you get:

  Z <- 1*(y<=5); Runs<-rle(Z); R.L<-Runs$lengths; R.V<-Runs$values
  R.L[R.V==1]
## [1] 19 12  1  1  1  5  6  1  1  1
##[11] 14  1  1  1  1  2 12  1  1  1 12  1

which better captures the 6 periods, except that the third one
is split (5,6). Only when you go up to 7 do you finally get

  Z <- 1*(y<=7); Runs<-rle(Z); R.L<-Runs$lengths; R.V<-Runs$values
  R.L[R.V==1]
## [1] 19 14  1  1 14  1  1 14  1  1  1  1  2 13  1  1  1 12  1

and now you can find your 6 "low activity" periods where you
have 19,14,14,14,13,12.

So, in terms of "activity goes down to a minimum value for a
defined period", you need (for this data set) "minimum value"
to be at least 7, and "defined period" could be anything at
most 12 but greater than 2; in this case therefore, for instance,
you can choose say 8 for the latter.

To really locate these periods in the original series, you now
need (say)

  R.L
## [1] 19 32 14 15  1 15  1  1 14  1
##[11]  1  9  1 19 14  4  1  5  1  2
##[21]  1 16  1  1  2  1 13  4  1  6
##[31]  1  1  1 17 12  1  1  5

  ix<-(R.V==1)&(R.L>8) ##(Activity<="min val"=7)&(runlength>8)
  1*ix
## [1]  1  0  1  0  0  0  0  0  1  0
##[11]  0  0  0  0  1  0  0  0  0  0
##[21]  0  0  0  0  0  0  1  0  0  0
##[31]  0  0  0  0  1  0  0  0

  (1-ix)
## [1]  0  1  0  1  1  1  1  1  0  1
##[11]  1  1  1  1  0  1  1  1  1  1
##[21]  1  1  1  1  1  1  0  1  1  1
##[31]  1  1  1  1  0  1  1  1

So "inactivity" is defined by 1*ix==1, and "activity" by (1-ix)==1.

You can now match "inactivity" to the original series using the
"rep" function:

  inact<-rep(1*ix,R.L)

and check this by adding lines to the plot:

  lines(inact,col="blue")

(where the high value "100" corresponds to inactivity).
Now anything you want to find out about the periods of "inactivity"
(as defined above) ccan be determined from the sequence "inact".

However, it is clear that this approach (which is primarily
motivated by trying to clarify your statement of your aims
in terms of the data you supplied) is clearly unlikely to be
useful when you have many such datasets, since you will have
to go into "groping" mode for each dataset, and are likely to
get many different answers over your several datasets.

At this point, I think, it is becoming clear that you need to
think about what you *really" mean by "actvity", and about
how to model it in terms which can be carried over across
datasets.

There is a fundamental point about how your data represent
real activity -- to what extent is there scope for the animal
to be active without breaking the light beam? -- but I'll leave
that one aside.

Taking the 24 hours (48*30min) to run from midnight at the
start of the series, there is an apparent cycle of activity
which, overall, consists of inactivity until somewhere in the
(approximate) time range 06:30-09:00, then an increasing trend
until a maximum which occurs between 18:00 and 23:00, followed by
a rapid decline to a very low level which takes place over some
2-4 hours.

You can get a numerical overview of this variation from day to
day by using commands like

  y[1:48]
  y[(1:48)+48*1]
  y[(1:48)+48*2]
  y[(1:48)+48*3]
  y[(1:48)+48*4]
  y[(1:48)+48*5]

It is tempting to try to model this rise and fall by a mathematical
curve. To the extent that this well represents the initial stage
of the rise, and also the final stage of the decline, you could
then solve that curve for the point at which you decide that
activity starts and ends.

However, it is apparent from the plot that the rising phase, at
least, is not simple. In most cases there is a "midday peak" over
approx 10:00-14:00 (of variable duration) followed by a brief
"siesta", then the sharp rise to the late night peak.

Also, it seems from this dataset that the "day" of this animal
is a bit shorter than 24 hours, since the major peak shifts
steadily from 23:00 on the first day to 19:00 on the 5th day,
i.e. 1 hour per day. In addition, the magnitude of the "midday
peak" decreases steadily over the days, while its distinctiveness
tends to increase. Apparently, there is some progressive factor in
operation which influences activity.

So (and I can't comment on whether such things are intrinsic to
the behaviour of your animal or have been brought about by your
experimental setup) a detailed modelling of the activity cycle
would need to bring such comsiderations into the model. How to
do this, only you can say!

Hoping that this helps!
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 25-Jul-06                                       Time: 12:21:24
------------------------------ XFMail ------------------------------


From kilian.plank at hotmail.de  Tue Jul 25 13:29:16 2006
From: kilian.plank at hotmail.de (Kilian Plank)
Date: Tue, 25 Jul 2006 13:29:16 +0200
Subject: [R] Axis Title in persp() Overlaps with Axis Labels
Message-ID: <BAY117-F6114A1C0ABA9F51B00C92F15A0@phx.gbl>

Good morning,

in a 3D plot based on persp() the axis title (of dimension z) overlaps with 
the axis labels.
How can the distance (between axis labels and axis title) be increased?

I would be grateful if anybody could help me.

Kilian

_________________________________________________________________
Die neue MSN Suche Toolbar mit Windows-Desktopsuche. Suchen Sie gleichzeitig 
im Web, Ihren E-Mails und auf Ihrem PC! Jetzt neu! http://desktop.msn.de/ 
Jetzt gratis downloaden!


From m_osm at gmx.net  Tue Jul 25 14:36:14 2006
From: m_osm at gmx.net (Mahdi Osman)
Date: Tue, 25 Jul 2006 14:36:14 +0200
Subject: [R] cluster analysis of microarray data
In-Reply-To: <mailman.11.1153821603.28940.r-help@stat.math.ethz.ch>
References: <mailman.11.1153821603.28940.r-help@stat.math.ethz.ch>
Message-ID: <20060725123614.213580@gmx.net>

Hi list,

I am interested in cluster analysis of microarray data. The data was generated using cDNA method and a loop design.


I was wondering if any one has a suggestion about which package I can use to analyse such data.


Many thanks in advance

Mahdi
-- 
-----------------------------------
Mahdi Osman (PhD)
E-mail: m_osm at gmx.net
-----------------------------------

Echte DSL-Flatrate dauerhaft f?r 0,- Euro*. Nur noch kurze Zeit!


From penel at biomserv.univ-lyon1.fr  Tue Jul 25 14:25:12 2006
From: penel at biomserv.univ-lyon1.fr (Simon Penel)
Date: Tue, 25 Jul 2006 14:25:12 +0200
Subject: [R] [R-pkgs] seqinr updated :  release 1.0-5
Message-ID: <44C60DA8.6070602@biomserv.univ-lyon1.fr>

Dear R users,


seqinR 1.0-5 has been released yesterday on CRAN, so that the source code
of the package should be available on all CRAN mirrors within the next 24h.

The updated package vignette is here:
http://pbil.univ-lyon1.fr/software/SeqinR/seqinr_1_0-5.pdf

User level visible changes are:

o  A  new function dotPlot() is now available.
   
http://pbil.univ-lyon1.fr/software/SeqinR/SEQINR_CRAN/DOC/html/dotPlot.html

o  A new function crelistfromclientdata() is now available to create a 
list on
   the server from a local file of sequence names, sequence accession 
numbers,
   species names, or keywords names.

http://pbil.univ-lyon1.fr/software/SeqinR/SEQINR_CRAN/DOC/html/crelistfromclientdata.html 


o  A new function pmw() to compute the molecular weight of a protein is now
   available.
   http://pbil.univ-lyon1.fr/software/SeqinR/SEQINR_CRAN/DOC/html/pmw.html

o  A new function reverse.align() contributed by Anamaria Necsulea is now
   available to align CDS at the protein level and then reverse translate
   this at the nucleic acid level from a clustalw output. This can be done
   on the fly if clustalw is available on your platform.

http://pbil.univ-lyon1.fr/software/SeqinR/SEQINR_CRAN/DOC/html/reverse.align.html 


o  An undocumented behavior was reported by Guy Perriere for uco() when
   computing RSCU on sequences where an amino-acid is missing. There is
   now a new argument NA.rscu that allows the user to force the missing
   values to his favorite magic value.
   http://pbil.univ-lyon1.fr/software/SeqinR/SEQINR_CRAN/DOC/html/uco.html

o  There was a bug in read.fasta(): some sequence names were truncated, 
this
   is now fixed (thanks to Marcus G. Daniels for pointing this).
   In order to be more consistent with standard functions such as 
read.table()
   or scan(), the file argument starts now with a lower case letter 
(i.e."file")
   in function read.fasta(), but the old-style "File" is still 
functional for
   forward-compatibility. There is a new logical argument in read.fasta()
   named as.string to allow sequences to be returned as strings instead of
   vector of single characters. The automatic conversion of DNA 
sequences into
   lower case letters can now be disabled with the new logical argument
   forceDNAtolower. It is also possible to disable the automatic attributes
   settings with the new logical argument set.attributes.

http://pbil.univ-lyon1.fr/software/SeqinR/SEQINR_CRAN/DOC/html/read.fasta.html 


o  A new function write.fasta() is now available.

http://pbil.univ-lyon1.fr/software/SeqinR/SEQINR_CRAN/DOC/html/write.fasta.html 


o  The function kaks() now forces character in sequences to upper case.
   This default behavior can be neutralized in order to save time by 
setting the
   argument forceUpperCase to FALSE.
   http://pbil.univ-lyon1.fr/software/SeqinR/SEQINR_CRAN/DOC/html/kaks.html

all the best,

Simon

-- 
Simon Penel
Laboratoire de Biometrie et Biologie Evolutive           
Bat 711  -   CNRS UMR 5558  -    Universite Lyon 1              
43 bd du 11 novembre 1918 69622 Villeurbanne Cedex       
Tel:   04 72 43 29 04      Fax:  04 72 43 13 88
http://pbil.univ-lyon1.fr/members/penel

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From f.harrell at vanderbilt.edu  Tue Jul 25 16:41:53 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 25 Jul 2006 09:41:53 -0500
Subject: [R] Sweave and tth
Message-ID: <44C62DB1.6030505@vanderbilt.edu>

I tried odfWeave to create an OpenOffice file and found that it 
exhausted the memory of my large linux machine and took a long time to 
run.  LaTeX with Sweave is blazing fast and extremely flexible.  Most of 
the time I can give clients a pdf file.  Sometimes I'd like to make 
Sweave LaTeX files more accessible (and editable) to (gulp) Word users, 
mainly so they can extract tables and other pieces of the output.  I 
tried latex2rtf, HeVeA,  and latex2html without much luck.  Has anyone 
been able to get tth to work with Sweave, defining Sweave.sty and other 
needed .sty files to be accepted by tth?

I really appreciate Max Kuhn's efforts with odfWeave and hope to keep up 
with its development.

Thanks.
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From ripley at stats.ox.ac.uk  Tue Jul 25 17:52:17 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 25 Jul 2006 16:52:17 +0100 (BST)
Subject: [R] running jobs in background
In-Reply-To: <44C5D872.7050105@statistik.uni-dortmund.de>
References: <A32055BDEA88C34BB3DBBCD2293807784CB894@iu-mssg-mbx109.ads.iu.edu>
	<44C5D872.7050105@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.64.0607251651010.3268@gannet.stats.ox.ac.uk>

On Tue, 25 Jul 2006, Uwe Ligges wrote:

> Nair, Murlidharan T wrote:
> > Can I run jobs in the background and the check the status of it from
> > time to time in Windows version of R?
> 
> Depends on your version of Windows. I am running the automatical 
> building of R packages in the background on a Windows Server 2003. 
> Unfortunately, if you are logged on, a windows appears when the job is 
> running.
> For other jobs such as simulations, I am always using a Linux machine.
> 

You can with no window using a decent shell -- I use tcsh.exe, and that 
should work on any NT-based version of Windows (and others are very old).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Jul 25 18:10:24 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 25 Jul 2006 17:10:24 +0100 (BST)
Subject: [R] deparse - width.cutoff
In-Reply-To: <20060724194541.48284.qmail@web56209.mail.re3.yahoo.com>
References: <20060724194541.48284.qmail@web56209.mail.re3.yahoo.com>
Message-ID: <Pine.LNX.4.64.0607251652500.3268@gannet.stats.ox.ac.uk>

On Mon, 24 Jul 2006, johan Faux wrote:

> 
> I have a question about "deparse" function in R
> What is the reason that "deparse" use an argument like "width.cutoff" ? 
> Why the maximum cutoff is 500?
> I was manipulating an R formula and used "deparse". Since the length of user's formula was greater then 500, my code didnt work.

Why do you want this all on one line?  deparse does work, just produces 
multiple lines if the output exceeeds the cutoff.

If you do want it, paste the lines together, as e.g. aov() does.

> 
> thanks
> Johan
> 
> johan Faux <johanfaux at yahoo.com> wrote: I have a question about "deparse" function in R
> What is the reason that "deparse" use an argument like "width.cutoff" ? 
> Why the maximum cutoff is 500?
> I was manipulating an R formula and used "deparse". Since the length of user's formula was greater then 500, my code didnt work.
> 
> thanks
> Johan
> 
> 
>     
> 
> ---------------------------------
> 
> 
>  			
> ---------------------------------
> See the all-new, redesigned Yahoo.com.  Check it out.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From dominik.grathwohl at rdls.nestle.com  Tue Jul 25 18:26:32 2006
From: dominik.grathwohl at rdls.nestle.com (Grathwohl, Dominik, LAUSANNE,
	NRC-BAS)
Date: Tue, 25 Jul 2006 18:26:32 +0200
Subject: [R] Multiple tests on 2 way-ANOVA
Message-ID: <63E04C5ADEDACB4989972239CDABF05782BEF3@chlsne01.nestle.com>

Spencer Graves <spencer.graves <at> pdf.com> writes:

> 
> <comments in line>
> 
> Grathwohl, Dominik, LAUSANNE, NRC-BAS wrote:
> > Dear r-helpers,
> > 
> > I have a question about multiple testing.
> > Here an example that puzzles me:
> > All matrixes and contrast vectors are presented in treatment contrasts.
> > 
> > 1. example:
> > library(multcomp)
> > n<-60; sigma<-20
> > # n = sample size per group
> > # sigma standard deviation of the residuals
> > 
> > cov1 <- matrix(c(3/4,-1/2,-1/2,-1/2,1,0,-1/2,0,1), nrow = 3, ncol=3, byrow=TRUE, 
> > 	dimnames = list(c("A", "B", "C"), c("C.1", "C.2", "C.3")))
> > # cov1 = variance covariance matrix of the beta coefficients of a 
> > # 2x2 factorial design (see Piantadosi 2005, p. 509)
> > 
> > cm1 <- matrix(c(0, 1, 0, 0, 0, 1), nrow = 2, ncol=3, byrow=TRUE, 
> > 	dimnames = list(c("A", "B"), c("C.1", "C.2", "C.3")))
> > # cm1 = contrast matrix for main effects
> > 
> > v1 <- csimint(estpar=c(100, 6, 5), df=4*n-3, covm=cov1*sigma^2/n, cmatrix=cm1, conf.level=0.95)
> > summary(v1)
> > 
> > The adjusted p-values are almost the Bonferroni p-values.
> > If I understood right: You need not to adjust for multiple testing 
> > on main effects in a 2x2 factorial design 
> > assuming the absence of interaction. 
> 
> SG:  Where did you get this idea?  A p value of 0.05 says that if the 
> null hypothesis of no effect is true, a result at least as extreme as 
> that observed work actually occur with probability 0.05.  Thus, with 2 
> independent tests, the probability of getting a result at least that 
> extreme in one or both of the tests is 1-(1-0.05)^2 = 0.0975, which is 
> almost 2*0.05.  Thus, if I were to consider only main effects in a 2x2 
> factorial design, this is what I would get from Bonferroni.
> 
How I get this idea? There are two viewpoints on multiple tests on factorial designs (lets restrict to a 2x2 factorial and absence of interaction):
1.) A factorial design, you are doing two trials for the price of one. In more detail: If you investigate a treatment A (present or absent) you can conduct a parallel group design with two groups. If you investigate treatment B, you need to conduct a second parallel group design. For the two parallel group designs no adjustment would have been considered. The factorial design is randomized in a way that investigating the two treatment effects can be seen as independent of each other like the two parallel group designs, so no adjustment is necessary.
2.) A experiment with a factorial design is still one experiment thus controlling experiment wise alpha error for two treatments need to be corrected. Since the treatments are randomized a way that they can be seen as independent, the Bonferroni correction is appropriate. And exactly this is doing the csimint function.
3.) My revised viewpoint: 
A 2x2 factorial design has four groups: 
group 1: non A, non B,
group 2: A, non B,
group 3: non A, B
group 4: A, B
For estimating effect of A as well the effect of B, the "non A, non B" group is involved. So strictly spoken the effects are not completely independent estimated. So csimint is doing a good job.

> > I do not think that there is a bug, 
> > I want to understand, why multcomp does adjust for multiple tests 
> > having all information about the design of the trial (variance covariance matrix)?
> > Or do I have to introduce somehow more information?
> > 
> > 2. example:
> > And I have second question: How do I proper correct for multiple testing 
> > if I want to estimate in the presence of interaction the two average main effects.
> > Can some one point me to some literature where I can learn these things?
> > Here the example, 2x2 factorial with interaction, estimation of average main effects:
> > 
> > cov2 <- matrix(
> > c(1,-1,-1, 1,
> >  -1, 2, 1,-2,
> >  -1, 1, 2,-2,
> >   1,-2,-2, 4)
> > , nrow=4, ncol=4, byrow=TRUE)
> > cm2 <- matrix(c(0, 1, 0, 1/2, 0, 0, 1, 1/2), nrow = 2, ncol=4, byrow=TRUE, 
> > 	dimnames = list(c("A", "B"), c("C.1", "C.2", "C.3", "C.4")))
> > v2 <- csimint(estpar=c(100, 6, 5, 2), df=4*n-4, covm=cov2*sigma^2/n, cmatrix=cm2, conf.level=0.95)
> > summary(v2)
> 
> SG:  The Bonferroni p-value is the observed times the number of rows in 
> the contrast matrix.  The number of columns is irrelevant to Bonferroni.
> 
> SG:  I'm not sure, but I believe that the the adjusted p value would 
> likely be close to (if not exactly) the rank of the contrast matrix; 
> given the rank, it is (I think) independent of the number of rows and 
> columns.
> 
> SG:  These two assertions are consistent with the following example, 
> where I increase the number of dimensions by a factor of 4 without 
> changing the rank.  The Bonferroni p value increased by a factor of 4 
> while the adjusted p value did not change, as predicted.
> 
> cm2.4 <- rbind(cm2, cm2, cm2, cm2)
> v2.4 <- csimint(estpar=c(100, 6, 5, 2), df=4*n-4,
>                covm=cov2*sigma^2/n,
>                cmatrix=cm2.4, conf.level=0.95)
> summary(v2.4)
> > 
> > I do not believe that this is the most efficient way for doing this, 
> > since I made already bad experience with the first example.
> 
> SG:  I hope this reply converts the "bad experience" to "good".  As for 
> efficiency, you did very well by including such simple but elegant 
> examples.  Your post might have more efficiently elicited more and more 
> elegant responses sooner with a more carefully chosen Subject, perhaps 
> like "Multiple Comparisons Questions".  However, the selection of a 
> possible better subject might rely on information you didn't have.
> 
> Hope this helps.
> Spencer Graves
> > 
> > My R.version:
> > 
> > platform i386-pc-mingw32
> > arch     i386           
> > os       mingw32        
> > system   i386, mingw32  
> > status                  
> > major    2              
> > minor    2.1            
> > year     2005           
> > month    12             
> > day      20             
> > svn rev  36812          
> > language R
> > 
> > ______________________________________________
> > R-help <at> stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>
Dear Spencer,

Thank you very much for this detailed answer to my problem.
I pick up the discussion quite late, because I was in holiday.
For this fast moving times, especially within the R-help group, 
a delay for an answer of 10 days is extraordinary. 

Hope for your understanding.

Dominik


From HStevens at MUOhio.edu  Tue Jul 25 18:44:52 2006
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Tue, 25 Jul 2006 12:44:52 -0400
Subject: [R] Citations relevant to lmer methods
Message-ID: <BD5F1CE6-4F5F-4074-A02A-9DF93D805270@MUOhio.edu>

Hi Mixed Modelers,
I was wondering if there are citations relevant to the concerns that  
Bates and others have regarding the inappropriateness of significance  
tests in mixed models that use comparison F-ratios to theoretical F  
distributions. It would just make my life a little easier regarding  
reviewers. Although it is hard to argue with the MCMC approach, it is  
just that it is less common.
Cheers,
Hank


Dr. M. Hank H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"


From Greg.Snow at intermountainmail.org  Tue Jul 25 18:56:37 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 25 Jul 2006 10:56:37 -0600
Subject: [R] RfW 2.3.1: regular expressions to detect pairs of identical
 word-final character sequences
Message-ID: <07E228A5BE53C24CAD490193A7381BBB4B8120@LP-EXCHVS07.CO.IHC.COM>

Using regular expression matching for this case may be overkill (the RE
engine will be doing a lot of backtracking looking at a lot of
non-matches).  Here is an alternative that splits the text into a vector
of words, extracts the last 2 letters of each word (remember if the last
3 letters match, then the last 2 have to match, so we only need to
consider the last 2), then looks at all pairwise comparisons for
matches, then pastes everything back together with the marked matches:

text<-"And this is a second rand  sentence"

tmp1 <- strsplit(text, ' ')[[1]]
tmp2 <- nchar(tmp1)
tmp3 <- substr(tmp1,tmp2-1,tmp2)

tmp4 <- which(lower.tri(diag(length(tmp3))), arr.ind=TRUE)
tmp5 <- tmp3[ tmp4[,1] ] == tmp3[ tmp4[,2] ]

tmp6 <- rep('', length(tmp1))
count <- 1
for( i in which(tmp5) ){
	tmp6[ tmp4[i,1] ] <- paste(tmp6[ tmp4[i,1] ],
'<r',count,'>',sep='')
	tmp6[ tmp4[i,2] ] <- paste(tmp6[ tmp4[i,2] ],
'<r',count,'>',sep='')
	count <- count + 1
}

out.text <- paste( tmp1,tmp6, sep='',collapse=' ')


If you are doing a lot of text processing like this, I would suggest
doing it in Perl rather than R.  S Poetry by Dr. Burns has a function to
take a vector of character strings in R and run a Perl script on it and
return the results.

Hope this helps,




-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Stefan Th. Gries
Sent: Saturday, July 22, 2006 7:49 PM
To: r-help at stat.math.ethz.ch
Subject: [R] RfW 2.3.1: regular expressions to detect pairs of identical
word-final character sequences

Dear all

I use R for Windows 2.3.1 on a fully updated Windows XP Home SP2 machine
and I have two related regular expression problems.

platform       i386-pc-mingw32           
arch           i386                      
os             mingw32                   
system         i386, mingw32             
status                                   
major          2                         
minor          3.1                       
year           2006                      
month          06                        
day            01                        
svn rev        38247                     
language       R                         
version.string Version 2.3.1 (2006-06-01)


I would like to find cases of words in elements of character vectors
that end in the same character sequences; if I find such cases, I want
to add <r> to both potentially rhyming sequences. An example:

INPUT:This is my dog.
DESIRED OUTPUT: This<r> is<r> my dog.

I found a solution for cases where the potentially rhyming words are
adjacent:

text<-"This is my dog."
gsub("(\\w+?)(\\W\\w+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)

However, with another text vector, I came across two problems I cannot
seem to solve and for which I would love to get some input.

(i) While I know what to do for non-adjacent words in general

gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", "This not is my
dog", perl=TRUE) # I know this is not proper English ;-)

this runs into problems with overlapping matches:

text<-"And this is the second sentence"
gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)
[1] "And<r> this is the second<r> sentence"

It finds the "nd" match, but since the "is" match is within the two
"nd"'s, it doesn't get it. Any ideas on how to get all pairwise matches?

(ii) How would one tell R to match only when there are 2+ characters
matching? If the above expression is applied to another character string

text<-"this is an example sentence."
gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)

it also matches the "e"'s at the end of example and sentence. It's not
possible to get rid of that by specifying a range such as {2,}

text<-"this is an example sentence."
gsub("(\\w{2,}?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text,
perl=TRUE)

because, as I understand it, this requires the 2+ cases of \\w to be
identical characters:

text<-"doo yoo see mee?"
gsub("(\\w{2,}?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text,
perl=TRUE)

Again, any ideas?

I'd really appreciate any snippets of codes, pointers, etc.
Thanks so much,
STG
--
Stefan Th. Gries
-----------------------------------------------
University of California, Santa Barbara
http://www.linguistics.ucsb.edu/faculty/stgries

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From msubianto at gmail.com  Tue Jul 25 19:02:17 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Tue, 25 Jul 2006 19:02:17 +0200
Subject: [R] convert decimals to fractions - sorted
Message-ID: <c7c17cef0607251002k6f6005e1h5b717d3fb23ff1b6@mail.gmail.com>

Dear all,
Based on my question a few months ago
https://stat.ethz.ch/pipermail/r-help/2006-January/086952.html
and solved with
https://stat.ethz.ch/pipermail/r-help/2006-January/086955.html
https://stat.ethz.ch/pipermail/r-help/2006-January/086956.html
and from
https://stat.ethz.ch/pipermail/r-help/2006-January/086958.html

frac.fun <- function(x, den){
    dec <- seq(0, den) / den
    nams <- paste(seq(0, den), den, sep = "/")
    sapply(x, function(y) nams[which.min(abs(y - dec))])
}
#######################
frac.fun(c(0, 1, 0.8266667, .066666, 0.2666666), 75)

Now, I have a dataset something like this:

a <-"1 0
    1 0.095238095238095
    1 0.214285714285714
   -1 0.5
    1 0.309523809523810
   -1 0.0476190476190476
    1 0.404761904761905
    1 0.119047619047619
   -1 0.214285714285714
   -1 0.309523809523810
    1 0
    1 0
    1 0.404761904761905
    1 0.095238095238095
    1 0.047619047619047
    1 0.380952380952381
    1 0.214285714285714
    1 0.523809523809524
    1 0
    1 0.095238095238095"

First, I make it as fractions and then sorted.
I have played around to make it sort, but it didn't succes.

df <- read.table(textConnection(a))
library(MASS)
as.fractions(as.numeric(df[,2]))
cbind(table(df[,2], df[,1]), summary(as.factor(df[,2])))
table(frac.fun(as.numeric(df[,2]),42), df[,1])
> table(frac.fun(as.numeric(df[,2]),42), df[,1])

        -1 1
  0/42   0 4
  13/42  1 1
  16/42  0 1
  17/42  0 2
  21/42  1 0
  22/42  0 1
  2/42   1 1
  4/42   0 3
  5/42   0 1
  9/42   1 2
>

How to make the result as sort (to increase) like this,

        -1 1
  0/42   0 4
  2/42   1 1
  4/42   0 3
  5/42   0 1
  9/42   1 2
  13/42  1 1
  16/42  0 1
  17/42  0 2
  21/42  1 0
  22/42  0 1

Thank's for any help.

Best, Muhammad Subianto


From bolker at ufl.edu  Tue Jul 25 19:16:01 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 25 Jul 2006 17:16:01 +0000 (UTC)
Subject: [R] Sweave and tth
References: <44C62DB1.6030505@vanderbilt.edu>
Message-ID: <loom.20060725T191300-286@post.gmane.org>

Frank E Harrell Jr <f.harrell <at> vanderbilt.edu> writes:

> Has anyone 
> been able to get tth to work with Sweave, defining Sweave.sty and other 
> needed .sty files to be accepted by tth?
>
> Thanks.

  I use tth and ttm (Tex to MathML) -- not for Word users,
just for easier web posting.

  I just use a crude little script to translate Sinput/Soutput into
color/verbatim chunks.

#!/bin/sh
for i in `echo $1-[0-9]*.eps | sed -e 's/\.eps//g'`; do
   convert $i.eps $i.png
done
sed -e 's/\\begin{Sinput}/ { \\color{red} \\begin{verbatim}/
        s/\\end{Sinput}/ \\end{verbatim} }/
        s/\\begin{Soutput}/ { \\color{blue} \\begin{verbatim}/
        s/\\end{Soutput}/ \\end{verbatim} }/
        s/\\begin{Schunk}/ /
        s/\\end{Schunk}/ /' $1.tex | \
 tth -e2 -L$1 >$1.html

## notes: a, e, i, o, s, u, y get transmuted by TtH
## when they have " in front of them -- protect with
## verbatim environment!

  Ben Bolker


From ggrothendieck at gmail.com  Tue Jul 25 19:40:47 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 25 Jul 2006 13:40:47 -0400
Subject: [R] RfW 2.3.1: regular expressions to detect pairs of identical
	word-final character sequences
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB4B8120@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBB4B8120@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <971536df0607251040k6d819a21q6d934ce794441c0c@mail.gmail.com>

Regarding having to do a lot of backtracking one can just
look at the relative comparison of speeds and we see
that they are comparable in speed.

In fact the bottleneck is not the backtacking but strapply.
I had coded the regexp version for compactness of code but if we replace
the strapply with custom gsub/strapply code for speed, the new
rexexp version is twice as fast as the for loop version.

Below f1 is the for loop version, f2 is the original regexp version
with strapply and f3 is the revised version using gsub/strsplit instead.

f1 <- function() {
	tmp1 <- strsplit(text, ' ')[[1]]
	tmp2 <- nchar(tmp1)
	tmp3 <- substr(tmp1,tmp2-1,tmp2)

	tmp4 <- which(lower.tri(diag(length(tmp3))), arr.ind=TRUE)
	tmp5 <- tmp3[ tmp4[,1] ] == tmp3[ tmp4[,2] ]

	tmp6 <- rep('', length(tmp1))
	count <- 1
	for( i in which(tmp5) ){
	       tmp6[ tmp4[i,1] ] <- paste(tmp6[ tmp4[i,1] ],
	'<r',count,'>',sep='')
	       tmp6[ tmp4[i,2] ] <- paste(tmp6[ tmp4[i,2] ],
	'<r',count,'>',sep='')
	       count <- count + 1
	}

	out.text <- paste( tmp1,tmp6, sep='',collapse=' ')
}

# places <...> around first occurrences of repeated suffixes

library(gsubfn)
f2 <- function() {
	text <- "And this is the second sentence"

	pat <- "(\\w+)(?=\\b.+\\1\\b)"
	# pat <- "(\\w\\w+)(?=\\b.+\\1\\b)"
	out <- gsub(pat, "\\<\\1\\>", text, perl = TRUE)

	suff <- strapply(out, "<([^>]+)>", function(x,y)y)[[1]]
	gsub(paste("(", paste(suff, collapse = "|"), ")\\b", sep = ""), "\\1<r>", text)
}


f3 <- function() {
	text <- "And this is the second sentence"

	pat <- "(\\w+)(?=\\b.+\\1\\b)"
	# pat <- "(\\w\\w+)(?=\\b.+\\1\\b)"
	out <- gsub(pat, "\\<\\1\\>", text, perl = TRUE)

	# redo this strapply by hand for speed purposes
	# suff <- strapply(out, "<([^>]+)>", function(x,y)y)[[1]]
	suff <- gsub("[^<>]*<|>[^<>]*<|>[^<>]*$", "<", out)
	suff <- gsub("^<|<$", "", suff)
	suff <- strsplit(suff, "<")[[1]]
	gsub(paste("(", paste(suff, collapse = "|"), ")\\b", sep = ""), "\\1<r>", text)
}


# for loop version
system.time(for (i in 1:100) f1())  #  0.32 0.00 0.36   NA   NA

# original regexp version with strapply
system.time(for (i in 1:100) f2()) #  0.36 0.00 0.38   NA   NA

# regexp version with strapply replaced with gsub/strsplit
system.time(for (i in 1:100) f3()) # 0.15 0.00 0.16   NA   NA




On 7/25/06, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
> Using regular expression matching for this case may be overkill (the RE
> engine will be doing a lot of backtracking looking at a lot of
> non-matches).  Here is an alternative that splits the text into a vector
> of words, extracts the last 2 letters of each word (remember if the last
> 3 letters match, then the last 2 have to match, so we only need to
> consider the last 2), then looks at all pairwise comparisons for
> matches, then pastes everything back together with the marked matches:
>
> text<-"And this is a second rand  sentence"
>
> tmp1 <- strsplit(text, ' ')[[1]]
> tmp2 <- nchar(tmp1)
> tmp3 <- substr(tmp1,tmp2-1,tmp2)
>
> tmp4 <- which(lower.tri(diag(length(tmp3))), arr.ind=TRUE)
> tmp5 <- tmp3[ tmp4[,1] ] == tmp3[ tmp4[,2] ]
>
> tmp6 <- rep('', length(tmp1))
> count <- 1
> for( i in which(tmp5) ){
>        tmp6[ tmp4[i,1] ] <- paste(tmp6[ tmp4[i,1] ],
> '<r',count,'>',sep='')
>        tmp6[ tmp4[i,2] ] <- paste(tmp6[ tmp4[i,2] ],
> '<r',count,'>',sep='')
>        count <- count + 1
> }
>
> out.text <- paste( tmp1,tmp6, sep='',collapse=' ')
>
>
> If you are doing a lot of text processing like this, I would suggest
> doing it in Perl rather than R.  S Poetry by Dr. Burns has a function to
> take a vector of character strings in R and run a Perl script on it and
> return the results.
>
> Hope this helps,
>
>
>
>
> --
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at intermountainmail.org
> (801) 408-8111
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Stefan Th. Gries
> Sent: Saturday, July 22, 2006 7:49 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] RfW 2.3.1: regular expressions to detect pairs of identical
> word-final character sequences
>
> Dear all
>
> I use R for Windows 2.3.1 on a fully updated Windows XP Home SP2 machine
> and I have two related regular expression problems.
>
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
>
>
> I would like to find cases of words in elements of character vectors
> that end in the same character sequences; if I find such cases, I want
> to add <r> to both potentially rhyming sequences. An example:
>
> INPUT:This is my dog.
> DESIRED OUTPUT: This<r> is<r> my dog.
>
> I found a solution for cases where the potentially rhyming words are
> adjacent:
>
> text<-"This is my dog."
> gsub("(\\w+?)(\\W\\w+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)
>
> However, with another text vector, I came across two problems I cannot
> seem to solve and for which I would love to get some input.
>
> (i) While I know what to do for non-adjacent words in general
>
> gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", "This not is my
> dog", perl=TRUE) # I know this is not proper English ;-)
>
> this runs into problems with overlapping matches:
>
> text<-"And this is the second sentence"
> gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)
> [1] "And<r> this is the second<r> sentence"
>
> It finds the "nd" match, but since the "is" match is within the two
> "nd"'s, it doesn't get it. Any ideas on how to get all pairwise matches?
>
> (ii) How would one tell R to match only when there are 2+ characters
> matching? If the above expression is applied to another character string
>
> text<-"this is an example sentence."
> gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)
>
> it also matches the "e"'s at the end of example and sentence. It's not
> possible to get rid of that by specifying a range such as {2,}
>
> text<-"this is an example sentence."
> gsub("(\\w{2,}?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text,
> perl=TRUE)
>
> because, as I understand it, this requires the 2+ cases of \\w to be
> identical characters:
>
> text<-"doo yoo see mee?"
> gsub("(\\w{2,}?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text,
> perl=TRUE)
>
> Again, any ideas?
>
> I'd really appreciate any snippets of codes, pointers, etc.
> Thanks so much,
> STG
> --
> Stefan Th. Gries
> -----------------------------------------------
> University of California, Santa Barbara
> http://www.linguistics.ucsb.edu/faculty/stgries
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From HDoran at air.org  Tue Jul 25 19:48:34 2006
From: HDoran at air.org (Doran, Harold)
Date: Tue, 25 Jul 2006 13:48:34 -0400
Subject: [R] Citations relevant to lmer methods
Message-ID: <2323A6D37908A847A7C32F1E3662C80E2766C4@dc1ex01.air.org>

I just sent this to you in a personal response, but for purposes of
archives, the following is one reference:

@book{mccu:sear:2002,
author			={Charles E. McCulloch and Shayle Searle},
year				={2002},
title				={Generalized, Linear, and Mixed
Models},
address			={New York}, 
publisher		={Wiley Interscience}
}

 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin 
> Henry H. Stevens
> Sent: Tuesday, July 25, 2006 12:45 PM
> To: R-Help
> Cc: Josh Banta
> Subject: [R] Citations relevant to lmer methods
> 
> Hi Mixed Modelers,
> I was wondering if there are citations relevant to the 
> concerns that Bates and others have regarding the 
> inappropriateness of significance tests in mixed models that 
> use comparison F-ratios to theoretical F distributions. It 
> would just make my life a little easier regarding reviewers. 
> Although it is hard to argue with the MCMC approach, it is 
> just that it is less common.
> Cheers,
> Hank
> 
> 
> Dr. M. Hank H. Stevens, Assistant Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
> 
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/~stevenmh/
> http://www.muohio.edu/ecology/
> http://www.muohio.edu/botany/
> "E Pluribus Unum"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jfox at mcmaster.ca  Tue Jul 25 20:36:26 2006
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 25 Jul 2006 14:36:26 -0400
Subject: [R] [R-pkgs] Version 0.9-4 of sem to CRAN
Message-ID: <20060725183626.IWTT10262.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Kurt,

I've uploaded a new version (0.9-4) of the sem package to CRAN.

I hope that everything is well with you.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From jz7 at duke.edu  Tue Jul 25 20:59:40 2006
From: jz7 at duke.edu (jz7 at duke.edu)
Date: Tue, 25 Jul 2006 14:59:40 -0400 (EDT)
Subject: [R] warning message when try to plot lm object
In-Reply-To: <Pine.GSO.4.58.0607122121010.9724@godzilla.acpub.duke.edu>
References: <Pine.GSO.4.58.0607122121010.9724@godzilla.acpub.duke.edu>
Message-ID: <Pine.GSO.4.58.0607251454590.8258@godzilla.acpub.duke.edu>

Dear all,

Suppose "h1.lm1" is my multiple regression object. When I tried to use
"plot(h1.lm1)", I got the following warning message.
---------------------
plot(hla4.lm1)
Warning messages:
1: NaNs produced in: sqrt(crit * p * (1 - hh)/hh)
2: NaNs produced in: sqrt(crit * p * (1 - hh)/hh)
---------------------
Does anyone know Where the problem might come from?

Thanks so much!


From Greg.Snow at intermountainmail.org  Tue Jul 25 21:38:50 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 25 Jul 2006 13:38:50 -0600
Subject: [R] RfW 2.3.1: regular expressions to detect pairs of identical
 word-final character sequences
Message-ID: <07E228A5BE53C24CAD490193A7381BBB4FDAB7@LP-EXCHVS07.CO.IHC.COM>

Before comparing times we should make sure that they functions return
the same thing.  My original function (f1 below) labels the potential
rymes with match numbers as well as finding possible rymes, if you just
want the <r> flag then the for loop can be eliminated giving f4 as
follows:

 f4 <- function(text) {
	tmp1 <- strsplit(text, ' ')[[1]]
	tmp2 <- nchar(tmp1)
	tmp3 <- substr(tmp1,tmp2-1,tmp2)

	tmp4 <- which(lower.tri(diag(length(tmp3))), arr.ind=TRUE)
	tmp5 <- tmp3[ tmp4[,1] ] == tmp3[ tmp4[,2] ]

	tmp6 <- rep('', length(tmp1))
	tmp6[ unique(c(tmp4[tmp5,])) ] <- '<r>'
	paste( tmp1,tmp6, sep='',collapse=' ') }

The speed of f4 is similar to the speed of f3 (even after correcting f3,
the original one just returns the original text string).

But that is on the sample string, what if a longer string is used (more
potential for backtracking).

Try the string generated by:

set.seed(1)
text <- paste( sample(c(letters,' ',' ',' '), 1000, replace=T),
collapse='')
text <- gsub(" {2,}"," ",text)

Now f4 is much faster than f3.  However f3 can be optimized by replacing
\\w+ in pat by \\w{2} and that makes it faster than f4 again

It would probably be even faster to use gregexpr to just find the
matching endings then create the new regexp based on those endings and
do one substitute rather than using multiple gsubs.



-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Tuesday, July 25, 2006 11:41 AM
To: Greg Snow
Cc: Stefan Th. Gries; r-help at stat.math.ethz.ch
Subject: Re: [R] RfW 2.3.1: regular expressions to detect pairs of
identical word-final character sequences

Regarding having to do a lot of backtracking one can just look at the
relative comparison of speeds and we see that they are comparable in
speed.

In fact the bottleneck is not the backtacking but strapply.
I had coded the regexp version for compactness of code but if we replace
the strapply with custom gsub/strapply code for speed, the new rexexp
version is twice as fast as the for loop version.

Below f1 is the for loop version, f2 is the original regexp version with
strapply and f3 is the revised version using gsub/strsplit instead.

f1 <- function() {
	tmp1 <- strsplit(text, ' ')[[1]]
	tmp2 <- nchar(tmp1)
	tmp3 <- substr(tmp1,tmp2-1,tmp2)

	tmp4 <- which(lower.tri(diag(length(tmp3))), arr.ind=TRUE)
	tmp5 <- tmp3[ tmp4[,1] ] == tmp3[ tmp4[,2] ]

	tmp6 <- rep('', length(tmp1))
	count <- 1
	for( i in which(tmp5) ){
	       tmp6[ tmp4[i,1] ] <- paste(tmp6[ tmp4[i,1] ],
	'<r',count,'>',sep='')
	       tmp6[ tmp4[i,2] ] <- paste(tmp6[ tmp4[i,2] ],
	'<r',count,'>',sep='')
	       count <- count + 1
	}

	out.text <- paste( tmp1,tmp6, sep='',collapse=' ') }

# places <...> around first occurrences of repeated suffixes

library(gsubfn)
f2 <- function() {
	text <- "And this is the second sentence"

	pat <- "(\\w+)(?=\\b.+\\1\\b)"
	# pat <- "(\\w\\w+)(?=\\b.+\\1\\b)"
	out <- gsub(pat, "\\<\\1\\>", text, perl = TRUE)

	suff <- strapply(out, "<([^>]+)>", function(x,y)y)[[1]]
	gsub(paste("(", paste(suff, collapse = "|"), ")\\b", sep = ""),
"\\1<r>", text) }


f3 <- function() {
	text <- "And this is the second sentence"

	pat <- "(\\w+)(?=\\b.+\\1\\b)"
	# pat <- "(\\w\\w+)(?=\\b.+\\1\\b)"
	out <- gsub(pat, "\\<\\1\\>", text, perl = TRUE)

	# redo this strapply by hand for speed purposes
	# suff <- strapply(out, "<([^>]+)>", function(x,y)y)[[1]]
	suff <- gsub("[^<>]*<|>[^<>]*<|>[^<>]*$", "<", out)
	suff <- gsub("^<|<$", "", suff)
	suff <- strsplit(suff, "<")[[1]]
	gsub(paste("(", paste(suff, collapse = "|"), ")\\b", sep = ""),
"\\1<r>", text) }


# for loop version
system.time(for (i in 1:100) f1())  #  0.32 0.00 0.36   NA   NA

# original regexp version with strapply
system.time(for (i in 1:100) f2()) #  0.36 0.00 0.38   NA   NA

# regexp version with strapply replaced with gsub/strsplit
system.time(for (i in 1:100) f3()) # 0.15 0.00 0.16   NA   NA




On 7/25/06, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
> Using regular expression matching for this case may be overkill (the 
> RE engine will be doing a lot of backtracking looking at a lot of 
> non-matches).  Here is an alternative that splits the text into a 
> vector of words, extracts the last 2 letters of each word (remember if

> the last
> 3 letters match, then the last 2 have to match, so we only need to 
> consider the last 2), then looks at all pairwise comparisons for 
> matches, then pastes everything back together with the marked matches:
>
> text<-"And this is a second rand  sentence"
>
> tmp1 <- strsplit(text, ' ')[[1]]
> tmp2 <- nchar(tmp1)
> tmp3 <- substr(tmp1,tmp2-1,tmp2)
>
> tmp4 <- which(lower.tri(diag(length(tmp3))), arr.ind=TRUE)
> tmp5 <- tmp3[ tmp4[,1] ] == tmp3[ tmp4[,2] ]
>
> tmp6 <- rep('', length(tmp1))
> count <- 1
> for( i in which(tmp5) ){
>        tmp6[ tmp4[i,1] ] <- paste(tmp6[ tmp4[i,1] ],
> '<r',count,'>',sep='')
>        tmp6[ tmp4[i,2] ] <- paste(tmp6[ tmp4[i,2] ],
> '<r',count,'>',sep='')
>        count <- count + 1
> }
>
> out.text <- paste( tmp1,tmp6, sep='',collapse=' ')
>
>
> If you are doing a lot of text processing like this, I would suggest 
> doing it in Perl rather than R.  S Poetry by Dr. Burns has a function 
> to take a vector of character strings in R and run a Perl script on it

> and return the results.
>
> Hope this helps,
>
>
>
>
> --
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at intermountainmail.org
> (801) 408-8111
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Stefan Th. 
> Gries
> Sent: Saturday, July 22, 2006 7:49 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] RfW 2.3.1: regular expressions to detect pairs of 
> identical word-final character sequences
>
> Dear all
>
> I use R for Windows 2.3.1 on a fully updated Windows XP Home SP2 
> machine and I have two related regular expression problems.
>
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
>
>
> I would like to find cases of words in elements of character vectors 
> that end in the same character sequences; if I find such cases, I want

> to add <r> to both potentially rhyming sequences. An example:
>
> INPUT:This is my dog.
> DESIRED OUTPUT: This<r> is<r> my dog.
>
> I found a solution for cases where the potentially rhyming words are
> adjacent:
>
> text<-"This is my dog."
> gsub("(\\w+?)(\\W\\w+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, 
> perl=TRUE)
>
> However, with another text vector, I came across two problems I cannot

> seem to solve and for which I would love to get some input.
>
> (i) While I know what to do for non-adjacent words in general
>
> gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", "This not is my 
> dog", perl=TRUE) # I know this is not proper English ;-)
>
> this runs into problems with overlapping matches:
>
> text<-"And this is the second sentence"
> gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)

> [1] "And<r> this is the second<r> sentence"
>
> It finds the "nd" match, but since the "is" match is within the two 
> "nd"'s, it doesn't get it. Any ideas on how to get all pairwise
matches?
>
> (ii) How would one tell R to match only when there are 2+ characters 
> matching? If the above expression is applied to another character 
> string
>
> text<-"this is an example sentence."
> gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)
>
> it also matches the "e"'s at the end of example and sentence. It's not

> possible to get rid of that by specifying a range such as {2,}
>
> text<-"this is an example sentence."
> gsub("(\\w{2,}?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text,
> perl=TRUE)
>
> because, as I understand it, this requires the 2+ cases of \\w to be 
> identical characters:
>
> text<-"doo yoo see mee?"
> gsub("(\\w{2,}?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text,
> perl=TRUE)
>
> Again, any ideas?
>
> I'd really appreciate any snippets of codes, pointers, etc.
> Thanks so much,
> STG
> --
> Stefan Th. Gries
> -----------------------------------------------
> University of California, Santa Barbara 
> http://www.linguistics.ucsb.edu/faculty/stgries
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From f.harrell at vanderbilt.edu  Tue Jul 25 21:45:36 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 25 Jul 2006 14:45:36 -0500
Subject: [R] Sweave and tth
In-Reply-To: <loom.20060725T191300-286@post.gmane.org>
References: <44C62DB1.6030505@vanderbilt.edu>
	<loom.20060725T191300-286@post.gmane.org>
Message-ID: <44C674E0.7020503@vanderbilt.edu>

Ben Bolker wrote:
> Frank E Harrell Jr <f.harrell <at> vanderbilt.edu> writes:
> 
>> Has anyone 
>> been able to get tth to work with Sweave, defining Sweave.sty and other 
>> needed .sty files to be accepted by tth?
>>
>> Thanks.
> 
>   I use tth and ttm (Tex to MathML) -- not for Word users,
> just for easier web posting.
> 
>   I just use a crude little script to translate Sinput/Soutput into
> color/verbatim chunks.
> 
> #!/bin/sh
> for i in `echo $1-[0-9]*.eps | sed -e 's/\.eps//g'`; do
>    convert $i.eps $i.png
> done
> sed -e 's/\\begin{Sinput}/ { \\color{red} \\begin{verbatim}/
>         s/\\end{Sinput}/ \\end{verbatim} }/
>         s/\\begin{Soutput}/ { \\color{blue} \\begin{verbatim}/
>         s/\\end{Soutput}/ \\end{verbatim} }/
>         s/\\begin{Schunk}/ /
>         s/\\end{Schunk}/ /' $1.tex | \
>  tth -e2 -L$1 >$1.html
> 
> ## notes: a, e, i, o, s, u, y get transmuted by TtH
> ## when they have " in front of them -- protect with
> ## verbatim environment!
> 
>   Ben Bolker

Thanks Ben.  That works great.  Word can open the resulting html file.
Frank


From dgerlanc at gmail.com  Tue Jul 25 21:48:00 2006
From: dgerlanc at gmail.com (Daniel Gerlanc)
Date: Tue, 25 Jul 2006 15:48:00 -0400
Subject: [R] Follow Up To: Splitting the left and right hand terms of a
	formula
Message-ID: <84c9e3cb0607251248g76259345k22cd8da81cc62b74@mail.gmail.com>

Hi All,

I sent the following message to R-help on July 14th, 2006:

Let's say I have the following formula:

a.formula <- x ~ y + z

I want to extract the left and right-hand sides of the function so
that I have two character vectors like the ones you would create using
the following assignments:

left.hand.side <- "x"
right.hand.side <- c("y", "z")

One way to do this follows:

left.hand.side <- unlist(dimnames(attr(terms(a.formula), "factors"))[1])
right.hand.side <- unlist(dimnames(attr(terms(a.formula), "factors"))[-1])

Is there a better or cleaner way to do this?

I got one reply to try this (thanks Gabor!):

> all.vars(update(a.formula, .~0))
[1] "x"

> all.vars(update(a.formula, 0~.))
[1] "y" "z"

This works, but seems a bit of a hack.

There are two methods, "getCovariateFormula", and "getCovariate,"
which return, respectively, objects of class "formula" for the left
and right hand sides of the formula.  To get the right hand side or
left hand side I wrap one of these method calls in a call to
"all.vars".  Is there a better way to get the left or right hand side
of the formula, or is this it?

Thanks!

Dan Gerlanc
Williams College


From Peter.Eiger at gmx.net  Tue Jul 25 21:50:52 2006
From: Peter.Eiger at gmx.net (Peter Eiger)
Date: Tue, 25 Jul 2006 21:50:52 +0200
Subject: [R] [RODBC] ERROR: Could not SQLExecDirect
Message-ID: <20060725195052.113090@gmx.net>

Hi,

I've got a problem with RODBC and saving (sqlSave) of a dataframe in Access.
R 2.0.1 is running on windows XP.

When executing the examples in R help for the "USArrests" data set "sqlSAve" works fine, but running sqlSave() for a dataframe "Adat" 

> str(Adat)
`data.frame':   1202 obs. of  18 variables: 

containing 18 columns and ca. 1200 rows fails.

I get the following error message:

> sqlSave(channel, Adat)
Error in sqlSave(channel, Adat) : [RODBC] ERROR: Could not SQLExecDirect

The data was fetched from the same Access database before and was not manipulated before the attempt to save.
What's wrong?

Any advice is welcome!
Peter

-- 


"Feel free" ? 10 GB Mailbox, 100 FreeSMS/Monat ...


From p.murrell at auckland.ac.nz  Tue Jul 25 22:02:12 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 26 Jul 2006 08:02:12 +1200
Subject: [R] Axis Title in persp() Overlaps with Axis Labels
In-Reply-To: <BAY117-F6114A1C0ABA9F51B00C92F15A0@phx.gbl>
References: <BAY117-F6114A1C0ABA9F51B00C92F15A0@phx.gbl>
Message-ID: <44C678C4.4000608@stat.auckland.ac.nz>

Hi


Kilian Plank wrote:
> Good morning,
> 
> in a 3D plot based on persp() the axis title (of dimension z) overlaps with 
> the axis labels.
> How can the distance (between axis labels and axis title) be increased?


Unfortunately, there is not the same level of control that is available
for 2D plots, but you can "cheat" a bit in this case.  For example, ...

     x <- seq(-10, 10, length= 30)
     y <- x
     f <- function(x,y) { r <- sqrt(x^2+y^2); 10 * sin(r)/r }
     z <- outer(x, y, f)
     z[is.na(z)] <- 1

     par(mfrow=c(2, 1))
     persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
           col = "lightblue", ticktype="detailed")

     persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
           col = "lightblue", ticktype="detailed",
           zlab="\n\n\n\nz")

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From adrian_d at eskimo.com  Tue Jul 25 22:12:09 2006
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Tue, 25 Jul 2006 13:12:09 -0700 (PDT)
Subject: [R] greek letters, text, and values in labels
Message-ID: <Pine.SUN.4.58.0607251302550.2754@eskimo.com>


Hello,

I want to have a title that will look something like:
"Results for \theta=2.1", given that I have a variable theta=2.1, and
\theta should show on the screen like the greek letter.

I've tried a lot of things:
theta <- 2.1
plot(1:10, main=expression(paste("Results for", theta, "=", eval(theta))))

or using bquote
plot(1:10, main=paste("Results for ", bquote(theta == .(theta))))

or using substitute, etc.  I could not make it work.  This should be easy.

I would appreciate your help.

Thanks,
Adrian


From vincent.goulet at act.ulaval.ca  Tue Jul 25 22:26:37 2006
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Tue, 25 Jul 2006 16:26:37 -0400
Subject: [R] Follow Up To: Splitting the left and right hand terms of a
	formula
In-Reply-To: <84c9e3cb0607251248g76259345k22cd8da81cc62b74@mail.gmail.com>
References: <84c9e3cb0607251248g76259345k22cd8da81cc62b74@mail.gmail.com>
Message-ID: <200607251626.37517.vincent.goulet@act.ulaval.ca>

Le Mardi 25 Juillet 2006 15:48, Daniel Gerlanc a ?crit?:
> Hi All,
>
> I sent the following message to R-help on July 14th, 2006:
>
> Let's say I have the following formula:
>
> a.formula <- x ~ y + z
>
> I want to extract the left and right-hand sides of the function so
> that I have two character vectors like the ones you would create using
> the following assignments:
>
> left.hand.side <- "x"
> right.hand.side <- c("y", "z")
>
> One way to do this follows:
>
> left.hand.side <- unlist(dimnames(attr(terms(a.formula), "factors"))[1])
> right.hand.side <- unlist(dimnames(attr(terms(a.formula), "factors"))[-1])
>
> Is there a better or cleaner way to do this?
>
> I got one reply to try this (thanks Gabor!):
> > all.vars(update(a.formula, .~0))
>
> [1] "x"
>
> > all.vars(update(a.formula, 0~.))
>
> [1] "y" "z"
>
> This works, but seems a bit of a hack.
>
> There are two methods, "getCovariateFormula", and "getCovariate,"
> which return, respectively, objects of class "formula" for the left
> and right hand sides of the formula.  To get the right hand side or
> left hand side I wrap one of these method calls in a call to
> "all.vars".  Is there a better way to get the left or right hand side
> of the formula, or is this it?

The following should help you:

> f <- x ~ y + z
> class(f)
[1] "formula"
> str(f)
Class 'formula' length 3 x ~ y + z
  ..- attr(*, ".Environment")=length 56 <environment> 
> f[[1]]
`~`
> f[[2]]
x
> f[[3]]
y + z
> all.vars(f[[2]])
[1] "x"
> all.vars(f[[3]])
[1] "y" "z"

If needed, the vertical bar | is also recognized as a separation symbol:

> f <- ~ x | y + z
> f[[2]]
x | y + z
> f[[2]][[2]]
x
> f[[2]][[3]]
y + z


Bye!    Vincent

-- 
  Vincent Goulet, Associate Professor
  ?cole d'actuariat
  Universit? Laval, Qu?bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca


From Charles.Annis at StatisticalEngineering.com  Tue Jul 25 22:36:21 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Tue, 25 Jul 2006 16:36:21 -0400
Subject: [R] greek letters, text, and values in labels
In-Reply-To: <Pine.SUN.4.58.0607251302550.2754@eskimo.com>
Message-ID: <05e801c6b029$fd9fe4e0$6600a8c0@DD4XFW31>

This'll work.

theta <- 2.1

plot(NA, xlim=c(0,1), ylim=c(0,1), xlab=bquote(theta == .(theta)),
ylab=bquote(theta == .(theta)), main=bquote(paste("Results for ",theta ==
.(theta))))





Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Adrian Dragulescu
Sent: Tuesday, July 25, 2006 4:12 PM
To: r-help at stat.math.ethz.ch
Subject: [R] greek letters, text, and values in labels


Hello,

I want to have a title that will look something like:
"Results for \theta=2.1", given that I have a variable theta=2.1, and
\theta should show on the screen like the greek letter.

I've tried a lot of things:
theta <- 2.1
plot(1:10, main=expression(paste("Results for", theta, "=", eval(theta))))

or using bquote
plot(1:10, main=paste("Results for ", bquote(theta == .(theta))))

or using substitute, etc.  I could not make it work.  This should be easy.

I would appreciate your help.

Thanks,
Adrian

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From mschwartz at mn.rr.com  Tue Jul 25 22:37:58 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 25 Jul 2006 15:37:58 -0500
Subject: [R] greek letters, text, and values in labels
In-Reply-To: <Pine.SUN.4.58.0607251302550.2754@eskimo.com>
References: <Pine.SUN.4.58.0607251302550.2754@eskimo.com>
Message-ID: <1153859878.9352.7.camel@localhost.localdomain>

On Tue, 2006-07-25 at 13:12 -0700, Adrian Dragulescu wrote:
> Hello,
> 
> I want to have a title that will look something like:
> "Results for \theta=2.1", given that I have a variable theta=2.1, and
> \theta should show on the screen like the greek letter.
> 
> I've tried a lot of things:
> theta <- 2.1
> plot(1:10, main=expression(paste("Results for", theta, "=", eval(theta))))
> 
> or using bquote
> plot(1:10, main=paste("Results for ", bquote(theta == .(theta))))
> 
> or using substitute, etc.  I could not make it work.  This should be easy.
> 
> I would appreciate your help.
> 
> Thanks,
> Adrian

Adrian,

Try this:

  theta <- 2.1
  plot(1:10, main = bquote(paste("Results For: ", theta == .(theta))))

You need to surround the full expression with bquote() so that the
paste()d text is within it. bquote() then returns an expression that is
passed to plotmath.

HTH,

Marc Schwartz


From ggrothendieck at gmail.com  Tue Jul 25 22:41:09 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 25 Jul 2006 16:41:09 -0400
Subject: [R] greek letters, text, and values in labels
In-Reply-To: <Pine.SUN.4.58.0607251302550.2754@eskimo.com>
References: <Pine.SUN.4.58.0607251302550.2754@eskimo.com>
Message-ID: <971536df0607251341l4464decfua7da3695239711cc@mail.gmail.com>

Try:

   plot(1:10, main = bquote("Results for" ~ theta == .(theta)))


On 7/25/06, Adrian Dragulescu <adrian_d at eskimo.com> wrote:
>
> Hello,
>
> I want to have a title that will look something like:
> "Results for \theta=2.1", given that I have a variable theta=2.1, and
> \theta should show on the screen like the greek letter.
>
> I've tried a lot of things:
> theta <- 2.1
> plot(1:10, main=expression(paste("Results for", theta, "=", eval(theta))))
>
> or using bquote
> plot(1:10, main=paste("Results for ", bquote(theta == .(theta))))
>
> or using substitute, etc.  I could not make it work.  This should be easy.
>
> I would appreciate your help.
>
> Thanks,
> Adrian
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bolker at ufl.edu  Tue Jul 25 23:11:22 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 25 Jul 2006 21:11:22 +0000 (UTC)
Subject: [R] Axis Title in persp() Overlaps with Axis Labels
References: <BAY117-F6114A1C0ABA9F51B00C92F15A0@phx.gbl>
	<44C678C4.4000608@stat.auckland.ac.nz>
Message-ID: <loom.20060725T230835-517@post.gmane.org>

Paul Murrell <p.murrell <at> auckland.ac.nz> writes:

> 
> Hi
> 
> Kilian Plank wrote:
> > Good morning,
> > 
> > in a 3D plot based on persp() the axis title (of dimension z) overlaps with 
> > the axis labels.
> > How can the distance (between axis labels and axis title) be increased?
> 

> Paul

  Another way to do it: get the perspective matrix
back from persp() and use trans3d() to redo essentially
the same calculations that persp() does to decide where
to put the label:

x <- seq(-10, 10, length= 30)
y <- x
f <- function(x,y) { r <- sqrt(x^2+y^2); 10 * sin(r)/r }
z <- outer(x, y, f)
z[is.na(z)] <- 1

par(mfrow=c(2, 2))
persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
      col = "lightblue", ticktype="detailed")

persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
      col = "lightblue", ticktype="detailed",
      zlab="\n\n\n\nz")

p1 <- persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
      col = "lightblue", ticktype="detailed",zlab="")

ranges <- t(sapply(list(x,y,z),range))
means <- rowMeans(ranges)

## label offset distance, as a fraction of the plot width
labelspace <- 0.12  ## tweak this until you like the result

xpos <- min(x)-(diff(range(x)))*labelspace
ypos <- min(y)-(diff(range(y)))*labelspace
labelbot3d <- c(xpos,ypos,min(z))
labeltop3d <- c(xpos,ypos,max(z))
labelmid3d <- c(xpos,ypos,mean(range(z)))

trans3dfun <- function(v) { trans3d(v[1],v[2],v[3],p1) }
labelbot2d <- trans3dfun(labelbot3d)
labelmid2d <- trans3dfun(labelmid3d)
labeltop2d <- trans3dfun(labeltop3d)
labelang <- 180/pi*atan2(labeltop2d$y-labelbot2d$y,labeltop2d$x-labelbot2d$x)
par(xpd=NA,srt=labelang)  ## disable clipping and set string rotation
text(labelmid2d[1]$x,labelmid2d$y,"z label")


From sdavis2 at mail.nih.gov  Tue Jul 25 23:16:02 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 25 Jul 2006 17:16:02 -0400
Subject: [R] [Way OT] New hardware
Message-ID: <C0EC0252.E077%sdavis2@mail.nih.gov>

Can anyone share experience with opteron versus the xeon (woodcrest) for R
under linux?  I am looking at using 16-32Gb of ram in a workstation (as
opposed to a server).

Thanks in advance....

Sean


From kantanantha at hotmail.com  Tue Jul 25 23:44:57 2006
From: kantanantha at hotmail.com (Nantachai Kantanantha)
Date: Tue, 25 Jul 2006 17:44:57 -0400
Subject: [R] how to fit with "lme" function
Message-ID: <BAY106-F27EEE8ACB5CB02EE4E1830A25A0@phx.gbl>

Hi everone,

I have a question on using lme on a mixed effects model.

The linear mixed model is in the form of:

y = bX +Zu + e

where "X" and "Z" are the matrices, "b" is the coefficient vector of fixed 
effects, "u" is the coefficient vector of random effects, and e is an error 
vector.

I would like to use "lme" function to fit the model and estimate "b" and 
"u".
However, there is an error when I call the lme function:

fit = lme(y ~ int + X1 + X2 + X3, data = data.fr, random = ~ Z1 + Z2 + Z3 + 
Z4 + Z5)

where:
int is a vector of 1 in matrix X, used for the intercept term "b0",
X1, X2, X3 are columns of matrix X,
Z1 to Z5 are columns of matrix Z,
data.fr = data.frame(y,X,Z).

I got the error message:

Error in getGroups.data.frame(dataMix, groups) :
        Invalid formula for groups

Could you please suggest me know to solve this problem?
Do I use incorrect arguments?

Thank you very much.
Nantachai


From f.harrell at vanderbilt.edu  Wed Jul 26 00:19:38 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 25 Jul 2006 17:19:38 -0500
Subject: [R] Tips on creating html reports from Sweave LaTeX documents
Message-ID: <44C698FA.5030703@vanderbilt.edu>

Thanks to Ben Bolker, I have put up the following page with tips and 
linux/unix tools for creating html reports from Sweave:

http://biostat.mc.vanderbilt.edu/SweaveConvert

Reports can be opened in Word, or you can copy and paste from a browser 
into OpenOffice or Word, keeping the formatting.  Improvements are welcomed.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From nayeemquayum at gmail.com  Wed Jul 26 00:27:02 2006
From: nayeemquayum at gmail.com (Nayeem Quayum)
Date: Tue, 25 Jul 2006 16:27:02 -0600
Subject: [R] Normalization problem
Message-ID: <af6d2ccc0607251527s3a5afba0wb0a927f43cf78283@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060725/6b943301/attachment.pl 

From bird.david at uqam.ca  Wed Jul 26 00:43:44 2006
From: bird.david at uqam.ca (david bird)
Date: Tue, 25 Jul 2006 15:43:44 -0700 (PDT)
Subject: [R] test regression against given slope for reduced major axis
 regression (RMA)
In-Reply-To: <87odvs7axf.fsf@pdrechsler.de>
References: <87lkr1osjm.fsf@pdrechsler.de> <87odvs7axf.fsf@pdrechsler.de>
Message-ID: <5494654.post@talk.nabble.com>


Patrick Drechsler wrote on 11 Jul 2006 02:10:21 MET: 

[...] 
> I am now confronted with the problem that I have data which 
> requires a modelII regression (also called reduced major axes 
> regression (RMA) or geometric mean regression). For this I use 
> the function "modelII" (see below). 
> 
> What would be a good way of adapting 
> "test_regression_against_slope" for use with RMA regression? 
> 
> The question I am trying to answer is: "Does the slope acquired 
> from experimental data differ significantly from theoretical 
> predictions?" 

JFTR: David Warton's "smatr" package solves the problem. 


DFB: Note that there are conflicting solutions to this question. The
confidence limits based on Pitman, EJG (1939) "A note on normal
correlation".  Biometrika 31: 9-12, as implemented in the smatr package, are 
liberal (i.e. narrow), compared to the alternative limits found in Tan, CY
and B. Iglewicz (1999) Measurement-methods comparisons and linear
statistical relationship, Technometrics 41: 192-201.

Note also that "equation error", if present, invalidates any confidence
limits calculations. That is, in using the RMA, you are looking for the
perfect underlying relationship between two variables. If the underlying
relationship is not perfect, because missing variables are also important,
the RMA is the wrong approach.
-- 
View this message in context: http://www.nabble.com/test-regression-against-given-slope-for-reduced-major-axis-regression-%28RMA%29-tf1921881.html#a5494654
Sent from the R help forum at Nabble.com.


From franco.mendolia at gmx.de  Wed Jul 26 00:48:42 2006
From: franco.mendolia at gmx.de (Franco Mendolia)
Date: Wed, 26 Jul 2006 00:48:42 +0200
Subject: [R] command completion in R-WinEdt
Message-ID: <44C69FCA.9070305@gmx.de>

Hello!

Is there any possibility to use command completion in R-WinEdt?

Thanks
Franco


From HDoran at air.org  Wed Jul 26 00:51:34 2006
From: HDoran at air.org (Doran, Harold)
Date: Tue, 25 Jul 2006 18:51:34 -0400
Subject: [R] how to fit with "lme" function
References: <BAY106-F27EEE8ACB5CB02EE4E1830A25A0@phx.gbl>
Message-ID: <2323A6D37908A847A7C32F1E3662C80E04E5A4@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060725/9b2e09af/attachment.pl 

From loki at math.ucsd.edu  Wed Jul 26 00:52:01 2006
From: loki at math.ucsd.edu (Loki Natarajan)
Date: Tue, 25 Jul 2006 15:52:01 -0700 (PDT)
Subject: [R] HELP with NLME
Message-ID: <Pine.GSO.4.64.0607251534400.3053@euclid.ucsd.edu>


Hi,

I was very much hoping someone could help me with the following.
I am trying to convert some SAS NLMIXED code to NLME in R (v.2.1),
but I get an error message. Does anyone have any suggestions?
I think my error is with the random effect "u" which seems to be
parametrized differently in the SAS code. In case it's helpful, 
what I am essentially trying to do is estimate parameters using ML in a 
measurement error setting with some validation data (indicated by 
vs.flag). Any help would be greatly appreciated.I apologize for the
clumsiness of the R code.
Many thanks in advance.

Sincerely,
Loki
#################################################################
SAS Code:

proc nlmixed data=repdat
parms b0 -3 b1 -.135 a0 3 a1 4 sigsq 0.25;

if vs.flag = 1 then do;
    eta1 = b0 + b1*lnbldT;
    llbin = anybc.cens.ind*eta1 - log(1+exp(eta1));
    eta2 = a0 + a1*lnndsTs;
    llnorm = -1/(2*sigsq)*(lnbldT - eta2)**2 - .5*log(sigsq);
    ll = llbin + llnorm;
    end;

else do;
    eta2 = a0 + a1*lnndsTs;
    eta1 = b0 + b1*eta2 + u;
    llbin = anybc.cens.ind*eta1 - log(1+exp(eta1));
    ll = llbin;
    end;

sigma2 = sigmasq*b1**2 /*variance of random effect;
model anybc.cens.indic ~ general(ll);
random u ~ normal(0,sigma2) subject = CaseID; */;
run;
####################################################################
R Code with error message:

me.km.nlme <- nlme(model = anybc.cens.indic ~ 
vs.flag*((anybc.cens.indic*(b0+b1*lnbldT) - log(1 + exp(b0+b1*lnbldT))) + 
(((-1/(2*sigsq))*(lnbldT -a0 -a1*lnndsTs)^2) - 0.5*log(sigsq))) + 
(1-vs.flag)*((anybc.cens.indic*(b0+b1*(a0 + a1*lnndsTs + u))) - log(1 + 
exp(b0+b1*(a0 + a1*lnndsTs + u)))), fixed = 
list(a0~1,a1~1,b0~1,b1~1,sigsq~1),na.action=na.omit, data=rc.df, 
method="ML", random=u~1|CaseID,
start = c(a0=0, a1=1, b0=-3, b1 = -0.135, sigsq = 0.25))

+ Error: Singularity in backsolve at level 0, block 1
In addition: There were 16 warnings (use warnings() to see them)
> >
>
> warnings()
Warning messages:
1: Singular precision matrix in level -1, block 5
2: Singular precision matrix in level -1, block 5
3: Singular precision matrix in level -1, block 5
4: NA/Inf replaced by maximum positive value
5: Singular precision matrix in level -1, block 5
6: Singular precision matrix in level -1, block 5
7: Singular precision matrix in level -1, block 5
8: NA/Inf replaced by maximum positive value
9: NaNs produced in: log(x)
10: NaNs produced in: log(x)
11: NaNs produced in: log(x)
12: NaNs produced in: log(x)
13: NaNs produced in: log(x)
14: NaNs produced in: log(x)
15: NaNs produced in: log(x)
16: NaNs produced in: log(x)
>
#####################################################################


Loki Natarajan
Associate Professor of Biostatistics
Moores UCSD Cancer Center
3855 Health Sciences Drive #0901
La Jolla, CA 92093-0901


phone:     858 822 4763
Fax:       858 822 6897


From mblanche at berkeley.edu  Wed Jul 26 01:45:22 2006
From: mblanche at berkeley.edu (Marco Blanchette)
Date: Tue, 25 Jul 2006 16:45:22 -0700
Subject: [R] Drosophila Genome 2.0 annaffy annotation
Message-ID: <C0EBFB22.29BF%mblanche@berkeley.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060725/2a50d8cf/attachment.pl 

From john_d_mchenry at yahoo.com  Wed Jul 26 02:08:04 2006
From: john_d_mchenry at yahoo.com (John McHenry)
Date: Tue, 25 Jul 2006 17:08:04 -0700 (PDT)
Subject: [R] Overplotting: plot() invocation looks ugly ... suggestions?
In-Reply-To: <971536df0607241908j12f4ce47yc390638e6e31a077@mail.gmail.com>
Message-ID: <20060726000804.35678.qmail@web35414.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060725/c3bb39e5/attachment.pl 

From sfalcon at fhcrc.org  Wed Jul 26 02:31:39 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 25 Jul 2006 17:31:39 -0700
Subject: [R] Drosophila Genome 2.0 annaffy annotation
In-Reply-To: <C0EBFB22.29BF%mblanche@berkeley.edu> (Marco Blanchette's message
	of "Tue, 25 Jul 2006 16:45:22 -0700")
References: <C0EBFB22.29BF%mblanche@berkeley.edu>
Message-ID: <m2k661jjf8.fsf@ziti.local>

Hi Marco,

I'm pretty sure you want to resend your question to the Bioconductor
mailing list.  AFAICT your question is rather BioC specific and you
will likely get a more helpful response there.

+ seth


From sfalcon at fhcrc.org  Wed Jul 26 02:36:53 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 25 Jul 2006 17:36:53 -0700
Subject: [R] cluster analysis of microarray data
In-Reply-To: <20060725123614.213580@gmx.net> (Mahdi Osman's message of "Tue,
	25 Jul 2006 14:36:14 +0200")
References: <mailman.11.1153821603.28940.r-help@stat.math.ethz.ch>
	<20060725123614.213580@gmx.net>
Message-ID: <m2fygpjj6i.fsf@ziti.local>

"Mahdi Osman" <m_osm at gmx.net> writes:

> Hi list,
>
> I am interested in cluster analysis of microarray data. The data was
> generated using cDNA method and a loop design.
>
>
> I was wondering if any one has a suggestion about which package I can
> use to analyse such data.

There are many packages within the Bioconductor project that provide
tools for analysis of microarray data.

I would start by taking a look at the Microarray and TwoChannel
BiocViews:

http://bioconductor.org/packages/1.8/Microarray.html
http://bioconductor.org/packages/1.8/TwoChannel.html

+ seth


From ggrothendieck at gmail.com  Wed Jul 26 03:05:32 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 25 Jul 2006 21:05:32 -0400
Subject: [R] RfW 2.3.1: regular expressions to detect pairs of identical
	word-final character sequences
In-Reply-To: <971536df0607222105v742027c0p4e1a79d61f6194ca@mail.gmail.com>
References: <21027919.1153619327251.JavaMail.ngmail@webmail18>
	<971536df0607222105v742027c0p4e1a79d61f6194ca@mail.gmail.com>
Message-ID: <971536df0607251805u3ae8b48q80a813784ec86202@mail.gmail.com>

Here is yet another solution.  This one consists only of
two gsubs and a function to reverse a string.  It runs
at about the same speed as f3 but its main advantage
is how compact it is.

pat could be the same as before however we have made
use of Greg's discussion to use \\w\\w to
avail ourself of his speedup idea.  If single letter
endings are ok use \\w instead of \\w\\w.
This time the first gsub simply appends <r> to the first in any
duplicated ending.  Then we reverse the string.
In the second gsub we look for any sequence at the
start of a word for which >r< followed by that sequence
is found later in the string and prepend >r< to that.
Finally we reverse the result.

text <- "And this is the second sentence"
strrev <- function(x) paste(rev(strsplit(x, "")[[1]]), collapse = "")

pat <- "(\\w\\w)(?=\\b.+\\1\\b)"
out <- strrev(gsub(pat, "\\1\\<r>", text, perl = TRUE))
strrev(gsub("\\b(\\w+)(?=.*>r<\\1)", ">r<\\1", out, perl = TRUE))


On 7/23/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> The following requires more than just a single gsub but it does solve
> the problem.  Modify to suit.
>
> The first gsub places <...> around the first occurrence of any
> duplicated suffixes.  We use the (?=...) zero width regexp
> to circumvent the nesting problem.
>
> Then we use strapply from the gsubfn package to extract
> the suffixes so marked and paste them together to pass
> to a second gsub which locates them in the original
> string appending an <r> to each.   Uncomment the commented
> pat if you only want to match 2+ character suffixes.
>
> library(gsubfn)
> # places <...> around first occurrences of repeated suffixes
> text <- "And this is the second sentence"
> pat <- "(\\w+)(?=\\b.+\\1\\b)"
> # pat <- "(\\w\\w+)(?=\\b.+\\1\\b)"
> out <- gsub(pat, "\\<\\1\\>", text, perl = TRUE)
>
> suff <- strapply(out, "<([^>]+)>", function(x,y)y)[[1]]
> gsub(paste("(", paste(suff, collapse = "|"), ")\\b", sep = ""), "\\1<r>", text)
>
>
> On 7/22/06, Stefan Th. Gries <stgries_lists at arcor.de> wrote:
> > Dear all
> >
> > I use R for Windows 2.3.1 on a fully updated Windows XP Home SP2 machine and I have two related regular expression problems.
> >
> > platform       i386-pc-mingw32
> > arch           i386
> > os             mingw32
> > system         i386, mingw32
> > status
> > major          2
> > minor          3.1
> > year           2006
> > month          06
> > day            01
> > svn rev        38247
> > language       R
> > version.string Version 2.3.1 (2006-06-01)
> >
> >
> > I would like to find cases of words in elements of character vectors that end in the same character sequences; if I find such cases, I want to add <r> to both potentially rhyming sequences. An example:
> >
> > INPUT:This is my dog.
> > DESIRED OUTPUT: This<r> is<r> my dog.
> >
> > I found a solution for cases where the potentially rhyming words are adjacent:
> >
> > text<-"This is my dog."
> > gsub("(\\w+?)(\\W\\w+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)
> >
> > However, with another text vector, I came across two problems I cannot seem to solve and for which I would love to get some input.
> >
> > (i) While I know what to do for non-adjacent words in general
> >
> > gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", "This not is my dog", perl=TRUE) # I know this is not proper English ;-)
> >
> > this runs into problems with overlapping matches:
> >
> > text<-"And this is the second sentence"
> > gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)
> > [1] "And<r> this is the second<r> sentence"
> >
> > It finds the "nd" match, but since the "is" match is within the two "nd"'s, it doesn't get it. Any ideas on how to get all pairwise matches?
> >
> > (ii) How would one tell R to match only when there are 2+ characters matching? If the above expression is applied to another character string
> >
> > text<-"this is an example sentence."
> > gsub("(\\w+?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)
> >
> > it also matches the "e"'s at the end of example and sentence. It's not possible to get rid of that by specifying a range such as {2,}
> >
> > text<-"this is an example sentence."
> > gsub("(\\w{2,}?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)
> >
> > because, as I understand it, this requires the 2+ cases of \\w to be identical characters:
> >
> > text<-"doo yoo see mee?"
> > gsub("(\\w{2,}?)(\\W.+?)\\1(\\W)", "\\1<r>\\2\\1<r>\\3", text, perl=TRUE)
> >
> > Again, any ideas?
> >
> > I'd really appreciate any snippets of codes, pointers, etc.
> > Thanks so much,
> > STG
> > --
> > Stefan Th. Gries
> > -----------------------------------------------
> > University of California, Santa Barbara
> > http://www.linguistics.ucsb.edu/faculty/stgries
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From etiennesky at yahoo.com  Wed Jul 26 03:15:01 2006
From: etiennesky at yahoo.com (etienne)
Date: Tue, 25 Jul 2006 18:15:01 -0700 (PDT)
Subject: [R] generating sequences with gaps
Message-ID: <20060726011501.51933.qmail@web36903.mail.mud.yahoo.com>

I need sequences that have gaps in them, such as the
following:

4 5 6 | 12 13 14 | 20 21 22

a simple question, I've been scratching my head for a
R function that will do this

The : and seq do not allow this, and the c() can be
used although not in an automatic way.  I'm sure there
is a way to do it without using a for() construct.

Thank you


From ggrothendieck at gmail.com  Wed Jul 26 03:30:33 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 25 Jul 2006 21:30:33 -0400
Subject: [R] generating sequences with gaps
In-Reply-To: <20060726011501.51933.qmail@web36903.mail.mud.yahoo.com>
References: <20060726011501.51933.qmail@web36903.mail.mud.yahoo.com>
Message-ID: <971536df0607251830g3962ce92s7674c38d3ad69fc7@mail.gmail.com>

Here are a few possibilies:

x <- c(4, 12, 20)

rep(x, each = 3) + 0:2

rep(x, each = 3) + sequence(rep(3, length(x))) - 1

c(sapply(x, seq, length = 3))


On 7/25/06, etienne <etiennesky at yahoo.com> wrote:
> I need sequences that have gaps in them, such as the
> following:
>
> 4 5 6 | 12 13 14 | 20 21 22
>
> a simple question, I've been scratching my head for a
> R function that will do this
>
> The : and seq do not allow this, and the c() can be
> used although not in an automatic way.  I'm sure there
> is a way to do it without using a for() construct.
>
> Thank you
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From xmeng at capitalbio.com  Wed Jul 26 05:36:43 2006
From: xmeng at capitalbio.com (XinMeng)
Date: Wed, 26 Jul 2006 11:36:43 +0800
Subject: [R] S curve via R
Message-ID: <353885003.24346@capitalbio.com>

Hello sir:
How can I get "S curve" function via R?
For SPSS,the function is:y=exp(b0+b1/x)

Thanks a lot!




------------------------------
*******************************************
Xin Meng 
Capitalbio Corporation
National Engineering Research Center 
for Beijing Biochip Technology 
BioPharma-informatics & Software Dept. 
Research Engineer
Tel: +86-10-80715888/80726868-6438
Fax: +86-10-80726790
Email??xmeng at capitalbio.com 
Address:18 Life Science Parkway, 
Changping District, Beijing 102206, China


From johannes at huesing.name  Wed Jul 26 08:21:17 2006
From: johannes at huesing.name (Johannes =?iso-8859-1?Q?H=FCsing?=)
Date: Wed, 26 Jul 2006 08:21:17 +0200 (CEST)
Subject: [R] S curve via R
In-Reply-To: <353885003.24346@capitalbio.com>
References: <353885003.24346@capitalbio.com>
Message-ID: <47619.129.206.90.2.1153894877.squirrel@mail.panix.com>

> Hello sir:
> How can I get "S curve" function via R?
> For SPSS,the function is:y=exp(b0+b1/x)
>

I am not sure if this is the answer you want, but

Scurve <- function(x, b0=0, b1=1) {
    exp(b0+b1/x)
}

should do what you request.

Greetings


Johannes


From maechler at stat.math.ethz.ch  Wed Jul 26 08:50:38 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 26 Jul 2006 08:50:38 +0200
Subject: [R] [Way OT] New hardware
In-Reply-To: <C0EC0252.E077%sdavis2@mail.nih.gov>
References: <C0EC0252.E077%sdavis2@mail.nih.gov>
Message-ID: <17607.4286.451032.962993@stat.math.ethz.ch>

>>>>> "Sean" == Sean Davis <sdavis2 at mail.nih.gov>
>>>>>     on Tue, 25 Jul 2006 17:16:02 -0400 writes:

    Sean> Can anyone share experience with opteron versus the
    Sean> xeon (woodcrest) for R under linux?  I am looking at
    Sean> using 16-32Gb of ram in a workstation (as opposed to a
    Sean> server).

Hmm, not that I'd be an expert...
If you want to use so much RAM you want to use a 64-bit
architecture and software (OS, libraries, compilers,...), right?
AFAIK, that's been known to work well with Opterons and different
flavors of Linuxen (e.g. we have dual
Opterons, one with Redhat Enterprise and two with Ubuntu 6.06).

Now I read that there are 64-bit Xeons with "EMT64" (which is
said to be Intel's emulation of AMD64), so in principle the same
versions of Linux and R should run there as well.
Since I haven't heard of any success stories
I'm interested as well, in reports from R users.

Martin Maechler, ETH Zurich


From ligges at statistik.uni-dortmund.de  Wed Jul 26 09:09:11 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 26 Jul 2006 09:09:11 +0200
Subject: [R] command completion in R-WinEdt
In-Reply-To: <44C69FCA.9070305@gmx.de>
References: <44C69FCA.9070305@gmx.de>
Message-ID: <44C71517.8050308@statistik.uni-dortmund.de>

Franco Mendolia wrote:
> Hello!
> 
> Is there any possibility to use command completion in R-WinEdt?

You can make use of the "Command Completion Wizard" for WinEdt available 
at http://www.winedt.org/Plugins/.
A list of function names ships with the RWiNEdt package (file R.lst). 
Simply make it known to the Command Completion Wizard.

Uwe Ligges



> Thanks
> Franco
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Wed Jul 26 09:21:10 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 26 Jul 2006 08:21:10 +0100 (BST)
Subject: [R] [Way OT] New hardware
In-Reply-To: <17607.4286.451032.962993@stat.math.ethz.ch>
References: <C0EC0252.E077%sdavis2@mail.nih.gov>
	<17607.4286.451032.962993@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0607260817470.3591@gannet.stats.ox.ac.uk>

On Wed, 26 Jul 2006, Martin Maechler wrote:

> >>>>> "Sean" == Sean Davis <sdavis2 at mail.nih.gov>
> >>>>>     on Tue, 25 Jul 2006 17:16:02 -0400 writes:
> 
>     Sean> Can anyone share experience with opteron versus the
>     Sean> xeon (woodcrest) for R under linux?  I am looking at
>     Sean> using 16-32Gb of ram in a workstation (as opposed to a
>     Sean> server).
> 
> Hmm, not that I'd be an expert...
> If you want to use so much RAM you want to use a 64-bit
> architecture and software (OS, libraries, compilers,...), right?
> AFAIK, that's been known to work well with Opterons and different
> flavors of Linuxen (e.g. we have dual
> Opterons, one with Redhat Enterprise and two with Ubuntu 6.06).
> 
> Now I read that there are 64-bit Xeons with "EMT64" (which is
> said to be Intel's emulation of AMD64), so in principle the same
> versions of Linux and R should run there as well.
> Since I haven't heard of any success stories
> I'm interested as well, in reports from R users.

There have been several posted here or R-devel.  Things do change, but 
every time we have had a formal test, Opterons were considerably better 
than Xeons on performance/?.

[BTW, I don't think you will get a workstation motherboard that takes 32Gb 
of RAM (although things change fast):  my own machine has a server 
motherboard in a small tower case.]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From h.wickham at gmail.com  Wed Jul 26 09:54:17 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 26 Jul 2006 08:54:17 +0100
Subject: [R] Overplotting: plot() invocation looks ugly ... suggestions?
In-Reply-To: <971536df0607241908j12f4ce47yc390638e6e31a077@mail.gmail.com>
References: <20060725012835.96890.qmail@web35405.mail.mud.yahoo.com>
	<971536df0607241844g2510ac51uc32233f4c855d8ff@mail.gmail.com>
	<971536df0607241908j12f4ce47yc390638e6e31a077@mail.gmail.com>
Message-ID: <f8e6ff050607260054p2306554am3b6dd6a669d45869@mail.gmail.com>

> And if lattice is ok then try this:
>
> library(lattice)
> xyplot(Consumption ~ Quarter, group = Year, data, type = "o")

Or you can use ggplot:

install.packages("ggplot")
library(ggplot)
qplot(Quarter, Consumption, data=data,type=c("point","line"), id=data$Year)

Unfortunately this has uncovered a couple of small bugs for me to fix
(no automatic legend, and have to specify the data frame explicitly)

The slighly more verbose example below shows you what it should look like.

data$Year <- factor(data$Year)
p <- ggplot(data, aes=list(x=Quarter, y=Consumption, id=Year, colour=Year))
ggline(ggpoint(p), size=2)

Regards,

Hadley


From JeeBee at troefpunt.nl  Wed Jul 26 11:43:23 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Wed, 26 Jul 2006 11:43:23 +0200
Subject: [R] convert decimals to fractions - sorted
References: <c7c17cef0607251002k6f6005e1h5b717d3fb23ff1b6@mail.gmail.com>
Message-ID: <pan.2006.07.26.09.43.22.948661@troefpunt.nl>


Hi Muhammad,

How about this?

at <- read.table(textConnection(a))
at2 <- cbind(at, jeebee=as.character(as.fractions(as.numeric(at[,2]))))

sort.order <- order(at2$V2)

at2[sort.order,]
at2[sort.order,c(1,3)]

JeeBee.


From constantinos.antoniou at gmail.com  Wed Jul 26 12:02:44 2006
From: constantinos.antoniou at gmail.com (Constantinos Antoniou)
Date: Wed, 26 Jul 2006 13:02:44 +0300
Subject: [R] Overplotting: plot() invocation looks ugly ... suggestions?
In-Reply-To: <f8e6ff050607260054p2306554am3b6dd6a669d45869@mail.gmail.com>
References: <20060725012835.96890.qmail@web35405.mail.mud.yahoo.com>
	<971536df0607241844g2510ac51uc32233f4c855d8ff@mail.gmail.com>
	<971536df0607241908j12f4ce47yc390638e6e31a077@mail.gmail.com>
	<f8e6ff050607260054p2306554am3b6dd6a669d45869@mail.gmail.com>
Message-ID: <fa66af1b0607260302i5150e12bq67c313eba762fa02@mail.gmail.com>

Hello,

I would like to make a question regarding the use of a grey background
(by ggplot in this case, but also in other settings - I seem to
remember a relevant lattice discussion). It seems that it is generally
discouraged by journals. I guess one practical reason is that it makes
photocopying difficult (in the sense that it may lead to low contrast
situations). It might have to do with printing costs, as it leads to
higher coverage of the page, but I do not know about that.

[Disclaimer: it does look nice, though.]

Any comments?

Thanks,

Costas

On 7/26/06, hadley wickham <h.wickham at gmail.com> wrote:
> > And if lattice is ok then try this:
> >
> > library(lattice)
> > xyplot(Consumption ~ Quarter, group = Year, data, type = "o")
>
> Or you can use ggplot:
>
> install.packages("ggplot")
> library(ggplot)
> qplot(Quarter, Consumption, data=data,type=c("point","line"), id=data$Year)
>
> Unfortunately this has uncovered a couple of small bugs for me to fix
> (no automatic legend, and have to specify the data frame explicitly)
>
> The slighly more verbose example below shows you what it should look like.
>
> data$Year <- factor(data$Year)
> p <- ggplot(data, aes=list(x=Quarter, y=Consumption, id=Year, colour=Year))
> ggline(ggpoint(p), size=2)
>
> Regards,
>
> Hadley
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From peter.robinson at charite.de  Wed Jul 26 12:11:17 2006
From: peter.robinson at charite.de (Robinson, Peter)
Date: Wed, 26 Jul 2006 12:11:17 +0200
Subject: [R] Power tests for ROC analysis
Message-ID: <9AAD8C1079BA01459BF88C31141AC957189FAF@EXCHANGE02.charite.de>

Dear List,

please forgive this question from a hobby statistician, but I was wondering if there is any way of doing power calculations to estimate how much data is needed so that the sensitivity/specificity values along a ROC curve will be within a certain confidence interval? I am not aware of any such method, but was recently asked how much data would be needed to perform ROC analysis for a study.

Thanks a lot,
Peter

Dr. med. Peter Robinson, MSc.
Institut f?r Medizinische Genetik
Universit?tsklinikum Charit? 
Humboldt-Universit?t
Augustenburger Platz 1
13353 Berlin
Phone: ++49-30-450 569124
Fax: ++49-30-450 569915
peter.robinson at charite.de
http://www.charite.de/ch/medgen/robinson


From karloh at mi.uib.no  Wed Jul 26 12:28:19 2006
From: karloh at mi.uib.no (Karl Ove Hufthammer)
Date: Wed, 26 Jul 2006 12:28:19 +0200
Subject: [R] Overplotting: plot() invocation looks ugly ... suggestions?
References: <20060725012835.96890.qmail@web35405.mail.mud.yahoo.com>
	<971536df0607241844g2510ac51uc32233f4c855d8ff@mail.gmail.com>
	<971536df0607241908j12f4ce47yc390638e6e31a077@mail.gmail.com>
	<f8e6ff050607260054p2306554am3b6dd6a669d45869@mail.gmail.com>
	<fa66af1b0607260302i5150e12bq67c313eba762fa02@mail.gmail.com>
Message-ID: <ea7g43$lro$1@sea.gmane.org>

Constantinos Antoniou skreiv:

> I would like to make a question regarding the use of a grey background
> (by ggplot in this case, but also in other settings - I seem to
> remember a relevant lattice discussion). It seems that it is generally
> discouraged by journals. I guess one practical reason is that it makes
> photocopying difficult (in the sense that it may lead to low contrast
> situations). It might have to do with printing costs, as it leads to
> higher coverage of the page, but I do not know about that.
> 
> [Disclaimer: it does look nice, though.]
>
> Any comments?

Just a small one: The grey background used by ggplot does look nice;
the one used by earlier versions of lattice did not. All IMHO, of course.

-- 
Karl Ove Hufthammer
E-mail and Jabber: karl at huftis.org


From h.wickham at gmail.com  Wed Jul 26 12:30:24 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 26 Jul 2006 11:30:24 +0100
Subject: [R] Overplotting: plot() invocation looks ugly ... suggestions?
In-Reply-To: <fa66af1b0607260302i5150e12bq67c313eba762fa02@mail.gmail.com>
References: <20060725012835.96890.qmail@web35405.mail.mud.yahoo.com>
	<971536df0607241844g2510ac51uc32233f4c855d8ff@mail.gmail.com>
	<971536df0607241908j12f4ce47yc390638e6e31a077@mail.gmail.com>
	<f8e6ff050607260054p2306554am3b6dd6a669d45869@mail.gmail.com>
	<fa66af1b0607260302i5150e12bq67c313eba762fa02@mail.gmail.com>
Message-ID: <f8e6ff050607260330k7ff711a9p7d13dee3360edda6@mail.gmail.com>

> I would like to make a question regarding the use of a grey background
> (by ggplot in this case, but also in other settings - I seem to
> remember a relevant lattice discussion). It seems that it is generally
> discouraged by journals. I guess one practical reason is that it makes
> photocopying difficult (in the sense that it may lead to low contrast
> situations). It might have to do with printing costs, as it leads to
> higher coverage of the page, but I do not know about that.
>
> [Disclaimer: it does look nice, though.]
>
> Any comments?

It is very easy to change to the usual black on white grid lines (see
?ggopt and ?ggsave), so if your journal does require it, it's easy to
turn off.

Here are a few reasons I like the gray background (in no particular order):

 * you can then use white gridlines, which miniminally impinge on the
plot, but still aid lookup to the relevant axis

 * the color of the plot more closely matches the "color" (in the
typographic sense) of the text, so that the plot fits into a printed
document without drawing so much attention to itself.

 * the contrast between the plot surface and the points is a little
lower, which makes it a bit more pleasant to read

Of course the big disadvantage is if you don't have a high quality
printer, or a looking at a photocopy of a photocopy etc.  This
disadvantage should go away with time as the quality of printed output
steadily improves.

Regards,

Hadley


From joseclaudio.faria at terra.com.br  Wed Jul 26 13:10:36 2006
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Wed, 26 Jul 2006 08:10:36 -0300
Subject: [R] Axis Title in persp() Overlaps with Axis Labels
Message-ID: <44C74DAC.5060503@terra.com.br>

Dear Kilian,

Also give a looked at: 
http://wiki.r-project.org/rwiki/doku.php?id=graph_gallery:new-graphics

You will see a new and very flexible function to 3D plot.

Regards,
__
Jose Claudio Faria
Brasil/Bahia/Ilheus/UESC/DCET
Estat?stica Experimental/Prof. Adjunto
mails: joseclaudio.faria em terra.com.br
        jc_faria em uesc.br
        jc_faria em uol.com.br

Paul Murrell <p.murrell <at> auckland.ac.nz> writes:

 >
 > Hi
 >
 > Kilian Plank wrote:
 > > Good morning,
 > >
 > > in a 3D plot based on persp() the axis title (of dimension z) 
overlaps with
 > > the axis labels.
 > > How can the distance (between axis labels and axis title) be increased?
 >

 > Paul

   Another way to do it: get the perspective matrix
back from persp() and use trans3d() to redo essentially
the same calculations that persp() does to decide where
to put the label:

x <- seq(-10, 10, length= 30)
y <- x
f <- function(x,y) { r <- sqrt(x^2+y^2); 10 * sin(r)/r }
z <- outer(x, y, f)
z[is.na(z)] <- 1

par(mfrow=c(2, 2))
persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
       col = "lightblue", ticktype="detailed")

persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
       col = "lightblue", ticktype="detailed",
       zlab="\n\n\n\nz")

p1 <- persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
       col = "lightblue", ticktype="detailed",zlab="")

ranges <- t(sapply(list(x,y,z),range))
means <- rowMeans(ranges)

## label offset distance, as a fraction of the plot width
labelspace <- 0.12  ## tweak this until you like the result

xpos <- min(x)-(diff(range(x)))*labelspace
ypos <- min(y)-(diff(range(y)))*labelspace
labelbot3d <- c(xpos,ypos,min(z))
labeltop3d <- c(xpos,ypos,max(z))
labelmid3d <- c(xpos,ypos,mean(range(z)))

trans3dfun <- function(v) { trans3d(v[1],v[2],v[3],p1) }
labelbot2d <- trans3dfun(labelbot3d)
labelmid2d <- trans3dfun(labelmid3d)
labeltop2d <- trans3dfun(labeltop3d)
labelang <- 
180/pi*atan2(labeltop2d$y-labelbot2d$y,labeltop2d$x-labelbot2d$x)
par(xpd=NA,srt=labelang)  ## disable clipping and set string rotation
text(labelmid2d[1]$x,labelmid2d$y,"z label")


From marco.boks at inter.nl.net  Wed Jul 26 13:17:32 2006
From: marco.boks at inter.nl.net (Marco Boks)
Date: Wed, 26 Jul 2006 13:17:32 +0200
Subject: [R] Main title of plot
Message-ID: <000f01c6b0a5$174e7cf0$0300a8c0@marco>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060726/4c251d0c/attachment.pl 

From ggrothendieck at gmail.com  Wed Jul 26 13:29:05 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 26 Jul 2006 07:29:05 -0400
Subject: [R] Main title of plot
In-Reply-To: <000f01c6b0a5$174e7cf0$0300a8c0@marco>
References: <000f01c6b0a5$174e7cf0$0300a8c0@marco>
Message-ID: <971536df0607260429t55f03bbdk33403b246f7ea834@mail.gmail.com>

This was just discussed yesterday.  See the thread:

https://www.stat.math.ethz.ch/pipermail/r-help/2006-July/109931.html

On 7/26/06, Marco Boks <marco.boks at inter.nl.net> wrote:
> I am a newbie, and I am afraid this may be a rather trivial question. However I could not find the answer anywhere.
>
>
>
> I am plotting a series of plots with different values for p. In the main title of a plot I have used the following code:
>
>
>
>
>
> plot(a,b,type="l",ylim=c(0,1), xlab="freq",ylab="power", main=c("maximum gain=",p) )
>
>
>
> That works fine. However the value of p is plotted on a new line, instead of just after the "="
>
>
>
> Is there anyway to print the value of p on the same line?
>
>
>
> Thanks
>
>
> Marco
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Arne.Muller at sanofi-aventis.com  Wed Jul 26 13:32:52 2006
From: Arne.Muller at sanofi-aventis.com (Arne.Muller at sanofi-aventis.com)
Date: Wed, 26 Jul 2006 13:32:52 +0200
Subject: [R] randomForest question
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE02E8CC61@CRBSMXSUSR04>

Hello,

I've a question regarding randomForest (from the package with same name). I've 16 featurs (nominative), 159 positive and 318 negative cases that I'd like to classify (binary classification).

Using the tuning from the e1071 package it turns out that the best performance if reached when using all 16 features per tree (mtry=16). However, the documentation of randomForest suggests to take the sqrt(#features), i.e. 4. How can I explain this difference? When using all features this is the same as a classical decision tree, with the difference that the tree is built and tested with different data sets, right?

example (I've tried different configurations, incl. changing ntree):
> param <- try(tune(randomForest, class ~ ., data=d.all318, range=list(mtry=c(4, 8, 16), ntree=c(1000))));
>
> summary(param)

Parameter tuning of `randomForest':

- sampling method: 10-fold cross validation 

- best parameters:
 mtry ntree
   16  1000

- best performance: 0.1571809 

- Detailed performance results:
  mtry ntree     error
1    4  1000 0.1928635
2    8  1000 0.1634752
3   16  1000 0.1571809

	thanks a lot for your help,

	kind regards,


From msubianto at gmail.com  Wed Jul 26 13:35:42 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Wed, 26 Jul 2006 13:35:42 +0200
Subject: [R] convert decimals to fractions - sorted
In-Reply-To: <pan.2006.07.26.09.43.22.948661@troefpunt.nl>
References: <c7c17cef0607251002k6f6005e1h5b717d3fb23ff1b6@mail.gmail.com>
	<pan.2006.07.26.09.43.22.948661@troefpunt.nl>
Message-ID: <c7c17cef0607260435l23376a10haa5d7cae28458fc2@mail.gmail.com>

Dear all,
Thanks for your help.
I played with you suggest and still didn't sort (summary) which I need.

> t(table(at2[sort.order,c(1,3)]))
       V1
jeebee  -1 1
  0      0 4
  11/21  0 1
  1/2    1 0
  1/21   1 1
  13/42  1 1
  17/42  0 2
  2/21   0 3
  3/14   1 2
  5/42   0 1
  8/21   0 1
>

I need the result summary (order) like,

      -1 1
  0/42   0 4
  2/42   1 1
  4/42   0 3
  5/42   0 1
  9/42   1 2
  13/42  1 1
  16/42  0 1
  17/42  0 2
  21/42  1 0
  22/42  0 1

Thanks very much for any suggestions.
Groeten & Regards, Muhammad Subianto


On 7/26/06, JeeBee <JeeBee at troefpunt.nl> wrote:
>
> Hi Muhammad,
>
> How about this?
>
> at <- read.table(textConnection(a))
> at2 <- cbind(at, jeebee=as.character(as.fractions(as.numeric(at[,2]))))
>
> sort.order <- order(at2$V2)
>
> at2[sort.order,]
> at2[sort.order,c(1,3)]
>
> JeeBee.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

On 7/25/06, Muhammad Subianto <msubianto at gmail.com> wrote:
> Dear all,
> Based on my question a few months ago
> https://stat.ethz.ch/pipermail/r-help/2006-January/086952.html
> and solved with
> https://stat.ethz.ch/pipermail/r-help/2006-January/086955.html
> https://stat.ethz.ch/pipermail/r-help/2006-January/086956.html
> and from
> https://stat.ethz.ch/pipermail/r-help/2006-January/086958.html
>
> frac.fun <- function(x, den){
>     dec <- seq(0, den) / den
>     nams <- paste(seq(0, den), den, sep = "/")
>     sapply(x, function(y) nams[which.min(abs(y - dec))])
> }
> #######################
> frac.fun(c(0, 1, 0.8266667, .066666, 0.2666666), 75)
>
> Now, I have a dataset something like this:
>
> a <-"1 0
>     1 0.095238095238095
>     1 0.214285714285714
>    -1 0.5
>     1 0.309523809523810
>    -1 0.0476190476190476
>     1 0.404761904761905
>     1 0.119047619047619
>    -1 0.214285714285714
>    -1 0.309523809523810
>     1 0
>     1 0
>     1 0.404761904761905
>     1 0.095238095238095
>     1 0.047619047619047
>     1 0.380952380952381
>     1 0.214285714285714
>     1 0.523809523809524
>     1 0
>     1 0.095238095238095"
>
> First, I make it as fractions and then sorted.
> I have played around to make it sort, but it didn't succes.
>
> df <- read.table(textConnection(a))
> library(MASS)
> as.fractions(as.numeric(df[,2]))
> cbind(table(df[,2], df[,1]), summary(as.factor(df[,2])))
> table(frac.fun(as.numeric(df[,2]),42), df[,1])
> > table(frac.fun(as.numeric(df[,2]),42), df[,1])
>
>         -1 1
>   0/42   0 4
>   13/42  1 1
>   16/42  0 1
>   17/42  0 2
>   21/42  1 0
>   22/42  0 1
>   2/42   1 1
>   4/42   0 3
>   5/42   0 1
>   9/42   1 2
> >
>
> How to make the result as sort (to increase) like this,
>
>         -1 1
>   0/42   0 4
>   2/42   1 1
>   4/42   0 3
>   5/42   0 1
>   9/42   1 2
>   13/42  1 1
>   16/42  0 1
>   17/42  0 2
>   21/42  1 0
>   22/42  0 1
>
> Thank's for any help.
>
> Best, Muhammad Subianto
>


From Max.Kuhn at pfizer.com  Wed Jul 26 13:53:40 2006
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Wed, 26 Jul 2006 07:53:40 -0400
Subject: [R]  Sweave and tth
Message-ID: <71257D09F114DA4A8E134DEAC70F25D305B00429@groamrexm03.amer.pfizer.com>

Dr. Harrell,

> I tried odfWeave to create an OpenOffice file and found that it 
> exhausted the memory of my large linux machine and took a long time to

> run. 

Do you have any details about the problem that you encountered? A bug
that someone else had pointed out might be the culprit. I have the
default image format as png, but since a lot of linux systems don't have
that device automatically available to them, I have a switch for the
device in odfWeaveControl:

 plotDevice = ifelse(.Platform$OS.type == "windows", "png", "bitmap"),

The bitmap device units are in inches and the bmp device is in pixels.
The bug is the default image size if 480 inches (whoops).

Can you try using:

  odfWeaveControl(
   plotHeight = 5,
   plotWidth = 5,
   dispHeight = 5,
   dispWidth = 5)

in your odfWeave call and let me know if this was the issue? I was able
to reproduce the error on our linux systems and this fix worked (strange
that the package passes R CMD check though).

If this works, Section 7 of the odfWeave manual lists two command line
tools (not included in my package) that can do the conversion from odt
to Word (or other formats)

> I really appreciate Max Kuhn's efforts with odfWeave and hope to keep
up 
> with its development.

No problem. I'll be releasing a bug fix version to solve the device
units issue. Also, others have reported problems with locales. I believe
I have a fix for this issue too.

> 
> Thanks.
> -- 
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   Vanderbilt
University

Max
----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From MSchwartz at mn.rr.com  Wed Jul 26 14:09:53 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 26 Jul 2006 07:09:53 -0500
Subject: [R] Main title of plot
In-Reply-To: <971536df0607260429t55f03bbdk33403b246f7ea834@mail.gmail.com>
References: <000f01c6b0a5$174e7cf0$0300a8c0@marco>
	<971536df0607260429t55f03bbdk33403b246f7ea834@mail.gmail.com>
Message-ID: <1153915793.5426.8.camel@localhost.localdomain>

Gabor,

I think that this is actually different, since it does not involve
plotmath.

The issue here is the use of c() in:

  main=c("maximum gain=",p)

rather than:

  main = paste("maximum gain =", p)

Marco, try this:

plot(a, b, type="l", ylim=c(0, 1), xlab = "freq", ylab = "power",
     main = paste("maximum gain =",p))

See ?paste for concatenating multiple vectors into a single character
vector (string).

HTH,

Marc Schwartz



On Wed, 2006-07-26 at 07:29 -0400, Gabor Grothendieck wrote:
> This was just discussed yesterday.  See the thread:
> 
> https://www.stat.math.ethz.ch/pipermail/r-help/2006-July/109931.html
> 
> On 7/26/06, Marco Boks <marco.boks at inter.nl.net> wrote:
> > I am a newbie, and I am afraid this may be a rather trivial
> question. However I could not find the answer anywhere.
> >
> >
> >
> > I am plotting a series of plots with different values for p. In the
> main title of a plot I have used the following code:
> >
> >
> >
> >
> >
> > plot(a,b,type="l",ylim=c(0,1), xlab="freq",ylab="power",
> main=c("maximum gain=",p) )
> >
> >
> >
> > That works fine. However the value of p is plotted on a new line,
> instead of just after the "="
> >
> >
> >
> > Is there anyway to print the value of p on the same line?
> >
> >
> >
> > Thanks
> >
> >
> > Marco
> >


From mcardeal at ufba.br  Wed Jul 26 14:22:19 2006
From: mcardeal at ufba.br (Mauricio Cardeal)
Date: Wed, 26 Jul 2006 09:22:19 -0300
Subject: [R] the first and last case
Message-ID: <44C75E7B.2080807@ufba.br>

Hi all

Sometime ago I asked for a solution about how to aggregate data and the 
help was wonderful. Now, I?d like to know how to extract for each 
individual case below the first and the last observation to obtain this:

ind  y
1    8
1    9
2    7
2   11
3    9
3   10
4   8
4   5

# Below the example:

ind <- c(1,1,1,2,2,3,3,3,4,4,4,4)
y <- c(8,10,9,7,11,9,9,10,8,7,6,5)
dat <- as.data.frame(cbind(ind,y))
dat
attach(dat)
mean.ind <- aggregate(dat$y, by=list(dat$ind), mean)
mean.ind

Thanks
Mauricio


From michael.watson at bbsrc.ac.uk  Wed Jul 26 14:41:31 2006
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 26 Jul 2006 13:41:31 +0100
Subject: [R] Faster alternative to by?
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950374D206@iahce2ksrv1.iah.bbsrc.ac.uk>

Hi

I have a data.frame, two columns, 12304 rows.  Both columns are factors.
I want to do an equivalent of an SQL "group by" statement, and count the
number of rows in the data frame for each unique value of the second
column.

I have:

countl <- by(mapped, mapped$col2, nrow)

Now, mapped$col2 has 10588 levels, so this statement takes a really long
time to run.  Is there a more efficient way of doing this in R?

Thanks

Mick


From jacques.veslot at good.ibl.fr  Wed Jul 26 14:49:26 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Wed, 26 Jul 2006 14:49:26 +0200
Subject: [R] Faster alternative to by?
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950374D206@iahce2ksrv1.iah.bbsrc.ac.uk>
References: <8975119BCD0AC5419D61A9CF1A923E950374D206@iahce2ksrv1.iah.bbsrc.ac.uk>
Message-ID: <44C764D6.9040101@good.ibl.fr>

table(mapped$col2)
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


michael watson (IAH-C) a ?crit :
> Hi
> 
> I have a data.frame, two columns, 12304 rows.  Both columns are factors.
> I want to do an equivalent of an SQL "group by" statement, and count the
> number of rows in the data frame for each unique value of the second
> column.
> 
> I have:
> 
> countl <- by(mapped, mapped$col2, nrow)
> 
> Now, mapped$col2 has 10588 levels, so this statement takes a really long
> time to run.  Is there a more efficient way of doing this in R?
> 
> Thanks
> 
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jacques.veslot at good.ibl.fr  Wed Jul 26 15:09:37 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Wed, 26 Jul 2006 15:09:37 +0200
Subject: [R] the first and last case
In-Reply-To: <44C75E7B.2080807@ufba.br>
References: <44C75E7B.2080807@ufba.br>
Message-ID: <44C76991.9010707@good.ibl.fr>

do.call(rbind,lapply(split(dat, dat$ind), function(x) x[c(1,nrow(x)),]))
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Mauricio Cardeal a ?crit :
> Hi all
> 
> Sometime ago I asked for a solution about how to aggregate data and the 
> help was wonderful. Now, I?d like to know how to extract for each 
> individual case below the first and the last observation to obtain this:
> 
> ind  y
> 1    8
> 1    9
> 2    7
> 2   11
> 3    9
> 3   10
> 4   8
> 4   5
> 
> # Below the example:
> 
> ind <- c(1,1,1,2,2,3,3,3,4,4,4,4)
> y <- c(8,10,9,7,11,9,9,10,8,7,6,5)
> dat <- as.data.frame(cbind(ind,y))
> dat
> attach(dat)
> mean.ind <- aggregate(dat$y, by=list(dat$ind), mean)
> mean.ind
> 
> Thanks
> Mauricio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Stefano.Guazzetti at ausl.re.it  Wed Jul 26 15:13:11 2006
From: Stefano.Guazzetti at ausl.re.it (Guazzetti Stefano)
Date: Wed, 26 Jul 2006 15:13:11 +0200
Subject: [R] R:  the first and last case
Message-ID: <B8A1EED732379B44A7E59D22E82E4442020D6BCE@IMHOTEP.ausl.org>

could it be

 dat[unlist(tapply(1:nrow(dat), ind, range)),]
?

stefano



   >-----Messaggio originale-----
   >Da: r-help-bounces at stat.math.ethz.ch
   >[mailto:r-help-bounces at stat.math.ethz.ch]Per conto di 
   >Mauricio Cardeal
   >Inviato: 26 July, 2006 14:22
   >A: r-help at stat.math.ethz.ch
   >Oggetto: [R] the first and last case
   >
   >
   >Hi all
   >
   >Sometime ago I asked for a solution about how to aggregate 
   >data and the 
   >help was wonderful. Now, I?d like to know how to extract for each 
   >individual case below the first and the last observation to 
   >obtain this:
   >
   >ind  y
   >1    8
   >1    9
   >2    7
   >2   11
   >3    9
   >3   10
   >4   8
   >4   5
   >
   ># Below the example:
   >
   >ind <- c(1,1,1,2,2,3,3,3,4,4,4,4)
   >y <- c(8,10,9,7,11,9,9,10,8,7,6,5)
   >dat <- as.data.frame(cbind(ind,y))
   >dat
   >attach(dat)
   >mean.ind <- aggregate(dat$y, by=list(dat$ind), mean)
   >mean.ind
   >
   >Thanks
   >Mauricio
   >
   >______________________________________________
   >R-help at stat.math.ethz.ch mailing list
   >https://stat.ethz.ch/mailman/listinfo/r-help
   >PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From cgb at datanalytics.com  Wed Jul 26 15:00:03 2006
From: cgb at datanalytics.com (Carlos J. Gil Bellosta)
Date: Wed, 26 Jul 2006 15:00:03 +0200
Subject: [R] the first and last case
In-Reply-To: <44C76991.9010707@good.ibl.fr>
References: <44C75E7B.2080807@ufba.br> <44C76991.9010707@good.ibl.fr>
Message-ID: <20060726150003.lbklmjd7ou1usokw@webmail.datanalytics.com>

Dear Jacques,

I believe you need dat ordered by ind and y before you apply your solution,
right?

Sincerely,

Carlos J. Gil Bellosta
http://www.datanalytics.com
http://www.data-mining-blog.com

Quoting Jacques VESLOT <jacques.veslot at good.ibl.fr>:

> do.call(rbind,lapply(split(dat, dat$ind), function(x) x[c(1,nrow(x)),]))
> -------------------------------------------------------------------
> Jacques VESLOT
>
> CNRS UMR 8090
> I.B.L (2?me ?tage)
> 1 rue du Professeur Calmette
> B.P. 245
> 59019 Lille Cedex
>
> Tel : 33 (0)3.20.87.10.44
> Fax : 33 (0)3.20.87.10.31
>
> http://www-good.ibl.fr
> -------------------------------------------------------------------
>
>
> Mauricio Cardeal a ?crit :
>> Hi all
>>
>> Sometime ago I asked for a solution about how to aggregate data and the
>> help was wonderful. Now, I?d like to know how to extract for each
>> individual case below the first and the last observation to obtain this:
>>
>> ind  y
>> 1    8
>> 1    9
>> 2    7
>> 2   11
>> 3    9
>> 3   10
>> 4   8
>> 4   5
>>
>> # Below the example:
>>
>> ind <- c(1,1,1,2,2,3,3,3,4,4,4,4)
>> y <- c(8,10,9,7,11,9,9,10,8,7,6,5)
>> dat <- as.data.frame(cbind(ind,y))
>> dat
>> attach(dat)
>> mean.ind <- aggregate(dat$y, by=list(dat$ind), mean)
>> mean.ind
>>
>> Thanks
>> Mauricio
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dieter.menne at menne-biomed.de  Wed Jul 26 15:30:13 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Wed, 26 Jul 2006 13:30:13 +0000 (UTC)
Subject: [R] [RODBC] ERROR: Could not SQLExecDirect
References: <20060725195052.113090@gmx.net>
Message-ID: <loom.20060726T152645-576@post.gmane.org>

Peter Eiger <Peter.Eiger <at> gmx.net> writes:

> I've got a problem with RODBC and saving (sqlSave) of a dataframe in Access.
> R 2.0.1 is running on windows XP.
> 
> When executing the examples in R help for the "USArrests" data set "sqlSAve"
works fine, but running
> sqlSave() for a dataframe "Adat" 
> 
> > str(Adat)
> `data.frame':   1202 obs. of  18 variables: 
> 
> containing 18 columns and ca. 1200 rows fails.
> 
> I get the following error message:
> 
> > sqlSave(channel, Adat)
> Error in sqlSave(channel, Adat) : [RODBC] ERROR: Could not SQLExecDirect
> 
> The data was fetched from the same Access database before and was not
manipulated before the attempt to save.

Try to set rownames = FALSE in sqlSave, it's TRUE by default which I believe is
a bit unfortunate. And probably append=TRUE. It's also good to try with
fast=FALSE first.

When I get an error of that type, I first save to a non-existing table, and do a
compare of what comes out with the original table.

Dieter


From ggrothendieck at gmail.com  Wed Jul 26 15:36:48 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 26 Jul 2006 09:36:48 -0400
Subject: [R] the first and last case
In-Reply-To: <44C75E7B.2080807@ufba.br>
References: <44C75E7B.2080807@ufba.br>
Message-ID: <971536df0607260636x331d7612wf94a60ef5077468a@mail.gmail.com>

Try these:

# 1
library(Hmisc)
summary(y ~ ind, dat, fun = range, overall = FALSE)

# 2
# or with specified column names
f <- function(x) c(head = head(x,1), tail = tail(x,1))
summary(y ~ ind, dat, fun = f, overall = FALSE)

# 3
# another approach using by - same f as above
do.call(rbind, by(dat$y, dat$ind, f))

# 4
# same but with with an ind column
g <- function(x) c(ind = x$ind[1], head = head(x$y,1), tail = tail(x$y,1))
do.call(rbind, by(dat, dat$ind, g))


On 7/26/06, Mauricio Cardeal <mcardeal at ufba.br> wrote:
> Hi all
>
> Sometime ago I asked for a solution about how to aggregate data and the
> help was wonderful. Now, I?d like to know how to extract for each
> individual case below the first and the last observation to obtain this:
>
> ind  y
> 1    8
> 1    9
> 2    7
> 2   11
> 3    9
> 3   10
> 4   8
> 4   5
>
> # Below the example:
>
> ind <- c(1,1,1,2,2,3,3,3,4,4,4,4)
> y <- c(8,10,9,7,11,9,9,10,8,7,6,5)
> dat <- as.data.frame(cbind(ind,y))
> dat
> attach(dat)
> mean.ind <- aggregate(dat$y, by=list(dat$ind), mean)
> mean.ind
>
> Thanks
> Mauricio
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mo2259 at columbia.edu  Wed Jul 26 15:50:47 2006
From: mo2259 at columbia.edu (mo2259 at columbia.edu)
Date: Wed, 26 Jul 2006 09:50:47 -0400
Subject: [R] SURVEY PREDICTED SEs:  Problem
Message-ID: <1153921847.44c77337e77c4@cubmail.cc.columbia.edu>


Hello R-list,
I'm attempting to migrate from Stata to R for my complex survey
work.    It has been straight-forward so far except for the
following problem:

I have some code below, but first I'll describe the problem.

When I compute predicted logits from a logistic regression, the
standard errors of the predicted logits are way off (but the
predicted logits are fine).  Furthermore, the model logit 
coefficients have appropriate SEs. As a comparison, I ran the same
model without the survey design; the predicted SEs come out fine.

Here is example code (first no survey design model and predictions;
then survey design model and predictions):

> #MODEL COEF. ESTIMATES (NO SURVEY DESIGN)
> model.l.nosvy <- glm(qn58~t8l,data=all.stratum,family=binomial)
> summary(model.l.nosvy)

Call:
glm(formula = qn58 ~ t8l, family = binomial, data = all.stratum)

Deviance Residuals:
   Min      1Q  Median      3Q     Max
-1.310  -1.245   1.050   1.111   1.158

Coefficients:
             Estimate Std. Error z value Pr(>|z|)
(Intercept)  0.175890   0.006176   28.48   <2e-16 ***
t8l         -0.018643   0.001376  -13.55   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 145934  on 105857  degrees of freedom
Residual deviance: 145750  on 105856  degrees of freedom
AIC: 145754

Number of Fisher Scoring iterations: 3

>
> #PREDICTED SEs
> phat.l.se.logit.nosvy <- predict(model.l.nosvy,se=TRUE)
> as.matrix(table(phat.l.se.logit.nosvy$se.fit))
                     [,1]
0.00632408017609573 14456
0.00633130215261306 15188
0.00741988836010757 12896
0.00743834214717549 10392
0.00923404822144662 13207
0.00925875968615561 15864
0.0114294663004145  12235
0.0114574202170594  11620
>
> #MODEL COEF. ESTIMATES (SURVEY DESIGN)
> model.l <- svyglm(qn58~t8l,design=all.svy,family=binomial)
> summary(model.l)

Call:
svyglm(qn58 ~ t8l, design = all.svy, family = binomial)

Survey design:
svydesign(id = ~psu, strata = ~stratum, weights = ~weight, data =
all.stratum,
    nest = T)

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept) -0.016004   0.023267  -0.688    0.492
t8l         -0.024496   0.004941  -4.958 1.13e-06 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

(Dispersion parameter for binomial family taken to be 0.934964)

Number of Fisher Scoring iterations: 2

>
> #PREDICTED SEs
> phat.l.logit.se <- predict(model.l,se=TRUE)
> as.matrix(table(phat.l.logit.se$se.fit))
                  [,1]
2.04867522818685 15188
2.05533753780321 14456
2.39885304369985 10392
2.41588959524594 12896
2.98273190185571 15864
3.00556161422958 13207
3.69102305734136 11620
3.71685978156846 12235

#THESE SEs are too large.


From andy_liaw at merck.com  Wed Jul 26 16:23:18 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 26 Jul 2006 10:23:18 -0400
Subject: [R] randomForest question  [Broadcast]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA029B1AF8@usctmx1106.merck.com>

When mtry is equal to total number of features, you just get regular bagging
(in the R package -- Breiman & Cutler's Fortran code samples variable with
replacement, so you can't do bagging with that).  There are cases when
bagging will do better than random feature selection (i.e., RF), even in
simulated data, but I'd say not very often.

HTH,
Andy


From: Arne.Muller at sanofi-aventis.com
> 
> Hello,
> 
> I've a question regarding randomForest (from the package with 
> same name). I've 16 featurs (nominative), 159 positive and 
> 318 negative cases that I'd like to classify (binary classification).
> 
> Using the tuning from the e1071 package it turns out that the 
> best performance if reached when using all 16 features per 
> tree (mtry=16). However, the documentation of randomForest 
> suggests to take the sqrt(#features), i.e. 4. How can I 
> explain this difference? When using all features this is the 
> same as a classical decision tree, with the difference that 
> the tree is built and tested with different data sets, right?
> 
> example (I've tried different configurations, incl. changing ntree):
> > param <- try(tune(randomForest, class ~ ., data=d.all318, 
> > range=list(mtry=c(4, 8, 16), ntree=c(1000))));
> >
> > summary(param)
> 
> Parameter tuning of `randomForest':
> 
> - sampling method: 10-fold cross validation 
> 
> - best parameters:
>  mtry ntree
>    16  1000
> 
> - best performance: 0.1571809 
> 
> - Detailed performance results:
>   mtry ntree     error
> 1    4  1000 0.1928635
> 2    8  1000 0.1634752
> 3   16  1000 0.1571809
> 
> 	thanks a lot for your help,
> 
> 	kind regards,
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From ricardosilva at serasa.com.br  Wed Jul 26 16:29:27 2006
From: ricardosilva at serasa.com.br (ricardosilva at serasa.com.br)
Date: Wed, 26 Jul 2006 11:29:27 -0300
Subject: [R] Moving Average
Message-ID: <OFA70B4F31.F087AE0C-ON032571B7.004F0536-032571B7.004F9A2D@serasa.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060726/65836ecf/attachment.pl 

From mariofalchi at yahoo.com  Wed Jul 26 16:34:07 2006
From: mariofalchi at yahoo.com (Mario Falchi)
Date: Wed, 26 Jul 2006 07:34:07 -0700 (PDT)
Subject: [R] String frequencies in rows
Message-ID: <20060726143407.58573.qmail@web32909.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060726/9c4fb4a2/attachment.pl 

From ggrothendieck at gmail.com  Wed Jul 26 16:55:25 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 26 Jul 2006 10:55:25 -0400
Subject: [R] Moving Average
In-Reply-To: <OFA70B4F31.F087AE0C-ON032571B7.004F0536-032571B7.004F9A2D@serasa.com.br>
References: <OFA70B4F31.F087AE0C-ON032571B7.004F0536-032571B7.004F9A2D@serasa.com.br>
Message-ID: <971536df0607260755s2ad943aesf46ed193adf72b90@mail.gmail.com>

See

?filter - simple and exponential are special cases
?runmean - in package caTools (the fastest)
?rollmean - in zoo package
?embed - can write your own using embed as basis
?sma - in package fSeries, also see ewma in same package

Probably other functions in other packages too.

On 7/26/06, ricardosilva at serasa.com.br <ricardosilva at serasa.com.br> wrote:
> Dear R-Users,
>
> How can I compute simple moving averages from a time series in R?
> Note that I do not want to estimate a MA model, just compute the MA's
> given a lenght (as excel does).
>
> Thanks
> ________________________________________
> Ricardo Gon?alves Silva, M. Sc.
> Apoio aos Processos de Modelagem Matem?tica
> Econometria & Inadimpl?ncia
> Serasa S.A.
> (11) - 6847-8889
> ricardosilva at serasa.com.br
>
> **********************************************************************************
> As informa??es contidas nesta mensagem e no(s) arquivo(s) anexo(s) s?o
> endere?adas exclusivamente ?(s) pessoa(s) e/ou institui??o(?es) acima
> indicada(s), podendo conter dados confidenciais, os quais n?o podem, sob
> qualquer forma ou pretexto, ser utilizados, divulgados, alterados,
> impressos ou copiados, total ou parcialmente, por pessoas n?o autorizadas.
> Caso n?o seja o destinat?rio, favor providenciar sua exclus?o e notificar
> o remetente imediatamente.  O uso impr?prio ser? tratado conforme as
> normas da empresa e da legisla??o em vigor.
> Esta mensagem expressa o posicionamento pessoal do subscritor e n?o
> reflete necessariamente a opini?o da Serasa.
> **********************************************************************************
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From markleeds at verizon.net  Wed Jul 26 16:56:10 2006
From: markleeds at verizon.net (markleeds at verizon.net)
Date: Wed, 26 Jul 2006 09:56:10 -0500 (CDT)
Subject: [R] Moving Average
Message-ID: <33245441.2523511153925770202.JavaMail.root@vms071.mailsrvcs.net>

>From: ricardosilva at serasa.com.br
>Date: 2006/07/26 Wed AM 09:29:27 CDT
>To: r-help at stat.math.ethz.ch
>Subject: [R] Moving Average

i think it was mave() in Splus. probably something similar
in R. do RSiteSearch("moving average")at an R prompt.


>Dear R-Users,
>
>How can I compute simple moving averages from a time series in R?
>Note that I do not want to estimate a MA model, just compute the MA's 
>given a lenght (as excel does).
>
>Thanks
>________________________________________
>Ricardo Gon?alves Silva, M. Sc.
>Apoio aos Processos de Modelagem Matem?tica
>Econometria & Inadimpl?ncia
>Serasa S.A.
>(11) - 6847-8889
>ricardosilva at serasa.com.br
>
>**********************************************************************************
>As informa??es contidas nesta mensagem e no(s) arquivo(s) anexo(s) s?o 
>endere?adas exclusivamente ?(s) pessoa(s) e/ou institui??o(?es) acima 
>indicada(s), podendo conter dados confidenciais, os quais n?o podem, sob 
>qualquer forma ou pretexto, ser utilizados, divulgados, alterados, 
>impressos ou copiados, total ou parcialmente, por pessoas n?o autorizadas. 
>Caso n?o seja o destinat?rio, favor providenciar sua exclus?o e notificar 
>o remetente imediatamente.  O uso impr?prio ser? tratado conforme as 
>normas da empresa e da legisla??o em vigor.
>Esta mensagem expressa o posicionamento pessoal do subscritor e n?o 
>reflete necessariamente a opini?o da Serasa.
>**********************************************************************************
>	[[alternative HTML version deleted]]
>
>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From bolker at ufl.edu  Wed Jul 26 16:59:35 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 26 Jul 2006 14:59:35 +0000 (UTC)
Subject: [R] String frequencies in rows
References: <20060726143407.58573.qmail@web32909.mail.mud.yahoo.com>
Message-ID: <loom.20060726T165802-135@post.gmane.org>

Mario Falchi <mariofalchi <at> yahoo.com> writes:

> I???m trying to evaluate the frequency of different strings in each row of a
data.frame :
> INPUT:
> ID G1 G2 G3 G4 ??? GN
> 1 AA BB AB AB ??? 

  Something like

z <- data[,-1]
table(z,row(z))

  ?

  Ben Bolker


From f.harrell at vanderbilt.edu  Wed Jul 26 17:15:47 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 26 Jul 2006 10:15:47 -0500
Subject: [R] Sweave and tth
In-Reply-To: <71257D09F114DA4A8E134DEAC70F25D305B00429@groamrexm03.amer.pfizer.com>
References: <71257D09F114DA4A8E134DEAC70F25D305B00429@groamrexm03.amer.pfizer.com>
Message-ID: <44C78723.2000204@vanderbilt.edu>

Kuhn, Max wrote:
> Dr. Harrell,
> 
>> I tried odfWeave to create an OpenOffice file and found that it 
>> exhausted the memory of my large linux machine and took a long time to
> 
>> run. 
> 
> Do you have any details about the problem that you encountered? A bug
> that someone else had pointed out might be the culprit. I have the
> default image format as png, but since a lot of linux systems don't have
> that device automatically available to them, I have a switch for the
> device in odfWeaveControl:
> 
>  plotDevice = ifelse(.Platform$OS.type == "windows", "png", "bitmap"),
> 
> The bitmap device units are in inches and the bmp device is in pixels.
> The bug is the default image size if 480 inches (whoops).
> 
> Can you try using:
> 
>   odfWeaveControl(
>    plotHeight = 5,
>    plotWidth = 5,
>    dispHeight = 5,
>    dispWidth = 5)
> 
> in your odfWeave call and let me know if this was the issue? I was able
> to reproduce the error on our linux systems and this fix worked (strange
> that the package passes R CMD check though).

Max,

I should have contacted you first - sorry about that.  png is working 
fine on my debian linux system, so I just ran

library(odfWeave)
odfWeave('/usr/local/lib/R/site-library/odfWeave/examples/examples.odt', 

   '/tmp/out.odt',
   control=
    odfWeaveControl(plotHeight = 5,plotWidth = 5,dispHeight = 
5,dispWidth = 5))

and it ran extremely fast, creating out.odt that loaded extremely fast 
into OpenOffice writer, unlike the first out.odt I had tried.

If you develop a way to include high-resolution graphics that will be 
even better.

I have updated http://biostat.mc.vanderbilt.edu/SweaveConvert accordingly.

> 
> If this works, Section 7 of the odfWeave manual lists two command line
> tools (not included in my package) that can do the conversion from odt
> to Word (or other formats)

Excellent

Thanks!
Frank

> 
>> I really appreciate Max Kuhn's efforts with odfWeave and hope to keep
> up 
>> with its development.
> 
> No problem. I'll be releasing a bug fix version to solve the device
> units issue. Also, others have reported problems with locales. I believe
> I have a fix for this issue too.
> 
>> Thanks.
>> -- 
>> Frank E Harrell Jr   Professor and Chair           School of Medicine
>>                       Department of Biostatistics   Vanderbilt
> University


From asr at ufl.edu  Wed Jul 26 17:16:25 2006
From: asr at ufl.edu (Allen S. Rout)
Date: 26 Jul 2006 11:16:25 -0400
Subject: [R] Branching on 'grep' returns...
Message-ID: <86irlk1jna.fsf@ufl.edu>




Greetings, all.

I'm fiddling with some text manipulation in R, and I've found
something which feels counterintuitive to my PERL-trained senses; I'm
hoping that I can glean new R intuition about the situation.

Here's an example, as concise as I could make it. 


trg<-c("this","that")

# these two work as I'd expected.
if ( grep("this",trg) ) { cat("Y\n") } else { cat("N\n") } 
if ( grep("that",trg) ) { cat("Y\n") } else { cat("N\n") } 

# These all fail with error 'argument is of length zero'
# if ( grep("other",trg) ) { cat("Y\n") } else { cat("N\n") } 
# if ( grep("other",trg) == TRUE) { cat("Y\n") } else { cat("N\n") } 
# if ( grep("other",trg) == 1) { cat("Y\n") } else { cat("N\n") } 


# This says that the result is a numeric zero.   Shouldn't I be able
#  to "if" on that, or at least compare it with a number?
grep("other", trg)

# I eventually decided this worked, but felt odd to me.
if ( any(grep("other",trg))) { cat("Y\n") } else { cat("N\n") } 


So, is the 'Wrap it in an any()' just normal R practice, and I'm too
new to know it?  Is there a more fundamental dumb move I'm making?




- Allen S. Rout


From andy_liaw at merck.com  Wed Jul 26 17:18:23 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 26 Jul 2006 11:18:23 -0400
Subject: [R] String frequencies in rows
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA029B1B3A@usctmx1106.merck.com>

It's usually faster to operate on columns of data frames, rather
than rows, so the following might help:

R> x
  G1 G2 G3 G4
1 AA BB AB AB
2 BB AB AB AA
3 AC CC AC AA
4 BB BB BB BB
R> xt <- as.data.frame(t(x))
R> sapply(xt, table)
$`1`

AA AB BB 
 1  2  1 

$`2`

AA AB BB 
 1  2  1 

$`3`

AA AC CC 
 1  2  1 

$`4`

BB 
 4 

Andy 

From: Mario Falchi
> 
> Hi All,
>  
> I???m trying to evaluate the frequency of different strings 
> in each row of a data.frame :
> INPUT:
> ID G1 G2 G3 G4 ??? GN
> 1 AA BB AB AB ???
> 2 BB AB AB AA ???
> 3 AC CC AC AA ???
> 4  BB BB BB BB??? 
> 
> The number of different strings can vary in each row.
>  
> My solution has been:
> for (i in 1:length(INPUT[,1])){
>  b=as.data.frame(table(t((INPUT[i,2:5]))))
> <some operations using the string values and frequencies> 
> (e.g. b for i==1 is:
>  AA 1
>  BB 1
>  AB 2 )
> } 
> 
> However my dataframe contains thousands rows and this script 
> takes a lot of time.
> Could someone suggest me a faster way?
>  
> Thank you very much,
> Mario Falchi
> 	[[alternative HTML version deleted]]
> 
>


From ggrothendieck at gmail.com  Wed Jul 26 17:31:43 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 26 Jul 2006 11:31:43 -0400
Subject: [R] Branching on 'grep' returns...
In-Reply-To: <86irlk1jna.fsf@ufl.edu>
References: <86irlk1jna.fsf@ufl.edu>
Message-ID: <971536df0607260831r1d3ad0bbr740c905dedbd2a83@mail.gmail.com>

If you are using grep then I think you have it right.  Note that

   "this" %in% trg

is also available.

On 26 Jul 2006 11:16:25 -0400, Allen S. Rout <asr at ufl.edu> wrote:
>
>
>
> Greetings, all.
>
> I'm fiddling with some text manipulation in R, and I've found
> something which feels counterintuitive to my PERL-trained senses; I'm
> hoping that I can glean new R intuition about the situation.
>
> Here's an example, as concise as I could make it.
>
>
> trg<-c("this","that")
>
> # these two work as I'd expected.
> if ( grep("this",trg) ) { cat("Y\n") } else { cat("N\n") }
> if ( grep("that",trg) ) { cat("Y\n") } else { cat("N\n") }
>
> # These all fail with error 'argument is of length zero'
> # if ( grep("other",trg) ) { cat("Y\n") } else { cat("N\n") }
> # if ( grep("other",trg) == TRUE) { cat("Y\n") } else { cat("N\n") }
> # if ( grep("other",trg) == 1) { cat("Y\n") } else { cat("N\n") }
>
>
> # This says that the result is a numeric zero.   Shouldn't I be able
> #  to "if" on that, or at least compare it with a number?
> grep("other", trg)
>
> # I eventually decided this worked, but felt odd to me.
> if ( any(grep("other",trg))) { cat("Y\n") } else { cat("N\n") }
>
>
> So, is the 'Wrap it in an any()' just normal R practice, and I'm too
> new to know it?  Is there a more fundamental dumb move I'm making?
>
>
>
>
> - Allen S. Rout
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jon3smith at hotmail.com  Wed Jul 26 17:34:06 2006
From: jon3smith at hotmail.com (Jonathan Smith)
Date: Wed, 26 Jul 2006 12:34:06 -0300
Subject: [R] implementing user defined covariance matirx
Message-ID: <BAY114-F2222F9F9D4748AF6F60F96FC5B0@phx.gbl>

I am trying to implement my own covariance matrix into R.    Then be able to 
use it in gls or lme or nlme to analyze some data.  I simply want to use 
corr=mymodel(form ~tim/peep) same as can use corr= corAR1 and many others.   
Having a terrible time trying to figure out how to do this.   I have found 
documentation saying that you can do this but cant find out how.  Any 
suggestings would be greatly  appreciated.

My covariance matrix is:

| 1	s1^g  	s2^g 	s3^g	? |
| s1^g	 1	s1^g	s2^g	? |
| s2^g	s1^g	 1	s1^g    ... |
| s3^g	s2^g	s1^g	1 	? |
|  :	   :	        :	:	: :: |
|  :	   :	       :	:	:::: |


Thankyou kindly for your time
Jonathan Smith


From quin.wills at googlemail.com  Wed Jul 26 17:44:17 2006
From: quin.wills at googlemail.com (Quin Wills)
Date: Wed, 26 Jul 2006 16:44:17 +0100
Subject: [R] PCA with not non-negative definite covariance
In-Reply-To: <1153815867.44c5d53b1a88b@webmail.univ-lyon1.fr>
Message-ID: <44c78dd3.06cf3667.027a.2ffb@mx.gmail.com>

Thanks.

I suppose that another option could be just to use classical
multi-dimensional scaling. By my understanding this is (if based on
Euclidian measure) completely analogous to PCA, and because it's based
explicitly on distances, I could easily exclude the variables with NA's on a
pairwise basis when calculating the distances.

Quin

-----Original Message-----
From: bady at univ-lyon1.fr [mailto:bady at univ-lyon1.fr] 
Sent: 25 July 2006 09:24 AM
To: Quin Wills
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] PCA with not non-negative definite covariance

Hi , hi all,

> Am I correct to understand from the previous discussions on this topic (a
> few years back) that if I have a matrix with missing values my PCA options
> seem dismal if:
> (1)     I don?t want to impute the missing values.
> (2)     I don?t want to completely remove cases with missing values.
> (3)     I do cov() with use=?pairwise.complete.obs?, as this produces
> negative eigenvalues (which it has in my case!).

(4) Maybe you can use the Non-linear Iterative Partial Least Squares
(NIPALS)
algorithm (intensively used in chemometry). S. Dray proposes a version of
this
procedure at http://pbil.univ-lyon1.fr/R/additifs.html.


Hope this help :)


Pierre



--------------------------------------------------------------------------
Ce message a ?t? envoy? depuis le webmail IMP (Internet Messaging Program)

-- 
No virus found in this incoming message.


 

--


From h.wickham at gmail.com  Wed Jul 26 17:58:47 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 26 Jul 2006 16:58:47 +0100
Subject: [R] PCA with not non-negative definite covariance
In-Reply-To: <44c78dd3.06cf3667.027a.2ffb@mx.gmail.com>
References: <1153815867.44c5d53b1a88b@webmail.univ-lyon1.fr>
	<44c78dd3.06cf3667.027a.2ffb@mx.gmail.com>
Message-ID: <f8e6ff050607260858n310b60edg1cb594c9303e24a@mail.gmail.com>

> I suppose that another option could be just to use classical
> multi-dimensional scaling. By my understanding this is (if based on
> Euclidian measure) completely analogous to PCA, and because it's based
> explicitly on distances, I could easily exclude the variables with NA's on a
> pairwise basis when calculating the distances.

I don't think it as straightforward as that because distances
calculated on observations with missing values will be smaller than
other distances.  I suspect adjusting for this would be in some way
equivalent to imputation.

Exactly what do you want a low-dimensional representation of your data
set for?  (And why are you concerned about negative eigenvalues?)

Hadley


From tkellermann at ukaachen.de  Wed Jul 26 17:58:53 2006
From: tkellermann at ukaachen.de (Thilo Kellermann)
Date: Wed, 26 Jul 2006 17:58:53 +0200
Subject: [R] implementing user defined covariance matirx
In-Reply-To: <BAY114-F2222F9F9D4748AF6F60F96FC5B0@phx.gbl>
References: <BAY114-F2222F9F9D4748AF6F60F96FC5B0@phx.gbl>
Message-ID: <200607261758.53310.tkellermann@ukaachen.de>

maybe the help of one of the corrStruct classes is of interest to you:
?corCompSymm
?corSymm
?corAR1
?corCar1
?corARMA
?corExp
?corGaus
?corLin
?corRatio
?corSphere

Good luck,
Thilo


On Wednesday 26 July 2006 17:34, Jonathan Smith wrote:
> I am trying to implement my own covariance matrix into R.    Then be able
> to use it in gls or lme or nlme to analyze some data.  I simply want to use
> corr=mymodel(form ~tim/peep) same as can use corr= corAR1 and many others.
> Having a terrible time trying to figure out how to do this.   I have found
> documentation saying that you can do this but cant find out how.  Any
> suggestings would be greatly  appreciated.
>
> My covariance matrix is:
> | 1	s1^g  	s2^g 	s3^g	? |
> | s1^g	 1	s1^g	s2^g	? |
> | s2^g	s1^g	 1	s1^g    ... |
> | s3^g	s2^g	s1^g	1 	? |
>
> Thankyou kindly for your time
> Jonathan Smith

-- 
________________________
Thilo Kellermann
Department of Psychiatry und Psychotherapy
RWTH Aachen University
Pauwelstr. 30
52074 Aachen
Tel.: +49 (0)241 / 8089977
Fax.: +49 (0)241 / 8082401
E-Mail: tkellermann at ukaachen.de


From mnair at iusb.edu  Wed Jul 26 18:02:56 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Wed, 26 Jul 2006 12:02:56 -0400
Subject: [R] Multcomp
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB4B80EC@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <A32055BDEA88C34BB3DBBCD2293807784CBA6D@iu-mssg-mbx109.ads.iu.edu>

Let me clarify with a simpler example what I want to accomplish
library(multcomp)
data(recovery)
Dcirec<-simint(minutes~blanket,data=recovery, conf.level=0.9,
alternative="less")
out.data.mat <- with(Dcirec,data.frame(estimate, conf.int, p.value.raw =
c(p.value.raw), p.value.bon, p.value.adj))


I want to generate the same type of plot using out.data.mat that I get
by plot(Dcirec) 

How do I specify the plot method how the data in out.data.mat is to be
plotted? 

I am interested in doing this because, I am running about 1500 different
comparisons, which creates 1500 different objects. I need to analyze
them and combine significant ones into one plot. 






-----Original Message-----
From: Greg Snow [mailto:Greg.Snow at intermountainmail.org] 
Sent: Tuesday, July 25, 2006 12:12 PM
To: Nair, Murlidharan T
Subject: RE: [R] Multcomp

Doing:

> str( fungus.cirec )

Suggests that fungus.cirec$conf.int contains the confidence intervals,
you can manually plot the subset that you are intereseted in (and label
them whatever you want)


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nair, Murlidharan
T
Sent: Saturday, July 22, 2006 11:00 AM
To: R-help at stat.math.ethz.ch
Subject: [R] Multcomp

Here it is again, hope this is more clear
 
I am using the following data (only a small subset is given):
 
Habitat Fungus.yield
Birch 20.83829053
Birch 22.9718181
Birch 22.28216829
Birch 24.23136797
Birch 22.32147961
Birch 20.30783598
Oak 27.24047258
Oak 29.7730014
Oak 30.12608508
Oak 25.76088669
Oak 30.14750974
Hornbeam 17.05307949
Hornbeam 15.32805111
Hornbeam 18.26920177
Hornbeam 21.30987049
Hornbeam 21.7173223

I am using the multcomp package to do multiple comparisons as follows 

library(multcomp) # loads the package

fungus<-read.table("fungi.txt", Header=T)    # Reads the data from file
saved as fungi.txt


fungus.cirec<-simint(Fungus.yield~Habitat,
data=fungus,conf.level=0.95,type =c("Tukey"))  # Computes cimultaneous
intervals using Tukey's method


plot(fungus.cirec)   # plots the data

The plot function plots all the comparisons, I want to plot only part of
the data since it clutters the graph. 

How do I plot only part of the data ?

How do I tell it to mark the significant comparisons?

How do I get rid of the field names in the plot? For eg. The plot labels
are HabitatBirch-HabitatOak, I want it to be labeled as Birch-Oak.

 

Hope I have posted it according to the guidelines, let me know
otherwise. 

Cheers .../Murli

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From gunter.berton at gene.com  Wed Jul 26 18:09:31 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 26 Jul 2006 09:09:31 -0700
Subject: [R] PCA with not non-negative definite covariance
In-Reply-To: <44c78dd3.06cf3667.027a.2ffb@mx.gmail.com>
Message-ID: <002901c6b0cd$e0f56280$711f210a@gne.windows.gene.com>

Not sure what "completely analagous" means; mds is nonlinear, PCA is linear.

In any case, the bottom line is that if you have high dimensional data with
"many" missing values, you cannot know what the multivariate distribution
looks like -- and you need a **lot** of data with many variables to usefully
characterize it anyway. So you must either make some assumptions about what
the distribution could be (including imputation methodology) or use any of
the many exploratory techniques available to learn what you can.
Thermodynamics holds -- you can't get something for nothing (you can't fool
Mother Nature).

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Quin Wills
> Sent: Wednesday, July 26, 2006 8:44 AM
> To: bady at univ-lyon1.fr
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] PCA with not non-negative definite covariance
> 
> Thanks.
> 
> I suppose that another option could be just to use classical
> multi-dimensional scaling. By my understanding this is (if based on
> Euclidian measure) completely analogous to PCA, and because it's based
> explicitly on distances, I could easily exclude the variables 
> with NA's on a
> pairwise basis when calculating the distances.
> 
> Quin
> 
> -----Original Message-----
> From: bady at univ-lyon1.fr [mailto:bady at univ-lyon1.fr] 
> Sent: 25 July 2006 09:24 AM
> To: Quin Wills
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] PCA with not non-negative definite covariance
> 
> Hi , hi all,
> 
> > Am I correct to understand from the previous discussions on 
> this topic (a
> > few years back) that if I have a matrix with missing values 
> my PCA options
> > seem dismal if:
> > (1)     I don?t want to impute the missing values.
> > (2)     I don?t want to completely remove cases with missing values.
> > (3)     I do cov() with use=?pairwise.complete.obs?, as 
> this produces
> > negative eigenvalues (which it has in my case!).
> 
> (4) Maybe you can use the Non-linear Iterative Partial Least Squares
> (NIPALS)
> algorithm (intensively used in chemometry). S. Dray proposes 
> a version of
> this
> procedure at http://pbil.univ-lyon1.fr/R/additifs.html.
> 
> 
> Hope this help :)
> 
> 
> Pierre
> 
> 
> 
> --------------------------------------------------------------
> ------------
> Ce message a ?t? envoy? depuis le webmail IMP (Internet 
> Messaging Program)
> 
> -- 
> No virus found in this incoming message.
> 
> 
>  
> 
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ariel at df.uba.ar  Fri Jul 28 07:30:08 2006
From: ariel at df.uba.ar (Ariel Chernomoretz)
Date: Fri, 28 Jul 2006 01:30:08 -0400
Subject: [R] odesolve loading problem
Message-ID: <44C9A0E0.70208@df.uba.ar>

Hi,
I get the following error message when loading the package odesolve ( R 
2.2.1 - odesolve 0.5.14 - AMD64 - Linux Fedora Core 4 ) :

 > library(odesolve)
Error in library.dynam(lib,package,package.lib) :
       shared library 'TRUE' not found
Error: package/namespace load failed for 'odesolve'

Any help would be greatly appreciated

Ariel./


From tlumley at u.washington.edu  Wed Jul 26 18:21:23 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 26 Jul 2006 09:21:23 -0700 (PDT)
Subject: [R] Branching on 'grep' returns...
In-Reply-To: <86irlk1jna.fsf@ufl.edu>
References: <86irlk1jna.fsf@ufl.edu>
Message-ID: <Pine.LNX.4.64.0607260912340.19845@homer21.u.washington.edu>

On Wed, 26 Jul 2006, Allen S. Rout wrote:
> # These all fail with error 'argument is of length zero'
> # if ( grep("other",trg) ) { cat("Y\n") } else { cat("N\n") }
> # if ( grep("other",trg) == TRUE) { cat("Y\n") } else { cat("N\n") }
> # if ( grep("other",trg) == 1) { cat("Y\n") } else { cat("N\n") }
>
>
> # This says that the result is a numeric zero.   Shouldn't I be able
> #  to "if" on that, or at least compare it with a number?
> grep("other", trg)

It is a numeric(0), that is, a zero-length vector of numbers. If you 
compare it with a number you get a zero-length logical vector. You can't 
get TRUE or FALSE, because a zero-length vector of 1s looks just like a 
zero-length vector of 0s, (or a zero-length vector of any other number)

In handling zero-length vectors (and in other vectorization contexts) it 
is useful to distinguish between vectorized functions, which return a 
vector of the same length as the input, and reducing functions, which 
return a vector of length 1.

The == operator is vectorized, but if() requires a condition of length 1, 
so they don't match.  The solution is to apply some reducing function. 
Two possible options are length() and (as you found) any().

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From mnair at iusb.edu  Wed Jul 26 18:23:15 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Wed, 26 Jul 2006 12:23:15 -0400
Subject: [R] Multcomp
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB4B80EC@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <A32055BDEA88C34BB3DBBCD2293807784CBA78@iu-mssg-mbx109.ads.iu.edu>

Let me clarify with a simpler example what I want to accomplish
library(multcomp)
data(recovery)
Dcirec<-simint(minutes~blanket,data=recovery, conf.level=0.9,
alternative="less") out.data.mat <- with(Dcirec,data.frame(estimate,
conf.int, p.value.raw = c(p.value.raw), p.value.bon, p.value.adj))


I want to generate the same type of plot using out.data.mat that I get
by plot(Dcirec) 

How do I specify the plot method how the data in out.data.mat is to be
plotted? 

I am interested in doing this because, I am running about 1500 different
comparisons, which creates 1500 different objects. I need to analyze
them and combine significant ones into one plot.

-----Original Message-----
From: Greg Snow [mailto:Greg.Snow at intermountainmail.org] 
Sent: Tuesday, July 25, 2006 12:12 PM
To: Nair, Murlidharan T
Subject: RE: [R] Multcomp

Doing:

> str( fungus.cirec )

Suggests that fungus.cirec$conf.int contains the confidence intervals,
you can manually plot the subset that you are intereseted in (and label
them whatever you want)


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nair, Murlidharan
T
Sent: Saturday, July 22, 2006 11:00 AM
To: R-help at stat.math.ethz.ch
Subject: [R] Multcomp

Here it is again, hope this is more clear
 
I am using the following data (only a small subset is given):
 
Habitat Fungus.yield
Birch 20.83829053
Birch 22.9718181
Birch 22.28216829
Birch 24.23136797
Birch 22.32147961
Birch 20.30783598
Oak 27.24047258
Oak 29.7730014
Oak 30.12608508
Oak 25.76088669
Oak 30.14750974
Hornbeam 17.05307949
Hornbeam 15.32805111
Hornbeam 18.26920177
Hornbeam 21.30987049
Hornbeam 21.7173223

I am using the multcomp package to do multiple comparisons as follows 

library(multcomp) # loads the package

fungus<-read.table("fungi.txt", Header=T)    # Reads the data from file
saved as fungi.txt


fungus.cirec<-simint(Fungus.yield~Habitat,
data=fungus,conf.level=0.95,type =c("Tukey"))  # Computes cimultaneous
intervals using Tukey's method


plot(fungus.cirec)   # plots the data

The plot function plots all the comparisons, I want to plot only part of
the data since it clutters the graph. 

How do I plot only part of the data ?

How do I tell it to mark the significant comparisons?

How do I get rid of the field names in the plot? For eg. The plot labels
are HabitatBirch-HabitatOak, I want it to be labeled as Birch-Oak.

 

Hope I have posted it according to the guidelines, let me know
otherwise. 

Cheers .../Murli

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Wed Jul 26 18:32:00 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 26 Jul 2006 12:32:00 -0400
Subject: [R] Multcomp
In-Reply-To: <A32055BDEA88C34BB3DBBCD2293807784CBA78@iu-mssg-mbx109.ads.iu.edu>
References: <07E228A5BE53C24CAD490193A7381BBB4B80EC@LP-EXCHVS07.CO.IHC.COM>
	<A32055BDEA88C34BB3DBBCD2293807784CBA78@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <971536df0607260932y1ece72cdt6a3a3f4c214d73ed@mail.gmail.com>

Look through
   multcomp:::plot.hmtest
to find out which components of an hmtest object are actually used.
Now look at what an hmtest object looks like by doing this

dput(Dcirec)

or looking through the source of the function that produces hmtest
objects.  With this information in hand we can construct one from
out.data.mat:

my.hmtest <- structure(list(
  estimate = t(t(structure(out.data.mat[,"estimate"],
     .Names = rownames(out.data.mat)))),
  conf.int = out.data.mat[,2:3],
  ctype = "Dunnett"),
  class = "hmtest")
plot(my.hmtest)

Note that this is a bit fragile since changes to the internal
representation of hmtest objects could cause your
object to cease working although as long as those
changes do not affect the three components we are
using it should be ok.  By the way I hard coded
"Dunnett" above since ctype is not available
in out.data.mat .

On 7/26/06, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> Let me clarify with a simpler example what I want to accomplish
> library(multcomp)
> data(recovery)
> Dcirec<-simint(minutes~blanket,data=recovery, conf.level=0.9,
> alternative="less") out.data.mat <- with(Dcirec,data.frame(estimate,
> conf.int, p.value.raw = c(p.value.raw), p.value.bon, p.value.adj))
>
>
> I want to generate the same type of plot using out.data.mat that I get
> by plot(Dcirec)
>
> How do I specify the plot method how the data in out.data.mat is to be
> plotted?
>
> I am interested in doing this because, I am running about 1500 different
> comparisons, which creates 1500 different objects. I need to analyze
> them and combine significant ones into one plot.
>
> -----Original Message-----
> From: Greg Snow [mailto:Greg.Snow at intermountainmail.org]
> Sent: Tuesday, July 25, 2006 12:12 PM
> To: Nair, Murlidharan T
> Subject: RE: [R] Multcomp
>
> Doing:
>
> > str( fungus.cirec )
>
> Suggests that fungus.cirec$conf.int contains the confidence intervals,
> you can manually plot the subset that you are intereseted in (and label
> them whatever you want)
>
>
> --
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at intermountainmail.org
> (801) 408-8111
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nair, Murlidharan
> T
> Sent: Saturday, July 22, 2006 11:00 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Multcomp
>
> Here it is again, hope this is more clear
>
> I am using the following data (only a small subset is given):
>
> Habitat Fungus.yield
> Birch 20.83829053
> Birch 22.9718181
> Birch 22.28216829
> Birch 24.23136797
> Birch 22.32147961
> Birch 20.30783598
> Oak 27.24047258
> Oak 29.7730014
> Oak 30.12608508
> Oak 25.76088669
> Oak 30.14750974
> Hornbeam 17.05307949
> Hornbeam 15.32805111
> Hornbeam 18.26920177
> Hornbeam 21.30987049
> Hornbeam 21.7173223
>
> I am using the multcomp package to do multiple comparisons as follows
>
> library(multcomp) # loads the package
>
> fungus<-read.table("fungi.txt", Header=T)    # Reads the data from file
> saved as fungi.txt
>
>
> fungus.cirec<-simint(Fungus.yield~Habitat,
> data=fungus,conf.level=0.95,type =c("Tukey"))  # Computes cimultaneous
> intervals using Tukey's method
>
>
> plot(fungus.cirec)   # plots the data
>
> The plot function plots all the comparisons, I want to plot only part of
> the data since it clutters the graph.
>
> How do I plot only part of the data ?
>
> How do I tell it to mark the significant comparisons?
>
> How do I get rid of the field names in the plot? For eg. The plot labels
> are HabitatBirch-HabitatOak, I want it to be labeled as Birch-Oak.
>
>
>
> Hope I have posted it according to the guidelines, let me know
> otherwise.
>
> Cheers .../Murli
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Setzer.Woodrow at epamail.epa.gov  Wed Jul 26 18:36:15 2006
From: Setzer.Woodrow at epamail.epa.gov (Setzer.Woodrow at epamail.epa.gov)
Date: Wed, 26 Jul 2006 12:36:15 -0400
Subject: [R] odesolve loading problem
In-Reply-To: <44C9A0E0.70208@df.uba.ar>
Message-ID: <OF768F7554.6E83810E-ON852571B7.005B1183-852571B7.005B35B2@epamail.epa.gov>

Hi Ariel,
The problem is that I specified the wrong dependency in the DEPENDS
field of the DESCRIPTION file of the package: I specified R version of
at least 2.2.1, but that should have been 2.3.1.  You have two choices
-- upgrade your R to 2.3.1, or install odesolve 0.5.13.  I will send an
updated version of the package to CRAN, with a note to the CRAN
maintainers about the problem, but that won't help you if you need to
use R version 2.2.1.  There is an archive of older R packages on CRAN,
linked to at the bottom of the "Contributed Packages" page on CRAN.
Please accept my apology for the inconvenience -- I rushed through a
change requested by a user, and did not take time to fully appreciate
the consequences.
Woody

R. Woodrow Setzer, Ph. D.
National Center for Computational Toxicology
US Environmental Protection Agency
Mail Drop B205-01/US EPA/RTP, NC 27711
Ph: (919) 541-0128    Fax: (919) 541-1194


                                                                        
             Ariel                                                      
             Chernomoretz                                               
             <ariel at df.uba.ar                                        To 
             >                        r-help at stat.math.ethz.ch          
             Sent by:                                                cc 
             r-help-bounces at s                                           
             tat.math.ethz.ch                                   Subject 
                                      [R] odesolve loading problem      
                                                                        
             07/28/2006 01:30                                           
             AM                                                         
                                                                        
                                                                        
                                                                        




Hi,
I get the following error message when loading the package odesolve ( R
2.2.1 - odesolve 0.5.14 - AMD64 - Linux Fedora Core 4 ) :

 > library(odesolve)
Error in library.dynam(lib,package,package.lib) :
       shared library 'TRUE' not found
Error: package/namespace load failed for 'odesolve'

Any help would be greatly appreciated

Ariel./

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Wed Jul 26 18:49:13 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 26 Jul 2006 12:49:13 -0400
Subject: [R] Multcomp
In-Reply-To: <971536df0607260932y1ece72cdt6a3a3f4c214d73ed@mail.gmail.com>
References: <07E228A5BE53C24CAD490193A7381BBB4B80EC@LP-EXCHVS07.CO.IHC.COM>
	<A32055BDEA88C34BB3DBBCD2293807784CBA78@iu-mssg-mbx109.ads.iu.edu>
	<971536df0607260932y1ece72cdt6a3a3f4c214d73ed@mail.gmail.com>
Message-ID: <971536df0607260949l79433c7eo1b2743989483cbb5@mail.gmail.com>

Here is a minor simplication:

my.hmtest <- structure(list(
   estimate = t(t(out.data.mat[,"estimate",drop=FALSE])),
   conf.int = out.data.mat[,2:3],
   ctype = "Dunnett"),
      class = "hmtest")
plot(my.hmtest)

On 7/26/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Look through
>   multcomp:::plot.hmtest
> to find out which components of an hmtest object are actually used.
> Now look at what an hmtest object looks like by doing this
>
> dput(Dcirec)
>
> or looking through the source of the function that produces hmtest
> objects.  With this information in hand we can construct one from
> out.data.mat:
>
> my.hmtest <- structure(list(
>  estimate = t(t(structure(out.data.mat[,"estimate"],
>     .Names = rownames(out.data.mat)))),
>  conf.int = out.data.mat[,2:3],
>  ctype = "Dunnett"),
>  class = "hmtest")
> plot(my.hmtest)
>
> Note that this is a bit fragile since changes to the internal
> representation of hmtest objects could cause your
> object to cease working although as long as those
> changes do not affect the three components we are
> using it should be ok.  By the way I hard coded
> "Dunnett" above since ctype is not available
> in out.data.mat .
>
> On 7/26/06, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> > Let me clarify with a simpler example what I want to accomplish
> > library(multcomp)
> > data(recovery)
> > Dcirec<-simint(minutes~blanket,data=recovery, conf.level=0.9,
> > alternative="less") out.data.mat <- with(Dcirec,data.frame(estimate,
> > conf.int, p.value.raw = c(p.value.raw), p.value.bon, p.value.adj))
> >
> >
> > I want to generate the same type of plot using out.data.mat that I get
> > by plot(Dcirec)
> >
> > How do I specify the plot method how the data in out.data.mat is to be
> > plotted?
> >
> > I am interested in doing this because, I am running about 1500 different
> > comparisons, which creates 1500 different objects. I need to analyze
> > them and combine significant ones into one plot.
> >
> > -----Original Message-----
> > From: Greg Snow [mailto:Greg.Snow at intermountainmail.org]
> > Sent: Tuesday, July 25, 2006 12:12 PM
> > To: Nair, Murlidharan T
> > Subject: RE: [R] Multcomp
> >
> > Doing:
> >
> > > str( fungus.cirec )
> >
> > Suggests that fungus.cirec$conf.int contains the confidence intervals,
> > you can manually plot the subset that you are intereseted in (and label
> > them whatever you want)
> >
> >
> > --
> > Gregory (Greg) L. Snow Ph.D.
> > Statistical Data Center
> > Intermountain Healthcare
> > greg.snow at intermountainmail.org
> > (801) 408-8111
> >
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nair, Murlidharan
> > T
> > Sent: Saturday, July 22, 2006 11:00 AM
> > To: R-help at stat.math.ethz.ch
> > Subject: [R] Multcomp
> >
> > Here it is again, hope this is more clear
> >
> > I am using the following data (only a small subset is given):
> >
> > Habitat Fungus.yield
> > Birch 20.83829053
> > Birch 22.9718181
> > Birch 22.28216829
> > Birch 24.23136797
> > Birch 22.32147961
> > Birch 20.30783598
> > Oak 27.24047258
> > Oak 29.7730014
> > Oak 30.12608508
> > Oak 25.76088669
> > Oak 30.14750974
> > Hornbeam 17.05307949
> > Hornbeam 15.32805111
> > Hornbeam 18.26920177
> > Hornbeam 21.30987049
> > Hornbeam 21.7173223
> >
> > I am using the multcomp package to do multiple comparisons as follows
> >
> > library(multcomp) # loads the package
> >
> > fungus<-read.table("fungi.txt", Header=T)    # Reads the data from file
> > saved as fungi.txt
> >
> >
> > fungus.cirec<-simint(Fungus.yield~Habitat,
> > data=fungus,conf.level=0.95,type =c("Tukey"))  # Computes cimultaneous
> > intervals using Tukey's method
> >
> >
> > plot(fungus.cirec)   # plots the data
> >
> > The plot function plots all the comparisons, I want to plot only part of
> > the data since it clutters the graph.
> >
> > How do I plot only part of the data ?
> >
> > How do I tell it to mark the significant comparisons?
> >
> > How do I get rid of the field names in the plot? For eg. The plot labels
> > are HabitatBirch-HabitatOak, I want it to be labeled as Birch-Oak.
> >
> >
> >
> > Hope I have posted it according to the guidelines, let me know
> > otherwise.
> >
> > Cheers .../Murli
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From Antonio_Paredes at aphis.usda.gov  Wed Jul 26 18:54:44 2006
From: Antonio_Paredes at aphis.usda.gov (Antonio_Paredes at aphis.usda.gov)
Date: Wed, 26 Jul 2006 11:54:44 -0500
Subject: [R] Bootstrap within litter
Message-ID: <OF2E819631.63927938-ON862571B7.005BEC33-862571B7.005C76FE@aphis.usda.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060726/78beecb0/attachment.pl 

From e.rapsomaniki at mail.cryst.bbk.ac.uk  Wed Jul 26 18:57:48 2006
From: e.rapsomaniki at mail.cryst.bbk.ac.uk (Eleni Rapsomaniki)
Date: Wed, 26 Jul 2006 17:57:48 +0100
Subject: [R] memory problems when combining randomForests
Message-ID: <1153933068.44c79f0c592da@webmail.cryst.bbk.ac.uk>

Dear all,

I am trying to train a randomForest using all my control data (12,000 cases, ~
20 explanatory variables, 2 classes). Because of memory constraints, I have
split my data into 7 subsets and trained a randomForest for each, hoping that
using combine() afterwards would solve the memory issue. Unfortunately,
combine() still runs out of memory. Is there anything else I can do? (I am not
using the formula version)

Many Thanks
Eleni Rapsomaniki


From Jahn at Mathematik.Uni-Mainz.DE  Wed Jul 26 19:47:06 2006
From: Jahn at Mathematik.Uni-Mainz.DE (Patrick Jahn)
Date: Wed, 26 Jul 2006 19:47:06 +0200
Subject: [R] bug?
Message-ID: <44C7C668.21160.1C63B05@localhost>

Dear All,
if you generate a sequence with small latitude like:

x<-seq(0,1,0.005)

and you ask for all points of this lattice how many points are in a neighbourhood with radius 0.01 
of each point:

v <- rep( 0 , length( x ) ) ; 
for (i in 1:length(x) )
     {  v[i] <- length(x[ abs(x-x[i]) < 0.01 ] ) ;   };

then the answer should be:  v = (2, 3, 3, 3, 3,.......,3, 3, 3, 3, 2), because every point instead 
of the borders has 3 points in a 0.01-neighbourhood.

but v contains also many 4 and also 5:

> v
  [1] 2 4 3 4 4 3 4 4 3 4 4 3 4 4 4 4 5 4 4 5 4 4 5 4 4 4 3 4 4 4 4 3 3 3 4 4 4
 [38] 4 3 3 4 4 4 4 3 3 4 4 4 4 3 3 3 3 3 3 4 4 4 4 3 3 3 3 3 3 3 3 3 4 4 4 4 3
 [75] 3 3 3 3 3 3 3 3 4 4 4 4 3 3 3 3 3 3 3 3 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3
[112] 3 3 3 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 3 3 3 3 3
[149] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
[186] 3 3 3 3 3 4 4 4 4 3 3 3 3 3 3 2

Could anyone explain this fact and help me to compute exactly on general data.

Thank you very much,
Patrick Jahn


From CodyH at BaylorHealth.edu  Wed Jul 26 19:45:51 2006
From: CodyH at BaylorHealth.edu (Hamilton, Cody)
Date: Wed, 26 Jul 2006 12:45:51 -0500
Subject: [R] R vs. Stata
Message-ID: <E52E20F6B4A2F548B16BB259DD5168CF02FB68B7@BHDAEXCH11.bhcs.pvt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060726/3eb2c97a/attachment.pl 

From tlumley at u.washington.edu  Wed Jul 26 20:11:38 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 26 Jul 2006 11:11:38 -0700 (PDT)
Subject: [R] bug?
In-Reply-To: <44C7C668.21160.1C63B05@localhost>
References: <44C7C668.21160.1C63B05@localhost>
Message-ID: <Pine.LNX.4.64.0607261105460.19845@homer21.u.washington.edu>

On Wed, 26 Jul 2006, Patrick Jahn wrote:

> Dear All,
> if you generate a sequence with small latitude like:
>
> x<-seq(0,1,0.005)
>
> and you ask for all points of this lattice how many points are in a neighbourhood with radius 0.01
> of each point:
>
> v <- rep( 0 , length( x ) ) ;
> for (i in 1:length(x) )
>     {  v[i] <- length(x[ abs(x-x[i]) < 0.01 ] ) ;   };
>
> then the answer should be:  v = (2, 3, 3, 3, 3,.......,3, 3, 3, 3, 2), because every point instead
> of the borders has 3 points in a 0.01-neighbourhood.
>
> but v contains also many 4 and also 5:
>
>> v
>  [1] 2 4 3 4 4 3 4 4 3 4 4 3 4 4 4 4 5 4 4 5 4 4 5 4 4 4 3 4 4 4 4 3 3 3 4 4 4
> [38] 4 3 3 4 4 4 4 3 3 4 4 4 4 3 3 3 3 3 3 4 4 4 4 3 3 3 3 3 3 3 3 3 4 4 4 4 3
> [75] 3 3 3 3 3 3 3 3 4 4 4 4 3 3 3 3 3 3 3 3 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3
> [112] 3 3 3 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 3 3 3 3 3
> [149] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
> [186] 3 3 3 3 3 4 4 4 4 3 3 3 3 3 3 2
>
> Could anyone explain this fact and help me to compute exactly on general 
> data.
>

Yes and no.

The fact is easily explained: 0.005 and 0.01 are not exactly representable 
in floating point, and so it will not be true for all x that x+0.005+0.005 
= x+0.01. This is a FAQ.

For this problem an easy solution is to multiply by 200 (or 1000) and work 
with integers, which can be exactly represented.  There is no solution for 
general data, although software for arbitrary precision floating point may 
come close (there was a message yesterday from someone trying to interface 
pari/gp, which does this, with R).



 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From gpagnon at emory.edu  Wed Jul 26 20:23:26 2006
From: gpagnon at emory.edu (Giuseppe Pagnoni)
Date: Wed, 26 Jul 2006 14:23:26 -0400
Subject: [R] (robust) mixed-effects model with covariate
In-Reply-To: <200607250937.03578.thilo@izkf.rwth-aachen.de>
References: <44C50E9A.30905@emory.edu> 
	<200607250937.03578.thilo@izkf.rwth-aachen.de>
Message-ID: <44C7B31E.5080503@emory.edu>

Dear Thilo,

many thanks for your reply.  I realized that there was an error in my 
formula which should have been:

aov(y ~ Group * (Time + Age) + Error (Subj/Time), data=df1)

or alternatively:

lme(RVP.A ~ Group*(Time+Age), random = ~ 1|Subj/Time,data=df1)

but I get different results in each case, and different still from the 
results of another stat program (JMP).
The problem is that I am not sure which one (if one indeed is) correct!

Also, in the model you proposed:

lme(y~Group*Time, random ~ age | Subj, data = df1)

it appears that age is not between the effects of interests, so I do not 
get an estimate of the significance of the Age or the Age*Group effect.

I have Pinheiro & Bates, and I read the first chapter but it didn't seem 
to provide an example analogous to my case.  Also, it looks like it 
would take me some months to study the book thoroughly and frankly that 
seems a bit excessive for such a (apparently?) simple problem....  I was 
hoping somebody would magically provide the correct syntax :-)  !

thanks again anyway for your help

best regards

   giuseppe



Thilo Kellermann wrote:
> On Monday 24 July 2006 20:16, Giuseppe Pagnoni wrote:
>   
>> Dear all,
>>
>> First of all I apologize if you received this twice: I was checking the
>> archive and I noticed that the text was scrubbed from the message,
>> probably due to some setting in my e-mail program.
>>
>>
>> I am unsure about how to specify a model in R and I thought of asking
>> some advice to the list. I have two groups ("Group"= A, B) of subjects,
>> with each subject undertaking a test before and after a certain
>> treatment ("Time"= pre, post). Additionally, I want to enter
>> the age of the subject as a covariate (the performance on the test is
>> affected by age), and I also want to allow different slopes for the
>> effect of age in the two groups of subjects (age might affect the
>> performance of the two groups differentially).
>>
>> Is the right model to use something like the following?
>>
>> aov (y ~ Group*Time + Group*Age + Error(Subj/Group), data=df1 )
>>
>> (If I enter that command, within summary, I get the following:
>> Error() model is singular in: aov(y ~ Group * Time + Group * Age +
>> Error(Subj/Group), data = df1))
>>
>>     
> try:
> aov(y~Group*Time*Age + Error(Subj*Time*Age), data = df1)
> which specifies an ANOVA (but not with mixed effects) with three main effects 
> and all interaction terms plus an error term that is independent between 
> groups (!) and relates to within subjects variability.
>
> For a "real" mixed effects analysis you should use the (n)lme function from 
> the nlme package and one possible model could look like this:
>
> lme(y~Group*Time, random ~ age | Subj, data = df1)
>
> but the exact specification depends on your assumptions, in which it is 
> possible to specify two or three models and compare their fits with anova(). 
> For more information on mixed effects you should consult:
> Jose C. Pinheiro & Douglas M. Bates (2000) Mixed-Effects Models in S and 
> S-PLUS. Springer, New York.
>
> Good luck,
> Thilo
>
>   
>> As a second question: I have an outlier in one of the two groups. The
>> outlier is not due to a measurement error but simply to the performance
>> of the subject (possibly related to his medical history, but I have no
>> way to determine that with certainty). This subject is
>> signaled to be an outlier within its group: averaging the pre and post
>> values for the performance of the subjects in his group, the Grubbs test
>> yields a probability of 0.002 for the subject to be an outlier (the
>> subject is marked as a significant outlier also if I
>> perform the test separately on the pre and the post data).
>>
>> If I remove this subject from its group, I get significant effects of
>> Group and Group X Age (not using the R formula above, but another stat
>> software), but if I leave the subject in those effects disappear. Since
>> I understand that removing outliers is always worrysome, I would like to
>> know if it is possible in R to estimate a model similar to that outlined
>> above but in a resistant/robust fashion, and what would be the actual
>> syntax to do that. I will very much appreciate any help or suggestion
>> about this.
>>
>> thanks in advance and best regards
>>
>> giuseppe
>>     
>
>   


-- 
---------------------------------
Giuseppe Pagnoni
Psychiatry and Behavioral Sciences
Emory University School of Medicine
1639 Pierce Drive, Suite 4000
Atlanta, GA, 30322
tel: 404.712.8431
fax: 404.727.3233


From pburns at pburns.seanet.com  Wed Jul 26 20:35:07 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 26 Jul 2006 19:35:07 +0100
Subject: [R] R vs. Stata
In-Reply-To: <E52E20F6B4A2F548B16BB259DD5168CF02FB68B7@BHDAEXCH11.bhcs.pvt>
References: <E52E20F6B4A2F548B16BB259DD5168CF02FB68B7@BHDAEXCH11.bhcs.pvt>
Message-ID: <44C7B5DB.8070600@pburns.seanet.com>

There is some discussion in:

http://www.burns-stat.com/pages/Tutor/R_relative_statpack.pdf

which can also be found at the UCLA website.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Hamilton, Cody wrote:

>I have read some very good reviews comparing R (or Splus) to SAS.  Does
>anyone know if there are any reviews comparing R (or Splus) to Stata?  I
>am trying to get others to try R in my department, and I have never used
>Stata.
>
>
>
>Regards, -Cody
>
>
>
>Cody Hamilton, Ph.D
>
>Institute for Health Care Research and Improvement
>
>Baylor Health Care System
>
>(214) 265-3618
>
>
>
>
>
>This e-mail, facsimile, or letter and any files or attachme...{{dropped}}


From bill.shipley at usherbrooke.ca  Wed Jul 26 21:01:07 2006
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Wed, 26 Jul 2006 15:01:07 -0400
Subject: [R] residual df in lmer and simulation results
Message-ID: <002401c6b0e5$dabcbf40$bb1ad284@BIO041>

Hello.  Douglas Bates has explained in a previous posting to R why he does
not output residual degrees of freedom, F values and probabilities in the
mixed model (lmer) function:  because the usual degrees of freedom (obs -
fixed df -1) are not exact and are really only upper bounds.  I am
interpreting what he said but I am not a professional statistician, so I
might be getting this wrong...
Does anyone know of any more recent results, perhaps from simulations, that
quantify the degree of bias that using such upper bounds for the demoninator
degrees of freedom produces?  Is it possible to calculate a lower bounds for
such degrees of freedom?

Thanks for any help.

Bill Shipley
North American Editor, Annals of Botany
Editor, "Population and Community Biology" series, Springer Publishing
D?partement de biologie, Universit? de Sherbrooke,
Sherbrooke (Qu?bec) J1K 2R1 CANADA
Bill.Shipley at USherbrooke.ca
http://pages.usherbrooke.ca/jshipley/recherche/


From dgerlanc at gmail.com  Wed Jul 26 21:04:00 2006
From: dgerlanc at gmail.com (Daniel Gerlanc)
Date: Wed, 26 Jul 2006 15:04:00 -0400
Subject: [R] How to split the left and right hand terms of a formula
Message-ID: <84c9e3cb0607261204v4de74c37t74de67f682a190d2@mail.gmail.com>

Hello All,

I've sent a few messages to the list regarding splitting a formula
into its right and left hand terms.  Thanks to everyone who has
responded.

I believe that the best way to extract the left and right hand terms
as character vectors follows:

library(nlme)

formula <- y ~ x + z

left.term <- all.vars(getResponseFormula(formula))
covariates <- all.vars(getCovariateFormula(formula))

Thanks!

Dan Gerlanc
Williams College


From sachinj.2006 at yahoo.com  Wed Jul 26 21:20:21 2006
From: sachinj.2006 at yahoo.com (Sachin J)
Date: Wed, 26 Jul 2006 12:20:21 -0700 (PDT)
Subject: [R] arima() function - issues
Message-ID: <20060726192021.6005.qmail@web37601.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060726/43893e24/attachment.pl 

From kylehall at vt.edu  Wed Jul 26 21:54:37 2006
From: kylehall at vt.edu (Kyle Hall)
Date: Wed, 26 Jul 2006 15:54:37 -0400
Subject: [R] ks.test exact p-value
Message-ID: <44EF9768@zathras>

R enthusiasts!

I have been simulating daily in-stream bacteria concentrations using a variety 
of scenarios.  I am using the ks.test (two sample,two-sided) for analysis. My 
data sets are both of equal size (n=64).

My question is this:  For the two sample, two sided ks.test, how is the exact 
P-value calculated?

I have not been able to find an explicit citation of how the P-value is 
calculated.  I have read the help file which cites three publications and 
assigns those pubs to one version or another of the ks.test (eg. one sample, 
one-sided, etc.).  I have also read to the Conover book (referenced but not 
cited).  Conover tables are adapted from a Birnbaum and Hall (1960) paper, and 
then I have also found tables by Kim and Jennrich, but I feel I have found 
some disagreement between these sources (with regards to critical D values and 
p-values).  I would like to compare the methods utilized by R version 2.3.0 on 
Windows XP.  
Can anybody tell me the exact method for calculating the p-value for a 
two-sample, two sided ks.test?
Any help would greatly appreciated!

Kyle Hall
Graduate Research Assistant
Biological Systems Engineering
Virginia Tech
(540) 231-2083


From btyner at gmail.com  Wed Jul 26 22:15:24 2006
From: btyner at gmail.com (Benjamin Tyner)
Date: Wed, 26 Jul 2006 16:15:24 -0400
Subject: [R] configure fails for R 2.3.1 on SunOS 5.8
Message-ID: <44C7CD5C.4090905@stat.purdue.edu>

Does this mean I need to use '--with-readline=no'? configure says:

checking build system type... sparc-sun-solaris2.8
checking host system type... sparc-sun-solaris2.8
loading site script './config.site'
loading build specific script './config.site'
checking for pwd... /usr/bin/pwd
checking whether builddir is srcdir... yes
checking for working aclocal... found
checking for working autoconf... found
checking for working automake... found
checking for working autoheader... found
checking for working makeinfo... found
checking for gawk... gawk
checking for egrep... egrep
checking whether ln -s works... yes
checking for ranlib... ranlib
checking for bison... bison -y
checking for ar... ar
checking for a BSD-compatible install... tools/install-sh -c
checking for sed... /usr/xpg4/bin/sed
checking for more... /usr/bin/more
checking for perl... no
checking for false... /usr/bin/false
configure: WARNING: you cannot build the object documentation system
checking for dvips... no
checking for tex... no
checking for latex... no
configure: WARNING: you cannot build DVI versions of the R manuals
checking for makeindex... no
checking for pdftex... no
checking for pdflatex... no
configure: WARNING: you cannot build PDF versions of the R manuals
checking for makeinfo... /p/gnu/makeinfo
checking for install-info... /p/gnu/install-info
checking for unzip... /usr/bin/unzip
checking for zip... /usr/bin/zip
checking for gzip... /usr/bin/gzip
checking for firefox... /p/firefox/firefox
using default browser ... /p/firefox/firefox
checking for acroread... no
checking for acroread4... no
checking for xpdf... no
checking for gv... /p/X11/gv
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking whether gcc needs -traditional... no
checking how to run the C preprocessor... gcc -E
checking for f95... f95
checking whether we are using the GNU Fortran 77 compiler... no
checking whether f95 accepts -g... yes
checking for g++... g++
checking whether we are using the GNU C++ compiler... yes
checking whether g++ accepts -g... yes
checking how to run the C++ preprocessor... g++ -E
checking whether __attribute__((visibility())) is supported... no
checking whether gcc accepts -fvisibility... no
checking whether f95 accepts -fvisibility... yes
checking for a sed that does not truncate output... /usr/xpg4/bin/sed
checking for ld used by gcc... /usr/ccs/bin/ld
checking if the linker (/usr/ccs/bin/ld) is GNU ld... no
checking for /usr/ccs/bin/ld option to reload object files... -r
checking for BSD-compatible nm... /usr/ccs/bin/nm -p
checking how to recognise dependent libraries... pass_all
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... no
checking for unistd.h... yes
checking dlfcn.h usability... yes
checking dlfcn.h presence... yes
checking for dlfcn.h... yes
checking the maximum length of command line arguments... 262144
checking command to parse /usr/ccs/bin/nm -p output from gcc object... ok
checking for objdir... .libs
checking for ranlib... (cached) ranlib
checking for strip... strip
checking if gcc static flag  works... yes
checking if gcc supports -fno-rtti -fno-exceptions... yes
checking for gcc option to produce PIC... -fPIC
checking if gcc PIC flag -fPIC works... yes
checking if gcc supports -c -o file.o... yes
checking whether the gcc linker (/usr/ccs/bin/ld) supports shared 
libraries... yes
checking whether -lc should be explicitly linked in... yes
checking dynamic linker characteristics... solaris2.8 ld.so
checking how to hardcode library paths into programs... immediate
checking whether stripping libraries is possible... no
checking if libtool supports shared libraries... yes
checking whether to build shared libraries... yes
checking whether to build static libraries... no
configure: creating libtool
appending configuration tag "CXX" to libtool
checking for ld used by g++... /usr/ccs/bin/ld
checking if the linker (/usr/ccs/bin/ld) is GNU ld... no
checking whether the g++ linker (/usr/ccs/bin/ld) supports shared 
libraries... yes
checking for g++ option to produce PIC... -fPIC
checking if g++ PIC flag -fPIC works... yes
checking if g++ supports -c -o file.o... yes
checking whether the g++ linker (/usr/ccs/bin/ld) supports shared 
libraries... yes
checking dynamic linker characteristics... solaris2.8 ld.so
checking how to hardcode library paths into programs... immediate
checking whether stripping libraries is possible... no
appending configuration tag "F77" to libtool
checking if libtool supports shared libraries... yes
checking whether to build shared libraries... yes
checking whether to build static libraries... no
checking for f95 option to produce PIC... -fPIC
checking if f95 PIC flag -fPIC works... no
checking if f95 supports -c -o file.o... yes
checking whether the f95 linker (/usr/ccs/bin/ld) supports shared 
libraries... yes
checking dynamic linker characteristics... f95: Warning: Option 
-print-search-dirs passed to ld, if ld is invoked, ignored otherwise
Undefined                       first referenced
 symbol                             in file
main                                /opt/SUNWspro/WS6U2/lib/crt1.o
ld: fatal: Symbol referencing errors. No output written to a.out
solaris2.8 ld.so
checking how to hardcode library paths into programs... immediate
checking whether stripping libraries is possible... no
checking whether makeinfo version is at least 4.7... yes
checking for cos in -lm... yes
checking for sin in -lm... yes
checking for dlopen in -ldl... yes
checking readline/history.h usability... no
checking readline/history.h presence... no
checking for readline/history.h... no
checking readline/readline.h usability... no
checking readline/readline.h presence... no
checking for readline/readline.h... no
checking for rl_callback_read_char in -lreadline... no
checking for main in -lncurses... no
checking for main in -ltermcap... yes
checking for rl_callback_read_char in -lreadline... no
checking for history_truncate_file... no
configure: error: --with-readline=yes (default) and headers/libs are not 
available


From kpappu at mednet.ucla.edu  Wed Jul 26 22:30:07 2006
From: kpappu at mednet.ucla.edu (Kartik Pappu)
Date: Wed, 26 Jul 2006 13:30:07 -0700
Subject: [R] creating a color display matrix
Message-ID: <d42f810106b8d7db2eb0e75af8c4fa14@mednet.ucla.edu>

Hello all,

I am trying to use R to create a colored data matrix.  I have my data 
which (after certain steps of normalization and log transformation) 
gives me a "x by y" matrix of numbers between 0 and -5.  I want to be 
able to create from this matrix of numbers a "x by y" image (box) that 
contains x*y squares and somehow uses the value in the original matrix 
to come up with a color that corresponds to the number.  Hence each box 
will be colored on a scale between 0 and 5.  For example my data could 
look like this:

                 X1.FcH      X2.FcH      X3.FcH     X4.FcH      X5.FcH   
  X6.FcH       X7.FcH
1-AP        0.09667593 -4.66298640 -1.28299697 -4.8739017 -4.95862831 
-5.178603 -4.878524750
2-AP       -4.69186869 -0.08547776 -4.56495440 -4.8348255 -4.80256152 
-5.121531 -4.894347108
3-AP       -1.71380667 -4.52626124 -0.06810053 -4.8703810 -4.65657593 
-5.024595 -4.824712621
4-AP       -4.47968850 -4.48604718 -4.44314403 -0.1569536 -4.86436977 
-4.988196 -4.550416356
5-AP       -4.64616469 -4.53888807 -4.78163386 -4.9162949 -0.01729274 
-5.061663 -0.769960777
6-AP       -4.61047573 -4.60917414 -4.72514817 -5.0084772 -4.87797740 
-0.284934 -1.782745357
7-AP       -4.48157167 -4.61850313 -4.72241281 -4.8694868 -1.66122821 
-3.887898 -0.002522157

How do I make a 7 x 7 box that has 49 squares and each square has a 
color in the RGB spectrum that corresponds to the value.  So for 
example in the matrix above the biggest number which is 0.00966 (at 
position 1 , 1) could be set to RED and the smallest number which is 
5.0084 (at position 6, 5) can be set to BLUE and all the other numbers 
be shades on a scale of Red going to Blue.

I hope this problem makes sense.  I am rather new to R and was 
wondering if there was a function or solution to this problem out 
there.

Thanks
Kartik


----------------------------------------------------------
IMPORTANT WARNING:  This email (and any attachments) is only intended for the use of the person or entity to which it is addressed, and may contain information that is privileged and confidential.  You, the recipient, are obligated to maintain it in a safe, secure and confidential manner.  Unauthorized redisclosure or failure to maintain confidentiality may subject you to federal and state penalties. If you are not the recipient, please immediately notify us by return email, and delete this message from your computer.


From davidr at rhotrading.com  Wed Jul 26 22:31:24 2006
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Wed, 26 Jul 2006 15:31:24 -0500
Subject: [R] creating a color display matrix
Message-ID: <F9F2A641C593D7408925574C05A7BE77076092@rhopost.rhotrading.com>

?image has worked for me.

David L. Reiner
Rho Trading Securities, LLC
Chicago  IL  60605
312-362-4963

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kartik Pappu
Sent: Wednesday, July 26, 2006 3:30 PM
To: r-help at stat.math.ethz.ch
Subject: [R] creating a color display matrix

Hello all,

I am trying to use R to create a colored data matrix.  I have my data 
which (after certain steps of normalization and log transformation) 
gives me a "x by y" matrix of numbers between 0 and -5.  I want to be 
able to create from this matrix of numbers a "x by y" image (box) that 
contains x*y squares and somehow uses the value in the original matrix 
to come up with a color that corresponds to the number.  Hence each box 
will be colored on a scale between 0 and 5.  For example my data could 
look like this:

                 X1.FcH      X2.FcH      X3.FcH     X4.FcH      X5.FcH

  X6.FcH       X7.FcH
1-AP        0.09667593 -4.66298640 -1.28299697 -4.8739017 -4.95862831 
-5.178603 -4.878524750
2-AP       -4.69186869 -0.08547776 -4.56495440 -4.8348255 -4.80256152 
-5.121531 -4.894347108
3-AP       -1.71380667 -4.52626124 -0.06810053 -4.8703810 -4.65657593 
-5.024595 -4.824712621
4-AP       -4.47968850 -4.48604718 -4.44314403 -0.1569536 -4.86436977 
-4.988196 -4.550416356
5-AP       -4.64616469 -4.53888807 -4.78163386 -4.9162949 -0.01729274 
-5.061663 -0.769960777
6-AP       -4.61047573 -4.60917414 -4.72514817 -5.0084772 -4.87797740 
-0.284934 -1.782745357
7-AP       -4.48157167 -4.61850313 -4.72241281 -4.8694868 -1.66122821 
-3.887898 -0.002522157

How do I make a 7 x 7 box that has 49 squares and each square has a 
color in the RGB spectrum that corresponds to the value.  So for 
example in the matrix above the biggest number which is 0.00966 (at 
position 1 , 1) could be set to RED and the smallest number which is 
5.0084 (at position 6, 5) can be set to BLUE and all the other numbers 
be shades on a scale of Red going to Blue.

I hope this problem makes sense.  I am rather new to R and was 
wondering if there was a function or solution to this problem out 
there.

Thanks
Kartik


----------------------------------------------------------
IMPORTANT WARNING:  This email (and any attachments) is only intended
for the use of the person or entity to which it is addressed, and may
contain information that is privileged and confidential.  You, the
recipient, are obligated to maintain it in a safe, secure and
confidential manner.  Unauthorized redisclosure or failure to maintain
confidentiality may subject you to federal and state penalties. If you
are not the recipient, please immediately notify us by return email, and
delete this message from your computer.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From p.dalgaard at biostat.ku.dk  Wed Jul 26 22:31:27 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Jul 2006 22:31:27 +0200
Subject: [R] configure fails for R 2.3.1 on SunOS 5.8
In-Reply-To: <44C7CD5C.4090905@stat.purdue.edu>
References: <44C7CD5C.4090905@stat.purdue.edu>
Message-ID: <x2ac6w2jmo.fsf@turmalin.kubism.ku.dk>

Benjamin Tyner <btyner at gmail.com> writes:

> Does this mean I need to use '--with-readline=no'? configure says:


> configure: error: --with-readline=yes (default) and headers/libs are not 
> available

Yes, or that you need to install readline headers/libs (in a
sufficiently recent version), or that you have installed them, but not
where R looks for them, so that you need to specify the location.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From btyner at gmail.com  Wed Jul 26 23:14:38 2006
From: btyner at gmail.com (Benjamin Tyner)
Date: Wed, 26 Jul 2006 17:14:38 -0400
Subject: [R] configure fails for R 2.3.1 on SunOS 5.8
In-Reply-To: <x2ac6w2jmo.fsf@turmalin.kubism.ku.dk>
References: <44C7CD5C.4090905@stat.purdue.edu>
	<x2ac6w2jmo.fsf@turmalin.kubism.ku.dk>
Message-ID: <44C7DB3E.3070401@stat.purdue.edu>

Thanks; configure completes succesfully using --with-readline=no. 
However, toward the end of running make, it reports

mkdir ../share/locale/en at quot
mkdir ../share/locale/en at quot/LC_MESSAGES
  en at quot
you should 'make docs' now ...
*** Error code 255
make: Fatal error: Command failed for target `R.1'
Current working directory ~/btyner/R-2.3.1/doc
*** Error code 1 (ignored)
building all R object docs (text, HTML, LaTeX, examples)
you need Perl version 5 to build the R object docs
*** Error code 1
make: Fatal error: Command failed for target `help-indices'
Current working directory ~/btyner/R-2.3.1/src/library
*** Error code 1
make: Fatal error: Command failed for target `docs'
Current working directory ~/btyner/R-2.3.1/src/library
*** Error code 1 (ignored)
begin installing recommended package VR

I do not have perl installed, as I thought one could install sans 
documentation.

Ben


Peter Dalgaard wrote:

>Benjamin Tyner <btyner at gmail.com> writes:
>
>  
>
>>Does this mean I need to use '--with-readline=no'? configure says:
>>    
>>
>
>
>  
>
>>configure: error: --with-readline=yes (default) and headers/libs are not 
>>available
>>    
>>
>
>Yes, or that you need to install readline headers/libs (in a
>sufficiently recent version), or that you have installed them, but not
>where R looks for them, so that you need to specify the location.
>
>  
>


From quin.wills at googlemail.com  Wed Jul 26 23:20:15 2006
From: quin.wills at googlemail.com (Quin Wills)
Date: Wed, 26 Jul 2006 22:20:15 +0100
Subject: [R] PCA with not non-negative definite covariance
In-Reply-To: <002901c6b0cd$e0f56280$711f210a@gne.windows.gene.com>
Message-ID: <44c7dc92.6462f47b.027a.ffffc08f@mx.gmail.com>

My apologies (in response to the last 2 replies). I should write sensibly -
including subject titles that make grammatical sense.

(1) By analogous, I mean that using classical MDS with Euclidian distance is
equivalent to plotting the first "k" principle components.
(2) Agreed re. distribution assumptions.
(3) Agreed re. the need to use some kind of imputation for calculating
distances. I'm thinking pairwise exclusion for correlation.

Re. why I want to do this is simply for graphically representing my data.

Quin



-----Original Message-----
From: Berton Gunter [mailto:gunter.berton at gene.com] 
Sent: 26 July 2006 05:10 PM
To: 'Quin Wills'; bady at univ-lyon1.fr
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] PCA with not non-negative definite covariance

Not sure what "completely analagous" means; mds is nonlinear, PCA is linear.

In any case, the bottom line is that if you have high dimensional data with
"many" missing values, you cannot know what the multivariate distribution
looks like -- and you need a **lot** of data with many variables to usefully
characterize it anyway. So you must either make some assumptions about what
the distribution could be (including imputation methodology) or use any of
the many exploratory techniques available to learn what you can.
Thermodynamics holds -- you can't get something for nothing (you can't fool
Mother Nature).

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Quin Wills
> Sent: Wednesday, July 26, 2006 8:44 AM
> To: bady at univ-lyon1.fr
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] PCA with not non-negative definite covariance
> 
> Thanks.
> 
> I suppose that another option could be just to use classical
> multi-dimensional scaling. By my understanding this is (if based on
> Euclidian measure) completely analogous to PCA, and because it's based
> explicitly on distances, I could easily exclude the variables 
> with NA's on a
> pairwise basis when calculating the distances.
> 
> Quin
> 
> -----Original Message-----
> From: bady at univ-lyon1.fr [mailto:bady at univ-lyon1.fr] 
> Sent: 25 July 2006 09:24 AM
> To: Quin Wills
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] PCA with not non-negative definite covariance
> 
> Hi , hi all,
> 
> > Am I correct to understand from the previous discussions on 
> this topic (a
> > few years back) that if I have a matrix with missing values 
> my PCA options
> > seem dismal if:
> > (1)     I don?t want to impute the missing values.
> > (2)     I don?t want to completely remove cases with missing values.
> > (3)     I do cov() with use=?pairwise.complete.obs?, as 
> this produces
> > negative eigenvalues (which it has in my case!).
> 
> (4) Maybe you can use the Non-linear Iterative Partial Least Squares
> (NIPALS)
> algorithm (intensively used in chemometry). S. Dray proposes 
> a version of
> this
> procedure at http://pbil.univ-lyon1.fr/R/additifs.html.
> 
> 
> Hope this help :)
> 
> 
> Pierre
> 
> 
> 
> --------------------------------------------------------------
> ------------
> Ce message a ?t? envoy? depuis le webmail IMP (Internet 
> Messaging Program)
> 
> -- 
> No virus found in this incoming message.
> 
> 
>  
> 
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
No virus found in this incoming message.


 

--


From whit.armstrong at hcmny.com  Wed Jul 26 23:52:00 2006
From: whit.armstrong at hcmny.com (Armstrong, Whit)
Date: Wed, 26 Jul 2006 17:52:00 -0400
Subject: [R] RODBC on linux
Message-ID: <E58BE6136618CF4C964F6EC7773AE5699FA5B4@ex4.nyc.hcmny.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060726/f16b6d3a/attachment.pl 

From mesomeris at yahoo.co.uk  Thu Jul 27 00:00:10 2006
From: mesomeris at yahoo.co.uk (Spiros Mesomeris)
Date: Wed, 26 Jul 2006 23:00:10 +0100 (BST)
Subject: [R] Codes; White's heteroscedasticity test and GARCH models
Message-ID: <20060726220010.63679.qmail@web86911.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060726/587c713c/attachment.pl 

From John.Kerpel at infores.com  Thu Jul 27 00:09:50 2006
From: John.Kerpel at infores.com (Kerpel, John)
Date: Wed, 26 Jul 2006 17:09:50 -0500
Subject: [R] Codes; White's heteroscedasticity test and GARCH models
Message-ID: <44A8B25381923D4F93B74B2676A50F6D02F87FE6@MAIL1.infores.com>

Check tseries and fSeries packages for GARCH

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spiros Mesomeris
Sent: Wednesday, July 26, 2006 5:00 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Codes; White's heteroscedasticity test and GARCH models

Hello,
   
  I have just recently started using R and was wondering whether anybody
had a code written for White's heteroscedasticity correction for
standard errors.
   
  Also, can anybody share a code for the GARCH(1,1) and GARCH-in-mean
models for modelling regression residuals?
   
   
  Thanks a lot in advance,
  Spyros

 		
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Achim.Zeileis at wu-wien.ac.at  Thu Jul 27 00:11:04 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 27 Jul 2006 00:11:04 +0200 (CEST)
Subject: [R] Codes; White's heteroscedasticity test and GARCH models
In-Reply-To: <20060726220010.63679.qmail@web86911.mail.ukl.yahoo.com>
References: <20060726220010.63679.qmail@web86911.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.58.0607270007300.21517@thorin.ci.tuwien.ac.at>

Spyros:

>   I have just recently started using R and was wondering whether anybody
>   had a code written for White's heteroscedasticity correction for
>   standard errors.

See package "sandwich", particularly functions vcovHC() and sandwich().

>   Also, can anybody share a code for the GARCH(1,1) and GARCH-in-mean
>   models for modelling regression residuals?

See function garch() in package "tseries".

Furthermore, the econometrics and finance task views might be helpful for
you:
  http://CRAN.R-project.org/src/contrib/Views/Econometrics.html
  http://CRAN.R-project.org/src/contrib/Views/Finance.html

hth,
Z


From john_d_mchenry at yahoo.com  Thu Jul 27 01:04:47 2006
From: john_d_mchenry at yahoo.com (John McHenry)
Date: Wed, 26 Jul 2006 16:04:47 -0700 (PDT)
Subject: [R] Overplotting: plot() invocation looks ugly ... suggestions?
In-Reply-To: <f8e6ff050607260054p2306554am3b6dd6a669d45869@mail.gmail.com>
Message-ID: <20060726230448.16476.qmail@web35414.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060726/b999d0ff/attachment.pl 

From CodyH at BaylorHealth.edu  Thu Jul 27 02:02:36 2006
From: CodyH at BaylorHealth.edu (Hamilton, Cody)
Date: Wed, 26 Jul 2006 19:02:36 -0500
Subject: [R] R vs. Stata
In-Reply-To: <44C7B5DB.8070600@pburns.seanet.com>
Message-ID: <E52E20F6B4A2F548B16BB259DD5168CF02FB68C4@BHDAEXCH11.bhcs.pvt>


Thanks Patrick! 
-Cody

-----Original Message-----
From: Patrick Burns [mailto:pburns at pburns.seanet.com]
Sent: Wednesday, July 26, 2006 13:35 PM
To: Hamilton, Cody
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] R vs. Stata

There is some discussion in:

http://www.burns-stat.com/pages/Tutor/R_relative_statpack.pdf

which can also be found at the UCLA website.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Hamilton, Cody wrote:

>I have read some very good reviews comparing R (or Splus) to SAS.  Does
>anyone know if there are any reviews comparing R (or Splus) to Stata?
I
>am trying to get others to try R in my department, and I have never
used
>Stata.
>
>
>
>Regards, -Cody
>
>
>
>Cody Hamilton, Ph.D
>
>Institute for Health Care Research and Improvement
>
>Baylor Health Care System
>
>(214) 265-3618
>
>
>
>
>
>This e-mail, facsimile, or letter and any files or attachments
transmitted with it contains information that is confidential and
privileged. This information is intended only for the use of the
individual(s) and entity(ies) to whom it is addressed. If you are the
intended recipient, further disclosures are prohibited without proper
authorization. If you are not the intended recipient, any disclosure,
copying, printing, or use of this information is strictly prohibited and
possibly a violation of federal or state law and regulations. If you
have received this information in error, please notify Baylor Health
Care System immediately at 1-866-402-1661 or via e-mail at
privacy at baylorhealth.edu. Baylor Health Care System, its subsidiaries,
and affiliates hereby claim all applicable privileges related to this
information.
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
> 
>

This e-mail, facsimile, or letter and any files or attachmen...{{dropped}}


From andy_liaw at merck.com  Thu Jul 27 02:12:29 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 26 Jul 2006 20:12:29 -0400
Subject: [R] memory problems when combining randomForests [Broadcast]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAA76D@usctmx1106.merck.com>

You need to give us more details, like how you call randomForest, versions
of the package and R itself, etc.  Also, see if this helps you:
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/32918.html

Andy
 
From: Eleni Rapsomaniki
> 
> Dear all,
> 
> I am trying to train a randomForest using all my control data 
> (12,000 cases, ~ 20 explanatory variables, 2 classes). 
> Because of memory constraints, I have split my data into 7 
> subsets and trained a randomForest for each, hoping that 
> using combine() afterwards would solve the memory issue. 
> Unfortunately,
> combine() still runs out of memory. Is there anything else I 
> can do? (I am not using the formula version)
> 
> Many Thanks
> Eleni Rapsomaniki
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From ggrothendieck at gmail.com  Thu Jul 27 02:53:57 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 26 Jul 2006 20:53:57 -0400
Subject: [R] Overplotting: plot() invocation looks ugly ... suggestions?
In-Reply-To: <20060726230448.16476.qmail@web35414.mail.mud.yahoo.com>
References: <f8e6ff050607260054p2306554am3b6dd6a669d45869@mail.gmail.com>
	<20060726230448.16476.qmail@web35414.mail.mud.yahoo.com>
Message-ID: <971536df0607261753o1f4d1130j62aadb06666b2314@mail.gmail.com>

With the lattice package it would be done like this (where
the panel.points function places large red pluses on
the plot):

xyplot(Consumption ~ Quarter, group = Year, data, type = "o")
trellis.focus("panel", 1, 1)
panel.points(1:4, mean.per.quarter, pch = "+", cex = 2, col = "red")
trellis.unfocus()


On 7/26/06, John McHenry <john_d_mchenry at yahoo.com> wrote:
> Hi Hadley,
>
> Thanks for your suggestion.
>
> The description of ggplot states:
>
> Description:   ... It combines the advantages of both base and lattice
>                graphics ... and you can still build up a plot step by
>               step from multiple data sources
>
> So I thought I'd try to enhance the plot by adding in the means from each quarter (this is snagged directly from ESS):
>
> >   qplot(Quarter, Consumption, data=data, type=c("point","line"), id=data$Year)
>    ( mean.per.quarter<- with(data, tapply(Consumption, Quarter, mean)) )
>    points(mean.per.quarter, pch="+", cex=2.0)
>
>  qplot(Quarter, Consumption, data=data, type=c("point","line"), id=data$Year)
> >  ( mean.per.quarter<- with(data, tapply(Consumption, Quarter, mean)) )
>    1     2     3     4
> 888.2 709.2 616.4 832.8
> >  points(mean.per.quarter, pch="+", cex=2.0)
> Error in plot.xy(xy.coords(x, y), type = type, ...) :
>    plot.new has not been called yet
> >
> >
>
> Now I'm green behind the ears when it comes to R, so I'm guessing that there is some major conflict between base graphics and lattice graphics, which I thought ggplot avoided, given the library help blurb.
>
> I'm assuming that there must be a way to add points / lines to lattice / ggplot graphics (in the latter case it seems to be via ggpoint, or some such)? But is there a way that allows me to add via:
>
> points(mean.per.quarter, pch="+", cex=2.0)
>
> and similar, or do I have to learn the lingo for lattice / ggplot?
>
> Thanks,
>
> Jack.
>
>
>
> hadley wickham <h.wickham at gmail.com> wrote: > And if lattice is ok then try this:
> >
> > library(lattice)
> > xyplot(Consumption ~ Quarter, group = Year, data, type = "o")
>
> Or you can use ggplot:
>
> install.packages("ggplot")
> library(ggplot)
> qplot(Quarter, Consumption, data=data,type=c("point","line"), id=data$Year)
>
> Unfortunately this has uncovered a couple of small bugs for me to fix
> (no automatic legend, and have to specify the data frame explicitly)
>
> The slighly more verbose example below shows you what it should look like.
>
> data$Year <- factor(data$Year)
> p <- ggplot(data, aes=list(x=Quarter, y=Consumption, id=Year, colour=Year))
> ggline(ggpoint(p), size=2)
>
> Regards,
>
> Hadley
>
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From hdevries at prgs.edu  Thu Jul 27 03:35:26 2006
From: hdevries at prgs.edu (Vries, Han de)
Date: Wed, 26 Jul 2006 18:35:26 -0700
Subject: [R] seq unexpected behavior
Message-ID: <D1F2CAEEE291E444A50DC3C5DD5847B3044BE2@smmail9.rand.org>

seq(0.1, 0.9 - 0.8, by = 0.1) gives the following error message:

Error in seq.default(0.1, 0.9 - 0.8, by = 0.1) : 
        wrong sign in 'by' argument

but seq(0.1, 0.8 - 0.7, by = 0.1) gives
[1] 0.1
(no error message)

Why do I get an error message in the first case?
Han



> sessionInfo()
R version 2.2.1, 2005-12-20, i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"
"datasets" 
[7] "base"   

(NB I also tried version 2.3.1 and got the same result - both versions
are precompiled)
  
> Sys.getlocale()
[1] "LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252"


--------------------

This email message is for the sole use of the intended recip...{{dropped}}


From MSchwartz at mn.rr.com  Thu Jul 27 03:47:44 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 26 Jul 2006 20:47:44 -0500
Subject: [R] seq unexpected behavior
In-Reply-To: <D1F2CAEEE291E444A50DC3C5DD5847B3044BE2@smmail9.rand.org>
References: <D1F2CAEEE291E444A50DC3C5DD5847B3044BE2@smmail9.rand.org>
Message-ID: <1153964864.5036.33.camel@localhost.localdomain>

On Wed, 2006-07-26 at 18:35 -0700, Vries, Han de wrote:
> seq(0.1, 0.9 - 0.8, by = 0.1) gives the following error message:
> 
> Error in seq.default(0.1, 0.9 - 0.8, by = 0.1) : 
>         wrong sign in 'by' argument
> 
> but seq(0.1, 0.8 - 0.7, by = 0.1) gives
> [1] 0.1
> (no error message)
> 
> Why do I get an error message in the first case?
> Han


See R FAQ 7.31 Why doesn't R think these numbers are equal?

> print(0.9 - 0.8, 20)
[1] 0.09999999999999997780

> print(0.8 - 0.7, 20)
[1] 0.10000000000000008882


In the first case, the result of the subtraction is slightly less than
0.1, resulting in a negative interval. In the second case, it is
slightly greater than 0.1, which is OK.

HTH,

Marc Schwartz


From phhs80 at gmail.com  Thu Jul 27 03:49:51 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Thu, 27 Jul 2006 02:49:51 +0100
Subject: [R] Non-parametric four-way interactions?
Message-ID: <6ade6f6c0607261849p576cc8c6of0e3247ef1c2134b@mail.gmail.com>

Dear All

I am trying to study four-way interactions in an ANOVA problem.
However, qqnorm+qqline result

(at http://phhs80.googlepages.com/qqnorm.png)

is not promising regarding the normality of data (960 observations).
The result of Shapiro-Wilk test is also not encouraging:

W = 0.9174, p-value < 2.2e-16

(I am aware of the fact that normality tests tend to reject normality
for large samples.)

By the way, the histogram is at:

http://phhs80.googlepages.com/hist.png

To circumvent the problem, I looked for non-parametric tests, but I
found nothing, but the article:

http://www.pgia.ac.lk/socs/asasl/journal_papers/PDFformat/g.bakeerathanpaper-2.pdf

Finally, my question is: has R got implemented functions to use
non-parametric tests to avoid the fulfillment of the normality
assumption required to study four-way interactions?

Thanks in advance,

Paul


From MSchwartz at mn.rr.com  Thu Jul 27 03:56:49 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 26 Jul 2006 20:56:49 -0500
Subject: [R] RODBC on linux
In-Reply-To: <E58BE6136618CF4C964F6EC7773AE5699FA5B4@ex4.nyc.hcmny.com>
References: <E58BE6136618CF4C964F6EC7773AE5699FA5B4@ex4.nyc.hcmny.com>
Message-ID: <1153965409.5036.42.camel@localhost.localdomain>

On Wed, 2006-07-26 at 17:52 -0400, Armstrong, Whit wrote:
> Anyone out there using Linux RODBC and unixODBC to connect to a
> Microsoft SQL server?
> 
> If possible can someone post a sample .odbc.ini file?
> 
> I saw a few discussions on the archives a few years ago, but no config
> file details were available.
> 
> Thanks,
> Whit

Whit,

Do you have a Linux ODBC driver for SQL Server?  unixODBC is simply the
driver manager, not the driver itself.

MS does not offer (not surprisingly) an ODBC driver for Unix/Linux.
There are resources available however and these might be helpful:

http://www.sommarskog.se/mssql/unix.html

Note that Easysoft provides (at a cost) an ODBC-ODBC bridge for
Unix/Linux platforms which supports ODBC connections to SQL Server:

http://www.easysoft.com/products/data_access/odbc_odbc_bridge/index.html

I am using RODBC to connect from a FC5 system to an Oracle 10g server
running on RHEL, however Oracle provides the ODBC driver for Linux that
can work with the unixODBC facilities.

Also, note that there is a R-sig-DB e-mail list:

https://stat.ethz.ch/mailman/listinfo/r-sig-db

HTH,

Marc Schwartz


From edd at debian.org  Thu Jul 27 04:38:05 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 26 Jul 2006 21:38:05 -0500
Subject: [R] RODBC on linux
In-Reply-To: <1153965409.5036.42.camel@localhost.localdomain>
References: <E58BE6136618CF4C964F6EC7773AE5699FA5B4@ex4.nyc.hcmny.com>
	<1153965409.5036.42.camel@localhost.localdomain>
Message-ID: <17608.9997.259881.170231@basebud.nulle.part>


On 26 July 2006 at 20:56, Marc Schwartz wrote:
| On Wed, 2006-07-26 at 17:52 -0400, Armstrong, Whit wrote:
| > Anyone out there using Linux RODBC and unixODBC to connect to a
| > Microsoft SQL server?
[...]
| Do you have a Linux ODBC driver for SQL Server?  unixODBC is simply the
| driver manager, not the driver itself.
| 
| MS does not offer (not surprisingly) an ODBC driver for Unix/Linux.

But there is the FreeTDS project (package freetds-dev in Debian) with its
associated ODBC drive (package tdsodbc, from the FreeTDS sources). At some
point a few years ago, a colleague and I were trying to coax that and
unixOBDC to let R (on Solaris) talk to Sybase (on Solaris) and got it to
work.  MS-SQL is (AFAIK) a descendant of Sybase code originally licensed by
MS, hence the common FreeTDS code lineage).  So it should be doable.

Luckily I haven't needed to talk to MS SQL myself so the usual grain of salt
alert...  And sorry, hence no working .odbc.ini to share.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From MSchwartz at mn.rr.com  Thu Jul 27 04:53:45 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 26 Jul 2006 21:53:45 -0500
Subject: [R] RODBC on linux
In-Reply-To: <17608.9997.259881.170231@basebud.nulle.part>
References: <E58BE6136618CF4C964F6EC7773AE5699FA5B4@ex4.nyc.hcmny.com>
	<1153965409.5036.42.camel@localhost.localdomain>
	<17608.9997.259881.170231@basebud.nulle.part>
Message-ID: <1153968825.5036.57.camel@localhost.localdomain>

On Wed, 2006-07-26 at 21:38 -0500, Dirk Eddelbuettel wrote:
> On 26 July 2006 at 20:56, Marc Schwartz wrote:
> | On Wed, 2006-07-26 at 17:52 -0400, Armstrong, Whit wrote:
> | > Anyone out there using Linux RODBC and unixODBC to connect to a
> | > Microsoft SQL server?
> [...]
> | Do you have a Linux ODBC driver for SQL Server?  unixODBC is simply the
> | driver manager, not the driver itself.
> | 
> | MS does not offer (not surprisingly) an ODBC driver for Unix/Linux.
> 
> But there is the FreeTDS project (package freetds-dev in Debian) with its
> associated ODBC drive (package tdsodbc, from the FreeTDS sources). At some
> point a few years ago, a colleague and I were trying to coax that and
> unixOBDC to let R (on Solaris) talk to Sybase (on Solaris) and got it to
> work.  MS-SQL is (AFAIK) a descendant of Sybase code originally licensed by
> MS, hence the common FreeTDS code lineage).  So it should be doable.
> 
> Luckily I haven't needed to talk to MS SQL myself so the usual grain of salt
> alert...  And sorry, hence no working .odbc.ini to share.

FreeTDS was one of the options listed on the first URL that I had
included.  :-)

Here is the direct link:

  http://www.freetds.org/ 

Regards,

Marc


From f.harrell at vanderbilt.edu  Thu Jul 27 05:42:58 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 26 Jul 2006 22:42:58 -0500
Subject: [R] Non-parametric four-way interactions?
In-Reply-To: <6ade6f6c0607261849p576cc8c6of0e3247ef1c2134b@mail.gmail.com>
References: <6ade6f6c0607261849p576cc8c6of0e3247ef1c2134b@mail.gmail.com>
Message-ID: <44C83642.2090806@vanderbilt.edu>

Paul Smith wrote:
> Dear All
> 
> I am trying to study four-way interactions in an ANOVA problem.
> However, qqnorm+qqline result
> 
> (at http://phhs80.googlepages.com/qqnorm.png)
> 
> is not promising regarding the normality of data (960 observations).
> The result of Shapiro-Wilk test is also not encouraging:
> 
> W = 0.9174, p-value < 2.2e-16
> 
> (I am aware of the fact that normality tests tend to reject normality
> for large samples.)
> 
> By the way, the histogram is at:
> 
> http://phhs80.googlepages.com/hist.png
> 
> To circumvent the problem, I looked for non-parametric tests, but I
> found nothing, but the article:
> 
> http://www.pgia.ac.lk/socs/asasl/journal_papers/PDFformat/g.bakeerathanpaper-2.pdf
> 
> Finally, my question is: has R got implemented functions to use
> non-parametric tests to avoid the fulfillment of the normality
> assumption required to study four-way interactions?
> 
> Thanks in advance,
> 
> Paul

Yes, although I seldom want to look at 4th order interactions.  You can 
fit a proportional odds model for an ordinal response which is a 
generalization of the Wilcoxon/Kruskal-Wallis approach, and allows one 
to have N-1 intercepts in the model when there are N data points (i.e., 
it works even with no ties in the data).  However if N is large the 
matrix operations will be prohibitive and you might reduce Y to 100-tile 
groups.  The PO model uses only the ranks of Y so is monotonic 
transformation invariant.

library(Design)  # also requires library(Hmisc)
f <- lrm(y ~ a*b*c*d)
f
anova(f)

Also see the polr function in VR
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From ripley at stats.ox.ac.uk  Thu Jul 27 07:17:00 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 27 Jul 2006 06:17:00 +0100 (BST)
Subject: [R] RODBC on linux
In-Reply-To: <1153965409.5036.42.camel@localhost.localdomain>
References: <E58BE6136618CF4C964F6EC7773AE5699FA5B4@ex4.nyc.hcmny.com>
	<1153965409.5036.42.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.64.0607270613210.7787@gannet.stats.ox.ac.uk>

On Wed, 26 Jul 2006, Marc Schwartz wrote:

> On Wed, 2006-07-26 at 17:52 -0400, Armstrong, Whit wrote:
> > Anyone out there using Linux RODBC and unixODBC to connect to a
> > Microsoft SQL server?
> > 
> > If possible can someone post a sample .odbc.ini file?
> > 
> > I saw a few discussions on the archives a few years ago, but no config
> > file details were available.
> > 
> > Thanks,
> > Whit
> 
> Whit,
> 
> Do you have a Linux ODBC driver for SQL Server?  unixODBC is simply the
> driver manager, not the driver itself.
> 
> MS does not offer (not surprisingly) an ODBC driver for Unix/Linux.
> There are resources available however and these might be helpful:
> 
> http://www.sommarskog.se/mssql/unix.html
> 
> Note that Easysoft provides (at a cost) an ODBC-ODBC bridge for
> Unix/Linux platforms which supports ODBC connections to SQL Server:
> 
> http://www.easysoft.com/products/data_access/odbc_odbc_bridge/index.html

Several people have successfully used that, from the earliest days of 
RODBC: I believe it was part of Michael Lapsley's motivation to write 
RODBC.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From robert-mcfadden at o2.pl  Thu Jul 27 08:43:31 2006
From: robert-mcfadden at o2.pl (Robert Mcfadden)
Date: Thu, 27 Jul 2006 08:43:31 +0200
Subject: [R] arima() function - issues
In-Reply-To: <20060726192021.6005.qmail@web37601.mail.mud.yahoo.com>
Message-ID: <000201c6b147$f9ec86e0$1191680a@robert>

Seasonal ARIMA is in the package: 
http://www.robhyndman.info/Rlibrary/forecast/index.html
If you are interested in time series look at
http://zoonek2.free.fr/UNIX/48_R/all.html


Hope this help
Robert


> -----Original Message-----
> From: r-help-bounces w stat.math.ethz.ch [mailto:r-help-
> bounces w stat.math.ethz.ch] On Behalf Of Sachin J
> Sent: Wednesday, July 26, 2006 9:20 PM
> To: R-help w stat.math.ethz.ch
> Subject: [R] arima() function - issues
> 
> Hi,
> 
>   My query is related to ARIMA function in stats package.
> While looking for the time series literature I found following link which
> highlights discrepancy in "arima" function while dealing with
> differenced time series. Is there a substitute function similar to
> "sarima" mentioned in the following website implemened in R? Any pointers
> would
> be of great help.
> 
>   http://lib.stat.cmu.edu/general/stoffer/tsa2/Rissues.htm
> 
>   Thanx in advance.
>   Sachin
> 
> 
> 
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help w stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dray at biomserv.univ-lyon1.fr  Thu Jul 27 10:04:09 2006
From: dray at biomserv.univ-lyon1.fr (=?UTF-8?B?U3TDqXBoYW5lIERyYXk=?=)
Date: Thu, 27 Jul 2006 10:04:09 +0200
Subject: [R] PCA with not non-negative definite covariance
In-Reply-To: <44c7dc92.6462f47b.027a.ffffc08f@mx.gmail.com>
References: <44c7dc92.6462f47b.027a.ffffc08f@mx.gmail.com>
Message-ID: <44C87379.5060205@biomserv.univ-lyon1.fr>

As said by Pierre Bady,
an answer to your question is NIPALS analysis.
PCA is usually obtained by the diagonalization of a variance-covariance 
matrix. But it can also be obtained by an iterative proedure which 
consists in two regressions. NIPLAS is an implementation of this 
iterative procedure and is strictly equivalent to PCA when there is no 
missing values.
The adavantage of NIPALS is that it can be used with missing values. 
However, note that the convergence is not always obtained (it depends of 
the number and distribution of missing values).
You can find a description of the method and the algorithm here:

http://biomserv.univ-lyon1.fr/~dray/articles/SD165.html

Sincerely,


Quin Wills wrote:

>My apologies (in response to the last 2 replies). I should write sensibly -
>including subject titles that make grammatical sense.
>
>(1) By analogous, I mean that using classical MDS with Euclidian distance is
>equivalent to plotting the first "k" principle components.
>(2) Agreed re. distribution assumptions.
>(3) Agreed re. the need to use some kind of imputation for calculating
>distances. I'm thinking pairwise exclusion for correlation.
>
>Re. why I want to do this is simply for graphically representing my data.
>
>Quin
>
>
>
>-----Original Message-----
>From: Berton Gunter [mailto:gunter.berton at gene.com] 
>Sent: 26 July 2006 05:10 PM
>To: 'Quin Wills'; bady at univ-lyon1.fr
>Cc: r-help at stat.math.ethz.ch
>Subject: RE: [R] PCA with not non-negative definite covariance
>
>Not sure what "completely analagous" means; mds is nonlinear, PCA is linear.
>
>In any case, the bottom line is that if you have high dimensional data with
>"many" missing values, you cannot know what the multivariate distribution
>looks like -- and you need a **lot** of data with many variables to usefully
>characterize it anyway. So you must either make some assumptions about what
>the distribution could be (including imputation methodology) or use any of
>the many exploratory techniques available to learn what you can.
>Thermodynamics holds -- you can't get something for nothing (you can't fool
>Mother Nature).
>
>-- Bert Gunter
>Genentech Non-Clinical Statistics
>South San Francisco, CA
> 
>"The business of the statistician is to catalyze the scientific learning
>process."  - George E. P. Box
> 
> 
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Quin Wills
>>Sent: Wednesday, July 26, 2006 8:44 AM
>>To: bady at univ-lyon1.fr
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] PCA with not non-negative definite covariance
>>
>>Thanks.
>>
>>I suppose that another option could be just to use classical
>>multi-dimensional scaling. By my understanding this is (if based on
>>Euclidian measure) completely analogous to PCA, and because it's based
>>explicitly on distances, I could easily exclude the variables 
>>with NA's on a
>>pairwise basis when calculating the distances.
>>
>>Quin
>>
>>-----Original Message-----
>>From: bady at univ-lyon1.fr [mailto:bady at univ-lyon1.fr] 
>>Sent: 25 July 2006 09:24 AM
>>To: Quin Wills
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] PCA with not non-negative definite covariance
>>
>>Hi , hi all,
>>
>>    
>>
>>>Am I correct to understand from the previous discussions on 
>>>      
>>>
>>this topic (a
>>    
>>
>>>few years back) that if I have a matrix with missing values 
>>>      
>>>
>>my PCA options
>>    
>>
>>>seem dismal if:
>>>(1)     I don?t want to impute the missing values.
>>>(2)     I don?t want to completely remove cases with missing values.
>>>(3)     I do cov() with use=?pairwise.complete.obs?, as 
>>>      
>>>
>>this produces
>>    
>>
>>>negative eigenvalues (which it has in my case!).
>>>      
>>>
>>(4) Maybe you can use the Non-linear Iterative Partial Least Squares
>>(NIPALS)
>>algorithm (intensively used in chemometry). S. Dray proposes 
>>a version of
>>this
>>procedure at http://pbil.univ-lyon1.fr/R/additifs.html.
>>
>>
>>Hope this help :)
>>
>>
>>Pierre
>>
>>
>>
>>--------------------------------------------------------------
>>------------
>>Ce message a ?t? envoy? depuis le webmail IMP (Internet 
>>Messaging Program)
>>
>>-- 
>>No virus found in this incoming message.
>>
>>
>> 
>>
>>--
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
>>    
>>
>
>  
>


-- 
St?phane DRAY (dray at biomserv.univ-lyon1.fr )
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - Lyon I
43, Bd du 11 Novembre 1918, 69622 Villeurbanne Cedex, France
Tel: 33 4 72 43 27 57       Fax: 33 4 72 43 13 88
http://biomserv.univ-lyon1.fr/~dray/


From jim at bitwrit.com.au  Fri Jul 28 00:56:40 2006
From: jim at bitwrit.com.au (Jim Lemon)
Date: Thu, 27 Jul 2006 18:56:40 -0400
Subject: [R] creating a color display matrix
In-Reply-To: <d42f810106b8d7db2eb0e75af8c4fa14@mednet.ucla.edu>
References: <d42f810106b8d7db2eb0e75af8c4fa14@mednet.ucla.edu>
Message-ID: <44C944A8.3030307@bitwrit.com.au>

Kartik Pappu wrote:
> Hello all,
> 
> I am trying to use R to create a colored data matrix.  I have my data 
> which (after certain steps of normalization and log transformation) 
> gives me a "x by y" matrix of numbers between 0 and -5.  I want to be 
> able to create from this matrix of numbers a "x by y" image (box) that 
> contains x*y squares and somehow uses the value in the original matrix 
> to come up with a color that corresponds to the number.  Hence each box 
> will be colored on a scale between 0 and 5.  For example my data could 
> look like this:
> 
>                  X1.FcH      X2.FcH      X3.FcH     X4.FcH      X5.FcH   
>   X6.FcH       X7.FcH
> 1-AP        0.09667593 -4.66298640 -1.28299697 -4.8739017 -4.95862831 
> -5.178603 -4.878524750
> 2-AP       -4.69186869 -0.08547776 -4.56495440 -4.8348255 -4.80256152 
> -5.121531 -4.894347108
> 3-AP       -1.71380667 -4.52626124 -0.06810053 -4.8703810 -4.65657593 
> -5.024595 -4.824712621
> 4-AP       -4.47968850 -4.48604718 -4.44314403 -0.1569536 -4.86436977 
> -4.988196 -4.550416356
> 5-AP       -4.64616469 -4.53888807 -4.78163386 -4.9162949 -0.01729274 
> -5.061663 -0.769960777
> 6-AP       -4.61047573 -4.60917414 -4.72514817 -5.0084772 -4.87797740 
> -0.284934 -1.782745357
> 7-AP       -4.48157167 -4.61850313 -4.72241281 -4.8694868 -1.66122821 
> -3.887898 -0.002522157
> 
> How do I make a 7 x 7 box that has 49 squares and each square has a 
> color in the RGB spectrum that corresponds to the value.  So for 
> example in the matrix above the biggest number which is 0.00966 (at 
> position 1 , 1) could be set to RED and the smallest number which is 
> 5.0084 (at position 6, 5) can be set to BLUE and all the other numbers 
> be shades on a scale of Red going to Blue.
> 
> I hope this problem makes sense.  I am rather new to R and was 
> wondering if there was a function or solution to this problem out 
> there.
> 
Hi Kartik,

Have a look at color2d.matplot in the plotrix package.

Jim


From puzi at pcb.chem.univ.gda.pl  Thu Jul 27 11:16:45 2006
From: puzi at pcb.chem.univ.gda.pl (Tomasz Puzyn)
Date: Thu, 27 Jul 2006 11:16:45 +0200
Subject: [R] Problems with RODBC
Message-ID: <44C8847D.6090108@pcb.chem.univ.gda.pl>

Dear Michael,
I have a problem with RODBC installation. When I try to install it, I 
get the output following output. Give me some ideas, why I can not to do 
it properly?
Regards,
Tomasz

****************************
* Installing *source* package 'RODBC' ...
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking for egrep... grep -E
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking sql.h usability... no
checking sql.h presence... no
checking for sql.h... no
checking sqlext.h usability... no
checking sqlext.h presence... no
checking for sqlext.h... no
configure: error: "ODBC headers sql.h and sqlext.h not found"
ERROR: configuration failed for package 'RODBC'
** Removing '/usr/local/lib64/R/library/RODBC'

The downloaded packages are in
        /tmp/RtmphYJYHP/downloaded_packages
Warning message:
installation of package 'RODBC' had non-zero exit status in: 
install.packages(c("RODBC"))
************************

-- 

=======================================================
TOMASZ PUZYN, Ph.D.
puzi at pcb.chem.univ.gda.pl

Department of Environmental Chemistry and Ecotoxicology
University of Gdansk, Faculty of Chemistry
ul. Sobieskiego 18 80-952 Gdansk, Poland
tel. +48 (58) 345 04 45
fax. +48 (58) 345 04 72
http://www.chem.univ.gda.pl/~zchsie


From wl at eimb.ru  Thu Jul 27 11:37:36 2006
From: wl at eimb.ru (Vladimir Eremeev)
Date: Thu, 27 Jul 2006 13:37:36 +0400
Subject: [R] how to resample (or resize) matrix?
Message-ID: <127678226.20060727133736@eimb.ru>

Dear r-help,

  I have a matrix, suppose, 10x10, and I need the matrix 5x5, having
  in each cell a mean value of the cells from the initial matrix.

  Please, point me to a function in R, which can help me doing that.

  Digging the documentation and mail archives didn't give me a result.
  
  Thank you.
  
---
Best regards,
Vladimir                mailto:wl at eimb.ru


From tuechler at gmx.at  Thu Jul 27 11:42:50 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Thu, 27 Jul 2006 10:42:50 +0100
Subject: [R] How to get the name of the first argument in an assignment
 function?
Message-ID: <3.0.6.32.20060727104250.00ad67c0@pop.gmx.net>

Dear All!

If I pass an object to an assignment function I cannot get it's name by 
deparse(substitute(argument)), but I get *tmp* and I found no way to get
the original name, in the example below it should be "va1".
Is there a way?

Thanks,

Heinz

## example
'fu1<-' <- function(var, value) {
print(c(name.of.var=deparse(substitute(var))))}
fu1(va1) <- 3

name.of.var 
    "*tmp*" 

## desired result:
## name.of.var 
##    "va1" 



version
               _                                        
platform       i386-pc-mingw32                          
arch           i386                                     
os             mingw32                                  
system         i386, mingw32                            
status         Patched                                  
major          2                                        
minor          3.1                                      
year           2006                                     
month          07                                       
day            23                                       
svn rev        38687                                    
language       R                                        
version.string Version 2.3.1 Patched (2006-07-23 r38687)


From quin.wills at googlemail.com  Thu Jul 27 11:48:22 2006
From: quin.wills at googlemail.com (Quin Wills)
Date: Thu, 27 Jul 2006 10:48:22 +0100
Subject: [R] PCA with not non-negative definite covariance
In-Reply-To: <44C87379.5060205@biomserv.univ-lyon1.fr>
Message-ID: <44c88be8.71617ddc.3296.fffff4c2@mx.gmail.com>

Thank you... I will definitely check that up.

Quin

-----Original Message-----
From: St?phane Dray [mailto:dray at biomserv.univ-lyon1.fr] 
Sent: 27 July 2006 09:04 AM
To: Quin Wills
Cc: 'Berton Gunter'; r-help at stat.math.ethz.ch
Subject: Re: [R] PCA with not non-negative definite covariance

As said by Pierre Bady,
an answer to your question is NIPALS analysis.
PCA is usually obtained by the diagonalization of a variance-covariance 
matrix. But it can also be obtained by an iterative proedure which 
consists in two regressions. NIPLAS is an implementation of this 
iterative procedure and is strictly equivalent to PCA when there is no 
missing values.
The adavantage of NIPALS is that it can be used with missing values. 
However, note that the convergence is not always obtained (it depends of 
the number and distribution of missing values).
You can find a description of the method and the algorithm here:

http://biomserv.univ-lyon1.fr/~dray/articles/SD165.html

Sincerely,


Quin Wills wrote:

>My apologies (in response to the last 2 replies). I should write sensibly -
>including subject titles that make grammatical sense.
>
>(1) By analogous, I mean that using classical MDS with Euclidian distance
is
>equivalent to plotting the first "k" principle components.
>(2) Agreed re. distribution assumptions.
>(3) Agreed re. the need to use some kind of imputation for calculating
>distances. I'm thinking pairwise exclusion for correlation.
>
>Re. why I want to do this is simply for graphically representing my data.
>
>Quin
>
>
>
>-----Original Message-----
>From: Berton Gunter [mailto:gunter.berton at gene.com] 
>Sent: 26 July 2006 05:10 PM
>To: 'Quin Wills'; bady at univ-lyon1.fr
>Cc: r-help at stat.math.ethz.ch
>Subject: RE: [R] PCA with not non-negative definite covariance
>
>Not sure what "completely analagous" means; mds is nonlinear, PCA is
linear.
>
>In any case, the bottom line is that if you have high dimensional data with
>"many" missing values, you cannot know what the multivariate distribution
>looks like -- and you need a **lot** of data with many variables to
usefully
>characterize it anyway. So you must either make some assumptions about what
>the distribution could be (including imputation methodology) or use any of
>the many exploratory techniques available to learn what you can.
>Thermodynamics holds -- you can't get something for nothing (you can't fool
>Mother Nature).
>
>-- Bert Gunter
>Genentech Non-Clinical Statistics
>South San Francisco, CA
> 
>"The business of the statistician is to catalyze the scientific learning
>process."  - George E. P. Box
> 
> 
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Quin Wills
>>Sent: Wednesday, July 26, 2006 8:44 AM
>>To: bady at univ-lyon1.fr
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] PCA with not non-negative definite covariance
>>
>>Thanks.
>>
>>I suppose that another option could be just to use classical
>>multi-dimensional scaling. By my understanding this is (if based on
>>Euclidian measure) completely analogous to PCA, and because it's based
>>explicitly on distances, I could easily exclude the variables 
>>with NA's on a
>>pairwise basis when calculating the distances.
>>
>>Quin
>>
>>-----Original Message-----
>>From: bady at univ-lyon1.fr [mailto:bady at univ-lyon1.fr] 
>>Sent: 25 July 2006 09:24 AM
>>To: Quin Wills
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] PCA with not non-negative definite covariance
>>
>>Hi , hi all,
>>
>>    
>>
>>>Am I correct to understand from the previous discussions on 
>>>      
>>>
>>this topic (a
>>    
>>
>>>few years back) that if I have a matrix with missing values 
>>>      
>>>
>>my PCA options
>>    
>>
>>>seem dismal if:
>>>(1)     I don?t want to impute the missing values.
>>>(2)     I don?t want to completely remove cases with missing values.
>>>(3)     I do cov() with use=?pairwise.complete.obs?, as 
>>>      
>>>
>>this produces
>>    
>>
>>>negative eigenvalues (which it has in my case!).
>>>      
>>>
>>(4) Maybe you can use the Non-linear Iterative Partial Least Squares
>>(NIPALS)
>>algorithm (intensively used in chemometry). S. Dray proposes 
>>a version of
>>this
>>procedure at http://pbil.univ-lyon1.fr/R/additifs.html.
>>
>>
>>Hope this help :)
>>
>>
>>Pierre
>>
>>
>>
>>--------------------------------------------------------------
>>------------
>>Ce message a ?t? envoy? depuis le webmail IMP (Internet 
>>Messaging Program)
>>
>>-- 
>>No virus found in this incoming message.
>>
>>
>> 
>>
>>--
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
>>    
>>
>
>  
>


-- 
St?phane DRAY (dray at biomserv.univ-lyon1.fr )
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - Lyon I
43, Bd du 11 Novembre 1918, 69622 Villeurbanne Cedex, France
Tel: 33 4 72 43 27 57       Fax: 33 4 72 43 13 88
http://biomserv.univ-lyon1.fr/~dray/

-- 
No virus found in this incoming message.


 

--


From ggrothendieck at gmail.com  Thu Jul 27 12:10:28 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 27 Jul 2006 06:10:28 -0400
Subject: [R] How to get the name of the first argument in an assignment
	function?
In-Reply-To: <3.0.6.32.20060727104250.00ad67c0@pop.gmx.net>
References: <3.0.6.32.20060727104250.00ad67c0@pop.gmx.net>
Message-ID: <971536df0607270310l1af4af09ibce7033623e12281@mail.gmail.com>

If you are willing to write fu2[Var] <- 3 instead of fu2(Var) <- 3
then this workaround may suffice:

fu2 <- structure(NA, class = "fu2")
"[<-.fu2" <- function(x, ..., value) { print(match.call()[[3]]); fu2 }

# test
fu2[Var] <- 3  # prints "Var"


On 7/27/06, Heinz Tuechler <tuechler at gmx.at> wrote:
> Dear All!
>
> If I pass an object to an assignment function I cannot get it's name by
> deparse(substitute(argument)), but I get *tmp* and I found no way to get
> the original name, in the example below it should be "va1".
> Is there a way?
>
> Thanks,
>
> Heinz
>
> ## example
> 'fu1<-' <- function(var, value) {
> print(c(name.of.var=deparse(substitute(var))))}
> fu1(va1) <- 3
>
> name.of.var
>    "*tmp*"
>
> ## desired result:
> ## name.of.var
> ##    "va1"
>
>
>
> version
>               _
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status         Patched
> major          2
> minor          3.1
> year           2006
> month          07
> day            23
> svn rev        38687
> language       R
> version.string Version 2.3.1 Patched (2006-07-23 r38687)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Thu Jul 27 12:20:13 2006
From: jholtman at gmail.com (jim holtman)
Date: Thu, 27 Jul 2006 06:20:13 -0400
Subject: [R] how to resample (or resize) matrix?
In-Reply-To: <127678226.20060727133736@eimb.ru>
References: <127678226.20060727133736@eimb.ru>
Message-ID: <644e1f320607270320p3afdb40by3b7d4f34d6437a96@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060727/aed8d652/attachment.pl 

From r.hankin at noc.soton.ac.uk  Thu Jul 27 12:42:51 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 27 Jul 2006 11:42:51 +0100
Subject: [R] how to resample (or resize) matrix?
In-Reply-To: <644e1f320607270320p3afdb40by3b7d4f34d6437a96@mail.gmail.com>
References: <127678226.20060727133736@eimb.ru>
	<644e1f320607270320p3afdb40by3b7d4f34d6437a96@mail.gmail.com>
Message-ID: <FEF46C3C-DCCF-45C8-8905-D56AFD228DF6@soc.soton.ac.uk>

Right, I think I understand the question.



library(magic)
?subsums

If you want a windowed moving average, do:

x <- matrix(1:100,10,10)
subsums(x,6,FUN="mean",pad=NA,wrap=F)[6:10,6:10]



If you want block average, do:

subsums(x,2,FUN="mean",pad=NA,wrap=F)[seq(2,10,2),seq(2,10,2)]

which agrees with Jim's solution below.


HTH

rksh




On 27 Jul 2006, at 11:20, jim holtman wrote:

> Is this what you want: the mean of the surrounding 4 cells?
>
>> x <- matrix(1:100, 10)  # create data
>> rmean <- matrix(0,5,5)  # result matrix
>> for (i in 1:5){
> +     for (j in 1:5){
> +         rmean[i, j] <- mean(x[c(-1,0) + 2 * i, c(-1,0) + 2 * j])
> +     }
> + }
>> x
>       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
>  [1,]    1   11   21   31   41   51   61   71   81    91
>  [2,]    2   12   22   32   42   52   62   72   82    92
>  [3,]    3   13   23   33   43   53   63   73   83    93
>  [4,]    4   14   24   34   44   54   64   74   84    94
>  [5,]    5   15   25   35   45   55   65   75   85    95
>  [6,]    6   16   26   36   46   56   66   76   86    96
>  [7,]    7   17   27   37   47   57   67   77   87    97
>  [8,]    8   18   28   38   48   58   68   78   88    98
>  [9,]    9   19   29   39   49   59   69   79   89    99
> [10,]   10   20   30   40   50   60   70   80   90   100
>> rmean
>      [,1] [,2] [,3] [,4] [,5]
> [1,]  6.5 26.5 46.5 66.5 86.5
> [2,]  8.5 28.5 48.5 68.5 88.5
> [3,] 10.5 30.5 50.5 70.5 90.5
> [4,] 12.5 32.5 52.5 72.5 92.5
> [5,] 14.5 34.5 54.5 74.5 94.5
>>
>
>
>
> On 7/27/06, Vladimir Eremeev <wl at eimb.ru> wrote:
>>
>> Dear r-help,
>>
>> I have a matrix, suppose, 10x10, and I need the matrix 5x5, having
>> in each cell a mean value of the cells from the initial matrix.
>>
>> Please, point me to a function in R, which can help me doing that.
>>
>> Digging the documentation and mail archives didn't give me a result.
>>
>> Thank you.
>>
>> ---
>> Best regards,
>> Vladimir                mailto:wl at eimb.ru
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
>
> --  
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From bren at juanantonio.info  Thu Jul 27 12:49:33 2006
From: bren at juanantonio.info (=?UTF-8?Q?Juan_Antonio_Bre=C3=B1a_Moral?=)
Date: Thu, 27 Jul 2006 03:49:33 -0700 (PDT)
Subject: [R] Moving Average
In-Reply-To: <OFA70B4F31.F087AE0C-ON032571B7.004F0536-032571B7.004F9A2D@serasa.com.br>
References: <OFA70B4F31.F087AE0C-ON032571B7.004F0536-032571B7.004F9A2D@serasa.com.br>
Message-ID: <5518837.post@talk.nabble.com>


Hi,

To make moving average, I advise you, to read a document in my web to make
forecasting,
http://www.juanantonio.info/p_research/statistics/r/forecasting.htm
http://www.juanantonio.info/p_research/statistics/r/forecasting.htm 

This scripts use the library forecast and the function pegels.
http://www-personal.buseco.monash.edu.au/~hyndman/Rlibrary/forecast/
http://www-personal.buseco.monash.edu.au/~hyndman/Rlibrary/forecast/ 

If you see the web, the author of the library has updated it, then the
function to make the prediction is ets.

http://www-personal.buseco.monash.edu.au/~hyndman/Rlibrary/forecast/forecast/ets.html 
http://www-personal.buseco.monash.edu.au/~hyndman/Rlibrary/forecast/forecast/ets.html 

Best Regards.

Juan Antonio Bre?a Moral
http://www.juanantonio.info http://www.juanantonio.info 
-- 
View this message in context: http://www.nabble.com/Moving-Average-tf2004135.html#a5518837
Sent from the R help forum at Nabble.com.


From phhs80 at gmail.com  Thu Jul 27 12:52:05 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Thu, 27 Jul 2006 11:52:05 +0100
Subject: [R] Non-parametric four-way interactions?
In-Reply-To: <44C83642.2090806@vanderbilt.edu>
References: <6ade6f6c0607261849p576cc8c6of0e3247ef1c2134b@mail.gmail.com>
	<44C83642.2090806@vanderbilt.edu>
Message-ID: <6ade6f6c0607270352q3d4e6b3anb48d76aeb1d00df8@mail.gmail.com>

On 7/27/06, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
> > I am trying to study four-way interactions in an ANOVA problem.
> > However, qqnorm+qqline result
> >
> > (at http://phhs80.googlepages.com/qqnorm.png)
> >
> > is not promising regarding the normality of data (960 observations).
> > The result of Shapiro-Wilk test is also not encouraging:
> >
> > W = 0.9174, p-value < 2.2e-16
> >
> > (I am aware of the fact that normality tests tend to reject normality
> > for large samples.)
> >
> > By the way, the histogram is at:
> >
> > http://phhs80.googlepages.com/hist.png
> >
> > To circumvent the problem, I looked for non-parametric tests, but I
> > found nothing, but the article:
> >
> > http://www.pgia.ac.lk/socs/asasl/journal_papers/PDFformat/g.bakeerathanpaper-2.pdf
> >
> > Finally, my question is: has R got implemented functions to use
> > non-parametric tests to avoid the fulfillment of the normality
> > assumption required to study four-way interactions?
>
> Yes, although I seldom want to look at 4th order interactions.  You can
> fit a proportional odds model for an ordinal response which is a
> generalization of the Wilcoxon/Kruskal-Wallis approach, and allows one
> to have N-1 intercepts in the model when there are N data points (i.e.,
> it works even with no ties in the data).  However if N is large the
> matrix operations will be prohibitive and you might reduce Y to 100-tile
> groups.  The PO model uses only the ranks of Y so is monotonic
> transformation invariant.
>
> library(Design)  # also requires library(Hmisc)
> f <- lrm(y ~ a*b*c*d)
> f
> anova(f)
>
> Also see the polr function in VR

Thanks, Frank. It is very encouraging to learn that, even without
normality, I can still study my four-way interactions. I am also aware
of transformations that may work in some non-normal cases, and I have
tried some of them, but with no success.

I am not familiar with the solutions that you suggest, and I would
like to learn how they work theoretically, in some book or on the
Internet. In particular, I would like to see, regarding power, how the
non-parametric suggested approach compares with the classical ANOVA
approach. Could you please indicate some references to help me with
that?

Again, thanks in advance.

Paul


From ggrothendieck at gmail.com  Thu Jul 27 12:54:44 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 27 Jul 2006 06:54:44 -0400
Subject: [R] how to resample (or resize) matrix?
In-Reply-To: <127678226.20060727133736@eimb.ru>
References: <127678226.20060727133736@eimb.ru>
Message-ID: <971536df0607270354k3b8a172dgd6f11d2073dc85c3@mail.gmail.com>

Assuming the problem is to partition the 10x10 matrix x into 25 two by two
squares and then average each of those squares, try this:

   apply(array(x, c(2,5,2,5)), c(2,4), mean)

On 7/27/06, Vladimir Eremeev <wl at eimb.ru> wrote:
> Dear r-help,
>
>  I have a matrix, suppose, 10x10, and I need the matrix 5x5, having
>  in each cell a mean value of the cells from the initial matrix.
>
>  Please, point me to a function in R, which can help me doing that.
>
>  Digging the documentation and mail archives didn't give me a result.
>
>  Thank you.
>
> ---
> Best regards,
> Vladimir                mailto:wl at eimb.ru
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wl at eimb.ru  Thu Jul 27 13:15:25 2006
From: wl at eimb.ru (Vladimir Eremeev)
Date: Thu, 27 Jul 2006 15:15:25 +0400
Subject: [R] how to resample (or resize) matrix?
In-Reply-To: <644e1f320607270320p3afdb40by3b7d4f34d6437a96@mail.gmail.com>
References: <127678226.20060727133736@eimb.ru>
	<644e1f320607270320p3afdb40by3b7d4f34d6437a96@mail.gmail.com>
Message-ID: <843343187.20060727151525@eimb.ru>

Dear jim,

Yes.
But, unfortunately, two nested for loops will execute very slow.

It is not very serious problem to do my task with an image
processing package, I am wondering if it is efficiently possible with
R.

Thursday, July 27, 2006, 2:20:13 PM, you wrote:

jh> Is this what you want: the mean of the surrounding 4 cells?

jh> ?

>> x <- matrix(1:100, 10)? # create data
>> rmean <- matrix(0,5,5)? # result matrix
>> for (i in 1:5){
jh> +???? for (j in 1:5){
jh> +???????? rmean[i, j] <- mean(x[c(-1,0) + 2 * i, c(-1,0) + 2 * j])
jh> +???? }
jh> + }
>> x
jh> ????? [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
jh> ?[1,]??? 1?? 11?? 21?? 31?? 41?? 51?? 61?? 71?? 81??? 91
jh> ?[2,]??? 2?? 12?? 22?? 32?? 42?? 52?? 62?? 72?? 82??? 92
jh> ?[3,]??? 3?? 13?? 23?? 33?? 43?? 53?? 63?? 73?? 83??? 93
jh> ?[4,]??? 4?? 14?? 24?? 34?? 44?? 54?? 64?? 74?? 84??? 94
jh> ?[5,]??? 5?? 15?? 25?? 35?? 45?? 55?? 65?? 75?? 85??? 95
jh> ?[6,]??? 6?? 16?? 26?? 36?? 46?? 56?? 66?? 76?? 86??? 96
jh> ?[7,]??? 7?? 17?? 27?? 37?? 47?? 57?? 67?? 77?? 87??? 97
jh> ?[8,]??? 8?? 18?? 28?? 38?? 48?? 58?? 68?? 78?? 88??? 98
jh> ?[9,]??? 9?? 19?? 29?? 39?? 49?? 59?? 69?? 79?? 89??? 99
jh> [10,]?? 10?? 20?? 30?? 40?? 50?? 60?? 70?? 80?? 90?? 100
>> rmean
jh> ???? [,1] [,2] [,3] [,4] [,5]
jh> [1,]? 6.5 26.5 46.5 66.5 86.5
jh> [2,]? 8.5 28.5 48.5 68.5 88.5
jh> [3,] 10.5 30.5 50.5 70.5 90.5
jh> [4,] 12.5 32.5 52.5 72.5 92.5
jh> [5,] 14.5 34.5 54.5 74.5 94.5
>> 


jh> ?

jh> On 7/27/06, Vladimir Eremeev <wl at eimb.ru> wrote:Dear r-help,

jh> I have a matrix, suppose, 10x10, and I need the matrix 5x5, having
jh> in each cell a mean value of the cells from the initial matrix.

jh> Please, point me to a function in R, which can help me doing that.

jh> Digging the documentation and mail archives didn't give me a result.

jh> Thank you.

jh> ---
jh> Best regards,
jh> Vladimir????????????????mailto:wl at eimb.ru

jh> ______________________________________________
jh> R-help at stat.math.ethz.ch mailing list
jh> https://stat.ethz.ch/mailman/listinfo/r-help
jh> PLEASE do read the posting guide
jh> http://www.R-project.org/posting-guide.html
jh> and provide commented, minimal, self-contained, reproducible code.










---
Best regards,
Vladimir                mailto:wl at eimb.ru
==========================================================================
Research Scientist, PhD                           Leninsky Prospect 33,
Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
Institute of Ecology,                             Phone: (095) 135-9972;
Russian Academy of Sciences                       Fax: (095) 135-9972


From wl at eimb.ru  Thu Jul 27 13:26:26 2006
From: wl at eimb.ru (Vladimir Eremeev)
Date: Thu, 27 Jul 2006 15:26:26 +0400
Subject: [R] how to resample (or resize) matrix?
In-Reply-To: <FEF46C3C-DCCF-45C8-8905-D56AFD228DF6@soc.soton.ac.uk>
References: <127678226.20060727133736@eimb.ru>
	<644e1f320607270320p3afdb40by3b7d4f34d6437a96@mail.gmail.com>
	<FEF46C3C-DCCF-45C8-8905-D56AFD228DF6@soc.soton.ac.uk>
Message-ID: <149226163.20060727152626@eimb.ru>

Dear Robin,

Thank you, seems it is what I need.

---
Best regards,
Vladimir                mailto:wl at eimb.ru


From whit.armstrong at hcmny.com  Thu Jul 27 14:06:10 2006
From: whit.armstrong at hcmny.com (Armstrong, Whit)
Date: Thu, 27 Jul 2006 08:06:10 -0400
Subject: [R] RODBC on linux
Message-ID: <E58BE6136618CF4C964F6EC7773AE5699FA5C0@ex4.nyc.hcmny.com>

Thanks, everyone.

I'll give freeTDS a try.

Cheers,
Whit


-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Thursday, July 27, 2006 1:17 AM
To: Marc Schwartz
Cc: Armstrong, Whit; r-help at stat.math.ethz.ch
Subject: Re: [R] RODBC on linux

On Wed, 26 Jul 2006, Marc Schwartz wrote:

> On Wed, 2006-07-26 at 17:52 -0400, Armstrong, Whit wrote:
> > Anyone out there using Linux RODBC and unixODBC to connect to a 
> > Microsoft SQL server?
> > 
> > If possible can someone post a sample .odbc.ini file?
> > 
> > I saw a few discussions on the archives a few years ago, but no 
> > config file details were available.
> > 
> > Thanks,
> > Whit
> 
> Whit,
> 
> Do you have a Linux ODBC driver for SQL Server?  unixODBC is simply 
> the driver manager, not the driver itself.
> 
> MS does not offer (not surprisingly) an ODBC driver for Unix/Linux.
> There are resources available however and these might be helpful:
> 
> http://www.sommarskog.se/mssql/unix.html
> 
> Note that Easysoft provides (at a cost) an ODBC-ODBC bridge for 
> Unix/Linux platforms which supports ODBC connections to SQL Server:
> 
> http://www.easysoft.com/products/data_access/odbc_odbc_bridge/index.ht
> ml

Several people have successfully used that, from the earliest days of
RODBC: I believe it was part of Michael Lapsley's motivation to write
RODBC.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595




This e-mail message is intended only for the named recipient(s) above. It may contain confidential information. If you are not the intended recipient you are hereby notified that any dissemination, distribution or copying of this e-mail and any attachment(s) is strictly prohibited. If you have received this e-mail in error, please immediately notify the sender by replying to this e-mail and delete the message and any attachment(s) from your system. Thank you.


From f.harrell at vanderbilt.edu  Thu Jul 27 14:29:26 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 27 Jul 2006 07:29:26 -0500
Subject: [R] Non-parametric four-way interactions?
In-Reply-To: <6ade6f6c0607270352q3d4e6b3anb48d76aeb1d00df8@mail.gmail.com>
References: <6ade6f6c0607261849p576cc8c6of0e3247ef1c2134b@mail.gmail.com>	<44C83642.2090806@vanderbilt.edu>
	<6ade6f6c0607270352q3d4e6b3anb48d76aeb1d00df8@mail.gmail.com>
Message-ID: <44C8B1A6.7000407@vanderbilt.edu>

Paul Smith wrote:
> On 7/27/06, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
>>> I am trying to study four-way interactions in an ANOVA problem.
>>> However, qqnorm+qqline result
>>>
>>> (at http://phhs80.googlepages.com/qqnorm.png)
>>>
>>> is not promising regarding the normality of data (960 observations).
>>> The result of Shapiro-Wilk test is also not encouraging:
>>>
>>> W = 0.9174, p-value < 2.2e-16
>>>
>>> (I am aware of the fact that normality tests tend to reject normality
>>> for large samples.)
>>>
>>> By the way, the histogram is at:
>>>
>>> http://phhs80.googlepages.com/hist.png
>>>
>>> To circumvent the problem, I looked for non-parametric tests, but I
>>> found nothing, but the article:
>>>
>>> http://www.pgia.ac.lk/socs/asasl/journal_papers/PDFformat/g.bakeerathanpaper-2.pdf
>>>
>>> Finally, my question is: has R got implemented functions to use
>>> non-parametric tests to avoid the fulfillment of the normality
>>> assumption required to study four-way interactions?
>> Yes, although I seldom want to look at 4th order interactions.  You can
>> fit a proportional odds model for an ordinal response which is a
>> generalization of the Wilcoxon/Kruskal-Wallis approach, and allows one
>> to have N-1 intercepts in the model when there are N data points (i.e.,
>> it works even with no ties in the data).  However if N is large the
>> matrix operations will be prohibitive and you might reduce Y to 100-tile
>> groups.  The PO model uses only the ranks of Y so is monotonic
>> transformation invariant.
>>
>> library(Design)  # also requires library(Hmisc)
>> f <- lrm(y ~ a*b*c*d)
>> f
>> anova(f)
>>
>> Also see the polr function in VR
> 
> Thanks, Frank. It is very encouraging to learn that, even without
> normality, I can still study my four-way interactions. I am also aware
> of transformations that may work in some non-normal cases, and I have
> tried some of them, but with no success.
> 
> I am not familiar with the solutions that you suggest, and I would
> like to learn how they work theoretically, in some book or on the
> Internet. In particular, I would like to see, regarding power, how the
> non-parametric suggested approach compares with the classical ANOVA
> approach. Could you please indicate some references to help me with
> that?
> 
> Again, thanks in advance.
> 
> Paul

Tony Lachenbruch has written some about this, also see my book (it's web 
page is biostat.mc.vanderbilt.edu/rms).  I don't know about the power of 
interaction tests, but for main effect tests in the absence of 
interaction, the Wilcoxon test (a special case of PO model) has 
efficiency of 3/pi compared to the t-test if normality holds.

Other approaches: Cox PH model, avas, ace.  My book covers these too.

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From whit.armstrong at hcmny.com  Thu Jul 27 14:18:42 2006
From: whit.armstrong at hcmny.com (Armstrong, Whit)
Date: Thu, 27 Jul 2006 08:18:42 -0400
Subject: [R] deparse(substitute(foo))
Message-ID: <E58BE6136618CF4C964F6EC7773AE5699FA5C1@ex4.nyc.hcmny.com>

I see that plot.default uses deparse(substitute(x)) to extract the
character name of an argument and put it on the vertical axis.

Hence:
foo <- 1:10
plot( foo )

will put the label "foo" on the vertical axis.

However, for a function that takes a "..." list as an input, I can only
extract the first argument name:

x <- 1:10
y <- 10:20

foo <- function(...) {
  print(deparse(substitute(...)))
}

foo(x,y)

returns: 

> foo(x,y) 
[1] "x"
> 

and when I try to convert the list to a local variable and then extract
names, that doesn't work either:

x <- 1:10 
y <- 10:20 
 
foo <- function(...) { 
 
    x <- list(...) 
    print(deparse(substitute(names(x)))) 
} 
 
foo(x,y) 

returns:

> foo(x,y)
[1] "names(list(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), c(10, 11, 12, 13, "
[2] "14, 15, 16, 17, 18, 19, 20)))"                                  
> 


Can someone suggest a way to extract the variable names when they are
passed as a list via "..." ?

Thanks,
Whit




This e-mail message is intended only for the named recipient(s) above. It may contain confidential information. If you are not the intended recipient you are hereby notified that any dissemination, distribution or copying of this e-mail and any attachment(s) is strictly prohibited. If you have received this e-mail in error, please immediately notify the sender by replying to this e-mail and delete the message and any attachment(s) from your system. Thank you.


From MSchwartz at mn.rr.com  Thu Jul 27 14:22:50 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 27 Jul 2006 07:22:50 -0500
Subject: [R] Problems with RODBC
In-Reply-To: <44C8847D.6090108@pcb.chem.univ.gda.pl>
References: <44C8847D.6090108@pcb.chem.univ.gda.pl>
Message-ID: <1154002970.5036.62.camel@localhost.localdomain>

On Thu, 2006-07-27 at 11:16 +0200, Tomasz Puzyn wrote:
> Dear Michael,
> I have a problem with RODBC installation. When I try to install it, I 
> get the output following output. Give me some ideas, why I can not to do 
> it properly?
> Regards,
> Tomasz
> 
> ****************************
> * Installing *source* package 'RODBC' ...
> checking for gcc... gcc
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -E
> checking for egrep... grep -E
> checking for ANSI C header files... yes
> checking for sys/types.h... yes
> checking for sys/stat.h... yes
> checking for stdlib.h... yes
> checking for string.h... yes
> checking for memory.h... yes
> checking for strings.h... yes
> checking for inttypes.h... yes
> checking for stdint.h... yes
> checking for unistd.h... yes
> checking sql.h usability... no
> checking sql.h presence... no
> checking for sql.h... no
> checking sqlext.h usability... no
> checking sqlext.h presence... no
> checking for sqlext.h... no
> configure: error: "ODBC headers sql.h and sqlext.h not found"
> ERROR: configuration failed for package 'RODBC'
> ** Removing '/usr/local/lib64/R/library/RODBC'
> 
> The downloaded packages are in
>         /tmp/RtmphYJYHP/downloaded_packages
> Warning message:
> installation of package 'RODBC' had non-zero exit status in: 
> install.packages(c("RODBC"))
> ************************


Tomasz,

You are missing the unixODBC header files, which are required to compile
the package from source.

You did not indicate which Linux distribution you are using, but if you
are on an RPM based distribution, you need to install 'unixODBC-devel'.
It looks like Debian based distributions use 'unixodbc-devel' (Note lack
of capitalization).

HTH,

Marc Schwartz


From ggrothendieck at gmail.com  Thu Jul 27 14:33:01 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 27 Jul 2006 08:33:01 -0400
Subject: [R] deparse(substitute(foo))
In-Reply-To: <E58BE6136618CF4C964F6EC7773AE5699FA5C1@ex4.nyc.hcmny.com>
References: <E58BE6136618CF4C964F6EC7773AE5699FA5C1@ex4.nyc.hcmny.com>
Message-ID: <971536df0607270533p7aeeec58mabef80a3959e1a4c@mail.gmail.com>

See ?match.call

On 7/27/06, Armstrong, Whit <whit.armstrong at hcmny.com> wrote:
> I see that plot.default uses deparse(substitute(x)) to extract the
> character name of an argument and put it on the vertical axis.
>
> Hence:
> foo <- 1:10
> plot( foo )
>
> will put the label "foo" on the vertical axis.
>
> However, for a function that takes a "..." list as an input, I can only
> extract the first argument name:
>
> x <- 1:10
> y <- 10:20
>
> foo <- function(...) {
>  print(deparse(substitute(...)))
> }
>
> foo(x,y)
>
> returns:
>
> > foo(x,y)
> [1] "x"
> >
>
> and when I try to convert the list to a local variable and then extract
> names, that doesn't work either:
>
> x <- 1:10
> y <- 10:20
>
> foo <- function(...) {
>
>    x <- list(...)
>    print(deparse(substitute(names(x))))
> }
>
> foo(x,y)
>
> returns:
>
> > foo(x,y)
> [1] "names(list(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), c(10, 11, 12, 13, "
> [2] "14, 15, 16, 17, 18, 19, 20)))"
> >
>
>
> Can someone suggest a way to extract the variable names when they are
> passed as a list via "..." ?
>
> Thanks,
> Whit
>
>
>
>
> This e-mail message is intended only for the named recipient(s) above. It may contain confidential information. If you are not the intended recipient you are hereby notified that any dissemination, distribution or copying of this e-mail and any attachment(s) is strictly prohibited. If you have received this e-mail in error, please immediately notify the sender by replying to this e-mail and delete the message and any attachment(s) from your system. Thank you.
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From wl at eimb.ru  Thu Jul 27 14:30:44 2006
From: wl at eimb.ru (Vladimir Eremeev)
Date: Thu, 27 Jul 2006 16:30:44 +0400
Subject: [R] how to resample (or resize) matrix?
In-Reply-To: <8ed68eed0607270431q36a0d4f0k93ab9a212374911b@mail.gmail.com>
References: <127678226.20060727133736@eimb.ru>
	<644e1f320607270320p3afdb40by3b7d4f34d6437a96@mail.gmail.com>
	<843343187.20060727151525@eimb.ru>
	<8ed68eed0607270431q36a0d4f0k93ab9a212374911b@mail.gmail.com>
Message-ID: <1909794646.20060727163044@eimb.ru>

Dear Sean,

Thursday, July 27, 2006, 3:31:31 PM, you wrote:

SOR> Hi Vladimir,
SOR> I was wondering whether this was image related :-)

Yes, that's right, I am doing image processing with R.

SOR> would one of the image related libraries do it for you?
SOR> looking at
SOR> http://cran.r-project.org/doc/FAQ/R-FAQ.html#Add_002don-packages-from-CRAN
SOR> and searching down for "image"

The EBImage seems to be able.
But I need windows binaries, which are unavailable.

SOR> cheers,
SOR> Sean


From MSchwartz at mn.rr.com  Thu Jul 27 14:41:12 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 27 Jul 2006 07:41:12 -0500
Subject: [R] deparse(substitute(foo))
In-Reply-To: <E58BE6136618CF4C964F6EC7773AE5699FA5C1@ex4.nyc.hcmny.com>
References: <E58BE6136618CF4C964F6EC7773AE5699FA5C1@ex4.nyc.hcmny.com>
Message-ID: <1154004073.5036.66.camel@localhost.localdomain>

On Thu, 2006-07-27 at 08:18 -0400, Armstrong, Whit wrote:
> I see that plot.default uses deparse(substitute(x)) to extract the
> character name of an argument and put it on the vertical axis.
> 
> Hence:
> foo <- 1:10
> plot( foo )
> 
> will put the label "foo" on the vertical axis.
> 
> However, for a function that takes a "..." list as an input, I can only
> extract the first argument name:
> 
> x <- 1:10
> y <- 10:20
> 
> foo <- function(...) {
>   print(deparse(substitute(...)))
> }
> 
> foo(x,y)
> 
> returns: 
> 
> > foo(x,y) 
> [1] "x"
> > 
> 
> and when I try to convert the list to a local variable and then extract
> names, that doesn't work either:
> 
> x <- 1:10 
> y <- 10:20 
>  
> foo <- function(...) { 
>  
>     x <- list(...) 
>     print(deparse(substitute(names(x)))) 
> } 
>  
> foo(x,y) 
> 
> returns:
> 
> > foo(x,y)
> [1] "names(list(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), c(10, 11, 12, 13, "
> [2] "14, 15, 16, 17, 18, 19, 20)))"                                  
> > 
> 
> 
> Can someone suggest a way to extract the variable names when they are
> passed as a list via "..." ?
> 
> Thanks,
> Whit

Try this:

x <- 1:10 
y <- 10:20 
 
foo <- function(...) { 
   sapply(match.call()[-1], deparse)
} 


> foo(x, y)
[1] "x" "y"


HTH,

Marc Schwartz


From whit.armstrong at hcmny.com  Thu Jul 27 15:01:26 2006
From: whit.armstrong at hcmny.com (Armstrong, Whit)
Date: Thu, 27 Jul 2006 09:01:26 -0400
Subject: [R] deparse(substitute(foo))
Message-ID: <E58BE6136618CF4C964F6EC7773AE5699FA5C6@ex4.nyc.hcmny.com>

That works perfectly.

Thanks,
Whit
 

-----Original Message-----
From: Marc Schwartz [mailto:MSchwartz at mn.rr.com] 
Sent: Thursday, July 27, 2006 8:41 AM
To: Armstrong, Whit
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] deparse(substitute(foo))

On Thu, 2006-07-27 at 08:18 -0400, Armstrong, Whit wrote:
> I see that plot.default uses deparse(substitute(x)) to extract the 
> character name of an argument and put it on the vertical axis.
> 
> Hence:
> foo <- 1:10
> plot( foo )
> 
> will put the label "foo" on the vertical axis.
> 
> However, for a function that takes a "..." list as an input, I can 
> only extract the first argument name:
> 
> x <- 1:10
> y <- 10:20
> 
> foo <- function(...) {
>   print(deparse(substitute(...)))
> }
> 
> foo(x,y)
> 
> returns: 
> 
> > foo(x,y)
> [1] "x"
> > 
> 
> and when I try to convert the list to a local variable and then 
> extract names, that doesn't work either:
> 
> x <- 1:10
> y <- 10:20
>  
> foo <- function(...) {
>  
>     x <- list(...) 
>     print(deparse(substitute(names(x))))
> }
>  
> foo(x,y)
> 
> returns:
> 
> > foo(x,y)
> [1] "names(list(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), c(10, 11, 12, 13, "
> [2] "14, 15, 16, 17, 18, 19, 20)))"                                  
> > 
> 
> 
> Can someone suggest a way to extract the variable names when they are 
> passed as a list via "..." ?
> 
> Thanks,
> Whit

Try this:

x <- 1:10
y <- 10:20 
 
foo <- function(...) { 
   sapply(match.call()[-1], deparse)
} 


> foo(x, y)
[1] "x" "y"


HTH,

Marc Schwartz





This e-mail message is intended only for the named recipient(s) above. It may contain confidential information. If you are not the intended recipient you are hereby notified that any dissemination, distribution or copying of this e-mail and any attachment(s) is strictly prohibited. If you have received this e-mail in error, please immediately notify the sender by replying to this e-mail and delete the message and any attachment(s) from your system. Thank you.


From neuro3000 at hotmail.com  Thu Jul 27 15:23:21 2006
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Thu, 27 Jul 2006 09:23:21 -0400
Subject: [R] Vector extracted from a matrix. How can I specify dimensions in
	as.matrix?
Message-ID: <BAY112-F36BE8D4802E9C5BF66511BAF580@phx.gbl>

Transpose vector extracted from a matrix

Hello,

I am doing a recursive analysis that uses every line (vector) of a matrix in 
a loop. In the model, I need to transpose those vectors that are extracted 
from a matrix.

Using simple vectors (no matrix involved) the transpose function works fine:

simplevector <-matrix(1:3,3,1)
tsimplevector <-t(simplevector) #transposed
dim(simplevector) #3x1 matrix (vector)
dim(tsimplevector)#1x3

PROBLEM: However, when the vector is extracted from a matrix, its dimension 
is NULL.  In this case the transposed dimension is correct, but you'll see 
the next example, that if a row was extracted, the dimension would be wrong:

initialmatrix <- matrix(1:9,3,3)
extractedvector <-initialmatrix[,1] # extract first columm as vector
textractedvector <-t(extractedvector)#transposed
dim(extractedvector) #NULL!!!! <- not working
dim(textractedvector)#1x3 as expected

I have tried to transform the extracted vector as.vector and as.matrix. 
as.vector does not give the what I want (still NULL) and as.matrix can't 
specify the number of row and columns so both vectors are vertical.

In this example, I extract a column and a row. Notice that both 
as.matrix(vector) have the same dimension. Notice that both transposed 
vectors have 1x3 dimensions.

initialmatrix <- matrix(1:9,3,3)
extractedvector3x1 <-initialmatrix[,1]  # extract first columm as vector
extractedvector1x3 <-initialmatrix[1,]  # extract first row as vector

#Both are NULL
dim(extractedvector3x1) #NULL!!!!
dim(extractedvector1x3) #NULL!!!!

#transposed dimensions both 1x3
dim(t(extractedvector3x1)) #1x3
dim(t(extractedvector1x3)) #1x3 <- not 3x1

#as.vector: still NULL
dim(as.vector(extractedvector3x1)) #NULL!!!!
dim(as.vector(extractedvector1x3)) #NULL!!!!

#as.matrix: Both dim at 3x1
dim(as.matrix(extractedvector3x1)) #3x1
dim(as.matrix(extractedvector1x3)) #3x1 <- Problem: not 1x3

#as.matrix transposed: Both dim at 1x3
dim(as.matrix(t(extractedvector3x1))) #1x3
dim(as.matrix(t(extractedvector1x3))) #1x3 <- Problem: not 3x1

How can I get correct dimensions? Is there a way to specify dimensions in 
as.matrix?

Neuro


From tuechler at gmx.at  Thu Jul 27 15:29:58 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Thu, 27 Jul 2006 14:29:58 +0100
Subject: [R] How to get the name of the first argument in an assignment
 function?
In-Reply-To: <971536df0607270310l1af4af09ibce7033623e12281@mail.gmail.co
 m>
References: <3.0.6.32.20060727104250.00ad67c0@pop.gmx.net>
	<3.0.6.32.20060727104250.00ad67c0@pop.gmx.net>
Message-ID: <3.0.6.32.20060727142958.00acf198@pop.gmx.net>

At 06:10 27.07.2006 -0400, Gabor Grothendieck wrote:
>If you are willing to write fu2[Var] <- 3 instead of fu2(Var) <- 3
>then this workaround may suffice:
>
>fu2 <- structure(NA, class = "fu2")
>"[<-.fu2" <- function(x, ..., value) { print(match.call()[[3]]); fu2 }
>
># test
>fu2[Var] <- 3  # prints "Var"
>
>
Thank you, Gabor for your response. My example was very reduced, just to
show the point, but in the way I would like to use it, probably your
solution may be difficult to apply. Seems, I have to accept that I cannot
solve it.

Thanks again,
Heinz

>On 7/27/06, Heinz Tuechler <tuechler at gmx.at> wrote:
>> Dear All!
>>
>> If I pass an object to an assignment function I cannot get it's name by
>> deparse(substitute(argument)), but I get *tmp* and I found no way to get
>> the original name, in the example below it should be "va1".
>> Is there a way?
>>
>> Thanks,
>>
>> Heinz
>>
>> ## example
>> 'fu1<-' <- function(var, value) {
>> print(c(name.of.var=deparse(substitute(var))))}
>> fu1(va1) <- 3
>>
>> name.of.var
>>    "*tmp*"
>>
>> ## desired result:
>> ## name.of.var
>> ##    "va1"
>>
>>
>>
>> version
>>               _
>> platform       i386-pc-mingw32
>> arch           i386
>> os             mingw32
>> system         i386, mingw32
>> status         Patched
>> major          2
>> minor          3.1
>> year           2006
>> month          07
>> day            23
>> svn rev        38687
>> language       R
>> version.string Version 2.3.1 Patched (2006-07-23 r38687)
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>


From ggrothendieck at gmail.com  Thu Jul 27 15:38:15 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 27 Jul 2006 09:38:15 -0400
Subject: [R] How to get the name of the first argument in an assignment
	function?
In-Reply-To: <3.0.6.32.20060727142958.00acf198@pop.gmx.net>
References: <3.0.6.32.20060727104250.00ad67c0@pop.gmx.net>
	<3.0.6.32.20060727142958.00acf198@pop.gmx.net>
Message-ID: <971536df0607270638s26273e38n7dc89203980d3e9b@mail.gmail.com>

The complexity of the function should not matter.

Here is another example of this technique:
http://tolstoy.newcastle.edu.au/R/help/04/06/1430.html

On 7/27/06, Heinz Tuechler <tuechler at gmx.at> wrote:
> At 06:10 27.07.2006 -0400, Gabor Grothendieck wrote:
> >If you are willing to write fu2[Var] <- 3 instead of fu2(Var) <- 3
> >then this workaround may suffice:
> >
> >fu2 <- structure(NA, class = "fu2")
> >"[<-.fu2" <- function(x, ..., value) { print(match.call()[[3]]); fu2 }
> >
> ># test
> >fu2[Var] <- 3  # prints "Var"
> >
> >
> Thank you, Gabor for your response. My example was very reduced, just to
> show the point, but in the way I would like to use it, probably your
> solution may be difficult to apply. Seems, I have to accept that I cannot
> solve it.
>
> Thanks again,
> Heinz
>
> >On 7/27/06, Heinz Tuechler <tuechler at gmx.at> wrote:
> >> Dear All!
> >>
> >> If I pass an object to an assignment function I cannot get it's name by
> >> deparse(substitute(argument)), but I get *tmp* and I found no way to get
> >> the original name, in the example below it should be "va1".
> >> Is there a way?
> >>
> >> Thanks,
> >>
> >> Heinz
> >>
> >> ## example
> >> 'fu1<-' <- function(var, value) {
> >> print(c(name.of.var=deparse(substitute(var))))}
> >> fu1(va1) <- 3
> >>
> >> name.of.var
> >>    "*tmp*"
> >>
> >> ## desired result:
> >> ## name.of.var
> >> ##    "va1"
> >>
> >>
> >>
> >> version
> >>               _
> >> platform       i386-pc-mingw32
> >> arch           i386
> >> os             mingw32
> >> system         i386, mingw32
> >> status         Patched
> >> major          2
> >> minor          3.1
> >> year           2006
> >> month          07
> >> day            23
> >> svn rev        38687
> >> language       R
> >> version.string Version 2.3.1 Patched (2006-07-23 r38687)
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >
>
>


From r.hankin at noc.soton.ac.uk  Thu Jul 27 15:54:02 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 27 Jul 2006 14:54:02 +0100
Subject: [R] inserting rows into a matrix
Message-ID: <6469ADFD-C518-47B0-9F3B-CF48EA046A61@soc.soton.ac.uk>

Hi


I have a little vector function that takes a vector A of strictly  
positive integers
and outputs a matrix M  each of whose columns is the vector, modified in
a complicated combinatorical way.

Now I want to generalize the function so that A can include zeroes.   
Given A,
I want to strip out the zeroes, pass it to my function, and pad  M
  with rows at positions corresponding to the zeroes of A.

Commented, minimal, self-contained, reproducible toy example follows.


f <- function(a){cbind(a,a+1,rev(a))}  #real function a ghastly  
nightmare

  A <- 1:5
  f(A)
      a
[1,] 1 2 5
[2,] 2 3 4
[3,] 3 4 3
[4,] 4 5 2
[5,] 5 6 1


# f() works as desired.

# Now introduce A2, that includes zeroes.  In my application, f(A2)  
would fail
because of the zeroes.

A2 <- c(1,0,0,2,4,0,3)

I can strip the zeroes out and call f():
  f(A2[A2>0])
      a
[1,] 1 2 3
[2,] 2 3 4
[3,] 4 5 2
[4,] 3 4 1

which is fine.  How to put the zeroes back in in the appropriate rows
and get the following:

 > cbind(c(1,0,0,2,4,0,3),c(2,0,0,3,5,0,4),c(3,0,0,4,2,0,1))
      [,1] [,2] [,3]
[1,]    1    2    3
[2,]    0    0    0
[3,]    0    0    0
[4,]    2    3    4
[5,]    4    5    2
[6,]    0    0    0
[7,]    3    4    1
 >



anyone?



--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From jessica.gervais at tudor.lu  Thu Jul 27 16:00:42 2006
From: jessica.gervais at tudor.lu (Jessica Gervais)
Date: Thu, 27 Jul 2006 16:00:42 +0200
Subject: [R] transformation matrice of vector into array
Message-ID: <1154008842.15282.7.camel@flores>

Hi,

I need some help

I have a matrix M(m,n) in which each element is a vector V of lenght 6
 1      2      3      4      5      6      7
1   List,6 List,6 List,6 List,6 List,6 List,6 List,6
2   List,6 List,6 List,6 List,6 List,6 List,6 List,6
3   List,6 List,6 List,6 List,6 List,6 List,6 List,6
4   List,6 List,6 List,6 List,6 List,6 List,6 List,6


i would like to make the sum on the matrix of each element of the
matrix, that is to say 
sum(on the matrix)(M[j,][[j]][[1]])
sum(on the matrix)(M[j,][[j]][[2]])
...
sum(on the matrix)(M[j,][[j]][[6]])  

I don't really know how to do.
I thought it was possible to transform the matrix M into an array A of
dimension (m,n,6), and then use the command sum(colsums(A[,,1]), which
seems to be possible and quite fast.
...but I don't know how to convert a matrix of vector into an array....

As anyone any little idea about that ?

Thanks by advance

Jessica


From christos at nuverabio.com  Thu Jul 27 16:10:21 2006
From: christos at nuverabio.com (Christos Hatzis)
Date: Thu, 27 Jul 2006 10:10:21 -0400
Subject: [R] inserting rows into a matrix
In-Reply-To: <6469ADFD-C518-47B0-9F3B-CF48EA046A61@soc.soton.ac.uk>
Message-ID: <003101c6b186$66273f60$0e010a0a@headquarters.silicoinsights>

This is not as elegant, but should work:

a3 <- f(A2)
a3[ which( apply(a3,1,prod) == 0 ), ] <- rep(0,ncol(a3))
a3

Essentially use the product to pick out the rows with at least one 0 and
replace these rows with 0s.

HTH.
-Christos 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robin Hankin
Sent: Thursday, July 27, 2006 9:54 AM
To: RHelp
Subject: [R] inserting rows into a matrix

Hi


I have a little vector function that takes a vector A of strictly positive
integers and outputs a matrix M  each of whose columns is the vector,
modified in a complicated combinatorical way.

Now I want to generalize the function so that A can include zeroes.   
Given A,
I want to strip out the zeroes, pass it to my function, and pad  M
  with rows at positions corresponding to the zeroes of A.

Commented, minimal, self-contained, reproducible toy example follows.


f <- function(a){cbind(a,a+1,rev(a))}  #real function a ghastly nightmare

  A <- 1:5
  f(A)
      a
[1,] 1 2 5
[2,] 2 3 4
[3,] 3 4 3
[4,] 4 5 2
[5,] 5 6 1


# f() works as desired.

# Now introduce A2, that includes zeroes.  In my application, f(A2) would
fail because of the zeroes.

A2 <- c(1,0,0,2,4,0,3)

I can strip the zeroes out and call f():
  f(A2[A2>0])
      a
[1,] 1 2 3
[2,] 2 3 4
[3,] 4 5 2
[4,] 3 4 1

which is fine.  How to put the zeroes back in in the appropriate rows and
get the following:

 > cbind(c(1,0,0,2,4,0,3),c(2,0,0,3,5,0,4),c(3,0,0,4,2,0,1))
      [,1] [,2] [,3]
[1,]    1    2    3
[2,]    0    0    0
[3,]    0    0    0
[4,]    2    3    4
[5,]    4    5    2
[6,]    0    0    0
[7,]    3    4    1
 >



anyone?



--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton European Way, Southampton SO14
3ZH, UK
  tel  023-8059-7743

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From r.hankin at noc.soton.ac.uk  Thu Jul 27 16:15:43 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 27 Jul 2006 15:15:43 +0100
Subject: [R] inserting rows into a matrix
In-Reply-To: <003101c6b186$66273f60$0e010a0a@headquarters.silicoinsights>
References: <003101c6b186$66273f60$0e010a0a@headquarters.silicoinsights>
Message-ID: <DC27DB01-9B11-4E2E-A8EF-989F69A66CA2@soc.soton.ac.uk>

Hi Christos

thanks for this, but it won't work because in my application
f(A2) will fail if there are any zeroes in A2.


cheers

rksh


On 27 Jul 2006, at 15:10, Christos Hatzis wrote:

> This is not as elegant, but should work:
>
> a3 <- f(A2)
> a3[ which( apply(a3,1,prod) == 0 ), ] <- rep(0,ncol(a3))
> a3
>
> Essentially use the product to pick out the rows with at least one  
> 0 and
> replace these rows with 0s.
>
> HTH.
> -Christos
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robin Hankin
> Sent: Thursday, July 27, 2006 9:54 AM
> To: RHelp
> Subject: [R] inserting rows into a matrix
>
> Hi
>
>
> I have a little vector function that takes a vector A of strictly  
> positive
> integers and outputs a matrix M  each of whose columns is the vector,
> modified in a complicated combinatorical way.
>
> Now I want to generalize the function so that A can include zeroes.
> Given A,
> I want to strip out the zeroes, pass it to my function, and pad  M
>   with rows at positions corresponding to the zeroes of A.
>
> Commented, minimal, self-contained, reproducible toy example follows.
>
>
> f <- function(a){cbind(a,a+1,rev(a))}  #real function a ghastly  
> nightmare
>
>   A <- 1:5
>   f(A)
>       a
> [1,] 1 2 5
> [2,] 2 3 4
> [3,] 3 4 3
> [4,] 4 5 2
> [5,] 5 6 1
>
>
> # f() works as desired.
>
> # Now introduce A2, that includes zeroes.  In my application, f(A2)  
> would
> fail because of the zeroes.
>
> A2 <- c(1,0,0,2,4,0,3)
>
> I can strip the zeroes out and call f():
>   f(A2[A2>0])
>       a
> [1,] 1 2 3
> [2,] 2 3 4
> [3,] 4 5 2
> [4,] 3 4 1
>
> which is fine.  How to put the zeroes back in in the appropriate  
> rows and
> get the following:
>
>> cbind(c(1,0,0,2,4,0,3),c(2,0,0,3,5,0,4),c(3,0,0,4,2,0,1))
>       [,1] [,2] [,3]
> [1,]    1    2    3
> [2,]    0    0    0
> [3,]    0    0    0
> [4,]    2    3    4
> [5,]    4    5    2
> [6,]    0    0    0
> [7,]    3    4    1
>>
>
>
>
> anyone?
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton European Way, Southampton  
> SO14
> 3ZH, UK
>   tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From neuro3000 at hotmail.com  Thu Jul 27 16:16:51 2006
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Thu, 27 Jul 2006 10:16:51 -0400
Subject: [R] inserting rows into a matrix
In-Reply-To: <6469ADFD-C518-47B0-9F3B-CF48EA046A61@soc.soton.ac.uk>
Message-ID: <BAY112-F353FFB7E2714ECA6A3A3E4AF580@phx.gbl>

Hello Robin,

Here's a solution.

library(micEcon)

nozeros <-f(A2[A2>0])
zeroslines <- which(A2==0) #identify rows to insert zeros
withzeros <-nozeros #initialize matrix with zeros
for (i in zeroslines){
withzeros <- insertRow(withzeros,i,0)
}
withzeros

Neurooo


>From: Robin Hankin <r.hankin at noc.soton.ac.uk>
>To: RHelp <r-help at stat.math.ethz.ch>
>Subject: [R] inserting rows into a matrix
>Date: Thu, 27 Jul 2006 14:54:02 +0100
>
>Hi
>
>
>I have a little vector function that takes a vector A of strictly
>positive integers
>and outputs a matrix M  each of whose columns is the vector, modified in
>a complicated combinatorical way.
>
>Now I want to generalize the function so that A can include zeroes.
>Given A,
>I want to strip out the zeroes, pass it to my function, and pad  M
>   with rows at positions corresponding to the zeroes of A.
>
>Commented, minimal, self-contained, reproducible toy example follows.
>
>
>f <- function(a){cbind(a,a+1,rev(a))}  #real function a ghastly
>nightmare
>
>   A <- 1:5
>   f(A)
>       a
>[1,] 1 2 5
>[2,] 2 3 4
>[3,] 3 4 3
>[4,] 4 5 2
>[5,] 5 6 1
>
>
># f() works as desired.
>
># Now introduce A2, that includes zeroes.  In my application, f(A2)
>would fail
>because of the zeroes.
>
>A2 <- c(1,0,0,2,4,0,3)
>
>I can strip the zeroes out and call f():
>   f(A2[A2>0])
>       a
>[1,] 1 2 3
>[2,] 2 3 4
>[3,] 4 5 2
>[4,] 3 4 1
>
>which is fine.  How to put the zeroes back in in the appropriate rows
>and get the following:
>
>  > cbind(c(1,0,0,2,4,0,3),c(2,0,0,3,5,0,4),c(3,0,0,4,2,0,1))
>       [,1] [,2] [,3]
>[1,]    1    2    3
>[2,]    0    0    0
>[3,]    0    0    0
>[4,]    2    3    4
>[5,]    4    5    2
>[6,]    0    0    0
>[7,]    3    4    1
>  >
>
>
>
>anyone?
>
>
>
>--
>Robin Hankin
>Uncertainty Analyst
>National Oceanography Centre, Southampton
>European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From kubovy at virginia.edu  Thu Jul 27 16:16:58 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Thu, 27 Jul 2006 10:16:58 -0400
Subject: [R] package/namespace load failed for 'Rgraphviz'
Message-ID: <1F805A73-A7C4-4ED0-B4AE-95B0C1875EAA@virginia.edu>

Dear R-helpers,

Can anyone tell me how to fix this:
 > library(Rgraphviz)
Loading required package: graph
Loading required package: Ruuid
Error in dyn.load(x, as.logical(local), as.logical(now)) :
	unable to load shared library '/Library/Frameworks/R.framework/ 
Resources/library/Rgraphviz/libs/ppc/Rgraphviz.so':
   dlopen(/Library/Frameworks/R.framework/Resources/library/Rgraphviz/ 
libs/ppc/Rgraphviz.so, 6): Library not loaded: /usr/local/lib/libpng. 
3.dylib
   Referenced from: /usr/local/lib/graphviz/libgvc.2.dylib
   Reason: image not found
Error: .onLoad failed in 'loadNamespace' for 'Rgraphviz'
Error: package/namespace load failed for 'Rgraphviz'

 > sessionInfo()
Version 2.3.1 (2006-06-01)
powerpc-apple-darwin8.6.0

attached base packages:
[1] "datasets"  "methods"   "stats"     "graphics"  "grDevices"  
"utils"     "base"

other attached packages:
      graph      Ruuid    lattice       MASS        JGR      
JavaGD      rJava
   "1.10.6"   "1.10.0"  "0.13-10" "7.2-27.1"    "1.4-2"    "0.3-3"     
"0.4-3"
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From valentin.to at gmail.com  Thu Jul 27 16:28:22 2006
From: valentin.to at gmail.com (Valentin Todorov)
Date: Thu, 27 Jul 2006 16:28:22 +0200
Subject: [R]  greek letters, text, and values in labels
Message-ID: <4c0e456b0607270728q69fc1b9an775df113a88d66c7@mail.gmail.com>

Unfortunately this does not work for lattice graphics. In such case I
do something like the following, but I still do not know how to plot
Greek letters in the panel titles:

theta <- 2.1
gr <- as.factor(c(1,2))
levels(gr)[1]<-"Group 1"
levels(gr)[2]<-"Group 2"
library(lattice)
print(xyplot(1~1|gr,
    xlab=eval(substitute(expression(paste(theta, " = ", tval)),
list(tval=theta))),
    ylab=eval(substitute(expression(paste(theta, " = ", tval)),
list(tval=theta))),
    main=eval(substitute(expression(paste("Results for ",theta, " = ",
tval)), list(tval=theta))),
    sub=eval(substitute(expression(paste(theta, " = ", tval)),
list(tval=theta)))
    ))


best,
valentin


From christos at nuverabio.com  Thu Jul 27 16:34:52 2006
From: christos at nuverabio.com (Christos Hatzis)
Date: Thu, 27 Jul 2006 10:34:52 -0400
Subject: [R] inserting rows into a matrix
In-Reply-To: <DC27DB01-9B11-4E2E-A8EF-989F69A66CA2@soc.soton.ac.uk>
Message-ID: <003e01c6b189$d324d480$0e010a0a@headquarters.silicoinsights>

Hi Robin,

Ok.  I see.  Try this then:

f <- function(a){if( any(a==0) ) stop("Zeros in input vector") else
cbind(a,a+1,rev(a))}
a2 <- c(1,0,0,2,4,0,3)

f(a2) # error message

f2 <- function(a) {
    indx.zero <- which(a==0)
    indx.nz <- (1:length(a))[-indx.zero]
    
    x <- f(a[indx.nz])
    xx <- matrix(NA,length(a),ncol(x))
    xx[ indx.zero, ] <- rep(0,ncol(x))
    xx[ indx.nz, ] <- x
    xx
}

> f2(a2)
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    0    0    0
[3,]    0    0    0
[4,]    2    3    4
[5,]    4    5    2
[6,]    0    0    0
[7,]    3    4    1

-Christos

-----Original Message-----
From: Robin Hankin [mailto:r.hankin at noc.soton.ac.uk] 
Sent: Thursday, July 27, 2006 10:16 AM
To: christos at nuverabio.com
Cc: 'Robin Hankin'; 'RHelp'
Subject: Re: [R] inserting rows into a matrix

Hi Christos

thanks for this, but it won't work because in my application
f(A2) will fail if there are any zeroes in A2.


cheers

rksh


On 27 Jul 2006, at 15:10, Christos Hatzis wrote:

> This is not as elegant, but should work:
>
> a3 <- f(A2)
> a3[ which( apply(a3,1,prod) == 0 ), ] <- rep(0,ncol(a3))
> a3
>
> Essentially use the product to pick out the rows with at least one 0 
> and replace these rows with 0s.
>
> HTH.
> -Christos
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robin Hankin
> Sent: Thursday, July 27, 2006 9:54 AM
> To: RHelp
> Subject: [R] inserting rows into a matrix
>
> Hi
>
>
> I have a little vector function that takes a vector A of strictly 
> positive integers and outputs a matrix M  each of whose columns is the 
> vector, modified in a complicated combinatorical way.
>
> Now I want to generalize the function so that A can include zeroes.
> Given A,
> I want to strip out the zeroes, pass it to my function, and pad  M
>   with rows at positions corresponding to the zeroes of A.
>
> Commented, minimal, self-contained, reproducible toy example follows.
>
>
> f <- function(a){cbind(a,a+1,rev(a))}  #real function a ghastly 
> nightmare
>
>   A <- 1:5
>   f(A)
>       a
> [1,] 1 2 5
> [2,] 2 3 4
> [3,] 3 4 3
> [4,] 4 5 2
> [5,] 5 6 1
>
>
> # f() works as desired.
>
> # Now introduce A2, that includes zeroes.  In my application, f(A2) 
> would fail because of the zeroes.
>
> A2 <- c(1,0,0,2,4,0,3)
>
> I can strip the zeroes out and call f():
>   f(A2[A2>0])
>       a
> [1,] 1 2 3
> [2,] 2 3 4
> [3,] 4 5 2
> [4,] 3 4 1
>
> which is fine.  How to put the zeroes back in in the appropriate rows 
> and get the following:
>
>> cbind(c(1,0,0,2,4,0,3),c(2,0,0,3,5,0,4),c(3,0,0,4,2,0,1))
>       [,1] [,2] [,3]
> [1,]    1    2    3
> [2,]    0    0    0
> [3,]    0    0    0
> [4,]    2    3    4
> [5,]    4    5    2
> [6,]    0    0    0
> [7,]    3    4    1
>>
>
>
>
> anyone?
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton European Way, Southampton
> SO14
> 3ZH, UK
>   tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html and provide commented, minimal, self-contained, 
> reproducible code.
>
>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton European Way, Southampton SO14
3ZH, UK
  tel  023-8059-7743


From ggrothendieck at gmail.com  Thu Jul 27 16:36:45 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 27 Jul 2006 10:36:45 -0400
Subject: [R] inserting rows into a matrix
In-Reply-To: <6469ADFD-C518-47B0-9F3B-CF48EA046A61@soc.soton.ac.uk>
References: <6469ADFD-C518-47B0-9F3B-CF48EA046A61@soc.soton.ac.uk>
Message-ID: <971536df0607270736w2a8d5994r8bbcb1da84991714@mail.gmail.com>

Try this where f and A2 are as in your post:

   out <-f(A2[A2>0])
   replace(matrix(0, length(A2), ncol(out)), A2 > 0, out)


On 7/27/06, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> Hi
>
>
> I have a little vector function that takes a vector A of strictly
> positive integers
> and outputs a matrix M  each of whose columns is the vector, modified in
> a complicated combinatorical way.
>
> Now I want to generalize the function so that A can include zeroes.
> Given A,
> I want to strip out the zeroes, pass it to my function, and pad  M
>  with rows at positions corresponding to the zeroes of A.
>
> Commented, minimal, self-contained, reproducible toy example follows.
>
>
> f <- function(a){cbind(a,a+1,rev(a))}  #real function a ghastly
> nightmare
>
>  A <- 1:5
>  f(A)
>      a
> [1,] 1 2 5
> [2,] 2 3 4
> [3,] 3 4 3
> [4,] 4 5 2
> [5,] 5 6 1
>
>
> # f() works as desired.
>
> # Now introduce A2, that includes zeroes.  In my application, f(A2)
> would fail
> because of the zeroes.
>
> A2 <- c(1,0,0,2,4,0,3)
>
> I can strip the zeroes out and call f():
>  f(A2[A2>0])
>      a
> [1,] 1 2 3
> [2,] 2 3 4
> [3,] 4 5 2
> [4,] 3 4 1
>
> which is fine.  How to put the zeroes back in in the appropriate rows
> and get the following:
>
>  > cbind(c(1,0,0,2,4,0,3),c(2,0,0,3,5,0,4),c(3,0,0,4,2,0,1))
>      [,1] [,2] [,3]
> [1,]    1    2    3
> [2,]    0    0    0
> [3,]    0    0    0
> [4,]    2    3    4
> [5,]    4    5    2
> [6,]    0    0    0
> [7,]    3    4    1
>  >
>
>
>
> anyone?
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Thu Jul 27 16:38:22 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 27 Jul 2006 10:38:22 -0400
Subject: [R] Vector extracted from a matrix. How can I specify
	dimensions in as.matrix?
In-Reply-To: <BAY112-F36BE8D4802E9C5BF66511BAF580@phx.gbl>
References: <BAY112-F36BE8D4802E9C5BF66511BAF580@phx.gbl>
Message-ID: <971536df0607270738v4dbfa4e9m9d4a8e666f5bbb9c@mail.gmail.com>

Use the notation x[ , 1, drop = FALSE]
See ?"["

On 7/27/06, Neuro LeSuperH?ros <neuro3000 at hotmail.com> wrote:
> Transpose vector extracted from a matrix
>
> Hello,
>
> I am doing a recursive analysis that uses every line (vector) of a matrix in
> a loop. In the model, I need to transpose those vectors that are extracted
> from a matrix.
>
> Using simple vectors (no matrix involved) the transpose function works fine:
>
> simplevector <-matrix(1:3,3,1)
> tsimplevector <-t(simplevector) #transposed
> dim(simplevector) #3x1 matrix (vector)
> dim(tsimplevector)#1x3
>
> PROBLEM: However, when the vector is extracted from a matrix, its dimension
> is NULL.  In this case the transposed dimension is correct, but you'll see
> the next example, that if a row was extracted, the dimension would be wrong:
>
> initialmatrix <- matrix(1:9,3,3)
> extractedvector <-initialmatrix[,1] # extract first columm as vector
> textractedvector <-t(extractedvector)#transposed
> dim(extractedvector) #NULL!!!! <- not working
> dim(textractedvector)#1x3 as expected
>
> I have tried to transform the extracted vector as.vector and as.matrix.
> as.vector does not give the what I want (still NULL) and as.matrix can't
> specify the number of row and columns so both vectors are vertical.
>
> In this example, I extract a column and a row. Notice that both
> as.matrix(vector) have the same dimension. Notice that both transposed
> vectors have 1x3 dimensions.
>
> initialmatrix <- matrix(1:9,3,3)
> extractedvector3x1 <-initialmatrix[,1]  # extract first columm as vector
> extractedvector1x3 <-initialmatrix[1,]  # extract first row as vector
>
> #Both are NULL
> dim(extractedvector3x1) #NULL!!!!
> dim(extractedvector1x3) #NULL!!!!
>
> #transposed dimensions both 1x3
> dim(t(extractedvector3x1)) #1x3
> dim(t(extractedvector1x3)) #1x3 <- not 3x1
>
> #as.vector: still NULL
> dim(as.vector(extractedvector3x1)) #NULL!!!!
> dim(as.vector(extractedvector1x3)) #NULL!!!!
>
> #as.matrix: Both dim at 3x1
> dim(as.matrix(extractedvector3x1)) #3x1
> dim(as.matrix(extractedvector1x3)) #3x1 <- Problem: not 1x3
>
> #as.matrix transposed: Both dim at 1x3
> dim(as.matrix(t(extractedvector3x1))) #1x3
> dim(as.matrix(t(extractedvector1x3))) #1x3 <- Problem: not 3x1
>
> How can I get correct dimensions? Is there a way to specify dimensions in
> as.matrix?
>
> Neuro
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From r.hankin at noc.soton.ac.uk  Thu Jul 27 16:42:46 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 27 Jul 2006 15:42:46 +0100
Subject: [R] inserting rows into a matrix
In-Reply-To: <003e01c6b189$d324d480$0e010a0a@headquarters.silicoinsights>
References: <003e01c6b189$d324d480$0e010a0a@headquarters.silicoinsights>
Message-ID: <A90129F2-25E0-4FC2-9D3E-2A2A25C2D30B@soc.soton.ac.uk>

Gabor, Christos

great, two elegant  vectorized  solutions.  I spent quite a long time
trying to generalize my f() to accept zeroes with no luck (long
story), so either of these solutions is fine.



TDH  [this *did* help]

rksh



On 27 Jul 2006, at 15:34, Christos Hatzis wrote:

> Hi Robin,
>
> Ok.  I see.  Try this then:
>
> f <- function(a){if( any(a==0) ) stop("Zeros in input vector") else
> cbind(a,a+1,rev(a))}
> a2 <- c(1,0,0,2,4,0,3)
>
> f(a2) # error message
>
> f2 <- function(a) {
>     indx.zero <- which(a==0)
>     indx.nz <- (1:length(a))[-indx.zero]
>
>     x <- f(a[indx.nz])
>     xx <- matrix(NA,length(a),ncol(x))
>     xx[ indx.zero, ] <- rep(0,ncol(x))
>     xx[ indx.nz, ] <- x
>     xx
> }
>
>> f2(a2)
>      [,1] [,2] [,3]
> [1,]    1    2    3
> [2,]    0    0    0
> [3,]    0    0    0
> [4,]    2    3    4
> [5,]    4    5    2
> [6,]    0    0    0
> [7,]    3    4    1
>
> -Christos
>
> -----Original Message-----
> From: Robin Hankin [mailto:r.hankin at noc.soton.ac.uk]
> Sent: Thursday, July 27, 2006 10:16 AM
> To: christos at nuverabio.com
> Cc: 'Robin Hankin'; 'RHelp'
> Subject: Re: [R] inserting rows into a matrix
>
> Hi Christos
>
> thanks for this, but it won't work because in my application
> f(A2) will fail if there are any zeroes in A2.
>
>
> cheers
>
> rksh
>
>
> On 27 Jul 2006, at 15:10, Christos Hatzis wrote:
>
>> This is not as elegant, but should work:
>>
>> a3 <- f(A2)
>> a3[ which( apply(a3,1,prod) == 0 ), ] <- rep(0,ncol(a3))
>> a3
>>
>> Essentially use the product to pick out the rows with at least one 0
>> and replace these rows with 0s.
>>
>> HTH.
>> -Christos
>>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robin Hankin
>> Sent: Thursday, July 27, 2006 9:54 AM
>> To: RHelp
>> Subject: [R] inserting rows into a matrix
>>
>> Hi
>>
>>
>> I have a little vector function that takes a vector A of strictly
>> positive integers and outputs a matrix M  each of whose columns is  
>> the
>> vector, modified in a complicated combinatorical way.
>>
>> Now I want to generalize the function so that A can include zeroes.
>> Given A,
>> I want to strip out the zeroes, pass it to my function, and pad  M
>>   with rows at positions corresponding to the zeroes of A.
>>
>> Commented, minimal, self-contained, reproducible toy example follows.
>>
>>
>> f <- function(a){cbind(a,a+1,rev(a))}  #real function a ghastly
>> nightmare
>>
>>   A <- 1:5
>>   f(A)
>>       a
>> [1,] 1 2 5
>> [2,] 2 3 4
>> [3,] 3 4 3
>> [4,] 4 5 2
>> [5,] 5 6 1
>>
>>
>> # f() works as desired.
>>
>> # Now introduce A2, that includes zeroes.  In my application, f(A2)
>> would fail because of the zeroes.
>>
>> A2 <- c(1,0,0,2,4,0,3)
>>
>> I can strip the zeroes out and call f():
>>   f(A2[A2>0])
>>       a
>> [1,] 1 2 3
>> [2,] 2 3 4
>> [3,] 4 5 2
>> [4,] 3 4 1
>>
>> which is fine.  How to put the zeroes back in in the appropriate rows
>> and get the following:
>>
>>> cbind(c(1,0,0,2,4,0,3),c(2,0,0,3,5,0,4),c(3,0,0,4,2,0,1))
>>       [,1] [,2] [,3]
>> [1,]    1    2    3
>> [2,]    0    0    0
>> [3,]    0    0    0
>> [4,]    2    3    4
>> [5,]    4    5    2
>> [6,]    0    0    0
>> [7,]    3    4    1
>>>
>>
>>
>>
>> anyone?
>>
>>
>>
>> --
>> Robin Hankin
>> Uncertainty Analyst
>> National Oceanography Centre, Southampton European Way, Southampton
>> SO14
>> 3ZH, UK
>>   tel  023-8059-7743
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html and provide commented, minimal, self-contained,
>> reproducible code.
>>
>>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton European Way, Southampton  
> SO14
> 3ZH, UK
>   tel  023-8059-7743
>
>
>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From jeff.horner at vanderbilt.edu  Thu Jul 27 16:53:52 2006
From: jeff.horner at vanderbilt.edu (Jeffrey Horner)
Date: Thu, 27 Jul 2006 09:53:52 -0500
Subject: [R] RODBC on linux
In-Reply-To: <E58BE6136618CF4C964F6EC7773AE5699FA5C0@ex4.nyc.hcmny.com>
References: <E58BE6136618CF4C964F6EC7773AE5699FA5C0@ex4.nyc.hcmny.com>
Message-ID: <44C8D380.1040209@vanderbilt.edu>

Armstrong, Whit wrote:
> Thanks, everyone.
> 
> I'll give freeTDS a try.

One final hitch to be aware of when using FreeTDS:

MS SQL Server authenticates db connections in two different ways (well 
actually three, but it's just the combination of 1 and 2):

1) Sql server authenticates the connection, meaning you must have
    an SQL server account. FreeTDS will handle this fine.
2) the SQL server OS authenticates the connection with an NT Domain
    login. That's the domain\username you use to login to your Windows
    machine. FreeTDS will ONLY handle this over TCP/IP, not over DCE/RPC,
    so be aware.

More info here:

http://www.freetds.org/userguide/domains.htm

and here:

http://www.freetds.org/userguide/troubleshooting.htm

Jeff

> 
> 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: Thursday, July 27, 2006 1:17 AM
> To: Marc Schwartz
> Cc: Armstrong, Whit; r-help at stat.math.ethz.ch
> Subject: Re: [R] RODBC on linux
> 
> On Wed, 26 Jul 2006, Marc Schwartz wrote:
> 
>> On Wed, 2006-07-26 at 17:52 -0400, Armstrong, Whit wrote:
>>> Anyone out there using Linux RODBC and unixODBC to connect to a 
>>> Microsoft SQL server?
>>>
>>> If possible can someone post a sample .odbc.ini file?
>>>
>>> I saw a few discussions on the archives a few years ago, but no 
>>> config file details were available.
>>>
>>> Thanks,
>>> Whit
>> Whit,
>>
>> Do you have a Linux ODBC driver for SQL Server?  unixODBC is simply 
>> the driver manager, not the driver itself.
>>
>> MS does not offer (not surprisingly) an ODBC driver for Unix/Linux.
>> There are resources available however and these might be helpful:
>>
>> http://www.sommarskog.se/mssql/unix.html
>>
>> Note that Easysoft provides (at a cost) an ODBC-ODBC bridge for 
>> Unix/Linux platforms which supports ODBC connections to SQL Server:
>>
>> http://www.easysoft.com/products/data_access/odbc_odbc_bridge/index.ht
>> ml
> 
> Several people have successfully used that, from the earliest days of
> RODBC: I believe it was part of Michael Lapsley's motivation to write
> RODBC.
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Thu Jul 27 16:55:58 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 27 Jul 2006 10:55:58 -0400
Subject: [R] greek letters, text, and values in labels
In-Reply-To: <4c0e456b0607270728q69fc1b9an775df113a88d66c7@mail.gmail.com>
References: <4c0e456b0607270728q69fc1b9an775df113a88d66c7@mail.gmail.com>
Message-ID: <971536df0607270755gc24d729p77fb2ba28b9d7659@mail.gmail.com>

Try this where gr and theta are as in your post:

xyplot(1~1|gr,
	main = as.expression(bquote(theta == .(theta))),
	strip = strip.custom(factor.levels = expression(theta, beta))
)


On 7/27/06, Valentin Todorov <valentin.to at gmail.com> wrote:
> Unfortunately this does not work for lattice graphics. In such case I
> do something like the following, but I still do not know how to plot
> Greek letters in the panel titles:
>
> theta <- 2.1
> gr <- as.factor(c(1,2))
> levels(gr)[1]<-"Group 1"
> levels(gr)[2]<-"Group 2"
> library(lattice)
> print(xyplot(1~1|gr,
>    xlab=eval(substitute(expression(paste(theta, " = ", tval)),
> list(tval=theta))),
>    ylab=eval(substitute(expression(paste(theta, " = ", tval)),
> list(tval=theta))),
>    main=eval(substitute(expression(paste("Results for ",theta, " = ",
> tval)), list(tval=theta))),
>    sub=eval(substitute(expression(paste(theta, " = ", tval)),
> list(tval=theta)))
>    ))
>
>
> best,
> valentin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tplate at acm.org  Thu Jul 27 16:59:44 2006
From: tplate at acm.org (Tony Plate)
Date: Thu, 27 Jul 2006 08:59:44 -0600
Subject: [R] transformation matrice of vector into array
In-Reply-To: <1154008842.15282.7.camel@flores>
References: <1154008842.15282.7.camel@flores>
Message-ID: <44C8D4E0.8050005@acm.org>

Here's a way to convert a matrix of vectors like you have into an array:

 > x <- array(lapply(seq(0,len=6,by=4), "+", c(a=1,b=2,c=3,d=4)), 
dim=c(2,3), dimnames=list(c("X","Y"),c("e","f","g")))
 > x
   e         f         g
X Numeric,4 Numeric,4 Numeric,4
Y Numeric,4 Numeric,4 Numeric,4
 > x[["Y","e"]]
a b c d
5 6 7 8
 > xa <- array(unlist(x, use.names=F), dim=c(length(x[[1,1]]),dim(x)), 
dimnames=c(list(names(x[[1,1]])),dimnames(x)))
 > x["Y","e"]
[[1]]
a b c d
5 6 7 8

 > xa[,"Y","e"]
a b c d
5 6 7 8
 >

Then you can do whatever sums you want over the array.

I have not extensively checked the above code, and if I were going to 
use it, I would do numerous spot checks of elements to make sure all the 
elements are going to the right places -- it's not too difficult to make 
mistakes when pulling apart and reassembling arrays like this.  (For 
simpler cases involving lists of vectors or matrices, the abind() 
function can help.)

-- Tony Plate

Jessica Gervais wrote:
> Hi,
> 
> I need some help
> 
> I have a matrix M(m,n) in which each element is a vector V of lenght 6
>  1      2      3      4      5      6      7
> 1   List,6 List,6 List,6 List,6 List,6 List,6 List,6
> 2   List,6 List,6 List,6 List,6 List,6 List,6 List,6
> 3   List,6 List,6 List,6 List,6 List,6 List,6 List,6
> 4   List,6 List,6 List,6 List,6 List,6 List,6 List,6
> 
> 
> i would like to make the sum on the matrix of each element of the
> matrix, that is to say 
> sum(on the matrix)(M[j,][[j]][[1]])
> sum(on the matrix)(M[j,][[j]][[2]])
> ...
> sum(on the matrix)(M[j,][[j]][[6]])  
> 
> I don't really know how to do.
> I thought it was possible to transform the matrix M into an array A of
> dimension (m,n,6), and then use the command sum(colsums(A[,,1]), which
> seems to be possible and quite fast.
> ...but I don't know how to convert a matrix of vector into an array....
> 
> As anyone any little idea about that ?
> 
> Thanks by advance
> 
> Jessica
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From e.rapsomaniki at mail.cryst.bbk.ac.uk  Thu Jul 27 17:07:55 2006
From: e.rapsomaniki at mail.cryst.bbk.ac.uk (Eleni Rapsomaniki)
Date: Thu, 27 Jul 2006 16:07:55 +0100
Subject: [R] memory problems when combining randomForests [Broadcast]
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAA76D@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAA76D@usctmx1106.merck.com>
Message-ID: <1154012875.44c8d6cbda927@webmail.cryst.bbk.ac.uk>

I'm using R (windows) version 2.1.1, randomForest version 4.15. 
I call randomForest like this:

my.rf=randomForest(x=train.df[,-response_index], y=train.df[,response_index],
 xtest=test.df[,-response_index], ytest=test.df[,response_index],
 importance=TRUE,proximity=FALSE, keep.forest=TRUE)

 (where train.df and test.df are my train and test data.frames and
 response_index is the column number specifiying the class)

I then save each tree to a file so I can combine them all afterwards. There are
no memory issues when keep.forest=FALSE. But I think that's the bit I need for
future predictions (right?). 

I did check previous messages on memory issues, and thought that
combining the trees afterwards would solve the problem. Since my
cross-validation subsets give me a fairly stable error-rate, I suppose I could
just use a randomForest trained on just a subset of my data. But would I not be
"wasting" data this way?

A bit off the subject, but should the order at which at rows (ie. sets of
explanatory variables) are passed to the randomForest function affect the
result? I have noticed that if I pick a random unordered sample from my control
data for training the error rate is much lower than if I a take an ordered
sample. This remains true for all my cross-validation results. 

I'm sorry for my many questions.
Many Thanks
Eleni Rapsomaniki


From cpujoll at gmail.com  Thu Jul 27 12:37:47 2006
From: cpujoll at gmail.com (claire pujoll)
Date: Thu, 27 Jul 2006 12:37:47 +0200
Subject: [R] Help
Message-ID: <ac12a4de0607270337k5fa9d577k1a1e2fd439ba1132@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060727/a1d60afd/attachment.pl 

From Cyril.Goutte at nrc-cnrc.gc.ca  Thu Jul 27 17:58:14 2006
From: Cyril.Goutte at nrc-cnrc.gc.ca (Cyril Goutte)
Date: Thu, 27 Jul 2006 11:58:14 -0400
Subject: [R] Vector extracted from a matrix. How can I
	specify	dimensions in as.matrix?
In-Reply-To: <BAY112-F36BE8D4802E9C5BF66511BAF580@phx.gbl>
References: <BAY112-F36BE8D4802E9C5BF66511BAF580@phx.gbl>
Message-ID: <1154015894.23341.31.camel@Soyinka>

I think the confusion relates to the fact that in R vectors are not 2D
arrays with one dimension set to 1.

So your 'simplevector' is not a vector, and your 'extractedvector3x1'
and 'extractedvector1x3' are in fact not 3x1 or 1x3, they are 3-element
vectors, which sometimes behave like 1x3 arrays, sometimes not.

I also found that very confusing in the beginning.  You have to either
keep that in mind in your code or use the 'drop' option if that is
convenient.

	Cyril.

On Thu, 2006-27-07 at 09:23 -0400, Neuro LeSuperH?ros wrote:
> Transpose vector extracted from a matrix
> 
> Hello,
> 
> I am doing a recursive analysis that uses every line (vector) of a matrix in 
> a loop. In the model, I need to transpose those vectors that are extracted 
> from a matrix.
> 
> Using simple vectors (no matrix involved) the transpose function works fine:
> 
> simplevector <-matrix(1:3,3,1)
> tsimplevector <-t(simplevector) #transposed
> dim(simplevector) #3x1 matrix (vector)
> dim(tsimplevector)#1x3
> 
> PROBLEM: However, when the vector is extracted from a matrix, its dimension 
> is NULL.  In this case the transposed dimension is correct, but you'll see 
> the next example, that if a row was extracted, the dimension would be wrong:
> 
> initialmatrix <- matrix(1:9,3,3)
> extractedvector <-initialmatrix[,1] # extract first columm as vector
> textractedvector <-t(extractedvector)#transposed
> dim(extractedvector) #NULL!!!! <- not working
> dim(textractedvector)#1x3 as expected
> 
> I have tried to transform the extracted vector as.vector and as.matrix. 
> as.vector does not give the what I want (still NULL) and as.matrix can't 
> specify the number of row and columns so both vectors are vertical.
> 
> In this example, I extract a column and a row. Notice that both 
> as.matrix(vector) have the same dimension. Notice that both transposed 
> vectors have 1x3 dimensions.
> 
> initialmatrix <- matrix(1:9,3,3)
> extractedvector3x1 <-initialmatrix[,1]  # extract first columm as vector
> extractedvector1x3 <-initialmatrix[1,]  # extract first row as vector
> 
> #Both are NULL
> dim(extractedvector3x1) #NULL!!!!
> dim(extractedvector1x3) #NULL!!!!
> 
> #transposed dimensions both 1x3
> dim(t(extractedvector3x1)) #1x3
> dim(t(extractedvector1x3)) #1x3 <- not 3x1
> 
> #as.vector: still NULL
> dim(as.vector(extractedvector3x1)) #NULL!!!!
> dim(as.vector(extractedvector1x3)) #NULL!!!!
> 
> #as.matrix: Both dim at 3x1
> dim(as.matrix(extractedvector3x1)) #3x1
> dim(as.matrix(extractedvector1x3)) #3x1 <- Problem: not 1x3
> 
> #as.matrix transposed: Both dim at 1x3
> dim(as.matrix(t(extractedvector3x1))) #1x3
> dim(as.matrix(t(extractedvector1x3))) #1x3 <- Problem: not 3x1
> 
> How can I get correct dimensions? Is there a way to specify dimensions in 
> as.matrix?
> 
> Neuro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jholtman at gmail.com  Thu Jul 27 18:17:14 2006
From: jholtman at gmail.com (jim holtman)
Date: Thu, 27 Jul 2006 12:17:14 -0400
Subject: [R] inserting rows into a matrix
In-Reply-To: <6469ADFD-C518-47B0-9F3B-CF48EA046A61@soc.soton.ac.uk>
References: <6469ADFD-C518-47B0-9F3B-CF48EA046A61@soc.soton.ac.uk>
Message-ID: <644e1f320607270917y691524afhc2620a59eef233ac@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060727/41d27198/attachment.pl 

From mc_gar_est at yahoo.es  Thu Jul 27 18:25:51 2006
From: mc_gar_est at yahoo.es (Mari Carmen Garcia Esteban)
Date: Thu, 27 Jul 2006 18:25:51 +0200 (CEST)
Subject: [R] replace values in a distance matrix
Message-ID: <20060727162551.41226.qmail@web27511.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060727/8750b083/attachment.pl 

From jholtman at gmail.com  Thu Jul 27 18:31:48 2006
From: jholtman at gmail.com (jim holtman)
Date: Thu, 27 Jul 2006 12:31:48 -0400
Subject: [R] replace values in a distance matrix
In-Reply-To: <20060727162551.41226.qmail@web27511.mail.ukl.yahoo.com>
References: <20060727162551.41226.qmail@web27511.mail.ukl.yahoo.com>
Message-ID: <644e1f320607270931m5ad68b00x1d14eba2af89b37f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060727/2f77fb1e/attachment.pl 

From dd4v135 at yahoo.com  Thu Jul 27 18:44:48 2006
From: dd4v135 at yahoo.com (Dave)
Date: Thu, 27 Jul 2006 09:44:48 -0700 (PDT)
Subject: [R] Moving from Splus to R - advice/opinions request from a
	management perspective
Message-ID: <20060727164448.4067.qmail@web55812.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060727/478bfe41/attachment.pl 

From ligges at statistik.uni-dortmund.de  Thu Jul 27 19:02:59 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 27 Jul 2006 19:02:59 +0200
Subject: [R] Moving from Splus to R - advice/opinions request from
 a	management perspective
In-Reply-To: <20060727164448.4067.qmail@web55812.mail.re3.yahoo.com>
References: <20060727164448.4067.qmail@web55812.mail.re3.yahoo.com>
Message-ID: <44C8F1C3.2070505@statistik.uni-dortmund.de>

Dave wrote:
> Hi,
>    
>   I've looked through the archives and seen several posts discussing technical differences between R and S(plus).  It appears to me that R can likely functionally replace Splus for my situation, but I'm more interested in looking at the risks and benefits of moving from Splus to R from a (project) management point of view.
>    
>   Background (a bit wordy, I'm afraid):
>    
>   - I'm not a stats guy, but rather a project manager responsible for an internal application that utilizes Splus as a back-end analysis engine to analyze manufacturing data at a med/large company.
>    
>   - I really don't know why Splus was chosen as the analysis engine for this app - that choice was made long before I inherited the project.
>    
>   - The developer currently in charge of the Splus code is not a stats guy either, but he is a very talented programmer that has been to one Insightful course and taught himself enough to maintain the existing code (the original authors are long gone).
>    
>   -  While there is quite a large quantity of existing code that is used by our application, I don't believe that it is terribly complex, from an applied statistics point of view.  Using Splus may be over-kill from a functionality standpoint, but we just haven't had the time to re-write in a more "appropriate" package - even if we knew what that more appropriate package might be.
>    
>   -  With the imminent release of Splus 8, I'm feeling uncomfortable with the risk associated with remaining on Splus 2000, which we are currently using.  ***Comments on this point would be appreciated.***
>    
>   -  The developer was able to modify the existing code to run in Splus 7.  Unfortunately, the code is significantly slower than before, and Insightful claims this is due to a change to the S language (at version 6) that was out of their control (and one reason that they bought the S language rights).
>    
>   -  We have found Insightful's telephone support to be rather unresponsive, and not very helpful, other than to recommend their consulting services.  Obtaining consulting services from Insightful to improve performance has proved challenging (don't ask), and if we ever actually do receive any consulting services from them, it will no doubt be quite expensive.
>    
>   (if you are still with me, thank you)
>    
>   I see our realistic options as:
>    
>   A)  Stick with Splus with the assumption that eventually, Insightful will help us migrate our code to the latest release, and performance will be comparable to what it is today.  The advantage of this is working with a known entity, if not one we are very pleased with.  In addition, if we can get the relationship to work, we can hopefully "outsource" future statistics development and support to Insightful.  
>    
>   The disadvantage is that it is costing us quite a bit of $$$ to maintain a relationship that we are not really happy with.  Incidentally, I'm not intending to bash Insightful here - it may just be that we are not a good fit for each other.
>    
>   B)  Drop Splus for R with the assumption that migrating to R and rewriting to improve performance will be little more difficult than rewriting by ourselves in Splus.  The obvious advantage is the cost savings, which is very significant.  In addition, based on the archives I've read, it appears that there is a very responsive and helpful R community.  
>    
>   The disadvantage is that we may be committing to maintaining statistical expertise in-house, whereas we were hoping to be able to outsource some or all of it (to Insightful).  In addition, we will be leaving behind an entity that has a mailing address, phone number, and stock symbol for one that is represented "only" by a mailing list - I'm rather conservative and risk averse.
>    
>   C)  Stick with Splus and either find some consulting help outside of Insightful or slog away internally.  I suppose that we could even end our M&S contract with Insightful and just continue to use Splus, knowing that we will never receive any releases newer than 8.x nor any future support.
>    
>   (almost done)
>    
>   So, I'd like to hear opinions of whether you think that the benefits of moving to R outweigh the risks.  Listing additional benefits and risks that I have not identified would be appreciated.  I'm particularly interested in hearing from anyone that has made this same S -> R transition in a business environment.
>    
>   Also, does anyone have any recommendations for either Splus or R consultants, other than Insightful?  I would prefer someone who is willing to work remotely, rather than anyone having to travel.
>    
>   Thats it.
>    
>   Thanks for your time,
>   Dave

Do you also write anonymous messages to Insightful? No surprise that you 
receive unconvincing replies. Beside beeing impolite, it is also hard to 
recommend consultants in such a case: If you live in New Zealand, it 
certainly is not that helpful to recommend someone in Germany ...

Uwe Ligges


>  __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Thu Jul 27 19:07:20 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 27 Jul 2006 13:07:20 -0400
Subject: [R] Moving from Splus to R - advice/opinions request from a
	management perspective
In-Reply-To: <20060727164448.4067.qmail@web55812.mail.re3.yahoo.com>
References: <20060727164448.4067.qmail@web55812.mail.re3.yahoo.com>
Message-ID: <971536df0607271007r655a57dfn8a95707b56103e06@mail.gmail.com>

How large is your data?

If its large you may or may not have problems.  If its small you  probably
won't.  Try prototyping the most data intensive portion in R before you
commit significant resources.

S Plus can time stamp objects and R cannot although you could come
up with some workarounds for specific objects if your app uses this.

If your app naturally divides into independent sections you could consider
an incremental approach converting one section over at a time.


On 7/27/06, Dave <dd4v135 at yahoo.com> wrote:
> Hi,
>
>  I've looked through the archives and seen several posts discussing technical differences between R and S(plus).  It appears to me that R can likely functionally replace Splus for my situation, but I'm more interested in looking at the risks and benefits of moving from Splus to R from a (project) management point of view.
>
>  Background (a bit wordy, I'm afraid):
>
>  - I'm not a stats guy, but rather a project manager responsible for an internal application that utilizes Splus as a back-end analysis engine to analyze manufacturing data at a med/large company.
>
>  - I really don't know why Splus was chosen as the analysis engine for this app - that choice was made long before I inherited the project.
>
>  - The developer currently in charge of the Splus code is not a stats guy either, but he is a very talented programmer that has been to one Insightful course and taught himself enough to maintain the existing code (the original authors are long gone).
>
>  -  While there is quite a large quantity of existing code that is used by our application, I don't believe that it is terribly complex, from an applied statistics point of view.  Using Splus may be over-kill from a functionality standpoint, but we just haven't had the time to re-write in a more "appropriate" package - even if we knew what that more appropriate package might be.
>
>  -  With the imminent release of Splus 8, I'm feeling uncomfortable with the risk associated with remaining on Splus 2000, which we are currently using.  ***Comments on this point would be appreciated.***
>
>  -  The developer was able to modify the existing code to run in Splus 7.  Unfortunately, the code is significantly slower than before, and Insightful claims this is due to a change to the S language (at version 6) that was out of their control (and one reason that they bought the S language rights).
>
>  -  We have found Insightful's telephone support to be rather unresponsive, and not very helpful, other than to recommend their consulting services.  Obtaining consulting services from Insightful to improve performance has proved challenging (don't ask), and if we ever actually do receive any consulting services from them, it will no doubt be quite expensive.
>
>  (if you are still with me, thank you)
>
>  I see our realistic options as:
>
>  A)  Stick with Splus with the assumption that eventually, Insightful will help us migrate our code to the latest release, and performance will be comparable to what it is today.  The advantage of this is working with a known entity, if not one we are very pleased with.  In addition, if we can get the relationship to work, we can hopefully "outsource" future statistics development and support to Insightful.
>
>  The disadvantage is that it is costing us quite a bit of $$$ to maintain a relationship that we are not really happy with.  Incidentally, I'm not intending to bash Insightful here - it may just be that we are not a good fit for each other.
>
>  B)  Drop Splus for R with the assumption that migrating to R and rewriting to improve performance will be little more difficult than rewriting by ourselves in Splus.  The obvious advantage is the cost savings, which is very significant.  In addition, based on the archives I've read, it appears that there is a very responsive and helpful R community.
>
>  The disadvantage is that we may be committing to maintaining statistical expertise in-house, whereas we were hoping to be able to outsource some or all of it (to Insightful).  In addition, we will be leaving behind an entity that has a mailing address, phone number, and stock symbol for one that is represented "only" by a mailing list - I'm rather conservative and risk averse.
>
>  C)  Stick with Splus and either find some consulting help outside of Insightful or slog away internally.  I suppose that we could even end our M&S contract with Insightful and just continue to use Splus, knowing that we will never receive any releases newer than 8.x nor any future support.
>
>  (almost done)
>
>  So, I'd like to hear opinions of whether you think that the benefits of moving to R outweigh the risks.  Listing additional benefits and risks that I have not identified would be appreciated.  I'm particularly interested in hearing from anyone that has made this same S -> R transition in a business environment.
>
>  Also, does anyone have any recommendations for either Splus or R consultants, other than Insightful?  I would prefer someone who is willing to work remotely, rather than anyone having to travel.
>
>  Thats it.
>
>  Thanks for your time,
>  Dave
>
>  __________________________________________________
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jz7 at duke.edu  Thu Jul 27 19:27:42 2006
From: jz7 at duke.edu (jz7 at duke.edu)
Date: Thu, 27 Jul 2006 13:27:42 -0400 (EDT)
Subject: [R] how to skip certain rows when reading data
In-Reply-To: <Pine.GSO.4.58.0607251454590.8258@godzilla.acpub.duke.edu>
References: <Pine.GSO.4.58.0607122121010.9724@godzilla.acpub.duke.edu>
	<Pine.GSO.4.58.0607251454590.8258@godzilla.acpub.duke.edu>
Message-ID: <Pine.GSO.4.58.0607271324570.16721@godzilla.acpub.duke.edu>

Dear all,

I am reading the data using "read.table". However, there are a few rows I
want to skip. How can I do that in an easy way? Suppose I know the row
number that I want to skip. Thanks so much!


From ggrothendieck at gmail.com  Thu Jul 27 20:23:39 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 27 Jul 2006 14:23:39 -0400
Subject: [R] how to skip certain rows when reading data
In-Reply-To: <Pine.GSO.4.58.0607271324570.16721@godzilla.acpub.duke.edu>
References: <Pine.GSO.4.58.0607122121010.9724@godzilla.acpub.duke.edu>
	<Pine.GSO.4.58.0607251454590.8258@godzilla.acpub.duke.edu>
	<Pine.GSO.4.58.0607271324570.16721@godzilla.acpub.duke.edu>
Message-ID: <971536df0607271123g1ee2d5a4n1b6ed0add2897763@mail.gmail.com>

Just read them in and throw them away:

   read.table("myfile.dat", ...whatever...)[-c(10, 12), ]


On 7/27/06, jz7 at duke.edu <jz7 at duke.edu> wrote:
> Dear all,
>
> I am reading the data using "read.table". However, there are a few rows I
> want to skip. How can I do that in an easy way? Suppose I know the row
> number that I want to skip. Thanks so much!


From andy_liaw at merck.com  Thu Jul 27 20:59:30 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 27 Jul 2006 14:59:30 -0400
Subject: [R] memory problems when combining randomForests
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAA8B5@usctmx1106.merck.com>

From: Eleni Rapsomaniki
> 
> I'm using R (windows) version 2.1.1, randomForest version 4.15. 
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^

Never seen such a version...

> I call randomForest like this:
> 
> my.rf=randomForest(x=train.df[,-response_index], 
> y=train.df[,response_index],  
> xtest=test.df[,-response_index], 
> ytest=test.df[,response_index],  
> importance=TRUE,proximity=FALSE, keep.forest=TRUE)
> 
>  (where train.df and test.df are my train and test 
> data.frames and  response_index is the column number 
> specifiying the class)
> 
> I then save each tree to a file so I can combine them all 
> afterwards. There are no memory issues when 
> keep.forest=FALSE. But I think that's the bit I need for 
> future predictions (right?). 

Yes, but what is your question?  (Do you mean each *forest*,
instead of each *tree*?)
 
> I did check previous messages on memory issues, and thought 
> that combining the trees afterwards would solve the problem. 
> Since my cross-validation subsets give me a fairly stable 
> error-rate, I suppose I could just use a randomForest trained 
> on just a subset of my data. But would I not be "wasting" 
> data this way?

Perhaps, but see Jerry Friedman's ISLE, where he argued
that RF with very small trees grown on small random samples
can give even better results some of the times.
 
> A bit off the subject, but should the order at which at rows 
> (ie. sets of explanatory variables) are passed to the 
> randomForest function affect the result? I have noticed that 
> if I pick a random unordered sample from my control data for 
> training the error rate is much lower than if I a take an 
> ordered sample. This remains true for all my cross-validation 
> results. 

I'm not sure I understand.  In randomForest() (as in other
functions) variables are in columns, rather than rows, so
are you talking about variables (columns) in different order 
or data (rows) in different order?

Andy
 
> I'm sorry for my many questions.
> Many Thanks
> Eleni Rapsomaniki
> 
> 
> 
> 
> ----------------------------------------------------------------
> This message was sent using IMP, the Internet Messaging Program.
> 
>


From dataanalytics at earthlink.net  Thu Jul 27 21:22:42 2006
From: dataanalytics at earthlink.net (Walter R. Paczkowski)
Date: Thu, 27 Jul 2006 15:22:42 -0400
Subject: [R] Non-metric Multidimensional Scaling in R
Message-ID: <7.0.1.0.2.20060727152040.033fb938@earthlink.net>


From jameson at coost.com  Thu Jul 27 21:22:35 2006
From: jameson at coost.com (Jameson C. Burt)
Date: Thu, 27 Jul 2006 15:22:35 -0400
Subject: [R] RandomForest vs. bayes & svm classification performance
In-Reply-To: <1153763971.44c50a838791d@webmail.cryst.bbk.ac.uk>
References: <1153763971.44c50a838791d@webmail.cryst.bbk.ac.uk>
Message-ID: <20060727192234.GA19303@coost.com>

With remiss, I haven't tried these R tools.
However, I tried a dozen Naive Bayes-like programs, often used to filter
email, where the serious problem with spam has resulted in many
innovations.
The most touted of the worldwide Naive Bayes programs seems to be
CRM114 (not in R, I expect, since its programming is peculiar), 
whose 275 pages of documentation is at
http://crm114.sourceforge.net/CRM114_Revealed_20051207.pdf
However, unless you have several weeks and some flexible programming
skills, don't consider it.
It took me about 3 months to find that crm114 worked best,
then another month to break thru his documentation to control 
his program from a single Perl program with no external parameter files.
Crm114 can form groups of 5 words as word word, taking all combinations
of 5 consecutive words in documents.
Using 5 words produced better results than any filters I used; eg,
filtering/altering car manufacturer's standard form prompts like 
   Fire? Yes_ No_

Initially, I expected correct results of 99% or better, 
like my use of Naive Bayes to filter my email. 
However, email must accomplish some goal (go to their webpage or see
their low cost), so Naive Bayes approaches work very well on email.

U.S. Department of Transportation (DOT), defects investigation, contracted with me 
to try what I'd successfully used for email (others' programs).
They were accumulating 50,000 early warning reports a quarter,
yet their engineers had read only 3,000.
DOT contracted for a dozen people to slug thru the accumulated 300,000
reports, identifying those that might portend the necessity of a recall.
But these contractors (probably costing $1 million a year) agreed with 
the engineers no more than 50% of the time.

After 2 months, I was able to correctly identify only 30% of reports.
Then I read that Naive Bayes was, after all, "naive".
It presumed independence between words.
There's an old statitical saying,
  "Do you prefer to perfectly solve the wrong problem,
  or wrongly solve the correct problem?"
People using Naive Bayes use many heuristics, as the CRM114 documents
mention, including,
a. TOE, "Train on Error"
   for which you retrain any document that Naive Bayes classifies
   incorrectly.
   Statistically, this is somewhat like having a learned population
   with more than one of the same document.
b. SSTTT, "Single Sided Thick Threshhold Training"
   for which you retrain a document when it doesn't identify correctly 
   with a sufficiently high probability.
c. TUNE, "Train Until No Error"
   for which you recycle thru your known records until you
   reach perfection, although often forced a stop when no improvement 
   resulted after 12 cycles.
All these techniques improved correct identification and concentration
(proportion of "flagged" reports that are correctly flagged) to about
67%.

Then the engineers (gearheads) did the inexplicable -- they read about
20,000 reports, jumping the correctness of the crm114 Naive Bayes
approach with the above heuristics to about 88%.
Suddenly, crm114 Naive Bayes "flagged" reports were fun to read.
For example, a report no-one had yet identified described a fellow's 
car modified with airbags to lift the car to a high height
using canisters of some air in the back of his pickup.
Driving down the road, he notice a warning light flashing on his air
supply.
Soon afterwards, the passenger seat caught fire.
Even though his pickup was moving down the road,
the flashing warning light and flaming passenger seat
prompted him to open his driver's door and leap from his moving pickup.

While I worked the Bayesian approach and contractors read reports as two
approaches to slug thru 300,000 reports,
big software/contractor companies hovered over the spending and
potential spending.
But their approaches were all judged foolish -- expensively foolish.

So, if you really have a problem worthy of solving well, 
some time, and some programming skills,
you can integrate a Naive Bayes procedure with some heuristic
procedures, probably with good correct identification 
and a high concentration of correctly "flagged"
documents among Bayes flagged documents.


On Mon, Jul 24, 2006 at 06:59:31PM +0100, Eleni Rapsomaniki wrote:
> Hi
> 
> This is a question regarding classification performance using different methods.
> So far I've tried NaiveBayes (klaR package), svm (e1071) package and
> randomForest (randomForest). What has puzzled me is that randomForest seems to
> perform far better (32% classification error) than svm and NaiveBayes, which
> have similar classification errors (45%, 48% respectively). A similar
> difference in performance is observed with different combinations of
> parameters, priors and size of training data. 
> 
> Because I was expecting to see little difference in the perfomance of these
> methods I am worried that I may have made a mistake in my randomForest call: 
> 
> my.rf=randomForest(x=train.df[,-response_index], y=train.df[,response_index],
> xtest=test.df[,-response_index], ytest=test.df[,response_index],
> importance=TRUE,proximity=FALSE, keep.forest=FALSE)
> 
> (where train.df and test.df are my train and test data.frames and response_index
> is the column number specifiying the class)
> 
> My main question is: could there be a legitimate reason why random forest would
> outperform the other two models (e.g. maybe one
> method is more reliable with Gaussian data, handles categorical data
> better etc)? Also, is there a way of evaluating the predictive ability of each
> parameter in the bayesian model as it can be done for random Forests (through
> the importance table)? 
> 
> I would appreciate any of your comments and suggestions on these.
> 
> Many thanks
> Eleni Rapsomaniki
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Jameson C. Burt, NJ9L   Fairfax, Virginia, USA
jameson at coost.com       http://www.coost.com
(202) 690-0380 (work)

LTSP.org:  magic "mysterious and awe-inspiring even though
                  we know they are real and not supernatural"


From w.paczkowski at att.net  Thu Jul 27 21:26:21 2006
From: w.paczkowski at att.net (w.paczkowski at att.net)
Date: Thu, 27 Jul 2006 19:26:21 +0000
Subject: [R] Non-metric Multidimensional Scaling in R
Message-ID: <072720061926.25659.44C9135D000A119D0000643B216028074807059C990105960C0E9FD299@att.net>

Hi,

I don't think my first email worked, so here it is again.

I'd like to know if there is a nonmetric MDS function or package for R.  I've beening trying to do a problem in Stata but I'm going crazy with it.  A simple problem is becoming difficult so maybe R can help.  Any suggestions?

Walt Paczkowski


From whit.armstrong at hcmny.com  Thu Jul 27 21:27:01 2006
From: whit.armstrong at hcmny.com (Armstrong, Whit)
Date: Thu, 27 Jul 2006 15:27:01 -0400
Subject: [R] FW:  RODBC on linux
Message-ID: <E58BE6136618CF4C964F6EC7773AE5699FA5F7@ex4.nyc.hcmny.com>

Thanks again for everyone's help.  I have a successful configuration.

I used the "ODBC-combined" configuration which is documented here:
http://freetds.org/userguide/odbcombo.htm

here are the config files residing in my home dir:

-bash-2.05b$ cat .freetds.conf
[global]
        tds version = 7.0

[QAI]
        host = Mqa3.xxx.yyy.com
        port = 1433
-bash-2.05b$ 


-bash-2.05b$ cat .odbc.ini
[ODBC Data Sources]
 QAIdsn = QAI database

[QAIdsn]
Driver  = /usr/local/freetds-0.64/lib/libtdsodbc.so
Description     = QAI database
Trace           = No
Servername      = QAI
Database        = qai

[Default]
Driver  = /usr/local/freetds-0.64/lib/libtdsodbc.so

-bash-2.05b$ 


Cheers,
Whit




This e-mail message is intended only for the named recipient(s) above. It may contain confidential information. If you are not the intended recipient you are hereby notified that any dissemination, distribution or copying of this e-mail and any attachment(s) is strictly prohibited. If you have received this e-mail in error, please immediately notify the sender by replying to this e-mail and delete the message and any attachment(s) from your system. Thank you.


From bkoch at decisiondevelopment.com  Thu Jul 27 21:53:15 2006
From: bkoch at decisiondevelopment.com (Brian Koch)
Date: Thu, 27 Jul 2006 14:53:15 -0500
Subject: [R] Moving from Splus to R - advice/opinions request from
	amanagement perspective
Message-ID: <2D1C2FB3D4E72D40B1BA2230075ADC9346BED2@DDISERVER.decisiondevelopment.com>

 
Hi Dave.  I'll try to offer some feedback from a fellow non-stats guy
working in a business setting.  I've not directly transitioned from S+
to R, but I've previously hit roadblocks with SAS and SPSS that
compelled me to investigate the S/R languages.

I think that as you read messages on this list, you'll find that most
users don't implement R as a back end to a chain of processes: Many of
the community members are importing data into R via an ASCII or SQL
connector (R has good base functions and/or packages for both), writing
custom code for their analyses, and using the R output to [manually]
compile a report or similar deliverable.

So my biggest concern for the migration is the import/export.  Given
your evaluation of your statistics needs such that S+ might be
"over-kill" - and because your organization seems to be at a crossroads
for this information system, maybe take a good look at what is serving
S+ its data.  If it's something to the tune of SAP, MySQL, or Oracle,
then that application might have enough mathematical/statistical/data
management features to accomplish your objectives -- not to mention that
all of these applications are extensively supported by their
manufacturers (for a price, of course - hopefully competitive with
Insightful and more customer-responsive).  If you're planning to
outsource your statistics expertise, then definitely consider this
option.

The biggest differences between S+ and R deal with data storage and
variable scoping -- and this impacts import/export especially.
Transitioning your code from S+ to R will be the most intensive when
routines based on these differing features need to be re-engineered.

Another consideration for management is how you might use R packages.
There are a large number of them available, but as you'll see in reading
this list's messages, the quality varies.  I've found the R base
software to be rock-solid, and I trust its accuracy.  But I've been
reluctant to deploy packages whose workings are beyond my expertise
(i.e., a package that I couldn't debug).  If you need capabilities
outside of the base functions, then take a good hard look at which
packages are out there (CRAN is where you find R packages - link to it
from the R main site) and examine their authors and the degree stability
and/or active maintenance.  If the package was written by a member of
the R Development Core Team, or by the author of a prominent book on R,
it's more likely that package is as reliable as the base.

I can attest that maintaining custom, from-scratch R analytics as a
non-stats-guy is a challenging but rewarding activity.  It forces you to
understand the structure of your data and the interconnection of your
analysis processes.  So in the end, it might come down to preference.
If your job is to ensure the reliability of this system and vouch for
its accuracy, then perhaps R might pull you from your other
responsibilities.  However, if your job is to grow this system with your
organization's needs, then investing in R is a great choice: When you
learn R, R returns the favor and enlightens you about the data under
your care.

And don't forget that you can always just "try" R.  It might take some
bravery, but if you can take a dump/export/snapshot of some data and
process it via R, you'll learn quickly whether the transition would
require minor updates vs. major overhauls.  And here the good folks of
this list could probably help -- if you can provide a representation of
your data and a description of what operations are involved (or the
relevant S+ code, better yet), I think you'll find somebody with the
willingness to help and the expertise to advise.

There's a curious number of Fortune 500 domain names that appear in the
addresses of this list's regulars... I wonder how many of them got their
start by "trying" R when at a juncture similar to your own.

Finally, to answer your last question about consultants:  You might try
guru.com to look for freelance talent.  Also, if you can define your
project such that someone could bid, you could post an RFP to this list
(or send a general query for contract help at an hourly rate).  There
have also been postings to the list from professional training firms
that run workshops -- perhaps they could provide a customized solution
for your business.


HTH,

-Brian J. Koch
Data Manager
Decision Development Inc
(A qualitative marketing research firm.  Evanston, IL)


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dave
Sent: Thursday, July 27, 2006 11:45 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Moving from Splus to R - advice/opinions request from
amanagement perspective

Hi,
   
  I've looked through the archives and seen several posts discussing
technical differences between R and S(plus).  It appears to me that R
can likely functionally replace Splus for my situation, but I'm more
interested in looking at the risks and benefits of moving from Splus to
R from a (project) management point of view.
   
  Background (a bit wordy, I'm afraid):
   
  - I'm not a stats guy, but rather a project manager responsible for an
internal application that utilizes Splus as a back-end analysis engine
to analyze manufacturing data at a med/large company.
   
  - I really don't know why Splus was chosen as the analysis engine for
this app - that choice was made long before I inherited the project.
   
  - The developer currently in charge of the Splus code is not a stats
guy either, but he is a very talented programmer that has been to one
Insightful course and taught himself enough to maintain the existing
code (the original authors are long gone).
   
  -  While there is quite a large quantity of existing code that is used
by our application, I don't believe that it is terribly complex, from an
applied statistics point of view.  Using Splus may be over-kill from a
functionality standpoint, but we just haven't had the time to re-write
in a more "appropriate" package - even if we knew what that more
appropriate package might be.
   
  -  With the imminent release of Splus 8, I'm feeling uncomfortable
with the risk associated with remaining on Splus 2000, which we are
currently using.  ***Comments on this point would be appreciated.***
   
  -  The developer was able to modify the existing code to run in Splus
7.  Unfortunately, the code is significantly slower than before, and
Insightful claims this is due to a change to the S language (at version
6) that was out of their control (and one reason that they bought the S
language rights).
   
  -  We have found Insightful's telephone support to be rather
unresponsive, and not very helpful, other than to recommend their
consulting services.  Obtaining consulting services from Insightful to
improve performance has proved challenging (don't ask), and if we ever
actually do receive any consulting services from them, it will no doubt
be quite expensive.
   
  (if you are still with me, thank you)
   
  I see our realistic options as:
   
  A)  Stick with Splus with the assumption that eventually, Insightful
will help us migrate our code to the latest release, and performance
will be comparable to what it is today.  The advantage of this is
working with a known entity, if not one we are very pleased with.  In
addition, if we can get the relationship to work, we can hopefully
"outsource" future statistics development and support to Insightful.  
   
  The disadvantage is that it is costing us quite a bit of $$$ to
maintain a relationship that we are not really happy with.
Incidentally, I'm not intending to bash Insightful here - it may just be
that we are not a good fit for each other.
   
  B)  Drop Splus for R with the assumption that migrating to R and
rewriting to improve performance will be little more difficult than
rewriting by ourselves in Splus.  The obvious advantage is the cost
savings, which is very significant.  In addition, based on the archives
I've read, it appears that there is a very responsive and helpful R
community.  
   
  The disadvantage is that we may be committing to maintaining
statistical expertise in-house, whereas we were hoping to be able to
outsource some or all of it (to Insightful).  In addition, we will be
leaving behind an entity that has a mailing address, phone number, and
stock symbol for one that is represented "only" by a mailing list - I'm
rather conservative and risk averse.
   
  C)  Stick with Splus and either find some consulting help outside of
Insightful or slog away internally.  I suppose that we could even end
our M&S contract with Insightful and just continue to use Splus, knowing
that we will never receive any releases newer than 8.x nor any future
support.
   
  (almost done)
   
  So, I'd like to hear opinions of whether you think that the benefits
of moving to R outweigh the risks.  Listing additional benefits and
risks that I have not identified would be appreciated.  I'm particularly
interested in hearing from anyone that has made this same S -> R
transition in a business environment.
   
  Also, does anyone have any recommendations for either Splus or R
consultants, other than Insightful?  I would prefer someone who is
willing to work remotely, rather than anyone having to travel.
   
  Thats it.
   
  Thanks for your time,
  Dave

 __________________________________________________



	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From tobias.verbeke at telenet.be  Thu Jul 27 21:56:15 2006
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Thu, 27 Jul 2006 21:56:15 +0200
Subject: [R] Non-metric Multidimensional Scaling in R
In-Reply-To: <072720061926.25659.44C9135D000A119D0000643B216028074807059C990105960C0E9FD299@att.net>
References: <072720061926.25659.44C9135D000A119D0000643B216028074807059C990105960C0E9FD299@att.net>
Message-ID: <44C91A5F.4040109@telenet.be>

w.paczkowski at att.net wrote:
> Hi,
>
> I don't think my first email worked, so here it is again.
>
> I'd like to know if there is a nonmetric MDS function or package for R.  I've beening trying to do a problem in Stata but I'm going crazy with it.  A simple problem is becoming difficult so maybe R can help.  Any suggestions?
>   
library(MASS)
?isoMDS

HTH,
Tobias
> Walt Paczkowski
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From tracysfeldman at yahoo.com  Fri Jul 28 04:00:20 2006
From: tracysfeldman at yahoo.com (Tracy Feldman)
Date: Thu, 27 Jul 2006 19:00:20 -0700 (PDT)
Subject: [R] negative binomial lmer
Message-ID: <20060728020020.33116.qmail@web37508.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060727/ae76b57b/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Jul 28 08:40:21 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 28 Jul 2006 07:40:21 +0100 (BST)
Subject: [R] how to skip certain rows when reading data
In-Reply-To: <Pine.GSO.4.58.0607271324570.16721@godzilla.acpub.duke.edu>
References: <Pine.GSO.4.58.0607122121010.9724@godzilla.acpub.duke.edu>
	<Pine.GSO.4.58.0607251454590.8258@godzilla.acpub.duke.edu>
	<Pine.GSO.4.58.0607271324570.16721@godzilla.acpub.duke.edu>
Message-ID: <Pine.LNX.4.64.0607280738380.21917@gannet.stats.ox.ac.uk>

On Thu, 27 Jul 2006, jz7 at duke.edu wrote:

> Dear all,
> 
> I am reading the data using "read.table". However, there are a few rows I
> want to skip. How can I do that in an easy way? Suppose I know the row
> number that I want to skip. Thanks so much!

The easy way is to read the whole data frame and using indexing (see `An 
Introduction to R') to remove the rows you do not want to retain.
E.g. to remove rows 17 and 137

mydf <- read.table(...)[-c(17, 137), ]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From kartik.pappu at gmail.com  Fri Jul 28 08:44:04 2006
From: kartik.pappu at gmail.com (Kartik Pappu)
Date: Thu, 27 Jul 2006 23:44:04 -0700
Subject: [R] Help with matrix manipulation
Message-ID: <97faa3210607272344x7ceaf76dr816b229a83840701@mail.gmail.com>

Hi all,

I have a square (a x a) matrix with values in a range. For example:

      [,1] [,2] [,3] [,4]
[1,]   -5   -1    3   -4
[2,]   -4    0    4   -3
[3,]   -3    1    5   -2
[4,]   -2    2   -5   -1

I want to take any number smaller than -4 (in this example -5) and
replace it with -4 and similarly take any number greater than 3 (in
this case 4 and 5) and replace it with 3. The other numbers (and the
overall structure of the matrix should remain unchanged.

Seems like something that would use an "if a<b then c else d" kind of
logic, but I cannot figure out how to manipulate the entire matrix.

Thanks much

Kartik


From r.hankin at noc.soton.ac.uk  Fri Jul 28 09:02:54 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 28 Jul 2006 08:02:54 +0100
Subject: [R] Help with matrix manipulation
In-Reply-To: <97faa3210607272344x7ceaf76dr816b229a83840701@mail.gmail.com>
References: <97faa3210607272344x7ceaf76dr816b229a83840701@mail.gmail.com>
Message-ID: <D5372D22-080D-42C1-950C-F261FA6211C0@soc.soton.ac.uk>

Hi

two solutions, your best option depends on the level
of generality you need.


(i)


M[M< -4] <- -4
M[M > 3] <- 3


(ii)

M <- pmax(pmin(M,3),-4)




HTH

rksh


On 28 Jul 2006, at 07:44, Kartik Pappu wrote:

> Hi all,
>
> I have a square (a x a) matrix with values in a range. For example:
>
>       [,1] [,2] [,3] [,4]
> [1,]   -5   -1    3   -4
> [2,]   -4    0    4   -3
> [3,]   -3    1    5   -2
> [4,]   -2    2   -5   -1
>
> I want to take any number smaller than -4 (in this example -5) and
> replace it with -4 and similarly take any number greater than 3 (in
> this case 4 and 5) and replace it with 3. The other numbers (and the
> overall structure of the matrix should remain unchanged.
>
> Seems like something that would use an "if a<b then c else d" kind of
> logic, but I cannot figure out how to manipulate the entire matrix.
>
> Thanks much
>
> Kartik
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From blomsp at ozemail.com.au  Fri Jul 28 09:14:37 2006
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Fri, 28 Jul 2006 17:14:37 +1000
Subject: [R] Help with matrix manipulation
In-Reply-To: <97faa3210607272344x7ceaf76dr816b229a83840701@mail.gmail.com>
References: <97faa3210607272344x7ceaf76dr816b229a83840701@mail.gmail.com>
Message-ID: <44C9B95D.9050406@ozemail.com.au>

apply is your friend:

fn <- function (x, a, b) {
    if (x < a) return(a) else
    if (x > b) return(b) else
    x
    }

apply(mat, c(1,2), fn, -4, 3)

     [,1] [,2] [,3] [,4]
[1,]   -4   -1    3   -4
[2,]   -4    0    3   -3
[3,]   -3    1    3   -2
[4,]   -2    2   -4   -1

HTH,

Simon.

Kartik Pappu wrote:
> Hi all,
>
> I have a square (a x a) matrix with values in a range. For example:
>
>       [,1] [,2] [,3] [,4]
> [1,]   -5   -1    3   -4
> [2,]   -4    0    4   -3
> [3,]   -3    1    5   -2
> [4,]   -2    2   -5   -1
>
> I want to take any number smaller than -4 (in this example -5) and
> replace it with -4 and similarly take any number greater than 3 (in
> this case 4 and 5) and replace it with 3. The other numbers (and the
> overall structure of the matrix should remain unchanged.
>
> Seems like something that would use an "if a<b then c else d" kind of
> logic, but I cannot figure out how to manipulate the entire matrix.
>
> Thanks much
>
> Kartik
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>   
-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C


From niederlein-rstat at yahoo.de  Fri Jul 28 09:50:49 2006
From: niederlein-rstat at yahoo.de (niederlein-rstat at yahoo.de)
Date: Fri, 28 Jul 2006 09:50:49 +0200 (CEST)
Subject: [R] mirror vector?
Message-ID: <20060728075049.61819.qmail@web27111.mail.ukl.yahoo.com>

Ein eingebundener Text mit undefiniertem Zeichensatz wurde abgetrennt.
Name: nicht verf?gbar
URL: https://stat.ethz.ch/pipermail/r-help/attachments/20060728/3f67aa27/attachment.pl 

From jacques.veslot at good.ibl.fr  Fri Jul 28 09:56:11 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Fri, 28 Jul 2006 09:56:11 +0200
Subject: [R] mirror vector?
In-Reply-To: <20060728075049.61819.qmail@web27111.mail.ukl.yahoo.com>
References: <20060728075049.61819.qmail@web27111.mail.ukl.yahoo.com>
Message-ID: <44C9C31B.2020300@good.ibl.fr>

 > mat <- matrix(1:16,4,4)
 > mat
      [,1] [,2] [,3] [,4]
[1,]    1    5    9   13
[2,]    2    6   10   14
[3,]    3    7   11   15
[4,]    4    8   12   16
 > apply(mat,2,rev)
      [,1] [,2] [,3] [,4]
[1,]    4    8   12   16
[2,]    3    7   11   15
[3,]    2    6   10   14
[4,]    1    5    9   13

-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


niederlein-rstat at yahoo.de a ?crit :
> Hello,
> 
> I'm an absolut beginner with R and now I got a 2D vector with numbers. I would like to mirror this vector now by the rows (so that the first row becomes last, second becomes one before last, ...).
> I don't know if there is any method I can use to do this.
> Could you please help me?
> 
> Antje
> 
>  		
> ---------------------------------
> Was Sie schon immer wissen wollten aber nie zu Fragen trauten? Yahoo! Clever hilft Ihnen.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From r.hankin at noc.soton.ac.uk  Fri Jul 28 10:00:09 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 28 Jul 2006 09:00:09 +0100
Subject: [R] mirror vector?
In-Reply-To: <20060728075049.61819.qmail@web27111.mail.ukl.yahoo.com>
References: <20060728075049.61819.qmail@web27111.mail.ukl.yahoo.com>
Message-ID: <41F1CEA3-F149-48F7-9DBB-A54F62902D54@soc.soton.ac.uk>

Hi Antje


use the fact that n:1 counts backwards from n, and then use this as
a row index:

 > m <- matrix(1:30,5,6)
 > m[nrow(m):1,]
      [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    5   10   15   20   25   30
[2,]    4    9   14   19   24   29
[3,]    3    8   13   18   23   28
[4,]    2    7   12   17   22   27
[5,]    1    6   11   16   21   26


[
a more general answer would be

library(magic)
arev(m,1)
]



HTH

rksh



On 28 Jul 2006, at 08:50, <niederlein-rstat at yahoo.de> wrote:

> Hello,
>
> I'm an absolut beginner with R and now I got a 2D vector with  
> numbers. I would like to mirror this vector now by the rows (so  
> that the first row becomes last, second becomes one before last, ...).
> I don't know if there is any method I can use to do this.
> Could you please help me?
>
> Antje
>
>  		
> ---------------------------------
> Was Sie schon immer wissen wollten aber nie zu Fragen trauten?  
> Yahoo! Clever hilft Ihnen.
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From r.hankin at noc.soton.ac.uk  Fri Jul 28 10:05:07 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 28 Jul 2006 09:05:07 +0100
Subject: [R] mirror vector?
In-Reply-To: <44C9C31B.2020300@good.ibl.fr>
References: <20060728075049.61819.qmail@web27111.mail.ukl.yahoo.com>
	<44C9C31B.2020300@good.ibl.fr>
Message-ID: <ECF2BB1C-EE63-48BD-8AA9-3FA7A511BC2F@soc.soton.ac.uk>

Hi

Not that there's anything wrong with Jacques's answer, but
the List might be interested in the following gotcha:



 > m <- matrix(1:30,5,6)
 > apply(m,2,rev)
      [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    5   10   15   20   25   30
[2,]    4    9   14   19   24   29
[3,]    3    8   13   18   23   28
[4,]    2    7   12   17   22   27
[5,]    1    6   11   16   21   26
 > apply(m,1,rev)
      [,1] [,2] [,3] [,4] [,5]
[1,]   26   27   28   29   30
[2,]   21   22   23   24   25
[3,]   16   17   18   19   20
[4,]   11   12   13   14   15
[5,]    6    7    8    9   10
[6,]    1    2    3    4    5
 >

See how the first usage of apply() works as expected (at least for  
me ;-)
but the second returns the transpose of what one might need.
That's why I wrote arev().


comments anyone?





On 28 Jul 2006, at 08:56, Jacques VESLOT wrote:

>> mat <- matrix(1:16,4,4)
>> mat
>       [,1] [,2] [,3] [,4]
> [1,]    1    5    9   13
> [2,]    2    6   10   14
> [3,]    3    7   11   15
> [4,]    4    8   12   16
>> apply(mat,2,rev)
>       [,1] [,2] [,3] [,4]
> [1,]    4    8   12   16
> [2,]    3    7   11   15
> [3,]    2    6   10   14
> [4,]    1    5    9   13
>
> -------------------------------------------------------------------
> Jacques VESLOT
>
> CNRS UMR 8090
> I.B.L (2?me ?tage)
> 1 rue du Professeur Calmette
> B.P. 245
> 59019 Lille Cedex
>
> Tel : 33 (0)3.20.87.10.44
> Fax : 33 (0)3.20.87.10.31
>
> http://www-good.ibl.fr
> -------------------------------------------------------------------
>
>
> niederlein-rstat at yahoo.de a ?crit :
>> Hello,
>>
>> I'm an absolut beginner with R and now I got a 2D vector with  
>> numbers. I would like to mirror this vector now by the rows (so  
>> that the first row becomes last, second becomes one before  
>> last, ...).
>> I don't know if there is any method I can use to do this.
>> Could you please help me?
>>
>> Antje
>>
>>  		
>> ---------------------------------
>> Was Sie schon immer wissen wollten aber nie zu Fragen trauten?  
>> Yahoo! Clever hilft Ihnen.
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From ligges at statistik.uni-dortmund.de  Fri Jul 28 10:25:16 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 28 Jul 2006 10:25:16 +0200
Subject: [R] mirror vector?
In-Reply-To: <44C9C31B.2020300@good.ibl.fr>
References: <20060728075049.61819.qmail@web27111.mail.ukl.yahoo.com>
	<44C9C31B.2020300@good.ibl.fr>
Message-ID: <44C9C9EC.20200@statistik.uni-dortmund.de>

Jacques VESLOT wrote:
>  > mat <- matrix(1:16,4,4)
>  > mat
>       [,1] [,2] [,3] [,4]
> [1,]    1    5    9   13
> [2,]    2    6   10   14
> [3,]    3    7   11   15
> [4,]    4    8   12   16
>  > apply(mat,2,rev)
>       [,1] [,2] [,3] [,4]
> [1,]    4    8   12   16
> [2,]    3    7   11   15
> [3,]    2    6   10   14
> [4,]    1    5    9   13


or a lot faster:

mat[nrow(mat):1, ]

Uwe Ligges




> -------------------------------------------------------------------
> Jacques VESLOT
> 
> CNRS UMR 8090
> I.B.L (2?me ?tage)
> 1 rue du Professeur Calmette
> B.P. 245
> 59019 Lille Cedex
> 
> Tel : 33 (0)3.20.87.10.44
> Fax : 33 (0)3.20.87.10.31
> 
> http://www-good.ibl.fr
> -------------------------------------------------------------------
> 
> 
> niederlein-rstat at yahoo.de a ?crit :
>> Hello,
>>
>> I'm an absolut beginner with R and now I got a 2D vector with numbers. I would like to mirror this vector now by the rows (so that the first row becomes last, second becomes one before last, ...).
>> I don't know if there is any method I can use to do this.
>> Could you please help me?
>>
>> Antje
>>
>>  		
>> ---------------------------------
>> Was Sie schon immer wissen wollten aber nie zu Fragen trauten? Yahoo! Clever hilft Ihnen.
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ronggui.huang at gmail.com  Fri Jul 28 10:41:22 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Fri, 28 Jul 2006 16:41:22 +0800
Subject: [R] negative binomial lmer
In-Reply-To: <20060728020020.33116.qmail@web37508.mail.mud.yahoo.com>
References: <20060728020020.33116.qmail@web37508.mail.mud.yahoo.com>
Message-ID: <38b9f0350607280141v1671850bv8615355c6364d8a3@mail.gmail.com>

I think you should use glmm.admb.
>library(glmmADMB)
>?glmm.admb

glmm.admb              package:glmmADMB              R Documentation

Generalized Linear Mixed Models using AD Model Builder

Description:

     Fits mixed-effects models to count data using Binomial, Poisson or
     negative binomial response distributions. Zero-inflated versions
     of  Poisson and negative binomial distributions are available.



2006/7/28, Tracy Feldman <tracysfeldman at yahoo.com>:
> To whom it may concern:
>
>   I have a question about how to appropriately conduct an lmer analysis for negative binomially distributed data.  I am using R 2.2.1 on a windows machine.
>
>   I am trying to conduct an analysis using lmer (for non-normally distributed data and both random and fixed effects) for negative binomially distributed data.  To do this, I have been using maximum likelihood, comparing the full model to reduced models (containing all but one effect, for all effects).  However, for negative binomially distributed data, I need to estimate the parameter theta.  I have been doing this by using a negative binomial glm of the same model (except that all the effects are fixed), and estimating mu as the fitted model like so:
>
>   model_1 <-glm.nb(y~x1+x2+x3, data = datafilename)
>   mu_1 <- fitted(model_1)
>   theta_1 <- theta.ml(y, mu_1, length(data), limit = 10, eps  = .Machine$double.eps^0.25, trace = FALSE)
>
>   Then, I conduct the lmer, using the estimated theta:
>
>   model_11 <-lmer(y~x1+x2+(1|x3), family = negative.binomial(theta = theta_1, link = "log"), method = "Laplace")
>
>   First, I wondered if this sounds like a reasonable method to accomplish my goals.
>
>   Second, I wondered if the theta I use for reduced models (nested within model_11) should be estimated using a glm.nb with the same combination of variables.  For example, should a glm.nb with x1 and x3 only be used to estimate theta for an lmer using x1 and x3?
>
>   Third, I wish to test for random effects of one categorical variable with 122 categories (effects of individual).  For this variable, the glm.nb (for estimating theta) does not work--it gives this error message:
>   Error in get(ctr, mode = "function", envir = parent.frame())(levels(x),  :
>         orthogonal polynomials cannot be represented accurately enough for 122 degrees of freedom
>   Is there any way that will allow me to accurately estimate theta using this particular variable (or without it)?  Or should I be using a Poisson distribution (lognormal?) instead, given these difficulties?
>
>   If anyone has advice on how to properly conduct this test (or any references that might tell me in a clear way), I would be very grateful.  Also, please let me know if I should provide additional information to make my question clearer.
>
>   Please respond to me directly, as I am not subscribed to this list.
>
>   Thank you very much,
>
>   Tracy S. Feldman
>
>   Postdoctoral Associate, the Noble Foundation, Ardmore, OK.
>
>  __________________________________________________
>
>
>
>         [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


-- 
??????
Department of Sociology
Fudan University


From e.rapsomaniki at mail.cryst.bbk.ac.uk  Fri Jul 28 10:44:20 2006
From: e.rapsomaniki at mail.cryst.bbk.ac.uk (Eleni Rapsomaniki)
Date: Fri, 28 Jul 2006 09:44:20 +0100
Subject: [R] memory problems when combining randomForests
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAA8B5@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAA8B5@usctmx1106.merck.com>
Message-ID: <1154076260.44c9ce64a26d5@webmail.cryst.bbk.ac.uk>

Hi Andy, 

> > I'm using R (windows) version 2.1.1, randomForest version 4.15. 
>                                        ^^^^^^^^^^^^^^^^^^^^^^^^^ 
> Never seen such a version...
Ooops! I meant 4.5-15
 
> > I then save each tree to a file so I can combine them all 
> > afterwards. There are no memory issues when 
> > keep.forest=FALSE. But I think that's the bit I need for 
> > future predictions (right?). 
> 
> Yes, but what is your question?  (Do you mean each *forest*,
> instead of each *tree*?)
I mean the component of the object that is created from randomForest that has
the name "forest" (and takes up all the memory!). 

> > A bit off the subject, but should the order at which at rows 
> > (ie. sets of explanatory variables) are passed to the 
> > randomForest function affect the result? I have noticed that 
> > if I pick a random unordered sample from my control data for 
> > training the error rate is much lower than if I a take an 
> > ordered sample. This remains true for all my cross-validation 
> > results. 
> 
> I'm not sure I understand.  In randomForest() (as in other
> functions) variables are in columns, rather than rows, so
> are you talking about variables (columns) in different order 
> or data (rows) in different order?

Yes, sorry I confused you. I mean the order at which data (rows) is passed, not
columns.

Finally, I see from
http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#inter

that there is a component in Breiman's implementation of randomForest that
computes interactions between parameters. Has this been implemented in R yet?

Many thanks for your time and help.
Eleni Rapsomaniki


From pburns at pburns.seanet.com  Fri Jul 28 11:12:06 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 28 Jul 2006 10:12:06 +0100
Subject: [R] mirror vector?
In-Reply-To: <44C9C31B.2020300@good.ibl.fr>
References: <20060728075049.61819.qmail@web27111.mail.ukl.yahoo.com>
	<44C9C31B.2020300@good.ibl.fr>
Message-ID: <44C9D4E6.2070805@pburns.seanet.com>

Another approach is:

mat[nrow(mat):1, ]

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Jacques VESLOT wrote:

> > mat <- matrix(1:16,4,4)
> > mat
>      [,1] [,2] [,3] [,4]
>[1,]    1    5    9   13
>[2,]    2    6   10   14
>[3,]    3    7   11   15
>[4,]    4    8   12   16
> > apply(mat,2,rev)
>      [,1] [,2] [,3] [,4]
>[1,]    4    8   12   16
>[2,]    3    7   11   15
>[3,]    2    6   10   14
>[4,]    1    5    9   13
>
>-------------------------------------------------------------------
>Jacques VESLOT
>
>CNRS UMR 8090
>I.B.L (2?me ?tage)
>1 rue du Professeur Calmette
>B.P. 245
>59019 Lille Cedex
>
>Tel : 33 (0)3.20.87.10.44
>Fax : 33 (0)3.20.87.10.31
>
>http://www-good.ibl.fr
>-------------------------------------------------------------------
>
>
>niederlein-rstat at yahoo.de a ?crit :
>  
>
>>Hello,
>>
>>I'm an absolut beginner with R and now I got a 2D vector with numbers. I would like to mirror this vector now by the rows (so that the first row becomes last, second becomes one before last, ...).
>>I don't know if there is any method I can use to do this.
>>Could you please help me?
>>
>>Antje
>>
>> 		
>>---------------------------------
>>Was Sie schon immer wissen wollten aber nie zu Fragen trauten? Yahoo! Clever hilft Ihnen.
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From buser at stat.math.ethz.ch  Fri Jul 28 11:13:00 2006
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Fri, 28 Jul 2006 11:13:00 +0200
Subject: [R] Help
In-Reply-To: <ac12a4de0607270337k5fa9d577k1a1e2fd439ba1132@mail.gmail.com>
References: <ac12a4de0607270337k5fa9d577k1a1e2fd439ba1132@mail.gmail.com>
Message-ID: <17609.54556.224587.385278@stat.math.ethz.ch>

Dear Claire

Thank you for providing an example, but it is not totally clear
to me how you want to transform your data. For example in row 1,
you have value 1 in column f and n, but the results after
transformation are not similar "A G" and "A A". Similar problem
for column h and i. They transform both to "G G" although the
original values 0,2 differ?

Furthermore it is not clear if you want to have a vector with
two elements as result of the transformation ->  "A"  "G" 
or one character "A G".
In the first case you cannot work with data.frames or matrices,
but you must use a list. An entry of a data.frame is always a
single element.

Regarding changing elements if they fulfil a condition, it is
not necessary to work with loops. E.g. for a vector

v[v==5] <- 7

changes all values of v which are 5 to 7. 
Please read "An Introduction to R" on the R home page about
indexing vectors, matrices, and so on.

The following code can show you an example how I understood your
problem and one possible solution. I hope it helps you.

## Create data.frame
DF <- data.frame(a = c(14,58), b = c(24,42), c = c("rcvf", "grde"),
                 d = c("AG","AC"), e = c(5,2), f = c(2,5), g = c(1,0),
                 h = c(0,5), i = c(2,1), n = c(1,0))
## work with matrix to avoid problems with factors
DF <- as.matrix(DF)
## transformation function
myfun <- function(v)
{
  v[v=="5"] <- "00"
  v[v=="1"] <- v["d"]
  v[v=="0"] <- paste(rep(substr(v["d"],1,1), 2), collapse = "")
  v[v=="2"] <- paste(rep(substr(v["d"],2,2), 2), collapse = "")
  v
}
## apply transformation function to all rows
result <- t(apply(DF, 1, myfun))


One final point. Providing examples is a good way to obtain fast
help from others. Could you please try to give some R-code in
the example as well to ease the handling for others, e.g. if you
provide an example data.frame, please include the code to
produce it:

## Create data.frame
DF <- data.frame(a = c(14,58), b = c(24,42), c = c("rcvf", "grde"),
                 d = c("AG","AC"), e = c(5,2), f = c(2,5), g = c(1,0),
                 h = c(0,5), i = c(2,1), n = c(1,0))

Have a nice week and enjoy further working with R.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH Zurich	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


claire pujoll writes:
 > Dear R members,
 > 
 > Sorry for this question. I have a dataframe (please see the example) and i
 > should do the following transformation:
 > 
 > DF
 > a    b    c       d      e  f   g   h   i   n
 > 14  24  rcvf    AG    5  2  1   0   2   1
 > 58  42  grde   AC    2  5  0   5   1   0
 > 
 > 
 > I should transforme my DF like :
 > 
 >  a    b    c        d       e        f         g       h       i        n
 > 14  24  rcvf     A G    0 0   G G    A G  G G   G G   A  A
 > 58  42  grde    A C    C C  0 0     A A    0 0    A C   A A
 > 
 > i.e: when DF[i,j]==5 =>  DF[i,j] <- 0 0
 >      When DF[i,j] == k (!=0) i should look to d column to do the
 > transormation
 > 
 > DF is 200000 * 10000 so i cant use loops.
 > 
 > Thanks for your help,
 > Claire
 > 
 > 	[[alternative HTML version deleted]]
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 > and provide commented, minimal, self-contained, reproducible code.


From ThadenJohnJ at uams.edu  Fri Jul 28 11:53:40 2006
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Fri, 28 Jul 2006 04:53:40 -0500
Subject: [R] order()  'decreasing =' argument must be typed in full
In-Reply-To: <mailman.11.1153994404.29801.r-help@stat.math.ethz.ch>
Message-ID: <0C6BF3FC506F664F90C8BA3E0160462D04A75920@EXCHANGE3.ad.uams.edu>

## While in R v. 2.3.1 (the mid-July patch for Windows)
## on a Windows XP machine, this call to order() works fine...

order(1:10,decreasing = TRUE)
## [1] 10  9  8  7  6  5  4  3  2  1

## ...however, the argument name 'decreasing'
## must be typed in toto (note the missing 'g'
## in the following):

> order(1:10,decreasin = TRUE)
## Error in order(na.last, decreasing, ...) : 
##         argument lengths differ

## Isn't is standard practice to have R base functions
## accept truncated argument names as long as they are
## unambiguous?
 
Thanks,

John Thaden, Ph.D.

Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From berwin at maths.uwa.edu.au  Fri Jul 28 12:12:55 2006
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Fri, 28 Jul 2006 18:12:55 +0800
Subject: [R] order()  'decreasing =' argument must be typed in full
In-Reply-To: <0C6BF3FC506F664F90C8BA3E0160462D04A75920@EXCHANGE3.ad.uams.edu>
References: <mailman.11.1153994404.29801.r-help@stat.math.ethz.ch>
	<0C6BF3FC506F664F90C8BA3E0160462D04A75920@EXCHANGE3.ad.uams.edu>
Message-ID: <17609.58151.227792.229794@bossiaea.maths.uwa.edu.au>

G'day John,

>>>>> "JT" == Thaden, John J <ThadenJohnJ at uams.edu> writes:

    JT> Isn't is standard practice to have R base functions accept
    JT> truncated argument names as long as they are unambiguous?
No. :)

The help page of order states:

Usage:

     order(..., na.last = TRUE, decreasing = FALSE)

Standard practice in R is that arguments behind ... are only matched
by "exact matching", i.e., you have to specify the tag of that
argument in full.  Details on how arguments are matched are in the
section "Argument matching" of "The R Language Definition" (and all
though this is a draft, that part seems to be authoritative :) ).

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin


From gunther.hoening at ukmainz.de  Fri Jul 28 12:39:19 2006
From: gunther.hoening at ukmainz.de (=?iso-8859-1?Q?Gunther_H=F6ning?=)
Date: Fri, 28 Jul 2006 12:39:19 +0200
Subject: [R] Calculate x-values from a spline
Message-ID: <003501c6b232$14f38390$0f1e0b0a@3med.klinik.unimainz.de>

Hello,

I calculate splines from messured points(x,y) of an unknown function f(x).
e.g.
x <- c(0.004115, 0.012345, 0.037037, 0.111110, 0.333330, 1.000000)
y <- c(37, 50, 45, 60, 50, 66)
w <- c(0.8540541, 0.8320000, 0.8822222, 0.7983333, 0.8220000, 0.8151515) as
weights
f <- smooth.spline(x,y,w)

Now I have y-values and want to calculate the x-value(s) from this spline.
There is no further limitation to the spline, like monotonicity or anything
else..

Does anyone know how to do this ?

Gunther



gunther.hoening at ukmainz.de


From hannesalazar at gmail.com  Fri Jul 28 13:13:45 2006
From: hannesalazar at gmail.com (yohannes alazar)
Date: Fri, 28 Jul 2006 12:13:45 +0100
Subject: [R] spliting
Message-ID: <ae94396d0607280413i6e1ac900pd33bbb2b95256840@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060728/4ba45b80/attachment.pl 

From jholtman at gmail.com  Fri Jul 28 13:22:38 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 28 Jul 2006 07:22:38 -0400
Subject: [R] spliting
In-Reply-To: <ae94396d0607280413i6e1ac900pd33bbb2b95256840@mail.gmail.com>
References: <ae94396d0607280413i6e1ac900pd33bbb2b95256840@mail.gmail.com>
Message-ID: <644e1f320607280422l6587312fyae5eea24655b8e31@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060728/12d1dee3/attachment.pl 

From helen.mills at yale.edu  Fri Jul 28 13:29:06 2006
From: helen.mills at yale.edu (helen.mills at yale.edu)
Date: Fri, 28 Jul 2006 07:29:06 -0400
Subject: [R] Non-metric Multidimensional Scaling in R (Tobias Verbeke)
Message-ID: <20060728072906.ky38tb9dcoossg04@www.mail.yale.edu>

The vegan package has non-metric multidimensional scaling in addition to a
variety of other ordination techniques.

Regards,
Helen Mills Poulos
Yale University


From Fabien.Lebugle at orleans.inra.fr  Fri Jul 28 13:41:06 2006
From: Fabien.Lebugle at orleans.inra.fr (Fabien Lebugle)
Date: Fri, 28 Jul 2006 13:41:06 +0200
Subject: [R] Comparison of linear models
Message-ID: <44C9F7D2.1020800@orleans.inra.fr>

Good afternoon,

I am a master student. I am currently doing an internship.
I would like to get some advices about the following issue: I have 2 
data sets,  both containing the same variables, but the data were 
measured using two different procedures. I want to know if the two 
procedures are equivalent.
Up to know, I have built one linear model for each dataset. The two 
models have the same form. I would like to compare these two models: are 
they identical? Are they different? By how much?

Please, could you tell me which R procedure I should use? I have been 
searching the list archive, but without success...

Regards.

Fabien


From JeeBee at troefpunt.nl  Fri Jul 28 13:42:22 2006
From: JeeBee at troefpunt.nl (JeeBee)
Date: Fri, 28 Jul 2006 13:42:22 +0200
Subject: [R] convert decimals to fractions - sorted
References: <c7c17cef0607251002k6f6005e1h5b717d3fb23ff1b6@mail.gmail.com>
	<pan.2006.07.26.09.43.22.948661@troefpunt.nl>
	<c7c17cef0607260435l23376a10haa5d7cae28458fc2@mail.gmail.com>
Message-ID: <pan.2006.07.28.11.42.21.814865@troefpunt.nl>


Ah I see, I did not read your story well enough.
You want to sort after applying table()
Well, the idea I suggested was to keep the real numbers in,
because the fractions are sorted as characters strings (alphabetically),
which is not what you want. So, now I suggest the following:

# First apply table()
tmp1 <- as.data.frame(table(df))

# Note that table() turned your numeric data into factors,
# this might not be a handy approach, anyways, it is possible I guess.
# You have to convert back using as.numeric(as.character(tmp1$V2))
# or, more efficiently, as.numeric(levels(tmp1$V2))[tmp1$V2]

# Add the column with the fractions
tmp2 <- cbind(tmp1, 
        fracs=as.character(as.fractions(as.numeric(as.character(tmp1$V2)))))

# Finally hide that sort colum if you want

( tmp2[-2] )

# Everybody happy?

JeeBee


From JZajd at constellagroup.com  Fri Jul 28 14:08:08 2006
From: JZajd at constellagroup.com (Zajd, John)
Date: Fri, 28 Jul 2006 08:08:08 -0400
Subject: [R] BUILD command fails with large R source file
Message-ID: <B341A577AB7B1E4C8CEA91349321B09C018A418F@dur-exchange.corp.constellagroup.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060728/69e1c2df/attachment.pl 

From rolf at erdos.math.unb.ca  Fri Jul 28 14:10:21 2006
From: rolf at erdos.math.unb.ca (Rolf Turner)
Date: Fri, 28 Jul 2006 09:10:21 -0300 (ADT)
Subject: [R] Comparison of linear models
Message-ID: <200607281210.k6SCAL0c022170@erdos.math.unb.ca>


Fabien Lebugle wrote:

> I am a master student. I am currently doing an internship.  I would
> like to get some advices about the following issue: I have 2 data
> sets,  both containing the same variables, but the data were measured
> using two different procedures. I want to know if the two procedures
> are equivalent.  Up to know, I have built one linear model for each
> dataset. The two models have the same form. I would like to compare
> these two models: are they identical? Are they different? By how
> much?
> 
> Please, could you tell me which R procedure I should use? I have been 
> searching the list archive, but without success...

	This is not a question of ``which R procedure'' but rather a
	question of understanding a bit about statistics and linear
	models.  You say you are a ``master's student''; I hope you
	are not a master's student in *statistics*, given that you
	lack this (very) basic knowledge!  If you are a student in
	some other discipline, I guess you may be forgiven.

	The ``R procedure'' that you need to use is just lm()!

	Briefly, what you need to do is combine your two data
	sets into a *single* data set (using rbind should work),
	add in a grouping variable (a factor with two levels,
	one for each measure procedure) e.g.

		my.data$gp <- factor(rep(c(1,2),c(n1,n2)))

	where n1 and n2 are the sample sizes for procedure 1 and
	procedure 2 respectively.

	Then fit linear models with formulae involving the
	grouping factor (``gp'') as well as the other predictors,
	and test for the ``significance'' of the terms in
	the model that contain ``gp''.  You might start with

		fit <- lm(y~.*gp,data=my.data)
		anova(fit)

	where ``y'' is (of course) your reponse.

	You ought to study up on the underlying ideas of inference
	for linear models, and the nature of ``factors''.  John Fox's
	book ``Applied Regression Analysis, Linear Models, and
	Related Methods'' might be a reasonable place to start.

	Bon chance.

				cheers,

					Rolf Turner
					rolf at math.unb.ca


From ecu at info.fundp.ac.be  Fri Jul 28 14:31:37 2006
From: ecu at info.fundp.ac.be (Cuvelier Etienne)
Date: Fri, 28 Jul 2006 14:31:37 +0200
Subject: [R] Calculate x-values from a spline
In-Reply-To: <003501c6b232$14f38390$0f1e0b0a@3med.klinik.unimainz.de>
References: <003501c6b232$14f38390$0f1e0b0a@3med.klinik.unimainz.de>
Message-ID: <44CA03A9.8010706@info.fundp.ac.be>

Gunther H?ning a ?crit :
> Hello,
> 
> I calculate splines from messured points(x,y) of an unknown function f(x).
> e.g.
> x <- c(0.004115, 0.012345, 0.037037, 0.111110, 0.333330, 1.000000)
> y <- c(37, 50, 45, 60, 50, 66)
> w <- c(0.8540541, 0.8320000, 0.8822222, 0.7983333, 0.8220000, 0.8151515) as
> weights
> f <- smooth.spline(x,y,w)
> 
> Now I have y-values and want to calculate the x-value(s) from this spline.
> There is no further limitation to the spline, like monotonicity or anything
> else..
> 
> Does anyone know how to do this ?

Perhaps this ?

finv = splinefun(f$y,f$x)

> 
> Gunther
> 
> 
> 
> gunther.hoening at ukmainz.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 

-- 
===============================================
Cuvelier Etienne
Assistant
FUNDP - Institut d'Informatique
rue Grandgagnage, 21   B-5000 Namur (Belgique)
tel: 32.81.72.49.93    fax: 32.81.72.49.67
===============================================


--


From phhs80 at gmail.com  Fri Jul 28 14:35:41 2006
From: phhs80 at gmail.com (Paul Smith)
Date: Fri, 28 Jul 2006 13:35:41 +0100
Subject: [R] Non-parametric four-way interactions?
In-Reply-To: <44C8B1A6.7000407@vanderbilt.edu>
References: <6ade6f6c0607261849p576cc8c6of0e3247ef1c2134b@mail.gmail.com>
	<44C83642.2090806@vanderbilt.edu>
	<6ade6f6c0607270352q3d4e6b3anb48d76aeb1d00df8@mail.gmail.com>
	<44C8B1A6.7000407@vanderbilt.edu>
Message-ID: <6ade6f6c0607280535jd463716s9165b44673359866@mail.gmail.com>

On 7/27/06, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
> Tony Lachenbruch has written some about this, also see my book (it's web
> page is biostat.mc.vanderbilt.edu/rms).  I don't know about the power of
> interaction tests, but for main effect tests in the absence of
> interaction, the Wilcoxon test (a special case of PO model) has
> efficiency of 3/pi compared to the t-test if normality holds.
>
> Other approaches: Cox PH model, avas, ace.  My book covers these too.

Thanks, Frank, for the useful information.

Paul


From andy_liaw at merck.com  Fri Jul 28 15:28:09 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 28 Jul 2006 09:28:09 -0400
Subject: [R] memory problems when combining randomForests
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAA982@usctmx1106.merck.com>

From: Eleni Rapsomaniki
> 
> Hi Andy, 
> 
> > > I'm using R (windows) version 2.1.1, randomForest version 4.15. 
> >                                        ^^^^^^^^^^^^^^^^^^^^^^^^^ 
> > Never seen such a version...
> Ooops! I meant 4.5-15
>  
> > > I then save each tree to a file so I can combine them all 
> > > afterwards. There are no memory issues when 
> > > keep.forest=FALSE. But I think that's the bit I need for 
> > > future predictions (right?). 
> > 
> > Yes, but what is your question?  (Do you mean each *forest*,
> > instead of each *tree*?)
> I mean the component of the object that is created from 
> randomForest that has
> the name "forest" (and takes up all the memory!). 

Yes, the forest can take up quite a bit of space.  You might 
consider setting nodesize larger and see if that gives you 
sufficient space saving w/o compromising prediction performance.
 
> > > A bit off the subject, but should the order at which at rows 
> > > (ie. sets of explanatory variables) are passed to the 
> > > randomForest function affect the result? I have noticed that 
> > > if I pick a random unordered sample from my control data for 
> > > training the error rate is much lower than if I a take an 
> > > ordered sample. This remains true for all my cross-validation 
> > > results. 
> > 
> > I'm not sure I understand.  In randomForest() (as in other
> > functions) variables are in columns, rather than rows, so
> > are you talking about variables (columns) in different order 
> > or data (rows) in different order?
> 
> Yes, sorry I confused you. I mean the order at which data 
> (rows) is passed, not
> columns.

Then I'm not sure what you mean by difference in performance, even
in cross-validation.  Perhaps you can show some example?  Each 
tree in the forest is grown on a random sample from the data, so
the order of the row can not matter.


> Finally, I see from
> http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#inter
> 
> that there is a component in Breiman's implementation of 
> randomForest that
> computes interactions between parameters. Has this been 
> implemented in R yet?

No.  Prof. Breiman told me that is very experimental, and he
wouldn't mind if that doesn't make it into the R package.  
Since I have other priorities for the package, that naturally
went to the backburner.

Cheers,
Andy

 
> Many thanks for your time and help.
> Eleni Rapsomaniki
> 
> 
> ----------------------------------------------------------------
> This message was sent using IMP, the Internet Messaging Program.
> 
>


From bates at stat.wisc.edu  Fri Jul 28 15:48:33 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 28 Jul 2006 08:48:33 -0500
Subject: [R] residual df in lmer and simulation results
In-Reply-To: <002401c6b0e5$dabcbf40$bb1ad284@BIO041>
References: <002401c6b0e5$dabcbf40$bb1ad284@BIO041>
Message-ID: <40e66e0b0607280648k25eeeacfvb15a215e0cd79b23@mail.gmail.com>

On 7/26/06, Bill Shipley <bill.shipley at usherbrooke.ca> wrote:

> Hello.  Douglas Bates has explained in a previous posting to R why he does
> not output residual degrees of freedom, F values and probabilities in the
> mixed model (lmer) function:  because the usual degrees of freedom (obs -
> fixed df -1) are not exact and are really only upper bounds.  I am
> interpreting what he said but I am not a professional statistician, so I
> might be getting this wrong...

> Does anyone know of any more recent results, perhaps from simulations, that
> quantify the degree of bias that using such upper bounds for the demoninator
> degrees of freedom produces?  Is it possible to calculate a lower bounds for
> such degrees of freedom?

I have not seen any responses to your request yet Bill.  I was hoping
that others might offer their opinions and provide some new
perspectives on this issue.  However, it looks as if you will be stuck
with my responses for the time being.

You have phrased your question in terms of the denominator degrees of
freedom associated with terms in the fixed-effects specification and,
indeed, this is the way the problem is usually addressed.  However,
that is jumping ahead two or three steps from the iniital problem
which is how to perform an hypothesis test comparing two nested models
- a null model without the term in question and the alternative model
including this term.

If we assume that the F statistic is a reasonable way of evaluating
this hypothesis test and that the test statistic will have an F
distribution with a known numerator degrees of freedom and an unknown
denominator degrees of freedom then we can reduce the problem of
testing the hypothesis to one of approximating the denominator degrees
of freedom.  However, there is a lot of assumption going on in that
argument.  These assumptions may be warranted or they may not.

As far as I can see, the usual argument made for those assumptions is
by analogy.  If we had a balanced design and if we used error strata
to get expected and observed mean squares and if we equated expected
and observed mean squares to obtain estimates of variance components
then the test for a given term in the fixed effects specification
would have a certain form.  Even though we are not doing any of these
things when estimating variance components by maximum likelihood or by
REML, the argument is that the test for a fixed effects term should
end up with the same form.  I find that argument to be a bit of a
stretch.

Because the results from software such as SAS PROC MIXED are based on
this type of argument many people assume that it is a well-established
result that the test should be conducted in this way.  Current
versions of PROC MIXED allow for several different ways of calculating
denominator degrees of freedom, including at least one, the
Kenward-Roger  method, that uses two tuning parameters - denominator
degrees of freedom and a scale factor.

Some simulation studies have been performed comparing the methods in
SAS PROC MIXED and other simulation studies are planned but for me
this is all putting the cart before the horse.  There is no answer to
the question "what is the _correct_ denominator degrees of freedom for
this test statistic" if the test statistic doesn't have a F
distribution with a known numerator degrees of freedom and an unknown
denominator degrees of freedom.

I don't think there is a perfect answer to this question.  I like the
approach using Markov chain Monte Carlo samples from the posterior
distribution of the parameters because it allows me to assess the
distribution of the parameters and it takes into account the full
range of the variation in the parameters (the F-test approach is
conditional on estimates of the variance components).  However, it
does not produce a nice cryptic p-value for publication.

I understand the desire for a definitive answer that can be used in a
publication.  However, I am not satisfied with any of the "definitive
answers" that are out there and I would rather not produce an answer
than produce an answer that I don't believe in.


From jh at jameshoward.us  Fri Jul 28 15:56:06 2006
From: jh at jameshoward.us (James P. Howard, II)
Date: Fri, 28 Jul 2006 09:56:06 -0400
Subject: [R] HTTP User-Agent header
Message-ID: <88a2333a0607280656r7f5b6e64ve6d03dd215433df2@mail.gmail.com>

I am using R in an environment where the HTTP proxy server blocks
browsers that do not send User-Agent strings.  Through some testing,
we have determined that R, by default (2.3.1) does not send a string
when performing the chooseCRANmirror() function.  Is there a setting
that allows us to manually set the User-Agent value?

Thank you, James

-- 
James P. Howard, II  -  jh at jameshoward.us
http://jameshoward.us/  -  (443)-430-4050


From xwyefelix at gmail.com  Fri Jul 28 16:04:13 2006
From: xwyefelix at gmail.com (xingwang ye)
Date: Fri, 28 Jul 2006 07:04:13 -0700
Subject: [R] could someone help me to install packages "gam" (ubuntu 6.06)
Message-ID: <1bff52c20607280704s673f0185q9465fc1269dfe88c@mail.gmail.com>

> install.packages("gam")
Warning in install.packages("gam") : argument 'lib' is missing: using
/usr/local/lib/R/site-library
--- Please select a CRAN mirror for use in this session ---
Loading Tcl/Tk interface ... done
trying URL 'http://cran.cnr.Berkeley.edu/src/contrib/gam_0.97.tar.gz'
Content type 'application/x-gzip' length 89613 bytes
opened URL
==================================================
downloaded 87Kb

* Installing *source* package 'gam' ...
** libs
g77   -fpic  -g -O2 -c backfit.f -o backfit.o
g77   -fpic  -g -O2 -c backlo.f -o backlo.o
g77   -fpic  -g -O2 -c bsplvd.f -o bsplvd.o
g77   -fpic  -g -O2 -c bvalue.f -o bvalue.o
g77   -fpic  -g -O2 -c bvalus.f -o bvalus.o
g77   -fpic  -g -O2 -c linear.f -o linear.o
gcc -I/usr/share/R/include -I/usr/share/R/include     -fpic  -g -O2
-std=gnu99 -c loessc.c -o loessc.o
g77   -fpic  -g -O2 -c loessf.f -o loessf.o
g77   -fpic  -g -O2 -c lo.f -o lo.o
g77   -fpic  -g -O2 -c qsbart.f -o qsbart.o
gcc -I/usr/share/R/include -I/usr/share/R/include     -fpic  -g -O2
-std=gnu99 -c sbart.c -o sbart.o
g77   -fpic  -g -O2 -c sgram.f -o sgram.o
g77   -fpic  -g -O2 -c sinerp.f -o sinerp.o
g77   -fpic  -g -O2 -c splsm.f -o splsm.o
g77   -fpic  -g -O2 -c sslvrg.f -o sslvrg.o
g77   -fpic  -g -O2 -c stxwx.f -o stxwx.o
gcc -shared  -o gam.so backfit.o backlo.o bsplvd.o bvalue.o bvalus.o
linear.o loessc.o loessf.o lo.o qsbart.o sbart.o sgram.o sinerp.o
splsm.o sslvrg.o stxwx.o -lblas-3 -lg2c -lm -lgcc_s -L/usr/lib/R/lib
-lR
/usr/bin/ld: cannot find -lblas-3
collect2: ld returned 1 exit status
make: *** [gam.so] Error 1
ERROR: compilation failed for package 'gam'
** Removing '/usr/local/lib/R/site-library/gam'

The downloaded packages are in
        /tmp/RtmpQmwp2v/downloaded_packages
Warning message:
installation of package 'gam' had non-zero exit status in:
install.packages("gam")


From s.wood at bath.ac.uk  Fri Jul 28 16:01:49 2006
From: s.wood at bath.ac.uk (Simon Wood)
Date: Fri, 28 Jul 2006 15:01:49 +0100
Subject: [R] Calculate x-values from a spline
In-Reply-To: <44CA03A9.8010706@info.fundp.ac.be>
References: <003501c6b232$14f38390$0f1e0b0a@3med.klinik.unimainz.de>
	<44CA03A9.8010706@info.fundp.ac.be>
Message-ID: <200607281501.49375.s.wood@bath.ac.uk>

On Friday 28 July 2006 13:31, Cuvelier Etienne wrote:
> Gunther H?ning a ?crit :
> > Hello,
> >
> > I calculate splines from messured points(x,y) of an unknown function
> > f(x). e.g.
> > x <- c(0.004115, 0.012345, 0.037037, 0.111110, 0.333330, 1.000000)
> > y <- c(37, 50, 45, 60, 50, 66)
> > w <- c(0.8540541, 0.8320000, 0.8822222, 0.7983333, 0.8220000, 0.8151515)
> > as weights
> > f <- smooth.spline(x,y,w)
> >
> > Now I have y-values and want to calculate the x-value(s) from this
> > spline. There is no further limitation to the spline, like monotonicity
> > or anything else..
> >
> > Does anyone know how to do this ?
>
> Perhaps this ?
>
> finv = splinefun(f$y,f$x)
>
I think this solution is a bit unstable, unless the spline is monotonic. For 
example...

set.seed(1)
x <- sort(runif(100)) ## divide this by 2 to get case where it works
y <- x*(1-x)+rnorm(100)*.01
par(mfrow=c(1,2))
plot(x,y,main="data and spline")
f <- smooth.spline(x,y) ## fit spline to simulated non-monotonic data
lines(x,fitted(f),col=2) ## plot fit
finv <- splinefun(f$y,f$x) ## "inverse function"
xx <- seq(0,0.25,length=1000) 
plot(xx,finv(xx),type="l",main="inverse!") ## plot of "inverse"
abline(0,0);abline(1,0) # plot range that x should lie in

The problem is that you can't really avoid dealing with the fact that the 
problem doesn't have a unique solution if the spline is not monotonic.

Simon

> > Gunther
> >
> >
> >
> > gunther.hoening at ukmainz.de
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.

-- 
> Simon Wood, Mathematical Sciences, University of Bath, Bath, BA2 7AY UK
> +44 1225 386603  www.maths.bath.ac.uk/~sw283


From mnair at iusb.edu  Fri Jul 28 16:42:11 2006
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Fri, 28 Jul 2006 10:42:11 -0400
Subject: [R] mult comp significance
Message-ID: <A32055BDEA88C34BB3DBBCD229380778570601@iu-mssg-mbx109.ads.iu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060728/cfaee1b1/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Jul 28 16:43:34 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 28 Jul 2006 15:43:34 +0100 (BST)
Subject: [R] HTTP User-Agent header
In-Reply-To: <88a2333a0607280656r7f5b6e64ve6d03dd215433df2@mail.gmail.com>
References: <88a2333a0607280656r7f5b6e64ve6d03dd215433df2@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0607281538020.20596@gannet.stats.ox.ac.uk>

What OS is this?

If Windows, see the rw-FAQ Q2.19.

Otherwise, see ?download.file and choose a different download method,
or look at the source code (src/modules/internet/nanohttp.c) and submit a 
patch.

On Fri, 28 Jul 2006, James P. Howard, II wrote:

> I am using R in an environment where the HTTP proxy server blocks
> browsers that do not send User-Agent strings.  

Given how incredibly easy they are to fake, why?

> Through some testing, we have determined that R, by default (2.3.1) does 
> not send a string when performing the chooseCRANmirror() function.  Is 
> there a setting

BTW, the chooseCRANmirror() function does not do HTTP: download.file() and 
url() do and the help is at ?download.file.

> that allows us to manually set the User-Agent value?
> 
> Thank you, James
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bolker at ufl.edu  Fri Jul 28 16:52:27 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Fri, 28 Jul 2006 14:52:27 +0000 (UTC)
Subject: [R] negative binomial lmer
References: <20060728020020.33116.qmail@web37508.mail.mud.yahoo.com>
Message-ID: <loom.20060728T165048-123@post.gmane.org>

Tracy Feldman <tracysfeldman <at> yahoo.com> writes:

> 
> To whom it may concern:
> 
>   I have a question about how to appropriately conduct an lmer analysis for
negative binomially distributed
> data.  I am using R 2.2.1 on a windows machine. 
> 
>   I am trying to conduct an analysis using lmer (for non-normally distributed
data and both random and fixed
> effects) for negative binomially distributed data.  To do this, I have been
using maximum likelihood,
> comparing the full model to reduced models (containing all but one effect, for
all effects).  However, for
> negative binomially distributed data, I need to estimate the parameter theta.
 I have been doing this by
> using a negative binomial glm of the same model (except that all the effects
are fixed), and estimating mu
> as the fitted model like so:
>\

 I haven't tried it, but you could also consider using
a Poisson-lognormal (rather than neg binomial, which is Poisson-gamma)
distribution, which might make this all work rather well
in lmer:

www.cefe.cnrs.fr/esp/TBElston_Parasitology2001.pdf

  cheers
    Ben Bolker


From isabelle.rivals at espci.fr  Fri Jul 28 16:56:27 2006
From: isabelle.rivals at espci.fr (Isabelle Rivals)
Date: Fri, 28 Jul 2006 16:56:27 +0200
Subject: [R] tests performed by anova
Message-ID: <a0611043ec0efcd0748f0@[193.54.88.248]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060728/c5140adc/attachment.pl 

From Markus.Preisetanz at clientvela.com  Fri Jul 28 17:40:03 2006
From: Markus.Preisetanz at clientvela.com (Markus Preisetanz)
Date: Fri, 28 Jul 2006 17:40:03 +0200
Subject: [R] arules package: using image() deliveres unexpected results
Message-ID: <161A202DD5D3394FB46136A00CF844B719D57E@mucmsrv.hq.clientvela.net>

Dear Collegues,
 
it seems like there is a problem with the image()-method in the package arules.
 
Using an ordninary matrix works fine:
image(matrix(rnorm(200), 10, 20), axes = FALSE, col=brewer.pal(9, "Blues") )
delivers an image with blue colors and no axes.
 
Using an object of the class "associations" (arules package) does not work:
 
image(items(ta.eclat), axes = FALSE, col=brewer.pal(9, "Blues") )
delivers an error message telling that the argument "col" matches to several formal arguments.
 
image(items(ta.eclat), axes = FALSE)
delivers no error message but draws axes, which clearly should not be the case

In the source code of arules I couldn't find any definition of the image-function (especially no image.R-file.)

What I want to do is drawing an image of itemsets as produced by eclat, with blue colors and the item labels at the x-axe.

Does anybody know help?

Thank You, Markus
 
____________________________
Markus Preisetanz
Consultant

Client Vela GmbH
Albert-Ro?haupter-Str. 32
81369 M?nchen
fon: +49 (89) 74217-113
main: +49 (89) 74217-150
fax: +49 (89) 74217-250
markus.preisetanz at clientvela.com <mailto:markus.preisetanz at clientvela.com> 
http://www.clientvela.com <http://www.clientvela.com/> 
 
Diese E-Mail enth?lt vertrauliche und/oder rechtlich gesch?t...{{dropped}}


From Achim.Zeileis at wu-wien.ac.at  Fri Jul 28 18:18:31 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 28 Jul 2006 18:18:31 +0200 (CEST)
Subject: [R] arules package: using image() deliveres unexpected results
In-Reply-To: <161A202DD5D3394FB46136A00CF844B719D57E@mucmsrv.hq.clientvela.net>
References: <161A202DD5D3394FB46136A00CF844B719D57E@mucmsrv.hq.clientvela.net>
Message-ID: <Pine.LNX.4.58.0607281803010.25130@thorin.ci.tuwien.ac.at>

Markus:

> it seems like there is a problem with the image()-method in the package
> arules.

For problems with packages, please contact the maintainer or at least have
im Cc as the posting guide tells you.

> Using an ordninary matrix works fine:
> image(matrix(rnorm(200), 10, 20), axes = FALSE, col=brewer.pal(9, "Blues") )

Note that image() is a generic function, the methods called in your
example above is rather different from the methods below.

> delivers an image with blue colors and no axes.
>
> Using an object of the class "associations" (arules package) does not work:
>
> image(items(ta.eclat), axes = FALSE, col=brewer.pal(9, "Blues") )
> delivers an error message telling that the argument "col" matches to
> several formal arguments.

So maybe you should check what arguments are allowed for the method in
arules:
  help("image", package = "arules")
tells you to look that essentially levelplot() is called and
  help("levelplot", package = "lattice")
shows that there are two arguments starting with `col' (col.regions and
colorkey) and that there is no argument `axes'.

> In the source code of arules I couldn't find any definition of the
> image-function (especially no image.R-file.)

It's in itemMatrix.R, because it is the image() method for "itemMatrix"
objects. You can also see in
  getMethod("image", "itemMatrix")
that it calls the "dgTMatrix" method:
  getMethod("image", "dgTMatrix")
which calls levelplot().

A note to Michael (arules maintainer): the link from the image()
documentation in arules to Matrix is broken (should be to
"dgTMatrix-class") and maybe you should also link levelplot() in lattice
explicitely?

hth,
Z


From res90sx5 at verizon.net  Fri Jul 28 18:25:59 2006
From: res90sx5 at verizon.net (Daniel Nordlund)
Date: Fri, 28 Jul 2006 09:25:59 -0700
Subject: [R] BUILD command fails with large R source file
In-Reply-To: <B341A577AB7B1E4C8CEA91349321B09C018A418F@dur-exchange.corp.constellagroup.com>
Message-ID: <001701c6b262$82db5830$6401a8c0@main>

John,

You need to supply a lot more information if you wish to receive any useful assistance.  You have not told us your operating system, version of R, or the error message you received.  You are convinced that the problem is too many lines of code, but I have seen many postings where people were convinced of things that were not true.  The lines of code you are adding may have effects on other sections of code (no one can tell because you haven't shown them to us).

If you follow the directions in the posting guide listed at the bottom, I am sure that someone can provide you with some assistance.

Dan Nordlund
Bothell, WA
USA

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch]
> On Behalf Of Zajd, John
> Sent: Friday, July 28, 2006 5:08 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] BUILD command fails with large R source file
> 
> Greetings,
> 
> I modified an existing R program (now it is larger than the previous
> version), and when I attempt to perform a BUILD using the command
> "R CMD BUILD --binary --force RAGG" the build fails.
> It outputs a totally bogus message when it fails; not related at all to
> the true problem.
> 
> I know the problem is the length of the R program because when I remove
> some lines of code the BUILD is successful, and when I add them back in,
> the BUILD fails.
> 
> Can you provide any help with this issue; is there a command line option
> that resolves this problem?
> 
> Thank you for your time and effort.
> 
> John Zajd
> Constella Group
> Raleigh, NC
> (919) 313-
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From caobg at email.uc.edu  Fri Jul 28 18:29:33 2006
From: caobg at email.uc.edu (Baoqiang Cao)
Date: Fri, 28 Jul 2006 12:29:33 -0400
Subject: [R] heatmap problem with clustering columns
Message-ID: <7.0.1.0.2.20060728122508.022706a0@email.uc.edu>

Dear All,

I'm trying to get a heatmap for my n*m matrix, I want to cluster the 
m samples (columns) using kmeans with manhattan distance. I was able 
to use hclust method to cluster the m samples, but failed to figure 
out how to use kmeans. Any suggestion is very welcome!

Best,
  Cao


From Camarda at demogr.mpg.de  Fri Jul 28 18:40:14 2006
From: Camarda at demogr.mpg.de (Camarda, Carlo Giovanni)
Date: Fri, 28 Jul 2006 18:40:14 +0200
Subject: [R] Extracting from a matrix w/o for-loop
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E601700006@HERMES.demogr.mpg.de>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060728/957eb23c/attachment.pl 

From andy_liaw at merck.com  Fri Jul 28 18:44:11 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 28 Jul 2006 12:44:11 -0400
Subject: [R] Extracting from a matrix w/o for-loop
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAAA19@usctmx1106.merck.com>

Yes, by matrix indexing:

R> M1[M2]
[1]  1  2 13 15  8
R> v
[1]  1  2 13 15  8

Andy 

From: Camarda, Carlo Giovanni
> 
> Dear R-users,
> 
> likely there is a simple solution for this problem, but I currently
> cannot see it.
> 
> I basically would like to get from a matrix values in particular
> positions which are the rows of another matrix, without using a
> for-loop.
> 
> In other words: is there any way to avoid the for-loop in the 
> following
> simple example:
> 
> M1 <- matrix(1:20, ncol=2)
> M2 <- rbind(c(1,1), c(2,1), c(3,2), c(5,2), c(8,1))
> v <- numeric(nrow(M2))
> for(i in 1:length(v)){
>     v[i] <- M1[M2[i,1], M2[i,2]]
> }
> 
> Any suggestion would be welcome,
> 
> Ciao,
> Carlo Giovanni Camarda
> 
> ===========================================
> Camarda Carlo Giovanni
> PhD-Student
> Max Planck Institute for Demographic Research
> Konrad-Zuse-Strasse 1
> 18057 Rostock, Germany
> Tel:  +49 (0)381 2081 172
> Fax: +49 (0)381 2081 472
> camarda at demogr.mpg.de
> ===========================================
> 
> 
> ----------
> This mail has been sent through the MPI for Demographic 
> Rese...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From bates at stat.wisc.edu  Fri Jul 28 18:48:30 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 28 Jul 2006 11:48:30 -0500
Subject: [R] residual df in lmer and simulation results
In-Reply-To: <002401c6b0e5$dabcbf40$bb1ad284@BIO041>
References: <002401c6b0e5$dabcbf40$bb1ad284@BIO041>
Message-ID: <40e66e0b0607280948r779a46bdvce54f7840d01846@mail.gmail.com>

On 7/26/06, Bill Shipley <bill.shipley at usherbrooke.ca> wrote:
> Hello.  Douglas Bates has explained in a previous posting to R why he does
> not output residual degrees of freedom, F values and probabilities in the
> mixed model (lmer) function:  because the usual degrees of freedom (obs -
> fixed df -1) are not exact and are really only upper bounds.  I am
> interpreting what he said but I am not a professional statistician, so I
> might be getting this wrong...
> Does anyone know of any more recent results, perhaps from simulations, that
> quantify the degree of bias that using such upper bounds for the demoninator
> degrees of freedom produces?  Is it possible to calculate a lower bounds for
> such degrees of freedom?

I can give another perspective on the issue of degrees of freedom for
a linear mixed model although it probably doesn't address the question
that you want to address.

The linear predictor in a mixed model has the form X\beta + Zb where
\beta is the fixed-effects vector and b is the random-effects vector.
The fitted values, y-hat, are the fitted values from a penalized least
squares fit of the response vector, y, to this linear predictor
subject to a penalty on b defined by the variance components.  When
the penalty is large, the fitted values approach those from the
ordinary least squares fit of y on X\beta only.  When the penalty is
small, the fitted values approach those from an unpenalized least
squares fit of y on the linear predictor.  (In this case estimates of
the coefficients are not well defined because the combined matrix
[X:Z] is generally rank deficient but the fitted values are well
defined.)

If the rank of X is p and the rank of [X:Z] is r then the effective
number of parameters in the linear predictor for the penalized least
squares fit is somewhere between p and r.  One way of defining the
effective number of parameters is as the trace of the hat matrix for
the penalized least squares problem.  This number will change as the
variance components change and is usually evaluated at the estimates
of the variance components.  This is exactly what Spiegelhalter, Best,
Carlin and van der Linde (JRSSB, 64(4), 583-639, 2002) define to be
their p_D for this model.  The next release of the lme4/Matrix
packages will include an extractor function to evaluate the trace of
the hat matrix for a fitted lmer model, using an algorithm due to
Jialiang Li.

This effective number of parameters in the linear predictor is like
the degrees of freedom for regression.  In the limiting cases it is
exactly the degrees of freedom for regression.  One might then argue
that the degrees of freedom for residuals would be n - hat trace and
use this for the denominator degrees of freedom in the F ratios.
However,  this number does not vary with the numerator term and many
people will claim that it must.  (I admit to being a bit perplexed as
to why the denominator degrees of freedom should change according to
the numerator term when the denominator of the F ratio itself doesn't
change, but many people insist that this is the way things must be.)

So it is possible to calculate a number that can reasonably be
considered to be the
degrees of freedom for the denominator that is actually used in the F
ratios but this will not correspond to what many people will insist is
the "obviously correct" number of degrees of freedom.


From edd at debian.org  Fri Jul 28 18:58:49 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 28 Jul 2006 11:58:49 -0500
Subject: [R] could someone help me to install packages "gam" (ubuntu
	6.06)
In-Reply-To: <1bff52c20607280704s673f0185q9465fc1269dfe88c@mail.gmail.com>
References: <1bff52c20607280704s673f0185q9465fc1269dfe88c@mail.gmail.com>
Message-ID: <20060728165849.GA23897@eddelbuettel.com>

$ apt-get install r-base-dev

and you should get the required dependencies, in this case for
Blas/Atlas.

Hth, Dirk

On Fri, Jul 28, 2006 at 07:04:13AM -0700, xingwang ye wrote:
> > install.packages("gam")
> Warning in install.packages("gam") : argument 'lib' is missing: using
> /usr/local/lib/R/site-library
> --- Please select a CRAN mirror for use in this session ---
> Loading Tcl/Tk interface ... done
> trying URL 'http://cran.cnr.Berkeley.edu/src/contrib/gam_0.97.tar.gz'
> Content type 'application/x-gzip' length 89613 bytes
> opened URL
> ==================================================
> downloaded 87Kb
> 
> * Installing *source* package 'gam' ...
> ** libs
> g77   -fpic  -g -O2 -c backfit.f -o backfit.o
> g77   -fpic  -g -O2 -c backlo.f -o backlo.o
> g77   -fpic  -g -O2 -c bsplvd.f -o bsplvd.o
> g77   -fpic  -g -O2 -c bvalue.f -o bvalue.o
> g77   -fpic  -g -O2 -c bvalus.f -o bvalus.o
> g77   -fpic  -g -O2 -c linear.f -o linear.o
> gcc -I/usr/share/R/include -I/usr/share/R/include     -fpic  -g -O2
> -std=gnu99 -c loessc.c -o loessc.o
> g77   -fpic  -g -O2 -c loessf.f -o loessf.o
> g77   -fpic  -g -O2 -c lo.f -o lo.o
> g77   -fpic  -g -O2 -c qsbart.f -o qsbart.o
> gcc -I/usr/share/R/include -I/usr/share/R/include     -fpic  -g -O2
> -std=gnu99 -c sbart.c -o sbart.o
> g77   -fpic  -g -O2 -c sgram.f -o sgram.o
> g77   -fpic  -g -O2 -c sinerp.f -o sinerp.o
> g77   -fpic  -g -O2 -c splsm.f -o splsm.o
> g77   -fpic  -g -O2 -c sslvrg.f -o sslvrg.o
> g77   -fpic  -g -O2 -c stxwx.f -o stxwx.o
> gcc -shared  -o gam.so backfit.o backlo.o bsplvd.o bvalue.o bvalus.o
> linear.o loessc.o loessf.o lo.o qsbart.o sbart.o sgram.o sinerp.o
> splsm.o sslvrg.o stxwx.o -lblas-3 -lg2c -lm -lgcc_s -L/usr/lib/R/lib
> -lR
> /usr/bin/ld: cannot find -lblas-3
> collect2: ld returned 1 exit status
> make: *** [gam.so] Error 1
> ERROR: compilation failed for package 'gam'
> ** Removing '/usr/local/lib/R/site-library/gam'
> 
> The downloaded packages are in
>         /tmp/RtmpQmwp2v/downloaded_packages
> Warning message:
> installation of package 'gam' had non-zero exit status in:
> install.packages("gam")
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From douglas.stave at wellsfargo.com  Thu Jul 27 20:40:12 2006
From: douglas.stave at wellsfargo.com (douglas.stave at wellsfargo.com)
Date: Thu, 27 Jul 2006 13:40:12 -0500
Subject: [R] Non-interpreted strings
Message-ID: <F12D8C78FA60A84AA6370216A74616E78AA937@msgswbiadsm33.wellsfargo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060727/3457e319/attachment.pl 

From ggrothendieck at gmail.com  Fri Jul 28 19:05:04 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 28 Jul 2006 13:05:04 -0400
Subject: [R] Non-interpreted strings
In-Reply-To: <F12D8C78FA60A84AA6370216A74616E78AA937@msgswbiadsm33.wellsfargo.com>
References: <F12D8C78FA60A84AA6370216A74616E78AA937@msgswbiadsm33.wellsfargo.com>
Message-ID: <971536df0607281005y20a98b98m22092f315a26971@mail.gmail.com>

Enter this at the console

   x <- scan(what = "")

and after pressing the enter after the right paren,
do a paste and then press enter twice.


On 7/27/06, douglas.stave at wellsfargo.com <douglas.stave at wellsfargo.com> wrote:
> I am new to R, so please forgive me if there is an obvious answer to
> this question.  I have done fairly extensive searching through R docs,
> google and a few R users and have not found an answer to my question.
>
> Is there a way to create a non-interpreted string object in R?
>
> For example, I am using R in a MS Windows environment and I would like
> to paste DOS paths into some R command:
>        setwd("c:\some\directory")
>
> Obviously this does not work because of the escaping mechanism.  And I
> know the obvious answer, use "\\".  But if you do a lot of pasting into
> R it could get tedious manually editing escape sequences.
>
> I did find a workable solution to this particular problem:
>        setwd(choose.dir())<Enter><Paste><Enter>
> This saves me from having to do the editing myself.  I can conceive of
> other examples of wanting to paste other more abstract stings into R
> that may happen to have a \ in it.  And now, thanks to choose.dir(), I
> have a way to do the translation automagically but...
>
> My question is, is there any way in R to not interpret the string and
> store the string as is?  For instance, Perl allows you to do interpreted
> (" ") and non-interpreted strings (' ').  This does not work in R; ' '
> acts just like " " and my testing indicates that the interpretation is
> done at parse time.  Is there any language level construct for creating
> a non-interpreted string in R?
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Fri Jul 28 19:07:21 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 28 Jul 2006 13:07:21 -0400
Subject: [R] Non-interpreted strings
In-Reply-To: <971536df0607281005y20a98b98m22092f315a26971@mail.gmail.com>
References: <F12D8C78FA60A84AA6370216A74616E78AA937@msgswbiadsm33.wellsfargo.com>
	<971536df0607281005y20a98b98m22092f315a26971@mail.gmail.com>
Message-ID: <971536df0607281007u7bd3fa0fge36e49e849e32e40@mail.gmail.com>

Or even easier,

x <- scan("clipboard", what = "")

at least on Windows.


On 7/28/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Enter this at the console
>
>   x <- scan(what = "")
>
> and after pressing the enter after the right paren,
> do a paste and then press enter twice.
>
>
> On 7/27/06, douglas.stave at wellsfargo.com <douglas.stave at wellsfargo.com> wrote:
> > I am new to R, so please forgive me if there is an obvious answer to
> > this question.  I have done fairly extensive searching through R docs,
> > google and a few R users and have not found an answer to my question.
> >
> > Is there a way to create a non-interpreted string object in R?
> >
> > For example, I am using R in a MS Windows environment and I would like
> > to paste DOS paths into some R command:
> >        setwd("c:\some\directory")
> >
> > Obviously this does not work because of the escaping mechanism.  And I
> > know the obvious answer, use "\\".  But if you do a lot of pasting into
> > R it could get tedious manually editing escape sequences.
> >
> > I did find a workable solution to this particular problem:
> >        setwd(choose.dir())<Enter><Paste><Enter>
> > This saves me from having to do the editing myself.  I can conceive of
> > other examples of wanting to paste other more abstract stings into R
> > that may happen to have a \ in it.  And now, thanks to choose.dir(), I
> > have a way to do the translation automagically but...
> >
> > My question is, is there any way in R to not interpret the string and
> > store the string as is?  For instance, Perl allows you to do interpreted
> > (" ") and non-interpreted strings (' ').  This does not work in R; ' '
> > acts just like " " and my testing indicates that the interpretation is
> > done at parse time.  Is there any language level construct for creating
> > a non-interpreted string in R?
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From lcoggins at usgs.gov  Fri Jul 28 19:13:00 2006
From: lcoggins at usgs.gov (Lewis G Coggins)
Date: Fri, 28 Jul 2006 13:13:00 -0400
Subject: [R] Running R on a 64 bit processor
Message-ID: <OFB546859C.9897339A-ON852571B9.005BEFCB-852571B9.005E9251@usgs.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060728/962b9f46/attachment.pl 

From Horace.Tso at pgn.com  Fri Jul 28 19:21:25 2006
From: Horace.Tso at pgn.com (Horace Tso)
Date: Fri, 28 Jul 2006 10:21:25 -0700
Subject: [R] Extracting from a matrix w/o for-loop
Message-ID: <s4c9e531.019@pgn.com>

Unless there is another level of complexity that i didn't see here,
wouldn't it be a simply application of sapply as follow,

sapply( 1:dim(M2)[[1]], function(x) M1[M2[x,1], M2[x,2]] )

Hope this helps.

Horace



>>> "Camarda, Carlo Giovanni" <Camarda at demogr.mpg.de> 7/28/2006 9:40 AM
>>>
Dear R-users,

likely there is a simple solution for this problem, but I currently
cannot see it.

I basically would like to get from a matrix values in particular
positions which are the rows of another matrix, without using a
for-loop.

In other words: is there any way to avoid the for-loop in the
following
simple example:

M1 <- matrix(1:20, ncol=2)
M2 <- rbind(c(1,1), c(2,1), c(3,2), c(5,2), c(8,1))
v <- numeric(nrow(M2))
for(i in 1:length(v)){
    v[i] <- M1[M2[i,1], M2[i,2]]
}

Any suggestion would be welcome,

Ciao,
Carlo Giovanni Camarda

===========================================
Camarda Carlo Giovanni
PhD-Student
Max Planck Institute for Demographic Research
Konrad-Zuse-Strasse 1
18057 Rostock, Germany
Tel:  +49 (0)381 2081 172
Fax: +49 (0)381 2081 472
camarda at demogr.mpg.de 
===========================================


----------
This mail has been sent through the MPI for Demographic
Rese...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.


From wade.wall at gmail.com  Fri Jul 28 19:24:25 2006
From: wade.wall at gmail.com (Wade Wall)
Date: Fri, 28 Jul 2006 13:24:25 -0400
Subject: [R] subset of rows from matrix
Message-ID: <e23082be0607281024w3c4c4667l3e66984acdb0bb44@mail.gmail.com>

Hi all,

I have a dataframe of rownames that I would like to extract from a
larger matrix to form a new matrix. I have tried to use subset, in
this manner

x<-subset(largematrix, rownames$names=largematrix$rownames)

where largematrix is the larger matrix and rownames$names is the
dataframe with the row names of the rows I want to extract from the
larger matrix.

Of course, I get error messages.  Any suggestions how I should proceed?

Thanks

Wade


From jholtman at gmail.com  Fri Jul 28 19:31:33 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 28 Jul 2006 13:31:33 -0400
Subject: [R] subset of rows from matrix
In-Reply-To: <e23082be0607281024w3c4c4667l3e66984acdb0bb44@mail.gmail.com>
References: <e23082be0607281024w3c4c4667l3e66984acdb0bb44@mail.gmail.com>
Message-ID: <644e1f320607281031s5a33eb3do3cd9a5a7016fc82d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060728/aeb019b8/attachment.pl 

From jholtman at gmail.com  Fri Jul 28 19:34:01 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 28 Jul 2006 13:34:01 -0400
Subject: [R] subset of rows from matrix
In-Reply-To: <644e1f320607281031s5a33eb3do3cd9a5a7016fc82d@mail.gmail.com>
References: <e23082be0607281024w3c4c4667l3e66984acdb0bb44@mail.gmail.com>
	<644e1f320607281031s5a33eb3do3cd9a5a7016fc82d@mail.gmail.com>
Message-ID: <644e1f320607281034s1c5fd7d4o72b5fdbe5fb07957@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060728/90370675/attachment.pl 

From Thomas.Adams at noaa.gov  Fri Jul 28 19:58:15 2006
From: Thomas.Adams at noaa.gov (Thomas Adams)
Date: Fri, 28 Jul 2006 13:58:15 -0400
Subject: [R] Normal score transform of spatial data
Message-ID: <44CA5037.2080901@noaa.gov>

List:

I have 2 related questions:

(1) first I have x-y-z data, where x & y are the geographic locations of 
point values, z. I need to perform a normal score transform on the 
z-values and maintain their geographic location. So, how do I go from 
columns x-y-z to x-y-z-t (or x-y-t), where the t-values are the normal 
score transforms of the z-values? Can I use qnorm(ppoints(data)) to do 
this; how?

(2) I also need to do a reverse transform of a matrix of normal score 
values (v) to my original data value units; I have seen that I can use 
something like approx(t, x, v)$y. Again, the matrix represents a spatial 
grid. So, how do I maintain the same spatial ordering resulting from the 
reverse transform?

Thank you for your help.

Regards,
Tom


-- 
Thomas E Adams
National Weather Service
Ohio River Forecast Center
1901 South State Route 134
Wilmington, OH 45177

EMAIL:	thomas.adams at noaa.gov

VOICE:	937-383-0528
FAX:	937-383-0033


From mwgrant2001 at yahoo.com  Fri Jul 28 21:02:51 2006
From: mwgrant2001 at yahoo.com (Michael Grant)
Date: Fri, 28 Jul 2006 12:02:51 -0700 (PDT)
Subject: [R] Normal score transform of spatial data
In-Reply-To: <44CA5037.2080901@noaa.gov>
Message-ID: <20060728190251.99661.qmail@web52011.mail.yahoo.com>


Hi Tom,

I do not know about R but you might take a look at the GSLIB codes. It has
executables for both tasks using a simplfied GEOEAS format. I recently did some
Normal Score Kriging and use a combination of GSLIB, GEOEAS, and R (for the
contouring of the resulting grid). BTW I used GEOEAS instead of one of the R
packages because of the way the distance in each lag is calculated. I have not
explored the matter but my impression is some codes/packages use the mid-point
of each lag interval and some use the average pair distance for the
interval...need to look at that sometime [Moral: use packages with caution--not
because of any inherent flaw but because of differences in approach. You need
to know what is being done.]

Regards,
Michael Grant


--- Thomas Adams <Thomas.Adams at noaa.gov> wrote:

> List:
> 
> I have 2 related questions:
> 
> (1) first I have x-y-z data, where x & y are the geographic locations of 
> point values, z. I need to perform a normal score transform on the 
> z-values and maintain their geographic location. So, how do I go from 
> columns x-y-z to x-y-z-t (or x-y-t), where the t-values are the normal 
> score transforms of the z-values? Can I use qnorm(ppoints(data)) to do 
> this; how?
> 
> (2) I also need to do a reverse transform of a matrix of normal score 
> values (v) to my original data value units; I have seen that I can use 
> something like approx(t, x, v)$y. Again, the matrix represents a spatial 
> grid. So, how do I maintain the same spatial ordering resulting from the 
> reverse transform?
> 
> Thank you for your help.
> 
> Regards,
> Tom
> 
> 
> -- 
> Thomas E Adams
> National Weather Service
> Ohio River Forecast Center
> 1901 South State Route 134
> Wilmington, OH 45177
> 
> EMAIL:	thomas.adams at noaa.gov
> 
> VOICE:	937-383-0528
> FAX:	937-383-0033
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Fri Jul 28 21:37:03 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 28 Jul 2006 20:37:03 +0100 (BST)
Subject: [R] Running R on a 64 bit processor
In-Reply-To: <OFB546859C.9897339A-ON852571B9.005BEFCB-852571B9.005E9251@usgs.gov>
References: <OFB546859C.9897339A-ON852571B9.005BEFCB-852571B9.005E9251@usgs.gov>
Message-ID: <Pine.LNX.4.64.0607282027560.10512@gannet.stats.ox.ac.uk>

Be careful not to use clock speed as a measure of computer performance.
Pentium 4s (and the comparable Xeons) were intended to be run very fast, 
but never managed it.  So a 2.4GHz P4 proved to be slower than a 1GHz 
PIII.

Unless you are running Windows 64, the chip having some 64-bit 
instructions is irrelevant, and even if you were it is irrelevant to 
binary builds of R for Windows (which will actually access less memory 
than is possible under 32-bit Windows XP).

R runs large tasks much better under Linux, and there having a 64-bit CPU
and 64-bit OS pay off once you have 2Gb or more of RAM.

On Fri, 28 Jul 2006, Lewis G Coggins wrote:

> Greetings,
> 
> We recently obtained a new computer in our lab with a Pentium 4 3.86 GHz 
> processor and 4 gb of ram running windows xp with service pack 2.  After 
> installing R on this machine, I ran a bit of code and found that the 
> execution time was actually significantly slower than a machine running 
> windows xp with an older Pentium chip 1.73 GHz and 1 gb of ram.  After 
> speaking with the manufacturer of the new machine, I am told that the 
> processor in the new machine is 64 bit whereas I believe the processor in 
> the  old machine is 32 bit.  I have tried to sort through the 
> documentation on the CRAN page relative to performance of R under the 32 
> vs 64 bit sub architecture, however, I am no computer genius and find some 
> of this stuff extremely confusing. 

Well, it is under the Unix/Linux section, and you are running Windows 
so it does not apply to you.

>  In a CRAN document entitled 
> "Installation and Administration" , there is reference to sub 
> architecture... it reads:
> 
> 8.1 Windows
> Currently the Windows build of R is a 32-bit executable. This runs happily 
> on Windows 64 on AMD64 and EM64T, but is limited to (we are told) a 2GB 
> address space. It will not be possible to build a native version for 
> Windows 64 until suitable compilers are available, and currently 
> (mid-2006) that is not in prospect. 
> So my question is: are there any options to allow R to take advantage of 
> the faster chip, (with 64 bit architecture), and more ram.  I see in the 
> documentation that a linux version of R may be able to take advantage of 
> this chip... is that true what would be involved in making that work?  Are 
> there other options?  As we are beginning to use R more and more around 
> here, we may send this computer back and get a celeron 3.2 GHz chip that 
> has 32 bit architecture... is this an intelligent choice?
> Thanks in advance for considering my question,
> Lew Coggins 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From srini_iyyer_bio at yahoo.com  Fri Jul 28 22:21:01 2006
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Fri, 28 Jul 2006 13:21:01 -0700 (PDT)
Subject: [R] Comparing two matrices  [Broadcast]
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA028848AF@usctmx1106.merck.com>
Message-ID: <20060728202101.68310.qmail@web38113.mail.mud.yahoo.com>

Hi Andy, Thank you very much for your previous tip.  


However, I am running into another problem now. 


previously, in my tab-delim file I had only two
columns.  now I added another column. and I want the
pairs value to go into the matrix instead of 1. 
pair (Apple, S)  value 10.  

> > > tb # tab- delim file. 
> >       V1 V2 V3
> > 1  Apple  S 10
> > 2  Apple  A 12
> > 3  Apple  O 34
> > 4 Orange  A 34
> > 5 Orange  O 59
> > 6 Orange  S 30
> > 7  Mango  M 395
> > 8  Mango  A 495
> > 9  Mango  S 302


Now I am trying to do the following:
R> tbm <- as.matrix(tb) # turn it into a character
 matrix
R> tmat[cbind(match(tbm[,2],
rownames(tmat)),match(tbm[,1],colnames(tmat)))] <-
tbm[,3]

Error: NAs are not allowed in subscripted assignments


I get the above error. 

What is wrong with this. I am using R.2.2.1 on a 
Dell Latutite windows XP with 1GB RAM. 

Could any one please help me what am I doing wrong. 

thank you. 



--- "Liaw, Andy" <andy_liaw at merck.com> wrote:

> It might be a bit faster to do matrix indexing:
> 
> R> tbm <- as.matrix(tb) # turn it into a character
> matrix
> R> tmat[cbind(match(tbm[,2], rownames(tmat)),
> match(tbm[,1],
> colnames(tmat)))] <- 1
> > tmat
>   Apple Orange Mango Grape Star
> A     1      1     1     0    0
> O     1      1     0     0    0
> M     0      0     1     0    0
> G     0      0     0     0    0
> S     1      1     1     0    0
> 
> HTH,
> Andy
>  
> 
> From: Srinivas Iyyer
> > 
> > hi:
> > 
> > I have matrix with dimensions(200 X 20,000). I
> have another 
> > file, a tab-delim file where first column
> variables are row 
> > names and second column variables are column
> names. 
> > 
> > 
> > For instance:
> > 
> > > tmat
> >   Apple Orange Mango Grape Star
> > A     0      0     0     0    0
> > O     0      0     0     0    0
> > M     0      0     0     0    0
> > G     0      0     0     0    0
> > S     0      0     0     0    0
> > 
> > 
> > 
> > > tb # tab- delim file. 
> >       V1 V2
> > 1  Apple  S
> > 2  Apple  A
> > 3  Apple  O
> > 4 Orange  A
> > 5 Orange  O
> > 6 Orange  S
> > 7  Mango  M
> > 8  Mango  A
> > 9  Mango  S
> > 
> > 
> > I have to read each line of the 'tb' (tab delim
> file), take 
> > the first variable, check if matches any rowname
> of the 
> > matrix. Take the second variable of the row in and
> check if 
> > it matches any column name.  If so,  put
> > 1 else leave it. 
> > 
> > 
> > The following is a small piece of code that, I
> felt is a 
> > solutions. However, since my original matrix and
> tab-delim 
> > file is very very huge, I am not sure if it is
> really doing 
> > the correct thing. Could any one please help me if
> I am doing 
> > this correct. 
> > 
> > 
> > 
> > > for(i in 1:length(tb[,1])){
> > +  r = tb[i,1]
> > +  c = as.character(tb[i,2])
> > +  tmat[rownames(tmat)==c,colnames(tmat)==r] <-1 }
> > 
> > 
> > 
> > > tmat
> >   Apple Orange Mango Grape Star
> > A     1      1     1     0    0
> > O     1      1     0     0    0
> > M     0      0     1     0    0
> > G     0      0     0     0    0
> > S     1      1     1     0    0
> > 
> > 
> > 
> > Thanks.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
>
------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any
> attachments, contains information of Merck & Co.,
> Inc. (One Merck Drive, Whitehouse Station, New
> Jersey, USA 08889), and/or its affiliates (which may
> be known outside the United States as Merck Frosst,
> Merck Sharp & Dohme or MSD and in Japan, as Banyu)
> that may be confidential, proprietary copyrighted
> and/or legally privileged. It is intended solely for
> the use of the individual or entity named on this
> message.  If you are not the intended recipient, and
> have received this message in error, please notify
> us immediately by reply e-mail and then delete it
> from your system.
>
------------------------------------------------------------------------------
>


From gregor.gorjanc at bfro.uni-lj.si  Fri Jul 28 22:19:48 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Fri, 28 Jul 2006 20:19:48 +0000 (UTC)
Subject: [R] negative binomial lmer
References: <20060728020020.33116.qmail@web37508.mail.mud.yahoo.com>
	<loom.20060728T165048-123@post.gmane.org>
Message-ID: <loom.20060728T221525-180@post.gmane.org>

Ben Bolker <bolker <at> ufl.edu> writes:

...
>  I haven't tried it, but you could also consider using
> a Poisson-lognormal (rather than neg binomial, which is Poisson-gamma)
> distribution, which might make this all work rather well
> in lmer:
> 
> www.cefe.cnrs.fr/esp/TBElston_Parasitology2001.pdf

Actually it is very simple

lmer(y ~ effA + (1 | effB), family=quasipoisson)

i.e. this fits the following model

y_ijk ~ Poisson(\lambda_ijk)
log(lambda_ijk) = \mu + effaA_i + effB_ij + e_ijk
effB_i ~ Normal(0, \sigma^2_b)
e_ijk ~ Normal(0, \sigma^2_e)

Gregor


From br44114 at gmail.com  Fri Jul 28 22:35:34 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Fri, 28 Jul 2006 16:35:34 -0400
Subject: [R] scatter plot with axes drawn on the same scale
Message-ID: <8d5a36350607281335j4078f1bw13807c640ff69b49@mail.gmail.com>

Dear useRs,

I'd like to produce some scatter plots where N units on the X axis are
equal to N units on the Y axis (as measured with a ruler, on screen or
paper). This approach
  x <- sample(10:200,40) ; y <- sample(20:100,40)
  windows(width=max(x),height=max(y))
  plot(x,y)
is better than plot(x,y) but doesn't solve the problem because of the
other parameters (margins etc). Is there an easy, official way of
sizing the axes to the same scale, one that would also work with
multiple scatter plots being sent to the same pdf() - plus perhaps
layout() or par(mfrow())?

Thank you,
b.


From sundar.dorai-raj at pdf.com  Fri Jul 28 22:46:04 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 28 Jul 2006 15:46:04 -0500
Subject: [R] scatter plot with axes drawn on the same scale
In-Reply-To: <8d5a36350607281335j4078f1bw13807c640ff69b49@mail.gmail.com>
References: <8d5a36350607281335j4078f1bw13807c640ff69b49@mail.gmail.com>
Message-ID: <44CA778C.4080905@pdf.com>

Try:

plot(x, y, asp = 1)

--sundar

bogdan romocea wrote:
> Dear useRs,
> 
> I'd like to produce some scatter plots where N units on the X axis are
> equal to N units on the Y axis (as measured with a ruler, on screen or
> paper). This approach
>   x <- sample(10:200,40) ; y <- sample(20:100,40)
>   windows(width=max(x),height=max(y))
>   plot(x,y)
> is better than plot(x,y) but doesn't solve the problem because of the
> other parameters (margins etc). Is there an easy, official way of
> sizing the axes to the same scale, one that would also work with
> multiple scatter plots being sent to the same pdf() - plus perhaps
> layout() or par(mfrow())?
> 
> Thank you,
> b.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From msubianto at gmail.com  Fri Jul 28 22:47:25 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Fri, 28 Jul 2006 22:47:25 +0200
Subject: [R] convert decimals to fractions - sorted
In-Reply-To: <pan.2006.07.28.11.42.21.814865@troefpunt.nl>
References: <c7c17cef0607251002k6f6005e1h5b717d3fb23ff1b6@mail.gmail.com>
	<pan.2006.07.26.09.43.22.948661@troefpunt.nl>
	<c7c17cef0607260435l23376a10haa5d7cae28458fc2@mail.gmail.com>
	<pan.2006.07.28.11.42.21.814865@troefpunt.nl>
Message-ID: <c7c17cef0607281347u12f4e8abt91f6e4867941123a@mail.gmail.com>

Dear JeeBee and all,
It is nice. Thanks you very much.
I must learn much more about ?as.fractions, ?as.numeric, ?as.character
and ?table functions.

Best wishes, Muhammad Subianto


On 7/28/06, JeeBee <JeeBee at troefpunt.nl> wrote:
>
> Ah I see, I did not read your story well enough.
> You want to sort after applying table()
> Well, the idea I suggested was to keep the real numbers in,
> because the fractions are sorted as characters strings (alphabetically),
> which is not what you want. So, now I suggest the following:
>
> # First apply table()
> tmp1 <- as.data.frame(table(df))
>
> # Note that table() turned your numeric data into factors,
> # this might not be a handy approach, anyways, it is possible I guess.
> # You have to convert back using as.numeric(as.character(tmp1$V2))
> # or, more efficiently, as.numeric(levels(tmp1$V2))[tmp1$V2]
>
> # Add the column with the fractions
> tmp2 <- cbind(tmp1,
>         fracs=as.character(as.fractions(as.numeric(as.character(tmp1$V2)))))
>
> # Finally hide that sort colum if you want
>
> ( tmp2[-2] )
>
> # Everybody happy?
>
> JeeBee
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From A.Robinson at ms.unimelb.edu.au  Fri Jul 28 23:15:30 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sat, 29 Jul 2006 07:15:30 +1000
Subject: [R] Comparison of linear models
In-Reply-To: <200607281210.k6SCAL0c022170@erdos.math.unb.ca>
References: <200607281210.k6SCAL0c022170@erdos.math.unb.ca>
Message-ID: <20060728211530.GB59927@ms.unimelb.edu.au>

I have one addition to Rolf's thorough advice: if your goal is to try
to find evidence that the two procedures are equivalent then the
tests that you should consider are called equivalence tests.  These do
not come from lm.

The most popular test is TOST, the two one-sided test, and it doesn't
really require a package to implement.  Briefly, the alpha=0.05 test
might proceed as follows.

1) You establish a subjective interval around the value that you wish
   to test.  In the case of trying to assess the evidence that two
   population means for measured heights are the same, for example,
   you might say that the subjective interval for the difference
   between the two means is 0, +/- 2 cm.  The magnitude of the
   interval depends on what you think is an important deviation.

2) Compute two one-sided 1-alpha confidence intervals for the
   difference between the two means, one upper, and one lower.  Take
   the intersection of the two intervals.  (NB in this example it is
   mathematically equivalent to a single, two-sided 1-2*alpha
   confidence interval but this is only true in simple cases).

3) If the intersection is entirely within the subjective interval
   established in step 1) then you reject the null hypothesis of
   difference between the population means.

There is not very much literature on the question.  The originating
articles are:

@Article{schuirmann-1981, 
author = {D. L. Schuirmann},
title ={On hypothesis testing to determine if the mean of a normal distribution is contained in a known interval},
journal ={Biometrics},
year = 1981,
volume = 37,
pages =617
} 

@Article{westlake-1981, 
author = {W. J. Westlake},
title ={Response to {T.B.L. Kirkwood}: bioequivalence testing--a need to rethink},
journal ={Biometrics},
year = 1981,
volume = 37,
pages ={589--594} 
} 
 
I also recommend:

@Article{BH96:equivalence,
  author =       {R. L. Berger and J. C. Hsu},
  title =        {Bioequivalence trials, intersection-union tests and
  equivalenc
e confidence sets},
  journal =      {Statistical Science},
  year =         1996,
  volume =       11,
  number =       4,
  pages =        {283--319}
}

Finally, there is a nice recent book: 

@Book{W03:equivalence,
  author =       {S. Wellek},
  title =        {Testing statistical hypotheses of equivalence},
  publisher =    {Chapman and Hall/CRC},
  year =         2003
}

There is also an equivalence package on CRAN, which has some other
tests, graphical procedures, and references to some expository
articles (mine and others).

Cheers

Andrew

On Fri, Jul 28, 2006 at 09:10:21AM -0300, Rolf Turner wrote:
> 
> Fabien Lebugle wrote:
> 
> > I am a master student. I am currently doing an internship.  I would
> > like to get some advices about the following issue: I have 2 data
> > sets,  both containing the same variables, but the data were measured
> > using two different procedures. I want to know if the two procedures
> > are equivalent.  Up to know, I have built one linear model for each
> > dataset. The two models have the same form. I would like to compare
> > these two models: are they identical? Are they different? By how
> > much?
> > 
> > Please, could you tell me which R procedure I should use? I have been 
> > searching the list archive, but without success...
> 
> 	This is not a question of ``which R procedure'' but rather a
> 	question of understanding a bit about statistics and linear
> 	models.  You say you are a ``master's student''; I hope you
> 	are not a master's student in *statistics*, given that you
> 	lack this (very) basic knowledge!  If you are a student in
> 	some other discipline, I guess you may be forgiven.
> 
> 	The ``R procedure'' that you need to use is just lm()!
> 
> 	Briefly, what you need to do is combine your two data
> 	sets into a *single* data set (using rbind should work),
> 	add in a grouping variable (a factor with two levels,
> 	one for each measure procedure) e.g.
> 
> 		my.data$gp <- factor(rep(c(1,2),c(n1,n2)))
> 
> 	where n1 and n2 are the sample sizes for procedure 1 and
> 	procedure 2 respectively.
> 
> 	Then fit linear models with formulae involving the
> 	grouping factor (``gp'') as well as the other predictors,
> 	and test for the ``significance'' of the terms in
> 	the model that contain ``gp''.  You might start with
> 
> 		fit <- lm(y~.*gp,data=my.data)
> 		anova(fit)
> 
> 	where ``y'' is (of course) your reponse.
> 
> 	You ought to study up on the underlying ideas of inference
> 	for linear models, and the nature of ``factors''.  John Fox's
> 	book ``Applied Regression Analysis, Linear Models, and
> 	Related Methods'' might be a reasonable place to start.
> 
> 	Bon chance.
> 
> 				cheers,
> 
> 					Rolf Turner
> 					rolf at math.unb.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From johanfaux at yahoo.com  Sat Jul 29 00:04:56 2006
From: johanfaux at yahoo.com (johan Faux)
Date: Fri, 28 Jul 2006 15:04:56 -0700 (PDT)
Subject: [R] dyn.load, dyn.unload -memory management
In-Reply-To: <20060728211530.GB59927@ms.unimelb.edu.au>
Message-ID: <20060728220456.29325.qmail@web56211.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060728/19a3152c/attachment.pl 

From jaa53 at cornell.edu  Sat Jul 29 00:13:30 2006
From: jaa53 at cornell.edu (Jose Andres)
Date: Fri, 28 Jul 2006 18:13:30 -0400
Subject: [R] distance matrix
In-Reply-To: <e23082be0607281024w3c4c4667l3e66984acdb0bb44@mail.gmail.com>
References: <e23082be0607281024w3c4c4667l3e66984acdb0bb44@mail.gmail.com>
Message-ID: <D8143503-91C2-44E8-B487-1FB85BA84515@cornell.edu>

Hi all,

I have a set of points (1...n) each of them defined on a 3D space  
(i.e. for each point I do have  x,y,z coordinates), and I would like  
to generate a mean pairwise geometrical distance between them. Is  
there anyway I can do this with R?

Conceptually is really simple. First I have to generate a distance  
matrix (let's say "distmat") of pairwise distances (d= sqrt((x1-x2)^2 
+ (y1-y2)^2+(z1-z2)^2) and then calculate the mean -- mean.matrix 
(distmat) -- . However, I do not know how I can generate the distance  
matrix using R. I'd really appreciate all your suggestions/comments.

So, I have
	x		y		z

1	x1		y1		z1

2	x2		y2		z2

:	:		:		:
n	xn		yn		zn

and need

		1		2	..	n
1		-

2		d12		-

:
n		d1n		d2n		-

So I can calculate mean din



Thanks in advance!!!!!

Best,

/Jose


From bolker at ufl.edu  Sat Jul 29 00:15:20 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Fri, 28 Jul 2006 22:15:20 +0000 (UTC)
Subject: [R] scatter plot with axes drawn on the same scale
References: <8d5a36350607281335j4078f1bw13807c640ff69b49@mail.gmail.com>
	<44CA778C.4080905@pdf.com>
Message-ID: <loom.20060729T001451-813@post.gmane.org>

Sundar Dorai-Raj <sundar.dorai-raj <at> pdf.com> writes:

> 
> Try:
> 
> plot(x, y, asp = 1)
> 
> --sundar
> 
> 

  or eqscplot from the MASS package.


From bonnet at gmail.com  Sat Jul 29 01:26:14 2006
From: bonnet at gmail.com (Alexandre Bonnet)
Date: Fri, 28 Jul 2006 20:26:14 -0300
Subject: [R] maximum likelihood
Message-ID: <13815b2d0607281626n21889dc2y79940a9e1443ca38@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060728/bf6ae766/attachment.pl 

From xwang at aviaradx.com  Sat Jul 29 01:43:38 2006
From: xwang at aviaradx.com (Xianqun (Wilson) Wang)
Date: Fri, 28 Jul 2006 16:43:38 -0700
Subject: [R]  random effects with  lmer() and lme(), three random factors
Message-ID: <425B10826271C048A4F65FC6C810F5D60142B5@mail.arcturusag.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060728/6de51a50/attachment.pl 

From xwang at aviaradx.com  Sat Jul 29 01:58:00 2006
From: xwang at aviaradx.com (Xianqun (Wilson) Wang)
Date: Fri, 28 Jul 2006 16:58:00 -0700
Subject: [R] random effects with  lmer() and lme(), three random factors
Message-ID: <425B10826271C048A4F65FC6C810F5D60142B6@mail.arcturusag.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060728/c52c95be/attachment.pl 

From tlumley at u.washington.edu  Sat Jul 29 02:05:21 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 28 Jul 2006 17:05:21 -0700 (PDT)
Subject: [R] dyn.load, dyn.unload -memory management
In-Reply-To: <20060728220456.29325.qmail@web56211.mail.re3.yahoo.com>
References: <20060728220456.29325.qmail@web56211.mail.re3.yahoo.com>
Message-ID: <Pine.LNX.4.64.0607281659500.20442@homer22.u.washington.edu>

On Fri, 28 Jul 2006, johan Faux wrote:

> hello everybody,
>
> I have some code which looks like:
>
> dyn.load("lpSolve.so")
> res <- lp(some.parameters)
>
> and everything runs fine.
>
> What lp() does, it's just calling a C function which is in "lpSolve.so"
>
> If I call lp() a large number of times:
>
> for(1 in 1:5000){
> gc(verbose=TRUE)
> res <- lp(some.parameters)
> }
>
> then gc() is showing that the memory used by R process remain almost constant all the time. But the system memory used is going up _very fast_ and the above code never succeed because the memory used rich the limit.
>> From this I'm drawing the conclusion that there is some memory leak on the C code called by lp() which gc() cannot report since this is a C process.
>
> Does all this makes any sense or I'm wrong?


gc() does track some C memory usage, but not memory obtained directly 
from the C implementation by malloc().  The symptoms are consistent with 
the C code allocating via malloc()/calloc() and then not free()ing the 
memory.


>
> The next thing I tried was
>
> for(1 in 1:5000){
> gc(verbose=TRUE)
> dyn.load("lpSolve.so")
> res <- lp(some.parameters)
> dyn.unload("lpSolve.so")
> }
>
> But the problem is still there. The memory used is going up really fast. 
> Now my other question is: When I call dyn.unload() doesnt it terminate 
> the previous C process?  Next time in the loop, a new C process starts, 
> and I will not have any problem with memory.

No, this won't help at all.  dyn.unload() may or may not do anything on 
your system, but it certainly won't affect the way the C implementation 
handles memory.  The only ways to return memory allocated by malloc() are 
to do a free() for every allocated chunk or to quit R.

> P.S. lpSolve.so above is from package "lpSolve". I thought that getting 
> "lpSolve.so" and doing the above trick will solve my problem, but I have 
> been surely wrong.
>

Indeed.

You need to either report this to the lpsolve package maintainers (with a 
reproducible example) and hope that they will fix it or look at the source 
code to find out which allocations are not being freed and fix them.


 	-thomas


From bonnet at gmail.com  Sat Jul 29 02:39:36 2006
From: bonnet at gmail.com (Alexandre Bonnet)
Date: Fri, 28 Jul 2006 21:39:36 -0300
Subject: [R] maximum likelihood
Message-ID: <13815b2d0607281739g4a78e562s2dd273aea4bcaf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060728/91fe8782/attachment.pl 

From hb at stat.berkeley.edu  Sat Jul 29 03:14:20 2006
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Fri, 28 Jul 2006 18:14:20 -0700
Subject: [R] how to skip certain rows when reading data
In-Reply-To: <Pine.LNX.4.64.0607280738380.21917@gannet.stats.ox.ac.uk>
References: <Pine.GSO.4.58.0607122121010.9724@godzilla.acpub.duke.edu>
	<Pine.GSO.4.58.0607251454590.8258@godzilla.acpub.duke.edu>
	<Pine.GSO.4.58.0607271324570.16721@godzilla.acpub.duke.edu>
	<Pine.LNX.4.64.0607280738380.21917@gannet.stats.ox.ac.uk>
Message-ID: <59d7961d0607281814l20313503g578acefa48d782e@mail.gmail.com>

Have a look at readTable() in the R.utils package.  It can do quite a
few thinks like reading subsets of rows, specify colClasses by column
names etc.  Implementation was done so that memory usage is as small
as possible.  Note the note on the help page: "WARNING: This method is
very much in an alpha stage. Expect it to change.".  It should work
though.

Examples:

# Read every forth row
df <- readTable(pathname, rows=seq(from=1, to=1000, by=4));

# Read only columns 'chromosome' and 'position'.
df <- readTable(pathname, colClasses=c("chromosome"="character",
"position"="double"), defColClass="NULL", header=TRUE, sep="\t");

# Read 'log2' data chromosome by chromosome
chromosome <- readTableIndex(pathname, indexColumn=3, header=TRUE, sep="\t")
for (cc in unique(chromosome)) {
  rows <- which(chromosome == cc);
  df <- readTable(pathname, rows=rows, colClasses=c("log2"="double"),
defColClass="NULL", header=TRUE, sep="\t");
  ...
}

Cheers

Henrik

On 7/27/06, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Thu, 27 Jul 2006, jz7 at duke.edu wrote:
>
> > Dear all,
> >
> > I am reading the data using "read.table". However, there are a few rows I
> > want to skip. How can I do that in an easy way? Suppose I know the row
> > number that I want to skip. Thanks so much!
>
> The easy way is to read the whole data frame and using indexing (see `An
> Introduction to R') to remove the rows you do not want to retain.
> E.g. to remove rows 17 and 137
>
> mydf <- read.table(...)[-c(17, 137), ]
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From mchaudha at jhsph.edu  Sat Jul 29 04:02:39 2006
From: mchaudha at jhsph.edu (Ashraf Chaudhary)
Date: Fri, 28 Jul 2006 22:02:39 -0400
Subject: [R] Help with clogit in survival - conditional logistic regression
Message-ID: <200607290203.k6T233E2006861@hypatia.math.ethz.ch>

Dear All:
I have been struggling to run the conditional logistic regression on a
dataset. I am using clogit function in survival package. Here is how the
data is generated. I tried to shorted the data by using the second count
variable but still the clogit did not run. Why I am not able to run clogit
on my dataset. I appreciate any input.
Regards,
Ashraf

age  <- c(rep(0,8),rep(1,8))
scc <-  c(1,1,1,0,0,1,0,0,1,1,1,0,0,1,0,0)
case <- rep(c(1,0),8)
count <- c(5,5,216,216,110,110,40,40,5,5,308,308,212,212,21,21)
#count <- c(2,5,3,5,4,6,2,4,2,5,1,3,1,4,2,5)
agescc <- age*scc

# Grouped data
clmatch  <- data.frame(age,case,scc,agescc,count)

# Ungrouping the data as clogit as I can not use weights=count
clmatch1 <- data.frame(lapply(clmatch, function(x)
rep(x,clmatch$count)))[,1:4]

# Sorting by age and case for clogit - I think it is not necessary
ii <- order(clmatch1$age,clmatch1$case)
clmatch2 <- clmatch1[ii,]
clmatch2

clogit(case~scc+agescc+strata(age),data=clmatch2)


-----------------______________________________________
M. Ashraf Chaudhary, Ph.D.
Associate Scientist/Biostatistician
Department of International Health
Johns Hopkins Bloomberg School of Public Health
615 N. Wolfe St.  Room W5506
Baltimore MD 21205-2179

(410) 502-0741/fax: (410) 502-6733
mchaudha at jhsph.edu
Web:http://faculty.jhsph.edu/?F=Mohammad&L=Chaudhary


From deepayan.sarkar at gmail.com  Sat Jul 29 04:57:15 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 28 Jul 2006 21:57:15 -0500
Subject: [R] scatter plot with axes drawn on the same scale
In-Reply-To: <loom.20060729T001451-813@post.gmane.org>
References: <8d5a36350607281335j4078f1bw13807c640ff69b49@mail.gmail.com>
	<44CA778C.4080905@pdf.com> <loom.20060729T001451-813@post.gmane.org>
Message-ID: <eb555e660607281957k391fbc0ew279e0d29ef345492@mail.gmail.com>

On 7/28/06, Ben Bolker <bolker at ufl.edu> wrote:
> Sundar Dorai-Raj <sundar.dorai-raj <at> pdf.com> writes:
>
> >
> > Try:
> >
> > plot(x, y, asp = 1)
> >
> > --sundar
> >
> >
>
>   or eqscplot from the MASS package.

or

library(lattice)
xyplot(y ~ x, aspect = "iso")

-Deepayan


From dvrecko at sfu.ca  Sat Jul 29 06:43:38 2006
From: dvrecko at sfu.ca (dvrecko at sfu.ca)
Date: Fri, 28 Jul 2006 21:43:38 -0700
Subject: [R] DOE in R
Message-ID: <200607290443.k6T4hcsn018549@rm-rstar.sfu.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060728/ea0645f4/attachment.pl 

From torsten.mathies at matec-gmbh.com  Sat Jul 29 08:52:14 2006
From: torsten.mathies at matec-gmbh.com (Torsten Mathies)
Date: Sat, 29 Jul 2006 08:52:14 +0200
Subject: [R] boxcox transformation
Message-ID: <002201c6b2db$86d81d50$5a03a8c0@msc.de>

I've got a vector of data (hours to drive from a to b) y. 
 
After a qqplot I know, that they don't fit the normal probability. 
 
I would like to transform these data with the boxcox transformation
(MASS), that they fit the model.
 
When I try
 
ybx<-boxcox(y~1,0) 
qqnorm(ybx)
 
the plot is different from
 
library (TeachingDemos)
ybct<-bct(y,0) //
qqnorm(ybct)
 
How can I transform y to fit with the normal probability model?
 
 
Yours
Torsten


From john at emiliem.com  Sat Jul 29 00:02:38 2006
From: john at emiliem.com (John Morrow)
Date: Fri, 28 Jul 2006 15:02:38 -0700
Subject: [R] (no subject)
Message-ID: <002801c6b291$8a9db070$9fd91150$@com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060728/26a20d1e/attachment.pl 

From Max.Kuhn at pfizer.com  Sat Jul 29 03:44:06 2006
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Fri, 28 Jul 2006 21:44:06 -0400
Subject: [R] [R-pkgs] Version 0.4.3 of odfWeave
Message-ID: <71257D09F114DA4A8E134DEAC70F25D305B922AF@groamrexm03.amer.pfizer.com>

A new version of odfWeave is on CRAN. Changes include:

 - handling of locales. Errors were being produced when locales were set
to anything but "C". The fix changes the locale to "C" and changes back
to the original locale when the user's code is executed.

 - a bug fix for default plot device units (to prevent bitmap graphics
from being 480x480 inches on non-Windows systems).

 - a check was inserted at the beginning of the odfWeave function to
test for zipping/unzipping utilities. This is only done if the default
value for zipCmd in odfWeaveControl was not used. A more meaningful
error message is produced.

 - the manual is now a "legal" package vignette and accessible using
vignette("odfWeave").

Thanks to Manuel Morales, Ronggui, Andrew Robinson, Frank Harrell and
David Hajage for their help in finding bugs.

Please send any comments, questions, bugs etc.

Max Kuhn
Research Statistics
Pfizer Global R&D
Max.Kuhn at pfizer.com
----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From ripley at stats.ox.ac.uk  Sat Jul 29 10:41:03 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 29 Jul 2006 09:41:03 +0100 (BST)
Subject: [R] Non-interpreted strings
In-Reply-To: <F12D8C78FA60A84AA6370216A74616E78AA937@msgswbiadsm33.wellsfargo.com>
References: <F12D8C78FA60A84AA6370216A74616E78AA937@msgswbiadsm33.wellsfargo.com>
Message-ID: <Pine.LNX.4.64.0607282038590.10512@gannet.stats.ox.ac.uk>

On Thu, 27 Jul 2006, douglas.stave at wellsfargo.com wrote:

> I am new to R, so please forgive me if there is an obvious answer to
> this question.  I have done fairly extensive searching through R docs,
> google and a few R users and have not found an answer to my question.
> 
> Is there a way to create a non-interpreted string object in R?
> 
> For example, I am using R in a MS Windows environment and I would like
> to paste DOS paths into some R command:
> 	setwd("c:\some\directory")
> 
> Obviously this does not work because of the escaping mechanism.  And I
> know the obvious answer, use "\\".  But if you do a lot of pasting into
> R it could get tedious manually editing escape sequences.

Why manually edit?  You can do this many ways, including with R (reading 
from a file or from a separate line).  E.g.

> setwd(readline("new dir: "))
new dir: c:\TEMP
> getwd()
[1] "c:/TEMP"

> I did find a workable solution to this particular problem:
> 	setwd(choose.dir())<Enter><Paste><Enter>
> This saves me from having to do the editing myself.  I can conceive of
> other examples of wanting to paste other more abstract stings into R
> that may happen to have a \ in it.  And now, thanks to choose.dir(), I
> have a way to do the translation automagically but...
> 
> My question is, is there any way in R to not interpret the string and
> store the string as is?  For instance, Perl allows you to do interpreted
> (" ") and non-interpreted strings (' ').  This does not work in R; ' '
> acts just like " " and my testing indicates that the interpretation is
> done at parse time.  Is there any language level construct for creating
> a non-interpreted string in R?

No.

> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

[Please do, not send HTML code and BTW the answer is in the recent list 
archives.]
 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From phddas at yahoo.com  Sat Jul 29 11:19:12 2006
From: phddas at yahoo.com (Fred J.)
Date: Sat, 29 Jul 2006 02:19:12 -0700 (PDT)
Subject: [R] fancier plotting
Message-ID: <20060729091912.13285.qmail@web54613.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060729/cd2cc4d4/attachment.pl 

From adi at roda.ro  Sat Jul 29 12:50:01 2006
From: adi at roda.ro (Adrian DUSA)
Date: Sat, 29 Jul 2006 13:50:01 +0300
Subject: [R] Extracting from a matrix w/o for-loop
In-Reply-To: <s4c9e531.019@pgn.com>
References: <s4c9e531.019@pgn.com>
Message-ID: <200607291350.01830.adi@roda.ro>

Hi,

On Friday 28 July 2006 20:21, Horace Tso wrote:
> Unless there is another level of complexity that i didn't see here,
> wouldn't it be a simply application of sapply as follow,
>
> sapply( 1:dim(M2)[[1]], function(x) M1[M2[x,1], M2[x,2]] )

Andy's previous answer involving matrix indexing (M1[M2]) is simpler but just 
for the sake of it, since we're dealing with matrices it is not a case of 
sapply but of _apply_:

apply(M2, 1, function(x) M1[x[1], x[2]])

My 2c,
Adrian

-- 
Adrian DUSA
Arhiva Romana de Date Sociale
Bd. Schitu Magureanu nr.1
050025 Bucuresti sectorul 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From jholtman at gmail.com  Sat Jul 29 14:14:02 2006
From: jholtman at gmail.com (jim holtman)
Date: Sat, 29 Jul 2006 08:14:02 -0400
Subject: [R] fancier plotting
In-Reply-To: <20060729091912.13285.qmail@web54613.mail.yahoo.com>
References: <20060729091912.13285.qmail@web54613.mail.yahoo.com>
Message-ID: <644e1f320607290514v47e2c3ecnce4d2dc97b0e1407@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060729/86edaee1/attachment.pl 

From jholtman at gmail.com  Sat Jul 29 14:34:32 2006
From: jholtman at gmail.com (jim holtman)
Date: Sat, 29 Jul 2006 08:34:32 -0400
Subject: [R] (no subject)
In-Reply-To: <002801c6b291$8a9db070$9fd91150$@com>
References: <002801c6b291$8a9db070$9fd91150$@com>
Message-ID: <644e1f320607290534ub30ecdey95158d0e867a5a35@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060729/ff211257/attachment.pl 

From e.rapsomaniki at mail.cryst.bbk.ac.uk  Sat Jul 29 15:14:55 2006
From: e.rapsomaniki at mail.cryst.bbk.ac.uk (Eleni Rapsomaniki)
Date: Sat, 29 Jul 2006 14:14:55 +0100
Subject: [R] memory problems when combining randomForests
Message-ID: <1154178895.44cb5f4f5b9e7@webmail.cryst.bbk.ac.uk>

Hello again,

The reason why I thought the order at which rows are passed to randomForest
affect the error rate is because I get different results for different ways of
splitting my positive/negative data. 

First get the data (attached with this email)
pos.df=read.table("C:/Program Files/R/rw2011/pos.df", header=T)
neg.df=read.table("C:/Program Files/R/rw2011/neg.df", header=T)
library(randomForest)
#The first 2 columns are explanatory variables (which incidentally are not
discriminative at all if one looks at their distributions), the 3rd is the
class (pos or neg) 

train2test.ratio=8/10
min_len=min(nrow(pos.df), nrow(neg.df))
class_index=which(names(pos.df)=="class") #is the same for neg.df
train_size=as.integer(min_len*train2test.ratio)

############   Way 1
train.indicesP=sample(seq(1:nrow(pos.df)), size=train_size, replace=FALSE)
train.indicesN=sample(seq(1:nrow(neg.df)), size=train_size, replace=FALSE)

trainP=pos.df[train.indicesP,]
trainN=neg.df[train.indicesN,]
testP=pos.df[-train.indicesP,]
testN=neg.df[-train.indicesN,]

mydata.rf <- randomForest(x=rbind(trainP, trainN)[,-class_index],
y=rbind(trainP, trainN)[,class_index], xtest=rbind(testP,
testN)[,-class_index], ytest=rbind(testP, testN)[,class_index],
importance=TRUE,proximity=FALSE, keep.forest=FALSE)
mydata.rf$test$confusion

##############   Way 2
ind <- sample(2, min(nrow(pos.df), nrow(neg.df)), replace = TRUE,
prob=c(train2test.ratio, (1-train2test.ratio)))
trainP=pos.df[ind==1,]
trainN=neg.df[ind==1,]
testP=pos.df[ind==2,]
testN=neg.df[ind==2,]

mydata.rf <- randomForest(x=rbind(trainP, trainN)[,-dir_index], y=rbind(trainP,
trainN)[,dir_index], xtest=rbind(testP, testN)[,-dir_index], ytest=rbind(testP,
testN)[,dir_index], importance=TRUE,proximity=FALSE, keep.forest=FALSE)
mydata.rf$test$confusion

########### Way 3
subset_start=1
subset_end=subset_start+train_size
train_index=seq(subset_start:subset_end)
trainP=pos.df[train_index,]
trainN=neg.df[train_index,]
testP=pos.df[-train_index,]
testN=neg.df[-train_index,]

mydata.rf <- randomForest(x=rbind(trainP, trainN)[,-dir_index], y=rbind(trainP,
trainN)[,dir_index], xtest=rbind(testP, testN)[,-dir_index], ytest=rbind(testP,
testN)[,dir_index], importance=TRUE,proximity=FALSE, keep.forest=FALSE)
mydata.rf$test$confusion

########### end

The first 2 methods give me an abnormally low error rate (compared to what I
get using the same data on a naiveBayes method) while the last one seems more
realistic, but the difference in error rates is very significant. I need to use
the last method to cross-validate subsets of my data sequentially(the first two
methods use random rows throughout the length of the data), unless there is a
better way to do it (?). Something must be very different between the first 2
methods and the last, but which is the correct one?

I would greatly appreciate any suggestions on this!

Many Thanks
Eleni Rapsomaniki


From bolker at ufl.edu  Sat Jul 29 16:14:11 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Sat, 29 Jul 2006 14:14:11 +0000 (UTC)
Subject: [R] maximum likelihood
References: <13815b2d0607281739g4a78e562s2dd273aea4bcaf@mail.gmail.com>
Message-ID: <loom.20060729T160910-649@post.gmane.org>


Alexandre Bonnet <bonnet <at> gmail.com> writes:

> 
> *hi,*
> 
> *using articial data, i'm supposed to estimate model*
> 
> *y(t) = beta(1) + beta(2)*x(t) + u(t), u(t) = gamma*u(t-1) + v(t), t =
> 1,...,100*
> 
> *which is correctly specified in two ways: ML ommiting the first
> observation, and ML using all 100 observation.*
> 

 [etc.]

  Alexandre,

  I think you're not getting any answers to this question
because it's a bit too vague (we have no context as to
_why_ you want to do this -- it also sounds a bit like
a homework question ...), and because it's stated much more
as a general statistical methodology question than as
an R question.  Please read the posting guide ...

  Doing this without specifying any parametric forms
would be tricky.  You may be able to do this by
searching for autoregressive model methods in RSiteSearch ...

  Ben Bolker


From bates at stat.wisc.edu  Sat Jul 29 17:09:14 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 29 Jul 2006 10:09:14 -0500
Subject: [R] (no subject)
In-Reply-To: <644e1f320607290534ub30ecdey95158d0e867a5a35@mail.gmail.com>
References: <002801c6b291$8a9db070$9fd91150$@com>
	<644e1f320607290534ub30ecdey95158d0e867a5a35@mail.gmail.com>
Message-ID: <40e66e0b0607290809l16f7bba9p8de6803451c1a82a@mail.gmail.com>

On 7/29/06, jim holtman <jholtman at gmail.com> wrote:
> Is this what you want?
>
> > set.seed(1)
> > x <- matrix(sample(c(1, NA), 100, TRUE), nrow=10) # creat some data
> > x
>       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
>  [1,]    1    1   NA    1   NA    1   NA    1    1     1
>  [2,]    1    1    1   NA   NA   NA    1   NA   NA     1
>  [3,]   NA   NA   NA    1   NA    1    1    1    1    NA
>  [4,]   NA    1    1    1   NA    1    1    1    1    NA
>  [5,]    1   NA    1   NA   NA    1   NA    1   NA    NA
>  [6,]   NA    1    1   NA   NA    1    1   NA    1    NA
>  [7,]   NA   NA    1   NA    1    1    1   NA   NA     1
>  [8,]   NA   NA    1    1    1   NA   NA    1    1     1
>  [9,]   NA    1   NA   NA   NA   NA    1   NA    1    NA
> [10,]    1   NA    1    1   NA    1   NA   NA    1    NA
> > # count number of NAs per row
> > numNAs <- apply(x, 1, function(z) sum(is.na(z)))

It's a minor point but on a large matrix it would be better to use

numNAs <- rowSums(is.na(z))

> > numNAs
>  [1] 3 5 5 3 6 5 5 4 7 5
> > # remove rows with more than 5 NAs
> > x[!(numNAs > 5),]
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
> [1,]    1    1   NA    1   NA    1   NA    1    1     1
> [2,]    1    1    1   NA   NA   NA    1   NA   NA     1
> [3,]   NA   NA   NA    1   NA    1    1    1    1    NA
> [4,]   NA    1    1    1   NA    1    1    1    1    NA
> [5,]   NA    1    1   NA   NA    1    1   NA    1    NA
> [6,]   NA   NA    1   NA    1    1    1   NA   NA     1
> [7,]   NA   NA    1    1    1   NA   NA    1    1     1
> [8,]    1   NA    1    1   NA    1   NA   NA    1    NA
> >
>
>
>
> On 7/28/06, John Morrow <john at emiliem.com> wrote:
> >
> > Dear R-Helpers,
> >
> > I have a large data matrix (9707 rows, 60 columns), which contains missing
> > data. The matrix looks something like this:
> >
> > 1) X X X X X X  NA  X X X X X X X X X
> >
> > 2) NA NA NA NA X NA NA NA X NA NA
> >
> > 3) NA NA X NA NA NA NA NA NA NA
> >
> > 5) NA X NA X X X NA X X X X NA X
> >
> > ..
> >
> > 9708) X NA NA X NA NA X X NA NA X
> >
> > .and so on. Notice that every row has a varying number of entries, all
> > rows
> > have at least one entry, but some rows have too much missing data.  My
> > goal
> > is to filter out/remove rows that have ~5 (this number is yet to be
> > determined, but let's say its 5) missing entries before I run pearsons to
> > tell me correlation between all of the rows.  The order of the columns
> > does
> > not matter here.
> > I think that I might need to test each row for a "data, at least one NA,
> > data" pattern?
> >
> > Is there some kind of way of doing this? I am at a loss for an easy way to
> > accomplishing this. Any suggestions are most appreciated!
> >
> > John Morrow
> >
> >
> >
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bates at stat.wisc.edu  Sat Jul 29 17:25:45 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 29 Jul 2006 10:25:45 -0500
Subject: [R] negative binomial lmer
In-Reply-To: <loom.20060728T221525-180@post.gmane.org>
References: <20060728020020.33116.qmail@web37508.mail.mud.yahoo.com>
	<loom.20060728T165048-123@post.gmane.org>
	<loom.20060728T221525-180@post.gmane.org>
Message-ID: <40e66e0b0607290825g14e2d70aq834b7763f56307a6@mail.gmail.com>

On 7/28/06, Gregor Gorjanc <gregor.gorjanc at bfro.uni-lj.si> wrote:
> Ben Bolker <bolker <at> ufl.edu> writes:
>
> ...
> >  I haven't tried it, but you could also consider using
> > a Poisson-lognormal (rather than neg binomial, which is Poisson-gamma)
> > distribution, which might make this all work rather well
> > in lmer:
> >
> > www.cefe.cnrs.fr/esp/TBElston_Parasitology2001.pdf
>
> Actually it is very simple
>
> lmer(y ~ effA + (1 | effB), family=quasipoisson)
>
> i.e. this fits the following model
>
> y_ijk ~ Poisson(\lambda_ijk)
> log(lambda_ijk) = \mu + effaA_i + effB_ij + e_ijk
> effB_i ~ Normal(0, \sigma^2_b)
> e_ijk ~ Normal(0, \sigma^2_e)
>
> Gregor

I would advise checking the results from lmer against those from
another way of fitting this model or the negative binomial model.
There may be a problem in the way that lmer handles the scale
parameter.  I haven't checked  generalized linear mixed models with a
scale parameter as extensively as I have checked those without a
separate scale parameter (the binomial and Poisson families).  If
anyone can provide me with an example of such a model and sample data
(preferably off-list) I would appreciate it.


From bates at stat.wisc.edu  Sat Jul 29 17:51:10 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 29 Jul 2006 10:51:10 -0500
Subject: [R] random effects with lmer() and lme(), three random factors
In-Reply-To: <425B10826271C048A4F65FC6C810F5D60142B5@mail.arcturusag.local>
References: <425B10826271C048A4F65FC6C810F5D60142B5@mail.arcturusag.local>
Message-ID: <40e66e0b0607290851h6bb774c9we18072da15e8b574@mail.gmail.com>

On 7/28/06, Xianqun (Wilson) Wang <xwang at aviaradx.com> wrote:
> Hi, all,

> I have a question about random effects model. I am dealing with a
> three-factor experiment dataset. The response variable y is modeled
> against three factors: Samples, Operators, and Runs. The experimental
> design is as follow:

> 4 samples were randomly chosen from a large pool of test samples. Each
> of the 4 samples was analyzed by 4 operators, randomly selected from a
> group of operators. Each operator independently analyzed same samples
> over 5 runs (runs nested in operator). I would like to know the
> following things:

> (1)                     the standard deviation within each run;

> (2)                     the standard deviation between runs;

> (3)                     the standard deviation within operator

> (4)                     the standard deviation between operator.

> With this data, I assumed the three factors are all random effects. So
> the model I am looking for is

> Model:  y  = Samples(random) + Operator(random) + Operator:Run(random) +
> Error(Operator) + Error(Operator:Run)  + Residuals

> I am using lme function in nlme package. Here is the R code I have

> 1.       using lme:

> First I created a grouped data using

> gx <- groupedData(y ~ 1 | Sample, data=x)

> gx$dummy <- factor(rep(1,nrow(gx)))

> then I run the lme

> fm<- lme(y ~ 1, data=gx,
> random=list(dummy=pdBlocked(list(pdIdent(~Sample-1),
>             pdIdent(~Operator-1),
>             pdIdent(~Operator:Run-1)))))

>     finally, I use VarCorr to extract the variance components

>            vc <- VarCorr(fm)
>                      Variance           StdDev
> Operator:Run 1.595713e-10:20   1.263215e-05:20
> Sample       5.035235e+00: 4   2.243933e+00: 4
> Operator     5.483145e-04: 4   2.341612e-02: 4
> Residuals    8.543601e-02: 1   2.922944e-01: 1

> 2.      Using lmer in Matrix package:

> fm <- lmer(y ~ (1 | Sample) + (1 | Operator) +
>            (1|Operator:Run), data=x)

That model specification can now be written as

fm <- lmer(y ~ (1|Sample) + (1|Operator/Run), x)

>      summary(fm)

> Linear mixed-effects model fit by REML
> Formula: H.I.Index ~ (1 | Sample.Name) + (1 | Operator) + (1 |
> Operator:Run)
>           Data: x
>       AIC      BIC    logLik MLdeviance REMLdeviance
>  96.73522 109.0108 -44.36761   90.80064     88.73522
> Random effects:
>  Groups       Name        Variance   Std.Dev.
>  Operator:Run (Intercept) 4.2718e-11 6.5359e-06
>  Operator     (Intercept) 5.4821e-04 2.3414e-02
>  Sample       (Intercept) 5.0352e+00 2.2439e+00
>  Residual                 8.5436e-02 2.9229e-01
> number of obs: 159, groups: Operator:Run, 20; Operator, 4; Sample.Name,
> 4

> Fixed effects:
>              Estimate Std. Error  t value
> (Intercept) 0.0020818  1.1222683 0.001855

> There is a difference between lmer and lme is for the factor
> Operator:Run.

It's just a matter of round-off.  It is possible for the ML or REML
estimates of a variance component to be zero, as is the case here, but
the current computational methods do not allow the value zero because
this will cause some of the matrix decompositions to fail.  In lmer we
use a constrained optimization with the relative variance (variance of
a random effect divided by the residual variance) constrained to be
greater than or equal to 5e-10, which is exactly the value you have
here.

I'll add code to the model fitting routine to warn the user when
convergence to the boundary value occurs.  I haven't done that in the
past because it is not always easy to explain what is occurring.
For a model with variance components only, like yours, convergence on
the boundary means that an estimated variance component is zero.  In
the case of bivariate or multivariate random effects the
variance-covariance matrix can be singular without either of the
variances being zero.

The bottom line for you is that the estimated variance for
Operator:Run is zero and you should reduce the model to y ~ (1|Sample)
+ (1|Operator)


I cannot find where the problem is. Could anyone point me
> out if my model specification is correct for the problem I am dealing
> with. I am pretty new user to lme and lmer. Thanks for your help!
>
>
>
>
>
> Wilson Wang
>
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From lorenzo.isella at gmail.com  Sat Jul 29 23:19:41 2006
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Sat, 29 Jul 2006 23:19:41 +0200
Subject: [R] Colours in Lattice
In-Reply-To: <mailman.9.1154167203.3941.r-help@stat.math.ethz.ch>
References: <mailman.9.1154167203.3941.r-help@stat.math.ethz.ch>
Message-ID: <44CBD0ED.1030500@gmail.com>

Dear All,

I am practicing with the image and wireframe (the latter in the lattice 
package) plotting tools.
I am a bit puzzled by the colors I observe in some test plots I have 
been generating.
Consider:

rm(list=ls())
library(lattice)
x <- seq(-2*pi, 2*pi, len = 100)
y <- seq(-2*pi, 2*pi, len = 100)
g <- expand.grid(x = x, y = y)
mylen<-length(x)


g$z <- exp(-(g$x^2 + g$y^2))+exp(-((g$x-2)^2 + (g$y-2)^2))

pdf("my-first-lattice-figure.pdf")
print(wireframe(z ~ x * y, g, drape = TRUE,shade=TRUE,scales = 
list(arrows = FALSE),pretty=FALSE, aspect = c(1,1), colorkey = TRUE
,zoom=0.8, zlab = list("my radial function",rot = 
90),distance=0.0,perspective=TRUE,screen = list(z = 150, x = -55,y= 
0),ylim=range(c(-2*pi,2*pi)),xlim=range(c(-2*pi,2*pi)),zlim=range(c(0,1))))
dev.off()

Now, I should have two peaks corresponding to values slightly above 1 
which should be  cyan, whereas I see a different color. Furthermore, the 
"floor" of the function should correspond to about zero,  so I would 
expect it to be light purple rather than dark red.
Am I making any mistake with the options in the plotting? I suppose it 
all has to do with the shade argument, but using shade=FALSE leads to a 
very ugly-looking plot.
Is it possible to overlap another function to the previous one? I am 
thinking about something like the "lines" command when I use "plot".
Finally, when I create a plot using "image", is there a way to have a 
legenda of the values corresponding to each color (like the "colorkey" 
argument in wireframe)?
Kind Regards

Lorenzo


From ggrothendieck at gmail.com  Sat Jul 29 23:20:29 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 29 Jul 2006 17:20:29 -0400
Subject: [R] placing rectangle behind plot
Message-ID: <971536df0607291420r456bccdfwd809fceaa3b98c17@mail.gmail.com>

I am trying to create a lattice plot and would like to later, i.e. after
the plot is drawn, add a grey rectangle behind a portion of it.
The following works except that the rectrangle is on top of and
obscures a portion of the chart.  I also tried adding col = "transparent"
to the gpar list but that did not help -- I am on windows and
perhaps the windows device does not support transparency?
At any rate, how can I place the rectangle behind the plotted
points without drawing the rectangle first?

library(lattice)
library(grid)
trellis.unfocus()
x <- 1:10
xyplot(x ~ x | gl(2,1), layout = 1:2)
trellis.focus("panel", 1, 1)
grid.rect(w = .5, gp = gpar(fill = "light grey"))
trellis.unfocus()


From m_nurza at yahoo.com  Sat Jul 29 23:52:05 2006
From: m_nurza at yahoo.com (nurza m)
Date: Sat, 29 Jul 2006 22:52:05 +0100 (BST)
Subject: [R] uniroot
Message-ID: <20060729215205.15794.qmail@web33608.mail.mud.yahoo.com>

Hello,

I am struggling to find the root of a exponent 
function.
"uniroot" is complaining about a values at end points
not of opposite sign?

 
s<- sapply(1:length(w),function(i)
+  {
+  
+ + 
+ 
+
uniroot(saeqn,lower=-5000,upper=0.01036597923,l=list(t=w[i],gp=gp))$root
+  })
Error in uniroot(saeqn, lower = -5000, upper =
0.01036597923, l = list(t = w[i],  : 
        f() values at end points not of opposite sign
> 


 
and here is my fonction "saeqn".


> saeqn<-function(s,l)
+  {
+ 
+ 
+ p<- exp(-l$gp$lambda+s)*l$gp$c
+ 
+ 
+
k11<-(l$gp$mu*(l$gp$lambda^2)*l$gp$c-s*l$gp$lambda*l$gp$c*l$gp$mu+l$gp$mu*l$gp$lambda)*p
+ 
+ k12 <-
-l$gp$mu*l$gp$lambda-s^2+2*s*l$gp$lambda-(l$gp$lambda^2)
+ 
+ k13 <-k11+k12
+ 
+
k14<-(l$gp$lambda-s)*(-l$gp$mu*s-s*l$gp$lambda+s^2+l$gp$mu*l$gp$lambda*p)
+ 
+ k1<- -k13/k14
+ 
+  k1-l$t
+  }

 

. There is something I must be missing since I never
had 
luck with "uniroot"!

Thanks,


From kartik.pappu at gmail.com  Sun Jul 30 03:16:51 2006
From: kartik.pappu at gmail.com (Kartik Pappu)
Date: Sat, 29 Jul 2006 18:16:51 -0700
Subject: [R] Reading multiple txt files into one data frame
Message-ID: <97faa3210607291816g4d282eb2j2f3894b1c1a6957@mail.gmail.com>

Hello All,

I have a device that spews out experimental data as a series of text
files each of which contains one column with several rows of numeric
data.  My problem is that for each trial it gives me one text file
(and I run between 30 to 50 trials at a time) and I would ideally like
to merge all these text files into one large data frame with each
column representing a single trial.  It is not a problem if NA
characters are added to make all the columna of eaqual length.  Right
now I am doing this by opening each file individually and cutting and
pasting the data into an excel file.  How can I do this in R assuming
all my text files are in one directory.

Is it also possible to customize the column headers.  For example if I
have 32 trials and 16 are experimental and 16 are control and I want
to name the columns "Expt1", Expt2",... "Expt16"  and the control
columns "Cntl1",...Cntl16".

Kartik


From jholtman at gmail.com  Sun Jul 30 04:09:42 2006
From: jholtman at gmail.com (jim holtman)
Date: Sat, 29 Jul 2006 22:09:42 -0400
Subject: [R] Reading multiple txt files into one data frame
In-Reply-To: <97faa3210607291816g4d282eb2j2f3894b1c1a6957@mail.gmail.com>
References: <97faa3210607291816g4d282eb2j2f3894b1c1a6957@mail.gmail.com>
Message-ID: <644e1f320607291909p18fd7bd8hb1ab9f695d852d23@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060729/5c64cf66/attachment.pl 

From kartik.pappu at gmail.com  Sun Jul 30 04:16:22 2006
From: kartik.pappu at gmail.com (Kartik Pappu)
Date: Sat, 29 Jul 2006 19:16:22 -0700
Subject: [R] Log color scale
Message-ID: <97faa3210607291916r2fade731w65e3b79fb37d6a1b@mail.gmail.com>

Hi all,

In response to a previous post about plotting a numeric square matrix
as a colored matrix, I was referred to both image and the
color2D.matplot function in the plotrix package.  Both have worked for
me thanks!!

However I need to plot my data in a log transformed color scale.  Is
this possible?  I will be happy to explain further, but basically I
need to do this because there are large variations in the max and min
values of my raw data and I am trying to highlight the differences in
the values at the lower end of my raw data (while still displaying the
values at the high end of the spectrum for comparison) so if figured
the best way to represent this on a (RGB) color scale is to do it with
a log transform.  I do not want to use too many colors because that
make the figure too busy.

Any suggestions on how to achieve this.

Kartik


From spluque at gmail.com  Sun Jul 30 05:06:22 2006
From: spluque at gmail.com (Sebastian P. Luque)
Date: Sat, 29 Jul 2006 22:06:22 -0500
Subject: [R] placing rectangle behind plot
References: <971536df0607291420r456bccdfwd809fceaa3b98c17@mail.gmail.com>
Message-ID: <87hd0zyeoh.fsf@arctocephalus.homelinux.org>

Hi Gabor,


On Sat, 29 Jul 2006 17:20:29 -0400,
"Gabor Grothendieck" <ggrothendieck at gmail.com> wrote:

> I am trying to create a lattice plot and would like to later, i.e. after
> the plot is drawn, add a grey rectangle behind a portion of it.  The
> following works except that the rectrangle is on top of and obscures a
> portion of the chart.  I also tried adding col = "transparent" to the
> gpar list but that did not help -- I am on windows and perhaps the
> windows device does not support transparency?  At any rate, how can I
> place the rectangle behind the plotted points without drawing the
> rectangle first?

If you only need to draw the rectangle behind the points, why not
'panel.polygon' before 'panel.xyplot'?


xyplot(x ~ x | gl(2, 1), layout=1:2,
       panel=function(x, y, ...) {
           panel.polygon(c(3, 3, 8, 8), c(0, 12, 12, 0), col=2)
           panel.xyplot(x, y, ...)
       })


Cheers,

-- 
Seb


From ggrothendieck at gmail.com  Sun Jul 30 05:19:32 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 29 Jul 2006 23:19:32 -0400
Subject: [R] placing rectangle behind plot
In-Reply-To: <87hd0zyeoh.fsf@arctocephalus.homelinux.org>
References: <971536df0607291420r456bccdfwd809fceaa3b98c17@mail.gmail.com>
	<87hd0zyeoh.fsf@arctocephalus.homelinux.org>
Message-ID: <971536df0607292019g51b555fbh8a89963ce375e132@mail.gmail.com>

The reason I explicitly specified in the problem that the rectangle should
not be drawn first is that the xyplot is issued as part of a
larger routine that I don't want to modify.

On 7/29/06, Sebastian P. Luque <spluque at gmail.com> wrote:
> Hi Gabor,
>
>
> On Sat, 29 Jul 2006 17:20:29 -0400,
> "Gabor Grothendieck" <ggrothendieck at gmail.com> wrote:
>
> > I am trying to create a lattice plot and would like to later, i.e. after
> > the plot is drawn, add a grey rectangle behind a portion of it.  The
> > following works except that the rectrangle is on top of and obscures a
> > portion of the chart.  I also tried adding col = "transparent" to the
> > gpar list but that did not help -- I am on windows and perhaps the
> > windows device does not support transparency?  At any rate, how can I
> > place the rectangle behind the plotted points without drawing the
> > rectangle first?
>
> If you only need to draw the rectangle behind the points, why not
> 'panel.polygon' before 'panel.xyplot'?
>
>
> xyplot(x ~ x | gl(2, 1), layout=1:2,
>       panel=function(x, y, ...) {
>           panel.polygon(c(3, 3, 8, 8), c(0, 12, 12, 0), col=2)
>           panel.xyplot(x, y, ...)
>       })
>
>
> Cheers,
>
> --
> Seb
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From spencer.graves at pdf.com  Sun Jul 30 05:53:12 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 30 Jul 2006 11:53:12 +0800
Subject: [R] nested repeated measures in R
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E04E59F@dc1ex01.air.org>
References: <200607220051.k6M0psvX023888@badlands.software.umn.edu>
	<2323A6D37908A847A7C32F1E3662C80E04E59F@dc1ex01.air.org>
Message-ID: <44CC2D28.7060005@pdf.com>

	  For nested, repeated measures of normally distributed data, the best 
capability available in R (and perhaps anywhere) is in the 'nlme' 
package.  Excellent documentation is available in Pinheiro and Bates 
(2000) Mixed-Effects Models in S and S-Plus (Springer).  This book is 
also a superlative research monograph by one of the leading contributors 
in this area (Bates) and one of his former graduate students (Pinheiro). 
  If your data are not normal, you may want to use 'lmer' in the 'lme4' 
and 'Matrix' packages;  if you need that, please search the archives for 
more information on that.

	  For help getting data into R, if you have R installed, "help.start()" 
provides easy access to standard documentation shipped with R and 
available without Internet access.  See especially "R Data 
Import/Export" and "An Introduction to R".

	  The simplest way I've found to get data into R is to first store it 
something like "*.csv" or "*.txt" format, plain text with a unique "sep" 
character that is only used to separate fields.  Then I store the file 
name (with the path if different from 'getwd()') in a variable like 
'File'.  Note that "\" is an escape character in R, and to get a 
character interpreted as "\", you must use either "\" or "/".

	  Then I use "readLines" to check the format, including identifying the 
"sep" character.  Then I use "count.fields" to determine the number of 
fields in each record and the number of records.  If I don't get the 
right answer there, I may not have the correct "sep" character.  Or I 
need to read the file in pieces, because of problems in the file.

	  Then I use "read.data".  It has many arguments that can be used to 
modify the "sep" character, skip lines at the beginning of the file and 
then read only a certain number of lines, per the output from 
"count.fields".

	  Hope this helps.
	  Spencer Graves

Doran, Harold wrote:
> You can read the manual, read the FAQs, search the help functions and the archives.
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch on behalf of Eric C Merten
> Sent: Fri 7/21/2006 8:51 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] nested repeated measures in R
>  
> R help,
> 
> How would I input data, verify assumptions, and run a nested repeated
> measures ANOVA using R?  I have STATION nested in SITE nested in BLOCK with
> measurements repeated for five YEARs.  All are random variables and it's only
> slightly unbalanced.  I'm trying to characterize spatiotemporal variation in
> stream habitat variables.  Thanks for your help!
> 
> Eric Merten
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From kantanantha at hotmail.com  Sun Jul 30 06:33:14 2006
From: kantanantha at hotmail.com (Nantachai Kantanantha)
Date: Sun, 30 Jul 2006 00:33:14 -0400
Subject: [R] Question about data used to fit the mixed model
Message-ID: <BAY106-F1324849CBE25BB139CBA06A25F0@phx.gbl>

Hi everyone,

I would like to ask a question regarding to the data used to fit the mixed 
model.

I wonder that, for the response variable data used to fit the mixed model 
(either via "spm" or "lme"), we must have several observations per subject 
(i.e. Yij,  i = 1,..,M,  j = 1,.., ni) or it can be just one observation per 
subject (i.e. Yi,  i = 1,...,M). Since we have to specify the groups for 
random effect components, if we have only one observation per subject, then 
each group will have only one observation.

Thank you vert much for your help.
Sincerely yours,

Nantachai


From vincent at 7d4.com  Sun Jul 30 10:12:52 2006
From: vincent at 7d4.com (vincent at 7d4.com)
Date: Sun, 30 Jul 2006 10:12:52 +0200
Subject: [R] Log color scale
In-Reply-To: <97faa3210607291916r2fade731w65e3b79fb37d6a1b@mail.gmail.com>
References: <97faa3210607291916r2fade731w65e3b79fb37d6a1b@mail.gmail.com>
Message-ID: <44CC6A04.9070103@7d4.com>

Kartik Pappu a ?crit :

> However I need to plot my data in a log transformed color scale.  Is
> this possible?  I will be happy to explain further, but basically I
> need to do this because there are large variations in the max and min
> values of my raw data and I am trying to highlight the differences in
> the values at the lower end of my raw data (while still displaying the
> values at the high end of the spectrum for comparison) so if figured
> the best way to represent this on a (RGB) color scale is to do it with
> a log transform.  I do not want to use too many colors because that
> make the figure too busy.
> Any suggestions on how to achieve this.

Don't use too much time to search a good palette, build it yourself.
build your own color palette, specific to your problem.
It's not very difficult, see ?rgb et al.

a small example (not a log scale) :
palblancbleu  = rgb(15:0, 15:0, 15, max=15);
palrougeblanc = rgb(15, 0:15, 0:15, max=15);
palblanc      = rep(rgb(1,1,1), 8);
palRBB        = c(palrougeblanc, palblanc, palblancbleu);

#palRBB = c(
#"#FF0000","#FF1111","#FF2222","#FF3333","#FF4444","#FF5555","#FF6666","#FF7777"
#,"#FF8888","#FF9999","#FFAAAA","#FFBBBB","#FFCCCC","#FFDDDD","#FFEEEE","#FFFFFF"
#,"#FFFFFF","#FFFFFF","#FFFFFF","#FFFFFF","#FFFFFF","#FFFFFF","#FFFFFF","#FFFFFF"
#,"#FFFFFF","#EEEEFF","#DDDDFF","#CCCCFF","#BBBBFF","#AAAAFF","#9999FF","#8888FF"
#,"#7777FF","#6666FF","#5555FF","#4444FF","#3333FF","#2222FF","#1111FF","#0000FF");

# to visualize it
# to visualize a palette

showpal = function(pal=palRBB)
{
n = 200;
fp = matrix(0,n,1);
for (i in 1:n) fp[i,] = i;

par(mar=c(0,0,0,0), mgp=c(0,0,0), ann=F);
image(fp, col=pal);
}

(images at http://7d4.com/r/pal.php)


From ligges at statistik.uni-dortmund.de  Sun Jul 30 12:22:13 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 30 Jul 2006 12:22:13 +0200
Subject: [R] uniroot
In-Reply-To: <20060729215205.15794.qmail@web33608.mail.mud.yahoo.com>
References: <20060729215205.15794.qmail@web33608.mail.mud.yahoo.com>
Message-ID: <44CC8855.20103@statistik.uni-dortmund.de>

nurza m wrote:
> Hello,
> 
> I am struggling to find the root of a exponent 
> function.
> "uniroot" is complaining about a values at end points
> not of opposite sign?

And you think it is not the case? Why?
We cannot help because you have not given a reproducible example (What 
is w and gp?), which the posting guide asks you to do.

Please also format your code in a way that is easy for others to read, 
including some blanks and without all those ">" and "+":


saeqn <- function(s, l)
{
     p <- exp(-l$gp$lambda + s) * l$gp$c
     k11 <- (l$gp$mu * (l$gp$lambda^2) * l$gp$c -
             s * l$gp$lambda * l$gp$c * l$gp$mu +
             l$gp$mu * l$gp$lambda) * p
     k12 <- -l$gp$mu * l$gp$lambda -
            s^2 + 2 * s * l$gp$lambda - (l$gp$lambda^2)
     k13 <- k11 + k12
     k14 <- (l$gp$lambda - s) *
            (-l$gp$mu * s - s * l$gp$lambda + s^2 +
             l$gp$mu * l$gp$lambda * p)
     k1 <- -k13 / k14
     k1 - l$t
}


s <- sapply(seq(along = w), function(i) {
     uniroot(saeqn, lower = -5000, upper = 0.01036597923,
             l = list(t = w[i], gp = gp))$root
     }
)



>  
> s<- sapply(1:length(w),function(i)
> +  {
> +  
> + + 
> + 
> +
> uniroot(saeqn,lower=-5000,upper=0.01036597923,l=list(t=w[i],gp=gp))$root
> +  })
> Error in uniroot(saeqn, lower = -5000, upper =
> 0.01036597923, l = list(t = w[i],  : 
>         f() values at end points not of opposite sign
> 
> 
> 
>  
> and here is my fonction "saeqn".
> 
> 
> 
>>saeqn<-function(s,l)
> 
> +  {
> + 
> + 
> + p<- exp(-l$gp$lambda+s)*l$gp$c
> + 
> + 
> +
> k11<-(l$gp$mu*(l$gp$lambda^2)*l$gp$c-s*l$gp$lambda*l$gp$c*l$gp$mu+l$gp$mu*l$gp$lambda)*p
> + 
> + k12 <-
> -l$gp$mu*l$gp$lambda-s^2+2*s*l$gp$lambda-(l$gp$lambda^2)
> + 
> + k13 <-k11+k12
> + 
> +
> k14<-(l$gp$lambda-s)*(-l$gp$mu*s-s*l$gp$lambda+s^2+l$gp$mu*l$gp$lambda*p)
> + 
> + k1<- -k13/k14
> + 
> +  k1-l$t
> +  }
> 
>  
> 
> . There is something I must be missing since I never
> had 
> luck with "uniroot"!
> 
> Thanks,
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Sun Jul 30 12:25:51 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 30 Jul 2006 12:25:51 +0200
Subject: [R] Log color scale
In-Reply-To: <44CC6A04.9070103@7d4.com>
References: <97faa3210607291916r2fade731w65e3b79fb37d6a1b@mail.gmail.com>
	<44CC6A04.9070103@7d4.com>
Message-ID: <44CC892F.8090906@statistik.uni-dortmund.de>

vincent at 7d4.com wrote:

> Kartik Pappu a ?crit :
> 
> 
>>However I need to plot my data in a log transformed color scale.  Is
>>this possible?  I will be happy to explain further, but basically I
>>need to do this because there are large variations in the max and min
>>values of my raw data and I am trying to highlight the differences in
>>the values at the lower end of my raw data (while still displaying the
>>values at the high end of the spectrum for comparison) so if figured
>>the best way to represent this on a (RGB) color scale is to do it with
>>a log transform.  I do not want to use too many colors because that
>>make the figure too busy.
>>Any suggestions on how to achieve this.
> 
> 
> Don't use too much time to search a good palette, build it yourself.

It is worth looking into the packages "RColorBrewer" and "colorspace". 
Some people thought about colors in graphics and wrote some code that 
might result in more appropriate colors and palettes than those that are 
quickly hacked...


Uwe Ligges




> build your own color palette, specific to your problem.
> It's not very difficult, see ?rgb et al.
> 
> a small example (not a log scale) :
> palblancbleu  = rgb(15:0, 15:0, 15, max=15);
> palrougeblanc = rgb(15, 0:15, 0:15, max=15);
> palblanc      = rep(rgb(1,1,1), 8);
> palRBB        = c(palrougeblanc, palblanc, palblancbleu);
> 
> #palRBB = c(
> #"#FF0000","#FF1111","#FF2222","#FF3333","#FF4444","#FF5555","#FF6666","#FF7777"
> #,"#FF8888","#FF9999","#FFAAAA","#FFBBBB","#FFCCCC","#FFDDDD","#FFEEEE","#FFFFFF"
> #,"#FFFFFF","#FFFFFF","#FFFFFF","#FFFFFF","#FFFFFF","#FFFFFF","#FFFFFF","#FFFFFF"
> #,"#FFFFFF","#EEEEFF","#DDDDFF","#CCCCFF","#BBBBFF","#AAAAFF","#9999FF","#8888FF"
> #,"#7777FF","#6666FF","#5555FF","#4444FF","#3333FF","#2222FF","#1111FF","#0000FF");
> 
> # to visualize it
> # to visualize a palette
> 
> showpal = function(pal=palRBB)
> {
> n = 200;
> fp = matrix(0,n,1);
> for (i in 1:n) fp[i,] = i;
> 
> par(mar=c(0,0,0,0), mgp=c(0,0,0), ann=F);
> image(fp, col=pal);
> }
> 
> (images at http://7d4.com/r/pal.php)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From HDoran at air.org  Sun Jul 30 14:27:31 2006
From: HDoran at air.org (Doran, Harold)
Date: Sun, 30 Jul 2006 08:27:31 -0400
Subject: [R] Question about data used to fit the mixed model
References: <BAY106-F1324849CBE25BB139CBA06A25F0@phx.gbl>
Message-ID: <2323A6D37908A847A7C32F1E3662C80E04E5AB@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060730/b3864078/attachment.pl 

From spencer.graves at pdf.com  Sun Jul 30 16:07:29 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 30 Jul 2006 22:07:29 +0800
Subject: [R] User defined covariate structure.
In-Reply-To: <1153770184.44c522c8eb19d@webmail.uoguelph.ca>
References: <1153770184.44c522c8eb19d@webmail.uoguelph.ca>
Message-ID: <44CCBD21.4030407@pdf.com>

	  Have you tried using corARMA?  Won't this give you the symmetric 
Toeplitz form you desire, albeit in a different parameterization?

	  Hope this helps.
	  Spencer Graves

jswansmi at uoguelph.ca wrote:
> I am trying to use nlme but instead of using one of the ?identity? variance or
> covariance matrixes such as compsymm or ar1.  Instead I want the covariance
> matrix to be represented in the following manor.  Is it possible to define my
> own covariance matrix?
> I have search and found papers saying I can define my own covariance matrixes
> and own correlation structures.  Said use corstruct but not sure how to
> implement it.  Also found documentation to use re.structur.  If able to help me
> out it be greatly appreciated as I am stuck.
> 
> |1	p1g	p2g	p3g	p4g	?|
> |p1g	1	p1g	p2g	p3g	?|
> |p2g 	p1g	1	p1g	p2g	?|
> |p3g	p2g	p1g	1	p1g 	?|
> |:	:	:	:	:	?|
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From spencer.graves at pdf.com  Sun Jul 30 16:31:55 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 30 Jul 2006 22:31:55 +0800
Subject: [R] standardized random effects with ranef.lme()
In-Reply-To: <44C527E9.3080400@uni-hamburg.de>
References: <44C527E9.3080400@uni-hamburg.de>
Message-ID: <44CCC2DB.4090809@pdf.com>

	  Have you tried RSiteSearch("MLWin")?  I just got 29 hits.  I wonder 
if any one of these might relate to your question?

	  If you would like more help on this issue from this listserve, please 
submit another post, preferably illustrating your question with the 
simplest possible self-contained example that illustrates your question, 
perhaps like the following:

	  fm1.16 <- lme(distance~age, data=Orthodont[1:16,],
            random=~age|Subject)

	  Hope this helps.
	  Spencer Graves
p.s.  PLEASE do read the posting guide 
"www.R-project.org/posting-guide.html" and provide commented, minimal, 
self-contained, reproducible code.

Dirk Enzmann wrote:
> Using ranef() (package nlme, version 3.1-75) with an 'lme' object I can 
> obtain random effects for intercept and slope of a certain level (say: 
> 1) - this corresponds to (say level 1) "residuals" in MLWin. Maybe I'm 
> mistaken here, but the results are identical.
> 
> However, if I try to get the standardized random effects adding the 
> paramter "standard=T" to the specification of ranef(), the results 
> differ considerably from the results of MLWin (although MLWin defines 
> "standardized" in the same way as "divided by its estimated (diagnostic) 
> standard error").
> 
> Why do the results differ although the estimates (random effects and 
> thus their variances) are almost identical? I noticed that lme() does 
> not compute the standard errors of the variances of the random effects - 
> for several reasons, but if this is true, how does ranef() calculate the 
> standardized random effects (the help says: '"standardized" (i.e. 
> divided by the corresponding estimated standard error)').
> 
> Is there a way to obtain similar results as in MLWin (or: should I 
> prefer the results of ranef() for certain reasons)?
> 
> Dirk
> 
> -----------------------------
> R version: 2.3.1 Patched (2006-06-21 r38367)
> 
> 
> *************************************************
> Dr. Dirk Enzmann
> Institute of Criminal Sciences
> Dept. of Criminology
> Edmund-Siemers-Allee 1
> D-20146 Hamburg
> Germany
> 
> phone: +49-(0)40-42838.7498 (office)
>         +49-(0)40-42838.4591 (Billon)
> fax:   +49-(0)40-42838.2344
> email: dirk.enzmann at uni-hamburg.de
> www: 
> http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From srini_iyyer_bio at yahoo.com  Sun Jul 30 17:18:10 2006
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Sun, 30 Jul 2006 08:18:10 -0700 (PDT)
Subject: [R] overlaying the values of tab-delim file in to a pre-existing
	matrix
In-Reply-To: <20060728202101.68310.qmail@web38113.mail.mud.yahoo.com>
Message-ID: <20060730151810.81828.qmail@web38111.mail.mud.yahoo.com>

Hello :

I have matrix with dimensions(200 X 20,000). I
have another 
file, a tab-delim file where first column
variables are row 
names and second column variables are column
names. Tab-delim file has smaller values than the
matrix. 

Matrix = tmat
tab-delim file read as data.frame = tb



My aim is to read in a line in # Apple, S , 21. 
Find column Apple and row S and fill the value 21.




For instance:

> tmat
     Apple Orange Mango Grape Star
A     0      0     0     0    0
O     0      0     0     0    0
M     0      0     0     0    0
G     0      0     0     0    0
S     0      0     0     0    0


> tb # tab- delim file read as a data.frame
     V1  V2  V3
1  Apple  S  21
2  Apple  A  21.6
3  Apple  O  43
4 Orange  A  45
5 Orange  O  64
6 Orange  S  32.5
7  Mango  M  40.3 
8  Mango  A  32.6
9  Mango  S  24.6


Now I have to fill in the values in tb (V3) into tmat.


For instance, (Apple, S) pair value is 21, I want 


     Apple Orange Mango Grape Star
A     21.6   0     0     0    0
O     0      0     0     0    0
M     0      0     0     0    0
G     0      0     0     0    0
S     21     0     0     0    0




> tbm <- as.matrix(tb)
>
tmat[cbind(match(tbm[,2],rownames(tmat)),match(tbm[,1],colnames(tmat)))]
<-tbm[,3]
Error: NAs are not allowed in subscripted assignments


I am using R.2.2.1 on a 
Dell Latutite windows XP with 1GB RAM. 

Could any one please help me whats wrong with above
code. 

thank you.


From ggrothendieck at gmail.com  Sun Jul 30 18:22:28 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 30 Jul 2006 12:22:28 -0400
Subject: [R] overlaying the values of tab-delim file in to a
	pre-existing matrix
In-Reply-To: <20060730151810.81828.qmail@web38111.mail.mud.yahoo.com>
References: <20060728202101.68310.qmail@web38113.mail.mud.yahoo.com>
	<20060730151810.81828.qmail@web38111.mail.mud.yahoo.com>
Message-ID: <971536df0607300922y6efc5bbal8fb4b756a12afa5e@mail.gmail.com>

Please provide reproducible examples (as discussed at end of each
posting):

Lines <- "Apple  S 21.0
 Apple  A 21.6
 Apple  O 43.0
Orange  A 45.0
Orange  O 64.0
Orange  S 32.5
 Mango  M 40.3
 Mango  A 32.6
 Mango  S 24.6
"
tb <- read.table(textConnection(Lines))

# alternative 1 - create a matrix
tmat <- matrix(0, nrow = nlevels(tb$V2), ncol = nlevels(tb$V1),
	dimnames = list(levels(tb$V2), levels(tb$V1)))
tmat[cbind(tb$V2, tb$V1)] <- tb$V3

# alternative 2 - out is a data frame in wide format
out <- reshape(tb, dir = "wide", timevar = "V1", idvar = "V2")
# fix up rownames and colnames and remove first column
rownames(out) <- out[,1]
out <- out[,-1]
colnames(out) <- sub(".*[.]", "", colnames(out))
out[is.na(out)] <- 0



On 7/30/06, Srinivas Iyyer <srini_iyyer_bio at yahoo.com> wrote:
> Hello :
>
> I have matrix with dimensions(200 X 20,000). I
> have another
> file, a tab-delim file where first column
> variables are row
> names and second column variables are column
> names. Tab-delim file has smaller values than the
> matrix.
>
> Matrix = tmat
> tab-delim file read as data.frame = tb
>
>
>
> My aim is to read in a line in # Apple, S , 21.
> Find column Apple and row S and fill the value 21.
>
>
>
>
> For instance:
>
> > tmat
>     Apple Orange Mango Grape Star
> A     0      0     0     0    0
> O     0      0     0     0    0
> M     0      0     0     0    0
> G     0      0     0     0    0
> S     0      0     0     0    0
>
>
> > tb # tab- delim file read as a data.frame
>     V1  V2  V3
> 1  Apple  S  21
> 2  Apple  A  21.6
> 3  Apple  O  43
> 4 Orange  A  45
> 5 Orange  O  64
> 6 Orange  S  32.5
> 7  Mango  M  40.3
> 8  Mango  A  32.6
> 9  Mango  S  24.6
>
>
> Now I have to fill in the values in tb (V3) into tmat.
>
>
> For instance, (Apple, S) pair value is 21, I want
>
>
>     Apple Orange Mango Grape Star
> A     21.6   0     0     0    0
> O     0      0     0     0    0
> M     0      0     0     0    0
> G     0      0     0     0    0
> S     21     0     0     0    0
>
>
>
>
> > tbm <- as.matrix(tb)
> >
> tmat[cbind(match(tbm[,2],rownames(tmat)),match(tbm[,1],colnames(tmat)))]
> <-tbm[,3]
> Error: NAs are not allowed in subscripted assignments
>
>
> I am using R.2.2.1 on a
> Dell Latutite windows XP with 1GB RAM.
>
> Could any one please help me whats wrong with above
> code.
>
> thank you.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From chris at psyctc.org  Sun Jul 30 20:53:46 2006
From: chris at psyctc.org (Chris Evans)
Date: Sun, 30 Jul 2006 19:53:46 +0100
Subject: [R] Power of a single sample binomial test
Message-ID: <44CD003A.3020304@psyctc.org>

The only references to this I can find searching the archives are to a
student who asked in relation to his course work on a stats course.
Promise I'm not doing that!

I have a situation in which we want to test proportions against an
expected proportion, binom.test() is great.  I'd like to do some post
hoc power tests (the x and n were beyond our control in the survey as
all we could set was an overall n.max where n < n.max, n is between  1
and 44).

I would love to work out our power to have detected a proportions
different from the expected (.307).  I've run two-tailed binomial tests
as we were interested in both high and low.  We can not unreasonably
confine to the directional prediction of observed x/n << .307, say <.15
if that makes the maths easier.  I can't see functions in R that will do
this for me.  The only book I seem to have to hand that addresses this is:
Kraemer, H. C. & Thiemann, S. (1988) How many subjects?  Statistical
power analysis in research. Newbury Park California, Sage Publications, Inc.
which I appreciate is ageing but I assume still correct.  The problem I
have is that I can use R to get Kraemer's upper case delta (p.77) and
look up in their "Master table" but I'd love a more flexible function
that would  say solve for power where p1, n, p0 and alpha are given.  I
think I ought to be able to work out how their master table was
calculated and work from that but I'm finding the mathematics a bit
opaque for my ageing brain.  Their model is clearly one-tailed.  I'm not
sure how one works a two-tailed power.

A search around for web calculators etc. turns up all manner of things,
some probably good, some dead etc.  I'd hugely appreciate if someone
here could share anything they may have in R or point me to R solutions
I may have missed.

TIA,

Chris

-- 
Chris Evans <chris at psyctc.org>
Professor of Psychotherapy, Nottingham University;
Consultant Psychiatrist in Psychotherapy, Rampton Hospital;
Research Programmes Director, Nottinghamshire NHS Trust;
Hon. SL Institute of Psychiatry, Hon. Con., Tavistock & Portman Trust
**If I am writing from one of those roles, it will be clear. Otherwise**

**my views are my own and not representative of those institutions    **


From HDoran at air.org  Sun Jul 30 21:40:16 2006
From: HDoran at air.org (Doran, Harold)
Date: Sun, 30 Jul 2006 15:40:16 -0400
Subject: [R] standardized random effects with ranef.lme()
Message-ID: <2323A6D37908A847A7C32F1E3662C80E2768C2@dc1ex01.air.org>

 
> > Why do the results differ although the estimates (random 
> effects and 
> > thus their variances) are almost identical? I noticed that 
> lme() does 
> > not compute the standard errors of the variances of the 
> random effects 
> > - for several reasons, but if this is true, how does 
> ranef() calculate 
> > the standardized random effects (the help says: 
> '"standardized" (i.e.
> > divided by the corresponding estimated standard error)').
> > 
> > Is there a way to obtain similar results as in MLWin (or: should I 
> > prefer the results of ranef() for certain reasons)?

I think there are two different issues here. The lme function does not
produce a standard error of the variance component as some other
multilevel packages do. It is often recommended in the multilevel
literature to consider the p-value of the variance components and "fix"
or retain the variance if p < .05. There are good reasons not to follow
this practice.

If you were using lmer(), you still wouldn't get this statistic, but you
could use the MCMCsamp() function to examine the distribution of the
random effects.

The second issue (I think) is that the conditional variance of the
random effect is not the same as the standard error of the variance of
the random effect. From the definition in the help of ranef.lme, I
believe it is the random effect divided by its conditional standard
error. I don't know how to get the posterior variance of the random
effects in lme, but I do in lmer, so we can experiment a bit. It has
been a while since I have really used lme and I do not think there is an
extractor function to get these and I didn't see the variances in the
model object.

Let's work through an example to see what we get. Here is what I see

library(nlme)
data(Orthodont)
detach(package:nlme)

library(Matrix)
fm1 <- lmer(distance ~ age + (age|Subject), data = Orthodont)
# equivalent to
# fm1 <- lme(distance ~ age, data=Orthodont)

# Extract the variances of the random effects
qq <- attr(ranef(fm1, postVar = TRUE)[[1]], "postVar")

# divide the random effects by their standard error of "age"
Sranef_lmer <- ranef(fm1)[[1]][,2]/ sqrt(qq[2,2,])

library(nlme)
# Now, run the lme model
fm2 <- lme(distance ~ age, Orthodont)

# get the standardized random effects from lme
Sranef_lme <- ranef(fm2, standard=T)[2]

cor(Sranef_lmer, Sranef_lme)
[1] 1

Notice the perfect correlation. But, the actual values in Sranef_lme and
Sranef_lmer are a bit different and I cannot see why just yet. I need to
go eat lunch, but I'll think about this. 

Maybe somebody else sees something.


From marco.boks at inter.nl.net  Sun Jul 30 23:15:50 2006
From: marco.boks at inter.nl.net (Marco Boks)
Date: Sun, 30 Jul 2006 23:15:50 +0200
Subject: [R] main= bquote(paste("Results for ", beta, "3",
	==.(b1)))) doesn't work.
Message-ID: <002401c6b41d$55988cf0$0300a8c0@marco>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060730/91dd0cd8/attachment.pl 

From mharbur at umn.edu  Sun Jul 30 23:20:01 2006
From: mharbur at umn.edu (Matthew Harbur)
Date: Sun, 30 Jul 2006 16:20:01 -0500
Subject: [R] manova and table of means by factor levels
Message-ID: <000001c6b41d$eb8b3d20$6601a8c0@UOFMSWDE0.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060730/aadd1c19/attachment.pl 

From ggrothendieck at gmail.com  Sun Jul 30 23:25:10 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 30 Jul 2006 17:25:10 -0400
Subject: [R] main= bquote(paste("Results for ", beta, "3",
	==.(b1)))) doesn't work.
In-Reply-To: <002401c6b41d$55988cf0$0300a8c0@marco>
References: <002401c6b41d$55988cf0$0300a8c0@marco>
Message-ID: <971536df0607301425j7f53d978x63c1f96b4e3d4cb2@mail.gmail.com>

I assume the 3 is supposed to be a subscript.  Try this:

  b1 <- x <- y <- 1
  plot(x,y, main = bquote("Results for " ~ beta[3] ==.(b1)))

On 7/30/06, Marco Boks <marco.boks at inter.nl.net> wrote:
> Hi,
>
> I need to plot the beta as the symbol, followed by the index 3 as the title of a graph.
>
> This code works> main= bquote(paste("Results for ", beta ==.(b1))
>
> but I also need the index 3.
> I tried (simplified):
>
> >plot(x,y, main= bquote(paste("Results for ", beta, "3", ==.(b1))))
>
> and a few other versions, but I can not get it to run properly.
>
> Any help would be greatly appreciated,
>
> Thanks
>
> Marco
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From p.dalgaard at biostat.ku.dk  Sun Jul 30 23:40:47 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Jul 2006 23:40:47 +0200
Subject: [R] main= bquote(paste("Results for ", beta, "3",
	==.(b1)))) doesn't work.
In-Reply-To: <002401c6b41d$55988cf0$0300a8c0@marco>
References: <002401c6b41d$55988cf0$0300a8c0@marco>
Message-ID: <x24pwy7ov4.fsf@turmalin.kubism.ku.dk>

"Marco Boks" <marco.boks at inter.nl.net> writes:

> Hi,
> 
> I need to plot the beta as the symbol, followed by the index 3 as the title of a graph.
> 
> This code works> main= bquote(paste("Results for ", beta ==.(b1)) 
> 
> but I also need the index 3.
> I tried (simplified):
> 
> >plot(x,y, main= bquote(paste("Results for ", beta, "3", ==.(b1)))) 
> 
> and a few other versions, but I can not get it to run properly.
> 
> Any help would be greatly appreciated,

Are you looking for 

 plot(0,0, main= bquote(paste("Results for ", beta[3] ==.(b1))))

?

Your fundamental problem is that expressions need to be syntactically
complete, so "==.(b1)" with no left hand side gets you a syntax error.
However, you can supply a dummy, and

 plot(0,0, main= bquote(paste("Results for ", beta, 3, {} ==.(b1))))

does work (although possibly not with your desired output).

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From p.murrell at auckland.ac.nz  Mon Jul 31 01:13:59 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 31 Jul 2006 11:13:59 +1200
Subject: [R] placing rectangle behind plot
In-Reply-To: <971536df0607291420r456bccdfwd809fceaa3b98c17@mail.gmail.com>
References: <971536df0607291420r456bccdfwd809fceaa3b98c17@mail.gmail.com>
Message-ID: <44CD3D37.4020106@stat.auckland.ac.nz>

Hi


Gabor Grothendieck wrote:
> I am trying to create a lattice plot and would like to later, i.e. after
> the plot is drawn, add a grey rectangle behind a portion of it.
> The following works except that the rectrangle is on top of and
> obscures a portion of the chart.  I also tried adding col = "transparent"
> to the gpar list but that did not help -- I am on windows and
> perhaps the windows device does not support transparency?


Correct.


> At any rate, how can I place the rectangle behind the plotted
> points without drawing the rectangle first?
> 
> library(lattice)
> library(grid)
> trellis.unfocus()
> x <- 1:10
> xyplot(x ~ x | gl(2,1), layout = 1:2)
> trellis.focus("panel", 1, 1)
> grid.rect(w = .5, gp = gpar(fill = "light grey"))
> trellis.unfocus()


The user-interface is a little rough, but this can be done by accessing
the underlying grid objects.  Here's an example, with explanatory bits
interspersed ...

# "grab" the lattice plot as a grid gTree
# There are warnings, but they are ignorable
latticeplot <- grid.grabExpr(print(xyplot(x ~ x | gl(2,1),
                                          layout = 1:2)))

# Demonstrate that the gTree faithfully replicates the
# original lattice plot (not necessary, just to to what's going on)
grid.newpage()
grid.draw(latticeplot)

# Explore the gTree (just to to show what's going on)
# Better user-interface would be nice here ...
childNames(latticeplot)
# Identify which children are which
# (appropriate grob names would be nice here)
lapply(latticeplot$children, class)
# Identify where each child is drawn
latticeplot$childrenvp
lapply(latticeplot$children, "[[", "vp")

# Add a rect (starts off on top of everything else)
# NOTE that rect has to have correct vpPath
plotwithrect <- addGrob(latticeplot,
                        rectGrob(w = .5, gp = gpar(fill = "light grey"),
                                 vp=vpPath("plot1.toplevel.vp",
                                           "plot1.panel.1.1.vp")))

# Check this draws what we expect (just to show what's going on)
grid.newpage()
grid.draw(plotwithrect)

# Reorder children to put rect at back
# Appropriate user-interface would be nice here ...
nc <- length(plotwithrect$childrenOrder)
plotwithrect$childrenOrder <-
    plotwithrect$childrenOrder[c(nc, 1:(nc - 1))]

# Final result
grid.newpage()
grid.draw(plotwithrect)

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From mharbur at umn.edu  Mon Jul 31 03:58:54 2006
From: mharbur at umn.edu (Matthew Harbur)
Date: Sun, 30 Jul 2006 20:58:54 -0500
Subject: [R] manova and table of means by factor levels
Message-ID: <000001c6b444$e13a0690$6601a8c0@UOFMSWDE0.local>

I am resubmitting my question from earlier today -- this time with the
proper plain text formatting.

I have a dataset in which I have identified a three-way interaction. Factor
A has 3 levels, factor B has 4 levels, and factor C has 3 levels.? I would
like to produce the following:

1) A manova in which I can test the effect of factor C for each combination
of factor A * factor B.? I know how to do this using a series of ?do?
commands, but I wonder whether there is a more significant solution. For
other converts from SAS, I am specifically looking for the R equivalent of
the ?by? command in PROC GLM.
2) Create a table of means that, for each level of factor B, shows the
interaction of factors A and C. Thus, I need to manipulate summary.formula
or another table-generating program to produce subtotals. If that is not
possible, is there another way to avoid generating and binding multiple
subsets into one table?

I have searched the R-help archives and the internet, but to no avail.? The
closest posting that I have found is:
http://tolstoy.newcastle.edu.au/R/help/05/04/3239.html. Would someone please
help me?

Thank you.

Matthew M. Harbur
University of Minnesota Southwest Research and Outreach Center
23669 130th Street
Lamberton, MN? 56152
(507) 752-5091 (office)
(507) 752-5097 (fax)


From ggrothendieck at gmail.com  Mon Jul 31 04:24:59 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 30 Jul 2006 22:24:59 -0400
Subject: [R] placing rectangle behind plot
In-Reply-To: <44CD3D37.4020106@stat.auckland.ac.nz>
References: <971536df0607291420r456bccdfwd809fceaa3b98c17@mail.gmail.com>
	<44CD3D37.4020106@stat.auckland.ac.nz>
Message-ID: <971536df0607301924u231c052eo8281f107db46dbb9@mail.gmail.com>

Thanks.  That's helpful.

I would be interested in the case where

1. one does not have a variable latticeplot, as per your example,
but just has the output of

   xyplot(x ~ x | gl(2,1), layout = 1:2)

sitting on the screen, having been "printed" by a prior
function.  We can assume that no other graphics have been
issued since then. Can one still create a grey rectangle behind
the lower panel?

2. In fact, ideally what I would like is to create a function,
put.in.bg, say, that works something like this:

   xyplot(x ~ x | gl(2,1), layout = 1:2)
   trellis.focus("panel", 1, 1)
   put.in.bg(grid.rect(w = 0.5))
   trellis.unfocus()

or maybe

   xyplot(x ~ x | gl(2,1), layout = 1:2)
   trellis.focus.bg("panel", 1, 1)
   grid.rect(w = 0.5)
   trellis.unfocus()

That allows one to add objects to a lattice panel behind the objects
that are already there. This would also be helpful for adding grid
lines afterwards or other lines, rectangles, etc.


On 7/30/06, Paul Murrell <p.murrell at auckland.ac.nz> wrote:
> Hi
>
>
> Gabor Grothendieck wrote:
> > I am trying to create a lattice plot and would like to later, i.e. after
> > the plot is drawn, add a grey rectangle behind a portion of it.
> > The following works except that the rectrangle is on top of and
> > obscures a portion of the chart.  I also tried adding col = "transparent"
> > to the gpar list but that did not help -- I am on windows and
> > perhaps the windows device does not support transparency?
>
>
> Correct.
>
>
> > At any rate, how can I place the rectangle behind the plotted
> > points without drawing the rectangle first?
> >
> > library(lattice)
> > library(grid)
> > trellis.unfocus()
> > x <- 1:10
> > xyplot(x ~ x | gl(2,1), layout = 1:2)
> > trellis.focus("panel", 1, 1)
> > grid.rect(w = .5, gp = gpar(fill = "light grey"))
> > trellis.unfocus()
>
>
> The user-interface is a little rough, but this can be done by accessing
> the underlying grid objects.  Here's an example, with explanatory bits
> interspersed ...
>
> # "grab" the lattice plot as a grid gTree
> # There are warnings, but they are ignorable
> latticeplot <- grid.grabExpr(print(xyplot(x ~ x | gl(2,1),
>                                          layout = 1:2)))
>
> # Demonstrate that the gTree faithfully replicates the
> # original lattice plot (not necessary, just to to what's going on)
> grid.newpage()
> grid.draw(latticeplot)
>
> # Explore the gTree (just to to show what's going on)
> # Better user-interface would be nice here ...
> childNames(latticeplot)
> # Identify which children are which
> # (appropriate grob names would be nice here)
> lapply(latticeplot$children, class)
> # Identify where each child is drawn
> latticeplot$childrenvp
> lapply(latticeplot$children, "[[", "vp")
>
> # Add a rect (starts off on top of everything else)
> # NOTE that rect has to have correct vpPath
> plotwithrect <- addGrob(latticeplot,
>                        rectGrob(w = .5, gp = gpar(fill = "light grey"),
>                                 vp=vpPath("plot1.toplevel.vp",
>                                           "plot1.panel.1.1.vp")))
>
> # Check this draws what we expect (just to show what's going on)
> grid.newpage()
> grid.draw(plotwithrect)
>
> # Reorder children to put rect at back
> # Appropriate user-interface would be nice here ...
> nc <- length(plotwithrect$childrenOrder)
> plotwithrect$childrenOrder <-
>    plotwithrect$childrenOrder[c(nc, 1:(nc - 1))]
>
> # Final result
> grid.newpage()
> grid.draw(plotwithrect)
>
> Paul
> --
> Dr Paul Murrell
> Department of Statistics
> The University of Auckland
> Private Bag 92019
> Auckland
> New Zealand
> 64 9 3737599 x85392
> paul at stat.auckland.ac.nz
> http://www.stat.auckland.ac.nz/~paul/
>
>


From jz7 at duke.edu  Mon Jul 31 04:45:30 2006
From: jz7 at duke.edu (jz7 at duke.edu)
Date: Sun, 30 Jul 2006 22:45:30 -0400 (EDT)
Subject: [R] question about dataframe ("sensory") in PLS package
Message-ID: <Pine.GSO.4.58.0607302232430.20874@godzilla.acpub.duke.edu>

Dear all,

I am trying to my dataframe for the PLS analysis using the PLS package.
However I have some trouble generating the correct dataframe. The main
problem is how to use one name to represent several columns in the
dataframe.

The example dataframe in PLS package is called "sensory". I cannot
directly read the data file since it's a binary file. If I use
"names(sensory)" command, I will get two names: "Quality" and "Panel". But
if I use "summary(sensory)" command, I will get information of five
columns for "Quality" and 6 columns for "Panel" (such as "Quality.Acidity"
"Quality.Peroxide"...). So when I use PLS regression, the function is
simply "Panel ~ Quality" (but it's actually multiple regression).

Does anyone know how to build such dataframe? Please share some
experience. Really appreciate the help!

Sincerely,
Jeny


From ggrothendieck at gmail.com  Mon Jul 31 05:09:55 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 30 Jul 2006 23:09:55 -0400
Subject: [R] question about dataframe ("sensory") in PLS package
In-Reply-To: <Pine.GSO.4.58.0607302232430.20874@godzilla.acpub.duke.edu>
References: <Pine.GSO.4.58.0607302232430.20874@godzilla.acpub.duke.edu>
Message-ID: <971536df0607302009p1e484b4asbed9bda8e9dbae76@mail.gmail.com>

Try:

?sensory
str(sensory)
dput(sensory)
lapply(sensory, class)
lapply(sensory, dim)

to see what it looks like inside.  Seems that sensory is a data frame
consisting of two columns each of which is a matrix except that each
has a class of "AsIs".  Thus try this (where I(...) creates objects of
class "AsIs"):

mat1 <- cbind(a = 1:5, b = 11:15)
mat2 <- cbind(x = 21:25, y = 31:35)
DF <- data.frame(A = I(mat1), B = I(mat2))



On 7/30/06, jz7 at duke.edu <jz7 at duke.edu> wrote:
> Dear all,
>
> I am trying to my dataframe for the PLS analysis using the PLS package.
> However I have some trouble generating the correct dataframe. The main
> problem is how to use one name to represent several columns in the
> dataframe.
>
> The example dataframe in PLS package is called "sensory". I cannot
> directly read the data file since it's a binary file. If I use
> "names(sensory)" command, I will get two names: "Quality" and "Panel". But
> if I use "summary(sensory)" command, I will get information of five
> columns for "Quality" and 6 columns for "Panel" (such as "Quality.Acidity"
> "Quality.Peroxide"...). So when I use PLS regression, the function is
> simply "Panel ~ Quality" (but it's actually multiple regression).
>
> Does anyone know how to build such dataframe? Please share some
> experience. Really appreciate the help!
>
> Sincerely,
> Jeny
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Mon Jul 31 05:18:33 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 30 Jul 2006 23:18:33 -0400
Subject: [R] placing rectangle behind plot
In-Reply-To: <971536df0607301924u231c052eo8281f107db46dbb9@mail.gmail.com>
References: <971536df0607291420r456bccdfwd809fceaa3b98c17@mail.gmail.com>
	<44CD3D37.4020106@stat.auckland.ac.nz>
	<971536df0607301924u231c052eo8281f107db46dbb9@mail.gmail.com>
Message-ID: <971536df0607302018x1a320897u8f46661b27191ea6@mail.gmail.com>

Just to answer my own question I just discovered trellis.panelArgs()
and that can be used to give the following solution:

  library(lattice)
  library(grid)
  x <- 1:10
  xyplot(x ~ x | gl(2,1), layout = 1:2)
  trellis.focus("panel", 1, 1)
  grid.rect(w = 0.5, gp = gpar(fill = "light grey"))
  # re-plot panel over rectangle
  do.call("panel.xyplot", trellis.panelArgs())
  trellis.unfocus()

nevertheless, as a point of general interest I would still be
interested to know
what a general grid-based solution might be.



On 7/30/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Thanks.  That's helpful.
>
> I would be interested in the case where
>
> 1. one does not have a variable latticeplot, as per your example,
> but just has the output of
>
>   xyplot(x ~ x | gl(2,1), layout = 1:2)
>
> sitting on the screen, having been "printed" by a prior
> function.  We can assume that no other graphics have been
> issued since then. Can one still create a grey rectangle behind
> the lower panel?
>
> 2. In fact, ideally what I would like is to create a function,
> put.in.bg, say, that works something like this:
>
>   xyplot(x ~ x | gl(2,1), layout = 1:2)
>   trellis.focus("panel", 1, 1)
>   put.in.bg(grid.rect(w = 0.5))
>   trellis.unfocus()
>
> or maybe
>
>   xyplot(x ~ x | gl(2,1), layout = 1:2)
>   trellis.focus.bg("panel", 1, 1)
>   grid.rect(w = 0.5)
>   trellis.unfocus()
>
> That allows one to add objects to a lattice panel behind the objects
> that are already there. This would also be helpful for adding grid
> lines afterwards or other lines, rectangles, etc.
>
>
> On 7/30/06, Paul Murrell <p.murrell at auckland.ac.nz> wrote:
> > Hi
> >
> >
> > Gabor Grothendieck wrote:
> > > I am trying to create a lattice plot and would like to later, i.e. after
> > > the plot is drawn, add a grey rectangle behind a portion of it.
> > > The following works except that the rectrangle is on top of and
> > > obscures a portion of the chart.  I also tried adding col = "transparent"
> > > to the gpar list but that did not help -- I am on windows and
> > > perhaps the windows device does not support transparency?
> >
> >
> > Correct.
> >
> >
> > > At any rate, how can I place the rectangle behind the plotted
> > > points without drawing the rectangle first?
> > >
> > > library(lattice)
> > > library(grid)
> > > trellis.unfocus()
> > > x <- 1:10
> > > xyplot(x ~ x | gl(2,1), layout = 1:2)
> > > trellis.focus("panel", 1, 1)
> > > grid.rect(w = .5, gp = gpar(fill = "light grey"))
> > > trellis.unfocus()
> >
> >
> > The user-interface is a little rough, but this can be done by accessing
> > the underlying grid objects.  Here's an example, with explanatory bits
> > interspersed ...
> >
> > # "grab" the lattice plot as a grid gTree
> > # There are warnings, but they are ignorable
> > latticeplot <- grid.grabExpr(print(xyplot(x ~ x | gl(2,1),
> >                                          layout = 1:2)))
> >
> > # Demonstrate that the gTree faithfully replicates the
> > # original lattice plot (not necessary, just to to what's going on)
> > grid.newpage()
> > grid.draw(latticeplot)
> >
> > # Explore the gTree (just to to show what's going on)
> > # Better user-interface would be nice here ...
> > childNames(latticeplot)
> > # Identify which children are which
> > # (appropriate grob names would be nice here)
> > lapply(latticeplot$children, class)
> > # Identify where each child is drawn
> > latticeplot$childrenvp
> > lapply(latticeplot$children, "[[", "vp")
> >
> > # Add a rect (starts off on top of everything else)
> > # NOTE that rect has to have correct vpPath
> > plotwithrect <- addGrob(latticeplot,
> >                        rectGrob(w = .5, gp = gpar(fill = "light grey"),
> >                                 vp=vpPath("plot1.toplevel.vp",
> >                                           "plot1.panel.1.1.vp")))
> >
> > # Check this draws what we expect (just to show what's going on)
> > grid.newpage()
> > grid.draw(plotwithrect)
> >
> > # Reorder children to put rect at back
> > # Appropriate user-interface would be nice here ...
> > nc <- length(plotwithrect$childrenOrder)
> > plotwithrect$childrenOrder <-
> >    plotwithrect$childrenOrder[c(nc, 1:(nc - 1))]
> >
> > # Final result
> > grid.newpage()
> > grid.draw(plotwithrect)
> >
> > Paul
> > --
> > Dr Paul Murrell
> > Department of Statistics
> > The University of Auckland
> > Private Bag 92019
> > Auckland
> > New Zealand
> > 64 9 3737599 x85392
> > paul at stat.auckland.ac.nz
> > http://www.stat.auckland.ac.nz/~paul/
> >
> >
>


From kartik.pappu at gmail.com  Mon Jul 31 06:36:34 2006
From: kartik.pappu at gmail.com (Kartik Pappu)
Date: Sun, 30 Jul 2006 21:36:34 -0700
Subject: [R] RGB function
Message-ID: <97faa3210607302136n7c1dc89bg152d829d4e771d4e@mail.gmail.com>

Hi all,

I created three separate square matrices (lets say R, G, and B).  each
one contains a series of values between 0 and 1.  I want to be able to
take for example R[1], G[1], B[1] and create a rgb color value into a
fourth matrix.  I tried using the rgb function but I must be doing
something wrong because it fails with the following error message.

(list) object cannot be coerced to 'double'.

Any ideas on how I could do this.

Kartik


From murdoch at stats.uwo.ca  Mon Jul 31 06:57:31 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 31 Jul 2006 00:57:31 -0400
Subject: [R] RGB function
In-Reply-To: <97faa3210607302136n7c1dc89bg152d829d4e771d4e@mail.gmail.com>
References: <97faa3210607302136n7c1dc89bg152d829d4e771d4e@mail.gmail.com>
Message-ID: <44CD8DBB.7060509@stats.uwo.ca>

On 7/31/2006 12:36 AM, Kartik Pappu wrote:
> Hi all,
> 
> I created three separate square matrices (lets say R, G, and B).  each
> one contains a series of values between 0 and 1.  I want to be able to
> take for example R[1], G[1], B[1] and create a rgb color value into a
> fourth matrix.  I tried using the rgb function but I must be doing
> something wrong because it fails with the following error message.
> 
> (list) object cannot be coerced to 'double'.
> 
> Any ideas on how I could do this.

I just tried what you said, and it worked:

> r <- matrix(runif(100), 10,10)
> g <- matrix(runif(100), 10,10)
> b <- matrix(runif(100), 10,10)
> matrix(rgb(r,g,b),10,10)

The final call to matrix() is needed because rgb() loses the dimension 
attribute from its result.

Duncan Murdoch


From mmpapenf at wisc.edu  Mon Jul 31 07:46:50 2006
From: mmpapenf at wisc.edu (Michael Papenfus)
Date: Mon, 31 Jul 2006 00:46:50 -0500
Subject: [R] Functions ,Optim, & Dataframe
Message-ID: <44CD994A.2070007@wisc.edu>

I have defined the following function:

fr<-function(x) {
    u<-x[1]
    v<-x[2]
    sqrt(sum((plnorm(c(3,6),u,v)-c(.55,.85))^2))
}
which I then solve using optim
y<-optim(c(1,1),fr)

 > y$par
[1] 1.0029771 0.7610545
This works fine.

Now I want to use these two steps on a dataframe:
 mydat<-data.frame(d1=c(3,5),d2=c(6,10),p1=c(.55,.05),p2=c(.85,.35))
 > mydat
  d1 d2   p1   p2
1  3  6 0.55 0.85
2  5 10 0.05 0.35

where for each row in mydat, I append the two parameter resulting from 
optim into mydat.
I want to do this for a larger dataset but thought I would start with a 
simple two row dataframe.

I have tried this with loops and the apply function, but seem to be 
getting nowhere.
Thanks for any input.
mike
mmpapenf at wisc.edu


From dieter.menne at menne-biomed.de  Mon Jul 31 08:30:29 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 31 Jul 2006 06:30:29 +0000 (UTC)
Subject: [R] Functions ,Optim, & Dataframe
References: <44CD994A.2070007@wisc.edu>
Message-ID: <loom.20060731T082940-291@post.gmane.org>

Michael Papenfus <mmpapenf <at> wisc.edu> writes:

> 
> I have defined the following function:
> 
> fr<-function(x) {
>     u<-x[1]
>     v<-x[2]
>     sqrt(sum((plnorm(c(3,6),u,v)-c(.55,.85))^2))
> }
> which I then solve using optim
> y<-optim(c(1,1),fr)
> 
>  > y$par
> [1] 1.0029771 0.7610545
> This works fine.
> 
> Now I want to use these two steps on a dataframe:
>  mydat<-data.frame(d1=c(3,5),d2=c(6,10),p1=c(.55,.05),p2=c(.85,.35))
>  > mydat
>   d1 d2   p1   p2
> 1  3  6 0.55 0.85
> 2  5 10 0.05 0.35
> 
> where for each row in mydat, I append the two parameter resulting from 
> optim into mydat.
> I want to do this for a larger dataset but thought I would start with a 
> simple two row dataframe.
> 

I would prefer a loop in this case.

fr<-function(x) {
    sqrt(sum((plnorm(c(3,6),x[1],x[2])-c(x[3],x[4]))^2))
}
y<-optim(c(1,2,0.55,0.85),fr)

mydat<-data.frame(d1=c(1,0.5),d2=c(1,0.1),p1=c(.55,.05),p2=c(.85,.35))
myres<-mydat   # simple way to allocate dataframe for results
names(myres) = paste("res",names(myres),sep=".")

for (i in 1:nrow(mydat)){
  y <- optim(mydat[i,1:4],fr)
  myres[i,] <- y$par
}
mydat = cbind(mydat,myres)


From rene.eschen at unifr.ch  Mon Jul 31 08:32:16 2006
From: rene.eschen at unifr.ch (ESCHEN Rene)
Date: Mon, 31 Jul 2006 08:32:16 +0200
Subject: [R] Random structure of nested design in lme
References: <2323A6D37908A847A7C32F1E3662C80E132CEE@dc1ex01.air.org>
	<E632249B3E11B14AAABA75FDB5F32B24132A0B@EXCHANGE4.unifr.ch>
	<44C26882.2090209@pdf.com>
Message-ID: <E632249B3E11B14AAABA75FDB5F32B24132A0F@EXCHANGE4.unifr.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060731/2fd51002/attachment.pl 

From paul.lemmens at gmail.com  Mon Jul 31 08:45:50 2006
From: paul.lemmens at gmail.com (Paul Lemmens)
Date: Mon, 31 Jul 2006 08:45:50 +0200
Subject: [R] Reading multiple txt files into one data frame
In-Reply-To: <97faa3210607291816g4d282eb2j2f3894b1c1a6957@mail.gmail.com>
References: <97faa3210607291816g4d282eb2j2f3894b1c1a6957@mail.gmail.com>
Message-ID: <b9065fc0607302345j31955315xb76ad997d244e65d@mail.gmail.com>

On 7/30/06, Kartik Pappu <kartik.pappu at gmail.com> wrote:
> Hello All,
>
> I have a device that spews out experimental data as a series of text
> files each of which contains one column with several rows of numeric
> data.  My problem is that for each trial it gives me one text file
> (and I run between 30 to 50 trials at a time) and I would ideally like
> to merge all these text files into one large data frame with each
> column representing a single trial.  It is not a problem if NA
> characters are added to make all the columna of eaqual length.  Right
> now I am doing this by opening each file individually and cutting and
> pasting the data into an excel file.  How can I do this in R assuming
> all my text files are in one directory.
>
> Is it also possible to customize the column headers.  For example if I
> have 32 trials and 16 are experimental and 16 are control and I want
> to name the columns "Expt1", Expt2",... "Expt16"  and the control
> columns "Cntl1",...Cntl16".
>
> Kartik
>
setwd("E:/Cooperation @ Delft-Nijmegen (Feb. 2006 - Sep.
2006)/Research/Study 20 - Roughness/Experiment 20a - Roughness Index
for CUReT textures/Statistics")

#  Concatenate the raw data files.
data.path = "../data files/"
(datafiles <- list.files(path=data.path, pattern="subject\_[0-9]+\.txt$"))
exp20a <- do.call('rbind',
  lapply(datafiles,
    function(x) read.table(paste(data.path, x, sep=""))))
rm(datafiles, data.path)


From HDoran at air.org  Mon Jul 31 02:29:52 2006
From: HDoran at air.org (Doran, Harold)
Date: Sun, 30 Jul 2006 20:29:52 -0400
Subject: [R] standardized random effects with ranef.lme()
Message-ID: <2323A6D37908A847A7C32F1E3662C80E2768C4@dc1ex01.air.org>

OK, I see how the standardized random effects are calculated. Here is
what I now see

library(nlme)

fm2 <- lme(distance~age, Orthodont)

# unstandardized
age_ranef <- ranef(fm2)[,2]

#standardized
age_Sranef <- ranef(fm2, standard=TRUE)[,2]

# We can use these to solve for the standard error, because the formula
according to help for ranef.lme is

# Standardized_randomEffects = random_effects/standard error

age_ranef/age_Sranef

# OK, now note the values are exactly the same. Now, look at

VarCorr(fm2)

You can see the value used to standardize is the standard deviation of
the random effect for age.

Now, the help function does say "divided by the corresponding standard
error". 

I've copied Doug Bates because the values in the stdDev column are the
standard deviations of the variance components and not standard errors
of those variance components. So, I'm not sure why the help says that
the standardized random effects are divided by the corresponding SE.
Maybe he can clarify if he has time.

I hope that helps
Harold


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Doran, Harold
> Sent: Sunday, July 30, 2006 3:40 PM
> To: Spencer Graves; Dirk Enzmann
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] standardized random effects with ranef.lme()
> 
>  
> > > Why do the results differ although the estimates (random
> > effects and
> > > thus their variances) are almost identical? I noticed that
> > lme() does
> > > not compute the standard errors of the variances of the
> > random effects
> > > - for several reasons, but if this is true, how does
> > ranef() calculate
> > > the standardized random effects (the help says: 
> > '"standardized" (i.e.
> > > divided by the corresponding estimated standard error)').
> > > 
> > > Is there a way to obtain similar results as in MLWin (or: 
> should I 
> > > prefer the results of ranef() for certain reasons)?
> 
> I think there are two different issues here. The lme function 
> does not produce a standard error of the variance component 
> as some other multilevel packages do. It is often recommended 
> in the multilevel literature to consider the p-value of the 
> variance components and "fix"
> or retain the variance if p < .05. There are good reasons not 
> to follow this practice.
> 
> If you were using lmer(), you still wouldn't get this 
> statistic, but you could use the MCMCsamp() function to 
> examine the distribution of the random effects.
> 
> The second issue (I think) is that the conditional variance 
> of the random effect is not the same as the standard error of 
> the variance of the random effect. From the definition in the 
> help of ranef.lme, I believe it is the random effect divided 
> by its conditional standard error. I don't know how to get 
> the posterior variance of the random effects in lme, but I do 
> in lmer, so we can experiment a bit. It has been a while 
> since I have really used lme and I do not think there is an 
> extractor function to get these and I didn't see the 
> variances in the model object.
> 
> Let's work through an example to see what we get. Here is what I see
> 
> library(nlme)
> data(Orthodont)
> detach(package:nlme)
> 
> library(Matrix)
> fm1 <- lmer(distance ~ age + (age|Subject), data = Orthodont) 
> # equivalent to # fm1 <- lme(distance ~ age, data=Orthodont)
> 
> # Extract the variances of the random effects qq <- 
> attr(ranef(fm1, postVar = TRUE)[[1]], "postVar")
> 
> # divide the random effects by their standard error of "age"
> Sranef_lmer <- ranef(fm1)[[1]][,2]/ sqrt(qq[2,2,])
> 
> library(nlme)
> # Now, run the lme model
> fm2 <- lme(distance ~ age, Orthodont)
> 
> # get the standardized random effects from lme Sranef_lme <- 
> ranef(fm2, standard=T)[2]
> 
> cor(Sranef_lmer, Sranef_lme)
> [1] 1
> 
> Notice the perfect correlation. But, the actual values in 
> Sranef_lme and Sranef_lmer are a bit different and I cannot 
> see why just yet. I need to go eat lunch, but I'll think about this. 
> 
> Maybe somebody else sees something.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tfjbl at mail.uas.alaska.edu  Sun Jul 30 19:17:17 2006
From: tfjbl at mail.uas.alaska.edu (tfjbl at mail.uas.alaska.edu)
Date: Sun, 30 Jul 2006 10:17:17 -0700
Subject: [R] re 11. uniroot  and function opposite signs warning
Message-ID: <9f6fc9905d.9905d9f6fc@mail.uas.alaska.edu>

Nurza,
Try running a while loop steping out until you have a start and finish 
thats the function is opposite in sign. You need a "start" and "finish"
where F is + and - on either side of the loop. Graphing F might help.


step<-10
checkme<-F(start)*F(finish+step)

while(checkme>0){
initialstep<-initialstep*2
checkme<-F(start)*F(finish+step)
}
answer<-uniroot(F,low=start,up=finish+step,maxiter=100)$root



Enjoy
Joe Liddle
tfjbl at uas.alaska.edu
-------------- next part --------------
An embedded message was scrubbed...
From: r-help-request at stat.math.ethz.ch
Subject: R-help Digest, Vol 41, Issue 30
Date: Sun, 30 Jul 2006 12:00:03 +0200
Size: 45962
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060730/f9063a18/attachment.mht 

From stuart.leask at nottingham.ac.uk  Mon Jul 31 09:52:41 2006
From: stuart.leask at nottingham.ac.uk (Stuart Leask)
Date: Mon, 31 Jul 2006 08:52:41 +0100
Subject: [R] main= bquote(paste("Results for ", beta, "3",
	==.(b1)))) doesn't work.
References: <002401c6b41d$55988cf0$0300a8c0@marco>
Message-ID: <01d601c6b476$4cd8b910$2de1f380@ad.nottingham.ac.uk>

b1<-3
plot(1,1, main= bquote(paste("Results for ", beta==.(b1))))

seems to work.

Stuart
Dr Stuart J Leask DM MRCPsych MA BChir
Senior Lecturer and Honorary Consultant in Clinical Psychiatry
University Dept of Psychiatry, Duncan Macmillan House
Porchester Road. Nottingham. NG3 6AA.
http://www.nottingham.ac.uk/psychiatry/staff/s_leask.html

----- Original Message ----- 
From: "Marco Boks" <marco.boks at inter.nl.net>
To: <r-help at stat.math.ethz.ch>
Sent: Sunday, July 30, 2006 10:15 PM
Subject: [R] main= bquote(paste("Results for ", beta, "3",==.(b1)))) doesn't 
work.


> Hi,
>
> I need to plot the beta as the symbol, followed by the index 3 as the 
> title of a graph.
>
> This code works> main= bquote(paste("Results for ", beta ==.(b1))
>
> but I also need the index 3.
> I tried (simplified):
>
>>plot(x,y, main= bquote(paste("Results for ", beta, "3", ==.(b1))))
>
> and a few other versions, but I can not get it to run properly.
>
> Any help would be greatly appreciated,
>
> Thanks
>
> Marco
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code. 


This message has been checked for viruses but the contents of an attachment
may still contain software viruses, which could damage your computer system:
you are advised to perform your own checks. Email communications with the
University of Nottingham may be monitored as permitted by UK legislation.


From petr.pikal at precheza.cz  Mon Jul 31 10:21:15 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 31 Jul 2006 10:21:15 +0200
Subject: [R] DOE in R
In-Reply-To: <200607290443.k6T4hcsn018549@rm-rstar.sfu.ca>
Message-ID: <44CDD99B.24295.80EA80@localhost>

See

?aov
?lm

something like
 
lm(result~yourfactor1+yourfactor2+yourfactor3+yourfactor5, 
data=yourdataframe)
or
aov(result~yourfactor1+yourfactor2+yourfactor3+yourfactor5, 
data=yourdataframe)

but the exact structure of lm or aov construction depends on what you 
want to test.

HTH
Petr


On 28 Jul 2006 at 21:43, dvrecko at sfu.ca wrote:

Date sent:      	Fri, 28 Jul 2006 21:43:38 -0700
From:           	dvrecko at sfu.ca
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] DOE in R
Send reply to:  	dvrecko at sfu.ca
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> Hi.
> 
> I'm a student in a graduate program at Simon Fraser University in
> Canada.
> 
> I am trying to run a simple screening experiment with some simulated
> data.
> 
> I simply want to do an ANOVA of an experiemnt with 5 factors (4 have 2
> levels, the last has 3 levels) and 48 runs (ie, full factorial).
> 
> The thing is that I have multiple observations for each level
> combination (run).
> 
> So,
> 
> 1) How do I do the anova based on the setup above?
> 
> and 
> 
> 2) More importantly, because of convergence issues for my simulations,
> I will likely have an unequal number of observations for the 48 runs.
> How can I do this?
> 
> Seems like a straightforward enough situation.
> 
> I am trying to avaoid writing my own C code to do the analysis since I
> am working under some pretty tight time constraints.
> 
> ANy help would be appreciated.
> 
> Thanks very much.
> 
> Dean Vrecko
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From arun.kumar.saha at gmail.com  Mon Jul 31 10:29:31 2006
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Mon, 31 Jul 2006 13:59:31 +0530
Subject: [R] Problem with allp ossible combination.
Message-ID: <d4c57560607310129g552f7359h9431db00bb781524@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060731/0830506a/attachment.pl 

From tamir at imp.univie.ac.at  Mon Jul 31 10:55:28 2006
From: tamir at imp.univie.ac.at (Ido M. Tamir)
Date: Mon, 31 Jul 2006 10:55:28 +0200
Subject: [R] Problem with allp ossible combination.
Message-ID: <200607311055.28092.tamir@imp.univie.ac.at>


>Now I want get a-b for all possible combinations of a and b.

outer(a,b,"-")

hth
ido


From rkrug at sun.ac.za  Mon Jul 31 11:01:56 2006
From: rkrug at sun.ac.za (Rainer M Krug)
Date: Mon, 31 Jul 2006 11:01:56 +0200
Subject: [R] Possible to subscribe to RNews?
Message-ID: <44CDC704.3080905@sun.ac.za>

Hi

is it possible to subscribe to RNews that it get's emailed as soon as it
is released or is there an announcement list for new issues?

Thanks

Rainer

-- 
Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Department of Conservation Ecology and Entomology
University of Stellenbosch
Matieland 7602
South Africa


From jacques.veslot at good.ibl.fr  Mon Jul 31 11:02:14 2006
From: jacques.veslot at good.ibl.fr (Jacques VESLOT)
Date: Mon, 31 Jul 2006 11:02:14 +0200
Subject: [R] Problem with allp ossible combination.
In-Reply-To: <d4c57560607310129g552f7359h9431db00bb781524@mail.gmail.com>
References: <d4c57560607310129g552f7359h9431db00bb781524@mail.gmail.com>
Message-ID: <44CDC716.6000707@good.ibl.fr>

diffs <- do.call(expand.grid, dt)
diffs$delta <- rowSums(expand.grid(dt$a, -dt$b))
-------------------------------------------------------------------
Jacques VESLOT

CNRS UMR 8090
I.B.L (2?me ?tage)
1 rue du Professeur Calmette
B.P. 245
59019 Lille Cedex

Tel : 33 (0)3.20.87.10.44
Fax : 33 (0)3.20.87.10.31

http://www-good.ibl.fr
-------------------------------------------------------------------


Arun Kumar Saha a ?crit :
> Dear R Users,
> 
> Suppose I have a dataset like this:
> 
>   a       b
> 
> 39700   485.00
> 39300   485.00
> 39100   480.00
> 38800   487.00
> 38800   492.00
> 39300   507.00
> 39500   493.00
> 39400   494.00
> 39500   494.00
> 39100   494.00
> 39200   490.00
> 
> Now I want get a-b for all possible combinations of a and b. Using two 'for'
> loop it is easy to calculate. But problem arises when row length of the data
> set is large eg. 1000 or more. Then R takes lot of time to do that. Can
> anyone please tell me whether there is any R-function to do such kind of job
> quickly?
> 
> Thanks and regards,
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From h.wickham at gmail.com  Mon Jul 31 11:09:24 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 31 Jul 2006 10:09:24 +0100
Subject: [R] Great R documentation
Message-ID: <f8e6ff050607310209i1e92fcc2yd8c42ca426f4d39b@mail.gmail.com>

Dear all,

I'm trying to improve the documentation I provide my R packages, and
to that end I'd like to find out what you think is great R
documentation.  I'm particularly interested in function documentation,
but great vignettes, websites or book are also of interest.

What is your favourite bit of R documentation, and why?

Thanks,

Hadley


From lewinger at usc.edu  Mon Jul 31 11:11:52 2006
From: lewinger at usc.edu (Juan Lewinger)
Date: Mon, 31 Jul 2006 02:11:52 -0700
Subject: [R] math symbols and text with mtext()
Message-ID: <f686f942e26b.44cd66e8@usc.edu>

Dear R users,

Two questions:

1) Is there a way to simplify the mtext() line below ?

beta=c(1,-1)
m=5
plot(1)
mtext( bquote(paste( beta == .(paste( "(", paste(beta, collapse=", "), ")" )) )), outer=TRUE,line=-3)

2) How do I get the embedded carriage return "\n" below to work, i.e for the text that follows it to appear on the next line?

beta=c(1,-1)
m=5
plot(1)
mtext( bquote(paste( beta == .(paste( "(", paste(beta, collapse=", "), ") " )), 
                         "\n Expected # of true positives = ", .(m))), outer=TRUE, line=-3)

Thanks in advance! 

Juan Pablo Lewinger, Ph.D. 
Department of Preventive Medicine 
Keck School of Medicine 
University of Southern California


From p.dalgaard at biostat.ku.dk  Mon Jul 31 11:14:35 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Jul 2006 11:14:35 +0200
Subject: [R] Possible to subscribe to RNews?
In-Reply-To: <44CDC704.3080905@sun.ac.za>
References: <44CDC704.3080905@sun.ac.za>
Message-ID: <x2u04yqgp0.fsf@viggo.kubism.ku.dk>

Rainer M Krug <rkrug at sun.ac.za> writes:

> Hi
> 
> is it possible to subscribe to RNews that it get's emailed as soon as it
> is released or is there an announcement list for new issues?

It is announced on R-announce, along with a few other selected items.
See, e.g.

http://tolstoy.newcastle.edu.au/R/announce/06/index.html


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ggrothendieck at gmail.com  Mon Jul 31 11:26:39 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 31 Jul 2006 05:26:39 -0400
Subject: [R] math symbols and text with mtext()
In-Reply-To: <f686f942e26b.44cd66e8@usc.edu>
References: <f686f942e26b.44cd66e8@usc.edu>
Message-ID: <971536df0607310226y7ef75ee3lec6419422e65156c@mail.gmail.com>

Try this:

m <- 5; beta <- c(-1, 1)
plot(1, main = bquote(atop(beta == .(deparse(beta)),
   "Expected number of true positives" == .(m))))


On 7/31/06, Juan Lewinger <lewinger at usc.edu> wrote:
> Dear R users,
>
> Two questions:
>
> 1) Is there a way to simplify the mtext() line below ?
>
> beta=c(1,-1)
> m=5
> plot(1)
> mtext( bquote(paste( beta == .(paste( "(", paste(beta, collapse=", "), ")" )) )), outer=TRUE,line=-3)
>
> 2) How do I get the embedded carriage return "\n" below to work, i.e for the text that follows it to appear on the next line?
>
> beta=c(1,-1)
> m=5
> plot(1)
> mtext( bquote(paste( beta == .(paste( "(", paste(beta, collapse=", "), ") " )),
>                         "\n Expected # of true positives = ", .(m))), outer=TRUE, line=-3)
>
> Thanks in advance!
>
> Juan Pablo Lewinger, Ph.D.
> Department of Preventive Medicine
> Keck School of Medicine
> University of Southern California
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From p.dalgaard at biostat.ku.dk  Mon Jul 31 11:40:20 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Jul 2006 11:40:20 +0200
Subject: [R] math symbols and text with mtext()
In-Reply-To: <f686f942e26b.44cd66e8@usc.edu>
References: <f686f942e26b.44cd66e8@usc.edu>
Message-ID: <x2psfmqfi3.fsf@viggo.kubism.ku.dk>

Juan Lewinger <lewinger at usc.edu> writes:

> Dear R users,
> 
> Two questions:
> 
> 1) Is there a way to simplify the mtext() line below ?
> 
> beta=c(1,-1)
> m=5
> plot(1)
> mtext( bquote(paste( beta == .(paste( "(", paste(beta, collapse=", "), ")" )) )), outer=TRUE,line=-3)
> 

The outer paste() is superfluous, but apart from that: Not really.
You're specifying a nonstandard formatting of beta, "(1, -1)" so you
also need to give all details. If you can live with a slightly
different format, try

mtext( bquote(beta == .(deparse(beta) ) ), outer=TRUE,line=-3)

and in fact,  

mtext(bquote( beta == .(substring(deparse(beta), 2)) ), outer=TRUE,line=-3)

will strip off the leading "c" and give you what you want, but only
slightly less cryptically.


> 2) How do I get the embedded carriage return "\n" below to work, i.e for the text that follows it to appear on the next line?
> 
> beta=c(1,-1)
> m=5
> plot(1)
> mtext( bquote(paste( beta == .(paste( "(", paste(beta, collapse=", "), ") " )), 
>                          "\n Expected # of true positives = ", .(m))), outer=TRUE, line=-3)
> 
> Thanks in advance! 
> 
> Juan Pablo Lewinger, Ph.D. 
> Department of Preventive Medicine 
> Keck School of Medicine 
> University of Southern California
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From karloh at mi.uib.no  Mon Jul 31 11:42:47 2006
From: karloh at mi.uib.no (Karl Ove Hufthammer)
Date: Mon, 31 Jul 2006 11:42:47 +0200
Subject: [R] Great R documentation
References: <f8e6ff050607310209i1e92fcc2yd8c42ca426f4d39b@mail.gmail.com>
Message-ID: <eakjao$fik$1@sea.gmane.org>

hadley wickham skreiv:

> I'm trying to improve the documentation I provide my R packages, and
> to that end I'd like to find out what you think is great R
> documentation.  I'm particularly interested in function documentation,
> but great vignettes, websites or book are also of interest.
> 
> What is your favourite bit of R documentation, and why?

I find that a graphic is worth *at least* a thousand words. I learn very
much from looking at examples of the graphical output of functions, and
it?s often much easier to look through ?example(function)? for a output
that looks similar to what I need, and to tweak it, than to read the
documentation to find out how to create the needed graphic (if it?s
possible at all).

And it?s fun too!

Example:
demo(graphics)
and
library(lattice)
example(xyplot)

These beautiful and interesting graphics.

My advice will therefore be to document every function with plenty of
interesting and useful and different (trivial variants on a graphic is not
interesting) and *pretty* examples.

And do not start the examples section with a very advanced example, with
many parameters and based on many transformations of a data set. For
example, do not write:

... 10 impossible-to-understand lines for generating or transforming
    the data set ...
fancyPlot(x,y,data=foo,lw=3,rty=2,bw="full",qrs="partial",method="bayes",
          nw="bar",clp=list(open.edge=TRUE,col=1,doubleMar=list(type="tr")),
          compute=c("o","p","lower","upper"),cex=1.2,xlim=range(x)*1.3)

Instead, start with:

fancyPlot(anscombe)

or

x=rnorm(100)
fancyPlot(x)

Then gradually make the examples more advanced or complete.

And do document/comment the examples. Say what?s going on, what the graphic
(or table, or textual output) shows and why it?s interesting.

One more thing: The ?lattice? package also has a nice introduction:

?Lattice

I believe all packages should have such a introduction, to give an overview
of the package, what it?s about and some examples of use.

One last advice: If you have a vignette or a demo, do tell in the
?Description? of ?library(help=package)?. It?s *very* easy to miss
otherwise (and many people don?t know that demos or even vignettes exist).

-- 
Karl Ove Hufthammer
E-mail and Jabber: karl at huftis.org


From jprocelewska at yahoo.com  Mon Jul 31 11:44:25 2006
From: jprocelewska at yahoo.com (Joanna Procelewska)
Date: Mon, 31 Jul 2006 11:44:25 +0200 (CEST)
Subject: [R] Algebraic operation on the missing values
Message-ID: <20060731094425.89904.qmail@web50204.mail.yahoo.com>

Hi all,

I have a large set of descriptors, which are stored as the vectors, each one
containing about 450 elements. Now I have to perform some algebraical
operations on this set to eliminate the redundant ones. The problem is, that
not all vales in the vectors are known. 
Are there any norm defined how should I process such vectors? Simple example:
having two vectors:
 
a      b
3      4
2      null
3      6
 
I can imagine that a+b is [7 null 9]', but what about scalar product? Is it
null or have it a value? 
I don't want to replace missing values with the concrete ones, but they
significantly complicate my computations. 

Does anyone know whether there are any ways to solve this problem?
Please share some experience. Really appreciate the help!

Sincerely,

Joanna


From e.rapsomaniki at mail.cryst.bbk.ac.uk  Mon Jul 31 11:56:12 2006
From: e.rapsomaniki at mail.cryst.bbk.ac.uk (Eleni Rapsomaniki)
Date: Mon, 31 Jul 2006 10:56:12 +0100
Subject: [R] memory problems when combining randomForests
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAA982@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAA982@usctmx1106.merck.com>
Message-ID: <1154339772.44cdd3bc767b7@webmail.cryst.bbk.ac.uk>

Hello

I've just realised attachments are not allowed, so the data for the example in
my previous message is:

pos.df=read.table("http://www.savefile.com/projects3.php?fid=6240314&pid=847249&key=119090",
header=T)

neg.df=read.table("http://fs07.savefile.com/download.php?pid=847249&fid=9829834&key=362779"",
header=T)

And my last two questions (promise!): 
The first is related to the order of columns (ie. explanatory variables). I get
different order of importance for my variables depending on their order in the
training data. Is there a parameter I could fiddle with (e.g. ntree) to get a
more stable importance order?

And finally, since interactions are not implemented, is there another method I
could use in R to find dependencies among categorical variables? (lm doesn't
accept categorical variables).

Many thanks
Eleni Rapsomaniki
Birkbeck College, UK


From jay.shin at pharma.ethz.ch  Mon Jul 31 12:35:39 2006
From: jay.shin at pharma.ethz.ch (Jay Shin)
Date: Mon, 31 Jul 2006 12:35:39 +0200
Subject: [R] LIMMA: makeContrasts() function
Message-ID: <ED97F4DF-B0EA-4379-9CE3-15BFA020FD49@pharma.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060731/b30546e6/attachment.pl 

From petr.pikal at precheza.cz  Mon Jul 31 13:25:10 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 31 Jul 2006 13:25:10 +0200
Subject: [R] Algebraic operation on the missing values
In-Reply-To: <20060731094425.89904.qmail@web50204.mail.yahoo.com>
Message-ID: <44CE04B6.19223.1295639@localhost>

Hi

see
?complete.cases and/or ?is.na for evaluating non missing entries. 

However in any operation in which you use NA value, result shall be 
NA as you do not know what actually is NA.

HTH
Petr





On 31 Jul 2006 at 11:44, Joanna Procelewska wrote:

Date sent:      	Mon, 31 Jul 2006 11:44:25 +0200 (CEST)
From:           	Joanna Procelewska <jprocelewska at yahoo.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Algebraic operation on the missing values

> Hi all,
> 
> I have a large set of descriptors, which are stored as the vectors,
> each one containing about 450 elements. Now I have to perform some
> algebraical operations on this set to eliminate the redundant ones.
> The problem is, that not all vales in the vectors are known. Are there
> any norm defined how should I process such vectors? Simple example:
> having two vectors:
> 
> a      b
> 3      4
> 2      null
> 3      6
> 
> I can imagine that a+b is [7 null 9]', but what about scalar product?
> Is it null or have it a value? I don't want to replace missing values
> with the concrete ones, but they significantly complicate my
> computations. 
> 
> Does anyone know whether there are any ways to solve this problem?
> Please share some experience. Really appreciate the help!
> 
> Sincerely,
> 
> Joanna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From JZajd at constellagroup.com  Mon Jul 31 14:10:04 2006
From: JZajd at constellagroup.com (Zajd, John)
Date: Mon, 31 Jul 2006 08:10:04 -0400
Subject: [R] Please HELP: Problem with BUILD command
Message-ID: <B341A577AB7B1E4C8CEA91349321B09C018E87D0@dur-exchange.corp.constellagroup.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060731/cbdf8ce6/attachment.pl 

From mscabral at fc.ul.pt  Mon Jul 31 11:57:18 2006
From: mscabral at fc.ul.pt (=?iso-8859-1?Q?Maria_Salom=E9_Esteves_Cabral?=)
Date: Mon, 31 Jul 2006 10:57:18 +0100
Subject: [R] glmmNQ
Message-ID: <3AA0B59C9640784C8956888BF8AFC5DD1FF241@fc-mailserver01.ul.pt>

Hi!
 
Can anyone let me know where is the function glmmNQ? It's said that it is in the MASS library but I can not find it.
 
Thanks
 
Salom?


From info at aghmed.fsnet.co.uk  Mon Jul 31 14:15:26 2006
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Mon, 31 Jul 2006 13:15:26 +0100
Subject: [R] Great R documentation
In-Reply-To: <f8e6ff050607310209i1e92fcc2yd8c42ca426f4d39b@mail.gmail.co
 m>
References: <f8e6ff050607310209i1e92fcc2yd8c42ca426f4d39b@mail.gmail.com>
Message-ID: <7.0.0.16.0.20060731130556.019f9e90@aghmed.fsnet.co.uk>

At 10:09 31/07/2006, hadley wickham wrote:
>Dear all,
>
>I'm trying to improve the documentation I provide my R packages, and
>to that end I'd like to find out what you think is great R
>documentation.  I'm particularly interested in function documentation,

Hadley, I do not think any bit of function documentation, if you mean 
the manual page type documents, is ever that enlightening unless you 
already know what the function does. For me the useful documents are 
the more extended narrative documents.

What I find helpful is:
start with what is the aim of this document
tell me what I am assumed to know first (and ideally tell me where to 
look if I do not)
start with an example using the defaults
tell me what the output means in terms of the scientific problem
tell me what action I might take when it gives me a warning (if that 
is predictable)
tell me about other packages with the same aim and why I should use this one
tell me where to go for more information

>but great vignettes, websites or book are also of interest.
>
>What is your favourite bit of R documentation, and why?
>
>Thanks,
>
>Hadley
>
>

Michael Dewey
http://www.aghmed.fsnet.co.uk


From ripley at stats.ox.ac.uk  Mon Jul 31 14:27:01 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 31 Jul 2006 13:27:01 +0100 (BST)
Subject: [R] glmmNQ
In-Reply-To: <3AA0B59C9640784C8956888BF8AFC5DD1FF241@fc-mailserver01.ul.pt>
References: <3AA0B59C9640784C8956888BF8AFC5DD1FF241@fc-mailserver01.ul.pt>
Message-ID: <Pine.LNX.4.64.0607311325130.22017@gannet.stats.ox.ac.uk>

On Mon, 31 Jul 2006, Maria Salom? Esteves Cabral wrote:

> Hi!
>  Can anyone let me know where is the function glmmNQ? It's said that it 
> is in the MASS library but I can not find it.

Where is it said to be in the MASS *package*?  Not in MASS the book, for 
sure.  The function of that name used in MASS the book has never been made 
available for use with R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Mon Jul 31 14:30:36 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 31 Jul 2006 13:30:36 +0100 (BST)
Subject: [R] Please HELP: Problem with BUILD command
In-Reply-To: <B341A577AB7B1E4C8CEA91349321B09C018E87D0@dur-exchange.corp.constellagroup.com>
References: <B341A577AB7B1E4C8CEA91349321B09C018E87D0@dur-exchange.corp.constellagroup.com>
Message-ID: <Pine.LNX.4.64.0607311323030.22017@gannet.stats.ox.ac.uk>

As we have said to you before, we need a reproducible example, and it is 
very likely that you are speculating.

There is no 'BUILD' command in R:  do you mean 'R CMD build'?  If so, 
that does not parse any `R program'.

On Mon, 31 Jul 2006, Zajd, John wrote:

> Greetings,
>  
> I am unable to successfully BUILD due to a file that apears to be too
> long.
>  
> I know it it due to the length of a particular R program because if I
> remove a line, even a comment line,
> from the file it then successfully builds. However, if I add the line
> back in, the build fails.
>  
> The file is only 14Kb long.
>  
> Any help or suggestions are greatly appreciated.
>  
> Thank you for your time.
> John Zajd
> Constella Group
> Raleigh, NC
> 919 313-7746
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ligges at statistik.uni-dortmund.de  Mon Jul 31 14:34:32 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 31 Jul 2006 14:34:32 +0200
Subject: [R] Please HELP: Problem with BUILD command
In-Reply-To: <B341A577AB7B1E4C8CEA91349321B09C018E87D0@dur-exchange.corp.constellagroup.com>
References: <B341A577AB7B1E4C8CEA91349321B09C018E87D0@dur-exchange.corp.constellagroup.com>
Message-ID: <44CDF8D8.5020302@statistik.uni-dortmund.de>

Zajd, John wrote:
> Greetings,
>  
> I am unable to successfully BUILD due to a file that apears to be too
> long.
>  
> I know it it due to the length of a particular R program because if I
> remove a line, even a comment line,
> from the file it then successfully builds. However, if I add the line
> back in, the build fails.
>  
> The file is only 14Kb long.


Strange. Can you make this file available, please?

Uwe Ligges



> Any help or suggestions are greatly appreciated.
>  
> Thank you for your time.
> John Zajd
> Constella Group
> Raleigh, NC
> 919 313-7746
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From A.Robinson at ms.unimelb.edu.au  Mon Jul 31 14:36:39 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Mon, 31 Jul 2006 22:36:39 +1000
Subject: [R] Three questions about a model for possibly periodic data with
	varying amplitude
Message-ID: <20060731123639.GH59927@ms.unimelb.edu.au>

Hi dear R community,

I have up to 12 measures of a protein for each of 6 patients, taken
every two or three days.  The pattern of the protein looks periodic,
but the height of the peaks is highly variable.  It's something like
this:

patient <- data.frame(
	day = c(1, 3, 5, 8, 10, 12, 15, 17, 19, 22, 24, 26),
	protein =  c(5, 3, 10, 7, 2, 8, 25, 12, 7, 20, 10, 5)
	)
plot(patient$day, patient$protein, type="b") 

My goal is two-fold: firstly, I need to test for periodicity, and
secondly, I need to try to predict the temporal location of future
peaks.  Of course, the peaks might be occurring on unmeasured days.

I have been looking at this model:

wave.form <- 
  deriv3( ~ sin(2*pi*((day-offset)/period + 0.25)) * amplitude + mean,
         c("period", "offset", "amplitude", "mean"),
         function(day, period, offset, amplitude, mean){})

curve(wave.form(x, period=7, offset=2, mean=5, amplitude=4), 
		   from=1, to=30)

So, for my purposes, the mean and the amplitude seem to be irrelevant;
I want to estimate the location and the offset.  To get going I've
been using the following approach:

require(nlme)

wave.1 <- gnls(protein ~ wave.form(day, period, offset, amplitude, mean),
               start=list(period=7, offset=0, amplitude=10, mean=0),
               weights=varPower(), data=patient)

 ... which seems to fit the wave form pretty nicely.  

Question 1) I'm wondering, however, if anyone can suggest any
            improvements to my model or fitting procedure, or general
            approach.

Generalizing to a non-linear mixed effects model is proving difficult.
For example,

#################################################################

set.seed(1234)

patients <- expand.grid(patient.no = 1:6,
			day = patient$day)

patient.params <- data.frame(patient.no = 1:6,
			     period = c(5,6,7,8,7,6),
			     offset = c(1,2,3,1,2,3),
			     amplitude = c(10,6,10,6,10,6),
			     mean = c(22,14,22,14,22,14))

patients <- merge(patients, patient.params)

patients$protein <- with(patients, 
		 wave.form(day, period, offset, amplitude, mean)) +
		 rnorm(n=dim(patients)[1], mean=5, sd=2)

patients <- groupedData(protein ~ day | patient.no, data=patients)

protein.nlme <- nlme(protein ~ 
	     wave.form(day, period, offset, amplitude, mean),
                     fixed = period + offset + mean + amplitude ~ 1,
                     random = period + offset ~ 1 | patient.no,
                     start = c(period=2*pi, offset=0, mean=10,
			   amplitude=5),
                     data = patients)

random.effects(protein.nlme) 

#################################################################

I get the following values for the BLUPS:

  period        offset
2      0 -5.738510e-09
4      0 -6.426104e-08
6      0  6.847430e-09
1      0  6.275570e-07
5      0 -1.486590e-06
3      0  9.221848e-07

It seems clear to me that these BLUPS are quite unsuitable.  

Question 2) Can anyone suggest how I might improve my use of nlme?
            Other than using more data :) 

Question 3) Finally, I'm also wondering what would be a suitable test for
            periodicity for these data. I'd like to test the null
            hypothesis that the data are not periodic.

All suggestions, discussion, welcome!

Best wishes

Andrew
-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au


From jprocelewska at yahoo.com  Mon Jul 31 14:37:33 2006
From: jprocelewska at yahoo.com (Joanna Procelewska)
Date: Mon, 31 Jul 2006 14:37:33 +0200 (CEST)
Subject: [R] Algebraic operation on the missing values
In-Reply-To: <44CE04B6.19223.1295639@localhost>
Message-ID: <20060731123733.31125.qmail@web50213.mail.yahoo.com>

Thanks for the answer.

The problem is I have to perform a forward selection on the set and in every
step construct an orthonormal base for the subspace spanned on the selected
vectors. This means that I can use only the "full" vectors for the constructing
a base, or? 

Joanna

--- Petr Pikal <petr.pikal at precheza.cz> schrieb:

> Hi
> 
> see
> ?complete.cases and/or ?is.na for evaluating non missing entries. 
> 
> However in any operation in which you use NA value, result shall be 
> NA as you do not know what actually is NA.
> 
> HTH
> Petr
> 
> > Hi all,
> > 
> > I have a large set of descriptors, which are stored as the vectors,
> > each one containing about 450 elements. Now I have to perform some
> > algebraical operations on this set to eliminate the redundant ones.
> > The problem is, that not all vales in the vectors are known. Are there
> > any norm defined how should I process such vectors? Simple example:
> > having two vectors:
> > 
> > a      b
> > 3      4
> > 2      null
> > 3      6
> > 
> > I can imagine that a+b is [7 null 9]', but what about scalar product?
> > Is it null or have it a value? I don't want to replace missing values
> > with the concrete ones, but they significantly complicate my
> > computations. 
> > 
> > Does anyone know whether there are any ways to solve this problem?
> > Please share some experience. Really appreciate the help!
> > 
> > Sincerely,
> > 
> > Joanna
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
>


From d.j.stekel at bham.ac.uk  Mon Jul 31 15:22:14 2006
From: d.j.stekel at bham.ac.uk (Dov Stekel)
Date: Mon, 31 Jul 2006 14:22:14 +0100
Subject: [R] Random Effects Model with Interacting Covariates
Message-ID: <b9fe0e51e311121f03b0b8768caf9c74@bham.ac.uk>

Hi

I have been asked by a colleague to perform a statistical analysis 
which uses random effects - but I am struggling to get this to work 
with nlme in R. Help would be very much appreciated!

Essentially, the data consists of:

10 patients. Each patient has been given three different treatments (on 
three separate days). 15 measurements (continuous variable) have been 
taken from each patient both before and after each of the treatments.  
So the data looks like:

Patient	When	Treat	Measurement
a		before	A		10.3
a		before	A		11.2
...
a		after		A		12.4
...
a		before	B		11.6
...
a		after		B		...

and the same for treatment C, patients, b,c,d, etc.

My colleague would like to test to see if the treatments are different 
from each other. i.e., is the change (before to after) due to the 
treatments different between the treatments. It would seem to me like a 
random effects model in which we are interested in the significance of 
the interaction terms Treat:When, with repeated measures in the 
patients (who are random effects, but crossed with the covariates). 
Unfortunately, the groupedData formula only lets me put a single 
covariate on the LHS - nothing as complicated as this!

I could, of course, advise her to simply combine all 30 data points for 
each treatment in each patient into a single number (representing 
difference between before and after), but is there a way to use all the 
data in an LME?

Thanks!


Dov



**************************************************************

Dr Dov Stekel
Lecturer in Bioinformatics
School of Biosciences
University of Birmingham
Birmingham B15 2TT
Tel: +44 121 414 4209
Email: d.j.stekel at bham.ac.uk


From dirk.enzmann at uni-hamburg.de  Mon Jul 31 15:35:45 2006
From: dirk.enzmann at uni-hamburg.de (Dirk Enzmann)
Date: Mon, 31 Jul 2006 15:35:45 +0200
Subject: [R] standardized residuals (random effects) using nlme and ranef
In-Reply-To: <44CCC2DB.4090809@pdf.com>
References: <44C527E9.3080400@uni-hamburg.de> <44CCC2DB.4090809@pdf.com>
Message-ID: <44CE0731.2030008@uni-hamburg.de>

As suggested I try another post.

First I give a reproducible example. The example data set has been 
provided by I. Plewis (1997), Statistics in Education. London: Arnold (I 
include the residuals obtained by MLWin):

# ---------------------------------------------

library(nlme)

# Example data from Plewis

BaseURL="http://www.ottersbek.de/"
ds32 = read.fwf(paste(BaseURL,'ds32.dat',sep=''),
                       widths=c(9,10,10,10,10,10),
                       col.names=c('class','pupil','zcurric',
                                   'curric','zmath1','math2'),
                       colClasses=c('factor','factor','numeric',
                                    'numeric','numeric','numeric'))

# Random slopes model (p. 44):
model.4b = lme(math2~zcurric,random=~zcurric|class,method='ML',data=ds32)

# Random effects (intercept and slope residuals) of level 1 (class):
RandEff = ranef(model.4b,level=1)

# Results of MLWin (c300 = intercept residuals of level "class",
#                   c301 = slope residuals of level "class",
#                   c304 = standardized intercept residuals,
#                   c305 = standardized slope residuals):
MLWinRes = read.fwf(paste(BaseURL,'resid_ex32.txt',sep=''),
                     widths=c(15,15,15,15),
                     col.names=c('c300','c301','c304','c305'),
                     colClasses=c('numeric','numeric','numeric','numeric'))

# They are identical to the results of MLWin (c300, c301):
round(cbind(RandEff,MLWinRes[,1:2]),5)

# However, using option "standard" the results differ considerably:
round(cbind(ranef(model.4b,level=1,standard=T),MLWinRes[,3:4]),5)

# ---------------------------------------------

 From the RSiteSearch("MLWin") there are two hits that seem to be 
relevant: One of Douglas Bates

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/53097.html

where he explains why he regards getting standard errors of the 
estimates of variance components as not being important. I think (but 
I'm not sure) that this implies that I should not use standardized 
residuals, as well.

Secondly a post of Roel de Jong

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/62280.html

However, I can't figure out how I could make use of his suggestion to 
obtain the standardized residuals I am looking for.

A part of the answer has been given in an answer to my previous post by 
Harold Doran. He showed that the so called "standardized residuals" 
obtained by ranef() using the option "standard=T" are the residuals 
devided by the standard deviation of the random effects, not divided by 
the "corresponding standard error" as stated in the ranef() help - if I 
understood him correctly.

In the multilevel list he also showed how to create a caterpillar plot 
using lmer() and the errbar() function of Hmisc (I modified his example 
to adapt it to my data):

# ------------------------------------

detach(package:nlme)
library(Matrix)
library(Hmisc)

# Fit a sample model:
fm1 = lmer(math2 ~ zcurric + (zcurric|class), ds32, method='ML', 
control=list(gradient=FALSE, niterEM=0))

.Call("mer_gradComp", fm1, PACKAGE = "Matrix")

# extract random effects:
fm1.blup = ranef(fm1)$class

#extract variances and multiple by scale factor:
fm1.post.var = fm1 at bVar$class * attr(VarCorr(fm1),"sc")**2

# posterior variance of slope on fm1:
fm1.post.slo = sqrt(fm1.post.var[2,2,])

# Slopes:
x = fm1.blup[,2]+outer(fm1.post.slo, c(-1.96,0,1.96))

# This code creates a catepillar plot using the errbar function:
x <- x[order(x[,2]) , ]
print(errbar(1:dim(x)[1], x[,2],x[,3],x[,1],ylab='Slopes', xlab='Classes'))
abline(h=0)

# ------------------------------------

Is it correct that I can obtain a respecive plot for the intercepts in a 
similar way?

# ------------------------------------

# posterior variance of intercept on fm1.post.int:
fm1.post.int = sqrt(fm1.post.var[1,1,])

# Intercepts:
x = fm1.blup[,1]+outer(fm1.post.int, c(-1.96,0,1.96))

# This code creates a catepillar plot using the errbar function
x <- x[order(x[,2]) , ]
print(errbar(1:dim(x)[1], x[,2],x[,3],x[,1],ylab='Intercepts', 
xlab='Classes'))
abline(h=0)

# ------------------------------------

To sum up, I can't figure out how MLWin calculates the standardized 
residuals. But I understand that this is not a question for the R list. 
Nevertheless, it would help if someone could point me to some arguments 
why not to use them and stick to the results obtainable by ranef().

Spencer Graves wrote:

>       Have you tried RSiteSearch("MLWin")?  I just got 29 hits.  I 
> wonder if any one of these might relate to your question?
> 
>       If you would like more help on this issue from this listserve, 
> please submit another post, preferably illustrating your question with 
> the simplest possible self-contained example that illustrates your 
> question, perhaps like the following:
> 
>       fm1.16 <- lme(distance~age, data=Orthodont[1:16,],
>            random=~age|Subject)
> 
>       Hope this helps.
>       Spencer Graves
> p.s.  PLEASE do read the posting guide 
> "www.R-project.org/posting-guide.html" and provide commented, minimal, 
> self-contained, reproducible code.
> 
> Dirk Enzmann wrote:
> 
>> Using ranef() (package nlme, version 3.1-75) with an 'lme' object I 
>> can obtain random effects for intercept and slope of a certain level 
>> (say: 1) - this corresponds to (say level 1) "residuals" in MLWin. 
>> Maybe I'm mistaken here, but the results are identical.
>>
>> However, if I try to get the standardized random effects adding the 
>> paramter "standard=T" to the specification of ranef(), the results 
>> differ considerably from the results of MLWin (although MLWin defines 
>> "standardized" in the same way as "divided by its estimated 
>> (diagnostic) standard error").
>>
>> Why do the results differ although the estimates (random effects and 
>> thus their variances) are almost identical? I noticed that lme() does 
>> not compute the standard errors of the variances of the random effects 
>> - for several reasons, but if this is true, how does ranef() calculate 
>> the standardized random effects (the help says: '"standardized" (i.e. 
>> divided by the corresponding estimated standard error)').
>>
>> Is there a way to obtain similar results as in MLWin (or: should I 
>> prefer the results of ranef() for certain reasons)?
>>
>> Dirk
>>
>> -----------------------------
>> R version: 2.3.1 Patched (2006-06-21 r38367)

-- 
*************************************************
Dr. Dirk Enzmann
Institute of Criminal Sciences
Dept. of Criminology
Edmund-Siemers-Allee 1
D-20146 Hamburg
Germany

phone: +49-(0)40-42838.7498 (office)
        +49-(0)40-42838.4591 (Billon)
fax:   +49-(0)40-42838.2344
email: dirk.enzmann at uni-hamburg.de
www: 
http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html


From erich.neuwirth at univie.ac.at  Mon Jul 31 15:37:31 2006
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Mon, 31 Jul 2006 15:37:31 +0200
Subject: [R] Monospaced fonts in legends
Message-ID: <44CE079B.9020805@univie.ac.at>

Is there a way of using monospaced fonts in legends in a plot,
but still using standard proportionally spaced fonts for all the titles?


-- 
Erich Neuwirth, University of Vienna
Faculty of Computer Science
Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-39464 Fax: +43-1-4277-39459


From Greg.Snow at intermountainmail.org  Mon Jul 31 16:06:46 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 31 Jul 2006 08:06:46 -0600
Subject: [R] boxcox transformation
References: <002201c6b2db$86d81d50$5a03a8c0@msc.de>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB12A096@LP-EXCHVS07.CO.IHC.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060731/67b64431/attachment.pl 

From jon3smith at hotmail.com  Mon Jul 31 16:08:22 2006
From: jon3smith at hotmail.com (Jonathan Smith)
Date: Mon, 31 Jul 2006 11:08:22 -0300
Subject: [R] user defined covariance structure
Message-ID: <BAY114-F26DF68791CC358DA25CFBFFC5C0@phx.gbl>

I am writing as I am still having trouble trying to define my own covariance 
matrix.  My code is displayed below.  I am defining the covariance matrix in 
the form of an AR1 process so it can be easily checked if working correctly. 
  Another question I have is if it is possible to define the matrix without 
giving p a specific value and leaving it in as a coefficient.  For the 
reason that it can be estimated when model is run.  I figure that is the way 
AR1 works in GLS.
Thank you
Jon Smith
I would appreciate any help s I am stuck here and need to figure this out 
prior to continuing my research.

function ()
{
library(nlme)

tim<-c(1,2,3,4,1,2,3,4,1,2,3,4,1,2,3,4)
peep<-c(1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4)
y<-c(11.78,9.53,11.03,9.89,10.80,8.74,10.25,10.69,5.60,7.27,6.81,4.56,7.01,5.64,6.30,8.31)
#This y data was created from and AR1 model with correlation coefficient 
equaling 0.7.

timMat<-matrix(c(tim),ncol=1,nrow=16)
peepMat<-matrix(c(peep),ncol=1,nrow=16)
yMat<-matrix(c(y),ncol=1,nrow=16)
dataframe<-data.frame(yMat,timMat,peepMat)
p=0.7
g=1

tester2<-corSymm(value = c(p^(1),p^(2),p^(3),p^(1),p^(2),p^(1)),form = ~ 
timMat|peepMat)
tester2<-Initialize(tester2, data = dataframe)
testMat2<-corMatrix(tester2)
print(testMat2)
# this appears to be working correctly

#smanGls<-gls(yMat~timMat,data = dataframe, corr = corAR1(form = 
~timMat|peepMat))
#		works perfectly

smanGls<-gls(yMat~timMat,data = dataframe, corr = corSymm(tester2))

arsum<-summary(smanGls)
print(arsum)

#this is what message I get when I try to use the covarince matrix I 
defined.
#Error in gls(yMat ~ timMat, data = dataframe, corr = corSymm(tester2)) :
  #      false convergence (8)

}


From jrkrideau at yahoo.ca  Mon Jul 31 16:22:54 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 31 Jul 2006 10:22:54 -0400 (EDT)
Subject: [R] Great R documentation
In-Reply-To: <f8e6ff050607310209i1e92fcc2yd8c42ca426f4d39b@mail.gmail.com>
Message-ID: <20060731142254.27386.qmail@web33806.mail.mud.yahoo.com>


--- hadley wickham <h.wickham at gmail.com> wrote:

> Dear all,
> 
> I'm trying to improve the documentation I provide my

I am a new user and at moment I am finding "An
Introduction to S and the Hmisc and Design Libraries?
by Carlos Alzola and Frank E. Harrell is very helpful
as it explains some "simple" data manipulations that
other documentation seems to assume one will know. 

My opinion is that much of the documentation is very
good but very terse and at least in the help files
sometimes a bit too clever.   There seldom seems to be
enough  explaination as to why something does
something which has often left me able to do exactly
what I want to do but having a difficult time
generalizing.


Of course, if I could track down whoever at my local
university has taken out all the R books I might be
much better off. :)


From ripley at stats.ox.ac.uk  Mon Jul 31 16:24:56 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 31 Jul 2006 15:24:56 +0100 (BST)
Subject: [R] Monospaced fonts in legends
In-Reply-To: <44CE079B.9020805@univie.ac.at>
References: <44CE079B.9020805@univie.ac.at>
Message-ID: <Pine.LNX.4.64.0607311522160.17346@gannet.stats.ox.ac.uk>

On Mon, 31 Jul 2006, Erich Neuwirth wrote:

> Is there a way of using monospaced fonts in legends in a plot,
> but still using standard proportionally spaced fonts for all the titles?

Yes, just use the family= argument as appropriate, e.g. par(family="mono") 
before calling legend.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Greg.Snow at intermountainmail.org  Mon Jul 31 16:22:10 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 31 Jul 2006 08:22:10 -0600
Subject: [R] placing rectangle behind plot
References: <971536df0607291420r456bccdfwd809fceaa3b98c17@mail.gmail.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB12A097@LP-EXCHVS07.CO.IHC.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060731/043c9822/attachment.pl 

From tlumley at u.washington.edu  Mon Jul 31 16:25:54 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 31 Jul 2006 07:25:54 -0700 (PDT)
Subject: [R] Log color scale
In-Reply-To: <44CC892F.8090906@statistik.uni-dortmund.de>
References: <97faa3210607291916r2fade731w65e3b79fb37d6a1b@mail.gmail.com>
	<44CC6A04.9070103@7d4.com> <44CC892F.8090906@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.64.0607310724500.32031@homer23.u.washington.edu>

On Sun, 30 Jul 2006, Uwe Ligges wrote:

> vincent at 7d4.com wrote:
>
>> Kartik Pappu a ?crit :
>>
>>
>>> However I need to plot my data in a log transformed color scale.  Is
>>> this possible?  I will be happy to explain further, but basically I
>>> need to do this because there are large variations in the max and min
>>> values of my raw data and I am trying to highlight the differences in
>>> the values at the lower end of my raw data (while still displaying the
>>> values at the high end of the spectrum for comparison) so if figured
>>> the best way to represent this on a (RGB) color scale is to do it with
>>> a log transform.  I do not want to use too many colors because that
>>> make the figure too busy.
>>> Any suggestions on how to achieve this.
>>
>>
>> Don't use too much time to search a good palette, build it yourself.
>
> It is worth looking into the packages "RColorBrewer" and "colorspace".
> Some people thought about colors in graphics and wrote some code that
> might result in more appropriate colors and palettes than those that are
> quickly hacked...
>

colorRampPalette() will interpolate between colors (eg those from one of 
the ColorBrewer palettes) and has a bias= argument to make the colors 
closer together at one end of the scale.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

From Greg.Snow at intermountainmail.org  Mon Jul 31 16:49:17 2006
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 31 Jul 2006 08:49:17 -0600
Subject: [R] Power of a single sample binomial test
References: <44CD003A.3020304@psyctc.org>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB12A098@LP-EXCHVS07.CO.IHC.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060731/cecc963a/attachment.pl 

From HDoran at air.org  Mon Jul 31 17:20:32 2006
From: HDoran at air.org (Doran, Harold)
Date: Mon, 31 Jul 2006 11:20:32 -0400
Subject: [R] standardized residuals (random effects) using nlme and ranef
Message-ID: <2323A6D37908A847A7C32F1E3662C80E2768FD@dc1ex01.air.org>

 > To sum up, I can't figure out how MLWin calculates the 
> standardized residuals. But I understand that this is not a 
> question for the R list. 
> Nevertheless, it would help if someone could point me to some 
> arguments why not to use them and stick to the results 
> obtainable by ranef().


Hi Dirk:

Well, it is interesting that mlWin and lmer generate the same exact
random effects but different results for the standardized random
effects. Now, my prior post showed exactly how lme calculates the
standardized random effects, so this is now totally transparent.

What I would recommend you do is this

1) Calculate the unstandardized random effects in mlWin
2) Calculate the standardized random effects in mlWin
3) Divide the mlWin unstandarized random effects by the standarized
random effects. This will show what denominator is used to standardize
the random effects.

Basically, just replicate what I did in my prior email using the mlWin
data.

Clearly, since R and mlWin generate the exact same unstandardized random
effects, but different standardized results, the difference lies in what
denominator is used. In R, we know what denominator is used and it is
the standard deviation of the variance component for the random effect.

If you do the steps above, you will solve for the denominator used in
mlWins calculation. This will solve your problem.

Harold


From helprhelp at gmail.com  Mon Jul 31 17:38:17 2006
From: helprhelp at gmail.com (Weiwei Shi)
Date: Mon, 31 Jul 2006 10:38:17 -0500
Subject: [R] memory problems when combining randomForests
In-Reply-To: <1154339772.44cdd3bc767b7@webmail.cryst.bbk.ac.uk>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAA982@usctmx1106.merck.com>
	<1154339772.44cdd3bc767b7@webmail.cryst.bbk.ac.uk>
Message-ID: <cdf817830607310838lce85b1dy6107884b7f25561e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060731/22c06af3/attachment.pl 

From gattuso2 at obs-vlfr.fr  Mon Jul 31 18:12:37 2006
From: gattuso2 at obs-vlfr.fr (Gattuso, Jean-Pierre)
Date: Mon, 31 Jul 2006 18:12:37 +0200
Subject: [R] Help with documentation of a data set
Message-ID: <44CE2BF5.1040108@obs-vlfr.fr>

Hi:

I am trying to add a data set in my package but get the following error 
when the package is checked:

	Undocumented data sets:
   	seacarb_test
	All user-level objects in a package should have documentation entries.
	See chapter 'Writing R documentation files' in manual 'Writing R
	Extensions'.

However, I do have a file "seacarb_test.Rd" in the man directory. 
Reading the manual 'Writing R Extensions' did not help.

What am I doing wrong?

Thanks,
jp


From e.rapsomaniki at mail.cryst.bbk.ac.uk  Mon Jul 31 18:45:37 2006
From: e.rapsomaniki at mail.cryst.bbk.ac.uk (Eleni Rapsomaniki)
Date: Mon, 31 Jul 2006 17:45:37 +0100
Subject: [R] memory problems when combining randomForests
In-Reply-To: <cdf817830607310838lce85b1dy6107884b7f25561e@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAA982@usctmx1106.merck.com>
	<1154339772.44cdd3bc767b7@webmail.cryst.bbk.ac.uk>
	<cdf817830607310838lce85b1dy6107884b7f25561e@mail.gmail.com>
Message-ID: <1154364337.44ce33b17dced@webmail.cryst.bbk.ac.uk>

Hi Andy,

> > I get different order of importance for my variables depending on their
order in the training data.

Perhaps answering my own question, the change in importance rankings could be
attributed to the fact that before passing my data to randomForest I impute the
missing values randomly (using the combined distributions of pos+neg), so the
data seen by RF is slightly different. Then combining this with the fact that
RF chooses data randomly it makes sense to see different rankings.

In a previous thread regarding simplifying variables:
http://thread.gmane.org/gmane.comp.lang.r.general/6989/focus=6993

you say:
"The basic problem is that when you select important variables by RF and then
re-run RF with those variables, the OOB error rate become biased downward.
As you iterate more times, the "overfitting" becomes more and more severe
(in the sense that, the OOB error rate will keep decreasing while error rate
on an independent test set will be flat or increases)" 

But if every time you remove a variable you pass some test data (ie data not
used to train the model) and base the performance of the new, reduced model on
the error rate on the confusion matrix for the test data, then this
"overfitting" should not be an issue, right?  (unless of course you were
referring to unsupervised learning).

Best regards
Eleni Rapsomaniki
Birkbeck College, UK


From bates at stat.wisc.edu  Mon Jul 31 18:48:39 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 31 Jul 2006 11:48:39 -0500
Subject: [R] standardized residuals (random effects) using nlme and ranef
In-Reply-To: <44CE0731.2030008@uni-hamburg.de>
References: <44C527E9.3080400@uni-hamburg.de> <44CCC2DB.4090809@pdf.com>
	<44CE0731.2030008@uni-hamburg.de>
Message-ID: <40e66e0b0607310948r3c451e7dve4776b2fc1480db9@mail.gmail.com>

Thank you for providing the reproducible example and the explanation
of what you are seeking to calculate.

On 7/31/06, Dirk Enzmann <dirk.enzmann at uni-hamburg.de> wrote:
> As suggested I try another post.
>
> First I give a reproducible example. The example data set has been
> provided by I. Plewis (1997), Statistics in Education. London: Arnold (I
> include the residuals obtained by MLWin):
>
> # ---------------------------------------------
>
> library(nlme)
>
> # Example data from Plewis
>
> BaseURL="http://www.ottersbek.de/"
> ds32 = read.fwf(paste(BaseURL,'ds32.dat',sep=''),
>                        widths=c(9,10,10,10,10,10),
>                        col.names=c('class','pupil','zcurric',
>                                    'curric','zmath1','math2'),
>                        colClasses=c('factor','factor','numeric',
>                                     'numeric','numeric','numeric'))
>
> # Random slopes model (p. 44):
> model.4b = lme(math2~zcurric,random=~zcurric|class,method='ML',data=ds32)
>
> # Random effects (intercept and slope residuals) of level 1 (class):
> RandEff = ranef(model.4b,level=1)
>
> # Results of MLWin (c300 = intercept residuals of level "class",
> #                   c301 = slope residuals of level "class",
> #                   c304 = standardized intercept residuals,
> #                   c305 = standardized slope residuals):
> MLWinRes = read.fwf(paste(BaseURL,'resid_ex32.txt',sep=''),
>                      widths=c(15,15,15,15),
>                      col.names=c('c300','c301','c304','c305'),
>                      colClasses=c('numeric','numeric','numeric','numeric'))
>
> # They are identical to the results of MLWin (c300, c301):
> round(cbind(RandEff,MLWinRes[,1:2]),5)
>
> # However, using option "standard" the results differ considerably:
> round(cbind(ranef(model.4b,level=1,standard=T),MLWinRes[,3:4]),5)

Your summation below of the cause of this inconsistency is correct.
As Harold explains the "standardized" random effects returned by ranef
in the nlme package are the "estimates" (actually the "predictors") of
the random effects divided by the estimated standard deviation of
those random effects, not by the estimated standard error as stated in
the documentation.  I will correct the documentation.

That is, "standardized random effects" in the nlme package are
produced by dividing all the intercept random effects by the same
number (2.9723) and all the slope random effects by 2.0014.
> rr1 <- ranef(model.4b)
> rr2 <- ranef(model.4b, standard = TRUE)
> rr1/rr2
          (Intercept)  zcurric
        8    2.972373 2.001491
       12    2.972373 2.001491
       17    2.972373 2.001491
       22    2.972373 2.001491
       23    2.972373 2.001491
       28    2.972373 2.001491
       29    2.972373 2.001491
       30    2.972373 2.001491
       31    2.972373 2.001491
       32    2.972373 2.001491
       33    2.972373 2.001491
       34    2.972373 2.001491
       35    2.972373 2.001491
       36    2.972373 2.001491
       37    2.972373 2.001491
       38    2.972373 2.001491
       39    2.972373 2.001491
       41    2.972373 2.001491
       42    2.972373 2.001491
       43    2.972373 2.001491
       44    2.972373 2.001491
       45    2.972373 2.001491
       46    2.972373 2.001491
       47    2.972373 2.001491

Another way of defining a standardization would be to use what Harold
Doran and others call the "posterior variance-covariance" of the
random effects.  On reflection I think I would prefer the term
"conditional variance" but I use "posterior" below.

Strictly speaking the random effects are not parameters in the model -
they are unobserved random variables.  Conditional on the values of
the parameters in the model and on the observed data, the random
effects are normally distributed with a mean and variance-covariance
that can be calculated.

Influenced by Harold I added an optional argument "postVar" to the
ranef extractor function for lmer models (but apparently I failed to
document it - I will correct that).  If you fit your model with lmer,
as you do below, you can standardize the  random effects according to
the conditional variances by

> fm1 <- lmer(math2 ~ zcurric + (zcurric|class), ds32, method='ML')
> rr <- ranef(model.4b, postVar = TRUE)
> str(rr)  # shows the structure of the object rr
Formal class 'ranef.lmer' [package "Matrix"] with 1 slots
  ..@ .Data:List of 1
  .. ..$ :`data.frame':	24 obs. of  2 variables:
  .. .. ..$ (Intercept): num [1:24]  0.860 -1.962 -1.105  4.631 -0.781 ...
  .. .. ..$ zcurric    : num [1:24]  0.2748 -1.3194  0.0841  0.5958  0.8080 ...
  .. .. ..- attr(*, "postVar")= num [1:2, 1:2, 1:24]  2.562 -0.585
-0.585  1.837  3.027 ...

The two sets of conditional standard deviations are

> sqrt(attr(rr1, "postVar")[1,1,])
 [1] 1.600589 1.739768 1.466261 1.542172 1.942629 1.506112 1.892815 1.665414
 [9] 1.377917 1.574244 1.755679 2.039292 1.887651 1.451916 1.732030 1.659786
[17] 1.987397 1.795273 1.542049 1.784441 1.629663 1.753223 1.585520 1.470161
> sqrt(attr(rr1, "postVar")[2,2,])
 [1] 1.355537 1.630075 1.506560 1.378282 1.790906 1.310406 1.341185 1.512708
 [9] 1.397492 1.808535 0.859909 1.867273 1.626857 1.669984 1.520560 1.489380
[17] 1.989123 1.326610 1.613399 1.074365 1.764128 1.455202 1.489311 1.942117

However, as you have seen, standardizing the conditional means by
these conditional standard deviations still does not reproduce the
results from MLWin.

If you can determine what is being calculated by MLWin or if you can
determine that there is a bug in the lmer calculations I would be
delighted to hear about it.

Thanks again for a very thorough report and for the reproducible example.

> # ---------------------------------------------
>
>  From the RSiteSearch("MLWin") there are two hits that seem to be
> relevant: One of Douglas Bates
>
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/53097.html
>
> where he explains why he regards getting standard errors of the
> estimates of variance components as not being important. I think (but
> I'm not sure) that this implies that I should not use standardized
> residuals, as well.
>
> Secondly a post of Roel de Jong
>
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/62280.html
>
> However, I can't figure out how I could make use of his suggestion to
> obtain the standardized residuals I am looking for.
>
> A part of the answer has been given in an answer to my previous post by
> Harold Doran. He showed that the so called "standardized residuals"
> obtained by ranef() using the option "standard=T" are the residuals
> devided by the standard deviation of the random effects, not divided by
> the "corresponding standard error" as stated in the ranef() help - if I
> understood him correctly.
>
> In the multilevel list he also showed how to create a caterpillar plot
> using lmer() and the errbar() function of Hmisc (I modified his example
> to adapt it to my data):
>
> # ------------------------------------
>
> detach(package:nlme)
> library(Matrix)
> library(Hmisc)
>
> # Fit a sample model:
> fm1 = lmer(math2 ~ zcurric + (zcurric|class), ds32, method='ML',
> control=list(gradient=FALSE, niterEM=0))
>
> .Call("mer_gradComp", fm1, PACKAGE = "Matrix")
>
> # extract random effects:
> fm1.blup = ranef(fm1)$class
>
> #extract variances and multiple by scale factor:
> fm1.post.var = fm1 at bVar$class * attr(VarCorr(fm1),"sc")**2
>
> # posterior variance of slope on fm1:
> fm1.post.slo = sqrt(fm1.post.var[2,2,])
>
> # Slopes:
> x = fm1.blup[,2]+outer(fm1.post.slo, c(-1.96,0,1.96))
>
> # This code creates a catepillar plot using the errbar function:
> x <- x[order(x[,2]) , ]
> print(errbar(1:dim(x)[1], x[,2],x[,3],x[,1],ylab='Slopes', xlab='Classes'))
> abline(h=0)
>
> # ------------------------------------
>
> Is it correct that I can obtain a respecive plot for the intercepts in a
> similar way?
>
> # ------------------------------------
>
> # posterior variance of intercept on fm1.post.int:
> fm1.post.int = sqrt(fm1.post.var[1,1,])
>
> # Intercepts:
> x = fm1.blup[,1]+outer(fm1.post.int, c(-1.96,0,1.96))
>
> # This code creates a catepillar plot using the errbar function
> x <- x[order(x[,2]) , ]
> print(errbar(1:dim(x)[1], x[,2],x[,3],x[,1],ylab='Intercepts',
> xlab='Classes'))
> abline(h=0)
>
> # ------------------------------------
>
> To sum up, I can't figure out how MLWin calculates the standardized
> residuals. But I understand that this is not a question for the R list.
> Nevertheless, it would help if someone could point me to some arguments
> why not to use them and stick to the results obtainable by ranef().
>
> Spencer Graves wrote:
>
> >       Have you tried RSiteSearch("MLWin")?  I just got 29 hits.  I
> > wonder if any one of these might relate to your question?
> >
> >       If you would like more help on this issue from this listserve,
> > please submit another post, preferably illustrating your question with
> > the simplest possible self-contained example that illustrates your
> > question, perhaps like the following:
> >
> >       fm1.16 <- lme(distance~age, data=Orthodont[1:16,],
> >            random=~age|Subject)
> >
> >       Hope this helps.
> >       Spencer Graves
> > p.s.  PLEASE do read the posting guide
> > "www.R-project.org/posting-guide.html" and provide commented, minimal,
> > self-contained, reproducible code.
> >
> > Dirk Enzmann wrote:
> >
> >> Using ranef() (package nlme, version 3.1-75) with an 'lme' object I
> >> can obtain random effects for intercept and slope of a certain level
> >> (say: 1) - this corresponds to (say level 1) "residuals" in MLWin.
> >> Maybe I'm mistaken here, but the results are identical.
> >>
> >> However, if I try to get the standardized random effects adding the
> >> paramter "standard=T" to the specification of ranef(), the results
> >> differ considerably from the results of MLWin (although MLWin defines
> >> "standardized" in the same way as "divided by its estimated
> >> (diagnostic) standard error").
> >>
> >> Why do the results differ although the estimates (random effects and
> >> thus their variances) are almost identical? I noticed that lme() does
> >> not compute the standard errors of the variances of the random effects
> >> - for several reasons, but if this is true, how does ranef() calculate
> >> the standardized random effects (the help says: '"standardized" (i.e.
> >> divided by the corresponding estimated standard error)').
> >>
> >> Is there a way to obtain similar results as in MLWin (or: should I
> >> prefer the results of ranef() for certain reasons)?
> >>
> >> Dirk
> >>
> >> -----------------------------
> >> R version: 2.3.1 Patched (2006-06-21 r38367)
>
> --
> *************************************************
> Dr. Dirk Enzmann
> Institute of Criminal Sciences
> Dept. of Criminology
> Edmund-Siemers-Allee 1
> D-20146 Hamburg
> Germany
>
> phone: +49-(0)40-42838.7498 (office)
>         +49-(0)40-42838.4591 (Billon)
> fax:   +49-(0)40-42838.2344
> email: dirk.enzmann at uni-hamburg.de
> www:
> http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bates at stat.wisc.edu  Mon Jul 31 18:53:13 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 31 Jul 2006 11:53:13 -0500
Subject: [R] standardized residuals (random effects) using nlme and ranef
In-Reply-To: <40e66e0b0607310948r3c451e7dve4776b2fc1480db9@mail.gmail.com>
References: <44C527E9.3080400@uni-hamburg.de> <44CCC2DB.4090809@pdf.com>
	<44CE0731.2030008@uni-hamburg.de>
	<40e66e0b0607310948r3c451e7dve4776b2fc1480db9@mail.gmail.com>
Message-ID: <40e66e0b0607310953u177b5b6am4b222aca657e3eed@mail.gmail.com>

I have a cut-and-paste error in this message that I sent a few minutes
ago. I show the evaluation of rr as
> rr <- ranef(model.4b, postVar = TRUE)
when it was
> rr <- ranef(fm1, postVar = TRUE)

I also omitted the evaluation of rr1, which is

rr1 <- rr$class


On 7/31/06, Douglas Bates <bates at stat.wisc.edu> wrote:
> Thank you for providing the reproducible example and the explanation
> of what you are seeking to calculate.
>
> On 7/31/06, Dirk Enzmann <dirk.enzmann at uni-hamburg.de> wrote:
> > As suggested I try another post.
> >
> > First I give a reproducible example. The example data set has been
> > provided by I. Plewis (1997), Statistics in Education. London: Arnold (I
> > include the residuals obtained by MLWin):
> >
> > # ---------------------------------------------
> >
> > library(nlme)
> >
> > # Example data from Plewis
> >
> > BaseURL="http://www.ottersbek.de/"
> > ds32 = read.fwf(paste(BaseURL,'ds32.dat',sep=''),
> >                        widths=c(9,10,10,10,10,10),
> >                        col.names=c('class','pupil','zcurric',
> >                                    'curric','zmath1','math2'),
> >                        colClasses=c('factor','factor','numeric',
> >                                     'numeric','numeric','numeric'))
> >
> > # Random slopes model (p. 44):
> > model.4b = lme(math2~zcurric,random=~zcurric|class,method='ML',data=ds32)
> >
> > # Random effects (intercept and slope residuals) of level 1 (class):
> > RandEff = ranef(model.4b,level=1)
> >
> > # Results of MLWin (c300 = intercept residuals of level "class",
> > #                   c301 = slope residuals of level "class",
> > #                   c304 = standardized intercept residuals,
> > #                   c305 = standardized slope residuals):
> > MLWinRes = read.fwf(paste(BaseURL,'resid_ex32.txt',sep=''),
> >                      widths=c(15,15,15,15),
> >                      col.names=c('c300','c301','c304','c305'),
> >                      colClasses=c('numeric','numeric','numeric','numeric'))
> >
> > # They are identical to the results of MLWin (c300, c301):
> > round(cbind(RandEff,MLWinRes[,1:2]),5)
> >
> > # However, using option "standard" the results differ considerably:
> > round(cbind(ranef(model.4b,level=1,standard=T),MLWinRes[,3:4]),5)
>
> Your summation below of the cause of this inconsistency is correct.
> As Harold explains the "standardized" random effects returned by ranef
> in the nlme package are the "estimates" (actually the "predictors") of
> the random effects divided by the estimated standard deviation of
> those random effects, not by the estimated standard error as stated in
> the documentation.  I will correct the documentation.
>
> That is, "standardized random effects" in the nlme package are
> produced by dividing all the intercept random effects by the same
> number (2.9723) and all the slope random effects by 2.0014.
> > rr1 <- ranef(model.4b)
> > rr2 <- ranef(model.4b, standard = TRUE)
> > rr1/rr2
>           (Intercept)  zcurric
>         8    2.972373 2.001491
>        12    2.972373 2.001491
>        17    2.972373 2.001491
>        22    2.972373 2.001491
>        23    2.972373 2.001491
>        28    2.972373 2.001491
>        29    2.972373 2.001491
>        30    2.972373 2.001491
>        31    2.972373 2.001491
>        32    2.972373 2.001491
>        33    2.972373 2.001491
>        34    2.972373 2.001491
>        35    2.972373 2.001491
>        36    2.972373 2.001491
>        37    2.972373 2.001491
>        38    2.972373 2.001491
>        39    2.972373 2.001491
>        41    2.972373 2.001491
>        42    2.972373 2.001491
>        43    2.972373 2.001491
>        44    2.972373 2.001491
>        45    2.972373 2.001491
>        46    2.972373 2.001491
>        47    2.972373 2.001491
>
> Another way of defining a standardization would be to use what Harold
> Doran and others call the "posterior variance-covariance" of the
> random effects.  On reflection I think I would prefer the term
> "conditional variance" but I use "posterior" below.
>
> Strictly speaking the random effects are not parameters in the model -
> they are unobserved random variables.  Conditional on the values of
> the parameters in the model and on the observed data, the random
> effects are normally distributed with a mean and variance-covariance
> that can be calculated.
>
> Influenced by Harold I added an optional argument "postVar" to the
> ranef extractor function for lmer models (but apparently I failed to
> document it - I will correct that).  If you fit your model with lmer,
> as you do below, you can standardize the  random effects according to
> the conditional variances by
>
> > fm1 <- lmer(math2 ~ zcurric + (zcurric|class), ds32, method='ML')
> > rr <- ranef(model.4b, postVar = TRUE)
> > str(rr)  # shows the structure of the object rr
> Formal class 'ranef.lmer' [package "Matrix"] with 1 slots
>   ..@ .Data:List of 1
>   .. ..$ :`data.frame': 24 obs. of  2 variables:
>   .. .. ..$ (Intercept): num [1:24]  0.860 -1.962 -1.105  4.631 -0.781 ...
>   .. .. ..$ zcurric    : num [1:24]  0.2748 -1.3194  0.0841  0.5958  0.8080 ...
>   .. .. ..- attr(*, "postVar")= num [1:2, 1:2, 1:24]  2.562 -0.585
> -0.585  1.837  3.027 ...
>
> The two sets of conditional standard deviations are
>
> > sqrt(attr(rr1, "postVar")[1,1,])
>  [1] 1.600589 1.739768 1.466261 1.542172 1.942629 1.506112 1.892815 1.665414
>  [9] 1.377917 1.574244 1.755679 2.039292 1.887651 1.451916 1.732030 1.659786
> [17] 1.987397 1.795273 1.542049 1.784441 1.629663 1.753223 1.585520 1.470161
> > sqrt(attr(rr1, "postVar")[2,2,])
>  [1] 1.355537 1.630075 1.506560 1.378282 1.790906 1.310406 1.341185 1.512708
>  [9] 1.397492 1.808535 0.859909 1.867273 1.626857 1.669984 1.520560 1.489380
> [17] 1.989123 1.326610 1.613399 1.074365 1.764128 1.455202 1.489311 1.942117
>
> However, as you have seen, standardizing the conditional means by
> these conditional standard deviations still does not reproduce the
> results from MLWin.
>
> If you can determine what is being calculated by MLWin or if you can
> determine that there is a bug in the lmer calculations I would be
> delighted to hear about it.
>
> Thanks again for a very thorough report and for the reproducible example.
>
> > # ---------------------------------------------
> >
> >  From the RSiteSearch("MLWin") there are two hits that seem to be
> > relevant: One of Douglas Bates
> >
> > http://finzi.psych.upenn.edu/R/Rhelp02a/archive/53097.html
> >
> > where he explains why he regards getting standard errors of the
> > estimates of variance components as not being important. I think (but
> > I'm not sure) that this implies that I should not use standardized
> > residuals, as well.
> >
> > Secondly a post of Roel de Jong
> >
> > http://finzi.psych.upenn.edu/R/Rhelp02a/archive/62280.html
> >
> > However, I can't figure out how I could make use of his suggestion to
> > obtain the standardized residuals I am looking for.
> >
> > A part of the answer has been given in an answer to my previous post by
> > Harold Doran. He showed that the so called "standardized residuals"
> > obtained by ranef() using the option "standard=T" are the residuals
> > devided by the standard deviation of the random effects, not divided by
> > the "corresponding standard error" as stated in the ranef() help - if I
> > understood him correctly.
> >
> > In the multilevel list he also showed how to create a caterpillar plot
> > using lmer() and the errbar() function of Hmisc (I modified his example
> > to adapt it to my data):
> >
> > # ------------------------------------
> >
> > detach(package:nlme)
> > library(Matrix)
> > library(Hmisc)
> >
> > # Fit a sample model:
> > fm1 = lmer(math2 ~ zcurric + (zcurric|class), ds32, method='ML',
> > control=list(gradient=FALSE, niterEM=0))
> >
> > .Call("mer_gradComp", fm1, PACKAGE = "Matrix")
> >
> > # extract random effects:
> > fm1.blup = ranef(fm1)$class
> >
> > #extract variances and multiple by scale factor:
> > fm1.post.var = fm1 at bVar$class * attr(VarCorr(fm1),"sc")**2
> >
> > # posterior variance of slope on fm1:
> > fm1.post.slo = sqrt(fm1.post.var[2,2,])
> >
> > # Slopes:
> > x = fm1.blup[,2]+outer(fm1.post.slo, c(-1.96,0,1.96))
> >
> > # This code creates a catepillar plot using the errbar function:
> > x <- x[order(x[,2]) , ]
> > print(errbar(1:dim(x)[1], x[,2],x[,3],x[,1],ylab='Slopes', xlab='Classes'))
> > abline(h=0)
> >
> > # ------------------------------------
> >
> > Is it correct that I can obtain a respecive plot for the intercepts in a
> > similar way?
> >
> > # ------------------------------------
> >
> > # posterior variance of intercept on fm1.post.int:
> > fm1.post.int = sqrt(fm1.post.var[1,1,])
> >
> > # Intercepts:
> > x = fm1.blup[,1]+outer(fm1.post.int, c(-1.96,0,1.96))
> >
> > # This code creates a catepillar plot using the errbar function
> > x <- x[order(x[,2]) , ]
> > print(errbar(1:dim(x)[1], x[,2],x[,3],x[,1],ylab='Intercepts',
> > xlab='Classes'))
> > abline(h=0)
> >
> > # ------------------------------------
> >
> > To sum up, I can't figure out how MLWin calculates the standardized
> > residuals. But I understand that this is not a question for the R list.
> > Nevertheless, it would help if someone could point me to some arguments
> > why not to use them and stick to the results obtainable by ranef().
> >
> > Spencer Graves wrote:
> >
> > >       Have you tried RSiteSearch("MLWin")?  I just got 29 hits.  I
> > > wonder if any one of these might relate to your question?
> > >
> > >       If you would like more help on this issue from this listserve,
> > > please submit another post, preferably illustrating your question with
> > > the simplest possible self-contained example that illustrates your
> > > question, perhaps like the following:
> > >
> > >       fm1.16 <- lme(distance~age, data=Orthodont[1:16,],
> > >            random=~age|Subject)
> > >
> > >       Hope this helps.
> > >       Spencer Graves
> > > p.s.  PLEASE do read the posting guide
> > > "www.R-project.org/posting-guide.html" and provide commented, minimal,
> > > self-contained, reproducible code.
> > >
> > > Dirk Enzmann wrote:
> > >
> > >> Using ranef() (package nlme, version 3.1-75) with an 'lme' object I
> > >> can obtain random effects for intercept and slope of a certain level
> > >> (say: 1) - this corresponds to (say level 1) "residuals" in MLWin.
> > >> Maybe I'm mistaken here, but the results are identical.
> > >>
> > >> However, if I try to get the standardized random effects adding the
> > >> paramter "standard=T" to the specification of ranef(), the results
> > >> differ considerably from the results of MLWin (although MLWin defines
> > >> "standardized" in the same way as "divided by its estimated
> > >> (diagnostic) standard error").
> > >>
> > >> Why do the results differ although the estimates (random effects and
> > >> thus their variances) are almost identical? I noticed that lme() does
> > >> not compute the standard errors of the variances of the random effects
> > >> - for several reasons, but if this is true, how does ranef() calculate
> > >> the standardized random effects (the help says: '"standardized" (i.e.
> > >> divided by the corresponding estimated standard error)').
> > >>
> > >> Is there a way to obtain similar results as in MLWin (or: should I
> > >> prefer the results of ranef() for certain reasons)?
> > >>
> > >> Dirk
> > >>
> > >> -----------------------------
> > >> R version: 2.3.1 Patched (2006-06-21 r38367)
> >
> > --
> > *************************************************
> > Dr. Dirk Enzmann
> > Institute of Criminal Sciences
> > Dept. of Criminology
> > Edmund-Siemers-Allee 1
> > D-20146 Hamburg
> > Germany
> >
> > phone: +49-(0)40-42838.7498 (office)
> >         +49-(0)40-42838.4591 (Billon)
> > fax:   +49-(0)40-42838.2344
> > email: dirk.enzmann at uni-hamburg.de
> > www:
> > http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Enzmann.html
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From andy_liaw at merck.com  Mon Jul 31 18:54:46 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 31 Jul 2006 12:54:46 -0400
Subject: [R] memory problems when combining randomForests [Broadcast]
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAAB93@usctmx1106.merck.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060731/0c21c88d/attachment.pl 

From mmpapenf at wisc.edu  Mon Jul 31 19:32:47 2006
From: mmpapenf at wisc.edu (Michael Papenfus)
Date: Mon, 31 Jul 2006 12:32:47 -0500
Subject: [R] Functions ,Optim, & Dataframe
Message-ID: <44CE3EBF.8090305@wisc.edu>

I think I need to clarify a little further on my original question.

I have the following two rows of data:
mydat<-data.frame(d1=c(3,5),d2=c(6,10),p1=c(.55,.05),p2=c(.85,.35))
 >mydat
  d1 d2 p1 p2
1 3 6 0.55 0.85
2 5 10 0.05 0.35

I need to optimize the following function using  optim for each row in mydat
fr<-function(x) {
    u<-x[1]
    v<-x[2]
    sqrt(sum((plnorm(c(d1,d2,u,v)-c(p1,p2))^2))
}
x0<-c(1,1)    # starting values for two unknown parameters
y<-optim(x0,fr)

In my defined function fr, (d1 d2 p1 p2) are known values which I need 
to read in from my dataframe and u & v are the TWO unknown parameters.  
I want to solve this equation for each row of my dataframe.

I can get this to work when I manually plug in the known values (d1 d2 
p1 p2).  However, I would like to apply this to each row in my dataframe 
where the known values are automatically passed to my function which 
then is sent to optim which solves for the two unknown parameters for 
each row in the dataframe.

thanks again,
mike

-- 

mmpapenf at wisc.edu


From Lbrannma at yahoo.com  Mon Jul 31 19:35:46 2006
From: Lbrannma at yahoo.com (LL)
Date: Mon, 31 Jul 2006 19:35:46 +0200
Subject: [R] Sweave error in example code
Message-ID: <000601c6b4c7$c2fd4e70$6500a8c0@mynewbox>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060731/7f5fd3a3/attachment.pl 

From bates at stat.wisc.edu  Mon Jul 31 19:38:31 2006
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 31 Jul 2006 12:38:31 -0500
Subject: [R] Random Effects Model with Interacting Covariates
In-Reply-To: <b9fe0e51e311121f03b0b8768caf9c74@bham.ac.uk>
References: <b9fe0e51e311121f03b0b8768caf9c74@bham.ac.uk>
Message-ID: <40e66e0b0607311038m5efffefah1cb387692a257475@mail.gmail.com>

On 7/31/06, Dov Stekel <d.j.stekel at bham.ac.uk> wrote:
> Hi
>
> I have been asked by a colleague to perform a statistical analysis
> which uses random effects - but I am struggling to get this to work
> with nlme in R. Help would be very much appreciated!
>
> Essentially, the data consists of:
>
> 10 patients. Each patient has been given three different treatments (on
> three separate days). 15 measurements (continuous variable) have been
> taken from each patient both before and after each of the treatments.
> So the data looks like:
>
> Patient When    Treat   Measurement
> a               before  A               10.3
> a               before  A               11.2
> ...
> a               after           A               12.4
> ...
> a               before  B               11.6
> ...
> a               after           B               ...
>
> and the same for treatment C, patients, b,c,d, etc.
>
> My colleague would like to test to see if the treatments are different
> from each other. i.e., is the change (before to after) due to the
> treatments different between the treatments. It would seem to me like a
> random effects model in which we are interested in the significance of
> the interaction terms Treat:When, with repeated measures in the
> patients (who are random effects, but crossed with the covariates).
> Unfortunately, the groupedData formula only lets me put a single
> covariate on the LHS - nothing as complicated as this!

I'm not sure I understand what the LHS of a formula for a groupedData
object has to do with your question.

You will need to specify the model that you wish to fit by lme and,
for that, you will need to decide which terms should be fixed effects
and which random effects.  Do you think that the patients contribute
only an additive shift in the response or do you think that the
patients may have different initial values and different levels of
change in the Before/After responses?

It seems that you could begin by fitting

fm1 <- lme(Measurement ~ When*Treat, random = ~ 1 | Patient, data = ...)

and

fm2 <- lme(Measurement ~ When*Treat, random = ~ 1|Patient/When, data = ...)

There are many other variations that you could consider but we can
only guess at because we don't know enough of the context of the data.
 For example, it is possible that it would be appropriate to eliminate
a main effect for Treat because the Treatment cannot be expected to
influence the measurement before the Treatment is applied.  The
fixed-effects term would then be specified as

fm3 <- lme(Measurement ~ When + When:Treat, random = ...)

>
> I could, of course, advise her to simply combine all 30 data points for
> each treatment in each patient into a single number (representing
> difference between before and after), but is there a way to use all the
> data in an LME?
>
> Thanks!
>
>
> Dov
>
>
>
> **************************************************************
>
> Dr Dov Stekel
> Lecturer in Bioinformatics
> School of Biosciences
> University of Birmingham
> Birmingham B15 2TT
> Tel: +44 121 414 4209
> Email: d.j.stekel at bham.ac.uk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tplate at acm.org  Mon Jul 31 19:52:14 2006
From: tplate at acm.org (Tony Plate)
Date: Mon, 31 Jul 2006 11:52:14 -0600
Subject: [R] Functions ,Optim, & Dataframe
In-Reply-To: <44CE3EBF.8090305@wisc.edu>
References: <44CE3EBF.8090305@wisc.edu>
Message-ID: <44CE434E.9070207@acm.org>

Supply your additional arguments to optim() and they will get passed to 
your function:

 > mydat<-data.frame(d1=c(3,5),d2=c(6,10),p1=c(.55,.05),p2=c(.85,.35))
 >
 > fr<-function(x, d) {
+     # d is a vector of d1, d2, p1 & p2
+     u <- x[1]
+     v <- x[2]
+     d1 <- d[1]
+     d2 <- d[2]
+     p1 <- d[3]
+     p2 <- d[4]
+     sqrt(sum((plnorm(c(d1,d2,u,v)-c(p1,p2))^2)))
+ }
 > x0 <- c(1,1)    # starting values for two unknown parameters
 > y1 <- optim(x0,fr,d=unlist(mydat[1,]))
 > y2 <- optim(x0,fr,d=unlist(mydat[2,]))
 > y1$par
[1] 0.462500 0.828125
 > y2$par
[1] -1.0937500  0.2828125
 > yall <- apply(mydat, 1, function(d) optim(x0,fr,d=d))
 > yall[[1]]$par
[1] 0.462500 0.828125
 > yall[[2]]$par
[1] -1.0937500  0.2828125
 >

One thing you must be careful of is that none of the arguments to your 
function match or partially match the named arguments of optim(), which are:
 > names(formals(optim))
[1] "par"     "fn"      "gr"      "method"  "lower"   "upper"   "control"
[8] "hessian" "..."
 >

For example, if your function has an argument 'he=', you will not be 
able to pass it, because if you say optim(x0, fr, he=3), the 'he' will 
match the 'hessian=' argument of optim(), and it will not be interpreted 
as being a '...' argument.

-- Tony Plate

Michael Papenfus wrote:
> I think I need to clarify a little further on my original question.
> 
> I have the following two rows of data:
> mydat<-data.frame(d1=c(3,5),d2=c(6,10),p1=c(.55,.05),p2=c(.85,.35))
>  >mydat
>   d1 d2 p1 p2
> 1 3 6 0.55 0.85
> 2 5 10 0.05 0.35
> 
> I need to optimize the following function using  optim for each row in mydat
> fr<-function(x) {
>     u<-x[1]
>     v<-x[2]
>     sqrt(sum((plnorm(c(d1,d2,u,v)-c(p1,p2))^2))
> }
> x0<-c(1,1)    # starting values for two unknown parameters
> y<-optim(x0,fr)
> 
> In my defined function fr, (d1 d2 p1 p2) are known values which I need 
> to read in from my dataframe and u & v are the TWO unknown parameters.  
> I want to solve this equation for each row of my dataframe.
> 
> I can get this to work when I manually plug in the known values (d1 d2 
> p1 p2).  However, I would like to apply this to each row in my dataframe 
> where the known values are automatically passed to my function which 
> then is sent to optim which solves for the two unknown parameters for 
> each row in the dataframe.
> 
> thanks again,
> mike
>


From sundar.dorai-raj at pdf.com  Mon Jul 31 19:52:57 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 31 Jul 2006 12:52:57 -0500
Subject: [R] Sweave error in example code
In-Reply-To: <000601c6b4c7$c2fd4e70$6500a8c0@mynewbox>
References: <000601c6b4c7$c2fd4e70$6500a8c0@mynewbox>
Message-ID: <44CE4379.5080204@pdf.com>



LL wrote:
> Hi.. I am running R version 2.3.1 on a Windows XP machine with the latest Miktex 2.5 installed. I get no errors from R when running the Sweave example, 
> 
> testfile <- system.file("Sweave", "Sweave-test-1.Rnw", package = "utils")
> 
> However, when I tex the resulting .tex file (after installing a4.sty) I get the error below. 
> 
> This is pdfeTeX, Version 3.141592-1.30.6-2.2 (MiKTeX 2.5 RC 1)
> entering extended mode
> (Sweave-test-1.tex
> LaTeX2e <2005/12/01>
> Babel <v3.8g> and hyphenation patterns for english, dumylang, nohyphenation, ge
> rman, ngerman, french, loaded.
> ("C:\Program Files\MiKTeX 2.5\tex\latex\base\article.cls"
> Document Class: article 2005/09/16 v1.4f Standard LaTeX document class
> ("C:\Program Files\MiKTeX 2.5\tex\latex\base\size10.clo"))
> ("C:\Program Files\MiKTeX 2.5\tex\latex\ltxmisc\a4wide.sty"
> ("C:\Program Files\MiKTeX 2.5\tex\latex\ntgclass\a4.sty"))
> ! Missing \endcsname inserted.
> <to be read again> 
>                    \protect 
> l.11 \begin
>            {document}
> ? 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


This works for me. However, when I ran this, MiKTeX prompted me to 
install the ntgclass package, which I did. Everything ran smoothly after 
that. I'm using R-2.3.1 with MiKTeX 2.4 in WinXP Pro.

HTH,

--sundar


From d.j.stekel at bham.ac.uk  Mon Jul 31 20:02:51 2006
From: d.j.stekel at bham.ac.uk (Dov Stekel)
Date: Mon, 31 Jul 2006 19:02:51 +0100
Subject: [R] Random Effects Model with Interacting Covariates
In-Reply-To: <40e66e0b0607311038m5efffefah1cb387692a257475@mail.gmail.com>
References: <b9fe0e51e311121f03b0b8768caf9c74@bham.ac.uk>
	<40e66e0b0607311038m5efffefah1cb387692a257475@mail.gmail.com>
Message-ID: <493d844a5a0b701a0ea138923bae2d29@bham.ac.uk>

Douglas

That's very helpful! It's just a syntax error in my use of lme (I find 
the documentation hard to figure!). I'm actually also using the formula

lme(Measurement~Treatment/When etc)

as this gives the right contrasts to look at the interactions between 
each of the treatments and before/after. I'm still working on a model 
formula that will give me a single p-value for 'is the difference 
between before and after different for different treatments'.

And this all feels much happier than not using a random effects model 
and simply using patient as a blocking variable (i.e. Measurement ~ 
Treat/When + Patient) which seems unsatisfactory for independence 
reasons. (I'm not really a statistician - just the most stats-savvy 
person in my department!)

Thanks,

Dov


On 31 Jul 2006, at 18:38, Douglas Bates wrote:

> On 7/31/06, Dov Stekel <d.j.stekel at bham.ac.uk> wrote:
>> Hi
>>
>> I have been asked by a colleague to perform a statistical analysis
>> which uses random effects - but I am struggling to get this to work
>> with nlme in R. Help would be very much appreciated!
>>
>> Essentially, the data consists of:
>>
>> 10 patients. Each patient has been given three different treatments 
>> (on
>> three separate days). 15 measurements (continuous variable) have been
>> taken from each patient both before and after each of the treatments.
>> So the data looks like:
>>
>> Patient When    Treat   Measurement
>> a               before  A               10.3
>> a               before  A               11.2
>> ...
>> a               after           A               12.4
>> ...
>> a               before  B               11.6
>> ...
>> a               after           B               ...
>>
>> and the same for treatment C, patients, b,c,d, etc.
>>
>> My colleague would like to test to see if the treatments are different
>> from each other. i.e., is the change (before to after) due to the
>> treatments different between the treatments. It would seem to me like 
>> a
>> random effects model in which we are interested in the significance of
>> the interaction terms Treat:When, with repeated measures in the
>> patients (who are random effects, but crossed with the covariates).
>> Unfortunately, the groupedData formula only lets me put a single
>> covariate on the LHS - nothing as complicated as this!
>
> I'm not sure I understand what the LHS of a formula for a groupedData
> object has to do with your question.
>
> You will need to specify the model that you wish to fit by lme and,
> for that, you will need to decide which terms should be fixed effects
> and which random effects.  Do you think that the patients contribute
> only an additive shift in the response or do you think that the
> patients may have different initial values and different levels of
> change in the Before/After responses?
>
> It seems that you could begin by fitting
>
> fm1 <- lme(Measurement ~ When*Treat, random = ~ 1 | Patient, data = 
> ...)
>
> and
>
> fm2 <- lme(Measurement ~ When*Treat, random = ~ 1|Patient/When, data = 
> ...)
>
> There are many other variations that you could consider but we can
> only guess at because we don't know enough of the context of the data.
> For example, it is possible that it would be appropriate to eliminate
> a main effect for Treat because the Treatment cannot be expected to
> influence the measurement before the Treatment is applied.  The
> fixed-effects term would then be specified as
>
> fm3 <- lme(Measurement ~ When + When:Treat, random = ...)
>
>>
>> I could, of course, advise her to simply combine all 30 data points 
>> for
>> each treatment in each patient into a single number (representing
>> difference between before and after), but is there a way to use all 
>> the
>> data in an LME?
>>
>> Thanks!
>>
>>
>> Dov
>>
>>
>>
>> **************************************************************
>>
>> Dr Dov Stekel
>> Lecturer in Bioinformatics
>> School of Biosciences
>> University of Birmingham
>> Birmingham B15 2TT
>> Tel: +44 121 414 4209
>> Email: d.j.stekel at bham.ac.uk
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>

**************************************************************

Dr Dov Stekel
Lecturer in Bioinformatics
School of Biosciences
University of Birmingham
Birmingham B15 2TT
Tel: +44 121 414 4209
Email: d.j.stekel at bham.ac.uk


From d.j.stekel at bham.ac.uk  Mon Jul 31 20:09:01 2006
From: d.j.stekel at bham.ac.uk (Dov Stekel)
Date: Mon, 31 Jul 2006 19:09:01 +0100
Subject: [R] Random Effects Model with Interacting Covariates
In-Reply-To: <493d844a5a0b701a0ea138923bae2d29@bham.ac.uk>
References: <b9fe0e51e311121f03b0b8768caf9c74@bham.ac.uk>
	<40e66e0b0607311038m5efffefah1cb387692a257475@mail.gmail.com>
	<493d844a5a0b701a0ea138923bae2d29@bham.ac.uk>
Message-ID: <47009795dc5c4fea1249356ffd55a7e5@bham.ac.uk>


Ooh,

> lme(Measurement~Treatment/When etc)
>

and

  lm(Measurement ~ Treat/When + Patient)

give exactly the same results! How interesting!


Dov


> which seems unsatisfactory for independence
> reasons. (I'm not really a statistician - just the most stats-savvy
> person in my department!)
>
> Thanks,
>
> Dov
>
>
> On 31 Jul 2006, at 18:38, Douglas Bates wrote:
>
>> On 7/31/06, Dov Stekel <d.j.stekel at bham.ac.uk> wrote:
>>> Hi
>>>
>>> I have been asked by a colleague to perform a statistical analysis
>>> which uses random effects - but I am struggling to get this to work
>>> with nlme in R. Help would be very much appreciated!
>>>
>>> Essentially, the data consists of:
>>>
>>> 10 patients. Each patient has been given three different treatments
>>> (on
>>> three separate days). 15 measurements (continuous variable) have been
>>> taken from each patient both before and after each of the treatments.
>>> So the data looks like:
>>>
>>> Patient When    Treat   Measurement
>>> a               before  A               10.3
>>> a               before  A               11.2
>>> ...
>>> a               after           A               12.4
>>> ...
>>> a               before  B               11.6
>>> ...
>>> a               after           B               ...
>>>
>>> and the same for treatment C, patients, b,c,d, etc.
>>>
>>> My colleague would like to test to see if the treatments are 
>>> different
>>> from each other. i.e., is the change (before to after) due to the
>>> treatments different between the treatments. It would seem to me like
>>> a
>>> random effects model in which we are interested in the significance 
>>> of
>>> the interaction terms Treat:When, with repeated measures in the
>>> patients (who are random effects, but crossed with the covariates).
>>> Unfortunately, the groupedData formula only lets me put a single
>>> covariate on the LHS - nothing as complicated as this!
>>
>> I'm not sure I understand what the LHS of a formula for a groupedData
>> object has to do with your question.
>>
>> You will need to specify the model that you wish to fit by lme and,
>> for that, you will need to decide which terms should be fixed effects
>> and which random effects.  Do you think that the patients contribute
>> only an additive shift in the response or do you think that the
>> patients may have different initial values and different levels of
>> change in the Before/After responses?
>>
>> It seems that you could begin by fitting
>>
>> fm1 <- lme(Measurement ~ When*Treat, random = ~ 1 | Patient, data =
>> ...)
>>
>> and
>>
>> fm2 <- lme(Measurement ~ When*Treat, random = ~ 1|Patient/When, data =
>> ...)
>>
>> There are many other variations that you could consider but we can
>> only guess at because we don't know enough of the context of the data.
>> For example, it is possible that it would be appropriate to eliminate
>> a main effect for Treat because the Treatment cannot be expected to
>> influence the measurement before the Treatment is applied.  The
>> fixed-effects term would then be specified as
>>
>> fm3 <- lme(Measurement ~ When + When:Treat, random = ...)
>>
>>>
>>> I could, of course, advise her to simply combine all 30 data points
>>> for
>>> each treatment in each patient into a single number (representing
>>> difference between before and after), but is there a way to use all
>>> the
>>> data in an LME?
>>>
>>> Thanks!
>>>
>>>
>>> Dov
>>>
>>>
>>>
>>> **************************************************************
>>>
>>> Dr Dov Stekel
>>> Lecturer in Bioinformatics
>>> School of Biosciences
>>> University of Birmingham
>>> Birmingham B15 2TT
>>> Tel: +44 121 414 4209
>>> Email: d.j.stekel at bham.ac.uk
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>
> **************************************************************
>
> Dr Dov Stekel
> Lecturer in Bioinformatics
> School of Biosciences
> University of Birmingham
> Birmingham B15 2TT
> Tel: +44 121 414 4209
> Email: d.j.stekel at bham.ac.uk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

**************************************************************

Dr Dov Stekel
Lecturer in Bioinformatics
School of Biosciences
University of Birmingham
Birmingham B15 2TT
Tel: +44 121 414 4209
Email: d.j.stekel at bham.ac.uk


From helprhelp at gmail.com  Mon Jul 31 20:18:28 2006
From: helprhelp at gmail.com (Weiwei Shi)
Date: Mon, 31 Jul 2006 13:18:28 -0500
Subject: [R] memory problems when combining randomForests
In-Reply-To: <cdf817830607310838lce85b1dy6107884b7f25561e@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA02AAA982@usctmx1106.merck.com>
	<1154339772.44cdd3bc767b7@webmail.cryst.bbk.ac.uk>
	<cdf817830607310838lce85b1dy6107884b7f25561e@mail.gmail.com>
Message-ID: <cdf817830607311118x1cd307c8sb83786467a51b5fd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060731/e7b7dbcb/attachment.pl 

From tplate at acm.org  Mon Jul 31 21:05:15 2006
From: tplate at acm.org (Tony Plate)
Date: Mon, 31 Jul 2006 13:05:15 -0600
Subject: [R] Functions ,Optim, & Dataframe
In-Reply-To: <44CE3EBF.8090305@wisc.edu>
References: <44CE3EBF.8090305@wisc.edu>
Message-ID: <44CE546B.1000908@acm.org>

I added an example of passing additional arguments through optim() to 
the objective and gradient functions to the Discussion section of the 
Wiki-fied R documentation.  See it at 
http://wiki.r-project.org/rwiki/doku.php?id=rdoc:stats:optim

-- Tony Plate

PS.  I had to add "&purge=true" to the end of the URL, i.e., 
http://wiki.r-project.org/rwiki/doku.php?id=rdoc:stats:optim&purge=true 
in order to see the original documentation the first time -- it's 
something to do with bad cache entries for the page.

Michael Papenfus wrote:
> I think I need to clarify a little further on my original question.
> 
> I have the following two rows of data:
> mydat<-data.frame(d1=c(3,5),d2=c(6,10),p1=c(.55,.05),p2=c(.85,.35))
>  >mydat
>   d1 d2 p1 p2
> 1 3 6 0.55 0.85
> 2 5 10 0.05 0.35
> 
> I need to optimize the following function using  optim for each row in mydat
> fr<-function(x) {
>     u<-x[1]
>     v<-x[2]
>     sqrt(sum((plnorm(c(d1,d2,u,v)-c(p1,p2))^2))
> }
> x0<-c(1,1)    # starting values for two unknown parameters
> y<-optim(x0,fr)
> 
> In my defined function fr, (d1 d2 p1 p2) are known values which I need 
> to read in from my dataframe and u & v are the TWO unknown parameters.  
> I want to solve this equation for each row of my dataframe.
> 
> I can get this to work when I manually plug in the known values (d1 d2 
> p1 p2).  However, I would like to apply this to each row in my dataframe 
> where the known values are automatically passed to my function which 
> then is sent to optim which solves for the two unknown parameters for 
> each row in the dataframe.
> 
> thanks again,
> mike
>


From fs at fs-analyse.dk  Mon Jul 31 21:19:15 2006
From: fs at fs-analyse.dk (=?ISO-8859-1?Q?Finn_Sand=F8?=)
Date: Mon, 31 Jul 2006 21:19:15 +0200
Subject: [R] read.spss  'error reading system-file header'
Message-ID: <44CE57B3.9090901@fs-analyse.dk>

When I try to import an spss sav file with read.spss() I am getting the 
following error
'Error in read.spss("X:\\xxxx.sav") : error reading system-file header' 
and the import process is aborted.
I have tried in v. 2.3.0 and 2.3.1
The sav-file loads without problems in spss v14 I have tried saving in 
older spss v7 but are getting the same result.
The read.spss() has other errors (the 'Unrecognized record type 7, 
subtype 7 encountered in system file') but it does not seem to have any 
impact.
This leads me to thinking that the spss.read() slowly is growing out of 
date which would be sad.
So question 1:
Does anyone know if these problems are going to be solved? I know the 
read.spss() function is build on the PSPP project so maybe it takes 
someone with c-knowledge to do something about it.
If someone is going to work on the problem I will be happy to help by 
testing and providing problematic test-files.
Question 2
Is there some way to import spss-sav files in this case other than save 
in a non-spss format?
Regards
FS


From xwang at aviaradx.com  Mon Jul 31 21:27:39 2006
From: xwang at aviaradx.com (Xianqun (Wilson) Wang)
Date: Mon, 31 Jul 2006 12:27:39 -0700
Subject: [R] random effects with lmer() and lme(), three random factors
In-Reply-To: <40e66e0b0607290851h6bb774c9we18072da15e8b574@mail.gmail.com>
Message-ID: <425B10826271C048A4F65FC6C810F5D60142B7@mail.arcturusag.local>

Dr. Bates,

Thanks for the notes! It helps. So now I see consistent resluts from
both lme and lmer. Since I have several response variables to look at, I
will reduce the model separately. 

Speaking of the model reduction, it is clear in this example that the
trivial variance of Operator:Run could be ignored. For a
non-trivial-variance multivariate model, I wonder if there is any
function (like step function for lm and glm) would work with lme or lmer
and allow me to do the AIC-based model selection.   


Wilson
  

-----Original Message-----
From: dmbates at gmail.com [mailto:dmbates at gmail.com] On Behalf Of Douglas
Bates
Sent: Saturday, July 29, 2006 7:51 AM
To: Xianqun (Wilson) Wang
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] random effects with lmer() and lme(), three random
factors

On 7/28/06, Xianqun (Wilson) Wang <xwang at aviaradx.com> wrote:
> Hi, all,

> I have a question about random effects model. I am dealing with a 
> three-factor experiment dataset. The response variable y is modeled 
> against three factors: Samples, Operators, and Runs. The experimental 
> design is as follow:

> 4 samples were randomly chosen from a large pool of test samples. Each

> of the 4 samples was analyzed by 4 operators, randomly selected from a

> group of operators. Each operator independently analyzed same samples 
> over 5 runs (runs nested in operator). I would like to know the 
> following things:

> (1)                     the standard deviation within each run;

> (2)                     the standard deviation between runs;

> (3)                     the standard deviation within operator

> (4)                     the standard deviation between operator.

> With this data, I assumed the three factors are all random effects. So

> the model I am looking for is

> Model:  y  = Samples(random) + Operator(random) + Operator:Run(random)

> +
> Error(Operator) + Error(Operator:Run)  + Residuals

> I am using lme function in nlme package. Here is the R code I have

> 1.       using lme:

> First I created a grouped data using

> gx <- groupedData(y ~ 1 | Sample, data=x)

> gx$dummy <- factor(rep(1,nrow(gx)))

> then I run the lme

> fm<- lme(y ~ 1, data=gx,
> random=list(dummy=pdBlocked(list(pdIdent(~Sample-1),
>             pdIdent(~Operator-1),
>             pdIdent(~Operator:Run-1)))))

>     finally, I use VarCorr to extract the variance components

>            vc <- VarCorr(fm)
>                      Variance           StdDev
> Operator:Run 1.595713e-10:20   1.263215e-05:20
> Sample       5.035235e+00: 4   2.243933e+00: 4
> Operator     5.483145e-04: 4   2.341612e-02: 4
> Residuals    8.543601e-02: 1   2.922944e-01: 1

> 2.      Using lmer in Matrix package:

> fm <- lmer(y ~ (1 | Sample) + (1 | Operator) +
>            (1|Operator:Run), data=x)

That model specification can now be written as

fm <- lmer(y ~ (1|Sample) + (1|Operator/Run), x)

>      summary(fm)

> Linear mixed-effects model fit by REML
> Formula: H.I.Index ~ (1 | Sample.Name) + (1 | Operator) + (1 |
> Operator:Run)
>           Data: x
>       AIC      BIC    logLik MLdeviance REMLdeviance
>  96.73522 109.0108 -44.36761   90.80064     88.73522
> Random effects:
>  Groups       Name        Variance   Std.Dev.
>  Operator:Run (Intercept) 4.2718e-11 6.5359e-06
>  Operator     (Intercept) 5.4821e-04 2.3414e-02
>  Sample       (Intercept) 5.0352e+00 2.2439e+00
>  Residual                 8.5436e-02 2.9229e-01
> number of obs: 159, groups: Operator:Run, 20; Operator, 4; 
> Sample.Name,
> 4

> Fixed effects:
>              Estimate Std. Error  t value
> (Intercept) 0.0020818  1.1222683 0.001855

> There is a difference between lmer and lme is for the factor 
> Operator:Run.

It's just a matter of round-off.  It is possible for the ML or REML
estimates of a variance component to be zero, as is the case here, but
the current computational methods do not allow the value zero because
this will cause some of the matrix decompositions to fail.  In lmer we
use a constrained optimization with the relative variance (variance of a
random effect divided by the residual variance) constrained to be
greater than or equal to 5e-10, which is exactly the value you have
here.

I'll add code to the model fitting routine to warn the user when
convergence to the boundary value occurs.  I haven't done that in the
past because it is not always easy to explain what is occurring.
For a model with variance components only, like yours, convergence on
the boundary means that an estimated variance component is zero.  In the
case of bivariate or multivariate random effects the variance-covariance
matrix can be singular without either of the variances being zero.

The bottom line for you is that the estimated variance for Operator:Run
is zero and you should reduce the model to y ~ (1|Sample)
+ (1|Operator)


I cannot find where the problem is. Could anyone point me
> out if my model specification is correct for the problem I am dealing 
> with. I am pretty new user to lme and lmer. Thanks for your help!
>
>
>
>
>
> Wilson Wang
>
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Mon Jul 31 21:35:47 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 31 Jul 2006 20:35:47 +0100 (BST)
Subject: [R] Sweave error in example code
In-Reply-To: <44CE4379.5080204@pdf.com>
References: <000601c6b4c7$c2fd4e70$6500a8c0@mynewbox>
	<44CE4379.5080204@pdf.com>
Message-ID: <Pine.LNX.4.64.0607312033580.28171@gannet.stats.ox.ac.uk>

On Mon, 31 Jul 2006, Sundar Dorai-Raj wrote:

> 
> 
> LL wrote:
> > Hi.. I am running R version 2.3.1 on a Windows XP machine with the latest Miktex 2.5 installed. I get no errors from R when running the Sweave example, 
> > 
> > testfile <- system.file("Sweave", "Sweave-test-1.Rnw", package = "utils")
> > 
> > However, when I tex the resulting .tex file (after installing a4.sty) I get the error below. 
> > 
> > This is pdfeTeX, Version 3.141592-1.30.6-2.2 (MiKTeX 2.5 RC 1)
> > entering extended mode
> > (Sweave-test-1.tex
> > LaTeX2e <2005/12/01>
> > Babel <v3.8g> and hyphenation patterns for english, dumylang, nohyphenation, ge
> > rman, ngerman, french, loaded.
> > ("C:\Program Files\MiKTeX 2.5\tex\latex\base\article.cls"
> > Document Class: article 2005/09/16 v1.4f Standard LaTeX document class
> > ("C:\Program Files\MiKTeX 2.5\tex\latex\base\size10.clo"))
> > ("C:\Program Files\MiKTeX 2.5\tex\latex\ltxmisc\a4wide.sty"
> > ("C:\Program Files\MiKTeX 2.5\tex\latex\ntgclass\a4.sty"))
> > ! Missing \endcsname inserted.
> > <to be read again> 
> >                    \protect 
> > l.11 \begin
> >            {document}
> > ? 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> 
> This works for me. However, when I ran this, MiKTeX prompted me to 
> install the ntgclass package, which I did. Everything ran smoothly after 
> that. I'm using R-2.3.1 with MiKTeX 2.4 in WinXP Pro.

But he is using MiKTeX 2.5: looks like a problem with MiKTeX, as the latex 
error is in the initial processing.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rajarshi.guha at gmail.com  Mon Jul 31 22:09:26 2006
From: rajarshi.guha at gmail.com (Rajarshi Guha)
Date: Mon, 31 Jul 2006 16:09:26 -0400
Subject: [R] RCurl
Message-ID: <1154376566.5525.83.camel@localhost>

Hi, does anybody know where I might the RCurl package - the omegahat.org
server seems to be down

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rguha at indiana.edu>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Every little picofarad has a nanohenry all its own.
-- Don Vonada


From jaa53 at cornell.edu  Mon Jul 31 22:30:33 2006
From: jaa53 at cornell.edu (Jose Andres)
Date: Mon, 31 Jul 2006 16:30:33 -0400
Subject: [R]  resampling mean distances
In-Reply-To: <Pine.LNX.4.64.0607312033580.28171@gannet.stats.ox.ac.uk>
References: <000601c6b4c7$c2fd4e70$6500a8c0@mynewbox>
	<44CE4379.5080204@pdf.com>
	<Pine.LNX.4.64.0607312033580.28171@gannet.stats.ox.ac.uk>
Message-ID: <01421F01-E8EB-45C7-BBB9-EAA68BCE8693@cornell.edu>

  Hi all,

I am trying to generate a distribution for the mean euclidean  
distance between a group of n elements in a given surface (the  
elements are randomly picked).  Fo doing so I've written the  
following code:

sampling<- function(x,size) {

x<- x[sample(1:nrow(x),size),]

mat<- matrix(c(x$V6,x$V7,x$V8), ncol=3)

mean.dist<- mean(dist(mat,"euclidean"))


}

x is the file where the data are stored
size is the size of the group
mat generates a simple matrix. V6, V7, and V8 are the 3D (x,y,z)  
coordinates of the group elements .
mean.dist  calculates the mean pairwise distance between the objects  
of the group.

Everything works fine but I want  to repeat this many times (e.g.  
10000) and  store the mean.dist values in a new variable so I can   
generate the distribution of mean pairwise distances of a group of  
size n in my surface.

Is there any easy way to do this? I'd really appreciate all your  
comments.

Thanks in advance,

/Jose










On Jul 31, 2006, at 15:35, Prof Brian Ripley wrote:

> On Mon, 31 Jul 2006, Sundar Dorai-Raj wrote:
>
>>
>>
>> LL wrote:
>>> Hi.. I am running R version 2.3.1 on a Windows XP machine with  
>>> the latest Miktex 2.5 installed. I get no errors from R when  
>>> running the Sweave example,
>>>
>>> testfile <- system.file("Sweave", "Sweave-test-1.Rnw", package =  
>>> "utils")
>>>
>>> However, when I tex the resulting .tex file (after installing  
>>> a4.sty) I get the error below.
>>>
>>> This is pdfeTeX, Version 3.141592-1.30.6-2.2 (MiKTeX 2.5 RC 1)
>>> entering extended mode
>>> (Sweave-test-1.tex
>>> LaTeX2e <2005/12/01>
>>> Babel <v3.8g> and hyphenation patterns for english, dumylang,  
>>> nohyphenation, ge
>>> rman, ngerman, french, loaded.
>>> ("C:\Program Files\MiKTeX 2.5\tex\latex\base\article.cls"
>>> Document Class: article 2005/09/16 v1.4f Standard LaTeX document  
>>> class
>>> ("C:\Program Files\MiKTeX 2.5\tex\latex\base\size10.clo"))
>>> ("C:\Program Files\MiKTeX 2.5\tex\latex\ltxmisc\a4wide.sty"
>>> ("C:\Program Files\MiKTeX 2.5\tex\latex\ntgclass\a4.sty"))
>>> ! Missing \endcsname inserted.
>>> <to be read again>
>>>                    \protect
>>> l.11 \begin
>>>            {document}
>>> ?
>>> 	[[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>>> guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>> This works for me. However, when I ran this, MiKTeX prompted me to
>> install the ntgclass package, which I did. Everything ran smoothly  
>> after
>> that. I'm using R-2.3.1 with MiKTeX 2.4 in WinXP Pro.
>
> But he is using MiKTeX 2.5: looks like a problem with MiKTeX, as  
> the latex
> error is in the initial processing.
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sonal at deepfoo.com  Mon Jul 31 22:33:33 2006
From: sonal at deepfoo.com (sonal at deepfoo.com)
Date: Mon, 31 Jul 2006 16:33:33 -0400
Subject: [R] na.rm problem
Message-ID: <20060731202426.M69111@deepfoo.com>

hi,

i am a new member.

i am using R in finding correlation between two variables of unequal length.

when i use 

cor(x,y,na.rm=T,use="complete")

where x has observations from 1928 to 2006 & y has observations from 1950 to
2006. I used na.rm=T to use the "complete observations".  So missing values
should be handled by casewise deletion. But it gives me error

Error in cov(close, close1, na.rm = T, use = "complete") : 
        unused argument(s) (na.rm ...)


Please help me with this as I am new to R.

Thanks,
Sonal


From sonal at deepfoo.com  Mon Jul 31 22:37:47 2006
From: sonal at deepfoo.com (sonal at deepfoo.com)
Date: Mon, 31 Jul 2006 16:37:47 -0400
Subject: [R] Fw:  na.rm problem
In-Reply-To: <20060731202426.M69111@deepfoo.com>
References: <20060731202426.M69111@deepfoo.com>
Message-ID: <20060731203645.M66654@deepfoo.com>

hi,

i am a new member.

i am using R in finding correlation between two variables of unequal length.

when i use

cor(x,y,na.rm=T,use="complete")

where x has observations from 1928 to 2006 & y has observations from 1950 to
2006. I used na.rm=T to use the "complete observations".  So missing values
should be handled by casewise deletion. But it gives me error

Error in cor(close, close1, na.rm = T, use = "complete") : 
        unused argument(s) (na.rm ...)

Please help me with this as I am new to R.

Thanks,
Sonal


From sfalcon at fhcrc.org  Mon Jul 31 23:39:17 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Mon, 31 Jul 2006 14:39:17 -0700
Subject: [R] RCurl
In-Reply-To: <1154376566.5525.83.camel@localhost> (Rajarshi Guha's message of
	"Mon, 31 Jul 2006 16:09:26 -0400")
References: <1154376566.5525.83.camel@localhost>
Message-ID: <m2k65t8nei.fsf@ziti.fhcrc.org>

Rajarshi Guha <rajarshi.guha at gmail.com> writes:

> Hi, does anybody know where I might the RCurl package - the omegahat.org
> server seems to be down

The Bioconductor project hosts a mirror of a subset of Omegahat
packages (RCurl is included).  You can find the listing here:

http://www.bioconductor.org/packages/release/omegahat/

There is a browsable HTML package listing at the above URL which is
also a valid CRAN-style package repository.

So, for example, you should be able to do:

install.packages("RCurl",
                 repos="http://www.bioconductor.org/packages/release/omegahat/",
                 dependencies=TRUE)


+ seth


From p_connolly at ihug.co.nz  Mon Jul 31 23:47:35 2006
From: p_connolly at ihug.co.nz (Patrick Connolly)
Date: Tue, 1 Aug 2006 09:47:35 +1200
Subject: [R] How does biplot.princomp scale its axes?
Message-ID: <20060731214735.GL12850@ihug.co.nz>

I'm attempting to modify how biplot draws its red vectors (among other
things).  This is how I've started:


Biplot <- function(xx, comps = c(1, 2), cex = c(.6, .4))
{
  ## Purpose: Makes a biplot with princomp() object to not show arrows
  ## ----------------------------------------------------------------------
  ## Arguments: xx is an object made using princomp()
  ## ----------------------------------------------------------------------
scores <- xx$scores[, paste("Comp", comps, sep = ".")]
loadings <- xx$loadings[, paste("Comp", comps, sep = ".")]
plot(range(scores), range(scores), xlab = "", ylab = "", xaxt = "n",
           yaxt = "n", pch = " ")
text(scores[,1], scores[,2], rownames(scores), cex = cex[1])
axis(2)
axis(1)
}

I can make part of a biplot using that function with the USArrests data:
Biplot(princomp(USArrests, cor = TRUE), c(1,2), cex = c(.6, .4))

Compare that with what we get using biplot.princomp:
biplot(princomp(USArrests, cor = TRUE), c(1,2), cex = c(.6, .4))

It seems to me that the y-values are the same in both plots, but some
sort of scaling on the x-axis is happening.  Something similar seems
to happen with the loadings as well.  

I notice in the documentation for biplot, mention is made of "... many
variations on biplots".  Would I be doing something inexcusable if I
ignored the differences I've noticed here?

TIA

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From jrkrideau at yahoo.ca  Mon Jul 31 23:54:28 2006
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 31 Jul 2006 17:54:28 -0400 (EDT)
Subject: [R] if function and  apply
Message-ID: <20060731215428.15996.qmail@web33811.mail.mud.yahoo.com>

Runninn R.2.3.1  Windows XP

I have a dataset just imported from SPSS.  It has any
number of 99's as missing data and it looks like the
next dataset will have custom missing codes. I have
abouat 120 variables and an N of 2000.

"I think" thatI would like to apply a function to the
data.frame (or to a matrix of the data if needed) to
recode all the 99's to NA.  I thought that I could
adapt an example from the list using "apply" but with
no success.  

Is there a decent source of examples of how to write
an if statement on the web? I'm missing something
simple.
Here is an example of what I have been trying.

######  
cat <- c( 3,5,6,8)
dog <- c(3,5,3,6)
rat <- c (5, NA, 4, 9)
mat <- (cbind(cat,dog, rat))
Df <- data.frame(cbind(cat, dog, rat)

#  define function
fn <- function (x a) {
if (x==a)return  (b) else x
}

apply(mat, c(1,2), fn, 99, NA)
#####################

Thanks


